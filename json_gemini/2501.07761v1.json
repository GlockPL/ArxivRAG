{"title": "Impatient Bandits: Optimizing for the Long-Term Without Delay", "authors": ["Kelly W. Zhang", "Thomas Baldwin-McDonald", "Kamil Ciosek", "Lucas Maystre", "Daniel Russo"], "abstract": "Increasingly, recommender systems are tasked with improving users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a bandit problem with delayed rewards. There is an apparent trade-off in choosing the learning signal: waiting for the full reward to become available might take several weeks, slowing the rate of learning, whereas using short-term proxy rewards reflects the actual long-term goal only imperfectly. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Rewards as well as shorter-term surrogate outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that quickly learns to identify content aligned with long-term success using this new predictive model. We prove a regret bound for our algorithm that depends on the Value of Progressive Feedback, an information theoretic metric that captures the quality of short-term leading indicators that are observed prior to the long-term reward. We apply our approach to a podcast recommendation problem, where we seek to recommend shows that users engage with repeatedly over two months. We empirically validate that our approach significantly outperforms methods that optimize for short-term proxies or rely solely on delayed rewards, as demonstrated by an A/B test in a recommendation system that serves hundreds of millions of users.", "sections": [{"title": "1 Introduction", "content": "The multi-armed bandit problem stands as a cornerstone in machine learning, statistics, and operations research, crystallizing the challenge of learning to make effective decisions through intelligent trial and error. In its classical formulation, the model assumes immediate reward observation following an action, facilitating rapid adaptation. However, this instantaneous feedback structure often fails to capture the complexity of real-world applications, particularly in the realm of digital platforms serving millions or billions of users.\nOn these large-scale platforms, algorithms need to make decisions at a rapid pace, since the time between the start of one user's interaction and the start of the next user's interaction is extremely brief. Consequently, decision-making algorithms that optimize for rewards that are observed before the start of the subsequent user interaction (or even before a few interactions) inevitably skew towards extremely short-term goals, such as whether a user clicked on an advertisement. Selecting longer-term outcomes for the reward (like long-term engagement with a recommendation) better aligns with genuine long-term goals. However, waiting even a brief amount of time to observe the reward following each decision can greatly hinder the efficacy of bandit algorithms, since these brief delays can correspond to waiting an enormous number of interactions or \u201crounds\" in classical bandit formulations. In Figure 1, we depict this apparent tradeoff between speed of learning versus the alignment of the reward with the long-term outcome of interest.\nIn this work, we investigate the problem of optimizing for true long-term goals while mitigating the impact of delay. Our key insight is that most long-term outcomes become increasingly predictable over time. In the context of digital platforms, a user's long-term engagement is revealed incrementally. For instance, total spending is signaled by initial purchases, and the likelihood of conversion following a marketing message diminishes as time passes without a response. We term this phenomenon \u201cprogressive feedback,\" distinguishing it from the widely studied delayed feedback, where no information is gleaned until long after an action is taken.\nOur interest in this problem stems from a real challenge encountered at Spotify, a leading audio streaming service. Following a content recommendation decision, engagement feedback from each user is progressively revealed; we illustrate examples of the progressive engagement feedback in Figure 2. Recent efforts to optimize for longer-term metrics, such as user engagement over a 60-day period, brought substantial benefits to the recommendation system, as compared to optimizing for shorter-term outcomes [Maystre et al., 2023, Wang et al., 2022, Zou et al., 2019, Yang et al., 2024]. However, these long-term metrics are only realized after a significant delay, meaning such data is never available for recently released content.\nIn this context, we focus on the cold-start problem-the challenge of rapidly learning to make effective recommendations for newly released content with little to no historical data. The core challenge lies in optimizing for the long-term outcome and identifying content with superior long-term performance, but doing so without waiting for these long-term metrics to become fully revealed. We address this problem in a novel way, by training an (empirical) Bayesian filtering model that draws inferences about content quality and appeal from progressively revealed user engagement. A/B tests have shown that incorporating progressive feedback has large benefits in practice (see Section 8), and this idea is now used in production as a core part of the recommender system at Spotify, which powers personalized audio recommendations for hundreds of millions of users."}, {"title": "1.1 Contributions", "content": "This work expands upon the algorithm originally presented in McDonald et al. [2023]. Key additions include a) a formalization of the model, b) a highly novel regret analysis, c) A/B test results from an implementation of the algorithm at Spotify, and d) several additional synthetic experiments.\nModel of bandits with progressive feedback. Motivated by this real-world problem faced at Spotify and recognizing its broader relevance, we formulate a model termed bandits with progressive feedback (Section 3). In this setting, the reward from selecting an \"arm\" is fully observed only after a significant delay but becomes progressively more predictable over time. We primarily focus on a key special case where rewards are a linear function of a set of correlated Gaussian engagement outcomes. The crux of the problem lies in the interplay between the delay structure, where some outcomes are revealed much earlier than others, and the correlation structure, which determines the extent to which initial observations are predictive of later ones. In our problem setting, the algorithm confronts two sources of limited feedback. The first is bandit feedback (in contrast to full feedback), where the algorithm only has data on items it has recommended in the past, necessitating active exploration for learning. The second is progressive feedback, where engagement outcomes are revealed gradually over time, introducing delay in the resolution of uncertainty regardless of the number of users receiving recommendations.\nImpatient bandit algorithm. To address the above challenges within our model, we propose an algorithm that integrates two key components: Thompson sampling and a (empirical) Bayesian filter (Section 4). Thompson sampling [Thompson, 1933] is employed to tackle the bandit feedback challenge. The Bayesian filter is designed to mitigate the issues arising from progressively revealed feedback. The Bayesian filtering model projects the true average reward of each based on partially observed engagement trajectories, maintaining appropriate residual uncertainty. Both mean and uncertainty estimates are incrementally updated as further feedback is observed. Our model assumes this filter is computed based on highly informed prior beliefs. In practice, this rich prior comes from historical data, by training the hyperparameters of the Bayesian filter on complete trajectories of user interactions with historical content (Section 6)."}, {"title": "2 Related Work", "content": "Surrogate Outcomes. The surrogate outcomes literature uses short-term outcomes as a proxy for long-term outcomes. For example, for a treatment aimed at preventing heart disease, changes in cholesterol level may be used as a surrogate outcome for the distal outcome of developing heart disease in the next year. Most of this literature focuses on estimating causal effects, rather than decision-making. There are three general approaches to using surrogate outcomes: a) assume a causal relationship between the surrogate and long-term outcome, and effectively replace the distal outcome with surrogates [Athey et al., 2016, Duan et al., 2021, Prentice, 1989, Fleming and Powers, 2012, Weintraub et al., 2015, VanderWeele, 2013], b) use surrogates that are predictive of the long-term outcome to improve imputation of missing distal outcomes [Kallus and Mao, 2020, Cheng et al., 2021], and c) use surrogates that are predictive of the long-term outcome to fit a joint Bayesian model of these outcomes [Tripuraneni et al., 2024, Richardson et al., 2023, Anderer et al., 2022]. Our work is most similar in spirit to that last approach, with a focus on decision-making.\nOptimizing for Long-Term Outcomes in Recommendation Systems. In the recommendation systems literature, there are two foremost approaches to designing algorithms that optimize for long-term outcomes. The first focuses on designing algorithms that account for the delayed effects of actions on the same users over time, e.g., by moving beyond bandit algorithms and designing algorithms that account for how a recommendation decision today may affect that same user's future engagement [Xu et al., 2023, Wu et al., 2017, Zheng et al., 2018, Ji et al., 2021]. The second approach involves designing algorithms that optimize for long-term, delayed rewards and incorporate short-term outcomes in some way to speed up learning, e.g., by using the short-term outcome as a surrogate or combining short-term and long-term outcomes in the definition of the reward [Wang et al., 2022, Zou et al., 2019, Liu et al., 2024, Zhang et al., 2021]; None of these existing works focus on the cold-start recommendation setting in which there is little to no historical data for certain actions (the focus of this work). With the exception of Zhang et al. [2021], which we discuss below, these existing works develop offline RL methods, which use large datasets in which each action has been played many times already.\nOnline Decision-Making Algorithms that use Intermediate Outcomes. Delayed rewards are a significant challenge in online decision-making settings, and many bandit algorithms have been developed specifically to accommodate such delays [Howson et al., 2023, Mandel et al., 2015, Desautels et al., 2014, Bistritz et al., 2019, Thune et al., 2019, Vernade et al., 2020, Pike-Burke et al., 2018, Joulani et al., 2013, Zhou et al., 2019]. We approach the delayed rewards problem by incorporating intermediate, surrogate outcomes that may be predictive of the reward. Several previous works also consider algorithms with access to intermediate outcomes, but consider different problem settings and/or make different assumptions on the type of intermediate feedback.\nAnderer et al. [2022] use surrogate outcomes to design a Bayesian adaptive clinical trial which adaptively decides whether to stop the trial at a pre-specified decision point, subject to a power constraint. Wu and Wager [2022] develop a Thompson Sampling algorithm for survival outcomes based on the proportional hazards model, which leverages incrementally revealed information on how long individuals have survived so far. Grover et al. [2018] consider a UCB algorithm for best arm identification in a setting with stochastically delayed rewards in which the next decision cannot be made until the delayed reward is observed; this delay structure differs fundamentally from ours. Moreover, they assume that their algorithm sees i.i.d. intermediate outcomes (which may be biased or unbiased estimates of the mean reward) while it waits, which does not accommodate progressively revealed user engagement feedback (see Figure 2), which critically are dependent over time.\nYang et al. [2024] apply a surrogate index approach [Athey et al., 2016] to develop a Thompson sampling algorithm that optimizes a fitted function of short-term outcomes instead of the true long-term reward (customer retention). While they assume short-term outcomes act as perfect surrogates for the long-term reward, our approach incorporates imperfect leading indicators. Finally, Zhang et al. [2021] use importance weighting to ensure the short-term surrogate is an unbiased estimate of the mean, long-term reward. Their algorithm (developed for a setting with binary outcomes and a single intermediate outcome) does not easily extend to settings with many intermediate outcomes (> 50 in our applications) that are continuous, i.e., where estimating propensities to form importance weights is more difficult.\nDistinguishing features of our work. Our work advances the literature in several key ways. First, unlike most existing models which treat feedback as either delayed or immediate, or assume perfect surrogate outcomes, we formalize a model of progressive feedback where information becomes incrementally more predictive over time. Our model addresses the practical challenge of synthesizing information from many (> 50) sequentially revealed intermediate signals that serve as imperfect leading indicators rather than perfect surrogates of true rewards. Second, we introduce a"}, {"title": "3 Problem Formulation", "content": "We now formally describe our problem setup, which is motivated by recommender systems applications. In our problem setup, a large batch of actions (items) are selected at each decision time. Specifically, each decision time $t \\in \\mathbb{N}$ represents a day during which many users, denoted by the set $U_t$, are simultaneously presented with item recommendations. We use $U \\equiv \\bigcup_{t \\in \\mathbb{N}} U_t$ to denote the collection of all users. For simplicity, we assume that the batches $U_t$ are of the same size each day; We use $m = |U_1| = |U_2| = |U_3| = \\ldots$ to denote the batch size. While batching is common in practice for operational reasons, we employ it primarily as a device to clearly distinguish between two crucial aspects of the problem. The first aspect is the passage of time, which is represented by the number of batches. The second is the rate at which exploratory recommendations are made, which is determined by the size of each batch. This model enables us to separately vary the pace of decision-making from the pace at which outcomes are revealed (amount of delay).\nEach day $t\\in \\mathbb{N}$, for each user $u \\in U_t$ in the batch, the system selects an item $A_u$ from a finite collection of items $\\mathcal{A}$ to recommend ($\\mathcal{A}$ is the action space). Each item $a \\in \\mathcal{A}$ has an associated item feature vector $Z_a \\in \\mathcal{Z}$. For example, in the podcast recommendation setting, $Z_a$ may include the category of the podcast show (comedy, news, lifestyle). The item features are used as they may be predictive of or correlated with the item's performance. Note that this vector is not a typical context or state vector, rather it represents features of the item (action). Note that we are able to extend the algorithm we present to settings with user contexts (see Appendix C for details); throughout the main body of this paper we focus on the setting without context for clarity.\nUpon recommending an item $A_u \\in \\mathcal{A}$ for a user $u \\in U_t$, the algorithm subsequently observes a collection of user outcomes $Y_u = (Y_u^{(1)}, Y_u^{(2)}, \\ldots, Y_u^{(J)})$. We use $Y_u = (Y_u^{(1)}, Y_u^{(2)}, \\ldots, Y_u^{(J)}) \\in \\mathbb{R}^J$ to refer to the entire vector of engagement outcomes observed from selecting action $A_u$. In our setting, these outcomes are progressively revealed over time. Specifically, in our application, $(Y_u^{(j)})_{j=1}^J$ refer to measures of engagement with the recommended item that are revealed over $J$ days. An engagement trajectory is associated with a long-term reward $R(Y_u)$ by applying a fixed, known, real-valued, reward function $R(\\cdot)$. For example, the reward could represent the total user engagement over time, $R(Y_u) = \\Sigma_{j=1}^J Y_u^{(j)}$, or if the outcome of interest is the final engagement, one could choose $R(Y_u) = Y_u^{(J)}$. In general, the outcomes $(Y_u^{(j)})_{j=1}^J$ could be a series of predictions of a long-term user outcome from a previously trained model or simply leading indicators that are correlated with a long-term metric of interest.\nFormally, we use $d_j$ to refer to the deterministic \u201cdelay\u201d in seeing the $j$th outcome $Y_u^{(j)}$. For example, if $d_1 = 0, d_2 = 1, \\ldots, d_J = J-1$ and user $u$ is in batch $t$, then $Y_u^{(1)}$ is revealed immediately after selecting the action $A_u$ at time $t$. Following decision time $t+1$, additional engagement information $Y_u^{(2)}$ is observed, and so on until following decision time $t+J-1$ the final engagement $Y_u^{(J)}$ is revealed."}, {"title": "3.1 Counterfactual Outcomes and Learning Objective", "content": "We use the potential outcomes framework [Imbens and Rubin, 2015, Rubin, 1974] to formally represent counterfactual outcomes. We assume there exist latent variables\n$\\{Y_u(a) = (Y_u^{(1)}(a), Y_u^{(2)}(a), \\ldots, Y_u^{(J)}(a))\\}_{a \\in \\mathcal{A}, u \\in \\mathcal{U}}$\ncalled potential engagement outcomes, such that the engagement outcome of selecting action $A_u = a$ for some $a \\in \\mathcal{A}$ is $Y_u(a) = (Y_u^{(1)}(a), Y_u^{(2)}(a), \\ldots, Y_u^{(J)}(a))$; the observed outcome is $Y_u = Y_u(A_u)$. This means that the potential engagement outcome is a function of only the recommendation that user receives and not the recommendations received by other users, ruling out, for instance, complicated social network effects.$^1$\nThe next assumption formalizes a symmetry of the distribution of outcomes among users in the cohort $\\mathcal{U}$.\nAssumption 1 (Exchangeability). For any fixed item $a \\in \\mathcal{A}$, the potential outcome vectors $(Y_u(a))_{u \\in \\mathcal{U}}$ are exchangeable over users $u \\in \\mathcal{U}$ conditional on $Z_a = z$ for any $z \\in \\mathcal{Z}$. That is, the joint distribution satisfies\n$P((Y_u(a))_{u \\in \\mathcal{U}} | (Z_a = z)) \\ = \\ P((Y_{\\sigma(u)}(a))_{u \\in \\mathcal{U}} | (Z_a = z))$\nfor any permutation $\\sigma$ over $\\mathcal{U}$.\nRemark 1 (Interpreting Exchangeability). To interpret this assumption, it is helpful to think of $\\mathcal{U} = \\{1,2,3...\\}$ as a list of user IDs that are randomly assigned to users in the cohort. Then, because of the random assignment, there is nothing a priori to differentiate the distribution of outcomes for two users who receive the same recommendation at the same time (i.e., $u, u' \\in U_t$ with $A_u = A_{u'}$). This part of the assumption is not a major restriction. The meaningful assumed structure is a symmetry in outcomes among users who receive the same recommendation at different points in time. This is critical to allowing us to learn from initial recommendations how to make recommendations to future batches of users.\nBy De Finetti's theorem [Heath and Sudderth, 1976, De Finetti, 1937], the exchangeability assumption is equivalent to assuming that there exists a random, latent (i.e., unobserved) variable $\\theta_a$ such that the user's potential engagements $\\{Y_u(a)\\}_{u \\in \\mathcal{U}}$ are i.i.d. given $\\theta_a, Z_a$. This means there exists a distribution $P_{\\theta_a, Z_a}$ such that\n$Y_1(a), Y_2(a), Y_3(a),... \\ | \\theta_a, Z_a \\ \\overset{i.i.d}{\\sim} P_{\\theta_a, Z_a}$\nWhile $Z_a$ captures an item's observed features, we can think of $\\theta_a$ as reflecting latent features of the item-like its quality and style-which cannot be inferred prior to recommending it. Intuitively, user responses $Y_u(a)$ are not i.i.d. (only conditionally i.i.d.) because observing $Y_u(a)$ gives one more information about the latent $\\theta_a$, which reduces uncertainty in $Y_{u'}(a)$ for some user $u' \\neq u$.\n$^1$This assumption is called the Stable Unit Treatment Value Assumption (SUTVA) in the causal inference literature. A growing literature on experimentation in the presence of interference tries to make sensible inferences when this assumption is relaxed [Li and Wager, 2022, Han et al., 2023]."}, {"title": "4 Algorithm Overview", "content": "The impatient bandit algorithm we develop next builds on the classical Thompson sampling bandit algorithm [Russo et al., 2020, Thompson, 1933]. However, our approach generalizes the classical Thompson sampling algorithm to take advantage of progressive feedback in settings with delayed rewards. In particular, our algorithm does this by forming a posterior for the vector $\\theta_a$, which, according to Lemma 1, defines the mean of all the engagement outcomes $(Y_u^{(1)}(a), \\ldots, Y_u^{(J)}(a))$, rather than just the reward. This allows us to update the posterior distribution even after observing partial feedback. Then at decision time, we sample a random draw from the posterior of $\\theta_a$, and use that posterior sample to compute a posterior draw of the mean reward under that arm using the function $R$ (which is linear by Assumption 3). See Algorithm 1 for more details. Note that the posterior update rule used in line 11 of Algorithm 1 is can be derived using standard formulas (see further discussion in Appendix B). Additionally, we extend our algorithm to settings with user context features, where expected outcomes are linear functions of the context; See Appendix C for details."}, {"title": "4.1 Lower Variance Version of Algorithm", "content": "For our regret bounds, we will focus on proving results for a slightly lower-variance version of the Impatient Thompson Sampling Bandit Algorithm from Algorithm 1. We do this mainly to avoid repeating concentration of measure arguments from Qin and Russo [2023], which are quite long and complicated. We introduce this lower variance version in Algorithm 2. As m becomes large, the difference between Algorithm 1 and Algorithm 2 vanishes. Later, in Section 7.2, we show empirically that Algorithm 1 behaves the same way whether m is small or large. As such, we believe that the theoretical insights we provide into the reduced-variance version carry over to the version presented in Algorithm 1.\nNote that as stated in Algorithm 1, the probability that any user $u \\in U_t$ is assigned action $a$ is\n$p_{t,a}^{TS} \\triangleq P(a = \\underset{a}{\\operatorname{argmax}} \\ f(\\theta_a) | \\mathcal{H}_t)$,\nwhere the probability above averages over the draw of $\\theta$ from the posterior $\\mathcal{N}(\\mu_{t,a}, \\Sigma_{t,a})$. Furthermore, lines 7-9 in Algorithm 1 can be equivalently be written as for each $u \\in U_t$,\n$A_u \\longleftarrow \\begin{cases} a & w.p.\\ p_{t,a}^{TS} \\ \\ \\ \\text{ for all } a \\in \\mathcal{A}\\end{cases}$\nWhile typical Thompson sampling sets $A_u$ to action $a$ with probability $p_{t,a}^{TS}$, for each $u \\in U_t$, a lower variance of the algorithm can be defined which assigns exactly $|U_{t,a}| \\approx |U_t|p_{t,a}^{TS}$ users to action $a$. Since $|U_t|p_{t,a}^{TS}$ is not necessarily a whole number, this would involve rounding $p_{t,a}^{TS}$ to some $p_{t,a}^{TS-rnd}$ which ensures that $|U_{t,a}| = |U_t p_{t,a}^{TS-rnd}|$ is a whole number. This lower-variance version of the algorithm is presented in Algorithm 2.\nIn Algorithm 2, Round$(\\{p\\}, m)$ is a function that takes the probabilities to round and the total number of samples m as input, and outputs probabilities $\\{p_{t,a}^{TS-rnd}\\}_{a \\in \\mathcal{A}}$. In order to ensure $p_{t,a}^{TS-rnd}$ is a whole number, it rounds each $p_{t,a}^{TS}$ to a value in $\\{0, \\frac{1}{m}, \\frac{2}{m}, \\ldots, 1\\}$. The rounding procedure also ensures that the rounding maintains a proper probability distribution, i.e., $1 = \\Sigma_{a \\in \\mathcal{A}}p_{t,a}^{TS-rnd}$. The formal assumptions we make on the rounding procedure are written in Assumption 4."}, {"title": "5 Theoretical Guarantee", "content": "Nearly all bandit analyses focus on the rate at which regret-the suboptimality of selected arms-vanishes as the number of arm selections grows. When reward observations are severely delayed, there is a long delay before anything is learned, regardless of how many arm selections are made in the interim; In our motivating application, for example, the primary reward metric is computed after observing a user's 60-day engagement with a recommended item. Our theoretical focus, aligned with the practical objective, is not only on converging to the optimal arm in the limit $T \\rightarrow \\infty$, but also on leveraging progressive feedback to substantially improve decision quality relative to the case where no progressive feedback is available for every $t \\in \\{1, \\ldots, T\\}$. Two factors make this learning problem challenging:\nDelayed feedback. After a user receives a recommendation, their downstream activity is revealed progressively across time. The resulting delay in information revelation limits how quickly the recommender system can learn and adapt.\nBandit feedback. The recommender system only observes a user's reward for the selected action $A_u$-not for all actions $\\mathcal{A}$ (as is the case for full-feedback learning problems). To learn, the system needs to engage in costly exploration over the actions in $\\mathcal{A}$."}, {"title": "5.1 Warm-up: Setting without Action Features Za", "content": "In this section we present a regret bound for the Impatient Bandit Algorithm (Algorithm 2) in a setting in which action features $Z_a$ are not used to tailor the prior distribution, nor the noise covariance matrix $V_z$ (i.e., $\\mu_z, \\Sigma_z, V_z$ are constant across $z \\in \\mathcal{Z}$). See Section 5.2 for the generalization to the setting with action features $Z_a$.\n5.1.1 Defining the Value of Progressive Feedback\nThe regret of the Impatient Bandit Algorithm fundamentally depends on the quality of the intermediate progressive feedback that the algorithm receives. The intermediate feedback is of higher quality if it provides more information on the true mean reward. To formalize this, below, we define a metric that captures the benefit of the progressive feedback in an information-theoretic sense. We later use this metric in our regret bound.\nWe define the \"Value of Progressive Feedback\" (VoPF) to equal the following conditional mutual information between the mean reward and the progressive (potential) outcomes, conditional on the delayed (potential) outcomes:\n$VoPF(t) = I(R_a; \\{Y_{u,t}(a) : u \\in U_{<t}\\} \\ | \\{Y_{u,t}(a) : u \\in U_{<(t-d_{max})}\\})$.\nAbove, the outcomes $\\{Y_{u,t}(a) : u \\in U_{<(t-d_{max})}\\} = \\{Y_{u,t}(a) : u \\in U_{<(t-d_{max})}\\}$ refer to the entire vector of full-feedback outcomes, which are not censored due to a deterministic delay and are not incrementally revealed. Also note that $\\{Y_{u,t}(a) : u \\in U_{<t}\\}$ contains much more information about $R_a$ than the history $\\mathcal{H}_t$. Here, $\\{Y_{u,t}(a) : u \\in U_{<t}\\}$ represents the outcomes one would observe in a full-feedback setting, where the outcomes of all arms are revealed after taking an action, rather than just the selected arm's outcomes are revealed (bandit-feedback).\nNote that by design, the VoPF metric is entirely a property of the underlying environment, i.e., the potential outcomes distributions of the rewards and intermediate outcomes. This means that the VoPF metric does not change depending on what decision-making algorithm is used to select actions. By well-known properties of mutual information, we can rewrite the VoPF as a difference of two conditional entropies:\n$VoPF(t) = H(R_a \\ | \\{Y_{u,t}(a) : u \\in U_{<(t-d_{max})}\\}) - H(R_a | \\{Y_{u,t}(a) : u \\in U_{<t}\\} )$.\nBy the above, we can interpret the expected value of the VoPF as the reduction in entropy in $R_a$ by including progressive feedback."}, {"title": "6 Fitting the Prior", "content": "We now discuss how to use historical data (training set), collected prior to the deployment of the Impatient Bandit algorithm, to fit the prior distribution parameters $\\{\\mu_{1,z}, \\Sigma_{1,z}\\}_{z \\in \\mathcal{Z}}$ and noise covariance matrices $\\{V_z\\}_{z \\in \\mathcal{Z}}$. We use $\\mathcal{U}^{hist}$ to denote the historical users (training set) and $\\mathcal{A}^{hist}$ to denote the historical actions. Throughout, we assume that there are a finite number of action features, i.e., $|\\mathcal{Z}| < \\infty$. In the podcast recommendation case, $Z_a$ could be the category of the podcast (e.g., comedy, news).\nFitting the Noise Covariance Matrices. Recall that $Y_u(a) | (\\theta_a, Z_a = z) \\sim \\mathcal{N}(\\theta_a, V_z)$. We estimate the $V_z$ by averaging the noise covariance matrices $\\widehat{V}^{(a)}$ estimates for each action $a \\in \\mathcal{A}^{hist}$.\n$V_z \\ \\triangleq \\ \\frac{\\sum_{a \\in \\mathcal{A}^{hist}} \\mathbb{1}_{Z_a=z} \\widehat{V}^{(a)}}{\\sum_{a \\in \\mathcal{A}^{hist}} \\mathbb{1}_{Z_a=z}}$ where $\\widehat{V}^{(a)} \\ \\triangleq \\ \\frac{1}{N(a)} \\ \\sum_{u \\in \\mathcal{U}^{hist}} \\mathbb{1}_{A_u=a} (Y_u - \\overline{Y}^{(a)}) (Y_u - \\overline{Y}^{(a)})^T$.\nAbove, we use, $N(a) = \\sum_{u \\in \\mathcal{U}^{hist}} \\mathbb{1}_{A_u=a}$ and $\\overline{Y}^{(a)} = \\frac{1}{N(a)} \\sum_{u \\in \\mathcal{U}^{hist}} \\mathbb{1}_{A_u=a} Y_u$.\nFitting the Prior Distribution Parameters. In the statistics and machine learning literature, a common approach to fitting an informative prior distribution using data is type II maximum likelihood [Murphy, 2022, page 173]\u2014also called empirical Bayes [Casella, 1985]. For the Impatient Bandit algorithm, the prior hyperparameters we want to fit using historical data are $\\{\\mu_{1,z}, \\Sigma_{1,z}\\}_{z \\in \\mathcal{Z}}$, i.e., the mean vectors and covariance matrices for the multi-variate Gaussian prior for $\\theta_a | Z_a$. For now, we assume that we already have estimators of the noise covariance matrices $\\{V_z\\}_{z \\in \\mathcal{Z}}$.\nThe basic idea behind type II maximum likelihood is to choose the prior hyperparameters $\\{\\mu_{1,z}, \\Sigma_{1,z}\\}$ that maximize the marginal likelihood of the data. In our Gaussian prior case, the maximum marginal likelihood criterion can be written as the following minimization problem:\n$(\\widehat{\\mu}_{1,z}, \\widehat{\\Sigma}_{1,z}) \\ = \\ \\underset{\\mu, \\Sigma}{\\operatorname{argmin}} \\ \\ \\sum_{a \\in \\mathcal{A}^{hist}: Z_a=z} \\{ \\log(|\\Sigma + V_z/N(a)|) + (\\overline{Y}^{(a)} - \\mu)^T [\\Sigma + V_z/N(a)]^{-1} (\\overline{Y}^{(a)} - \\mu)\\} \\\nNote that by the above criterion, it is clear that if $\\Sigma_{1,z}$ was known, then $\\widehat{\\mu}_{1,z}$ would be the solution to a weighted least squares criterion. In general, when $\\Sigma_{1,z}$ is unknown, one can minimize the above criterion using Newton-Raphson or Expectation maximization [Normand, 1999, SAS, 2018].\nIn the special case that $N(a) = n$ for all $a \\in \\mathcal{A}^{hist}$ such that $Z_a = z$, then the solution to the above $\\widehat{\\mu}_{1,z}$ is a simple mean over $\\overline{Y}^{(a)}$'s (no matter the value of $\\Sigma_{1,z}$):\n$\\widehat{\\mu}_{1,z} = \\frac{1}{\\sum_{a \\in \\mathcal{A}^{hist}} \\mathbb{1}_{Z_a=z}} \\sum_{a \\in \\mathcal{A}^{hist}: Z_a=z} \\overline{Y}^{(a)}.$"}, {"title": "7 Empirical Evaluation", "content": "Next, we investigate different aspects of the performance of our bandit algorithm empirically with two decision-making problems. After briefly introducing these two problems in Section 7.1, we address five questions.\n1. How does the empirical improvement from progressive feedback compare to our theory's predictions? (Section 7.2)\n2. Are there real-world applications that can benefit substantially from progressive feedback? (Section 7.3)\n3. How important is fitting the prior, as described in Section 6? (Section 7.4)\n4. How do bandits with and without progressive feedback fare in a realistic setting with perpetually changing actions? (Section 7.5)\n5. How does Thompson sampling compare to sequential elimination in problems with progressive feedback? (Section 7.6)\nTo this end, we quantify empirically the performance of our impatient bandit algorithm on the two problems. A natural benchmark for a bandit algorithm that makes use of progressive feedback is to compare it to a copy of the same algorithm, but one where we pretend that all outcomes are revealed at once after delay $d_{max}$. Equivalently, we can think of that benchmark as ignoring the outcomes $Y_u^{(1)}, Y_u^{(2)}, \\ldots$ as they become available, and only using the final delayed reward $R(Y_u)$. This benchmark is implicit in Theorem 1, and will appear throughout our experiments.\n7.1 Data-Generating Environments\nAcross our experiments, we study two decision-making problems. The first one is an idealized, synthetic problem, where we can explicitly vary the Value of Progressive Feedback. The second one is inspired by a real-world product problem at Spotify (a problem we will later re-examine in Section 8). In this second problem, actions, rewards and progressive feedback represent actual interactions of users with content on Spotify.\n7.1.1 Synthetic Environment\nWe postulate a simple, synthetic generative model. There are $J=25"}]}