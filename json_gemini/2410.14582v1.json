{"title": "Do LLMS ESTIMATE UNCERTAINTY WELL IN INSTRUCTION-FOLLOWING?", "authors": ["Juyeon Heo", "Miao Xiong", "Christina Heinze-Deml", "Jaya Narain"], "abstract": "Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of uncertainty estimation abilities of LLMs in the context of instruction-following. Our study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stemming from instruction-following, complicating the isolation and comparison across methods and models. To address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling comprehensive comparison of uncertainty estimation methods under various conditions. Our findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. The insights from our controlled evaluation setups provide crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have garnered interest for their potential as personal AI agents across various domains, such as healthcare, fitness, nutrition, and psychological counseling (Li et al., 2024; Wang et al., 2023a; Tu et al., 2024). A key to building safe and useful personal AI agents with LLMs lies in their ability to follow instructions precisely. Deployed models must adhere to the constraints and guidelines provided by users to ensure that the outputs are both aligned with user intentions and safe. Yet recent research has exposed significant limitations in LLMs' ability to follow instructions (Zhou et al., 2023; Zeng et al., 2023; Qin et al., 2024; Xia et al., 2024; Kim et al., 2024; Yan et al., 2024). For example, even large models like GPT-4 achieve only around 80% instruction-following accuracy on simple and non-ambiguous instructions from benchmark datasets, and smaller models perform even worse, with accuracy less than 50% (Sun et al., 2024).\nSince LLMs are prone to errors, their ability to accurately assess and communicate their own uncertainty is essential. This becomes particularly important in high-stakes applications, where mistakes can have serious consequences. For instance, an LLM developed for personal psychological counseling must strictly adhere to guidelines that avoid topics that might potentially cause trauma. If the LLM misinterprets or deviates from these instructions but accurately recognizes and signals"}, {"title": "1.1 CONTRIBUTIONS", "content": "\u2022 Systematic Evaluation: We present the first systematic evaluation of uncertainty estimation methods in instruction-following tasks, addressing a gap in existing research.\n\u2022 Benchmark Dataset: We identify key challenges in existing datasets and introduce a new benchmark dataset specifically tailored for direct comparison and fine-grained analysis of uncertainty estimation methods and models in both controlled and real-world conditions.\n\u2022 Findings: Our evaluation results highlight the potential of self-evaluation and probing methods and point out limitations in handling more complex tasks, underscoring the need for further research to advance uncertainty estimation in instruction-following tasks."}, {"title": "2 UNCERTAINTY ESTIMATION ABILITY IN INSTRUCTION-FOLLOWING ON IFEVAL", "content": "In this section, we evaluate LLMs' uncertainty estimation abilities using the IFEval dataset (Zhou et al., 2023), applying six baseline methods across four LLMs. We selected IFEval because it is designed so that a simple and deterministic program can verify whether a response follows the instructions. This enables a fully automatic and accurate assessment of a model's instruction-following capability, thereby minimizing uncertainties from ambiguous evaluation criteria."}, {"title": "2.1 METHODS", "content": "Data We evaluate uncertainty estimation with the IFEval dataset (Zhou et al., 2023), which is de-signed to evaluate the instruction-following ability of LLMs on 25 verifiable instruction types under 9 categories across 541 total prompts. Each prompt consists of two components: a task and an instruction, where the instruction specifies the action (e.g., \u201cplease do not use keywords\u201d, \u201cplease start/finish your response with exact sentence\") and the task provides the context for executing the instruction (e.g., \"please write a resume\u201d, \u201cplease give a summary about solar system\u201d).\nModels and Metrics We evaluate four LLMs of varying sizes: LLaMA2-chat-7B (Touvron et al., 2023), LLaMA2-chat-13B (Touvron et al., 2023), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), and Phi-3-mini-128k-instruct (Abdin et al., 2024). To avoid randomness in decoding, we employ greedy decoding without sampling. Area Under the Receiver Operating Characteristic curve (AUROC) (Pedregosa et al., 2011) is used to measure if the models' uncertainty estimation matches the ground truth labels on correctness in instruction following, generated using the automated evaluation functions from IFEval.\nBaseline uncertainty estimation methods To evaluate uncertainty in instruction-following, we em-ploy several baseline methods, including self-evaluation of their own uncertainty (verbalized con-fidence, normalized p(true) and p(true)) and logits-based method (perplexity, sequence probability, and mean token entropy):\""}, {"title": "2.2 FINDINGS", "content": "Table 1 summarizes uncertainty evaluation results using IFEval.\nLLMs struggle to estimate uncertainty in instruction-following. Average AUROC values across models and uncertainty estimation methods hover around chance levels (between 0.43 and 0.53), indicating that the models consistently fail to reliably assess their own uncertainty in instruction-following. This underscores the challenge LLMs face in detecting when their responses deviate from the instructions.\nSequence probability outperforms perplexity, revealing a length signal in uncertainty estima-tion. Sequence probability consistently achieves higher AUROC scores than perplexity across most models. Sequency probability is tied to by sequence length, whereas perplexity is not. For example, in LLaMA2-chat-13B, the AUROC for sequence probability averages 0.61 (above chance) across instruction types, whereas perplexity lags at 0.44. This finding implies that response length may in-advertently provide a signal in some uncertainty estimation metrics, even though it may not correlate directly with the correctness of the response in instruction-following.\nNo model or method consistently excels across instruction types. As shown in Table 5 in Ap-pendix, there is no consistent pattern of performance of uncertainty estimation method or model across different instruction types. This lack of consistency indicates that none of the uncertainty estimation methods evaluated reliably capture uncertainty across all instruction types and models."}, {"title": "2.3 CHALLENGES IN EVALUATING UNCERTAINTY ESTIMATION USING EXISTING DATASETS", "content": "An instruction-following dataset with clear evaluation criteria, like IFEval, is important for eval-uating instruction-following. However, we identified the importance of several additional factors to aid in comparatively evaluating instruction-following uncertainty of LLMs. In our systematic evaluation, we identified multiple factors affecting uncertainty that are entangled within naturally generated responses, making it difficult to isolate the strengths and weaknesses of each method or model. Below, we outline the primary challenges we identified and present analyses demonstrating each challenge."}, {"title": "3 UNCERTAINTY ESTIMATION ABILITY IN INSTRUCTION-FOLLOWING ON OUR BENCHMARK DATA", "content": "The challenges identified in the previous section\u2014length bias, the entanglement of task execution quality with instruction-following, and varying difficulty levels across models\u2014underscore the need for a controlled and robust framework for evaluating uncertainty estimation. To address these issues, we develop a new benchmark dataset comprising two versions: Controlled and Realistic version. These versions allow for the evaluation of uncertainty estimation under controlled conditions (Controlled) and real-world scenarios (Realistic)."}, {"title": "3.1 BENCHMARK DATASET FOR CONTROLLED EVALUATION SETUPS", "content": "To disentangle the complexities that can obscure uncertainty estimation, we design two distinct versions of the dataset: Controlled and Realistic. The Controlled version neutralizes the influence of token length. Meanwhile, the Realistic version leverages actual LLM-generated responses that naturally incorporate real-world signals, including length signal, without manual intervention.\nIn both versions, we use GPT-4 to filter out low-quality responses, ensuring that the uncertainty being measured comes from instruction-following, not poor task execution (addressing the second challenge in section 2.3). We apply a filtering process using GPT-4 evaluations of task quality of each response on a scale 0-9. Only responses that received a high task quality score (>8) are included. Also, these datasets enable using the same responses with all models in the uncertainty evaluation task, controlling the difficulty of uncertainty evaluation across models. This allowing for direct comparisons in uncertainty estimation (addressing the third challenge in section 2.3)."}, {"title": "3.1.1 CONTROLLED VERSION", "content": "In this version, we eliminate the length effect, along with ensuring consistent levels of difficulty across responses to ensure that the evaluation focuses purely on uncertainty estimation. To neu-tralize the impact of token length, we use GPT-4 to generate both correct and incorrect responses with similar token length (see Appendix for the prompt used to generate responses). Figure 2c shows the absence of length bias in the token length distribution. We introduce two levels of controlled difficulty\u2014Controlled-Easy and Controlled-Hard\u2014by generating three categories of re-sponses: completely incorrect, correct, and subtly off-target. In the Controlled-Easy, we calculate AUROC based on distinguishing between correct and completely incorrect responses, while in the Controlled-Hard, we calculate AUROC based on distinguishing between correct and subtly off-target responses. These are more challenging cases where the responses only slightly deviate from the instructions, testing the model's ability to recognize subtle mistakes. While these mistakes are subtle, they could still be important in real-world deployments."}, {"title": "3.1.2 REALISTIC VERSION", "content": "In the Realistic version, we retain the natural length and signals inherent in responses generated by multiple LLMs (LLaMA2-chat-7B, LLaMA2-chat-13B, Mistral-7B-Instruct-v0.3, Phi-3-mini-128k, and LLaMA2-chat-70B). Here, the goal is to evaluate uncertainty estimation methods in a scenario that reflects actual model behavior. In this version, we do not control for token length, allowing for the natural variance found in actual model-generated responses. Though, we still control for task execution quality and provide generated responses to enable clear comparisons between models."}, {"title": "3.2 RESULTS ON OUR BENCHMARK DATA", "content": "Table 4, Figure 5, and Appendix Table 8 show findings from our evaluation of uncertainty estima-tion methods on the crafted dataset. By analyzing performance across different uncertainty meth-"}, {"title": "3.2.1 COMPARISON OF UNCERTAINTY METHODS", "content": "Controlled-Easy: Self-evaluation methods outperform logit-based ones In simpler tasks, self-evaluation methods such as verbalized confidence (short answer) and normalized-p(true) (binary choice) consistently outperformed logit-based approaches like perplexity, sequence probability, and entropy. Furthermore, for models like LLaMA2-chat-7B, LLaMA2-chat-13B, and Mistral-7B-Instruct, verbalized confidence performed better than normalized p(true), meaning short-form an-swer verbalized scores were more calibrated than binary choices in these cases. This overall trend indicates that for Controlled-Easy tasks, models are better at assessing their own correctness using self-evaluation methods compared to logit-based uncertainty estimation.\nControlled-Hard: Mixed performance between logit-based methods and self-evaluation meth-ods In the instruction-following samples with more nuanced mistakes, there was a mixed perfor-mance between logit-based methods and Normalized p(true). For LLaMA2-chat-7B and LLaMA2-chat-13B, logit-based methods like perplexity and entropy outperformed verbalized confidence and normalized p(true), suggesting that as task complexity increased, logit-based methods became more reliable with those models. However, for Mistral-7B-Instruct and Phi-3-mini, normalized p(true) continued to provide better uncertainty estimation than logit-based methods, demonstrating that the best method can vary depending on the model and its underlying architecture in Controlled-Hard tasks. Uncertainty estimation scores in Controlled-Hard were consistently lower than in other set-tings and no methods had reliably estimated uncertainty in this setting.\nRealistic: Sequence probability excels, but normalized p(true) remains a strong contender In the Realistic evaluation, where models were evaluated on responses with realistic patterns (includ-ing length effect), sequence probability performed best across all models, likely due to its ability to exploit the length effect in incorrect responses. However, aside from sequence probability, normal-ized p(true) consistently ranked as the second-best method across all models, outperforming other logit-based methods like perplexity and mean token entropy. This demonstrates that while sequence probability can take advantage of length bias, normalized p(true) offers a more balanced and reliable uncertainty estimation when length effect is less prominent."}, {"title": "3.2.2 COMPARISON OF MODELS", "content": "Mistral-7B-Instruct consistently demonstrates the strongest performance in verbalized confi-dence across all tasks (Controlled-Easy, Controlled-Hard, and Realistic), outperforming even the larger LLaMA2-13B model, highlighting its effective internal calibration for self-assessment. On the other hand, Phi-3-mini-128k leads in normalized p(true), consistently achieving the best AU-ROC across both easy and hard tasks in Controlled, as well as in Realistic versions, showcasing its strength in binary choice settings. In contrast, LLaMA-2-13B excels in Perplexity, indicating its proficiency in logit-based uncertainty estimation. These findings are particularly interesting be-cause smaller models, such as Mistral-7B-Instruct and Phi-3-mini-128k outperform the larger LLaMA-2-chat-13B in self-evaluation methods. This suggests that factors beyond model size, such as tuning or architecture, may contribute to better uncertainty quantification in certain tasks. However, LLaMA2-13B still shines in logit-based approaches like perplexity, indicating that differ-ent methods may favor different models."}, {"title": "3.2.3 COMPARISON OF INSTRUCTION TYPES", "content": "The relationship between success rate and AUROC varies significantly across instruction types\nAs shown in Appendix Table 8, models can correctly follow instructions but still struggle to accu-rately gauge their own uncertainty, as reflected in the original IFEval evaluations. For instruction types like 'detectable-content' and 'keywords, there is a clear positive correlation between higher success rates and higher AUROC scores for uncertainty estimation, indicating that when models excel at following instructions, they also tend to estimate uncertainty more reliably. However, this correlation is weaker for other instruction types, such as 'language', 'startend', and 'change-case'.\nIn particular, 'punctuation' presents a unique challenge. In these tasks, models are instructed to not to use any punctuation in responses. Although this instruction type had the lowest success rates across all models-0.24 (7B), 0.14 (13B), 0.17 (Mistral), and 0.10 (Phi), 'punctuation' yields relatively high AUROC scores, especially in Controlled version with verbalized confidence. This discrepancy may arise because models recognize when they fail to follow the instructions but strug-gle to adhere to them due to strong priors in their training data. Since most training data includes proper punctuation, instructions to avoid it may conflict with learned patterns, making it difficult for models to follow the instructions while still being somewhat aware of their failure. These findings highlight that while success rates provide some insight into model performance, they do not fully capture the ability to estimate uncertainty in instruction-following."}, {"title": "4 DO INTERNAL STATES OF LLMS KNOW MORE ABOUT THEIR UNCERTAINTY?", "content": "In the previous section, we demonstrated that both self-evaluation and logit-based methods per-formed poorly in the Controlled-Hard setting, where instruction-following mistakes were subtle. AUROC for uncertainty estimation was often below 0.6. Drawing inspiration from Liu et al. (2024), which suggests that model representations contain valuable information for uncertainty estimation in tasks like question and answering, we investigated whether this internal knowledge could sim-ilarly improve uncertainty estimation in instruction-following tasks. We trained a linear model as an uncertainty estimation function that maps the internal representations of LLMs to instruction-following success labels, where the probability predicted by the linear model used as uncertainty scores. The ground truth for success or failure is determined by a deterministic program that verifies instruction adherence using the IFEval dataset. We refer to this as Probe."}, {"title": "4.1 SETTINGS", "content": "Representations We analyzed four language models: LLaMA-2-7B-chat, LLaMA-2-13B-chat, Mistral-7B-Instruct-v0.3, and Phi-3-mini-128k-instruct. For each model, we collected represen-tations across three different layers: early, middle, and the final layer. Specifically, we use layers 16, 32, and 40 and for LLaMA-2-13B-chat and 14, 26, and 32 for other three models.\nLinear Model and metric We train a linear model on representations on instruction-following suc-cess labels, optimized with AdamW, a 0.001 learning rate, 0.1 weight decay. The model is trained for 1000 epochs on 70% training set and is evaluated on 30% test set. We use AUROC as metric."}, {"title": "4.2 RESULTS", "content": "Probe generally outperformed baseline methods in both Controlled-Easy and Realistic ver-sion, as shown in Table 4. Probe consistently outperformed even self-evaluation methods such as berbalized confidence and normalized-p(true). This gap between what the internal states of the model \"know\" and what they are able to express suggests that their internal layers hold richer, more reliable indicators of uncertainty, which are not fully captured in the model's explicit responses. This points to promising directions for future work, specifically in improving self-evaluation methods by leveraging the rich information within a model's internal representations.\nMiddle layers consistently offer the most informative representations for uncertainty estima-tion, particularly in simpler tasks (Controlled-Easy). Appendix Table 9 shows how Probe perfor-mance varies across instruction types and layers within the LLMs. The middle layers consistently"}, {"title": "5 RELATED WORK", "content": "Uncertainty estimation in LLMs. Existing uncertainty estimation methods can be broadly cate-gorized into four types based on the source of information: verbalized, logit-based, multi-sample, and probing-based methods. Among them, verbalized methods (Lin et al., 2022; Xiong et al., 2023; Tian et al., 2023) rely on model's self-evaluation by prompting LLMs to explicitly express their uncertainty in theiroutput. Logit-based methods, such as perplexity (Jelinek et al., 1977), sequence probability (Fomicheva et al., 2020), and mean token entropy (Fomicheva et al., 2020) mainly uti-lize information from the next token prediction distribution. Multi-sample methods (Aichberger et al., 2024; Kuhn et al., 2023; Farquhar et al., 2024) generate multiple responses for the same question, estimating uncertainty through the semantic diversity among the responses. However, these multi-sample methods are less applicable to instruction-following tasks, which only focus on strict adherence to instructions rather than variations in semantic meaning. Lastly, probing-based methods (Liu et al., 2024; Ahdritz et al., 2024) train external supervised model on model repre-sentations to infer uncertainty. In addition, it is worth noting that most existing works focus on factuality-related tasks such as question answering (Xiong et al., 2023; Tian et al., 2023) and sum-marization tasks (Kuhn et al., 2023), with little attention on instruction-tuning tasks. Our work seek to bridge this gap by evaluating how well current uncertainty metrics capture uncertainties specific to instruction-following scenarios."}, {"title": "6 CONCLUSION", "content": "In this paper, we conduct the first comprehensive evaluation of uncertainty estimation in LLMs specifically in the context of instruction-following tasks, addressing a gap in existing research that primarily focuses on fact-based tasks. We identify limitations associated with existing benchmark datasets and introduce a new benchmark with two versions\u2014Controlled and Realistic\u2014designed to provide a comprehensive framework for evaluating uncertainty estimation methods and models un-der both controlled and real-world conditions. Our analysis revealed that verbalized self-evaluation methods outperform logit-based approaches in Controlled-Easy tasks, while internal model states provide more reliable uncertainty signals in both Controlled-Easy and Realistic settings. However,"}, {"title": "A APPENDIX", "content": "A.1 DETAILED RESULTS ON IFEVAL DATASET\nIn the main paper, Table 1 presented the average AUROC scores across all instruction types, pro-viding an overview of how well the baseline uncertainty estimation methods perform in instruction-following tasks. This section provides detailed AUROC results for each individual instruction type in the IFEval dataset in Table 5."}, {"title": "A.2 PROMPT OF UNCERTAINTY ESTIMATION METHODS", "content": "To evaluate uncertainty in instruction-following tasks, we employed several baseline methods, in-cluding self-evaluation methods (verbalized confidence, normalized p(true), and p(true)) and logits-based methods (perplexity, sequence probability, and mean token entropy). For self-evaluation meth-"}, {"title": "A.3 PROMPTS FOR GENERATING OUR CONTROLLED BENCHMARK DATASET", "content": "To create the Controlled version of our benchmark dataset, we employed carefully designed prompts that generate responses with controlled token lengths, varying difficulty levels, and high task quality. The aim was to ensure that the evaluation focuses solely on uncertainty from instruction-following, without interference from factors such as task execution quality or response length. Below, we provide the full prompts used to generate both subtle off-target and completely wrong responses, as well as the prompt for scoring task quality or instruction-following."}, {"title": "A.4 STATS OF OUR BENCHMARK DATA", "content": "This section provides comprehensive statistics for our benchmark datasets, detailing the number of data points and token length distributions. These statistics illustrate the construction and character-istics of both the Controlled and Realistic versions of our benchmark dataset.\nFigures 3a and Figures 3b show the token length distributions for the Controlled and Realistic datasets, respectively, highlighting the absence of length bias in the Controlled version and the presence of natural length variation in the Realistic version. Figures 4a and Figures 4b display the model contribution to total correct and incorrect responses in the Realistic version, ensuring diverse data sources. Table 6 presents the number of correct and incorrect responses in both the Controlled and Realistic versions. Table 7 provides the mean token lengths across different instruction types, underscoring the careful balancing of token lengths in the Controlled version."}, {"title": "A.5 MORE RESULTS ON OUR BENCHMARK DATA", "content": "In this section, we provide additional results on uncertainty estimation performance using our crafted benchmark datasets. These results offer deeper insights into how different LLMs and uncertainty estimation methods perform across various evaluation scenarios, both controlled and realistic. The radar charts and tables present a detailed comparison of models' performance, using AUROC averaged across instruction types for different uncertainty estimation methods. Figure 5 and Figure 6 shows the radar charts on IFEval data and our benchmark data. Table 8 provides detailed AUROC scores for each instruction type."}, {"title": "A.7 LENGTH EFFECT MODELS", "content": "This section analyzes the impact of token length on uncertainty estimation across different instruc-tion types and models. We examine the relationship between token length and correctness in both correct and incorrect responses, revealing a prevalent length signal in existing instruction-following datasets like IFEval. This effect can introduce biases in uncertainty estimation methods that rely on token length, complicating accurate assessments of model performance.\nTable 10 presents the mean and standard deviation of token lengths for both correct and incorrect responses across four LLMs. The results show that, on average, incorrect responses are consistently longer than correct ones, further underscoring the influence of token length on uncertainty estima-tion. Figure 10 provides a visual representation of the token length distributions for three models: LLaMA-2-chat-13B, Mistral-7B-Inst-v0.3, and Phi-3-128k-inst. The figure illustrates how incor-rect responses tend to be longer than correct responses, reinforcing the presence of a length signal in naturally generated datasets like IFEval(Zhou et al., 2023)."}, {"title": "Verbalized confidence", "content": "$\\exp \\left\\{-\\sum_{i=1}^{t} \\log P_{\theta}\\left(x_{i} \\mid x_{<i}\right)\right\\}$"}, {"title": "Sequence probability", "content": "$\\exp \\left\\{-\\sum_{i=1}^{t} \\log P_{\theta}\\left(x_{i} \\mid x_{<i}\right)\right\\}$"}, {"title": "Mean token entropy for LLMs", "content": "$H=-\\sum_{i=1}^{t} P_{\\theta}\\left(x_{i} \\mid x_{<i}\\right) \\log P_{\\theta}\\left(x_{i} \\mid x_{<i}\\right)$"}, {"title": "Normalized p(true) and p(true)", "content": "$p(true)/(p(true) + p(false))$"}, {"title": "Normalized p(true) and p(true)", "content": "$p(A)/(p(A) + p(\u0392))$"}]}