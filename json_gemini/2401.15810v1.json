{"title": "Green Runner: A tool for efficient deep learning component selection", "authors": ["Jai Kannan", "Scott Barnett", "Anj Simmons", "Taylan Selvi", "Lu\u00eds Cruz"], "abstract": "For software that relies on machine-learned functionality, model selection is key to finding the right model for the task with desired performance characteristics. Evaluating a model requires develop-ers to i) select from many models (e.g. the Hugging face model repository), ii) select evaluation metrics and training strategy, and iii) tailor trade-offs based on the problem domain. However, current evaluation approaches are either ad-hoc resulting in sub-optimal model selection or brute force leading to wasted compute. In this work, we present GreenRunner, a novel tool to automatically select and evaluate models based on the application scenario pro-vided in natural language. We leverage the reasoning capabilities of large language models to propose a training strategy and extract desired trade-offs from a problem description. GreenRunner fea-tures a resource-efficient experimentation engine that integrates constraints and trade-offs based on the problem into the model selection process. Our preliminary evaluation demonstrates that GreenRunner is both efficient and accurate compared to ad-hoc evaluations and brute force. This work presents an important step toward energy-efficient tools to help reduce the environmental impact caused by the growing demand for software with machine-learned functionality. Our tool is available at Figshare GreenRunner.", "sections": [{"title": "1 INTRODUCTION", "content": "While deep learning (DL) significantly enhances software, it also brings various challenges that need careful attention. A central con-cern is that of the environmental impact of deep learning models [10, 17, 20, 27]. The environmental impact of training DL models has primarly been the focus [16, 22]. However, environmental impact that occurs when reusing deep learning models through additional training and evaluation of models is less studied. Selecting an ap-propriate deep learning component (a pretrained model or an API web service) requires a) evaluating against a large sample dataset, b) comparing multiple alternatives, and c) discovering an optimal reuse strategy (i.e. fine tuning, transfer learning, ensembling etc.) All this evaluation wastes compute resources. Finding energy effi-cient strategies to compare and select DL components is important to mitigate the environmental impact of deep learning.\nEfficient approaches to select a DL component include 1) relying on recorded metrics on a benchmark dataset [2, 15], 2) using proxy metrics that approximate the actual performance [11, 26], or 3) transferability estimation [1, 3, 12, 21]. These approaches aim to approximate the actual performance of a DL component to stream-line evaluations. However, all approaches rely on approximating model performance neglecting crucial aspects like memory require-ments, computational demands, and hardware compatibility. This narrow focus leads to suboptimal selection that does not align to a specific application context. The gap addressed in this paper is that existing DL component selection approaches do not consider the tradeoffs faced by software engineers when considering an application specific context.\nTo illustrate tradeoffs that arise when selecting DL components consider the following examples. An agricultural drone to detect weeds requires a balance between prediction accuracy and energy efficiency to extended flight times. In contrast, an autonomous ve-hicle has less concerns around energy efficiency (i.e. they have larger batteries) but has greater demand for low-latency detection to ensure occupant safety [14]. The same DL task, detecting objects in images, produces different tradeoffs depending on the a) deploy-ment scenario, b) safety criticality of the system, and c) hardware requirements. As a consequence, the application context where the DL component is going to be used influences the criteria used to make a selection.\nSoftware engineers have to select more than a DL component (i.e. a model on Huggingface) but also the configurations for eval-uation. A selection of tasks, models, metrics, reuse process and tradeoffs to consider are depicted in Figure 1, which presents only the computer-vision models on Huggingface\u00b9. For image object de-tection, a developer must select from 1245 models\u00b2, select the right combination of metrics and training process and iteratively evaluate the models and then compute the tradeoffs. This highlights the need for a resource-efficient and cost-effective DL component selection strategy that considers 1) operational criteria, 2) environmental impact, and 3) comparisons between competing components.\nTo assist software engineers in selecting deep learning (DL) components, we introduce GreenRunner. This tool streamlines the evaluation process by incorporating an energy-efficient experi-mentation engine based on a multi-armed bandit framework and utilizing the reasoning capabilities of a large language model (LLM). The LLM suggests application-specific configurations, including the model, metrics, reuse strategies, and trade-offs. Engineers input a description of their application and its operational context, and GreenRunner generates the necessary evaluation configuration.\nOur approach is grounded in the hypothesis that an LLM, trained on scientific publications, blog articles, and open-source machine learning repositories [5, 7, 23], can produce near-optimal configura-tions. We posit that the computational cost of training and operating a single LLM is offset by the reduced compute requirements for future applications developed using DL components.\nWe demonstrate the effectiveness of our approach with a prelimi-nary evaluation using the ObjectNet dataset and 71 object detection models, showcasing GreenRunner's potential in optimizing DL component selection.\nKey Contributions arising from this work:"}, {"title": "2 MOTIVATING EXAMPLE", "content": "To motivate the need for GreenRunner, consider Tom, a software engineer at an agricultural drone manufacturing company. The WeedWhack drones are to detect and then spray weeds without affecting the main crop. Tom is tasked with implementing the weed detection component. Tom has been loosely following the advances in object detection but is primarily a software engineer, not a ma-chine learning expert. To complete the task Tom needs to select an appropriate DL component by 1) finding a set of candidate com-ponents (i.e. online models or web services), 2) select evaluation metrics, reuse strategy, and hyperparameters, and 3) evaluate and compare the results. These tasks have to be completed while con-sidering the constraints of the problem domain namely a) resource efficient models (cover the whole field without recharging), b) ac-curate detection (only target weeds not crops), and c) adapt to a multitude of weed types.\nTom starts identifying DL components for WeedWhack by select-ing object detection models from Huggingface and online services (e.g. AWS Rekognition API\u00b9). Tom selects a handful of components to evaluate against the dataset of weeds and starts evaluating them all against the benchmark dataset. Each component needs to be adapted to the weed dataset to find the ideal model which wastes time, increases compute costs, and is energy inefficient. Tom is also unaware of the range of strategies suitable for the problem domain"}, {"title": "3 GREENRUNNER", "content": "To design GreenRunner we took inspiration from the tool proposed by Cummaudo et al. [6], we describe the core components of GreenRunner, shown in figure Figure 2. The core components are 1) a GPT Based Reasoning Module to suggest metric choice & weights for a target use case, which forms a reward function, and 2) a resource-efficient experimentation Engine that evaluates pre-trained models on a target dataset against using a reward function to produce a set of top-ranked components for the target use case."}, {"title": "3.1 GPT Based Reasoning Module", "content": "The integration of GreenRunner with an LLM enables users to provide a concise plain text prompt that describes the specific use case for the application which is the first step in identifying an optimal DL component for a particular use case. For this paper, the LLM used by the reasoning module was GPT-4. Analysing the description, the LLM generates a set of metrics and weights, which are used as part of a reward function for performing the selection process. These weights are distributed across the metrics suggested by the LLM such as: i) Model accuracy, ii) Model size, and iii) Model complexity.\nAdditionally, the LLM offers comprehensive justifications for the selected weights, optimizing them in accordance with the use case at hand. These metric weights serve as configuration parameters for resource-efficient evaluation algorithms, which effectively navigate the extensive model repository and select the most appropriate component. Prior to conducting the experiment, the staging module allows users to thoroughly examine and refine the metric weights, ensuring alignment with the intended use case."}, {"title": "3.2 Resource Efficient Experimentation Engine", "content": "GreenRunner utilizes multi-armed bandit (MAB) algorithms [19] to streamline the selection of deep learning components for specific applications, reducing the need for extensive evaluations. These algorithms adeptly balance the exploration of diverse options with the exploitation of known advantageous actions.\nIn GreenRunner, deep learning models from a repository function as \"arms\". The system is designed to identify top-performing models with a limited number of evaluations. It employs a custom reward function that considers metrics such as accuracy, size, complexity generated by the reasoning module subsection 3.1, thereby adapting the selection to the unique requirements of each use case.\nGreenRunner offers three MAB selection strategies: Epsilon Greedy, Upper Confidence Bound, and Thompson Sampling. The evaluation of these models is based on a user-defined budget, and strategy where a larger budget enhances the likelihood of finding the optimal model, whereas a smaller budget limits evaluations, potentially leading to the selection of less-than-ideal models. As more data is processed, the MAB algorithm continuously refines its understanding of each model's performance, effectively pinpointing the most suitable models for the specific dataset."}, {"title": "4 USAGE EXAMPLE", "content": "To address Tom's challenge, we present an innovative tool designed to optimize model selection while minimizing resource consump-tion. Tom inputs a simple query like \"Recommend a model for drone-based object detection\" and provides the target dataset. This triggers our GPT-Based reasoning module, which proposes a set of metrics and their importance as shown in Figure 3 1. The module also justifies these choices in 2, linking them to key use case consider-ations such as model size, complexity, and performance, detailed in section 2.\nTom can then adjust these metric weights prior to running the ex-periment 3, which will affect the outcome of the resource-efficient experiment engine. After running the experiment, Tom reviews the results on the analysis screen in \u2463, which displays the top models in 5, the number of evaluations made per model in \u2465, and the computational savings in \u2466 compared to a brute force approach. From here, Tom can select the best model to fine-tune and deploy on his drone."}, {"title": "5 PRELIMINARY EVALUATION", "content": "To evaluate our approach we proposed two research questions: 1. Does GreenRunner find the most suitable model for inferred trade-offs? and 2. How does GreenRunner compare to other transferability metrics for model selection? This evaluates if our approach balances tradeoffs and how it compares to state-of-the-art. We compared GreenRunner to three baselines: 1) benchmark results (results of the model trained on a benchmark dataset), 2) brute force (compare all models on all data points), and 3) a transferability metric from the literature [12].\nDataset: In this research, DL components refer to deep learning models selected from a repository. We initially considered the com-plete collection of image classification models available on PyTorch Hub5, amounting to 80 models. However, due to API issues that prevented the download of 9 models, our experiment proceeded with the remaining 71 models. PyTorch Hub was chosen for its prevalence as an open-source framework in the machine learning community and for the ease of conducting standardized compar-isons, as it hosts models pre-trained on the benchmark ImageNet dataset [8].\nIn our experiments, we used the ObjectNet dataset [2] as our target for model evaluation. ObjectNet, known for challenging benchmark models, is tailored for testing vision models in realis-tic settings. For our initial assessment, we randomly selected 200 images from 113 classes corresponding to ImageNet categories to evaluate the models.\nGreenRunner configuration: To calculate the metrics and weights for the use case we used a GPT Based Reasoning mod-ule (based on GPT-4) using the following prompt Recommend a"}, {"title": "6 RELATED WORK", "content": "To approach the model selection problem recent research has pro-posed several online and offline approaches [4, 9, 13, 18, 25]. \u03a4\u03bf provide an approach to solve this problem one approach is to dis-sect models into distinctive building blocks and reassemble them to produce a customised network [24], however reassembling models is complex, and the proposed approach requires significant compu-tational resources and expertise to implement. Research has also proposed platforms which assess the adaptation to a domain task for models in a model zoo [4]. These platforms require specialised knowledge and need empirical evaluation requiring computational resources.\nProposed approaches have also calculated the class similarity between the dataset the models are trained on vs the target dataset for a downstream task [18]. The assumption being a high class similarity between datasets produces similar performance between the models. Research has also produced lightweight tools to esti-mate transferability such as [12, 25]. These approaches utilise a small amount of data to be passed through the models to assess the potential performance of a target task. However, these approaches require at least one pass of the data through the models to estimate the potential performance of the models on the dataset where re-sources are utilised to compute the performance on the dataset. The way these approaches differ to our approach is that GreenRunner is able to select an appropriate amount of evaluation data on a per model basis and considers trade-offs rather than just optimising for accuracy."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "GreenRunner addresses a practical challenge that developers en-counter when integrating machine learning functionality into soft-ware. It efficiently selects machine learning models for specific use cases, surpassing traditional benchmark and brute force methods in terms of both optimality and efficiency. Benchmark methods often fall short, as they rely on source data accuracy, which might not reflect the target data performance. brute force methods, while accurate, consume significant resources in evaluating all models. GreenRunner balances high accuracy with manageable model sizes and complexity by leveraging metrics and weights tailored to each use case. This expedites the model selection process and signifi-cantly reduces computational resources, thus minimizing resource consumption and environmental impact and contributing to sus-tainable DL software development.\nOur current work focuses on only one part of the DL component selection i.e. choosing appropriate pre-trained models from reposi-tories for a downstream task. Looking ahead, we aim to expand our scope to include model fine-tuning and the selection of machine learning API web services. These areas align with our framework and reward function but will necessitate distinct multi-armed ban-dit (MAB) strategies and user interfaces. Further developments will explore enabling the GPT-based Reasoning Module to recommend a broader range of metrics beyond accuracy, size, and complexity. This expansion is planned to be seamlessly integrated into our ex-isting system but will call for adjustments to the reward function and strategies for addressing newly important metrics as suggested by the LLM."}]}