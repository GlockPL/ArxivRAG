{"title": "Automating Exploratory Proteomics Research via Language Models", "authors": ["Ning Ding", "Shang Qu", "Linhai Xie", "Yifei Li", "Zaoqu Liu", "Kaiyan Zhang", "Yibai Xiong", "Yuxin Zuo", "Zhangren Chen", "Ermo Hua", "Xingtai Lv", "Youbang Sun", "Yang Li", "Dong Li", "Fuchu He", "Bowen Zhou"], "abstract": "With the development of artificial intelligence, its contribution to science is evolving from simulating a complex problem to automating entire research processes and producing novel discoveries. Achieving this advancement requires both specialized general models grounded in real-world scientific data and iterative, exploratory frameworks that mirror human scientific methodologies. In this paper, we present PROTEUS, a fully automated system for scientific discovery from raw proteomics data. PROTEUS uses large language models (LLMs) to perform hierarchical planning, execute specialized bioinformatics tools, and iteratively refine analysis workflows to generate high-quality scientific hypotheses. The system takes proteomics datasets as input and produces a comprehensive set of research objectives, analysis results, and novel biological hypotheses without human intervention. We evaluated PROTEUS on 12 proteomics datasets collected from various biological samples (e.g. immune cells, tumors) and different sample types (single-cell and bulk), generating 191 scientific hypotheses. These were assessed using both automatic LLM-based scoring on 5 metrics and detailed reviews from human experts. Results demonstrate that PROTEUS consistently produces reliable, logically coherent results that align well with existing literature while also proposing novel, evaluable hypotheses. The system's flexible architecture facilitates seamless integration of diverse analysis tools and adaptation to different proteomics data types. By automating complex proteomics analysis workflows and hypothesis generation, PROTEUS has the potential to considerably accelerate the pace of scientific discovery in proteomics research, enabling researchers to efficiently explore large-scale datasets and uncover biological insights.", "sections": [{"title": "1 Introduction", "content": "Proteomics research [1], which focuses on the large-scale analysis of protein expression, functions, and interactions, is a crucial avenue for understanding biological processes and their underlying mechanisms. Modern technologies [2] have facilitated high-throughput proteomics sequencing and large-scale data collection. The resulting datasets hold copious information on proteins, cells, pathways, as well as their complicated relationships and interactions. When combined with scientific analysis methods and domain knowledge, they have the potential to reveal valuable biological insights, including novel biomarkers [3], disease mechanisms [4], and therapeutic targets [5]. On the other hand, the sheer volume and complexity of proteomics data also pose challenges for conventional research techniques and paradigms. Current proteomics research relies heavily on human experts to design and perform data analysis using professional methods and tools, making decisions ranging from specific data manipulation to general research directions. This process brings forward two main issues. First, analysis can be extremely time-consuming, especially when it involves trial-and-error over large sets of possible proteins or sample groups. Second, the researcher's personal knowledge and habits may bias experimental design, potentially impeding comprehensive analysis and limiting its overall scope.\nWe propose that large language models (LLMs) [6-9], the cornerstone of generative artificial intelligence, can enable unprecedented extents of automation in proteomics research. State-of-the-art LLMs possess powerful instruction-following abilities and extensive general knowledge, which have expanded their use cases from simple language tasks to a myriad of professional domains [10, 11]. They have also demonstrated impressive competence and flexibility in realms such as planning complex tasks and calling diverse tools [12, 13]. Additionally, for knowledge-intensive or time-sensitive scenarios, augmenting LLMs with information retrieved from external sources effectively reduces hallucinations and improves accuracy [14, 15]. The primary advantage of LLMs over previous machine learning approaches for scientific tasks is their versatility: instead of being confined to a narrowly defined problem, LLMs can simulate a broad range of tasks integral to scientific discovery workflows.\nIn other words, through representing all decision-making steps, intermediate results, and reasoning processes as sequences of"}, {"title": "2 Results", "content": "Towards the goal of automated proteomics research from raw data, we develop PROTEUS, a system that combines the general abilities of language models with the accuracy of domain-specialized analysis tools and knowledge sources. The system's input is a proteomics dataset consisting of protein expression data and cell or sample metadata. A large language model orchestrates the analysis process and arrives at a list of specific, data-grounded scientific hypotheses."}, {"title": "2.1 PROTEUS System", "content": "Towards the goal of automated proteomics research from raw data, we develop PROTEUS, a system that combines the general abilities of language models with the accuracy of domain-specialized analysis tools and knowledge sources. The system's input is a proteomics dataset consisting of protein expression data and cell or sample metadata. A large language model orchestrates the analysis process and arrives at a list of specific, data-grounded scientific hypotheses."}, {"title": "2.1.1 Large Language Models", "content": "Currently, the gap between proprietary and open-source language models is narrowing, with both showing capabilities for executing complex planning and reasoning tasks. Given the confidential and privacy-sensitive nature of proteomics data, as well as the need for iterative model improvements, we train a general-purpose model with a focus on the biomedical field. Specifically, we train models on biomedical instruction datasets to enhance their capabilities for analysis, planning, and knowledge in biomedicine, thereby improving their performance within our proteomics scientific discovery system. We predominantly adopt the state-of-the-art open-source LLM, Llama 3.1 [7], as our backbone architecture. With 70 billion parameters, it outperforms previous open-source models [6] across a range of tasks. For further domain specialization, we fine-tuned Llama 3.1 70B on the UltraMedical dataset [11], which contains diverse and high-quality biomedical instructions, including a wealth of open-ended questions on biomedical research and literature. The resulting models demonstrate superior performance on downstream tasks compared to other open-source models."}, {"title": "2.1.2 System Framework", "content": "PROTEUS arrives at a comprehensive and meaningful set of hypotheses through navigating possible objectives, executing statistical analysis, and iteratively improving its analysis plans. Considering the complexity and diversity of proteomics research, we devise a hierarchical planning framework consisting of three levels, ranging from general to specific: research objectives, analysis workflows, and analysis steps. This design increases flexibility and robustness in the LLM planning process, which"}, {"title": "2.1.3 Features of PROTEUS", "content": "From Raw Data to Scientific Discoveries. PROTEUS exemplifies a paradigm shift in bioinformatics research by achieving a fully automated pipeline that produces scientific discoveries from raw data. Unlike traditional methods that rely heavily on human intervention and manual data processing, PROTEUS leverages the capabilities of LLMs to autonomously navigate the complete research process. This end-to-end pipeline ensures consistency, reduces the potential for human error, and significantly reduces the time spent between data acquisition and hypothesis generation.\nScalable Integration. Due to its hierarchical planning framework, PROTEUS can seamlessly integrate diverse proteomics analysis tools and knowledge sources. The three-tiered structure, encompassing research objectives, analysis workflows, and individual tools, enables flexible adaptation to heterogeneous proteomics datasets and diverse research directions. Therefore, PROTEUS can conveniently incorporate new analysis methods and external data while maintaining a fixed system framework. Furthermore, the LLM's role in parameter assignment and result interpretation allows PROTEUS to execute specialized bioinformatics tools while maintaining a unified interface. This approach of scalable integration positions PROTEUS to evolve alongside advancements in bioinformatics methods and technologies, ensuring its long-term effectiveness.\nDynamic Feedback Loop. PROTEUS implements an iterative refinement process that mimics the recursive process of scientific inquiry commonly employed by human experts. After each workflow execution and objective analysis, the system reevaluates and refines its subsequent plans based on newly obtained results. This dynamic approach allows PROTEUS to adapt to unexpected findings and pursue promising avenues of research that may not have been initially apparent. By incorporating feedback loops at multiple levels of analysis, PROTEUS can conduct thorough and nuanced investigations, uncovering insights that might be overlooked under linear analysis approaches."}, {"title": "2.2 Evaluation", "content": "We aim for the base language model of the system to be a specialized generalist, achieving enhanced specialization in the biomedical field without compromising its generalization abilities on common tasks. We present the evaluation results of our custom Llama 3.1 models tailored to UltraMedical specifications. We primarily base our evaluation methodology on the protocols outlined in [11] and assess the models across widely recognized medical and general benchmarks. For medical benchmarking, we selected MultiMedQA, which has been extensively employed in MedPaLM-related studies [22, 23]. This benchmark comprises MedQA [24], PubMedQA [25], MedMCQA [26], and biomedical categories within MMLU [27]. We select these tasks to assess the LLMs' application of biomedical knowledge. Additionally, for general instruction following and knowledge integration, we primarily evaluate the models across the comprehensive set of MMLU, GPQA [28], and Alpaca Eval 2 [29] benchmarks.\nThe overall results are listed in Table 2, and the detailed performance in medical domain is reported in Table 3. Our model demonstrates impressive performance across both biomedical and general domain benchmarks. On the MultiMedQA benchmark, our model achieves an average accuracy of 86.30%, surpassing other biomedical-focused models such as Med42-70B (70.74%), OpenBioLM-70B (86.06%), and Med-PaLM 2 (ER) (85.46%). Notably, it also outperforms general domain models including GPT-3.5-Turbo (67.80%) and comes close to GPT-4-Turbo (87.00%). On the Alpaca Eval 2 benchmark, our model shows strong performance with a win rate (WR) of 46.09% and a Likert score (LC) of 43.45%, considerably outperforming other biomedical models and many general domain models. The MMLU benchmark presents similar results. On the GPQA benchmark, our model demonstrates an accuracy of 45.76%, competitive with top-performing general models such as GPT-4-Turbo (49.10%) and Llama-3.1-70B-Instruct (46.70%). Our model acts as the core orchestrator within PROTEUS, supporting its comprehensive planning and reasoning, leading to novel scientific hypotheses."}, {"title": "2.2.1 Base Language Models", "content": "We aim for the base language model of the system to be a specialized generalist, achieving enhanced specialization in the biomedical field without compromising its generalization abilities on common tasks. We present the evaluation results of our custom Llama 3.1 models tailored to UltraMedical specifications. We primarily base our evaluation methodology on the protocols outlined in [11] and assess the models across widely recognized medical and general benchmarks. For medical benchmarking, we selected MultiMedQA, which has been extensively employed in MedPaLM-related studies [22, 23]. This benchmark comprises MedQA [24], PubMedQA [25], MedMCQA [26], and biomedical categories within MMLU [27]. We select these tasks to assess the LLMs' application of biomedical knowledge. Additionally, for general instruction following and knowledge integration, we primarily evaluate the models across the comprehensive set of MMLU, GPQA [28], and Alpaca Eval 2 [29] benchmarks.\nThe overall results are listed in Table 2, and the detailed performance in medical domain is reported in Table 3. Our model demonstrates impressive performance across both biomedical and general domain benchmarks. On the MultiMedQA benchmark, our model achieves an average accuracy of 86.30%, surpassing other biomedical-focused models such as Med42-70B (70.74%), OpenBioLM-70B (86.06%), and Med-PaLM 2 (ER) (85.46%). Notably, it also outperforms general domain models including GPT-3.5-Turbo (67.80%) and comes close to GPT-4-Turbo (87.00%). On the Alpaca Eval 2 benchmark, our model shows strong performance with a win rate (WR) of 46.09% and a Likert score (LC) of 43.45%, considerably outperforming other biomedical models and many general domain models. The MMLU benchmark presents similar results. On the GPQA benchmark, our model demonstrates an accuracy of 45.76%, competitive with top-performing general models such as GPT-4-Turbo (49.10%) and Llama-3.1-70B-Instruct (46.70%). Our model acts as the core orchestrator within PROTEUS, supporting its comprehensive planning and reasoning, leading to novel scientific hypotheses."}, {"title": "2.2.2 Quantitative Evaluation", "content": "We conducted experiments and quantitative evaluation on two types of proteomics data. First, we used the Single-cell Proteomic DataBase (SPDB) [36] to obtain 10 single-cell datasets which used cytometry by time-of-flight (CyTOF) [37] sequencing technology. 9 datasets were sequenced on various human tissues, and 1 dataset covered mouse brain tumor tissue. For all experiments on SPDB, PROTEUS 's input was a SingleCellExperiment [38] object directly downloaded from SPDB and a textual data description constructed using information from the data object and the SPDB website. For every dataset, we set the maximum number of total research objectives to 3 and instructed PROTEUS to generate 5 hypotheses for each objective. On several objectives, PROTEUS produced less than 5 hypotheses due to a lack of notable results from the analysis. We collected a total of 147 hypotheses for CyTOF data. In addition, to demonstrate the flexibility of PROTEUS, we obtained two clinical proteomics datasets from previous publications on hepatocellular carcinoma (HCC) [39] and glioblastoma (GBM) [40]. These datasets contain bulk proteomics data sequenced using mass spectrometry (MS) [41] and cover significantly larger numbers of proteins than the previously described SPDB datasets. The system's input for each dataset was 2 files containing the raw protein"}, {"title": "2.2.3 Verifying Evaluation Quality.", "content": "Comparison with human scoring. We randomly selected two datasets (Datasets 3 and 4) from SPDB, corresponding to 30 hypotheses. Human experts in proteomics research scored these hypotheses over the 5 metrics, following the same instructions provided to the LLM evaluator. These results, shown in Figure 7, demonstrate the rigor and validity of automatic scoring. For all metrics except Novelty, the average scores given by human evaluators were higher than those from LLM automatic evaluation, indicating that automatic evaluation was generally more sensitive to minor errors or discrepancies. In addition, automatic and human scoring showed reasonable levels of agreement across all metrics.\nComparison of different evaluator LLMs. In all previous experiments, we used GPT-4o as the automatic evaluator. We subsequently reran the evaluation procedure on all SPDB results using three other language models as evaluators: Gemini-flash-1.5, Qwen2.5-7B-Instruct, and Llama-3.1-8B-Instruct. We present the average scores on the 5 metrics and the overall average scores in Figure 8. The resulting scores were close on all metrics, particularly for the overall average score, confirming the robustness of our automatic evaluation procedure."}, {"title": "2.2.4 Case Studies", "content": "We highlight PROTEUS 's ability to propose insightful and novel hypotheses through in-depth evaluation by human experts. We selected subsets of hypotheses from both types of proteomics data. Human experts reviewed the stated statistical trends and resulting hypotheses with reference to the original research paper and dataset. In the following section, we expand on several notable hypotheses to demonstrate PROTEUS 's advantages as well as limitations.\nPROTEUS identifies trends on rarely-studied biological topics and proposes novel hypotheses. In our experiments, PROTEUS often focused on proteins or cell types that were seldom studied in the considered context, enabling novel and valuable"}, {"title": "3 Discussion", "content": "In this paper, we introduced PROTEUS, an end-to-end, fully automatic system for scientific discovery from raw proteomics data. An LLM acts as the core coordinator of the system, performing hierarchical planning, analysis tool calling, iterative feedback and refinement, and hypothesis proposal. We incorporated a large number of professional bioinformatics tools and organized them into analysis workflows that can be conveniently called by the system to investigate specific datasets. In this way, we have built upon the capabilities of the base LLM to form a system that better adheres to the empirically grounded and exploratory nature of scientific research.\nWe performed detailed evaluation on PROTEUS 's outputs both quantitatively and qualitatively. We constructed a set of 5 metrics and corresponding instructions, then used LLMs to perform large-scale automatic evaluation on a total of 191 hypotheses from two proteomics dataset types. Detailed reviews and scoring from 4 human experts corroborated the reliability and rigor of our automatic evaluation method. Experts also identified a number of novel hypotheses that point out promising directions for further research. Through examining these notable cases, we highlighted PROTEUS 's ability to pinpoint underexplored biological topics, couple specific quantitative results with general domain knowledge, and establish connections between multiple statistical trends or biological entities. Capabilities such as these empower the system to progress past surface-level observations to perform in-depth scientific reasoning and discovery. In general, results demonstrate that PROTEUS consistently produces reliable results, is capable of forming novel and insightful hypotheses, and can be easily adapted to different data types. Our"}, {"title": "4 Method", "content": "In this section, we provide an overview of the main analysis workflows and tools available to PROTEUS in our main experiments, explaining the role of language models in flexibly and correctly configuring the tools."}, {"title": "4.1 Analysis Workflows and Tools", "content": "In this section, we provide an overview of the main analysis workflows and tools available to PROTEUS in our main experiments, explaining the role of language models in flexibly and correctly configuring the tools."}, {"title": "4.1.1 Analyzing CyTOF Data", "content": "Workflows in this section are tailored towards analyzing proteomics data obtained from CyTOF sequencing.\nFlowSOM Clustering and Cell Type Annotation This workflow clusters single cells based on protein expression, extracts highly expressed cell marker proteins of each cluster, then performs cell type annotation on the clusters. For clustering, we use the CATALYST [74] package to execute the FlowSOM [75] algorithm, a self-organizing map-based method designed for flow or mass cytometry data. We set the inital cluster number to 30. We use the scran [76] package to identify the top 10 cell markers of each cluster to prepare for automatic cell type labeling.\nPrevious research [77] has shown that GPT4 can generate cell type annotations given cell markers and the tissue type, achieving higher degrees of agreement with human annotations compared with conventional reference-based approaches. Therefore, we similarly use GPT-40 for labeling and designed the following prompt:"}, {"title": "4.1.2 Analyzing Clinical Cohort Data", "content": "We provide a different set of workflows for PROTEUS to analyze clinical cohort data from mass spectrometry sequencing. All workflows excluding survival analysis and correlation analysis were conducted using the BioEnricher [81] package. Here we operate on a BioEnricher object that is constantly updated with each executed workflow, instead of a SingleCellExperiment object for CyTOF data. We create the data object using two csv files containing protein expression data and clinical metadata respectively, filtering out entries containing more than 25% missing values and imputing all remaining missing values using kNN. The system automatically generates the data description from the raw csv files and takes no additional input information.\nDifferential Expression Analysis For parameter selection of Differential Expression Analysis (DEA), we adopt a similar procedure as differential analysis for CyTOF data, allowing PROTEUS to select a single metadata field, followed by one or more sets of conditions for comparison. We use the limma algorithm in BioEnricher to calculate top up-regulated and down-regulated proteins, as well as relevant statistics such as log fold changes and P values."}, {"title": "4.1.3 Accessing External Data", "content": "We include additional workflows for PROTEUS to reference external information from datasets or databases based on biological molecules and diseases of interest."}, {"title": "4.2 Prompt Engineering in Automatic Evaluation", "content": "Here we provide the full prompts we used for performing automatic scoring on each of the 5 metrics."}]}