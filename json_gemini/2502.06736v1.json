{"title": "Low-power Spike-based Wearable Analytics on RRAM Crossbars", "authors": ["Abhiroop Bhattacharjee", "Jinquan Shi", "Wei-Chen Chen", "Xinxin Wang", "Priyadarshini Panda"], "abstract": "This work introduces a spike-based wearable analytics system utilizing Spiking Neural Networks (SNNs) deployed on an In-memory Computing engine based on RRAM crossbars, which are known for their compactness and energy-efficiency. Given the hardware constraints and noise characteristics of the underlying RRAM crossbars, we propose online adaptation of pre-trained SNNs in real-time using Direct Feedback Alignment (DFA) against traditional backpropagation (BP). Direct Feedback Alignment (DFA) learning, that allows layer-parallel gradient computations, acts as a fast, energy & area-efficient method for online adaptation of SNNs on RRAM crossbars, unleashing better algorithmic performance against those adapted using BP. Through extensive simulations using our in-house hardware evaluation engine called DFA_Sim, we find that DFA achieves upto 64.1% lower energy consumption, 10.1% lower area overhead, and a 2.1\u00d7 reduction in latency compared to BP, while delivering upto 7.55% higher inference accuracy on human activity recognition (HAR) tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "The rise of wearable technologies in edge computing has drawn attention to tasks such as diagnostics, smart healthcare, and fitness monitoring, which often involve real-time time-series data processing (Fig. 1(a)) [1]-[3]. Wearable devices typically operate under low power budgets of less than 1W, especially for tasks like human activity recognition (HAR), physiological monitoring, and predictive health diagnostics [4]\u2013[6]. This makes energy-efficiency crucial in edge systems. Traditional deep learning models, while powerful, are energy-intensive due to dense matrix multiplications and frequent memory access, making them unsuitable for low-power wearables [7], [8]. Today, Spiking Neural Networks (SNNs) have emerged as an energy-efficient alternative with their sparse & event-driven binary spike processing, particularly suited for real-time temporal tasks like ECG, EEG, motion tracking, and speech analysis [1], [6], [9], [10].\nFrom a hardware implementation standpoint, In-memory Computing (IMC) with analog crossbar arrays enables compact, energy-efficient dot-product operations with high parallelism [11], [12]. Unlike traditional von-Neumann architectures like GPUs and TPUs, IMC crossbars keep neural network's weights stationary, reducing data transfer overhead between the memory and the compute units. This is ideal for wearables, which have stringent area and power constraints. IMC-implemented SNNs, with their high spike sparsity and binary computations, offer reduced peripheral circuit and data communication overhead, enhancing energy-efficiency and throughput [13]-[15].\nHowever, IMC-implemented SNNs are prone to non-idealities due to the analog nature of dot-product operations over multiple timesteps [13], [16], [17]. These non-idealities arise from the limited precision and variations in the non-volatile memory (NVM) devices in the crossbars, leading to inaccurate dot-products and reduced inference accuracy [16], [18]. Variation-aware Training (VAT) is widely employed to improve robustness of neural networks against hardware non-idealities [19], [20]. Thus, online adaptation of pre-trained models to the specific hardware conditions & non-idealities is imperative for resource-constrained edge platforms like IMC crossbars (see Fig. 1).\nAdapting SNNS on edge devices with minimal energy, latency, and area overhead, while maintaining algorithmic performance, is paramount. However, traditional backpropagation (BP) for online adaptation on IMC platforms faces several challenges: (a) layer-by-layer gradient propagation across multiple timesteps is latency-intensive, and (b) BP requires transposable crossbars, thereby increasing the energy and area costs of peripheral circuits [21], [22]. To address these limitations, we propose a Direct Feedback Alignment (DFA) strategy for online adaptation of pre-trained SNNs on RRAM crossbars, enabling robust inference. Unlike BP, DFA uses localized gradient learning to simultaneously fine-tune all SNN layers on hardware [23]. The key contributions of our work are as follows:\n1) Development of DFA_Sim, our in-house evaluation engine for analyzing the hardware costs of DFA-based online adaptation compared to BP on RRAM crossbars.\n2) For hardware-realistic SNN accuracy evaluations using DFA_Sim, we propose an accurate noise prediction model for RRAM devices in the crossbars using Gaussian Process Regression [24] with experimental data from a real IMC chip called NeuRRAM [25].\n3) Our experiments show that DFA-based SNN adaptation on a HAR task [26] incurs 2.1\u00d7 lower latency, 64.1% lower energy, 10.1% lower area, and 7.55% higher inference accuracy on RRAM crossbars compared to BP (see Fig. 2)."}, {"title": "II. BACKGROUND", "content": "Spiking Neural Networks: As shown in Fig. 3(a), a key characteristic of SNNs is their use of a distinct neuronal activation function, typically the Leaky-Integrate-and-Fire (LIF) model, for temporal signal processing, in contrast to the ReLU activation commonly employed in Artificial Neural Networks (ANNs). The LIF neuron i, with its associated membrane potential $u_i^t$, integrates a series of spike inputs as follows:\n$u_i^t = \\lambda u_i^{t-1} + \\sum_j W_{ij}o_j$.\nHere, t stands for the timestep, $W_{ij}$ for weight connections between neuron i and neuron j and $\\lambda$ denotes the leak factor. The LIF neuron i generates an output spike $o_i^t$ at the end of timestep t if the membrane potential exceeds a threshold $\\theta$:\n$o_i^t =\\begin{cases}1 & \\text{if } u_i^t > \\theta, \\\\ 0 & \\text{otherwise.} \\end{cases}$\nWhen the neuron fires, its membrane potential resets to zero. The integrate-and-fire mechanism of an LIF neuron leads to a non-differentiable function, which complicates the use of backpropagation for training SNNs. To overcome this challenge, techniques such as Surrogate Gradient Learning or Backpropagation Through Time (BPTT) approximate the backward gradient function [27], enabling direct learning from spikes with fewer timesteps. Additionally, BPTT-based training of SNNs can be implemented using widely-used machine learning frameworks like PyTorch [28].\nMoreover, in line with prior work [29], we use direct encoding to convert the input tensor into spike trains across a total of T timesteps. For the final prediction, we run the inference over T timesteps (t = 1, 2, ..., T) and compute the average of the outputs from the SNN classifier.\nAnalog IMC Crossbars: Analog crossbars comprise of a 2D array of NVM devices, interfaced with Digital-to-Analog Converters (DACs), Analog-to-Digital Converters (ADCs), and write circuits dedicated towards programming the NVM devices [11], [12]. The SNN's spike inputs are encoded as analog voltages $V_i$ to each row of the crossbar by the DACs, while weights are programmed as NVM device conductances ($G_{ij}$) at the cross-points (we use RRAM as NVM devices in this work), as shown in Fig. 3(b).\nTo emulate dot-product operations in an ideal N\u00d7N crossbar during inference, input voltages interact with device conductances, generating currents according to Ohm's Law. Based on Kirchhoff's current law, the total output current sensed at each column j by the ADCs is the sum of currents flowing through all devices, expressed as $I_j(ideal) = \\sum_i G_{ij} * V_i$. However, in practical applications, the analog nature of computation introduces non-idealities, such as variations in non-volatile memory (NVM) devices [18], [30]. Consequently, the net output current at each column j deviates from the ideal value $I_j(ideal)$, leading to significant accuracy degradation in SNNs implemented on crossbars [13], [16].\nDirect Feedback Alignment (DFA): Direct Feedback Alignment (DFA) is a recent learning approach designed to address some of the key bottlenecks of traditional BP [23]. During the backward pass in DFA-based learning, as shown in Fig. 4, the feedback signals are aligned directly with the output errors e by fixed, randomized feedback matrix connections (B). This method decouples the layerwise sequential process in BP to compute gradients for each layer 1 ($\\delta_l = e.B_l$) simultaneously, contrary to the gradient computation $\\delta_l = e_l.W^T$ in BP. DFA, thus, enables parallel weight update of all layers by locally calculating gradients. We will see in Section IV that the compatibility of DFA with analog IMC platforms makes it a promising solution to perform online noise adaptation of deployed SNNs in real-time."}, {"title": "III. DFA_Sim: HARDWARE EVALUATION ENGINE", "content": "Architectural Details: DFA_Sim is a Python-based hardware evaluation engine to benchmark energy, latency & area costs of DFA-based online adaptation of SNNs on a monolithic IMC chip built upon analog RRAM crossbars. As shown in Fig. 5, it deploys SNN models on a hierarchical, weight-stationary tiled architecture, similar to SpikeSim [17]. DFA_Sim features an array of interconnected tiles with global buffers, accumulators, LIF activation units, and pooling units implemented digitally. The global LIF activation unit (GradLIF), based on [31], supports LIF operations in the forward pass and gradient calculations in the backward pass. Each tile contains 4 Processing Engines (PEs), input/output buffers, and accumulation modules for partial sums. Each PE includes 4 analog 256\u00d7256 IMC crossbars using 4-bit RRAM devices [25] that perform dot-product operations, along with peripheral circuits such as input decoders, ADCs, write drivers, shift adders, etc. The PEs compute dot-products $x_l.W_l$ in the forward pass and $\\delta_l = e.B_l$ during the backward pass (see Fig. 4) for a given SNN layer l. Weight gradients are calculated using dot-products $\\delta_l.\\alpha_l$ in specialized digital SRAM-based Weight Gradient Units (WGUs) [21]. Global H-trees connect tiles and the global buffer, while local H-trees manage communication within each tile and PE.\nDifferences between Hardware for DFA and BP: BP computes gradients $\\delta_l = e_l.W_l^T$ layer-by-layer (see Fig. 4), requiring transposable crossbars in PEs to handle both $x_l.W_l$ in the forward pass and $e_l.W_l^T$ in the backward pass. This is achieved by duplicating peripheral circuits on the row-side of the crossbars [21], [25]. In contrast, DFA removes the need for transposable crossbars, reducing the significant area overhead of peripheral circuits. However, DFA requires additional RRAM crossbars for feedback error signal computation using random B matrices. Section IV shows that the area savings from eliminating transposable crossbars outweigh the extra cost of the feedback crossbars. Another interesting facet of using RRAM crossbars is that the intrinsic stochasticity of RRAM devices can be exploited to generate and store the random B matrices [32], [33].\nMapping SNNs: For SNN weight mapping, we adopt the standard approach proposed in SpikeSim [17], assuming that no two layers of an SNN can be mapped onto the PEs within a single tile. To enable DFA, we allocate an additional tile to store random B matrices in the RRAM crossbars. Given that this work focuses on tasks like Human Activity Recognition (HAR) that are executed on highly resource-constrained hardware, simpler SNN architectures such as multi-layer perceptrons (MLPs) are preferred (see Table I) [6]. Due to the smaller scale of the SNN architectures and the smaller size of the B matrices in DFA compared to $W^T$ in BP, all B matrices can easily fit within a single tile (comprising 16 RRAM crossbars of size 256x256) and we consider duplication of B matrices in the crossbars to accelerate batch-training."}, {"title": "Integrating Realistic RRAM Noise Model", "content": "Integrating Realistic\nRRAM Noise Model:\nDFA_Sim is equipped\nwith an accurate noise\nmodel for RRAM\ndevices that predicts\nnon-ideal conductance\n$G_{non-ideal}$ from the\nideal conductance\n$G_{ideal}$. SNN weights\nare first mapped to\nRRAM conductances\nin the range $G_{ideal} \\in [G_{min}, G_{max}]$ with 4-bit precision. However, when dot-product operations are carried out in the crossbars, the conductances suffer from non-idealities stemming from the RRAM device variations.\nWe use Gaussian Process Regression (GPR) to train our RRAM noise prediction model with experimental data acquired from a real IMC chip called NeuRRAM [25] and use it to predict the noise $\\Delta G = G_{non-ideal} - G_{ideal}$ injected into the programmed RRAM conductances. Before noise-modelling with GPR using the GPyTorch package, data acquired from the NeuRRAM chip was cleaned to select conductances in the range of $G_{min} = 1\\mu S$ to $G_{max} = 10\\mu$.\nThereafter, the raw data from the chip was randomly divided into training (80%, 6920 samples) and testing (20%, 1730 samples) datasets to train the noise prediction model for 100 epochs. Fig.7 shows the fitting results with the testing dataset. We find good agreement between the ground-truth $G_{real}$ and the predicted $\\Delta G_{pred}$ with an RMSE = 0.83$\\mu S$."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "Experimental Setup: We conducted experiments using BPTT-trained SNN MLPs on two human activity recognition (HAR) tasks (UCI-HAR [34] and HHAR [26]), and one image classification task (Fashion MNIST [35]), as summarized in Table I. UCI-HAR includes data from 30 subjects performing six activities (walking, walking upstairs, walking downstairs, sitting, standing, and lying) using accelerometer and gyroscope sensors from a Samsung Galaxy SII (50 Hz sampling rate). HHAR involves data from 9 subjects performing six daily activities (biking, sitting, standing, walking, stair up, and stair down) using accelerometers from 8 smartphones and 4 smartwatches (sampling rates between 50 and 200 Hz). Fashion MNIST consists of 28\u00d728 grayscale images from 10 classes. The pre-trained SNNs were adapted online on non-ideal RRAM crossbars for 25 epochs. We used the DFA_Sim engine to estimate energy, latency, area and inference accuracy for DFA-based online adaptation and compared it with BP-based adaptation. Energy and latency were calculated per epoch, with a fixed training batch size of 50. Hardware details for DFA_Sim are listed in Table II.\nFrom Fig. 6(a), we observe that DFA-based online adaptation for the HHAR task results in a 64.1% reduction in training energy, primarily due to lower tile-level computation and H-tree data communication costs. Note that both BP and DFA incur a constant DRAM access energy cost (17.7 mJ for the HHAR task), which is not shown in Fig. 6(a) or Table I. Additionally, DFA reduces the overall area by 10.1% compared to BP, largely because of the elimination of transposable crossbars and their associated peripherals. While DFA requires an extra tile for the $\\delta_l = e.B_l$ operations and a larger WGU area to handle simultaneous weight updates, the 13.4% area saved by removing transposable crossbars offsets the 3.29% increase in WGU area. Fig. 6(b) also demonstrates that DFA achieves a 2.1\u00d7 overall speedup by processing all SNN layers concurrently, primarily by reducing latency at the tile, WGU, and data communication levels.\nTable I presents overall results to underscore the efficacy of DFA-based online adaptation of SNNs in real-time. While naively deploying SNNs on the IMC platform significantly reduces their performance (~ 27 - 51% loss in accuracy), online adaptation with DFA can restore their performance by reducing the accuracy losses to ~ 38%, compared to the FP32 software baseline. In fact, DFA leads to better performance (~ 1 8% higher non-ideal accuracy) than traditional BP-based online adaptation. This is because the layer-sequential gradient propagation in BP results in error accumulation due to non-idealities affecting the SNN weights. However, as DFA decouples gradient computations at a given SNN layer from its predecessors, error accumulation is eliminated. Furthermore, DFA achieves ~ 60 - 68% lower energy at ~ 10-13% lower area and ~ 2\u00d7 lower latency than BP."}, {"title": "V. CONCLUSION", "content": "To the best of our knowledge, this work for the first time proposes DFA as a low-cost and efficient method for online adaptation of pre-trained SNNs on resource-constrained and non-ideal edge devices. Our in-house DFA_Sim engine highlights the significant energy, area, and latency benefits of DFA over traditional BP for real-time learning on an RRAM-based IMC platform. Furthermore, with a realistic RRAM noise prediction model integrated with DFA_Sim, we show SNNs adapted using DFA to achieve better non-ideal accuracy compared to BP. These findings underscore the potential of DFA-based online adaptation for advancing low power, spike-based analytics in wearable and edge computing applications."}]}