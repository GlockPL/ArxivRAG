{"title": "Data-Driven Radio Propagation Modeling using Graph Neural Networks", "authors": ["Adrien Bufort", "Laurent Lebocq", "St\u00e9fan Cathabard"], "abstract": "Modeling radio propagation is essential for wireless network design and performance optimization. Traditional methods rely on physics models of radio propagation, which can be inaccurate or inflexible. In this work, we propose using graph neural networks to learn radio propagation behaviors directly from real-world network data. Our approach converts the radio propagation environment into a graph representation, with nodes corresponding to locations and edges representing spatial and ray-tracing relationships between locations. The graph is generated by converting images of the environment into a graph structure, with specific relationships between nodes. The model is trained on this graph representation, using sensor measurements as target data.\nWe demonstrate that the graph neural network, which learns to predict radio propagation directly from data, achieves competitive performance compared to traditional heuristic models. This data-driven approach outperforms classic numerical solvers in terms of both speed and accuracy. To the best of our knowledge, we are the first to apply graph neural networks to real-world radio propagation data to generate coverage maps, enabling generative models of signal propagation with point measurements only.", "sections": [{"title": "I. INTRODUCTION", "content": "Radio propagation is a crucial aspect of wireless communication and has been the subject of extensive research for decades. Accurate models of radio propagation are essential for the design and optimization of wireless networks. Recently, the field of machine learning has shown tremendous promise in a variety of applications, including the modeling of radio propagation. In this work, we will use deep learning (and specifically graph neural network [1]) to create radio propagation model that is able to generate radio coverage without any physical heuristic.\nCurrently \"classic\" numerical solver dominate the radio propagation simulator landscape but a lot of work have already been done to create machine learning based radio propagation model [2]. For example work have been done to distill classic solver into neural network using simulated data to calibrated neural network for indoor or outdoor environment ( [3], [4]). Using real world data it is possible to create a radio propagation simulator from scratch that take into account the geographic topology (buildings, forest etc) ( [5], [6]). Those work often forecast only point values which limit their ability to be fast to estimate a coverage map (due to the necessity of doing features engineering for each points)."}, {"title": "II. RELATED WORK", "content": "In this work we will have several contributions :\n- Create a specific neural network architecture that enable ray tracing behaviour to be incorporated smoothly into the neural network output. We will use graph neural network, a specific kind of neural network that can \"pass\" information from points to points according to a graph topology.\n- A training procedure to learn to do radio propagation coverage from point measurements using neural masking to create models that are able to output full coverage of radio propagation signal but trained only on point measurements, which demonstrates its ability to generalize and generate coverage maps from limited data."}, {"title": "A. Machine learning and neural networks", "content": "Machine learning is a sub-field of artificial intelligence that focuses on the development of algorithms that can learn patterns in data and make predictions based on that learning. Supervised learning is a type of machine learning in which the algorithms are trained on labeled data, and the goal is to learn a mapping from input variables to output variables. Supervised learning can be mathematically described as a function approximation problem. Given a set of input-output pairs, (x1,Y1), (x2,Y2), ..., (xn, Yn), the goal is to find a function f(x) that maps input variables x to output variables y such that f(xi) \u2248 yi for all i = 1, 2, ..., n. The goal is to find the parameters of the function f(x) that minimize the average loss over the entire dataset. The loss here is often the Mean Square Error (MSE) :\nMSE = \\frac{1}{n}\\sum_{i=1}^{n}(f(x_i) - y_i)^2 \\qquad(1)\nThis process is known as training and the learned function f(x) is then used for prediction on new, unseen data. There is a various set of learning algorithm, in this work we will choose artificial neural network as our main support to learn. An artificial neural network (ANN) is a type of machine learning model. It consists of interconnected nodes, called artificial neurons, which are organized into layers. We refer the reader to the extensive litterature around neural network for a better understanding of its properties [7], [8].\nThe ANN is a function f(x;0) parameterized by weights 0, the goal of training is to find the optimal weights 0* that minimize the loss function L(0):\n0* = arg min L(0) \\qquad(2)"}, {"title": "B. Graph neural networks", "content": "A graph neural network (GNN) is a type of neural network designed to process graph-structured data. In a graph, nodes represent entities and edges represent relationships between those entities. A GNN operates on the nodes of the graph and leverages the relationships between them to make predictions. Unlike traditional neural networks, which are designed for Euclidean structured data, GNNs can effectively handle non-Euclidean structured data, such as social networks, molecular graphs, and road networks. GNN have also been successfully applied to simulate physics [9]\u2013[11] and this motivated our approach to use them.\nEach node in a GNN has a feature representation, denoted as hi, which summarizes the information of the node. The feature representations are updated iteratively based on the representations of its neighboring nodes, using a message-passing mechanism. Mathematically, the update can be described as two functions :\nMessage from node j to node i:\nmji = MessageFunction(hj, hi, ej\u2192i) \\qquad(3)\nNew representation of node i:\nh'; = ReduceFunction(mj\u2192i | j \u2208 N\u2081) \\qquad(4)\nIn these equations, i and j represent nodes in the graph, h represents the hidden state of a node, e represents the edge between two nodes, and m represents the message passed between two nodes. Ni represents the set of neighboring nodes of node i, and MessageFunction and ReduceFunction are functions that operate on the hidden states and edges to generate messages and reduce them to new node representations.\nIn this work, we employ a variant of the graph neural network called graphNetworks blocks (Message Passing GNN) [11]. We choose this neural architecture for two main reasons: We wanted to leverage the invariance property of GNN, which takes into account the relations between positions. We wanted to benefit from the topological flexibility of GNN, allowing us to create a customized graph to better account for ray tracing behavior."}, {"title": "C. Masked output for training", "content": "In a masked output training procedure, a portion of the output is masked, or hidden, during training, and the neural network must predict the masked values. This can be used to evaluate the performance of the neural network on previously unseen data, or to perform multi-task learning, where the neural network must perform multiple prediction tasks simultaneously.\nLet y be the ground truth output and \u0177 be the predicted output, with a portion of the output denoted as m, where mi = 1 indicates that the i-th component of the output is masked (which means we have the information about the data point) and mi = 0 indicates that it is not masked (we don't have any information). The loss function for masked output training is defined as:\nL(0) = \\frac{1}{n}\\sum_{i=1}^{n}m_i (y_i \u2013 \\hat{y}_i(0))^2 \\qquad(5)\nwhere @ are the parameters of the neural network, n is the number of samples, and (Yi, \u0177i(0)) is the i-th sample of the ground truth output and the predicted output, respectively. The loss function only penalizes the masked components of the output, allowing the neural network to make predictions for the unmasked components while learning to predict the masked components. The parameters 0 are updated during training to minimize the loss function, using a optimization algorithm such as gradient descent or Adam.\nWe will use the masked output training procedure in our case because we only have partial measurement concerning the area we want to forecast.\nGNN are particulary useful for doing this kind of semi-supervised learning and have been design to handle those partial target behaviour [13]."}, {"title": "III. DATA COLLECTION AND PREPROCESSING", "content": "To create our radio propagation model, we utilized three main datasets: the measurement dataset, the geographic dataset, and the antenna dataset. The measurement dataset consists of actual field measurements of radio signal strength, obtained from various locations in France. The geographic dataset provides information about the surrounding environment, including building type, building height, ground height, and other relevant factors that may affect radio propagation. Finally, the antenna dataset contains information about the antenna configurations used in the measurements, such as azimuth, height, frequency, and antenna diagram.\nBefore we could use these datasets to train our machine learning model, we performed several preprocessing steps to ensure that the data was clean and appropriately formatted. In the following sections, we describe the details of each dataset and the preprocessing steps performed to prepare the data for machine learning."}, {"title": "A. Overview of the dataset and its characteristics", "content": "Here is a quick description of the different datasets used for the training and validation :\n- An actual field measurement dataset that contains 300M points of signal power measurement on all the territory of France in the year 2022.\n- The geographic context about the position of buildings near the antennas (buildings heights / type of building structure) and also the ground height. We limit ourself to a surface of 2kmx2km at a resolution of 5m (which create a 400x400 pixels images).\n- The antenna configuration and antenna diagram (the azimut and tilt of the antenna, its height, its frequency, its antenna gains and its EIRP (Effective Isotropic Radiated Power)) of all the set of antennas in France."}, {"title": "B. Dataset of measurements", "content": "Thanks to the collection of data through the 'Orange et Moi' mobile application, we have access to 300 million data points of cell signal power measurements across France (figures 1 and 2). These measurements were taken by Orange employees and are limited to 4G cells only. Each measurement point contains information such as the GPS coordinates, which are obtained directly from the mobile and have a high level of precision (with an estimated location error of less than 20 meters), making them critical for the model's performance. Other information available in our dataset includes the ID of the connected cell (from the 20,000 sites and 200,000 cells in the dataset), the date of the measurement (limited to the year 2022 and provided in UTC), the actual signal power received from the connected cell (in dB, although the precision of this value is speculative and dependent on the mobile type and configuration), and the speed of the mobile/user. The latter is a strong indicator of the type of environment the mobile is in.\nTo get an idea of the dataset scale, here is an image of all the points of measurements in France and in the Paris city :"}, {"title": "C. Geographic dataset", "content": "Various types of geographic information around the antenna are relevant for radio propagation modeling, such as the topology of nearby buildings (including their type and height). In this work we limit ourself to a square of 2kmx2km around the antenna located at the center of the image. We take a resolution of 5m. In future work, we plan to incorporate information about the types of materials used in these buildings to gain more insight into how radio waves interact with different materials.\nTo obtain geographic data, we explored open-source datasets in France, such as those provided by the IGN (Institut national de l'information g\u00e9ographique et foresti\u00e8re / National Institute of Geographic and Forestry Information, https://ign.fr/) or the BNB (base nationale des b\u00e2timents), as well as private datasets. Ultimately, we extracted and rasterized (going from vector data format to image data format) information about all the buildings in the area surrounding the antenna (which is located in the center of the image).\nThe information about the building configuration is important because allows us to estimate the impact of building on radio propagation and thus improving the prediction power of our model."}, {"title": "D. Antennas dataset", "content": "In addition to the geographic dataset, we also incorporate another input into our model to estimate radio propagation around the antenna: the antenna configuration. This includes scalar values such as antenna height, frequency, gain, and EIRP, as well as the antenna diagram. To represent the antenna diagram as input to our model, we generate an image of the antenna diagram attenuation, essentially calculating the attenuation in the direction of the user position . We also use the antenna height (hantenna) and frequency (f) as inputs to the neural network, which are available for all the antenna cells in the Orange network."}, {"title": "IV. GRAPH NEURAL NETWORK MODEL", "content": "There exists a lot of models that can take image as inputs and output another image-like output (UNET [14] FNO [16]). Those models have already been used to model radio propagation [15]. Interestingly Graph Neural Network can also manage images as input (for example in [17]) : one just have to convert the image into a graph using the grid like structure of an image as the graph structure. Here the pixels are the nodes and the edges can be represented as links between pixels that are close to each others.\nOne of the big advantages of GNN is that they can include invariance properties into the model, thus reducing the need for data. Also one can include implicit physics knowledge into the graph structure: we will exploit those 2 opportunities to improve the model performance.\nThe global architecture is represented in the appendix but we represent the inputs-outputs in the image below (figure 8) :"}, {"title": "A. Constructing the Input Graph", "content": "We represent the input image as a graph by converting each pixel into a node. We construct two graphs from this node representation to model different factors affecting radio propagation.\nThe first graph connects nodes that are spatially close in the image, capturing the correlation between received power levels of nearby points due to radio diffusion.\nThe second graph connects nodes that are aligned along potential ray tracing paths from the transmitter antenna. This represents the effect of line-of-sight propagation between the antenna and nodes. We link those node without taking in consideration buildings that could mask the line of sight (no visibility condition) propagation so we just have to compute this graph once (regardless of the buildings / ground height structure).\nWe then pass these two graphs through a graph neural network to learn a model that combines both the influence of spatial node relationships and antenna-node ray tracing on the predicted radio propagation characteristics. By using two graphs to represent different aspects of the environment, the graph neural network can learn how both diffusion and line-of-sight propagation impact received signal strength.\nOne can visualize those 2 graphs in the figures 9 and 10."}, {"title": "B. Explanation of the graph neural network architecture", "content": "As our main neural architecture we choose to use the graph network block paradigm [9] [11]. The final GNN (graph neural network) architecture looks like the figure 12. It is composed of several elements :\n- the node encoder and the edge encoder: those are simply a MLP (Multi Layer Perceptron) that project the nodes of dimension (Nnodes,dnodes) (Nnodes being the number of nodes in the graph and dnodes the original dimension of the input node, here 4 for the 4 spatials inputs) and the edges of dimension (Nedges, dedges) (Nedges being the number of edges in the graph and dedges the original dimension of the input edges, here 2), to standard dimension (Nnodes, dencoder) and (Nedges, dencoder). dencoder is the output dimension of the encoder.\n- the FiLM layers [18] (Feature-wise Linear Modulation) is used to combine scalar inputs with spatial inputs. It is inspired by [19]. The FiLM layer is typically used in conditional deep learning tasks: here we used it to condition the radio propagation to scalar inputs that are the frequency of emision, the antenna height and the EIPR. It's a simple transformation of the nodes features x : FiLM(x) = \u03b3 \u2299 x + \u03b2 where y and \u03b2 are values computed with another MLP.\n- the GraphNetwork block [11] : it is used to propagate messages though the graph (and so nodes features can influence each other in order to improve performance). There is multiple message-passing round with different weights each time. We did test multiple types of GNN (GAT [28], graphnetwork block [11]) and we obtain similar results.\n- the node decoder: a MLP (Multi Layer Perceptron) that map the nodes dimension (Nnodes, ddecoder) to (Nnodes, doutput) where doutput)."}, {"title": "C. Details on the training process", "content": "In this section, we provide details on the training process for our data-driven radio propagation model. We utilized PyTorch and PyTorch Geometric to code the graph neural network and trained the model with various parameters such as learning rate, number of graph network blocks, and dimension of the encoder/decoder output. The training was performed on an NVIDIA GPU A100 40G and took approximately 100 hours to complete.\nTo train the graph neural network, we adopted a semi-supervised approach that allows us to train the GNN with partial output targets, as described in [13]. Specifically, we employed the masked output training procedure explained in Section II.C to train the GNN with partial target values.\nAs our problem is a regression task, we utilized the L2 loss function between the predicted attenuation (pi) and the actual measurement (pi). To update the neural network, we computed the resulting propagation map with the GNN (f(xspatial, Xscalar)) and then calculated the error loss on the points where we have actual measurement values (Lmapi = \\sum_{j\\in M}(f(x_{spatial}, x_{scalar})[pos_j] \u2013 p_j)\u00b2). In the above equation, posj represents the spatial position of the j-th measurement point and M represents the set of all measurement points where we have actual measurement values."}, {"title": "D. Evaluation metrics used to assess model performance", "content": "The main evaluation metric we use is the root mean square error (RMSE), which measures the difference between the predicted signal strength values and the actual values from the validation dataset.\nOne of the key point to make is that we clearly separate the training set from the validation set by separating sites between the training set and the validation set (so one radio sites cannot be in both the validation set and the training set). This is done to avoid data leakage and performance overestimation [22]."}, {"title": "V. RESULTS AND ANALYSIS", "content": "In this section we will details the training procedure and make comparisons with different type of models (physics-based models / heuristics / and different types of GNN)."}, {"title": "A. Training convergence and performance", "content": "The training was done using the Adam optimizer [25] with a learning rate of 1.10\u20134. Due to the high memory requirement of GNN, we could only train one image (graph) per batch. So we technically have a batch size of only one. Here we plot the training loss evolution in the annex section. Three of notable elements during the training :\n- The model doesn't overfit the train set (the loss didn't go to 0 for the training loss) as it is often the case with deep learning model. It is possible that the dataset have a lot of noise and the model can't forecast this pure noise.\n- The training is unstable: we don't have a smooth convergence toward a lower training loss value. We could have stabilized the train loss variation by improving the batch size but the memory requierements to do that were too high (our GPU didn't have enough memory to increase the batch size).\n- There is a very fast convergence as it seems that after 50k training steps the model has already converge.\nBelow a comparison of performance between differents models :"}, {"title": "B. Discussion of the key factors that affect radio propagation, as identified by the model", "content": "One can see the impact of the ray tracing edge on the final result in term of quality of predictions. Also the final visualisation is also very different in term final result (figure 11 vs figure 12). Here an example of the propagation result with ray tracing edges and without raytracing edges.\nThe model (the ray tracing one) is capable of accurately capturing the impact of obstacles located between the antenna and the receiver, which results in a reduction of the received power.\nBy purely visual interpretation of the coverages map, the model also accounts for the significant reduction in received power levels inside buildings compared to outdoor locations. Additionally, the model estimates higher received power levels in the direction of the antenna orientation, by considering the antenna direction/diagram as a contributing factor (appendix section to observe more coverage map estimation)."}, {"title": "VI. FUTURE WORK", "content": ""}, {"title": "A. Areas for future research", "content": "One promising direction for future work is exploring modifications to the graph neural network architecture. The particular GNN architecture used in this work could be adjusted by changing the number of layers and hidden units, or using different message passing approaches such as graphformer [23]. These architectural changes may allow the GNN to better capture the complex relationships in radio propagation. For example, deeper layers or residual connections could enable learning longer-range dependencies, while different message passing methods may be more suited to modeling the spatial and ray tracing relationships in the graph. Testing alternative architectures could reveal insights into the strengths and limitations of GNNs for this application and lead to accuracy improvements.\nAnother promising direction for future work is incorporating physical loss characteristics into the graph neural network like the Physic-Informed Neural Network loss [24] (PINN loss). Sources of loss such as free space loss, material absorption, and diffraction can be encoded as edge or node attributes in the input graph. The GNN can then learn to integrate knowledge of these physical loss mechanisms into its propagation predictions.\nOne potential area of future work is to improve the machine learning methodology. In our approach we try to directly minimize the L2 error in order to calibrate the weights of the GNN. But one other approach and perhaps more accurate one would be to maximize the likelihood of the radio mapping using something like conditional GAN [26]. We let this avenue for future work."}, {"title": "B. Limitations of the current study", "content": "One of the main limitations of our study is that it is based on data point measurements, which can introduce bias into the model. Specifically, the bias arises from the fact that we only get measurements at locations where we are able to perform the measurements, which typically excludes areas behind the antenna where users do not receive a direct signal. Instead, data points measurements in these areas are typically the result of reflections off buildings, which can lead to an overestimation of the radio power received behind the antenna (the model only see data points measurement that are the results of reflections behind antennas and wrongly overestimate the radio power received behind antennas).\nThe bias in our model due to the limited availability of data points is an example of survivor-ship bias [27], which occurs when we only consider the data that has survived a particular selection process (here the user have selected the cell with the strongest received power)."}, {"title": "VII. CONCLUSION", "content": "In conclusion, our research has demonstrated the effectiveness of using graph neural networks for data-driven radio propagation modeling. By leveraging the power of machine learning, we were able to construct a model that accurately predicts radio power levels in complex urban environments, taking into account the effects of buildings, terrain, and other obstacles on radio wave propagation.\nOur approach provides a promising framework for future research in this area, offering a data-driven alternative to traditional models that rely on complex mathematical calculations and approximations. By training our model on a comprehensive dataset of geographical information and radio power measurements, we were able to achieve high levels of accuracy and improve upon existing models that are limited in their ability to account for the complexities of real-world environments."}, {"title": "VIII. APPENDIX", "content": "This section contains details regarding the model architecture and training, with a focus on providing precise information about the model parameters. The aim is to enable accurate reproduction of the neural network architecture."}, {"title": "A. Model global architecture", "content": "The overall architecture of the neural network (figure 13) is a standard encoder-preprocess-decoder graph neural network, which also takes inputs from scalar features. The input features are color-coded in green, with the scalar features represented as a concatenation vector of cell frequency, antenna height, and EIPR, and the node features as spatial information corresponding to a concatenation of building type information, building height, ground height, and antenna diagram loss in LoS (line of sight). The graph information contains all the information at the edge level, including the relationship between the different pixels/nodes of the graph, represented here by the difference in polar coordinates between the pixels/nodes (\u0394\u03b8, \u0394r).\nThe various components of the graph neural network are color-coded in red and orange. The different neural network components are already detailed in section IV.B. Finally, the output propagation map is represented in blue."}]}