{"title": "ADAWM: ADAPTIVE WORLD MODEL BASED PLANNING FOR AUTONOMOUS DRIVING", "authors": ["Hang Wang", "Xin Ye", "Feng Tao", "Abhirup Mallik", "Burhaneddin Yaman", "Liu Ren", "Junshan Zhang"], "abstract": "World model based reinforcement learning (RL) has emerged as a promising approach for autonomous driving, which learns a latent dynamics model and uses it to train a planning policy. To speed up the learning process, the pretrain-finetune paradigm is often used, where online RL is initialized by a pretrained model and a policy learned offline. However, naively performing such initialization in RL may result in dramatic performance degradation during the online interactions in the new task. To tackle this challenge, we first analyze the performance degradation and identify two primary root causes therein: the mismatch of the planning policy and the mismatch of the dynamics model, due to distribution shift. We further analyze the effects of these factors on performance degradation during finetuning, and our findings reveal that the choice of finetuning strategies plays a pivotal role in mitigating these effects. We then introduce AdaWM, an Adaptive World Model based planning method, featuring two key steps: (a) mismatch identification, which quantifies the mismatches and informs the finetuning strategy, and (b) alignment-driven finetuning, which selectively updates either the policy or the model as needed using efficient low-rank updates. Extensive experiments on the challenging CARLA driving tasks demonstrate that AdaWM significantly improves the finetuning process, resulting in more robust and efficient performance in autonomous driving systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Automated vehicles (AVs) are poised to revolutionize future mobility systems with enhanced safety and efficiency Yurtsever et al. (2020); Kalra & Paddock (2016); Maurer et al. (2016). Despite significant progress Teng et al. (2023); Hu et al. (2023); Jiang et al. (2023), developing AVs capable of navigating complex, diverse real-world scenarios remains challenging, particularly in unforeseen situations Campbell et al. (2010); Chen et al. (2024). Autonomous vehicles must learn the complex dynamics of environments, predict future scenarios accurately and swiftly, and take timely actions such as emergency braking. Thus motivated, in this work, we devise adaptive world model to advance embodied AI and improve the planning capability of autonomous driving systems.\nWorld model (WM) based reinforcement learning (RL) has emerged as a promising self-supervised approach for autonomous driving Chen et al. (2024); Wang et al. (2024); Guan et al. (2024); Li et al. (2024). This end-to-end method maps sensory inputs directly to control outputs, offering improved efficiency and robustness over traditional modular architectures Yurtsever et al. (2020); Chen et al. (2024). By learning a latent dynamics model from observations and actions, the system can predict future events and optimize policy decisions, enhancing generalization across diverse environments. Recent models like DreamerV2 and DreamerV3 have demonstrated strong performance across both 2D and 3D environments Hafner et al. (2020; 2023)."}, {"title": "2 ADAWM: ADAPTIVE WORLD MODEL BASED PLANNING", "content": "Basic Setting. Without loss of generality, we model the agent's decision making problem as a Markov Decision Process (MDP), defined as $(S, A, P, r, \\gamma)$. In particular, $S \\subset \\mathbb{R}^{ds}$ represents state space, and $A$ is the action space for agent, respectively. $\\gamma$ is the discounting factor. At each time step t, based on observations $s_t \\in S$, agent takes an action $a_t$ according to a planning policy $\\pi : S \\rightarrow A$. The environment then transitions from state $s_t$ to state $s_{t+1}$ following the state transition dynamics $P(s_{t+1} | s_t, a_t): S \\times A \\times S \\rightarrow [0, 1]$. In turn, the agent receives a reward $r_t := r(s_t, a_t)$.\nPretraining: World Model based RL. In the pretraining phase, we aim to learn a dynamics model WM to capture the latent dynamics of the environment and predict the future environment state. To train a world model, the observation $s_t$ is encoded into a latent state $z_t \\in Z \\subset \\mathbb{R}^{dz}$ using an autoencoder $q$ Kingma (2013). Meanwhile, the hidden state $h_t$ is incorporated into the encoding process to capture contextual information from current observations Hafner et al. (2023; 2020), i.e., $z_t \\sim q_{\\phi}(z_t | h_t, s_t)$, where $h_t$ stores historical information relevant to the current state. We further denote $x_t$ as the model state, defined as follows,\nModel State $x_t := [h_t, z_t] \\in X$,\n(1)\nThe dynamics model WM$_{\\phi}$ predicts future states using a recurrent model, such as an Recurrent Neural Network (RNN) Medsker et al. (2001), which forecasts the next hidden state $h_{t+1}$ based on the current action $a_t$ and model state $x_t$.\nThe obtained hidden state $h_{t+1}$ is then used to predict the next latent state $z_{t+1} \\sim P_{\\phi}(\\cdot | h_{t+1})$. The dynamics model WM$(x_{t+1}|x_t, a_t)$ is trained by minimizing the prediction error between these predicted future states and the actual observations. Additionally, the learned (world) model can also predict rewards and episode termination signals by incorporating the reward prediction loss Hafner et al. (2023).\nOnce trained, the dynamics model is used to guide policy learning. The agent learns a planning policy $\\pi_{\\omega}(x_t)$ by maximizing its value function\n$V^{WM}_{\\pi_{\\omega}}(x_t) = \\mathbb{E}_{x \\sim WM, a \\sim \\pi_{\\omega}} [\\sum_{i=1}^{K} \\gamma^{i}r (x_{t+i-1}, a_{t+i-1}) + \\gamma^{K} Q^{\\omega}(x_{t+K}, a_{t+K})]$,\n(2)\nwhere the first term is the cumulative reward from a K-step lookahead using the learned dynamics model, and the second term is the Q-function $Q^{\\omega}(x_t, a_t) = \\mathbb{E}_{a \\sim \\pi_{\\omega}} [\\sum_{t} \\gamma^{t}r(x_{t}, a_{t})]$, which corresponds to the expected cumulative reward when action $a_t$ is taken at state $x_t$, and the policy $\\pi_{\\omega}$ is followed thereafter. In this way, the policy learning is informed by predictions from the dynamics model regarding future state transitions and expected rewards.\nFinetuning with the Pretrained Dynamics Model and Policy. Once the pretraining is complete, the focus is to finetune the pretrained dynamics model WM and policy $\\pi_{\\omega}$ to adapt to the new task while minimizing performance degradation due to the distribution shift. In particular, at each finetuning step t, the agent conducts planning using the current policy $\\pi_{\\omega}$, and dynamics model WM$_{\\phi_t}$. By interacting with environments, the agent is able to collect samples $\\{x_t, a_t, r_t\\}$ for the new task. In this work, the primary objective is to develop an efficient finetuning strategy to mitigate the overall performance degradation during finetuning. To this end, in what follows, we first analyze the performance degradation that occurs during finetuning due to the distribution shift. From this analysis, we identify two root causes contributing to the degradation: mismatch of the dynamics model and mismatch of the policy."}, {"title": "2.1 IMPACT OF MISMATCHES ON PERFORMANCE DEGRADATION", "content": "Performance Gap. When the pretrained model and policy align well with the state transition dynamics and the new task, the learning performance should remain consistent. However, in practice, distribution shifts between the pretraining tasks and the new task can lead to suboptimal planning and degraded performance when directly using the pretrained model and policy. To formalize this, we let WM$(P)$ denote the pretrained dynamics model with probability transition matrix $P$, and WM$_{\\phi}(P)$ the dynamics model of the new task with probability transition matrix $P'$. For simplicity, we denote the policy as $\\pi = \\pi_{\\omega}$ when the context is clear to avoid ambiguity.\nLet $\\eta$ denote the learning performance of using model WM and policy $\\pi$ in the pretraining task, and let $\\hat{\\eta}$ represent the performance of applying WM$(P)$ and policy $\\pi$ to the new task with environment dynamics $P'$. Using the value function $V(x)$ defined in Equation (2), the performance gap can be expressed as follows:\n$\\eta-\\hat{\\eta} = \\mathbb{E}_{p_0} [V^{WM(P)}(x_0) - V^{WM_{\\phi}(P)}(x_0)]$,\n(3)\nwhere $p_0$ is the initial state distribution and the second term captures the expected return when the agent applies the pretrained model and policy to the new task with the underlying dynamics $P'$. Next, we introduce the latent dynamics model to capture temporal dependencies of the environment.\nLatent Dynamics Model. At each time step t, the agent will leverage the dynamics model WM$_{\\phi}$ to generate imaginary trajectories $\\{x_{t+k}, a_{t+k}\\}_{k=1}^{K}$ over a lookahead horizon $K > 0$. These trajectories are generated based on the current model state $x_t$ and actions $a_t$ sampled from policy $\\pi$. Particularly, the dynamics model is typically implemented as a RNN, which computes the hidden states $h_t$ and state presentation $z_t$ as follows,\n$h_{t+1} = f_h(x_t, a_t), z_{t+1} = f_z(h_{t+1})$,\nwhere $f_h$ maps the current state and action to the next hidden state and $f_z$ maps the hidden state to the next state representation. In our theoretical analysis, following the formulation as in previous works Lim et al. (2021); Wu et al. (2021), we choose $f_h = A x_t + \\sigma_h(W x_t + U a_t + b)$ and $f_z = \\sigma_z(V h_{t+1})$, where matrices $A, W, U, V, b$ are trainable parameters. Meanwhile, $\\sigma_z$ is the $L_z$-Lipschitz activation functions for the state representation and $\\sigma_h$ is a $L_h$-Lipschitz element-wise activation function (e.g., ReLU Agarap (2018)). Next, we make the following standard assumptions on latent dynamics model, action and reward functions.\nAssumption 1 (Weight Matrices) The Frobenius norms of weight matrices $W, U$ and $V$ are upper bounded by $B_W, B_U$ and $B_V$, respectively.\nAssumption 2 (Action and Policy) The action input is upper bounded, i.e., $|a_t| < B_a, t = 1,\\dots$. Additionally, the policy $\\pi$ is $L_a$-Lipschitz, i.e., for any two states $x, x' \\in X$, we have $d_A(\\pi(\\cdot|x) - \\pi(\\cdot|x')) \\leq L_a d_X(x, x')$, where $d_A$ and $d_X$ are the corresponding distance metrics defined in the action space and state space.\nAssumption 3 The reward function $r(x, a)$ is $L_r$-Lipschitz, i.e., for all $x, x' \\in X$ and $a, a' \\in A$, we have $|r(x, a) - r(x', a')| \\leq L_r(d_X(x, x') + d_A(a, a'))$, where $d_X$ and $d_A$ are the corresponding metrics in the state space and action space, respectively.\nCharacterization of Performance Gap. We start by analyzing the state prediction error at prediction step $k = 1, 2, \\dots, K$, defined as $e_k = x_k - \\hat{x}_k$, where $x_k$ is the underlying true state representation in the new task and $\\hat{x}_k$ is the predicted state representation by using the pretrained dynamics model WM. The prediction error arises due to a combination of factors such as distribution shift between tasks and the generalization limitations of the pretrained dynamics model. To this end, we decompose the prediction error into two terms,\n$e_k = (x_k - \\bar{x}_k) + (\\bar{x}_k - \\hat{x}_k)$\n(4)\nwhere $\\bar{x}_k$ is the underlying true state representation when planning is conducted in the pretraining tasks. The first term $(x_k - \\bar{x}_k)$ captures the difference between the true states in the current and"}, {"title": "2.2 ADAWM: MISMATCH IDENTIFICATION AND ALIGNMENT-DRIVEN FINETUNING", "content": "Based on the above analysis, we propose AdaWM with efficient finetuning strategy while minimizing performance degradation. As outlined in Algorithm 1, AdaWM consists of two key components: mismatch identification and adaptive finetuning.\nMismatch Identification. The first phase of AdaWM is dedicated to identifying the dominant mismatch between the pretrained task and the current task and two main types of mismatches are evaluated (line 3 in Algorithm 1): 1) Mismatch of Dynamics Model. AdaWM estimates the Total Variation (TV) distance between dynamics model by measuring state-action visitation distribution Janner et al. (2019). This metric helps quantify the model's inability to predict the current task's state accurately, revealing weaknesses in the pretrained model's understanding of the new task; and 2) Mismatch of Policy. AdaWM calculates the state visitation distribution shift using TV distance between the state visitation distributions from pretraining and the current task.\nAlignment-driven Finetuning. Once the dominant mismatch is identified, AdaWM selectively finetunes either the dynamics model or the policy based on which component contributes more to the performance gap. In particular, AdaWM uses the following finetuning method to further reduce computational overhead and ensures efficiency (line 4-8 in Algorithm 1). 1) Finetuning the Dynamics Model. AdaWM leverages a LoRA-based Hu et al. (2021) low-rank adaptation strategy (as known as NoLa Koohpayegani et al. (2024)) to efficiently update the dynamics model. The model parameters are decomposed into two lower-dimensional vectors: the latent representation base vector Z and vector \u03a6. During finetuning, only the weight B of the base vector is updated, i.e., B' \u2190 B, \u03c6' = (B'Z)T\u03a6. 2) Finetuning Planning Policy. AdaWM decomposes the policy network \u03c9 into a convex combination of sub-units $\u03a9 = \\sum_{i=1}^{D} \u03b4_i\u03c9$. During finetuning, only the weight vector $\u0394 = [\u03b4_1, ..., \u03b4_D]$ of these sub-units are updated, i.e., \u0394' \u2190 \u0394."}, {"title": "3 EXPERIMENTAL RESULTS", "content": "In this section, we evaluate the effectiveness of AdaWM by addressing the following two questions: 1) Can AdaWM help to effectively mitigate the performance drop through finetuning in various CARLA tasks? 2) How does the parameter C in AdaWM impact the finetuning performance.\nExperiments Environment. We conduct our experiments in CARLA, an open-source simulator with high-fidelity 3D environment Dosovitskiy et al. (2017). At each time step t, agent receives bird-eye-view (BEV) as observation, which unifies the multi-modal information Liu et al. (2023); Li et al. (2023). Furthermore, by following the planned waypoints, agents navigate the environment by executing action $a_t$, such as acceleration or brake, and receive the reward $r_t$ from the environment. We define the reward as the weight sum of five attributes: safety, comfort, driving time, velocity and distance to the waypoints. The details of the CARLA environment and reward design are relegated to Appendix C."}, {"title": "3.1 PERFORMANCE COMPARISONS", "content": "In this section, we compare the learning performance among our proposed AdaWM and the baseline algorithms in terms of the time-to-collision (TTC) and success rate (SR,, also known as completion rate), i.e., the percentage of trails that the agent is able to achieve the goal without any collisions.\nEvaluation Tasks. In our experiments, we evaluate the proposed AdaWM and the baseline methods on a series of increasingly difficult autonomous driving tasks. These tasks are designed to assess each model's ability to generalize to the new task and traffic condition. The first task closely mirrors the pretraining scenario, while subsequent tasks introduce more complexity and challenge. In particular, during pretraining, AdaWM is trained on Pre-RTM03 task, which involves a Right Turn in Moderate traffic in Town 03. Following the pretraining, we evaluate the learning performance in four tasks, respectively: 1) Task ROM03: This task is a Roundabout in Moderate traffic in Town 03. While it takes place in the same town as the pretraining task, the introduction of a roundabout adds complexity to the driving scenario; 2) Task RTD12: This task features a Right Turn in Dense traffic in Town 12. The increased traffic density and different town environment make this task more challenging than the pretraining task; 3) Task LTM03: This task involves a Left Turn in Moderate traffic in Town 03. Although it takes place in the same town and traffic conditions as the pretraining task, the switch to a left turn introduces a new challenge; and 4) Task LTD03: The most challenging task as it involves a Left Turn in Dense traffic in Town 03. The combination of heavy traffic and a left turn in a familiar town environment makes this task the hardest in the evaluation set. We summarize the evaluation results in Table 2 and Table 3. We also include the learning curve in Figure 3. Meanwhile, we also include another five challenging scenarios for verify the capability of AdaWM in Appendix G."}, {"title": "3.1.1 EVALUATION RESULTS", "content": "The Impact of Finetuning in Autonomous Driving. Following previous works on the effectiveness of fine-tuning in robotic learning Julian et al. (2021); Feng et al. (2023), we first validate this finding in the autonomous driving domain. As shown in Table 2, AdaWM consistently outperforms its pretrained only counterparts and two supervised learning based methods, i.e., VAD and UniAD, across various tasks. Starting with Task ROM03, which closely resembles the pretraining scenario"}, {"title": "3.2 ABLATION STUDIES", "content": "Determine the Dominating Mismatch during Finetuning. We compare the changes of two mismatches, mismatch of dynamics model and mismatch of policy, for different finetuning strategies in Figure 4. It can be seen from Figure 4a that model-only finetuning often leads to a deterioration in policy performance, as indicated by a significant increase in TV distance. This suggests that while the model adapts to new task, the policy struggles to keep pace, resulting in suboptimal decisionmaking. Conversely, as shown in Figure 4b, policy-only finetuning reduces the TV distance between policies, but this comes at the cost of an increased mismatch of dynamics model, signaling a growing discrepancy between the learned dynamics model and the actual environment. In contrast, in Figure 4c, we show that our proposed alignment-driven finetuning method in AdaWM can effectively align both factors in the new task. By selectively adjusting the model or policy at each step, this adaptive method prevents either error from escalating dramatically, maintaining stability and ensuring better performance throughout the finetuning process.\nThe Impact of Parameter C. In Table 4, we study the effect of different parameter C on four tasks (ROM03, RTD12, LTM03, LTD03) in terms of TTC and SR. Notably, when C becomes too large, AdaWM's performance deteriorates, as it essentially reduces to policy-only finetuning. This is reflected in the sharp drop in both TTC and SR for high C values, such as C = 100, across all tasks. On the other hand, very small C values result in suboptimal performance due to insufficient updates to the dynamics model, underscoring the importance of alignment-driven finetuning for achieving robust learning. Meanwhile, the results demonstrate that AdaWM performs well across a wide range of C values (between 2 and 50). As further illustrated in Figure 5, AdaWM effectively controls mismatches in both the policy and the dynamics model for C = 2, 10, and 50. For instance, in Figure 5b,"}, {"title": "4 CONCLUSION", "content": "In this work, we propose AdaWM, an Adaptive World Model based planning method that mitigates performance drops in world model based reinforcement learning (RL) for autonomous driving. Building on our theoretical analysis, we identify two primary causes for the performance degradation: mismatch of the dynamics model and mismatch of the policy. Building upon our theoretical analysis, we propose AdaWM with two core components: mismatch identification and alignmentdriven finetuning. AdaWM evaluates the dominating source of performance degradation and applies selective low-rank updates to the dynamics model or policy, depending on the identified mismatch. Extensive experiments on CARLA demonstrate that AdaWM significantly improves both route success rate and time-to-collision, validating its effectiveness. This work emphasizes the importance of choosing an efficient and robust finetuning strategy in solving challenging real-world tasks. There are several promsing avenues for future research. First, exploring the generalization of AdaWM to other domains beyond autonomous driving could broaden its applicability. Additionally, extending AdaWM to multi-agent setting that accounts for the interaction among agents could further enhance its robustness in complex real-world environments."}, {"title": "ETHICS STATEMENT", "content": "AdaWM is developed to improve finetuning in world model based reinforcement learning, specifically for autonomous driving applications. The focus is on addressing performance degradation due to distribution shifts, ensuring safer and more reliable decision-making in dynamic environments. While AdaWM aims to enhance the adaptability and robustness of autonomous systems, ethical considerations are paramount. These include ensuring that the system operates safely under real-world conditions, minimizing unintended biases from pretraining data, and maintaining transparency in how decisions are made during finetuning. Additionally, the societal implications of deploying autonomous driving technologies, such as their impact on public safety and employment, require ongoing attention. Our commitment is to ensure that AdaWM contributes positively and responsibly to the future of autonomous driving systems."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure the reproducibility of our results, we will provide detailed documentation and instructions to replicate the experiments conducted on the CARLA environment, along with hyperparameters and experimental settings. These resources will enable researchers and practitioners to reproduce our results and apply AdaWM to other tasks or environments, facilitating further research in adaptive world model based reinforcement learning."}, {"title": "D TERMINOLOGY", "content": "In this work, we distinguish world model and dynamics model. The key difference lies in their scope and functionality. A world model Ha & Schmidhuber (2018) is a comprehensive internal representation that an agent builds to understand its environment, including not just the state transitions (dynamics) but also observations, rewards, and potentially agent intentions. It enables agents to simulate future trajectories, plan, and predict outcomes before acting. In contrast, a dynamics model is a more specific component focused solely on predicting how the environment's state will evolve based on the current state and the agent's actions. While the dynamics model predicts state transitions, the world model goes further by incorporating how the agent perceives the environment (observation model) and the rewards it expects to receive (reward model)."}, {"title": "E WORLD MODEL TRAINING", "content": "We use Dreamer v3 Hafner et al. (2023) structure, i.e., encoder-decoder, RSSM Hafner et al. (2019), to train the world model and adopt the Large model for all experiments with dimension summarized in Table 5. We first restate the hyper-parameters in Table 6.\nLearning BEV Representation. The BEV representation can be learnt by using algorithms such as BevFusion Liu et al. (2023), which is capable of unifying the cameras, LiDAR, Radar data into a BEV representation space. In our experiment, we leverage the privileged information provided by CARLA Dosovitskiy et al. (2017), such as location information and map topology to construct the BEVS."}, {"title": "Actor-Critic Learning.", "content": "We consider the prediction horizon to be 16 as the same as in Dreamer v3 while training the actor-critic networks. We follow the same line as in Dreamer v3 and consider the actor and critic defined as follows.\nActor: $a_t \\sim \\pi_\u03b8(a_t|x_t)$\nCritic: $v_\u03c8(x_t) \u2248 \\mathbb{E}_{\u03c1_\u03c6,\u03c0_\u03b8} [R_t]$,\n(14)\nwhere $R_t = \\sum_{t'=0}^{100} \u03b3r_{t+t'}$ with discounting factor \u03b3 = 0.997. Meanwhile, to estimate returns that consider rewards beyond the prediction horizon, we compute bootstrapped \u03bb-returns that integrate the predicted rewards and values:\n$R_t = r_t + \u03b3v_\u03c8 ((1 \u2212 \u03bb)v(s_{t+1}) + \u03bbR_{t+1}^\u03bb )$, $R_{ST}^\u03bb = v_\u03c8(s_T)$"}, {"title": "E.1 TRAINING DATASET: BENCH2DRIVE", "content": "In our experiments, we use the open source Bench2Drive dataset Jia et al. (2024); Li et al. (2024), which is a comprehensive benchmark designed to evaluate end-to-end autonomous driving (E2EAD) systems in a closed-loop manner. Unlike existing benchmarks that rely on open-loop evaluations or fixed routes, Bench2Drive offers a more diverse and challenging testing environment. The dataset consists of 2 million fully annotated frames, collected from 10,000 short clips distributed across 44 interactive scenarios, 23 weather conditions, and 12 towns in CARLA. This diversity allows for a more thorough assessment of autonomous driving capabilities, particularly in corner cases and complex interactive situations that are often underrepresented in other datasets.\nA key feature of Bench2Drive is its focus on shorter evaluation horizons compared to the CARLA v2 leaderboard. While the CARLA v2 leaderboard uses long routes (7-10 kilometers) that are challenging to complete without errors, Bench2Drive employs shorter, more manageable scenarios1. This approach allows for a more granular assessment of driving skills and makes the benchmark more suitable for reinforcement learning applications."}, {"title": "G SUPPLEMENTARY EXPERIMENTS", "content": "Furthermore, we conduct experiments on another five diverse and challenging scenarios to validate the performance of AdaWM. In particular, we consider the following environment settings, respetively,\n\u2022 HI13: HighwayCutIn_Town13_Route23823_Weather7\n\u2022 HE12: HighwayExit_Town12_Route2233_Weather9\n\u2022 YE12: YieldToEmergencyVehicle_Town12_Route1809_Weather9\n\u2022 BI12: BlockedIntersection_Town12_Route12494_Weather9\n\u2022 VT11: VehicleTurningRoutePedestrian_Town11_Route27267_Weather12"}, {"title": "G.1 EXPERIMENTS RESULTS", "content": "Impact of Finetuning. The experiments evaluate AdaWM across a diverse set of challenging autonomous driving scenarios, including highway maneuvers (cut-in and exit), emergency response (yielding to emergency vehicles), intersection navigation (blocked intersections), and complex interactions with pedestrians across different town environments and weather conditions. Looking at the impact of finetuning in Tables 7 and 8, AdaWM demonstrates substantial improvements over baseline methods (UniAD, VAD, and DreamerV3) across all scenarios. In highway scenarios (HC13 and HE12), AdaWM achieves remarkable gains, with the Success Rate (SR) improving from 0.33 to 0.73 in HC13 and from 0.52 to 0.89 in HE12, while Time-To-Collision (TTC) metrics show similar impressive improvements. The performance gains extend to more complex scenarios, with AdaWM showing particularly strong results in emergency vehicle response (YE12, SR from 0.15 to 0.52), blocked intersection handling (BI12, SR from 0.42 to 0.59), and vehicle-pedestrian interactions (VT11, SR from 0.58 to 0.88)."}]}