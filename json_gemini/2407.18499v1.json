{"title": "Non-Overlapping Placement of Macro Cells based on Reinforcement Learning in Chip Design", "authors": ["Tao Yu", "Peng Gao", "Fei Wang", "Ru-Yue Yuan"], "abstract": "Due to the increasing complexity of chip design, existing placement methods still have many shortcomings in dealing with macro cells coverage and optimization efficiency. Aiming at the problems of layout overlap, inferior performance, and low optimization efficiency in existing chip design methods, this paper proposes an end-to-end placement method, SRLPlacer, based on reinforcement learning. First, the placement problem is transformed into a Markov decision process by establishing the coupling relationship graph model between macro cells to learn the strategy for optimizing layouts. Secondly, the whole placement process is optimized after integrating the standard cell layout. By assessing on the public benchmark ISPD2005, the proposed SRLPlacer can effectively solve the overlap problem between macro cells while considering routing congestion and shortening the total wire length to ensure routability.", "sections": [{"title": "I. INTRODUCTION", "content": "Placement in chip design represents the paramount and most labor-intensive phase [1]\u2013[3]. This critical step involves assigning locations within the chip layout to netlist compo- nents, encompassing macro and standard cells. Standard cells refer to elementary logic units, like logic gates, while macro cells denote more complex functional units, such as static random access memory (SRAM). The optimal layout enhances chip area utilization, timing performance, and routability. Consequently, the quest for methods to execute the macro cell placement task swiftly and efficiently has emerged as a prominent research topic in chip design.\nAdvanced automated placement methods play a crucial role in producing practical chip layouts, which markedly diminish the intricacy associated with chip designs and abbreviate the duration to market. Concurrently, optimizing layout can mitigate the circuit's resistance, inductance, and parasitic ca- pacitance, thereby augmenting the signal transmission rate and diminishing the delay of logic gates [4], [5]. Moreover, this optimization can alleviate the leakage and thermal impacts, bolster reliability, decrease the occupied area, and amplify wafer utilization. These improvements are paramount for re- ducing manufacturing costs, underlining the significance of placement optimization in chip design.\nExisting placement methods include partition-based, sim- ulated annealing, and analytical placement, but these meth- ods often show limitations when handling the placement of macro cells and standard cells. Traditional methods struggle to provide efficient solutions, especially when faced with complex interactions and high-dimensional action spaces. As an emerging technology, reinforcement learning (RL) can optimize different circuit performance metrics by designing various reward functions. The application of RL algorithms [6] has substantially increased the efficiency of chip design in recent years [7]-[9]. These algorithms, notable for their enhanced adaptability and generalization capabilities, stand out as particularly effective for various chip design endeavors compared to conventional placement strategies. The ongoing advancement in computer hardware capabilities has also fa- cilitated the broader adoption of end-to-end deep learning approaches [10], [11]. These sophisticated approaches can learn the intricate end-to-end mappings from unprocessed data, presenting more effective solutions to complex chip placement challenges. Nevertheless, such challenges often entail many constraints and objectives, encompassing wiring length, power consumption, and signal delay. Traditional placement methods focus on minimizing wiring length to optimize the chip layout. Yet, they exhibit limitations in addressing macro cell coverage effectively, revealing gaps in their practical application [4], [5], [12]. Furthermore, considerations for subsequent processes, such as routing, are imperative, yet layouts generated by current placement methods suffer from suboptimal routability. This underscores the necessity for advancements in placement methods that can reconcile these multifaceted requirements and constraints.\nThis paper introduces a novel end-to-end placement method of macro cells in chip design, termed SRLPlacer, which lever- ages RL to tackle prevalent issues such as insufficient macro cell coverage, suboptimal macro cell layouts, and lackluster optimization efficacy observed in existing placement meth- ods. Specifically, we utilize RL to transform the macro cell placement task into a Markov decision process [13], enabling dynamic learning and optimizing placement strategies. Unlike traditional heuristic or deterministic methods, RL can easily transfer the experience accumulated during the macro cell placement process to new circuits, reducing the need for re-exploration and significantly decreasing the time required for optimization during placement. This transformation facilitates the learning and refining placement strategies pertinent to the macro cell [14]. Subsequently, SRLPlacer achieves compre- hensive optimization throughout the placement process by incorporating the placed standard cells. A major innovation of this paper is the application of RL to the macro cell placement task, using a graph attention network (GAT) [15] in the policy network (a.k.a., RL agent). GATs effectively"}, {"title": "II. RELATED WORK", "content": "The exploration and development of placement methods in chip design date back to the 1960s [16] and predominantly encompass three distinct categories: partition-based, simulated annealing, and analytical approaches.\nInitially, the placement was approached using the divide- and-conquer strategy, hinging on partitioning methods. These approaches enjoyed widespread adoption and included pio- neers such as the Kernighan-Lin [17] and Fiduccia-Mattheyses [18] algorithms. Drawing on graph theory's minimum cut principles, these approaches segmented the circuit into smaller blocks, refining the partitioning results through iterative en- hancement. However, as circuit scales expanded, the sim- plistic nature of these partitioning approaches began to fall short of addressing the evolving practical requirements. This shortfall prompted the exploration of alternative placement approaches. Notably, the simulated annealing approaches [19] emerged as a prevalent choice for placement in chip design, characterized by their comprehensive optimization capability. These approaches employ random perturbations coupled with a strategy that tolerates less-than-ideal solutions, facilitating the surmounting of local optima in pursuit of the global optimum. Their robustness and adaptability have led to their widespread application, demonstrated by the development of simulated annealing-based algorithms like Hill-Climbing [20] and Tabu Search [21], particularly in placement tasks. Despite the flexibility and comprehensive optimization capabilities of the simulated annealing approaches, their time-intensive nature poses challenges, particularly as the scale of chip designs continues to escalate.\nAnalytical approaches formulate mathematical models for placement tasks and derive optimal solutions. Typically, an- alytical approaches involve certain assumptions, like the dif- ferentiability of the objective function or treating placement cells as point-like entities. Among the analytical approaches, quadratic wire length [22] and nonlinear optimization [23] placement methods are notably prevalent. The quadratic wire length placement method, known for its computational expedi- ence, nonetheless falls short in yielding optimal layouts com- pared to its counterpart. The nonlinear optimization placement method engages a more complex loss function to enhance the layout quality. Existing methods like ePlace [10] and RePlAce [11] have leveraged an electrostatics-inspired smooth density function and a nonlinear optimizer rooted in the Nes- terov paradigm, showcasing superior performance in standard benchmarks. These methods conceptualize each netlist node as a positively charged particle, adjusting node positions based on inter-nodal repulsive forces, with the density function mirroring the system potential energy and facilitating node position updates through gradient optimization. To expedite computation, placement tasks may be segmented into sub- tasks and disseminated across multi-thread CPUs to efficiently manage the layouts of millions of standard cells. However, these methods necessitate substantial numerical computations, a domain where GPUs excel. Considering the analogies be- tween placement optimization in the analytical approaches and neural network training, Lin et al. proposed DREAMPlace [14], adapting RePlAce [11] using the PyTorch toolkit [24] and harnessing GPU acceleration. As a result, DREAMPlace achieves a remarkable speed, operating over 30 times faster than RePlAce.\nIn the evolving landscape of artificial intelligence, adopting learning-based methodologies for placement in chip design, particularly those founded on RL [6], has gained signifi- cant traction within academic and industrial communities. A noteworthy contribution from Google Research conceptualized the macro cell placement procedure as a series of sequential decisions, pioneering an end-to-end learning framework [7]. Within this framework, the RL agent (a.k.a., policy net- work) finalizes the position of each macro cell incrementally, earning a specific reward with each placement action until the arrangement of the final macro cell is completed. The framework utilizes a graph neural network (GNN) [25] within the value network to encapsulate the netlist, while the policy network employs a deconvolutional architecture to delineate the preferred place position for a macro cell. This innovative framework has dramatically accelerated the placement process, accomplishing in hours what traditional placement approaches might take weeks or months to complete. Further, Vashisht et al. have merged RL with heuristic strategies to address the placement problem, introducing a cyclical framework that intertwines RL with simulated annealing [26]. The RL agent is tasked with adjusting the spatial relationships between netlist components, while the simulated annealing algorithm expands on this preliminary arrangement to explore the layout solution further. Cheng et al. have broken new ground by proposing DeepPR [9], an end-to-end learning method for joint placement and routing macro cells. DeepPR deploys two distinct RL agents: one for placement and another for routing.\nDespite advancements in chip design, the persistent escala- tion in circuit integration and complexity continues to render the placement task exceedingly formidable. The design of a chip profoundly influences its performance and is intricately connected to critical aspects such as power consumption, layout, and reliability [2]. An optimal placement method"}, {"title": "III. METHODOLOGY", "content": "In this section, we describe the problem and design con- straints associated with the placement process in chip design. The placement problem is as follows: Given a set of macro cells with specific dimensions and pin configurations, deter- mine their locations on the chip layout to ensure that no two macro cells overlap. Additionally, the layout must account for the positions of standard cells and optimize key design constraints, including total wire length, routing congestion, and macro cell layout density. The primary goal of the placement process is to achieve non-overlapping placement of all macro and standard cells while optimizing various circuit design constraints. The total wire length affects the transmission delay of the circuit; shorter total wire length results in lower transmission delay, better circuit performance, and reduced chip size. Routing congestion indicates the difficulty of the subsequent routing process and the heat dissipation perfor- mance of the circuit; lower routing congestion simplifies the routing process and enhances heat dissipation. Lower macro cell layout density allows more standard cells to be distributed around the macro cells, reducing the total wire length and avoiding macro cell coverage issues. This paper leverages an RL agent (a.k.a., policy network) to determine the optimal positions for each macro cell sequentially. Once a macro cell's position is determined, it checks if all macro cells have been placed. If so, their positions are fixed, and all standard cells are placed using the gradient optimization-based placement method, DREAMPlace [14]. During the placement of standard cells, the half-perimeter wire length (HPWL) of a randomly sampled state is used as a reward to encourage the RL agent to explore more states. After obtaining the final placement solution, the total wire length, layout density, and routing congestion are used as the overall reward for update the policy network via the PPO algorithm [27]. If the macro cell placement is not yet finished, the layout density and wire length of the current macro cell placement state are used as the immediate reward to update the RL agent. The overall pipeline of our proposed method is illustrated in Figure 1. Using RL to solve a new problem often requires implementing an environment for interaction with the agent and designing the critical elements of the Markov decision process. For the placement problem faced in this paper, the environment corresponds to the chip layout quality assessment process, the state corresponds to whether the layout currently marks placed macro cells, the action corresponds to selecting an unmarked position in the layout that can place the current macro cell, and the reward corresponds to indicators such as HPWL, layout density, and routing congestion."}, {"title": "A. State Space", "content": "This paper designs a two-dimensional discrete 0-1 state space to mark the positions where macro cells can be placed in the chip layout. The state representation comprises four parts:\nnetlist metadata, netlist graph H, macro feature F, and current macro id. H describes the adjacency relationships among all macro cells before outlined in the netlist metadata. F describes the geometric information of all macro cells, including size, number of pins, and types of pins. id represents the identity number of the macro cell to be placed currently. In the specific implementation, a N \u00d7 N matrix represents H, where N indicating the number of macro cells; a vector represents the netlist metadata; another N \u00d7 4 matrix represents F; id is an integer, used as an index to extract mask information processed by the GNN [25], for the policy network's output layer to decide the position where the macro cell corresponding to the current macro id should be placed."}, {"title": "B. Action Space", "content": "The core design concept of the action space is to provide a highly flexible and precise framework to support complex macro cell placement decisions. In chip design, the placed position of macro cells directly impacts critical metrics such as performance, power, and area. Therefore, the action space needs to be able to finely represent all possibilities of macro cell placed position, allowing the method to explore and find the optimal placement strategy. Thus, after constructing a state space that reflects the features and connection information of macro cells, this paper also carefully designs a matching action space. The action space includes W \u00d7 W actions, represented by a W \u00d7 W matrix, where W is the size of the chip layout. The matrix covers all possible macro cell placement positions on the chip layout. With such representation, the action space can finely describe the placement strategy of macro cells with W \u00d7 W actions, each corresponding to a possible placed position for a macro cell. During the placement process, the action space is not just a candidate set for macro cell placement decisions, and it can also reflect the dynamic changes of chip layouts. As each macro cell is successfully placed, the action space will be updated to show the new placement state and the remaining placement possibilities. This dynamic updating mechanism allows the method to adapt to changes during placement, continually optimizing the placement strategy for the remaining macro cells."}, {"title": "C. Rewards", "content": "In the placement process, an effective reward design is crucial because it directly affects the learning efficiency of the RL agent and the quality of the final layout. The reward design proposed in this paper carefully considers the critical performance indicators of chip layout, namely HPWL, layout density, and routing congestion (see Section IV-B), to ensure that the resulting layout meets physical requirements and electrical performance. The basic idea of the reward design is to guide the policy network in making the best macro cell placement decisions by precisely calculating the impact of each action on layout quality. The reward of SRLPlacer consists of two parts: one is the overall reward $R_t$ after the placement of all macro and standard cells has been completed, and the other is the immediate reward $R_i$ after one macro cell is placed. Together, these two rewards constitute the feedback for learning the policy network, aiming to achieve an optimal layout."}, {"title": "1) Overall Reward:", "content": "Although chip placement aims to mini- mize power and area, traditional methods require several hours to complete an evaluation, making this approach unfeasible for an RL agent that needs continuous iterative learning. Optimizing wire length can reduce transmission delays to some extent; optimizing the routing congestion can reduce the difficulty of subsequent routing. In this way, the performance and routability of macro cells placement can be optimized more effectively. Therefore, we propose a overall reward $R_t$, including wire length and routing congestion, to maximize performance and routability simultaneously as follows,\n$R_t = - \\alpha_t \\cdot WL(P, H) - \\beta_t \\cdot C(P, H)$                                                                                                       (1)\nwhere $\\alpha_t$ and $\\beta_t$ are hyperparameters that respectively repre- sent the importance of total wire length and routing conges- tion; P represents the state when the positions of all macro and standard cells have been placed; H represents the connection information between macro and standard cells; C is used to measure the level of routing congestion for a given state; WL is used to calculate HPWL of all connections appearing in the netlist for placed units. For each net connection $n_i$, with its endpoint set {$x$} and {$y$}, HPWL can be calculated as follows,\n$HPWL(n_i) = (max\\{x\\} - min\\{x\\}) + (max\\{y\\} - min\\{y\\})$                  (2)\nHPWL can be approximated as the connection length on the critical path. Optimizing this metric can help reduce signal"}, {"title": "2) Immediate Reward:", "content": "Because the RL agent cannot re- ceive reward signals long before the standard cell placement by DREAMPlace is completed, the initial macro cell placement process becomes a random RL task. During training, the inability to obtain rewards for a long time can lead to low- quality layouts. Considering that the number of macro cells is much less than that of standard cells, the wire length of already placed macro cells is insufficient to determine the quality of the final layout. Still, the area occupied by macro cells is much greater than that of standard cells. The layout density of macro cells can better represent the quality of the current layout. We propose a immediate reward $R_i$ that includes macro cell placement wire length and layout density to encourage the RL agent to explore potential high-quality layouts as follows,\n$R_i = - \\alpha_i \\cdot WL(P, H) - \\beta_i \\cdot D(P)$                                                                                                   (3)\nwhere $\\alpha_i$ and $\\beta_i$ are hyperparameters to balance the impor- tance of immediate reward wire length and layout density; $WL$ and $H$ are as described in Section III-C1; P represents the state of placed macro cells; D is used to approximate the layout density for a given placement state.\nSRLPlacer primarily determines the positions of macro cells sequentially, then identifies the placement of standard cells by DREAMPlace, jointly optimizing the layout problem. The main decision-making occurs during the macro cell placement phase. The introduction of immediate rewards is to prevent the RL agent from experiencing slow convergence due to long periods without reward signals. Macro cells are often hundreds or thousands of times larger than standard cells. To avoid overlapping macro cells and leave adequate space for standard cells, we introduced the macro cell layout density (a.k.a., D) into the immediate reward. This metric reflects the dispersion of macro cells on the layout. Since macro cells are sufficiently dispersed and large, the HPWL can effectively reflect the connection length of the critical path during the macro cell placement process. Optimizing this metric can also improve circuit performance and decrease chip size. Additionally, this metric helps balance the dispersion, preventing excessively dispersed macro cell layouts that could lead to larger chip sizes."}, {"title": "D. Policy Network", "content": "The policy network consists of multiple modules, each containing several submodules, which process the input data and ultimately output a probability distribution corresponding to the actions, as shown in Figure 2. We use the policy network to learn how to place macro cells to maximize the rewards. Unlike traditional neural networks, our policy network is unsupervised learning. Its most important feature is that it needs to interact with the environment to obtain rewards. In the macro cell placement problem, the policy network must output a probability distribution corresponding to each action based on the current state and action space. An action is selected based on this probability distribution and applied to the environment, providing a new state and reward. The policy network aims to learn an optimal strategy through interaction with the environment, such that the optimal action under the current state yields the maximum rewards.\nThe input layer of the policy network needs to be capable of receiving and processing two-dimensional feature information from the state space. The policy network backbone adopts a multi-layer GAT architecture [15] to identify critical factors affecting placement quality, such as the relative positions of macro cells and the spatial distribution within the layout. The output layer maps the high-dimensional features extracted from the backbone onto a vector with dimensions equal to the size of the action space, where each element represents the probability of placing a macro cell. To achieve this mapping, the output layer uses the Softmax function as the activation to ensure that the sum of all action probabilities equals 1 and that each action probability is non-negative. Thus, the policy network can choose an action to execute randomly or greedily based on the output probability distribution. Specifically, to better map the output results of the policy network to the po- sitions on the layout, this paper reshapes the output dimensions of the policy network into two dimensions."}, {"title": "E. Environment", "content": "The environment needs to be capable of updating the placement state based on the actions of the RL agent. This includes calculating the positions of newly placed macro cells, updating the features of the placed area, such as HPWL, layout density, and routing congestion, and detecting potential constraint conflicts, such as macro cell overlaps. We divide the $W \u00d7 W$ action space (a.k.a., layouts) into 20 equal grids both horizontally and vertically, resulting in a total of 400. The estimated number of routing lines in each grid is weighted summed (with weights decreasing uniformly from 1 at the centroid of the layouts to 0 at the edges) to approximate the level of routing congestion. The layout density is determined by the average distance between each macro cell and its nearest neighboring macro cell. HPWL is calculated for each pair of connected cells, and the overall HPWL of the circuit is the summation of the HPWLs of all connections. When the RL agent selects an illegal action, it is given a negative reward to guide it toward learning placement strategies that meet constraints. If the updated placement strategy is valid, calculate the immediate reward of the current layout and use the square of the immediate reward of the current layout minus the immediate reward of the layout before performing the action as the score for this action of the RL agent. If the immediate reward after the action is less than before, the difference is negative; otherwise, it is positive. Additionally, the environment also controls the start and end of training rounds. When a training round begins, the environment is initialized. Specifically, the placement state is reset to zero, i.e., all the placed cells are removed. As the RL agent optimizes parameters, the environment determines whether all macro"}, {"title": "C. Ablation Study", "content": "To validate the effectiveness of GAT and immediate re- wards, this paper conducted a series of ablation studies to assess different combinations of whether the policy network backbone uses GCN or GAT and whether immediate rewards are introduced. Specifically, the research is divided into the fol- lowing parts: using only GAT for placement (SRLPlacerGAT), using only the immediate rewards (SRLPlacerR\u2081); and using GCN without immediate rewards (SRLPlacerGCN). The re- sults of the ablation study are shown in Table II. It can be seen that using GAT and introducing immediate rewards both lead to a significant reduction in the HPWL for all 8 test cases. This indicates that compared to GCN, GAT can more effectively capture and utilize the relationships between nodes when dealing with placement problems, thus achieving a better layout. Introducing immediate rewards provides more timely feedback to the RL agent, helping it learn better placement strategies more quickly. From the results of these ablation experiments, replacing GCN with GAT in the policy network backbone and combining it with immediate rewards signif- icantly enhances the placement performance of SRLPlacer. This further demonstrates the effectiveness and importance of GAT and immediate rewards in the problem of macros placement."}, {"title": "D. Quantitative Analysis", "content": "In exploring the application of RL methods in macros placement tasks, SRLPlacer has demonstrated significant per- formance advantages, especially in handling macro cell place- ment problems. Compared with other popular placement meth- ods, SRLPlacer successfully generated effective placement strategies and achieved the best performance in the critical performance metrics of HPWL across eight different scale and complexity test cases, as shown in Table III. This achievement can be attributed to SRLPlacer's deep understanding and innovative solutions to both macro cell and standard cell placement problems, especially its ability to effectively avoid macro cell overlaps and optimize congestion and density, a significant challenge in traditional placement methods.\nThe experimental design meticulously considered the char- acteristics of different test cases, such as the ratio of macro cells to standard cells and the size of the chip area, ensuring the comprehensiveness and reliability of the experimental results. The sequential learning technique adopted by SRLPlacer is crucial for accurately predicting and optimizing the placement positions of macro cells, which was fully demonstrated in the comparative experiments. Compared with layout tools like DREAMPlace, SRLPlacer showed its absolute advantages in handling complex chip layouts with a large number of macro cells, mainly due to its efficient algorithm design, which significantly improves execution efficiency while maintaining layout quality.\nThe performance of SRLPlacer is reflected in its excellent result of HPWL and the significant improvement in placement efficiency, as shown in Table IV. This indicates that SRLPlacer is not only suitable for current chip design needs but also has"}, {"title": "V. CONCLUSION", "content": "The SRLPlacer method proposed in this paper adopts a phased strategy that can simultaneously solve both macro and standard cell placement problems. SRLPlacer models and learns the interactions between macro cells, converting the placement problem into a Markov decision process, which can generate high-quality placement strategies. The routing congestion is also considered to ensure the routability of the final placement strategy while shortening the wire length. In experiments on the public benchmark ISPD2005, the per- formance and efficiency of SRLPlacer have shown signifi- cant improvements over several advanced existing placement methods. The results indicate that SRLPlacer is an innovative solution capable of effectively solving the non-overlapping macro cell placement problem in chip design. Future work aims to improve the parallelization and hardware acceleration capabilities of SRLPlacer to accommodate VLSI design tasks. As an end-to-end method, SRLPlacer also needs to be ex- panded to adapt to other design objectives relative to place- ment tasks, such as power optimization, thermal characteristic optimization, and signal integrity enhancement."}]}