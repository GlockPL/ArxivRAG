{"title": "Scaling Granite Code Models to 128K Context", "authors": ["Matt Stallone", "Vaibhav Saxena", "Leonid Karlinsky", "Bridget McGinn", "Tim Bula", "Mayank Mishra", "Adriana Meza Soria", "Gaoyuan Zhang", "Aditya Prasad", "Yikang Shen", "Saptha Surendran", "Shanmukha Guttula", "Hima Patel", "Parameswaran Selvam", "Xuan-Hong Dang", "Yan Koyfman", "Atin Sood", "Rogerio Feris", "Nirmit Desai", "David D. Cox", "Ruchir Purit", "Rameswar Panda"], "abstract": "This paper introduces long-context Granite code models that support effective context windows of up to 128K tokens. Our solution for scaling context length of Granite 3B/8B code models from 2K/4K to 128K consists of a light-weight continual pretraining by gradually increasing its ROPE base frequency with repository-level file packing and length-upsampled long-context data. Additionally, we also release instruction-tuned models with long-context support which are derived by further finetuning the long context base models on a mix of permissively licensed short and long-context instruction-response pairs. While comparing to the original short-context Granite code models, our long-context models achieve significant improvements on long-context tasks without any noticeable performance degradation on regular code completion benchmarks (e.g., HumanEval). We release all our long-context Granite Code models under an Apache 2.0 license for both research and commercial use.", "sections": [{"title": "1 Introduction", "content": "With the emergence and development of repository-level coding tasks (Liu et al., 2024; 2023b) and software development agents (OpenDevin Team, 2024), long context length becomes an important feature for code language models. While many proprietary large language models, like GPT4, Gemini, and Claude, support very long context windows, most open-source code language models could only provide relatively short context windows (CodeGemma Team et al., 2024; Rozi\u00e8re et al., 2023). This short context length limits the practicality of open-source code language models in real-world software development.\nIn this paper, we introduce the long-context Granite code 3B and 8B, a series of code language models that support effective context lengths up to 128K tokens. To achieve the extended context length, we first continue pretrain Granite Code 3B/8B base models with a repository-level code corpus and upsample the longer context repositories. Then, we instruction tune the continue pretrained model on a combination of short and long context instruction data. Due to the lack of long context instruction data, we generate multi-turn instruction data from repository-level file-packed documents with our original Granite-8B-Code-Instruct model to avoid the dependency on an existing long context model. More details of long context extension can be found in Section 2.\nTo evaluate the ability of long-context Granite Code models, we conduct extensive experiments on both short and long-context tasks, including HumanEvalPack, Long Code Completion, RepoBench-P, RepoQA, and Key Retrieval. Experiment results show that our long-context models significantly improve long-context performances without noticeable degradation in short-context performances. We open-source all our long-context Granite Code models under an Apache 2.0 license for research and commercial use."}, {"title": "2 Long Context Modeling", "content": "Our solution for scaling context length of Granite code models consists of a continual pretraining and an instruction tuning phase. Similar to prior works (Fu et al., 2024), we hold the basic hypothesis that the ability to utilize information at arbitrary input locations, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during original pretraining (e.g., 4K to 128K) through lightweight training on appropriate data mixture."}, {"title": "2.1 Continual Pretraining", "content": "We continue pretrain the full attention Granite code base models using sequence parallelism (Li et al., 2021) by gradually increasing its RoPE base frequency without using any sparse or linear attention. Specifically, we continue pretrain Granite Code 3B/8B base models using the original pretraining data used in Mishra et al. (2024) but with repository-level file packing and per-language context length upsampling, that we found to be critical for long-context continual pretraining. This continued training stage focused on a curated selection of programming languages, such as Python, C, C++, Go, Java, JavaScript, and TypeScript, as in Pinnaparaju et al. (2024).\nTo create long-context data, we develop a new approach that packs files from the same repository together, arranging them to prioritize semantic dependencies. We identify these dependencies by analyzing file imports and create a directed acyclic graph, where each file is a node and edges represent API imports between files. After breaking any cycles in the graph, we perform a topological sort to establish an ordering of files based on their semantic dependencies. We then organize the files in a repository by placing documentation and build files first, followed by the ordered set of files with semantic dependencies, and finally the remaining non-connected files. These non-connected files are arranged according to their folder structure, using a depth-first search to traverse the repository. Finally, we determine the dominant programming language of a repository based on file extensions and presence of build files, to organise repo-ordered files by programming languages.\nThe documents' lengths and their source domains/languages are two closely related confounding factors in data engineering because long data usually come from particular sources. Thus, in addition to repository-level file packing, we artificially oversampled longer document sequences on a per-language basis to ensure the quantity of long sequences, thereby improving the overall quality of our training data corpus, as in Fu et al. (2024); Yu (2023). In particular, we downsample documents under 4096 tokens to a rate of 10%, which we find to ensure a sufficient number of total tokens and documents. The total number of documents within the training corpus after processing is 173,336 with a mean length of 73,451.\nWe adjust the RoPE base frequency, introduced in Xiong et al. (2023), to support long context windows up to 128K where the base model itself is trained on 2K/4K context length. For training, we adopt a progressive approach where we doubled the context window until it reached the desired length of 128K. We train for 500 steps with a batch size of 32 and search for the optimal RoPE theta and learning rate for each iteration. For RoPE theta, we finf optimal values of 100K, 250K, 500K, 2M, and 10M for context windows of 8K, 16K, 32K, 64K, and 128K, respectively. We train with data parallelism and Flash Attention 2 until 64K tokens and then used Ring Attention (Liu et al., 2023a) to reach 128K tokens. The final models are trained for an extra 4B tokens which is only 0.1% of original pretraining data."}, {"title": "2.2 Instruction Tuning", "content": "Our training data for long context instruct models consists of a combination of permissively licensed data used in training the original Granite code instruct models (Mishra et al., 2024), in addition to synthetically generated code instruction datasets tailored for solving long context problems. Specifically, the 128K long context instruct models are derived by further finetuning the long context base models on a mix of short and long context data as follows."}, {"title": "3 Results", "content": "We evaluate our long-context Granite code models on a wide variety of benchmarks by measuring key retrieval accuracy and performance during generation on code completion tasks at both short and long-context length as follows."}, {"title": "3.1 Benchmarks", "content": "Long Code Completion. Long Code Completion (LCC) (Guo et al., 2023) tests a model's ability to predict the next line of code from long repository-based context for Python, Java, and C#. While the benchmark's context length spans 1/2K through 8K+ tokens, it is heavily weighted around 2K tokens. Thus, following Bai et al. (2024) and Rozi\u00e8re et al. (2023), we rebalance this dataset for equal representation with each context length bucket (<4K, 2 \u2013 4K, 4 \u2013 8K, 8K+), where each bucket has 100 samples when possible.\nRepoBench-P. Like LCC, RepoBench-P (Liu et al., 2023c) tests the model's next line code completion ability for long-context input. We follow the methodology in (Bai et al., 2024) by selecting the Cross-File-First data but then we rebalance the buckets based on the Starcoder tokenizer used for training out Granite code models.\nRepoQA. RepoQA (Liu et al., 2024) is an advanced Needle-in-the-Haystack test that focuses on testing LLMs' capabilities on long-context code understanding and retrieval. Specifically,\ngiven a long chunk of source code and a precise function description, and the model is asked to find the function in the context that corresponds to the description. This benchmark focuses on retrieving 10 needle functions from each of 5 languages x 10 repositories (500 sub-tasks/tests) with a set context size of 16K tokens.\nKey Retrieval. This is a synthetic benchmark that tests the model's ability to find and execute a Python function buried within high-quality, syntactically correct Python code. As proposed in Rozi\u00e8re et al. (2023), we took the Code Contest finetuning dataset from Li et al. (2022) and concatenated Python solutions around the key function. We then asked the model to return the output of the key function by emulating a Python interpreter shell. We created sequences of lengths of 512 tokens and key offsets of 512 tokens.\nHuman EvalPack. To evaluate model performance at short-context length, we adopt HumanEvalPack (Muennighoff et al., 2023), which extends Python problems of Humaneval Benchmark to five additional commonly used programming languages, namely JavaScript, Java, Go, C++, Rust to test three coding tasks (generation, explanation and fixing). We evaluate our long-context models in a zero-shot manner using greedy decoding with completion format for the base models, and with instruction template for the instruction-tuned models."}, {"title": "3.2 Base Model Evaluations", "content": "Table 1 and Table 2 show the results of Granite 3B/8B code models before and after long-context extension on LCC and RepoBench-P benchmarks respectively. Prior Granite code models with 2K/4K support fail to generate meaningful completions on long sequences. On the other hand, across all the context length (4K to 32K), models scaled to handle long contexts up to 128K achieve significantly higher performance. This demonstrates that long contexts are informative for code completion, and long-context Granite code models are able to effectively leverage this information to improve their generations on both benchmarks.\nIn Table 3, we compare the performance of Granite code base models to their counterparts prior to long-context extension. Our long-context models exhibit strong retrieval performance across different matching thresholds, while the short context versions mostly fail in finding the needle function successfully. The absolute differences averaged over 5 programming languages are very significant, e.g., +38.6% for Granite 8B model with a"}, {"title": "3.3 Instruct Model Evaluations", "content": "Table 4 compares the performance of long-context instruct models to their short-context counterparts on RepoQA benchmark. As can be seen, our long-context instruct models significantly outperforms short-context versions on all 5 programming languages across different similarity thresholds. As an illustration, figure 1 demonstrates the difference between short and long-context models at similarity threshold of 0.5, where the performance of both 3B and 8B instruct models with 2K/4K context length support fails to achieve a retrieval accuracy of more than 2% across 5 languages (on average 0.6% vs 61.6% for 8B instruct model). We attribute the improvements to the knowledge learned from newly introduced synthetic long data for instruction tuning.\nIn Figure 2, we investigate key retrieval performance of our long-context instruct models on a synthetic benchmark built on top of Python solutions around a key function from Code Contest finetuning dataset (Li et al., 2022). Note that this retrieval task is analogous to the famous famous Needle-in-a-Haystack test, albeit tailored to code models. As can be seen from Figure 2, our 8B instruct model before long-context extension only exhibit strong retrieval performance up to 4K length, i.e., on the sequence length they were originally trained on."}, {"title": "3.4 Short Context Evaluations", "content": "While our long-context models are very effective on long sequences, we observe that our long-context scaling does not significantly change the short-context generic capability on standard code synthesis benchmarks consisting of short sequences. Table 5 summarizes the results on HumanEvalPack, where we find only an average ~1% degradation for the pass@1 metric on 3B and 8B models respectively. We also test the HumanEval-Python performance in Figure 3 and observe that long context extension has any noticeable performance degradation. Interestingly, we notice improvements in HumanEval performance of long-context instruct models, which we attribute to our new long-context synthetic data added to instruction tuning. To summarize, while long-context extension comes at a minimal cost for short sequences, we believe this cost is more than offset by the potential of handling long sequences for many real downstream applications."}, {"title": "4 Conclusion", "content": "We present long-context Granite code models (3B and 8B) that support effective context lengths up to 128K tokens. We perform long context scaling by leveraging a simple yet effective strategy consisting of a lightweight continual pretraining followed by instruction tuning on a mix of short and long-context data. Our long-context models demonstrate much superior performance compared to their short-context counterparts without significantly affecting the short-context generic capability. We believe that given our current results,\nmethods to enable even longer context length and circumvent the quadratic computational complexity of attention computation will continue to further evolve (Gu & Dao, 2023). We plan to continuously release updates to these models to improve their performance and bringing the best of breed approaches to IBM Granite Family."}]}