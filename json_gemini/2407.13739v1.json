{"title": "Scaling Granite Code Models to 128K Context", "authors": ["Matt Stallone", "Vaibhav Saxena", "Leonid Karlinsky", "Bridget McGinn", "Tim Bula", "Mayank Mishra", "Adriana Meza Soria", "Gaoyuan Zhang", "Aditya Prasad", "Yikang Shen", "Saptha Surendran", "Shanmukha Guttula", "Hima Patel", "Parameswaran Selvam", "Xuan-Hong Dang", "Yan Koyfman", "Atin Sood", "Rogerio Feris", "Nirmit Desai", "David D. Cox", "Ruchir Purit", "Rameswar Panda"], "abstract": "This paper introduces long-context Granite code models that support ef-\nfective context windows of up to 128K tokens. Our solution for scaling\ncontext length of Granite 3B/8B code models from 2K/4K to 128K consists\nof a light-weight continual pretraining by gradually increasing its ROPE\nbase frequency with repository-level file packing and length-upsampled\nlong-context data. Additionally, we also release instruction-tuned mod-\nels with long-context support which are derived by further finetuning\nthe long context base models on a mix of permissively licensed short and\nlong-context instruction-response pairs. While comparing to the origi-\nnal short-context Granite code models, our long-context models achieve\nsignificant improvements on long-context tasks without any noticeable\nperformance degradation on regular code completion benchmarks (e.g.,\nHumanEval). We release all our long-context Granite Code models under\nan Apache 2.0 license for both research and commercial use.", "sections": [{"title": "Introduction", "content": "With the emergence and development of repository-level coding tasks (Liu et al., 2024; 2023b)\nand software development agents (OpenDevin Team, 2024), long context length becomes an\nimportant feature for code language models. While many proprietary large language models,\nlike GPT4, Gemini, and Claude, support very long context windows, most open-source\ncode language models could only provide relatively short context windows (CodeGemma\nTeam et al., 2024; Rozi\u00e8re et al., 2023). This short context length limits the practicality of\nopen-source code language models in real-world software development.\nIn this paper, we introduce the long-context Granite code 3B and 8B, a series of code\nlanguage models that support effective context lengths up to 128K tokens. To achieve the\nextended context length, we first continue pretrain Granite Code 3B/8B base models with\na repository-level code corpus and upsample the longer context repositories. Then, we\ninstruction tune the continue pretrained model on a combination of short and long context\ninstruction data. Due to the lack of long context instruction data, we generate multi-turn\ninstruction data from repository-level file-packed documents with our original Granite-8B-\nCode-Instruct model to avoid the dependency on an existing long context model. More\ndetails of long context extension can be found in Section 2.\nTo evaluate the ability of long-context Granite Code models, we conduct extensive ex-\nperiments on both short and long-context tasks, including HumanEvalPack, Long Code\nCompletion, RepoBench-P, RepoQA, and Key Retrieval. Experiment results show that our\nlong-context models significantly improve long-context performances without noticeable\ndegradation in short-context performances. We open-source all our long-context Granite\nCode models under an Apache 2.0 license for research and commercial use."}, {"title": "Long Context Modeling", "content": "Our solution for scaling context length of Granite code models consists of a continual\npretraining and an instruction tuning phase. Similar to prior works (Fu et al., 2024), we hold\nthe basic hypothesis that the ability to utilize information at arbitrary input locations, is\na capability that is mostly already acquired through large-scale pretraining, and that this\ncapability can be readily extended to contexts substantially longer than seen during original\npretraining (e.g., 4K to 128K) through lightweight training on appropriate data mixture."}, {"title": "Continual Pretraining", "content": "We continue pretrain the full attention Granite code base models using sequence paral-\nlelism\u00b9 (Li et al., 2021) by gradually increasing its RoPE base frequency without using any\nsparse or linear attention. Specifically, we continue pretrain Granite Code 3B/8B base mod-\nels using the original pretraining data used in Mishra et al. (2024) but with repository-level\nfile packing and per-language context length upsampling, that we found to be critical for\nlong-context continual pretraining. This continued training stage focused on a curated\nselection of programming languages, such as Python, C, C++, Go, Java, JavaScript, and\nTypeScript, as in Pinnaparaju et al. (2024).\nTo create long-context data, we develop a new approach that packs files from the same\nrepository together, arranging them to prioritize semantic dependencies. We identify these\ndependencies by analyzing file imports and create a directed acyclic graph, where each\nfile is a node and edges represent API imports between files. After breaking any cycles\nin the graph, we perform a topological sort to establish an ordering of files based on their\nsemantic dependencies. We then organize the files in a repository by placing documentation\nand build files first, followed by the ordered set of files with semantic dependencies, and\nfinally the remaining non-connected files. These non-connected files are arranged according\nto their folder structure, using a depth-first search to traverse the repository. Finally, we\ndetermine the dominant programming language of a repository based on file extensions\nand presence of build files, to organise repo-ordered files by programming languages.\nThe documents' lengths and their source domains/languages are two closely related con-\nfounding factors in data engineering because long data usually come from particular sources.\nThus, in addition to repository-level file packing, we artificially oversampled longer docu-\nment sequences on a per-language basis to ensure the quantity of long sequences, thereby\nimproving the overall quality of our training data corpus, as in Fu et al. (2024); Yu (2023). In\nparticular, we downsample documents under 4096 tokens to a rate of 10%, which we find to\nensure a sufficient number of total tokens and documents. The total number of documents\nwithin the training corpus after processing is 173,336 with a mean length of 73,451.\nWe adjust the RoPE base frequency, introduced in Xiong et al. (2023), to support long context\nwindows up to 128K where the base model itself is trained on 2K/4K context length. For\ntraining, we adopt a progressive approach where we doubled the context window until\nit reached the desired length of 128K. We train for 500 steps with a batch size of 32 and\nsearch for the optimal RoPE theta and learning rate for each iteration. For RoPE theta, we\nfinf optimal values of 100K, 250K, 500K, 2M, and 10M for context windows of 8K, 16K, 32K,\n64K, and 128K, respectively. We train with data parallelism and Flash Attention 2 until 64K\ntokens and then used Ring Attention (Liu et al., 2023a) to reach 128K tokens. The final\nmodels are trained for an extra 4B tokens which is only 0.1% of original pretraining data."}, {"title": "Instruction Tuning", "content": "Our training data for long context instruct models consists of a combination of permissively\nlicensed data used in training the original Granite code instruct models (Mishra et al., 2024),\nin addition to synthetically generated code instruction datasets tailored for solving long\ncontext problems. Specifically, the 128K long context instruct models are derived by further\nfinetuning the long context base models on a mix of short and long context data as follows."}, {"title": "Results", "content": "We evaluate our long-context Granite code models on a wide variety of benchmarks by\nmeasuring key retrieval accuracy and performance during generation on code completion\ntasks at both short and long-context length as follows."}, {"title": "Benchmarks", "content": "Long Code Completion. Long Code Completion (LCC) (Guo et al., 2023) tests a model's\nability to predict the next line of code from long repository-based context for Python, Java,\nand C#. While the benchmark's context length spans 1/2K through 8K+ tokens, it is heavily\nweighted around 2K tokens. Thus, following Bai et al. (2024) and Rozi\u00e8re et al. (2023), we\nrebalance this dataset for equal representation with each context length bucket (<4K, 2 \u2013 4K,\n4 \u2013 8K, 8K+), where each bucket has 100 samples when possible.\nRepoBench-P. Like LCC, RepoBench-P (Liu et al., 2023c) tests the model's next line code\ncompletion ability for long-context input. We follow the methodology in (Bai et al., 2024) by\nselecting the Cross-File-First data but then we rebalance the buckets based on the Starcoder\ntokenizer used for training out Granite code models.\nRepoQA. RepoQA (Liu et al., 2024) is an advanced Needle-in-the-Haystack test that focuses\non testing LLMs' capabilities on long-context code understanding and retrieval. Specifically,"}, {"title": "Base Model Evaluations", "content": "Table 1 and Table 2 show the results of Granite 3B/8B code models before and after long-\ncontext extension on LCC and RepoBench-P benchmarks respectively. Prior Granite code\nmodels with 2K/4K support fail to generate meaningful completions on long sequences.\nOn the other hand, across all the context length (4K to 32K), models scaled to handle long\ncontexts up to 128K achieve significantly higher performance. This demonstrates that long\ncontexts are informative for code completion, and long-context Granite code models are able\nto effectively leverage this information to improve their generations on both benchmarks.\nIn Table 3, we compare the performance of Granite code base models to their counter-\nparts prior to long-context extension. Our long-context models exhibit strong retrieval\nperformance across different matching thresholds, while the short context versions mostly\nfail in finding the needle function successfully. The absolute differences averaged over\n5 programming languages are very significant, e.g., +38.6% for Granite 8B model with a"}, {"title": "Instruct Model Evaluations", "content": "Table 4 compares the performance of long-context instruct models to their short-context\ncounterparts on RepoQA benchmark. As can be seen, our long-context instruct models\nsignificantly outperforms short-context versions on all 5 programming languages across\ndifferent similarity thresholds. As an illustration, figure 1 demonstrates the difference\nbetween short and long-context models at similarity threshold of 0.5, where the performance\nof both 3B and 8B instruct models with 2K/4K context length support fails to achieve a\nretrieval accuracy of more than 2% across 5 languages (on average 0.6% vs 61.6% for 8B\ninstruct model). We attribute the improvements to the knowledge learned from newly\nintroduced synthetic long data for instruction tuning.\nIn Figure 2, we investigate key retrieval performance of our long-context instruct models on\na synthetic benchmark built on top of Python solutions around a key function from Code\nContest finetuning dataset (Li et al., 2022). Note that this retrieval task is analogous to the fa-\nmous famous Needle-in-a-Haystack test, albeit tailored to code models. As can be seen from\nFigure 2, our 8B instruct model before long-context extension only exhibit strong retrieval\nperformance up to 4K length, i.e., on the sequence length they were originally trained on."}, {"title": "Short Context Evaluations", "content": "While our long-context models are very effective on long sequences, we observe that our\nlong-context scaling does not significantly change the short-context generic capability on\nstandard code synthesis benchmarks consisting of short sequences. Table 5 summarizes the\nresults on HumanEvalPack, where we find only an average ~1% degradation for the pass@1\nmetric on 3B and 8B models respectively. We also test the HumanEval-Python performance\nin Figure 3 and observe that long context extension has any noticeable performance degra-\ndation. Interestingly, we notice improvements in HumanEval performance of long-context\ninstruct models, which we attribute to our new long-context synthetic data added to in-\nstruction tuning. To summarize, while long-context extension comes at a minimal cost for\nshort sequences, we believe this cost is more than offset by the potential of handling long\nsequences for many real downstream applications."}, {"title": "Conclusion", "content": "We present long-context Granite code models (3B and 8B) that support effective context\nlengths up to 128K tokens. We perform long context scaling by leveraging a simple yet\neffective strategy consisting of a lightweight continual pretraining followed by instruction\ntuning on a mix of short and long-context data. Our long-context models demonstrate much\nsuperior performance compared to their short-context counterparts without significantly\naffecting the short-context generic capability. We believe that given our current results,\nmethods to enable even longer context length and circumvent the quadratic computational\ncomplexity of attention computation will continue to further evolve (Gu & Dao, 2023). We\nplan to continuously release updates to these models to improve their performance and\nbringing the best of breed approaches to IBM Granite Family."}]}