{"title": "3D-PROVER: DIVERSITY DRIVEN THEOREM PROVING\nWITH DETERMINANTAL POINT PROCESSES", "authors": ["Sean Lamont", "Michael Norrish", "Christian Walder", "Amir Dezfouli", "Paul Montague"], "abstract": "A key challenge in automated formal reasoning is the intractable search space,\nwhich grows exponentially with the depth of the proof. This branching is caused\nby the large number of candidate proof tactics which can be applied to a given\ngoal. Nonetheless, many of these tactics are semantically similar or lead to an\nexecution error, wasting valuable resources in both cases. We address the prob-\nlem of effectively pruning this search, using only synthetic data generated from\nprevious proof attempts. We first demonstrate that it is possible to generate se-\nmantically aware tactic representations which capture the effect on the proving\nenvironment, likelihood of success, and execution time. We then propose a novel\nfiltering mechanism which leverages these representations to select semantically\ndiverse and high quality tactics, using Determinantal Point Processes. Our ap-\nproach, 3D-Prover, is designed to be general, and to augment any underlying tactic\ngenerator. We demonstrate the effectiveness of 3D-Prover on the miniF2F-valid\nand miniF2F-test benchmarks by augmenting the ReProver LLM. We show that\nour approach leads to an increase in the overall proof rate, as well as a significant\nimprovement in the tactic success rate, execution time and diversity.", "sections": [{"title": "1 INTRODUCTION", "content": "Interactive Theorem Proving, as the name suggests, has traditionally involved a human guiding a\nproving system to verify a formal proposition. It has found applications in a wide range of fields,\nfrom secure software (Tan et al., 2019) to the verification of mathematical results (Hales et al.,\n2017). There has been significant interest in automating this process, with formalization efforts\nrequiring a high level of human expertise (Klein et al., 2009). Beyond this, it is considered a 'grand\nchallenge' for AI, requiring a high level of reasoning and planning to be successful (Reddy, 1988).\nEven the largest current models struggle with the complexity of the task, with for example GPT-4\nonly able to solve 13.5% (Thakur et al., 2023) of the high school level miniF2F-test (Zheng et al.,\n2021) benchmark. This has motivated the development of specialized models and search algorithms\nto address the unique challenges of the domain (Wang et al., 2024; Polu et al., 2022; Jiang et al.,\n2022b; Han et al., 2022; First et al., 2023; Zhao et al., 2023; Wang et al., 2023; Whalen, 2016; Wu\net al., 2021b; Wang et al., 2018; Wang & Deng, 2020; Rabe et al., 2020; Polu & Sutskever, 2020;\nMiku\u0142a et al., 2023; Loos et al., 2017; Li et al., 2021; Lewkowycz et al., 2022; Jiang et al., 2021;\n2022a; Gauthier et al., 2017).\nWith most non-trivial proofs requiring long chains of correct reasoning, it is a challenge to generate\nthem in one pass without mistakes. The addition of a search algorithm is common for addressing\nthis, as is done by the current state-of-the-art DeepSeek-Prover-V1.5 (Xin et al., 2024). Under this\nparadigm, candidate tactics are generated and executed in the proving system, which (if successful)\nresults in new subgoals to prove. This generates a tree of possible proof paths, where a search algo-\nrithm selects the most promising nodes to expand. The primary challenge faced by these approaches"}, {"title": "2 TRANSITION AWARE REPRESENTATION LEARNING", "content": "One proof attempt can generate a rather large amount of data. A single pass of the miniF2F-valid\nbenchmark of 244 proofs results in approximately 500,000 transitions, capturing rich information\nabout the error likelihood, execution time and resulting proof state or error message. This section\nexplores the feasibility of using this transition data to learn how tactics affect the environment.\nWe operationalise this as a supervised learning task: given a goal and tactic, we predict the error\nstatus, execution time and environment output. We effectively learn these targets from only this\nsynthetic data, and further embed this information into a compact tactic representation. The upshot,\nas we show in Section 3, is that these representations can be utilised to improve the performance of\nsubsequent proof attempts."}, {"title": "2.1 TRANSITION MODELS", "content": "We assume a dataset D of transition tuples {(gk,tki, Ski, Tki, Oki)}, as defined in 1.2. We learn a\ntransition model $\\mathcal{E}$ : S\u00d7T \u2192 {0,1}\u00d7R\u00d7O which maps a goal gk and tactic tki to an estimate of the\nstatus ski, time Tki and output oki. We construct our transition model $\\mathcal{E}$ with three components. For\nd\u2208 N, the Encoder E : S \u00d7 T \u2192 Rd takes the goal gk and tactic tki as input, and outputs a single\nembedding vector with unit norm, E(gk,tki) = eki, ||eki|| = 1. The Predictor P : Rd \u2192 [0, 1] \u00d7 R"}, {"title": "2.2 EXPERIMENTS", "content": "For our experiments, we use an Encoder-Decoder Transformer for the Decoder D, and an Encoder-\nOnly Transformer for the Encoder E. We take the pretrained ReProver (Yang et al., 2023) LLM\nto initialise both components. We implement the Predictor P as a single hidden layer MLP, with\nhidden dimension d/2 (where d = 1472) and two real valued output nodes. The time prediction Tki\nis the output of the first node, and the status prediction \u015dki is taken as the sigmoid of the second.\nWe investigate four instances of the transition model $\\mathcal{E}$. For the COMBINED model (Figure 2),\nthe tactic is concatenated with the goal, and the embeddings from the Encoder are computed for\nall tokens. We then generate a single tactic embedding by mean-pooling over the tactic tokens.\nWe compare this with the SEPARATE model which encodes the tactic without attending to the\ngoal. We hypothesise that allowing the tactic tokens to attend to the goal will allow the Encoder to\nbetter represent the semantics of the tactic. To form a naive baseline, we implement a NO TACTIC\nmodel which does not use the tactic at all, and instead uses only the goal tokens. We do this to\naccount for any inherent patterns in the goal which may be predictive of the outcome, for example a\nparticular goal which has a high error rate. This allows us to ground our results in the performance\nof this baseline, so we can observe the direct effect of the tactic in predictive performance. We also\ncompare with an ALL TOKENS model which uses all tactic tokens for the Decoder without reducing\nto a single embedding. We maintain the pooling operation over the tactic tokens for the status and\ntime prediction tasks, but allow the Decoder to attend to all tokens for the output prediction. We\nimplement this comparison to see the degree of information loss induced by reducing tactics to a\nsingle vector."}, {"title": "2.2.1 RESULTS", "content": "We obtain the dataset D from a vanilla ReProver attempt on miniF2F-valid, which results in 498,236\ntransitions, which we split randomly into 95% training, 5% testing. There is the possibility of\ndependence between the splits, as the test set includes goals seen in training with different tactics.\nThe NO TACTIC baseline should capture any of this, with our results in Section 3.3.1 showing our\nrepresentations generalise from miniF2F-valid to miniF2F-test. For the error prediction task, we\nreweight classes to account for imbalance, which is approximately 75% error, 25% success. We use\nthe AdamW optimizer, with a learning rate of 10-5 and a batch size of 1. We train each model for 2\nepochs on a single RTX4090, and report the results on the test set."}, {"title": "3 FILTERING MODEL", "content": "In the previous section we used synthetic proof data to generate semantically aware tactic represen-\ntations, allowing us to predict the likelihood of success, execution time and environment response.\nWe now take this a step further by using these representations to augment proof search. We first\npresent the necessary background on Determinantal Point Processes, enabling us to introduce our\nfiltering model 3D-Prover, which prunes tactic candidates based on their quality and the semantic\ndiversity of their representations. We show that 3D-Prover is able to improve the performance of\nthe ReProver LLM on the miniF2F-valid and miniF2F-test benchmarks, particularly when a deeper\nsearch configuration is used. We conclude with a multifaceted ablation study showing the effect of\nour filtering model on the success rate, number of unique responses and execution time."}, {"title": "3.1 DETERMINANTAL POINT PROCESSES", "content": "Determinantal Point Processes (DPPs) are a class of probabilistic models for sampling subsets from\na ground set Y. In line with Kulesza (2012), for Y = n we define the kernel L \u2208 Rn\u00d7n of a DPP\nas the Gram matrix L = BTB for B \u2208 Rn\u00d7d, where column b\u00bf \u2208 Rd of B is a vector representing\nelement i \u2208 {1, ..., n} of y. These vectors bi are commonly decomposed into a set of unit norm\ndiversity features i \u2208 Rd and quality scores qi \u2208 R+, so that bi = qi\u0444\u0456, ||i|| = 1 for all\ni\u2208 {1, ..., n}. The similarity matrix S is then defined as Sij = jT\u03a6i.\nThe probability of sampling a subset A C y from a DPP is then proportional to the determinant\nof the submatrix of L indexed by A, P(A) x det(LA) = (\u03a0i\u2208aq)det(SA). Geometrically, this\ndeterminant is the volume of the parallelepiped spanned by the submatrix LA, which as we see in\nFigure 3, is maximised based on a combination of the similarity and length (quality) of the chosen\nelements. In this way, DPPs elegantly trade off between the quality and diversity of elements.\nNormally the size of the sampled subset |A| is variable, however Kulesza & Taskar (2011) introduce\nk-DPPs which restricts the size of the subset to a fixed k \u2208 N, and where the probability of sampling\nA is normalised over subsets of size k. That is, for a k-DPP, P(A) \u221d det(LA)/\u03a3|A'|=k det(LA')."}, {"title": "3.2 FILTERING MODEL", "content": "Algorithm 1 defines our filtering model, 3D-Prover, which maps a list of tactics T from the under-\nlying tactic policy \u03c00 to a subset T' of size K. We use the Encoder E and Predictor P defined in\nSection 1.2 to generate unit norm tactic embeddings \u03c6i and predict the time and error likelihood.\nThe embeddings \u03c6i encode the predicted environment response through their direction only, as they\nare unit norm (Figure 3). The quality scores qi then scale these tactics based on the underlying model\nlogits mi, as well as the predicted error likelihood si and execution time \u03c4i. We have hyperparame-\nters for the normalisation temperature \u03b8, as well as the error and time weights \u03bbs, \u03bb\u03c4. The parameter\n\u03b8 controls the scaling temperature of the model logits, with a higher temperature flattening out the\ndistribution. It therefore adjusts the diversity bias of the filtering model by reducing the impact of\nthe quality scores when sampling. We then compute the kernel L from qi and \u03c6i, and sample a\nsubset of tactics T' using the k-DPP algorithm (Kulesza & Taskar, 2011)."}, {"title": "3.3 EXPERIMENTS", "content": "To test the performance of 3D-Prover, we use ReProver (Yang et al., 2023) as the underlying tactic\npolicy \u03c00, with the Encoder E and Predictor P components as defined in Section 1.2. We chose\nReProver as it is a small (~ 300M parameters), popular and performant open source model, allowing\nus to run our experiments in a reasonable timeframe. We run our experiments in Lean 3 (De Moura\net al., 2015) using the BAIT (Lamont et al., 2024) platform with a modified LeanDojo (Yang et al.,\n2023) environment, where we set an environment timeout of 600 seconds per proof attempt. We train\na combined transition model on the miniF2F-valid benchmark, and use the Encoder and Predictor\ncomponents to generate tactic embeddings and quality scores as per Algorithm 1. We first examine\nthe performance of 3D-Prover without any hyperparameter tuning, setting \u03bbs = \u03bb\u03c4 = 0, \u03b8 = 1. We\nthen perform ablation studies using miniF2F-valid to examine the influence of the hyperparameters\non the tactic success rate, execution time and diversity of the environment response. For miniF2F-\ntest, we allow the model four attempts per proof to increase confidence in the results, while for\nminiF2F-valid we allow one attempt per configuration to facilitate a wider set of ablations.\nWe set the search policy for all experiments to be Best First Search (BestFS), where nodes are\nexpanded in order of their cumulative log probability. For each node selected for expansion, we\ngenerate N = 64 candidate tactics from the underlying ReProver model using beam search with\ndefault settings, as done in the original ReProver implementation. This forms the ground set for\nthe node, to be sub-sampled by the filtering algorithm. We use beam search decoding because\nit is deterministic and so ensures that the ground set for a given node remains fixed across runs,\nallowing us to isolate and compare the effect of the filtering algorithm. The filtering algorithm\nreturns K tactics, which are then executed in the environment and used to update the proof tree, as"}, {"title": "3.3.1 PROOF PERFORMANCE", "content": "outlined in 1.2. We test three different levels of filtering, with K \u2208 {8, 16, 32}. Lower values of K\ncorrespond to more filtering, for which the choice of filtering algorithm will have a greater impact.\nWe compare the filtering approach of 3D-Prover, as outlined in Algorithm 1, with two baselines.\nThe Top-K baseline takes the top K tactics from the ground set as judged by their log probabilities,\ncorresponding to the top K beams. We take K tactics at random from the ground set to form the\nRandom baseline, as an exploration-focused comparison."}, {"title": "3.3.2 ABLATION STUDY", "content": "To demonstrate the utility of our transition model representations,\nwe compare to an ablated 3D-Prover where the transition model Encoder is replaced by an Autoen-\ncoder of the same size. The Autoencoder is trained to reconstruct the original tactic, and therefore\ngenerates representations which reflect only the syntax of the tactic. In this way, we can test our hy-\npothesis that semantically aware tactic representations are useful for proofs, justifying the inclusion\nof the transition model. As we observe in Table 3, the performance of 3D-Prover with the transition\nmodel embeddings is indeed superior to that of the Autoencoder across all values of K. This shows\nthat selecting for diversity with respect to the predicted semantics, rather than the syntax, leads to a\ndirect improvement in proof performance."}, {"title": "Success Rate", "content": "We observe from Table 4 that the success rate of tactics chosen by 3D-Prover is\nsignificantly improved compared to both baselines. We also note that as K decreases, this improve-\nment increases in magnitude, reflecting the heightened influence of the filtering model. We see that\nthis improvement increases with the error weight \u03bbs, which scales the quality scores of tactics by\ntheir predicted probability of success. This suggests the error weight term is directly influencing the\ntactic success rate, showing that it is working as intended."}, {"title": "Diversity", "content": "To examine the diversity of a proof attempt, we consider two metrics. For the first, as\nin Table 5, we look at the percentage of unique environment responses to tactics executed per node,"}, {"title": "4 CONCLUSION", "content": "Future work One might consider structured DPPs (Kulesza, 2012), which operate at the tree level\nto select diverse paths, rather than the node level, which selects diverse edges. Continual learning\nof the transition model is another avenue, where training on new data as it is generated could lead\nto more accurate assessments of diversity and quality. Our approach could also be combined with\na separate search algorithm such as HTPS (Lample et al., 2022), rather than BestFS. Testing larger\nmodels would be a natural extension, for both the transition model and the underlying tactic gen-\nerator. Our methodology may also be useful to enhance search in domains beyond formal proving,\nsuch as code generation or game playing.\nSummary We introduce 3D-Prover, a method to augment proof search by filtering candidate tac-\ntics to generate diverse and high quality subsets. By generating tactic representations which reflect\nthe response of the proving environment, 3D-Prover is able to filter tactics based on their likely out-\ncome. We evaluate 3D-Prover by augmenting the ReProver LLM on the standard miniF2F bench-\nmark, where we find an improvement in the overall proof success rate (Table 2), particularly for"}, {"title": "A APPENDIX", "content": "Table 8 shows the Pass@4 results for miniF2F-test, which is the number of proofs found at least\nonce over four attempts. We compare 3D-Prover to the Random baseline, taking the same four runs\nfrom Table 2, where \u03bbs = \u03bb\u03c4 = 0, \u03b8 = 1. With Top-K being deterministic, the Pass@k rate is\nthe same as the Pass@1 rate. Given several attempts, K = 16 appears to provide a good tradeoff\nbetween breadth and depth, performing the best overall. 3D-Prover maintains a large improvement\nfor K = 8, with a modest improvement for K = 16.\nAs discussed by Chen et al. (2021), the Pass@k metric favours exploratory approaches as k in-\ncreases, at the cost of lower performance for smaller k. This is because, over many attempts, a\nhighly exploratory approach is more likely to find at least one proof of a given goal, even though\nit may find fewer proofs in a single attempt than a more exploitative approach. Further discussion\nin Lample et al. (2022) finds that randomly sampling search parameters also improves Pass@k.\nWith Pass @k being expensive to estimate, we fix our parameters over the four runs to give a more\naccurate estimate of Pass@1. Given this, a large scale experiment sampling these hyperparameters\ncould lead to improved Pass@k results, as Lample et al. (2022) show for their HTPS approach."}, {"title": "A.3 EMBEDDING DISCUSSION", "content": "We now investigate whether the transition model (Figure 2) captures\ntactic semantics rather than syntax in its tactic embeddings. To test this, we examine the cosine\nsimilarity of tactic embeddings which lead to unique subgoals. Figure 4 takes an example node,\nexamining all tactics which lead to a unique subgoal. The upper value displays the cosine similarity\ngiven by the transition model, while the lower value displays that given by the Autoencoder in\nSection 3.3.2. We observe that in most cases, the similarity given by the transition model is much\nlower than that given by the Autoencoder, which is only considering the syntax of the tactic. For\nexample, the similarity between tactic 3 and 4 is very high for the Autoencoder, given the similar\nsyntax between the two as they use the same lemma. Despite this similar syntax, the transition\nmodel embeddings show a high degree of dissimilarity, reflecting the different outcome they have\non the environment. We present additional examples in the supplementary code. To generalise\nbeyond these examples, we ran this comparison over the tactic embeddings which lead to unique\nsubgoals for all 244 root nodes in minF2F-valid. Figure 5 shows the distribution of the average\ncosine similarity for each node, for both the transition model and the Autoencoder. The average"}, {"title": "A.4 COMPUTATIONAL OVERHEAD", "content": "On our hardware, we found 3D-Prover adds a constant overhead, taking approximately 2x as long for\ntactic generation. The majority of this is in generating embeddings for the 64 tactics, which we were\nunable to batch on our hardware due to memory constraints. The DPP algorithm itself added almost\nno overhead once the embeddings were generated. This could be sped up by batching (if memory\npermits), or through a different architecture. For example, the SEPARATE model in Section 2.2 could\nbe used, where tactics can be batched with much less memory. An augmented architecture which\nembeds the goal in isolation, which is then given to the tactic encoder as a single vector, could be\nused. This would provide a speed up while allowing some attention between the tactic and the goal,"}]}