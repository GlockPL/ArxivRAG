{"title": "Exploring the Lands Between: A Method for Finding Differences between Al-Decisions and Human Ratings through Generated Samples", "authors": ["LUKAS MECKE", "DANIEL BUSCHEK", "UWE GRUENEFELD", "FLORIAN ALT"], "abstract": "Many important decisions in our everyday lives, such as authentication via biometric models, are made by Artificial Intelligence (AI) systems. These can be in poor alignment with human expectations, and testing them on clear-cut existing data may not be enough to uncover those cases. We propose a method to find samples in the latent space of a generative model that is challenging for a decision-making model with regard to matching human expectations. By presenting those samples to both the model and human raters, we can identify areas where its decisions align with human intuition and where they contradict it. We apply this method to a face recognition model and collect a dataset of 11,200 human ratings from 100 participants. We discuss findings from our dataset and how our approach can be used to explore the performance of AI models in different contexts and for different user groups.", "sections": [{"title": "1 INTRODUCTION", "content": "Today, more and more people are confronted with Artificial Intelligence (AI) decision-making systems in their everyday lives. Applications of these systems cover a wide range, from professional (for example, supporting doctors in their diagnosis [37]) to personal scenarios such as identifying friends in photos taken [32] or authentication through biometric systems. However, these systems can make decisions that are sometimes hard to understand, surprising, inconsistent, or in little alignment with human perception or intuitions. As an example, Pixel phones authenticated users with closed eyes, allowing for unwanted unlocks in their sleep\u00b9. Some sensor-based decision systems may enact hidden biases learned from the training data [4, 36]. For example, some facial recognition systems have been reported to work worse for people of colour [4, 41]. Furthermore, system performance can vary greatly depending on the user [40] or specific user groups [4]. Such unfair, unexpected, and inexplicable decisions are particularly critical in biometric systems as they often serve as gatekeepers for access to facilities, financial and other resources, data, or devices.\nThus, global performance metrics, like precision or recall of a system, that are the current state of the art for describing model behavior may not be relevant and applicable to the individual. Such methods of testing decision-making models can fall short, as they do not account for decision alignment with users' expectations and rely on real-world data with clear-cut expected decisions. The model's performance and potential weaknesses may become more apparent when exploring the space between, where it is less clear if two images belong to different identities, making decisions harder.\nFollowing this idea, this paper presents a method to explore the decision boundaries of (black-box) decision-making systems using artificially generated inputs. We propose three main areas of inputs: samples that are expected to lead to a positive decision, samples that are expected to lead to a negative decision, and samples that explore the space in between where the decision may be unclear. For authentication models like biometrics, ratings of those samples can give direct insights into their performance under different conditions. Generating artificial samples has two main advantages: no real-world samples are needed (beyond training the generative model), and we can explore cases that would not be possible in real life to better understand model performance under certain conditions (e.g., a voice that is modified to sound more like a different speaker). By presenting these samples both to human raters and decision models, it is then possible to find areas of agreement and disagreement that can inform the design and improvement of decision-making models and user interfaces for them.\nAs a concrete application of a model making security-relevant decisions for its users, we evaluate our method by exploring face recognition with color images as input (e.g., face unlock with a phone camera). This is both easy to interpret (humans themselves are excellent at recognizing faces [8]) and of high societal relevance and impact\u00b2. We generate meaningful alterations (see Figure 1) for 40 base images using StyleGAN2 [19] and collected a dataset of perceived similarity and identity of the presented image pairs in an online comparison task with 100 participants to demonstrate the viability and potential use cases for our method.\nWe find interesting mismatches between the analyzed face recognition model and human raters. The model rated images of children more similar and semantic changes (like the addition of glasses) less similar than humans would. Our optimized samples successfully fooled the decision model while mostly being perceived as different by humans. Latent distance and perceptual distance were good predictors for perceived identity except for those optimized samples. Those results show that our method can provide insights into the performance of biometric models, support developers in improving them, and help to communicate a more nuanced view of model performance to end-users. We conclude with a discussion of those opportunities and the applicability of our method to other models, use cases, and user groups."}, {"title": "2 RELATED WORK", "content": "In this section, we focus on approaches for generating artificial content and previous uses of such approaches. We introduce methods of introspection into model performance as well as the related topic of adversarial samples."}, {"title": "2.1 Artificial Content Generation", "content": "In the last years, approaches for artificial sample generation have gained public awareness through methods like ChatGPT\u00b3 for text production or Stable Diffusion [31] and DALL-E [28] for generating images from text prompts. Other approaches include autoencoders [21, 26, 38], normalizing flows [20, 30], and Generative Adversarial Networks [9, 10, 17, 19](GANs), to name just a few. Those models learn a representation of the distribution of their training data (latent space) that can then be used to generate new and altered samples. In particular for GANs, there exist many approaches showing how their latent space can be used to generate semantic edits (for example, making a person older) [16, 34, 39], find meaningful dimensions [13], and mixing samples both on a style and content level [5, 19]. Beyond artistic purposes, they can also be used to generate synthetic data for training and evaluating machine learning models [6, 18] and finding biases [2, 7]. With our approach, we utilize those models to produce challenging samples for a decision-making model. Given the recent advances in the field, we believe that our method is (or soon will be) applicable to many other (security) fields (e.g. speaker recognition using synthetic voice [25]) as well.\nA related approach is generating or finding so-called adversarial examples [1, 11, 14]. Those are characterized as small changes to the input of a neural network that are not perceptible by humans but cause the model to flip its decision or predict a different class. We conceptually follow a similar approach to adversarial samples: we propose to generate inputs to a decision-making model that leads to unexpected results. However, we are explicitly interested in cases where changes are perceptible, but their impact does not align with human expectations."}, {"title": "2.2 Model Evaluation and Introspection", "content": "The state of the art for reporting on decision-making performance are global metrics like precision, recall, or F-scores [27]. Beyond these metrics, the field of explainable artificial intelligence (XAI) has proposed different methods for further model introspection [3, 12, 15, 22-24]. Some researchers divide these approaches into categories such as model explanations, outcome explanations, black box inspection, and transparent design [22], while others distinguish based on whether they focus on explaining single predictions (local) or the model as a whole (global), and whether they are model-specific (applicable to one model or a group of models) or model-agnostic (suitable for any model) [3]. Models can be transparent by design or explored with post-hoc explanations [3]. Our method provides such post-hoc explanations and is model agnostic, meaning it can be applied to various models. The novelty of our approach is in its inclusion of human elements in the explainability process."}, {"title": "3 METHOD", "content": "We suggest comparing the outputs of a decision-making model on strategically sampled inputs to answers by human raters to better understand the model. Note how both the humans and the generated samples are needed. Without human ratings, we don't gain insights into mismatches in perception, and by using only real-world data, we cannot access the space between clear-cut decisions where we expect those mismatches to be found. Here, we use a generative model to produce samples inspired by the outputs of a classical classification task: true and false positives as well as their negative counterparts. We illustrate and explore our method for the concrete case of face recognition as an example of a decision-making model. All empirical claims are limited to this use case and we have no evidence if our findings generalize to different decision-making models or sample types. However, our modular design should allow for adapting it to different use-cases and we discuss possible extensions and applications of our method in Section 6."}, {"title": "3.1 Generator", "content": "The core component of our approach is a generative model to provide samples that can serve as input to the model we wish to test. For the case of evaluating face recognition, we propose using GANs as they have been shown very capable of generating realistic face images [18, 19] and their continuous latent space can be leveraged for targeted manipulations [34, 39]. They can thus be used to produce alterations of a given starting point and samples between existing real-world data points for which no ground-truth \"true\" labeling exists. Note that while we suggest using GANs for face images, other generative approaches (see Section 2) are possible as long as they can produce targeted manipulations. In some cases, no model may be required (e.g., if the decision-making model uses only numerical inputs)."}, {"title": "3.2 Samples", "content": "We now illustrate the sets of samples we propose to generate and the rationale behind choosing them. Each sample is always generated in relation to a base (i.e., the starting point in the latent space) that will later be used for the comparison (see Section 3.3). For the example of face recognition, this would be the face image to test the decision model on. In this paper, we define samples by their expected decision (e.g., we expect positive samples to be rated as similar) and find them by their relation in latent space to avoid making assumptions on possible influencing factors. However, when such assumptions exist (e.g., light conditions affecting recognition), samples could be generated to reflect them instead (see, e.g., [7]). Note that our generated samples are independent of the decision-making model to be tested. As such, our approach is also suited to explore black-box models as long as they can be queried."}, {"title": "3.2.1 Genuine Samples", "content": "Genuine samples (Figure 2a) are identical to the base image and, thus, what would be called a true positive in traditional classification tasks, i.e., they preserve the identity of the base image. Note that this is the only possible true positive as it is not necessarily clear to which identity a sample should be attributed. This is the case because we can continuously sample images from the latent space between two identities in contrast to the real world. We include this sample as a baseline for participants to calibrate their rating of similarity against and as a check to ensure participants pay attention while rating the image pairs."}, {"title": "3.2.2 Positive Samples", "content": "We propose positive samples (Figure 2b) as slight modifications to the base. We find those by sampling a (random) direction in the latent space and taking a small step away from the base in that direction. The assumption behind this approach is that, given a locally stable latent space, this should produce minor alterations to the input and preserve the model's decision on the base (for example, identity for our case of face recognition)."}, {"title": "3.2.3 Negative Samples", "content": "We choose random samples from the latent space as negative samples (Figure 2c), i.e., samples that we expect to be attributed to a different identity as the base. This is based on the assumption that the latent space is big enough that randomly generating a sample similar to the base is unlikely. For practical reasons, we propose using other base samples as negative samples. This way, bases can serve as both genuine and negative samples, and their function is only defined relative to their respective base."}, {"title": "3.2.4 Optimized Samples", "content": "We propose to use each negative sample as a starting point for a black box optimization algorithm to find samples that maximize the decision-making model's target function (for example, similarity). In contrast to the other samples, this step requires access to either the decision-making model itself or a similar one. In our results (see Section 5.3) we explore if two different models can be used for this (i.e., one to be explored, one for generation). Given the different starting points, we assume that the generated samples (Figure 2d) should represent local maxima in the latent space (instead of finding the original base sample). Thus, they may very well not be similar to a human observer, even though they are similar according to the optimization function. As such optimized samples fulfill the role of potential false positives."}, {"title": "3.2.5 Interpolation Samples", "content": "To better understand where the model's decision between two (base) samples changes, we introduce interpolation samples (Figure 2e). Those are generated by following the latent vector between the base and each negative sample. Generating candidates for false negatives through optimization would require human ratings as a target function. As those are not available at generation time, interpolation samples are our best attempt at provoking this type of misclassification."}, {"title": "3.3 (Human) Raters", "content": "The final component of our approach is having the generated samples rated by humans as a baseline to compare against the model's decisions (remember that for the samples we propose, no ground truth is available; so this step is necessary). This is based on the premise that the decision-making model is supposed to decide similarly to a human or at least in a way that aligns with human intuition. To reflect this, we propose that human raters judge the model's target function and their expected decision. For the example of a face recognition model, this maps to a similarity score for the presented faces and the decision whether two images show the same person. Depending on the use case, it may be possible to have the samples rated by a different model instead of humans if only differences between models should be explored with no focus on whether they adhere to human perception."}, {"title": "4 EXPERIMENTS", "content": "We now illustrate how we implemented the approach described above for the example of face recognition. We explain how we generated the samples and implemented the comparison task for human raters. With this experiment, we explore the robustness of our proposed method and uncover potential misalignment between human raters and face recognition models. Note that latent spaces allow for many potential comparisons, and our choice can only capture a fraction of them. The choices of both samples and parameters presented here reflect our best attempt at striking a balance between many potential comparisons, sufficiently many samples to observe trends, and a number of comparisons that participants can realistically make."}, {"title": "4.1 Dataset Generation", "content": "We generate all samples using StyleGAN2 [19]. Base images (and consequently negative and genuine images) are sampled as random seeds from the latent space. The number of samples scales with the number of base images, as, e.g., interpolation samples are generated between all available bases. Thus, we decided on a batched approach with four base images per batch. For each base, we generate one genuine sample (i.e., a copy). We chose two random directions and generated three positive samples (at distances 0.2, 0.4, and 0.6) for each of them (6 total). As negative samples, we include the remaining three base images. Furthermore, we generate interpolation samples at 25%, 50%, and 75% of the distance between the base and each negative sample (9 total). Finally, we use the Covariance Matrix Adaptation Evolution Strategy (short CMA-ES)\u2074 as a black box optimizer to find optimized samples. This approach empirically finds a gradient by sampling points around a given starting location and optimizing a given score function. We encoded images as a 512-dimensional vector of their latent representation and used a Python face recognition algorithm\u2075 as the optimization function. We ran 100 iterations with a truncation factor of 0.3 (i.e., we stopped optimization when reaching this distance score) using \u03c3 = 1. We ran one optimization starting from each of the negative samples. We chose the first results that achieved a recognition distance below 0.3, 0.4, and 0.5 respectively as optimized samples (9 in total). The best sample was chosen if the optimization did not reach this score. For reference: the suggested default recognition distance for deciding on identity in the used Python library is 0.6, so all of those samples would be accepted. Overall, this approach yielded 112 samples in each batch (28 samples for each base image)."}, {"title": "4.2 Procedure", "content": "To collect human ratings on our samples, we designed an online survey. First, participants would be informed about the purpose of the study and had to consent to their data being collected and analyzed. Next, we collected basic demographic data before participants got to the main task. Here, participants were repeatedly presented with an image pair where one was always a base image and the other was one of the samples described in Section 4.1. Each participant rated 112 image pairs with their order being randomized both between and within participants. For each image pair, we asked participants for their perception of the similarity of the two faces and a binary decision if they believed they showed the same person. We kept the overall study duration short to avoid the repetitive nature of the task influencing our results. We concluded by asking participants for their strategy in rating both similarity and identity."}, {"title": "4.3 Participants and Recruitment", "content": "We recruited 100 participants (50 female, 48 male, 1 non-binary, 1 no answer) with a mean age of 27.6 (STD=7.8, range: 19-61) using Prolific. People from all continents contributed to our dataset. The study took about 20 minutes and was compensated with \u00a32.25. The study was approved by our institute's ethics commission under Nr. EK-MIS-2023-204."}, {"title": "5 RESULTS", "content": "In this section, we describe the collected dataset and verify the effectiveness and robustness of our sampling methods. We conduct a correlation analysis to assess the alignment of human ratings and model scores and propose disagreement scores to find visual examples for the cases where ratings differ the most."}, {"title": "5.1 Dataset overview", "content": "Our dataset consists of 1,120 image pairs (10 batches of 112 image pairs each) that were rated by 10 participants each, resulting in a total of 11,200 ratings of similarity (0: not similar to 100: very similar) and identity (fraction of participants rating as identical). In addition, we post-hoc calculated distance scores for four common state-of-the-art face recognition algorithms using the DeepFace library by Serengil and Ozpinar [33], as well as latent distance (based on the distance of embeddings in the StyleGAN2 latent space) and perceptual distance (lpips) [43]. For the sake of brevity, we only compare against Dlib face distance when making comparisons to a face recognition model (unless otherwise stated)."}, {"title": "5.2 Robustness of Sample Generation based on Human Ratings", "content": "As a first step, we validate the robustness of our approach. An overview of the distribution of human similarity ratings by sample type is given in Figure 4a."}, {"title": "5.3 Alignment of Human Ratings with Face Recognition Models and Distance Metrics", "content": "Next, we assessed the alignment of human ratings with model decisions. Points farther away in the latent space were generally rated less similar, which is in line with related work [35]. However, our optimized samples break both trends, being rated less similar by humans but more similar by the algorithm while also farther away in"}, {"title": "5.4 Finding Disagreement between Human Raters and Models", "content": "One of the goals of our approach was to find samples that lead to a mismatch between human raters and a decision-making model or are generally challenging to decide. To find such cases, we calculated a disagreement score between human-rated identity and the (inverted) Dlib distance score. We do not use an absolute value, as both directions are interesting and map to likely candidates for false positives (face recognition rating similarity higher) and false negatives (humans rating similarity higher). To find disagreements between participants, we calculate the standard deviation of their ratings of similarity and identity. Figure 6 shows the samples with the largest disagreement with respect to the described measures. We observe that the candidates for false positives are mainly children and were generated by optimized samples, hinting at a weakness in the assessed decision-making model to correctly judge the similarity of children. Potential false negatives were mainly generated through negative and interpolation samples. They have in common, that they differ in meaningful ways like age, pose, or accessories."}, {"title": "5.5 Determining Similarity", "content": "We asked participants for the strategies they used to determine the similarity and identity of the given image pairs. Most participants stated that they compared facial features and decided based on faces looking similar or following their intuition. The main features participants looked at were eyes and eye color, hair (color), the nose, and the mouth area. When judging identity, participants also particularly focused on identifying details, for example, P3: \"I was looking for particular facial features (for example, dimples, teeth, wrinkles, nose shape, etc.)\". They also tried to ignore parts of"}, {"title": "6 DISCUSSION", "content": "In this work, we studied one example case for our proposed method to explore decision-making models. Here, we reflect on our insights from this test, the limitations of our approach, and further target groups and applications to explore."}, {"title": "6.1 Insights", "content": "We found that human perception overall aligned with the outputs of our test face recognition model. We found a strong correlation between human identity ratings and perceptual distance (lpips) [43], indicating it may be well suited as an approximation for face recognition. However, our generated samples were also successful in uncovering misalignments. Optimized samples were very successful at generating potential cases of false positives. Similarly, positive and interpolation samples led to cases where the decision-making model indicated less similarity compared to human raters. Visual inspection of the samples with the largest disagreement suggests that the tested model struggled"}, {"title": "6.2 Reflections on the Method", "content": "Using a generative model to explore a decision-making model can introduce new biases or hide existing shortcomings of the tested models if biases align. While this cannot completely be avoided, we believe that introducing human raters can uncover some of those effects. At the same time, the inclusion of human raters also limits our approach to tasks that actually can be reliably rated by humans. This includes in particular decisions that people already make in their daily lives, like recognizing others by their face, voice, or the way they walk. However, comparing something like fingerprints or making a diagnosis on medical data is uncommon or requires experts, making it less suited for our method. While we had a diverse set of raters and tested face recognition algorithms, our results may not generalize to other populations and models. Future work could further investigate such influencing factors. Finally, we used StyleGAN2 [18] in this work. However, our approach should be compatible with other GAN variants and potentially also other generative methods, as long as their latent space is locally stable and can be navigated."}, {"title": "6.3 Implications for (Security) User Interfaces", "content": "Our analysis has shown that our proposed sample sets reliably produce potential mismatches between human perception and model decisions. This opens up opportunities for interface design. As an example, our approach could be used to make model performance graspable for users by illustrating how someone would look like that could unlock their device or how changes in appearance affect the decision. No external human ratings will be needed, as users can directly assess alignment with their perception. Model alignment with human perception (based on the results in Section 5.3) can be shown in addition to global performance metrics to help users to adjust decision thresholds, avoid failure cases, choose a different model or more generally - make an informed decision if and how to use it."}, {"title": "6.4 Extension to other Use Cases", "content": "We used our proposed method for face recognition in this paper. However, we chose a modular design that allows for replacing components to facilitate adaptations and thus expect it to be applicable to other models as well. The key prerequisites we see are: (1) humans can make the same decisions as the model (e.g. assess identity or similarity), and (2) generative models exist for the type of input (e.g., voice recognition [29] or gait and gesture recognition [42]). In addition, our approach also generates a labeled dataset of synthetic samples that can be used to improve the model."}, {"title": "7 CONCLUSION", "content": "In this paper, we proposed leveraging generative models to strategically produce alterations (positive, negative, interpo- lation, and optimized) to the input of a decision-making model and compare its ratings to human ratings. We collected a dataset of 11,200 ratings of similarity and identity for pairs of face images and compared them to the output of a face recognition model, providing insights into how the perception of humans and the algorithm differs. We hope, that our approach can be leveraged by developers of decision-making models to explore and improve weaknesses in their models and support the design of user interfaces for individuals to better understand the models they interact with in their daily lives and empower them to make informed decisions on if and how to use them."}]}