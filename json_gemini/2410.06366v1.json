{"title": "Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling", "authors": ["Zijie Huang", "Wanjia Zhao", "Jingdong Gao", "Ziniu Hu", "Xiao Luo", "Yadi Cao", "Yuanzhou Chen", "Yizhou Sun", "Wei Wang"], "abstract": "Learning complex physical dynamics purely from data is challenging due to the intrinsic properties of systems to be satisfied. Incorporating physics-informed priors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision modeling for energy-conservative systems. However, real-world systems often deviate from strict energy conservation and follow different physical priors. To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a novel regularization term. It helps preserve energies for conservative systems while serving as a strong inductive bias for non-conservative, reversible systems. While TRS is a domain-specific physical prior, we present the first theoretical proof that TRS loss can universally improve modeling accuracy by minimizing higher-order Taylor terms in ODE integration, which is numerically beneficial to various systems regardless of their properties, even for irreversible systems. By integrating the TRS loss within neural ordinary differential equation models, the proposed model TREAT demonstrates superior performance on diverse physical systems. It achieves a significant 11.5% MSE improvement in a challenging chaotic triple-pendulum scenario, underscoring TREAT's broad applicability and effectiveness. Code and further details are available at here.", "sections": [{"title": "1 Introduction", "content": "Dynamical systems, spanning applications from physical simulations (Kipf et al., 2018; Wang et al., 2020; Lu et al., 2022; Huang et al., 2023; Luo et al., 2023a; Xu et al., 2024; Luo et al., 2024) to robotic control (Li et al., 2022; Ni and Qureshi, 2022), are challenging to model due to intricate dynamic patterns and potential interactions under multi-agent settings. Traditional numerical simulators require extensive domain knowledge for design, which is sometimes unknown (Sanchez-Gonzalez et al., 2020), and can consume significant computational resources (Wang et al., 2024). Therefore, directly learning dynamics from the observational data becomes an attractive alternative.\nExisting deep learning approaches (Sanchez-Gonzalez et al., 2020; Pfaff et al., 2021; Han et al., 2022a) usually learn a fixed-step transition function to predict system dynamics from timestamp t to timestamp t + 1 and rollout trajectories recursively. The transition function can have different inductive biases, such as Graph Neural Networks (GNNs) (Pfaff et al., 2020; Martinkus et al., 2021; Lam et al., 2023; Cao et al., 2023) for capturing pair-wise interactions among agents through message passing. Most recently, neural ordinary differential equations (Neural ODEs) (Chen et al.,"}, {"title": "2 Preliminaries and Related Work", "content": "We represent a dynamical system as a graph G = (V,E), where V denotes the node set of N agents and E denotes the set of edges representing their physical interactions. For simplicity, we assumed G to be static over time. Single-agent dynamical system is a special case where the graph only has one node. In the following, we use the multi-agent setting by default to illustrate our model. We denote X(t) \u2208 RN\u00d7d as the feature matrix at timestamp t for all agents, with d as the feature dimension. Model input consists of trajectories of feature matrices over M historical timestamps X(t\u2212M:\u22121) = {X(t\u2212M),..., X(t\u22121)} and G. The timestamps t\u22121,\u2026, t\u2212M < 0 can have non-uniform intervals and take any continuous values. Our goal is to learn a neural simulator fo() : [X(t\u2212M:\u22121),G] \u2192 Y (to:K), which predicts node dynamics Y (t) in the future on timestamps 0 = to < \u2026 < tk = T sampled within [0, T]. We use yi(t) to denote the targeted dynamic vector of agent i at time t. In some cases when we are only predicting system feature trajectories, Y(\u00b7) = X(.)."}, {"title": "2.1 NeuralODE for Dynamical Systems", "content": "NeuralODEs (Chen et al., 2018; Rubanova et al., 2019) are a family of continuous models that define the evolution of dynamical systems by ordinary differential equations (ODEs). The state evolution can be described as: $\\dot{z_i}(t) := \\frac{dz_i(t)}{dt} = g(z_1(t), z_2(t)\u2026z_n(t))$, where $z_i(t) \\in \\mathbb{R}^d$ denotes the latent state variable for agent i at timestamp t. The ODE function g is parameterized by a neural network such as Multi-Layer Perception (MLP), which is automatically learned from data. GraphODEs (Poli et al., 2019; Huang et al., 2020; Luo et al., 2023b; Wen et al., 2022; Huang et al., 2024) are special cases of NeuralODEs, where g is a Graph Neural Network (GNN) to capture the continuous interaction among agents.\nGraphODEs have been shown to achieve superior performance, especially in long-range predictions and can handle data irregularity issues. They usually follow the encoder-processor-decoder archi-tecture, where an encoder first computes the latent initial states $z_1 (t_0),\\ldots, z_n (t_0)$ for all agents simultaneously based on their historical observations as in Eqn 1.\n$z_1 (t_0), z_2 (t_0), ..., z_n(t_0) = f_{enc}(X(t\u2212M:\u22121),G)$ (1)\nThen the GNN-based ODE predicts the latent trajectories starting from the learned initial states. The latent state $z_i(t)$ can be computed at any desired time using a numerical solver such as Runge-Kuttais (Schober et al., 2019) as:\n$z_i(t) = ODE\\text{-}Solver(g, [z_1 (t_0), ...z_n (t_0)], t) = z_i(t_0) + \\int_{t_0}^t g \\big(z_1(t), z_2(t)...z_n(t)\\big) dt.$ (2)\nFinally, a decoder extracts the predicted dynamics $\\hat{y}_i(t)$ based on the latent states $z_i(t)$ for any timestamp t:\n$\\hat{y}_i(t) = f_{DEC}(z_i(t)).$ (3)"}, {"title": "2.2 Time-Reversal Symmetry (TRS)", "content": "Consider a dynamical system described in the form of $\\frac{dx(t)}{dt} = F(x(t))$, where $x(t) \\in \\Omega$ is the observed states such as positions. The system is said to follow the Time-Reversal Symmetry if there exists a reversing operator $R : \\Omega \\rightarrow \\Omega$ such that (Lamb and Roberts, 1998):\n$\\frac{d(R \\circ x(t))}{dt} = -F(R \\circ x(t)),$ (4)\nwhere $\\circ$ denote the action of functional R on the function x.\nIntuitively, we can assume x(t) is the position of a flying ball and the conventional reversing operator is defined as $R : x \\rightarrow R \\circ x$, $R \\circ x(t) = x(-t)$. This implies when x(t) is a forward trajectory position with initial position x(0), x(-t) is then a position in the time-reversal trajectory, where x(-t) is calculated using the same function F, but with the integration time reversed, i.e. dt \u2192 d(\u2212t). Eqn 4 shows how to create the reverse trajectory of a flying ball: at each position, the velocity (i.e., the derivative of position with respect to time) should be the opposite. In neural networks, we usually model trajectories in the latent space via z (Sanchez-Gonzalez et al., 2020), which can be decoded back to real observation state i.e. positions. Therefore, we apply the reversal operator for z.\nNow we introduce a time evolution operator $\\Phi_\\tau$ such that $\\Phi_\\tau \\circ z(t) = z(t + \\tau)$ for arbitrary t, \u03c4\u2208R. It satisfies $\\Phi_{\\tau_1} \\circ \\Phi_{\\tau_2} = \\Phi_{\\tau_1+\\tau_2}$, where \u25cb denotes composition. The time evolution operator helps us to move forward (when \u03c4 > 0) or backward (when \u03c4 < 0) through time, thus forming a trajectory. Based on (Lamb and Roberts, 1998), in terms of the evolution operator, Eqn 4 implies:\n$R \\circ \\Phi_t = \\Phi_{-t} \\circ R = \\Phi^{-1}_t \\circ R,$ (5)\nwhich means that moving forward t steps and then turning to the opposite direction is equivalent to firstly turning to the opposite direction and then moving backwards t steps. Eqn 5 has been widely used to describe time-reversal symmetry in existing literature (Huh et al., 2020; Valperga et al., 2022). Nevertheless, we propose the following lemma, which is more intuitive to understand and straightforward to guide the design of our time-reversal regularizer.\nLemma 2.1. Eqn 5 is equivalent to $R \\circ \\Phi_t \\circ \\Phi_t \\circ R = I$, where I denotes identity mapping.\nLemma 2.1 means if we move t steps forward, then turn to the opposite direction, and then move forward for t more steps, it shall restore back to the same state. This is illustrated in Figure 2 where the reverse trajectory should be the same as the forward trajectory. It can be understood as rewinding a video to the very beginning. The proof of Lemma 2.1 is in Appendix A.2."}, {"title": "3 Method: TREAT", "content": "We present a novel framework TREAT that achieves high-precision modeling for a wide range of systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a regularization"}, {"title": "3.1 Time-Reversal Symmetry Loss and Training", "content": "Forward Trajectory Prediction and Reconstruction Loss. For multi-agent systems, we utilize the GNN operator described in (Kipf et al., 2018) as our ODE function g(\u00b7), which drives the system to move forward and output the forward trajectories for latent states $z^{fwd}_i(t)$ at each continuous time t\u2208 [0, T] and each agent i.We then employ a Multilayer Perceptron (MLP) as a decoder to predict output trajectories $\\hat{y}^{fwd}_i(t)$ based on the latent states. We summarize the whole procedure as:\n$\\begin{aligned}\n\\frac{dz^{fwd}_i(t)}{dt} &:= g\\big(z^{fwd}_1(t), z^{fwd}_2(t),\\ldots z^{fwd}_N(t)\\big), \\\\\nz^{fwd}_i(t_0) &= f_{ENC}(X(t\u2212M:\u22121), G), \\\\\n\\hat{y}^{fwd}_i(t) &= f_{DEC}(z^{fwd}_i(t)).\n\\end{aligned}$ (6)"}, {"title": "3.2 Theoretical Analysis of Time-Reversal Symmetry Loss", "content": "We next theoretically show that the time-reversal symmetry loss numerically helps to improve prediction accuracy in general, regardless of systems' physical properties. Specifically, we show that it minimizes higher-order Taylor expansion terms during the ODE integration steps.\nTheorem 3.1. Let \u0394t denote the integration step size in an ODE solver and T be the prediction length. The reconstruction loss Lpred defined in Eqn 7 is O(T\u00b3 \u2206t\u00b2). The time-reversal loss Lreverse defined in Eqn 9 is O(T5 \u2206t4).\nWe prove Theorem 3.1 in Appendix A.3. From Theorem 3.1, we can see two nice properties of our proposed time-reversal loss: 1) Regarding the relationship to \u0394t, Lreverse is optimizing a high-order term \u0394t4, which forces the model to predict fine-grained physical properties such as jerk (the derivatives of accelerations). In comparison, the reconstruction loss optimizes \u0394t\u00b2, which mainly guides the model to predict the locations/velocities accurately. Therefore, the combined loss enables our model to be more noise-tolerable; 2) Regarding the relationship to T, Lreverse is more sensitive to total sequence length (T5), thus it provides more regularization for long-context prediction, a key challenge for dynamic modeling."}, {"title": "4 Experiments", "content": "Datasets. We conduct systematic evaluations over five multi-agent systems including three 5-body spring systems (Kipf et al., 2018), a complex chaotic pendulum system and a real-world motion capture dataset (Carnegie Mellon University, 2003); and four single-agent systems including three spring systems (with only one node) and a chaotic strange attractors system (Huh et al., 2020).\nThe settings of spring systems include: 1) conservative, i.e. no interactions with the environments, we call it Simple Spring; 2) non-conservative with frictions, we call it Damped Spring; 3) non-conservative with periodic external forces, we call it Forced Spring. The Pendulum system contains three connected sticks in a 2D plane. It is highly sensitive to initial states, with minor disturbances leading to significantly different trajectories (Shinbrot et al., 1992; Awrejcewicz et al., 2008). The real-world motion capture dataset (Carnegie Mellon University, 2003) describes the walking trajectories of a person, each tracking a single joint. We call it Human Motion. The strange attractor consists of symmetric attractor/repellor force pairs and is chaotic (Sprott, 2015). It is also highly sensitive to the initial states (Koppe et al., 2019). We call it Attractor.\nTowards physical properties, Simple Spring and Pendulum are conservative and reversible; Force Spring and Attractor are reversible but non-conservative; Damped Spring are irreversible and non-conservative. For Human Motion, it does not adhere to specific physical laws since it is a real-world dataset. Details of the datasets and generation pipelines can be found inAppendix C.\nTask Setup. We conduct evaluation by splitting trajectories into two halves: [t1,t\u043c], [tM+1,tk] where timestamps can be irregular. We condition the first half of observations to make predictions for the second half as in (Rubanova et al., 2019). For spring datasets and Pendulum, we generate irregular-sampled trajectories and set the training samples to be 20,000 and testing samples to be 5,000 respectively. For Attractor, We generate 1,000 and 50 trajectories for training and testing respectively following Huh et al. (2020). 10% of training samples are used as validation sets and the maximum trajectory prediction length is 60. Details can be found in Appendix C.\nBaselines. We compare TREAT against three baseline types: 1) pure data-driven approaches including LG-ODE (Huang et al., 2020) and LatentODE (Rubanova et al., 2019), where the first one is a multi-agent approach considering pair-wise interactions, and the second one is a single-agent approach that predicts each trajectory independently; 2) energy-preserving HODEN (Greydanus et al., 2019); and 3) time-reversal TRS-ODEN (Huh et al., 2020).\nThe latter two are single-agent approaches and require initial states as given input. To handle missing initial states in our dataset, we approximate the initial states for the two methods via linear spline interpolation (Endre S\u00fcli, 2003). In addition, we substitute the ODE network in TRS-ODEN with a GNN (Kipf et al., 2018) as TRS-ODENGNN, which serves as a new multi-agent approach for fair comparison. HODEN cannot be easily extended to the multi-agent setting as replacing the ODE function with a GNN can violate energy conservation of the original HODEN. For running LGODE and TREAT on single-agent datasets, we only include self-loop edges in the graph G = (V, E), which makes the ODE function g a simple MLP. Implementation details can be found in Appendix D.2."}, {"title": "4.1 Main Results", "content": "Table 1 shows the prediction performance on both multi-agent systems and single-agent systems measured by mean squared error (MSE). We can see that TREAT consistently surpasses other models, highlighting its generalizability and the efficacy of the proposed TRS loss.\nFor multi-agent systems, approaches that consider interactions among agents (LG-ODE, TRS-ODENGNN, TREAT) consistently outperform single-agent baselines (LatentODE, HODEN, TRS-ODEN), and TREAT achieves the best performance across datasets.\nThe chaotic nature of the Pendulum system and the Attractor system, with their sensitivity to initial states 6, poses extreme challenges for dynamic modeling. This leads to highly unstable predictions for models like HODEN and TRS-ODEN, as they estimate initial states via inaccurate linear spline interpolation (Endre S\u00fcli, 2003). In contrast, LatentODE, LG-ODE, and TREAT employ advanced encoders that infer latent states from observed data and demonstrate superior accuracy. Among them, TREAT achieves the most accurate predictions, further showing its robust generalization capabilities.\nWe observe that misapplied inductive biases can degrade results, which limits the usage of physics-informed methods that are designed for individual physical prior such as HODEN. HODEN only excels on energy-conservative systems, such as Simple Spring compared with LatentODE and TRS-ODEN in the multi-agent setting. Its performance drop dramatically on Force Spring, Damped Spring, and Attractor. Note that HODEN naively forces each agent to be energy-conservative, instead of the whole system. Therefore, it performs poorly than LG-ODE, TREAT in the multi-agent settings.\nFor the Human Motion dataset, characterized by its dynamic ambiguity as it does not adhere to specific physical laws, we cannot directly determine whether it is conservative or time-reversal. For such a system with an unknown nature, TREAT outperforms other purely data-driven methods significantly, showcasing its strong numerical benefits in improving prediction accuracy across diverse system types. This is also shown by its superior performance on Damped Spring, which is irreversible."}, {"title": "4.2 Ablation and Sensitivity Analysis", "content": "Ablation on implementation of Lreverse. We conduct two ablation by changing the implementation of Creverse discussed in Sec. 3.2: 1) TREATLrev=gt-rev, which computes the reversal loss as the L2 distance between ground truth trajectories to model backward trajectories; 2) TREATLrev=rev2, which implements the TRS loss based on Eqn 5 as in TRS-ODEN (Huh et al., 2020). From the last block of Table 1, we can clearly see that our implementation achieves the best performance against the two.\nEvaluation across prediction lengths. We vary the maximum prediction lengths from 20 to 60 and report model performance as shown in Figure 4. As the prediction step increases, TREAT consistently maintains optimal prediction performance, while other baselines exhibit significant error accumulations. The performance gap between TREAT and baselines widens when making long-range predictions, highlighting the superior predictive capability of TREAT.\nEvaluation across different \u03b1. We vary the values of the coefficient \u03b1 defined in Eqn 10, which balances the reconstruction loss and the TRS loss. Figure 5 demonstrates that the optimal \u03b1 values being neither too high nor too low. This is because when \u03b1 is too small, the model tends to neglect the TRS physical bias, resulting in error accumulations. Conversely, when \u03b1 becomes too large, the model can emphasize TRS at the cost of accuracy. Nonetheless, across different \u03b1 values, TREAT consistently surpasses the purely data-driven LG-ODE, showcasing its superiority and flexibility in modeling diverse dynamical systems.\nWe study TREAT's sensitivity towards solver choice and observation ratios in Appendix E.1 and Appendix E.2 respectively."}, {"title": "5 Conclusions", "content": "We propose TREAT, a deep learningframework that achieves high-precision modeling for a wide range of dynamical systems by injecting time-reversal symmetry as an inductive bias. TREAT features a novel regularization term to softly enforce time-reversal symmetry by aligning predicted forward and reverse trajectories from a GraphODE model. Notably, we theoretically prove that the regularization term effectively minimizes higher-order Taylor expansion terms during the ODE integration, which serves as a general numerical benefit widely applicable to various systems (even irreversible systems) regardless of their physical properties. Empirical evaluations on different kinds of datasets illustrate TREAT's superior efficacy in accurately capturing real-world system dynamics."}, {"title": "6 Limitations", "content": "Currently, TREAT only incorporates inductive bias from the temporal aspect, while there are many important properties in the spatial aspect such as translation and rotation equivariance (Satorras et al., 2021; Han et al., 2022b; Xu et al., 2022). Future endeavors that combine biases from both temporal and spatial dimensions could unveil a new frontier in dynamical systems modeling."}, {"title": "A Theoretical Analysis", "content": null}, {"title": "A.1 Implementation of the Time-Reversal Symmetry Loss", "content": null}, {"title": "A.2 Proof of Lemma 1", "content": "Proof. The definition of time-reversal symmetry is given by:\n$R \\circ \\Phi_t = \\Phi_{-t} \\circ R = \\Phi^{-1}_t \\circ R$ (11)\nHere, R is an involution operator, which means $R \\circ R = I$.\nFirst, we apply the time evolution operator $\\Phi_t$ to both sides of Eqn 11:\n$\\Phi_t \\circ R \\circ \\Phi_t = \\Phi_t \\circ \\Phi_{-t} \\circ R$ (12)\nSimplifying, we obtain:\n$\\Phi_t \\circ R = \\Phi_{-t} \\circ R$ (13)\nNext, we apply the involution operator R to both sides of the equation:\n$R \\circ \\Phi_t \\circ R \\circ \\Phi_t = R \\circ R$ (14)\nSince $R \\circ R = I$, we finally arrive at:\n$R \\circ \\Phi_t \\circ R \\circ \\Phi_t = I$ (15)\nwhich means the trajectories can overlap when evolving backward from the final state."}, {"title": "A.3 Proof of Theorem 3.1", "content": "Let \u0394t denote the integration step size in an ODE solver and T be the prediction length. The time stamps of the ODE solver are $\\{t_j\\}_{j=0}^T$, where $t_{j+1} - t_j = \\Delta t$ for $j = 0,\\ldots ,T(T > 1)$. Next suppose during the forward evolution, the updates go through states $z^{fwd}(t_j) = (q^{fwd}(t_j), p^{fwd}(t_j))$ for $j = 0,\\ldots,T$, where $q^{fwd}(t_j)$ is position, $p^{fwd}(t_j)$ is momentum, while during the reverse evolution they go through states $z^{rev}(t_j) = (q^{rev}(t_j), p^{rev}(t_j))$ for $j = 0,\\ldots,T$, in reverse order. The ground truth trajectory is $z^{gt}(t_j) = (q^{gt}(t_j), p^{gt}(t_j))$ for $j = 0,\\ldots,T$.\nFor the sake of brevity in the ensuing proof, we denote $z^{gt}(t_j)$ by $z^{gt}$, $z^{fwd}(t_j)$ by $z^{fwd}$ and $z^{rev}(t_j)$ by $z^{rev}$, and we will use Mathematical Induction to prove the theorem."}, {"title": "A.3.1 Reconstruction Loss (Lpred) Analysis.", "content": "First, we bound the forward loss $\\sum_{j=0}^T || z^{fwd} \u2013 z^{gt}||_2$. Since our method models the momentum and position of the system, we can write the following Taylor expansion of the forward process, where"}, {"title": "A.4 Proof of Lemma 3.2", "content": null}, {"title": "B Example of varying dynamical systems", "content": "We illustrate the energy conservation and time reversal of the three n-body spring systems used in our experiments. We use the Hamiltonian formalism of systems under classical mechanics to describe their dynamics and verify their energy conservation and time-reversibility characteristics.\nThe scalar function that describes a system's motion is called the Hamiltonian, H, and is typically equal to the total energy of the system, that is, the potential energy plus the kinetic energy (North, 2021). It describes the phase space equations of motion by following two first-order ODEs called Hamilton's equations:\n$\\frac{dq}{dt} = \\frac{\\partial H(q, p)}{\\partial p} \\qquad \\frac{dp}{dt} = -\\frac{\\partial H(q, p)}{\\partial q},$ (27)\nwhere q \u2208 Rn, p \u2208 Rn, and H : R2n \u2192 R are positions, momenta, and Hamiltonian of the system.\nUnder this formalism, energy conservative is defined by dH/dt = 0, and the time-reversal symmetry is defined by H(q,p,t) = H(q, -p, -t) (Lamb and Roberts, 1998)."}, {"title": "B.1 Conservative and reversible systems.", "content": "A simple example is the isolated n-body spring system, which can be described by :\n$\\begin{aligned}\n\\frac{dq_i}{dt} &= \\frac{p_i}{m} \\\\\n\\frac{dp_i}{dt} &= \\sum_{j\\in N} -k(q_i - q_j),\n\\end{aligned}$ (28)\nwhere q = (q1, q2, \u2026\u2026\u2026, qn) is a set of positions of each object, p = (P1, P2, \u2026, PN) is a set of momenta of each object, mi is mass of each object, k is spring constant."}, {"title": "B.2 Non-conservative and reversible systems.", "content": "A simple example is a n-body spring system with periodical external force, which can be described by:\n$\\begin{aligned}\n\\frac{dq_i}{dt} &= \\frac{p_i}{m} \\\\\n\\frac{dp_i}{dt} &= \\sum_{j\\in N} -k(q_i - q_j) \u2013 k_1 cos\\omega t,\n\\end{aligned}$ (33)\nThe Hamilton's equations are:\n$\\begin{aligned}\n\\frac{\\partial H(q, p)}{\\partial p_i} &= \\frac{dq_i}{dt} = \\frac{p_i}{m} \\\\\n\\frac{\\partial H(q, p)}{\\partial q_i} &= \\frac{dp_i}{dt} = \\sum_{j\\in N} - k(q_i - q_j) + k_1 cos \\omega t,\n\\end{aligned}$ (34)\nHence, we can obtain the Hamiltonian through the integration of the above equation:\n$H(q, p) = \\sum_{i=1}^N \\frac{p_i^2}{2m_i} \u2013 \\sum_{i=1}^N \\sum_{j\\in N} \\frac{1}{2}k(q_i \u2013 q_j)^2 + \\sum_{i=1}^N q_i * k_1 cos\\omega t, $(35)\nVerify the systems' energy conservation\n$\\frac{dH (q, p)}{dt} = \\frac{d}{dt} ( \\sum_{i=1}^N \\frac{p_i}{2m_i}) + \\frac{d}{dt} (-\\sum_{i=1}^N \\sum_{j\\in N} \\frac{1}{2}k(q_i \u2013 q_j)^2 ) + \\frac{d}{dt} (\\sum_{i=1}^N q_i * k_1 cos \\omega t) $\n$= 0 + (\\sum_i -\\omega q_i k_1 sin \\omega t) \\neq 0$ (36)"}, {"title": "B.3 Non-conservative and irreversible systems.", "content": "A simple example is an n-body spring system with frictions proportional to its velocity,\u03b3 is the coefficient of friction, which can be described by:\n$\\begin{aligned}\n\\frac{dq_i}{dt} &= \\frac{p_i}{m} \\\\\n\\frac{dp_i}{dt} &= -k q_i - \\gamma \\frac{p_i}{m}\n\\end{aligned}$ (38)\nThe Hamilton's equations are:\n$\\begin{aligned}\n\\frac{\\partial H(q, p)}{\\partial p_i} &= \\frac{dq_i}{dt} = \\frac{p_i}{m} \\\\\n\\frac{\\partial H(q, p)}{\\partial q_i} &= \\frac{dp_i}{dt} = \\sum_{j\\in N} -k(q_i - q_j) + \\gamma \\frac{p_i}{m}\n\\end{aligned}$ (39)\nHence, we can obtain the Hamiltonian through the integration of the above equation:\n$H(q, p) = \\sum_{i=1}^N \\frac{p_i^2}{2m_i} + \\sum_{j\\in N} -\\frac{1}{2} k(q_i - q_j)^2 + \\sum_{i=1}^N \\gamma \\int_0^t \\frac{p_i^2}{m} dt, $(40)\nVerify the systems' energy conservation\n$\\frac{dH (q, p)}{dt} = \\frac{d}{dt} (\\sum_{i=1}^N \\frac{p_i^2}{2m_i}) + \\frac{d}{dt} (-\\sum_{i=1}^N \\sum_{j\\in N} \\frac{1}{2} k(q_i - q_j)^2 ) + \\frac{d}{dt} (\\sum_{i=1}^N \\gamma \\int_0^t \\frac{p_i^2}{m} dt)$\n$=\\gamma \\frac{1}{m}\\sum_{i=1}^N (\\int_0^t \\frac{p_i^2}{m} dt) \\neq 0 $(41)"}, {"title": "C Dataset", "content": "In our experiments, all datasets are synthesized from ground-truth physical law via sumulation. We generate five simulated datasets: three n-body spring systems under damping, periodic, or no external force, one chaotic tripe pendulum dataset with three sequentially connected stiff sticks that form and a chaotic strange attractor. We name the first three as Sipmle Spring, Forced Spring, and Damped Spring respectively. For multi-agent systems, all n-body spring systems contain 5 interacting balls, with varying connectivities. Each Pendulum system contains 3 connected stiff sticks. For single-agent systems, all spring systems contain only one ball. For the chaotic single Attractor, we follow the setting of (Huh et al., 2020).\nFor the n-body spring system, we randomly sample whether a pair of objects are connected, and model their interaction via forces defined by Hooke's law. In the Damped spring, the objects have an additional friction force that is opposite to their moving direction and whose magnitude is proportional to their speed. In the Forced spring, all objects have the same external force that changes direction periodically. We show in Figure 1(a), the energy variation in both of the Damped spring and Forced spring is significant. For the chaotic triple Pendulum, the equations governing the motion are inherently nonlinear. Although this system is deterministic, it is also highly sensitive to the initial condition and numerical errors (Shinbrot et al., 1992; Awrejcewicz et al., 2008; Stachowiak and Okada, 2006). This property is often referred to as the \"butterfly effect\", as depicted in Figure 9. Unlike for n-body spring systems, where the forces and equations of motion can be easily articulated, for the Pendulum, the explicit forces cannot be directly defined, and the motion of objects can only be described through Lagrangian formulations (North, 2021), making the modeling highly complex and raising challenges for accurate learning. We simulate the trajectories by using Euler's method for"}, {"title": "C.4 Human Motion", "content": "For the real-world motion capture dataset(Carnegie Mellon University, 2003), we focus on the walking sequences of subject 35. Each sample in this dataset is represented by 31 trajectories, each corresponding to the movement of a single joint. For each joint, we first randomly sample the number of observations from a uniform distribution U(30, 42) and then sample uniformly from the first 50 frames for training and validation trajectories. For testing, we additionally sampled 40 observations from frames [51, 99].We split different walking sequences into training (15 trials) and test sets (7 trials). For each walking sequence, we further split it into several non-overlapping small sequences with maximum length 50 for training, and maximum length 100 for testing. In this way, we generate total 120 training samples and 27 testing samples. We normalize all features (position/velocity) to maximum absolute value of 1 across training and testing datasets."}, {"title": "D Model Details", "content": "In the following we introduce in details how we implement our model and each baseline."}, {"title": "D.1 Initial State Encoder", "content": "For multi-agent systems", "2020)": "n$\\begin{aligned}\nh_j(t) &= h_j^0(t) + \\sigma\\bigg(\\sum_{i(t') \\in N_j(t)} d^{(t') \\rightarrow (t)} \\times W_h^{\\nu}h_i(t') \\bigg) \\\\\nd^{(t') \\rightarrow (t)} &= (W_h^"}]}