{"title": "Monte Carlo Tree Search based Space Transfer\nfor Black-box Optimization", "authors": ["Shukuan Wang", "Ke Xue", "Lei Song", "Xiaobin Huang", "Chao Qian"], "abstract": "Bayesian optimization (BO) is a popular method for computationally expensive\nblack-box optimization. However, traditional BO methods need to solve new prob-\nlems from scratch, leading to slow convergence. Recent studies try to extend BO to\na transfer learning setup to speed up the optimization, where search space transfer\nis one of the most promising approaches and has shown impressive performance\non many tasks. However, existing search space transfer methods either lack an\nadaptive mechanism or are not flexible enough, making it difficult to efficiently\nidentify promising search space during the optimization process. In this paper,\nwe propose a search space transfer learning method based on Monte Carlo tree\nsearch (MCTS), called MCTS-transfer, to iteratively divide, select, and optimize in\na learned subspace. MCTS-transfer can not only provide a well-performing search\nspace for warm-start but also adaptively identify and leverage the information of\nsimilar source tasks to reconstruct the search space during the optimization process.\nExperiments on synthetic functions, real-world problems, Design-Bench and hyper-\nparameter optimization show that MCTS-transfer can demonstrate superior perfor-\nmance compared to other search space transfer methods under different settings.\nOur code is available at https://github.com/lamda-bbo/mcts-transfer.", "sections": [{"title": "1 Introduction", "content": "In many real-world tasks such as neural architecture search [56; 41; 42], hyper-parameter optimiza-\ntion [55; 4], and integrated circuit design [19; 30], we often need to solve black-box optimization\n(BBO) problems, where the objective function has no analytical form and can only be evaluated\nby different inputs, regarded as a \"black-box\" function. BBO problems are often accompanied by\nexpensive computational costs of the evaluations, requiring a BBO algorithm to find a good solution\nwith a small number of objective function evaluations.\nBayesian optimization (BO) [29; 8] is a widely used sample-efficient method for such problems.\nAt each iteration, BO fits a surrogate model, typically Gaussian process (GP) [26], to approximate\nthe objective function, and maximizes an acquisition function to determine the next query point.\nUnder the limited evaluation budget, traditional BO methods only have a few observations, which\nare, however, insufficient for constructing a precise surrogate model, leading to slow convergence.\nThus, traditional BO methods struggle to effectively solve expensive BBO problems, preventing their\nbroader applications.\nTo tackle this issue, recent research tries to apply transfer learning methods for BO [2]. Transfer\nBO methods operate under the assumption that similar tasks may share common characteristics, and"}, {"title": "2 Background", "content": "the knowledge acquired from similar source tasks can be helpful for optimizing the current target\ntask. They typically gather offline datasets from the source tasks and utilize them to expedite the\ntarget task. These methods can be categorized based on the learned components of BO, such as the\nacquisition function [51; 40], initialization point [45; 47], or search space [49; 23; 17].\nAmong these, learning the search space is a promising research area due to its effectiveness and\northogonal relationship with optimization methods. By learning and partitioning the search space, we\ncan more effectively utilize potential subspaces and significantly accelerate the algorithm's search\nfor optimal solutions. While the methods for partitioning the search space have demonstrated great\npotential [22; 14; 23; 17], they still have several limitations, particularly in terms of flexibly adjusting\nthe search space for the current target task. In many scenarios, we are uncertain about which tasks\nare truly \"similar\" to the target task before optimizing it. Our comprehension about this similarity\ncan only deepen gradually as we progress in optimizing the target task. Therefore, we hope that a\nsearch space transfer algorithm can automatically identify the most relevant source tasks during the\noptimization process, and give them more consideration when constructing the subspaces. However,\nexisting methods have limited flexibility in adjusting in this manner.\nIn this paper, we propose a search space transfer learning method based on Monte Carlo tree search\n(MCTS), called MCTS-transfer, to iteratively divide, select, and optimize in a learned subspace. Each\nnode of MCTS-transfer represents a subspace, whose potential is calculated as a weighted sum of the\nsource and target sample values, assessing the node's utilization value. To better identify and leverage\nthe information from source tasks, we assign different weights to different source tasks based on\ntheir similarity to the target task, which are adjusted dynamically during the optimization process.\nThese weights are then used to calculate the potential of the node. Our proposed MCTS-transfer\noffers several notable advantages. First, it can provide a better initial search space for a warm-start of\nthe optimization of the target task. Second, it can provide multiple promising compact subspaces by\nMCTS, to improve optimization efficiency. The upper confidence bound (UCB)-based node selection\nof MCTS can also automatically balance the trade-off between exploration and exploitation. Third, it\ncan automatically identify source tasks similar to the current target task based on new observations,\nand re-construct the search space to make it more consistent with the current target task.\nTo evaluate the effectiveness of the proposed method, we compare MCTS-transfer with various\nsearch space transfer methods and conduct experiments on multiple BBO tasks, including synthetic\nfunctions, real-world problems, Design-Bench and hyper-parameter optimization problems. In\ndifferent scenarios, such as varying similarity between the source tasks and target task, MCTS-\ntransfer demonstrates superior performance. We also analyze the effectiveness of the adaptive weight\nadjustment, showing that MCTS-transfer can identify source tasks that are similar to the target\ntask and assign them higher weights accordingly. Note that MCTS-transfer can be combined with\nany BBO algorithm. We only implemented it with the basic BO algorithm (i.e., using GP as the\nsurrogate model and expected improvement (EI) as the acquisition function) in the experiments, and\nthe performance can be further enhanced by advanced techniques."}, {"title": "2.1 Bayesian Optimization", "content": "We consider the problem $\\max_{x \\in \\mathcal{X}} f(x)$, where $f$ is a black-box function and $\\mathcal{X} \\subset \\mathbb{R}^D$ is the\ndomain. The basic framework of BO contains two critical components: a surrogate model and\nan acquisition function. GP is the most popular surrogate model. Given the sampled data points\n$\\{(x_i, y_i)\\}_{i=1}^t$, where $y_i = f(x_i) + \\epsilon_i$ and $\\epsilon_i \\sim \\mathcal{N}(0, \\eta^2)$ is the observation noise, GP at iteration $t$\nseeks to infer $f \\sim \\mathcal{GP}(\\mu(\\cdot), k(\\cdot, \\cdot) + \\eta^2I)$, specified by the mean $\\mu(\\cdot)$ and covariance kernel $k(\\cdot,\\cdot)$,\nwhere $I$ is the identity matrix of size $D$. After that, an acquisition function, e.g., probability of\nimprovement (PI) [15], EI [12] or UCB [36], is optimized to determine the next query point $x_t$,\nbalancing exploration and exploitation."}, {"title": "2.2 Transfer Bayesian Optimization", "content": "When solving new BBO problems, traditional BO methods need to conduct optimization from scratch,\nleading to slow convergence. Transfer learning reuses knowledge from source tasks to boost the\nperformance of current tasks, and thus can be naturally applied to address this issue, i.e., it can utilize"}, {"title": "2.3 Search Space Transfer", "content": "Compared to the aforementioned methods, search space transfer has many advantages, especially\nin not limiting the transfer to a specific algorithm component (e.g., acquisition function in BO), but\nconsidering the search space that can be used by all BBO algorithms as the transfer object. That is,\nthe learned search space is orthogonal to the optimization process and can be integrated with any\nadvanced optimizer. A well-learned search space can greatly improve optimization efficiency and\nguide the optimization process to a good result.\nThe concept of search space transfer was first proposed by Wistuba et al. [49], which defined a region\nby a center point and a diameter, and pruned away regions deemed less promising. Instead of pruning\nspace, Perrone and Shen [23] considered designing a promising search space for the target task. This\napproach extracts an optimal solution $x^\u2217$ from each task, and employs a simple geometric form (e.g.,\na box or ellipsoid) to define the smallest subspace encompassing all optimal solutions $x^\u2217$. However,\nit ignores the correlation between tasks, and the space constructed with regular geometry may be too\nloose for the target task. To address this issue, Li et al. [17] selected the most similar source tasks to\nthe target task in a certain proportion, and used a binary classification method to learn good spaces\nand bad spaces among these tasks. A voting mechanism is then employed to aggregate information\nfrom all selected tasks to determine a newly generated search space for the target task.\nThese current space transfer methods, however, lack mechanisms to adjust the search space when it\nis not well-suited for the target task. Furthermore, when source tasks differ significantly from the\ntarget task, they tend to devolve into full-space search. The binary nature of evaluating the search\nspace as simply \"good\" or \"bad\" is another limitation. Our proposed method will overcome these\ndrawbacks by adapting the search space dynamically according to the similarity between source tasks\nand the target task, applying MCTS to find a proper subspace, and using a more nuanced and precise\nnumerical evaluation for the goodness of a search subspace."}, {"title": "2.4 Monte Carlo Tree Search", "content": "MCTS [5] is a search algorithm combining random sampling with a tree search structure, which\nhas been widely applied in the filed of game-playing and decision-making [32; 34]. A tree node\nrepresents a particular state in the search space, e.g., stone positions on the board in a GO game. Each\ntree node has an UCB [1] value during the search procedure, to balance exploration and exploitation."}, {"title": "3 MCTS-transfer", "content": "In this section, we propose a search space transfer learning method based on MCTS, called MCTS-\ntransfer, which can be divided into two major stages: pre-learning stage and optimizing stage. The\nmain idea is to apply MCTS to divide the entire space based on source task data in the pre-learning\nstage, and adaptively adjust the partition based on newly generated target task data during the\noptimition process. MCTS-transfer iteratively chooses one partitioned subspace for search and\nadjusts the partition after sampling each new data point. Assume that there are $K$ source tasks\n$\\left\\{f_i\\right\\}_{i=1}^K$ and a target task $T$ that we are going to optimize $\\max_{x\\in \\Omega} f_T(x)$, where $\\Omega$ is the entire\nsearch space. For the $i$-th source task, we have offline dataset $D_i = \\{(X_{i,j}, Y_{i,j})\\}_{j=1}^{|D_i|}$, where $Y_{i,j}$ is\nthe observed objective value of $x_{i,j}$, and $|D_i|$ is the number of data points. Let tree $\\mathcal{T}$ denotes the\nsearch space division process by MCTS.\nIn the pre-learning stage, $\\mathcal{T}$ initially has only one root node, representing the entire search space\n$\\Omega$. The expansion of a tree node $m$ corresponds to the division of the search space $\\Omega_m$ that the\nnode $m$ represents. The node expansion follows the rules below. With a set of source task samples\n$\\left\\{(x_i, Y_i)\\right\\}_{i=1}^{|D_i|}$ in the space $\\Omega_m$, we use k-means clustering to divide them into two groups. The\ncluster with a higher average objective value is regarded as the \"good\" cluster, while the other is the\n\"bad\" one. A binary classifier then establishes a decision boundary between the two clusters. In our\napproach, the space associated with the good cluster becomes the left node, while the space of the\nbad cluster becomes the right node. We recursively apply this partition process within each node,\nas shown in Figure 1. The depth of the resulting Monte Carlo tree $\\mathcal{T}$ is determined by a threshold\nhyperparameter $\\theta$: a node is divided if it contains more than $\\theta$ samples and the contained samples can\nbe clustered into two clusters. The tree $\\mathcal{T}$ generated at this stage can give a suitable space partition\nin advance based on source task data, which serves as a warm-start for the following optimization\nprocess. Details of pre-learning will be introduced in Section 3.1.\nIn the optimization process, we follow the four key steps of MCTS: selection, expansion, simulation,\nand back-propagation. At each iteration, we select a target node $m$ by tracing the nodes' UCB values.\nThat is, starting from the root, we recursively choose the child node with higher UCB value, until a\nleaf node $m$ is reached, as Figure 1 displayed. The space $\\Omega_m$ represented by $m$ is then considered as\na promising search space, where a BO optimizer is used to optimize. The BO optimizer can build a\nsurrogate model using samples in either $\\Omega_m$ or $\\Omega$. The newly sampled point will be evaluated and\nused for expanding the node $m$ after updating the clustering in the node. It will further be utilized in\nback-propagation, where the number of visits and the potential value of each node will be updated.\nThe potential value of a node is calculated by a weighted sum of objective values of the source and\ntarget task samples. Note that the tree structure will be reconstructed if there exists a node that the"}, {"title": "3.1 Search Space Pre-Learning", "content": "Compared to standard BBO algorithms, which sample randomly across the entire space at the\nbeginning, our method leverages information from source tasks to concentrate sampling within a\ngenerally \"good\" space, providing a warm-start initialization.\nIn the pre-learning stage, tree $\\mathcal{T}$ initially has only one node, i.e., ROOT node. All source task samples\nare collected in this node, recursively clustered and classified, leading to node expansion. When none\nof the leaf nodes can be further bifurcated, i.e., contains more than $\\theta$ samples and can be clustered\ninto two clusters, $\\mathcal{T}$ is finally formed. In this process, we keep the rule that the left node is \u201cbetter\u201d\n(i.e., has a larger potential value) than the right node, so that we can easily identify that the leftmost\nleaf is the best and the rightmost leaf is the worst. The potential value of a node $m$ in the pre-learning\nstage is estimated by the average $Y_{i,j}$ of all source task points within it, i.e.,\n$p_m = \\frac{\\sum_{i<K} \\sum_{(Y_{i,j})\\in D_i:\\Omega_i \\cap \\Omega_m} Y_{i,j}}{\\sum_{i<K} |D_i \\cap \\Omega_m|}$"}, {"title": "3.2 Node Potential Update", "content": "This tree embed historically good regions into specific tree nodes. Intuitively, the spaces represented\nby left leaves have higher potential of being good spaces than right ones.\nThe first iteration of MCTS-transfer will utilize the resulting tree $\\mathcal{T}$ generated in the pre-learning\nstage. Starting from the root node, it recursively selects nodes with higher UCB values until reaching\na leaf node. The UCB value is calculated as Eq. (1), where $u_m$ is replaced by the potential value $p_m$ in\nEq. (2). In MCTS-transfer, $n_m$ and $n_p$ represent the number of samples, including those from source\nand target tasks, in the subspace $\\Omega_m$ and the parent subspace of $\\Omega_m$, respectively. Consequently, the\nalgorithm preferentially samples from those regions that have shown to yield favorable outcomes in\nthe source tasks.\nIn each iteration of MCTS-transfer, after evaluating a new sample, we use it to update the similarity\nbetween the source tasks and the target task, and then update each node potential. Note that the\npotential value $p_m$ used in the pre-learning stage only contains information from source tasks. But in"}, {"title": "3.3 Tree Structure Reconstruction", "content": "each node. We also consider other ways of calculating distances and weights, which are introduced\nand empirically compared in Appendix C.\nWhen constructing the search tree, the left child of a node is always better than the right child, i.e.,\nthe potential of the left child always exceeds that of the right child. However, after sampling a new\ndata point and updating the potential of each node in each iteration of MCTS-transfer, this property\nmay be violated. If this happens, it indicates that the current space partition is not ideal and needs\nadjustment. Specifically, we backtrack from the problematic leaf nodes to the highest-level ancestor\nnode that upholds the desired property, and then proceed to reconstruct the subtree from that ancestor\nnode. The subtree reconstruction process is consistent with the process of node expansion.\nThe detailed process is presented in Algorithm 2 in Appendix A. We apply breadth-first search to\ntraverse all tree nodes and use a queue $Q$ to manage the sequence of nodes. In addition, we set a\nqueue $N$ to store the nodes that need to be reconstructed. If the potential of the right child of a node\nis better than that of the left child (line 6), the subtree of this node is deleted (line 7), and it should be\nreconstructed and is added into the queue $N$ (line 8). Otherwise, the two child nodes will enter into\nthe queue $Q$ (lines 10-11) and will be examined later. After traversing the tree $\\mathcal{T}$, we reconstruct the\nsubtrees of nodes in $N$ (lines 15\u201319). If a node in $N$ is splitable, i.e., the contained samples in the\nnode exceeds $\\theta$ and can be clustered and divided by a binary classifier, it will be further expanded.\nThus, Treeify can fine-tune the tree structure and reserve some history information; meanwhile, it can\nadaptively be more suitable to the target task."}, {"title": "3.4 Details of MCTS-transfer", "content": ""}]}