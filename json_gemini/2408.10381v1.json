{"title": "Efficient Reinforcement Learning in Probabilistic Reward Machines", "authors": ["Xiaofeng Lin", "Xuezhou Zhang"], "abstract": "In this paper, we study reinforcement learning in Markov Decision Processes with Probabilistic Reward Machines (PRMs), a form of non-Markovian reward commonly found in robotics tasks. We design an algorithm for PRMs that achieves a regret bound of \\(\u00d5 (\\sqrt{HOAT}+H^2O^2A^{3/2}+H \\sqrt{T})\\), where H is the time horizon, O is the number of observations, A is the number of actions, and T is the number of time-steps. This result improves over the best-known bound, \\(O(H\\sqrt{OAT})\\) of Bourel et al. [2023] for MDPs with Deterministic Reward Machines (DRMs), a special case of PRMs. When \\(T > H^3O^3A^2\\) and OA > H, our regret bound leads to a regret of \\(\u00d5(\\sqrt{HOAT})\\), which matches the established lower bound of \\(\\Omega(\\sqrt{HOAT})\\) for MDPs with DRMs up to a logarithmic factor. To the best of our knowledge, this is the first efficient algorithm for PRMs. Additionally, we present a new simulation lemma for non-Markovian rewards, which enables reward-free exploration for any non-Markovian reward given access to an approximate planner. Complementing our theoretical findings, we show through extensive experiment evaluations that our algorithm indeed outperforms prior methods in various PRM environments.", "sections": [{"title": "Introduction", "content": "Reinforcement learning traditionally focuses on the setting where the reward function is Markovian, meaning that it depends solely on the current state and action, and independent of history. However, in many real-world scenarios, the reward is a function of the history of states and actions. For example, consider a robot tasked with patrolling various locations in an industrial park. The performance of robot is measured by how thorough it regularly covers different zones in the park, which cannot easily be represented as a function of its current state and action, but rather would depend on its whole trajectory during the patrol.\nOne emerging tool to model such performance metrics is called the Reward Machine (RM)[Icarte et al., 2018, 2022], which is a Deterministic Finite-State Automaton (DFA) that can compress the sequence of past events into one single state. Combined with the current observation, the state of RM can fully specify the reward function. Hence, for an MDP with RM, we can obtain an equivalent cross-product MDP by leveraging the information of RM(see Lemma 1) and applying off-the-shelf RL algorithms e.g., Q-learning of Sutton and Barto [2018] to learn an optimal policy. However, as we shall see later, this naive approach will incur a large regret.\nOne limitation of the classic RM framework is that the transition between the state of RM is restricted to be deterministic, whereas stochastic transitions are much more common in practice, especially with uncertainty in the environment. For instance, suppose a robot working in a warehouse is tasked with managing a warehouse by performing simple tasks of fetching and delivering items (as shown in Figure 1). The robot starts at a charging station, navigates to the item pickup location, collects the item, and then proceeds to the delivery location to deliver the item and receives a reward. Based"}, {"title": "Related Work", "content": "Reward Machines Icarte et al. [2018] introduced the concept of Deterministic Reward Machines (DRMs), a type of finite state machine that specifies reward functions while exposing the structure of these functions to the learner. They also proposed an algorithm called QRM, which outperforms standard Q-learning and hierarchical RL algorithms in environments where the reward can be specified by a DRM. Simultaneously, there has been extensive research on RL with temporal specifications expressed in Linear Temporal Logic (LTL) [Sadigh et al., 2014, Wen et al., 2015, Li et al., 2017], which can be directly translated into DRMs. Of particular interest to us is the recent work of Bourel et al. [2023], which studies online RL with DRMs in the context of infinite horizon average reward MDPs. They establish a regret lower-bound of \\(\\Omega(\\sqrt{HOAT})\\) in the episodic setting. They also propose two algorithms differing in the confidence interval design, that achieve a regret bound of order \\(\u00d5(H\\sqrt{OAT})\\) and \\(\u00d5(HO\\sqrt{AT})\\), respectively. We improve upon this work with an algorithm that outperforms theirs both empirically and theoretically.\nEmpirically, RL with LTL or DRMs has been successfully applied to complex robotics tasks, such as manipulation [Camacho et al., 2021] and locomotion [DeFazio et al., 2024]. DRMs have also been employed in multi-agent learning scenarios [Neary et al., 2020, Hu et al., 2024, Zheng and Yu, 2024]. However, all of these works have focused exclusively on Deterministic RMs. Dohmen et al."}, {"title": "Reward-free exploration", "content": "Reward-free exploration [Jin et al., 2020] studies the problem where an agent needs to collect data by interacting with the environment during the exploration stage, preparing for learning the optimal policy for an unknown reward function during the offline planning stage. The number of potential rewards can be arbitrarily large or even infinite. Jin et al. [2020] proposed an algorithm that can return an e-optimal policies for an arbitrary Markovian reward function, after at most collecting \\(O (\\frac{H^3SA}{\\epsilon^2})\\) samples in the exploration stage. M\u00e9nard et al. [2021] further improved the sample complexity to \\(\u014c (\\frac{H^3SA}{\\epsilon^2})\\). Later work extends this problem to MDPs with more general structures, such as linear MDPs[Wang et al., 2020, Zhang et al., 2021a, Wagenmaker et al., 2022] and kernel MDPs[Qiu et al., 2021], and other settings such as multitask RLZhang et al. [2020] and multi-agent RL[Bai and Jin, 2020, Liu et al., 2021]. However, all of the above works restrict their attention on Markovian rewards. This is because their analysis crucially relies on the well-known simulation lemma that quantifies the value difference of a policy in two different MDPs. This classic version of simulation lemma only holds for Markovian reward. We provide the first extension of reward-free RL to the non-Markovian reward setting via a new simulation lemma that holds for arbitrary non-Markovian reward (Lemma 2)."}, {"title": "Problem Formulation", "content": "We start with a few definitions.\nLabeled MDPs with Probabilistic Reward Machines A labeled MDP[Xu et al., 2022] with PRM[Dohmen et al., 2022] is defined as a tuple \\(M = (O, A, p,R,P, L, H)\\). O represents a finite set of observations with cardinality O, and A represents a finite set of actions available at each observation with cardinality A. The transition function \\(p : O \\times A \\rightarrow \\Delta_o\\) dictates the probability \\(p(o'|o, a)\\) of transitioning to observation o' when action a is taken in observation o. The set P consists of atomic propositions, and the labeling function \\(L : O \\times A \\times O \\rightarrow 2^P\\) assigns a subset of P to each transition (o, a, o'). These labels represent high-level events associated with transitions that can be detected from the observation space. The component R is a Probabilistic Reward Machine (PRM), which generates history-dependent reward functions. A PRM is defined as a tuple \\(R = (Q, 2^P, \\tau, \\nu)\\), where Q is a finite set of states with cardinality Q. The probabilistic transition function \\(\\tau : Q \\times 2^P \\rightarrow \\Delta_\\Omega\\) determines the probability \\(\\tau(q'|q, o)\\) of transitioning to state q' given that the event o occurs in state q. The function \\(\\nu : Q \\times 2^P \\times Q \\rightarrow \\Delta[0,1]\\) maps each transition to a reward. The horizon H defines the length of each episode. Note that MDP with PRM is a special class of Non-Markovian Reward Decision Processes (NMRDPs)[Bacchus et al., 1996], which is identical to a MDP except that the reward depends on the history of state and action. For a NMRDP, we define \\(F(\\eta)\\) as the expected reward collected by trajectory \\(\\eta \\stackrel{\\text{def}}{=} \\{(o_t, a_t)\\}_{t=1}^n\\) and \\(G \\stackrel{\\text{def}}{=} \\max_\\eta F(\\eta)\\).\nA special class of PRMs is Deterministic Reward Machines (DRMs) [Icarte et al., 2018, 2022], where the transition function \\(\\tau\\) is deterministic. In a DRM, given a state q and an event \\(\\sigma\\), the next state q' is uniquely determined, and the rewards are generated deterministically based on these transitions.\nThe agent interacts with the MDP with PRM M as follows: At each time step t, the agent is in obser- vation \\(o_t \\in O\\) and selects an action \\(a_t \\in A\\) based on the history \\(h_t = (o_1, a_1, ..., o_{t-1}, a_{t-1}, o_t)\\). After executing the action \\(a_t\\) in observation \\(o_t\\), the environment transitions to the next observation \\(o_{t+1} \\sim p(\\cdot|o_t, a_t)\\) and assigns a label \\(o_t = L(o_t, a_t, o_{t+1})\\). Simultaneously, the PRM, which is in state \\(q_t\\), transitions to state \\(q_{t+1} \\sim \\tau(\\cdot|q_t, o_t)\\) and outputs a reward \\(r_t = \\nu(q_t, \\varsigma_t, q_{t+1})\\). The agent then receives this reward, and both the environment and the PRM proceed to their next observation \\(o_{t+1}\\) and state \\(q_{t+1}\\), respectively.\nWe define the joint state space as the cross-product of Q and O, i.e. \\(S = Q \\times O\\). The transition function \\(P : S \\times A \\rightarrow A_S\\) dictates the probability \\(p(s'|s, a)\\) of transitioning to state s' when action a is taken in s. The policy is defined as a mapping \\(\\pi : S \\times [H] \\rightarrow A\\). For every \\(\\pi\\), the occupancy"}, {"title": "Learning Algorithms and Results", "content": "In this section, we present our RL algorithm for PRMs, UCBVI-PRM.UCBVI-PRM follows the algorith- mic skeleton of a classic model-based RL algorithm [Azar et al., 2017], while incorporating designs that leverage the structure of PRMs. Our key contribution is a regret bound of \\(\u00d5(\\sqrt{HOAT})\\) when T is large enough and OA > H. The regret bound matches the established lower bound \\(\\Omega(\\sqrt{HOAT})\\) up to a logarithmic factor for MDP with DRM, and is notably independent of the joint state space size.\nIntuitively, UCBVI-PRM (Algorithm 1) proceeds in 3 stages: (i) From lines 1 to 7, the algorithm first constructs an empirical transition matrix based on the data collected thus far; (ii) Using this empirical transition matrix, the algorithm then performs value iteration from lines 8 to 23 to update the value function. Notably, between lines 8 and 19, the algorithm computes the new action-value function using the updated empirical transition matrix (line 12) and the exploration bonus (line 13); (iii) Finally, from lines 24 to 28, the agent selects actions based on the updated action-value function and collects new data, which is then incorporated into the dataset.\nThe main technical novelty lies in how we utilize the PRM structure. Denote \\(W_h : Q \\times O \\times A \\times O \\rightarrow R\\) a function that measures the expected return when being in state (q, o), executing action a at time step h - 1 and observing o' at time step h. W is defined as follows:\n\n\\(W_h(q, o, a, o') = \\sum_{q' \\in Q} \\tau(q'|q, L(o, a, o'))V_h(q', o')\\)"}, {"title": "Conclusion", "content": "We study sample-efficient reinforcement learning in episodic Markov decision processes with proba- bilistic reward machines. We introduce an algorithm tailored for PRMs that matches the established lower bound when \\(T > H\u00b3O\u00b3A\u00b2\\) and OA > H. We also present a lemma that characterizes the difference in policy evaluations between two MDPs with non-Markovian rewards. Building upon the new lemma, we establish the reward-free learning result for non-Markovian reward. Finally, we validate our algorithms through a series of experiments. Interesting future direction includes designing effective and efficient algorithms for the multi-agent setting, and exploring connections with reward structures such as submodular rewards."}, {"title": "RL with Non-Markoivian Rewards", "content": "In this section, we introduce an explore-then-commit style algorithm for MDPs with generic non- Markovian rewards: in the exploration stage, the agent collects trajectories from the environment without the information of rewards; in the planning stage, the agent computes a near-optimal policy given the data gathered in the exploration stage, assuming access to an approximate planner. We give an efficient algorithm that conducts \\(\u00d5(\\frac{O^5A^3H^2G^2}{\\epsilon^2})\\) episodes of exploration and returns an (\\(e + \\alpha\\))- optimal policy for any general non-Markovian rewards, given an \\(\\alpha\\)-approximate planner, formally stated below.\nDefinition 1. A planner is \\(\\alpha\\)-approximate if given any NMRDP \\(M = (O, A, p,R, H)\\), the planner returns a policy \\(\\pi\\) that satisfies\n\n\\(J(\\pi) \\geq J(\\pi^*) - \\alpha\\)\n\nwhere \\(J(\\pi)\\) is the expected return of executing policy \\(\\pi\\) in M and \\(\\pi^*\\) is the optimal policy in M.\nTheorem 2. The exists an absolute constant c > 0, such that, for any \\(\\rho \\in (0, 1)\\), with probability at least 1 - \\(p\\), given the information collected by algorithm 3, algorithm 4 can output (\\(e + \\alpha\\))-optimal policies for any non-Markovian rewards assuming access to \\(\\alpha\\)-approximate planner. The number of episodes in algorithm 3 is bounded by\n\n\\(c \\cdot \\frac{[O^5A^3H^2G^2\u03b9']}{\\epsilon^2} + \\frac{[O^4 AH^4G^{1/3}\u03b9']}{\\epsilon} \\)\n\nwhere \\(\u03b9' \\stackrel{\\text{def}}{=} ln(\\frac{OAHG}{\\rho})\\).\nThis result is made possible by a new simulation lemma that can be applied to generic non-Markovian rewards and non-Markovian policies.\nLemma 2. For any two NMRDPs \\(M = (O, A, p, R, H)\\) and \\(\\tilde{M} = (O, A,\\tilde{p},R, H)\\), for any policy \\(\\pi\\)\n\n\\(|J(\\pi) - \\tilde{J}(\\pi)| \\leq \\sum_{m=1}^H \\sum_{o_m,a_m, o_{m+1}} \\epsilon(o_{m+1}|o_m, a_m)\\mu^\\pi (o_m, a_m) G\\)\n\nwhere\n\n\\(\\epsilon(o_{m+1}|o_m, a_m) = |P(o_{m+1}|o_m, a_m) - \\tilde{p}(o_{m+1}|o_m, a_m)|\\)"}, {"title": "Exploration stage", "content": "It turns out that a procedure (Algorithm 3) similar to the Markovian reward case suffices for our purpose [Jin et al., 2020]. Intuitively, algorithm 3 perform two steps. In the first step, from lines 2 to 7, the algorithm constructs a set of exploratory policies each designed to visit an observation state \\(o \\in O\\) as often as possible. To accomplish this, for each observation o, the algorithm creates a reward function that is 0 everywhere except at observation o, where it is set to 1 (line 3). The algorithm then employs a no-regret RL algorithm (e.g. EULER of Zanette and Brunskill [2019]) to find a policy that maximizes this reward, which is equivalent to maximizing the probability of visiting o. In the second stage, from lines 8 to 12, the algorithm collects new data by sampling and executing policies from this exploratory policy set. We prove that, with this framework, the collected data can be used to learn a transition kernel that is sufficiently close to the true transition characterized by the divergence in Lemma 2. Towards this, we introduce the notion of significant observation:\nDefinition 2 (Significant Observation). A observation o is \\(\\delta\\)-significant if there exists a policy \\(\\pi\\), so that the probability to reach o following policy \\(\\pi\\) is greater than \\(\\delta\\). In symbol:\n\n\\(\\max_{\\pi} \\mu^{\\pi} (o) \\geq \\delta\\)\n\nwhere \\(\\mu^{\\pi} (o) = \\sum_{a} \\mu^{\\pi} (o, a)\\).\nIntuitively, since insignificant observations are rarely reachable by any policy, their contributions to the divergence in Lemma 2 will be limited. Thus, it suffices to only visit significant observations. Algorithm 3 is designed specifically for this purpose, and achieves the following guarantee.\nTheorem 3. (Theorem 3 of [Jin et al., 2020]) There exists absolute constant c > 0 such that for any \\(\\epsilon > 0\\) and \\(\\rho \\in (0, 1)\\), if we set \\(N_o \\geq c\\frac{O^3AH^4\u03b9_0^{1/3}}{\\delta}\\) where \\(\u03b9_0 = log(\\frac{OAH}{\\rho \\delta})\\), then with probability at least 1 - \\(p\\), that Algorithm 3 will return a dataset D consisting of N trajectories \\(\\{z_n\\}_{n=1}^N\\), which are i.i.d sampled from a distribution \\(\\lambda\\) satisfying:\n\n\\(\\text{\u03b4 - significant o, } \\max_{\\pi} \\frac{\\mu^{\\pi} (o, a)}{N \\lambda(o, a)} \\leq \\frac{c \\epsilon}{2 OAH}.\\)\n\nAs we can see from theorem 3, all significant observations can be visited by distribution A with reasonable probability. Hence, with algorithm 3, we can learn our model by visiting significant observations without the guidance of any rewards."}, {"title": "Planning stage", "content": "After collecting enough data in the exploration stage, algorithm 4 use the data to compute an empirical transition matrix \\(\\tilde{p}\\), on which the approximate planner is employed. We prove that (see Appendix D.2), any policy will have small value gap in the learned transition under \\(\\tilde{p}\\) vs. the ground truth transition p.\nLemma 3. There exists an absolute constant c > 0, for any \\(\\epsilon > 0, \\rho \\in (0,1)\\), assume the dataset D has N i.i.d. samples from distribution \\(\\lambda\\) which satisfies equation 1 with \\(\\delta = \\frac{\\epsilon}{80G}\\), and\n\n\\(N > \\frac{CO^5A^3H^2G^2\u03b9'}{\\epsilon^2}\\), where \\(\u03b9' = ln(\\frac{OAHG}{\\rho})\\)\n\nthen w.p. at least 1 - 2\\(p\\):\n\n\\(|J(\\pi) - \\tilde{J}(\\pi)| \\leq \\frac{\\epsilon}{2}\\)\n\nThe reason for the increased sample complexity compared to the original analysis by Jin et al. [2020] lies in the fact that more samples are required to reduce the model error associated with significant observations than in the Markovian setting. Specifically, in our analysis, it is necessary to account for model errors across every (o, a, o') triplet. In contrast, in the standard Markovian setting, the modeling error can be further decomposed into the error of the empirical next-state value function (see the proof of Lemma 3.6 in Jin et al. [2020]), which allows for tighter bounds. After we obtain our empirical transition matrix \\(\\tilde{p}\\), given any non-Markovian rewards R, we can find a near optimal policy by running \\(\\alpha\\)-approximate planner, as a result of our simulation Lemma.\nLemma 4. Under the preconditions of lemma 3, with probability of 1 - 2\\(p\\) for any rewards R, the output policy of algorithm 4 is \\(e + \\alpha\\)-optimal, that is\n\n\\(|J(\\pi^*) - \\tilde{J}(\\pi)| \\leq \\epsilon + \\alpha\\)\n\nwhere \\(\\pi^*\\) is the optimal policy.\nNote that, for general non-Markovian rewards, the optimization error won't be reduced to 0, but for any PRMs, the optimization can be reduced to 0, since we can run value iteration given the cross-product MDP and solve it optimally. In addition, there are some cases where the rewards possess special structural properties, for which performance with guarantees can be achieved[Prajapat et al., 2023, De Santi et al., 2024]."}, {"title": "Experiments", "content": "In this section, we present a series of experiments comparing the empirical performance of our algorithm, UCBVI-PRM, with existing baselines. We evaluate our algorithm in MDPs with both DRM and PRM against different baselines. For DRMs, we compare with UCRL-RM-L and UCRL-RM-B of Bourel et al. [2023]. For PRM, since there is no existing algorithm, we compare with the naive approach of directly applying UCBVI[Azar et al., 2017] onto the cross-product MDP. In our experiment, we tune the exploration coefficient for all algorithms by selecting from a equally large set of options (see Appendix E.2). This is to make sure that an algorithm with a larger hyper-parameter set does not get an unfair advantage. In addition, we apply the doubling trick (detailed in Appendix E.1) to speed up UCBVI-PRM, which is a common technique in the literature[Auer et al., 2008, Dann and Brunskill, 2015] and won't affect the O regret."}, {"title": "DRM Experiments", "content": "In the RiverSwim environment, shown in Figure 2, the agent has two actions corresponding to swimming left or right. Going right results in stochastic transitions, as shown by the solid lines in Figure 2(a). Going left results in deterministic transitions as shown by the dashed lines in Figure 2(a). The agent receives reward when visit two extreme locations in RiverSwim(i.e., \\(o_1\\) and \\(o_N)\\) in sequence.\nFigure 3(a), 3(b), and 3(c) show the regret over time in the RiverSwim domain, with the results averaged over 16 runs. The shaded area shows the standard variance of the corresponding quantity. Specifically, Figures 3(a), 3(b), and 3(c) present the regrets of the agent running in a RiverSwim MDP with 5 observations and a horizon length of 10, a RiverSwim MDP with 10 observations and a horizon length of 20, and a RiverSwim MDP with 15 observations and a horizon length of 30, respectively. As we can see from the figures, in simpler environments (fewer observations and shorter horizons), the advantage of UCBVI-PRM is not obvious (see Figure 3(a)). However, with longer horizons and more observations, the gap between UCBVI-PRM and the baselines of Bourel et al. [2023] becomes larger. These results align with our regret analysis, where the regret of UCBVI-PRM grows slower than UCRL-RM-L in the order of H and slower than UCRL-RM-B in the order of H and O."}, {"title": "PRM Experiments", "content": "In the warehouse environment(see Figure 1), the robot has five actions corresponding to moving up, right, down, left, and stay. Moving up, right, down, or left leads to moving in the intended direction with probability 0.7, in each perpendicular direction with probability 0.1, or staying in the same place with probability 0.1. The stay action will result in the robot staying in the same place deterministically. The robot receives reward when successfully picks up an item and delivers it to the delivery location in sequence. Figures 4 show the regret over time, with the results averaged over 16 runs. Specifically, Figures 4(a), 4(b), and 4(c) present the results of the agent running in a 3\u00d73 warehouse with a horizon length of 9, a 4 \u00d7 4 warehouse with a horizon length of 12, and a 5 \u00d7 5 warehouse with a horizon length of 15, respectively. In all experiments, UCBVI-PRM outperforms the baseline. In addition, as the horizon becomes longer and with larger warehouse, UCBVI-PRM beats the baseline with a larger margin."}, {"title": "Exploration coefficient", "content": "UCBVI-PRM, UCBVI, UCRL2-RM-L, and UCRL2-RM-B all apply the principle of optimism in the face of uncertainty, where the algorithms either adjust the reward functions (UCBVI-PRM and UCBVI) or modify their models (UCRL2-RM-L and UCRL2-RM-B) to balance exploration and exploitation. Specifically, UCBVI-PRM and UCBVI carefully design exploration bonuses to ensure that \\(V_{k,h} \\geq V^*\\). In contrast, UCRL2-RM-L and UCRL2-RM-B construct a set of MDPs that likely contains the true MDP according to different concentration inequalities, then alter the model to be the best possible MDP within that set. However, due to the theoretical pessimism, these approaches often lead to over-exploration in practice, resulting in higher regret. To mitigate this, we tune the exploration coefficient of each algorithm to better balance exploration and exploitation in each environment, improving performance. For fairness, we select the optimal exploration coefficient for each algorithm from an equally large set of candidates.\nSpecifically, the exploration coefficient \\(\\gamma\\) of UCBVI-PRM is defined as the empirical bonus used in the experiments divided by the theoretical bonus function calculated using Algorithm 2. This modifies line 13 of Algorithm 1 to be: \\(b_{k}(s, a) = \\gamma\\cdot \\text{bonus}(p_{k}(o, a), V_{k, h + 1}, N_{k}, N_{k.h})\\). UCBVI applies the same rule as UCBVI-PRM. For UCRL2-RM-L, the algorithm designs confidence sets for the transition function p for every (o, a) pair, such that the true dynamics lie within a confidence interval centered on the empirical mean p. Formally, \\(C(o,a) = \\{p' \\in \\Delta_\\mathcal{O}, ||p' - \\tilde{p}(o, a)||_1 \\leq \\gamma\\cdot \\beta(o,a)\\} \\), P[\\(p \\notin C(o,a)\\)] \\(\\leq \\rho\\), where \\(\\gamma\\) = 1 is the original parameter. The exploration coefficient for UCRL2-RM-B follows the same principle, with the only distinction being the confidence interval design. For more detailed implementation, please refer to our codes. For each algorithm, we choose parameters from an equally large set for each environment. Following is the table of candidates of exploration coefficient \\(\\gamma\\) for every algorithm,"}, {"title": "Technical Lemmas", "content": "Lemma 21. [Maurer and Pontil, 2009] Let \\(Z_1,..., Z_n(n \\geq 2)\\) be i.i.d. random variables with values in [0, 1] and let \\(\\rho \\in (0, 1)\\). Define \\(\\bar{Z} = \\frac{1}{n} \\sum_{i=1}^{n} Z_i\\) and \\(\\hat{V}_n = \\frac{1}{n-1}\\sum_{i=1}^{n} (Z_i - \\bar{Z})^2\\). Then we have\n\n\\(P[|E[Z]-\\bar{Z}|> \\sqrt{\\frac{2\\hat{V}_n ln(2/\\rho)}{n}} + \\frac{7 ln(2/\\rho)}{3(n-1)}] \\leq \\rho\\)\n\nLemma 22. [Cesa-Bianchi and Lugosi, 2006] Let \\(Z_1,..., Z_n(n \\geq 2)\\) be i.i.d. random variables with values in [0, 1] and let \\(\\rho \\in (0, 1)\\). Define \\(\\bar{Z} = \\frac{1}{n} \\sum_{i=1}^{n} Z_i\\) and \\(\\hat{V}_n = \\frac{1}{n} \\sum_{i=1}^{n} (Z_i - \\bar{Z})^2\\). Then we have\n\n\\(P[|E[Z]-\\bar{Z}|> \\sqrt{\\frac{2\\hat{V}_n ln(2/\\rho)}{n}} + \\frac{2ln (2/\\rho)}{3n}] \\leq \\rho\\)"}, {"title": "Martingale difference sequences", "content": "We define the following martingale operator for every \\(k \\in [K], h \\in [H]\\) and \\(F : S \\rightarrow R\\), let \\(t = (k \u2212 1)H + h\\) denote the time stamp at step h of episode k then\n\n\\(M_tF \\stackrel{def}{=} P^\\pi_tF - F(s_{k,h+1})\\).\n\nLet \\(\\mathcal{H}_t\\) be the history of all random events up to (and including) step h of episode k then we have that \\(E(M_tF |\\mathcal{H}_t) = 0\\). Hence \\(M_tF\\) is a martingale difference w.r.t. \\(\\mathcal{H}_t\\)."}, {"title": "High probability events", "content": "Denote \\(\u03b9 = ln(\\frac{6QOAT}{\\rho})\\). We define the high probability events \\(\\mathcal{E}\\) and \\(\\Omega_{k,h}\\). We define four confidence levels as follows.\n\n\\(C_1(v, u, n) \\stackrel{\\text{def}}{=} 2\u03b9 \\sqrt{\\frac{v\u03b9}{n}} + \\frac{14u\u03b9}{3n}\\)\n\n\\(C_2(\\rho, n) \\stackrel{\\text{def}}{=} 1 \\sqrt{\\frac{2\\rho(1 - \\rho)\u03b9}{n}} + \\frac{2\u03b9}{3n}\\)\n\n\\(C_3(n) \\stackrel{\\text{def}}{=} 2\u03b9 \\sqrt{\\frac{\u039f\u03b9}{n}}\\)\n\n\\(C_4(u, n) \\stackrel{\\text{def}}{=} \\sqrt{\\frac{\u03b9}{n}}\nLet P be the set of all probability distributions on O. Define the following confidence set for every \\(k = 1, 2, . . ., K, n > 0\\) and \\((o, a) \\in O \\times A\\).\n\n\\(P(k, h, n, o, a, z) \\stackrel{\\text{def}}{=} \\{p(\\cdot|s, a) \\in P : \\forall q \\in Q\\)\n\n|\\((P-\\tilde{p})W(q, o, a)| \\leq min (C_1(\\tilde{W}(s, a), H, n), C_1(\\tilde{W}_h(s, a), H, n))\\)\n\n\\(|p(z|o, a) - \\tilde{p}(z|o, a)| \\leq C_2(\\tilde{p}(z|o, a), n)\\)\n\n\\(||p(\\cdot|o, a) - \\tilde{p}(\\cdot|o, a)||_1 \\leq C_3(n)\\)\n\n\\(|(P - \\tilde{p})(q, o, a)| \\leq C_4(1,n)\\}\nWe now define the random event \\(\\mathcal{E}\\) as follows\n\n\\(\\mathcal{E} \\stackrel{\\text{def}}{=} \\{\\forall p_k (z|o, a) \\in P(k, h, N_{k}(o, a), o, a, z), \\forall k \\in [K], \\forall h \\in [H], \\forall (z, o, a) \\in O \\times O \\times A\\}\\)\n\nLet t be a positive integer, and let \\(\\mathcal{F} = \\{f_s\\}_{s \\in [t]}\\) be a set of real-valued functions defined on \\(\\mathcal{H}_{t+s}\\) for some integer s > 0. We define the following random events for given parameters \\(w > 0, u > 0\\), and \\(c > 0\\):\n\n\\(\\mathcal{E}_{AZ}(F, u, w) \\stackrel{\\text{def}}{=} \\{\\sum_{s=1}^t M_s f_s \\leq 2 \\sqrt{t u c}\\}\\)\n\n\\(\\mathcal{E}_{FR}(F, w, u) \\stackrel{\\text{def}}{=} \\{\\sum_{s=1}^t M_s f_s \\leq \\sqrt{2wc ln\\frac{1}{P}} + \\frac{14uc}{3}\\}\\)"}, {"title": "\"Typical\" state-actions and steps"}]}