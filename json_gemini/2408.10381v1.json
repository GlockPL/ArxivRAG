[{"title": "Efficient Reinforcement Learning in Probabilistic Reward Machines", "authors": ["Xiaofeng Lin", "Xuezhou Zhang"], "abstract": "In this paper, we study reinforcement learning in Markov Decision Processes\nwith Probabilistic Reward Machines (PRMs), a form of non-Markovian reward\ncommonly found in robotics tasks. We design an algorithm for PRMs that achieves\na regret bound of \\( \\tilde{O} (\\sqrt{HOAT}+H^2O^2A^{3/2}+H \\sqrt{T})\\), where H is the time horizon,\nO is the number of observations, A is the number of actions, and T is the number\nof time-steps. This result improves over the best-known bound, O(H\\sqrt{OAT}) of\nBourel et al. [2023] for MDPs with Deterministic Reward Machines (DRMs), a\nspecial case of PRMs. When \\(T > H^3O^3A^2\\) and OA > H, our regret bound\nleads to a regret of \\(\\tilde{O}(\\sqrt{HOAT})\\), which matches the established lower bound of\n\\(\\Omega(\\sqrt{HOAT})\\) for MDPs with DRMs up to a logarithmic factor. To the best of our\nknowledge, this is the first efficient algorithm for PRMs. Additionally, we present\na new simulation lemma for non-Markovian rewards, which enables reward-free\nexploration for any non-Markovian reward given access to an approximate planner.\nComplementing our theoretical findings, we show through extensive experiment\nevaluations that our algorithm indeed outperforms prior methods in various PRM\nenvironments.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning traditionally focuses on the setting where the reward function is Markovian,\nmeaning that it depends solely on the current state and action, and independent of history. However, in\nmany real-world scenarios, the reward is a function of the history of states and actions. For example,\nconsider a robot tasked with patrolling various locations in an industrial park. The performance of\nrobot is measured by how thorough it regularly covers different zones in the park, which cannot easily\nbe represented as a function of its current state and action, but rather would depend on its whole\ntrajectory during the patrol.\nOne emerging tool to model such performance metrics is called the Reward Machine (RM)[Icarte\net al., 2018, 2022], which is a Deterministic Finite-State Automaton (DFA) that can compress the\nsequence of past events into one single state. Combined with the current observation, the state of\nRM can fully specify the reward function. Hence, for an MDP with RM, we can obtain an equivalent\ncross-product MDP by leveraging the information of RM(see Lemma 1) and applying off-the-shelf\nRL algorithms e.g., Q-learning of Sutton and Barto [2018] to learn an optimal policy. However, as\nwe shall see later, this naive approach will incur a large regret.\nOne limitation of the classic RM framework is that the transition between the state of RM is restricted\nto be deterministic, whereas stochastic transitions are much more common in practice, especially\nwith uncertainty in the environment. For instance, suppose a robot working in a warehouse is tasked\nwith managing a warehouse by performing simple tasks of fetching and delivering items (as shown\nin Figure 1). The robot starts at a charging station, navigates to the item pickup location, collects\nthe item, and then proceeds to the delivery location to deliver the item and receives a reward. Based"}, {"title": "2 Related Work", "content": "Reward Machines Icarte et al. [2018] introduced the concept of Deterministic Reward Machines\n(DRMs), a type of finite state machine that specifies reward functions while exposing the structure\nof these functions to the learner. They also proposed an algorithm called QRM, which outperforms\nstandard Q-learning and hierarchical RL algorithms in environments where the reward can be specified\nby a DRM. Simultaneously, there has been extensive research on RL with temporal specifications\nexpressed in Linear Temporal Logic (LTL) [Sadigh et al., 2014, Wen et al., 2015, Li et al., 2017],\nwhich can be directly translated into DRMs. Of particular interest to us is the recent work of Bourel\net al. [2023], which studies online RL with DRMs in the context of infinite horizon average reward\nMDPs. They establish a regret lower-bound of \\(\\Omega(\\sqrt{HOAT})\\) in the episodic setting. They also\npropose two algorithms differing in the confidence interval design, that achieve a regret bound of\norder \\(\\tilde{O}(H\\sqrt{OAT})\\) and \\(\\tilde{O}(HO\\sqrt{AT})\\), respectively. We improve upon this work with an algorithm\nthat outperforms theirs both empirically and theoretically.\nEmpirically, RL with LTL or DRMs has been successfully applied to complex robotics tasks, such\nas manipulation [Camacho et al., 2021] and locomotion [DeFazio et al., 2024]. DRMs have also\nbeen employed in multi-agent learning scenarios [Neary et al., 2020, Hu et al., 2024, Zheng and Yu,\n2024]. However, all of these works have focused exclusively on Deterministic RMs. Dohmen et al."}, {"title": "2.0.1 Reward-free exploration", "content": "Reward-free exploration [Jin et al., 2020] studies the problem where an agent needs to collect data\nby interacting with the environment during the exploration stage, preparing for learning the optimal\npolicy for an unknown reward function during the offline planning stage. The number of potential\nrewards can be arbitrarily large or even infinite. Jin et al. [2020] proposed an algorithm that can return\nan \\(\\epsilon\\)-optimal policies for an arbitrary Markovian reward function, after at most collecting \\(O \\left(\\frac{H^3SA}{\\epsilon^2}\\right)\\)\nsamples in the exploration stage. M\u00e9nard et al. [2021] further improved the sample complexity to\n\\(\\tilde{O} \\left(\\frac{H^3SA}{\\epsilon^2}\\right)\\). Later work extends this problem to MDPs with more general structures, such as linear\nMDPs[Wang et al., 2020, Zhang et al., 2021a, Wagenmaker et al., 2022] and kernel MDPs[Qiu et al.,\n2021], and other settings such as multitask RLZhang et al. [2020] and multi-agent RL[Bai and Jin,\n2020, Liu et al., 2021]. However, all of the above works restrict their attention on Markovian rewards.\nThis is because their analysis crucially relies on the well-known simulation lemma that quantifies the\nvalue difference of a policy in two different MDPs. This classic version of simulation lemma only\nholds for Markovian reward. We provide the first extension of reward-free RL to the non-Markovian\nreward setting via a new simulation lemma that holds for arbitrary non-Markovian reward (Lemma\n2)."}, {"title": "3 Problem Formulation", "content": "We start with a few definitions.\nLabeled MDPs with Probabilistic Reward Machines A labeled MDP[Xu et al., 2022] with\nPRM[Dohmen et al., 2022] is defined as a tuple M = (O, A, p,R,P, L, H). O represents a finite set\nof observations with cardinality O, and A represents a finite set of actions available at each observation\nwith cardinality A. The transition function p : O \u00d7 A \u2192 \u2206o dictates the probability p(o' o, a) of\ntransitioning to observation o' when action a is taken in observation o. The set P consists of atomic\npropositions, and the labeling function L : O \u00d7 A \u00d7 O \u2192 2P assigns a subset of P to each transition\n(o, a, o'). These labels represent high-level events associated with transitions that can be detected\nfrom the observation space. The component R is a Probabilistic Reward Machine (PRM), which\ngenerates history-dependent reward functions. A PRM is defined as a tuple R = (Q, 2P, \u03c4, \u03bd), where\nQ is a finite set of states with cardinality Q. The probabilistic transition function 7 : Q \u00d7 2\u00ba \u2192 \u2206\u03a9\ndetermines the probability (q' q, o) of transitioning to state q' given that the event o occurs in state\nq. The function v : Q \u00d7 2P \u00d7 Q \u2192 \u2206[0,1] maps each transition to a reward. The horizon H defines\nthe length of each episode. Note that MDP with PRM is a special class of Non-Markovian Reward\nDecision Processes (NMRDPs)[Bacchus et al., 1996], which is identical to a MDP except that the\nreward depends on the history of state and action. For a NMRDP, we define F(\u03b7) as the expected\nreward collected by trajectory \u03b7 def {(Ot, at)1} and G def max\u03b7 F(\u03b7).\nA special class of PRMs is Deterministic Reward Machines (DRMs) [Icarte et al., 2018, 2022], where\nthe transition function 7 is deterministic. In a DRM, given a state q and an event \u03c3, the next state q'\nis uniquely determined, and the rewards are generated deterministically based on these transitions.\nThe agent interacts with the MDP with PRM M as follows: At each time step t, the agent is in obser-\nvation ot \u2208 O and selects an action at \u2208 A based on the history ht = (01, a1, ..., Ot\u22121, At\u22121, Ot).\nAfter executing the action at in observation ot, the environment transitions to the next observation\nOt+1 ~ p(\u00b7|Ot, at) and assigns a label ot = L(ot, at, Ot+1). Simultaneously, the PRM, which is in\nstate qt, transitions to state qt+1 ~ \u315c(\u00b7|qt, ot) and outputs a reward rt = v(qt, 5t, qt+1). The agent\nthen receives this reward, and both the environment and the PRM proceed to their next observation\nOt+1 and state qt+1, respectively.\nWe define the joint state space as the cross-product of Q and O, i.e. S = Q \u00d7 O. The transition\nfunction P : S \u00d7 A \u2192 As dictates the probability p(s'|s, a) of transitioning to state s' when action\na is taken in s. The policy is defined as a mapping \u03c0 : S \u00d7 [H] \u2192 A. For every \u03c0, the occupancy"}, {"title": "4 Learning Algorithms and Results", "content": "In this section, we present our RL algorithm for PRMs, UCBVI-PRM.UCBVI-PRM follows the algorith-\nmic skeleton of a classic model-based RL algorithm [Azar et al., 2017], while incorporating designs\nthat leverage the structure of PRMs. Our key contribution is a regret bound of \\(\\tilde{O}(\\sqrt{HOAT})\\) when T\nis large enough and OA > H. The regret bound matches the established lower bound \\(\\Omega(\\sqrt{HOAT})\\)\nup to a logarithmic factor for MDP with DRM, and is notably independent of the joint state space\nsize.\nIntuitively, UCBVI-PRM (Algorithm 1) proceeds in 3 stages: (i) From lines 1 to 7, the algorithm\nfirst constructs an empirical transition matrix based on the data collected thus far; (ii) Using this\nempirical transition matrix, the algorithm then performs value iteration from lines 8 to 23 to update\nthe value function. Notably, between lines 8 and 19, the algorithm computes the new action-value\nfunction using the updated empirical transition matrix (line 12) and the exploration bonus (line 13);\n(iii) Finally, from lines 24 to 28, the agent selects actions based on the updated action-value function\nand collects new data, which is then incorporated into the dataset.\nThe main technical novelty lies in how we utilize the PRM structure. Denote Wh : Q \u00d7 O \u00d7 A \u00d7 O \u2192\nR a function that measures the expected return when being in state (q, o), executing action a at time\nstep h 1 and observing o' at time step h. W is defined as follows:\n\\[\nWh(q, \u03bf, \u03b1, \u03bf') = \\sum_{q'\\in Q} \\tau(q'|q, L(\u03bf, \u03b1, \u03bf'))V_h(q', o')\n\\]"}, {"title": "5 RL with Non-Markoivian Rewards", "content": "In this section, we introduce an explore-then-commit style algorithm for MDPs with generic non-\nMarkovian rewards: in the exploration stage, the agent collects trajectories from the environment\nwithout the information of rewards; in the planning stage, the agent computes a near-optimal policy\ngiven the data gathered in the exploration stage, assuming access to an approximate planner. We give\nan efficient algorithm that conducts \\(\\tilde{O}(\\frac{O^5A^3H^2G^2}{\\epsilon^4})\\) episodes of exploration and returns an (e + a)-\noptimal policy for any general non-Markovian rewards, given an a-approximate planner, formally\nstated below.\nDefinition 1. A planner is a-approximate if given any NMRDP M = (O, A, p,R, H), the planner\nreturns a policy \u03c0 that satisfies\n\\[\nJ(\\pi) \\geq J(\\pi^*) - \\alpha\n\\]\nwhere J(\u03c0) is the expected return of executing policy \u03c0 in M and \u03c0* is the optimal policy in M.\nTheorem 2. The exists an absolute constant c > 0, such that, for any p \u2208 (0, 1), with probability at\nleast 1 p, given the information collected by algorithm 3, algorithm 4 can output (e + a)-optimal\npolicies for any non-Markovian rewards assuming access to a-approximate planner. The number of\nepisodes in algorithm 3 is bounded by\n\\[\nc \\cdot \\left[ \\frac{O^5A^3H^2G^2\\iota'}{\\epsilon^4} + \\frac{O^4 A H^4 G^{1/3}}{\\epsilon} \\right]\n\\]\nwhere \\(\\iota' \\stackrel{\\text { def }}{=} \\ln \\left(\\frac{OAHG}{\\rho \\epsilon}\\right)\\).\nThis result is made possible by a new simulation lemma that can be applied to generic non-Markovian\nrewards and non-Markovian policies.\nLemma 2. For any two NMRDPs M = (O, A, p, R, H) and \\(\\tilde{M} = (O, A, \\tilde{p}, R, H)\\), for any policy\n\u03c0\n\\[\n|J(\\pi) - \\tilde{J}(\\pi)| \\leq \\sum_{m=1}^H \\sum_{o_m, a_m, o_{m+1}} \\epsilon(o_{m+1}|o_m, a_m) \\mu_m^{\\pi}(o_m, a_m) G\n\\]\nwhere\n\\[\n\\epsilon(o_{m+1}|o_m, a_m) = |\\tilde{P}(o_{m+1}|o_m, a_m) - p(o_{m+1}|o_m, a_m)|\n\\]"}, {"title": "5.1 Exploration stage", "content": "It turns out that a procedure (Algorithm 3) similar to the Markovian reward case suffices for our\npurpose [Jin et al., 2020]. Intuitively, algorithm 3 perform two steps. In the first step, from lines 2 to\n7, the algorithm constructs a set of exploratory policies each designed to visit an observation state\no \u2208 O as often as possible. To accomplish this, for each observation o, the algorithm creates a reward\nfunction that is 0 everywhere except at observation o, where it is set to 1 (line 3). The algorithm then\nemploys a no-regret RL algorithm (e.g. EULER of Zanette and Brunskill [2019]) to find a policy that\nmaximizes this reward, which is equivalent to maximizing the probability of visiting o. In the second\nstage, from lines 8 to 12, the algorithm collects new data by sampling and executing policies from\nthis exploratory policy set. We prove that, with this framework, the collected data can be used to\nlearn a transition kernel that is sufficiently close to the true transition characterized by the divergence\nin Lemma 2. Towards this, we introduce the notion of significant observation:\nDefinition 2 (Significant Observation). A observation o is \\(\\delta\\)-significant if there exists a policy \u03c0, so\nthat the probability to reach o following policy \u03c0 is greater than 8. In symbol:\n\\[\n\\max_{\\pi} \\mu^{\\pi}(o) \\geq \\delta\n\\]\nwhere \\(\\mu^{\\pi}(o) = \\sum_{a} \\mu^{\\pi}(o, a)\\).\nIntuitively, since insignificant observations are rarely reachable by any policy, their contributions to\nthe divergence in Lemma 2 will be limited. Thus, it suffices to only visit significant observations.\nAlgorithm 3 is designed specifically for this purpose, and achieves the following guarantee.\nTheorem 3. (Theorem 3 of [Jin et al., 2020]) There exists absolute constant c > 0 such that for any\n\\(\\epsilon > 0\\) and \\(\\rho \\in (0, 1)\\), if we set \\(N_o \\geq c \\frac{O^2AH^4 \\iota_0^{1/3}}{\\delta}\\) where \\(\\iota_0 = \\log(\\frac{OAH}{\\rho \\delta})\\), then with probability\nat least 1 - \\(\\rho\\), that Algorithm 3 will return a dataset D consisting of N trajectories \\({z_n\\}_{n=1}\\), which\nare i.i.d sampled from a distribution \\(\\lambda\\) satisfying:\n\\[\n\\delta \\text{ - significant o, }\\max_\\pi \\frac{\\mu^{\\pi}(o, a)}{\\lambda(o, a)} < \\frac{\\epsilon}{2OAH}.\n\\]\nAs we can see from theorem 3, all significant observations can be visited by distribution A with\nreasonable probability. Hence, with algorithm 3, we can learn our model by visiting significant\nobservations without the guidance of any rewards."}, {"title": "5.2 Planning stage", "content": "After collecting enough data in the exploration stage, algorithm 4 use the data to compute an empirical\ntransition matrix \\(\\tilde{p}\\), on which the approximate planner is employed. We prove that (see Appendix\nD.2), any policy will have small value gap in the learned transition under \\(\\tilde{p}\\) vs. the ground truth\ntransition p.\nLemma 3. There exists an absolute constant c > 0, for any \\(\\epsilon > 0\\), \\(\\rho \\in (0,1)\\), assume the\ndataset D has N i.i.d. samples from distribution \\(\\lambda\\) which satisfies equation 1 with \\(\\delta = \\frac{\\epsilon}{8OG}\\), and\n\\[\nN > c\\frac{O^5A^3H^2G^2}{\\epsilon^4}\\iota', \\text{ where } \\iota' = \\ln \\left(\\frac{OAHG}{\\rho \\epsilon}\\right)\n\\]\nthen w.p. at least 1 - 2\\(\\rho\\):\n\\[\n|J(\\pi) - \\tilde{J}(\\pi)| \\leq \\frac{\\epsilon}{2}\n\\]\nThe reason for the increased sample complexity compared to the original analysis by Jin et al. [2020]\nlies in the fact that more samples are required to reduce the model error associated with significant\nobservations than in the Markovian setting. Specifically, in our analysis, it is necessary to account\nfor model errors across every (o, a, o') triplet. In contrast, in the standard Markovian setting, the\nmodeling error can be further decomposed into the error of the empirical next-state value function\n(see the proof of Lemma 3.6 in Jin et al. [2020]), which allows for tighter bounds. After we obtain\nour empirical transition matrix \\(\\tilde{p}\\), given any non-Markovian rewards R, we can find a near optimal\npolicy by running a-approximate planner, as a result of our simulation Lemma.\nLemma 4. Under the preconditions of lemma 3, with probability of 1 \u2013 2p for any rewards R, the\noutput policy of algorithm 4 is e + a-optimal, that is\n\\[\n|J(\\pi^*) - J(\\hat{\\pi})| \\leq \\epsilon + \\alpha\n\\]\nwhere \u03c0* is the optimal policy.\nNote that, for general non-Markovian rewards, the optimization error won't be reduced to 0, but\nfor any PRMs, the optimization can be reduced to 0, since we can run value iteration given the\ncross-product MDP and solve it optimally. In addition, there are some cases where the rewards\npossess special structural properties, for which performance with guarantees can be achieved[Prajapat\net al., 2023, De Santi et al., 2024]."}, {"title": "6 Experiments", "content": "In this section, we present a series of experiments comparing the empirical performance of our\nalgorithm, UCBVI-PRM, with existing baselines. We evaluate our algorithm in MDPs with both DRM\nand PRM against different baselines. For DRMs, we compare with UCRL-RM-L and UCRL-RM-B\nof Bourel et al. [2023]. For PRM, since there is no existing algorithm, we compare with the\nnaive approach of directly applying UCBVI[Azar et al., 2017] onto the cross-product MDP. In our\nexperiment, we tune the exploration coefficient for all algorithms by selecting from a equally large set\nof options (see Appendix E.2). This is to make sure that an algorithm with a larger hyper-parameter\nset does not get an unfair advantage. In addition, we apply the doubling trick (detailed in Appendix\nE.1) to speed up UCBVI-PRM, which is a common technique in the literature[Auer et al., 2008, Dann\nand Brunskill, 2015] and won't affect the O regret."}, {"title": "7 Conclusion", "content": "We study sample-efficient reinforcement learning in episodic Markov decision processes with proba-\nbilistic reward machines. We introduce an algorithm tailored for PRMs that matches the established\nlower bound when \\(T > H\u00b3O3A2\\) and OA > H. We also present a lemma that characterizes the\ndifference in policy evaluations between two MDPs with non-Markovian rewards. Building upon\nthe new lemma, we establish the reward-free learning result for non-Markovian reward. Finally,\nwe validate our algorithms through a series of experiments. Interesting future direction includes\ndesigning effective and efficient algorithms for the multi-agent setting, and exploring connections\nwith reward structures such as submodular rewards."}, {"title": "A Table of notation", "content": ""}, {"title": "B Notation", "content": "We follow the notations of Azar et al. [2017].\n\\[\nW_{k,h} (q, 0, \\alpha, o') \\stackrel{\\text { def }}{=} \\sum_{q'} \\tau(q'|q, L(o, a, o')) V_{k,h}(d', o')\n\\]\n\\[\nW(9, 0, a, o') \\stackrel{\\text { def }}{=} \\sum_{q'} \\tau(q'|q, L(o, a, o')) V_h(d', o')\n\\]\n\\[\n\\phi(q, o, a, o') \\stackrel{\\text { def }}{=} \\sum_{q'} \\tau(q'|q, L(o, a, o'))v(q, L(o, a, o'), q')\n\\]\n\\[\n\\mathcal{W}_{k,h}(q, o, a) \\stackrel{\\text { def }}{=} \\text{Var} _{o'\\sim p(-\\circ,a)} (W_{k,h} (q, o, a, o'))\n\\]\n\\[\n\\mathcal{W}(q, o, a) \\stackrel{\\text { def }}{=} \\text{Var} _{o'\\sim p(-\\circ,a)} (W (q, o, a, o'))\n\\]\n\\[\n\\tilde{\\mathcal{W}}_{k,h}(q, o, a) \\stackrel{\\text { def }}{=} \\text{Var} _{o'\\sim p(-\\circ,a)} (W (q, o, a, o'))\n\\]\n\\[\n\\tilde{\\mathcal{W}}(q, o, a) \\stackrel{\\text { def }}{=} \\text{Var} _{o'\\sim p(-\\circ,a)} (W (q, o, a, o'))\n\\]\n\\[\nV_{k,h}(s, a) \\stackrel{\\text { def }}{=} \\mathbb{Vary}_{y\\sim P(s,a)} (V_{k,h+1}(y))\n\\]\n\\[\nV_h(s, a) \\stackrel{\\text { def }}{=} \\mathbb{Vary}_{y\\sim P(s,a)} (V_h (y))\n\\]\n\\[\nV_{k,h}(s, a) \\stackrel{\\text { def }}{=} \\mathbb{Vary}_{y\\sim P(s,a)} (V (y))\n\\]\n\\[\nV(s, a) \\stackrel{\\text { def }}{=} \\mathbb{Vary}_{y\\sim P(s,a)} (V (y))\n\\]\nWe use the lower case to denote the functions evaluated at the current state-action pair. e.g., let\n\\(n_{k,h} = N_{k}(O_{k,h}, \\pi_k(S_{k,h},h))\\). We also define \\(b_{ij}(o) = \\min \\left(\\frac{100^2 O^2 H^2}{N_j}, \\frac{100^2 O^2 A^2 H^2}{\\epsilon H^2}\\right)\\). We also denote\n\\[\npW (q, \u03bf, \u03b1) = \\sum_{o'\\in O} p(o'|\u03bf, \u03b1)W(q, o, a, o') \\text{ and } p\\phi(q, o, a) = \\sum_{o'\\in O} p(o'|\u03bf, \u03b1)\\phi(q, o, a, o').\n\\]\nB.1 \u201cTypical\u201d state-actions and steps\nWe split the episodes into 2 sets: the set of \u201ctypical\u201d episodes in which the number of visits to the\nencountered state-actions are large and the rest of the episodes. Further, we define\n\\[\n[(o,a)]_k \\stackrel{\\text { def }}{=} \\{(o,a) : (o, a) \\in O \\times A, N_h(o, a) \\geq H, N_{k,h}(0) \\geq H\\}\n\\]\nWe define typical episodes and the set of typical state-dependent episodes as follows\n\\[\n[k]_{typ} \\stackrel{\\text { def }}{=} \\{i : i \\in [k], \\forall h \\in [H], (O_{i,h}, \\pi_i(q_{i,h}, O_{i,h}, h)) \\in [(o, a)]_k, i \\geq 650HO^2 A^2\\iota\\}\n\\]\n\\[\n[k]_{typ,o} \\stackrel{\\text { def }}{=} \\{i : i \\in [k], \\forall h \\in [H], (O_{i,h}, \\pi_i(q_{i,h}, O_{i,h}, h)) \\in [(o, a)]_k, N_{k,h}(0) \\geq 650HO^2A^2\\iota\\}\n\\]\nB.2 surrogate regrets\nDefine \\(\\Delta_{k,h} \\stackrel{\\text { def }}{=} V_h^* - V_{k,h}\\), \\(\\Delta_{k,h} \\stackrel{\\text { def }}{=} V_{k,h} - V_h\\), \\(\\delta_{k,h} \\stackrel{\\text { def }}{=} V(S_{k,h}) - V_{k,h}(S_{k,h})\\), and \\(\\delta_{k,h} \\stackrel{\\text { def }}{=} V(S_{k,h}) - V_k (S_{k,h})\\) for every \\(s \\in S, h \\in [H]\\) and \\(k \\in [K]\\). We denote Then the upper-bound\nregret Regret(k) is defined as follows\n\\[\n\\text{Regret}(k) \\stackrel{\\text { def }}{=} \\sum_{i=1}^{k} \\delta_{i, 1}\n\\]\nWe also define regret of every state and its corresponding upper bounds as\n\\[\n\\text{Regret}(k, s, h) \\stackrel{\\text { def }}{=} \\sum_{i=1}^k \\mathbb{I}(S_{i,h} = s) \\delta_{i,h}\n\\]\n\\[\n\\text{Regret}(k, s, h) \\stackrel{\\text { def }}{=} \\sum_{i=1}^k \\mathbb{I}(S_{i,h} = s) \\delta_{i,h}\n\\]"}, {"title": "B.3 Martingale difference sequences", "content": "We define the following martingale operator for every k \u2208 [K], h \u2208 [H] and F : S \u2192 R, let\nt = (k \u2212 1)H + h denote the time stamp at step h of episode k then\n\\[\nM_tF \\stackrel{\\text { def }}{=} P_{h}^\\pi F-F(S_{k,h+1}).\n\\]\nLet Ht be the history of all random events up to (and including) step h of episode k then we have\nthat E(MtF Ht) = 0. Hence MtF is a martingale difference w.r.t. Ht."}, {"title": "B.4 High probability events", "content": "Denote \\(\\iota = \\ln \\left(\\frac{6QOAT"}, {"iota}{3n}\n\\": "n\\[\nc_2(p"}, {"iota}{3n}\n\\": "n\\[\nc_3(n) \\stackrel{\\text { def"}]}, {"mathcal{P}": "forall q \\in Q\n\\]\n\\[\n|(p-\\tilde{p"}, "W (q", "o", "a)| \\leq \\min (c_1 (\\mathcal{W", "s", "a)", "H", "n)", "c_1 (\\tilde{\\mathcal{W", "h(s", "a)", "H", "n))\n\\]\n\\[\n|\\tilde{p", "z|o", "a) - p(z|o", "a)| \\leq c_2(\\tilde{p", "z|o", "a)", "n)\n\\]\n\\[\n||\\tilde{p", "cdot|o", "a) - p(\\cdot|o", "a) ||_1 \\leq C_3(n)\n\\]\n\\[\n|(p - \\tilde{p", "phi(q", "o", "a)| \\leq \u0441\u2084(1", "n)\\", "n\\]\nWe now define the random event \\(\\mathcal{E", "as follows\n\\[\n\\mathcal{E", {"0": "n\\[\n\\mathcal{E}_{az}(F, u, \\omega) \\stackrel{\\text { def }}{=} \\left{ \\left| \\sum_{s=1}^{t} M_s f_s \\right| \\leq 2\\sqrt{t u \\omega}\\right}\n\\]\n\\[\n\\mathcal{E}_{fr}(F, \\varpi, u, \\omega) \\stackrel{\\text { def }}{=} \\left{ \\left| \\sum_{s=1}^{t} M_s"}]