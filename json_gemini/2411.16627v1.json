{"title": "Inference-Time Policy Steering through Human Interactions", "authors": ["Yanwei Wang", "Lirui Wang", "Yilun Du", "Balakumar Sundaralingam", "Xuning Yang", "Yu-Wei Chao", "Claudia P\u00e9rez-D'Arpino", "Dieter Fox", "Julie Shah"], "abstract": "Generative policies trained with human demonstrations can autonomously accomplish multimodal, long-horizon tasks. However, during inference, humans are often removed from the policy execution loop, limiting the ability to guide a pre-trained policy towards a specific sub-goal or trajectory shape among multiple predictions. Naive human intervention may inadvertently exacerbate distribution shift, leading to constraint violations or execution failures. To better align policy output with human intent without inducing out-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS) framework that leverages human interactions to bias the generative sampling process, rather than fine-tuning the policy on interaction data. We evaluate ITPS across three simulated and real-world benchmarks, testing three forms of human interaction and associated alignment distance metrics. Among six sampling strategies, our proposed stochastic sampling with diffusion policy achieves the best trade-off between alignment and distribution shift.", "sections": [{"title": "I. INTRODUCTION", "content": "Behavior cloning [1] has fueled a recent wave of generalist policies [2], [3], [4] capable of solving multiple tasks using a single deep generative model [5]. As these models acquire an increasing number of dexterous skills [6], [7], [8] from multimodal\u00b9 human demonstrations, the natural next question arises: how can these skills be tailored to follow specific user objectives? Currently, there are few mechanisms to directly intervene and correct the behavior of these out-of-the-box policies at inference time, particularly when their actions misalign with user intent\u2014often due to task underspecification or distribution shift during deployment.\nOne strategy for adapting policies designed for autonomous behavior generation to real-time human-robot interaction is to fine-tune them on interaction data, such as language corrections [9]. However, this approach requires additional data collection and training, and language may not always be the best modality for capturing low-level, continuous intent [10]. In this work, we explore whether a frozen pre-trained policy can be steered to generate behaviors aligned with user intent specified directly in the task space through point goals [11], trajectory sketches [10], and physical corrections [12] (Figure 1)-without fine-tuning.\nWhile inference-time interventions in the task space offer a direct way to guide behavior, they can inadvertently exacerbate distribution shift-a well-known issue in behavior cloning that often leads to execution failures [13]. Prior works addressing this issue [14], [15], [16], [17] largely focus on single-task settings, limiting their applicability to multi-task policies. To overcome this limitation, we leverage multimodal generative models to produce trajectories that respect likelihood constraints [18], [19], [20], ensuring the policy generates valid actions even after steering. Specifically, we frame policy steering as conditional sampling from the likelihood distribution of a learned generative policy. The likelihood constraints learned from successful demonstrations allow us to consistently synthesize valid trajectories, while conditional sampling ensures that these trajectories align with user objectives. By composing pre-trained policies with inference-time objectives, we can flexibly adapt generalist policies to each new downstream interaction modality, without needing to modify the pre-trained policy in any way.\nTo evaluate the effectiveness of inference-time steering, we formulate discrete and continuous alignment metrics to capture human preferences in discrete task execution and continuous motion shaping. We study a suite of six methods for converting interaction inputs into conditional sampling on generative models. We identify an alignment-constraint satisfaction trade-off: as these methods improve alignment, they tend to produce more constraint violations and task failures. To address this, we propose an MCMC procedure [21] for diffusion policy [6] that alleviates distribution shift during interaction-guided sampling, achieving the best alignment-"}, {"title": "II. POLICY STEERING", "content": "In this work, we explore how to produce trajectories $\\mathcal{T}$ from frozen generative models that align with user intent specified either as discrete tasks (e.g. picking left or right bowl as shown in Figure 1) or continuous motions. For discrete preferences, we aim to maximize Task Alignment (TA) as the percentage of predicted skills that execute intended tasks. For continuous preferences, we aim to maximize Motion Alignment (MA) as the negative $L_2$ distance between generated trajectories and target trajectories. In addition to explicitly specified user objectives, we measure the percentage of generated plans that satisfy physical constraints\u2014implicit user intents such as avoiding collisions or completing tasks-referred to as the Constraint Satisfaction rate (CS). We define steering towards user intent as increasing TA or MA while maximizing CS. Specifically, maximizing CS is achieved through sampling in distribution of a pre-trained policy, while increasing TA or MA is achieved through minimizing an objective function $\\xi(\\tau, z)$, where user informs his intent through interactions $z$ to score the space of trajectories $\\mathcal{T}$. We consider the following three interaction types and objective functions.\nPoint Input. The first objective function $\\xi$ has a user specify a point coordinate on an image we wish to have a robot trajectory reach. Given a generated trajectory $\\mathcal{T} = (s_1, s_2,..., s_T) \\in \\mathbb{R}^3$, we map the specified pixel using the depth information in an RGB-D scene camera to a corresponding 3D state $z^{point} \\in \\mathbb{R}^3$. The alignment to user intent is then defined as minimizing the objective function:\n$\\xi(\\tau, z^{point}) = \\frac{1}{T} \\sum_{t=1}^{T} ||s_t - z^{point}||_2$,\n(1)\nwhich captures the average $L_2$-distance between all states in the generated trajectory and the target 3D state $z$. This objective function allows users to flexibly point goals in a scene, by specifying which objects to manipulate in a real-world kitchen environment (Figure 1).\nSketch Input. The next objective function $\\xi$ we consider allows a user to specify a more continuous intent, by generating a partial trajectory sketch $z^{sketch} \\in \\mathbb{R}^{T \\times 3}$ that we wish to have the robot follow. Given this sketch, we define $\\xi$ as:\n$\\xi(\\tau, z^{sketch}) = \\sum_{t=1}^{T} ||s_t - z^{sketch}_t||_2$.\n(2)\nWhen the sketch $z$ has a different length than generated trajectories $\\mathcal{T}$, we uniformly resampled $z^{sketch}$ to match the temporal dimension of generated samples. In comparison to the point input, this objective function allows users to specify shape preferences of a trajectory through a directional path in a robot's workspace (Figure 5).\nPhysical Correction Input. Finally, we consider an objective $\\xi$ which allows a user to specify intent through physical corrections $z^{nudge}$ on the robot. Minimizing the objective\n$\\xi(\\tau, z^{nudge}) = $\n(3)\ncorresponds to overwriting the beginning portion (e.g. first $k$ steps) of a trajectory $\\mathcal{T}$ with a user-specified $z^{nudge}$:\n$\\mathcal{T}= [z^{nudge}_1,..., z^{nudge}_k, s_{k+1},..., s_T]$.\n(4)\nCompared to previous interaction types, physical corrections intervene directly in the robot's motion execution (Figure 1)."}, {"title": "B. Inference-Time Interaction-Conditioned Sampling", "content": "Given an inference-time alignment objective $\\xi(\\tau, z)$ on trajectories $\\mathcal{T}$, we explore six methodsfor biasing trajectory generation to minimize this objective. The first three methods are applicable across generative models parameterized by $\\theta$, while the latter three specifically leverage the implicit optimization procedure within the diffusion process. Figure 2 illustrates these optimization procedures.\nRandom Sampling (RS). In the Random Sampling baseline, we sample a trajectory $\\tau \\sim \\pi_{\\theta}$ directly from the pre-trained model without any modification. This approach does not explicitly optimize any objective function $\\xi$, but serves as a baseline for trajectory generation.\nOutput Perturbation (OP). In Output Perturbation, we first sample a trajectory from $\\pi_{\\theta}$ and apply a post-hoc perturbation to minimize the objective $\\xi(\\tau, z^{nudge})$. We then resample from $z^{nudge}_k$ to complete the remainder of trajectory $\\mathcal{T}$. If a user cannot provide direct physical correction, the first $k$ states of a sketch input can be used as $z^{nudge}$. Although this sampling strategy maximizes alignment up to step $k$, it does not guarantee that synthesized trajectories from the perturbed state $z^{nudge}$ will be constraint satisficing.\nPost-Hoc Ranking (PR). In Post-Hoc Ranking, we generate a batch of N trajectories $\\{\\tau_i\\}_{i=1}^{N}$ from $\\pi_{\\theta}$ and select $\\tau^*$ that minimizes the objective $\\xi(\\tau, z^{point})$ or $\\xi(\\tau, z^{sketch})$. This approach performs well when at least one generated sample closely aligns with the input $z$, which may not hold if the robot is in a state without multimodal policy predictions.\nBiased Initialization (BI). In Biased Initialization, inspired by [23], we modify the initialization of the reverse diffusion process. Instead of initializing with a noise trajectory $T_N^2 \\sim \\mathcal{N}(0, I)$, we use a Gaussian-corrupted version of the user input $z^{point}$ or $z^{sketch}$ as $T_N$, bringing the process closer to the desired mode from the outset\u00b3. While this approach specifies user intent at initialization, the sampling process may still deviate from this input.\nGuided Diffusion (GD). In Guided Diffusion, we use the objective function $\\xi(\\tau, z)$ to guide the trajectory synthesis in the diffusion process [18]. Specifically, at each diffusion timestep $i$, given $z^{point}$ or $z^{sketch}$, we compute the alignment gradient $\\nabla_{\\tau_i}\\xi(\\tau_i, z)$ to bias sampling:\n$\\tau_{i-1} = A_i(\\tau_i - \\gamma_i(\\epsilon_{\\theta}(\\tau_i, i) + \\beta_i \\nabla_{\\tau_i}\\xi(\\tau_i, z))) + \\sigma_i \\eta$,\n(5)\nwhere $\\epsilon_{\\theta}(\\tau_i, i)$ is the denoising network, $\\eta \\sim \\mathcal{N}(0, I)$ is Gaussian noise, $\\beta_i$ is the guide ratio that controls the alignment gradient's influence, $\\alpha_i, \\gamma_i, \\sigma_i$ are diffusion-specific hyperparameters. This alignment gradient steers the reverse process toward trajectories aligned with $z$, potentially discovering new behavior modes in states where unconditional predictions would otherwise be unimodal and far from $z$. However, sampling with a weighted sum of denoising and alignment gradients in Equation 5 approximates sampling from the unnormalized weighted sum of the policy distribution and the objective function rather than their product [21], which can result in out-of-distribution samples (Figure 3).\nStochastic Sampling (SS). Finally, in Stochastic Samplingwe use annealed MCMC to optimize the composition of the diffusion model $\\pi_{\\theta}$ and the objective $\\xi(T_i, z)$ [21]. Here, the denoising function $\\epsilon_{\\theta}(T_i, i)$ at each timestep $i$ represents the score $\\nabla \\text{Vlog } p_i(\\mathcal{T})$ for a sequence of probability distributions $\\{p_i(\\mathcal{T})\\}_{0<i<N}$, where $p_N(\\mathcal{T})$ is Gaussian and $p_0(\\mathcal{T})$ is the distribution of valid trajectories in the environment. Simultaneously, the objective $\\xi(\\tau, z)$ defines an energy-based model (EBM) distribution $q(\\mathcal{T}) \\propto e^{-\\xi(\\tau, z)}$. Steering toward user intent then corresponds to sequentially sampling from $p_n(\\mathcal{T})q(\\mathcal{T})$ to $p_0(\\mathcal{T})q(\\mathcal{T})$, yielding final samples from $p_0(\\mathcal{T})q(\\mathcal{T})$ that are both valid within the environment and minimize the specified objective.\nWe implement this sequential sampling using the annealed ULA MCMC sampler, which can be implemented in a similar form to the guided diffusion code [21]. First, we initialize a noisy trajectory $T_N \\sim \\mathcal{N}(0, I)$, corresponding to a sample from $p_n(\\mathcal{T})q(\\mathcal{T})$. We then run $M$ steps of MCMC sampling at timestep $i$ using the update equation:\n$\\mathcal{T}_i = \\mathcal{T}_i - \\gamma_i(\\epsilon_{\\theta}(\\mathcal{T}_i, i) + \\beta_i \\nabla_{\\tau_i}\\xi(\\mathcal{T}_i, z)) + \\sigma_i \\eta$,\n(6)\nrepeated $M - 1$ times, followed by a final reverse step in Equation 5 to obtain a sample $\\tau_{i-1}$ from $p_{i-1}(\\mathcal{T})q(\\mathcal{T})$. These steps closely resemble reverse sampling in Equation 5 and can be implemented by modifying four lines in the guided diffusion code (Algorithm 1). To implement the sampling of Equation 6, we take the intermediate clean trajectory prediction $\\tau_0$ obtained via reverse sampling on $\\mathcal{T}_i$, followed by a forward diffusion step with noise level $i$ to update $\\mathcal{T}_i$. The addition of multiple reverse sampling steps at a fixed noise level better approximates sampling from a product distribution, as shown in Figure 3, producing samples that satisfy likelihood constraints and user objectives. Across our experiments, SS provides the most proficient policy steering."}, {"title": "III. EXPERIMENTS", "content": "We evaluate the effectiveness of inference-time steering methods in improving continuous Motion Alignment (MA) in Maze2D and discrete Task Alignment (TA) in the Block Stacking and Real World Kitchen Rearrangement tasks. Additionally, we report how steering affect Constraint Satisfaction (CS) among samples."}, {"title": "A. Maze2D - Continuous Motion Alignment (MA)", "content": "For continuous motion alignment, we use Maze2D [26] to evaluate whether a generative policy trained exclusively on collision-free navigation demonstrations can remain on a collision-free motion manifold when steered with sketches that violate constraints. To test the impact of the pre-trained policy class, we train a VAE-based action chunking transformer (ACT) [7] and a diffusion policy (DP) [6] on 4 million navigation steps between random locations in a maze environment. DP is trained with a DDIM [24] scheduler over 100 training steps ($N = 100$). The training objective focuses solely on modeling the data distribution (i.e., collision-free random walk) without any goal-oriented objectives.\nAt inference time, a given policy is kept frozen to benchmark various steering methods. We generate 100 random locations in the maze, each paired with a user sketch $z^{sketch}$ that may not be collision-free. These sketch inputs steer the generation of a batch of 32 trajectories per trial from the policy. For DP, the scheduler is allocated 10 inference steps, with a guide ratio of $\\beta_i<n = 20$ for GD and $\\beta_i<n = 60$ for SS where the MCMC sampling steps are set to $M = 4$. To incorporate $z^{sketch}$ in the OP sampling procedure, an early portion of the sketch is sampled to identify a non-collision state, resetting the starting location accordingly. To evaluate steering, we report the collision rate (1 - CS) and the $L_2$ distance between the sketch and the closest trajectory (Min $L_2$) or all trajectories (Avg $L_2$) per batch, which measures negative MA. Min $L_2$ shows the best alignment, while Avg $L_2$ captures the overall distribution alignment after steering.\nOur findings, illustrated in Figure 4, reveal a tradeoff between alignment and constraint satisfaction. Specifically, aggressive steering improves MA but reduces CS and increases collisions. Additionally, a policy with multimodal predictions (DP) combined with PR effectively improves alignment without exacerbating distribution shift. However, if the intended plan is absent from the initial sampled batch, PR cannot discover it (Figure 5). In contrast, a policy with unimodal predictions (ACT) cannot be steered to improve alignment with PR. If the policy lacks robustness (Figure 6), OP can introduce significant distribution shift. Finally, diffusion-specific steering methods can transform constraint-violating sketches into the nearest collision-free samples on the data manifold. Among these, SS achieves the best MA and CS tradeoff, as shown in Table I and Figure 5."}, {"title": "B. Block Stacking - Discrete Task Alignment (TA)", "content": "We evaluate discrete task alignment by testing whether a multistep generative policy, with multimodal predictions at each step, can be steered to solve a long-horizon task following a user-preferred execution sequence. For this, we design a 4-block stacking domain in the Isaac Sim environment [27]. The simulation initializes four blocks at random positions, and motion trajectories are generated using CuRobo [28]. The planner randomly selects blocks to pick and place, sometimes disassembling partial towers to rebuild them elsewhere. We train a DP (DDIM with $N = 100$) on 5 million steps from this dataset to learn a motion manifold of valid pick-and-place actions without goal-oriented behavior. As shown in Figure 7(a), the learned policy exhibits multi-modality across a discrete set of trajectories.\nAt inference time, we steer the policy to achieve a specific stacking sequence, completing a 4-block tower. To facilitate 3D steering, we develop a virtual reality (VR)-based system that allows users to provide 3D sketches within the simulation environment. In each interaction trial, the user observes the policy's unconditional rollouts before providing a sketch for conditional sampling. If the conditional sample with the smallest $L_2$ distance to the sketch corresponds to the intended block, the trial is considered successfully aligned. If the policy execution also succeeds, the trial is deemed constraint-satisfying. We report TA and CS across interaction trials for PR and GD with $\\beta_i<N = 25$ in Table II. Again, we see that higher TA correlates with lower CS.\nAdditionally, we experiment with a strategy to mitigate distribution shift during sampling with GD. Rather than keeping the guide ratio $\\beta_i$ constant for all $i = N,..., 1$, we deactivate steering by setting $\\beta_{i1} = 0$ for later steps. This approach aligns the low-frequency component of the noisy sample with user input in early diffusion steps while reverting to unconditional sampling after step I. Figure 7(c-d) demonstrate that the original GD produces a curved trajectory resembling the sketch, while the modified GD (I = 50) retrieves a straight-line trajectory from the CuRobo training dataset with the correct discrete alignment."}, {"title": "C. Real World Kitchen - Discrete Task Alignment (TA)", "content": "To evaluate inference-time steering of multistep, multimodal policies in a real-world setting, we construct a toy kitchen environment and generate demonstrations using kinesthetic teaching. We focus on two tasks: (1) placing a bowl in the microwave and (2) placing a bowl in the sink. For each task, we collect 60 demonstrations and combine them into a dataset to train a diffusion policy (DP) over 40,000 steps. Figure 8 illustrates that the learned motion manifold exhibits distinct multimodal skills based on the end-effector pose and gripper state. Unlike the block stacking experiment, merging datasets from different tasks introduces scenarios where skill sequences are not feasible\u2014for example, placing a bowl in the microwave before opening the microwave door. Therefore, in this context, the CS metric not only measures the success of individual skills but also evaluates whether the resulting sequence is valid as shown in Figure 9.\nAt inference time, users can steer execution towards a preferred, valid sequence by clicking a pixel in the scene camera view to specify the intended skill. The corresponding 3D location of the pixel is visualized with a red sphere that turns green upon activation of the steering input. We also experiment with physical corrections to the end-effector pose to trigger behavior switches, but as shown in Figure 10, this type of interaction often leads to execution failures.\nWe evaluate the effectiveness of GD, SS, and OP in enabling users to achieve specific sequences of discrete skills. During real-time policy rollouts (7 Hz), users observe a randomly sampled skill and select a different one through interactions. We report whether the interaction successfully causes the intended behavior switch and whether it results in successful execution in Table III. For GD, we use a guide ratio $\\beta_i = 5$ for all diffusion steps ($N = 100$), while for SS, $\\beta_i = 100$ is used. These choices are based on the observation that increasing the guide ratio for GD disrupts the diffusion process without improving alignment (Figure 11). In contrast, higher guide ratios for SS enhance alignment without producing noisy trajectories. Thus, GD with $\\beta_i = 5$ serves as a baseline for weak steering, while OP-allowing users to physically correct the robot end-effector trajectory during execution-functions as an aggressive steering baseline. Both GD and SS are steered with pixel inputs. In Table III, as alignment TA increases, the constraint satisfaction rate CS decreases. The best steering method (SS) has a higher failure rate than rolling out randomly (RS) but improves Aligned Success by 21% without any fine-tuning."}, {"title": "IV. RELATED WORKS", "content": "Learning for Human-Robot Interaction. Recently, learning from demonstrations [6], [7] has achieved significant success in robotic manipulation. Despite this progress, real-time human input is often absent during inference-time policy rollouts. To address this gap, natural human-robot interfaces [29], [30] have been employed when deploying robots in human environments. Various input forms, such as language, sketches, and goals [3], [31], [32], [33], [34], [9], have also been studied to convey human intent to robots. Inspired by [23], [35], our framework repurposes pre-trained generative policies for HRI settings, accommodating real-time human input. In this work, we focus on physical interactions, as they often provide grounding information that complements language prompts.\nLearning from Human Demonstrations. Generative modeling [5], [6], [36] has advanced imitation learning from multimodal, long-horizon demonstrations, enabling dexterous skill acquisition. Diffusion models [6], are particularly effective at capturing the multimodal nature of human demonstrations, with their implicit function representation allowing flexible composition with external probability distributions [18], [37], [21]. Previous research has explored using latent plans to support long-horizon tasks [38], [7], [39], but these focus on demonstrations with a single, high-quality behavior mode. In this work, we focus on generative modeling of multiple behavior modes [17], which is essential for enabling user interactions that require policies to adapt to inputs at inference time.\nInference-Time Behavior Synthesis. In robotics, inference-time composition has been explored as a method for achieving structured generalization [40], [18], [41], [42], [43], [44], [45]. Approaches like BESO [43] leverage learned score functions combined with classifier-free guidance to enable goal-conditioned behavior generation. Similarly, SE3 Diffusion Fields [45] use learned cost functions to generate gradients for joint motion and grasping planning, while V-GPS [46] employs a learned value function to guide a generalist policy through re-ranking. PoCo [47] synthesizes behavior across diverse domains, modalities, constraints, and tasks through gradient-based policy composition, supporting out-of-distribution generalization. Building on PoCo, our work investigates how different types of real-time physical interaction can effectively steer policy at inference time."}, {"title": "V. CONCLUSION", "content": "In this work, we propose the Inference-Time Policy Steering (ITPS) framework, which integrates real-time human interactions to control policy behaviors during inference without requiring explicit policy training. We demonstrate how this approach enables humans to steer policies and benchmark several algorithms across both simulation and real-world experiments. One limitation of our work is the reliance on an expensive sampling procedure to produce behaviors aligned with human intent. In future work, we aim to distill the steering process into an interaction-conditioned policy to achieve faster responses to human interactions and conduct a user study to further validate steerability. We hope this work sheds light on the integration of human interaction with learned generative policies."}]}