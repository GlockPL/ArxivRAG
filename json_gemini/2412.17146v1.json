{"title": "LLM Agent for Fire Dynamics Simulations", "authors": ["Leidong Xu", "Danyal Mohaddest", "Yi Wang"], "abstract": "Significant advances have been achieved in leveraging foundation models, such as large language models (LLMs), to accelerate complex scientific workflows. In this work we introduce FoamPilot, a proof-of-concept LLM agent designed to enhance the usability of FireFOAM, a specialized solver for fire dynamics and fire suppression simulations built using OpenFOAM, a popular open-source toolbox for computational fluid dynamics (CFD). FoamPilot provides three core functionalities: code insight, case configuration, and simulation execution. Code Insight is an alter- native to traditional keyword searching leveraging retrieval-augmented generation (RAG) and aims to enable efficient navigation and summarization of the FireFOAM source code for developers and experienced users. For case configuration, the agent interprets user requests in natural language and aims to modify existing simula- tion setups accordingly to support intermediate users. FoamPilot's job execution functionality seeks to manage the submission and execution of simulations in high- performance computing (HPC) environments and provide preliminary analysis of simulation results to support less experienced users. Promising results were achieved for each functionality, particularly for simple tasks, and opportunities were identified for significant further improvement for more complex tasks. The integration of these functionalities into a single LLM agent is a step aimed at accel- erating the simulation workflow for engineers and scientists employing FireFOAM for complex simulations critical for improving fire safety.", "sections": [{"title": "1 Introduction", "content": "Computational fluid dynamics (CFD) is an indispensable tool in the study and simulation of fire dynamics and combustion phenomena (Ren et al. [2017], Nmira and Consalvi [2022]). Among the various CFD solvers, FireFOAM\u00b3 (Wang et al. [2011]), a solver developed specifically for fire dynamics simulations, stands out for its ability to model complex fire and fire suppression physics, including buoyancy-driven turbulence, gas-phase chemical reactions, solid-phase heat transfer and pyrolysis, liquid film flow and spray transport. FireFOAM is built using OpenFOAM4, a popular C++- based open-source toolbox for CFD. A rendered image of a typical FireFOAM simulation of a large- scale fire suppression scenario in a warehouse is shown in Fig. 1a. Despite its advanced capabilities, the process of setting up, running and post-processing FireFOAM simulations remains a challenging and time-consuming task, especially on high-performance computing (HPC) architectures. Users must navigate a multitude of keywords, configurations, and code parameters to achieve accurate and reliable results, creating a steep learning curve for entry-level users and remaining time-consuming even for experienced users. A high-level overview of a typical FireFOAM case structure is shown in"}, {"title": "2 The FoamPilot agent", "content": "An overview of the FoamPilot agent is illustrated in Fig. 2. The agent is implemented using the LangChain/LangGraph 0.2 framework (Chase [2022]) and is designed to be agnostic to the choice of large language model (LLM). This design allows for flexibility in using local or cloud-hosted, open- source or closed-source LLMs, enabling the agent to leverage the ongoing performance improvements in these models, such as those trained for complex reasoning tasks (e.g., o1 from OpenAI [2024a], still in limited preview at the time of writing).\n\nThe agent structure follows a graph consisting of three nodes, as shown in Fig. 2: user, LLM, and tools, with edges connecting them to facilitate message transfer. The tool node provides access to utilities, which in the present work consist of a Shell Command Tool, a Python Interpreter Tool, and a Retrieval-Augmented Generation (RAG) tool. These tools were chosen due to their direct utility for the agent's desired functionalities. The Shell Command Tool executes Linux commands, the Python Interpreter Tool runs Python scripts generated by the LLM, and the RAG Tool identifies the embeddings closest to the query to retrieve information from the database, which, in this work, is a vector store of the FireFOAM source code. Further tools could have been added in the design of the"}, {"title": "2.1 Code Insight", "content": "A user of a simulation code, particularly a developer-user, will often need to refer to the source code to understand the details of sub-model implementations, as well as how to invoke those sub-models in the simulation configuration files. Traditional methods for navigating and understanding source code often rely on basic text search tools like 'grep' and 'find'. These methods can be inefficient and cumbersome because variable names and in-line comments depend on the code author's personal"}, {"title": "2.2 Case Configuration", "content": "FoamPilot interprets user requests for case configurations expressed in natural language. Currently, it is capable of modifying an existing simulation case provided by the user. Initially, we sought only to point the agent to the location of an existing case, allowing it to autonomously find and choose the necessary configuration files to modify to meet the user's request. We found that this was an inefficient approach, with the agent issuing calls to the Shell Tool to individually check every file within the case folder. The case folder for a FireFOAM simulation contains many individual files, as shown in Fig. 1b.\n\nTo address this, we applied an approach similar in spirit to the RAG Tool described in Section 2.1: we stripped out all boilerplate headers and license information, prepended the relative file path to each configuration file, then compressed the entire case folder into a single long string. We then provided the case configuration string to the agent within the prompt. For FireFOAM simulations, the"}, {"title": "2.3 Job Execution", "content": "Running FireFOAM simulations in a Linux environment can be challenging for entry-level users, particularly since large-scale simulations are often conducted in an HPC environment using a job scheduler like SLURM (Yoo et al. [2003]), which they are unlikely to have previously used. Here, we sought to develop a functionality that would allow FoamPilot to handle the execution of simulations. We sought to be able to execute without a scheduler, such as on a local machine or on the head node of an HPC system as a serial job, as this is a common configuration for debugging and testing purposes. We also sought to be able to execute using a job scheduler to run large simulations on numerous multi-core HPC nodes.\n\nWhen running a FireFOAM simulation locally or on the head node, the agent must prepare the mesh and execute the simulation directly on the command line. If requested by the user, the agent can also provide a preliminary analysis of the results once the simulation has completed. When running on an HPC system with a job scheduler like SLURM, the agent must first execute commands like scontrol to identify available resources. It must then prepare the mesh and determine the size of the mesh. Then, it must perform domain decomposition based on the number of nodes available and the size of the mesh, and finally write a job submission script and submit the job to the queue.\n\nThe Job Execution functionality was achieved by providing detailed instructions through prompting, which cause the agent to use the Shell Tool to fulfill the request. The prompts used for serial and HPC jobs are provided in Fig. 5. For the HPC job, as an additional challenge, the selection of the number of nodes and cores based on the mesh size was left to the agent."}, {"title": "3 Experimental results and discussion", "content": "To ensure consistency across all our experiments, we utilized the same LLM, version, and temperature setting: Azure/OpenAI's GPT-40, version 2024-05-13, with a temperature setting of 0.0. The LLM was chosen due to its performance on reasoning tasks and its apparent familiarity with some aspects of the OpenFOAM toolbox. Despite efforts to maintain reproducibility, the agent still exhibits some variability in success rates when completing tasks with the same user query under identical conditions- a temperature setting of 0.0 does not guarantee deterministic results. Therefore, each experiment was repeated five times to assess stability. A single user prompt was used in each experiment, and the same prompt was used in each repeat. All of the experiments considered are unambiguous and have a single correct outcome, where a successful outcome is determined by comparing the agent's actions to that of an experienced FireFOAM user. The results are summarized in Table 1.\n\nA system prompt was used to define the model's role, behavior, and objectives, and to inform it what tools it has access to, thereby guiding it to produce relevant, accurate, and safe responses. It also helped eliminate redundant content from user queries. We implemented a system prompt inspired by Chase [2024], shown in Fig. 6.\n\nGiven the exploratory nature of this project, and given the Shell Tool's ability to execute arbitrary commands, there existed a risk of causing damage to the system on which it was run during our experimentation. Thus, all experiments were conducted on a dedicated AWS EC2 instance, and HPC jobs were submitted to a dedicated AWS parallelCluster using the SLURM job scheduler. FoamPilot is a small Python-based code, and we employed a cloud-hosted LLM in our experiments, thus the system requirements for the instances were driven entirely by the FireFOAM code. We note that"}, {"title": "4 Conclusions and future work", "content": "An LLM agent, FoamPilot, was developed as a proof-of-concept with the aim of reducing the complexity and time required for source code navigation, simulation setup and simulation execution, thereby making fire dynamics simulations more accessible and efficient for both new and experienced users. In our exploration, we found that the agent was consistently successful for tasks of low complexity, but that its success rate dropped precipitously with increasing task complexity.\n\nImportant functionalities that were not addressed in this work include the ability to run simulations asynchronously, which requires the LLM agent to save and recover its state between sessions. This would be particularly useful to users that are less familiar with the HPC environment and workflow. In addition, a robust ability for optional human feedback during FoamPilot's operations was not achieved due to technical challenges with the chosen agentization framework. A request for human approval of any tool usage was included as a necessary safety precaution during testing; however, challenges arose in the LangChain/LangGraph framework with respect to the solicitation and inclusion of substantial human feedback between tool usages by the LLM agent.\n\nOur experiments considered only GPT-40 for the agent's LLM, and it is plausible that there are LLMs available at the time of writing that would have achieved better results in our experiments. We anticipate greater success for case configuration tasks when reliably-structured LLM outputs are combined synergistically with non-AI case configuration tools that embed expert knowledge. We expect that near-future LLMs specialized for reasoning tasks will perform better on the multi-step, complex tasks required for setting up and executing simulations. Furthermore, we expect that present and future LLM-enhanced developer tools, such as GitHub CoPilot and Cursor AI Code Editor, will certainly outperform our implementation of the Code Insight functionality, since this functionality is useful for all software developers, not just FireFOAM developers. Indeed, the challenges identified and the solution approaches developed in this work are not unique to FireFOAM, they are applicable to many scientific simulation workflows. The development of generalized frameworks for LLM agents to interact with and control simulation software will be beneficial.\n\nLooking ahead, we note that an increasing number of large language models are becoming multimodal, capable of processing image data as input. We believe this capability could improve the agent's understanding of simulation configurations if the geometry and mesh are presented visually to provide further context regarding the simulation at hand.\n\nLastly, we found that the limited domain-specific knowledge of general-purpose LLMs reduces FoamPilot's ability to handle complex tasks using FireFOAM, particularly those involving multiple functionalities. We note that FireFOAM's codebase is itself around 1M tokens, whereas the Open- FOAM toolbox on which it is built is approximately 10x larger. Thus, FireFOAM, or large parts of it, may fit in the expanded context windows of future LLMs. Additionally, continued pre-training on the OpenFOAM or combined FireFOAM/OpenFOAM codebase may potentially allow accurate zero-shot prompting for analyzing and supporting further code developments."}]}