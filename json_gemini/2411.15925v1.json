{"title": "Making Images from Images: Interleaving Denoising and Transformation", "authors": ["Shumeet Baluja", "David Marwood", "Ashwin Baluja"], "abstract": "Simply by rearranging the regions of an image, we can create a new image of any subject matter. The definition of regions is user definable, ranging from regularly and irregularly-shaped blocks, concentric rings, or even individual pixels. Our method extends and improves recent work in the generation of optical illusions by simultaneously learning not only the content of the images, but also the parameterized transformations required to transform the desired images into each other. By learning the image transforms, we allow any source image to be pre-specified; any existing image (e.g. the Mona Lisa) can be transformed to a novel subject. We formulate this process as a constrained optimization problem and address it through interleaving the steps of image diffusion with an energy minimization step. Unlike previous methods, increasing the number of regions actually makes the problem easier and improves results. We demonstrate our approach in both pixel and latent spaces. Creative extensions, such as using infinite copies of the source image and employing multiple source images, are also given.", "sections": [{"title": "1 Introduction and Background", "content": "The generation of images that change their appearance under specific transforms, such as rotations, tile rearrangement, and flips, has captivated the minds of visual artists for centuries exemplified by Salvador Dal\u00ed and M. C. Escher.\nOften grouped broadly as optical illusions, two prominent early examples include the work of Giuseppe Arcimboldo (1590), shown in Figure 1A, and the classic example of the rabbit and duck illusion, shown in Figure 1B. Beyond their artistic appeal as intriguing visual puzzles, these and other optical illusions have been studied for understanding human perception, including intrinsic biases and fundamental cognitive processes [7,22,39]. [19,26] found that modern large-scale image/text embedding systems are also susceptible to illusions (a type of adversarial example [9, 14, 15]).\nWithin the computer vision community, the quest to create complex visual illusions that can trick the human visual system has encompassed many approaches. For example, hybrid images combine the high frequencies from one image with the low frequencies from another to create a hybrid image that appears as one object when viewed close-up and another when viewed from afar [28]. Other work includes creating solids that can be interpreted as different objects"}, {"title": "2 Diffusion and Constrained Optimization", "content": "To address the task of using a static source image for creating a new image, we first reformulate the more general task of simultaneously denoising two non-static images. We employ a diffusion process [16] common to text-to-image systems that denoises an image over T steps, starting from pure noise: $x_{T} \\sim N(0, 1)$. t proceeds from T down to 0. $x_t$ is the input to step t which produces $x_{t-1}$, an image with less noise, as the input to the next step. $x_0$ is the final output of diffusion: the fully denoised image. By incorporating deep neural networks with classifier free guidance [17] to predict the noise at each step, the diffusion process gradually changes pure noise into an image described by a text prompt, p.\nWe base our system on Visual Anagrams [12] which uses diffusion to synthesize N images from N different prompts such that the images at step t are the same under the inverse of each transformation $\u03c8^{i}$, denoted $i^{-1}$. The corresponding pixels in the images, as determined using $\u03c8^{i}$, are exactly the same. For simplicity, going forward, we will define the first of N diffusion inputs to be the input image, $x_t$. The remaining diffusion inputs can be derived from the first as $L_i(x_t) = \u03c8^{i}(1^{-1}(x_t))$ where $1 \u2264 i \u2264 N$ (1 is identity); see Figure 3. In [12], they a priori select the transforms, \u03c8. \u03a4\u03bf synchronize the N parallel diffusion processes, the computed guidance for each prompt is averaged at each step.\nFor the majority of this study, we will concentrate on transforms, \u03c8, that divide the image into M \u00d7 M tiles and permute the tiles intact rather than treating pixels individually.\nAs a baseline, we show results obtained by using the approach in [12] using N = 2, see Figure 4. As in [12], 1 is an identity function and $\u03c8^{2}$ is a randomly chosen tile permutation. Also, to match their studies, we use the pre-trained diffusion model DeepFloyd [35] that operates directly on pixels. Our qualitative observations of the results match those in [12]: prompts that allow more room for interpretation, such as \"a painting of..\" perform better than those with strict expectations, e.g. \"a photograph of..\". Additionally, the broader the query, the higher the likelihood of a pleasing result, e.g. the pair (\u201ca car\u201d and \u201ca tree\") will appear better than the pair (\u201ca red Mustang\u201d and \u201ca willow tree\u201d)."}, {"title": "2.1 Simultaneously Learning Permutations and Images", "content": "It is impossible to know a priori what the suitable permutations for a set of prompts will be. We will develop a procedure that allows the transformation permutation to change such that the permutations are selected that best match the images. Before proceeding, it is important to state the intrinsic \"chicken-and-egg\" problem. If the two final images were known, we could calculate the best permutation in advance. However, the two images are being synthesized simultaneously along with their permutations. Therefore, we need to repeatedly re-optimize the permutation as the image is synthesized.\nWe find the permutation using dynamic matching. Consider two images, each composed as a grid of M \u00d7 M non-overlapping tiles of equal size. The distance between tiles is their $L_2$ pixel distance, though more elaborate mechanisms such as perceptual similarity metrics can also be employed [1]. The goal is to assign"}, {"title": null, "content": "each tile in the first image to exactly one distinct tile in the second such that the total summed distances of assignments is minimized. This can be characterized as a complete bipartite graph (S,T; E) where vertex sets S and T represent tiles in the first and second images and E has an edge distance equal to the distance between its endpoint tiles. The solution is a perfect matching (a matching where each vertex has exactly one edge) that minimizes the total edge distance. This formulation will be used again in Section 5.1.\nTo compute this, the tiles of each image are flattened to an array of length $M^2$ and the distances between every tile in the first image and every tile in the other image are arranged in a matrix, D. We permute the rows of D to minimize its trace.\n$\n\\min_{P} \\text{tr}(PD)\n$\nwhere P is a permutation matrix, a (0,1)-matrix with exactly one 1 in each row and at most one 1 in each column, with the same shape as D. This assignment problem is optimally solved in polynomial time using the Kuhn-Munkres algorithm [20,25]. $(Q)$ implements the tile assignment P on the pixels of Q.\nWe incorporate dynamic matching as part of the \"mainline\", the sequence of \"Visual Anagrams steps\" described so far. Its role is to update the matching tiles between Visual Anagrams steps as the images are created. Specifically, dynamic matching updates & at step t, referred to as $Vt$ (and similarly $It$), to $Vt-1$. A naive approach is to simply recalculate the assignment in 4 at each Visual Anagrams step; however, this was unstable - likely because matching noisy images in the beginning (closer to t = T) resulted in unhelpful assignment changes.\nTo address this, we introduce a \"rollout\" at the beginning of each mainline step. A rollout is an entirely separate diffusion process starting from $(x_t)$ and stepping forward I steps as a form of lookahead approximation to the a posteriori image for each prompt. (Numerous values were tested for l and, experimentally, we found a lookahead of 5 steps was sufficient.) This approximation outputs an \"idealized\" image to show where each prompt would lead if used in diffusion independently. We then use dynamic matching on the idealized images to compute"}, {"title": "3 Using a Known Source Image", "content": "In the previous section, dynamic matching allowed us to generate multiple images from multiple prompts while progressively discovering the optimal tile assignments between them. Here, we exploit this new freedom to a priori specify the tiles that all the images must contain. The tiles can be taken from any \"source\" image; if the Mona Lisa is used, then the generated images will only use its exact tiles.\nStarting with a source image changes the task from finding the RGB-pixel values to rearranging the tiles of the source image to create a new image described by the specified prompt. On first glance, this may seem like a trivial problem: given the Mona Lisa, find the tiles that best match another image, such as an image of a dog. However, this is not what is being done here. Instead, we are"}, {"title": null, "content": "creating an image of the dog that is well suited to the images allowed by the (e.g. 162! for the 16 \u00d7 16 tile case) permutations of the Mona Lisa's tiles.\nTo use a source image, B, as the basis for the image creation, we start with the method described in the previous section. Since we no longer need to determine the actual values of the pixels (only where the pixels will be moved from the source), we can simplify the process. Within the context of the procedures shown thus far, we set the first image to \u1e9e and the prompts start at i = 2; see Figure 8. The salient differences between the previous procedures and the current are:\n1. $\u03c8^1$ is set to the identity function. It is no longer an input since all rollout inputs can be derived as $I_t(\u03b2)$.\n2. Given t, we directly compute $\u03a8_i(\u03b2)$ for initialization. Recall that $\u03a8_i(\u03b2)$ is a permutation of a noise-free source image. For a rollout to be useful (since rollouts use diffusion internally), it expects a noisy image, not one that is noise-free. Therefore, we re-add noise to $\u03a8^i(\u03b2)$ to create the rollout input, a process termed \"rollout mixing\". We can characterize rollout mixing as:\n$\nwimage \\Psi^{i}( \\beta)+ \\wnoise \\epsilon$\nwhere $ \\epsilon \\sim N(0, 1)$ is noise. $w_{image}$ and $w_{noise}$ for step t are computed directly from the diffusion schedule (see [16,32] for details). An effect of the diffusion schedule is that, the earlier in the mainline process, the more weight is given to $w_{noise}$, allowing larger changes.\n3. We perform each rollout (see Figure 8) to completion, which yields an idealized noise-free image that we call Q. The tiles in $Q_i$ are matched with \u1e9e revealing the new $\u03c8_{t-1}$. This is analogous to the matching process earlier, where we matched to the first idealized image.\n4. Once dynamic matching has computed the new assignment of tiles, there is no further need to adjust the pixel values. They are directly taken from \u03b2. The Visual Anagram step is no longer required."}, {"title": "4 Working in Latent Spaces", "content": "To this point, we have used DeepFloyd to allow direct comparison to [12], but its pixel-based representation of x is uncommon in modern diffusion systems. Since latent diffusion-based systems are rapidly progressing in quality, we have also extended our system to work with latent diffusion systems, specifically Stable Diffusion's version 2.1 (SD 2.1) [31]. Rather than operating directly on pixels, the diffusion of $x_t$ is in a more compact \"latent\" representation that can be encoded and decoded from/to pixels by using a pre-trained encoder and decoder, both of which are a core part of all pre-trained latent-diffusion systems.\nUpon first glance, the simplest approach in using a latent diffusion system would be to simply translate all the mechanisms with DeepFloyd directly, and work entirely in latent space, with a decode only for the final output. However, latent diffusion introduces two significant challenges. First, as witnessed in [12], transforms on the latent representation may not preserve spatial characteristics. This problem is exacerbated by dynamic matching, which also occurs in the pixel-space. Second, latent diffusion is hyper-sensitive to early stages of denoising; rolling out from intermediate steps produces little change in the idealized image.\nWe closely follow the steps in Section 3 and introduce modifications as necessary. As before, the rollout mixing step adds noise before the start of each rollout. Each rollout proceeds 50 steps, S = 50. (While 50 steps is more than we used with DeepFloyd, we empirically found this to be necessary with latent diffusion.) We keep the mainline steps at T = 15, sufficient to reach convergence. As before, the output of each rollout is labeled Q and it is a noise-free idealized image.\nExperimentally, we found that latent diffusion solidifies many characteristics of the final image in the very early diffusion steps. Starting a rollout with higher fractions of the image (as opposed to noise), as we did with DeepFloyd, causes output Qt to be too similar to the rollout input. This prevents rollouts from creating novel images and eliminates their usefulness. To allow the rollout enough freedom to create new images, we always start rollouts from the beginning of the diffusion process (i.e. s = 50 \u2192 s = 0). To do this, we start with rollout mixing:\n$\nwimage \\Psi^{i}(x_{t}) + \\wnoise \\epsilon$ using a \"mixing ratio\" $\\frac{w_{image}}{w_{image}+w_{noise}}$ of 2%. At 2%, the image provides enough direction when combined with the noise to influence the rollout, but not too much so as to limit the rollout's ability to change the input. Numerous other values were tried; ranges of 1 - 4% were most often successful.\nOnce the idealized image has been created, it is again tempting to simply translate all the mechanisms with DeepFloyd directly, and work entirely in latent space, with only a decode for the final output. However, the dynamic matching"}, {"title": null, "content": "is spatial over pixels - i.e. in the decoded image space. To perform the matching, we must, therefore, decode the output of each rollout to produce the pixels for Q, the input to dynamic matching. See Figure 10. 5\nWe note that it is important to consider when to add noise (Figure 10). One choice is to add noise to the pixel image and then encode; however, $E(L_i(w_{image} x_t + w_{noise}\\epsilon))$ doesn't guarantee the noise component is preserved as N(0, 1) in latent space. Instead, we add noise after encoding. The input to each rollout is therefore: $w_{image}E(I_i(x_t)) + w_{noise}\\epsilon$.\nAn additional implementation detail: when using dynamic matching, the first image $Q_1$ is used to produce $\u03a8_{t-1}$ (see red box in Figure 10). We experimented with using $\u03a8_1(Q_1)$ as the output of the mainline step. However, we found that using an average of the rollout outputs worked a bit better:\n$\n\\Psi_{t-1}(Q_t) = \\frac{1}{N} \\sum_{i=1}^{N}(\\Psi^{i-1}(Q_{i}))\n$\nThis may be due to the specifics of the diffusion process (SD 2.1) and may be switched back when other systems are employed."}, {"title": "4.1 Known Source Image", "content": "Analagous to Section 3, we describe how to use an a priori specified source image within a latent diffusion-based system. We continue to use the same overall settings as the latent diffusion described above: we retain the full independent rollouts of length S = 50 as well as the 2% rollout mixing ratio.\nIn order to accommodate the source image, we note that $Q_1$ is set to \u03b2. As earlier, there is no rollout required for the source image. Importantly, unlike Section 3, where we switched to explicitly adding noise back into \u03a9 (Figure 8), we note that the latent diffusion system already uses this approach, so this"}, {"title": "5 Creative Extensions", "content": "In Section 2.1, we characterized the tile assignment as a perfect matching. However, when using a fixed source image, we can consider an interesting variant - what if we had c (where c could be infinite) copies of the source image, \u03b2, and could select the tiles to use from any of the copies? In this scenario, as before, every tile in the permuted image must be assigned to a tile in \u1e9e, but not every tile in \u1e9e needs to be used. This can be characterized as a matching in a bipartite graph (S,T; E) where vertices S contains c vertices per fixed image tile and T contains the tiles of the permuted image. The solution is a (non-perfect) matching on the graph such that every vertex in T is part of the match.\nRevisiting Equation 1, let Dc be a $cM^2 \u00d7 M^2$ matrix in which the the rows are duplicated c times. The $M^2 \u00d7 cM^2$ assignment from the permuted image to the source is, as before, a (0,1)-matrix Pc with exactly one 1 in each row and at most one 1 in each column. We still have:"}, {"title": null, "content": "$\n\\min_{P_c} \\text{tr}(P_cD_c)\n$\nHere, we define the trace of a non-square matrix A with shape $(a_1,a_2)$ as $\\text{tr}(A) = \\min(a_1,a_2) \\sum^{min(a_1,a_2)}_{i}$. The solution is implemented as the non-square variant of the Kuhn-Munkres algorithm. Results are shown in Figure 14 (see column copies=5 & copies=10). Finally, as a further extension to this approach, we note that c can be set per tile, thereby allowing fine-grained control over the specific tiles that should appear more often in the results.\nA straightforward variation of using multiple copies of the same image as the source material is using multiple different images as the source material. This easily fits into the framework presented in this section with minimal changes; selections of tiles from any of the basis images are allowed for each position in the synthesized image."}, {"title": "5.2 Alternate Transformations", "content": "To this point, we have focused upon the Permutation transformation. This transformation worked particularly well since using the Kuhn-Munkres algorithm provided a known, and efficient, method for matching tiles in multiple images.\nHere, we demonstrate that the procedure of interleaving diffusion's denoising step with energy minimization matching is general and works across other transformations. We will demonstrate this using Concentric Rings and Flips. For both, we are able to use simple greedy and efficient matching procedures.\nIn the Concentric Rings transformation, we place C concentric rings, each with equal radial size, around the center of the image. The rings can be rotated independently to transform image A into image B, see Figure 15b. Analogously to the work in the previous sections, the rotations of the rings are not pre-specified. Instead, the best rotation of the rings is computed at every mainline step using the rolled-out images. Differences between corresponding rings in image A and B are computed and their sum is minimized."}, {"title": null, "content": "Results are shown in Figure 16, middle three rows. Here, we evaluated 72 rotations for each ring (in 5\u00b0 increments) independently of the other rings. The larger the number of rings that were used, the clearer the results.\nFor the Flip transformation, we experimented with tile rotation and mirroring effects at multiple levels in the image pyramid. Given a division, d, we divide the original square image into d x d equal sized blocks. The allowed manipulations for each block are a flip (horizontally) and rotation by $(0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0)$. Rather than doing this at a single level, we allow this to happen at several divisions, D. For example, a division set D = (1,5), where the full image can be modified as well as at the boundaries, might yield a result as shown in Figure 15c.\nResults are shown in Figure 16, bottom two rows. For simplicity, we only allowed two resolutions of operation: the entire image, and the lowest resolution (shown in each row). This first provides a mechanism to make a global change, and the second a mechanism for smaller, local, changes.\nThough it is difficult to make quantitative, general, statements on these quali- tative results, we provide a few observations. In general, the original Permutation transformation provided the most compelling results. The permutations of tiles has two attributes that we suspect are important. First, the permutation itself provides a substantial amount of freedom of movement. Blocks can be moved throughout the image, thereby allowing very different images to be created from the same set of pixels. Second, the movement keeps together most spatially localized pixels. As natural images exhibit a high amount of spatial cohesiveness, moving entire blocks preserves patches of consistent colors and textures.\nThe Concentric Rings transformation does substantially better with more rings. However, note that each ring rotation (especially in the outer rings) controls the position of spatially disparate pixels and the region of a single ring manipulation may extend the full width of the image. This necessitates creating images using potentially unnatural constraints.\nThe Flips transformation, on the other hand, maintains spatial cohesiveness. In the examples shown, we allowed flips & rotations at full and at either 8 \u00d7 8 or 16 \u00d7 16 blocks. In our initial trials, we experimented with not including the full level transformation. If the top level was omitted, the system often failed in creating two recognizable distinct images. In the extreme, although very fine- grained tiling adds parameters to the transformation, it results in excessively local changes that make it impossible to create noticeably different images - there is an empirical compromise between d = 1 global changes and very fine-grained tiling. Allowing the d = 1 global change also helped ameliorate locality effects. Nevertheless, the constraints on this transformation are far more restrictive than on the Permutation transformation."}, {"title": "6 Conclusions & Discussion", "content": "Simply by rearranging an image's tiles, we presented methods to transform an image into a novel one of any subject matter. This method extends and improves recent work in generation of optical illusions in four fundamental aspects."}, {"title": null, "content": "1. The arrangement of tiles need not be a priori specified. Rather, through a dynamic matching step, the arrangement is jointly created with the image. Beyond creating qualitatively and quantitatively better images, this opens new functionality.\n2. A source image can be pre-specified. Previously, all images had to be created by the process to allow their content to be matched. Dynamic matching removes that constraint, allowing a user-specified source image.\n3. The problem becomes easier as the number of tiles grows; the opposite was true with static matching.\n4. An infinite number of copies of the source image can be used in matching. Multiple different images can also be used for the source.\nWe have extended this study to parameterized transforms beyond randomized- patch permutations. The interleaving of energy minimization and de-noising applies equally well to a variety of transforms.\nLooking forward, we note that though this work presented results on a vision-processing / graphics task, in a broader sense, the system was a method to navigate a system of constraints. Understanding the use of the diffusion process to search through the valid spaces in this potentially alternate category of problems remains an exciting prospect for future work."}, {"title": "7 Appendix", "content": null}]}