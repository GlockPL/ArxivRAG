{"title": "Game-theoretic LLM:\nAgent Workflow for Negotiation Games", "authors": ["Wenyue Hua", "Ollie Liu", "Lingyao Li", "Alfonso Amayuelas", "Julie Chen", "Lucas Jiang", "Mingyu Jin", "Lizhou Fan", "Fei Sun", "William Wang", "Xintong Wang", "Yongfeng Zhang"], "abstract": "This paper investigates the rationality of large language models (LLMs) in strategic\ndecision-making contexts, specifically within the framework of game theory. We\nevaluate several state-of-the-art LLMs across a spectrum of complete-information\nand incomplete-information games. Our findings reveal that LLMs frequently\ndeviate from rational strategies, particularly as the complexity of the game increases\nwith larger payoff matrices or deeper sequential trees.\nTo address these limitations, we design multiple game-theoretic workflows that\nguide the reasoning and decision-making processes of LLMs. These workflows\naim to enhance the models' ability to compute Nash Equilibria and make ratio-\nnal choices, even under conditions of uncertainty and incomplete information.\nExperimental results demonstrate that the adoption of these workflows signifi-\ncantly improves the rationality and robustness of LLMs in game-theoretic tasks.\nSpecifically, with the workflow, LLMs exhibit marked improvements in identifying\noptimal strategies, achieving near-optimal allocations in negotiation scenarios,\nand reducing susceptibility to exploitation during negotiations. Furthermore, we\nexplore the meta-strategic considerations of whether it is rational for agents to\nadopt such workflows, recognizing that the decision to use or forgo the workflow\nconstitutes a game-theoretic issue in itself.\nOur research contributes to a deeper understanding of LLMs' decision-making ca-\npabilities in strategic contexts and provides insights into enhancing their rationality\nthrough structured workflows. The findings have implications for the development\nof more robust and strategically sound AI agents capable of navigating complex\ninteractive environments. Code and data supporting this study are available at\nhttps://github.com/Wenyueh/game_theory.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as GPT-4 and Claude, have achieved remarkable progress in\nnatural language understanding and generation [1, 2, 3], driving advancements in fields ranging from\nconversational AI [4, 5] to content creation [6, 7] and agentic task delegation [8, 9, 10]. LLMs are\nincreasingly integrated into applications that influence everyday activities, such as planning, acting,\nand decision-making. Therefore, the ability of LLMs to navigate complex situations has significant\nimplications for their deployment in applications requiring strategic interaction, such as automated\nnegotiations, economic modeling, and collaborative problem-solving [11, 12, 13, 14, 15].\nDespite the wide exploration and utilization, LLM's capacity for rational behavior, particularly in\nstrategic settings represented by game theory, remains an open question [16, 17, 18, 19, 20]. In this\ncontext, rationality implies an agent's ability to make decisions that maximize expected utility based\non available information, an essential component of intelligent and adaptive decision-making. In\nthe realm of game theory, rational agents are expected to act strategically, considering not only their\nown preferences but also the potential actions and preferences of others. This is especially critical\nin incomplete-information games, where uncertainty about other players' information necessitates\nsophisticated reasoning and belief updating.\nThis paper investigates the capacity of LLMs to behave rationally in game-theoretic scenarios and\nexplores methodologies to enhance their rational decision-making capabilities. We begin by assessing\nthe performance of several state-of-the-art LLMs, including Claude-3.5 Sonnet, Claude-3 Opus,\nGPT-40 and o1 [21], in both complete-information and incomplete-information games such as the\nPrisoner's Dilemma, Battle of the Sexes, the Escalation Game, and Deal-or-No-Deal [22], presented\nin Figure 1. Our analysis reveals LLMs often deviate from rational strategies, particularly as the\ncomplexity of the game increases with larger payoff matrices or deeper sequential trees (Section\n4). They also exhibit a lack of robustness to noise and uncertainty, leading to suboptimal outcomes\n(Section 6).\nTo address these limitations, we introduce a novel approach by proposing game-theory-inspired\nworkflows specifically designed to guide the reasoning and decision-making processes of LLMs.\nThis is the first attempt to systematically integrate classic game-theoretic strategies into LLM-\nbased agent workflow, aiming to enhance their rational behavior and decision-making capa-\nbilities in strategic settings. These workflows incorporate principles such as Dominant Strategy\nSearch, which involves identifying strategies that yield the highest payoff regardless of the opponent's\nactions; Backward Induction, a method of solving extensive-form games by analyzing them from the\nend states backward to the initial decision nodes to determine optimal strategies; and Bayesian belief\nupdating, which allows agents to refine their beliefs about other players' valuations based on observed\nactions and signals during the game. Cringed on these well-defined and well-studied game-theoretic\nmethods, we design algorithms to guide the behavior and thinking process of LLM-based agents.\nAdditionally, we integrate fairness considerations like envy freeness and pareto optimality, which\npromote equitable and efficient outcomes in negotiations by ensuring that no agent prefers another\nagent's allocation to their own and that no improvements can be made without making at least one\nagent worse off."}, {"title": "2 Related Work", "content": "LLMs in game-theoretic environments Understanding strategic behaviors of LLMs entails\nimportant societal ramifications, as online users increasingly rely on intelligent assistants to in-\nteract with other agents, potentially also LLMs. To characterize these behaviors, prior studies\n[23, 24, 19, 18, 25, 26, 27, 28] often adopt game theory, a mathematical framework that models\ncooperative behaviors of humans. These analyses involve comparing qualitative behaviors of LLM\ninteractions against stylized entities, such as Pareto optimal solutions and subgame-perfect equilibria\n[29]. [28, 30] observe LLMs to perform in games driven by self-interest, but falter in those that\nrequire coordination, a behavior that emerges under altruistic and/or submissive personalities [25, 28].\n[31, 32] observe different LLM families to exhibit varying levels of risk tolerance. Deferring details\nto experiments, we identify another source of brittleness as LLMs behave poorly when presented with\nnumerically perturbed payoffs, even if they do not alter qualitative solutions to the game of interest.\nIn short, there lacks a system that elicits optimal behaviors for LLMs in game-theoretic settings.\nEnhancing LLMs to solve games [23] is a representative approach that aims to elicit game-solving\ncapabilities in LLMs. It models natural dialogues as incomplete-information games, and synthesizes\noptimal actions by instructing an LLM to respond with specific personalities. [33] proposes an\nLLM-based self-play algorithm that emulates Monte-Carlo Tree Search to solve zero-sum games.\nBroadly, these methods belong to a family of prompting strategies [34, 35, 36] that tackle decision\nmaking instances. Our method is no exception, but differs as we imbue classical game theory in\nLLMs to allow for fine-grained control and analyses at each information state.\nGame-theoretic testbeds Stylized games such as Battle of Sexes, Prisoner's Dilemma, Rock-\nPaper-Scissors, Stag Hunt, and Ultimatum Game have been extensively analyzed in the context of\nmulti-agent systems [24, 25, 26, 31]. They represent a minimal setting characterized by small action\nspaces, limited number of terms, and the existence of analytical equilibria; yet they cannot capture\ncomplex interactions of real-world, multi-agent dialogues. Games that emulate real interactions\nare more challenging to analyze, but practically useful. Exemplar games in this category include\nDeal-or-No-Deal [22], Multi-Round Auction [27], Schedule-a-Meeting, Trading Fruit, Public Debate\n[23], Avalon [37], Pok\u00e9mon [38], Chess [33] and Bargaining [39]. And there are efforts to collect\nmultiple games and evaluate them [40]. They often involve intractable action spaces (e.g. natural\nlanguage) and do not attain analytical solutions; but they are nevertheless endowed with a well-defined\npayoff function for practitioners to analyze the optimality of their strategy, albeit in an end-to-end\nmanner.\nWorkflow-aided LLM-based agent Workflow-based agents have great potential to achieve high\nperformance and adaptability in game scenarios [10]. There have been multiple agents presented"}, {"title": "3 Preliminary for Game Theory", "content": "In this section, we introduce fundamental notations, concepts, and definitions essential for under-\nstanding game theory and the strategic behavior of rational agents [53, 54].\nConsider a game involving $n$ agents, indexed by $i \\in N = \\{1,2,..., n\\}$. Each agent begins by\ninterpreting the game's introduction and rules, which include detailed descriptions of the payoff\nmatrices. Each agent has an action set defined as:\n$A_i = \\{a_i^1,...,a_i^k\\}$\nwhere $k$ is the number of available actions for each player. The agents are also given the payoff\nmatrix\n$U_i(a)$\nfor all strategy profiles $a = (a_1,a_2,..., a_n)$ for $a_i \\in A_i$. If there are only two players, we use\n$i \\in \\{1, -1\\}$ to denote the two players and the payoff matrix is\n$U_i(a_i,a_{-i})$\nfor all $a_i \\in A_i$ and $a_{-i} \\in A_{-i}$, where $A_{-i}$ is the action set of the other player.\nWe now present key definitions that will be utilized throughout this study.\nDefinition 1 (Complete-information Game) A complete information game is a strategic game\nwhere all players have full knowledge of the game's structure, including the set of players, the action\nsets, and the payoff functions of all players. Specifically, each player knows:\n\\begin{itemize}\n    \\item the total number of players involved in the game\n    \\item $A_j$: The set of actions available to each player $j$\n    \\item $U_j (a_1, a_2,..., a_n)$: The payoff function for each player $j$, which assigns a real number to every\n    possible strategy profile $(a_1, a_2,...,a_n)$\n\\end{itemize}\nThis comprehensive knowledge allows each player to make informed strategic decisions, anticipating\nthe choices and payoffs of other players based on complete information.\nDefinition 2 (Incomplete-information Game) An incomplete information game is a strategic game\nwhere there exists at least one player $i < n$ who does not have complete information about the payoff\nfunctions or action sets of other players $j < n$.\nIn incomplete information games, players form beliefs about the unknown elements based on available\ninformation and update these beliefs according to Bayesian principles as the game progresses. The\nlack of complete information requires players to strategize under uncertainty, considering not only the\npossible actions of others but also their possible types and the likelihood of various game structures."}, {"title": "4 Complete-information Games", "content": "In this section, we delve into several classic game-theoretic scenarios involving complete information\nto assess whether LLMs can act as rational decision-makers in reaching Nash Equilibria and achieving"}, {"title": "4.1 Introduction to Complete-information Games TestBed", "content": "In our exploration of classic game-theoretic scenarios, we constructed a comprehensive testbed\ncomprising 10 classic complete-information games to evaluate the rationality and strategic decision-\nmaking capabilities of LLM-based agents. This testbed includes 5 simultaneous-move games and\n5 sequential-move games. For the simultaneous-move games, 3 are coordination games, wherein\nachieving the Pareto optimal Nash Equilibrium necessitates effective negotiation between agents.\nThese games are instrumental in examining the agents' ability to communicate, build trust, and align\ntheir strategies for mutual benefit."}, {"title": "4.4 Workflow Design based on Classic Game Theory", "content": "In this section, we present the workflow employed for complete-information games, which leverages\nclassic game-theoretic strategies to guide decision-making and optimize outcomes. This structured\napproach aims to align LLMs' responses with rational game-theoretic principles, thereby enhancing\ntheir ability to identify optimal strategies and maintain robust rationality, even in the context of\nnegotiation. Through this workflow, we assess whether LLMs can sustain rational choices and\navoid strategic vulnerabilities, particularly in scenarios where negotiation might otherwise lead to\nsuboptimal or exploitable decisions, i.e. pareto optimal strategies that are not Nash Equilibrium."}, {"title": "4.4.1 Workflow for Simultaneous Game", "content": "In the simultaneous game workflow, each agent (player) seeks to determine their optimal strategy\nby considering both their own possible actions and the potential responses of the other player. This\ninvolves conditional reasoning, generating thinking chains, and summarizing these into an overall\nstrategy under the guidance of the workflow. Here, we will explain the workflow with corresponding\nmathematical formulations."}, {"title": "Game Setup", "content": "Each agent $i \\in \\{1, -1\\}$ begins by interpreting the game introduction and rules, which\ninclude detailed descriptions of the payoff matrices. The agents are provided with the exact payoffs\nfor all possible action combinations. They are given the action sets\n$A_i = \\{a_i^1,...,a_i^k\\}$\nwhere $k$ is the number of available actions for each player. The agents are also given the payoff\nmatrix $U_i(a_i, a_{-i})$ for all $a_i \\in A_i$ and $a_{-i} \\in A_{-i}$, where $A_{-i}$ is the action set of the other player."}, {"title": "Strategy Formulation", "content": "With full knowledge of the payoff matrix, agents perform best response\nanalysis to determine their optimal strategies. The goal for each player is to choose an action $a_i^*$ that\nmaximizes their own payoff, anticipating the rational response of the other player. The optimization\nis done by iterating over the player's own actions and predicts the opponent's responses. It also\nconsiders the opponent's possible actions and determines their best responses:\nFor each possible action $a_i$ that player $i$ can take: the LLM-based agent computes the opponent's\nbest response based on the resulting payoff by computing $a^*_{-i}(a_i) = arg \\max_{a_{-i} \\in A_{-i}} U_{-i}(a_i, a_{-i})$ and the\ncorresponding expected payoff $U_i(a_i, a^*_{-i}(a_i))$. This basically means that if player $i$ chooses $a_i$, then\nthe other player will choose $a^*_{-i}(a_i)$ resulting in a payoff of $U_i (a_i, a^*_{-i}(a_i))$. In the workflow, the\nLLM is guided to compute $a^*_{-i}(a_i)$ and $U_i (a_i, a^*_{-i}(a_i))$."}, {"title": "4.4.2 Workflow for Sequential Game", "content": "For sequential game, we adopt the traditional game-theoretic method: backward induction. Backward\ninduction is a fundamental method in game theory used to solve sequential games with complete\ninformation. It involves analyzing the game from the end backward to the beginning, determining the\noptimal strategy at each decision point by considering the future consequences of current actions."}, {"title": "Game Setup", "content": "Each agent $i \\in \\{1, -1\\}$ begins by interpreting the game introduction and rules, which\ninclude detailed descriptions of the payoff matrices. In a sequential game, players make decisions\none after another and for each action, the player is fully aware of all previous actions taken. The\ngame can be represented as a game tree with nodes being decision points and edges being actions."}, {"title": "Notations", "content": "\\begin{itemize}\n    \\item $H$ be the set of non-terminal decision nodes in the game tree.\n    \\item $Z$ be the set of terminal nodes (endpoints of the game).\n    \\item $A(h)$ be the set of possible actions available at node $h \\in H$\n    \\item $player(h)$ be the player who makes a decision at node $h$\n    \\item $U_i(z)$ be the payoff to player $i$ at terminal node $z \\in Z$\n\\end{itemize}\nWe define the value function $V_i(h)$ for player $i$ at node $h$ as the maximum expected utility the player\ncan achieve from that node onward. For terminal nodes $z \\in Z$:\n$V_i(z) = U_i(z)$\nFor decision nodes $h \\in H$, if it is player $i$ who moves at node $h$:\n$V_i(h) = \\max_{\\alpha \\in A(h)} V_i(h\\cdot a)$"}, {"title": "Strategy Formulation", "content": "The backward induction starts from terminal nodes. For each terminal node\n$z \\in Z$, set $V_i(z) = U_i(z)$ for all player $i$, and then proceed to preceding decision nodes. Therefore,\nfor each decision node $h$, the LLM-based agent is guided to comput the optimal action $a^* (h)$ and\nvalue $V_i(h)$ based on the player who moves at $h$.\nTo determine the optimal action, if $player(h) = i$, which is the player in question, LLM is guided to\ncompute:\n$a^* (h) = arg \\max_{\\alpha \\in A(h)} V_i (h\\cdot a)$\nOtherwise, if $player(h) = j \\neq i$, assuming player $j$ will choose their optimal action $a^*(h)$, LLM is\nguided to compute:\n$a^*(h) = arg \\max_{\\alpha \\in A(h)} V_j (h\\cdot a)$\nThen, $V_i(h) = V_i(h\\cdot a^*(h))$. The workflow directs the LLM to employ backward induction by\nsystematically traversing the game decision tree from the terminal nodes back to the initial node\nto formulate an optimal strategy. The strategy derived from this backward induction process is\nsubsequently incorporated into the contextual framework for each decision-making and negotiation\nstep, thereby guiding the LLM's strategic choices throughout the game. Figure 3 presents the\nworkflow in diagram."}, {"title": "4.5 Experiments for Classic Game Theory with Workflow", "content": "In this section, we present the results of LLM-based agents employing the workflow to summarize\nstrategies in Table 7 and 8. We begin by examining the experimental outcomes without negotiation.\nNotably, with the introduction of the workflow, the performance of all language models, except for\n01, has improved significantly."}, {"title": "5 Incomplete-information Game with Negotiation", "content": "Building upon our exploration of LLMs' capabilities in classical game-theoretic scenarios and the\ndevelopment of workflows to enhance their reasoning processes, we now shift our focus to a more\nrealistic and complex setting: incomplete-information games. Specifically, we consider a common\nresource allocation scenario where a shared pool of resources must be distributed among several\nplayers. Each player possesses private valuations of the resources that sum to a common total value,\nreflecting their individual preferences and priorities. Crucially, no player has knowledge of the other\nplayers' valuations, embodying the incomplete-information aspect of the game."}, {"title": "5.1 Introduction to Common Resource Allocation with Private Valuation", "content": "Here we provide a formal definition for the incomplete-information game that we will focus in this\nsection.\nDefinition 9 (Common Resource Allocation with Private Valuation) Consider a game involving\na set of players $N = \\{1, 2, . . ., n\\}$ and a set of items or resources $K = \\{1, 2, . . ., k\\}$. Each player\n$i \\in N$ possesses a private valuation vector $v_i = (v_i^1, v_i^2, ..., v_i^k)$ where $v_i^j \\geq 0$ represents the value\nthat player $i$ assigns to item $k$. These valuations reflect the individual preferences of the players and\nare private information; that is, each player knows their own valuations but not those of the other\nplayers.\nThe valuations satisfy the normalization condition:\n$\\sum_{j=1}^{k} v_i^j = V \\forall i \\in N$\nwhere $V$ is a constant total value common to all players. This condition ensures that while players\nmay value items differently, the total valuation of all items is the same for each player.\nAn allocation is a partition of the item set $K$ among the players, represented as $L = (L_1, L_2, ..., L_n)$\nwhere $L_i\\subseteq K$ is the set of items allocated to player $i$. The allocation must satisfy:\n$L_i \\cap L_j = \\O \\forall i \\neq j$ and $\\bigcup_{i=1}^{n}L_i = K$\nEach player $i$ receives a utility $U_i$ based on the private valuations and the allocation:\n$U_i = \\sum_{k} v_i^k \\times \\mathbb{1}(k \\in L_i)$\nwhere $\\mathbb{1}(\\cdot)$ is the characteristic function."}, {"title": "5.2 Workflow Design", "content": "The workflows employed for complete-information games are based on well-established game-\ntheoretic frameworks, leveraging the extensive research and thorough understanding available in\nthis domain. In contrast, common resource allocation problems characterized by incomplete\ninformation lack a standardized solution framework. To address this gap, we develop a novel\nalgorithm for this setting. This workflow is designed to guide the multi-round negotiation process,\nfacilitating the attainment of allocations that are both mutually agreeable and optimized for each\nplayer's self-interest. Figure 4 presents a high-level diagram of the workflow.\nAssumption 1 Each participant to the game defined in Definition 9 is rational and attains the\nfollowing objectives:\n\\begin{itemize}\n    \\item Objective 1: Achieve an agreement (i.e., successfully complete the allocation).\n    \\item Objective 2: Maximize their own utility, given that an agreement can be reached.\n\\end{itemize}\nAll negotiation proposals and communications between the players revolve around these two objec-\ntives. Regarding the first objective, we assume that an agreement can only be reached if the allocation\nsatisfies the criterion of envy freeness. That is, each player must not prefer the allocation received by\nthe other player over their own allocation. For the second objective, each player seeks to maximize\ntheir own utility under the condition that the agreement remains envy free. Essentially, players aim to\nmaximize their rewards as much as possible while ensuring the allocation is envy free.\nFollowing Definition 9, each agent's valuation of common resources remains private and undis-\nclosed to others. Therefore, it becomes essential for agents to estimate the valuations held by their\ncounterparts. A common approach involves constructing a belief distribution.\nDefinition 10 (Belief Distribution) A belief system for agent $i$ is a probability mass function $B_i$\ndefined over the set of all feasible valuations $\\Omega$:\n$\\Omega := \\{v_{-i} = (v_{-i}^1, v_{-i}^2, ..., v_{-i}^k) \\in \\mathbb{R}^k_+ | \\sum_{j=1}^{k}v_{-i}^j = V\\}$\nwhere $0 \\leq v_{-i}^j < V$ for each item $j \\in [k]$. We denote $V_{-i}$ as the random vector with support over $\\Omega$.\nInitially, $B_i$ is assumed to be uniform, reflecting the agents' lack of information about the valuations\nof others. This distribution is subsequently updated based on evidence gathered at each round of the\nnegotiation, presented next."}, {"title": "Allocation Proposal Process.", "content": "At each round, the agent $i$ searches for an allocation $L_i$ that maxi-\nmizes their own utility, while maintaining that the allocation is envy free according to their belief\ndistribution $B_i$. This procedure can be formally defined as an optimization problem under chance\nconstraint:\n$\\max_{L_i} U_i (L_i)$,\ns.t. $\\mathbb{P}_{EF}(L_i; B_i) > 0$.  (1)\nWith discrete allocations, this objective can be maximized by enumerating all possible allocations\nthat attain a non-zero envy-free probability, decomposed as follows:\n$\\mathbb{P}_{EF}(L; B_i) := \\mathbb{P}$(Allocation $L$ is envy free; $B_i$)\n= $\\mathbb{P}$ (U_i(L_i) \\geq U_i(L_{-i}) and U_{-i}(L_{-i}) \\geq U_{-i}(L_i); B_i)\n= $\\mathbb{E}_{v_{-i}\\sim B_i} [\\mathbb{1}(U_i(L_i) \\geq U_i(L_{-i}) and U_{-i}(L_{-i}) \\geq U_{-i}(L_i))]$.   (2)\nThe expectation in Equation (2) can be approximated via Monte-Carlo samples drawn from the\nagent's belief distribution [36]. We instead defer to our LLM agents to decide whether the allocation\n$L$ is envy free and maximizes self-interest.\nAllocation Proposal. Upon identifying an optimal allocation according to Problem 1, the player\nproposes this allocation to the other player, which decides whether to accept this proposal or propose\nproposes a counter offer. Formally, this proposal procedure can be defined as follows:\npropose_offer : P(K) \\rightarrow O \\times P(K),\nwhere P(K) denotes the power set of resources K that contains the set of all valid proposals, and\nO denotes the set of all possible outcomes consisting of three disjoint events \\{A, R_1, R_2\\}. We let\nA denote the event of acceptance, wherein the negotiation concludes. On the other hand, a rational\nopponent satisfying Assumption 1 must reject the proposal for either of the following reasons:\n\\begin{itemize}\n    \\item R_1: The allocation $L$ is not envy free according to the opponent;\n    \\item R_2: The allocation $L$ is envy free, but there exists an alternative allocation that provides the\n    opponent with higher utility.\n\\end{itemize}\nIn the event of rejection, an agent must update their belief in order to refine their proposals.\nBayesian Update. If an allocation is rejected, it is essential to update our belief about the opponent's\nvaluation vector $v_{-i}$ to better inform future proposals. In what follows, we denote $R = R_1 \\cup R_2$ as\nthe union of possible reasons for rejection. Then, the belief update formula [56] for each possible\nv_i is given by:\n$B_i(v_{-i}) = (1 - \\lambda)B_i(v_{-i}) + \\lambda \\frac{\\mathbb{P}(v_{-i} | R)}{\\sum_{v_{-j} \\in \\Omega}\\mathbb{P}(v_{-j} | R)B_i(v_{-j})}\n= (1 - \\lambda)B_i(v_{-i}) + \\lambda \\frac{\\mathbb{P}(R | v_{-i})B_i(v_{-i})}{\\sum_{v_{-j} \\in \\Omega}\\mathbb{P}(R | v_{-j})B(v_{-j})}$.   (3)\nwhere $\\lambda \\in [0, 1]$ is a hyperparameter of the update step, and the likelihood $\\mathbb{P}(R | v_{-i})$ represents\nthe probability that the opponent rejects the allocation $L$ assuming the opponent's valuation is $v_{-i}$.\nThis likelihood depends on whether $L$ is acceptable and self-interest-maximizing for the opponent.\nWe propose the following formula to the likelihood that satisfies Assumption 1.\n$\\mathbb{P}(R | v_{-i}) \\propto\n\\begin{cases}\n1,  & \\text{Event } R_1: U_{-i}(L_{-i}) < U_{-i}(L_i)\\\\\n\\gamma,  & \\text{Event } R_2 : U_{-i}(L_{-i}) \\geq U_{-i}(L_i) \\text{ and } \\exists L' \\text{ s.t. } U_{-i}(L'_{-i}) > U_{-i}(L_{-i}),\\\\\n  & \\text{and } \\mathbb{P}_{EF}(L') > 0;\\\\\\n-\\infty, & \\text{otherwise.}\n\\end{cases}$"}, {"title": "5.3 Introduction to \"Deal or No Deal\"", "content": "\"Deal or No Deal\u201d is a representative game for incomplete-information resource allocation game. It is\ndesigned to facilitate research in developing AI agents capable of engaging in human-like negotiation\ndialogues. It consists of over 5,800 human-human negotiation dialogues collected via Amazon\nMechanical Turk, with 1052 dialogues in the test dataset. Each negotiation involves three types\nof items - books, hats, and balls \u2013 with random quantities. Each participant is randomly assigned\npoint values for each item type, summing up to 10, which are hidden from the other participant.\nParticipants communicate through natural language to agree on how to divide the items to maximize\ntheir individual point totals, without revealing the true value systems during negotiation.\nTo comprehensively evaluate the negotiation performance of the LLM-based agents, we assess not\nonly whether they reach a Nash Equilibrium, i.e., whether an agreement is achieved, but also examine\nthe fairness and effectiveness of the resulting distribution. For fairness, we adopt the concept of envy\nfreeness."}, {"title": "5.4 Experiment Setting", "content": "To observe whether LLMs are capable of negotiation and whether our workflow design is effective,\nwe evaluate multiple SOTA LLMs on the dataset. We choose the top-50 most difficult datapoints2\nthat have an envy free allocation instead of the 526 cases to converse expense of experiment.\nWe define the difficulty of a datapoint by computing the $l_1$ distance between the real valuations of\nthe two players:\nDefinition 11 (Difficulty) A datapoint d has a difficulty level defined by the $l_1$ norm of the difference\nbetween the players' valuation vectors:\nDifficulty$(d) = \\frac{3}{k}\\sum_{k} |v_i^k - v_{-i}^k| = \\frac{3}{k} \\Vert v_i - v_{-i} \\Vert$"}, {"title": "5.5.1 Both Agents without Workflow", "content": "We present the results of our experiments involving four LLM-based agents-Claude-3.5 Sonnet,\nClaude-3 Opus, GPT-40, and Model 01-alongside human performance provided by the original\ndataset and the best possible outcomes for the selected data points. By \u201cbest possible outcome,\" we\nrefer to an allocation that is both pareto optimal and envy free while maximizing the total reward for\nboth players. For each metric, we report the average scores across the 50 data points selected based\non the difficulty metric defined earlier.\""}, {"title": "5.5.2 Both Agents with Workflow", "content": "In this set of experiments, we employ the proposed negotiation workflow for both agents. We did not\ninclude Model ol in our experiments for two main reasons: (1) the computational cost associated\nwith running Model 01 is prohibitively high, and (2) preliminary experiments indicated that Model\no1 does not perform optimally when utilizing external workflows. An example of negotiation from\nOpus is presented on page 25."}, {"title": "5.5.3 One Agent with Workflow", "content": "In this section, we present experimental results where only one LLM-based agent employs the\nproposed negotiation workflow, while the other agent negotiates using direct prompting without the\nworkflow. Specifically, we conduct experiments in two scenarios:\n\\begin{itemize}\n    \\item Workflow-LLM vs. Raw-LLM: OnlyAlice uses the workflow, and Bob uses direct prompting.\n    \\item Raw-LLM vs. Workflow-LLM: Only Bob uses the workflow, andAlice uses direct prompting.\n\\end{itemize}"}, {"title": "5.6 To adopt the workflow or not?", "content": "The experiment result in section 5.5.3 raises a critical game-theoretic question: is it rational for\nan agent to adopt the workflow when the opponent may not do the same? Should a player use the\nworkflow given the potential for exploitation by a non-cooperative opponent? To address this, we\nrepresent the decision-making scenario using payoff matrices, where each agent has two strategic\nchoices: (1) Use Workflow: Apply the proposed negotiation workflow, and (2) Do Not Use Workflow:\nEngage in direct prompting without the workflow. The payoff matrix for each model is presented in\nTable 19."}, {"title": "5.6.1 Comparison and Implications", "content": "The analyses of Claude-3.5 Sonnet", "matrices": "n\\begin{itemize"}, "n    \\item Claude-3.5 Sonnet: Exhibits a classic Prisoner's Dilemma structure where the dominant strategy\n    for both agents is to defect (not use the workflow), leading to a Nash Equilibrium that is not pareto\n    optimal.\n    \\item GPT-40 and Claude-3 Opus: Both present scenarios where using the workflow is a dominant\n    strategy for both agents. This leads to a Nash Equilibrium that is also pareto optimal, where both\n    agents achieve higher individual and combined payoffs compared to other strategy profiles. These\n    cases demonstrate that when strategies are aligned and cooperation is incentivized, agents can\n    achieve mutually beneficial outcomes.\n\\end{itemize}\nAdopting the workflow consistently results in pareto optimal outcomes across different language\nmodels. However, the rationality of this choice \u2013 specifically, whether using the workflow constitutes\na Nash Equilibrium-depends on the particular characteristics of each language model. In the case\nof Claude-3.5 Sonnet, which"]}