{"title": "Aioli: A unified optimization framework for language model data mixing", "authors": ["Mayee F. Chen", "Michael Y. Hu", "Nicholas Lourie", "Kyunghyun Cho", "Christopher R\u00e9"], "abstract": "Language model performance depends on identifying the optimal mixture of data groups to train on (e.g., law, code,\nmath). Prior work has proposed a diverse set of methods to efficiently learn mixture proportions, ranging from fitting\nregression models over training runs to dynamically updating proportions throughout training. Surprisingly, we find that\nno existing method consistently outperforms a simple stratified sampling baseline in terms of average test perplexity per\ngroup. In this paper, we study the cause of this inconsistency by unifying existing methods into a standard optimization\nframework. We show that all methods set proportions to minimize total loss, subject to a method-specific mixing law-an\nassumption on how loss is a function of mixture proportions. We find that existing parameterizations of mixing laws can\nexpress the true loss-proportion relationship empirically, but the methods themselves often set the mixing law parameters\ninaccurately, resulting in poor and inconsistent performance. Finally, we leverage the insights from our framework to derive\na new online method named AIOLI, which directly estimates the mixing law parameters throughout training and uses them\nto dynamically adjust proportions. Empirically, AIOLI outperforms stratified sampling on 6 out of 6 datasets by an average\nof 0.28 test perplexity points, whereas existing methods fail to consistently beat stratified sampling, doing up to 6.9 points\nworse. Moreover, in a practical setting where proportions are learned on shorter runs due to computational constraints,\nAIOLI can dynamically adjust these proportions over the full training run, consistently improving performance over existing\nmethods by up to 12.01 test perplexity points.", "sections": [{"title": "1 Introduction", "content": "It is critical to determine what data to train on for a language model (LM) to acquire a range of capabilities, from generating\ncode to understanding scientific literature and conversing with users [3, 34, 39]. To achieve this, practitioners mix data from\nvarious groups (such as code files, scientific papers, and chat logs) in specific proportions to compose an overall training\ndataset-a procedure known as data mixing. Identifying the optimal mixture proportions is critical to LLM performance.\nHowever, a brute-force trial-and-error search over the proportions is computationally expensive, requiring many training\nruns.\nRecent work introduces two types of data mixing algorithms that learn mixture proportions: offline and online methods.\nOffline methods conduct multiple training runs with varying proportions, fit a regression model to predict performance, and\nuse this model to determine the optimal static mixture [37, 72]. Online methods adjust the mixture proportions dynamically\nthroughout training using information from the model, such as its loss and gradients [2, 13, 21, 70]. These mixing methods\nrequire at least one training run to learn the proportions but are more efficient than a brute-force search.\nGiven the wide range of methods available, it is important to determine which ones are effective. However, when we\nevaluated existing methods, we found that no method consistently outperformed stratified sampling\u2014a simple baseline\nthat uniformly mixes groups and requires zero extra training runs across all sets of data groups in terms of average test\nperplexity (Table 2). This surprising outcome suggests that all existing methods suffer from some common weaknesses. To\nmake progress in data mixing, we identify three objectives for our work: 1) improve our understanding of the underlying\nassumptions of existing methods, 2) assess the fidelity of these assumptions in practice to better understand performance,\nand 3) apply our insights to develop principled new data mixing methods."}, {"title": "2 Problem Setup", "content": "We formalize the data mixing problem and establish notation. In data mixing, we have m data groups of text, such as\nGitHub, BooksCorpus, and arXiv. We are given train, validation, and test sets for each data group, which we denote as\n$D_{i}^{train}, D_{i}^{val}, D_{i}^{test}$ for the ith group. Define $D^{train} = {D_{1}^{train},..., D_{m}^{train}}$, and similarly define $D^{val}$ and $D^{test}$.\nData & Mixing. During training, we show the model a total of N examples from Dtrain over S training steps. To express\nhow data proportions can change throughout training, we divide training into T equal rounds. Each round t uses a mixture\nproportion from the probability simplex: $p^{t} = [p_{1},..., p_{m}] \\in \\triangle^{m}$. Static mixtures use only a single round (T = 1):\n$p = (p^{1})$, while dynamic mixtures use several (T > 1): $p = (p^{1},...,p^{T})$.\nModel & Loss. Let f (p, t) refer to the language model (LM), f, at the beginning of round t where the model has been\ntrained on data sampled using mixture proportions $p^{1}, ..., p^{t-1}$ so far. Given a model f, we can compute its loss on each\ngroup using the training data, $L^{train}(f) = (L^{train,1}(f), ..., L^{train,m} (f))$, and similarly using the validation data, $L^{val}(f)$, and\ntest data, $L^{test}(f)$. In this notation, the loss at the end of training can be expressed as $L_{(.)}(.,f(p, T + 1))$. When the f being\nreferred to is obvious, we simply write $L_{(.)}(p)$, and for static mixtures we drop the superscript: $L_{(.)}(p)$.\nData Mixing Problem. Given a set of data groups, an LM f to train for S steps with N samples, and T rounds of training\n(i.e., determining whether we use static or dynamic proportions), we aim to determine the p that minimizes the total test loss\nacross groups: $\\text{minimize} \\sum_{i=1}^{m} L^{T+1}_{test,i}(p)$.\nThis objective aims to produce a trained model that does well on many data groups, which can serve as a proxy for\ndownstream performance. However, without assuming additional structure on $L^{T+1}_{test}(p)$, this problem can only be solved\nwith a brute-force search over p, which requires training many different models. Existing methods overcome this by imposing\nan implicit constraint on the problem, thereby setting p without searching. In the next section, our LMO framework unifies\nmany existing methods using a single explicit constraint."}, {"title": "3 A Unified Optimization Framework for Data Mixing", "content": "We introduce the LMO framework by stating the general optimization problem (Section 3.1). Then, we explain how this\nframework can express several existing methods (Section 3.2, 3.3), with a summary of our insights regarding these methods\nin Section 3.3.3."}, {"title": "3.1 Linear Mixing Optimization (LMO)", "content": "The LMO framework consists of an optimization problem that is equivalent to the data mixing problem (Section 2), subject\nto an additional constraint:\n$\\begin{aligned}\n&\\underset{p \\in \\triangle^{T \\times m}}{\\text{minimize}}  \\sum_{i=1}^{m} L^{T+1}_{val,i}(p)\\\\\n&\\text { s.t. } L^{t+1}_{val, i}(p) = c_i + \\sigma \\left(b_i-\\sum_{j=1}^{m} A_{i j} p_{j}^{t}\\right) \\forall i \\in[m], t \\in[T],\n\\end{aligned}$\nfor some $A^{t}, b^{t}, c^{t}$, and $\\sigma$. $A^{t} \\in \\mathbb{R}^{m \\times m}$ is a matrix that encodes cross-group interactions, where $A^{t}_{ij}$ intuitively describes\nhow much training on group j at t impacts group i's loss. $b^{t}, c^{t} \\in \\mathbb{R}^{m}$ are group-specific parameters. $\\sigma : \\mathbb{R} \\rightarrow \\mathbb{R}$ is either\nthe identity function (Id) or the exponential function (exp). We refer to the constraint in (2) as a mixing law that specifies the\nassumed relationship between loss and proportions.\nThere are three components of this optimization problem that need to be specified to yield a way to set p: a) the\nparameterization of the mixing law, defined by T and $\\sigma$; b) the values of the parameters $A^{t}, b^{t}$, and $c^{t}$; and c) how to solve\nthe problem. We express existing methods in LMO by specifying these components."}, {"title": "3.2 Preliminaries for expressing methods in the LMO framework", "content": "We discuss preliminaries before presenting existing methods and explaining how they can be expressed in the LMO\nframework. First, we formally define what it means for a method to be expressed in the LMO framework. Then, we present\na result that allows us to convert between linear dynamic mixing laws (T > 1, $\\sigma$ = Id) and a way to set p, which we will to\nuse to express online methods in our framework in Section 3.3."}, {"title": "3.3 Existing methods", "content": "We discuss four existing data mixing methods and express them as specific instances of the LMO framework. A summmary\nof our insights is provided in Section 3.3.3 and Table 1. In Appendix B.1, we comment on how several other online and\noffline data mixing methods are related to our framework, and all proofs for this section are in Appendix B.2."}, {"title": "3.3.1 Offline methods", "content": "Data Mixing Laws (DML). Ye et al. [72] propose an offline method using a static mixing law (T = 1): $L_{val,i}(p) =$\n$C_{i}+b_{i} \\exp \\left(-\\sum_{j=1}^{m} A_{i j} p_{j}\\right)$ for $i \\in[m]$, with A, b, c learned by sweeping training runs over static proportions (at least m+1\nruns to avoid being underdetermined). Their method selects the proportion that minimizes the predicted validation loss. This\nlaw can be derived from (2) with $\\sigma$ = exp, showing that LMO with a) log-linear static mixing law, b) fitted parameters, and\nc) direct computation of p can express DML."}, {"title": "3.3.2 Online Methods", "content": "We provide a colloquial description and an algorithmic description of three online methods. Then, in Theorem 1, we\ndemonstrate how they all are expressed in LMO using a linear dynamic mixing law, the EGD update rule, and method-\nspecific mixing law parameters.\nSkill-It. Chen et al. [13] is an online method motivated by curriculum learning that dynamically adjusts mixture proportions.\nData group interactions are expressed in a \u201cskills graph,\u201d where each edge denotes how much the loss on one group changes\nwhen trained on another. The skills graph is learned in advance using m additional training runs and is then used to update\nproportions pt throughout training.\nConcretely, the skills graph matrix $A^{SG}$ has entries $A^{SG}_{i j} = \\left(L^{t}_{val, i}(1)-L^{t}_{val, i}(1_{j})\\right) / L^{t}_{val,i}(1_{j})$ indicating the relative\ndecrease in loss on group i when training a model on group j only. This is used in the Skill-It update rule, $p_{j}^{t+1} \\propto$"}, {"title": "4 Analyzing Fidelity of Existing Methods with the LMO Framework", "content": "We examine the fidelity of the assumptions made by existing methods in terms of the three components of the LMO\nframework: a) the mixing law parameterization, b) values of the mixing law parameters, and c) how to solve the optimization\nproblem for p. After providing experiment details (Section 4.1), we discuss these three components in order (Section 4.2-4.4)."}, {"title": "4.1 Experiment Details", "content": "Data settings. We use a sampled version of SlimPajama [53, 73], a pre-processed version of the RedPajama pretraining\ndataset [62], which has been used to train open-source LMs [24, 59]. SlimPajama consists of 7 data groups: ArXiv,\nBooks, CommonCrawl, C4 [50], Github, StackExchange, and Wikipedia. To develop a fine-grained understanding of\ndata mixing, we create 6 settings by extracting combinations of these groups. We study three settings with m = 2:\nArxiv/StackExchange, Github/C4, and Book/StackExchange. We study two settings with m = 3: Arxiv/Book/StackExchange\nand CommonCrawl/Github/Wikipedia. Finally, we study mixing over the full SlimPajama dataset with m = 7."}, {"title": "4.2 Mixing law parameterization", "content": "We examine whether existing methods' mixing law parameterizations\u2014log-linear static and linear dynamic\u2014capture the\ntrue loss-proportion relationship. By empirically fitting them to loss-proportion pairs, we find that both parameterizations\nare indeed well-specified. Full results for both mixing laws are in Table 5 in Appendix C.1. We also discuss the generality\nof these parameterizations across training scales and other SlimPajama subsets in Appendix C.1.1, and study if these\nparameterizations hold over mixtures of instruction-tuning tasks in Appendix C.1.2.\nSetup. For the log-linear static mixing law, we study if there exists A, b, c such that $L_{val, i}(p)$ can be expressed as\n$C_{i}+b_{i} \\exp \\left(-\\sum_{j=1}^{m} A_{i j} p_{j}\\right)$ for all $i \\in[m]$. We fit the parameters using full training runs on P. For the linear dynamic\nmixing law, we study if there exists $A^t$ such that $L_{val, i}^{t+1}(p)$ can be expressed as $L_{val, i}^{t} (P) - \\sum_{j=1}^{m} A_{i j}^{t} p_{j}^{t}$, for all $i \\in [m]$ ($b^t$ is\nabsorbed into $A^t$). To fit $A^t$, we select a timestep t and train on a static proportion $p^0 \\in \\mathbb{P}$ until round t, and at t + 1 we\nsweep the values of $p^{t+1} \\in \\mathbb{P}$.\nResults. On average across our 6 data settings, the mean squared error (MSE) of the fitted log-linear static mixing law is\n8.9 \u00d7 10-4, and the $R^2$ coefficient of determination is 0.991. The average MSE of the fitted linear dynamic mixing law is\n1.0 \u00d7 10-4 and the $R^2$ is 0.947. See Figure 2 for examples. Since both parameterizations have high $R^2$ and low MSE, we\nconclude that they capture the true loss-proportion relationship well and are of high fidelity."}, {"title": "4.3 Values of mixing law parameters", "content": "As shown in Table 1, each method sets the parameters of its mixing law differently. We study how close the method-\nspecific parameters are to the optimal parameters that are obtained when fitting the method's mixing law to the true\nloss-proportion relationship, and if these parameter disparities are reflected in method performance. We find that existing\nmethods' differences in parameters are largely responsible for their performance. We omit studying DML since its parameters\nare fitted from full training runs and hence differ from the optimal parameters in estimation error only.\nSetup. For each online method\u2014Skill-It, DoReMi, and DoGE\u2014we select a step t and obtain the method-specific $A^t$.\nWe then sweep P for the next round t + 1. This sweep is used to fit an approximately optimal $A^{t*}$ that captures the true\nloss-mixture relationship, $L^{t+1}_{val} (p) = L_{val}^{t} (P) - A^{t*}p^{t}$, as well as fit a $b^{t} \\in \\mathbb{R}$ used for scaling $A^{t}$ (details in Appendix C.2).\nWe study the relationship between $\\tilde{A}^{t} := b^{t}A^{t}$ and $A^{t*}$, and how this relationship is connected to the performance of the\nmethod.\nTo express similarity between $\\tilde{A}^{t}$ and $A^{t*}$ in a way that is reflected in performance, we observe that from Lemma 1, $p^{t}$ is\nupdated using the column sum of $A^{t}$, $1^{T} \\tilde{A}^{t}$. Moreover, the magnitude of $A^{t}$ is not critical to performance since the step size \u03b7"}, {"title": "4.4 Solving strategy", "content": "We study the assumptions made in how existing methods solve the LMO optimization problem. We find that the greedy\napproximation used by EGD, minimize $L^{t+1}_{val,i}(p)$, does not significantly compromise performance compared to full\noptimization of dynamic proportions, which has an exponentially large solution space. In particular, we study if greedily\nselecting $p^{t}$ from P at each t yields the optimal dynamic proportions in $\\mathbb{P}^T$, and we find that this holds in 2 out of 3 data\nsettings (Table 10). This suggests that the greedy approximation can simplify optimization without substantial performance\nloss. We also comment on other possible solving strategies in Appendix C.3."}, {"title": "5 AIOLI: a Method for Improved Data Mixing", "content": "To validate our insights from Section 4, we develop AIOLI, an online method derived from the LMO framework. We have\nthree takeaways from section 4:\na) A linear dynamic mixing law, $L^{t+1}_{val,i}(p) = L^{t}_{val,i}(P) - \\sum_{j=1}^{m} A_{i j} p_{j}$ for all $i \\in [m]$, can capture the loss-proportion\nrelationship with high fidelity (Section 4.2).\nb) Existing online methods often set the parameters $A^{t}$ to be very different from true $A^{t*}$ (Section 4.3).\nc) Exponentiated gradient descent can recover near-optimal performance while simplifying the optimization problem,\navoiding an exponential solution space (Section 4.4).\nWe thus directly specify the linear dynamic mixing law parameterization and EGD as two out of three LMO components\nof AIOLI since we found that their assumptions generally hold in practice. According to Lemma 1, the update rule given\nthese two components is is $p_{j}^{t} \\propto p_{j}^{t-1} \\exp \\left(\\eta \\sum_{i=1}^{m} A^{t}_{i j}\\right)$ ($b^{t}$ is absorbed into $A^{t}$). Thus, our primary mandate in creating AIOLI"}, {"title": "6 Experimental Results", "content": "We evaluate all methods in the LMO framework, including AIOLI, in two settings. First, we consider an unrestricted\nadditional training budget setting to assess how AIOLI compares to other methods in their original form, since each method\nuses a different number of extra training runs to learn proportions (Section 6.1). Second, we consider a restricted training"}, {"title": "8 Discussion", "content": "We propose an optimization framework, LMO, that can express several existing data mixing methods using their implicit\nmixing law. Using LMO, we show that linear dynamic mixing laws are well-specified in capturing the true loss-proportion\nrelationship across datasets, but existing online methods still perform poorly on some datasets because their parameters are\ninaccurate. This insight inspires AIOLI, whose performance gains are rooted in its ability to estimate parameters $A^{t}$ of the\nlinear dynamic mixing law throughout training.\nLimitations and Future Work. AIOLI introduces extra inference cost during training via the repeated evaluations in\nLEARNPARAMS (Alg. 2). This overhead can be reduced or completely mitigated by using the training loss as a proxy for the\nvalidation loss, computing $L_{val}$ over a subset of $D_{val}$, or by using each $A^{t}$ for longer (equivalently, decreasing T).\nThe LMO framework itself is an invitation for future work. We hope that LMO identifies clear axes of improvement for\ndata mixing (the parameterization of the mixing law, parameter estimation, and how to solve for p) and inspires the next\ngeneration of principled data mixing methods. Another direction for future work is understanding how to define data groups\nand how data group partitions impact data mixing. For example, C4 is a subset of CommonCrawl, so repartitioning these\ndatasets into more disjoint groups could result in better data mixing performance. Furthermore, it is unclear if linear mixing\nlaws results from some specific property of the data groups we studied. It would be interesting to consider in future work\nhow often linear mixing laws hold, and how to exploit potential non-linearities in data mixing."}, {"title": "8.1 Reproducibility Statement", "content": "See Appendix B.2 for the full proofs on how to express Skill-it, DoReMi, and DoGE using the LMO framework. See\nAppendix C for details on how to reproduce our analyses of mixing law parametrization validity, $A^t$ parameter fit, and\nassessing whether greedy optimization is sufficient for data mixing. Finally, to reproduce the experimental results, please see\nAppendix D."}, {"title": "8.2 Ethics Statement", "content": "Our work focuses on improving the efficiency and performance of language model training. While our research does not\ndirectly address ethical concerns, it can contribute to more responsible AI development by optimizing training, which can\nreduce computational costs and energy consumption."}, {"title": "Appendix", "content": "In Appendix A, we provide a glossary of notation used in the paper. In Appendix B, we discuss how additional data mixing\nmethods are related to the LMO framework and provide proofs that existing methods can be expressed in our framework. In\nAppendix C, we provide additional results on our analysis of existing data mixing methods. In Appendix D we provide\nadditional details for our results in Section 6, and in Appendix E we provide additional results, including downstream\nevaluation and ablations."}, {"title": "A Notation", "content": "The glossary is given in Table 4 below."}, {"title": "B LMO framework details", "content": "We comment on two other popular data mixing methods, Online Data Mixing (ODM) [2] and RegMix [37]."}, {"title": "B.1 Additional existing methods", "content": "In ODM [2], data mixing is framed as a multi-armed bandit problem, where each arm is a data group that a batch is\ntrained on, and the reward function is defined in terms of the training loss of each group. ODM uses the EXP3 algorithm to\nexplore training on different data groups. pt, which is used to determine which group the entire training batch is comprised\nof, is updated according to $p_{j}^{t+1} = \\frac{\\left(1-\\eta_{t}\\right) \\exp \\left(\\eta_{t} R_{j}^{t}\\right)}{Z_{t}}+E_{t}$. $E_{t}$ is an exploration rate, and the reward function is"}, {"title": "B.2 Proofs for section 3.3", "content": "We provide background on exponentiated gradient descent (EGD) taken from Kakade [29]. In EGD, we have a sequence of\ndecisions $w^{1},..., w^{T}$, where $w^{t} = [w_{1},..., w_{m}] \\in \\triangle^{m}$. We also have a sequence of cost functions $c^{1},..., c^{T} : \\triangle^{m} \\rightarrow \\mathbb{R}$.\nTo minimize the total cost $\\sum_{t=1}^{T} c^{t}\\left(w^{t}\\right)$, the EGD update rule sets $w^{0} = Unif(m)$, and updates according to $w_{j}^{t+1} =$\\n$w_{j}^{t} \\exp \\left(-\\eta \\nabla_{j} c^{t}\\left(w^{t}\\right)\\right) / Z_{t}$. $Z_{t}$ ensures that $w^{t+1} \\in \\triangle^{m}$, $\\eta$ is a step size, and $\\nabla_{j} c^{t}\\left(w^{t}\\right)$ denotes $\\frac{\\partial c^{t}\\left(w^{t}\\right)}{\\partial w_{j}}$. EGD is known to have\ncertain regret guarantees on the value of costs incurred by playing $w^{1}, ..., w^{T}$ versus always playing the best fixed point in\nhindsight, i.e., the quantity $\\sum_{t=1}^{T} c^{t}(w^{t}) - \\inf_{w \\in \\triangle^{m}} \\sum_{t=1}^{T} c^{t}(w)$. We now are ready to prove Lemma 1."}, {"title": "B.2.1 Background on Exponentiated Gradient Descent", "content": "To prove Theorem 1, we write out individual propositions 1, 2, 3 for expressing each online method in the LMO framework.\nBy our definition of what it means to express a method in LMO, we must consider how each method 1) trains f and 2)\nsets pt. We must see if this procedure can be replicated by solving some specification of the LMO optimization problem in\nour data mixing setup.\nCritically, note that this definition of \u201cexpression\" does not claim that the optimization problems proposed in existing\nmethods are exactly the same as the LMO optimization problem. Instead, we are stating that the training procedures used in\ntheir methods can be equivalently viewed as a way of solving the LMO optimization problem subject to certain assumptions\non the loss-proportion relationship."}, {"title": "B.2.2 Proof of Theorem 1", "content": "Proposition 1 (Skill-It Derivation). Using a) a linear dynamic parameterization $L_{v a l, i}^{t+1}(p) = L_{v a l, i}^{t} (P) - \\sum_{j=1}^{m} A_{i j}^{t} p_{j}^{t}$, b)\nparameters $A_{i j}^{t} = L_{v a l, i}^{t}(p) \u00b7 \\left(L_{v a l, i}^{t}\\left(1_{j}\\right)-L_{v a l, i}^{t}\\left(1_{j}\\right)\\right) / L_{v a l, i}^{t}\\left(1_{j}\\right)$, and c) exponentiated gradient descent (EGD) to solve\nfor p, the LMO framework (1) can express Skill-It."}, {"title": "C Analysis Details and Additional Results", "content": "We describe how we performed the linear and log-linear parameterization experiments. For the log-linear static parame-\nterizations, we train our model on p \u2208 P sweeps and fit the parameters using code provided in Ye et al. [72] (i.e., using\nPyTorch and L-BFGS to minimize the Huber loss of the mixing law). We do this over 5 random seeds for k = 2, 3 and over\n3 seeds for the full SlimPajama.\nFor the linear dynamic parameterizations, for k = 2,3 we train the model for 2000 steps according to some $p^{0} \\in \\mathbb{P}$,\nand then sweep over P for the next 100 steps. We do this for one random seed, performing $\\left|\\mathbb{P}\\right|^{2}$ total runs. For the full\nSlimPajama setting, we train the model for 10000 steps using stratified sampling, and then sweep over P for the next 5000\nsteps. We fit the parameters using Pytorch and L-BFGS."}, {"title": "C.1 Mixing Law Parameterization", "content": "We investigate whether the log-linear static and linear dynamic mixing laws remain\nwell-specified in later stages of training and on other datasets. To do so, we take various Pythia 160M checkpoints [8],\nsweep mixing proportions, and fit the linear dynamic and log-linear static mixing laws. We train for 2000 steps according\nto the learning rates and learning rate scheduler reported in [8]. We fit the static mixing law on full runs of 2000 steps,\nand the linear dynamic mixing law at t = 500, after which we do a training sweep over the next 500 steps. In Tables 6\nand 7, we find that the strong fit for log-linear static mixing laws continues to hold during pre-training at checkpoint 72K\n(roughly halfway through training Pythia-160M) and after pre-training, with an average $R^2$ of 0.982 and 0.991, respectively.\nHowever, the linear dynamic mixing law's $R^2$ coefficient is lower, averaging 0.815 at checkpoint 72K and 0.830 at the\nend of pre-training. It may be interesting to further study if the dynamics of the loss-proportion relationship evolve in a\nstructured way throughout training, or if these results are due to more noise in how models learn at later stages of training."}, {"title": "C.1.1 Additional parameterization experiments", "content": "We identify an example set of data groups that exhibits\na non-linear relationship between loss and proportion: Books/C4 from SlimPajama. For these two data groups, we see that\nas the proportion of Books increases while C4 decreases, the loss on Books starts increasing past a certain p, suggesting\nquite counterintuitively that performance on Books is optimized by allocating some proportion to C4. In this case, neither\nlog-linear static or linear dynamic mixing laws have good fit to the proportion-loss relationship, as neither can represent the\nnon-linearity. In particular, the average MSE and $R^2$ for the log-linear static mixing law is 0.003 and 0.558, respectively,\nand the average MSE and $R^2$ for the linear dynamic mixing law is 0.0002 and 0.721.\nFortunately, because these nonlinearities exist on the boundary of the simplex and tend to incur high loss, they tend to\nhave little impact on the optimization of p, which strives to minimize the average loss for the standard data mixing problem.\nFor instance, we found that the optimal proportion according to Ye et al. [72]'s log-linear static mixing law on one random\nseed was [0.176, 0.824], and the true optimal from grid search was [0.2, 0.8]. However, it is important to further investigate\nthis non-linear phenomenon on additional data groups and training regimes, which we defer to future work."}, {"title": "C.1.2 Parameterization across other sets of data groups", "content": "We studied if training on SlimPajama (from scratch, at a pre-training checkpoint, and at the end of pre-training)\nexhibited linear dynamic or log-linear static mixing. We now study if supervised fine-tuning on a mixture of task types"}, {"title": "C.2 Values of mixing law parameters", "content": "We explain how to compare method-specific $A^{t}$'s to an approximation of the true $A^{t*}$. First, after performing method-specific\ninitialization, such as training reference models, we run each online method (Skill-It, DoReMi's proxy model DoGE's proxy\nmodel, Skill-it, and AIOLI) for t steps. For Skill-It, DoReMi, and DoGE, we use the unrestricted setting configuration of\nhyperparameters presented in Section D. For AIOLI, we analyze the parameters of AIOLI +GS from the restricted setting,\nsince we found that this had less noisy fluctuation in the weights than in the unrestricted setting. For m = 2, we set t 1000\nfor Skill-It and t = 500 for DoGE, DoReMi, and AIOLI since Skill-It is updated less frequently. For m = 3, we set t = 1000\nfor DOGE, DoReMi and Skill-It, and t = 1500 for AIOLI. We then checkpoint the language model and the method's $A^{t}$. For\nDOGE and DoReMi, we compute a smoothed $A^{t} = \\frac{1}{100} \\sum_{j=t-100}^{t-1} A_{j}$ because each $A^{t}$ is computed at the batch level,\nand can thus be noisy. For AIOLI, we also smooth the $A^{t}$ by averaging the previous timestep parameters."}, {"title": "C.2.1 Properties of $A^{t*}$", "content": "We discuss some properties of $A^{t*}$, finding that 1) $A^{t*}$ can vary significantly across time, and 2) $A^{t*}$ needs to be modeled as\na full matrix. In Appendix E.2, we also present ablations on AIOLI that test how capturing these two properties in AIOLI are\nimportant to performance.\nFor each initial mixture $p^{0} \\in \\mathbb{P}$, we train for t = 2000 steps and then sweep over P for the next 100 steps. We repeat this\nsetup for t = 4000 to obtain $A^{2000*}$ and $A^{4000*}$. We do this experiment for Arxiv/Stackexchange and Github/C4."}, {"title": "C.3 Solving strategy", "content": "We present our results on examining the assumptions made in how existing methods solve the LMO optimization problem.\nAll online methods use exponentiated gradient descent, which updates pt using the gradient at the current timestep. This\ninvolves a greedy approximation of the objective function. We study if the greedy approximation yields a p is close to the\ntrue optimal p.\nFor m = 2 data settings, we take our S = 5000 steps and split it into T = 2 rounds. We perform a brute-force sweep at\neach round over P, which sweeps p\u2081 ="}]}