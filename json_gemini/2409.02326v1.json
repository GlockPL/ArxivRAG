{"title": "Arctic-SnowCoder: Demystifying High-Quality Data\nin Code Pretraining", "authors": ["Yuxiang Wei", "Hojae Han", "Rajhans Samdani"], "abstract": "Recent studies have been increasingly demonstrating that high-quality data is cru-\ncial for effective pretraining of language models. However, the precise definition of\n\"high-quality\" remains underexplored. Focusing on the code domain, we introduce\nArctic-SnowCoder-1.3B, a data-efficient base code model pretrained on 555B to-\nkens through three phases of progressively refined data: (1) general pretraining with\n500B standard-quality code tokens, preprocessed through basic filtering, deduplica-\ntion, and decontamination, (2) continued pretraining with 50B high-quality tokens,\nselected from phase one by a BERT-style quality annotator trained to distinguish\ngood code from random data, using positive examples drawn from high-quality\ncode files, along with instruction data from Magicoder and StarCoder2-Instruct,\nand (3) enhanced pretraining with 5B synthetic data created by Llama-3.1-70B\nusing phase two data as seeds, adapting the Magicoder approach for pretraining.\nDespite being trained on a limited dataset, Arctic-SnowCoder achieves state-of-\nthe-art performance on BigCodeBench, a coding benchmark focusing on practical\nand challenging programming tasks, compared to similarly sized models trained on\nno more than 1T tokens, outperforming Phi-1.5-1.3B by 36%. Across all evaluated\nbenchmarks, Arctic-SnowCoder-1.3B beats StarCoderBase-3B pretrained on 1T\ntokens. Additionally, it matches the performance of leading small base code mod-\nels trained on trillions of tokens. For example, Arctic-SnowCoder-1.3B surpasses\nStarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a benchmark\nthat evaluates function-level code generation, and remains competitive on Big-\nCodeBench. Our evaluation presents a comprehensive analysis justifying various\ndesign choices for Arctic-SnowCoder. Most importantly, we find that the key to\nhigh-quality data is its alignment with the distribution of downstream applications.", "sections": [{"title": "1 Introduction", "content": "Pretraining large language models (LLMs) has generally relied on vast quantities of data. This\nemphasis on data volume is especially true in specialized domains like code, where researchers\nobtain massive code pretraining datasets by crawling platforms like GitHub [19, 33, 14, 23, 27, 10].\nRecent studies, however, have increasingly showed that high-quality data is crucial for effective\npretraining [9, 30, 18, 1], including the code domain [13, 20, 10].\nIn the general domain, researchers have explored various techniques to curate high-quality\npretraining data for language models. FineWeb-Edu [30] uses a linear regressor built on\nSnowflake-arctic-embed-m [26] embeddings to assess the educational value of web pages and\nselect high-quality content, while the DCLM [18] approach employs a fastText-based [5] filter\ntrained on positive examples from high-quality online sources [39] and instruction data [41], and\nrandom negative web pages to identify high-quality text. These model-based quality filters have\nbeen shown to significantly enhance language model performance on downstream tasks, compared\nto using unfiltered, large-scale datasets. Similarly, researchers have recognized the importance of\nhigh-quality code data for pretraining, with Phi-1 [13] using a random forest classifier on Code-\nGen [28] embeddings to select educational code samples, and DeepSeek-Coder-V2 [9] employing a\nmulti-stage fastText-based [5] pipeline to recall web-related code data and high-quality code from\nGitHub, achieving state-of-the-art coding performance.\nIn this paper, we introduce Arctic-SnowCoder-1.3B, a high-performing small code model created by\na novel three-step training methodology focused on progressive improvements in data quality. As\na result of this methodology, Arctic-SnowCoder-1.3B outperforms StarCoderBase-3B [19] across\nall evaluated benchmarks and exceeds Phi-1.5-1.3B [20] by 36% on the complex and practical\nBigCodeBench benchmark [46], a benchmark that truly matters for real-world programming. As\nshown in Figure 1, Arctic-SnowCoder is developed through a three-stage, data-efficient pretraining\nprocess that progressively refines the quality of the data used. The first stage involves general\npretraining for a 500B token horizon using 400B unique raw code data, which have been preprocessed\nthrough basic filtering, deduplication, and decontamination. The 400B raw corpus is primarily\nderived from the coding data used to train Snowflake Arctic [32], combining cleaned The Stack\nv1 [19] and GitHub crawls. This is followed by continued pretraining on 50B tokens, utilizing\na smaller, high-quality subset of 12.5B code files, repeated four times. The high-quality tokens\nare selected from phase one by a BERT-based [11] quality annotator trained to distinguish good\ncode from random data, using positive examples drawn from publicly available high-quality code\nfiles [39], along with instruction data from Magicoder [41] and StarCoder2-Instruct [40]. Finally,\nthe model undergoes an enhanced pretraining phase for 5B tokens, leveraging roughly 2B synthetic\ndata generated by Llama-3.1-70B [12]. This process uses the phase two data as seeds and adapts\nthe OSS-Instruct methodology from Magicoder [41] by transforming lower-quality seed code into\nhigh-quality code documents. Notably, all training phases of Arctic-SnowCoder derive data from the\nsame raw pretraining corpus, ensuring that minimal new knowledge is introduced.\nArctic-SnowCoder-1.3B achieves state-of-the-art results on BigCodeBench [46], a coding benchmark\nfocusing on practical and challenging programming tasks, among models of similar size trained with\n< 1T tokens. Particularly, it outperforming Phi-1.5-1.3B [20] by 36%. Despite being trained on\n555B tokens, compared to other state-of-the-art small code models trained on trillions of tokens,\nArctic-SnowCoder matches or surpasses the performance of these models on several benchmarks. For\ninstance, Arctic-SnowCoder-1.3B beats StarCoderBase-3B [19], trained on over 1T tokens, across\nall evaluated benchmarks. Arctic-SnowCoder-1.3B outperforms StarCoder2-3B [23], trained on\nover 3T tokens, on HumanEval+ [7, 21] (28.0 vs. 27.4), a benchmark evaluating function-level\ncode generation, while remaining competitive on BigCodeBench (19.4 vs. 21.4). We conduct\ncomprehensive ablation studies to validate the design decisions behind training Arctic-SnowCoder:\n\u2022 First, our findings indicate that, in general pretraining, organizing file-level data into reposi-\ntories after partitioning by programming language significantly outperforms the approach of\ngrouping data solely by repository names.\n\u2022 Additionally, we determine the optimal learning rate schedule, which involves a re-warmup\nphase followed by linear decay, as well as the ideal repetition of high-quality data during\ncontinued pretraining, which we find to be four times."}, {"title": "2 Arctic-SnowCoder", "content": "In this section, we provide a detailed explanation of the training methodology used for\nArctic-SnowCoder-1.3B, as illustrated in Figure 1. We begin by discussing the composition of\nthe raw training data in \u00a72.1, followed by an overview of the general pretraining phase in \u00a72.2.\nNext, we describe the continued pretraining process using high-quality data in \u00a72.3, and finally, we\nelaborate on the enhanced pretraining with synthetic data in \u00a72.4. The model architecture is based on\nLlama-2 [38], with specific details provided in Table 1."}, {"title": "2.1 Raw data", "content": "The raw pretraining data used to train Arctic-SnowCoder-1.3B consists exclusively of code, primarily\nderived from the coding data used to train Snowflake Arctic [32]. This data combines cleaned\nversions of The Stack v1 [19] and GitHub crawls. From this data, we select 18 popular programming\nlanguages for training, similar to StarCoder2-3B [23]. These languages include Python, Java, C++,\nC, JavaScript, PHP, C#, Go, TypeScript, SQL, Ruby, Rust, Jupyter Notebook, Scala, Kotlin, Shell,\nDart, Swift, amounting to a total of 400B unique tokens."}, {"title": "2.2 General pretraining", "content": "In general pretraining, the model is trained for 500B tokens with a sequence length of 8,192 and a\nbatch size of 512 using Adam [16]. The learning rate follows a cosine decay after a linear warmup of\n600 iterations. We set the maximum learning rate to 5.3 \u00d7 10-4 and the minimum to 5.3 \u00d7 10\u22125,\nfollowing DeepSeek-Coder [14]. In this phase, we use the entire 400B raw data without applying\nadditional quality filtering. We start by partitioning code files by programming language, grouping\nthem by repository, and then concatenating them in random order, similar to the StarCoder2 [23]\napproach. In \u00a73.3, we show the advantage of first partitioning code files by programming language.\nWe name the model produced by this phase as Arctic-SnowCoder-alpha."}, {"title": "2.3 Continued pretraining with high-quality data", "content": "After general pretraining, we continue pretraining Arctic-SnowCoder-alpha with 50B high-quality\ntokens sourced from the same raw pretraining corpus. The 50B high-quality tokens are formed by\nrepeating 12.5B top-percentile code file tokens for 4 times scored by our code quality annotator.\nInspired by FineWeb-Edu [30] and DCLM [18], we train a linear classification head on top of\nSnowflake-arctic-embed-m [26], a state-of-the-art embedding model based on BERT [11]. The\ntraining data comprises 300k positive examples, sampled from a blend of 220k high-quality open-\nsource code files [39], 80k high-quality instruction data from Magicoder [41] and StarCoder2-\nInstruct [40], and 300 randomly selected code documents from the pretraining corpus. Prior research\non code quality, such as Phi-1 [13], often overemphasizes the \"educational value\" of code, skewing\nmodels towards simpler benchmarks like HumanEval+ [7, 21]. In \u00a73.2, we show that our annotation\nleads to a more balanced enhancement of model capabilities. Furthermore, given that these code\ndocuments typically exceed 1000 tokens, surpassing the BERT context window size of 512, we\nimprove over FineWeb-Edu's pipeline to calculate the score for each file by averaging the scores\nfrom the top, middle, and bottom sections as produced by the quality annotator. In this phase, we\nrewarmup the learning rate for 1000 iterations from 0 to 5.3 \u00d7 10-4, the maximum pretraining\nlearning rate, followed by a linear decay to 0. The model produced in this phase is referred to as\nArctic-SnowCoder-beta. In \u00a73.4, we validate all of our design choices."}, {"title": "2.4 Enhanced pretraining with synthetic data", "content": "In the enhanced pretraining stage, we generate even higher-quality data than in continued pretraining\nleveraging Llama-3.1-70B-Instruct [12] and increase the Python mix ratio to approximately 50% while\nkeeping the proportions of the other languages unchanged. Phi-1 [13] demonstrates that synthetic,\ntextbook-like pretraining data can significantly enhance model performance. However, overemphasis\non such data risks skewing the model's distribution, potentially impairing its effectiveness in real-\nworld coding tasks. For example, we show in \u00a73.2 that Phi-1.5 excels in HumanEval+ [7, 21] and\nMBPP+ [4, 21], which resemble textbook exercises, but performs less effectively on the more complex\nand practical coding tasks in BigCodeBench [46]. To address this, we adapt the OSS-Instruct method\nfrom Magicoder [41] for pretraining purposes. Originally, OSS-Instruct was originally designed\nto generate realistic instruction-tuning data by prompting a model to create question-answer pairs\ninspired by open-source code snippets. In contrast, we produce high-quality synthetic pretraining\ndata by using Llama-3.1-70B-Instruct to generate high-quality and problem-solving oriented code\nfiles, seeded with code documents scored in the top percentile during the continued pretraining\nphase. In \u00a73.2, we demonstrate that each pretraining phase significantly outperforms the previous\none, highlighting the effectiveness of progressively enhancing data quality."}, {"title": "3 Experiments", "content": "In this section, we compare Arctic-SnowCoder with state-of-the-art small language models and show\nperformance boost over each pretraining stage (\u00a73.2), evaluate two strategies of forming repo-level\ndata in general pretraining (\u00a73.3), and perform detailed ablation to justify our design choices in\ncontinued pretraining (\u00a73.4)."}, {"title": "3.1 Experimental setup", "content": "We consider the following four diverse programming benchmarks to comprehensively evaluate the\ncode generation capability of different code models:\nHumanEval+ and MBPP+ [21]. HumanEval [7] and MBPP [4] are the two most widely-used\nbenchmarks for function-level code generation. We adopt their augmented version powered\nby EvalPlus [21], with 80x/35\u00d7 more test cases for rigorous evaluation. HumanEval+ and\nMBPP+ include 164 and 378 coding problems, respectively.\nEvoEval [43] is a program synthesis benchmark suite created by evolving existing benchmarks into\ndifferent targeted domains. We employ its five default transformation categories, namely\ndifficult, creative, subtle, combine and tool_use, totaling 500 tasks."}, {"title": "3.3 Repo-level data in general pretraining", "content": "In the general pretraining phase, we adopt StarCoder2's approach to group file-level data randomly\ninto repositories through a random concatenation of file contents [23]. In Table 3, we study two\nmethods: (1) grouping files just by repository names, meaning that each training document can be a\nmix of multi-lingual code files if the repository is written in different languages, and (2) partitioning\nfiles into different programming languages before grouping them into repositories, meaning that each\ntraining document only focuses on one single language."}, {"title": "3.4 Design choices in continued pretraining", "content": "In continued pretraining, we source high-quality tokens from our pretraining corpus and train an\nimproved base model. To obtain high-quality tokens, a model-based quality annotator is employed. In\nthis section, we experiment with various design choices, including the training data for the annotator,\nthe learning rate used in continued pretraining, and the optimal repetitions of high-quality tokens.\nModel-based quality annotator Similar to FineWeb-Edu [30], we train a linear head on top of\nthe Snowflake-arctic-embed-m [26] embedding model to score each code file. In Table 4, we\nexperiment with 4 variants:\n\u2022 ANN-EDU: We prompt Mixtral-8x7B-Instruct [15] to annotate the educational value of\neach code file (1 to 5). 400k annotations are used to train a linear regression head. For the\nfollowing variants, similar to DCLM [18], we sample negative documents randomly and\nchange the positive parts only. A linear classification head is used instead.\n\u2022 ANN-INS: Positives are a mix of 100k educational data (3.5+) bootstrapped from ANN-EDU\nand 100k high-quality instruction data from Magicoder [41] and StarCoder2-Instruct [40].\n\u2022 ANN-HQ: Positives are 220k open-source, synthetic, high-quality code files [39].\n\u2022 ANN-HQINS: Positives are a mix of 220k ANN-HQ training data and 80k instruction data\nfrom Magicoder [41] and StarCoder2-Instruct [40].\nAfter training the annotators, we first apply each annotator to the entire pretraining corpus to obtain\na score for each file. Unlike FineWeb-Edu, which only scans the top 2k characters, we scan the\ntop, middle, and bottom parts of a code file and average the scores. We then rank the code files"}, {"title": "5 Conclusion", "content": "We introduce Arctic-SnowCoder-1.3B, a high-performing code model that underscores the critical\nimportance of data quality in the pretraining process. Trained on 555B tokens, Arctic-SnowCoder-\n1.3B achieves competitive results with state-of-the-art small code models while using significantly\nfewer tokens. Our three-stage pretraining process begins with 500B tokens of general pretraining on\na raw code corpus, followed by 50B high-quality tokens scored by a quality annotator, and concludes\nwith 5B tokens of synthetic data for further enhancement. This work demystifies the notion of\nhigh-quality data in code pretraining by demonstrating the key to high-quality data is its alignment\nwith the distribution of downstream applications. Additionally, the paper offers practical guidelines\nfor repo-level data grouping, learning rate scheduling, and the repetition of high-quality data, paving\nthe way for more efficient and effective code model development."}]}