{"title": "HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding", "authors": ["Heqing Zou", "Tianze Luo", "Guiyang Xie", "Victor (Xiao Jie) Zhang", "Fengmao Lv", "Guangcong Wang", "Junyang Chen", "Zhuochen Wang", "Hansheng Zhang", "Huaijian Zhang"], "abstract": "Multimodal large language models have become a popular topic in deep visual understanding due to many promising real-world applications. However, hour-long video understanding, spanning over one hour and containing tens of thousands of visual frames, remains under-explored because of 1) challenging long. term video analyses, 2) inefficient large-model approaches, and 3) lack of large-scale benchmark datasets. Among them, in this paper, we focus on building a large-scale hour-long long video benchmark, HLV-1K, designed to evaluate long video understanding models. HLV-1K comprises 1009 hour-long videos with 14,847 high-quality question answering (QA) and multi-choice question asnwering (MCQA) pairs with time-aware query and diverse annotations, covering frame-level, within-event-level, cross-event-level, and long-term reasoning tasks. We evaluate our benchmark using existing state-of-the-art methods and demonstrate its value for testing deep long video understanding capabilities at different levels and for various tasks. This includes promoting future long video understanding tasks at a granular level, such as deep understanding of long live videos, meeting recordings, and movies.", "sections": [{"title": "I. INTRODUCTION", "content": "The advent of large language models, particularly multimodal large language models (MM-LLMs), has revolutionized deep visual understanding [1], [2]. These models excel in tasks such as image captioning [3], visual question answering [4], and video summarization [5], [6]. Long videos, spanning over an hour and containing tens of thousands of frames, pose unique challenges, including maintaining long-term dependencies [7], managing complex temporal dynamics [8], and processing vast amounts of visual information [9]. Understanding long videos remains difficult due to: 1) the inherent complexity of extended video content [10], 2) the lack of efficient models for hour-level tokens, and 3) the absence of large-scale benchmark datasets.\nDespite the advancements in MM-LLMs, the specific chal- lenges posed by long videos necessitate specialized benchmarks. The sheer length and complexity of long videos introduce issues such as noise and redundancy, memory and computation constraints, and the need for effective temporal reasoning [10], [11]. Existing benchmarks [8], [12] often fall short in addressing these challenges comprehensively, focusing primarily on shorter video clips or lacking detailed temporal annotations. This gap highlights the necessity for a dedicated benchmark that can rigorously evaluate the capabilities of models in understanding long videos, ensuring they can handle the intricacies of extended video content.\nTo advance the field of long-video understanding, we intro- duce HLV-1K, a large-scale benchmark specifically designed to evaluate models on hour-long videos. As shown in Fig. 1, HLV-1K includes over 1,000 hour-long videos, annotated with 20,000 high-quality general question answering (QA) and multiple-choice question answering (MCQA) pairs with time-aware queries. As shown in Fig. 1(c), the query lengths vary, mapping different information in the long videos. These annotations encompass a wide range of tasks, including frame- level, within-event-level, cross-event-level, and long-term-level reasoning. The benchmark aims to provide a comprehensive evaluation framework that challenges models to maintain long- term dependencies, understand intricate temporal relationships, and process extensive visual information.\nThe creation of HLV-1K involves a rigorous selection and annotation process to ensure the inclusion of diverse and high-quality content. Compared with existing long video benchmarks in Table I, our HLV-1K has the following key features:\nHour-long video benchmark. While most video bench- marks, such as MSVD-QA [13], VideoVista [14] and LongVideoBench [8], focus on second-level or minute- level video understanding, the videos in the HLV-1K dataset are all over half an hour in length.\nDiverse tasks across different levels. HLV-1K includes both QA and MCQA tasks, encompassing a variety of reasoning tasks. This is in contrast to other long video benchmarks, such as Video-MME [15] and LVBench [16], which primarily use only MCQA in benchmark designing. Additionally, HLV-1K provides reasoning questions based on visual information at the frame-level, within-event- level, cross-event-level, and long-term-level, offering a comprehensive assessment of a model's capabilities in long-video understanding.\nTime-specific queries. HLV-1K annotations include pre- cise time information linking QA queries to video content,"}, {"title": "II. RELATED WORK", "content": "Multimodal LLMs integrate LLMs with visual modality encoders and show promising performance in visual under- standing tasks, especially image and short video understanding tasks [17]\u2013[20]. However, visual understanding for long videos that span minutes or even hours can be challenging due to the varying scenes and visual content, which dramatically increase spatial and temporal details and require capturing long-term correlations over extended time spans [10]. On one hand, most existing long video understanding methods focus on compressing sequential visual frame tokens, providing reasonable performance on minute-level long video under- standing tasks [21], [22]. On the other hand, some methods opt to fine-tune models on longer video datasets, such as Moment-10M [23], and employ more efficient visual data compression [24] or long-term information preserving methods [25] to enhance the capability of handling hour-level long video understanding. These approaches aim to balance the need for detailed visual information with the computational efficiency required to process extended video content effectively."}, {"title": "B. Long Video Benchmarking", "content": "To benchmark models' abilities in long video understanding, several new long video benchmarks have been introduced recently. Some datasets feature videos lasting several minutes [8], [26], with some extending over an hour [11], [15], [26], significantly larger than commonly used short video bench- marks with videos shorter than one minute [12], [13]. These benchmarks are designed to evaluate models' performance in handling the increased complexity and diverse content of long videos, featuring varying topics and multiple task types.\nAs shown in Table I, compared to these pioneering hour- level datasets, our proposed HLV-1K dataset includes more videos and high-quality question-answer pairs. We have clearly constructed two tasks, QA and MCQA, at different levels of long videos, with a greater diversity of task types. Additionally, our question-answer pairs are all time-specific, introducing an accurate decomposition of the time dimension in long videos. This facilitates a better exploration of the time perception ability of large models, enabling more precise and effective long video understanding."}, {"title": "III. CONSTRUCTION OF HLV-1K", "content": "In this section, we detail the construction of HLV-1K, including video collection, QA and MCQA labeling, and data refinement for high-quality annotations, as shown in Fig. 2."}, {"title": "A. Data Collection", "content": "Based on the HD-VILA [27], a high-resolution and diverse video dataset, we collect raw videos from public resources. To obtain long videos, we select those with the largest sub-clip timestamps exceeding 30 minutes from HD-VILA and download over 1,500 long videos from YouTube. During the collection stage, we manually filter out low-quality videos and those with redundant content. Finally, we curate around 1,009 long videos covering various topics, including Entertainment, Film, Travel, Animation, Blogs, Comedy, Technology, Animals, and others such as Gaming and Music."}, {"title": "B. Data Annotation", "content": "To provide high-quality long video annotations with accurate time information, we introduce a four-step labeling process to generate video QA and MCQA pairs. First, we extract dense keyframes by compressing the raw videos to obtain more content-related keyframes. Next, we label the frame descriptions using GPT-40 [28], incorporating details from YOLO [29] and frame timestamp information. After that, we introduce a sliding-window-based event detection method and label the event descriptions with GPT-40. Finally, we label the videos based on the corresponding frame and event descriptions to generate time-aware QAs and MCQAs at the frame-level, within-event-level, cross-event-level, and long-term-level."}, {"title": "1) Frame Extraction", "content": "Raw hour-long videos typically have over 100,000 frames, containing a lot of redundant information and introducing excessive complexity to video annotation. To address this, we compress the long videos to one frame per second to reduce data redundancy. Subsequently, we extract keyframes using I-frame detection methods [30]. I-frames, which are the least compressible and do not require other frames for decoding, contain most of the visual information in a video. To preserve temporal continuity, we extract both the I-frame and its preceding frame. This process results in an average of 810 frames per long video."}, {"title": "2) Frame Labeling", "content": "Multimodal-LLMs show remarkable performance in image understanding, and we adopt the com- monly used method of annotating frame descriptions with"}, {"title": "3) Event Labeling", "content": "The content of continuous frames is usually coherent, sharing similar information, and can be cate- gorized into the same event. Frames from the same event share continuous visual information, while frames from different events can be significantly different. Given the limitation of inputting all frame descriptions to LLMs, we introduce a sliding- window method to generate one event description from a fixed number of frame descriptions. We set the window size to 100 frames, approximately 10 minutes of video, typically larger than most events' duration. This approach results in an average of 20 events per video, with each event description averaging around 200 words and an average event duration of 60 seconds."}, {"title": "4) QA Labeling", "content": "Based on the frame descriptions and event descriptions, we create QA and MCQA pairs as video annotations. Unlike image and short video understanding, which focus more on frame-level spatial reasoning and within-event-level spatiotemporal reasoning, long videos require additional cross-event-level and long-term-level reasoning. To align with the long video understanding task, we introduce frame-level, within-event-level, cross-event-level, and long- term-level reasoning tasks. Frame-level reasoning pairs are generated using a single frame description, while within-event reasoning pairs are derived from a single event description. In contrast, cross-event reasoning pairs are generated using two adjacent event descriptions, and long-term reasoning pairs are created using all the event descriptions from the target video."}, {"title": "C. Data Filtering", "content": "Data filtering and label revision are crucial steps to ensure the high quality of the benchmark. Initially, we filter out question-answer pairs with incorrect formats or abnormal time information. Subsequently, we manually check the annotated labels following three specific guidelines: (1) low-quality question-answer pairs, such as those with repeated options or missing-answer options, are removed directly; (2) answers that do not match the video content are revised to ensure accuracy; and (3) time information in the questions that does not align with the videos is modified to ensure a proper mapping between question-answer pairs and the video content. By adhering to these guidelines, we ultimately obtain a total of 14,847 high-quality question-answer pairs."}, {"title": "D. Tasks of HLV-1K", "content": "As shown in Fig. 4, we annotate the query labels with two types: QA and MCQA, across frame level, within-event level, cross-event level, and long-term level. The source details with varying-duration information for the multi-level annotations are summarized in the supplementary materials. The question- answer pairs cover various tasks, including character under- standing, counting, object understanding, spatial relationships, descriptive scenes, speed, object direction, scene understanding, attribute change, time order, temporal relationships, causality, plot understanding, camera direction, and action understanding."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we provide results of the state-of-art commer- cial and open-source multimodal LLMs on our benchmarks.\nThe HLV-1K dataset consists of MCQAs and QAs, with"}, {"title": "A. Settings", "content": "We evaluate three commercial models, including GPT-40 [28], Gemini 1.5 Pro [35], and Claude 3.5 Sonnet [34], as well as state-of-the-art open-source video understanding methods, including LLaVA-OneVision [2], LLaVA-Video [33], QWen2- VL [4], Kangaroo [32], LongVA [24], and InternVL 2.5 [31], on our benchmark. We evaluate the models with a fixed frame number of 120, uniformly sampled from the video sources, except for Claude 3.5, which accepts a maximum of 20 video frames only and with same sampling method. To fairly evaluate the performance of various models, we design unified prompts for QA and MCQA tasks, respectively (details in our supplementary materials)."}, {"title": "B. Results & Analysis", "content": "The quantitative results for frame-level, within-event-level, cross-event-level, and long-term-level tasks, as shown in Table II, highlight several key insights. 1) Specialized models with more parameters, such as LLaVA-Video, LLaVA-OneVision, and QWen2-VL (each with 72 billion parameters), exhibit significantly improved performance, achieving the highest overall scores. However, 2) larger commercial models like Gemini 1.5 Pro and GPT-40 do not perform as well in long-video understanding tasks, with Gemini 1.5 Pro scoring significantly lower at 62.41 compared to LLaVA-Video's score at 78.93. 3) Models that accept fewer frames, such as Claude 3.5 Sonnet, which processes only 20 frames, show a marked decline in understanding and performance, underscoring the importance of frame count in video comprehension. 4) The performance of different models varies across task levels. For frame-level tasks, LLaVA-Video and LLaVA-OneVision excel, while within-event- level and cross-event-level tasks present greater challenges, with LLaVA-Video maintaining strong performance. Long- term-level tasks, requiring deep understanding of temporal dependencies, are the most challenging, with LLaVA-Video achieving the highest score, demonstrating its superior capabil- ity in handling complex, long-term video understanding."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced HLV-1K, a large-scale long- video benchmark specifically designed to evaluate and advance the field of time-specific long video understanding. HLV-1K comprises 1009 hour-long videos, meticulously annotated with around 14,847 high-quality QA and MCQA annotations with time-aware query. These annotations span a wide range of tasks, including frame-level, within-event-level, cross-event- level, and long-term reasoning tasks, providing a comprehensive evaluation framework for multimodal large language models (MLLMs). Our benchmark addresses the unique challenges posed by long videos, such as maintaining long-term dependen- cies, managing complex temporal dynamics, and processing extensive visual information. Through rigorous evaluation using state-of-the-art methods, we demonstrated the value of HLV- 1K in pushing the boundaries of what current models can achieve in long video understanding. We believe that HLV-1K will serve as a critical resource for the research community, fostering the development of more advanced and capable models that can effectively process and understand long video content, ultimately leading to more sophisticated applications and technologies."}, {"title": "APPENDIX 1: OVERVIEW OF HLV-1K", "content": "We extract frame descriptions and further summarize event descriptions using GPT-40 [28]. The prompts used for frame description extraction and event description summarization are shown in Fig. 5."}, {"title": "B. Frame and Event Descriptions", "content": "An example of the extracted frame description and summa- rized event description is shown in Fig. 6. We also provide statistics on the number of frame descriptions and event descriptions for event videos, as well as the word count for event frame descriptions and event descriptions, as shown in Fig. 7."}, {"title": "C. Source of Multi-level Video Annotations from Frame and Event Descriptions", "content": "The multi-level annotations are derived from various sources of visual descriptions, as summarized in Table III."}, {"title": "D. Examples of Multi-task Annotation", "content": "Examples of annotations for various tasks, including char- acter understanding, counting, object understanding, spatial relationships, descriptive scenes, speed, object direction, scene understanding, attribute change, time order, temporal relation- ships, causality, plot understanding, camera direction, and action understanding, are shown in Fig. 8 and Fig. 9."}, {"title": "VI. APPENDIX 2: MODEL EVALUATION", "content": "To fairly evaluate the performance of various models, we design two unified prompts for both QA and MCQA tasks, respectively. The prompts used for these two different types of tasks across different models are shown in Fig. 10."}]}