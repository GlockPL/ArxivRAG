{"title": "Towards Lightweight Time Series Forecasting: a Patch-wise Transformer with Weak Data Enriching", "authors": ["Meng Wang", "Jintao Yang", "Bin Yang", "Hui Li", "Tongxin Gong", "Bo Yang", "Jiangtao Cui"], "abstract": "Patch-wise Transformer based time series forecasting achieves superior accuracy. However, this superiority relies heavily on intricate model design with massive parameters, rendering both training and inference expensive, thus preventing their deployments on edge devices with limited resources and low latency requirements. In addition, existing methods often work in an autoregressive manner, which take into account only historical values, but ignore valuable, easy-to-obtain context information, such as weather forecasts, date and time of day. To contend with the two limitations, we propose LiPFormer, a novel Lightweight Patch-wise Transformer with weak data enriching. First, to simplify the Transformer backbone, LiPFormer employs a novel lightweight cross-patch attention and a linear transformation-based attention to eliminate Layer Normalization and Feed Forward Network, two heavy components in existing Transformers. Second, we propose a lightweight, weak data enriching module to provide additional, valuable weak supervision to the training. It enhances forecasting accuracy without significantly increasing model complexity as it does not involve expensive, human-labeling but using easily accessible context information. This facilitates the weak data enriching to plug-and-play on existing models. Extensive experiments on nine benchmark time series datasets demonstrate that LiPFormer outperforms state-of-the-art methods in accuracy, while significantly reducing parameter scale, training duration, and GPU memory usage. Deployment on an edge device reveals that LiPFormer only takes only 1/3 inference time compared to classic Transformers. In addition, we demonstrate that the weak data enriching can integrate seamlessly into various Transformer based models to enhance their accuracy, suggesting its generality.", "sections": [{"title": "I. INTRODUCTION", "content": "Time series constitutes a chronological sequence of data points that record successive states of an event. Forecasting is a fundamental task in time series data analysis, which aims to predict future values by tracking historical observations. Time series forecasting has received considerable research attention due to its crucial role in a spectrum of applications, such as finance [1], weather [2], energy [3], and traffic [4], [5], [6], [7], [8], [9], [10]. Accurate forecasts can provide reliable data support, facilitating sound decision-making. For instance, industrial embedded sensors [11] collect real-time operational data (e.g., temperature, pressure) from machinery to anticipate equipment health and failure risks. Power grids [12] utilize historical data from smart device (e.g., power loads, renewable energy production) to predict future power demand and supply, ensuring grid stability.\nTime series forecasting has achieved remarkable advancement with a variety of architectures [13], [14], [15], [16], [17], [18], [19], including the recurrent neural networks (RNNs) [20], [21], [22] and Transformers [23], [24]. Compared to RNNs, Transformers have revolutionized in both natural language processing (NLP) [25] and computer vision (CV) [26] fields via their attention mechanism, enabling better understanding of global correlations. Benefiting from that, specialized Transformer-based architectures for time series forecasting emerged, e.g., Informer [27], Autoformer [28], FEDformer [29]. These works attempt to preserve order information among time series data elements by Positional Encoding (PE). However, unlike natural language, the lack of semantics in numerical data makes PE invalid to capture sequential dependencies. Fortunately, the Patching technique, inspired by a recent linear strategy (DLinear) [30], segments time series data into subseries-level patches and assists Transformer-based models [31], [32], [33] in perceiving the order information.\nDespite the progress made by patch-wise Transformers in time series forecasting, they face two substantial challenges.\nChallenge 1: Intricate Models with Massive Parameters. Heavyweight Transformer models, characterized by complicated modules and extensive parameters, incur prohibitive resource requirements and latency. The vanilla Transformer [23] exhibits $O(N^2)$ complexity (N denotes time series length), rendering it unfriendly for training and deployment in resource-constrained scenarios. Rapid-response time series analysis tasks, increasingly prevalent in industrial [34] and networking [35] domains, are hindered by edge devices' limited computational power and memory to execute intricate algorithms, especially early and low-cost devices. Designing a tailored Transformer model that simultaneously achieves lightweight architecture and enhanced predictive performance is urgently required. The crucial issue lies in differentiating between components in Transformers effective for time series"}, {"title": "II. RELATED WORK", "content": "In this section, we overview existing time series forecasting models classified into four categories: Transformer-based, MLP-based, Deep Learning models, and patch-wise models.\nTransformers: RNNs were introduced to model temporal dependencies [39], [40], [41] for short-term series prediction. A recent study [37] followed the general autoregressive manner to predict PM2.5, leveraging easy-to-obtain categorical information to supervise the prediction. However, RNNs were susceptible to the vanishing gradient when handling long-term series. Based on self-attention mechanism, Transformer models [23] avoided the recurrent structure and were superior in capturing long-term dependencies. Nonetheless, attention mechanism suffers from high time and space complexity $O(N^2)$ and insensitivity to local context. LogTrans [42] proposed sparse convolutional self-attention to reduce complexity to $O(N(log N)^2)$ incorporating local context into attention. Reformer [43] employed a locality-sensitive hashing attention, taking into account attention vectors only within the same hash buckets. Reversible residuals reduced complexity to $O(N log N)$. Zhou et al. [27] introduced ProbSparse self-attention, exploiting sparsity in attention parameters to decrease the complexity. Conformer [44] reduces the complexity of the attention mechanism by $O(N)$ using window attention and a novel RNN network. Triformer [45] leveraged a triangular structure and matrix factorization to achieve linear complexity. Inspired by stochastic processes, Wu et al. [28] devised an autocorrelation mechanism, substituting the traditional attention with a series-wise one. Instead of optimizing attention, PatchTST [31] and Crossformer [32] explored a different patching strategy to achieved efficiency gains. Recently, iTransformer [46] employed variate-wise attention to facilitate information exchange among variables. Despite the above advancements in tackling the local-agnostics issue from various attention mechanisms or patching techniques, they still inadequately capture global sequential trend information, which limited the performance of Transformer models [30].\nMLPs: Transformers had been the prevailing method for time series forecasting until a linear alternative challenged their supremacy. Zeng et al. [30] proposed a direct multi-step DLinear method, essentially an MLP, outperforming conventional Transformers. The authors decomposited time series into trend and seasonal components, which inspired numerous linear studies [33], [47], [48], [49], [50]. MLP-Mixer [33] effectively replaced self-attention with an MLP and contained patch processing. Inspired by visual MLP mixers, Chen et al. [47] devised a two-stage framework with mixer layers and temporal projections. TiDE [48] excelled under channel independence assumptions using a residual structure. Recently, a LightTS [49] framework employed distillation and Pareto optimality techniques to substitute computationally intensive ensemble learning. TimeMixer [51] leveraged multi-resolution sequences in two Mixing modules for past and future feature extraction. While some of these models accounted for the impact of date-related implicit features on predictions, the lack of modeling future weak label (external covariates) limits their ability to exploit prior knowledge, which provides valuable supervision to enhance forecasting.\nDeep learning models: FourierGNN [52] innovatively treats time series values as graph nodes and performs predictions on hypervariable graphs. Deng et al. [53] opted for an alternative policy, designing an SCNN network to individually model each component of the spatio-temporal patterns.\nPatching Models: Most existing approaches directly utilized entire time series as model inputs, whereas PatchTST [31] and TSMixer [33] adopted a distinctive patching strategy. Patch-wise methods divide time series into subseries-level patches. Then it treats patches as input tokens to learn dependencies via attention mechanism. Data in a patch preserve local order information. However, as discussed earlier, patching attention lacks of global order awareness and the fixed patch size fails to accommodate different temporal scales, degrading model generalization."}, {"title": "III. METHODOLOGY", "content": "In this section, we first define the notations and formally describe the time series forecasting task. Then we elaborate the training paradigm and our proposed lightweight Transformer architecture, following the order of data flow."}, {"title": "A. Notations", "content": "Frequently used notations in this paper are defined as follows. c and $c_f$: channels (features) of a mutivarite time series and the corresponding future covariates, they possibly are different; $X_{0:T}^c$: an input time series (sequences) of length T that consists of T historical data points (a.k.a. time steps);\n$F_{T+1:T+L}^{c_f}$: future covariates of length L; $Y_{1:T+L}^{0:c}$ and $Y_{T+1:T+L}^{0:c}$: the ground truth and forecast future sequence values of length L, respectively; $\\hat{Y}_{base}$: intermediate prediction results of Base Predictor, independent of future covariates; n: number of patches; $p_l$: patch length; $n_t$: number of target patches, which is the ratio of predictive length divided by patch length; $h_d$: hidden feature dimensions; b: batch size.\nGiven a historical time series $X_{0:T}^c$ and future covariates $F_{T+1:T+L}^{0:c_f}$, the multivariate forecasting task can be formally defined as the prediction of future values:\n$\\hat{Y}_{T+1:T+L}^{0:c} = H(X_{0:T}^c, F_{T+1:T+L}^{0:c_f}),$\nwhere H denotes the proposed forecasting architecture."}, {"title": "B. Training Methodologies", "content": "LiPFormer involves two main training process, pre-training and prediction-oriented training. The weak label enriching part, depicted as Weakly Supervised Architecture in the top part of Figure 1, is a contrastive learning-based pre-training module with dual encoders that characterize target sequences (i.e., ground truth future time series) and weak labels, respectively. The backbone network, Base Predictor, performs prediction-oriented training over input sequences.\nDeparting from prior representation learning methods that focus on encoding input sequences, we aim to extract representation vectors for easily accessible weak labels. As discussed in Section I, weak label enriching can provide additional and valuable weak supervision to the prediction. We leverage both explicit (textual and numerical external factors, e.g., weather, temperature) and implicit (temporal attributes, e.g., date, time of day) weak labels as future covariates. We follow a paradigm to regard future covariates as expert annotations, and utilize temporal information to augment weak data. Notably, the pre-training of weak labels serves to guide the Base Predictor in making predictions, instead of functioning as encoders to embed themselves as extra features of input time series.\nSince patch-wise Transformers do not possess any decoder, in the absence of explicit weak labels, they cannot leverage future temporal features for guiding the Base Predictor. To this end, we devise the contrastive learning-based pre-training architecture (detailed in Section III-C2) to attain the implicit future feature representations via a joint encoding of \"covariate-target\" pair. Given a batch of b covariate-target pairs, the \u201ccovariate\u201d (resp., \"target\") element corresponds to a representative vector $V_C^{(t)}$ (resp., $V_T^{(t)}$) of future covariates $V_C = \\{V_C^{(1)},...,V_C^{(b)}\\}$ (resp., target sequences $V_T = \\{V_T^{(1)},...,V_T^{(b)}\\}$), where $V_C^{(t)}$ (resp., $V_T^{(t)}$) is of length L. This pre-training process is conducted upon learning the two representation vectors $V_C, V_T$ to estimate which of the $b^2$ pairs actually occurred. Specifically, we train dual encoders, Covariate Encoder and Target Encoder, aiming at maximizing"}, {"title": "C. Model Components", "content": "In this subsection, the first part delves into the intricate architecture of the Base Predictor, outlining its constituent components and functional mechanisms. In the second part (Section III-C2), we elaborate the details of Weakly Supervised dual encoder framework, specifically tailored for the purpose of learning meaningful representations of future weak labels. The entire end-to-end workflow, inclusive of the modular structures and data processing steps for each mini-batch, is depicted in Figure 1.\n1) Base Predictor: As illustrated in Figure 4, the backbone network of LiPFormer first eliminates LN, as the fixed patch size evades the issue with varying input sequence lengths. Secondly, adopting patching and channel independence preserves local order information and extends the receptive horizon to learn correlations between features and time series. Notably, existing patch-wise models commonly adhere to a fixed patch size, treating each patch as an individual token akin to text-oriented models. However, this rigidity in patch scales often hampers the capacity to effectively capture temporally varying periodic attributes inherent in time series. As a result, such models demonstrate limited adaptability to the task of time series prediction. This presents a significant challenge in holistically capturing information encapsulated within both individual patches and the broader sequence context, while maintaining a constant patch length. To overcome this issue, based on the patch division of input sequences, LiPFormer applies two novel patch-wise attention mechanisms, Cross-Patch and Inter-Patch, to comprehensively understand global and local sequential dependencies. Finally, in light of the inherent differences between numerical and textual sequences, the heavyweight FFNs module originally devised for understanding language information is replaced with two different single-layer MLPs, resulting in the base prediction $\\hat{Y}_{base}$.\nInstance Normalization. Time series may encounter distribution shifts. To mitigate this issue, we employ a straightforward normalization approach [30] without significantly increasing model complexity. The initial input time series $x_{0:T}^c$ are subtracted by the last value $x_T^c$ from each element, resulting in a new input sequences $x_{0:T}^c$, and subsequently re-"}, {"title": "D. Univariate Forecasting", "content": "Table V showcases univariate long-term time series forecast outcomes for LiPFormer and competing baselines across the entire ETT benchmark datasets. Notably, among the 32 evaluation metrics, LiPFormer ranks within the top-two in 26 metrics and achieves the best performance in 16 of them. This outstanding performance highlights the superiority of LiP-Former over other methods. These findings further demonstrate the robustness of LiPFormer and reconfirm the capability of attention mechanism in time series forecasting, irrespective of multivariate or univariate contexts."}, {"title": "E. More Analysis", "content": "1) Model Efficiency: We present a thorough comparison of our model's parameters count, training speed, inference time and memory consumption against existing time series forecasting models, using official configurations and identical batch sizes. Tables III illustrate the efficiency comparisons under multivariate datasets.\nBoth LiPFormer and PatchTST significantly outperform the Vanilla Transformer-based models, due to the patching technique. Compared to the state-of-the-art patch-wise PatchTST, LiPFormer still reduces training and inference time by 57%"}, {"title": "V. CONCLUSION", "content": "This paper proposes a Lightweight Patch-wise Transformer with weak label enriching (LiPFormer) for time series forecasting. To simplify the Transformer backbone, it integrates a novel Cross-Patch mechanism into existing patching attention and devises a linear transformation-based attention to eliminate Positional Encoding and two heavy components, Layer Normalization and Feed Forward Networks. A weak label enriching architecture is presented to leverage valuable context information for modeling multimodel future covariates via a dual encoder contrastive learning framework. These innovative mechanisms collectively make LiPFormer a significantly lightweight model, achieving outstanding prediction performance. Deployment on an CPU-only edge device and transplant trials of the weak label enriching module further demonstrate the scalability and versatility of LiPFormer."}]}