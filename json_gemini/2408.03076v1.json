{"title": "Solving QUBO on the Loihi 2 Neuromorphic Processor", "authors": ["Alessandro Pierro", "Philipp Stratmann", "Gabriel Andres Fonseca Guerra", "Sumedh Risbud", "Timothy Shea", "Ashish Rao Mangalore", "Andreas Wild"], "abstract": "In this article, we describe an algorithm for solving Quadratic Unconstrained Binary Optimization problems on\nthe Intel Loihi 2 neuromorphic processor. The solver is based on a hardware-aware fine-grained parallel simulated\nannealing algorithm developed for Intel's neuromorphic research chip Loihi 2. Preliminary results show that our\napproach can generate feasible solutions in as little as 1 ms and up to 37x more energy efficient compared to two\nbaseline solvers running on a CPU. These advantages could be especially relevant for size-, weight-, and power-\nconstrained edge computing applications.", "sections": [{"title": "Introduction", "content": "Mathematical optimization (henceforth, simply optimization) underlies solutions to many problems across industry,\nscience, and society. The goal is to optimize a cost function over continuous or discrete decision variables of these\nproblems and arrive at an optimal decision. Many algorithms solve optimization problems by iteratively updating\nvariables connected by sparse, weighted connections. This approach aligns well with the architecture of neuromorphic\nprocessors. These chips provide fine-granular parallelism to accelerate computation of objective functions and apply\nvariable updates, they integrate compute with memory to reduce the time and energy cost of data movement, and\nthey support sparse message passing to optimize communication for complex, real-world problems. Inspired by this\nfinding, we have previously applied the Intel Loihi 2 neuromorphic processor to two broad optimization problem\ntypes, continuous, convex quadratic programming and combinatorial constraint satisfaction, we have shown that\nthe Loihi architecture can solve such polynomial-time and NP-complete problems faster and orders of magnitude\nmore efficiently than state-of-the-art solvers running on CPU and GPU platforms, our results include general QP\n[1], unconstrained QP with Lagrangian augmentation [2] and constraint satisfaction [2, 3].\nIn this paper, we apply Loihi 2 to the task of solving NP-hard combinatorial problems with discrete variables,\nspecifically quadratic unconstrained binary optimization (QUBO) problems. QUBO is a problem type that, despite\nhaving a simple form, has broad applicability [4]. The goal is to identify the binary variable assignment that optimizes\na quadratic cost function,\nmin E(x) = min x^T Qx\nx\u2208{0,1}^n\nx\u2208{0,1}^n\n(1)\nwithout any constraints\u00b9. Preliminary results presented in this paper, together with the prior work on quadratic\nprogramming [1], demonstrate that Loihi 2 has the potential to solve a wide range of mathematical optimization\nproblems efficiently.\nFinding a global optimum of a general QUBO problem is known to be NP-hard [5]; however, many applications\u2014\nespecially those operating under latency and energy constraints- -are well served by good approximate solutions\nfound by heuristics or stochastic solvers. State-of-the-art solvers for QUBO include variants of Tabu Search and\nSimulated Annealing [6]. Tabu Search [7] explores the search space of a problem in an iterative fashion and creates\na tabu list, which filters out prohibited moves based on different criteria. The efficacy of the tabu search algorithm is\ndependent on a trade-off between speed and memory-usage. A longer tabu list leads to faster approach to a solution\nat the expense of more memory required to store the list. In prior, unpublished work, we have found that the tabu\nsolver included in the D-Wave Samplers package [8] is the fastest and most optimal open-source CPU solver available\nand represents the state-of-the-art for non-parallel QUBO solvers.\nSimulated Annealing (SA) [9], in contrast, harnesses Markov chain Monte Carlo (MCMC) sampling to efficiently\nexplore solution spaces. In the classic formulation, the computational complexity of SA is dominated by the com-\nputation of a vector-matrix product representing the local cost of each variable update. For that reason, a variety\nof parallel implementations have been proposed [10] and implemented on GPUs [11, 12, 13]. However, many QUBO\nformulations of important optimization problems include high levels of unstructured sparsity [14]. GPUs, in contrast,\nhave been optimized for dense matrix arithmetic and their dense parallelism may be underutilized and not efficient\ncompared to a dedicated sparse, serial implementation of SA (e.g. [8]). Second, for large QUBO problems, we observe\nthat SA solver performance on conventional processors tends to be memory-bound due to the need to move matrix\nchunks in and out of the processor cache. Several digital application specific integrated circuits (digital ASICs)\nhave been specifically designed and optimized for efficient, parallel SA, including Toshiba's FPGA-based Simulated\nBifurcation Machine [13, 15, 16, 17, 18], Fujitsu's Digital Annealer [19], Hitachi's CMOS Annealer [20], various\nanalogue compute mechanisms [21, 22], and D-Wave's Quantum Annealer [23, 24]. So far, several factors have lim-\nited applicability of these platforms: analogue and quantum hardware platforms suffer from noise and impose strict"}, {"title": "Hardware-aware simulated annealing", "content": "We set out to develop an algorithm inspired by classic simulated annealing (SA) algorithm, that leverages the features\nand satisfies the constraints of Intel Loihi 2. This endeavor is inspired by the observation that SA matches the unique\nset of features of neuromorphic computing well, as elaborated in table 1."}, {"title": "Conventional simulated annealing", "content": "SA is a a widely used meta-heuristic that has a Boltzmann machine at its core. These are neural networks wherein\neach neuron n_i encodes the evolution of a binary variable x_i and computes the change \\Delta E_i in the overall energy\npotentially induced by flipping of x_i (0 \\rightarrow 1 or 1 \\rightarrow 0). The Boltzmann machine flips a variable with probability\n1 if such a change of state reduces the overall energy, i.e. \\Delta E_i < 0. If flipping x_i increases the over energy (i.e.,\n\\Delta E_i > 0), the variable is flipped with probability\np_x exp(-\\frac{\\Delta E}{T}).\n(2)"}, {"title": "Simulated annealing by neural dynamics", "content": "The SA framework was used to design a spiking neural network (SNN) architecture which (1) represents a QUBO\nproblem, (2) stores a candidate solution for this problem, and (3) computes the QUBO cost of the candidate solution.\nGiven a variable assignment x, the computation of the cost function can be decomposed as\nC(x) = x^T Qx\n(3)\n=\\sum_{i,j=0}^{n-1} q_{ij} x_i x_j\n(4)\n=\\sum_{i=0}^{n-1} \\Big( x_i q_{ii} + \\sum_{j \\neq i} q_{ij} x_j \\Big),\n(5)\nand we will exploit this last formulation to parallelize the computation.\nIn the QUBO SNN architecture, outlined in Figure 1, the variable assignment is maintained by an n-dimensional\narray of variable neurons. At a given step t, the i-th variable neuron stores the i-th binary component of the current\ncandidate solution x^{(t)}, as well as its value in the previous two steps. These neurons are connected together by a\nsynaptic layer encoding the off-diagonal elements of Q matrix. Thus, spiking the current binary variable assignment,"}, {"title": "Parallel variable updates", "content": "The inherent parallelism of the QUBO SNN architecture can be exploited to evaluate the Metropolis criterion for all\npossible moves with unit Hamming distance. For ease of notation, let's denote with \\Delta C_i(x) the variation in total cost\nassociated to flipping the state of x_i, i.e., changing it from 0 to 1 or vice-versa. It can be computed from equation\n(5), considering only the elements containing x_i, as\n\\Delta C_i(x) = \\pm \\Big[ q_{ii} + x_i \\sum_{j \\neq i} q_{ji} + \\sum_{j \\neq i} x_j q_{ij} \\Big]\n(11)\n= \\pm \\Big[ q_{ii} + \\sum_{j \\neq i} (q_{ji} + q_{ij}) x_j \\Big]\n(12)\nand, exploiting the assumption of symmetry q_{ij} = q_{ji}, reduced to\n= \\pm \\Big[ q_{ii} + 2 \\sum_{j \\neq i} q_{ji} \\Big]\n(13)\nwhere the sign is chosen based on the direction of the change, positive for changing x_i from 0 to 1 and negative\notherwise. Each neuron can compute equation (13) purely based on information that is locally available in the\nmemory of this neuron, thus the parallel processors implementing the neurons can independently and in parallel\ncalculate the equation to the impact of flipping its state on the total cost in parallel as\n\\Delta C_i(x^{(t)}) = \\Big( q_{ii} + 2 z_i^{(t+1)} \\Big) \\begin{cases}\n+1 & \\text{if } x_i^{(t)} = 0 \\\\\n-1 & \\text{if } x_i^{(t)} = 1\n\\end{cases}\n(14)\nHence, each neuron can sample a random number from the uniform distribution U(0, 1) and, following equation (2),\nupdate its state with probability\np_i = \\text{min} \\Big( \\text{exp} \\big( - \\frac{\\Delta C_i(x^{(t)})}{T} \\big), 1 \\Big)\n(15)\nEach neuron encodes a copy of the temperature T at the beginning of the run and update it according to the same\nannealing schedule. This, again, allows the processors encoding the neurons to work independently on their own\nintegrated memory and thus to update all neurons in parallel."}, {"title": "Stochastic refractory periods", "content": "Parallel simulated annealing raises the issue that if two or more variables fulfill the Boltzmann condition, their joint\nflipping can worsen the solution due to interaction terms q_{ij}. Several solutions have been proposed to solve this\nissue [10]. Fujitsu's parallel Digital Annealer [36], for example, solves this by using parallelism solely to determine\nin parallel which of the variables are suitable to fulfill the Boltzmann condition, while flipping only a single one of\nthem.\nTo speed up the convergence, we also enable parallel updates of many neurons, which pushes the Boltzmann\nmachine away from a near-equilibrium state where neurons flip one at a time. In this non-equilibrium Boltzmann\nmachine (NEBM), we introduce a stochastic refractory period, preventing neurons from repeatedly flipping variables\nin successive steps. Specifically, after a variable neuron changes its state, from 0 to 1 or vice-versa, further changes\nare inhibited for a random number of iterations. This change in the neural dynamics results in a reduced number of\nsimultaneous variables updates per step, thus addressing the reported issue. Moreover, the distribution of duration\nof the refractory period can be tuned to explicitly control the level of parallelization, with longer refractory periods\nresulting in more sequential updates."}, {"title": "Loihi 2 implementation", "content": "Since Loihi 2 is conceived as an accelerator for spiking neural networks, its instruction set is optimized for the\nmost common operations in this domain. The instruction set is thus constrained, which requires careful re-design\nof algorithms. As a reward, the resulting Loihi 2 architecture enables orders of magnitude gains in terms of latency\nand energy requirements, compared to classical CPUs.\nEach Loihi 2 chip [37] features 128 neurocores that can simulate up to 8192 neurons each in parallel. Each\nneurocore is equipped with its own integrated memory, so that each neuron is equipped with its own local memory\nthat allows quick and efficient iterative updates of neuronal states. The neural dynamics can be defined by the user\nusing flexible micro-code, although computationally expensive operations like exponentiation and division are not\nsupported for performance reasons. In each time step, each neuron receives synaptic inputs, updates its memory\nstates, and can send a 32bit graded spike via synapses of 8-bit weights to other neurons. For more general but less\nperformant instructions, each Loihi 2 chip features also embedded CPUs.\nEvaluating the Metropolis criterion detailed in equation (2) on Loihi 2 neuro-cores presents different challenges.\nIn particular, given the missing support for exponentiation and division, the inequality cannot be computed directly.\nMoreover, the condition needs to be evaluated at each step for all the variable neurons, making it inefficient to\ndelegate its execution to an embedded CPUs. For this reason, we introduce an integer approximation to evaluate\nthe stochastic switching condition.\nAt each step, given an estimated change in cost \\Delta C, the temperature level T, and a random number r\\in [0, 1],\nthe change in the variable state is accepted if and only if\n\\text{exp}\\big( - \\frac{\\Delta C}{T} \\big) > 2 r.\n(16)\nEach Loihi 2 neuron has access to a new a pseudo-random number in each step. The generator produces integer\nvalues \\text{rand} \\in [0, 2^{24} \u2013 1] which can be used in the modified switching condition\n\\text{exp}\\big( - \\frac{\\Delta C}{T} \\big) > \\frac{\\text{rand}}{2^{24} - 1}.\n(17)\nChanging the exponentiation to base 2, which can be evaluated on Loihi, we obtain\n\\text{exp}_2 \\big( - \\frac{\\Delta C}{T} \\big) > \\frac{\\text{rand}}{2^{24} - 1},\n(18)\nwhere the previous temperature T is substituted with the change of variable\n\\hat{T} = \\frac{T}{\\text{log}_2 e},\n(19)"}, {"title": "Benchmarking methods", "content": "We analyzed the performance of our neuromorphic SA algorithm as an optimization solver for QUBO problems\non a preliminary version of the NeuroBench benchmark [38]. The benchmark features a set of QUBO worklodas\nthat search for the maximum independent sets of graphs. The goal of our benchmarking was to understand how\nthe proposed solution compares to existing solutions on CPU, in terms of quality of the solutions, latency, energy\nconsumption and scalability.\nWhen benchmarking optimization algorithms, two common approaches are typically possible: one defines a target\nsolution quality and measures the time taken by different solvers to achieve it, while the other sets a target run-time\nand evaluates the quality of the solutions obtained. In our experiments, we opted for the latter approach. At any\ngiven timeout, the quality of the solution is evaluated as the percentage gap from the best known solution (BKS) of\nthe associated instance:\n\\text{gap}\\% (x) = 100 \\frac{\\text{min}(C(x), 0) - C(x_{BKS})}{C(x_{BKS})},\n(24)\nwhere the cost C(x) is truncated with a zero upper-bound. This choice intends to address the fact that some solvers\nreport a best cost of 0, associated to the trivial all-zeros solutions, when no better solution is found. Hence, the\nmetric can only assume values in [0, 100], with a value of 0% corresponding to a solution as good as the best known\none, and a value of 100% corresponding to the worst case of the all-zero solutions.\nWe adopted two different CPU solvers for the comparison: the SA solver and the TS solver implemented in\nthe D-Wave Samplers v1.1.0 library [8]. The library was compiled on Ubuntu 20.04.6 LTS with GCC 9.4.0 and\nPython 3.8.10. All measurements were obtained on a machine with Intel Core i9-7920X CPU @ 2.90 GHz 2 and\n128GB of DDR4 RAM, using Intel SoC Watch for Linux OS 2023.2.0. Access to the code of our Loihi 2 solver used\nin these experiments is provided through the Intel Neuromorphic Research Community\u00b3. The neuromorphic SA was\nexecuted on a Kapoho Point board using a single Loihi 2 chip, with Lava 0.8.0 and Lava Optimization 0.3.0."}, {"title": "Maximum independent sets as benchmarking data set", "content": "Given an undirected graph G = (V,E), an independent set I is a subset of V such that, for any two vertices u, v \u2208 I,\nthere is no edge connecting them. The Maximum Independent Set (MIS) problem consists in finding an independent"}, {"title": "Quality of the solutions", "content": "Figure 2 reports the results on solution quality for the instances with 15% edge density, as measured by the percentage\ngap from the BKS.\nThe relative performance of the solvers can be categorized in three different regimes. For tight time constraints,\nat 10^{-2} s timeout or less, the CPU solvers struggle to produce good solution for large instances. In particular,\nour neuromorphic solver provides feasible solutions for instances up to 40\u00d7 larger compared to SA, and 4\u00d7 larger\ncompared to TABU. At intermediate timeout values, the three solvers demonstrate similar performance (with a small\nadvantage for TABU), all producing solutions with a percentage gap lower than 20% across all problem sizes. For\nlonger run times, lasting 10 s or more, TABU provides the lowest percentage gap, followed by SA on CPU and our\nproposed solution. However, the differences in percentage gap between the three solvers are quite limited and keep\nreducing with increased timeouts, suggesting that all three algorithmic approaches would eventually reach the same\nlevel of optimality.\nOur results in figure 2 present an unexpected behaviour. In particular, for some combinations of solvers and\ntimeouts, instances with 1000 variables were solved with a better percentage gap compared to instances with 500"}, {"title": "Power consumption", "content": "To evaluate the energy efficiency of neuromorphic hardware compared to baseline algorithms, we profiled the power\nconsumption of the three solvers on the full set of MIS instances. Since MIS workloads were run for a fixed timeout,\ndifferences in power consumption are equivalent to differences in energy consumption of the chips. The main results\nare reported in figure 3.\nOur neuromorphic SA running on Loihi 2 has an average total power of 2.62 W, while the SA and TABU solvers\nreach respectively 87.78 W and 97.56 W. Hence, the proposed solution results in 33.5\u00d7 lower energy consumption\ncompared to SA on CPU, and 37.24\u00d7 lower compared to TABU. Moreover, the plots show how the energy consump-\ntion of Loihi slightly increases with problem size, while the CPU solvers have a constant power overall. This trend\ncan be explained by the fact that, while Loihi can exploit multiple cores based on problem size, the CPU solvers are\nlimited to a single core."}, {"title": "Discussion", "content": "This paper details the architecture, implementation, and performance of our hardware-aware, fine-grained parallel\nsimulated annealing algorithm on Loihi 2 for solving QUBO problems. We observe that the structure of combinatorial\noptimization algorithms is well-suited to neuromorphic hardware architectures like Loihi 2. Our solver exploited this\nsynergy to find feasible solutions to QUBO problems in as little as 1 ms, and with up to 37\u00d7 lower energy consumption\nthan a state-of-the-art Tabu solver on CPU. Continuing work on the Loihi 2-aware algorithm, advanced partitioning\non the Loihi 2 chip, and scaling to multi-chip systems promises to further improve the time to solution, energy\nconsumption, and optimality of the solver.\nA variety of well-known problems can be formulated as QUBO, often exhibiting a significant degree of unstruc-\ntured sparsity well-suited to our approach [14]. For commercial applications with size, weight, and power (SWaP)\nconstraints, such as those faced in edge computing contexts, our approach could enable significant benefits compared\nto state-of-the-art methods. For example, in mobile robotics, a fast, energy efficient QUBO solver could support\nrouting, path planning [44], and robotic scheduling [45] with lower latency, longer battery life, and the capacity to\nhandle more complex scenarios. Many industrial applications running on high-performance computing could also\nbenefit from significantly faster QUBO solvers. In finance for example, this could include problems such as portfolio\noptimization [46], high-frequency arbitrage trading [47], and credit-risk assessment [48].\nThe presented algorithm and preliminary results leave several limitations to be addressed in future research. First,\nthe present study provides results for problems up to 1000 variables, and we have further verified the capacity to\nscale to roughly 4000 variables on a single Loihi 2 chip. But in its current implementation, the solver is not capable\nof scaling up to very large-scale problems (e.g. up to 1M variables) due to limitations in the synaptic encoding and\ntransmission of messages in multi-chip networks on Loihi 2. Future work will investigate algorithmic and software\nsolutions to achieve state-of-the-art problem size, such as problem decomposition. Second, these initial results focus\non maximum independent set problems due to the ability to arbitrarily scale and sparsify these synthetic problems.\nIn additional testing, we observe that many other QUBO problems possess significantly greater complexity than\nMIS, as expressed in the roughness of the QUBO cost landscape and in the number of acceptably near-optimal\nsolutions in the search space. Additional research should extend these initial benchmarks to a broader representation\nof real-world applications to properly understand the performance benefits of our approach. Third, some established\nQUBO formulations, such as the Traveling Salesman Problem, require a numerical precision that is unachievable\nwith our current implementation of the algorithm, which is restricted to 8-bit integer quadratic cost coefficients.\nFor such problems, the solver could combine multiple Loihi 2 synapses or leverage scaled numerical representations\nto achieve higher dynamic range. Finally, future work should evaluate the performance of our algorithm against"}]}