{"title": "Solving QUBO on the Loihi 2 Neuromorphic Processor", "authors": ["Alessandro Pierro", "Philipp Stratmann", "Gabriel Andres Fonseca Guerra", "Sumedh Risbud", "Timothy Shea", "Ashish Rao Mangalore", "Andreas Wild"], "abstract": "In this article, we describe an algorithm for solving Quadratic Unconstrained Binary Optimization problems on the Intel Loihi 2 neuromorphic processor. The solver is based on a hardware-aware fine-grained parallel simulated annealing algorithm developed for Intel's neuromorphic research chip Loihi 2. Preliminary results show that our approach can generate feasible solutions in as little as 1 ms and up to 37x more energy efficient compared to two baseline solvers running on a CPU. These advantages could be especially relevant for size-, weight-, and power- constrained edge computing applications.", "sections": [{"title": "Introduction", "content": "Mathematical optimization (henceforth, simply optimization) underlies solutions to many problems across industry, science, and society. The goal is to optimize a cost function over continuous or discrete decision variables of these problems and arrive at an optimal decision. Many algorithms solve optimization problems by iteratively updating variables connected by sparse, weighted connections. This approach aligns well with the architecture of neuromorphic processors. These chips provide fine-granular parallelism to accelerate computation of objective functions and apply variable updates, they integrate compute with memory to reduce the time and energy cost of data movement, and they support sparse message passing to optimize communication for complex, real-world problems. Inspired by this finding, we have previously applied the Intel Loihi 2 neuromorphic processor to two broad optimization problem types, continuous, convex quadratic programming and combinatorial constraint satisfaction, we have shown that the Loihi architecture can solve such polynomial-time and NP-complete problems faster and orders of magnitude more efficiently than state-of-the-art solvers running on CPU and GPU platforms, our results include general QP [1], unconstrained QP with Lagrangian augmentation [2] and constraint satisfaction [2, 3].\nIn this paper, we apply Loihi 2 to the task of solving NP-hard combinatorial problems with discrete variables, specifically quadratic unconstrained binary optimization (QUBO) problems. QUBO is a problem type that, despite having a simple form, has broad applicability [4]. The goal is to identify the binary variable assignment that optimizes a quadratic cost function,\nmin E(x) = min x^T Qx\nx\u2208{0,1}^n x\u2208{0,1}^n\nwithout any constraints\u00b9. Preliminary results presented in this paper, together with the prior work on quadratic programming [1], demonstrate that Loihi 2 has the potential to solve a wide range of mathematical optimization problems efficiently.\nFinding a global optimum of a general QUBO problem is known to be NP-hard [5]; however, many applications\u2014 especially those operating under latency and energy constraints- -are well served by good approximate solutions found by heuristics or stochastic solvers. State-of-the-art solvers for QUBO include variants of Tabu Search and Simulated Annealing [6]. Tabu Search [7] explores the search space of a problem in an iterative fashion and creates a tabu list, which filters out prohibited moves based on different criteria. The efficacy of the tabu search algorithm is dependent on a trade-off between speed and memory-usage. A longer tabu list leads to faster approach to a solution at the expense of more memory required to store the list. In prior, unpublished work, we have found that the tabu solver included in the D-Wave Samplers package [8] is the fastest and most optimal open-source CPU solver available and represents the state-of-the-art for non-parallel QUBO solvers.\nSimulated Annealing (SA) [9], in contrast, harnesses Markov chain Monte Carlo (MCMC) sampling to efficiently explore solution spaces. In the classic formulation, the computational complexity of SA is dominated by the com- putation of a vector-matrix product representing the local cost of each variable update. For that reason, a variety of parallel implementations have been proposed [10] and implemented on GPUs [11, 12, 13]. However, many QUBO formulations of important optimization problems include high levels of unstructured sparsity [14]. GPUs, in contrast, have been optimized for dense matrix arithmetic and their dense parallelism may be underutilized and not efficient compared to a dedicated sparse, serial implementation of SA (e.g. [8]). Second, for large QUBO problems, we observe that SA solver performance on conventional processors tends to be memory-bound due to the need to move matrix chunks in and out of the processor cache. Several digital application specific integrated circuits (digital ASICs) have been specifically designed and optimized for efficient, parallel SA, including Toshiba's FPGA-based Simulated Bifurcation Machine [13, 15, 16, 17, 18], Fujitsu's Digital Annealer [19], Hitachi's CMOS Annealer [20], various analogue compute mechanisms [21, 22], and D-Wave's Quantum Annealer [23, 24]. So far, several factors have lim- ited applicability of these platforms: analogue and quantum hardware platforms suffer from noise and impose strict"}, {"title": "Hardware-aware simulated annealing", "content": "We set out to develop an algorithm inspired by classic simulated annealing (SA) algorithm, that leverages the features and satisfies the constraints of Intel Loihi 2. This endeavor is inspired by the observation that SA matches the unique set of features of neuromorphic computing well, as elaborated in table 1."}, {"title": "Conventional simulated annealing", "content": "SA is a a widely used meta-heuristic that has a Boltzmann machine at its core. These are neural networks wherein each neuron ni encodes the evolution of a binary variable xi and computes the change \u2206\u0395i in the overall energy potentially induced by flipping of xi (0 \u2192 1 or 1 \u2192 0). The Boltzmann machine flips a variable with probability 1 if such a change of state reduces the overall energy, i.e. \u0394\u0395i < 0. If flipping xi increases the over energy (i.e., \u0394\u0395i > 0), the variable is flipped with probability\npx exp(-\\frac{\u0394\u0395}{T})."}, {"title": "Simulated annealing by neural dynamics", "content": "The SA framework was used to design a spiking neural network (SNN) architecture which (1) represents a QUBO problem, (2) stores a candidate solution for this problem, and (3) computes the QUBO cost of the candidate solution. Given a variable assignment x, the computation of the cost function can be decomposed as\nC(x) = x^T Qx\n= \u2211 Aij xix j\ni,j=0\n= \u2211xi [qi+\u2211qixj],\ni=0 j\u2260i"}, {"title": "Parallel variable updates", "content": "The inherent parallelism of the QUBO SNN architecture can be exploited to evaluate the Metropolis criterion for all possible moves with unit Hamming distance. For ease of notation, let's denote with ACi(x) the variation in total cost associated to flipping the state of xi, i.e., changing it from 0 to 1 or vice-versa. It can be computed from equation (5), considering only the elements containing xi, as\n\u25b3Ci(x) = \u00b1 [qi+xjqji+\u2211Xjqij]\nj\u2260i j\u2260i\n= \u00b1 [qi+\u2211(qji + qij)Xj]\nj\u2260i\nand, exploiting the assumption of symmetry qij = qji, reduced to\n= \u00b1 [qi+2\u2211qjj]\nj\u2260i\nwhere the sign is chosen based on the direction of the change, positive for changing xi from 0 to 1 and negative otherwise. Each neuron can compute equation (13) purely based on information that is locally available in the memory of this neuron, thus the parallel processors implementing the neurons can independently and in parallel calculate the equation to the impact of flipping its state on the total cost in parallel as\n\u25b3Ci(x(t)) = (qi + 2z+1) {+1 if x(t) = 0\n-1 if x(t) = 1\nHence, each neuron can sample a random number from the uniform distribution U(0, 1) and, following equation (2), update its state with probability\nPi = min (exp(-\\frac{ACi(x(t))}{T}), 1)\nEach neuron encodes a copy of the temperature T at the beginning of the run and update it according to the same annealing schedule. This, again, allows the processors encoding the neurons to work independently on their own integrated memory and thus to update all neurons in parallel."}, {"title": "Stochastic refractory periods", "content": "Parallel simulated annealing raises the issue that if two or more variables fulfill the Boltzmann condition, their joint flipping can worsen the solution due to interaction terms Qij. Several solutions have been proposed to solve this issue [10]. Fujitsu's parallel Digital Annealer [36], for example, solves this by using parallelism solely to determine in parallel which of the variables are suitable to fulfill the Boltzmann condition, while flipping only a single one of them.\nTo speed up the convergence, we also enable parallel updates of many neurons, which pushes the Boltzmann machine away from a near-equilibrium state where neurons flip one at a time. In this non-equilibrium Boltzmann machine (NEBM), we introduce a stochastic refractory period, preventing neurons from repeatedly flipping variables in successive steps. Specifically, after a variable neuron changes its state, from 0 to 1 or vice-versa, further changes are inhibited for a random number of iterations. This change in the neural dynamics results in a reduced number of simultaneous variables updates per step, thus addressing the reported issue. Moreover, the distribution of duration of the refractory period can be tuned to explicitly control the level of parallelization, with longer refractory periods resulting in more sequential updates."}, {"title": "Loihi 2 implementation", "content": "Since Loihi 2 is conceived as an accelerator for spiking neural networks, its instruction set is optimized for the most common operations in this domain. The instruction set is thus constrained, which requires careful re-design of algorithms. As a reward, the resulting Loihi 2 architecture enables orders of magnitude gains in terms of latency and energy requirements, compared to classical CPUs.\nEach Loihi 2 chip [37] features 128 neurocores that can simulate up to 8192 neurons each in parallel. Each neurocore is equipped with its own integrated memory, so that each neuron is equipped with its own local memory that allows quick and efficient iterative updates of neuronal states. The neural dynamics can be defined by the user using flexible micro-code, although computationally expensive operations like exponentiation and division are not supported for performance reasons. In each time step, each neuron receives synaptic inputs, updates its memory states, and can send a 32bit graded spike via synapses of 8-bit weights to other neurons. For more general but less performant instructions, each Loihi 2 chip features also embedded CPUs.\nEvaluating the Metropolis criterion detailed in equation (2) on Loihi 2 neuro-cores presents different challenges. In particular, given the missing support for exponentiation and division, the inequality cannot be computed directly. Moreover, the condition needs to be evaluated at each step for all the variable neurons, making it inefficient to delegate its execution to an embedded CPUs. For this reason, we introduce an integer approximation to evaluate the stochastic switching condition.\nAt each step, given an estimated change in cost AC, the temperature level T, and a random number r\u2208 [0, 1], the change in the variable state is accepted if and only if\nexp(-\\frac{AC}{T}) > 2r.\nEach Loihi 2 neuron has access to a new a pseudo-random number in each step. The generator produces integer values rand \u2208 [0, 224 \u2013 1] which can be used in the modified switching condition\nexp(-\\frac{AC}{T}) > \\frac{rand}{2^{24}-1}.\nChanging the exponentiation to base 2, which can be evaluated on Loihi, we obtain\nexp2(-\\frac{AC}{T}) > \\frac{rand}{2^{24}-1}\nwhere the previous temperature T is substituted with the change of variable\nT = \\frac{T}{log_2 e},"}, {"title": "Benchmarking methods", "content": "We analyzed the performance of our neuromorphic SA algorithm as an optimization solver for QUBO problems on a preliminary version of the NeuroBench benchmark [38]. The benchmark features a set of QUBO worklodas that search for the maximum independent sets of graphs. The goal of our benchmarking was to understand how the proposed solution compares to existing solutions on CPU, in terms of quality of the solutions, latency, energy consumption and scalability.\nWhen benchmarking optimization algorithms, two common approaches are typically possible: one defines a target solution quality and measures the time taken by different solvers to achieve it, while the other sets a target run-time and evaluates the quality of the solutions obtained. In our experiments, we opted for the latter approach. At any given timeout, the quality of the solution is evaluated as the percentage gap from the best known solution (BKS) of the associated instance:\ngap% (x) = 100 \\frac{min(C(x), 0) - C(xBKS)}{C(xBKS)}\nwhere the cost C(x) is truncated with a zero upper-bound. This choice intends to address the fact that some solvers report a best cost of 0, associated to the trivial all-zeros solutions, when no better solution is found. Hence, the metric can only assume values in [0, 100], with a value of 0% corresponding to a solution as good as the best known one, and a value of 100% corresponding to the worst case of the all-zero solutions.\nWe adopted two different CPU solvers for the comparison: the SA solver and the TS solver implemented in the D-Wave Samplers v1.1.0 library [8]. The library was compiled on Ubuntu 20.04.6 LTS with GCC 9.4.0 and Python 3.8.10. All measurements were obtained on a machine with Intel Core i9-7920X CPU @ 2.90 GHz 2 and 128GB of DDR4 RAM, using Intel SoC Watch for Linux OS 2023.2.0. Access to the code of our Loihi 2 solver used in these experiments is provided through the Intel Neuromorphic Research Community\u00b3. The neuromorphic SA was executed on a Kapoho Point board using a single Loihi 2 chip, with Lava 0.8.0 and Lava Optimization 0.3.0."}, {"title": "Maximum independent sets as benchmarking data set", "content": "Given an undirected graph G = (V,E), an independent set I is a subset of V such that, for any two vertices u, v \u2208 I, there is no edge connecting them. The Maximum Independent Set (MIS) problem consists in finding an independent set with maximum cardinality. It has been shown in the literature that the MIS problem is strongly NP-hard for a variety of graph structures [39]. MIS has been applied to solve many industrial problems, e.g., the distribution of frequencies to 5G or WiFi access points without interference [40], the design of error-correcting code [41] or the design of fault-tolerant semiconductor chips with redundant communication vias [42].\nThe MIS problem has a natural QUBO formulation: for each node u \u2208 V in the graph, a binary variable xu is introduced to model the inclusion or not of u in the candidate solution. Summing the quadratic terms 2 will thus result in the size of the set of selected nodes. To penalize the selection of nodes that are not mutually independent, a penalization term is associated to the interactions xuxv if u and v are connected. The resulting Q matrix coefficients are defined as\nq_{uv} = {-1 if u = v\n\u03bb if u != v and (u, v) \u2208 \u0395\n0 otherwise\nwhere \u03bb 0 is a large penalization term."}, {"title": "Instances", "content": "We benchmarked on a set of randomly-generated MIS instances from the NeuroBench data set [38]: given a number of nodes n, a density value d and a random seeds, the MISProblem generator produces an adjacency matrix. The associated QUBO formulation is then obtained based on equation (25). As reported in table 2, we considered sizes from 10 to 103 and edge densities from 5% to 30%, for a total of 105 instances.\nIn order to obtain the BKS, we adopted two different strategies depending on problem size. The instances with up to 250 nodes were solved using a branch-and-bound method from Gurobi [43], obtaining the optimal solutions. However, solving to optimality larger instances would have required substantially more time and computational power\u00b9, and was thus beyond the scope of this thesis. Hence, for instances with 500 or more variables, we obtained the BKS executing the TABU solver on CPU with a timeout of 600 s."}, {"title": "Benchmarking results", "content": "We applied the three solvers on the MIS set of instances for different timeout values, from 10-3 s to 103 s, repeating each experiment with five initial points and random seeds."}, {"title": "Quality of the solutions", "content": "Figure 2 reports the results on solution quality for the instances with 15% edge density, as measured by the percentage gap from the BKS.\nThe relative performance of the solvers can be categorized in three different regimes. For tight time constraints, at 10-2 s timeout or less, the CPU solvers struggle to produce good solution for large instances. In particular, our neuromorphic solver provides feasible solutions for instances up to 40\u00d7 larger compared to SA, and 4\u00d7 larger compared to TABU. At intermediate timeout values, the three solvers demonstrate similar performance (with a small advantage for TABU), all producing solutions with a percentage gap lower than 20% across all problem sizes. For longer run times, lasting 10 s or more, TABU provides the lowest percentage gap, followed by SA on CPU and our proposed solution. However, the differences in percentage gap between the three solvers are quite limited and keep reducing with increased timeouts, suggesting that all three algorithmic approaches would eventually reach the same level of optimality.\nOur results in figure 2 present an unexpected behaviour. In particular, for some combinations of solvers and timeouts, instances with 1000 variables were solved with a better percentage gap compared to instances with 500"}, {"title": "Power consumption", "content": "To evaluate the energy efficiency of neuromorphic hardware compared to baseline algorithms, we profiled the power consumption of the three solvers on the full set of MIS instances. Since MIS workloads were run for a fixed timeout, differences in power consumption are equivalent to differences in energy consumption of the chips. The main results are reported in figure 3.\nOur neuromorphic SA running on Loihi 2 has an average total power of 2.62 W, while the SA and TABU solvers reach respectively 87.78 W and 97.56 W. Hence, the proposed solution results in 33.5\u00d7 lower energy consumption compared to SA on CPU, and 37.24\u00d7 lower compared to TABU. Moreover, the plots show how the energy consump- tion of Loihi slightly increases with problem size, while the CPU solvers have a constant power overall. This trend can be explained by the fact that, while Loihi can exploit multiple cores based on problem size, the CPU solvers are limited to a single core."}, {"title": "Discussion", "content": "This paper details the architecture, implementation, and performance of our hardware-aware, fine-grained parallel simulated annealing algorithm on Loihi 2 for solving QUBO problems. We observe that the structure of combinatorial optimization algorithms is well-suited to neuromorphic hardware architectures like Loihi 2. Our solver exploited this synergy to find feasible solutions to QUBO problems in as little as 1 ms, and with up to 37\u00d7 lower energy consumption than a state-of-the-art Tabu solver on CPU. Continuing work on the Loihi 2-aware algorithm, advanced partitioning on the Loihi 2 chip, and scaling to multi-chip systems promises to further improve the time to solution, energy consumption, and optimality of the solver.\nA variety of well-known problems can be formulated as QUBO, often exhibiting a significant degree of unstruc- tured sparsity well-suited to our approach [14]. For commercial applications with size, weight, and power (SWaP) constraints, such as those faced in edge computing contexts, our approach could enable significant benefits compared to state-of-the-art methods. For example, in mobile robotics, a fast, energy efficient QUBO solver could support routing, path planning [44], and robotic scheduling [45] with lower latency, longer battery life, and the capacity to handle more complex scenarios. Many industrial applications running on high-performance computing could also benefit from significantly faster QUBO solvers. In finance for example, this could include problems such as portfolio optimization [46], high-frequency arbitrage trading [47], and credit-risk assessment [48].\nThe presented algorithm and preliminary results leave several limitations to be addressed in future research. First, the present study provides results for problems up to 1000 variables, and we have further verified the capacity to scale to roughly 4000 variables on a single Loihi 2 chip. But in its current implementation, the solver is not capable of scaling up to very large-scale problems (e.g. up to 1M variables) due to limitations in the synaptic encoding and transmission of messages in multi-chip networks on Loihi 2. Future work will investigate algorithmic and software solutions to achieve state-of-the-art problem size, such as problem decomposition. Second, these initial results focus on maximum independent set problems due to the ability to arbitrarily scale and sparsify these synthetic problems. In additional testing, we observe that many other QUBO problems possess significantly greater complexity than MIS, as expressed in the roughness of the QUBO cost landscape and in the number of acceptably near-optimal solutions in the search space. Additional research should extend these initial benchmarks to a broader representation of real-world applications to properly understand the performance benefits of our approach. Third, some established QUBO formulations, such as the Traveling Salesman Problem, require a numerical precision that is unachievable with our current implementation of the algorithm, which is restricted to 8-bit integer quadratic cost coefficients. For such problems, the solver could combine multiple Loihi 2 synapses or leverage scaled numerical representations to achieve higher dynamic range. Finally, future work should evaluate the performance of our algorithm against"}]}