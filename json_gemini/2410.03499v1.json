{"title": "FEDSTEIN: ENHANCING MULTI-DOMAIN FEDERATED\nLEARNING THROUGH JAMES-STEIN ESTIMATOR", "authors": ["Sunny Gupta", "Nikita Jangid", "Amit Sethi"], "abstract": "Federated Learning (FL) facilitates data privacy by enabling collaborative in-situ training across\ndecentralized clients. Despite its inherent advantages, FL faces significant challenges of performance\nand convergence when dealing with data that is not independently and identically distributed (non-\ni.i.d.). While previous research has primarily addressed the issue of skewed label distribution across\nclients, this study focuses on the less explored challenge of multi-domain FL, where client data\noriginates from distinct domains with varying feature distributions. We introduce a novel method\ndesigned to address these challenges \u2013 FedStein: Enhancing Multi-Domain Federated Learning\nThrough the James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS) estimates of\nbatch normalization (BN) statistics across clients, while maintaining local BN parameters. The non-\nBN layer parameters are exchanged via standard FL techniques. Extensive experiments conducted\nacross three datasets and multiple models demonstrate that FedStein surpasses existing methods\nsuch as FedAvg and FedBN, with accuracy improvements exceeding 14% in certain domains leading\nto enhanced domain generalization. The code is available at https://github.com/sunnyinAI/\nFedStein", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) represents a transformative paradigm in machine learning, enabling collaborative modelling\nacross decentralized devices while maintaining local data privacy. Unlike traditional centralized methods, FL conducts\nmodel training directly on individual devices, transmitting only model updates instead of raw data. This approach not\nonly preserves data privacy, but also aligns with stringent data governance standards, making FL particularly appealing\nin a variety of domains, including healthcare [1, 2], mobile devices [3, 4], and autonomous vehicles [5, 6, 7]. However,\nFL faces significant challenges when applied to data that are not independently and identically distributed (non-i.i.d.)\nbetween different clients [8]. These challenges manifest themselves as performance degradation [9, 10, 11], instability\nduring training [12, 13, 14], and biases in the resulting models. Most research addressing non-i.i.d. challenges in\nFL has focused on skewed label distributions, where each client has a different distribution of labels [15, 16, 10, 8].\nWhile this focus is crucial, it often overlooks a critical aspect of real-world FL applications: multi-domain federated\nlearning. In multi-domain FL, client data come from diverse domains, each characterized by unique feature distributions\nrather than merely differing in label distributions. For example, autonomous vehicles may collect data under various\nweather conditions or at different times of the day, leading to domain gaps in the images captured by a single client"}, {"title": "2 Related Work", "content": ""}, {"title": "Batch Normalization:", "content": "Batch Normalization [20] process involves normalizing the inputs of each layer by computing\nthe mean and variance from a mini-batch, followed by scaling and shifting through learnable parameters. Specifically,\ngiven a batch of inputs {x1,x2,..., Im}, BN calculates the mean \u00b5B and variance of as follows:\n\\\u03bc\u03b2 = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n\n\\\u03c3_\u03b2^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \u03bc_\u03b2)^2.\n\nThe normalized output is then computed as:\n\n\\hat{x_i} = \\frac{x_i - \u03bc_\u03b2}{\\sqrt{\u03c3_\u03b2^2 + \u03b5}}\n\nTo provide the network with the flexibility to adjust the normalization, BN incorporates learnable scaling and shifting\nparameters, y and \u03b2, applied as follows:\n\nYi = \u03b3\\hat{x_i} + \u03b2.\n\nwhere \u03bc and \u03c3\u00b2, hereafter denoted as BN statistics, are calculated as the running means and variances, respectively,\nof each channel computed across both spatial and batch dimensions, and y and \u1e9e are learned affine renormalization\nparameters, and where all computations are performed along the channel axis. The term e is a small positive constant\nadded for numerical stability."}, {"title": "2.1 Federated Learning with Batch Normalization", "content": "In centralized training paradigms, batch normalization (BN) has faced significant challenges when modelling statistics\nacross multiple domains. This limitation has spurred the development of domain-specific BN techniques designed to\nbetter accommodate variability in data distributions [26, 27]. These challenges are further exacerbated in the context of\nmulti-domain Federated Learning (FL), where deep neural networks (DNNs) that rely on BN may struggle to accurately\ncapture the statistical characteristics of diverse domains while attempting to train a unified global model.\n\nA prominent issue addressed for non-i.i.d. Federated learning (FL) is the skewed label distribution, where label\ndistributions vary significantly across clients. This disparity can lead to biased model performance and hinder\ngeneralization. To address this challenge, several strategies have been proposed, including specialized operations in BN\nto tailor the models to the unique data distributions of each client [8, 28, 29]. For example, SiloBN [30] retains BN\nstatistics locally on each client, ensuring that the normalization process is customized to the specific data distribution of\nthe client. Similarly, FixBN [31] mitigates the problem by training BN statistics during the early stages and subsequently\nfreezing them to maintain consistency.\n\nIn contrast, multi-domain FL has received comparatively less attention [32, 33]. To address the unique challenges posed\nby multi-domain FL approaches such as FedBN [19] and FedNorm [2] have been developed. These methods retain BN\nlayers locally on clients while aggregating only the remaining model parameters. Similarly, PartialFed [34] preserves\nmodel initialization strategies on clients, leveraging these strategies to load models in subsequent training rounds."}, {"title": "2.2 Alternative Normalization Methods", "content": "Despite its widespread effectiveness, BN encounters several limitations in certain scenarios. For example, BN may\nstruggle to accurately capture the statistical properties of training data originating from multiple domains [26, 27]. \u03a4\u03bf\naddress these limitations, researchers have proposed alternative normalization techniques, such as Group Normalization\n(GN) [23] and Layer Normalization (LN) [22]. Although these methods alleviate some of the constraints associated\nwith BN, such as its dependence on batch size, they introduce their own set of challenges. For example, both GN and\nLN require additional computational overhead during inference, which can limit their practicality, particularly in edge-\nto-edge deployment scenarios where computational resources are constrained. Recent studies have further highlighted"}, {"title": "2.3 Normalization with James-Stein Estimator", "content": "One notable concern about Equation 1 lies in the estimation of the mean and variance. The conventional approach\nsuggests independently calculating the mean and variance using \u201cusual estimators\u201d. For batch normalization, the\nestimators are given as follows:\nE[xi] = \\frac{1}{n} \\sum_{j=1}^{n} Xi,j\n\nVar[xi] = \\frac{1}{n} \\sum_{j=1}^{n} (X_ij \u2013 E[Xi])2.\n\nGiven that all the features contribute to a shared loss function, according to Stein's paradox [38], these estimators are\ninadmissible when c \u2265 3. Notably, in computer vision networks, it is consistently observed that c > 3. To address this,\na novel method was adopted by [24] to adopt admissible shrinkage estimators, which effectively enhance the estimation\nof the mean and variance in normalization layers.\n\nLet X = {x1,x2,...,xc} with unknown means 0 = {01, 02, ..., \u03b8c} and estimates 6 = {01, 02, ..., 0}. The basic\nformula for the James-Stein estimator is: \u03b8\u03c5\u03c2 = 0 + \u03c2(\u03bc\u00f4 \u2013 \u04e8),\n\nwhere \u03bc @ is the difference between the total mean (average of averages) and each estimated mean, and s is a\nshrinking factor. Among the numerous perspectives that motivate the James-Stein estimator, the empirical Bayes\nperspective [39] is the most insightful. Taking a Gaussian prior on the unknown means leads us to the following formula\n[40]:\n\n\\hat{\u03b8_{JS}} = (1 - \\frac{(c - 2) \u03c3^2}{||\u03b8||^2})(\u03b8 \u2013 \u03c5) + \u03c5,\n\nwhere || ||2 denotes the L2 norm of the argument, \u03c3\u00b2 is the variance, v is an arbitrarily fixed vector that shows the\nshrinkage direction, and c \u2265 3. Setting v = 0 results in the following:\n\\hat{JS} = (1 - \\frac{(c - 2) \u03c3^2}{||\u03b8||^2}) \u03b8.\n\nThe above estimator shrinks the estimates towards the origin 0. Using Equation 6 helps to mitigate the 'mean shift'\nproblem [41, 42]. To incorporate this equation into normalization layers, we replace the 9 in Equation 6 with the\nestimated mean and variance derived from the original method. By applying the James-Stein estimator to these\nestimated statistics, the additional computational overhead is minimal and can be considered negligible [24]. Thus, the\nJames-Stein estimator is utilized for both the mean and variance within the normalization layers. In the context of batch\nnormalization, E[x] and Var[x] are vectors of length c (where e represents the number of channels in the batch). These\nvectors can be directly substituted in place of 0.\n\nMore recently, [24] has shown that the mean and variance estimators commonly used in normalization layers are\ninadmissible. They introduced a novel approach that employs the James-Stein estimator [43]to enhance the estimation\nof mean and variance within these layers. This improved normalization technique consistently yields superior accuracy\nacross a variety of tasks without imposing additional computational burdens. Their method achieves competitive results\ncompared to Batch Normalization (BN) on prominent architectures such as ResNet [44], EfficientNet [45], and Swin\nTransformer v2 [46]."}, {"title": "3 Methodology", "content": "Batch Normalization offers several key advantages, including the mitigation of internal covariate shift, stabilization\nof the training process, and acceleration of convergence [47]. BN also reduces the number of iterations required to\nreach convergence, thereby improving overall performance [48]. Moreover, it enhances robustness to variations in"}, {"title": "3.1 James-Stein Estimator", "content": "Estimating the mean of a multivariate normal distribution is a fundamental problem in statistics. Typically, the sample\nmean is employed, which also serves as the maximum-likelihood estimator. However, the James-Stein (JS) estimator,\ndespite being biased, is utilized for estimating the mean of c correlated Gaussian-distributed random vectors with\nunknown means. The development of the JS estimator is rooted in two pivotal papers, with the initial version introduced\nby Charles Stein in 1956 [43]. Stein's work led to the surprising revelation that the standard mean estimate is admissible\nwhen c < 2 but becomes inadmissible when c > 3. This breakthrough suggested an improvement by shrinking the\nsample means towards a central vector of means, a concept commonly referred to as Stein's paradox or Stein's example\n[50]."}, {"title": "3.2 Applying James-Stein Estimation to Batch Normalization", "content": "Implementing James-Stein Normalization (JSNorm) by incorporating the James-Stein estimator into the standard\nBatch Normalization (BN) framework to enhance the robustness of BN in federated learning scenarios, where data\ndistributions can be heterogeneous and high-dimensional, the batch mean \u00b5B and variance of are adjusted using the\nJames-Stein estimator, resulting in the modified statistics as per Algorithm 1:"}, {"title": "3.3 JSNorm in Federated Learning", "content": "While Batch Normalization (BN) layers are integral components of many modern neural network architectures\n[44, 51, 45], their application in federated learning settings has not been thoroughly investigated and is often overlooked\nor omitted entirely [25]. The na\u00efve implementation of FedAvg, for instance, does not differentiate between the local\nactivation statistics (\u03bc, \u03c32) and the trained renormalization parameters (\u03b3, \u03b2), leading to a straightforward aggregation\nof both at each federated round, as illustrated in Figure 2 (left). This simplistic approach serves as a baseline for using\nFedAvg in networks that include BN layers. Moreover, BN layers can serve a dual purpose by distinguishing between\nlocal and domain-invariant information. Specifically, the BN statistics and the learned BN parameters fulfil different\nroles [26]: the former encapsulates local domain-specific information, while the latter can be transferred across different\ndomains. In traditional federated learning approaches, all Batch Normalization (BN) parameters are typically treated\nequally during aggregation. However, this overlooks the distinct roles played by BN statistics (\u03bc, \u03c3\u00b2) and learned\nparameters (\u03b3, \u03b2). We propose a new method called FedStein, which differentiates between these roles by sharing\nonly the James-Stein (JS) estimates of BN statistics across different federated centres while keeping the learned BN\nparameters local to each centre. The parameters of non-BN layers are shared using the standard federated learning\napproach. This method is illustrated in Figure 2 (right). By synchronizing the JS estimates of BN statistics, FedStein\nenables the federated training of a model that is more robust to the heterogeneity present across different centres,\nthereby improving the overall model performance in multi-domain federated learning scenarios. By incorporating the\nJames-Stein adjustment, JSNorm provides a more robust estimation of these statistics by guiding the sample means\ntoward a more centralized mean vector, resulting in improved model performance across clients. This adjustment is"}, {"title": "4 Experimental Evaluation", "content": "4.1 Results on Cross-silo Federated Learning\n\n4.2 Results on medical images.\n\n4.3 Results on Domain Adaptation and Generalization"}, {"title": "5 Conclusion", "content": "In this paper, we introduced FedStein, a novel approach for enhancing multi-domain Federated Learning (FL) that\nrelies on the James-Stein (JS) estimation of BN statistics across clients. This method effectively addresses feature\nshifts in non-i.i.d. data, particularly in multi-domain scenarios where data characteristics can vary significantly across\nclients. Through extensive experimentation on diverse federated datasets, we demonstrate that FedStein significantly\nenhances both convergence behaviour and model performance in non-i.i.d. settings. Privacy remains a critical concern\nin federated learning, and retaining the BN parameters locally at each client in FedStein adds a layer of security, making\nit more challenging to compromise local data. Our extensive experiments across three multi-domain datasets and models\ndemonstrate that FedStein consistently surpasses state-of-the-art techniques, proving its versatility in both cross-silo\nand domain generalization scenarios. Future work could explore this method further by evaluating it across a broader\nrange of datasets and model architectures, especially in the context of medical imaging."}]}