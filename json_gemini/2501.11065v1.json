{"title": "ENHANCING NEURAL SPOKEN LANGUAGE RECOGNITION: AN EXPLORATION WITH MULTILINGUAL DATASETS", "authors": ["Or Haim Anidjar", "Roi Yozevitch"], "abstract": "In this research, we advanced a spoken language recognition system, moving beyond traditional feature vector-based models. Our improvements focused on effectively capturing language characteristics over extended periods using a specialized pooling layer. We utilized a broad dataset range from Common-Voice, targeting ten languages across Indo-European, Semitic, and East Asian families. The major innovation involved optimizing the architecture of Time Delay Neural Networks. We introduced additional layers and restructured these networks into a funnel shape, enhancing their ability to process complex linguistic patterns. A rigorous grid search determined the optimal settings for these networks, significantly boosting their efficiency in language pattern recognition from audio samples. The model underwent extensive training, including a phase with augmented data, to refine its capabilities. The culmination of these efforts is a highly accurate system, achieving a 97% accuracy rate in language recognition. This advancement represents a notable contribution to artificial intelligence, specifically in improving the accuracy and efficiency of language processing systems, a critical aspect in the engineering of advanced speech recognition technologies.", "sections": [{"title": "1 Introduction", "content": "Over the years, the field of spoken language recognition (SLR) has developed remarkably, becoming both a fascinating and crucial area of study. The history of SLR can be traced back to early rule-based systems, followed by statistical and machine learning-based approaches in the 1980s and 1990s, respectively. In recent years, the field has seen the successful application of advanced statistical models like GMMs, as well as deep learning-based approaches such as CNNs and LSTM networks [1, 2, 3].\n\nThe importance of SLR is underscored by its diverse applications, ranging from security systems that detect unauthorized access to customer service and virtual assistants. It also plays a crucial role in language acquisition, providing feedback on pronunciation and intonation, and in speech therapy, where it can diagnose speech abnormalities and offer tailored treatment. These applications make SLR an indispensable tool in modern society.\n\nThe major purpose of SLR research is to develop technology and systems for the automatic and more accurate identification, analysis, and understanding of spoken language. This is particularly vital in an era where communication crosses the borders of countries and cultures [4]. As technology advances, SLR systems are likely to become even more accurate and robust, paving the way for increased cross-cultural communication and breaking down language barriers."}, {"title": "1.1 Challenges in SLR Research", "content": "Despite the significant advancements in spoken language recognition, the field still faces several challenges that hinder the production of accurate and dependable results [14]. One of the most pressing issues is background noise, which can originate from various sources such as traffic, wind, or even other speakers in the vicinity. This noise can significantly reduce the accuracy of SLR systems [15, 16].\n\nAnother major hurdle is the variability in dialects and accents. Even within the same language, people often speak with different accents and dialects, complicating the task for voice recognition algorithms. This challenge is particularly pronounced in linguistically diverse countries like India, where each region may have its own unique dialect and accent.\n\nThe scarcity of high-quality training data [17] poses an additional challenge. Speech recognition algorithms heavily rely on training data to improve their accuracy over time. However, obtaining large volumes of quality data can be problematic, especially for less commonly spoken languages or dialects. This often results in under-trained models that lack both accuracy and reliability.\n\nReal-time processing of spoken language adds another layer of complexity. SLR systems often struggle to identify language in real-time, particularly during spontaneous speech, such as genuine conversations. The need for fast and efficient algorithms capable of handling massive data volumes in real-time further complicates this issue.\n\nThese challenges underscore the critical need for ongoing research and development in SLR. To enhance the performance of these systems, technological breakthroughs like improved algorithms and innovative data collection methods are essential.\n\nIn light of these challenges, our work aims to introduce innovative solutions that enhance the accuracy and reliability of SLR systems."}, {"title": "1.2 Our Contribution", "content": "To address the existing challenges in SLR, the primary contribution of this paper is the exploration and application of the Vector-X technique. This novel approach allows us to effectively represent various aspects of speech, including words, grammatical structure, and specific speech processing like tone, in a vector space. These representations serve as a robust and consistent feature set for machine learning models aimed at recognizing and classifying spoken language. Furthermore, we introduce methods to enhance the performance of x-vector and other embedding models through different architectures and various data augmentations. Remarkably, we achieve enhanced accuracy in Time Delay Neural Network (TDNN) and x-vector algorithms while utilizing less data compared to baseline models. Our al-"}, {"title": "1.3 Paper Structure", "content": "The remainder of this paper is structured as follows: Section 2 presents previous works that mainly used x-vector and TDNN and different methods; Section 3 discusses the dataset used in the article, how it is built and what it consists of how it was chosen, and the pre-processing; Section 4 presents the framework employed in the article and the modification made to the new TDNN model; Section 5 shows the process of the experiment and the results of the new model including the modification and exploitation of the architectural structure that is different from the basic model; Finally, Sections 6 and 7 provide a discussion, conclusions, and suggestions for future follow-up work. For ease of reading, Table 2 provides a list of abbreviations that are commonly used in this paper."}, {"title": "2 Related Work", "content": "Spoken language recognition (SLR) has been a topic of significant interest in the speech processing community. Various techniques and methodologies have been proposed to enhance the accuracy and robustness of SLR systems."}, {"title": "2.1 Feature Extraction Techniques", "content": "The extraction of meaningful features from speech signals is a crucial step in SLR. Traditional systems have relied on i-vectors for this purpose. The conventional method for i-vector extraction utilizes a Universal Background Model (UBM) and employs a projection matrix T, which is trained to enhance the likelihood of the training data. [18]. Recent advancements have introduced x-vectors and d-vectors as alternatives to i-vectors. As observed, i-vectors utilize a low-dimensional representation and are based on a generative modeling approach, typically employing a Gaussian Mixture Model-Universal Background Model (GMM-UBM) architecture. In contrast, x-vectors use a high-dimensional representation, adopt a discriminative modeling approach, and are often associated with Convolutional Neural Network (CNN) architectures. D-vectors, on the other hand, combine features of both, resulting in a hybrid modeling approach. They also use a high-dimensional representation and can be trained using both supervised and unsupervised methods, typically leveraging Long Short-Term Memory (LSTM) architectures [19]."}, {"title": "2.2 Deep Learning Approaches", "content": "Deep neural networks (DNNs) have shown promise in enhancing SLR systems. DNNs, when trained as acoustic models for Automatic Speech Recognition (ASR), have been incorporated into i-vector systems for example [22]. Another study, titled \"Generative Adversarial Networks for Noise-robust Language Identification\" [23], leveraged generative adversarial nets (GAN) combined with DNN-based i-vector approaches for language identification. Furthermore, the application of deep convolutional recurrent neural networks has been explored for speech emotion recognition in specific languages, as discussed in [24]"}, {"title": "2.3 Multilingual and Dataset Considerations", "content": "Training models on multilingual datasets can enhance their generalization capabilities. One study, titled \"Multilingual Joint Training of Sequence-to-sequence Models for Spoken Language Identification\u201d [20], trained a model on nine different Indian languages and found improved recognition performance. Another research effort investigated the use of web audio data collected from YouTube for 107 languages, emphasizing the importance of post-filtering to ensure data quality [25]."}, {"title": "2.4 Enhancements and Optimizations", "content": "Various optimizations have been proposed to further improve SLR systems. For instance, the use of conditional generative adversarial nets (cGAN) has been explored for optimizing parameters, as discussed in \"Generative Adversarial Networks for Noise-robust Language Identification\u201d [23]. Another study, [26], introduced a time delay neural network (TDNN) for SLR, achieving significant gains."}, {"title": "2.5 X vectors", "content": "Contemporary language recognition systems predominantly depend on i-vectors. The standard approach employs a UBM and a substantial projection matrix T, both trained to optimize the likelihood of the training data [18]. This method transforms the high-dimensional statistics of UBM into a compact, low-dimensional i-vector. Post-extraction, i-vectors are classified using Gaussian models, logistic regression, or cosine scoring. Advanced i-vector systems incorporate deep neural networks (DNN) trained as acoustic models for automated speech recognition (ASR) [22], often in one of two ways: replacing Gaussian mixture model (GMM) posteriors with ASR DNN posteriors, or training the i-vector system with bottleneck features derived from the DNN.\n\nIn their study, [20] trained i-vectors with Multiclass Discriminative, demonstrating a method where a Gaussian i-vector classifier is trained using Maximum Mutual Information (MMI) to directly optimize the multiclass calibration criterion, eliminating the need for a separate back-end. This approach, confirmed by results on the NIST LRE11 standard evaluation task, maintains high performance and calibration with this streamlined methodology. Additionally,"}, {"title": "3 Datasets", "content": "In the field of speech recognition, the ability to accurately transcribe and understand diverse languages is of utmost importance. To address this challenge, our work utilized datasets sourced from Common Voice, a valuable resource for multilingual speech data [28]. We specifically focused on ten languages, carefully selected from three distinct language families: Indo-European, Semitic, and East Asian. This article aims to shed light on the significance of recognizing and supporting speech recognition in these languages, considering the substantial number of non-English speakers worldwide [29]. We believe that with these languages Our work aimed to contribute to the development of multilingual speech recognition systems. Such systems have the potential to bridge language barriers, empower non-English speakers, and foster inclusivity on a global scale."}, {"title": "3.1 Indo-European Languages", "content": "Among the Indo-European language family, we chose to include Spanish, French, Italian, and Russian in our study. These languages were selected for their remarkable diversity and widespread usage across multiple continents. By incorporating languages from different regions, we aimed to capture the richness and variation present within this language family [30]. Detailed information about each of these languages, including the number of recorded hours, validated hours, and the number of different speakers, is presented in Table 3."}, {"title": "3.2 Semitic Languages", "content": "n our research, we recognized the importance of Semitic languages, which have historical and cultural significance in various parts of the world [31]. To represent this language family, we focused on Arabic, Hausa, and Farsi. These languages are spoken by millions of people globally and play a vital role in their respective communities. By including Semitic languages in our study, we aimed to address the specific challenges and intricacies associated with their speech recognition. Detailed statistics about each of these languages, including the number of recorded hours, validated hours, and the number of different speakers, are presented in Table 4."}, {"title": "3.3 East Asian Languages", "content": "The third group we explored consisted of East Asian languages. Chinese, Japanese, and Thai were selected as representatives of this diverse and vibrant language family. East Asian languages possess unique phonetic structures and tonal characteristics, making them intriguing subjects for speech recognition research. Given the sizable populations that speak these languages, their accurate recognition holds great significance for enabling effective communication and access to technology for non-English speakers [32]. Detailed statistics for each language in the East Asian language family, including the number of recorded hours, validated hours, and the number of different speakers, are presented in Table 5.\n\nIt's important to note that our dataset includes both male and female speakers across various age groups and dialects. However, specific distribution statistics for each language were not available.\n\nThe Common Voice dataset distinguishes between Recorded and Validated hours. Recorded hours represent the total duration of submitted voice recordings, while Validated hours are those that have been manually reviewed and approved by the community for quality criteria such as clarity and naturalness [33]."}, {"title": "3.4 Modeling Objectives and Their Influence on Dataset Selection", "content": "Our model prioritizes understanding languages over individual speakers for several reasons:\n\n\u2022 Generalization: Ensuring universal recognition irrespective of the speaker for real-world efficacy [33].\n\n\u2022 Variability: A diverse set of speakers in training data guarantees robustness against unique speech attributes.\n\n\u2022 Privacy: Emphasizing language over speaker identity enhances privacy.\n\n\u2022 Scalability: Given the vast number of potential speakers, focusing on languages is more efficient.\n\n\u2022 Data Imbalance: Addressing potential biases from imbalanced speaker representation [34]."}, {"title": "3.5 Dataset Pre-processing and Validation", "content": "The Common Voice dataset differentiates between Recorded hours, the total duration of submitted recordings, and Validated hours, which have undergone community review for quality. Our research evaluated the necessity of pre-processing on the Common Voice dataset. Although considered clean, we converted the mp3 files to wav format for compatibility and consistency reasons. Furthermore, we removed \"dead\u201d segments, or portions with minimal voice activity, to enhance data accuracy [35]."}, {"title": "3.6 Balanced Data and Cleaning", "content": "We balanced our dataset by selecting an equal number of voice segments from each language, mitigating potential biases from imbalanced representation. This approach ensures unbiased analysis and improved model generalization [36]."}, {"title": "3.7 Data Augmentation", "content": "Data augmentation enhances dataset diversity and robustness. Adhering to methodologies in [37], we employed several techniques:\n\n\u2022 Speed Variation: Modulating audio clip speed to simulate speech rate variations among speakers [38].\n\n\u2022 Pitch Perturbation: Shifting audio signal pitch to capture human voice variability [39].\n\n\u2022 Noise Addition: Introducing various types of noise, including white, pink, and environmental, to prepare the model for real-world noisy scenarios [37]."}, {"title": "4 Framework", "content": ""}, {"title": "4.1 Baseline Model and New Model", "content": "This subsection compares the baseline Deep Neural Network (DNN) architecture with the new model proposed in this paper. The baseline DNN, as illustrated in Figure 1, processes an input sequence of T speech frames with initial layers maintaining a limited temporal context. The statistical pooling layer aggregates data across temporal dimensions, resulting in a 3000-dimensional vector fed through segment-level layers to the softmax output layer, as detailed in Figure 1."}, {"title": "4.2 Architecture improvements", "content": ""}, {"title": "4.2.1 Context Size and Dilation TDNN", "content": "In Temporal Convolutional Neural Network (TDNN) layers, optimizing context size and dilation is essential for effectively modeling sequential data, particularly in speech recognition tasks [43]. Context size, the number of adjacent time frames considered during convolution, determines the range of temporal information captured, with larger sizes enabling the incorporation of a broader temporal context crucial for understanding long-term dependencies [44]. Dilation, on the other hand, refers to the spacing between frames in the convolution process and is key to capturing non-adjacent temporal relationships, thus aiding in modeling long-range dependencies without markedly increasing computational load. The optimization of these parameters, a critical aspect of TDNN layers, demands a balance between capturing necessary temporal dependencies and avoiding computational inefficiency or overfitting. In this context, grid search proves invaluable. Our refined approach goes beyond basic grid search methodologies, which typically iterate over a preset range of values, by tailoring the search range with task-specific insights. This targeted approach focuses on optimizing context size and dilation specifically for speech recognition, adding an extra layer of specificity and effectiveness to the optimization process.\n\nA grid search was conducted to optimize the dilation and context size hyperparameters in TDNN layers, a systematic method ensuring thorough exploration of all possible hyperparameter combinations [45]. This exhaustive approach, albeit computationally demanding, was crucial for determining the most effective settings for our model. The search was conducted separately for each of the first three layers due to computational constraints, with a limited number of epochs per layer to expedite the process. The optimal hyperparameters identified were:\n\n\u2022 Layer 1 with a context size of 3 and dilation of 2, achieving an accuracy of 0.72 and a loss of 0.88.\n\n\u2022 Layer 2 with a context size of 5 and dilation of 2, resulting in an accuracy of 0.66 and a loss of 1.07.\n\n\u2022 Layer 3 with a context size of 2 and dilation of 1, leading to an accuracy of 0.85 and a loss of 0.49.\n\nIt's important to note that these values, while indicating a direction for model improvement, may not be the absolute optimal due to the limited epoch count and the restricted range of tested hyperparameter values."}, {"title": "4.2.2 1x1 TDNN Intermediate Layers", "content": "In our research, we integrated 1x1 TDNN intermediate layers into the baseline model to enhance its performance [21]. 1x1 TDNN layers, characterized by a context size and dilation of 1, are specialized temporal convolutional layers that process sequential data such as speech by focusing exclusively on the current frame or time step. This approach is effective in capturing immediate temporal relationships and fine-grained local patterns in the data [46, 47].\n\nThese layers introduce non-linear transformations to the input data, making them particularly valuable for tasks requiring detailed analysis of the current time step. In speech recognition and language processing, 1x1 TDNN layers can significantly enhance the model's ability to detect subtle temporal patterns at a granular level.\n\nIn our model, we added two specific 1x1 TDNN layers, tdnn2 and tdnn4, positioned as intermediate layers. These layers, taking outputs from tdnn1 and tdnn3 respectively, focus solely on the current frame, enhancing feature transformation and increasing the model's non-linearity. By doing so, they help in capturing complex patterns in the input more effectively. Their inclusion is vital for increasing the model's capacity to discern intricate patterns, thereby boosting its discriminative power for tasks like language detection.\n\nThese intermediate layers contribute to the model's overall capability by providing additional non-linear mappings and refining the representations of input features. This is particularly beneficial for tasks that require a nuanced understanding of both global and local language patterns, such as language detection, where capturing the specific linguistic characteristics at various abstraction levels is crucial for accurate classification."}, {"title": "4.2.3 TDNN Funnel Structure", "content": "Our research modifies the TDNN network structure from the original configuration of five layers with 512 neurons each to a funnel-shaped, or \u201cbottleneck,\u201d architecture, as discussed in the literature [48]. This funnel structure offers several advantages:\n\n1. Dimensionality Reduction: The funnel structure progressively reduces data dimensionality, leading to efficient and compact representations.\n\n2. Hierarchical Feature Extraction: It enables hierarchical processing, where each layer abstracts higher-level features from the data, aiding in learning complex patterns.\n\n3. Information Bottleneck and Regularization: The narrowing structure acts as an information bottleneck, prioritizing critical information and preventing overfitting by focusing on the most informative features for improved model performance.\n\n4. Efficient Computation: The reduced dimensionality in deeper layers decreases computational requirements, resulting in faster training and inference, beneficial for large datasets and constrained resources.\n\n5. Parameter Efficiency and Robustness: The network becomes more parameter-efficient and robust to noise and variations in the input data, with initial layers providing a global view and subsequent layers refining localized information.\n\n6. Adaptation to Data Size: Given the specific data size in our research, the layer sizes were adapted differently from the suggested Fourth root of the data size, considering the unique requirements of our dataset and model.\n\nThis TDNN funnel structure is particularly relevant to our research focus, enhancing the network's ability to process and learn from complex data patterns effectively."}, {"title": "4.3 Integrated Model and Augmentations", "content": "In our final model, we combined three key enhancements: values optimized through grid search, 1x1 TDNN intermediate layers, and a funnel structure. This integrated approach significantly strengthened the model's capabilities, as detailed in the previous sections. To further refine its performance, the model was trained on augmented data, which included adjustments in audio speed, added noise, and altered pitch. This augmentation aimed to enhance the model's robustness and unreliability, exposing it to a wider range of realistic audio conditions. As a result of these combined improvements, the model achieved an impressive accuracy of 95% on the validation set, demonstrating its effectiveness in diverse and challenging audio environments."}, {"title": "5 Experimental Evaluation and Results", "content": "In accordance with the methodologies outlined in the framework, several modifications were independently evaluated. Below is a tabulated comparison illustrating the differences among these variations:"}, {"title": "6 Discussion", "content": "Advancements in language recognition technologies have been noteworthy, yet they confront several challenges and limitations. This discussion highlights some of the prevalent issues:\n\n1. Linguistic Ambiguity. Distinguishing between languages with shared linguistic traits or diverse dialects remains challenging. For instance, accurately differentiating languages like Norwegian, Swedish, and Danish is complicated by their close linguistic similarities."}, {"title": "7 Conclusions and Future Work", "content": "In this study, we have enhanced a spoken language recognition model based on x-vectors, focusing on capturing long-term language features through a temporal pooling layer. Utilizing datasets from Common Voice, a premier"}]}