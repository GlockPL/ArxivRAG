{"title": "AIDE: AI-Driven Exploration in the Space of Code", "authors": ["Zhengyao Jiang", "Dominik Schmidt", "Dhruv Srikanth", "Dixing Xu", "Ian Kaplan", "Deniss Jacenko", "Yuxiang Wu"], "abstract": "Machine learning, the foundation of modern artificial intelligence, has driven innovations that have fundamentally transformed the world. Yet, behind advancements lies a complex and often tedious process requiring labor and compute intensive iteration and experimentation. Engineers and scientists developing machine learning models spend much of their time on trial-and-error tasks instead of conceptualizing innovative solutions or research hypotheses. To address this challenge, we introduce AI-Driven Exploration (AIDE), a machine learning engineering agent powered by large language models (LLMs). AIDE frames machine learning engineering as a code optimization problem, and formulates trial-and-error as a tree search in the space of potential solutions. By strategically reusing and refining promising solutions, AIDE effectively trades computational resources for enhanced performance, achieving state-of-the-art results on multiple machine learning engineering benchmarks, including our Kaggle evaluations, OpenAI's MLE-Bench and METR's RE-Bench. The implementation of AIDE is publicly available at https://github.com/WecoAI/aideml.", "sections": [{"title": "1 Introduction", "content": "Machine learning engineering supports many modern AI achievements, from basic regression on tabular data to the recent surge in large generative models. However, building a high-performance machine learning model is always time consuming. Due to the inherent stochasticity of both the data and the optimization process, engineers and scientists rely heavily on trial-and-error. Researchers have long sought to automate these iterative processes, leading to advancements in fields like AutoML (Feurer et al., 2015, 2020; LeDell and Poirier, 2020a; Olson and Moore, 2016; Jin et al., 2023, 2019; Thornton et al., 2013a,b; Mueller and et al., 2024), Neural Architecture Search (Zoph and Le, 2017; Pham et al., 2018; Liu et al., 2019; Real et al., 2019; Elsken et al., 2019), and hyperparameter optimization (Falkner et al., 2018; Yang and Shami, 2020). These methods typically require a predefined search space of configurations, such as hyperparameters and network architectures, within which the algorithm explores potential solutions (Elsken et al., 2019; Yang and Shami, 2020; White et al., 2023). Defining this space often requires significant domain expertise. Furthermore, search algorithms for hyperparameter tuning are often somewhat brute force compared to human experts, resulting in lower compute efficiency and a risk of overfitting to the validation set.\nThe emergence of advanced coding capabilities in large language models (LLMs) (OpenAI, 2023; Jimenez et al., 2024; Anthropic, 2024; Google, 2024; Jain et al., 2025; OpenAI, 2025a,b) has introduced an exciting new possibility: searching directly within the space of code rather than the space of predefined configurations. Code-space optimization offers greater flexibility and leverages"}, {"title": "2 Preliminaries", "content": "Many general-purpose LLM agents, including ReACT (Yao et al., 2023), frame their tasks as Partially Observable Markov Decision Processes (POMDPs) (Kaelbling et al., 1998), a widely used framework in reinforcement learning. In a POMDP, the agent tries to maximize a cumulative reward by choosing actions based on all past observations, essentially treating the entire interaction history as the state. While this approach is flexible and unifies a range of tasks, it lacks a principled way to break down the problem when there is a clear structure available. Moreover, for LLM-based agents, continually appending all historical data can lead to oversized prompts and limit scalability, because the model's context window eventually fills up.\nIn this work, we adopt an alternative framework for LLM-driven iterative problem solving by modeling the task as an optimization problem: Let S be a space of possible solutions (e.g., Python scripts), and let h: S \u2192 R be a stateless objective function (for example, validation accuracy or loss). The goal is to find an optimal solution:\n$S^* = \\arg \\max_{s \\in S} h(s)$.\nEach candidate solution s can be evaluated independently via an objective function h(s). This perspective simplifies the problem considerably: rather than unrolling a single, long-horizon decision process, we can directly evaluate and compare solutions. It also aligns naturally with existing optimization methods, like tree search, which depend on standalone evaluations of candidate solutions."}, {"title": "3 Methodology", "content": "In this section, we introduce our approach to automating machine learning engineering with AIDE. By employing the tree search method, AIDE systematically explores solutions that optimize validation metrics, breaking down the monolithic optimization task into atomic improvement steps. We begin by outlining the high-level optimization algorithm. And then delve into key implementation details, such as the search policy and specialized prompts that drive the iterative generation and refinement of machine learning code."}, {"title": "3.1 AI-Driven Exploration in the Space of Solutions", "content": "In AIDE, a solution s is the code to be optimized, with $s_0$ denoting the empty root solution. An evaluator, h : S \u2192 R, evaluates the code and provides a scalar score. All discovered solutions are stored in a solution tree, T, whose nodes correspond to scripts and edges represent an improvement attempt (e.g., s \u2192 s' is an improvement of s). A search policy, \u03c0(T), selects which solution s \u2208 T will serve as the base solution to be improved. To keep language model prompts concise while being aware of the historical attempts, a summarization operator, \u03a3(T), extracts relevant information from the tree, such as the high level idea of each improvement attempt and its corresponding performance metrics. Finally, a coding operator, f(s, \u03a3(T)), proposes new scripts by drafting an initial version from $s_0$, fixing bugs, or refining a promising solution based on the summarized context.\nWith these components in place, AIDE can systematically explore the code solution space, as shown in Algorithm 1."}, {"title": "3.2 AIDE for Machine Learning", "content": "Here we present more implementation details of AIDE for machine learning engineering, providing a concrete instantiation of the core components from Section 3.1. In particular, we build upon the following design elements:\nSearch Policy (\u03c0). In AIDE, the search policy \u03c0 (algorithm 1, line 7) follows a simple hard-coded rule, determining whether to draft, debug, or improve based on an existing solution. Specifically, it selects:\n\u2022 Drafting if we have not yet reached the desired number of initial solutions.\n\u2022 Debugging if a buggy node remains within a certain debug depth.\n\u2022 Improving otherwise, typically targeting the best (non-buggy) solution.\nThis policy imposes practical heuristics, such as 1) first exploring a set of diverse initial solutions and continuously improving the best one, and 2) constraining the number of debug attempts for a broken solution.\nCoding Operator (f). The coding operator has three main entry points, each with its own specialized prompts:\n\u2022 Drafting, which is invoked when we need a completely new solution from scratch. It prompts an LLM to outline a brief plan for a model (e.g., specifying a particular network architecture or feature-engineering idea), then emits a single-file Python program implementing that plan.\n\u2022 Debugging, which focuses on repairing buggy solutions. By inspecting error logs and execution traces, it attempts to rectify issues in the code like broken imports, incorrect tensor dimensions, or other coding errors while preserving the overall approach.\n\u2022 Improving, which is called when a valid, non-buggy solution already exists but could benefit from data preprocessing, architectural or optimization modifications. Here, the LLM"}, {"title": "4 Evaluation", "content": "In this section we report empirical evaluations of AIDE. We did our own evaluation on Kaggle competitions with a focus on tabular machine learning tasks (Weco AI, 2024). On the other hand, after the open sourcing of the AIDE in April 2024, the community has done larger scale independent"}, {"title": "4.1 Weco Kaggle Benchmark", "content": "We curated a diverse set of Kaggle competitions to build Weco's internal Kaggle benchmark, called Weco-Kaggle, for evaluating AIDE's performance in machine learning. This set consists of 63 competitions of varied complexity and data size, spanning domains such as tabular machine learning, image classification, and time-series prediction. Some of these competitions require a GPU to solve. Full details of the competitions in Weco-Kaggle are provided in Appendix C. From Weco-Kaggle, we selected a subset of 16 tabular machine learning tasks with relatively lower complexity and primarily CPU-based runtime requirements. This subset, referred to as Weco-Kaggle Lite, is shown in Table 2.\nEvaluation Protocol. We evaluate the performance of AIDE by comparing its results to that of human competitors in each Kaggle competition, and averaging across competitions. We follow the evaluation protocol below to evaluate AIDE's and other frameworks' performance:\n1. Before running the agent on a competition, we split the competition's training data into an agent train set and a holdout test set. This split is defined manually for each competition following similar parameters as Kaggle's official private test set (e.g. similar train-test percentages), but is not necessarily the same, since Kaggle's test set is not released publicly for most competitions. Note that our holdout test set is also distinct from the train-validation split that AIDE itself generates as part of its internal node evaluation protocol.\n2. During code generation, AIDE is given access to the holdout test inputs (but not labels) and prompted to evaluate its model on this data. In particular, we prompt AIDE to generate a submission.csv file, analogously to how human competitors submit their competition results.\n3. We define an Exceeds % of Human metric as 100(1 \u2013 q), where q is the quantile of AIDE's score on the official Kaggle leaderboard. This metric represents the percentage of human competitors whose performance AIDE surpasses. Whenever possible, we use Kaggle's private leaderboard because it is less prone to overfitting by competitors; if a private leaderboard is unavailable, we default to the public leaderboard. In addition, we report the Above Median metric, originally proposed by Chan et al. (2024), which indicates how frequently AIDE outperforms the median Kaggler performance across competitions.\n4. This metric is then averaged across all competitions.\nWe chose our evaluation protocol based on leaderboard-quantiles since, unlike each competition's included metric, these scores are similarly distributed between competitions, making it possible to simply average across competitions to obtain aggregated scores. Leaderboard quantiles are also more fine-grained, allowing us to evaluate, for example, the performance of a single run on a single task, unlike medal-counts (Chan et al., 2024) which collapse to a binary metric in this case. Finally, our scores are interpretable and useful in assessing AIDE's performance relative to humans."}, {"title": "Baselines.", "content": "To evaluate AIDE's effectiveness, we compare it against three baselines that illustrate different approaches to automated or assisted machine learning:\n1. Conventional H2O AutoML. We select H2O, one of the leading AutoML platforms, to exemplify traditional AutoML tools. In each competition, the data is split into an 80%/20% train/validation set, and model selection is performed within a 600-second search window.\n2. AutoGPT. A workflow automation framework that surged in popularity in early 2024. It generates a plan and automatically executes the necessary steps to complete a task. We adapt its task descriptor to produce solutions for our competitions.\n3. Human Assisted with ChatGPT. An increasingly common scenario involves human engineers leveraging ChatGPT to assist with coding tasks. We adopt this baseline to understand how AIDE performs relative to a human engineer directing ChatGPT to develop solutions.\nThese baselines collectively provide a robust comparative foundation for evaluating AIDE against both traditional AutoML workflows and modern LLM-assisted strategies. Further details about the baselines' configuration can be found in Appendix A."}, {"title": "AIDE's Results on Weco-Kaggle Lite.", "content": "Table 1 compares AIDE against multiple baselines, including H2O AutoML, AutoGPT, and a human competitor utilizing ChatGPT, averaged over the 16 tabular Kaggle tasks of Weco-Kaggle Lite. AIDE achieves an Exceeds % of humans score of 51.38%, outperforming half of the Kaggle participants on average, and surpasses the human median in 50% of these tasks. By contrast, H2O AutoML and LangChain AutoGPT attain lower Exceeds % of humans scores (35.34% and 32.34%, respectively). Table 2 offers a detailed breakdown for each competition, indicating that AIDE's performance ranges from surpassing roughly 13% of human participants (for more challenging tasks) to nearly 92% (for tasks it handles more effectively). Across half of the competitions, AIDE ranks above the human median, underscoring its robustness in consistently delivering competitive results against a diverse set of real-world machine learning challenges."}, {"title": "AIDE's Results on Full Weco-Kaggle.", "content": "Figure 2 illustrates AIDE's performance distribution across our extended set of Kaggle competitions, sorted by its Exceeds % of Humans value. Notably, AIDE achieves near-top-tier performance on several tasks, surpassing the vast majority of human participants, while on other tasks it exceeds only a small fraction. Overall, the average Exceeds % of Humans rate is 48.23%, and AIDE outperforms the human median in 49.21% of the competitions. These results underscore that AIDE can be highly competitive in certain domains, yet there remains variability in its performance depending on the dataset and task requirements."}, {"title": "4.2 AIDE in MLE-Bench", "content": "MLE-Bench (Chan et al., 2024) is an offline evaluation framework comprising 75 real Kaggle competitions. Here, we present the results related to AIDE reported by Chan et al. (2024) and encourage readers to check and cite the original paper if they are interested in the results presented here. In these evaluations, AIDE emerged as the top-performing agent framework when paired with state-of-the-art large language models. Other agent frameworks such as ResearchAgent from MLAB (Huang et al., 2024) and OpenHands (Wang et al., 2024) tended to terminate early or struggle with iterative refinement. AIDE's optimization-centric approach led to better scalability in terms of trial-and-error interactions, therefore higher valid-submission rates and ultimately more competition medals.\nTable 3 highlights key results of AIDE compared to other agents. The reported Any Medal (%) column shows the fraction of competitions on which the agent and model combination achieved a medal (bronze, silver, or gold) in a single pass (i.e. pass@1). AIDE with o1-preview earned medals in 16.9% of competitions, nearly four times that of the follow-up agent OpenHands."}, {"title": "AIDE's Key Advantages.", "content": "By explicitly implementing a solution tree search strategy, AIDE keeps node-level code concise and focuses each language model call on a localized problem (e.g. debugging only the most promising script). This design helps avoid oversized prompts, preserves a clear performance record for each node, and repeatedly refines partial solutions over the entire 24-hour timeframe. Consequently, AIDE systematically addresses coding bugs and suboptimal hyperparameters rather than abandoning failed solutions. As shown in Table 3, these iterative improvements translate into higher medal acquisition rates in comparison to generic agents.\nMoreover, when given additional attempts per competition (increasing k in pass@k), AIDE significantly increases its success rate; for instance, GPT-40 and ol-preview nearly double their medals from pass@1 to pass@6 (Chan et al., 2024). These observations underscore the specialized nature of AIDE, which often outperforms other agents through persistent, Kaggle-style iteration, highlighting the efficacy of a competition-targeted design in real-world ML tasks.\nThe impact of AIDE becomes particularly evident when comparing performance on the MLE-bench Lite subset, as shown in Figure 3. Using o1-preview with AIDE significantly improved performance across all metrics compared to using ol-preview alone. The valid submission rate increased from 63.6% \u00b1 4.5% to 92.4% \u00b1 2.6%, demonstrating AIDE's effectiveness in guiding the model through the submission process. More importantly, the fraction of solutions scoring above the median human performance increased dramatically from 13.6% to 59.1% \u00b14.5%, and both medal-related metrics showed substantial improvements: the gold medal achievement rate more than tripled from 6.1% \u00b12.6% to 21.2% \u00b1 6.9%, while the overall medal achievement rate increased nearly fivefold from 7.6% \u00b1 2.6% to 36.4% \u00b1 7.9%. These improvements are statistically significant (p < 0.01 for all metrics, two-tailed t-test). The dramatic performance gains across all metrics demonstrate that AIDE's iterative optimization approach substantially enhances the model's problem-solving capabilities, enabling more reliable and higher-quality solutions through systematic refinement."}, {"title": "4.3 AIDE in RE-Bench", "content": "While AIDE is designed for building machine learning pipelines, METR applied it to much more challenging AI R&D tasks by formulating these tasks into optimization tasks. The tasks range from optimizing a Triton Kernel to finetuning GPT-2 for QA. Surprisingly, AIDE performs quite well on these tasks, and is even comparable with the top human AI scientists from Google DeepMind, Google, Anthropic, OpenAI, FAR Labs, Redwood Research, University of California Berkeley, Carnegie Mellon University, Stanford University, or Massachusetts Institute of Technology (METR, 2024)."}, {"title": "5 Related Work", "content": "5.1 LLM Agents\nRecent advances in large language models have spurred the development of agents that combine natural language reasoning with task execution. General-purpose agents such as ReAct (Yao et al., 2023) and HuggingGPT (Shen et al., 2023) interleave planning with action selection to perform tasks ranging from information retrieval to multi-modal processing. In contrast, specialized agents like Voyager (Wang et al., 2023) and AlphaCode (Li and et al., 2022) are tailored to specific domains such as embodied reasoning and competitive code generation. These systems integrate execution feedback into the LLM's reasoning process, enabling iterative refinement of candidate solutions.\n5.2 Automated Machine Learning\nAutomated Machine Learning (AutoML) aims to eliminate manual intervention in model selection, hyperparameter tuning, and pipeline configuration. Early frameworks such as Auto-WEKA (Thornton et al., 2013b) and TPOT (Olson and Moore, 2016) employed Bayesian optimization and genetic programming, respectively, to search over predefined model spaces. Later systems like Auto-Sklearn (Feurer et al., 2020) and AutoGluon (Mueller and et al., 2024) have leveraged meta-learning and ensemble techniques to further improve performance. Despite their success, many conventional AutoML methods rely on static search spaces and lack the dynamic adaptability required for more complex problem settings."}, {"title": "5.3 Neural Architecture Search", "content": "Neural Architecture Search (NAS) focuses on automatically designing neural network topologies. Initial methods based on reinforcement learning (Zoph and Le, 2017) and evolutionary strategies (Real et al., 2019) demonstrated that automated search could yield competitive architectures. Differentiable approaches such as DARTS (Liu et al., 2019) have reduced the computational cost by enabling gradient-based optimization over a relaxed search space. However, NAS still faces challenges in computational expense and search space design. AIDE, on the other hand, avoids such problems above with code space search and efficient design exploration powered by LLMs."}, {"title": "6 Conclusion", "content": "In conclusion, we have presented AI-Driven Exploration (AIDE), an LLM Agent for machine learning engineering. By systematically drafting, debugging, and refining solutions, AIDE achieves superior performance on Kaggle tasks as well as on more research-oriented benchmarks. While developed for tabular machine learning tasks, third-party experiments show that this approach can generalize to challenges such as neural architecture search, Triton Kernel optimization, and other AI R&D tasks. We believe AIDE represents a promising step toward the future of automated ML engineering, offering a principled way to combine iterative LLM prompting with a tree-based exploration of code solutions."}, {"title": "A Baseline Specifications", "content": "A.1 H2O AutoML Baseline\nThe machine learning algorithm selection process of H2O AutoML LeDell and Poirier (2020b) proceeds as follows. First, it searches over a set of six algorithms:\n1. Distributed Random Forest (DRF) and Extremely Randomized Trees (XRT)\n2. Generalized Linear Model (GLM) with regularization\n3. XGBoost\n4. H2O Gradient Boosting Machines\n5. Fully connected multi-layer artificial neural network (DeepLearning)\n6. Stacked Ensembles (including an ensemble of all base models and ensembles using subsets of the base models)\nIt then performs a random search over a predefined grid of hyperparameter combinations, avoiding the computational expense of an exhaustive grid search. After training individual models, H2O AutoML creates stacked ensembles by combining the predictions of the best-performing models from each algorithm. This ensemble method leverages the strengths of multiple models to improve overall performance. All trained models, including individual models and ensembles, are evaluated using cross-validation and ranked based on performance metrics such as accuracy, AUC, or RMSE, depending on the problem type. The configurations are shown in Table 4.\nA.2 AutoGPT Baseline\nWe use the LangChain implementation of AutoGPT, which includes LangChain primitives such as PromptTemplates, VectorStores, and Embeddings. Inspired by Huang et al. (2024), we introduce a task descriptor for AutoGPT in each competition to provide a basic task planner and minimize human intervention. The task descriptor includes information retrieved from the Kaggle page, such as the dataset description, file details (train.csv, test.csv, sample_submission.csv), evaluation metrics, submission file format, and a sample training script. An example task descriptor is shown in Figure 5.\nWe also provide the agent with tools to read and write files, list directories, and run Python REPL evaluations. The agent reads the task descriptor with predefined goals, as shown below:"}, {"title": "A.3 ChatGPT with Human Assistance", "content": "A human operator is tasked with solving a Kaggle competition using only the information provided in the overview and data tabs, which include the available dataset. The operator is permitted to utilize the ChatGPT web interface. The LLM is set to gpt-4-0125-preview in comparison with Auto-GPT. Due to limitations in ChatGPT's capabilities, such as the potential for generating hallucinated results and occasionally using outdated packages, iterative interactions are required. The human operator will continue to issue instructions until a valid submission is produced. Upon completion, the operator submits the results to Kaggle, where the submission is ranked against the competition leaderboard."}, {"title": "B Analysis of AIDE", "content": "B.1 Code Complexity Growth\nIn Figure 6, we observe that the aggregated code complexity (combining LOC, LLOC, Volume, N1, and MI) exhibits an overall increasing trend as the number of iterative steps grows. Initially, there is a slight dip in complexity, but after the first step, the metrics begin a generally steady rise. This suggests that as AIDE (GPT-4 Turbo) produces successive iterations of code, the solutions tend to become more elaborate, with additional lines of code and logical structures contributing to higher values for traditional software complexity measures. The progressive increase implies that, over multiple generation steps, the model accumulates more intricate functionality-potentially reflecting deeper problem-solving processes or additional features-leading to an increasingly complex codebase by the final iteration.\nB.2 Cost Analysis\nFigure 7 illustrates the per-task LLM inference cost for AIDE across the Weco-Kaggle benchmark, using GPT-4 Turbo (gpt-4-0125-preview) with pricing data from early 2024. Although certain tasks incur higher costs due to more extensive prompting (up to approximately $2.50 per task), the majority remain under $1.50, reflecting moderate token usage and minimal manual intervention. Overall, these expenditures are much lower than the investment required for human experts or conventional AutoML services, especially when considering the significant performance gains achieved by AIDE's fully automated design. Moreover, as language model costs continue to decline, AIDE's approach becomes increasingly competitive in terms of both performance and budget.\nC Weco Kaggle Benchmark"}]}