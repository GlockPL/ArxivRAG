{"title": "CAN CHATGPT DIAGNOSE ALZHEIMER'S DISEASE?", "authors": ["Quoc-Toan Nguyen", "Linh Le", "Xuan-The Tran", "Thomas Do", "Chin-Teng Lin"], "abstract": "Can ChatGPT diagnose Alzheimer's Disease (AD)? AD is a devastating neurodegenerative condition that affects approximately 1 in 9 individuals aged 65 and older, profoundly impairing memory and cognitive function. This paper utilises 9300 electronic health records (EHRs) with data from Magnetic Resonance Imaging (MRI) and cognitive tests to address an intriguing question: As a general-purpose task solver, can ChatGPT accurately detect AD using EHRs? We present an in-depth evaluation of ChatGPT using a black-box approach with zero-shot and multi-shot methods. This study unlocks ChatGPT's capability to analyse MRI and cognitive test results, as well as its potential as a diagnostic tool for AD. By automating aspects of the diagnostic process, this research opens a transformative approach for the healthcare system, particularly in addressing disparities in resource-limited regions where AD specialists are scarce. Hence, it offers a foundation for a promising method for early detection, supporting individuals with timely interventions, which is paramount for Quality of Life (QoL).", "sections": [{"title": "1 Introduction", "content": "Dementia is the seventh most prevalent cause of death globally and is a major contributor to disability and dependence in older adults [1]. Alzheimer's Disease (AD), the most prevalent form of dementia, is responsible for 60\u201380% of cases [2], with a high incidence among individuals aged 65 and above [3, 4, 5, 6]. AD is characterised by progressive cognitive decline, memory impairment, and neuronal damage, leading to brain atrophy and tissue deterioration [7]. Although a cure remains unavailable [2], early diagnosis plays a critical role in slowing disease progression and enhancing Quality of Life (QoL) through prompt interventions and comprehensive care plans [8, 9]. The progression of AD is typically categorised into three stages [10]: 1) preclinical, 2) Mild Cognitive Impairment (MCI, also referred to as prodromal AD), and 3) dementia. MCI is characterised by memory deficits but without significant disruptions in daily living activities like dementia [11].\nIn recent years, large language models (LLMs) have dramatically advanced in the field of natural language processing (NLP), demonstrating exceptional performance across various NLP tasks [12, 13, 14, 15, 16, 17, 18]. Among these, ChatGPT [12] stands out as a prime example, excelling not only in NLP tasks but also in its ability to follow instructions effectively, generating coherent and informative outputs [19, 20, 21, 22]. Despite their notable capabilities, LLMs may still be hindered by issues of uncertainty, often producing overly confident yet inaccurate responses, a phenomenon is known as 'hallucination' [23, 24]. Current research predominantly addresses the uncertainty problem in LLMs using a white-box approach. For instance, Kadavath et al. [25] reveal that LLMs are largely aware of their uncertainty by analysing the softmax probabilities. Similarly, Lin et al. [26] highlight that LLMs can be trained to articulate their uncertainty verbally through model fine-tuning. Nevertheless, the white-box approach is not practical. Not all users have the ability or would like to do it. Therefore, evaluation using a black-box approach [27, 28] without accessing model internal states is relevantly vital to support users who are not experts in artificial intelligence."}, {"title": "2 Related Work", "content": "The number of research leveraging ChatGPT for supporting dementia and AD is likely limited; it has been applied and analysed in just some research. Firstly, a recent pilot study by Aguirre et al. [38] assessed the potential of ChatGPT-3.5 to support dementia caregivers by providing high-quality responses to real-world questions. Using posts from caregivers on Reddit, researchers evaluated ChatGPT's responses across topics like memory loss, aggression, and driving using a formal rating scale. ChatGPT demonstrated consistently high-quality responses, with 78% scoring 4 or 5 points out of 5, excelling in synthesizing information and offering recommendations. Next, a study comparing 60 dementia-related queries found Google excelled in currency and reliability, while ChatGPT scored higher in objectivity and relevance. ChatGPT had lower readability (mean grade level 12.17, SD 1.94) than Google (9.86, SD 3.47). Response similarity was high for 13 (21.7%), medium for 16 (26.7%), and low for 31 (51.6%) queries [39]. ChatGPT was developed to interpret the findings of the output of the introduced TriCOAT model by Diego et al. In particular, chatGPT has tremendous potential in AD research, such as early detection [40]. A study evaluated ChatGPT's ability to diagnose AD using four samples as cases with MCI and AD. ChatGPT accurately diagnosed these cases, matching the performance of two AD specialists. The findings highlight ChatGPT's potential as a tool for AD diagnosis [35].\nDespite its promising potential, ChatGPT's application in AD detection remains underexplored, particularly with a large-scale dataset. This paper aims to open and disclose ChatGPT's capability to accurately diagnose AD, paving the way for its broader adoption in clinical and research settings."}, {"title": "3 Material", "content": "Publicly available data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) [41, 42, 43] was utilized for this research due to its large amount of samples. We include data from 1480 individuals, comprising 9300 electronic health records (EHRs) with corresponding MRI volumes and cognitive test scores. Medical professionals labeled these"}, {"title": "4 Method", "content": "In this section, the methods employed in this paper are described in Figure 1. However, before delving into the technical details, it is essential to understand the overarching workflow of this research. The workflow is designed to bridge the gap between real-world applications and the methods studied in this research. On the left, the workflow represents a real-world use case where data from an individual's MRI scans and cognitive tests are either collected or analysed by a medical professional who is not necessarily an AD specialist. This medical staff member can input the available data into ChatGPT, which provides a diagnostic prediction by outputting the individual as NC, MCI, or AD.\nOn the right, the workflow illustrates how this research is conducted using ChatGPT to diagnose AD. The study explores two distinct prompting approaches-zero-shot prompting and multi-shot prompting-detailed in Sections 4.1 and 4.2, respectively. Both approaches leverage ChatGPT to predict diagnosis. To ensure the reliability of the outputs, each method is executed five times for MRI or cognitive test scores only, and MRI combined cognitive test scores to evaluate the consistency of ChatGPT's responses."}, {"title": "4.1 Zero-Shot Prompting", "content": "Zero-shot prompting is an approach in NLP where a model is given a task without any task-specific examples [44, 45, 46, 47]. Instead, the task is described directly in the prompt, relying on the model's general knowledge and understanding to generate a response. This method leverages pre-trained models to generalize across tasks without additional fine-tuning. In this paper, a zero-shot learning approach is utilized to develop a prompt using general comprehension of ChatGPT to predict whether the EHR in a CSV file (EHRSZERO) is classified as NC, MCI, or AD based on MRI and/or cognitive test scores. The detail of the proposed prompt is illustrated in Figure 1.\nLet $T_{ZERO}$ represent the task of categorizing NC, MCI, and AD, $P_{ZERO}$ represent the prompt provided to ChatGPT, R represent the response generated by the model, and C represent the confidence score associated with the response. The confidence score quantifies ChatGPT's confidence about the response R.\nThe zero-shot prompting process can now be described as:\n$(R, C) = \\arg \\max_{r,c} P(r, c | T_{ZERO}, P_{ZERO})$\nwhere:\n\u2022 r is a possible response in the space of all potential outputs,\n\u2022 c is the associated confidence score for the response r,\n\u2022 $P(r,c| T_{ZERO}, P_{ZERO})$ is the joint probability of generating a response r with a confidence score c, given the task $T_{ZERO}$ and the prompt $P_{ZERO}$.\nBreaking this down further, the response R and its confidence score C are determined based on the model's ability to evaluate the probability of r and its confidence c using pre-trained knowledge K:\n$P(r, c | T_{ZERO}, P_{ZERO}) = g(r, c; T, P_{ZERO}, \u039a)$\nwhere:\n\u2022 $g(r, c; T_{ZERO}, P_{ZERO}, K)$ is a scoring function that the model uses to calculate both the likelihood of the response and the confidence score based on the task, prompt and its pre-trained knowledge,\n\u2022 The confidence score C is typically derived from the model's internal probability distribution over possible outputs, often normalized to a percentage for interpretability.\nIn the context of classifying EHRs to detect AD, the zero-shot approach generates a predicted class R (NC, MCI, or AD) and an associated confidence score C, which quantifies the model's confidence about the prediction based on MRI and/or cognitive test scores."}, {"title": "4.2 Multi-Shot Prompting", "content": "Multi-shot prompting leverages multiple example question-and-answer pairs to guide the model. By utilizing these examples, the model may gain a clearer understanding of the intended output [48, 49, 50, 51, 52]. In this paper, examples with ground truth labels of the three classes (NC, MCI, and AD) are provided to improve predictions. Specifically, multi-shot prompting is leveraged to predict EHRs in a CSV file (EHRSMULTI) using example files E(E1, E2, E3) containing ground truth labels. The conducted prompt is presented in Figure 1.\nLet $T_{MULTI}$ represent the task of classifying NC, MCI, and AD, $P_{MULTI}$ represent the prompt provided to the model, E represent the set of example question-and-answer pairs, R represent the response generated by the model, and C represent the confidence score associated with the response. The confidence score quantifies the model's confidence about the response R.\nThe multi-shot prompting process can now be described as:\n$(R, C) = \\arg \\max_{r,c} P(r,c|T_{MULTI}, P_{MULTI}, E)$\nwhere:\n\u2022 r is a possible response in the space of all potential outputs,\n\u2022 c is the associated confidence score for the response r,\n\u2022 $P(r, c | T_{MULTI}, P_{MULTI}, E)$ is the joint probability of generating a response r with a confidence score c, given the task T, the prompt $P_{MULTI}$, and the example pairs E.\nBreaking this down further, the response R and its confidence score C are determined based on the model's ability to evaluate the probability of r and its confidence c using pre-trained knowledge K and the examples E:\n$P(r, c|T_{MULTI}, P_{MULTI}, E) = g(r, c; T_{MULTI}, P_{MULTI}, E, K)$\nwhere:\n\u2022 $g(r, c; T_{MULTI}, P_{MULTI}, E, K)$ is a scoring function that the model uses to calculate both the probability of the response and the confidence score based on the task, prompt, examples, and its pre-trained knowledge,\n\u2022 The confidence score C is typically derived from the model's internal probability distribution over possible outputs, often normalized to a percentage for interpretability.\nFor classifying EHRs, the multi-shot approach utilizes ChatGPT's general knowledge and example question-and-answer pairs E to provide a predicted class R (NC, MCI, or AD) and an associated confidence score C."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Evaluation Metrics", "content": "This study employs five essential performance metrics, which are highly relevant for evaluating AI systems in healthcare applications [53]: accuracy, recall, precision and F1-score. These metrics are represented as percentages, with values ranging from 0 to 1. A higher value generally indicates better performance across the mentioned metrics.\nThe calculations of these metrics rely on four foundational components: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). TP refers to the number of positive cases that are correctly identified, while TN represents the number of negative cases correctly classified. FP corresponds to negative cases that are mistakenly classified as positive, and FN accounts for positive cases that are incorrectly labelled as negative. The metrics are computed using the following formulas:\naccuracy $\\frac{TP+TN}{TP+FP+TN+FN}$\nrecall $\\frac{TP}{TP + FN}$"}, {"title": "", "content": "precision $\\frac{TP}{TP + FP}$\nF1-score $2 \\times \\frac{precision \\times recall}{precision + recall}$\nOn top of that, besides metrics, evaluating the calibration of the model is vital. Hence, two metrics were used in this paper, including Expected Calibration Error (ECE) and Maximum Calibration Error (MCE) with B=10 as the reference studies [27, 54, 55, 56]. They are formulated using the following equations:\n$ECE = \\sum_{i=1}^{B}P(i) \\cdot |o_i - c_i|$\n$MCE = \\max_{i=1}^{B}(|o_i - c_i|)$\nwhere:\n\u2022 or is the true fraction of positive instances in bin i,\n\u2022 er is the mean of the post-calibrated probabilities for the instances in bin i,\n\u2022 P(i) is the empirical probability (fraction) of all instances that fall into bin i,\n\u2022 B is the total number of bins.\nThe lower the values of ECE and MCE, the better the calibration of a model."}, {"title": "5.2 Experimental Settings", "content": "The experiments assessing ChatGPT's capability to diagnose AD in this research were conducted using OpenAI ChatGPT Version 4 Plus (GPT-4-turbo) [12, 57]. Both zero-shot and multi-shot approaches were executed five times under three conditions: using MRI data alone, cognitive test scores alone, and a combination of MRI and cognitive test scores. In total, 30 runs were performed across both methods. To ensure independence between runs, all previous chat histories were cleared before initiating each new run. Regarding the examples for multi-shot prompting, 100 samples of each class were selected to put into the prompt. The thresholds are values of confidence scores that are equal or greater."}, {"title": "6.1 Zero-Shot Prompting", "content": "To begin with, about the performance metrics of zero-shot prompting, as we can see in Table 3 and Figures 2, 3 and 5. Firstly, combining MRI and cognitive test data yields superior performance across all metrics compared to using either modality alone. At a 75% threshold, the combined modality achieves the highest accuracy (0.744\u00b10.110), recall (0.746 \u00b1 0.111), precision (0.791 \u00b1 0.050), and F1-score (0.720 \u00b1 0.112), outperforming the individual modalities of"}, {"title": "6.2 Multi-shot Prompting", "content": "The performance and calibration metrics presented in Tables 5 and Figures 2, 3 and 5 demonstrate the effectiveness of multi-shot prompting with ChatGPT for AD detection across different modalities and thresholds. From this table, it is noticeable that the integration of MRI and cognitive tests consistently outperforms individual modalities. At a 75% threshold, the combined modality achieves the highest accuracy (0.946 \u00b1 0.001), recall (0.950 \u00b1 0.001), precision (0.946 \u00b1 0.001), and F1-score (0.948 \u00b1 0.001). This reflects the ability of multi-shot prompting to leverage complementary information from multiple data sources, leading to outstanding performance. The performance of using only MRI or cognitive tests' scores also demonstrates improvements, especially for cognitive tests, which reach an"}, {"title": "6.3 Accurate Samples with Confidence Scores", "content": "The analysis of accurately predicted samples, as illustrated in Figures 4 and 5, reveals notable differences in the confidence scores across prompting methods and modalities. When using MRI data alone, the average confidence score for zero-shot prompting is higher at 85%, compared to multi-shot prompting, which falls below 70%. However, the median confidence score for zero-shot is significantly lower-by more than 10%-indicating greater variability and less consistency in its predictions. In contrast, for multi-shot prompting, the mean and median are closely aligned, suggesting a more stable and consistent distribution of confidence scores.\nFor cognitive tests only, both methods exhibit relatively high confidence, but multi-shot prompting outperforms zero-shot. The mean confidence score for zero-shot is approximately 76%, while multi-shot achieves a higher 81%. This trend is similarly reflected in the median, where multi-shot prompting exceeds zero-shot by around 5%, indicating that multi-shot prompting achieves not only higher average confidence but also a more robust distribution.\nFinally, when combining MRI and cognitive tests, the confidence scores for zero-shot and multi-shot prompting are nearly equal. Both methods yield a mean confidence score of approximately 80% and a median of 79% and 78%, respectively. This suggests that integrating MRI and cognitive tests significantly improves prediction consistency, regardless of the prompting method. Overall, while zero-shot prompting demonstrates higher confidence for MRI-only predictions, it comes with greater variability. In contrast, multi-shot prompting consistently delivers more stable confidence scores across all modalities, particularly excelling when cognitive tests or combined data are used. This highlights the advantage of multi-shot prompting in enhancing predictive confidence and minimizing uncertainty."}, {"title": "7 Conclusion and Discussion", "content": "First and foremost, based on the results developed with 9300 samples in this paper, we may conclude that ChatGPT can be a supportive tool to diagnose AD. However, there are some notices to leverage its capabilities. To begin with, this study addresses the research questions outlined in Section 1, focusing on the emerging yet underexplored application of ChatGPT for AD detection. The findings demonstrate that ChatGPT can effectively diagnose AD using both zero-shot and multi-shot prompting approaches. Notably, combining MRI and cognitive tests as predictors outperform using either modality alone, highlighting the advantage of multimodal data integration.\nWhen examining performance in detail, multi-shot prompting significantly surpasses zero-shot prompting, achieving an accuracy of 0.946 compared to 0.744 for zero-shot. Both results were obtained with an optimal confidence threshold of 75%. Furthermore, other performance metrics consistently tend to multi-shot prompting, underscoring its precision.\nIn addition, the calibration results strengthen the effectiveness of multi-shot prompting. Using combined MRI and cognitive tests, multi-shot prompting achieves ECE and MCE values of 0.066 and 0.005, respectively, at a threshold of 75%. While zero-shot prompting does not perform as well, it still demonstrates notable calibration improvements when combining MRI and cognitive tests. Specifically, zero-shot achieves its lowest ECE of 0.129 at a 25% threshold and an MCE of 0.207 at a 75% threshold.\nOverall, these results highlight the clear advantage of multi-shot prompting for AD detection, both in predictive accuracy and calibration capability, particularly when leveraging the combined MRI and cognitive test data. This paper can open a new approach to AD detection, which is paramount for the QoL in societies [58]. Especially it also provides opportunities for further research to leverage this technology for resource-limited regions in the world, to be a supportive tool easing the problem of shortage of AD specialists."}, {"title": "", "content": "Regarding future developments of this research, several key objectives have been identified to enhance its scope and impact. Firstly, incorporating larger and more diverse datasets is essential to achieve more comprehensive and generalizable results, ensuring the robustness of the proposed approach. Secondly, conducting a fairness assessment [59, 60] is critical to evaluate potential biases in the model, particularly concerning its performance across different demographic groups or underprivileged populations. This will help address fairness concerns and ensure fair outcomes. Furthermore, ChatGPT should be compared with other techniques, such as Gemini or Llama 2 [61, 62], to conduct a comparative evaluation and determine whether ChatGPT remains the most effective method for this application."}]}