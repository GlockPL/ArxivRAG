{"title": "PAIRBENCH: A Systematic Framework for Selecting Reliable Judge VLMs", "authors": ["Aarash Feizi", "Sai Rajeswar", "Adriana Romero-Soriano", "Reihaneh Rabbany", "Spandana Gella", "Valentina Zantedeschi", "Jo\u00e3o Monteiro"], "abstract": "As large vision language models (VLMs) are increasingly used as automated evaluators, understanding their ability to effectively compare data pairs as instructed in the prompt becomes essential. To address this, we present PAIRBENCH, a low-cost framework that systematically evaluates VLMs as customizable similarity tools across various modalities and scenarios. Through PAIRBENCH, we introduce four metrics that represent key desiderata of similarity scores: alignment with human annotations, consistency for data pairs irrespective of their order, smoothness of similarity distributions, and controllability through prompting. Our analysis demonstrates that no model, whether closed- or open-source, is superior on all metrics; the optimal choice depends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp judge), highlighting risks of widespread adoption of VLMs as evaluators without thorough assessment. For instance, the majority of VLMs struggle with maintaining symmetric similarity scores regardless of order. Additionally, our results show that the performance of VLMs on the metrics in PAIRBENCH closely correlates with popular benchmarks, showcasing its predictive power in ranking models.", "sections": [{"title": "1. Introduction", "content": "Vision language models (VLMs) have progressed to the point of having impressive performance on a wide array of tasks (Achiam et al., 2023; Lauren\u00e7on et al., 2024; Reid et al., 2024; Abdin et al., 2024; Wang et al., 2024c; Grattafiori et al., 2024) These tasks range from summarization, visual question answering, image captioning, common sense reasoning question answering (Kembhavi et al., 2016;"}, {"title": "2. Related Work", "content": "Using language models as automatic evaluators has become a somewhat common practice with popular approaches such as GPTSCORE and G-eval (Fu et al., 2023; Liu et al., 2023) being used to rank responses in the NLP domain. Due to that, there has been a significant amount of recent work that has investigated the capabilities and limitations of using LLMs as judges (Thakur et al., 2024; Chiang & Lee, 2023; Murugadoss et al., 2024; Shankar et al., 2024). Chiang & Lee (2023) have shown that LLM evaluations are consistent and reproducible, making them suitable alternatives for human evaluation, they argue that these models inherent biases should prevent them using independently rather than alongside human experts. Furthermore, Zheng et al. (2023) reveal that large VLMs, e.g., GPT-4 Turbo, align well with human judgments and Thakur et al. (2024) further states that simpler models may still outperform GPT-4 Turbo in ranking tasks due to superior alignment metrics. Also, recent work assessed how humans can help LLMs evaluate better by testing different instruction types or designing tools that result in more balanced evaluations (Murugadoss et al., 2024; Shankar et al., 2024).\nIt is worth noting that known limitations of LLMs such as their lack of invariance to the order of examples given in a prompt, which is a well studied issue of natural language models (Fang et al., 2024), may render auto evaluation unreliable. Similarly, Berglund et al. (2023) show failure cases where models trained on unidirectional relationships do not infer the reverse, indicating systemic limitations even in state-of-the-art LLMs such as GPT-4 (as seen in Figure 1 and in Appendix A for VLMs). Our main goal is to investigate the reliability of automated evaluation in the multimodal context, by probing the models to compare data pairs.\nNamely, the evaluations we carry out focus on testing in multiple different ways how good VLMs are when it comes to comparing data instances, such as whether VLMs prompted"}, {"title": "3. PAIRBENCH", "content": null}, {"title": "3.1. Dataset Creation", "content": "The PAIRBENCH framework takes in a source dataset and creates augmented versions of the data to obtain data pairs to probe the evaluation skills of a model. In our instances, we use COCO (Lin et al., 2014), IN100 (Deng et al., 2009) and WhatsUp (Kamath et al., 2023) datasets as the source for the original data points. We utilize COCO and IN100 as image-only datasets and WhatsUp as an image-text dataset. We select 500 random images from each of COCO and IN100 and all the image-text pairs from both subsets provided by the WhatsUp dataset to be used in our instantiation of PAIRBENCH. Full details of our released datasets are given in Table 8.\nTo isolate the effect of different data characteristics on model performance, PAIRBENCH creates pairs of image-image and image-text data that are identical except for one or a few controlled features. The generated data consists of points from the original dataset paired with their transformed version. For COCO and IN100, we create a different control sample for each one of the transformations in {color jitter, rotation, gaussian blur, perspective shift, elastic transformation}, which defines the characteristic that differs between images. For the data from WhatsUp, we construct the data pairs by either only using the 'spatial position' transform, or 'spatial position' transform in addition to one of the previous five characteristics to additionally assess coupling effects. However, since transforms are not well-defined for texts, only 'spatial position' transform is applied for the image-text pairs. Note that the image-image pairs from WhatsUp are the most challenging since they all have at least the 'spatial position' transform, which is a well-known blind-spot of VLMs as shown by previous literature (Kamath et al., 2023; Wang et al., 2024a). As a result, we end up creating five image-image sub-datasets for each of COCO and IN100, six subsets for each of the two subsets of WhatsUp, using each of the transformations, and one image-text sub-dataset for each of the subsets of WhatsUp. The details of the transforms applied to each category are shown in Figure 2.\nNext, for each original image, we construct three types of pairs: an identical, a transformed, and an irrelevant pair. In all three versions of these pairs, the first data point is the original (non-transformed) image. For the 'identical' pair, the second data point is another version of the image with 95% of its original size for the image-image pair and the correct caption for the image-text pair. The second data point in the 'transformed' pair is the original image (caption) with the transformation applied to it for the image-image (image-text) pair. Finally, the 'irrelevant' pair's second data point is a transformed version of a random image (caption) from the rest of the dataset.\nEquipped with the constructed control samples, PAIR-BENCH prompts the VLM to score the similarity of each data pair based on a set of criteria. The criteria consists of the conditions indicating whether the model under examination should be 'sensitive' or 'invariant' to the transformations applied for that specific sub-dataset. These two settings (sensitive or invariant) measure how well each model can recognize the differences between the data pair and follow the prompt's criteria. If a model can successfully capture a specific feature, it will have no problem being variant or invariant to it; however, if it cannot detect it or has a bias"}, {"title": "3.2. Metrics", "content": "To measure the reliability of VLMs in scoring data pairs, we define four metrics that we measure across datasets and models: MMScore, \u025b-RelaxSym, Smoothness (SM), and Controllability (Cont).\nWe adopt the following notation to formulate the metrics: we denote the VLM being evaluated as M and the condition, which determines if the prompt instructs the model to be sensitive or invariant to a visual feature, as C\u2208 {sens,inv}. Finally, given a dataset DN = {(d1, d2), (d3, d4), ..., (d2N\u22121, d2N))}, we denote the similarity score of a data pair (di, dj) \u2208 Dn returned by an VLM (M) for a given condition (C) as:\nSM(di, dj) := M(C, di, dj),\nwhere (di, dj) could be an image-image or image-text pair. Note that we instruct the model to generate the output in a structured format to make sure the predicted score is parsable from the model output. If s\u2081(di, dj) is valid, it would fall in the set V = [1, 10]. However, models often do not consistently follow the details of the prompt and may produce scores not in V or outputs that do not satisfy the output format, in which case we set s\u2081(di, dj) = \u22121.\nFinally, to evaluate a model M on DN given condition C, we create and annotate the set of all its outputs as:\nSM(DN) = {$(di, dj) | (di, dj) \u2208 DN U rev(DN)},\nwhere rev(DN) = {(d2, d\u2081), (d4, d3), ..., (d2N, d2N-1))} are the data pairs in reverse order."}, {"title": "3.2.1. MMSCORE", "content": "We consider the normalized mutual information (MMScore) between the predicted scores and the ground-truth ones as the main metric of PAIRBENCH. Instead of accuracy or squared error metric, we consider MMScore since we do not explicitly prompt the VLM with examples of the correct scores and hence, cannot expect it to predict them directly. MMScore is better suited for PAIRBENCH as it focuses on whether the VLM's scores are predictive of the ground-truth ones without penalizing outputs that do not exactly match them. The better a model can reproduce the variance in the ground-truth score, the better it is able to recognize that characteristic. Hence we write,\nMMScore(M, C, DN) = NMI(SM(DN), GTC(DN)),\nwhere NMI(., .) is the normalized mutual information and GTC(.) is the ground truth of the input dataset considering the condition of C."}, {"title": "3.2.2. \u03b5-RELAXSYM", "content": "When leveraging VLMs as similarity kernels or auto evaluators, a fundamental characteristic one would expect is their symmetry as a kernel. Surprisingly, however, we found that most models do not satisfy exact symmetry, i.e., the equality of sim(a,b) and sim(b,a). We thus introduce \u025b-RelaxSym, which tolerates a difference of & between the scores that should be equal. More specifically, to analyze the symmetry of VLMs on a dataset DN, we compute the \u025b-RelaxSym of (M) on DN:\n\u025b-RelaxSym(M, DN) = \u2211(di,d\u2081)\u2208DN SoftEq\u300f(M, di, dj),\nwhere SoftEq (M, di, dj) is defined as:\nSoftEq (M, di, dj) =\n(1(1s(di, dj) \u2013 8 (dj,di) \u2264 \u025bl), s\u2081(di, dj), s\u2081(dj, di) \u2208 V,\n\u0967\u0966, otherwise.\nIn the continuation of this paper, we set \u025b = 1 and provide ablation studies in Figure 7 in the Appendix."}, {"title": "3.2.3. SMOOTHNESS", "content": "We aim to measure how smooth kernels induced by VLMs are. For instance, a non-smooth kernel would be such that pairs are either exactly the same or completely different, while a smoother kernel is more nuanced. We measure for smoothness via the diversity of the predicted scores. Given SM, smoothness (SM) is computed as:\nSM(M,DN, C) = Ent({s|s \u2208 SM(DN) and s \u2208 V}),\nwhere Ent(.) is the entropy of a set relative to its support, i.e., the set of candidate inputs."}, {"title": "3.2.4. CONTROLLABILITY", "content": "To measure how responsive a model is to the given prompt, we define a metric based on the difference of its MMScore in the sensitive and invariant settings. The more controllable a model is, the less discrepancy is observed between the sens and invar settings. Hence, when measuring the controllability on DN for a model M is defined as\nCont(M, DN) =\n|MMScore(M,sens,DN)-MMScore(M,inv,DN)|\n\u221a(MMScore(M,sens,DN)\u00d7MMScore(M,inv,DN))"}, {"title": "4. Evaluation Results", "content": null}, {"title": "4.1. Experimental Setting", "content": "We choose a comprehensive set of open- and closed-source vision-language models and evaluate them using the instantiations of PAIRBENCH. We evaluated the following openly available models:\n\u2022 Chameleon-7B (Lu et al., 2024),\n\u2022 LLAVA-OneVision-7B (Li et al., 2024),\n\u2022 Pixtral-12B (Agrawal et al., 2024),\n\u2022 Phi-3.5-vision (Abdin et al., 2024),\n\u2022 four versions of InternVL2 (Wang et al., 2024c): InternVL2-8B, InternVL2-4B, InternVL2-2B, InternVL2-1B,\n\u2022 four versions of InternVL2.5 (Chen et al., 2024c): InternVL2.5-8B, InternVL2.5-4B, InternVL2.5-2B, InternVL2.5-1B,\n\u2022 two versions of Qwen2-VL (Wang et al., 2024b): Qwen2-VL-2B, Qwen2-VL-7B,\n\u2022 three versions of Molmo (Deitke et al., 2024): MolmoE-1B, Molmo-7B-O, Molmo-7B-D.\nWe also considered commercial grade models and benchmarked 3 versions of GPT-40 (Achiam et al., 2023)(GPT-40-0513, GPT-40-0806, GPT-40-1120), GPT-40-mini-0718, and two versions of Gemini-1.5 (Reid et al., 2024) (Gemini-1.5-Flash, Gemini-1.5-Pro). Note that we consider multiple versions of the same architecture, as opposed to using the newest/largest version, to understand better how model capacity affects each of the metrics. We provide an extended analysis of different model versions in Appendix B.0.2.\nWe run all open-source models on a single NVIDIA H100 GPU and use API calls for closed-source models either from OPENROUTER\u00b9 or OpenAI \u00b2.\n'https://openrouter.ai/\n\u00b2https://platform.openai.com/"}, {"title": "4.2. Results", "content": "We analyze and plot the results of the best models in Figure 3 and provide an aggregated version of the metrics over all four datasets in Table 1. We aggregate different splits/datasets by taking the average of them to give each sub-dataset equal importance in the final number. The full set of benchmarking results of all models for PAIRBENCH on all datasets and metrics are reported in Appendix B."}, {"title": "4.2.1. GENERAL OBSERVATIONS", "content": "As illustrated in Figure 3, we observe no model, whether closed- or open-source, is the best performer across all four metrics. Moreover, we further observe that for each metric, no model is the 'best' similarity kernel across the four different datasets either. This shows how features of the dataset and also the metrics a user might want to optimize play a crucial role in which VLM to choose as the best similarity kernel/judge. For instance, among open-source models, although InternVL2.5-8B outperforms the rest in MMScore, it is less controllable and smooth than Qwen2-VL-7B or LLAVA-OneVision-7B.\nWhen considering PAIRBENCH's main metric, MMScore, we notice that the performance of models is generally better on image-image pairs rather than image-text pairs. Furthermore, we observe that although open-source VLMs are roughly comparable to closed-source ones on PAIR-BENCHWU-IT, the gap between the two groups is larger in the image-image pairs. However, InternVL2.5-8B is a strong competitor to closed-source models considering all four metrics and could potentially be used as a substitute to closed-source models as a similarity kernel.\nInterestingly, we further observe a pattern regarding GPT-40-1120, a common default judge used in the literature, and its lower cost version, GPT-40-mini-0718; they both suffer from low 1-RelaxSym when comparing image-text pairs, and the cheaper model's Cont and SM is higher or comparable to that of the expensive one across datasets. This emphasizes the importance of PAIRBENCH in analyzing the capabilities of models as similarity kernels to be better used as judges. We analyze and plot these"}, {"title": "4.2.2. ENCODERS VS VLMS", "content": "For the image-image task, we explore how image encoders compare to VLMs on our metrics. To this end, three DI-NOv2 versions (DINOv2-Base, DINOv2-Small, and DINOv2-Large) and the LAION- and OpenAI- CLIP-trained ViTs (base and large) are chosen to encode images. Since feature controllability on image-encoders is limited to the image augmentation transformation (CJ, R, PS, GB, ET), we only compare image-encoders to VLMs on PAIR-BENCHCOCO and PAIRBENCHIN100.\nTo generate the similarity score of a given image-pair with an image-encoder, we compute the cosine similarity of the representation of each image and scale the scores between 1-10, and round them to the nearest integer. To generate the criteria-sensitive similarity score, we create the representations of the image-pair by simply using the representations output by the encoder for each image. On the other hand, when generating the criteria-invariant score, where the criteria is a specific transformation (T), we generate the representation of each image as the average of the representations of the encoder for k versions of the image where random amounts of T are applied to the image. In our experiments, we set k = 5.\nWe report results in Figure 4. We see encoders do better than open-source VLMs most of the time and are comparable to closed-source models (besides CJ). This shows although significantly smaller, encoders can be at least as good as VLMs, enabling similarity scoring at a much lower cost. Also, encoder-generated scores are trivially symmetric as well since the underlying cosine similarity is symmetric. However, they lack in controllability as they are limited to image-only comparisons and can only consider criteria that can be applied to the image using augmentations, i.e., spatial position transform cannot be applied to images for encoders."}, {"title": "4.3. Correlation with Benchmarks", "content": "To showcase the effectiveness of our introduced metrics with PAIRBENCH in predicting model performance, we compute the Spearman correlation with other popular benchmarks used in the literature. By showing correlations of our metrics with these benchmarks, we show that although the PAIRBENCH framework introduces simple and cheap methods focused on evaluating similarity kernels induced by prompted VLMs, these metrics are predictive of an VLM's performance on other benchmarks.\nWe collect all the model performances from the OPENVLM LEADERBOARD(Duan et al., 2024) and filter out the models we evaluate, resulting in all 23 (including different version-s/capacities of closed- and open-source) models. Next, by filtering out the benchmarks that have evaluation scores for all 23 models on OpenVLM, we end up with AI2D (Kembhavi et al., 2016), HallusionBench (Guan et al., 2024), MMBench (Liu et al., 2025), MMStar (Chen et al., 2024b), MMMU (Yue et al., 2024), MathVista (Lu et al., 2023), MM-Vet (Yu et al., 2023), OCRBench (Liu et al., 2024b).\nBefore computing the correlations, each metric is first aggregated for each model across all the configurations created by PAIRBENCH. Specifically, we aggregate all features within each dataset (e.g., CJ, SP, etc.) and further across all datasets (e.g., COCO, WhatsUp) to have a single number per metric for each model. As seen in Table 2, all metrics in PAIRBENCH have a high positive correlation with benchmarks. Note that since MMScore has the highest significant correlation, we choose it to be the main metric of PAIRBENCH. However, measuring any of these metrics incurs a low cost as it does not require expert-generated or costly annotations, and since they have high correlations, they can serve as a low-cost surrogate of a model's performance during training or validation. We further show scatter plots that highlight correlations in Figure 5, and more comprehensively in Figure 10 in the Appendix B."}, {"title": "5. Conclusion and Future Work", "content": "We introduced PAIRBENCH, a comprehensive framework for evaluating the reliability of VLMs when used to define similarity kernels. PAIRBENCH enables assessing how different models will behave when acting as evaluators by measuring kernel properties such as alignment with ground truth relevance, symmetry, smoothness, and controllability. Interestingly, by leveraging controlled data transformations, we found that PAIRBENCH not only allows for fine-grained analysis of model biases and strengths, but it also offers a cost-effective alternative to large-scale benchmarks.\nWe carried out a large-scale benchmarking covering several VLMs and demonstrated that no single model excels across all four metrics or dataset configurations. While commercial-grade models generally performed better on image-image comparisons, openly available models such as InternVL2.5-8B showed competitive results, particularly in MMScore. Furthermore, our findings indicate that commonly used judge models exhibit limitations. For instance, GPT-40-1120 lacks in terms of symmetry and smoothness, highlighting the necessity of careful selection based on specific evaluation needs.\nFrom a more practical perspective, we established that PAIR-BENCH metrics, particularly MMScore, correlate strongly with model performance on well-known benchmarks, reinforcing its utility as a low-cost surrogate for ranking models or guiding cross-validation during training. As the field progresses, we anticipate that PAIRBENCH will serve as a valuable tool for improving model evaluation practices."}, {"title": "Impact Statement", "content": "This work contributes to the multimodal AI research community by introducing a systematic evaluation framework for assessing vision-language models capabilities as automated evaluators. By providing a comprehensive benchmarking methodology, we examine these models capabilities in comparing multimodal data pairs and generating meaningful similarity scores. Our analysis reveals critical limitations in employing VLMs as evaluators including asymmetric scoring patterns and inconsistent alignment with human judgments. These insights are particularly important as VLMs are increasingly adopted as automated evaluation tools across various domains. While our framework helps surface these limitations, addressing the underlying challenges will require continued research effort from the broader AI community."}, {"title": "A. Error Analysis", "content": "In this section, we look into the outputs of each model and their errors given different data pairs. More specifically, we look into errors made by Gemini-1.5-Pro, GPT-40-1120, InternVL2.5-8B,Pixtral-12B, Qwen2-VL-7B, and Phi-3.5-vision."}, {"title": "A.1. Gemini Models", "content": "Below is an error example of Gemini-1.5-Pro on a data-pair from PAIRBENCHCOCOwith color jittering (CJ)."}, {"title": "A.2. GPT40", "content": "Below is an error example of GPT-40-1120 on a data-pair from PAIRBENCHcocowith color jittering (CJ)."}, {"title": "A.3. InternVL2.5", "content": "Below is an error example of InternVL2.5-8B on a data-pair from PAIRBENCHCocowith that are irrelevant to each other."}, {"title": "A.4. Pixtral-12B", "content": "Below is an error example of Pixtral-12B on a data-pair from PAIRBENCHCOcowith rotation (R)."}, {"title": "A.5. Qwen2-VL", "content": "Below is an error example of Qwen2-VL-7B on a data-pair from PAIRBENCHCocowith gaussian blur (GB)."}, {"title": "A.6. Phi3.5", "content": "Below is an error example of Phi-3.5-vision on a data-pair from PAIRBENCHCOCO with elastic transform (ET)."}, {"title": "B. Full Results", "content": "In this section, we provide the MMScoreof all models on all the different splits of PAIRBENCHCOCO, PAIRBENCHIN100, PAIRBENCHWU-II, and PAIRBENCHWU-IT in Tables 3, 4, 3, 4, 5, 6, and 7."}, {"title": "B.0.1. ALL E-RELAXSYMFOR DIFFERENT ES", "content": "To show the &-RelaxSym for different values of 8, we plot Figure 7 and show as \u025b gets higher, the values go higher. However, some models such as the GPT40 models struggle with symmetry. Please note that if \u025b = 0, it is the same as not having a threshold and hence calculating exact symmetry rather than a relaxed version."}]}