{"title": "FLIP: FLOW-CENTRIC GENERATIVE PLANNING FOR GENERAL-PURPOSE MANIPULATION TASKS", "authors": ["Chongkai Gao", "Haozhuo Zhang", "Zhixuan Xu", "Zhehao Cai", "Lin Shao"], "abstract": "We aim to develop a model-based planning framework for world models that can be scaled with increasing model and data budgets for general-purpose manipulation tasks with only language and vision inputs. To this end, we present FLow-CentrIc generative Planning (FLIP), a model-based planning algorithm on visual space that features three key modules: 1) a multi-modal flow generation model as the general-purpose action proposal module; 2) a flow-conditioned video generation model as the dynamics module; and 3) a vision-language representation learning model as the value module. Given an initial image and language instruction as the goal, FLIP can progressively search for long-horizon flow and video plans that maximize the discounted return to accomplish the task. FLIP is able to synthesize long-horizon plans across objects, robots, and tasks with image flows as the general action representation, and the dense flow information also provides rich guidance for long-horizon video generation. In addition, the synthesized flow and video plans can guide the training of low-level control policies for robot execution. Experiments on diverse benchmarks demonstrate that FLIP can improve both the success rates and quality of long-horizon video plan synthesis and has the interactive world model property, opening up wider applications for future works.", "sections": [{"title": "1 INTRODUCTION", "content": "World models refer to learning-based representations or models that learn to simulate the environment (LeCun, 2024; Ha & Schmidhuber, 2018). With world models, agents can imagine, reason, and plan inside world models to solve tasks more safely and efficiently. Recent advancements in generative models, especially in the area of video generation (Brooks et al., 2024; Blattmann et al., 2023; Yang et al., 2023), have demonstrated the application of generating high-quality videos as world simulators with internet-scale training data. World models have opened new avenues across various fields, particularly in the domain of robotic manipulation (Yang et al., 2023; Mendonca et al., 2023; Seo et al., 2023), which is the focus of this paper.\nThe intelligence of generalist robots involves two levels of abilities (Caucheteux & King, 2022; Manto et al., 2012): 1) high-level planning of the abstraction sequence of the task with multi-modal inputs, and 2) low-level execution of the plan by interacting with the real world. A well-designed world model could serve as an ideal way to realize the first function, for which it should enable model-based planning. This requires the world model to be interactive, i.e., can simulate the world according to some given actions. The core of this framework is to find a scalable action representation that serves as the connection between high-level planning and low-level control. This representation should: 1) be able to represent various kinds of movements across diverse objects, robots, and tasks in the whole scene; 2) be easy to obtain or label a large amount of training data for scaling up. Regarding this, Yang et al. (2023); Du et al. (2023); Zhou et al. (2024) use languages from VLMs (Driess et al., 2023) as high-level actions, while Wu et al. (2024) directly use"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 WORLD MODELS FOR DECISION MAKING", "content": "Early works of world models learn system dynamics in low dimensional state space (Lesort et al., 2018; Ferns et al., 2004), perform planning in latent space (Nasiriany et al., 2019), or train networks to predict the future observations (Finn & Levine, 2017) and actions (Kaiser et al., 2019). Modern model-based reinforcement learning methods (Hafner et al., 2020; 2023; Hansen et al., 2023; Baker et al., 2022; Micheli et al., 2022) focus on latent space imagination with coupled dynamics and action modules. Recent works leverage powerful scalable video generation architectures like Diffusion Transformer (Peebles & Xie, 2023) and large-scale training data (Grauman et al., 2022) to develop video generation networks to simulate an interactive environment (Yang et al., 2023; Bruce et al., 2024; Shridhar et al., 2024; Valevski et al., 2024; Wu et al., 2024; Zhu et al., 2024; Wu et al., 2024). In this work, we build a world model with separate flow-centric action and dynamics modules as well as a vision-language value model for model-based planning for robot manipulation tasks."}, {"title": "2.2 FLOW AND VIDEO MODELS FOR MANIPULATION", "content": "Flows are the future trajectories of query points on images or point clouds. They are universal descriptors for motions in the video, while video data contains rich knowledge of behaviors, physics, and semantics, and have unparalleled scalability in terms of both content diversity and data acquisition. For robotics, people have been trying to use flows as policy guidance (Wen et al., 2023; Bharadhwaj et al., 2024), learn dense correspondence (Jiang et al., 2024b), tool using (Seita et al., 2023), or cross-embodiment representations (Xu et al., 2024a; Zhu et al., 2024; Yuan et al., 2024). Videos are usually used for learning inverse dynamics (Du et al., 2024; Finn & Levine, 2017; Brandfonbrener et al., 2024; Gao et al., 2021), rewards (Ma et al., 2022; 2023; Nair et al., 2022; Zakka et al., 2022), transferrable visual representations such as latent embeddings (Sermanet et al., 2018; Nair et al., 2022; Liu et al., 2024a), key points (Huang et al., 2024; Di Palo & Johns, 2024), affordance (Bahl et al., 2023; Shu et al., 2017), flows (Wen et al., 2023; Xu et al., 2024a; Bharadhwaj et al., 2024), scene graphs (Zhang et al., 2024; Jiang et al., 2024a; Kumar et al., 2023), or acquire similar manipulation knowledge from human videos (Wang et al., 2023; Mendonca et al., 2023; Shao et al., 2021; Liang et al., 2024). Recent works also use video generation techniques as visual simulation (Yang et al., 2023; Liu et al., 2024b). In this work, we build our action, dynamics, and value modules all based on video and language inputs, enabling the scalability of our framework."}, {"title": "3 THREE FUNDAMENTAL MODULES OF FLIP", "content": ""}, {"title": "3.1 PROBLEM FORMULATION", "content": "We model a manipulation task T as a goal-conditioned Partially Observable Markov Decision Process (POMDP) parameterized by (S, O, O, A, P, R, \u03b3, g) where S, A, O are state, action, and observation spaces, $ : S \u2192 O is the state-observation mapping function, P : S \u00d7 A \u2192 S is the transition function, R: S \u00d7 A \u2192 R is the reward function, y is the discount factor, and g is the goal state. In this work, the observation space is the image space: O = $R^{H\u00d7W\u00d73}$, where Hand W are the height and width of the image, and R(s,g) = I(s == g) 1 is a goal-conditioned sparse reward. The task is solved if the agent maximizes the return $\\sum_{t=0}^{T} R(s_{t}, g)$.\nWe aim to solve this problem by learning a world model and a low-level policy. The world model performs model-based planning on image and flow spaces to maximize the return, synthesizing long-horizon plans, and the low-level policy executes the plan in the real environment. We aim to train the world model only on language-annotated video datasets to make it general-purpose and scalable,"}, {"title": "3.2 FLOW GENERATION NETWORK AS ACTION MODULE", "content": "Overview. The action module of FLIP is a flow generation network \u03c0f that generates image flows (future trajectories on query points) as actions for planning. The reason why we use a generation model rather than a predictive model is that we are doing model-based planning, where the action module should give different action proposals for sampling-based planning. Formally, given h step image observation history Ot-h:t at timestep t, the language goal g, and a set of 2D query points coordinates pt = {p}k=1, where pt,k = (u, v) is the k-th query point coordinate on ot, the flow generation network \u03c0f generates coordinates of query points in future L timesteps (including the current step): Pt:t+L = \"f(Ot\u2212h:t, Pt, g) \u2208 RL\u00d7K\u00d72.\nTraining Data Annotation. The flows of query points can be extracted from pure video data by the off-the-shelf point tracking models. The problem is how to select query points. Previous works either use SAM (Ravi et al., 2024) to select query points on the region of interest or select query points on active/stationary regions with a predefined ratio (Wen et al., 2023). These methods face two problems: 1) for diverse kinds of videos with complex scenes, it is hard for modern segmentation models (Ravi et al., 2024) to segment perfect regions of interest with no human assistance; 2) for long-horizon videos, there may be objects appearing/disappearing in the video, and using query points only from the initial frame become problematic. To this end, in this work, we uniformly sample dense grid query points for the whole image (for the first problem) at each timestep, and track them for only a short-horizon video clip, i.e., tracking on video clips starting from every frame of the long-horizon video (for the second problem). This can mitigate the second problem because even if some objects appear/disappear, their influences are restricted in a short horizon. Formally, for each frame in the dataset, we uniformly sample a grid of Ng points, then use Co-Tracker (Karaev et al., 2023) to generate the flows {Pt+L}1 within a future video clip of L steps.\nModel Design. We design a Conditional VAE (Kingma, 2013) with transformer (Vaswani, 2017) architecture for flow generation, as illustrated in Figure 2. As opposed to previous flow prediction works (Wen et al., 2023; Xu et al., 2024a; Bharadhwaj et al., 2024), we observe enhanced performance when predicting relative displacements rather than absolute coordinates, i.e., we predict k$\\Delta p = p_{t+1}^{k}-p_{t}^{k}$ for the k-th point at each time step.\nFor the VAE encoder, we encode ground truth flow {pt}=1, patchify observation history Ot\u2212h:t, and encode language embedding from Llama 3.1 8B (Dubey et al., 2024) to tokens, concatenate them with a CLS token for gathering the information, and then send them to a transformer encoder to extract the output at the CLS token position as the latent variable of VAE. For the VAE decoder,"}, {"title": "3.3 FLOW-CONDITIONED VIDEO GENERATION NETWORK AS DYNAMICS MODULE", "content": "Overview. The flow-conditioned video generation network D generates the following L frames based on current image observation history ot-h:t, the language goal g, and the predicted flow Pt:t+L to enable iterative planning for the next planning step: \u00d4t+1:t+L = D(ot-h:t, g, Pt:t+L).\nModel Design. We design a new latent video diffusion model that can effectively take as input different kinds of conditions such as images, flows, and language. This model is built on the DiT (Peebles & Xie, 2023) architecture with spatial-temporal attention mechanism (Ma et al., 2024; Bruce et al., 2024; Zhu et al., 2024). The background knowledge of latent video diffusion models is in Appendix A.1. Here we introduce the design of the multi-modal condition mechanism.\nIn the original DiT (Peebles & Xie, 2023) and previous trajectory-conditional video diffusion paper (Zhu et al., 2024), they use adaptive layer norm zero (AdaLN-Zero) blocks to process conditional inputs (such as diffusion timestep and class labels), which regress the scale and shift parameters of the layer norm layers from all conditions with a zero-initialized MLP. However, AdaLN will compress all conditional information to scalars, and cannot enable fine-grained interaction between different parts of conditions with the inputs. Thus, this mechanism is not suitable for complex conditions such as image and flow (Zhang et al., 2023; Bao et al., 2023). To this end, we propose a mixed conditioning mechanism for multi-modal conditional generation. We use cross attention for fine-grained interactions between flow conditions (tokenized as Nq tokens) and observation conditions and noisy frames. For image history conditions, we concatenate them on the Gaussian noise frames. We use AdaLN-Zero to process the global conditions including the diffusion timestep and language instruction, as shown in Figure 2. To keep the observation condition clean, we do not add noise to Ot-h:t during the diffusion process and do not perform denoising on them either."}, {"title": "3.4 VISION-LANGUAGE REPRESENTATION LEARNING AS VALUE MODULE", "content": "Overview. The value module V assigns an estimated value V\u2081 for each frame or to enable model-based planning on the image space, based on the language goal g: V\u2081 = V(ot, g). In this work, we adopt LIV (Ma et al., 2023) to instantiate the value function. LIV first learns a shared language-vision representation from action-free videos with language annotations. It then computes the similarity between current frame ot and g as the value for timestep t: V\u2081 = S(\u03c81(ot), \u03c8L(g)) = \\frac{\u03c81(0t)\u22c5\u03c8L(g)}{||\u03c81(Ot)||\u22c5||\u03c8L(g)||}, where 41 and VL are the encoding network for image and language respectively, and S is the y-weighted cosine similarity metric.\nThe pretrained LIV model needs to be fine-tuned to give good value representation on new tasks (Ma et al., 2023). The original fine-tuning loss LLIV = L1(41) + LL(\u00a51,4L) is calculated on sampled sub-trajectory batch data\n{0,..., 0, 0+1,..., or, g}, from each task Ti, where s \u2208 [0,T; \u2212 1],s \u2264 k < Ti. For\n\u2200 task i, L1(41) will use time contrastive learning to increase the similarity S(0, of) between"}, {"title": "4 FLOW-CENTRIC GENERATIVE PLANNING", "content": ""}, {"title": "4.1 MODEL-BASED PLANNING WITH FLOWS, VIDEOS, AND VALUE FUNCTIONS", "content": "Directly generating long-horizon videos autoregressively is usually not accurate (Wen et al., 2023; Yang et al., 2023; Du et al., 2024) due to compounding errors. In this work, we use model-based planning to search for a sequence of flow actions and video plans that maximizes the discounted return:\n$O_{0: L}^{*}=\\arg \\max _{O_{0: L} \\sim \\pi_{f}, D} \\sum_{i=0}^{L} \\gamma^{i} R\\left(o_{i}, g\\right)$.\nAccording to Bellman Equation (Sutton, 2018), this equals stepping towards the next state that max-imizes rt + V*(st+1,g) at each time step given an optimal value function V*. In our problem, rt = -1 is a constant for every step before reaching the goal, and we assume our learned value function V = V*, thus our problem is simplified to find the next state that maximizes V at each time step. Note this reward design also encourages finding the shortest plan. We use hill climbing (Selman & Gomes, 2006) to solve this problem. It initializes B plan beams. At each timestep t, given current image history Ot-h:t and the language goal g, it employs \u03c0f to generate multiple flow ac-tions pt+1:t+L = \"f(Ot\u2212h:t, Pt, g) on uniformly sampled query points as candidates for tree search,"}, {"title": "4.2 PLAN-CONDITIONED LOW-LEVEL POLICY", "content": "The low-level policy TL are given the image observation history Ot\u2212h:t, the language goal g, and the predicted flow plan pt:t+L as well as the video plan ot+1:t+L = D(0t\u2212h:t, g, Pt+1:t+L) to predict the low-level robot action at:t+L that drive the robot to operate in the environment. We train different policies that take as input different kinds of condition information, with all of them trained on a few demonstrations with action labels. The policy architectures are similar to diffusion policy (Chi et al., 2023). Details can be found in Appendix A.3."}, {"title": "5 EXPERIMENTS", "content": "In this section, we first demonstrate that FLIP can: 1) perform model-based planning for different manipulation tasks; 2) synthesize long-horizon videos (> 200 frames); and 3) can guide the low-level policy for executing the plan for both simulation and real-world tasks. We also evaluate the action, dynamics, and value modules separately compared to corresponding baselines and show the interactive, zero-shot, scalability properties of FLIP. More results and videos are on our website."}, {"title": "5.1 MODEL-BASED PLANNING FOR MANIPULATION TASKS", "content": "Setup. In this section, we train FLIP on four benchmarks to show its model-based planning ability. The model is given an initial image and a language instruction, and it is required to search the flow and video spaces to synthesize the plan for this task. The first one is LIBERO-LONG (Liu et al., 2024a), a long-horizon table-top manipulation benchmark of 10 tasks in simulation. We train FLIP on 50 \u00d7 10 long-horizon videos with a resolution of 128 \u00d7 128 \u00d7 3 and test on 50 \u00d7 10 new random initializations. The second one is the FMB benchmark (Luo et al., 2023), a long-horizon object manipulation and assembly benchmark with varying object shapes and appearances. We train FLIP on 1K single-object multi-stage videos and 100 multi-object multi-stage videos with a resolution of 128 \u00d7 128 x 3 and test on 50 new initialization for each. The third and fourth suites are cloth folding and cloth unfolding. These two datasets are collected by ourselves. We train each suite on 40 videos with varying viewpoints and test on 10 new viewpoints for each with a resolution of 96 \u00d7 128 x 3.\nWe follow previous works(Du et al., 2023; Zhu et al., 2024) and evaluate our model-based planning results by human evaluating the correctness of generated video plans. That is, we visually assess the percentage of time the video successfully solved the given task. We compare FLIP to two baselines: 1) UniPi (Du et al., 2024), a text-to-video generation method with long-horizon text goals. 2) FLIP-NV, an ablation of FLIP that performs the same beam search but with no value module as guidance.\nResults. Table 1 shows the results. We can see that UniPi achieves low success rates across all tasks, which shows that directly synthesizing long-horizon videos is difficult. FLIP-NV achieves"}, {"title": "5.2 LONG-HORIZON VIDEO GENERATION EVALUATION", "content": "Setup. In this section, we quantitatively evaluate the long-horizon video generation quality of FLIP compared to other video generation models. We choose the same datasets as in Section 5.1 as well as Bridge-V2 (Walke et al., 2023) as the evaluation benchmarks. Here all videos are longer than 200 frames except for Bridge-V2. For Bridge-V2, we train on 10k videos and test on 256 videos with a resolution of 96 \u00d7 128 \u00d7 3. We choose two baselines: 1) LVDM (He et al., 2022b), a state-of-the-art text-to-video method for video generation; 2) IRASim (Zhu et al., 2024), a conditional video generation method with the end-effector trajectories as the condition. We use SAM2 (Ravi et al., 2024) to label the end-effector trajectory for IRASim. We choose model-based metrics including Latent L2 loss and FVD (Unterthiner et al., 2018) as well as a computation-based metric PSNR (Hore & Ziou, 2010). Latent L2 loss and PSNR measure the L2 distance between the predicted video and the ground-truth video in the latent space and pixel space, and FVD assess video quality by analyzing the similarity of video feature distributions\nResults. Table 2 shows the results. We can see that our method consistently outperforms baselines in all datasets. LVDM performs badly on LIBERO-LONG and FMB, and better on Bridge-V2. This is because the videos in Bridge-V2 are shorter than the previous two benchmarks. IRASim performs better than LVDM, which shows the importance of trajectory guidance. However, it generates long-horizon videos in an auto-regressive manner, which has worse results than our method, showing that model-based planning can also help generate high-quality videos by concatenating short-horizon videos generated with rich flow guidance. The results on the FMB benchmark are the worst for all methods. This is because the training videos have many discontinuous transitions, where the robot gripper instantly moves to where the next stage begins. Since our model leverages history observations as input conditions, it can sometimes overcome this discontinuous gap. We qualitatively show the model-based planning results on the four tasks in Figure 4.\nSince FLIP is a universal framework for all manipulation tasks as long as they have language-annotated video datasets, here we qualitatively show FLIP can be used for complex long-horizon video generation including the ALOHA tasks (Aldaco et al., 2024), pen spinning (Wang et al., 2024), robot pilling (Chen et al., 2024), tying plastic bags (Gao et al., 2023), and human peeling eggs, as shown in Figure 7. More video demos are on our website."}, {"title": "5.3 PLAN-GUIDED LOW-LEVEL POLICY", "content": "Setup. In this evaluation we explore how the generated flow and video plans can be used as con-ditions for training a manipulation policy to accomplish the task. We aim to answer the question: which one, flow or video (or both at the same time), is more suitable to be used as the condition to guide the learning of the underlying strategy? We use LIBERO-LONG (Liu et al., 2024a) for evalu-"}, {"title": "5.4 EXPERIMENTS ON FUNDAMENTAL MODULES OF FLIP", "content": "Action Module Experiments. We use two metrics to assess the flow generation model \u03c0\u03c1 quantitatively (Jiang et al., 2024b): 1) Average Distance Error (ADE) between the generated and the ground truth flows in pixel units on all query points; 2) Less Than Delta Ratio (LTDR): the average percentage of points within the dis-tance threshold of 1, 2, 4, and 8 pixels between the reconstructed and the ground truth flows at each time step. Since most of the points are stationary points, in order to better demonstrate the results, we only calculate points with \u03b4\u03b5 \u2265 1. We also do experiments that compare using CVAE and diffusion models as the action module in Appendix B.3.\nWe use LIBERO-LONG (Liu et al., 2024a) and Bridge-V2 (Walke et al., 2023) for evaluation. We compare our method with 3 baselines: 1) ATM (Wen et al., 2023), the state-of-the-art flow prediction module for manipulation tasks; 2) Ours-ABS: directly generating absolute flow coordinates at each timestep rather than generating the scale and direction; 3) Ours-NoAUX: the same architecture of ours with no auxiliary training losses (the flow and image reconstruction losses).\nFrom Table 4, we can see that Ours-ABS generally achieves the same results as ATM, and predicting the scale and directions are better than ATM and Ours-ABS, showing that directly regressing the absolute coordinates is worse than predicting the delta of flows at each timestep. We can also see that the auxiliary losses can help improve the final results.\nDynamics Module Experiments. We evaluate our dynamics module separately with the ground truth flows as conditions on short-horizon video generation. We use PSNR (Hore & Ziou, 2010),"}, {"title": "5.5 APPLICATIONS AND SCALING", "content": "Finally, we train FLIP on LIBERO-90, a large-scale simulation manipulation dataset to show three properties of FLIP. We use 50 videos for each task in the resolution of 3 \u00d7 64 \u00d7 64.\nInteractive World Model. We first show that the trained dynamics module is interactive: it can generate corresponding videos given image flows specified by humans. We use SAM2 (Ravi et al., 2024) to select the region of the robot arm and manually give flows in different directions. Results are shown in Figure 8. We can see the robot arm can move left or right according to the given flow.\nZero-Shot Generation. Secondly, we show that the trained FLIP has zero-shot transfer ability. We test the trained model on LIBERO-LONG. Results are shown in Figure 9. Interestingly, we can see that the pretrained model, without fine-tuning, can generate natural movement for the robot arm with unseen observations and instructions. This shows FLIP has a certain knowledge transfer ability.\nModel Scaling. We show that the action and dynamics module are scalable with increasing model sizes. Figure 10 shows the smoothed ADE and Latent L2 loss on the validation set. It shows that increasing the model size can consistently help achieve better performance for both modules."}, {"title": "6 CONCLUSION AND LIMITATION", "content": "In this work, we present FLIP, a flow-centric generative planning method for general-purpose manip-ulation tasks. FLIP is trained on only video and language data, can perform model-based planning on"}, {"title": "A METHOD DETAILS", "content": ""}, {"title": "A.1 LATENT DIFFUSION MODELS", "content": "Diffusion Models. Diffusion models (Song et al., 2020; Ho et al., 2020) typically contain a forward nosing process and a reverse denoising process. During the forward process, we grad-ually apply noise to real data xo: $q(x_{t}|x_{0}) = N(x_{t}; \\sqrt{\\bar{a}_{t}}x_{0}, (1 - \\bar{a}_{t})I)$ over T timesteps, where constants \u0101t are hyperparameters. By applying the reparameterization trick, we can sam-ple $x_{t} = \\sqrt{a_{t}}x_{0} + \\sqrt{1 - \\bar{a}_{t}}\\epsilon_{t}$, where et ~ N(0, I). During the reverse process, it starts from Gaussian noise xT ~ N(0,I) and gradually removes noises to recover xo: $p_{\\theta}(X_{t-1}|X_{t}) =N(\\mu_{\\theta}(x_{t}), \\Sigma_{\\theta}(x_{t}))$. With reparameterizing \u03bc\u0473 as a noise prediction network e\u0473, the model can be trained with Lsimple (0) = $||\\epsilon_{\\theta}(x_{t}) - \\epsilon_{t}||^{2}$. We also learn the covariance \u03a3\u0473 following Nichol & Dhariwal (2021); Peebles & Xie (2023) with the full KL loss.\nLatent Diffusion and Tokenization. Latent diffusion models (Rombach et al., 2022; Ma et al., 2024) perform diffusion process in a low-dimensional latent space zld rather than the original pixel space. We leverage the pre-trained VAE in SDXL (Podell et al., 2023) to compress each frame ot to latent representations: z\u0142d = Enc(ot), and after the denoising process, the latent representation can be decoded back to the pixel space with the VAE decoder: ot = Dec(z\u0142d). For each zld, it is divided into image patches and tokenized by convolutional networks to P tokens with D dimensions (hidden size). Sequencing the image tokens of all T frames, we get the video token in the shape of TxPxD.\nSpatial-Temporal Attention Mechanism. We leverage transformers (Vaswani, 2017) to imple-ment the dynamics module, and use the memory-efficient spatial-temporal attention mechanism (Ma et al., 2024; Bruce et al., 2024; Zhu et al., 2024), where each attention block consists of a spatial attention block and a temporal attention block. The spatial attention operates on the 1 \u00d7 P tokens within each frame, and the temporal attention operates on the T \u00d7 1 tokens across T timesteps at the same location."}, {"title": "A.2 LANGUAGE-VISION REPRESENTATION", "content": "The original fine-tuning loss LLIV = L1(Y1)+LL(V1, VL) is calculated on sampled sub-trajectory batch data {0,...,010,9}, from each task Ti, where s \u2208 [0, T\u2081 \u2013 1], s \u2264 k < Ti. They have the following forms:\n$\n\\begin{aligned}\nL_{I}(\\psi_{I}) &= \\frac{1}{B} \\sum_{i=1}^{B}\\left[-\\mathcal{S}\\left(\\psi_{I}\\left(o_{s}^{i}\\right), \\psi_{I}\\left(o_{k}^{i}\\right)\\right)\\right]+\\log \\frac{\\exp \\left[\\mathcal{S}\\left(\\psi_{I}\\left(o_{s}^{i}\\right), \\psi_{I}\\left(o_{k}^{i}\\right)\\right)\\right]}{\\sum_{j=1}^{B} \\exp \\left[\\mathcal{S}\\left(\\psi_{I}\\left(o_{k+1}^{i}\\right), \\psi_{I}\\left(o_{s^{\\prime}}^{i}\\right)\\right)\\right]}\\right],\n\\\\\nL_{L}\\left(\\psi_{I}, \\psi_{L}\\right) &= -\\frac{1}{B} \\sum_{i=1}^{B} \\log \\frac{\\exp \\left[(1-\\lambda) \\mathcal{S}\\left(\\psi_{I}\\left(o_{k}^{i}\\right), \\psi_{L}\\left(g^{i}\\right)\\right)\\right]}{\\sum_{b=1}^{B} \\exp \\left[\\mathcal{S}\\left(\\psi_{I}\\left(o_{k}^{i}\\right), \\psi_{L}\\left(g^{i}\\right)\\right)\\right]}\n\\end{aligned}\n$\n(2)"}, {"title": "A.3 Low-LEVEL POLICY", "content": "In this work, we use a low-level policy with a similar structure to diffusion policy (Chi et al., 2023). We show the architecture of this policy in Figure 11. We employ a spatial-temporal attention mech-anism. Specifically, the input contains the agent view observation history of-12:t \u2208 R12\u00d73\u00d7128\u00d7128 and the eye in hand observation history of\u221212:t \u2208 R12\u00d73\u00d7128\u00d7128 at timestep t, the proprioception state history St-12:t_\u2208 R12\u00d7123, the language tokens extracted from Meta Llama 3.1 8B (Dubey et al., 2024) g \u2208 RT\u00d74096, the predicted flow for both the agent view put+16 \u2208 R16\u00d7529\u00d72 and eye in hand view p:t+16 \u2208 R16\u00d7529\u00d72, and the predicted future videos for both the agent view O\u00bat:t+16 \u2208 R16\u00d73\u00d7128\u00d7128 and eye in hand view o\u00bat:t+16 \u2208 R16\u00d73\u00d7128\u00d7128. The low-level policies are a denoising model that will predict the gradient field of the action chunking VE(At), and after 500 denoising steps, we can get the future action sequences a \u2208 R16\u00d77 where 7 is the action size. We use the action in a receding horizon manner where we only execute 8 steps and we replan the future flow and videos and predict another 16 action steps iteratively."}]}