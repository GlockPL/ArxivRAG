{"title": "Crafting desirable climate trajectories with RL explored socio-environmental simulations", "authors": ["James Rudd-Jones", "Fiona Thendean", "Mar\u00eda P\u00e9rez-Ortiz"], "abstract": "Climate change poses an existential threat, necessitating effective climate policies to enact impactful change. Decisions in this domain are incredibly complex, involving conflicting entities and evidence. In the last decades, policymakers increasingly use simulations and computational methods to guide some of their decisions. Integrated Assessment Models (IAMs) are one of such methods, which combine social, economic, and environmental simulations to forecast potential policy effects. For example, the UN uses outputs of IAMs for their recent Intergovernmental Panel on Climate Change (IPCC) reports. Traditionally these have been solved using recursive equation solvers, but have several shortcomings, e.g. struggling at decision making under uncertainty. Recent preliminary work using Reinforcement Learning (RL) to replace the traditional solvers shows promising results in decision making in uncertain and noisy scenarios. We extend on this work by introducing multiple interacting RL agents as a preliminary analysis on modelling the complex interplay of socio-interactions between various stakeholders or nations that drives much of the current climate crisis. Our findings show that cooperative agents in this framework can consistently chart pathways towards more desirable futures in terms of reduced carbon emissions and improved economy. However, upon introducing competition between agents, for instance by using opposing reward functions, desirable climate futures are rarely reached. Modelling competition is key to increased realism in these simulations, as such we employ policy interpretation by visualising what states lead to more uncertain behaviour, to understand algorithm failure. Finally, we highlight the current limitations and avenues for further work to ensure future technology uptake for policy derivation.", "sections": [{"title": "Impact Statement", "content": "Deriving climate policy is a challenging problem, with an expansive solution space. Policymakers have turned to simulation based approaches in order to aid their decisions, however these traditionally have various limitations. Our work is a preliminary study on improving aspects of these simulation based approaches with multi-entity agent interactions. This allows for improved modelling of stakeholder/nation competition, cooperation, and communication that is the key driver for much of anthropogenic climate change."}, {"title": "1. Introduction", "content": "According to the 2022 Intergovernmental Panel on Climate Change (IPCC) report - \u201cHaving the right policies, infrastructure and technology in place to enable changes to our lifestyles and behaviour can result in a 40-70% reduction in greenhouse gas emissions by 2050", "economic and environmental positive future\" within the models framework. They focused on adapting agent initial states and reward functions to understand the impact these had on the exploration of the IAM, as well as test the agents under the injection of noise in the environment. This has guided our experiments to ensure a wide range of initialisations to understand the exploration of agents. Both Strnad et al., 2019 and our previous work in Wolf et al., 2023 use a singular agent, hence assuming a \\\"unified\\\" earth, in which there is a collectively shared goal. In this work we aim to move one step further and model inter-world interactions, that are the driver for much of anthropogenic climate change and must be understood for many policy decisions (Stone, 2008). Towards this aim we adapt the IAM accordingly, based on ABM extensions of IAMs (Giarola et al., 2022; W. Nordhaus, 2015; Zhang et al., 2022), in order to implement a multi-agent IAM with MARL.\nThe only work that has used MARL within the climate policy domain in the literature is that of Zhang et al., 2022, which created the RICE-N model used for the AI for Global Climate Cooperation Challenge\u00b9. Itself an extension of the Regional Integrated model of Climate and the Economy (RICE) model developed in W. D. Nordhaus, 2010 that models twelve global regions. Zhang et al., 2022 invited various domain experts to create and edit interaction and negotiation protocols to achieve the best Pareto Frontier of the socio-economic system variables in the environment. The RICE-N model combines a climate-economic IAM with trade and negotiation dynamics enabling high levels of interaction between countries/regions (a.k.a agents). Agents can adjust their savings rates, climate mitigation rates, as well as trade and negotiate with each other at each time step, leading to a large range of potential interactions between each other and the environment (Zhang et al., 2022). Their findings show the potential of MARL based applications to IAMs with a large call to action for further research on the topic. RICE-N is an extensive environment that we aim to use for future work, however we prioritise increased intepretability of the trained agent and as such focus on the multi-agent extension of the more simplistic environment as used in Strnad et al., 2019 and Wolf et al., 2023. This simplified environment enables a visual understanding and easier interpretation of the trained agent's interactions, which are key to analyse the use of MARL within IAMs.\nRL algorithms however, lack inherent explainability, raising concerns about their trustworthiness for informing real-world policy decisions. Using explainability methods, we can reinforce human confidence by providing insights into how decisions were made and visibility to vulnerabilities (Adadi & Berrada, 2018; Glanois et al., 2021; Lipton, 2018). The explainability methods explored in this work specifically target explaining model policy through a quantification technique, determining the states at which taking a certain action is crucial, critical in applications related to informing climate change policy.\nIn summary, we attempt to model whether agents prioritising economic or environmental gain can affect climate policy derivation. As well as simulate, within this framework, whether \u201cclimate positive\" futures are possible when agents conflict in their prioritisations. We have extended previous literature's single agent IAM to a multi-agent scenario in order to incorporate inter-nation behaviour. Utilising this technology, policies can be derived and enacted in reality, depending on the validity of our underlying IAM. For a single agent setting, one can fully implement the projected policies as they can have full agency over the singular agent in reality. However, moving to multiple agents if we want to follow a similar optimisation approach it assumes we can have control over all agents in reality. A heavy assumption in practice. Instead in this paper we focus on the setting of having control over one or a subset of the agents, but still model all agents learning collectively. This necessitates the need for decentralised training decentralised execution (DTDE) algorithms. We have arbitrarily assumed the learning algorithm and parameters behind each stylised agent, which will directly affect the outcome trajectories. Aiming to highlight the challenges with employing certain existing algorithms. However in future work, the other agents in the simulation (that we may not have agency over in reality) could be trained using imitation learning (Hussein et al., 2017) on historical data to represent in-silico versions of real world entities. MARL can then be used to train an agent to act as a best response to these imitation pre-trained agents within a multiple agent IAM, providing us with a range of possible future trajectories. Again dependent on the validity not only of the IAM, but also the agent representations of real world entities. As with any forecasting tool, long range trajectories lead to large accumulations of error. As an alternative the algorithm can be further trained as more data about other agents is received.\"\n    },\n    {\n      \"title\": \"2. Materials and methods\",\n      \"content\": \"In this section, we introduce the core themes required for our contribution: the IAM environment, the MARL algorithm and requirements for its application, and the interpretability framework we have used in order to improve insight.\"\n    },\n    {\n      \"title\": \"2.1. The IAM Environment\",\n      \"content\": \"The AYS environment, created by Kittel et al., 2021, is a low complexity IAM, made up of a social, economic, and environmental variable. These three variables each relate to an ordinary differential equation (ODE) defining the system:\n$\\frac{dA}{dt} = E - \\frac{A}{T_A}$ (2.1)\n$\\frac{dy}{dt} = \\beta Y - \\Theta A Y$ (2.2)\n$\\frac{ds}{dt} = R - \\frac{S}{T_S}$ (2.3)\nwhere A is the excess atmospheric carbon (GtC), Y the economic output ($yr^{-1}$), and S the renewable knowledge stock (GJ). Each variable is inextricably linked with each other, creating a dynamic cycle. In words:\n\u2022 A is proportional to emissions produced from the use of fossil fuels, minus a natural carbon decay out of the atmosphere.\n\u2022 Y naturally grows by 3% each time period however, is reduced by a economic climate damage function where increasing A increases the reduction in Y.\n\u2022 S is proportional to the amount of renewable energy produced, however, has a natural knowledge decay rate over time.\nThe following equations are required for deeper analysis of the AYS ODEs, with further numerical parameters listed in Appendix A.\n$E = \\frac{\\Gamma U}{\\phi}$ (2.4)\n$\\Gamma = \\frac{1}{1 + (\\frac{S}{Y})^p}$ (2.5)\n$U = \\frac{Y}{\\epsilon}$ (2.6)\n$R = (1 - \\Gamma)U$ (2.7)\nWhilst A and Y are easily quantifiable with real life implications, S is harder to define. Generally social factors require greater levels of detail than economic or environmental attributes. For instance, in Zhang et al., 2022 they incorporate many layers of complex socio-economic equations in order to have a functioning model with quantifiable social impact. In the AYS model this is simplified down to a single equation enabling a much reduced state space towards lower computational requirements and more interpretable understanding of agent behaviour.\nThe AYS model has been specifically tuned so that an agent tends towards one of two points:\n\u2022 Green fixed point - $\\begin{pmatrix}0\\\\ \\infty\\end{pmatrix}$\n\u2022 Black fixed point - $\\begin{pmatrix} \\frac{\\phi\\beta \\epsilon}{\\Theta T_A} \\\\\\ 350 GtC\\\\\\ 4.84 x 10^{13} \\$yr^{-1} \\\\ 0 GJ \\end{pmatrix}$ (2.8)\nThe green fixed point denotes a \u201csustainable\\\" future, one where there is no atmospheric carbon but limitless capital and renewable knowledge. The black fixed point however, denotes a stagnant economy solely dependant on fossil fuels. This is a future we ideally want to avoid. Included with these \\\"drain\\\" points are Planetary Boundaries (PB). The AYS model incorporates one PB set in the reports from Steffen et al., 2015 and Rockstr\u00f6m et al., 2009 of a maximum excess atmospheric carbon at $PBA = 345 GtC$, with a social foundation for prosperity from Dearing et al., 2014 defining a minimum yearly economic output at $PBy = 4 \u00d7 10^{13} $yr-1 (Kittel et al., 2021). For brevity throughout this paper we will make reference to these boundaries as the two PBs, although by definition our economic output boundary is in fact a social goal, not a planetary boundary.\nTo mimic the current state of the Earth within this model, the starting point is defined as $st=0 = {240 GtC, 7\u00d710^{13} yr^{-1}, 5\u00d710^{11} GJ}$. Not only is this starting location very close to the PBs creating a challenging control problem, but also from this location the agent will tend towards the black fixed point if no actions are taken. Figure 1 highlights the AYS environment with black and green fixed points, and the two translucent grey planes indicating the two PBs. Strnad et al., 2019 and Wolf et al., 2023 incorporate noise into the starting position over episodes to improve training, however, noise is omitted from the S state variable as this dramatically reduces the agents' ability to learn. Kittel et al., 2021 and subsequent work normalised the environment between 0 and 1 to prevent numerical explosions.\nWe carry this through, normalising the states and then incorporating noise, setting the starting state as:\n$St=0 = \\begin{pmatrix}0.5+U(-0.05, 0.05) \\\\ 0.5+U(-0.05, 0.05)\\\\ 0.5\\end{pmatrix}$ (2.9)\nwhere U is the uniform distribution.\nAt its current state the model will tend towards the black fixed point. To avoid this an agent is able to undertake four actions, described in Kittel et al., 2021:\n0. Default - Default parameters are used and the agent follows the flow lines without any resistance.\n1. Degrowth - Economic growth parameter \u03b2 is halved, fluctuating between 3% and 1.5% growth.\n2. Energy Transition - Break-even renewable knowledge \u03c3 is reduced by 31.3%, equivalent to halving the renewable to fossil fuel energy cost ratio.\n3. Both - The two non default actions are combined within one timestep.\nFor each integration timestep of the environment, an agent is able to select one of these four options, mimicking an action taken every year (Kittel et al., 2021).\nThe AYS model in its current format depends on only one agent driving the simulation. We propose an extension enabling simple interactions between multiple agents. Global variables are denoted with no subscript, however, local (to each agent) variables are denoted with a subscript. There is now only one global variable - the excess atmospheric carbon A. Figure 2 visualises the extended multi-agent environment differential equation cycle.\"\n    },\n    {\n      \"title\": \"2.2. MARL Algorithm\",\n      \"content\": \"Focusing on DTDE algorithms as stated in the introduction, the Independent Proximal Policy Optimisation (IPPO) algorithm acts as an effective starting point (Schulman et al., 2017; Yu et al., 2022). This relates to n (number of agent) versions of PPO based agents within an environment that do not share parameters between them, so are fully independent. Each (0 to n) PPO agent (Schulman et al., 2017) has no awareness of other agents in the system, and since we are in a POSG, only has access to its observations of the environment. The state and observation space is a vector of values \u2208 [0, 1": "relating to the three AYS variables. A is global, but Y and S are independent to each agent leading to the partially observable nature. The action space contains values from the discrete set {0,1,2,3} relating to the actions in List 2.1. Our previous work in Wolf et al., 2023 found PPO to achieve impressive results and thus further posits its use within our experiments. Rewards are derived from the \"Planetary Boundary\" (PB) reward function, maximising the euclidean distance between the agent and the two PBs and a lower bound of 0 on the S parameter. If a boundary is crossed the reward equals 0:\n$R_{PB} = ||o - o_{PB}||^2$ (2.10)\nwhere o relates to an individual agent's observations of the environment. As an agent aims to maximise its reward, it looks to achieve a point as far away from the PBs as possible, thus tending towards the green fixed point. Using the PB rather than the limits of the simulation incentivises the agent to avoid the PBs. For further experiments we look at competitive agents and thus need two new reward functions:\n$R_{maxA} = o_A$ (2.11)\n$R_{maxY} = o_Y - P_{BY}$, (2.12)\nwhere $o_A$ is the agent observation of the A variable, $o_Y$ is the agent observation of the Y variable, and $P_{BY}$ is the planetary boundary (social goal) for the Y variable. The former directly rewards an agent on the A variable, the excess atmospheric carbon (GtC), relating to an entity that prioritises environmental degradation. The latter at maximising the agent's distance to the Y planetary boundary, the economic output ($yr^{-1}$) social goal, which can be seen as an entity that prioritises economic gain over environmental impact."}, {"title": "2.3. Critical States", "content": "Explainability and interpretability in RL is an open question, with most methods focusing on explaining the neural networks that are used as functional approximators in deep RL (Heuillet et al., 2021). There are very few methods that are specific to RL algorithms, and even fewer that are usable rather than purely conceptual (Heuillet et al., 2021). Critical states, based on Huang et al., 2018, serves as a form of explainability specific to RL for model policy. This work elaborates that there are a set of few specific states (critical states) in an agent's trajectory in which it greatly matters which action the agent takes (Huang et al., 2018). In theory, certain states lead to a large difference between policy outputs over the set of actions. Generally, one action would lead to a much larger policy value than the rest, as the agent is more sure this is the only action option in that state. We proceed with this method of explainability, as it is crucial to know which locations in a trajectory correspond to the most vital actions for actionable climate policies. In more concrete terms, the set of critical states $C_r$ are identified as those with a high logit difference, calculated from the outputs of the neural network representation of the agent's policy, mathematically formalised as:\n$C_r = {s | max \\pi_\\theta(s,a) - \\frac{1}{|A|} \\Sigma _a\\pi_\\theta(s,a) > t}$ (2.13)\nwhere $\\pi_\\theta(s, a)$ represents the logits of the policy distribution (as output by the actor network), t a critical state threshold, and A is the set of potential actions. A requirement is that entropy regularisation is used in the policy objective \u2013 without it, policies can collapse prematurely to almost deterministic states, signifying that almost all states are critical (Huang et al., 2018). We have included entropy regularisation into our implementation of PPO, ensuring the policy acts purposefully in critical states and more randomly in others (Huang et al., 2018). We expand on the idea of critical states by plotting the logit differences across 1000 sampled trajectories (post-training) to analyse how \u201ccritical\" each state is, rather than defining a critical state threshold. The value of this threshold is arbitrary and we prefer to highlight the full range over states, although one could consider states with a logit difference over 0.5 as the critical states. In particular, we ask: Are there locations in the trajectories that the policy finds more critical than others, and are these critical areas distributed in a way that is interpretable with regard to the agent's behaviour? To some extent, this can be loosely interpreted as policy uncertainty, as critical states are those in which the policy has a higher logit difference and is thus more certain of the correct action to take. However, we try to avoid using this term, as this method does not provide an exact uncertainty quantification of the policy."}, {"title": "3. Experimental results", "content": "Our overarching ambition is towards applicable and deployable systems that guide climate policy. Whilst this is an expansive open question that can't be fully answered in this paper, we begin by experimenting on the simplest cases and slowly increase complexity. This lines up the following research questions that we tackle within this work:\n\u2022 RQ1 - Assuming agents are homogeneous (having the same starting state and thus the same initial IAM variables), can they achieve an \u201ceconomic and environmental positive future\" when acting towards a shared goal through having the same reward functions (a.k.a interacting cooperatively)?\n\u2022 RQ2 - Relaxing agent homogeneity, are cooperative agents still able to achieve a successful future at a similar rate?\n\u2022 RQ3 - Finally, does introducing competition between agents, for example by having reward functions that oppose each other to discourage cooperation, significantly hinder a strategic interaction convergence on reaching the green fixed point?\nTowards RQ1 our first experiment incorporates increasing numbers of homogeneous cooperative agents into the AYS environment. For RQ2 we repeat the same experiments as RQ1 but allow agents to start in varying locations to each other, initialising an agent's state at different AYS variables, thus mimicking the variability seen between entities/nations in reality. Furthering agent heterogeneity we also vary the agent independent values for climate damages $\\xi_i$ mimicking agents not all experiencing the same damaging effects as the climate degrades. Finally for RQ3 we reduce the number of agents in our environment to two to compare varying reward functions and their effects on an agent's ability to reach the green fixed point. Then extend this to three agents highlighting that the trend continues as agent numbers increase. By keeping the number of agents low as well as incorporating the critical states visualisation we show greater insight into the agent's action decisions.\nA key theme within our research questions is the ability for an agent to reach the green fixed point. We define the win rate as the percentage of times that the simulation (as a whole) reaches the green fixed point over a set number of episodes. However, the definition of success within this environment is not a Pareto Frontier and instead stakes claims on what is negative or positive, as such we focus in on the environmental positives. For clarity an episode is the collection of timesteps between an initial state and a terminal state, be that due to reaching the green fixed point, breaching a planetary boundary, or reaching the fixed maximum number of steps per episode. We run all experiments for six seeds and plot the average of these seeds with translucent standard error bounds."}, {"title": "3.1. Experiment 1 - Homogeneous Agents", "content": "We begin by instantiating homogeneous agents, i.e. agents that have the same initial AYS variables. This relates to all agents starting in the same location. Agents here have the same objective towards a common goal, each following the $R_{PB}$ reward function. The greater the distance to the PBs the greater the reward. Agents are not predefined with a top-down restraint that they must cooperate, instead by using a reward with a shared goal we show the emergence of cooperation.\nIn Figure 3 for a single agent case IPPO (which reduces to PPO for one agent) quickly learns a consistent policy, as it avoids any complexity from the non-stationarity of the transition function caused by other agents. Increasing the number of agents (ranging from 2 to 8 agents together), increases training time taken until a consistent policy is reached which can be attributed to the increasing complexity stemming from the non-stationarity and interactions between agents.\nFigure 4 shows (with only two seeds leading to a larger variance during the middle of training) that with enough time steps a similar win rate is achieved between agents. We have not run the experiments in Figure 3 to a stable state for large numbers of agents due to the computational resources required, and instead focus on a smaller total of agents (and for fewer random seeds) for greater insight. For a singular agent, the win rate after $1.2 \u00d7 10^8$ steps is 87.740% \u00b1 8.225. For six and eight agents after $3 \u00d7 10^8$ steps the win rates are 90.935 \u00b1 0.010 and 90.143 \u00b1 0.035 respectively. The lower standard deviations here stem from the policy convergence gained from much longer time steps. Answering RQ1 it is clear that agents are able to reach the green fixed point consistently, independently of the number of agents. Cooperation thus emerges between agents, with the shared reward function of a common goal being the only predefined signal towards cooperating."}, {"title": "3.2. Experiment 2 - Heterogeneous Agents", "content": "Increasing the applicability we now look at heterogeneous, but still cooperative, agents. Heterogeneity is very important in the climate domain, especially when dealing with anthropogenic factors as it can apply to: spatial variability, temporal variability, and variability in socio-economic impacts, among others (Madani, 2013). The various sources of heterogeneity between agents in the AYS MARL environment are: AYS variables, AYS parameters, Reward Functions, MARL algorithm. Varying the AYS variables and parameters can be seen as representing different traits of a representative agent, for example a larger initial Y may indicate an economically wealthy entity. Similarly changing for the economic growth parameter $\\beta$ again represents an entity with increased economic function. There are limitless combinations one could make from these for experimentation. Values could also be based on real world data to provide an in silico entity representation, or verify results on a well known case study. Reward functions represent what an entity may \"value\" or be looking to optimise for, changing these between agents can lead to conflicting behaviour as these may directly oppose one another. Finally we can represent each agent with different MARL algorithms since we are constrained to the use of DTDE algorithms which have no overarching centralised controller. For example we could represent certain agents with less capable algorithms to understand the effect on the resulting equilibrium. We do not adjust the MARL algorithm, using PPO for all, as we want to understand some of the limitations of RL specific algorithms being applied to MARL in this domain. Instead we vary the AYS variables and parameters, with our subsequent experiments adjusting the reward function. Agents can start at any location within the predefined uniform distribution of starting points. A new starting point is sampled at each episode.\nFigure 5 shows that scaling up agents here has a larger impact on the win rate due to the more complex heterogeneous nature of the agents. Still again with enough timesteps agents reach a consistent policy, as seen in Figure 6. Win rates for six and eight agents after $6 \u00d7 10^8$ steps are 93.007 \u00b1 0.054 and 94.121 \u00b1 0.067 respectively. Closely matching the results found in Experiment 1.\nMultiple heterogeneous agents acting towards the same goal have similar performance to a singular agent, although require a much longer set of episodes for convergence due to the increased complexity. Here we prove that RQ2 is possible, without any loss of performance.\nFurthering these experiments we also look at heterogeneity in the AYS parameters, specifically scaling the agent independent climate damage $\\xi_i$. We carry over the same heterogeneous starting point variation as in the previous experiment and only focus on two agents together. In reality negative environmental effects such as extreme weather scenarios or rising water levels that impact economic output may affect certain regions more than others (Dellink et al., 2019). In the worst scenarios the biggest polluters may rarely see the negative climate effects, which are instead fully experienced at other geographical locations. To naively model this we scale the climate damage parameter $\\xi_i$ between 0 and 1, the former an extreme case where the economy is not affected by parameter A, and the latter the usual AYS ODE dynamics.\nFigure 7 indicate that as an agent is impacted less by climate damages, i.e. as $\\xi_i$ tends towards 0, it gains more independent return (total individual reward over an episode) than the other agent that has $\\xi_i = 1$. Importantly though it comes at the cost of globally reaching the green fixed point, even with cooperative reward functions, as seen in Figure 8. As $\\xi_i$ reduces in the AYS ODE interaction Figure 2, Y becomes less affected by the value of A which has knock on effects in further increasing an agent's own Emissions E. However an agent therefore also receives less signal in the observations about how the A variable affects the Y variable, and how this all relates to its own actions and reward function. Therefore these agents seem to prefer maximising Y as they are unaware of the impact this has on A. In Figure 9 one can see how the trajectories evolve from a two agent scenario both following $R_{PB}$ and having $\\xi_i$ of 1, to very different pathways when $\\xi_2$ is 0.25 for Agent 2. Interestingly the trajectories for Agent 2 in Figure 9d are very similar to those of an agent following the $R_{maxY}$ reward function, with example trajectories found in Figure 13c and 13d, even though the agent is still following $R_{PB}$. Without staking too many claims in reality, an agent that has minimal understanding of how the actions it takes impact the environmental variable on a global scale, will be unable to enact the desired actions to reach the \"climate positive\" future."}, {"title": "3.3. Experiment 3 - Competitive Agents", "content": "We have shown that agents are able to consistently reach the green fixed point when working together. However, how will they fare when dealing with more competitive agents, e.g. ones that prioritise capital over detrimental environmental effects? Or in an extreme (yet slightly unrealistic) case, agents that only care to maximise the excess carbon in the atmosphere. For this, we use the two other reward functions: $R_{maxY}$ and $R_{maxA}$. The former rewarding an agent for maximising the distance to the Y planetary boundary, the economic output ($yr^{-1}$) social goal. The latter rewarding an agent for maximising the A variable, the excess atmospheric carbon (GtC). We also assume that agents start in heterogeneous locations as our experiments have shown this does not negatively impact the win rate. The choice of $R_{maxA}$ may be a peculiar one, but we have included the experiments to show more adversarial behaviour than can be expected with $R_{maxY}$. The definition of $R_{PB}$ in some ways includes maximising Y, or at least ensuring that the agent avoids the Y social goal boundary, and as such $R_{maxY}$ can be seen as a mixed motivation reward function. Whereas $R_{maxA}$ greatly opposes the aims of $R_{PB}$, leaning towards more competition. This choice helps us understand the performance of the IPPO algorithm in these more challenging competitive scenarios, which will arise in future applications.\nAs seen in previous experiments and in Figure 10, two agents following $R_{PB}$ consistently reach the green fixed point. Interestingly agents following $R_{maxY}$ are also able to reach the green fixed point, although at a much reduced capacity. This is due to the AYS environment, wherein the Y variable is directly driven by the atmospheric carbon A, greatly incentivising an agent to reduce A in order to maximise Y.\nHowever, as we would unfortunately expect, an agent that only aims to maximise its carbon output (following $R_{maxA}$) overrules any potential climate positive actions from the $R_{PB}$ following agent. This clearly highlights the need for cooperation, or at the least, ways to shape \"opponents\" actions to more closely align to the desired behaviour.\nIn Figure 11 a similar trend carries over with an increasing number of agents. Agents that work together on a shared goal succeed but agents that have different incentives fail, although combinations of a majority of $R_{PB}$ with $R_{maxY}$ have the potential to succeed but at a much reduced rate. Our results confirm RQ3 - increasing competition reduces the ability for agents to reach the green fixed point. Highlighting the need for the use of algorithms with increased opponent awareness over IPPO to improve performance.\nIn RL defining the reward can be tricky, as agents can \"hack\" these values and act in non-predictable ways (Laidlaw et al., 2024; Skalse et al., 2022). Due to the possibility for early termination from reaching goal states or boundary conditions before the max number of time steps, if agents aren't correctly given potential future rewards they can be incentivised to take \"longer\" in the environment as there are no temporal negatives. This was clear in some competitive environments where without the notion of discounted future rewards, agents following the $R_{PB}$ would receive more reward if they never reached the green fixed point but slowed down the impact of an agent following $R_{maxA}$. Therefore we use discounted rewards within this environment. Correctly defining rewards is relatively easy here but a key question for future applications is how to quantify rewards."}, {}]}