{"title": "Learning to Coordinate without Communication under Incomplete Information", "authors": ["Shenghui Chen", "Shufang Zhu", "Giuseppe De Giacomo", "Ufuk Topcu"], "abstract": "Achieving seamless coordination in cooperative games is a crucial challenge in artificial intelligence, particularly when players operate under incomplete information. A common strategy to mitigate this information asymmetry involves leveraging explicit communication. However, direct communication is not always feasible due to factors such as transmission loss. We explore how effective coordination can be achieved without verbal communication, relying solely on observing each other's actions. We demonstrate how an autonomous agent can learn to cooperate by interpreting its partner's actions, which are used to hint at its intents. Our approach involves developing an agent strategy by constructing deterministic finite automata for each possible action and integrating them into a non-Markovian finite-state transducer. This transducer represents a non-deterministic strategy for the agent that suggests actions to assist its partner during gameplay. Experimental results in a testbed called Gnomes at Night show that the learned no-communication coordination strategy achieves significantly higher success rates and requires fewer steps to complete the game compared to uncoordinated scenarios, performing almost as well as an oracle baseline with direct communication.", "sections": [{"title": "1 Introduction", "content": "In artificial intelligence, autonomous agents either compete or cooperate with one another, reflecting the diverse nature of real-world interactions. Games are often employed as models to study these interactions, offering structured environments where the dynamics of agent behavior can be observed and analyzed. Much of the research has focused on adversarial games, where agents act as protagonists striving to accomplish specific tasks despite the adversarial behaviors of the environment (Cimatti, Roveri, and Traverso 1998; Cimatti et al. 2003; Geffner and Bonet 2013). Conversely, in games requiring cooperation (Dafoe et al. 2021), multiple agents must collaborate to achieve a shared goal. The challenge here lies in how agents effectively understand and anticipate each other's intentions to coordinate seamlessly. An example of this is the card game Hanabi (Bard et al. 2020), where players must communicate private information within strict constraints on what they can convey and when. These cooperative games become particularly challenging when players have incomplete or different information. This information asymmetry, often from partial observations or a limited understanding of game dynamics, can lead to misaligned or suboptimal actions.\nConsider the game Gnomes at NightTM by Peaceable Kingdom shown in Figure 1 as a running example. In this cooperative game under incomplete information, two players must collaborate to move two magnetically connected gnome pieces through a maze. One player, the seeker, aims to reach a treasure in a maze that lacks a direct path to it. The other player, the helper, can open connections in the maze without seeing the seeker's maze or knowing the treasure's location. Both players must effectively coordinate their moves to collect the treasure.\nThis game mirrors human-AI coordination, with the human as the seeker and the autonomous agent as the helper, strategizing and cooperating to achieve some common objective. Agents with such assistive abilities have the potential to enhance collaboration with humans in various settings, from virtual environments like action video games (Carroll et al. 2019) and strategy board games (Bard et al. 2020) to physical applications like assistive wheelchairs (Goil, Derry, and Argall 2013).\nSuch cooperative game dynamics with incomplete information typically require effective communication during decision-making, as the helper must understand the seeker's guidance and make corresponding moves for successful coordination. Direct communication offers a solution by enabling the exchange of relevant information be-"}, {"title": "2 Preliminaries", "content": "We utilize a shared-control game under incomplete information (Chen, Fried, and Topcu 2024) to model the interactions in scenarios described in the introduction. In this game, two players, the seeker and the helper, collectively control a single token to achieve an objective under incomplete information. Below we list the original definition.\nDefinition 1 (Shared-control game). A shared-control game where seeker S and helper H collectively control a token in alternating turns is a tuple $\\Gamma = (S, S_{init}, S_{final}, A_S, A_H, T_S, T_H)$, where S is a finite set of states, $S_{init} \\in S$ is an initial state, $S_{final} \\in S$ is a final state, $A^i$ is a finite set of actions available to each player $i \\in {S, H}$, and $T^i : S \\times A^i \\rightarrow S$ is a private deterministic transition function for each player $i \\in {S, H}$. In this game, players have incomplete information about the game dynamics in that each player i only knows its own transition function $T^i$ but not that of the other player. The final state $s_{final}$ is visible to the seeker S only. We augment $\\Gamma$ with a common reward function $R : S \\times \\bigcup_{i \\in {S,H}} A^i \\rightarrow \\mathbb{R}$ that assigns equal real-valued reward R(s, a) to both players following the execution of action a by either player in state s. This function can capture cooperative objectives, like minimizing the number of steps to reach the final state."}, {"title": "2.2 A* Shortest Path", "content": "The A* algorithm is a widely used graph traversal algorithm that finds the shortest path in a graph by combining the ac- tual distance from a start node to a goal. It selects the path that minimizes $f(n) = g(n) + h(n)$, where g(n) is the cost from the start node to n, and h(n) is the heuristic estimate from n to the goal. A* guarantees optimal and complete so- lutions as long as the heuristic does not overestimate the ac- tual cost (Hart, Nilsson, and Raphael 1968). The heuristic function is problem-specific and can incorporate various cri- teria beyond distance, such as time, energy consumption, or safety, making A* adaptable to many applications."}, {"title": "2.3 Deterministic Finite-Word Automata", "content": "A deterministic finite-word automaton (DFA) is represented as a tuple $D = (2^{Prop}, Q, q_0, \\delta, F)$, where Prop is the al-"}, {"title": "2.4 Angluin's L* algorithm", "content": "Automata learning, or model learning, aims to construct black-box state models by providing inputs and observ- ing outputs. In this paper, we utilize Angluin's L* algorithm (Angluin 1987) to learn a DFA, which works by al- lowing a learner to ask two types of queries to a teacher, who already has the DFA to be learned in mind.\n\u2022 Membership queries: The learner can try different traces by asking \"Can trace \u03c3 be accepted by the DFA?\" to have an initial idea of what the DFA looks like.\n\u2022 Equivalence queries: Once the learner has a guess (a hy- pothesis) of the DFA, it asks, \u201cIs my guess correct?\u201d If the guess is wrong, the teacher provides a counterexam- ple that the learner can use to refine the guess.\nThe L* algorithm can learn DFAs by asking a polynomial number of membership and equivalence queries. Despite the development of faster algorithms, the most efficient learning methods today still follow Angluin's minimally adequate teacher approach."}, {"title": "3 Problem Definition", "content": "In this paper, we explore the challenge of achieving effective coordination between two players without communication in shared-control games. The original definition of the games in Definition 1 requires players to alternate control after each action. However, our approach extends this framework by allowing each player to perform a sequence of actions before passing control to the other player.\nMulti-Step Game Dynamics. The shared-control game $\\Gamma$ begins with the token in the initial state $S_{init}$. The seeker S and the helper H alternate turns moving the token, but not necessarily after every action. Only the seeker knows the goal position and may reach it without help, thus we let the seeker initiate the game. In each player i's turn (i \u2208 {S, H}), the game undergoes a multi-step transition based on the sequence of actions $a^{t+1} = a^t_1, ..., a^t_n$ chosen from its action space $A^i$. The transition is defined by:\n$T_i(s_t, a^{t+1}) = T_i (s_t, [a^t_1, ..., a^t_n])$\n$= T^i(...T^i(T^i(s_t, a^t_1), a^t_2), ..., a^t_n)$,\nwhere t denotes the turn the current state is in. Once a player completes their sequence, control passes to the other player: If S just played, H takes the next turn, and vice versa. The game continues with players alternating turns until it reaches a final state $S_{final}$."}, {"title": "Description.", "content": "We study the problem of synthesizing a cooperative strategy for the helper in the shared-control game, where communication between players is not allowed. Our objective is to develop a strategy for the helper that effectively responds to the behavior of the seeker.\nProblem 1. In a shared-control game $\\Gamma = (S, S_{init}, S_{final}, A_S, A_H, T_S, T_H)$ with a common reward function R as introduced in Definition 1, the seeker's behav- ior is modeled as a policy $\\pi^S : S \\times A_H \\rightarrow (A_S)^+$. We aim to compute the policy for the helper $\\pi^H : S \\times (A_S)^+ \\rightarrow A_H$ that maximizes the cumulative total reward while adhering to the game transitions:\n$\\max_{\\pi_H} \\mathbb{E} \\sum_{t=0}^T R(s_t, a_t)$          (1a)\ns.t. $s_0 = S_{init}$, $a_0= [] $ (1b)\n$\\begin{cases}a_{t+1} = \\pi^S(s_t, a^t_H) & \\text{ on S's turn,} \\\\a_{t+1} = \\pi^H(s_t, a^{t+1}) & \\text{ on H's turn,}\\end{cases}$ (1c)\n$s_{t+1} = T^i(s_t, a^{t+1})$, on player i's turn, (1d)\n$s_k = S_{final}$, for $0 \\leq k \\leq T$ (1e)\nHere, the subscript t denotes the turn number, not the step count. The game transition notation might appear redundant when $a^{t+1} = [a^{t+1}_i]$ for player i = H."}, {"title": "4 Solution Technique", "content": "The key challenge in constructing a strategy for the helper to effectively coordinate with the seeker lies in recognizing the seeker's behavior to determine the action they expect the helper to take. Since communication is disallowed, both players can only observe each other's action sequences. The seeker expresses its intentions solely through these sequences, and the helper must reason based on them to determine its actions. We propose an automata-learning-based approach that constructs a DFA for each helper action, designed to recognize the seeker's behaviors that indicate an expectation for the helper to perform that action. Finally, we combine these DFAs into a non-Markovian finite-state transducer representing a non-deterministic strategy for the helper that suggests actions to assist the seeker."}, {"title": "4.1 Learning Intent DFAs", "content": "Note that the seeker pre-determines a policy $\\pi^S$ to express its intentions through action sequences while moving in a game. The helper must learn to strategically perform the actions expected by the seeker when the seeker cannot proceed. To develop a corresponding strategy $\\pi^H$ for the helper, we introduce an automata-learning-based technique. The key insight is that when the seeker doesn't need assistance, it will naturally follow the shortest path. In this case, if the action sequence taken deviates from the shortest path, the extra actions taken are interpreted as intent information. Additionally, there should be consistent patterns in this intent information when the seeker expresses the same intention. To capture this consistency, we use deterministic finite-word"}, {"title": "4.2 Strategy Construction", "content": "It is worth noting that every intent DFA $D^a = {2^{A_S^*}, Q_a, q^a_0, \\delta_a, F_a}$ in D are defined only based on the seeker's actions. Consequently, as long as the seeker utilizes the same policy $\\pi^S$ to express its intentions, we can apply these DFAs D across various games that share the same action space of both players. Given a game $\\Gamma = (S, S_{init}, S_{final}, A_S, A_H, T_S, T_H)$ and the learned intent DFAs D, we define a non-Markovian strategy generator $\\mathcal{T}$ to obtain a strategy $\\pi^H$ for the helper. This strategy generator avoids the Cartesian product of the intent DFAs and returns a non-deterministic strategy that suggests on-the-fly helper's actions to assist the seeker. The strategy generator $\\mathcal{T} = (S, S_{init}, A_S, A_H, T_S, T_H, \\varrho, \\tau)$ is constructed as follows:\n\u2022 $S, S_{init}, A_S, A_H, T_S, T_H$ are the same as in $\\Gamma$;\n\u2022 $\\varrho: S \\times a^{t+1} \\rightarrow 2^S$ is the transition function, where $a^{t+1} = a^t_1, ..., a^t_n$ is the action sequence performed by the seeker in its preceding turn, such that\n$\\varrho(s, a^{t+1}) = {s' | s' = T_H(s, a_H) \\text{ and } a_H \\in \\tau(s, a^{t+1})}$;\n\u2022 $\\tau: S \\times a^{t+1} \\rightarrow 2^{A_H}$ is the output function such that $\\tau(s, a^{t+1}) = NCC(s, a^{t+1}, A_H, T_H, D)$, see Algorithm 1.\nThe transducer $\\mathcal{T}$ represents a non deterministic strategy for the helper, suggesting all possible actions to assist the seeker in the game. This transducer is non-Markovian, because each output produced by $\\mathcal{T}$ is not determined solely by the current state and the previous action of the seeker. Instead, it requires analyzing the entire action sequence produced by the seeker during its preceding turn. Specifically in Algorithm 1, every output is generated by running each informative segment abstracted from the seeker's action sequence (line 2) through every intent DFA and evaluating the acceptance results (lines 3-7). The helper then filters out ac- tions that are invalid according to its own transition function (line 8). Consequently, output generation takes linear time. Additionally, we never conduct a Cartesian product of the intent DFAs. Instead, we run each DFA on every segment in parallel to determine the intended action. Consequently, the"}, {"title": "5 Experiment Design", "content": "We outline the seeker's behavior model in the Gnomes at Night testbed, describe the setup for evaluating the proposed solution technique, and state the experimental hypotheses."}, {"title": "5.1 Seeker Implementation", "content": "The seeker uses Algorithm 2, a modified A* algorithm to generate a path from its current position to the goal, minimizing wall violations and taking into account walls that it believes are present for its partner as hard constraints. These constraints are strictly enforced during re-planning. The algorithm produces an action sequence marked with violation points, and the seeker follows the sequence until the next wall violation. The seeker then inserts intent-expressing segments to indicate the next action it desires from the helper to cross the wall. If the seeker notices the helper's actions do not match its intended actions, it assumes an unknown wall is blocking the helper and adds this wall to its memory as a new hard constraint.\nAlgorithm 2 operates similarly to the standard A* algorithm, using a priority queue H to track which paths to explore next. The algorithm starts by adding the current state $s_0$ to the queue with an initial priority of 0 (line 1). Two dic- tionaries, cost and violations, are initialized to store the minimum cost and violations needed to reach each state from the start (line 2). The algorithm then enters a loop, selecting the path with the lowest combined cost and violations from H (line 4). If the current state is the final state $S_{final}$, the algorithm returns the action sequence taken (lines 5-6). For each possible action, the algorithm checks if the move stays within bounds and does not violate any hard constraints C stored in memory. For valid actions, it calculates the new path cost c'. If this new path to state s' is either cheaper in cost or with fewer violations than previously known paths to that state, the algorithm updates cost and violations. It then calculates a new priority f for the path, which in- cludes the new cost c', the number of violations v' multiplied by a violation cost $c_v$, and the Manhattan distance from the current state to the final state (line 15). The updated action sequence, along with its priority, is added to H for future exploration (line 16). This process continues until the algo- rithm either finds a valid path to the final state or exhausts all possibilities. If no valid path exists under the given con- straints, the algorithm returns an empty list, indicating that the goal is unreachable."}, {"title": "5.2 Evaluation Details", "content": "We evaluate the effectiveness of the proposed solution technique in the Gnomes at Night testbed. In this two-player maze game, each maze configuration is defined by a specific maze layout and a unique treasure position. To test for"}, {"title": "6 Experimental Results and Analysis", "content": "We present the experiment results for success rates and steps taken in Figure 2, and the impact on seeker memory in Table 1. These results allow us to address the three hypotheses.\nRegarding H1. The left plot in Figure 2 compares the average success rates of the three coordination types across all trials and maze configurations for both sizes. The plot shows that the proposed NCC significantly outperforms NC, with success rate increases of 61.54% in 9 \u00d7 9 mazes and 72.84% in 12 \u00d7 12 mazes, highlighting the necessity of coordination in this game. DCC, represented by the rightmost bar, acts as an oracle baseline with nearly perfect success rates for both maze sizes. NCC closely matches this baseline, with only a 3.92% difference in 9 \u00d7 9 mazes and a 7.48% difference in 12 \u00d7 12 mazes. A Mann-Whitney U test confirms a statistically significant difference between the success rates of NC and NCC for both the 9 \u00d7 9 maze (U = 444.5, p < 0.001) and the 12 x 12 maze (U = 209.0, p < 0.001), suggesting that NCC is superior to NC. Additionally, the success rates between NCC and DCC are not significantly different for the 9 \u00d7 9 maze (U = 1201.0, p = 0.477) or the 12 \u00d7 12 maze (U = 1152.0, p = 0.102), indicating that NCC's performance is comparable to DCC. These results not only support H1 but surpass our initial expectations.\nRegarding H2. We compare the steps taken until termination for the three coordination types. The right plot in Figure 2 shows that NCC takes significantly fewer steps on average than NC in both the 9 \u00d7 9 maze (U = 1915.0, p < 0.001) and the 12\u00d712 maze (U = 2221.0, p < 0.001). However, there is a significant difference in the mean steps taken between NCC and DCC in both the 9\u00d79 maze (U = 2203.5, p < 0.001) and the 12 \u00d7 12 maze (U = 2284.0, p < 0.001). This difference is expected, as NCC requires more steps to effectively express its intentions through its trajectory. These results support H2.\nRegarding H3. Table 1 compares the impact of coordination on constraint memorization and error rates in both maze sizes. In the 9 \u00d7 9 maze, NCC reduces the number of"}, {"title": "7 Related Works", "content": "The problem of achieving coordination in multi-agent systems involves enabling multiple autonomous agents to work together effectively to accomplish shared goals. This problem is widely studied in fields such as distributed artificial intelligence (Genesereth, Ginsberg, and Rosenschein 1988), swarm intelligence (stigmergy (Marsh and Onof 2008)), and game theory (correlated equilibrium (Aumann 1974)). However, in many settings, all agents are aware of the goal. In contrast, this paper focuses on agent-agent coordination, in which only one agent possesses information about the goal.\nA common approach to address these challenges under incomplete information is through explicit communication, using discrete signals as in Hanabi (Bard et al. 2020), or natural language in negotiation and coordination games like Deal-or-No-Deal (Lewis et al. 2017; He et al. 2018), Diplomacy (Paquette et al. 2019), and MutualFriends (He et al. 2017). Recently, the Gnomes at Night task (Chen, Fried, and Topcu 2024) highlights the challenges of shared control under incomplete information, though also leveraging natural language dialogue. In contrast, our study examines coordination without direct communication, using a mute version of the Gnomes at Night testbed.\nAnother approach to understanding coordination is through multi-agent reinforcement learning (MARL), where agents learn cooperative strategies via trial and error in complex environments, particularly through self-play and opponent modeling (Foerster et al. 2018). However, most MARL approaches use neural networks to represent policies, which often obscure the intent inference process within the learning model. The automata-learning-based solution technique proposed in this paper provides a more explicit representation, potentially offering better explainability.\nPedestrian trajectory prediction is another relevant problem, which uses data and models to anticipate pedestrian movements based on factors like past behavior, environmental conditions, and interactions with others. This is analogous to the helper's task of inferring the seeker's desired actions. Common approaches include the knowledge-based approach (Helbing and Moln\u00e1r 1995) and supervised deep learning methods (Alahi et al. 2016).\nA process mirroring the challenge of helper attempting to infer the seeker's intended actions is plan recognition in planning (Kautz, Allen et al. 1986). Goal recognition involves identifying all potential goals an agent might pursue based on a sequence of observed actions (Vilain 1990; Charniak and Goldman 1993; Lesh and Etzioni 1995; Goldman, Geib, and Miller 1999; Avrahami-Zilberbrand and Kaminka 2005; Ramirez and Geffner 2009). In this context, the domain is entirely visible, allowing for the calculation of possible goals that can be achieved through an optimal policy aligning with these observations. However, in our setting, the helper lacks information on the seeker's domain. An efficient coordination could help in mutual understanding of each player's domain. Exploring how to develop such a coordination aligns with the focus of this study."}, {"title": "8 Conclusion and Future Works", "content": "In this paper, we investigate how an autonomous agent (helper) can learn to coordinate with a seeker in games requiring cooperation yet communication is disallowed. We propose an automata-learning-based technique that obtains a strategy for the helper by constructing a DFA for each helper action to recognize the seeker's intentions in its behavior. Using Gnomes at Night as a testbed, our experiments show that this approach nearly matches the performance of an oracle baseline with direct communication. For future work, we would like to explore an iterative version of our automata-learning-based technique that continuously refines the helper's strategy. Another direction is to extend our approach from standard reachability objectives to temporal objectives, which would necessitate more structured planning for both the seeker and the helper. Additionally, we see potential in adapting this method to handle games with greater nondeterminism, such as those involving human or environmental interventions."}]}