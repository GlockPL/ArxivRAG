{"title": "Multiobjective Vehicle Routing Optimization with Time Windows: A Hybrid Approach Using Deep Reinforcement Learning and NSGA-II", "authors": ["Rixin Wu", "Ran Wang", "Jie Hao", "Qiang Wu", "Ping Wang", "Dusit Niyato"], "abstract": "This paper proposes a weight-aware deep reinforcement learning (WADRL) approach designed to address the multiobjective vehicle routing problem with time windows (MOVRPTW), aiming to use a single deep reinforcement learning (DRL) model to solve the entire multiobjective optimization problem. The Non-dominated sorting genetic algorithm-II (NSGA-II) method is then employed to optimize the outcomes produced by the WADRL, thereby mitigating the limitations of both approaches. Firstly, we design an MOVRPTW model to balance the minimization of travel cost and the maximization of customer satisfaction. Subsequently, we present a novel DRL framework that incorporates a transformer-based policy network. This network is composed of an encoder module, a weight embedding module where the weights of the objective functions are incorporated, and a decoder module. NSGA-II is then utilized to optimize the solutions generated by WADRL. Finally, extensive experimental results demonstrate that our method outperforms the existing and traditional methods. Due to the numerous constraints in VRPTW, generating initial solutions of the NSGA-II algorithm can be time-consuming. However, using solutions generated by the WADRL as initial solutions for NSGA-II significantly reduces the time required for generating initial solutions. Meanwhile, the NSGA-II algorithm can enhance the quality of solutions generated by WADRL, resulting in solutions with better scalability. Notably, the weight-aware strategy significantly reduces the training time of DRL while achieving better results, enabling a single DRL model to solve the entire multiobjective optimization problem.", "sections": [{"title": "I. INTRODUCTION", "content": "VEHICLE routing problem (VRP), a pivotal sector in transportation logistics, plays a crucial role in modern traffic management and operational efficiency. This significance is underscored by the fact that optimized vehicle routing can significantly reduce operational costs, a critical factor in the highly competitive and cost-sensitive transportation industry [1]. As the demand for precise delivery schedules escalates and the transportation sector becomes more competitive, incorporating customer delivery time windows has become not just a preference but a necessity. This evolution has catapulted the study of the vehicle routing problem with time windows (VRPTW) to the forefront of transportation research, marking it as a key area for innovative solutions in traffic and logistics management [2], [3]. The advancements in VRPTW are not just academic pursuits; they directly translate to more efficient, reliable, and cost-effective transportation systems, reinforcing the backbone of global trade and commerce.\nIn the study of VRPTW, time windows are typically categorized into two distinct types: hard time windows and soft time windows. In scenarios involving hard time windows, vehicles are obligated to adhere strictly to the scheduled delivery times. Specifically, for a VRP with a hard time window, the vehicle must arrive and conduct deliveries within the predefined time window. In instances where the vehicle arrives before the onset of the hard time window, it is requisite for the vehicle to wait until the specified start time before proceeding with the delivery. Conversely, if the vehicle arrives within the defined hard time window, it is authorized to proceed with the delivery directly. However, delivering goods subsequent to the expiration of the hard time window is strictly prohibited [4], [5].\nIn the VRP with soft time windows, there exists a permissible degree of flexibility for delivery vehicles to operate outside the designated time windows [6], [7]. This flexibility, however, often incurs a consequential trade-off with customer satisfaction levels. An optimization strategy prioritizing the shortest path distance (or minimal cost) can result in the infringement of time windows for certain customers, thereby adversely affecting overall customer satisfaction [8]. To enhance customer satisfaction, managers may find themselves compelled to augment the fleet size or extend the driving routes, which paradoxically escalates transportation costs. This scenario elucidates a prevalent conflict in VRPTW: the dichotomy between minimizing transportation costs and maximizing customer satisfaction. Consequently, the incorporation of multiobjective optimization emerges as a pivotal consideration within VRPTW.\nIn the domain of single-objective VRPTW (SOVRPTW) which predominantly focuses on minimizing travel costs, constraints such as time windows, vehicle capacity, and the number of available vehicles need to be considered simul-"}, {"title": "II. RELATED WORK", "content": "In recent years, there has been significant research focus on the vehicle routing problem with time windows (VRPTW), leading to the development of numerous algorithms aimed at solving this problem. These algorithms can be categorized into three main groups: exact algorithms [11]\u2013[13], heuristic algorithms [9], [10], [14]\u2013[19], and learning-based algorithms [20], [21].\nExact algorithms approach problems by rigorously deriving mathematical formulations, thus establishing mathematical models for the problem, and proposing algorithms to find optimal solutions based on mathematical principles. The authors in [11] employed the column generation method to achieve the shortest paths in VRPTW. Furthermore, branch and bound [12] as well as branch and price [13] methods have also been applied in the context of VRPTW. However, it's worth noting that exact algorithms, which seek optimal solutions from all feasible solutions according to certain rules, face exponential growth in search space and computational complexity as the number of customers in VRPTW increases. Moreover, these algorithms tend to be less effective when dealing with multiobjective VRPTW (MOVRPTW).\nHeuristic algorithms are the most commonly used methods in solving VRPTW. In solving single-objective VRPTW (SOVRPTW), common approaches encompass genetic algorithm (GA) [9], [14], particle swarm algorithm (PSO) [10], and tabu search (TS) algorithm [15], among others. In [9], Ursani et al. employed a local genetic algorithm to implement small-scale VRPTW targeting the shortest path and produced better solutions than most other heuristics methods. Another genetic algorithm, which was centered on the insertion heuristic, was proposed for the resolution of VRPTW, as documented in [14]. The experimental results revealed the algorithm was able to find the solution in less time. Furthermore, both PSO with local search [10] and the TS algorithm [15] were also used to solve SOVRPTW.\nIn real-life scheduling optimization, increasing demands have rendered SOVRPTW less capable. Consequently, many researchers have delved into the study of MOVRPTW. Jaber Jemai et al. employed the non-dominated sorting genetic algorithm-II (NSGA-II) to simultaneously optimize the minimization of both total travel distance and carbon dioxide emissions [16]. In ambulance route optimization, NSGA-II and the multiobjective particle swarm optimization (MOPSO) algorithm were employed to jointly optimize the latest service completion times and the number of patients whose medical conditions deteriorated due to delayed medical services [17]. Additionally, multiobjective evolutionary algorithm based on decomposition (MOEA/D) [18] and multiobjective simulated annealing (MOSA) algorithm [19] have also been utilized in addressing MOVRPTW."}, {"title": "III. PROBLEM STATEMENT", "content": "In the process of employing heuristic algorithms for solving MOVRPTW, it is common practice to begin by generating a set of initial solutions that adhere to the problem's constraints. Subsequently, a set of heuristic strategies, such as those found in genetic algorithms (GA), including selection, crossover, and mutation operations, in conjunction with non-dominated sorting strategies, is applied to obtain the Pareto front. This approach is advantageous due to its simplicity and its capability to yield high-quality solutions. However, it is worth noting that heuristic algorithms typically generate initial solutions using random strategies, and for MOVRPTW instances with numerous constraints, obtaining a feasible solution can be time-consuming. Moreover, the overall effectiveness of the algorithm is greatly influenced by the quality of the initial solutions [22]. Additionally, whenever there is any modification to the problem's information, even minor changes, it necessitates rerunning the heuristic algorithm, which is known as the No Free Lunch Theorem [23]. Moreover, when the dimension of the problem is particularly large, these algorithms require a substantial number of iterations for overall updates and iterative searches to achieve relatively favorable results, leading to extended computation times [20].\nWith the advancement of artificial intelligence (AI) technology, deep reinforcement learning (DRL) has also been employed to tackle multiobjective optimization problems. DRL was initially proposed for solving multiobjective travelling salesman problems (MOTSP) in [20], in which the authors decomposed the MOTSP into several subproblems through weight combinations. They employed a sequence-to-sequence pointer network with two recurrent neural networks (RNNs) to train models for each subproblem. Additionally, a parameter transfer strategy was used in the initialization process of model parameters for adjacent subproblem training. In [21], a similar approach was adopted, where the authors decomposed multiobjective vehicle routing problem (MOVRP) into multiple scalar subproblems based on weights. They utilized the pointer network to address each subproblem and trained the parameters of the policy network using the policy gradient algorithm of reinforcement learning to obtain the Pareto front of MOVRP.\nThe general method of deep reinforcement learning to solve multiobjective optimization problems is to convert the problem into multiple subproblems based on multiple sets of weight combinations with the same interval. Each sub-problem can be regarded as a single objective optimization problem. Subsequently, a DRL-based model is trained for each subproblem, and only after training all models for the subproblems can the entire multiobjective optimization problem be addressed. While this method may be effective for biobjective optimization problems, as the number of objective functions increases, the training time escalates exponentially, which becomes impractical for solving it. Additionally, it is challenging to ensure that the DRL-based models for each subproblem are adequately trained, causing the solutions of some subproblems to be dominated by the solutions of other subproblems [20], rendering these solutions meaningless in multiobjective optimization problems.\nConsidering the limitations of DRL in solving multiob-jective optimization problems, we propose WADRL, which enables a single DRL model to address the entire multiobjective optimization problem. Furthermore, to overcome the drawbacks of heuristic algorithms in solving multiobjective optimization problems, we utilize the solutions generated by WADRL as the initial solutions of NSGA-II, ensuring that the initial solutions are feasible and of high quality. After undergoing evolution with NSGA-II, it is basically guaranteed that all solutions are Pareto optimal."}, {"title": "A. System Model", "content": "The logistics businesses typically schedule service appointments with customers through phone calls or text messages before delivering goods or providing services. However, the delivery vehicles may arrive earlier or later than the scheduled time due to inefficient delivery routes or traffic jams. Customers usually have some level of tolerance for delays or early arrivals, but this may reduce customer satisfaction. Therefore, logistics companies should aim to improve customer satisfaction while simultaneously reducing travel cost [24].\nWe consider a multivehicle routing system in which K vehicles are instructed to deliver goods to the set H of customers. As illustrated in Fig. 1, the scenario involves three vehicles tasked with delivering goods to eight customers. Our system is formally conceptualized as a graph G = (V, E), where the set of vertices V comprises a depot vertex \\(v_0\\) and a set of customer vertices \\(C = \\{v_i, i = 1, 2, ..., h\\}\\). The edges E in the graph represent the roads connecting these vertices, each associated with a specific travel cost.\nThe coordinate of the depot is \\(u_0\\), where each vehicle k must depart from and return to. The information of each customer \\(v_i\\) includes the coordinate \\(c_i\\), the demand for goods \\(d_i\\), the interval \\([e_i, l_i]\\) and the interval \\([E_i, L_i]\\). Among them, the interval \\([e_i, l_i]\\) denotes the soft time window for customer i. Service provided to the customer within the intervals \\([E_i, e_i]\\) or \\([l_i, L_i]\\) leads to a decrease in customer satisfaction. The customer being served outside of the hard time interval \\([E_i, L_i]\\) is not permitted. The definition of the hard time window interval is typically as follows:"}, {"title": "B. Problem Formulation", "content": "In this section, we propose a multiobjective vehicle routing problem with time windows model that considers the cost incurred by travel and vehicle usage as well as customer satisfaction, as follows:\nThe decision variables in this paper are defined as follows:\n\\(x_{ijk} = \\begin{cases} 1, & \\text{if vehicle k travels from customer i to j} \\\\ 0, & \\text{otherwise,} \\end{cases}\n\\)\n\\(y_{ik} = \\begin{cases} 1, & \\text{if customer i is served by vehicle k} \\\\ 0, & \\text{otherwise.} \\end{cases}\n\\)\nThe objective functions in this paper are defined as follows: Objective function 1: minimizing the total travel cost, which can be expressed as:\n\\(min f_1 = \\sum_{k \\in K} \\sum_{i \\in H^*} \\sum_{j \\in H^*} c_k d_{is_{ij}} x_{ijk} + \\sum_{k \\in K} c_k^2 \\sum_{j \\in H^*} x_{0jk}, \n\\)\nwhere \\(c_k^1\\) represents the travel cost of vehicle k per unit distance, \\(c_k^2\\) denotes the fixed cost associated with the utilization of vehicle k, and \\(d_{is_{ij}}\\) represents the distance between customer i and customer j.\nObjective function 2: maximizing the average customer satisfaction, which can be expressed as:\n\\(max f_2 = \\frac{1}{h} \\sum_{i} S_i (a_i),\n\\)\nwhere \\(a_i\\) represents the arrival time of the vehicle at the location of customer i. Furthermore, \\(S_i (a_i)\\) is defined as the satisfaction of customer i when the vehicle arrives at customer i at time \\(a_i\\).\nSubject to:\n\\[\\sum_{i \\in H^*} d_i y_{ik} < U_k \\quad \\forall k \\in K\\]\n\\[\\sum_{k \\in K} y_{ik} = 1 \\quad \\forall i \\in H^*\\]\n\\[\\sum_{j \\in H^*} x_{0jk} - \\sum_{i \\in H^*} x_{i0k} = 0 \\quad \\forall k \\in K\\]\n\\[\\sum_{i \\in H^*} x_{ijk} = y_{jk} \\quad \\forall k \\in K,\\forall j \\in H^*\\]\n\\[\\sum_{j \\in H^*} x_{ijk} = y_{ik} \\quad \\forall k \\in K,\\forall i \\in H^*\\]\n\\[w_0 = v_0 = 0\\]\n\\[w_i = max \\{0, E_i - a_i \\} \\quad \\forall i \\in H^*\\]\n\\[E_i \\leq a_i + w_i \\leq L_i \\quad \\forall i \\in H^*\\]\n\\[a_j = \\sum_{k \\in K} \\sum_{i \\in H^*} x_{ijk} (a_i + w_i + v_i + t_{ij}) \\quad \\forall j \\in H^*\\]\n\\[x_{ijk} \\in \\{0,1\\}, y_{ik} \\in \\{0,1\\}, a_i \\geq 0 \\quad \\forall i \\in H, j \\in H, k \\in K\\]\nIn the aforementioned mathematical model, let \\(H^*\\) represent the set of customer vertices, H represent the combined set of customer and depot vertices, defined as \\(H = H^* \\cup \\{0\\}\\). The set of vehicles is denoted by K. The constraint (8) ensures that the demand of customers does not exceed the capacity of each vehicle, where \\(u_k\\) represents the capacity of vehicle k. The constraint (9) mandates that each customer is served by exactly one vehicle. The constraint (10) guarantees that each vehicle must start and end its route at the depot. The constraints (11) and (12) ensure that each customer is served exactly once. The constraint (13) defines the waiting and service times of the depot, with \\(w_i\\) and \\(v_i\\) denoting the waiting and service"}, {"title": "IV. SOLUTIONS AND ALGORITHMS", "content": "Considering the complexity of multiobjective vehicle routing problem with time windows (MOVRPTW), particularly in scenarios involving a large number of customers, existing scheduling methods seem insufficient to effectively solve the problem. Therefore, this section introduces an innovative approach that amalgamates a weight-aware deep reinforcement learning (WADRL) methodology with the non-dominated sorting genetic algorithm-II (NSGA-II). We design this hybrid method to tackle the challenges posed by MOVRPTW. Initially, the WADRL algorithm is employed to generate the initial population for the NSGA-II algorithm. Subsequently, the NSGA-II algorithm undertakes the task of optimizing this initial population, thus enhancing the solution's efficacy."}, {"title": "A. General Framework", "content": "In traditional deep reinforcement learning (DRL) methods for solving multiobjective optimization problems, as illustrated in Fig. 2, a biobjective optimization problem is decomposed into N subproblems according to N weight combinations. For the solution of each subproblem, the training of a DRL model is required. Consequently, a total of N DRL models need to be trained to solve the biobjective optimization problem [20]. This approach appears to be effective for solving biobjective optimization problems. However, as the number of objective functions increases, the time required to solve the entire multiobjective optimization problem will increase exponentially.\nConsidering the shortcomings of the existing DRL methods to solve multiobjective optimization problems, our proposed algorithm takes a different approach. Similarly, we decompose the entire multiobjective optimization problem into N subproblems based on N weight combinations. During the training process of DRL, the algorithm randomly selects a weight combination for training each time. Furthermore, during the testing phase, the algorithm generates the corresponding optimal solutions for these N subproblems. This approach enables a single DRL model to solve the entire multiobjective optimization problem, significantly reducing the model training time, particularly for many-objective optimization problems. The WADRL framework is drawn in Fig. 3, in which a novel transformer architecture is also adopted.\nIn the original NSGA-II algorithm, the initial population is typically generated through a random process, and if the solution does not satisfy the constraints, it will be regenerated. In MOVRPTW, it is obviously impossible to generate solutions that satisfy constraints through a completely random method. While there exist methods to expedite the initial solution generation, they are still relatively inefficient [25], and the quality of these initial solutions is generally low. In our proposed algorithm, solutions generated by WADRL are used as the initial solutions for the NSGA-II algorithm. NSGA-II is then applied to optimize these solutions. The advantage of this approach is that the initial solutions generated by WADRL are guaranteed to satisfy the constraints. Furthermore, the generation of these initial solutions adheres to a carefully designed decomposition strategy. This approach guarantees a relatively uniform distribution across the Pareto front of the initial solutions. Importantly, this strategy not only facilitates coverage but also significantly enhances the quality of the initial solutions."}, {"title": "B. The Markov Decision Process of MOVRPTW in Weight-aware Deep Reinforcement Learning", "content": "Given the proficiency of reinforcement learning in managing sequential decision-making problems, we employ deep reinforcement learning to address the multiobjective vehicle routing problem with time windows (MOVRPTW). In this approach, MOVRPTW is modeled as a markov decision process (MDP), characterized by specific components: state S, action A, state transition function, and reward R. These elements are defined as follows.\n1) State: At any given step t, the state of the system encompasses three primary components: the vehicle state \\(v_t\\), customer state \\(c_t\\), and weight state \\(w_t\\), defined as follows. The vehicle state \\(v_t\\) is characterized by two dynamic attributes: its load, denoted as \\(u_t\\), and the traveled time, symbolized as \\(T_t\\). The term 'dynamic' implies that these states may change as actions occur. The customer states encompass static states, including soft and hard time windows \\(e\\), \\(I\\), \\(E\\) and \\(L\\), coordinate \\(c\\), and service time \\(v\\). Static states indicate that these states will not change with the occurrence of actions. The dynamic state of a customer is demand d. The weight state is static, signifying the weights of two objective functions \\(w_1\\) and \\(w_2\\).\n2) Action: The action at each step t, denoted as \\(a_t\\), represents the next vertex to which the vehicle will travel. The entire"}, {"title": "C. The Policy network in Weight-aware Deep Reinforcement Learning", "content": "In our system model, various information needs to be considered simultaneously, such as customer coordinates, demand, time windows, and the weights of the objective functions. Therefore, learning-based methods need to be carefully designed to process the information. However, simple neural networks or learning strategies often struggle to handle the complex information mentioned above. Meanwhile, transformer architecture, as a type of self-attention mechanism, has been proven to perform well in many fields, including natural language processing (NLP) [26], computer vision (CV) [27], and recommender systems [28]. With the development of transformer technology, it has found wide applications in deep reinforcement learning, displaying excellence in problems like path planning [7], the knapsack problem [29], and reservoir operation [30]. Its advantage is that the attention mechanism in the transformer can effectively extract information through key-value-query maps.\nIn this paper, we employ the transformer architecture [31] to model the delivering agent for solving MOVRPTW, as illustrated in Fig. 3. The model primarily comprises an encoder module, a weight embedding module, and a decoder module [31]. In the encoder process, the encoder embeds the original information of MOVRPTW into a high-dimensional space and utilizes a self-attention mechanism to extract features of the problem. In the weight embedding process, the embedding module encapsulates the state of vehicles and the weights associated with the two objectives of MOVRPTW. In the decoder process, the decoder generates vertex probabilities based on contextual information.\n1) The information encoder process: Initially, the encoder employs a linear layer to transform the features of the vertices (including both the depot and customers) into a high-dimensional space. This transformation results in the generation of initial embedding information, represented as \\(h^{(0)} = \\{h_1^{(0)}, h_2^{(0)},..., h_n^{(0)}\\}, where \\(h_i^{(0)}\\) corresponds to the initial embedded representation of vertex i. We employ separate linear projections to handle distinct information of warehouse and customer vertices, as follows [32]:\n\\(h_i^{(0)} = \\begin{cases} W_o[c_o, L_o] + b_o, & \\text{if } i = 0 \\\\ W_c[c_i, E_i, e_i, l_i, L_i, d_i, v_i] + b_c, & \\text{if } i \\neq 0, \\end{cases}\n\\)\nwhere the depot vertex only needs to embed coordinate and latest arrival time information, while the customer vertices need to embed coordinate, soft and hard time window, demand, and service time information. The operation \\([., ., . . ., \u00b7]\\) concatenates the information of the same dimension together. \\(W_o, b_o, W_c\\) and \\(b_c\\) are trainable linear projection parameters, and \\(W_o \\in \\mathbb{R}^{d_h \\times 2}\\), \\(b_o \\in \\mathbb{R}^{d_h}\\), \\(W_c \\in \\mathbb{R}^{d_h \\times 8}\\), \\(b_c \\in \\mathbb{R}^{d_h}\\), where \\(d_h\\) represents the dimension of \\(h^{(0)}\\). In our algorithm \\(d_h = 128\\).\nSubsequent to the initial embedding, the encoded information undergoes processing through L identical layers to yield the final embedding. Each of these layers is composed of several key components: multi-head attention layer (MHA), skip connection layer, batch normalization (BN) layer,"}, {"title": "IV. SOLUTIONS AND ALGORITHMS", "content": "= BN^{(l)} \\left(h^{(l-1)} + MHA \\left(h^{(l-1)}\\right)\\right).\n\\)\nUltimately, the output of the node embedding at layer l is denoted as \\(h^{(l)}\\), which can be obtained through an FF layer and a skip connection layer as follows:\n\\(h^{(l)} = BN^{(l)} \\left(h^{(l-1)} + FF \\left(h^{(l-1)}\\right)\\right).\n\\)\n2) The weight embedding process: As MOVRPTW is modeled as an MDP, each vehicle is treated as an agent. When the agent selects actions, it needs to consider the weights of the two objective functions in MOVRPTW. Hence, we incorporate a specialized weight embedding module within our framework. This module is carefully designed to capture the current state of the vehicle and the state of the weights associated with the objective functions. The strategy allows the agent to focus on the weights of the objective functions when making decisions. The output of the weight embedding module is defined as follows:\n\\[o^* = FF \\left(W_o [u_t, r_t, w_1, w_2] + b_o\\right).\\]\n3) The tour decoder process: At each given step t, the agent selects a decision \\(a_t\\) based on the current state \\(s_t\\), which contains the embedding of the entire graph denoted as \\(\\hat{h}^{(L)}\\), the embedding information of the vertex corresponding to the vehicle's current location \\(h_{a_{t-1}}^{(L)}\\) and the output \\(o^*\\) of the weight embedding module as follows:"}, {"title": "Algorithm 1 The Training Process of Weight-Aware Deep Reinforcement Learning", "content": "Input: Batch size and training epoch\nOutput: The trained parameters \\({\\theta}\\)\n1: Initialization \\({\\theta}\\) for policy network \\({\\pi_{\\theta}}\\), \\({\\theta}_{BL}\\) for baseline network \\({\\pi}_{BL}\n2: Generate an MOVRPTW instance randomly;\n3: for each epoch do\n4:\tfor each batch do\n5:\t\tfor each step t do\n6:\t\t\tCalculate vertex embedding in Eqs.(22)-(29);\n7:\t\t\tCalculate weight embedding in Eq.(30);\n8:\t\t\tCalculate vertex probability vector in Eqs.(31)-(37);\n9:\t\t\tChoose action \\(a_t\\);\n10:\t\t\tif All available vehicles are returned to the depot vertex or all customer vertices are visited then\n11:\t\t\t\tBreak;\n12:\t\t\tend if\n13:\t\tend for\n14:\t\tCalculate the reward \\(R\\left(\\pi_{\\theta}\\right)\\) and \\(R\\left(\\pi_{BL}\\right)\\) in Eq.(21);\n15:\t\tCalculate \\(\\nabla_{\\theta} L\\left({\\theta}\\right)\\) ;\n16:\t\tUpdate the parameters \\({\\theta}\\);\n17:\tend for\n18: \tif T-Test result < 0.95 then\n19:\t\t{\\theta}_{BL}\\gets{\\theta}\n20: \tend if\n21: end for\nwhere \\(s_t = [\\hat{h}^{(L)}, h_{a_{t-1}}^{(L)}, o^*]\\).\nSubsequently, we calculate the contextual information of \\(s_t\\) through the MHA layer, defining the query vector as the embedding of the agent, and the key vector and value vector as the embedding of customers:\n\\(Q_d^t = W_q s_t,\\)\n\\(K_d^t = W_k \\hat{h}^{(L)},\\)\n\\(V_d^t = W_v \\hat{h}^{(L)}.\\)\nThe compatibility between each vertex and the vehicle is calculated through an attention mechanism:\n\\(A_t = C\\cdot \\tanh \\left(\\frac{(Q_d^t)^T K_d}{\\sqrt{d_k}}\\right).\\)\nIn MOVRPTW, various constraints are established, meaning that at each step t, certain vertices are inaccessible due to these constraints. To navigate the diverse constraints inherent in the system, we also implement a masking rule. This rule functions to selectively exclude vertices that are not feasible for visitation during the current step. The primary masking rules include the following: \u2460 Apart from the depot vertex, other\n\npreviously visited vertices are masked; \u2461 Customer vertices with demand exceeding the load of vehicle are masked; \u2462 Vertices where the vehicle's arrival time at the node exceeds the hard time window are masked. Whether the vertex i is masked at step t is defined as:\n\\(mask_i^t = \\begin{cases} 1, & \\text{if node i is masked at step t} \\\\ 0, & \\text{otherwise.} \\end{cases}\n\\)\nFinally, the probability vector for vertex selection is generated by combining the compatibility vector between the vehicle and the vertices:\n\\(p_t = softmax \\left(x + O\\cdot mask_t\\right),\\)\nwhere O is a large negative number, e.g., \\(O = -999999\\), which means that the customer vertex cannot be visited.\n4) Model training: We employ a policy gradient method to train all parameters \\({\\theta}\\) within the neural network. The algorithm comprises two networks, namely the policy network and the baseline network. Both the policy and baseline networks share the same network structure, with the only difference being that the policy network selects actions based on sampling from the probability vector, while the baseline network adopts a strategy where actions are selected based on the action possessing the highest probability, namely 'greedy policy'. The gradient of the loss function is defined as follows [32]\n\\[\\nabla_{\\theta} L(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[(R(\\pi_{\\theta})-R(\\pi_{BL}))\\nabla_{\\theta}log\\pi_{\\theta}]\\]\nwhere \\(R\\left(\\pi_{\\theta}\\right)\\) and \\(R\\left(\\pi_{BL}\\right)\\) are utilized to denote the rewards accrued by the policy network and the baseline network, respectively. During each training batch, a t-test is employed to ascertain the statistical significance of the difference in performance between these two networks at a 95% confidence level. Should this test yield a significant result, it prompts an update wherein the parameters of the policy network supersede those of the baseline network, thereby optimizing the learning process. The detailed procedural steps of this algorithm are given in Algorithm 1."}, {"title": "V. RESULTS AND ANALYSIS", "content": "To demonstrate the advantages of our proposed approach that combines weight-aware deep reinforcement learning (WADRL) and non-dominated sorting genetic algorithm-II (NSGA-II), we apply it to the Solomon dataset [34]. Furthermore, we conduct comparative studies on datasets with varying"}, {"title": "A. Experimental Setup", "content": "1) Training sets: The model is trained on instances with 20-, 50- and 100-customer vertices and a single depot vertex, respectively. In the training dataset, the demand for customer vertices is uniformly sampled from the range [1,40]. For the instances with 20-, 50-, and 100-customer instances, the coordinates of the vertices are generated from a uniform distribution within the range [0, 100]. We randomly generate"}, {"title": "1) The time used by our NSGAII-500", "content": "Soft Computing, 3,0.500\nFor each iteration, a set of weight combinations is extracted according to the Dirichlet distribution. We use the Adam optimizer with a learning rate of 10\\(^{-4}\\) with decay of 10\\(^{-6}\\) [37]. In total, we train the model for 300 epochs and the batch size is set to 64.\nDRL and WADRL use weight combinations with an interval of 0.02 to decompose MOVRPTW. Specifically, these weight combinations are defined as [[1.00, 0.00], [0.98, 0.02], [0.00, 1.00]], resulting in a total of 51 subproblems. Furthermore, we configure the population size of the NSGA-II algorithm to also be 51."}, {"title": "B. Comparison of Initial Solutions", "content": "We first compare the performance of our method and the original NSGA-II in generating initial solutions, as shown in Fig. 5. The results indicate that our method consistently outperforms the traditional NSGA-II algorithm in terms of the quality of initial solutions, across instances with 20-, 50-, 100-customer. Furthermore, the average execution time together with the average objective function value of the initial solution are presented in TABLE III.\nAs shown in Fig. 5, all initial solutions generated by the WADRL-based NSGA-II outperform the initial solutions generated by the original NSGA-II using random policies, both in terms of objective functions and time efficiency. This improvement is particularly pronounced as the number of customers increases, with our method exhibiting more significant enhancements over the traditional NSGA-II approach. For example, in the case of the 100-customer instance, our method, on average, reduces the cost by 4008.6, increases customer satisfaction by 0.3715, and decreases the time required for generating initial solutions by 28.45 seconds. Furthermore, we observe that within the solutions generated by WADRL, many"}, {"title": "C. Experimental Results", "content": "We implement NSGA-II algorithms with 200, 500, 1000, and 2000 iterations, as well as NSGA-II with 500 iterations using the solution of WADRL as the initial solution. We compare these results with DRL and WADRL, and the average objective function values and running times are presented in TABLE III.\nFig. 6, Fig. 7 and Fig. 8 depict the experimental results on instances of 20-, 50-, and 100-customer, respectively. Our approach (WADRL+NSGA-II) consistently achieves optimal results on all instances, particularly as the problem scale increases. WADRL itself has achieved relatively competitive results, and when combined with the NSGA-II algorithm, WADRL's solutions exhibit higher quality and extended coverage on the Pareto front.\nIn the 20-customer instance, our method, NSGAII-1000 and NSGAII-2000 show almost the same performance, but our method still finds solutions which have a lower travel cost. Not to mention that in the 100-customer instance, the advantage of our method is overwhelming, using less than 200 seconds to obtain effects that the NSGA-II algorithm cannot obtain in 1000 seconds.\nIn terms of the average objective function values, as shown in TABLE III, the WADRL+NSGA-II algorithm outperforms other benchmark algorithms under all instances. Meanwhile, the runtime of WADRL is significantly lower than that of the traditional DRL algorithms and all NSGA-II algorithms."}, {"title": "D. Effectiveness of Weight-aware Strategy", "content": "In this section, the weight-aware strategy in deep reinforcement learning is experimentally validated. Fig8 displays the results of all solutions generated by WADRL and DRL on instances on 20-, 50-, and 100-customer instances. TABLE IV presents the average training times. The results indicate that WADRL achieves superior results while utilizing only about 1/10 of the training time compared of the traditional DRL."}, {"title": "VI. CONCLUSION", "content": "This paper proposed a weight-aware deep reinforcement learning (WADRL) framework combined with non-dominated sorting algorithm-II (NSGA-II) to solve the multiobjective vehicle routing problem with time windows (MOVRPTW). Firstly, a comprehensive MOVRPTW model was proposed, which takes into account both travel cost and customer satisfaction. Subsequently, a WADRL framework was introduced, where the weights of objective functions were integrated into the state of deep reinforcement learning (DRL), enabling a single DRL model to solve the entire multiobjective optimization problem. An innovative DRL framework was introduced, which is centered around a transformer-based policy network. The architecture of this policy network was carefully designed, comprising three integral components: the encoder module, which is responsible for embedding customer information, the weight embedding module, which plays a critical role in capturing and encoding the weights of objective functions, and the decoder module, tasked with generating executable actions based on the contextual information. Furthermore, considering the limitations of DRL, we employed the NSGA-II algorithm to optimize the solutions generated by WADRL. Experimental results demonstrated the promising performance of our method across all MOVRPTW instances. Specifically, our method produced Pareto fronts with better coverage and"}]}