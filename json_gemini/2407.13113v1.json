{"title": "Multiobjective Vehicle Routing Optimization with Time Windows: A Hybrid Approach Using Deep Reinforcement Learning and NSGA-II", "authors": ["Rixin Wu", "Ran Wang", "Jie Hao", "Qiang Wu", "Ping Wang", "Dusit Niyato"], "abstract": "This paper proposes a weight-aware deep rein-forcement learning (WADRL) approach designed to address the multiobjective vehicle routing problem with time windows (MOVRPTW), aiming to use a single deep reinforcement learning (DRL) model to solve the entire multiobjective optimization problem. The Non-dominated sorting genetic algorithm-II (NSGA-II) method is then employed to optimize the outcomes produced by the WADRL, thereby mitigating the limitations of both ap-proaches. Firstly, we design an MOVRPTW model to balance the minimization of travel cost and the maximization of customer sat-isfaction. Subsequently, we present a novel DRL framework that incorporates a transformer-based policy network. This network is composed of an encoder module, a weight embedding module where the weights of the objective functions are incorporated, and a decoder module. NSGA-II is then utilized to optimize the solutions generated by WADRL. Finally, extensive experimental results demonstrate that our method outperforms the existing and traditional methods. Due to the numerous constraints in VRPTW, generating initial solutions of the NSGA-II algorithm can be time-consuming. However, using solutions generated by the WADRL as initial solutions for NSGA-II significantly reduces the time required for generating initial solutions. Meanwhile, the NSGA-II algorithm can enhance the quality of solutions generated by WADRL, resulting in solutions with better scalability. Notably, the weight-aware strategy significantly reduces the training time of DRL while achieving better results, enabling a single DRL model to solve the entire multiobjective optimization problem.", "sections": [{"title": "I. INTRODUCTION", "content": "VEHICLE routing problem (VRP), a pivotal sector in transportation logistics, plays a crucial role in modern traffic management and operational efficiency. This signifi-cance is underscored by the fact that optimized vehicle routing can significantly reduce operational costs, a critical factor in the highly competitive and cost-sensitive transportation industry [1]. As the demand for precise delivery schedules escalates and the transportation sector becomes more com-petitive, incorporating customer delivery time windows has become not just a preference but a necessity. This evolution has catapulted the study of the vehicle routing problem with time windows (VRPTW) to the forefront of transportation research, marking it as a key area for innovative solutions in traffic and logistics management [2], [3]. The advancements in VRPTW are not just academic pursuits; they directly translate to more efficient, reliable, and cost-effective transportation systems, reinforcing the backbone of global trade and commerce.\nIn the study of VRPTW, time windows are typically catego-rized into two distinct types: hard time windows and soft time windows. In scenarios involving hard time windows, vehicles are obligated to adhere strictly to the scheduled delivery times. Specifically, for a VRP with a hard time window, the vehicle must arrive and conduct deliveries within the predefined time window. In instances where the vehicle arrives before the onset of the hard time window, it is requisite for the vehicle to wait until the specified start time before proceeding with the delivery. Conversely, if the vehicle arrives within the defined hard time window, it is authorized to proceed with the delivery directly. However, delivering goods subsequent to the expiration of the hard time window is strictly prohibited [4], [5].\nIn the VRP with soft time windows, there exists a per-missible degree of flexibility for delivery vehicles to operate outside the designated time windows [6], [7]. This flexibility, however, often incurs a consequential trade-off with customer satisfaction levels. An optimization strategy prioritizing the shortest path distance (or minimal cost) can result in the infringement of time windows for certain customers, thereby adversely affecting overall customer satisfaction [8]. To en-hance customer satisfaction, managers may find themselves compelled to augment the fleet size or extend the driving routes, which paradoxically escalates transportation costs. This scenario elucidates a prevalent conflict in VRPTW: the di-chotomy between minimizing transportation costs and maxi-mizing customer satisfaction. Consequently, the incorporation of multiobjective optimization emerges as a pivotal consider-ation within VRPTW.\nIn the domain of single-objective VRPTW (SOVRPTW) which predominantly focuses on minimizing travel costs, constraints such as time windows, vehicle capacity, and the number of available vehicles need to be considered simul-"}, {"title": "II. RELATED WORK", "content": "In recent years, there has been significant research focus on the vehicle routing problem with time windows (VRPTW), leading to the development of numerous algorithms aimed at solving this problem. These algorithms can be categorized into three main groups: exact algorithms [11]\u2013[13], heuristic algorithms [9], [10], [14]\u2013[19], and learning-based algorithms [20], [21].\nExact algorithms approach problems by rigorously deriv-ing mathematical formulations, thus establishing mathemat-ical models for the problem, and proposing algorithms to find optimal solutions based on mathematical principles. The authors in [11] employed the column generation method to achieve the shortest paths in VRPTW. Furthermore, branch and bound [12] as well as branch and price [13] methods have also been applied in the context of VRPTW. However, it's worth noting that exact algorithms, which seek optimal solutions from all feasible solutions according to certain rules, face exponential growth in search space and computational complexity as the number of customers in VRPTW increases. Moreover, these algorithms tend to be less effective when dealing with multiobjective VRPTW (MOVRPTW).\nHeuristic algorithms are the most commonly used meth-ods in solving VRPTW. In solving single-objective VRPTW (SOVRPTW), common approaches encompass genetic algo-rithm (GA) [9], [14], particle swarm algorithm (PSO) [10], and tabu search (TS) algorithm [15], among others. In [9], Ursani et al. employed a local genetic algorithm to implement small-scale VRPTW targeting the shortest path and produced better solutions than most other heuristics methods. Another genetic algorithm, which was centered on the insertion heuristic, was proposed for the resolution of VRPTW, as documented in [14]. The experimental results revealed the algorithm was able to find the solution in less time. Furthermore, both PSO with local search [10] and the TS algorithm [15] were also used to solve SOVRPTW.\nIn real-life scheduling optimization, increasing demands have rendered SOVRPTW less capable. Consequently, many researchers have delved into the study of MOVRPTW. Jaber Jemai et al. employed the non-dominated sorting genetic algorithm-II (NSGA-II) to simultaneously optimize the min-imization of both total travel distance and carbon dioxide emissions [16]. In ambulance route optimization, NSGA-II and the multiobjective particle swarm optimization (MOPSO) algorithm were employed to jointly optimize the latest service completion times and the number of patients whose medical conditions deteriorated due to delayed medical services [17]. Additionally, multiobjective evolutionary algorithm based on decomposition (MOEA/D) [18] and multiobjective simulated annealing (MOSA) algorithm [19] have also been utilized in addressing MOVRPTW."}, {"title": "III. PROBLEM STATEMENT", "content": "A. System Model\nThe logistics businesses typically schedule service appoint-ments with customers through phone calls or text messages before delivering goods or providing services. However, the delivery vehicles may arrive earlier or later than the sched-uled time due to inefficient delivery routes or traffic jams. Customers usually have some level of tolerance for delays or early arrivals, but this may reduce customer satisfaction. Therefore, logistics companies should aim to improve cus-tomer satisfaction while simultaneously reducing travel cost [24].\nWe consider a multivehicle routing system in which K vehicles are instructed to deliver goods to the set H of customers. As illustrated in Fig. 1, the scenario involves three vehicles tasked with delivering goods to eight customers. Our system is formally conceptualized as a graph G = (V, E), where the set of vertices V comprises a depot vertex vo and a set of customer vertices C = {vi, i = 1, 2, ..., h}. The edges E in the graph represent the roads connecting these vertices, each associated with a specific travel cost.\nThe coordinate of the depot is uo, where each vehicle k must depart from and return to. The information of each customer vi includes the coordinate ci, the demand for goods di, the interval [ei, li] and the interval [Ei, Li]. Among them, the interval [ei, li] denotes the soft time window for customer i. Service provided to the customer within the intervals [Ei, ei] or [li, Li] leads to a decrease in customer satisfaction. The customer being served outside of the hard time interval [Ei, Li] is not permitted. The definition of the hard time window interval is typically as follows:\nEi = ei - \u015ai (li \u2013 ei), (1)\nLi = li + \u015ai (li \u2013 ei), (2)\nwhere \u015ai and \u0139i are the parameters used to define the allowable maximum violation. We define Si (ar) as the satisfaction of customer i when the vehicle arrives at customer i at time ai, and Si (ai) can be defined as:\n{\n0, ai < Ei Vai > Li\nai-Ei\nEi < ai < ei\nei-Ei\n1, ei \u2264 ai \u2264 li\nLi-ai\nli < ai \u2264 Li\nLi-li\nSi (ai) =\n(3)\nThe key nomenclature used in this paper, along with their respective definitions, is outlined in TABLE I.\nB. Problem Formulation\nIn this section, we propose a multiobjective vehicle routing problem with time windows model that considers the cost incurred by travel and vehicle usage as well as customer satisfaction, as follows:\nThe decision variables in this paper are defined as follows:\nXijk =\n{\n1, if vehicle k travels from customer i to j\n0, otherwise,\n(4)\nYik =\n{\n1, if customer i is served by vehicle k\n0, otherwise.\n(5)\nThe objective functions in this paper are defined as follows:\nObjective function 1: minimizing the total travel cost, which can be expressed as:\nmin f1 = \u03a3\u03a3\u03a3 d_isij x_ijk + \u03a3C_k \u03a3 x_0jk, (6)\nk\u2208K i\u2208H* j\u2208H*\nk\u2208K j\u2208H*\nwhere c_k^1 represents the travel cost of vehicle k per unit distance, c_k^2 denotes the fixed cost associated with the utiliza-tion of vehicle k, and disij represents the distance between customer i and customer j.\nObjective function 2: maximizing the average customer satisfaction, which can be expressed as:\nmax f2 = 1/h \u03a3_i S_i (a_i), (7)\nwhere ai represents the arrival time of the vehicle at the location of customer i. Furthermore, Si (ai) is defined as the satisfaction of customer i when the vehicle arrives at customer i at time ai.\nSubject to:\n\u2211 d_i y_ik \u2264 U_k \u2200k \u2208 K (8)\ni\u2208H*\n\u2211 y_ik = 1 \u2200i \u2208 H* (9)\nk\u2208K\n\u2211 x_0jk - \u2211 x_i0k = 0 \u2200k \u2208 K (10)\nj\u2208H* i\u2208H*\n\u2211 x_ijk = y_jk \u2200k \u2208 K,\u2200j \u2208 H* (11)\ni\u2208H\n\u2211 x_ijk = y_ik k \u2208 K,\u2200i \u2208 H* (12)\nj\u2208H\nW_0 = v_0 = 0 (13)\nW_i = max {0, E_i \u2212 a_i } \u2200i \u2208 H* (14)\nE_i \u2264 a_i + W_i \u2264 L_i \u2200i \u2208 H* (15)\na_j = \u03a3 \u03a3 x_ijk (a_i + W_i + v_i + t_ij) \u2200j \u2208 H* (16)\nk\u2208K i\u2208H\nx_ijk \u2208 {0,1}, y_ik \u2208 {0,1}, a_i \u2265 0 \u2200i \u2208 H, j\u2208 H, k \u2208 K (17)\nIn the aforementioned mathematical model, let H* represent the set of customer vertices, H represent the combined set of customer and depot vertices, defined as H = H* U {0}. The set of vehicles is denoted by K. The constraint (8) ensures that the demand of customers does not exceed the capacity of each vehicle, where uk represents the capacity of vehicle k. The constraint (9) mandates that each customer is served by exactly one vehicle. The constraint (10) guarantees that each vehicle must start and end its route at the depot. The constraints (11) and (12) ensure that each customer is served exactly once. The constraint (13) defines the waiting and service times of the depot, with wi and vi denoting the waiting and service-"}, {"title": "IV. SOLUTIONS AND ALGORITHMS", "content": "Considering the complexity of multiobjective vehicle rout-ing problem with time windows (MOVRPTW), particularly in scenarios involving a large number of customers, existing scheduling methods seem insufficient to effectively solve the problem. Therefore, this section introduces an innovative ap-proach that amalgamates a weight-aware deep reinforcement learning (WADRL) methodology with the non-dominated sort-ing genetic algorithm-II (NSGA-II). We design this hybrid method to tackle the challenges posed by MOVRPTW. Ini-tially, the WADRL algorithm is employed to generate the initial population for the NSGA-II algorithm. Subsequently, the NSGA-II algorithm undertakes the task of optimizing this initial population, thus enhancing the solution's efficacy.\nA. General Framework\nIn traditional deep reinforcement learning (DRL) methods for solving multiobjective optimization problems, as illustrated in Fig. 2, a biobjective optimization problem is decomposed into N subproblems according to N weight combinations. For the solution of each subproblem, the training of a DRL model is required. Consequently, a total of N DRL models need to be trained to solve the biobjective optimization problem [20]. This approach appears to be effective for solving biobjective optimization problems. However, as the number of objective functions increases, the time required to solve the entire mul-tiobjective optimization problem will increase exponentially.\nConsidering the shortcomings of the existing DRL methods to solve multiobjective optimization problems, our proposed algorithm takes a different approach. Similarly, we decom-pose the entire multiobjective optimization problem into N subproblems based on N weight combinations. During the training process of DRL, the algorithm randomly selects a weight combination for training each time. Furthermore, during the testing phase, the algorithm generates the corre-sponding optimal solutions for these N subproblems. This approach enables a single DRL model to solve the entire multiobjective optimization problem, significantly reducing the model training time, particularly for many-objective optimiza-tion problems. The WADRL framework is drawn in Fig. 3, in which a novel transformer architecture is also adopted.\nIn the original NSGA-II algorithm, the initial population is typically generated through a random process, and if the solution does not satisfy the constraints, it will be regenerated. In MOVRPTW, it is obviously impossible to generate solutions that satisfy constraints through a completely random method. While there exist methods to expedite the initial solution generation, they are still relatively inefficient [25], and the quality of these initial solutions is generally low. In our proposed algorithm, solutions generated by WADRL are used as the initial solutions for the NSGA-II algorithm. NSGA-II is then applied to optimize these solutions. The advantage of this approach is that the initial solutions generated by WADRL are guaranteed to satisfy the constraints. Furthermore, the generation of these initial solutions adheres to a carefully designed decomposition strategy. This approach guarantees a relatively uniform distribution across the Pareto front of the initial solutions. Importantly, this strategy not only facilitates coverage but also significantly enhances the quality of the initial solutions.\nB. The Markov Decision Process of MOVRPTW in Weight-aware Deep Reinforcement Learning\nGiven the proficiency of reinforcement learning in man-aging sequential decision-making problems, we employ deep reinforcement learning to address the multiobjective vehicle routing problem with time windows (MOVRPTW). In this approach, MOVRPTW is modeled as a markov decision process (MDP), characterized by specific components: state S, action A, state transition function, and reward R. These elements are defined as follows.\n1) State: At any given step t, the state of the system encompasses three primary components: the vehicle state vt, customer state ct, and weight state wt, defined as follows. The vehicle state vt is characterized by two dynamic attributes: its load, denoted as ut, and the traveled time, symbolized as Tt. The term 'dynamic' implies that these states may change as actions occur. The customer states encompass static states, including soft and hard time windows e, l, E and L, coordinate c, and service time v. Static states indicate that these states will not change with the occurrence of actions. The dynamic state of a customer is demand d. The weight state is static, signifying the weights of two objective functions w1 and w2.\n2) Action: The action at each step t, denoted as at, repre-sents the next vertex to which the vehicle will travel. The entire-"}, {"title": "C. The Policy network in Weight-aware Deep Reinforcement Learning", "content": "In our system model, various information needs to be con-sidered simultaneously, such as customer coordinates, demand, time windows, and the weights of the objective functions. Therefore, learning-based methods need to be carefully de-signed to process the information. However, simple neural networks or learning strategies often struggle to handle the complex information mentioned above. Meanwhile, trans-former architecture, as a type of self-attention mechanism, has been proven to perform well in many fields, including natural language processing (NLP) [26], computer vision (CV) [27], and recommender systems [28]. With the development of transformer technology, it has found wide applications in deep reinforcement learning, displaying excellence in problems like path planning [7], the knapsack problem [29], and reservoir operation [30]. Its advantage is that the attention mechanism in the transformer can effectively extract information through key-value-query maps.\nIn this paper, we employ the transformer architecture [31] to model the delivering agent for solving MOVRPTW, as illustrated in Fig. 3. The model primarily comprises an encoder module, a weight embedding module, and a decoder module [31]. In the encoder process, the encoder embeds the original information of MOVRPTW into a high-dimensional space and utilizes a self-attention mechanism to extract features of the problem. In the weight embedding process, the embedding module encapsulates the state of vehicles and the weights associated with the two objectives of MOVRPTW. In the decoder process, the decoder generates vertex probabilities based on contextual information.\n1) The information encoder process: Initially, the encoder employs a linear layer to transform the features of the vertices (including both the depot and customers) into a high-dimensional space. This transformation results in the generation of initial embedding information, represented as h^(0) = {h^0_1, h^0_2,...,h^0_i}, where h^(0)_i corresponds to the initial embedded representation of vertex i. We employ separate linear projections to handle distinct information of warehouse and customer vertices, as follows [32]:\n{\n W_a [c_0, L_0 ] + b_0, if i = 0\n W_c [c_i, E_i, e_i, l_i, L_i, d_i, v_i ] + b_c, if i \u2260 0,\nh^(0)_i =\n(22)\nwhere the depot vertex only needs to embed coordinate and latest arrival time information, while the customer vertices need to embed coordinate, soft and hard time window, de-mand, and service time information. The operation [., ., . . ., \u00b7] concatenates the information of the same dimension together. W_a , b_0 , W_c and b_c are trainable linear projection parameters, and W_a \u2208 R^(dh\u00d72) , b_0 \u2208 R^(dh) , W_c \u2208 R^(dh\u00d78) , b_c \u2208 R^(dh) , where dh represents the dimension of h^(0). In our algorithm dh = 128.\nSubsequent to the initial embedding, the encoded infor-mation undergoes processing through L identical layers to yield the final embedding. Each of these layers is com-posed of several key components: multi-head attention layer (MHA), skip connection layer, batch normalization (BN) layer,-"}, {"title": "V. RESULTS AND ANALYSIS", "content": "To demonstrate the advantages of our proposed approach that combines weight-aware deep reinforcement learning (WADRL) and non-dominated sorting genetic algorithm-II (NSGA-II), we apply it to the Solomon dataset [34]. Further-more, we conduct comparative studies on datasets with varying\n{\n1, if node i is masked at step t\n0, otherwise.\nmask^t_i =\n(36)\nFinally, the probability vector for vertex selection is gen-erated by combining the compatibility vector between the vehicle and the vertices:\np_t = softmax (x + O \u00b7 mask^t), (37)\nwhere O is a large negative number, e.g., O = -999999, which means that the customer vertex cannot be visited.\n4) Model training: We employ a policy gradient method to train all parameters \u03b8 within the neural network. The algorithm comprises two networks, namely the policy network and the baseline network. Both the policy and baseline networks share the same network structure, with the only difference being that the policy network selects actions based on sampling from the probability vector, while the baseline network adopts a strategy where actions are selected based on the action possessing the highest probability, namely 'greedy policy'. The gradient of the loss function is defined as follows [32]\n\u2207_\u03b8L(\u03b8) = \u0395_(p_\u03b8(\u03c0_\u03b8))[(R(\u03c0_\u03b8 ) \u2212 R(\u03c0^(BL))) \u2207_\u03b8 log p_\u03b8(\u03c0_\u03b8)], (38)\nwhere R(\u03c0_\u03b8 ) and R(\u03c0^(BL) ) are utilized to denote the rewards accrued by the policy network and the baseline network, respectively. During each training batch, a t-test is employed to ascertain the statistical significance of the difference in performance between these two networks at a 95% confidence level. Should this test yield a significant result, it prompts an update wherein the parameters of the policy network supersede those of the baseline network, thereby optimizing the learning process. The detailed procedural steps of this algorithm are given in Algorithm 1."}, {"title": "A. General Framework", "content": "In traditional deep reinforcement learning (DRL) methods for solving multiobjective optimization problems, as illustrated in Fig. 2, a biobjective optimization problem is decomposed into N subproblems according to N weight combinations. For the solution of each subproblem, the training of a DRL model is required. Consequently, a total of N DRL models need to be trained to solve the biobjective optimization problem [20].\nThis approach appears to be effective for solving biobjective optimization problems. However, as the number of objective functions increases, the time required to solve the entire mul-tiobjective optimization problem will increase exponentially.\nConsidering the shortcomings of the existing DRL methods to solve multiobjective optimization problems, our proposed algorithm takes a different approach. Similarly, we decom-pose the entire multiobjective optimization problem into N subproblems based on N weight combinations. During the training process of DRL, the algorithm randomly selects a weight combination for training each time. Furthermore, during the testing phase, the algorithm generates the corre-sponding optimal solutions for these N subproblems. This approach enables a single DRL model to solve the entire multiobjective optimization problem, significantly reducing the model training time, particularly for many-objective optimiza-tion problems. The WADRL framework is drawn in Fig. 3, in which a novel transformer architecture is also adopted.\nIn the original NSGA-II algorithm, the initial population is typically generated through a random process, and if the solution does not satisfy the constraints, it will be regenerated. In MOVRPTW, it is obviously impossible to generate solutions that satisfy constraints through a completely random method. While there exist methods to expedite the initial solution generation, they are still relatively inefficient [25], and the quality of these initial solutions is generally low. In our proposed algorithm, solutions generated by WADRL are used as the initial solutions for the NSGA-II algorithm. NSGA-II is then applied to optimize these solutions. The advantage of this approach is that the initial solutions generated by WADRL are guaranteed to satisfy the constraints. Furthermore, the generation of these initial solutions adheres to a carefully designed decomposition strategy. This approach guarantees a relatively uniform distribution across the Pareto front of the initial solutions. Importantly, this strategy not only facilitates coverage but also significantly enhances the quality of the initial solutions.\nB. The Markov Decision Process of MOVRPTW in Weight-aware Deep Reinforcement Learning\nGiven the proficiency of reinforcement learning in man-aging sequential decision-making problems, we employ deep reinforcement learning to address the multiobjective vehicle routing problem with time windows (MOVRPTW). In this approach, MOVRPTW is modeled as a markov decision process (MDP), characterized by specific components: state S, action A, state transition function, and reward R. These elements are defined as follows.\n1) State: At any given step t, the state of the system encompasses three primary components: the vehicle state vt, customer state ct, and weight state wt, defined as follows. The vehicle state vt is characterized by two dynamic attributes: its load, denoted as ut, and the traveled time, symbolized as Tt. The term 'dynamic' implies that these states may change as actions occur. The customer states encompass static states, including soft and hard time windows e, l, E and L, coordinate c, and service time v. Static states indicate that these states will not change with the occurrence of actions. The dynamic state of a customer is demand d. The weight state is static, signifying the weights of two objective functions w1 and w2.\n2) Action: The action at each step t, denoted as at, repre-sents the next vertex to which the vehicle will travel. The entire-"}, {"title": "C. The Policy network in Weight-aware Deep Reinforcement Learning", "content": "In our system model, various information needs to be con-sidered simultaneously, such as customer coordinates, demand, time windows, and the weights of the objective functions. Therefore, learning-based methods need to be carefully de-signed to process the information. However, simple neural networks or learning strategies often struggle to handle the complex information mentioned above. Meanwhile, trans-former architecture, as a type of self-attention mechanism, has been proven to perform well in many fields, including natural language processing (NLP) [26], computer vision (CV) [27], and recommender systems [28]. With the development of transformer technology, it has found wide applications in deep reinforcement learning, displaying excellence in problems like path planning [7], the knapsack problem [29], and reservoir operation [30]. Its advantage is that the attention mechanism in the transformer can effectively extract information through key-value-query maps.\nIn this paper, we employ the transformer architecture [31] to model the delivering agent for solving MOVRPTW, as illustrated in Fig. 3. The model primarily comprises an encoder module, a weight embedding module, and a decoder module [31]. In the encoder process, the encoder embeds the original information of MOVRPTW into a high-dimensional space and utilizes a self-attention mechanism to extract features of the problem. In the weight embedding process, the embedding module encapsulates the state of vehicles and the weights associated with the two objectives of MOVRPTW. In the decoder process, the decoder generates vertex probabilities based on contextual information.\n1) The information encoder process: Initially, the encoder employs a linear layer to transform the features of the vertices (including both the depot and customers) into a high-dimensional space. This transformation results in the generation of initial embedding information, represented as h^(0) = {h^0_1, h^0_2,...,h^0_i}, where h^(0)_i corresponds to the initial embedded representation of vertex i. We employ separate linear projections to handle distinct information of warehouse and customer vertices, as follows [32]:\n{\n W_a [c_0, L_0 ] + b_0, if i = 0\n W_c [c_i, E_i, e_i, l_i, L_i, d_i, v_i ] + b_c, if i \u2260 0,\nh^(0)_i =\n(22)\nwhere the depot vertex only needs to embed coordinate and latest arrival time information, while the customer vertices need to embed coordinate, soft and hard time window, de-mand, and service time information. The operation [., ., . . ., \u00b7] concatenates the information of the same dimension together. W_a , b_0 , W_c and b_c are trainable linear projection parameters, and W_a \u2208 R^(dh\u00d72) , b_0 \u2208 R^(dh) , W_c \u2208 R^(dh\u00d78) , b_c \u2208 R^(dh) , where dh represents the dimension of h^(0). In our algorithm dh = 128.\nSubsequent to the initial embedding, the encoded infor-mation undergoes processing through L identical layers to yield the final embedding. Each of these layers is com-posed of several key components: multi-head attention layer (MHA), skip connection layer, batch normalization (BN) layer,-"}, {"title": "V. RESULTS AND ANALYSIS", "content": "To demonstrate the advantages of our proposed approach that combines weight-aware deep reinforcement learning (WADRL) and non-dominated sorting genetic algorithm-II (NSGA-II), we apply it to the Solomon dataset [34]. Further-more, we conduct comparative studies on datasets with varying\n{\n1, if node i is masked at step t\n0, otherwise.\nmask^t_i =\n(36)\nFinally, the probability vector for vertex selection is gen-erated by combining the compatibility vector between the vehicle and the vertices:\np_t = softmax (x + O \u00b7 mask^t), (37)\nwhere O is a large negative number, e.g., O = -999999, which means that the customer vertex cannot be visited.\n4) Model training: We employ a policy gradient method to train all parameters \u03b8 within the neural network. The algorithm comprises two networks, namely the policy network and the baseline network. Both the policy and baseline networks share the same network structure, with the only difference being that the policy network selects actions based on sampling from the probability vector, while the baseline network adopts a strategy where actions are selected based on the action possessing the highest probability, namely 'greedy policy'. The gradient of the loss function is defined as follows [32]\n\u2207_\u03b8L(\u03b8) = \u0395_(p_\u03b8(\u03c0_\u03b8))[(R(\u03c0_\u03b8 ) \u2212 R(\u03c0^(BL))) \u2207_\u03b8 log p_\u03b8(\u03c0_\u03b8)], (38)\nwhere R(\u03c0_\u03b8 ) and R(\u03c0^(BL) ) are utilized to denote the rewards accrued by the policy network and the baseline network, respectively. During each training batch, a t-test is employed to ascertain the statistical significance of the difference in performance between these two networks at a 95% confidence level. Should this test yield a significant result, it prompts an update wherein the parameters of the policy network supersede those of the baseline network, thereby optimizing the learning process. The detailed procedural steps of this algorithm are given in Algorithm 1."}, {"title": "D. Combine Weight-Aware Deep Reinforcement Learning with NSGA-II", "content": "In the pursuit of solving the MOVRPTW, the application of WADRL may reveal promising results. However, several lim-itations still exist: \u2460 The neural network sometimes struggles to iterate optimally, falling into a local optimal solution; \u2461 Its performance may exhibit instability under specific objective weight combinations; \u2462 Most of the obtained solutions may be dominated by other solutions.\nTo address these limitations, we propose the approach that combines WADRL with NSGA-II. Our approach in-volves utilizing WADRL to generate initial solutions for the MOVRPTW. These initial solutions serve as the foundation for subsequent optimization through NSGA-II. WADRL en-sures that the initial solutions are of high quality, meet all constraints, and have a relatively uniform distribution due to the applied decomposition strategy.\nThis combined approach is designed to improve the overall optimization process, enhance solution quality, and mitigate the inherent limitations of WADRL. The integration of NSGA-II complements the strengths of WADRL, providing a more robust and effective solution strategy for MOVRPTW. The process of NSGA-II optimizing the solution generated by WADRL is shown in Fig. 4."}, {"title": "V. RESULTS AND ANALYSIS", "content": "To demonstrate the advantages of our proposed approach that combines weight-aware deep reinforcement learning (WADRL) and non-dominated sorting genetic algorithm-II (NSGA-II)", "34": ".", "sets": "The model is trained on instances with 20-", "1,40": ".", "100": ".", "240": ".", "set": "For the 20-customer instance", "35": ".", "settings": "In MOVRPTW, the travel cost of vehicle k per unit distance and fixed cost generated by using vehicle, denoted as k (c_k^1 , c_k^2), are set to be fixed, as described in (2.0, 400) [36", "37": ".", "0.00": [0.98, 0.02], "1.00": "resulting in a total of 51 subproblems. Furthermore, we configure the population size of the NSGA-II algorithm to also be 51.\nB. Comparison of Initial Solutions\nWe first compare the performance of our method and the original NSGA-II in generating initial solutions, as shown in Fig. 5. The results indicate that our method consistently outperforms the traditional NSGA-II algorithm in terms of the quality of initial solutions, across instances with 20-, 50-, 100-customer. Furthermore, the average execution time together with the average objective function value of the initial solution are presented in TABLE III.\nAs shown in Fig. 5, all initial solutions generated by the WADRL-based NSGA-II outperform the initial solutions generated by the original NSGA-II using random policies, both in terms of objective functions and time efficiency. This improvement is particularly pronounced as the number of customers increases, with our method exhibiting more signifi-cant enhancements over the traditional NSGA-II approach. For example, in the case of the 100-customer instance, our method, on average, reduces the cost by 4008.6, increases customer satisfaction by"}]}