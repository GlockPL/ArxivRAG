{"title": "A Collection of Question Answering Datasets for Norwegian", "authors": ["Vladislav Mikhailov", "Petter M\u00e6hlum", "Victoria Ovedie Chruickshank Lang\u00f8", "Erik Velldal", "Lilja \u00d8vrelid"], "abstract": "This paper introduces a new suite of question answering datasets for Norwegian; NorOpenBookQA, NorCommonSenseQA, NorTruthfulQA, and NRK-Quiz-QA. The data covers a wide range of skills and knowledge domains, including world knowledge, commonsense reasoning, truthfulness, and knowledge about Norway. Covering both of the written standards of Norwegian \u2013 Bokm\u00e5l and Nynorsk \u2013 our datasets comprise over 10k question-answer pairs, created by native speakers. We detail our dataset creation approach and present the results of evaluating 11 language models (LMs) in zero- and few-shot regimes. Most LMs perform better in Bokm\u00e5l than Nynorsk, struggle most with commonsense reasoning, and are often untruthful in generating answers to questions. All our datasets and annotation materials are publicly available.", "sections": [{"title": "Introduction", "content": "An essential part of developing language models (LMs) is benchmarking \u2013 i.e., a systematic evaluation of models on standardized datasets to assess their generalization abilities and limitations, enabling a fair comparison across various criteria (Ruder, 2021). One of the well-established benchmarking areas is question answering (QA), which tests the LM's ability to apply knowledge acquired from diverse domains to answer user questions (Kwiatkowski et al., 2019; Hendrycks et al., 2021; Zhong et al., 2024).\nWhile there is a rich ecosystem of QA resources for typologically diverse languages (Rogers et al., 2023), a significant gap remains for lesser-resourced languages (Joshi et al., 2020), including Norwegian. Existing Norwegian QA datasets primarily focus on the machine reading comprehension task, limiting the evaluation scope of LM's abilities in Norwegian language understanding and generation (Ivanova et al., 2023; Bandarkar et al., 2024; Liu et al., 2024). Furthermore, prior work relies on English-to-Norwegian machine translation as the dataset creation method (Liu et al., 2024), which fails to capture the linguistic nuances and aspects of history, geography, and culture that are relevant to the end user. To the best of our knowledge, no single dataset covers both official written standards of the Norwegian language: Bokm\u00e5l (NB) and Nynorsk (NN; the minority variant).\nTo address this gap, we introduce four new QA datasets in both Norwegian NB and NN: NorOpenBookQA\u00b9, NorCommonSenseQA\u00b2, NorTruthfulQA\u00b3,\u2074, and NRK-Quiz-QA\u2075. Our datasets are designed to evaluate the LM's Norwegian-specific & world knowledge, commonsense reasoning abilities, and truthfulness in the form of multiple-choice and free-form QA. The 10.5k question-answer pairs are created by a team of native Norwegian speakers through manual translation and localization of English-oriented datasets - OpenBookQA (Mihaylov et al., 2018), CommonSenseQA (Talmor et al., 2019), and TruthfulQA (Lin et al., 2022) \u2013 with a dedicated effort to also create novel Norwegian-specific examples from scratch. NRK-Quiz-QA comprises examples from more than 500 quizzes published by NRK, the national public broadcaster in Norway.\nOur main contributions are summarized as follows: (i) we create a collection of four QA datasets that target the least addressed QA directions for"}, {"title": "Related Work", "content": "The design of QA datasets differs based on how the answer is formulated and which evidence is required to answer the question (Rogers et al., 2023)."}, {"title": "Standard Design of QA Datasets", "content": "Answer Format There are several standard answer formats which correspond to different QA task formulations. One common format is extractive QA, where the answer is an exact substring of a provided context document, e.g., SQuAD-style (Rajpurkar et al., 2016, 2018) datasets in various languages (d'Hoffschmidt et al., 2020; M\u00f6ller et al., 2021; So et al., 2022; Lim et al., 2019; Efimov et al., 2020). Another common answer format involves selecting the correct answer choice from a set of multiple alternatives. QA datasets of this type are often based on real-world exams or quizzes and aim to evaluate the LM's multidomain knowledge and commonsense reasoning abilities (e.g., OpenBookQA, CommonsenseQA, and MMLU; Hendrycks et al., 2021). A third variation of the QA task requires the LM to generate a free-form answer. These datasets are often based on naturally occurring web queries (e.g., Natural\nQuestions; Kwiatkowski et al., 2019) and human-written questions (e.g., TruthfulQA).\nAnswer Evidence QA datasets feature various types of answer evidence provided to the LM. Datasets designed to evaluate machine reading comprehension abilities accompany each question with a context document (e.g., SQuAD) or a collection of context documents (e.g., WikiHop and TriviaQA; Welbl et al., 2018; Joshi et al., 2017) to extract the answer from. Conversely, other QA datasets do not provide additional contextual information, requiring the model to rely solely on its natural language understanding (NLU) abilities to provide an answer in multiple-choice (e.g., MMLU, OpenBookQA and CommonSenseQA) or free-form formats (TruthfulQA). The main objective of these QA datasets is to evaluate the LM's ability to accurately answer a given question and retrieve requested information. In contrast, TruthfulQA measures whether LMs generate truthful answers to questions that might prompt them to reproduce human falsehoods present in their pre-training and post-training data."}, {"title": "Norwegian QA Datasets", "content": "NorQuAD (Ivanova et al., 2023) focuses on extractive QA and represents the first Norwegian QA dataset created from scratch by two native Norwegian speakers. Each of its 4.7k question-"}, {"title": "Datasets", "content": "This section outlines our approach to adapting and localizing English-oriented QA resources to the specific contexts of Norwegian society, culture, and knowledge. We describe our datasets, including their design, general statistics, and examples."}, {"title": "Annotation Design", "content": "We conduct a two-stage in-house annotation to create NorOpenBookQA, NorCommonSenseQA,\nand NortruthfulQA (see \u00a73.1.1), followed by a separate stage for curating NRK-Quiz-QA (see \u00a73.1.2). Each stage includes training and main annotation phases. Our annotation team consists of 21 BA/BSc and MA/MSc students in linguistics and computer science, all native Norwegian speakers. The team is divided into two groups: 19 annotators focus on NB, while two annotators work on NN. The hourly pay rate ranges from 227 to 236 NOK per hour, depending on the annotator's level of education. We hold a joint seminar describing the annotation project. Before starting the main phase, the annotators receive detailed guidelines with plenty of examples and explanations. Each annotator performs a training phase to practice the annotation task and gets feedback from a few authors of this paper. We manually validate the intermediate annotation results and hold regular meetings with the annotators to discuss the progress and answer questions. Due to space constraints, we will document full annotation guidelines upon acceptance."}, {"title": "Adaptation of English Datasets", "content": "We ask our annotators to study the previous works on OpenBookQA (Mihaylov et al., 2018), CommonSenseQA (Talmor et al., 2019), and TruthfulQA (Lin et al., 2022) to learn more about the design. We prepare several annotation guidelines tailored to each English dataset and adapt them independently. Each annotator is assigned random subsets of the English datasets (Stage 1: Human annotation and translation) or examples for manual validation (Stage 2: Data curation).\nStage 1: Human Annotation and Translation\nThe annotation task here involves adapting the English examples from OpenBookQA, CommonSenseQA, and TruthfulQA using two strategies.\nManual translation and localization: The annotators manually translate the original examples, with localization that reflects Norwegian contexts where necessary.\nCreative adaptation: The annotators create new examples in NB and NN from scratch, drawing inspiration from the shown English examples.\nStage 2: Data Curation This stage aims to filter out low-quality examples collected during the first"}, {"title": "Adaptation of NRK Quiz Data", "content": "Our NRK-Quiz-QA dataset is based on a collection of quizzes from between the years of 2017 and 2024, provided by NRK. The quiz data is of high quality, but we perform a targeted adaptation to ensure correct time references. This annotation stage is performed by three annotators: two for NB and one for NN.\nTemporal adjustment: The annotators adjust temporal references to fit the current time.\nContent filtering: The annotators discard examples requiring images or sounds for answering.\nData cleaning: The annotators remove unnecessary text segments (e.g., web page artifacts), and irrelevant content in the questions (e.g., comments that guide the user through the quiz)."}, {"title": "NorOpenBookQA", "content": "NorOpenBookQA is designed to evaluate the LM's world knowledge. NorOpenBookQA counts 3.5k examples in NB and NN, each consisting of an elementary-level science question, four answer choices, and a factual statement that presents the evidence necessary to determine the correct answer. Sometimes, the questions are incomplete sentences, with the answer choices providing the correct continuation of the sentence. Below is an example of an English question \u201cWhich is likely considered soft?\u201d that is both translated and localized with regards to the two food items."}, {"title": "NorCommonsenseQA", "content": "NorCommonsenseQA is developed to assess the LM's commonsense reasoning abilities. It includes 1.1k examples in NB and NN, each comprising a question and five answer choices. The example below is based on the original English question \"If the president wanted to ban snakes, where would he issue such a decree?\u201d In this translation, the main content is the same, but the president is swapped with the prime minister, as Norway does not have a president, and two of the five alternatives are also localized, as options D and E were originally \"New Mexico\u201d and \u201cThe White House\u201d."}, {"title": "NorTruthfulQA", "content": "NorTruthfulQA aims to assess whether an LM generates or selects answers that convey false beliefs or misconceptions. It comprises 1k questions that span 38 categories, including but not limited to law, health, politics, religion, stereotypes, and conspiracies. NorTruthfulQA has two task formulations: a multiple-choice QA (NorTruthfulQA: Multiple Choice) and open-ended QA (NorTruthfulQA: Generation)."}, {"title": "NRK-Quiz-QA", "content": "NRK-Quiz-QA allows for evaluation of the LM's Norwegian-specific and world knowledge. NRK-Quiz-QA includes 4.9k examples in NB and NN"}, {"title": "Dataset Statistics & Analysis", "content": "General Statistics summarizes the general statistics for each dataset by NB and NN: the number of examples, the average token length of questions and answers, and the number of unique wordforms. The average number of tokens in the questions ranges from 10.50 (NorOpenBookQA) to 18.78 (NRK-Quiz-QA) for NB and 9.61 (NorTruthfulQA) to 18.60 (NRK-Quiz-QA) for NN. On average, there are 1.90\u20139.50 and 2.77-9.44 tokens in answer choices for NB and NN, respectively. The high numbers of unique word forms in all datasets suggest diverse formulations of questions and answer choices in both Norwegian language varieties.\nSplits All datasets are designed as zero-shot evaluation test sets, except for NorOpenBookQA. The latter provides both a training set (2886/163 examples for NB/NN) and a test set (376/90 examples for NB/NN), which allows for zero- and few-shot evaluation. The split choice is based on the following factors: (i) technical properties of the source NRK quiz data do not allow for a stratified sampling to promote a balanced distribution"}, {"title": "Experimental Setup", "content": "Language Models We evaluate 11 pretrained decoder-only LMs of varying sizes publicly available in Transformers (Wolf et al., 2020): NorGLM (NorLlama-3B\u00b9\u2070 and NorGPT-3B\u00b9\u00b9; Liu et al., 2024), NorwAI-Mistral-7B-pretrain,\u00b9\u00b2 NorwAI-Mistral-7B,\u00b9\u00b3 NorwAI-Llama2-7B,\u00b9\u2074 Viking-7B,\u00b9\u2075 Viking-13B,\u00b9\u2076 NORA.LLM (NorBLOOM-7B-scratch,\u00b9\u2077 NorMistral-7B-scratch,\u00b9\u2078 and NorMistral-7B-warm;\u00b9\u2079 Samuel et al., 2025), and Mistral-7B\u00b2\u2070 (Jiang et al., 2023).\nMethod We utilize NorEval,\u00b2\u00b9 a framework for evaluating Norwegian generative LMs built on"}, {"title": "Results", "content": "This section describes our empirical evaluation results, which are summarized in ; fine-grained results for each task, LM, and prompt can be found in our GitHub repository.\u00b2\u00b2 Overall, we observe that no single LM performs best on all datasets, which suggests that the LMs' behavior varies depending on the Norwegian language variety, QA category, and the k-shot regime. Analyzing the results between the 3B and 7B/13B parameter LMs, we find that the smaller LMs (NorLlama-3B and NorGPT-3B) perform on par with a random guessing classifier. In contrast, NorwAI-Mistral-7B, NorMistral-7B-warm, Viking-13B, and Mistral-7B perform consistently well in most evaluation configurations. Notably, Mistral-7B performs best on NorTruthfulQA Multiple-choice and NorOpenBookQA, which we attribute to strong cross-lingual generalization abilities due to the high quality of the pre-training corpus. Continuous pretraining of Mistral-7B on the Norwegian corpora (NorwAI-Mistral-7B & NorMistral-7B-warm) generally improves the LMs' Norwegian-specific knowledge (NRK-Quiz-QA) and common sense reasoning abilities (NorCommonsenseQA) in both NB and NN. Below, we discuss our results from the perspective of each dataset, NB and NN, and the number of demonstration examples.\nMost LMs Perform Better in NB Most LMs perform better in NB than NN on all datasets except for NRK-Quiz-QA and NorTruthfulQA Generation. The accuracy d-scores range from 5% to 8% on NorCommonSenseQA (e.g., NorwAI-Mistral-7B-pretrain and Mistral-7B) and from 1%"}, {"title": "Conclusion and Future Work", "content": "This paper introduces a collection of four new QA datasets for Norwegian NB and NN created by native speakers and tailored to evaluate the LMs' abilities with respect to the Norwegian language and culture. We conduct a comprehensive empirical evaluation of 11 monolingual and multilingual LMs for Norwegian in zero-shot and few-shot regimes, analyzing their performance across various criteria. Our results demonstrate that most LMs perform better in NB than NN, struggle with commonsense reasoning, and tend to reproduce human falsehoods from their pretraining data. Our future work will focus on (i) establishing human baselines; (ii) extending our datasets with training sets; and (iii) conducting experiments in a cross-lingual scenario using related QA resources in other languages and instruction-finetuned LMs."}, {"title": "Limitations", "content": "Annotation Design The data curation stage is a standard practice to ensure the high quality of annotated data. Due to limited resources, we curate only 80% of all 10.5k collected examples, with each example validated by one annotator. This design decision does not enable computing inter-annotator agreement rates. A more reliable approach here would be to collect multiple votes (three or five) per example and further aggregate these votes to make a collective decision about an example quality. Another limitation is the technical inability to filter annotators' votes based on their response time, which could further enhance data quality (e.g., Karpinska et al., 2021).\nLack of Human Baseline Human-level performance serves as an upper bound in NLP benchmarking, allowing to track progress in the field and"}, {"title": "Ethical Considerations", "content": "Data Annotation The annotators' submissions are stored anonymously. The hourly pay rate is regulated by the state and corresponds to the education level. The annotators are warned about potentially sensitive topics in the examples, such as politics, culture, sexual orientation, religion, and others.\nUse of AI-assistants We use Grammarly\u00b2\u00b3 to correct grammar, spelling, and phrasing errors.\nTransparency & License We release our datasets under the MIT license following standard open-source research practices. Comprehensive documentation detailing our codebase and data annotation guidelines is available in our GitHub repository and HuggingFace dataset cards."}]}