{"title": "RESQUE: Quantifying Estimator to Task and Distribution Shift for Sustainable Model Reusability", "authors": ["Vishwesh Sangarya", "Jung-Eun Kim"], "abstract": "As a strategy for sustainability of deep learning, reusing an existing model by retraining it rather than training a new model from scratch is critical. In this paper, we propose REpresenta-tion Shift QUantifying Estimator (RESQUE), a predictive quantifier to estimate the retraining cost of a model to distributional shifts or change of tasks. It provides a single concise index for an estimate of resources required for re-training the model. Through extensive experiments, we show that RESQUE has a strong correlation with various retraining measures. Our results validate that RESQUE is an effective indicator in terms of epochs, gradient norms, changes of parameter magnitude, energy, and carbon emissions. These measures align well with RESQUE for new tasks, multi-ple noise types, and varying noise intensities. As a result, RESQUE enables users to make informed decisions for re-training to different tasks/distribution shifts and determine the most cost-effective and sustainable option, allowing for the reuse of a model with a much smaller footprint in the environment. The code for this work is available here: https://github.com/JEKimLab/AAAI2025RESQUE", "sections": [{"title": "Introduction", "content": "With the rapid and ever increasing use of deep learning mod-els in everyday applications, a crucial aspect that is often overlooked is the adaptation of a model to changes and new data. Models need to be adaptive and capable of learning from these new inputs. This dynamic adaptation is essen-tial for maintaining model performance and relevance over time. Particularly, the continuous series of changes a model may need to undergo subject to distributional shifts, or need for primary task updates. For example, new data with bet-ter relevant features may be introduced, the environment of deployment may undergo a change leading to distribution shift of the input data, or the prediction classes and task may change. Retraining helps the model remain effective with satisfactory performance for the desired deployed ap-plication while shortening the adaptation time. While new models can be developed from scratch when the data under-goes a distribution shift or the task changes, leveraging ex-isting learned features is a resource-efficient option. Reusing models not only saves time and resources but also aligns with the principles of sustainable AI development. This is of primary importance with the overwhelmingly increas-ing deployments of AI models in the current era, which in turn has given rise to the socially important field of Green AI (Schwartz et al. 2019; Wu et al. 2022; Verdecchia, Sal-lou, and Cruz 2023; Salehi and Schmeink 2024). It is not uncommon for models to have several billion parameters (Dehghani et al. 2023; Menghani 2023), and as these mod-els grow in complexity, their computational demands also increase. With the increased computational requirements, there is an accompanying need for resources required to per-form the computation and byproducts of the computation to deal with. These demands place significant strain on avail-able resources and highlight the importance of addressing both the direct and indirect impacts of AI model use. Conse-quently, there exists a need for sustainable and efficient de-velopment of AI models so as to reduce the environmental impact, especially when looking at the carbon footprint (Ali et al. 2023; Strubell, Ganesh, and McCallum 2019; Bannour et al. 2021; Wu et al. 2024) and energy consumption (Dodge et al. 2022; Strubell, Ganesh, and McCallum 2019; Ali et al. 2023; Wu et al. 2024) involved in the research, evolution, and utilization of AI models.\nWith the goal of reusing models and reducing com-putation, energy consumption, and carbon emissions, we propose a novel estimating index called the REpresenta-tion Shift QUantifying Estimator (RESQUE). This estima-tor provides a single index to predict the cost of retraining a model for new distributions or tasks before any compu-tation is performed. RESQUE enables deep learning practi-tioners and researchers to quickly estimate the costs asso-ciated with adapting a model, offering valuable insights for informed decision-making. This quantifiable estimator helps them achieve sustainability goals when using and develop-ing the models. For the context of distribution shifts, the es-timator referred to as RESQUEdist, measures the shift in the model's representation outputs between the original and new distribution. To obtain RESQUEdist, the input data is propa-gated through the model just once with no backward propa-gation or computation. For the case of a new target task, the estimator, specifically termed as RESQUEtask, quantifies the separation in class decision boundary of the new target task in the representation space, using the original task model."}, {"title": "Related Work", "content": "Sustainable computing and Green AI (Schwartz et al. 2019; Verdecchia, Sallou, and Cruz 2023; Wu et al. 2022; Salehi and Schmeink 2024) are gaining more and more attention due to their potential impacts on the modern AI era of large models. As the demand for Al grows (Liu, Liu, and Lee 2024; Wu et al. 2024), the need for efficient and sustainable computing practices has become important, leading to an important research domain. (Verdecchia, Sallou, and Cruz 2023; Wu et al. 2024) perform a detailed study regarding the potential quantification of undesirable effects of AI, they highlight the importance of reporting energy consumption and carbon emissions as key sustainability measures. Other studies (Wu et al. 2022; Schwartz et al. 2019; Salehi and Schmeink 2024) bring forth the need for a sustainable and greener development of AI, while highlighting the environ-mental consequences of neglecting the impact of inconsid-erate use of resources.\n(Georgiou et al. 2022; Strubell, Ganesh, and McCallum 2019) conduct detailed studies regarding energy usage of neural networks. In particular (Strubell, Ganesh, and Mc-Callum 2019; Bannour et al. 2021; Georgiou et al. 2022) explore the increasing energy demands of neural networks from the perspective of computing power, operational costs, and training time. Along with energy consumption, (Hender-son et al. 2020; Patterson et al. 2021) highlight the absence of carbon emissions reporting associated with deep learn-ing research. Besides these studies, other works such as (Xu et al. 2023; Garc\u00eda-Mart\u00edn et al. 2019; McDonald et al. 2022) discuss the environmental impact of training models from a carbon emissions perspective. To aid in tracking carbon emissions and energy usage associated with deep learning development, (Schmidt et al. 2021; Anthony, Kanding, and Selvan 2020; Lacoste et al. 2019) propose useful quantify-ing strategies, including frameworks and libraries which can accurately log all energy consumption and resulting carbon emissions from training. (Dodge et al. 2022) demonstrate that training Vision Transformers (Dosovitskiy et al. 2021) results in significantly higher carbon emissions than convo-lutional networks due to their greater energy requirements from extensive training characteristics. On a similar stream, (Strubell, Ganesh, and McCallum 2019) assess the substan-tial energy consumption of transformer-based models.\nChanges in data due to varying input distributions can arise due to various factors, as elaborated in (Hendrycks and Dietterich 2019; Arjovsky et al. 2020; Hendrycks et al. 2021). To adapt to the specific distributional changes, aug-mentation strategies (Hendrycks et al. 2020; Liu et al. 2022; Zhang et al. 2018; Kim, Choo, and Song 2020; Lee et al. 2020) were effective to a limited extent. Although these techniques improve performance compared to a standard trained model against certain types of shifts, they are com-putationally costly and require training models from scratch. Moreover, the performance improvements are for a limited number of shifts in the distributions, while providing no ben-efits or a drop in performance for other types of distribution shifts. Studies such as (Geirhos et al. 2020; Yin et al. 2020; Ford et al. 2019) detail the non-uniform performance gains, where improvements in performance cannot be controlled and may result in lower than acceptable performance at the end of training. Adaptation during test time, such as (Lim et al. 2023; Niu et al. 2022; Goyal et al. 2022; Wang et al. 2022), addresses the computational cost concerns by having additional tuned components that can be added to the model. However, these approaches heavily rely on batch data and tend to provide uncertain and low-confidence outputs when there is a greater variation of distribution shifts in the input.\nGradients play a crucial role in evaluating the complexity and difficulty of learning when training models, as demon-strated in (Agarwal, D'souza, and Hooker 2022; Lee and AlRegib 2020; Huang, Geng, and Li 2021). These studies tell us that difficult samples produce greater gradients and thus require larger computation costs for convergence. (San-garya, Bradford, and Kim 2024, 2023) provide measures for evaluating distribution shifts when a shifted input sample is only derived from a corresponding original sample, which is not a realistic scenario. In particular, (Sangarya, Bradford, and Kim 2023) only operates on individual shifts without the ability to evaluate multiple types of shifts simultaneously. Moreover, both studies only focused on shifts in data but did not take into account the practical scenario where the task itself undergoes a change. Additionally, studies such as (Stacke et al. 2020) measure changes in layer-wise represen-tations to analyze different shifts in pathology data.\nAdjusted Rand Index (Hubert and Arabie 1985), is a use-ful metric for comparing clustering algorithms. However, it can also be used to assess the performance of super-vised classification and feature selection as demonstrated by (Santos and Embrechts 2009). With regard to the case of task change, we refer to research in various subfields of deep learning, such as (Ravi and Larochelle 2016; Oreshkin, Rodr\u00edguez L\u00f3pez, and Lacoste 2018; Caruana 1997), which defines a new task as any change in the class data or the introduction of completely new classes of associated data. (Achille et al. 2019) is designed to measure task similar-ity and perform an evaluation of pretrained models. However, adapting to a new task from a current task model re-"}, {"title": "RESQUE", "content": "In this section, we provide details of RESQUEdist for distri-butional shifts and RESQUEtask for change of task."}, {"title": "Distribution Shift", "content": "In this scenario, we consider the case when a model needs to be updated and adapted as it saw a distributional shift in data - for example, because some ground truths might be changed, some data samples might become stale, or some new data samples might need to come into play, etc. We ex-plain how to obtain and use RESQUEdist, which is a predic-tive quantifier for model retraining to distributional shifts. It quantifies the distance between representations of the orig-inal and the shifted distribution so as to determine how fa-miliar or foreign the distribution is."}, {"title": "Normalized Embedding Vectors for Distributions", "content": "For a realistic scenario where a distribution-shifted sample has no corresponding clean image, or the number of distributed shifted samples may not be equal to the number of origi-nal clean samples, we make use of all the samples to gen-erate representation embedding vectors. For the case of dis-tribution shifts, since there is no change in the original task and therefore no change in the number of classes, we obtain the class-wise representation embedding vectors for each class individually. To obtain the data representations, we use the output representation from the final convolutional layer in convolutional layers and the final dense layer in Vision Transformers. We perform a forward pass of each dataset and obtain a summed embedding representation vector of each class as follows,\n$V_{l,sum}^{D} = \\sum_{i=0}^{n_l} V_{l,i}^{D}$"}, {"title": "RESQUEdist from Normalized Embeddings", "content": "The final representation angle is obtained by averaging the inverse co-sine angle between each class' normed representation vector for the original dataset and the distribution-shifted dataset. Let O represent the original dataset, S represent the distri-bution shifted dataset, and k represent the number of classes. RESQUEdist is then obtained as,\n$RESQUE_{dist} = \\frac{\\sum_{k=0}^{K} arccos \\left(V_{l,norm}^{O}, V_{l,norm}^{S}\\right)}{k}$"}, {"title": "Change of Task", "content": "For the first, we consider a case in which a model is required to be updated for application toward a new target task. We show how a model can be retrained and reused to meet such a requirement. To characterize the resources and cost of re-training to a new task, we make use of the separation of classes within the latent space of representations. The sepa-ration of classes in the representation latent space is an ef-fective gauge of the model's ability to provide accurate out-puts. We relate this to the distance between decision bound-aries for classes within the representation space, where dis-tinct boundaries result in correct and confident predictions, whereas class boundaries that overlap or are close to each other lead to low confidence and incorrect predictions. We use this trait of deep learning models to evaluate how well a model's current learned features and representation space translate into learning a new task."}, {"title": "Quantifying Class Separation from Known Task Data to New Task Data", "content": "We use the representation of the data produced by the model and a clustering algorithm to assign cluster labels for each data sample in the representation la-tent space. Using the assigned cluster labels, we derive our estimator, RESQUEtask, by applying the Adjusted Rand In-dex (Hubert and Arabie 1985). RESQUEtask can quantify the effectiveness of class separation by using the cluster labels and the true data labels. To obtain the cluster labels, we first retrain the original model with the new task just for a single epoch. This creates a representation that is primarily of the original task but incorporates features of the new task and updates the output classes of the model to match the new task classes. Following this step, the new task data is used to perform a forward pass and assign a label, which is a repre-sentation label, obtained via clustering on all samples."}, {"title": "Clustering Mechanism for the New Unknown Task", "content": "The data representation is obtained from the final convolution layer for a convolutional network' case while from the fi-nal dense layer in the last transformer encoder block for a"}, {"title": "Adjusted Rand Index to Quantify Class Separation", "content": "Adjusted Rand Index takes the value 0 for purely random clustering, and 1 for identical clustering. For our estima-tor, it is required to have a low value for decision bound-aries which are well separated and high value for boundaries which overlap and result in incorrect representation cluster labels. Hence, for our estimator, RESQUEtask, we take the complement of Adjusted Rand Index and define it in Eq. (4). In Eq. (4), tl represents the total count of true labels for each label in the contingency table of true labels vs. representa-tion labels via clustering. rc represents the summed values of representation cluster labels in the contingency table. A contingency table in this scenario is a matrix that summa-rizes the number of samples belonging to the same cluster or having the same label in both clustering scenarios. Here, by 'both clustering scenarios,' we refer to the representation-based clustering labels and the true labels. nc is the number of cluster labels, which is equal to the number of class labels. ni,j represents the value in each entry of the contingency ta-ble, which is common to both cluster labels for a given label i and j. ns is the total number of samples.\nThe overall formulation RESQUEtask is as follows,\n$RESQUE_{task} = 1- \\frac{2 \\left[ \\sum_{i=0}^{n_c} \\binom{t l_i}{2} + \\sum_{i=0}^{n_c} \\binom{r c_i}{2} \\right] - \\left[ \\sum_{i=0}^{n_c} \\sum_{j=0}^{n_c} \\binom{n_{i,j}}{2} \\right]}{\\binom{n_s}{2}} $"}, {"title": "Retraining Measures", "content": "Retraining measures refer to the resources and costs ex-pended when a model is adapted to a new task or dis-tribution. We demonstrate that RESQUE is strongly corre-lated with these measures and serves as an effective estima-tor. Using RESQUE, users can estimate the resource expen-diture needed for retraining the models. We evaluate sev-eral measures including carbon emissions, energy consump-"}, {"title": "Distribution Shift", "content": "In this section, we assess RESQUEdist in the context of distri-butional shifts using three datasets : CIFAR10 (Krizhevsky, Hinton et al. 2009), CIFAR100 (Krizhevsky, Hinton et al. 2009), and SVHN (Netzer et al. 2011). We utilize 3 types of noises - Gaussian noise, Image Blur, and Salt-Pepper noise, which represent various real world noises that can occur due to hardware changes, environmental factors, or artifacts in the data, respectively. For each noise type, we generate 10 levels of noise intensity, with level 1 corresponding to minimal noise and level 10 aligning with severity 4 as de-scribed in (Hendrycks and Dietterich 2019). To represent the reusability of pre-existing models, we initially train a randomly initialized model on the original data distribution until it achieves the minimum required accuracy for each dataset for fair comparisons. While tuning hyper-parameters for higher noise levels could expedite convergence, it may hinder comparisons across different noise types and levels. Therefore, we maintain consistent hyper-parameters across all experiments for consistency.\nIt is important to note that, for the initial model training on original data without distributional shifts, we utilize 70% of the entire dataset. For retraining to data with distributional shifts, we utilize 50% of the dataset with added noise. The 20% excess is the overlap data that was common in the orig-inal distribution and the new distribution shifted data.\nFig. 1 illustrates the retraining measures for VGG16 trained from scratch versus retrained on SVHN under vary-ing intensities of Gaussian noise. The results clearly show that retraining a model requires significantly fewer resources than training from scratch. All retraining measures for re-training are substantially lower compared to those for train-ing a new model, highlighting the efficiency of retraining to distribution shifts.\nFor evaluating RESQUEdist as an estimator for the retrain-ing measures, we evaluate convolutional networks and vi-sion transformers, retrained to various noise types and lev-els on different datasets. We train and retrain ResNet18 (He et al. 2016) on CIFAR100, VGG16 (Simonyan 2014) on SVHN, and ViT (Dosovitskiy et al. 2021) on CIFAR10. For the Vision Transformer, we utilize a ViT model with a patch size of 4, comprising 8 transformer blocks, a latent vector size of 512, 8 attention heads, and an MLP with a hidden layer size of 1024.\nFigure 2 displays the correlation between RESQUEdist and the retraining measures, for all three models on the three datasets. Across various datasets and architectures, RESQUEdist consistently aligns with the retraining measures for different types of distribution shifts. Furthermore, Ta-ble 1 provides the Pearson and Spearman correlation coef-ficients, along with associated p-values, between RESQUE and all the retraining measures, demonstrating a strong cor-relation between RESQUEdist and the retraining measures."}, {"title": "Task Change", "content": "We evaluate the effectiveness of RESQUE to estimate re-sources for learning a new task from an original task. First, we compare training a model to a target task from scratch vs. retraining a model from the original task. For training from scratch, we randomly initialize a new model and train it on the target task. We set a common cutoff accuracy for a fair"}, {"title": "Experiments Across Different Original Tasks", "content": "We show how well RESQUEtask and the retraining measures are aligned for different original tasks. The original tasks evalu-ated in Fig. 4 are trained on CIFAR10 (Krizhevsky, Hinton et al. 2009), CIFAR100 (Krizhevsky, Hinton et al. 2009), EMNIST (Cohen et al. 2017), Fashion MNIST (Xiao, Ra-sul, and Vollgraf 2017), Food101 (Bossard, Guillaumin, and Van Gool 2014), GTSRB (Stallkamp et al. 2012), MNIST (LeCun et al. 1998), and SVHN (Netzer et al. 2011). We use ResNet18 and ViT-B/16 (Dosovitskiy et al. 2021), and per-form clustering to obtain RESQUEtask using the last convolu-tional layer of ResNet18, and the final dense layer in the last transformer encoder block of the ViT. During training and retraining, images are resized to 224x224 for ViT-B/16 and to 32x32 for ResNet18. To achieve better performance and to ensure consistent training conditions across all new target tasks, we retrain all layers of the model, which provides bet-ter accuracy performance than retraining only the final few layers as outlined in (Deng et al. 2023).\nFig. 4 depicts the relation between RESQUEtask and all the retraining measures for ResNet and ViT on different target tasks. For the target tasks, CIFAR10 and GTSRB, we use multiple cutoff accuracies to evaluate how learning differs from the early stage to the later stage. From the figure, it is evident that RESQUEtask aligns with the retraining measures and has a strong correlation.\nA lower value of RESQUEtask indicates that the model will require fewer resources to converge. For the target tasks, CIFAR10 and GTSRB, during the early learning stage, the trends exhibited are linear for all the cases of the original tasks. However, as training progresses, the number of epochs required by models with higher RESQUEtask increases sub-stantially. We relate this to the difficulty of learning and the usefulness of task and feature similarity.\nWe see that, in an original task model with aligned and well-learned initial features, the increase in epochs from low accuracy to high cutoff accuracy is smaller. As opposed to that, for a model with poorly learned initial features, there is a substantial increase in epochs from low accuracy to high cutoff accuracy.\nThis is due to the fact that learning becomes progressively more difficult for models with less aligned or poorly learned features. This implies two crucial learning challenges - not"}, {"title": "Experiments Across Different Target Tasks", "content": "RESQUEtask can also accurately estimate the performance an original task model can achieve on different new target tasks. This can help profile and categorize task similarity and provide use-ful information for retraining a current model in future in-stances. For retraining to different target tasks, we retrain the original task model for a fixed number of epochs across all new target tasks. Fig. 5 illustrates the relation between RESQUEtask and the highest test accuracy reached on each of the new task datasets (original task is CIFAR10). Using RESQUEtask, we obtain an accurate estimation regarding the peak performance a model can attain on the new task."}, {"title": "Conclusion", "content": "We introduced a novel metric, RESQUE, to estimate the vari-ous resources that would be expended when reusing a model by adapting to distributional shifts or retraining to new target tasks. We validate the effectiveness of RESQUE on CNNs and ViTs. RESQUE is shown to be an effective estimator when retraining to various distributional shifts. RESQUE is also utilized to estimate resources and performance when retraining to different target tasks. It is evaluated on differ-ent original tasks that are retrained to new target tasks. All the results consistently validate that RESQUE is an effective estimator of various retraining measures, including energy consumption and carbon emission, enabling sustainable de-cisions with regard to model reusability."}, {"title": "Appendix", "content": "Hardware and software setup\nAll experiments were carried out on NVIDIA RTX GPUs, specifically utilizing the NVIDIA RTX 2060, 2070, 2080, 3060Ti, 4060Ti, and A100 models. For measuring carbon emissions and energy consumption, all experiments were run on RTX 2060 node. The codebase was developed and run on machines with Ubuntu 20.4 OS. Library and frame-work versions are submitted in the requirements file of the codebase."}, {"title": "Hyperparameters", "content": "The ResNet and VGG models were trained using the Adam optimizer, while the ViT model was trained with the SGD optimizer. For training the original models on the clean dis-tribution, an initial learning rate of 0.001 was set for ResNet and VGG, and 0.01 for ViT, with the ViT learning rate be-ing reduced by a factor of 10 after the 70th epoch. To mit-igate overfitting, L2 regularization with a weight decay of 0.0001 was applied. Additionally, image augmentations, in-cluding random horizontal flip, random rotation, random affine transformation, color jitter, and normalization, were performed. When retraining the model to distribution shifted data or new task, the initial learning rate for models opti-mized using the Adam optimizer was set to 0.0001, and it was reduced by a factor of 10 after the 40th, 70th, and 90th epoch. A similar learning rate schedule was used for models optimized using SGD optimizer, but the initial learning rate for retraining was set to 0.001. For retraining to new distri-butions and new tasks, the L-2 regularization term was set to le 05, and similar image augmentation schemes as clean training was utilized.\nFor setting the hyperparameters, other values of learning rates starting from 0.1 to 0.0001 were explored, as well as L-2 regularization term with value 0.001, and different image augmentation magnitudes. We performed a single seed run with lowered target accuracy to find the most optimal learn-ing rates and tuned it based on the actual target accuracy and training duration. For image augmentations, random hori-zontal flip with probability of 50%, random rotation between 25\u00b0to 45\u00b0, random affine of 20\u00b0and color jitter value of 0.1 was set. All images are normalized using a mean and stan-dard deviation of 0.5, across all channels."}]}