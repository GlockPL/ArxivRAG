{"title": "Role Play: Learning Adaptive Role-Specific Strategies in Multi-Agent Interactions", "authors": ["Weifan Long", "Wen Wen", "Peng Zhai", "Lihua Zhang"], "abstract": "Zero-shot coordination problem in multi-agent reinforcement learning (MARL), which requires agents to adapt to unseen agents, has attracted increasing attention. Traditional approaches often rely on the Self-Play (SP) framework to generate a diverse set of policies in a policy pool, which serves to improve the generalization capability of the final agent. However, these frameworks may struggle to capture the full spectrum of potential strategies, especially in real-world scenarios that demand agents balance cooperation with competition. In such settings, agents need strategies that can adapt to varying and often conflicting goals. Drawing inspiration from Social Value Orientation (SVO)\u2014where individuals maintain stable value orientations during interactions with others\u2014we propose a novel framework called Role Play (RP). RP employs role embeddings to transform the challenge of policy diversity into a more manageable diversity of roles. It trains a common policy with role embeddings observation and employ a role predictor to estimate the joint role embeddings of other agents, helping the learning agent adapt to its assigned role. We theoretically prove that an approximate optimal policy can be achieved by optimizing the expected cumulative reward relative to an approximate role-based policy. Experimental results in both cooperative (Overcooked) and mixed-motive games (Harvest, CleanUp) reveal that RP consistently outperforms strong baselines when interacting with unseen agents, highlighting its robustness and adaptability in complex environments.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) has achieved remarkable success in mastering a wide range of strategic and competitive games, as demonstrated by notable research in this area (Silver et al., 2018; Vinyals et al., 2019; Berner et al., 2019). Significant advancements have also been made in cooperative settings, where agents are trained to collaborate with either humans or other agents to achieve common goals (Carroll et al., 2019; Zhao et al., 2023). However, in real-world applications such as autonomous driving, interactions among agents often display mixed motives, combining elements of both cooperation and competition (Schwarting et al., 2019). In these mixed-motive environments, agents face complex interactions where each participant has distinct objectives. For example, in a public goods game, agents must carefully balance the benefits of contributing to a collective resource against the costs of their individual contributions (Gibbons et al., 1992). These environments present significant challenges, requiring agents to develop sophisticated adaptation strategies to effectively interact with others who have varying incentives.\nZero-shot coordination is well-recognized in the context of multi-agent reinforcement learning (MARL), particularly for agents that need to interact effectively with new partners they have not encountered during training (Hu et al., 2020). Self play (SP) is an effective framework for this challenge (Lucas and Allen, 2022; Lupu et al., 2021; Zhao et al., 2023). The SP-based frameworks typically build a policy pool through SP, utilized to enhance the generalization capabilities of the final agent. Various techniques aim to increase the diversity of this policy pool to improve the agent's ability to generalize across different scenarios (Garnelo et al., 2021; Zhao et al., 2023). In mixed- motive games, there is a greater need for policy diversity to adapt to the varying and often conflicting goals resulting from the imperfect alignment of incentives among group members. However, the policy pool, primarily composed of past iterations of policies from a given population, captures only a limited range of the policy space. This limitation can prevent agents from effectively managing novel situations or policies not previously encountered in the training set.\nUnlike existing policy pool based works, our approach try to develop a general model which can generate policies with different value orientation. Given the inherent challenges in representing policies directly due to their complexity, we propose projecting the policy space into a more compact dimension. Inspired by Social Value Orientation (SVO) (McKee et al., 2020), in which individuals maintain stable value orientations (roles) in interactions with others, we proposed Role Play (RP), which compress the vast MARL policy space into a more manageable \u201chuman role space.\" This simplification aims to improve both the interpretability and efficiency of agent interactions. Fur- thermore, drawing on social intuition (Lieberman, 2000; Jellema et al., 2024) that humans estimate the behaviors of others during interactions to make better decisions, we introduce a role predictor to estimate the joint role embeddings of other agents, aiding the learning agent in adapting to its assigned role. This setup enables agents to learn and adapt to their assigned roles more effectively, enhancing their performance across various interactive scenarios.\nIn this work, we introduce a novel framework, Role Play (RP), specifically designed to address the zero-shot coordination problem in multi-agent interactions. Our approach is distinguished by several key innovations:\n\u2022 Role Embedding: We utilize a sophisticated reward mapping function to project the extensive policy space into a more manageable role embedding space. This transformation facilitates structured and strategic navigation through the complex landscape of agent behaviors. We theoretically prove that an approximate optimal policy can be obtained by optimizing the expected cumulative reward with respect to an approximate role-based policy.\n\u2022 Role Predictor: Inspired by social intuition, we have developed a role predictor that estimates the joint role embeddings of other agents. This module enhances the agent's ability to accurately predict and adapt to the role-based policies of other agents, enabling the learning agent to adapt more effectively to its assigned role.\n\u2022 Meta-task Learning: We employ meta-learning techniques to model agent interactions as meta-tasks, which allows the learning agent to extrapolate from limited experiences to new, unseen scenarios. This approach significantly improve the adaptability of the learning agent to different roles and strategies.\nThese innovations collectively enhance the capability of agents to adapt and perform in complex multi-agent environments, establishing RP as a robust solution to zero-shot coordination challenges in MARL. To gain a deeper understanding of our framework and explore additional visualizations, we invite readers to visit our project website, where more detailed results are provided\u00b2."}, {"title": "2 Related works", "content": "A significant body of research focuses on enhancing the zero-shot coordination of MARL agents when interacting with unfamiliar partners (Kirk et al., 2021). Most existing methods that aim to improve generalization across diverse agent interactions utilize the SP framework (Lanctot et al., 2017; Bai et al., 2020), typically employing a policy pool to train agents in adapting to varied"}, {"title": "3 Preliminary", "content": "In this work, we study a role-based Markov game, which is represented by $\\mathcal{M} = (m, S, A, P, R, O, \\psi, z, \\gamma)$. Here, $m$ denotes the number of agents, and $S$ represents a finite set of states. The joint action space is given by $A = \\prod_{i=1}^m A^i$, where $A^i$ is the action space for agent $i$. The transition function $P : S \\times A \\times S \\rightarrow [0, 1]$ defines the probability of transitioning from one state to another given a joint action. Each agent $i$ has a specific reward function $R^i : S \\times A \\rightarrow \\mathbb{R}$, and the collection of these functions forms the joint reward function $R = \\{R^i | i = 1,...,m\\}$."}, {"title": "4 Method", "content": "Role play (RP) is designed to train a single policy which can adapt all roles in role space, enabling agents to dynamically adapt their strategies based on their assigned roles. Different with existing policy-pool-based methods, RP transforms the policy diversity challenges into more manageable diversity of role representations. The overall framework is illustrated in Fig. 2."}, {"title": "4.1 Role embedding", "content": "In RP framework, each agent i is assigned a randomly sampled role embedding $z^i$ in every trial and learns to effectively adapt to and play this role. The challenge lies in defining the role space and enabling agents to adapt efficiently to their assigned roles. To address this, we employ a reward shaping method and introduce a reward feature mapping function $\\psi(r^i, z^i)$ that processes the reward $r^i$ and the role embedding $z^i$ to generate a new reward value for agent $i$. This function $\\psi(r^i, z^i)$ is specifically designed to capture the role-specific reward information, which is crucial for the agent to adapt to its role. During training, agents interact with a variety of other randomly sampled roles, optimizing their strategies based on these interactions. Consequently, if the role space is sufficiently diverse to adequately represent the entire policy space adequately, agents are equipped to effectively handle interactions with previously unseen agents.\nEach agent i takes action without communications, based on its observation $o^i$ and role embedding $z^i$, receiving individual rewards $r^i$. Its goal is to maximize its expected cumulative discount reward\n$J(\\pi) = \\mathbb{E}_{z^i, z^{-i} \\sim Z}[J(\\pi(z^i), \\pi(z^{-i}))],$ (3)\nwhere $J(\\pi(z^i), \\pi^{-i}(z^{-i}))$ denotes the expected sum reward achieved by agent $i$ using policy $\\pi(z^i)$ while other agents use policies $\\pi^{-i}(z^{-i})$. The detailed form of $J(\\pi(z^i), \\pi(z^{-i}))$ is given by\n$J(\\pi(z^i), \\pi(z^{-i})) = \\mathbb{E}_{s_{t+1} \\sim P(s_t, a^i, a^{-i}) \\atop a^i \\sim \\pi(z^i), a^{-i} \\sim \\pi^{-i}(. \\mid o^{-i}, z^{-i})} [\\sum_{t=0}^{\\infty} \\gamma^t \\psi(R(s_t, a, a^{-i}), z^i)].$ (4)\nGiven the vast size of the policy space, it is challenging for the mapped role space $Z$ to fully represent the entire policy space one-to-one. Inevitably, this reduction results in some loss of information. To address this issue, we analyze the adaptability with respect to missing strategies using an $\\epsilon$-close (Ko, 2006; Zhao et al., 2023) approximation."}, {"title": "Definition 4.1.", "content": "(Zhao et al., 2023) We define that a random policy $\\pi'$ is $\\epsilon$-close to policy $\\pi(z')$ at observation $o_t$ if\n$\\frac{\\pi'(a' \\mid o_t)}{\\pi(a \\mid o_t, z')} - 1 < \\epsilon$ (5)\nfor all $a' \\in A$. If this condition is satisfied at every $o_t \\in O$, we call $\\pi'$ is $\\epsilon$-close to $\\pi(z')$."}, {"title": "Theorem 4.1.", "content": "If the random policy $\\pi'$ is $\\epsilon$-close to the role policy $\\pi(z')$, we can derive the following theorem."}, {"title": "4.2 Role predictor", "content": "We aim for the learning agent to accurately predict the joint role embeddings of other agents. To achieve this, we introduce a role predictor $q_\\phi$, which is trained to predict the joint role embeddings of other agents, denoted $\\hat{z}^{-i}$, based on the history of the observation $o_{0:t}$ and its own role embedding $z^i$ at time step $t$. Specifically, the prediction model is formulated as:\n$\\hat{z}^{-i} = \\arg \\max_{z^{-i}} q_{\\phi}(z^{-i} | o_{0:t}, z^i).$ (7)\nThis model architecture enables the agent to integrate observational history and predefined role to infer the joint role of other agents within the environment, enhancing its predictive accuracy and adaptability.\nUtilizing the role predictor $q_\\phi$ and theorem 4.1, the learning agent optimizes its policy with the role embedding $z^i$ by maximizing its expected cumulative discount reward\n$J(\\pi(z^i), \\pi(z^{-i})) \\approx J(\\pi(z^i, \\hat{z}^{-i})) = \\mathbb{E}_{s_{t+1} \\sim P(s_t, a^i, a^{-i}) \\atop a^i \\sim \\pi(o_t, z^i, \\hat{z}^{-i})} [\\sum_{t=0}^{\\infty} \\gamma^t \\psi(R(s_t, a_t, a_t^i), z^i)]$ (8)\nOnce the learning agent is assigned a role, it approximates the dynamics of the MARL problem into a unified single-agent RL framework and optimizes its policy independently.\nUp to this point, the learning agent can adaptively interact with any unseen policies for a given role, assuming that both the policy and the role predictor are effectively trained. Unfortunately, the vast space of joint policies from other agents presents significant challenges in terms of generalization and stability."}, {"title": "4.3 Modeling role interactions as meta-tasks", "content": "To address the challenges of generalization and stability highlighted in the preceding discussion, we introduce meta-learning techniques (Duan et al., 2016; Finn et al., 2017; Fakoor et al., 2019) into RP. Meta-learning enables agents to generalize from a limited set of experiences to new, unseen scenarios effectively. In our approach, we model the interactions between the learning agent and other agents as meta-tasks, utilizing meta-learning techniques to develop the learning agent's policy.\nWe treat the role embeddings of other agents as context variables that encapsulate relevant information about the task environment. These role embeddings provide the contextual backdrop for the meta- learning process, offering a rich representation of the dynamic interactions within the multi-agent system. The sampling method of role embedding generates a probability distribution over tasks. Thus, we view the entire learning process as meta-learning tasks, where the agent must adapt to various roles in the environment, guided by the context provided by the role embeddings."}, {"title": "4.4 Role play framework", "content": "In this subsection, we summarize the RP framework and provide detailed insights into its practical training phase. The algorithm is delineated in Algorithm 1.\nIn practical training, we employ the typically meta-learning algorithm, RL2 (Duan et al., 2016), to optimize the policy, as illustrated in Fig. 3. Throughout the iterative process, the algorithm dynamically assigns role embeddings $z^i$ to agents, enabling them to predict joint role embeddings of other agents $\\hat{z}^{-i}$. This facilitates action selection $a^i$ based on these roles and observed interactions. Rewards are subsequently mapped using the defined feature function $\\psi$. The role predictor $q_\\phi$ is updated by minimizing the prediction error between the predicted role embedding and the true role embedding. The learning agent is trained to maximize the expected cumulative mapped reward by interacting with other agents."}, {"title": "5 Experiments", "content": "In this study, we initially assess the effectiveness of RP using the cooperative two-player game Overcooked (Carroll et al., 2019), which is the common benchmark for evaluating the collaboration ability of agents (Zhao et al., 2023; Yu et al., 2023; Rahman et al., 2023). We further assess its performance in two-player mixed-motive games, such as Harvest and CleanUp, where agents need to balance cooperation with competition. In these settings, agents must develop strategies that navigate conflicting goals to maximize both individual and collective outcomes. All subsequent internal analysis experiments are conducted within more complex mixed-motive environments. Ablation studies are conducted to verify the effectiveness of the role predictor and the meta-learning methods employed. Role behavior analysis is performed to evaluate the adaptability of the learning agent to different roles. Finally, we analyze performance of the role predictor in predicting the role embeddings of other agents.\nBaselines. We compare the performance of our proposed RP method against several baseline strategies that represent a range of common and state-of-the-art approaches in zero-shot coordination challenge. These include Trajectory Diversity (TrajeDi) (Lupu et al., 2021), AnyPlay (Lucas and Allen, 2022), Best-Response Diversity (BRDiv) (Rahman et al., 2023) and Hidden-Utility Self-Play (HSP) (Yu et al., 2023). Each of these methods follows a two-stage process: generating a policy pool and then training a final policy. Notably, while all baseline methods train 16 policies in the first stage to ensure sufficient diversity and adaptability, RP achieves comparable or superior performance by training a single policy in a streamlined, single-stage process, significantly reducing both computational overhead and complexity."}, {"title": "5.1 Zero Shot Results on Cooperative Games", "content": "Overcooked (Carroll et al., 2019) is a two-player cooperative game intended to test the collaboration ability of agents. In this game, agents work collaboratively to fulfill soup orders using ingredients like onions and tomatoes. Agents can move and interact with items, such as grabbing or serving soups, based on the game state. To complete an order, agents must combine the correct ingredients in a pot, cook them for a specified time, and then serve the soup with a dish to earn rewards. Each order has distinct cooking times and rewards.\nExperimental setup. In Overcooked, we employ an event-based reward map function, which is a sparse reward function that rewards agents for some events happening. We select certain events already implemented in Overcooked and assign three preferences levels-hate, neutral and like-for each event which will lead to a negative, zero and positive reward. The reward shaping function is defined as $\\psi(r^i, z^i) = r^i + \\sum_k (z * E_k)$, where $z \\in [-1, 0, 1]$ is the preference of event k, and $E_k$ is the reward of event k. Details of the experimental settings in Overcooked are provided in Appendix B.1."}, {"title": "5.2 Zero Shot Results on Mixed-Motive Games", "content": "In this subsection, we evaluate the performance of the RP framework in two-player mixed-motive games, Harvest and CleanUp (Hughes et al., 2018; Leibo et al., 2021), which require agents to balance individual contributions against collective benefits. These games introduce a higher level of complexity due to inherent conflicting interests, providing a challenging test environment for agents. To clarify the mixed-motive game settings, we provide detailed explanations of these games.\nHarvest: agents face a common-pool resource dilemma where apple resources regenerate more effectively when left unharvested in groups. Agents can choose to harvest aggressively, risking future availability, or sustainably, by leaving some apples to promote long-term regeneration. Additionally, agents can use beams to penalize others, causing the penalized agent to temporarily disappear from the game for a few steps.\nCleanUp: a public goods game in which apple growth in an orchard is hindered by rising pollution levels in a nearby river. When pollution is high, apple growth stops entirely. Agents can reduce pollution by leaving the orchard to work in polluted areas, highlighting the importance of individual efforts in maintaining shared resources. Agents are also able to use beams to penalize others.\nExperimental setup. We implemented the scenarios using the MeltingPot framework (Leibo et al., 2021) and applied SVO (Murphy and Ackermann, 2014) to categorize agent roles. Utilizing the ring formulation of SVO, we introduced SVO-based (role-based) rewards as intrinsic motivators for agents to learn role-specific strategies. The reward feature mapping function $\\psi$ is defined as:\n$\\psi(r, z^i) = w * r^i + (1 - w) * |cos(z^i)r^i + sin(z^i)r^{-i}|,$ (9)\nwhere $r^i$ denotes the individual reward for agent i, $r^{-i}$ the average reward of other agents, and $w$ a hyperparameter adjusting the impact of role-based rewards. We use eight main roles $\\{z^i = \\frac{k \\pi}{4} | k = -4, ..., 3\\}$ for training, as introduced in 3.\nIn this experiment, we evaluate performance across three tasks for each game. Each task involves a policy pretrained with a specific reward function, with the learning agent interacting with this pretrained agent to assess performance. The tasks are:\n\u2022 Selfish Agent: the agent pretrained with its own reward function.\n\u2022 Prosocial Agent: the agent pretrained with the collective reward function.\n\u2022 Inequity-Averse Agent: the agent pretrained with the inequity-averse reward function, as described in (Hughes et al., 2018)."}, {"title": "5.3 Ablation Studies", "content": "In this subsection, we examine the impact of the role predictor and the meta-learning method on the performance of the RP framework through comprehensive ablation studies. We assess the contributions of each component by systematically removing the role predictor and the meta-learning method. We analyze interactions by having each role engage separately with all eight distinct roles. For each unique role pairing, agents interact over a series of 1000 episodes. Given the considerable variation in rewards associated with different roles, we employ the mean episode reward of each role as a metric for assessing performance."}, {"title": "5.4 Role Behavior Analysis", "content": "In our analysis of the RP framework, we investigated the principal role behaviors exhibited in the Harvest and CleanUp games. Experimental setting is the same with 5.3. Fig. 9 visualizes our findings for four main roles that reflect real-life behavioral patterns: Competitive ($z = -1$), Individualistic ($z = 0$), Prosocial ($z = \\frac{\\pi}{4}$), and Altruistic ($z = \\frac{\\pi}{2}$).\nThe Individualistic role, marked by a significantly larger area on the radar chart, suggests an advan- tageous position in interpersonal dynamics, a phenomenon supported by findings in individualism studies (Triandis, 2018). In contrast, the Competitive role, adept at penalizing others and monop- olizing resources such as apples, frequently triggers retaliatory actions from other agents, often culminating in considerable negative rewards. The radar chart shows a large number of blank spaces in the lower half, indicating frequent failures in interactions with antisocial roles.\nIn the more competitive Harvest game, the Altruistic role demonstrates vulnerabilities as it primarily focuses on the welfare of others, often to its own detriment. Meanwhile, the Prosocial role, which aims to harmonize self-interest with the collective good, shows robust performance in this environments by fostering optimal group outcomes.\nConversely, in the CleanUp game, the Prosocial and Altruistic roles engage in environmentally beneficial behaviors, such as pollution removal. While these actions yield lower direct rewards for themselves, they facilitate greater social welfare for other agents. However, roles driven by self-interest, such as the Competitive and Individualistic roles, often reap benefits from these altruistic actions without bearing any of the associated costs. Additionally, when interacting with the more self-sacrificing Martyr role, both the Prosocial and Altruistic roles experience enhanced social welfare.\nOverall, the role behavior analysis demonstrates the adaptability of the RP framework across diverse roles, with each role maintaining its distinct SVO."}, {"title": "5.5 Prediction Analysis", "content": "In the RP framework, the role predictor is utilized to estimate the joint role embeddings of other agents. Fig. 10 displays the prediction outcomes of the role predictor $q(\\phi)$ within both the Harvest and CleanUp games. Each heatmap represents the prediction results of a role interacting with all roles."}, {"title": "6 Conclusion", "content": "In this study, we introduced a novel framework called Role Play (RP) to enable agents to adapt varying roles, effectively addressing the zero-shot coordination challenge in MARL. Drawing inspiration from social intuition, RP incorporates a role predictor that estimates the joint role embeddings of other agents, which in turn facilitates the adaptation of the learning agent's role based on these predictions. We demonstrated the effectiveness of RP across both cooperative and mixed-motive environments, where agents must balance individual contributions against collective benefits, achieving comparable performance compared to state-of-the-art baselines.\nHowever, the RP framework is not without its limitations. As the number of agents increases, the complexity of role prediction escalates, potentially challenging the ability of role predictor to accurately forecast the joint role embeddings of other agents. Additionally, the reward feature mapping function $\\psi$ plays a crucial role in the framework, with its selection significantly influencing the learning process.\nFuture work will aim to tackle these challenges by enhancing the accuracy of role prediction and optimizing the choice of the reward feature mapping function. We also plan to extend the RP framework to more complex multi-agent scenarios, further testing its scalability and adaptability."}, {"title": "A Theorem Proof", "content": "Theorem 4.1. For a finite MDP with T time steps and a specific role policy \u03c0(z), if any random policy \u03c0' is \u03f5-close to the role policy \u03c0(z'), then we have\n$\\frac{J(\\pi(z), \\pi'))}{J(\\pi(z), \\pi(z')))} - 1 < \\epsilon T.$ (10)\nProof. We start by examining the trajectory probabilities under policies \u03c0(z) and \u03c0':\n$P_{\\pi(z), \\pi'}(\\tau) = p(S_0) \\prod_{t=0}^{T-1} [\\pi(a_t | O_t, z) \\pi'(a'_t | O_t) p(s_{t+1} | s_t, a_t, a'_t)].$ (11)\nGiven the assumption\n$\\frac{\\pi'(a | O_t)}{\\pi(a | O_t, z)} - 1 < \\epsilon,$ (12)\nwe can approximate the ratio of trajectory probabilities under \u03c0' and \u03c0(z'):\n$\\frac{P_{\\pi(z), \\pi'}(\\tau)}{P_{\\pi(z), \\pi(z')}(\\tau)} - 1 \\approx |\\sum_{t=0}^{T-1} (\\frac{\\pi'(a | O_t)}{\\pi(a | O_t, z)} - 1)| < \\epsilon T.$ (13)\nAnd the expected reward can then be written as:\n$J(\\pi(z), \\pi') = \\sum_\\tau P_{\\pi(z), \\pi'}(\\tau) \\psi (R(\\tau), z),$ (14)\nwhere\n$R(\\tau) = \\sum_t R(s_t, a_t, a'_t),$ (15)\nand \u03c8 is a predefined mapping function. Therefore, we have\n$|\\frac{J(\\pi(z), \\pi'))}{J(\\pi(z), \\pi(z')))} - 1 < \\epsilon T.$ (16)"}, {"title": "B Experimental Details", "content": "The baselines were established using the implementation details provided in the repositories from Rahman et al. (2023) (AnyPlay, TrajeDi, BRDiv) and Yu et al. (2023) (HSP). The network architecture for these baselines is identical to that of the RP framework, which is detailed below. The additional role predictor q\u03c6 for RP is a one-layer fully connected neural network with 64 hidden units and ReLU activation function. We used the OpenAI gym implementation of Overcooked in HSP (Yu et al., 2023) and adapted it for other algorithms. The Harvest and CleanUp games were implemented using the MeltingPot framework (Leibo et al., 2021). Detailed experimental settings for Overcooked, Harvest, and CleanUp are provided in the following subsections. For more details, please refer to our code repository.7"}, {"title": "B.1 Experimental Details of Overcooked", "content": "The reward shaping function in Overcooked is defined as:\n$\\psi(r^i, z^i) = r^i + \\sum_k (z * E_k),$ (17)\nwhere $z \\in [-1, 0, 1]$ is the preference of agent i for event k, and $E_k$ is the reward associated with event k. The reward shaping function is designed to encourage agents to exhibit specific behaviors based on their role preferences. Table 2 shows the event reward we set in Overcooked. HSP uses the same reward shaping function as RP, while AnyPlay, TrajeDi, and BRDiv use the original collective reward directly from the environment."}, {"title": "B.2 Experimental Details in Harvest and CleanUp", "content": "We employ ring formulation of SVO to categorize agent roles. We introduced SVO-based (role-based) rewards as intrinsic motivators for agents to learn role-specific strategies. The reward feature mapping function \u03c8 is defined as:\n$\\psi(r, z^i) = w * r^i + (1 - w) * |cos(z^i)r^i + sin(z^i)r^{-i}|,$ (18)\nwhere $r^i$ denotes the individual reward for agent i, $r^{-i}$ the average reward of other agents, and w a hyperparameter adjusting the impact of role-based rewards. In implementation, we set w = 0.3, and the role embedding $z^i$ is one-hot encoded."}, {"title": "We pre-trained three distinct policies using a naive SP framework and PPO for zero-shot evaluation:", "content": "a selfish agent, a prosocial agent, and an inequity-averse agent (Hughes et al., 2018). Each agent is trained with a different reward function tailored to its role:\n\u2022 Selfish Agent: This agent is trained with the original reward directly from the environment.\n\u2022 Prosocial Agent: This agent is trained using a collective reward that sums the rewards of all agents, formalized as $r_i = \\sum_j r_j$.\n\u2022 Inequity-Averse Agent: This agent's training includes an inequity shaping reward designed to address social fairness, defined by the formula:\n$r_i = r_i - \\frac{\\alpha \\sum_{j \\neq i} max(0, r_j - r_i) + \\beta \\sum_{j \\neq i} max(0, r_i - r_j)}{n - 1}$"}]}