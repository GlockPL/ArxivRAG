{"title": "Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation", "authors": ["Jingjing Xie", "Yuxin Zhang", "Mingbao Lin", "Liujuan Cao", "Rongrong Ji"], "abstract": "This paper presents the first study to explore the potential of parameter quantization for multimodal large language models to alleviate the significant resource constraint encountered during vision-language instruction tuning. We introduce a Quantization-aware Scale LeArning method based on multimodal Warmup, termed QS-LAW. This method is grounded in two key innovations: (1) The learning of group-wise scale factors for quantized LLM weights to mitigate the quantization error arising from activation outliers and achieve more effective vision-language instruction tuning; (2) The implementation of a multimodal warmup that progressively integrates linguistic and multimodal training samples, thereby preventing overfitting of the quantized model to multimodal data while ensuring stable adaptation of multimodal large language models to downstream vision-language tasks. Extensive experiments demonstrate that models quantized by QSLAW perform on par with, or even surpass, their full-precision counterparts, while facilitating up to 1.4 times reduction in VL tuning time and GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW", "sections": [{"title": "1 INTRODUCTION", "content": "The remarkable performance of large language models (LLMs) has been well-established in recent literature [4, 9, 35, 36, 39], sparking a growing interest in the development of multimodal large language models (MLLMs) [2, 3, 5, 24, 28, 32, 42]. This burgeoning field has led to substantial progress in a wide array of vision-language (VL) tasks. To accomplish this, contemporary MLLMs primarily utilize multimodal instruction following examples for VL instruction tuning and adopt modular architectures [2, 21, 24, 28] to transform visual features into the word embedding space of the LLM. This innovative approach enables LLMs to execute multimodal tasks in an autoregressive fashion. One notable example of this technique is LLaVA [24], which employs a linear projection layer to bridge the gap between the visual encoder and the LLM. By doing so, LLaVA fully harnesses the power of pre-trained LLMs, thereby significantly enhancing its visual comprehension capabilities.\nDespite the advancements, the current VL instruction tuning for MLLMs exhibits considerable redundancy in terms of computation and memory burden. This limitation primarily stems from the inherently large size of LLMs compared to other components within MLLM architectures. For instance, LLaVA-13B fully fine-tunes the entire LLM during VL instruction tuning, often requiring hundreds of GPU hours [28]. Although recent efforts have introduced more efficient adapters and the freezing of LLMs to reduce training overheads [15, 32], VL tuning within current MLLM frameworks still demands substantial memory usage and computational resources, necessitating at least 8 NVIDIA Tesla A100 GPUs [32]. This poses great challenges to the rapid adaptation of LLMs for cross-modal tasks, particularly in situations characterized by limited training resources and needs for on-the-fly, task-specific tuning.\nTo address this constraint, this paper explores the potential of parameter quantization for MLLMs, aiming to alleviate the extensive training demands encountered during VL instruction tuning while preserving the original performance. Quantization, a network compression technique, transforms the full-precision weights into low-bit representations, consequently reducing both computational load and storage requirement. It has been adopted for parameter-efficient fine-tuning (PEFT) of LLMs [12, 17, 43], notably"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Model Quantization", "content": "Quantization methods can be broadly classified into quantization-aware training (QAT) and post-training quantization (PTQ). QAT"}, {"title": "2.2 Multimodal Large Language Models", "content": "Traditional VL instruction tuning commonly employs various task-related losses, including image-text contrastive loss, image-text matching loss, and language modeling loss, to supervise the training of both visual and language branches. To compute these losses, it is typically necessary to perform multiple forward passes on the image-text pairs, consuming thousands of GPU hours. However, with the emergence of large language models (LLMs), the paradigm of VL tuning has shifted towards treating LLMs as a universal interface and adopting a modular structure to align representations from vision with LLMs. In these approaches, LLMs and the modular structure are trained on multimodal examples using a simple cross-entropy loss. Recent advances in this area include Flamingo [2], which introduces the Perceiver Resampler as the modular structure, and BLIP2 [24], which proposes a lightweight Q-Former to align different modalities. LLaVA [28] employs a simple MLP as a modular structure and introduces VL instruction tuning, enabling LLMs to execute multimodal tasks in an autoregressive manner. Despite these advancements, the current VL instruction tuning for massive MLLMs remains expensive. For example, LLaVA-13B fully fine-tunes the entire LLM during VL instruction tuning, often requiring hundreds of GPU hours. LaVIN [32], which utilizes an adapter to"}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 Preliminary", "content": "The objective of vision-language (VL) instruction tuning for MLLMs is to adapt an LLM backbone from processing unimodal text data to encompassing multimodal data. Specifically, given a multimodal instruction following example that consists of an image $I \\in \\mathbb{R}^{h \\times w \\times 3}$ and a text sequence $T \\in \\mathbb{R}^{L}$, the image I is initially fed into an image encoder, typically a pre-trained vision transformer [13], to extract the informative visual representation as:\n$F_I = f_{\\theta_I}(I),$                                                                                                 (1)\nwhere $\\theta_I$ represents the encoder's parameters. Then, the visual representation is projected to the word embedding space of LLMs through a modular structure parameterized by $\\theta_\\alpha$:\n$F_I' = f_{\\theta_\\alpha}(F_I).$                                                                                                     (2)\nSubsequently, the LLM with pre-trained weights $W$ receives the embedded image feature and the text sequence $T$ to generate a probability distribution $P \\in \\mathbb{R}^{L \\times N}$ for each word in the target response $R \\in \\mathbb{R}^{L}$:\n$P = g_W(F_I', E_T),$                                                                                                          (3)\nwhere $E_T = f_{\\theta_T}(T)$ is the word embedding of the input text sequence and $N$ denotes the vocabulary size of the pretrained LLM. Finally, the modular structure and LLM are jointly fine-tuned by minimizing the cross-entropy loss, which can be formulated as:\n$L = - \\sum_{i=1}^{L} log P_{i,j},$                                                                                                   (4)\nwhere $j$ represents the position of $R_i$ in the vocabulary.\nAlbeit the efficacy, it requires considerable computational resources and memory usage, mainly streaming from the significantly large parameters in LLMs. Although recent advancements [12, 23, 32, 47] have shown the potential of freezing the LLM backbone to eliminate partial backward costs of the LLM backbone, existing VL tuning frameworks still require a minimum of 8 NVIDIA Tesla A100 GPUs. This necessity poses significant challenges to the efficient adaptation of LLMs to cross-modal tasks, particularly in situations characterized by limited training resources and the need for on-the-fly, task-specific fine-tuning."}, {"title": "3.2 The Potential of LLMs Quantization", "content": "Quantizing the parameters of an LLM backbone into lower-bit representations offers a promising solution to the above problem and has shown remarkable efficacy in traditional unimodal PEFT scenarios for LLMs [12, 17, 22, 26]. Therefore, we initiate an exploratory investigation into the potential of quantization for MLLMs, starting with a trial using the QLoRA [12] in PEFT contexts.\nSpecifically, QLORA compresses the normalized weight $\\tilde{W}$ into quantized weight $W$ with 4-bit NormalFloat (NF) format $q_i$ as:"}, {"title": "3.3 Quantization-Aware Scale Learning", "content": "We formally present our Quantizration-aware Scale LeArning based on multimodal Warmup (QSLAW) method, specifically designed for efficient Visual Language (VL) instruction tuning in MLLMs. QSLAW addresses the challenges associated with MLLMs quantization from two aspects: (1) it uniquely learns scale factors for different weight groups, reducing the quantization error resulting from activation outliers and demonstrates to be more effective for VL instruction tuning on quantized LLMs; (2) it employs a modality-aware warmup strategy called multimodal warmup, which blends linguistic and multimodal training samples, thus preventing the quantized model from overfitting to multimodal data while ensuring a stable adaptation to the target VL tasks.\nQuantization-Aware Scale Learning. During the VL instruction tuning process, we assign learnable group-wise scale factors $s$ to the LLM weights $W$ as follows:\n$\\hat{W} = \\frac{W}{s}$                                                                                                          (7)\nAnd then uniform quantization is utilized to convert $\\hat{W}$ into pseudo-quantized weight $W$:\n$W = \\Delta \\times (clamp(\\lfloor \\frac{\\hat{W}}{\\Delta} \\rfloor + zp, 0, 2^k - 1)) - zp,$                                                  (8)\nwhere $\\lfloor \\cdot \\rfloor$ denotes the round-to-nearest integer operation and $k$ represents the quantization bit. $\\Delta$ and $zp$ are the quantization step-size and zero-point, respectively."}, {"title": "4 EXPERIMENTATION", "content": ""}, {"title": "4.1 Experimental Settings", "content": ""}, {"title": "4.1.1 Networks and Datasets", "content": "To validate the effectiveness of our approach, we select two types of MLLMs: LLaVA [28], which employs a two-stage full fine-tuning strategy, and LaVIN [32], which utilizes one-stage parameter-efficient fine-tuning strategy. We evaluate performance in line with most multimodal LLMs [10, 28, 32, 44], focusing on visual reasoning and instruction-following capabilities. For a straightforward comparison, we follow the precedent set by LLaVA and LaVIN to choose the ScienceQA dataset [31] for visual reasoning. The dataset, split into train, val, and test, spans diverse domains, including natural science, language science, and social science, and consists of both text-image and text-only inputs. We report the average accuracy on its test split. For instruction-following, we construct a multimodal ChatBot using LLaVA trained with LLaVA-80k [28]. LLaVA-80k is a high-quality vision-language instruction-following dataset generated by ChatGPT/GPT-4 [1]. The responses from the ChatBot will be evaluated by GPT-4, with higher-quality responses receiving a score ranging from 1 to 10."}, {"title": "4.1.2 Implementation Details", "content": "Following papers [28, 32], we adopt the ViT-L/14 in CLIP as the image encoder. For LaVIN and LLaVA, we use two MLP layers with a hidden dimension of 128 and a simple linear layer as modular structure, respectively. For LLMs, we employ LLaMA-7B [39] and Vicuna-13B [7]. All parameter settings strictly adhere to the LLaVA and LaVIN papers, except for the 2 training epochs with a batch size of 64, and a 1:1 hybrid training dataset comprising WikiText and downstream data for scale learning."}, {"title": "4.2 Main Results", "content": ""}, {"title": "4.2.1 ScienceQA", "content": "We categorize MLLMs into one-stage parameter-efficient and two-stage full fine-tuning and select a renowned model for each category to validate our method's performance. Quantitative results on ScienceQA are presented in Table 2. Our approach significantly enhances quantization transfer performance on multimodal tasks, showing consistent improvement across all question classes compared to QLoRA. For LLaVA, our QSLAW achieves 84.20% accuracy, a 10.16% gain over QLoRA's 74.04%. With improved settings, our method even outperforms full-precision LLaVA-13B, which fine-tunes the entire LLM on ScienceQA. For LaVIN, our QSLAW achieves 89.79% accuracy, a 1.46% gain compared to QLoRA's 88.33%, and outperforms full-precision LaVIN-7B."}, {"title": "4.2.2 ChatBot", "content": "We also present qualitative results to demonstrate the multimodal instruction-following capabilities of models obtained using QSLAW. In Figure 3, we compare various VL instruction tuning paradigms with examples from different multimodal instruction-following tasks, including image captioning, multimodal"}, {"title": "4.3 Ablation Studies", "content": ""}, {"title": "4.3.1 Component Importance", "content": "We examine the effectiveness of each component to provide deeper insights into VL instruction tuning with quantization. Table 3 shows that when LLM is quantized by OmniQuant and undergoes VL instruction tuning on ScienceQA like LaVIN, it serves as our baseline and achieves higher accuracy compared to LaVIN-QLORA due to the consideration of activation outliers. When we introduce quantization-aware scale learning and train it on the same dataset used for VL instruction tuning, the performance drops significantly due to overfitting issues in the LLM backbone. Incorporating linguistic data to guide scale learning alleviates overfitting and improves average accuracy by 0.66% compared to the baseline. Nevertheless, it still lacks effective supervision and exhibits a performance gap compared to full-precision LaVIN (88.54% for hybrid data v.s. 89.41% for the full-precision). Our multimodal warmup allows for precise supervision with hybrid data"}, {"title": "4.3.2 Quantization Initialization", "content": "We further evaluate QSLAW's performance with different quantization initialization on ScienceQA. In Table 4, we examine the performance of LaVIN-7B under both NF4 and OmniQuant. Our method consistently enhances performance under these two different quantization initializations. Specifically, QSLAW demonstrates an improvement of 0.71% and 1.41% compared to LoRA for NF4 and OmniQuant, respectively. This result also validates that, for VL tuning where the density and frequency of activation outlier are markedly increased, the NF4 datatype, which aims to equalize the quantity of values across all quantization bins, is sub-optimal and may negatively impact VL tuning. Moreover, QSLAW outperforms LoRA under both quantization methods, illustrating that our proposed scale learning method is more suitable for VL instruction tuning with quantized LLMs. This can be attributed to QSLAW's ability to effectively adapt to the unique characteristics of each quantization method, ensuring optimal performance in various quantization scenarios.\nIn conclusion, QSLAW's versatility and adaptability make it a robust and effective solution for VL instruction tuning across"}, {"title": "4.3.3 Alignment Effect", "content": "To further elucidate the benefits of the proposed QSLAW method in this paper, we conduct in-depth experiments on a two-stage LLaVA model. This model features a separate stage dedicated to pre-training a modular structure, which allows us to exclude the influence of other trainable parameters. This setup enables us to observe how quantization-aware scale learning enhances the alignment between visual and language modalities, ultimately leading to improved performance results.\nAs depicted in Figure 5, QSLAW can stabilize and accelerate the training process for the modular structure. We also calculate the pair-wise cosine similarity between text tokens and image tokens across different layers. Figure 6 demonstrates that the modular structure of QSLAW enhances alignment capability, potentially surpassing the projector under full-precision training. However, such a advantage would be compromised without our multimodal warmup strategy. This findings highlight the importance of QSLAW's quantization-aware scale learning and multimodal warmup in achieving effective alginment between visual and language modalities. Such an improved alignment contributes to the model's overall performance and adaptability, making it a valuable approach for multimodal learning tasks."}, {"title": "5 CONCLUSION", "content": "In this paper, we are the first to investigate the potential of parameter quantization for MLLMs to reduce training overhead during VL instruction tuning. We propose a Quantization-aware Scale Learning method based on multimodal Warmup (QSLAW). QSLAW employs quantization and a minimal set of trainable scaling factors to achieve efficient VL instruction tuning. A novel modality-aware warmup is introduced to ensure that scale learning receives adequate multimodal instructional supervision while preserving its original linguistic knowledge. We validate QSLAW's effectiveness under various settings, demonstrating its excellent multimodal reasoning capabilities. QSLAW surpasses full-precision fine-tuning on"}]}