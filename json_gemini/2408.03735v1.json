{"title": "Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation", "authors": ["Jingjing Xie", "Yuxin Zhang", "Mingbao Lin", "Liujuan Cao", "Rongrong Ji"], "abstract": "This paper presents the first study to explore the potential of parameter quantization for multimodal large language models to alleviate the significant resource constraint encountered during vision-language instruction tuning. We introduce a Quantization-aware Scale LeArning method based on multimodal Warmup, termed QSLAW. This method is grounded in two key innovations: (1) The learning of group-wise scale factors for quantized LLM weights to mitigate the quantization error arising from activation outliers and achieve more effective vision-language instruction tuning; (2) The implementation of a multimodal warmup that progressively integrates linguistic and multimodal training samples, thereby preventing overfitting of the quantized model to multimodal data while ensuring stable adaptation of multimodal large language models to downstream vision-language tasks. Extensive experiments demonstrate that models quantized by QSLAW perform on par with, or even surpass, their full-precision counterparts, while facilitating up to 1.4 times reduction in VL tuning time and GPU consumption.", "sections": [{"title": "1 INTRODUCTION", "content": "The remarkable performance of large language models (LLMs) has been well-established in recent literature [4, 9, 35, 36, 39], sparking a growing interest in the development of multimodal large language models (MLLMs) [2, 3, 5, 24, 28, 32, 42]. This burgeoning field has led to substantial progress in a wide array of vision-language (VL) tasks. To accomplish this, contemporary MLLMs primarily utilize multimodal instruction following examples for VL instruction tuning and adopt modular architectures [2, 21, 24, 28] to transform visual features into the word embedding space of the LLM. This innovative approach enables LLMs to execute multimodal tasks in an autoregressive fashion. One notable example of this technique is LLaVA [24], which employs a linear projection layer to bridge the gap between the visual encoder and the LLM. By doing so, LLaVA fully harnesses the power of pre-trained LLMs, thereby significantly enhancing its visual comprehension capabilities.\nDespite the advancements, the current VL instruction tuning for MLLMs exhibits considerable redundancy in terms of computation and memory burden. This limitation primarily stems from the inherently large size of LLMs compared to other components within MLLM architectures. For instance, LLaVA-13B fully fine-tunes the entire LLM during VL instruction tuning, often requiring hundreds of GPU hours [28]. Although recent efforts have introduced more efficient adapters and the freezing of LLMs to reduce training overheads [15, 32], VL tuning within current MLLM frameworks still demands substantial memory usage and computational resources, necessitating at least 8 NVIDIA Tesla A100 GPUs [32]. This poses great challenges to the rapid adaptation of LLMs for cross-modal tasks, particularly in situations characterized by limited training resources and needs for on-the-fly, task-specific tuning.\nTo address this constraint, this paper explores the potential of parameter quantization for MLLMs, aiming to alleviate the extensive training demands encountered during VL instruction tuning while preserving the original performance. Quantization, a network compression technique, transforms the full-precision weights into low-bit representations, consequently reducing both computational load and storage requirement. It has been adopted for parameter-efficient fine-tuning (PEFT) of LLMs [12, 17, 43], notably"}, {"title": "2 RELATED WORK", "content": "2.1 Model Quantization\nQuantization methods can be broadly classified into quantization-aware training (QAT) and post-training quantization (PTQ). QAT"}, {"title": "3 METHODOLOGY", "content": "3.1 Preliminary\nThe objective of vision-language (VL) instruction tuning for MLLMs is to adapt an LLM backbone from processing unimodal text data to encompassing multimodal data. Specifically, given a multimodal instruction following example that consists of an image $I \\in R^{h\\times w\\times3}$ and a text sequence $T \\in R^{l}$, the image $I$ is initially fed into an image encoder, typically a pre-trained vision transformer [13], to extract the informative visual representation as:\n$F_I = f_{\\theta_I}(I),$\nwhere $\\theta_I$ represents the encoder's parameters. Then, the visual representation is projected to the word embedding space of LLMs through a modular structure parameterized by $\\theta_\\alpha$:\n$F_I' = f_{\\theta_\\alpha}(F_I)$.\nSubsequently, the LLM with pre-trained weights $W$ receives the embedded image feature and the text sequence $T$ to generate a probability distribution $P\\in R^{L\\times N}$ for each word in the target response $R \\in R^{L}$:\n$P = g_W(F_I', E_T)$,\nwhere $E_T=f_{\\theta_T}(T)$ is the word embedding of the input text sequence and $N$ denotes the vocabulary size of the pretrained LLM. Finally, the modular structure and LLM are jointly fine-tuned by minimizing the cross-entropy loss, which can be formulated as:\n$L = -\\sum_{i=1}^L log P_{i,j},$\nwhere $j$ represents the position of $R_i$ in the vocabulary.\nAlbeit the efficacy, it requires considerable computational resources and memory usage, mainly streaming from the significantly large parameters in LLMs. Although recent advancements [12, 23, 32, 47] have shown the potential of freezing the LLM backbone to eliminate partial backward costs of the LLM backbone, existing VL tuning frameworks still require a minimum of 8 NVIDIA Tesla A100 GPUs. This necessity poses significant challenges to the efficient adaptation of LLMs to cross-modal tasks, particularly in situations characterized by limited training resources and the need for on-the-fly, task-specific fine-tuning.\n3.2 The Potential of LLMs Quantization\nQuantizing the parameters of an LLM backbone into lower-bit representations offers a promising solution to the above problem and has shown remarkable efficacy in traditional unimodal PEFT scenarios for LLMs [12, 17, 22, 26]. Therefore, we initiate an exploratory investigation into the potential of quantization for MLLMs, starting with a trial using the QLoRA [12] in PEFT contexts.\nSpecifically, QLORA compresses the normalized weight $\\widehat{W}$ into quantized weight $W$ with 4-bit NormalFloat (NF) format $q_i$ as:"}, {"title": "3.3 Quantization-Aware Scale Learning", "content": "We formally present our Quantizration-aware Scale LeArning based on multimodal Warmup (QSLAW) method, specifically designed for efficient Visual Language (VL) instruction tuning in MLLMs. QSLAW addresses the challenges associated with MLLMs quantization from two aspects: (1) it uniquely learns scale factors for different weight groups, reducing the quantization error resulting from activation outliers and demonstrates to be more effective for VL instruction tuning on quantized LLMs; (2) it employs a modality-aware warmup strategy called multimodal warmup, which blends linguistic and multimodal training samples, thus preventing the quantized model from overfitting to multimodal data while ensuring a stable adaptation to the target VL tasks.\nQuantization-Aware Scale Learning. During the VL instruction tuning process, we assign learnable group-wise scale factors $s$ to the LLM weights $W$ as follows:\n$W = \\frac{\\widehat{W}}{s}$\nAnd then uniform quantization is utilized to convert $\\widehat{W}$ into pseudo-quantized weight $W$:\n$W = \\Delta \\times (clamp(\\frac{\\widehat{W}}{\\Delta}+zp, 0, 2^k-1)) - zp,$\nwhere $\\lfloor \\cdot \\rceil$ denotes the round-to-nearest integer operation and $k$ represents the quantization bit. $\\Delta$ and $zp$ are the quantization step-size and zero-point, respectively."}, {"title": "4 EXPERIMENTATION", "content": "4.1 Experimental Settings\n4.1.1 Networks and Datasets. To validate the effectiveness of our approach, we select two types of MLLMs: LLaVA [28], which employs a two-stage full fine-tuning strategy, and LaVIN [32], which utilizes one-stage parameter-efficient fine-tuning strategy. We evaluate performance in line with most multimodal LLMs [10, 28, 32, 44], focusing on visual reasoning and instruction-following capabilities. For a straightforward comparison, we follow the precedent set by LLaVA and LaVIN to choose the ScienceQA dataset [31] for visual reasoning. The dataset, split into train, val, and test, spans diverse domains, including natural science, language science, and social science, and consists of both text-image and text-only inputs. We report the average accuracy on its test split. For instruction-following, we construct a multimodal ChatBot using LLaVA trained with LLaVA-80k [28]. LLaVA-80k is a high-quality vision-language instruction-following dataset generated by ChatGPT/GPT-4 [1]. The responses from the ChatBot will be evaluated by GPT-4, with higher-quality responses receiving a score ranging from 1 to 10.\n4.1.2 Implementation Details. Following papers [28, 32], we adopt the ViT-L/14 in CLIP as the image encoder. For LaVIN and LLaVA, we use two MLP layers with a hidden dimension of 128 and a simple linear layer as modular structure, respectively. For LLMs, we employ LLaMA-7B [39] and Vicuna-13B [7]. All parameter settings strictly adhere to the LLaVA and LaVIN papers, except for the 2 training epochs with a batch size of 64, and a 1:1 hybrid training dataset comprising WikiText and downstream data for scale learning."}, {"title": "4.2 Main Results", "content": "4.2.1 ScienceQA. We categorize MLLMs into one-stage parameter-efficient and two-stage full fine-tuning and select a renowned model for each category to validate our method's performance. Quantitative results on ScienceQA are presented in Table 2. Our approach significantly enhances quantization transfer performance on multimodal tasks, showing consistent improvement across all question classes compared to QLoRA. For LLaVA, our QSLAW achieves 84.20% accuracy, a 10.16% gain over QLoRA's 74.04%. With improved settings, our method even outperforms full-precision LLaVA-13B, which fine-tunes the entire LLM on ScienceQA. For LaVIN, our QSLAW achieves 89.79% accuracy, a 1.46% gain compared to QLoRA's 88.33%, and outperforms full-precision LaVIN-7B.\n4.2.2 ChatBot. We also present qualitative results to demonstrate the multimodal instruction-following capabilities of models obtained using QSLAW. In Figure 3, we compare various VL instruction tuning paradigms with examples from different multimodal instruction-following tasks, including image captioning, multimodal"}, {"title": "4.3 Ablation Studies", "content": "4.3.1 Component Importance. We examine the effectiveness of each component to provide deeper insights into VL instruction tuning with quantization. Table 3 shows that when LLM is quantized by OmniQuant and undergoes VL instruction tuning on ScienceQA like LaVIN, it serves as our baseline and achieves higher accuracy compared to LaVIN-QLORA due to the consideration of activation outliers. When we introduce quantization-aware scale learning and train it on the same dataset used for VL instruction tuning, the performance drops significantly due to overfitting issues in the LLM backbone. Incorporating linguistic data to guide scale learning alleviates overfitting and improves average accuracy by 0.66% compared to the baseline. Nevertheless, it still lacks effective supervision and exhibits a performance gap compared to full-precision LaVIN (88.54% for hybrid data v.s. 89.41% for the full-precision). Our multimodal warmup allows for precise supervision with hybrid data"}, {"title": "5 CONCLUSION", "content": "In this paper, we are the first to investigate the potential of parameter quantization for MLLMs to reduce training overhead during VL instruction tuning. We propose a Quantization-aware Scale Learning method based on multimodal Warmup (QSLAW). QSLAW employs quantization and a minimal set of trainable scaling factors to achieve efficient VL instruction tuning. A novel modality-aware warmup is introduced to ensure that scale learning receives adequate multimodal instructional supervision while preserving its original linguistic knowledge. We validate QSLAW's effectiveness under various settings, demonstrating its excellent multimodal reasoning capabilities. QSLAW surpasses full-precision fine-tuning on"}]}