{"title": "Value Imprint: A Technique for Auditing the Human Values Embedded in RLHF Datasets", "authors": ["Ike Obi", "Rohan Pant", "Srishti Shekhar Agrawal", "Maham Ghazanfar", "Aaron Basiletti"], "abstract": "LLMs are increasingly fine-tuned using RLHF datasets to align them with human\npreferences and values. However, very limited research has investigated which\nspecific human values are operationalized through these datasets. In this paper, we\nintroduce Value Imprint, a framework for auditing and classifying the human values\nembedded within RLHF datasets. To investigate the viability of this framework, we\nconducted three case study experiments by auditing the Anthropic/hh-rlhf, OpenAI\nWebGPT Comparisons, and Alpaca GPT-4-LLM datasets to examine the human\nvalues embedded within them. Our analysis involved a two-phase process. During\nthe first phase, we developed a taxonomy of human values through an integrated\nreview of prior works from philosophy, axiology, and ethics. Then, we applied\nthis taxonomy to annotate 6,501 RLHF preferences. During the second phase,\nwe employed the labels generated from the annotation as ground truth data for\ntraining a transformer-based machine learning model to audit and classify the three\nRLHF datasets. Through this approach, we discovered that information-utility\nvalues, including Wisdom/Knowledge and Information Seeking, were the most\ndominant human values within all three RLHF datasets. In contrast, prosocial and\ndemocratic values, including Well-being, Justice, and Human/Animal Rights, were\nthe least represented human values. These findings have significant implications\nfor developing language models that align with societal values and norms. We\ncontribute our datasets to support further research in this area.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning From Human Feedback (RLHF) has emerged as a potent way of shaping the\nbehavior of AI models to ensure they produce positive responses and experiences that correspond\nwith user preferences and societal norms [1-3]. On one hand, several AI researchers have touted the\nefficacy of this approach as a proxy for embedding human values and preferences into AI models,\nresulting in its use in different domains, including the finetuning of LLMs [4, 5], vision models\n[6], and multi-modal systems [7]. Several users of these AI systems, on the other hand, are raising\nconcerns about the censorship and anti-democratic stance of models trained with these preferences,\nhighlighting that they are marginalized against their value systems while allowing others [8, 9]. As a\nresult, there is a growing concern among members of the public around the lack of transparency in the\nkinds of values these datasets embed into AI systems. In addition, considering that RLHF preferences\ninvolve complex value judgments of annotators, it is crucial to investigate how the subjective values\nand preferences of annotators \u2013 both human and AI \u2013 are embedded within these datasets in ways\nthat might misalign with societal values and norms.\nIn this paper, we introduce Value Imprint, a novel technique for auditing and classifying the human\nvalues embedded within RLHF datasets. To support this approach, we created a human values\ntaxonomy by conducting an integrated literature review of prior bodies of work from philosophy,\naxiology, and STS (Science, Technology, and Society) and, through a thematic analysis of these\nbodies of work, developed a taxonomy of human values to support our audit. Using this taxonomy,\nwe conducted a two-phase audit analysis, with each step building on the result from the previous stage.\nDuring the first phase, we employed the taxonomy to qualitatively annotate 6,501 RLHF preferences.\nDuring the second phase, we employed the labels derived from the qualitative annotation process as\nground truth data. This data was then utilized to a train transformer-based machine learning model,\nwhich we subsequently deployed for auditing and classifying the complete Anthropic/hh-rlhf, OpenAI\nWebGPT Comparisons, and Alpaca GPT-4-LLM datasets. We further conducted a human evaluation\nof a section of the classification output to examine their performance. We followed the evaluation\nwith an additional round of analysis to examine how the values embedded within the three RLHF\ndatasets differ. Through these approaches, we answered our research questions which included:\n1. RQ1: What kinds of human values are embedded within RLHF preferences?\n2. RQ2: In what ways do the human values embedded within the Anthropic/hh-rlhf, OpenAI\nWebGPT Comparisons, and Alpaca GPT-4-LLM datasets differ?\nFindings from our research revealed that the most dominant values within the ground truth RLHF\npreferences were Information Seeking and Wisdom/Knowledge. In contrast, the least represented\nvalues were Civility & Tolerance, Empathy & Helpfulness, Justice & Human Rights/Animal Rights,\nand Well-being & Peace. The findings also revealed instances of unethical responses selected\nas suitable preferences for training machine learning models. Furthermore, the machine learning\nclassification of human values produced an accuracy score range of 80% for the model we used for\nthis analysis. This demonstrates the viability of AI researchers and practitioners adopting this process\nto interrogate the human values embedded within RLHF datasets to foreground their value orientation\nand how they might lead to different societal impacts.\nAbove all, through this research, we make the following contributions: 1) we introduce a technique for\nauditing and classifying the underlying human values embedded within RLHF preferences, providing\nAl researchers with a technique for auditing and interrogating the quality of RLHF datasets, 2)\nwe conduct three case study experiments using this approach and through our findings reveal that\nWisdom/Knowledge and Information Seeking were the most dominant human values within the\ndatasets; validating our technique. 3) We contribute both our ground truth annotation and classification\ndatasets and, through this means, provide researchers with the pathway to take this work forward.\nIn the sections that follow, we situate our work within broader research on language models, data\nquality, and embedding human values into LLMs. We then describe our methods and report our\nfindings. We conclude with a discussion of the implications of these findings and provide suggestions\nfor future work."}, {"title": "2 Background", "content": "2.1 Embedding Human Values into LLMs and AI Systems\nAl researchers and computer scientists are increasingly interested in embedding human values into\nLLMs, AI, and robotic systems. This increased interest is motivated by several reasons, including the\nneed to move beyond just optimizing for metrics like efficiency and performance towards aligning\nthese systems with prosocial values like democracy, transparency, freedom of expression, and human\nrights [10-12]. It also includes the need to ensure that technology systems do not make users\nvulnerable or cause them harm [13\u201315]. To achieve these objectives, AI researchers are increasingly\ndeveloping sociotechnical approaches for encoding societal values into AI systems, using different\ntechniques such as value-oriented datasets as can be found in the works of [10, 16\u201319] and formalized\nethical frameworks as can be found in the works of [20-22], among other techniques [23-26].\nSolaiman & Dennison [10] introduced an approach for aligning AI models with human values by\nusing value-oriented datasets to finetune AI models. Findings from their research revealed that"}, {"title": "2.2 Data Quality and Language Models", "content": "There is a large and growing body of work at the intersection of data quality and language models [31\u2013\n33]. These works have examined issues relating to AI datasets from numerous perspectives, including\nissues of representation harms and demographic bias that propagate harmful stereotypes from\ndefective datasets into AI models, issues of toxicity/harmful content that perpetuate misogyny\nand racial slurs, lack of transparency and accountability around how datasets are collected, annotated,\ncleaned or versioned over time which hampers accountability and attribution, among many other\nissues at the intersection of data quality and language models.\nHirota et al. [31] investigated the issues of gender and racial bias in five visual question-answering\ndatasets. Findings from their research revealed instances of gender disparity and racial stereotypes\nthat favor males and Western cultures, respectively. They proposed approaches that researchers could\nadopt to mitigate these biases. Garcia et al. [34] annotated and audited the Google Captions vision\nand language model datasets to investigate instances of bias. Findings from their research showed\nan over-representation of males and persons with lighter skin tones compared to other users from\nother demographics. Dhamala et al. [35] introduced a large-scale benchmarking dataset to allow\nresearchers to measure bias in language models across different dimensions, including race, religion,\nand gender. Through this approach, they aim to induce transparency in reporting toxicity within\nlanguage models. Papakyriakopoulos et al. [36] investigated the lack of diversity in speech datasets\nacross different dimensions, including accent, dialect, and speech impairment. Findings from their\nresearch revealed that the absence of intentional structure plays a role in this lack of diversity. To\nresolve this, they introduced speech datasheets to foster ethical data collection practices around\nspeech datasets. Pushkarna et al. [37] introduced data cards to document the provenance and ethical\nimplications of using multi-modal datasets. Luccioni et al. [38] introduced a dataset deprecation\nframework as a means of ensuring proper documentation for datasets that are deprecated and retired\nfrom circulation.\nAlthough numerous scholars have extensively audited AI and machine learning datasets, very limited\nwork has focused on examining RLHF datasets to foreground the human values embedded within\nthem. In a small body of work in this area, Hendrycks et al. [16] introduced the ETHICS Dataset"}, {"title": "3 Method", "content": "3.1 Experiment Dataset\nWe collected the datasets for this research from different developer collaboration platforms. We\ncollected the Anthropic/hh-rlhf [2] dataset from Hugging Face, an open source machine learning\nplatform that provides datasets, models, and other computational resources for AI practitioners and\nresearchers. The Anthropic/hh-rlhf dataset (train - 161k rows, test - 8.55k rows) has been downloaded\nat least 109,200 times, used to train or fine-tune more than 156 AI models. Our analysis focused\non both the chosen and rejected columns of the data. We merged the train and test sections of the\nAnthropic/hh-rlhf dataset into one dataset corpus for analysis. We also collected the OpenAI WebGPT\nComparisons [41] dataset from the Hugging Face library and focused our case study experiment on\nthe content of the question and answer_0 columns. We created a function to extract only the full_text\nfrom the question column and dropped the non-essential metadata, including triviaqa, dataset, and\nid. Next, we concatenated the content of the updated question and answer_0 columns into a new\ncolumn to form a complete preference unit. We then used our model to classify these preferences and\nexamined the human values embedded within them. We fetched the Alpaca GPT-4-LLM [42] dataset\nfrom the GitHub repository dedicated to the project. Next, we concatenated the instruction and\noutput columns from the original dataset into a new combined column, creating a complete human\npreference conversation. We reduced the DataFrame to contain only this new combined column and\nthen conducted our case study classification analysis. See (Fig. 1) for our research process flow."}, {"title": "3.2 Human Value Taxonomy", "content": "We constructed a taxonomy of human values through an integrated literature review grounded in\nprior bodies of work from moral philosophy, axiology, and STS (Science, Technology, and Society).\nSpecifically, our literature search focused on nine journal databases within human values-related\ndisciplines, including the Journal of Value Inquiry; Axiomathes; The Journal of Ethics; No\u00fbs;\nEthics; The Philosophical Review; Science, Technology, & Human Values; Utilitas; and The Journal\nof Philosophy. Our search keyword for querying these databases was: \"human value.\" No date\nrestrictions were made on our search of these databases.\nWe followed a three-stage process to construct a taxonomy of human values using the curated\nresearch papers. In the first stage, we assigned each curated paper a human value based on the central\ntheme discussed in the paper. Next, we categorized papers with similar values into semantically\ncoherent hierarchical categories using a bottom-up approach, such as grouping papers about peace,\nsecurity, and well-being under an overarching well-being and peace category. Second, we conducted"}, {"title": "3.3 Data Annotation", "content": "Using the human values taxonomy as our codebook, we qualitatively annotated sampled 6,501\npreferences from the Anthropic/hh-rlhf dataset to examine the human values embedded in them. The\nqualitative annotations were performed by a team of 5 researchers with interdisciplinary expertise\nspanning Ethics, Computing, and HCI. The nationality of the annotators included India, USA, Nigeria,\nand Pakistan. Before coding all the 6,501 preferences, we held several rounds of extensive discussions\nand exploratory coding activities. These activities allowed us to engage with the dataset to better\nunderstand the dimensions of human values, their differences, and similarities and to establish a\nprotocol for resolving any discrepancies and challenges that might arise during the main annotation\nsession. Following this exploration, we conducted an inter-annotator agreement assessment by having\nall the annotators independently code the same 200 preferences and then compared the codes assigned\nby each annotator to the same preferences to assess the level of agreement between all the annotators.\nWe achieved an inter-annotator agreement score of 0.85 using Krippendorff's Alpha score. Through\nthis approach, we confirmed that multiple coders can consistently annotate and apply the same\nlabels to the same RLHF preferences once they understand the human values taxonomy. We then\ncommenced our main annotation session. Other infrequent discrepancies during our main annotation\nphase were resolved through discussions, codebook refinement, or reconciliation by a third annotator."}, {"title": "3.4 Value Classification", "content": "3.4.1 Problem Formulation\nTo formally frame the task of computationally auditing the human values embedded within RLHF\ndatasets, we modeled it as a multi-class classification problem over a vector space of human values.\nWe define as follows: Let $V = \\{V_1, V_2, ..., V_n\\}$ be the set of all possible human value labels,\nwhere $n$ is the number of distinct human value classes. We define a dataset $D = \\{(x_i, Y_i)\\}_{i=1}^m$,\nwhere $x_i \\in X$ is an RLHF preference instance (text), $Y_i \\in V$ is the corresponding human value\nlabel associated with $x_i$, and $m$ is the total number of instances. Split $D$ into disjoint train and\ntest sets: $D_{\\text{train}}$ and $D_{\\text{test}}$. Use a tokenizer $T : X \\rightarrow R^{d \\times l}$ to convert each text instance $x_i$ into a\nnumerical token representation $T(x_i) \\in R^{d \\times l}$, where $d$ is the embedding dimension, and $l$ is the\nsequence length. We define a multi-class classification model $f_{\\theta} : R^{d \\times l} \\rightarrow R^n$, where $\\theta \\in \\Theta$ are\nthe trainable parameters of the model (RoBERTaForSequenceClassification), and $\\Theta$ is the parameter\nspace. We use a cross-entropy loss function $L : V\\times R^n \\rightarrow R$ to measure the discrepancy between\nthe predicted and true labels for each instance $(T(x_i), y_i)$ in the training set: $L(y_i, f_{\\theta}(T(x_i)))$. We\nalso incorporated class weights $w = (W_1, W_2, ..., W_n) \\in R^n$ to handle class imbalance, computed\nusing compute_class_weight from scikit-learn. We further optimize the model parameters $\\theta$ by\nminimizing the weighted cross-entropy loss over the training set:\n$\\min_{\\theta} \\frac{1}{|D_{\\text{train}}|} \\sum_{(T(x_i), y_i) \\in D_{\\text{train}}}  W_{y_i} \\cdot L(y_i, f_{\\theta}(T(x_i)))$.\nWe used regularization (dropout), warm-up steps, and weight decay during training to improve\ngeneralization and prevent overfitting. We then used the trained model $f_{\\theta}$ to make predictions on\n$D_{\\text{test}}$: $Y_{\\text{pred}, i} = \\arg \\max_{u \\in V} f_{\\theta}(T(x_i))_j$, where $Y_{\\text{pred},i}$ is the predicted human value label for the\ninput instance $x_i$. We further evaluated model performance on $D_{\\text{test}}$ using metrics like accuracy and\nF1-score, with weighted averages to account for class imbalance."}, {"title": "3.4.2 Value Classification", "content": "Using the annotated ground truth dataset, we trained a RoBERTa model for the multi-class classifica-\ntion of the RLHF datasets. We split the training data into 80% train and 20% test set using sklearn's\ntrain_test_split. We trained the model for 8 epochs with a batch size of 64 using Hugging Face\nTrainer. We used CrossEntropy loss for the classification task. We further enabled early stopping\nto prevent overfitting, with the training stopping early if the validation loss does not improve for\n2 epochs. We saved the model checkpoints from the best validation loss. The hyperparameters\nincluded Max sequence length - 128, Batch size - 64, Epoch - 8, and Early stopping patience -2\nepochs. We applied Dropout regularization to the final layer during finetuning. We also computed\nclass weights to handle class imbalance and used weighted random sampling for the training batches.\nWe then employed the trained RoBERTa model for classifying the human values embedded within\nthe Anthropic/hh-rlhf (338,704), OpenAI WebGPT Comparisons (19,578), and Alpaca GPT-4-LLM\n(52,002) datasets. Following the value classification activities, we conducted a human evaluation of\n500 classification results, which showed that the models predicted the correct human value 84% of\nthe time. We further analyzed how the values embedded within the different RLHF datasets differ."}, {"title": "4 Findings", "content": "4.1 RQ1: What Kinds of Human Values are Embedded within RLHF Preferences?\n4.1.1 Results from Qualitative Annotation\nFindings from our analysis of the 6,501 ground truth preferences from the Anthropic/hh-rlhf dataset\nrevealed that the most dominant human values were Information Seeking for a specific use case\n(36.96%), Wisdom/Knowledge for personal enlightenment and edification (30.75%), and Duty &\nAccountability (9.52%). The least represented human values within the dataset were Civility &\nTolerance (7.61%), Empathy and Helpfulness (6.09%), Well-being & Peace (5.94%), and Justice,\nHuman & Animal Rights (3.12%). We characterize results from this analysis below and in (Table 1)."}, {"title": "4.1.2 Results from the Classification of Human Values within the three RLHF Datasets", "content": "Results from our analysis showed that the RoBERTa model demonstrated strong proficiency (F1\n> 0.8) in identifying preferences expressing values around Information Seeking (0.831), Justice &\nHuman/Animal Rights (0.883), Duty & Accountability (0.813), Civility & Tolerance (0.808), and\nWisdom & Knowledge (0.815). However, our results show that the model comparatively struggled to\naccurately classify values centered around Empathy & Helpfulness (0.629) and Well-being & Peace\n(0.649). This finding aligns with results from our qualitative analysis, which showed that those value\ncategories are significantly underrepresented in the RLHF dataset. Hence, a more extensive ground\ntruth dataset with those values will mitigate the results."}, {"title": "4.2 RQ2: In What Ways Does the Human Values Embedded within the Anthropic/hh-rlhf,\nOpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM Datasets Differ?", "content": "We examined results from the machine learning classification of the three RLHF datasets to investigate\nhow the values embedded within them differ, with the Anthropic/hh-rlhf dataset split into chosen and\nreject categories, resulting in a four-category comparison. Our analysis revealed that information-utility values (Wisdom/Knowledge & Information Seeking) were the most predominant values\nacross all the datasets. Specifically, the findings showed that Wisdom/Knowledge was the most\ncommon human value across all the three RLHF datasets (OpenAI WebGPT = 78.17%, Alpaca\nGPT-4 = 66.56%, Anthropic_chosen = 33.84%, Anthropic_rejected = 33.71%). This was followed\nby Information Seeking which was also the second most common value in all the datasets except\nfor the OpenAI WebGPT dataset where it placed third (Alpaca GPT-4 = 26.45% Anthropic_hh-\nrlhf_chosen= 31.71%, Anthropic_hh-rlhf_rejected= 31.82%, OpenAI WebGPT = 5.67%). In contrast,\nour analysis showed that Justice & Human/Animal Rights was the least represented value in all the\ndatasets (OpenAI WebGPT = 0.04%, Alpaca GPT-4 = 0.17%, Anthropic_hh-rlhf_chosen = 1.76%,\nAnthropic_hh-rlhf_rejected = 1.76%). We visually compare the differences and similarities of values\nembedded within the three datasets in Fig 3."}, {"title": "5 Discussion & Implications", "content": "5.1 Human Values Distribution & Underrepresentation\nOur audit revealed that values embedded within the three RLHF datasets were predominantly oriented\ntowards information-utility values (Information Seeking, Wisdom & Knowledge acquisition) and\nless towards prosocial, well-being, and civic values (Civility, Tolerance, Well-being, and Justice).\nWhile the numerical imbalance and distribution of human values within the datasets may not nec-\nessarily induce poor model performance depending on usage contexts, it is undoubtedly the case\nthat such datasets contain low variance of the underrepresented human values. Hence, the primary\nissue here lies not only in the quantity of human values but also in the variance and quality of\npreferences that represent the different human values. This means that for prosocial and civic values\nto be adequately captured, the RLHF datasets must cover the various dimensions and nuances of\nprosocial and civic values. For instance, Justice & Human/Animal Rights human value was severely\nunderrepresented in all the RLHF preference datasets (OpenAI WebGPT = 0.04%, Alpaca GPT-4 =\n0.17%, Anthropic_hh-rlhf_chosen = 1.76%, Anthropic_hh-rlhf_rejected = 1.76%). Such minimal\nrepresentation, irrespective of high classification accuracy score, makes capturing the full variance of\npreferences related to Justice & Human rights/Animal rights in the given datasets virtually impossible.\nIn that case, the relative underrepresentation of duty-oriented prosocial and democratic human values\nbecomes a cause for concern because prosocial and civic values play a crucial role in many of our\nsocial and legal systems. The concern becomes even more elevated if such models are used in legal\nor professional contexts that require significant ethical reasoning, like medicine and law enforcement.\nThe logical trajectory of this viewpoint highlights that LLMs designed for certain domains ought to\nmeet certain domain-specific human value thresholds before deployment. For instance, a medical\nLLM ought to be able to reason about medical ethics and as well be proficient at providing medical\ninformation. Similarly, an LLM designed for kids should meet certain value thresholds before being\nreleased to the younger generation. Through this work, we seek to foster rigorous research on the\nhuman values embedded within RLHF datasets and AI models."}, {"title": "5.2 Human Values in RLHF Datasets as an Affordance", "content": "The human values embedded in the RLHF datasets are an affordance that shapes how models\ntrained with such datasets behave. Like affordance in traditional software programs suggests, allows,\ndisallows, or restricts possible actions to users, the human values embedded in RLHF datasets\nimbue LLMs with the ability to suggest, shape, or guide user conversations or actions. Hence,\nunderrepresenting some human values might lead to an involuntary constraint on the ability of LLMs\nto navigate specific scenarios that require such values, such as empathy and democratic reasoning.\nHence, it is vital to pay attention to human values at the micro-level and ethical paradigms at the\nmacro-level to ensure reasonable diversity and balanced system behavior. In addition, the inclusion of\nunethical preferences in the dataset demonstrates how negative affordances can emerge from flawed\ntraining data and enable harmful or biased AI behaviors if not accurately identified and mitigated.\nThe Value Imprint framework aims to make human values more 'tangible,' allowing researchers to\nintentionally foreground, interrogate, and shape the affordance of LLMs through the values they\nembed into Al models. This allows for a more nuanced understanding of how different value\n'configurations' might influence the behavior of AI models across various contexts and use cases."}, {"title": "5.3 Conclusion", "content": "In this research, we introduced Value Imprint, a technique for auditing and classifying the human\nvalues embedded within RLHF datasets. Findings from our case study experiments revealed that\nInformation Seeking and Wisdom/Knowledge were the values most represented within the RLHF\ndatasets; in contrast, pro-democratic and prosocial values were underrepresented. This research\nprovides Al researchers and computer scientists with a computational approach for interrogating the\nhuman values embedded within RLHF datasets before using them to train models. We contribute our\nground truth dataset and the classification datasets from our audit to foster further research in this\narea."}, {"title": "B.1 Annotator Demographics", "content": "Our research team comprised five researchers from a large, research-intensive public university in\nthe Midwestern USA. Four researchers had graduate-level education with backgrounds spanning\nEthics, Computer science, Information Technology, and Design, including Machine Learning and\nNLP coursework. The four researchers also had prior experience participating in mixed-methods\nresearch. The fifth member was an undergraduate student majoring in Web Programming who had\nbeen exposed to research through coursework and was mentored by senior researchers throughout the\nproject."}, {"title": "B.2 Resolving Annotator Questions", "content": "We relied on the human values taxonomy as our guide during the annotation of the human values\nembedded within the RLHF preferences. Our process followed a diverge-converge approach. This\nmeant that researchers first worked independently to annotate their assigned RLHF preferences, then\nregularly convened as a team to discuss, review, and evaluate our process and the taxonomy. During\nthese convergence meetings, we engaged in pair coding, cross-checking each other's annotations,\nand answering any questions or concerns that any team member might have. Through these frequent\ndiscussions and reviews, our team continually assessed and reached a consensus on the suitability of\nthe taxonomy for our research objective."}, {"title": "C Comparison with Schwartz's Theory of Basic Human Values in the\nContext of AI", "content": "While there are some similarities with Schwartz's Theory [153] of Basic Human Values, the human\nvalues taxonomy developed in this paper presents a framework more specifically tailored to the ethical\nconsiderations and operational requirements of AI systems, particularly in auditing the human values\nembedded within RLHF datasets. Some juxtapositions between both frameworks are highlighted\nbelow:\n1. Contextual Specificity: The values identified in our paper (e.g., Information Seeking, Wis-\ndom/Knowledge, Duty & Accountability) are more directly applicable to human-AI interac-\ntions and decision-making processes. In contrast, Schwartz's values (e.g., Self-Direction,\nStimulation, Hedonism) are broad and more focused on general human motivations and\nbehavior."}, {"title": "D Potential Limitations of this Approach", "content": "Interpreting and characterizing human values is a complex endeavor. Human preferences often\nembody multiple values and require researchers to determine the dominant value subjectively. Fur-\nthermore, machine learning models do not inherently understand the nuances of human values. They\ncan only generate a basic conception of values based on the dataset they are trained on. Our objective\nin this research is not to provide a definitive characterization of human values but rather to equip AI\nresearchers with a framework to critically examine and probe RLHF datasets to better understand\nhuman values distribution with them and the potential societal impacts that could arise from them.\nAdditionally, the values represented in our dataset are primarily Western-focused because of the\nWestern-centric nature of our literature review sources and the Western-oriented focus of the discourse\nin the three RLHF datasets used for our case study experiments. This could affect the performance of\nour model if it is used for text classification of human values in non-Western RLHF preferences. It is\nalso worth noting that if researchers adopt a different value taxonomy, the human values within the\ndataset might be interpreted differently. There are other specialized forms of RLHF, such as code and\nmath. Our taxonomy will not work in those contexts.\nHence, future work could involve developing more diverse datasets that capture non-Western con-\nceptions of values. Future work could also include using these value classifications to train reward\nmodels to explore the benefits of systematically curating human values to introduce into LLMs. Other\nresearch could also explore breaking down the human values taxonomy to their sub-values to elicit\nand interrogate more human values embedded within the datasets at a granular level."}, {"title": "E Dataset Documentation: Datasheets for Datasets", "content": "E.1 Motivation\nFor what purpose was the dataset created?\nWho created the dataset and on behalf of which entity? The original dataset was created by\nAnthropic, OpenAI, and other AI researchers. The updated dataset with human values labels was\ncreated by researchers at Purdue University, West Lafayette.\nWho funded the creation of the dataset? Information regarding funding for the creation of the\noriginal dataset is not publicly available. But we can safely assume that it was funded by Anthropic,\nOpenAI, and other open source communities. The research that yielded the updated dataset was\nconducted as part of Ph.D. and class requirement and was not funded by any external agency."}, {"title": "E.2 Composition", "content": "What do the instances that comprise the dataset represent (for example, documents, photos,\npeople, countries)? The datasets consist of text-based RLHF preferences, which include: User\ninputs or questions posed to the language model. Responses selected by human annotators as the most\ndesirable or appropriate. Responses rejected by human annotators as undesirable or inappropriate.\nThe human value assigned to each preference either by human annotators for the ground truth dataset\nor via machine learning classification for the larger dataset.\nHow many instances of each type are there? It contains 169,352 per row. resulting in a combined\n338,704 if treated independently. OpenAI WebGPT Comparisons (19,578) and Alpaca GPT-4-LLM\n(52,002)\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set? Yes, the dataset comprises two instances: 1) the annotated small\ninstance from the larger dataset. We refer to this small dataset as the ground truth dataset. 2) the\nlarger dataset\nWhat data does each instance consist of? RLHF preferences related to specific scenarios that\ninvolve user interaction with an AI Assistant.\nIs there a label or target associated with each instance? If so, please provide a description.\nEach instance (preference) was assigned a human value based on the content of the preference.\nIs any information missing from individual instances? No\nAre relationships between instances made explicit in the data? Each preference contains a\nchosen and rejected column to show which option was selected by an annotator and the option that\nwas rejected.\nAre there recommended data splits or evaluation measures? There are no recommended data\nsplits. However, it is worth noting that we used an 80-20 split during our machine learning classifica-\ntion task.\nAre there any errors, sources of noise, or redundancies in the dataset? Does not apply.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (for\nexample, websites, tweets, and other datasets)? Everything is included and the data does not\ndepend on any external resource.\nDoes the dataset contain data that might be considered confidential (for example, data that is\nprotected by legal privilege or by doctor-patient confidentiality, data that includes the content\nof individuals' non-public communications)? No\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety? Yes the conversation with the AI Assistant does on some\noccasions contain offensive and repugnant words that might cause distress and require special\nattention before engaging with them.\nDoes the dataset identify any subpopulations (for example, by age, gender)? The conversation\nwith the assistant does sometimes refer to gender and age, but is not directly tied to any person or\nindividual.\nIs it possible to identify individuals (that is, one or more natural persons), either directly or\nindirectly (that is, in combination with other data) from the dataset? No"}, {"title": "E.3 Data Collection", "content": "How was the data associated with each instance acquired? Was the data directly observable\n(for example, raw text, movie ratings), reported by subjects (for example, survey responses),\nor indirectly inferred/derived from other data (for example, part-of-speech tags, model-based\nguesses for age or language)? A large language model generated two potential responses for a\ngiven prompt.\nWhat mechanisms or procedures were used to collect the data (for example, hardware ap-\nparatuses or sensors, manual human curation, software programs, software APIs)? Human\nannotators were shown the prompt and the two responses, and asked to choose which response they\npreferred in terms of being more \"helpful and harmless.\"\nIf the dataset is a sample from a larger set, what was the sampling strategy (for example,\ndeterministic, probabilistic with specific sampling probabilities)? The ground truth dataset was\ncurated from the larger dataset through random sampling.\nWho was involved in the data collection process (for example, students, crowdworkers, con-\ntractors) and how were they compensated (for example, how much were crowdworkers paid)?\nThe lead researcher retrieved the original datasets from Hugging Face and GitHub using a simple\nPython script.\nOver what timeframe was the data collected? Not available for the original dataset.\nWere any ethical review processes conducted (for example, by an institutional review board)?\nNot applicable\nDid you collect the data from the individuals in question directly, or obtain it via"}]}