{"title": "Fast Gradient Computation for RoPE Attention in Almost Linear Time", "authors": ["Yifang Chen", "Jiayan Huo", "Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "The Rotary Position Embedding (RoPE) mechanism has become a powerful enhancement\nto the Transformer architecture, which enables models to capture token relationships when\nencoding positional information. However, the ROPE mechanisms make the computations of\nattention mechanisms more complicated, which makes efficient algorithms challenging. Earlier\nresearch introduced almost linear time, i.e., $n^{1+o(1)}$ where n is the number of input tokens,\nalgorithms for the forward computation under specific parameter settings. However, achieving\na subquadratic time algorithm for other parameter regimes remains impossible unless the widely\naccepted Strong Exponential Time Hypothesis (SETH) is disproven. In this work, we develop\nthe first almost linear time algorithm for backward computations in the RoPE-based attention\nunder bounded entries. Our approach builds on recent advancements in fast RoPE attention\ncomputations, utilizing a novel combination of the polynomial method and the Fast Fourier\nTransform. Furthermore, we show that with lower bounds derived from the SETH, the bounded\nentry condition is necessary for subquadratic performance.", "sections": [{"title": "1 Introduction", "content": "The GPT-03 [Ope24], Llama 3.3 [LT24, AI24], Claude 3.5 [Ant24a] are transformed-based Large\nLanguage Models (LLMs), have become important tools in natural language processing, which\nenables applications from machine translation to sentiment analysis. In the Transformer archi-\ntecture, attention mechanisms, computationally intensive operations, compute token correlations\nwithin the sequence [VSP+17]. The efficiency of attention computations, both in forward com-\nputations and backward gradient computations, directly influenced the scalability and feasibility\nof training LLMs, especially when the size and input context length of these LLMs continue to\ngrow [AS24c, AS23]. In recent research, Rotating Position Embedding (ROPE) [SAL+24] has\nbecome a popular modification to the attention mechanism, and it has enabled models to cap-\nture positional relationships between tokens with better expressiveness. The RoPE mechanism has\nbeen adopted in state-of-the-art models, such as Llama [TLI+23, TMS+23, LT24], Claude [Ant24a],\nApple's LLMs [GWW+24, MGF+24], and many others, but the implementation of RoPE compli-\ncates attention computation due to the additional structure imposed by position-dependent ro-\ntations [SAL+24]. In recent work, [AS24a] has demonstrated an efficient algorithm for forward\ncomputation of RoPE attention in the bounded entry regime, but the backward computation, the\nprocess of calculating gradients for model optimization, is less explored.\nBackward computation introduces additional complexity because it requires the evaluation of\ngradients that involve non-linear transformations of the attention matrix and positional embed-\ndings. In [AS23], they present their algorithm to approximate forward computations of fast atten-\ntion with bounded entries using the polynomial methods and low-rank approximation. In [AS24b],\nthey propose almost linear time, i.e., $n^{1+o(1)}$ where n is the number of input tokens, an algorithm\nto compute backward gradients for fast attention with bounded entries. In recent work, [AS24a]\nproposes an efficient algorithm to perform the forward computation of RoPE-based attention us-\ning the polynomial methods and Fast Fourier Transform. Therefore, it is natural to raise the key\nquestion:\nCan backward computations for the RoPE attention match the efficiency of their forward\ncomputations in the bounded entry regime?\nIn this work, we aim to address the question by presenting the first efficient algorithm for\nbackward computation in RoPE attention under the bounded entry. Our main result shows that the\nbackward gradient computations for the RoPE attention match their forward version's efficiency.\nTherefore, by leveraging our algorithm in approximating backward computations in the RoPE\nattention with the forward algorithm from [AS24a], we will improve the overall time complexity of\nROPE attention to almost linear time with bounded entries.\nTo the best of our knowledge, this is the first work to characterize the fine-grained complexity\nof backward computations in RoPE attentions, extending prior results on forward computations in\nRoPE attention [AS24a].\nRoadmap. In Section 2, we present some relevant papers. In Section 3, we show important\ncomponents for the RoPE attention. In Section 4, we present the details of getting the gradients of\nROPE-based attention mechanisms. Section 5 discusses the exact time complexity for computing\nROPE attention gradients. Section 6 shows the fast computation of gradient using the polynomial\nmethod. Section 7 details the lower bounds of hardness. Finally, Section 8 provides conclusions\nand avenues for future work."}, {"title": "2 Related Work", "content": "Rotary Position Embedding. In this paper, we study a variant of attention called ROPE at-\ntention. At a high level, RoPE gives more expressive power to the model in exchange for making\nthe computational problem more complicated. In particular, many prior algorithms, such as the\nalgorithm of [AS23], no longer apply to RoPE for fundamental reasons we will discuss. ROPE\nwas proposed by [SAL+24] and has been used extensively in large-scale industrial models. Ex-\namples which are known to use RoPE include the open-source models released by Meta such as\nLlama [TLI+23] (see page 3), Llama 2 [TMS+23] (see page 5), Llama 3 [LT24] (see page 7), and\nthe close-source LLM Claude 3.5 [Ant24a] released by Anthropic. Apple also incorporates RoPE\ninto their LLM architecture (see [MGF+24], and page 3 of [GWW+24]). The idea behind ROPE\nis to rotate the query and key vectors in the self-attention mechanism. The rotation is position-\ndependent and designed such that the inner product between two position-encoded vectors reflects\ntheir relative position in the input so that in the ROPE attention mechanism, pairs of tokens with\na longer relative distance will have a smaller correlation.\nFast Attention Computation. The attention mechanism has often been criticized for its quadratic\ncomputational complexity concerning context length, a challenge that becomes more pronounced as\nthe sequence length grows in today's LLMS [AAA+23, Ope24, LT24, AI24, Ant24b, Ant24a]. How-\never, this issue can be addressed using polynomial kernel approximation methods [AA22], which\nfacilitate constructing the approximated attention matrix using low-rank approximations. Such\nmethods enable substantial improvements in computation speed, allowing a single attention layer\nto perform both training and inference nearly as fast as linear time [AS23, AS24b]. Our research\nfurther extends this efficiency to support multi-layer transformer architectures for both training\nand inference. In addition, these techniques can generalize to advanced attention mechanisms,\nsuch as tensor attention, while preserving the almost linear time complexity in both training and\nevaluation phases [AS24c, LSSZ24]. Beyond this, alternative theoretical methods also exist. For\nexample, the conv-basis approach introduced in [LLS+24c] offers another avenue for speeding up\nattention computation.\nGradient Approximation. Using low-rank approximation to approximate the gradient is a\ncommon approach for optimizing the training of transformers by reducing the complexity in the\ncomputations, such as [LSS+24b, LSSZ24, AS24b, HWSL24]. Specifically, [AS24b] extends the\nlow-rank approximation technique developed in [AS23], which studies the forward computation\nof attention to approximate the gradient of the attention computation. In [LSS+24b], they fur-\nther develop the low-rank approximation technique in [AS24b] to study multi-layer transformers,\nshowing they can use nearly linear time to approximate the backward computations of multi-layer\ntransformers. On the other hand, [LSSZ24] generalizes the gradient approximation of [AS24b] to\nanother direction: they use it to study the training of the tensor version of attention computa-\ntion that develops from the forward computation as in [AS24c]. Finally, [HWSL24] leverages the\nlow-rank approximation technique to study the training of Diffusion Transformers (DiTs).\nTheoretical Foundation of LLMs. The attention has become a cornerstone in AI, particu-\nlarly in large language models (LLMs), which excel in NLP tasks such as machine translation,\ntext generation, and sentiment analysis due to their ability to capture complex contextual relation-\nships. However, understanding the attention mechanism from a theoretical perspective remains an\nongoing challenge. Several works have explored the theoretical foundations and computational com-\nplexities of attention [TBY+19, ZHDK23, BSZ23, AS23, SYZ24, CLL+24a, CLL+24c, MOSW22,"}, {"title": "3 Preliminaries on RoPE Attention", "content": "In Section 3.1, we talk about the notation and foundational concepts. In Section 3.2, we formalize\nour problems. In Section 3.3, we talk about polynomial approximation of the exponential function.\nIn Section 3.4, we talk about the time complexity of matrix multiplications, setting up the frame-\nwork for analyzing efficiency in attention computation. In Section 3.5, we talk about the Strong\nExponential Time Hypothesis (SETH). In Section 3.6, we talk about mathematical properties and\ntricks, such as the tensor trick and row-wise Kronecker products, which enable efficient matrix-\nvector operations. In Section 3.7, we give definitions of our key terms. In Section 3.8, we talk\nabout the reformulation of the loss function using the tensor trick and analyze the computational\ncomplexity of the reformulated expressions."}, {"title": "3.1 Notation", "content": "For $n \\in \\mathbb{Z}+\\cup\\{0\\}$, for set $\\{1,2,\\dots,n\\}$, we denote the set by using the notation $[n]$. Here, we define\nthe concept of nearly linear time when the time is $O(n\\log n)$. We introduce the concept of almost\nlinear time when time is $O(n^{1+o(1)})$. Given a as any vector, we say the diagonal matrix of c is $diag(c)$\nwhere $c_i$ means the $i, i$-th entry in the matrix $diag(c)$. For any matrix, we denote the support of\nthe matrix using the notation $supp$, that is, the set of entries where the matrix is nonzero. $B^T$ is\ndefined as $(B^T)_{i,j} := B_{j,i}$. Suppose there are two vectors c, d of the same length. We denote the\nentry-wise multiplication by using the notation $c \\circ d$; that is, the $i$-th entry in that vector is $c_id_i$. To\ndenote the Frobenius norm, for any matrix $B$, we denote it as $||B||_F := \\sqrt{\\sum_{i,j} B_{ij}^2}$; to denote the\nmaximum norm of matrix $B$, we use $||B||_{\\infty} := max_{i,j} |B_{i,j}|$. Suppose there are two matrices $C, D$\nof the same dimensions. We represent the Hadamard product or the entry-wise multiplication by\nusing the notation $C \\circ D$, that is, $(i, j)$-th entry of the matrix is $C_{i,j} \\cdot D_{i,j}$. Let $C\\in \\mathbb{R}^{n_0 \\times m_0}$ and\n$D\\in \\mathbb{R}^{n_1\\times m_1}$. We define $C \\otimes D$ is an $n_0n_1 \\times m_0m_1$ matrix, where $(C \\otimes D)_{(j_0-1)n_1+j_1,(i_0-1)m_2+i_1}$ is\nequal to $C_{j_0, i_0}D_{j_1,i_1}$ for any $j_0 \\in [n_0], i_0 \\in [m_0], j_1 \\in [n_1], i_1 \\in [m_1]$."}, {"title": "3.2 Problem Definition", "content": "Let n be the number of input tokens, and let d be the hidden/feature dimensions. We state the\ngeneralization of the standard RoPE attention from [AS24a].\nDefinition 3.1 (A General Approximate RoPE Attention Computation, ARAttC, Definition 1.1\nin [AS24a]). Let $B > 0$ and $\\epsilon > 0$ denote two parameters. Given a set of matrices $W_{-(n-1)},\\dots,W_{-1},$\n$W_0, W_1,\\dots, W_{n-1} \\in \\mathbb{R}^{d\\times d}$ where $supp(W_i) \\subseteq S$ for all $i \\in \\{-(n - 1), \\dots, -1, 0, 1, \\dots, n - 1\\}$."}, {"title": "3.3 Polynomial Approximation of Exponential", "content": "Here, we will explain a technical tool for controlling the error dependence of our approximate\nalgorithm. In particular, we will use the following optimal-degree polynomial approximation of the\nexponential function.\nLemma 3.4 ([AA22]). Let $B > 1$ and suppose $\\epsilon$ in $(0,0.1)$. We can have P, which has input as a\nscalar and output as a scalar of degree g. g is defined as $\\Theta (max \\{log(1/\\epsilon)/(log(log(1/\\epsilon)/B)), B\\})$\nsuch that for all $x \\in [0, B]$, we can get\n$|P(x) - exp(x)| < \\epsilon$.\nBecause P's coefficients are rational values with numerators and denominators represented using\nintegers of poly(g)-bit size and these coefficients can be determined in poly(g) time, we can calculate\n$P$ in an efficient way."}, {"title": "3.4 Time Complexity of Multiplications", "content": "Here, we introduce the time complexity of multiplying matrices as follows:\nDefinition 3.5. We suppose $n_1, n_2, n_3$, denote any three positive integers. We define $A \\in \\mathbb{R}^{N_1\\times n_2}$\nand $B \\in \\mathbb{R}^{n_2\\times n_3}$. It costs $T_{mat}(n_1, n_2,n_3)$ time to perform $AB$.\nBased on [BCS97, Bl\u00e413], we have the following fact.\nFact 3.6. We suppose $n_1, n_2, n_3$, denote any three positive integers. $T_{mat}(n_1, n_2, n_3) = O(T_{mat}(n_1, n_3, n_2)) =$\n$O(T_{mat} (n_2, n_1, n_3)) = O(T_{mat}(n_2, n_3, n_1)) = O(T_{mat}(n_3, n_1, n_2)) = O(T_{mat} (n_3, n_2, n_1))$."}, {"title": "3.5 SETH Hypothesis", "content": "Here, we introduce the Strong Exponential Time Hypothesis (SETH):\nHypothesis 3.7 ( SETH). $\\forall \\epsilon > 0, \\exists k \\in \\mathbb{Z}+$ and k greater or equal to 3 such that, even when\nutilizing randomized algorithms, within the time of $O(2^{(1-\\epsilon)n})$, we cannot solve k-SAT problems\nwith n variables."}, {"title": "3.6 Basic Facts", "content": "In this section, we introduce some basic facts we will use.\nFact 3.8. We suppose there are three vectors of n dimension x, y, z. Thus, we get\n$\\bullet$ $(x \\circ y, z) = x^T diag(y)z$.\n$\\bullet$ $<x,y> = (x \\circ y, 1_n)$.\nFact 3.9 (Folklore). Let $U_1, V_1 \\in \\mathbb{R}^{n\\times k_1}$. Let $U_2, V_2 \\in \\mathbb{R}^{n\\times k_2}$. Then we have\n$(U_1V_1)^T (U_2V_2) = (U_1 \\otimes U_2)^T (V_1 \\otimes V_2)$\nHere, given $U_1 \\in \\mathbb{R}^{n\\times k_1}$ and $U_2 \\in \\mathbb{R}^{n\\times k_2}$, we define the row-wise Kronecker product as $U_1\\oslash U_2 \\in$\n$\\mathbb{R}^{n\\times k_1k_2}$. That is, $(U_1 \\oslash U_2)_{i,l_1+(l_2-1)k_1} := (U_1)_{i,l_1}U_{i,l_2}$ for all $i \\in [n], l_1 \\in [k_1]$ and $l_2 \\in [k_2]$\nFact 3.10. We suppose $n \\in \\mathbb{Z}+$, and we suppose then dimension vectors a,b,c and a scalar d.\nThen, we have\n$\\bullet$ $<da,b> = d<a,b> = <a, db> = d<b,a>$.\n$\\bullet$ $<a + c,b> = <a, b> + <c,b>$.\n$\\bullet$ $<a,b> = a^Tb$.\n$\\bullet$ $<a\\circ c,b> = <a,b \\circ c>$.\n$\\bullet$ $<a \\circ b, c> = b^T diag(a)c$\nHere, we introduce a technique called the tensor trick.\nFact 3.11 (Tensor trick). Let $X \\in \\mathbb{R}^{d \\times d}$. Let $x \\in \\mathbb{R}^{d^2}$ be the vectorization of $X$. Let there be two\nn \u00d7 d matrices $A_1, A_2$, and we define $A = A_1 \\otimes A_2$. Then, we can get $vec(A_1XA_2) = Ax$.\nNow, we introduce the following properties.\nFact 3.12. Let there be two n \u00d7 d matrices $A_1, A_2$, and we define $A = A_1 \\otimes A_2$. Let $X \\in \\mathbb{R}^{d\\times d}$.\nLet $A_{j_0} \\in \\mathbb{R}^{n\\times d^2}$ be a block of $A$. We introduce $x \\in \\mathbb{R}^{d^2}$ as the vectorization of $X$. Thus, we get\n$\\bullet$ $(exp(A_1XA_2)_{j_0,*})^T = exp(A_{j_0}x)$\n$\\bullet$ $vec(exp(A_1XA_2)) = exp(Ax)$,\nFor the $j_0$-th row of $exp(A_1XA_2) \\in \\mathbb{R}^{n \\times n}$, we use the notation $exp(A_1XA_2)_{j_0,*}$.\nProof. From Lemma and Def. 3.11, we are able to prove this fact."}, {"title": "3.7 Useful Definitions", "content": "Here, we denote $d^4$-dimensional vector $x \\in \\mathbb{R}^{d^2}$ as the vectorization of a $d^2 \\times d^2$ matrix $X$. We\nintroduce some terms to simplify our notations in calculation.\nDefinition 3.13. We suppose there are two $n^2 \\times d^2$ matrices A, W. We define $\\tilde{A}$ as $A\\oslash W$, which is\nan $n^2 \\times d^4$ matrix. We use $A_{j_0}$ to denote the an n \u00d7 $d^4$ subblock of $\\tilde{A}$, given that the total counts of\nsubblocks is n. The function is defined as $u(x)_{j_0}$ maps a $d^4$ dimensional vector to an n-dimensional\nvector with every $j_0 \\in [n]$ such that\n$u(x)_{j_0} := exp(\\tilde{A}_{j_0}x)$.\nDefinition 3.14. We suppose two $n^2 \\times d^2$ matrices A, W. Suppose that $\\tilde{A} := A\\oslash W\\in \\mathbb{R}^{n^2\\times d^4}$. We\nuse $\\bar{A}_{j_0}$ to denote the an n \u00d7 $d^4$ subblock of $\\tilde{A}$, given the counts of total subblocks is n. The function\nis defined as $a(x)_{j_0}$ maps from a $d^4$-dimensional vector to a scalar with every $j_0 \\in [n]$ such that\n$a(x)_{j_0} := <exp(\\bar{A}_{j_0}x), 1_n>$.\nDefinition 3.15. From Def. 3.13, it defines $u(\\cdot)_{j_0}$, and we have $a(\\cdot)_{j_0}$ based on Def. 3.14. The\nfunction $s(x)_{j_0}$ maps a $d^4$-dimensional vector to an n-dimensional vector given every $j_0 \\in [n]$ such\nthat\n$s(x)_{j_0} := a(x)_{j_0}^{-1} u(x)_{j_0}$\nDefinition 3.16. Let $A_3 \\in \\mathbb{R}^{n \\times d}$ be a matrix. We define $v(y)_{i_0}$ as the $i_0$-th column of $v(y)$. We\ndefine the function $v(y)_{i_0}$ maps a $d^2$-dimensional vector to an n-dimensional vector, given each $i_0$\nin the set $[d]$, such that\n$v(y)_{i_0} := A_3Y_{*,i_0}$\nwhere y $\\in \\mathbb{R}^{d^2}$ is the vectorization of n x n matrix Y.\nDefinition 3.17. From Def. 3.15, with every $j_0$ in the set $[n]$, it gives $s(x)_{j_0}$ as an n-dimensional\nnormalized vector, and we define $v(y)_{i_0}$ based on Def. 3.16 given that each $i_0 \\in [d]$. Defining a\nfunction $l(x)_{j_0,i_0}$ maps a $d^4$-dimensional vector to a scalar with each $j_0 \\in [n]$ and each $i_0 \\in [d]$ such\nthat\n$l(x)_{j_0,i_0} := <s(x)_{j_0}, v(Y)_{i_0}> - E_{j_0,i_0}$.\nHere $E_{j_0,i_0}$ is the $(j_0, i_0)$-th coordinate of $E \\in \\mathbb{R}^{n \\times d}$ for each $j_0$ in the set $[n]$ and $i_0$ in the set $[d]$,\nthat is $l(x) = s(x)v(y) - E$.\nDefinition 3.18. Here we let $Loss(x)_{j_0,i_0} := 0.5l(x)_{j_0,i_0}^2$ with every $j_0$ in the set $[n]$ and $i_0$ in the\nset $[d]$."}, {"title": "3.8 Reformulation of the Loss Function", "content": "Based on the notation in Section 3.7, we can reformulate and simplify our loss function.\nLemma 3.19. With the following conditions\n$\\bullet$ Given three n \u00d7 d input sequence matrices $A_1, A_2$, and $A_3$,"}, {"title": "4 ROPE Attention Gradient Calculation", "content": "In this section, we compute the gradient of RoPE attention.\nLemma 4.1. If we have for every i \u2208 [d],\n$\\bullet$ The column function $u(x)_{j_0} \\in \\mathbb{R}^n$ (Definitions 3.13),\n$\\bullet$ $a(x)_{j_0}$ is a real number (Def. 3.14),\n$\\bullet$ $s(x)_{j_0}$ is an arbitrary element in $\\mathbb{R}^n$ (Def. 3.15),\n$\\bullet$ $l(x)_{j_0,i_0}$ is a real number (Def. 3.17), and\n$\\bullet$ $Loss(x)_{j_0,i_0}$ is a real number (Def. 3.18).\nThen, we have $\\forall j_0 \\in [n]$, $\\forall i_0 \\in [d]$,"}, {"title": "5 Exact Gradient Computation Time", "content": "In this section, we analyze the time complexity of exact gradient computation. In Section 5.1, we\nreformulate the closed form of the gradient. In Section 5.2, we show the time complexity for s(x)\nand v(y). In Section 5.3, we show the time complexity for l(x). In Section 5.4, we show the time\ncomplexity for $\\beta(x)$ and $\\gamma(x)$. In Section 5.5, we show the total time complexity for computing the\ngradient of RoPE attention."}, {"title": "5.1 Reformulate the Gradient into Its Closed Form", "content": "Lemma 5.1 (Gradient Reformulation, $\\frac{dLoss(x)_{j_0,i_0}}{dx}$). If we have for every i \u2208 [d],\n$\\bullet$ The column function $u(x)_{j_0} \\in \\mathbb{R}^n$ (Definitions 3.13),\n$\\bullet$ $a(x)_{j_0}$ is a real number (Def. 3.14),\n$\\bullet$ $s(x)_{j_0}$ is an arbitrary element in $\\mathbb{R}^n$ (Def. 3.15),\n$\\bullet$ $l(x)_{j_0,i_0}$ is a real number (Def. 3.17), and\n$\\bullet$ $Loss(x)_{j_0,i_0}$ is a real number (Def. 3.18).\nThen, we have\n$\\frac{dLoss(x)_{j_0,i_0}}{dx} = l(x)_{j_0, i_0} (diag(s(x)_{j_0}) A_3Y_{+,i_0} - s(x)_{j_0}s(x)_{j_0}A_3Y_{+,i_0})$"}, {"title": "5.2 Time to Get s(x) and v(y) Functions", "content": "Lemma 5.4. Pick s(x) and v(y) from Def. 3.15 and Def. 3.16, then it costs $O(T_{mat}(n,d,d) +$\n$T_{mat} (n, d, n))$ time to get s(x), and it costs $T_{mat}(n,d,d)$ time to get v(y)."}, {"title": "5.3 Time Complexity for Computing l(x) Functions", "content": "Lemma 5.5. We have l(x) from Def. 3.17, then it costs $T_{mat}(n, n, d) + O(nd)$ to calculate l(x)."}, {"title": "5.4 Time Complexity for Computing \u03b2(x) and y(x) Functions", "content": "Lemma 5.6. Let $\\beta(x) \\in \\mathbb{R}^{n \\times n}$ be defined as $\\beta(x) := l(x)v(y)$ and $\\gamma(x)$ be defined as $\\gamma(x)_j :=$\n$(diag(s(x)_{j_0} - s(x)_{j_0}s(x)])\\beta(x)_{j_0} \\in\\mathbb{R}^n$, given that $s(x) \\in \\mathbb{R}^{n \\times n}$ then $\\beta(x)$ can be computed in time\nof $O(T_{mat} (n, n, d))$ and $\\gamma(x)$ can be computed in time of $O(n^2)$."}, {"title": "5.5 Time Complexity for Computing the Gradient of RoPE Attention", "content": "Lemma 5.7 (RoPE attention gradient computation time complexity). We define three n \u00d7 d input\nsequence matrices as $A_1, A_2, A_3$, and the n \u00d7 d approximated attention computation matrix as E.\nWe define several input fixed matrices as $X_1, X_2,Y \\in \\mathbb{R}^{d\\times d}$. We define $X = X_1 \\otimes X_2, A = A_1 \\backslash A_2$.\nWe define x := vec(X) and try to get the Loss function gradient. Let $g := \\frac{dLoss(X_1,X_2)}{dx}$ where\n$Loss(X_1, X_2)$ from Def. 3.3. Then, it costs $O(T_{mat}(n, d, d) + T_{mat}(n, d, n))$ time to get the gradient\n$g \\in \\mathbb{R}^d$."}, {"title": "6 Low Rank Approximation of Attention", "content": "This section presents the fast running time using the polynomial method and Fast Fourier transform.\nLemma 6.1 (Theorem 1.3 from [AS24a]). Suppose d = O(log n) and B = o($\\sqrt{log n}$). There is an\n$n^{1+o(1)}$ time algorithm to approximate ArAttC up to $\\epsilon$ = 1/ poly(n) additive error."}, {"title": "6.1 Approximate s Using Low Rank Approximation", "content": "Lemma 6.2. For any B = o($\\sqrt{log n}$), let $k_1$ equals to $n^{o(1)}$ such that: Suppose we have two n \u00d7 d\nmatrices $A_1, A_2, X_1, X_2 \\in \\mathbb{R}^{d\\times d}$ and $X = X_1 \\otimes X_2 \\in \\mathbb{R}^{d^2\\times d^2}$. Assume we can use O(log n) bits\nto write every entry from s(x). It holds that $max\\{||A_1X_1||_{\\infty}, ||A_2X_2||_{\\infty}\\} < B$, then there are three\nmatrices $U_1, V_1, W_1 \\in \\mathbb{R}^{n\\times k_1}$ such that $||U_1V_1 - s(x)||_{\\infty} \\leq \\epsilon/poly(n)$. Here s(x) = $D^{-1}A \\in \\mathbb{R}^{n \\times n}$\nwhere A is defined as the matrix representation of $exp((A \\oslash W) vec(X))$, and $D = diag(A/d)1_n$.\nMoreover, these matrices $U_1, V_1$ can be created explicitly in $n^{1+o(1)}$ time."}, {"title": "6.2 Approximate l Using Low Rank Approximation", "content": "Lemma 6.3. Let d equal O(log n). Suppose we can use O(log n) bits to write every entry in\nE,v(y) $\\in \\mathbb{R}^{n \\times d}$. Define the l(x) $\\in \\mathbb{R}^{n\\times d}$ as specified in Def. 3.17. Then, we have $U_1, V_1 \\in \\mathbb{R}^{n\\times k_1}$\nsuch that $||U_1V_1v(y) - E - l(x)||_{\\infty} \\leq \\epsilon/ poly(n)$."}, {"title": "6.3 Approximate \u00df Using Low Rank Approximation", "content": "Lemma 6.4. Let $k_2 = n^{o(1)}$. We define l(x) $\\in \\mathbb{R}^{n \\times d}$ based on Def. 3.17, and v(y) $\\in \\mathbb{R}^{n \\times d}$ based\non Def. 3.16. We suppose $\\beta(x)$ is equal to $v(y)l(x)^T$, which is an n \u00d7 n matrix. Let $U_2, V_2 \\in \\mathbb{R}^{n\\times k_2}$\nsuch that $||U_2V_2 - \\beta(x)||_{\\infty} \\leq \\epsilon/ poly(n)$. In $n^{1+o(1)}$ time, we can get $U_2, V_2$."}, {"title": "6.4 Approximate y Using Low Rank Approximation", "content": "Lemma 6.5. Let $k_1 = n^{o(1)}$. Let $k_2 = n^{o(1)}$. We suppose $\\gamma_1(x)$ is diag(s(x))$\\beta(x)$, and $U_1, V_1$ be\ntwo n\u00d7 $k_1$ matrices, in which $||U_1V_1 - \\beta(x)||_{\\infty} \\leq \\frac{\\epsilon}{poly(n)}$. We suppose two n\u00d7$k_2$ matrices $U_2, V_2$ in\nwhich $||U_2V_2 - \\beta(x)||_{\\infty} \\leq \\frac{\\epsilon}{poly(n)}$. Then we have two n \u00d7 $k_3"}]}