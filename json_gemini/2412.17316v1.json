[{"title": "Fast Gradient Computation for RoPE Attention in Almost Linear Time", "authors": ["Yifang Chen", "Jiayan Huo", "Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "The Rotary Position Embedding (RoPE) mechanism has become a powerful enhancement to the Transformer architecture, which enables models to capture token relationships when encoding positional information. However, the ROPE mechanisms make the computations of attention mechanisms more complicated, which makes efficient algorithms challenging. Earlier research introduced almost linear time, i.e., $n^{1+o(1)}$ where $n$ is the number of input tokens, algorithms for the forward computation under specific parameter settings. However, achieving a subquadratic time algorithm for other parameter regimes remains impossible unless the widely accepted Strong Exponential Time Hypothesis (SETH) is disproven. In this work, we develop the first almost linear time algorithm for backward computations in the RoPE-based attention under bounded entries. Our approach builds on recent advancements in fast ROPE attention computations, utilizing a novel combination of the polynomial method and the Fast Fourier Transform. Furthermore, we show that with lower bounds derived from the SETH, the bounded entry condition is necessary for subquadratic performance.", "sections": [{"title": "Introduction", "content": "The GPT-03 [Ope24], Llama 3.3 [LT24, AI24], Claude 3.5 [Ant24a] are transformed-based Large Language Models (LLMs), have become important tools in natural language processing, which enables applications from machine translation to sentiment analysis. In the Transformer architecture, attention mechanisms, computationally intensive operations, compute token correlations within the sequence [VSP+17]. The efficiency of attention computations, both in forward computations and backward gradient computations, directly influenced the scalability and feasibility of training LLMs, especially when the size and input context length of these LLMs continue to grow [AS24c, AS23]. In recent research, Rotating Position Embedding (ROPE) [SAL+24] has become a popular modification to the attention mechanism, and it has enabled models to capture positional relationships between tokens with better expressiveness. The RoPE mechanism has been adopted in state-of-the-art models, such as Llama [TLI+23, TMS+23, LT24], Claude [Ant24a], Apple's LLMs [GWW+24, MGF+24], and many others, but the implementation of RoPE complicates attention computation due to the additional structure imposed by position-dependent rotations [SAL+24]. In recent work, [AS24a] has demonstrated an efficient algorithm for forward computation of RoPE attention in the bounded entry regime, but the backward computation, the process of calculating gradients for model optimization, is less explored.\nBackward computation introduces additional complexity because it requires the evaluation of gradients that involve non-linear transformations of the attention matrix and positional embeddings. In [AS23], they present their algorithm to approximate forward computations of fast attention with bounded entries using the polynomial methods and low-rank approximation. In [AS24b], they propose almost linear time, i.e., $n^{1+o(1)}$ where $n$ is the number of input tokens, an algorithm to compute backward gradients for fast attention with bounded entries. In recent work, [AS24a] proposes an efficient algorithm to perform the forward computation of RoPE-based attention using the polynomial methods and Fast Fourier Transform. Therefore, it is natural to raise the key question:\nCan backward computations for the RoPE attention match the efficiency of their forward computations in the bounded entry regime?\nIn this work, we aim to address the question by presenting the first efficient algorithm for backward computation in RoPE attention under the bounded entry. Our main result shows that the backward gradient computations for the RoPE attention match their forward version's efficiency. Therefore, by leveraging our algorithm in approximating backward computations in the RoPE attention with the forward algorithm from [AS24a], we will improve the overall time complexity of RoPE attention to almost linear time with bounded entries.\nTo the best of our knowledge, this is the first work to characterize the fine-grained complexity of backward computations in RoPE attentions, extending prior results on forward computations in RoPE attention [AS24a].\nRoadmap. In Section 2, we present some relevant papers. In Section 3, we show important components for the RoPE attention. In Section 4, we present the details of getting the gradients of RoPE-based attention mechanisms. Section 5 discusses the exact time complexity for computing ROPE attention gradients. Section 6 shows the fast computation of gradient using the polynomial method. Section 7 details the lower bounds of hardness. Finally, Section 8 provides conclusions and avenues for future work."}, {"title": "Related Work", "content": "Rotary Position Embedding. In this paper, we study a variant of attention called RoPE attention. At a high level, RoPE gives more expressive power to the model in exchange for making the computational problem more complicated. In particular, many prior algorithms, such as the algorithm of [AS23], no longer apply to RoPE for fundamental reasons we will discuss. ROPE was proposed by [SAL+24] and has been used extensively in large-scale industrial models. Examples which are known to use RoPE include the open-source models released by Meta such as Llama [TLI+23] (see page 3), Llama 2 [TMS+23] (see page 5), Llama 3 [LT24] (see page 7), and the close-source LLM Claude 3.5 [Ant24a] released by Anthropic. Apple also incorporates RoPE into their LLM architecture (see [MGF+24], and page 3 of [GWW+24]). The idea behind ROPE is to rotate the query and key vectors in the self-attention mechanism. The rotation is position-dependent and designed such that the inner product between two position-encoded vectors reflects their relative position in the input so that in the ROPE attention mechanism, pairs of tokens with a longer relative distance will have a smaller correlation.\nFast Attention Computation. The attention mechanism has often been criticized for its quadratic computational complexity concerning context length, a challenge that becomes more pronounced as the sequence length grows in today's LLMS [AAA+23, Ope24, LT24, AI24, Ant24b, Ant24a]. However, this issue can be addressed using polynomial kernel approximation methods [AA22], which facilitate constructing the approximated attention matrix using low-rank approximations. Such methods enable substantial improvements in computation speed, allowing a single attention layer to perform both training and inference nearly as fast as linear time [AS23, AS24b]. Our research further extends this efficiency to support multi-layer transformer architectures for both training and inference. In addition, these techniques can generalize to advanced attention mechanisms, such as tensor attention, while preserving the almost linear time complexity in both training and evaluation phases [AS24c, LSSZ24]. Beyond this, alternative theoretical methods also exist. For example, the conv-basis approach introduced in [LLS+24c] offers another avenue for speeding up attention computation.\nGradient Approximation. Using low-rank approximation to approximate the gradient is a common approach for optimizing the training of transformers by reducing the complexity in the computations, such as [LSS+24b, LSSZ24, AS24b, HWSL24]. Specifically, [AS24b] extends the low-rank approximation technique developed in [AS23], which studies the forward computation of attention to approximate the gradient of the attention computation. In [LSS+24b], they further develop the low-rank approximation technique in [AS24b] to study multi-layer transformers, showing they can use nearly linear time to approximate the backward computations of multi-layer transformers. On the other hand, [LSSZ24] generalizes the gradient approximation of [AS24b] to another direction: they use it to study the training of the tensor version of attention computation that develops from the forward computation as in [AS24c]. Finally, [HWSL24] leverages the low-rank approximation technique to study the training of Diffusion Transformers (DiTs).\nTheoretical Foundation of LLMs. The attention has become a cornerstone in AI, particularly in large language models (LLMs), which excel in NLP tasks such as machine translation, text generation, and sentiment analysis due to their ability to capture complex contextual relationships. However, understanding the attention mechanism from a theoretical perspective remains an ongoing challenge. Several works have explored the theoretical foundations and computational complexities of attention [TBY+19, ZHDK23, BSZ23, AS23, SYZ24, CLL+24a, CLL+24c, MOSW22,"}, {"title": "Preliminaries on RoPE Attention", "content": "In Section 3.1, we talk about the notation and foundational concepts. In Section 3.2, we formalize our problems. In Section 3.3, we talk about polynomial approximation of the exponential function. In Section 3.4, we talk about the time complexity of matrix multiplications, setting up the framework for analyzing efficiency in attention computation. In Section 3.5, we talk about the Strong Exponential Time Hypothesis (SETH). In Section 3.6, we talk about mathematical properties and tricks, such as the tensor trick and row-wise Kronecker products, which enable efficient matrix-vector operations. In Section 3.7, we give definitions of our key terms. In Section 3.8, we talk about the reformulation of the loss function using the tensor trick and analyze the computational complexity of the reformulated expressions."}, {"title": "Notation", "content": "For $n \\in \\mathbb{Z_+}\\cup\\{0\\}$, for set $\\{1, 2,\\dots,n\\}$, we denote the set by using the notation $[n]$. Here, we define the concept of nearly linear time when the time is $O(n\\log n)$. We introduce the concept of almost linear time when time is $O(n^{1+o(1)})$. Given a as any vector, we say the diagonal matrix of c is $\\text{diag}(c)$ where $c_i$ means the $i, i$-th entry in the matrix $\\text{diag}(c)$. For any matrix, we denote the support of the matrix using the notation supp, that is, the set of entries where the matrix is nonzero. $B^T$ is defined as $(B^T)_{i,j} := B_{j,i}$. Suppose there are two vectors $c, d$ of the same length. We denote the entry-wise multiplication by using the notation $c \\circ d$; that is, the $i$-th entry in that vector is $c_id_i$. To denote the Frobenius norm, for any matrix $B$, we denote it as $||B||_F := \\sqrt{\\sum B_{ij}^2}$; to denote the maximum norm of matrix $B$, we use $||B||_{\\infty} := \\text{max}_{i,j} |B_{i,j}|$. Suppose there are two matrices $C, D$ of the same dimensions. We represent the Hadamard product or the entry-wise multiplication by using the notation $C \\circ D$, that is, $(i, j)$-th entry of the matrix is $C_{i,j} \\cdot D_{i,j}$. Let $C \\in \\mathbb{R}^{n_0 \\times m_0}$ and $D \\in \\mathbb{R}^{n_1 \\times m_1}$. We define $C \\otimes D$ is an $n_0n_1 \\times m_0m_1$ matrix, where $(C \\otimes D)_{(j_0-1)n_1+j_1,(i_0-1)m_2+i_1}$ is equal to $C_{j_0, i_0} D_{j_1, i_1}$ for any $j_0 \\in [n_0], i_0 \\in [m_0], j_1 \\in [n_1], i_1 \\in [m_1]$."}, {"title": "Problem Definition", "content": "Let $n$ be the number of input tokens, and let $d$ be the hidden/feature dimensions. We state the generalization of the standard RoPE attention from [AS24a].\nDefinition 3.1 (A General Approximate RoPE Attention Computation, ARAttC, Definition 1.1 in [AS24a]). Let $B > 0$ and $\\epsilon > 0$ denote two parameters. Given a set of matrices $W_{-(n-1)},\\dots,W_{-1}, W_0, W_1,\\dots, W_{n-1} \\in \\mathbb{R}^{d\\times d}$ where $\\text{supp}(W_i) \\subseteq S$ for all $i \\in \\{-(n - 1),\\dots, -1, 0, 1,\\dots, n - 1\\}$."}, {"title": null, "content": "Here $S \\subseteq [d] \\times [d]$ where $|S| = O(d)$. Given three $n \\times d$ matrices $Q, K, V$ with the guarantee that $||Q||_{\\infty}, ||K||_{\\infty}, ||V||_{\\infty} \\le B$ and $||W||_{\\infty} \\le 1$. We define matrix $A \\in \\mathbb{R}^{n \\times n}$ as, for $i, j\\in [n]$,\n$A_{i,j} := \\exp(Q_{i,*} W_{i-j} K_{j,*}/d)$.\nWe define $D := \\text{diag}(A\\mathbf{1}_n)$. The goal of General Approximate RoPE Attention Computation is to output a matrix $T \\in \\mathbb{R}^{n\\times d}$ such that $||T - \\text{ARAttC}||_{\\infty} < \\epsilon$ is small, where $\\text{ARAttC} := D^{-1}AV$. For matrix $M$, we use $||M||_{\\infty} := \\text{max}_{i,j}|M_{i,j}|$. Note that the $1/d$ factor inside $\\exp$ in the definition of $A$ is a normalization factor.\nOur focus is to find weights to fit the attention to a desired output. Let $Q := A_1X_1, K := A_2X_2$, and $V := A_3Y$. We use $X_1, X_2$, and $X_3$ to represent the weights $W_Q, W_K$ and $W_V$, respectively. We use $A_1, A_2$, and $A_3$ to replace the input matrix to handle the more general settings such as cross attention. Then, the attention matrix is as follows.\n$A(X_1, X_2)_{i,j} := \\exp((A_1X_1)_{i,*} W_{i-j} (A_2X_2)_{*,j}/d) = \\exp(A_{1,i,*}X_1W_{i-j}X_2 A_{2,j,*}/d)$.\nWe define $w_{i-j} := \\text{vec}(W_{i-j}) \\in \\mathbb{R}^{d^2}$ and define $W$ such that $W_{j,*}$ is an $1 \\times d^2$ block and $W_{i+(j-1)n,*} := w_{i-j}^T$. Here, let $A := A_1 \\otimes A_2 \\in \\mathbb{R}^{n^2 \\times d^2}$ and $X := X_1 \\otimes X_2 \\in \\mathbb{R}^{d^2 \\times d^2}$. We can show that\n$A_{1,i,*}X_1W_{i-j}X_2 A_{2,j,*} = (A_{1,i,*} A_{2,j,*}) (X_1 \\otimes X_2) \\text{vec}(W_{i-j}) = A_{i+(j-1)n,*} X W_{i-j}$,\nwhere the first step uses the tensor trick, and the second step uses the definitions of $w_{i-j}, A$, and $X$. Thus we can reformulate the attention matrix $A$ as, for $i, j\\in [n]$\n$A_{i,j}(X) = \\exp(A_{i+(j-1)n,*} \\frac{X}{d} W_{i-j}/d)$.\nUsing the tensor trick again, we have\n$A_{i,j}(X) = \\exp((A_{i+(j-1)n,*} \\frac{w_{i-j}}{1\\times d^2}) \\text{vec}(X)/d) = \\exp((A_{i+(j-1)n,*} W_{i+(j-1)n,*}) \\frac{\\text{vec}(X)}{d})$.\nHence, by definition of row-wise Kronecker product, we have\n$\\text{vec}(A(X)) = \\exp((\\frac{A \\otimes W}{n^2 \\times d^4}) \\frac{\\text{vec}(X)}{d})$.\nWe define the matrix $D(X) \\in \\mathbb{R}^{n \\times n}$ as\n$D(X) = \\text{diag}(A(X) \\mathbf{1}_n)$.\nThen, the optimization problem in the context of RoPE attention computation is described as:\nDefinition 3.2 (Optimize RoPE Attention). Let $B > 0$ and $\\epsilon > 0$ denote two parameters. Given a set of matrices $W_{-(n-1)},\\dots,W_{-1}, W_0, W_1,\\dots, W_{n-1} \\in \\mathbb{R}^{d\\times d}$ where $\\text{supp}(W_i) \\subseteq S$ for all $i \\in \\{-(n - 1),\\dots, -1, 0, 1,\\dots, n - 1\\}$. Here $S \\subseteq [d] \\times [d]$ where $|S| = O(d)$. For $i,j \\in [n]$, let $W \\in \\mathbb{R}^{n^2 \\times d^2}$ such that $W_{i+(j-1)n,*} = \\text{vec}(W_{i-j})$. Here, we suppose four $n\\times d$ matrices $A_1, A_2, A_3, E$, and we have three $d \\times d$ matrices $X_1, X_2, Y$. Let $X := X_1 \\otimes X_2 \\in \\mathbb{R}^{d^2 \\times d^2}$. We define the matrix"}, {"title": null, "content": "$A(X) \\in \\mathbb{R}^{n \\times n}$ as the matrix representation of $\\exp((\\frac{A \\otimes W}{n^2\\times d^4}) \\frac{\\text{vec}(X)}{d})$ and the $n \\times n$ matrix $D(X) :=\n\\text{diag}(A(X) \\mathbf{1}_n )$. The RoPE attention optimization problem is formulated as:\n$\\min_{X\\in \\mathbb{R}^{d^2 \\times d^2}} \\text{Loss}(X) :=\n\\min_{X\\in \\mathbb{R}^{d^2 \\times d^2}} 0.5||D(X)^{-1}A(X)A_3Y - E||_F^2$.\nNote that we are able to get the gradient computation of Loss with respect to $X_1$ or $X_2$ based on the chain rule because\n$\\frac{d \\text{Loss}(X_1, X_2)}{d X_1} = \\frac{d \\text{Loss}(X)}{d X} \\frac{dX}{d X_1} = \\frac{d \\text{Loss}(X)}{d X} \\frac{d (X_1 \\otimes X_2)}{d X_1} = (\\frac{d \\text{Loss}(X)}{d X}) (I_{d\\times d} \\otimes X_2)$.\nOur approximation task can be formalized as follows.\nDefinition 3.3 (The Approx of the gradient of RoPE Attention Loss Function, ARAttLGC(n, d, B, $\\epsilon$)). Let $B > 0$ and $\\epsilon > 0$ denote two parameters. Given a set of matrices $W_{-(n-1)},\\dots,W_{-1}, W_0, W_1,\\dots, W_{n-1} \\in \\mathbb{R}^{d\\times d}$ where $\\text{supp}(W_i) \\subseteq S$ for all $i \\in \\{-(n - 1),\\dots, -1, 0, 1,\\dots, n - 1\\}$. Here $S \\subseteq [d] \\times [d]$ where $|S| = O(d)$. For $i,j \\in [n]$, let $W \\in \\mathbb{R}^{n^2 \\times d^2}$ such that $W_{i+(j-1)n,*} = \\text{vec}(W_{i-j})$. Let $X_1, X_2, Y \\in \\mathbb{R}^{d\\times d}$. Let $X := X_1 \\otimes X_2 \\in \\mathbb{R}^{d^2 \\times d^2}$. We have four $n \\times d$ matrices Let $A_1, A_2, A_3, E$. Let $A \\in \\mathbb{R}^{n^2 \\times d^2}$ such that $A$ equals to an $n^2 \\times d^2$ matrix from $A_1 \\otimes A_2$. Assume $||A_1X||_{\\infty} < B, ||A_2X ||_{\\infty} \\le B, ||A_3Y||_{\\infty} < B, ||W||_{\\infty} \\le 1$. Assume that all the $\\log(n)$ bits model is applied throughout all numbers in matrices. We define $\\text{Loss}(X)$ from Def. 3.2. Here, we define $\\frac{d \\text{Loss}(X)}{d X}$ as the loss function gradient. Then, our target is to output a vector $g \\in \\mathbb{R}^{d^2}$ satisfying:\n$||g - \\frac{d \\text{Loss}(X)}{d X}||_{\\infty} \\le \\epsilon$."}, {"title": "Polynomial Approximation of Exponential", "content": "Here, we will explain a technical tool for controlling the error dependence of our approximate algorithm. In particular, we will use the following optimal-degree polynomial approximation of the exponential function.\nLemma 3.4 ([AA22]). Let $B > 1$ and suppose $\\epsilon$ in $(0,0.1)$. We can have $P$, which has input as a scalar and output as a scalar of degree $g$. g is defined as $\\Theta (\\text{max} \\{\\log(1/\\epsilon)/(\\log(\\log(1/\\epsilon)/B)), B\\})$ such that for all $x \\in [0, B]$, we can get\n$|P(x) - \\exp(x)| < \\epsilon$.\nBecause $P$'s coefficients are rational values with numerators and denominators represented using integers of poly(g)-bit size and these coefficients can be determined in poly(g) time, we can calculate $P$ in an efficient way."}, {"title": "Time Complexity of Multiplications", "content": "Here, we introduce the time complexity of multiplying matrices as follows:\nDefinition 3.5. We suppose $n_1, n_2, n_3$, denote any three positive integers. We define $A \\in \\mathbb{R}^{N_1\\times n_2}$ and $B \\in \\mathbb{R}^{n_2\\times n_3}$. It costs $T_{\\text{mat}}(n_1, n_2,n_3)$ time to perform $AB$.\nBased on [BCS97, Bl\u00e413], we have the following fact.\nFact 3.6. We suppose $n_1, n_2, n_3$, denote any three positive integers. $T_{\\text{mat}}(n_1, n_2, n_3) = O(T_{\\text{mat}}(n_1, n_3, n_2)) = O(T_{\\text{mat}}(n_2, n_1, n_3)) = O(T_{\\text{mat}}(n_2, n_3, n_1)) = O(T_{\\text{mat}}(n_3, n_1, n_2)) = O(T_{\\text{mat}}(n_3, n_2, n_1))$."}, {"title": "SETH Hypothesis", "content": "Here, we introduce the Strong Exponential Time Hypothesis (SETH):\nHypothesis 3.7 ( SETH). $\\forall \\epsilon > 0$, $\\exists k \\in \\mathbb{Z_+}$ and $k$ greater or equal to 3 such that, even when utilizing randomized algorithms, within the time of $O(2^{(1-\\epsilon)n})$, we cannot solve k-SAT problems with n variables."}, {"title": "Basic Facts", "content": "In this section, we introduce some basic facts we will use.\nFact 3.8. We suppose there are three vectors of n dimension x, y, z. Thus, we get\n*   $\\langle x \\circ y, z\\rangle = x \\text{diag}(y)z$.\n*   $(x,y) = \\langle x \\circ y, \\mathbf{1}_n\\rangle$.\nFact 3.9 (Folklore). Let $U_1, V_1 \\in \\mathbb{R}^{n\\times k_1}$. Let $U_2, V_2 \\in \\mathbb{R}^{n\\times k_2}$. Then we have\n$(U_1V_1)^T (U_2V_2) = (U_1 \\otimes U_2) (V_1 \\otimes V_2)$.\nHere, given $U_1 \\in \\mathbb{R}^{n\\times k_1}$ and $U_2 \\in \\mathbb{R}^{n\\times k_2}$, we define the row-wise Kronecker product as $U_1 \\oslash U_2 \\in [\\mathbb{R}^{n\\times k_1k_2}$. That is, $(U_1\\oslash U_2)_{i,l_1+(l_2-1)k_1} := (U_1)_{i,l_1}U_{i,l_2}$ for all $i \\in [n], l_1 \\in [k_1]$ and $l_2 \\in [k_2]$\nFact 3.10. We suppose $n \\in \\mathbb{Z_+}$, and we suppose the n dimension vectors a,b,c and a scalar d. Then, we have\n*   $\\langle da,b\\rangle = d\\langle a,b\\rangle = \\langle a, db\\rangle = d\\langle b,a\\rangle$.\n*   $\\langle a + c,b\\rangle = \\langle a, b\\rangle + \\langle c,b\\rangle$.\n*   $\\langle a,b \\rangle = a^Tb$.\n*   $\\langle a\\circ c,b\\rangle = \\langle a,b\\circ c\\rangle$.\n*   $\\langle a \\circ b, c\\rangle = b \\text{diag}(a)c$\nHere, we introduce a technique called the tensor trick.\nFact 3.11 (Tensor trick). Let $X \\in \\mathbb{R}^{d \\times d}$. Let $x \\in \\mathbb{R}^{d^2}$ be the vectorization of $X$. Let there be two $n \\times d$ matrices $A_1, A_2$, and we define $A = A_1 \\otimes A_2$. Then, we can get $\\text{vec}(A_1XA_2) = Ax$.\nNow, we introduce the following properties.\nFact 3.12. Let there be two $n \\times d$ matrices $A_1, A_2$, and we define $A = A_1 \\otimes A_2$. Let $X \\in \\mathbb{R}^{d \\times d}$. Let $A_{j_0,*}$  $\\in \\mathbb{R}^{n \\times d^2}$ be a block of $A$. We introduce $x \\in \\mathbb{R}^{d^2}$ as the vectorization of $X$. Thus, we get\n*   $(\\exp(A_1XA_2))_{j_0,*}^T = \\exp(A_{j_0,*}x)$\n*   $\\text{vec}(\\exp(A_1XA_2)) = \\exp(Ax)$,\nFor the $j_0$-th row of $\\exp(A_1XA_2) \\in \\mathbb{R}^{n\\times n}$, we use the notation $\\exp(A_1XA_2)_{j_0,*}$.\nProof. From Lemma and Def. 3.11, we are able to prove this fact."}, {"title": "Useful Definitions", "content": "Here, we denote $d^4$-dimensional vector $x \\in \\mathbb{R}^{d^2}$ as the vectorization of a $d^2 \\times d^2$ matrix $X$. We introduce some terms to simplify our notations in calculation.\nDefinition 3.13. We suppose there are two $n^2 \\times d^2$ matrices A, W. We define $\\tilde{A}$ as $A\\otimes W$, which is an $n^2 \\times d^4$ matrix. We use $A_{j0}$ to denote the an $n \\times d^4$ subblock of A, given that the total counts of subblocks is n. The function is defined as $u(x)_{j0}$ maps a $d^4$ dimensional vector to an n-dimensional vector with every $j_0 \\in [n]$ such that\n$u(x)_{j0} := \\exp(\\tilde{A}_{j0}x)$.\nDefinition 3.14. We suppose two $n^2 \\times d^2$ matrices A, W. Suppose that $\\tilde{A} := A\\otimes W \\in \\mathbb{R}^{n^2 \\times d^4}$. We use $A_{j0}$ to denote the an $n \\times d^4$ subblock of $\\tilde{A}$, given the counts of total subblocks is n. The function is defined as $a(x)_{j0}$ maps from a $d^4$-dimensional vector to a scalar with every $j_0 \\in [n]$ such that\n$a(x)_{j0} := \\langle \\exp(\\tilde{A}_{j0}x), \\mathbf{1}_n \\rangle$.\nDefinition 3.15. From Def. 3.13, it defines $u(\\cdot)_{j0}$, and we have $a(\\cdot)_{j0}$ based on Def. 3.14. The function $s(x)_{j0}$ maps a $d^4$-dimensional vector to an n-dimensional vector given every $j_0 \\in [n]$ such that\n$s(x)_{j0} := a(x)_{j0}^{-1}u(x)_{j0}$.\nDefinition 3.16. Let $A_3 \\in \\mathbb{R}^{n\\times d}$ be a matrix. We define $v(y)_{i0}$ as the $i_0$-th column of $v(y)$. We define the function $v(y)_{i0}$ maps a $d^2$-dimensional vector to an n-dimensional vector, given each $i_0$ in the set $[d]$, such that\n$v(y)_{i_0} := A_3Y_{*,i_0}$\nwhere y$\\in \\mathbb{R}^{d^2}$ is the vectorization of n$\\times$n matrix Y.\nDefinition 3.17. From Def. 3.15, with every $j_0$ in the set $[n]$, it gives $s(x)_{j0}$ as an n-dimensional normalized vector, and we define $v(y)_{i0}$ based on Def. 3.16 given that each $i_0 \\in [d]$. Defining a function $l(x)_{j0,i_0}$ maps a $d^4$-dimensional vector to a scalar with each $j_0 \\in [n]$ and each $i_0 \\in [d]$ such that\n$l(x)_{j0,i_0} := \\langle s(x)_{j0}, v(y)_{i_0} \\rangle - E_{j0,i_0}$.\nHere $E_{j0,i_0}$ is the $(j_0, i_0)$-th coordinate of $E \\in \\mathbb{R}^{n\\times d}$ for each $j_0$ in the set $[n]$ and $i_0$ in the set $[d]$, that is $l(x) = s(x)v(y) - E$.\nDefinition 3.18. Here we let $\\text{Loss}(x)_{j0,i_0} := 0.5 l(x)_{j0,i_0}^2$ with every $j_0$ in the set $[n]$ and $i_0$ in the set $[d]$."}, {"title": "Reformulation of the Loss Function", "content": "Based on the notation in Section 3.7, we can reformulate and simplify our loss function.\nLemma 3.19. With the following conditions\n*   Given three n \u00d7 d input sequence matrices $A_1, A_2$, and $A_3$,"}, {"title": null, "content": "*   Let $A = A_1 \\otimes A_2 \\in \\mathbb{R}^{n^2\\times d^2}$ and $X = X_1 \\otimes X_2 \\in \\mathbb{R}^{d^2 \\times d^2}$, where $\\otimes$ denotes Kronecker product.\n*   Given W is a $n^2 \\times d^2$ matrix, we define $\\tilde{A} = A\\otimes W$. Let $j_0 \\in [n", "n": "and $i_0 \\in [d"}, {"n": "and $i_0$ in the set$[d"}, {"n": ""}, "sum_{i_0\\in[d"]}, "text{Loss}(x)_{j0,i0}$\nProof. We present"]