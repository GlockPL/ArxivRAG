{"title": "TOWARDS THE DEVELOPMENT OF BALANCED SYNTHETIC DATA\nFOR CORRECTING GRAMMATICAL ERRORS IN ARABIC:\nAN APPROACH BASED ON ERROR TAGGING MODEL AND\nSYNTHETIC DATA GENERATING MODEL", "authors": ["Ahlam Alrehili", "Areej Alhothali"], "abstract": "Synthetic data generation is widely recognized as a way to enhance the quality of neural grammatical\nerror correction (GEC) systems. However, current approaches often lack diversity or are too simplistic\nto generate the wide range of grammatical errors made by humans, especially for low-resource\nlanguages such as Arabic. In this paper, we will develop the error tagging model and the synthetic\ndata generation model to create a large synthetic dataset in Arabic for grammatical error correction.\nIn the error tagging model, the correct sentence is categorized into multiple error types by using\nthe DeBERTav3 model. Arabic Error Type Annotation tool (ARETA) is used to guide multi-label\nclassification tasks in an error tagging model in which each sentence is classified into 26 error\ntags. The synthetic data generation model is a back-translation-based model that generates incorrect\nsentences by appending error tags before the correct sentence that was generated from the error\ntagging model using the ARAT5 model. In the QALB-14 and QALB-15 Test sets, the error tagging\nmodel achieved 94.42% F1, which is state-of-the-art in identifying error tags in clean sentences. As a\nresult of our syntactic data training in grammatical error correction, we achieved a new state-of-the-art\nresult of F1-Score: 79.36% in the QALB-14 Test set. We generate 30,219,310 synthetic sentence\npairs by using a synthetic data generation model. Our data is accessible to the public\u00b9.", "sections": [{"title": "Introduction", "content": "Arabic grammar error correction (ArabGEC) has gained popularity in recent years because of the growing demand\nfor robust and accurate natural language processing (NLP) tools for Arabic. A grammatical error correction system\n(GEC) corrects grammatical errors and other forms of error in writing automatically. It guarantees that written texts are\ncoherent, concise, clear, well-organized, and effectively communicate the intended message. The GEC problem can be\nthought of as seq2seq (sequence-to-sequence) (converting incorrect sentences to grammatically correct sentences) and\nis inspired by neural machine translation (NMT)[1][2]. A correct and best choice will automatically be substituted for\nthe incorrect ones in the context without modifying the syntax.\nAmong the official languages of the United Nations, Arabic is classified as a Semitic language, and there were 274\nmillion speakers around the world\u00b2. Natural language processing systems can find Arabic's grammatical structure\nchallenging because of its complex grammatical structure. Moreover, the Arabic language has a rich morphology and"}, {"title": "Related Work", "content": "In this section, we provide an overview of Arabic synthetic data techniques, grammatical error correction multi-label\nclassification, pre-trained BERT models, and large language Text-to-Text Transformers used in literature."}, {"title": "Arabic Synthetic Data Techniques", "content": "This section discusses the synthetic data technique used in prior ArabGEC research. Solyman et al. [8] generated\nonly spelling errors using the confusion function. The corrupted sentences were constructed by selecting random\nwords in sentences, then deleting them, duplicating them, substituting letters within them, or inserting new letters.\nEvery sentence had a probability distribution of 10% for duplicating and deleting words and 40% for changing letters\nor deleting characters. An Al-Watan 20021 3corpus was used. Originally, the corpus contained ten million words\nwritten by professional journalists in Modern Standard Arabic (MSA). Al-Watan newspaper provided the data, which is"}, {"title": "Arabic Grammatical Error Correction as Multi-label Classification Task", "content": "In Arabic text, the Arabic Error Type Annotation (ARETA) tool [12], a system for automatically annotating Modern\nStandard Arabic, several steps are required: First, align raw input text with reference text corrected versions Second:\nautomatically determines error types. It involves a four-part process:\n\u2022 Punctuation: Determines whether punctuation errors have occurred using regular expressions.\n\u2022 Regex: Identifies splits, mergers, deletions, insertions, and other orthographic errors using regular expressions.\n\u2022 Ortho-Morph: A morphological analyzer based on CAMEL Tools is used to handle complex orthographic\nand morphological errors. Possible edits are generated and the shortest paths to convert the raw word into the\nreference word are determined, which error tags are associated with these edits.\n\u2022 Multi-Word: Processes one-to-many word pairs by tokenizing them with Arabic Treebank and assigning errors\nto the tokens\nFinally, ARETA can be used to evaluate grammatical error correction systems by comparing predicted error tags with\nreference error tags. Furthermore, it can diagnose the output of a system directly as well as identify remaining error\ntypes. As shown in Table 1, ARETA encompasses several different errors based on extended ALC error types. ARETA's\nannotation process uses a combination of linguistic rules, morphological analysis, and alignment techniques to handle\nArabic's intricacies, including its rich morphology and ambiguous orthography.\nAloyaynaa et al. [13] analyzed Grammatical Error Detection (GED) in Arabic as a low-resource task. A7'ta [14] was\nused along with publicly available datasets such as QALB-14 [4], QALB-15 [5] and SCUT [9]. Several approaches were\nemployed for GED including token-level classification and sentence-level classification. A token-level classification\nwas performed on individual tokens (words). Furthermore, sentence-level classification is used to determine whether a\ncomplete sentence is correct or incorrect. They used pre-trained transformer architectures, specifically mBERT and\nAraBERT, for GED classification. For the AraBERT and mBERT models, F1 scores of 85% and 85% respectively\nshowed high performance metrics: Token-Level Classifications: 87% for the AraBERT model and 86% for the mBERT\nmodel. In terms of sentence-level classification, the AraBERT model scores 98% and is 98% accurate; in terms of the\nmBERT model, it scores 96% and is 96% accurate. Models built on monolingual data performed better than models\ntrained on multilingual data for GED tasks in Arabic, demonstrating the effectiveness of using AraBERT pre-trained\nmodels."}, {"title": "BERT Pre-trained Models", "content": "Arabic pre-trained models have been used in various research projects to achieve outstanding results on a wide range\nof NLP tasks. The Arabic language is supported by some, while others support multiple languages, including Arabic.\nTable 2 presents an overview of the Arabic and multilingual BERT language model.\nAraBERT [15] was an early attempt to train a monolingual BERT model with Arabic news extracted from a variety\nof Arabic news sources. There are approximately 23 gigabytes of text in AraBERTv1 of the model, consisting of 77\nmillion sentences and 2.7 billion tokens. The newest version (AraBERTv2) is pre-trained with 77 gigabytes of text, 3.5\ntimes more than the previous version. AraBERT consists of 12 transformer blocks, each containing 768 hidden units.\nAs well as 110 million trainable parameters, it has 12 self-attention heads. An additional 12,000 sentences written in\ndifferent dialects of Arabic were pre-trained on the model to support dialectical Arabic. SalamBERT was the name of\nthis customized version of AraBERT[16].\nARBERT [17] is a masked language model that is pre-trained and aimed at Modern Standard Arabic (MSA). Several\nsources were used in the training model, including Wikipedia, news sources, and books. There are 6.2 billion tokens in\nit and it's about 61 gigabytes in size. ARBERT's structure comprises 12 layers of transformer blocks, 768 hidden units,\nand 12 self-attention heads. However, ARBERT has 163 million parameters that can be trained.\nMARBERT [17] was pre-trained on a massive amount of Twitter data that included both MSA and various Arabic\ndialects. It was also developed specifically for the Arabic language. Almost 128 gigabytes of text comprise the\npretraining corpus, which contains 1 billion tweets. The number of tokens in the corpora is approximately 15.6 billion,\nwhich is more than double that of Version 2 of AraBERT. The MARBERT model is based on the multilingual BERT\narchitecture, but it does not include the next-sentence prediction feature. The next-sentence prediction was omitted\nbecause tweets are too short, according to the model's developers. A total of 160 million parameters can be trained in\nMARBERT.\nCAMELBERT [18] developed three pre-trained language models for Arabic: Modern Standard Arabic (MSA), dialectal\nArabic, and classical Arabic, as well as a fourth model that was pre-trained on a mix of these three. The pretraining\ncorpus of CAMELBERT-MSA comprises almost 107 gigabytes. There are approximately 12.6 billion tokens in the\ncorpora. The size of CAMELBERT-DA is about 54 gigabytes and it contains 5.8 billion tokens. CAMELBERT-CA\ncontains 864 million tokens and is about 6 gigabytes in size. CAMELBERT-Mix contains 17.3 billion tokens and is\nabout 167 gigabytes in size. The CAMELBERT model is based on the multilingual BERT architecture. A total of five\nNLP tasks were analyzed across 12 datasets, including sentiment analysis, part-of-speech tagging, dialect identification,\nnamed entity recognition (NER), and poetry classification. According to the average results, AraBERTv02 performed\nbest across several subtasks. CAMELBERT-Mix outperformed best models in dialectal contexts, but not consistently in\nother cases.\nThe DeBERTa model [19] is an enhancement to the BERT and ROBERTa architectures, incorporating two innovation\ntechniques: enhanced mask decoding and disentangled attention. The enhanced mask decoder predicts masked\ntokens using absolute positions during pretraining. Disentangled attention allows for more effective attention weight\ncalculations by representing each word with two vectors-one for content and another for position. Furthermore, the\nmodel's generalization on downstream tasks is improved using a new virtual adversarial training method. The DeBERTa\nmodel is mainly trained on English text, with 1.5 billion parameters, and has passed the human baseline of 89.816 on\nthe SuperGLUE benchmark, exceeding human performance for the first time. During the pre-training phase, DeBERTa\nuses about 78GB of data, which has been duplicated. This dataset contains several sources, including Wikipedia:\n12GB, BookCorpus: 6GB, OPENWEBTEXT: 38GB (public Reddit content), and STORIES: 31GB (a subset of\nCommonCrawl).\nIn contrast to the traditional masked language modeling (MLM) approach, DeBERTaV3 [20] uses a more efficient\ntraining objective called Replaced Token Detection (RTD) instead of the original DeBERTa. Gradient-disentangled\nembedding sharing is incorporated into this model to improve training efficiency. There are several variants of\nDeBERTaV3: DeBERTaV3large, DeBERTaV3base, DeBERTaV3small, and DeBERTaV3xsmall. Models developed"}, {"title": "Text-to-Text Transformer", "content": "A state-of-the-art model for natural language processing (NLP) tasks, Text-to-Text Transfer Transformer (T5) [28] can\nbe applied to a variety of NLP tasks by treating them as text-to-text tasks. In addition to classifying, summarizing,\nand translating, the model uses a unified architecture and loss function. T5 utilizes a transformer architecture that\nuses encoder-decoder structures to produce coherent and fluent results. This architecture is composed of two parts:\nThe encoder generates a contextual representation of the input sequence. In producing each entry of the output, the\nself-attention mechanism uses a fully visible attention mask to attend to any entry of the input. The decoder generates\nthe output sequence from the encoder's output. This technique uses both causal masking and encoder-decoder attention\nto maintain autoregressive properties while at the same time attending to the encoder's output. Multiple benchmarks\nhave been run on the model, and it performs better when scaled to various sizes, with the largest variant, T5-11B,\ncontaining 11 billion parameters. A powerful dataset known as the Colossal Clean Crawled Corpus (C4) was used to\npre-train the model, making it able to learn from a wide range of texts. Although T5 performed well on several tasks,\nincluding CNN/Daily Mail summarization benchmarks, it performed poorly on some translation tasks, perhaps due to\nits reliance on an English-only dataset.\nmT5 [29] is an enhancement of T5 that utilizes GeGLU nonlinearities and scales both feed-forward dimensions\nand model dimensions in larger models. The encoder-decoder model supports generative tasks such as abstractive\nsummarization and dialog, which is different from encoder-only models such as XLM-R. As a result of training\nmore than 250,000 words, the mT5 model has several sizes, such as Small (300M parameters), Base (580M), Large\n(1.2B), XL (3.7B), and XXL (13B). As part of the pre-training of the mT5 model, a large dataset named mC4, which\ncontains over 100 different languages of text, is used. In the training process, data is sampled from each language to\nbalance the representation of languages with high resources and languages with low resources. Several classification\nand question-answering tasks have been performed using the mT5 model, and it has proven to be state-of-the-art.\nVarious training strategies are employed, including in-language multitask training ( utilizing gold data within the\ntarget language), translate-train (utilizing machine translation from English ), and zero-shot transfer (utilizing only\nEnglish data for fine-tuning). Cross-lingual representation learning is influenced by model capacity, with larger models\noutperforming smaller ones, particularly in zero-shot scenarios.\nIn ByT5 [30], vocabulary does not need to be built or tokenized like in mT5 and the NLP pipeline is simplified. The\nByT5 architecture supports byte-level processing, so vocabulary parameters are reallocated to the transformer layers,\nimproving model efficiency. ByT5 models come in different sizes (300M, 582M, 1.23B, 3.74B, and 12.9B), all with\nvarying hidden sizes and feed-forward dimensions. A multilingual task can be effectively handled by both ByT5 and\nmT5. In multitasking settings where gold training data is available, ByT5 has shown competitive performance across a\nvariety of languages. Multiple benchmarks demonstrate its ability to manage tasks in multiple languages, surpassing\nmT5. ByT5 models range in parameter count from 300 million in the Small version up to 12.9 billion in the XXL\nversion. A comparison of mT5 models reveals that the Base model has 582 million parameters, while the Large model\nhas 1.23 billion. The ByT5 design improves performance and reduces system complexity, making it a viable alternative\nto token-based models like mT5, especially for applications without significant latency issues.\nAraT5 [31] utilizes the T5 (Text-to-Text Transfer Transformer) encoder-decoder architecture, in particular the T5Base\nencoder-decoder. An AraT5MSA variant was developed (trained on Modern Standard Arabic data), an AraT5TW\nvariant (trained on Twitter data), and an AraT5 variant (trained on both MSA and Twitter data). It is estimated that\nthere are approximately 220 million parameters in each model, which is composed of 12 layers, 12 attention heads, and\n768 hidden units. Various datasets were used to pre-train the models, including unlabeled MSA and Twitter data. A\nself-supervised (denoising) objective was used to train the model, with 15% of tokens randomly masked to reassemble\nthe original sequence. Besides the code-switched datasets, the data also includes monolingual French and English\ntranslations of Algerian and Jordanian Twitter. An Arabic language GENeration (ARGEN) benchmark was used to\nevaluate the models, which included seven tasks: machine translation, code-switched translation, text summarization,\nnews headline generation, question generation, paraphrasing, and transliteration. Over 52 out of 59 test sets, the models\noutperformed the mT5 model with an 88.14% performance rate. On the Arabic language understanding benchmark\nARLUE, they also set new state-of-the-art (SOTA) results."}, {"title": "Data", "content": "This section is to provide an overview of the parallel data that will primarily be used to train our methodologies models\nand monolingual corpus used to generate large synthetic data."}, {"title": "Parallel Corpus", "content": "The purpose of this section is to provide information about the data that will primarily be used to train our methodologies\nmodels. Table 3 shows how data are split, number of sentences, words, levels, and domains.\nQALB-14 and QALB-15 . The QALB-14 [4] corpus contains Al Jazeera News comments written in Modern Standard\nArabic by native speakers. Arabic speakers of all backgrounds were addressed in QALB-2015, including native\nArabic speakers and non-native Arabic speakers. QALB-15 [5], which collects texts from the Arabic Learner Corpus\n(ALC) [32] and the Arabic Learner Written Corpus (ALWC) [33] of Arabic learners, includes texts extracted from the\ntwo learner corpora. There are three phases to the annotation process: automatic preprocessing, automatic spelling"}, {"title": "Monolingual Corpus", "content": "The following paragraphs describe the corpus used in the synthetic data generation process. 1.5 billion words\ncorpus [21]. The corpus contains over 1.5 billion words from ten major Arabic news sources, including newspapers and\nnews agencies in eight Arab countries. The sources are as follows: Alittihad (United Arab Emirates), Echorouk Online\n(Algeria), Alriyadh (Saudi Arabia), Almustaqbal (Lebanon), Almasryalyoum (Egypt), youm7 (Egypt), Saba News\nAgency (Yemen). The data collection period for these sources ranged from January 2002 to June 2014, depending on\nthe source. Articles published online during these periods are included in the corpus. There are many topics covered in\nthe corpus, including politics, literature, the arts, technology, sports, and the economy. As a result of this diversity, it is\nan excellent resource for researchers in the fields of natural language processing and information retrieval. Furthermore,\nthe corpus contains more than five million articles and 1.5 billion words, with more than three million unique words.\nTwo different encoding schemes were used for the corpus: Windows CP-1256 and UTF-8. The corpus also contains\ntwo markup languages: SGML (Standard Generalized Markup Language) and XML (Extensible Markup Language).\nOpen Source International Arabic News (OSIAN) corpus [22]. An annotation-based corpus of Arabic news articles\naimed at addressing the limited resources nature of Arabic in linguistic computation. The corpus includes 3.5 million\narticles, 37 million sentences, and approximately 1 billion tokens annotated with metadata and linguistic information\nfrom international Arabic news websites. The data is encoded in XML format, which facilitates structured storage. The\ncorpus includes metadata about each article, including its source domain name, URL, and extraction date. For natural\nlanguage processing, the articles also provide linguistic information about each word, including its lemma and part\nof speech tags. In addition to covering a wide range of topics and sources, the corpus is designed to be a balanced\nrepresentation of international Arabic news. A separate directory for each country contains the articles, which are\nlemmatized and tagged for correct part-of-speech in XML files. Furthermore, statistics on word length distribution and\nword frequency lists are included in the corpus, which can help analyze the linguistic characteristics of text."}, {"title": "Methodology", "content": "As shown in Figure 1, we use two models to generate our data: an error tagging and a synthetic data generation model.\nIn the error tagging model, we used the deBERTav3 to assign and predicate an error label that may appear in incorrect\nsentences based on a subset of the 26 Arabic Error Type Annotation tool (ARETA) error tags. Then, a synthetic data\ngeneration model used an AraT5 model to generate an incorrect version of the data as a result of concatenating this\nlabel with the correct sentence."}, {"title": "Error Tagging Model", "content": "The error-tag model is a multi-label classification task that predicts the types of errors that may occur in a correct\nsentence. Our motivation for developing such a model was the availability of many correct and error-free data sources\nin news, newspapers, magazines, and books, as well as many monolingual corpora exist, but there are lack of parallel\ndata sources for training grammatical error correction (GEC) tasks. In our cases, Arabic is considered a low-resource\nlanguage and has one parallel corpus (QALB-14 and QALB-15) for GEC tasks. In addition, the Arabic language lacks\na model for predicting error type as a multi-label classification task. Based on what we described in the related work\nsection, most current research needs both correct and incorrect data to determine the error type or classify it as either\ncorrect or incorrect sentence or token, so this model will help identify errors in correct sentences and allow for further\nparallel data generation.\nError tagging models were trained on QALB-14 QALB-15 shard data and ZAEBUC, the only available grammatical\nerror data. Our goal is to train our model on real data so that it can predict human-like erroneous sentences. To train,\nwe used the QALB-14 train set, the QALB-15 train set, and ZAEBUC, which totaled 19935 samples. QALB-14"}, {"title": "Synthetic Data Generation Model", "content": "To generate synthetic data, we mainly used a back translation model. A reversed GEC model is trained to create\nungrammatical sentences from grammatical ones. Hence, we fine-tuned AraT5 two times to generate erroneous\nsentences. In the first fine-tuning process, the model input is the incorrect sentence and the target is the correct sentence.\nWe are using a pre-trained version of \"AraT5v2-base-1024\" from the Happy Transformer package. As training data, we\nused QALB-14, QALB-15, and ZEABAQ, and as development data, QALB-14, QALB-15.\nOur model is then further fine-tuned for a second time to generate syntactic data. We used the same data as in GEC but\nreversed the training process. Our input strings are prefixed with grammar_error as task conditioning for AraT5. There\nare 26 types of errors represented by bracketed 26 characters consisting of \"a\" and \"b\". As described in our previous\nsubsection, these tags are generated by the error tagging model. In the parallel corrupted sentence, \"b\" indicates the\npresence of error type i. In contrast, \"a\" is the opposite. Figure 3 shows how we can generate and represent sentences\nwith multiple errors as a result. In the final step, we append our corrupted sentence to these tags. As a result of the\nmodel, we get the corrupted sentence as an output. The batch size is 8, the learning rate is 2e-5, and the number of\nepochs is 300."}, {"title": "Data Generation", "content": "By using the 1.5 billion word and OSAIN datasets, we generate 30,219,310 synthetic sentence pairs after training our\nerror tagging model and syntactic data generation models. To generate parallel corpora, it must first be a preprocessed\nmonolingual corpus, as described in section 4.2. A 1.5 billion word corpus, consisting of XML files, was first analyzed\nby extracting only text tags and storing them in text files for easier processing. To further process some files, we split\nthem into two or more files as needed due to their large size. Conversely, the OSIAN corpus text already exists in\nseparate text files. However, it contains a sequence number at the beginning of each sentence, so we removed it. Then,\nwe discovered that both corpora contained spelling and grammatical errors. Several spelling errors were corrected\nmanually by replacing fatha Tanween with the letter preceding the alif, replacing the Ya with an Alif-Maqsura and vice\nversa, and modifying hazmat errors. The punctuation was also corrected by removing duplicate punctuation, adjusting\nthe shape of punctuation like commas and adjusting the punctuation between numbers to ensure that the text was\nclear and concise. Grammatical errors were not corrected because they needed to be annotated by humans. We then\npre-processed the data using natural language processing techniques, including removing empty lines, sentences with\nless than 10 words, non-UTF8 encoding, and over-space.\nIn our final step, we will use the syntactic data generation models we described in the 5.2 subsections to generate our\ncorpus using the prefix generated in the 5.1 subsections and shown in Figure 3."}, {"title": "Grammatical Error Correction", "content": "Our synthetically generated data was used to train a GEC model using the AraT5 model to evaluate the effectiveness of\nperformance gain. For evaluating our model with other Arabic synthetic data generating models, we used synthetic\ndata from [10], which is the most recent Arabic study that generated synthetic data. Data is organized into seven\nfolders, each of which contains a specific type of error. The errors were generated by Misspelling (producing spelling\nerrors), Swap (changing the order of target words), Reverse (transposing left-to-right words into right-to-left words),\nand Replace (aligning target and source words), which achieved the best performance, respectively. Token (adding\nUNK tokens to target sentences), and Source (replacing the source sentence with the target sentence) have achieved\nlower performance. There is a file for training and another for validation in each folder. The best-performing data is in\nthe eighth file, which contains (spelling + swap + reverse). Data was collected in two files, one for training and one for\nvalidation. The number of training data was 333908 and the number of validation data was 18206. These synthetic data\nwere generated from QALB-14 and QALB-15, which are derived from the QALB-14 and QALB-14 data. To generate\nour synthetic data, we extract only the correct text from [10] synthetic data, then predict the expected error tag using an\nerror tagging model, and then produce the incorrect text by using our synthetic data generation. Our evaluations are\nbased on using the QALB-14-L1, the QALB-15-L1, and the QALB-15-L2 test sets.\nTwo models are fine-tuned, one that uses [10] data, and the other that uses the same correct sentences but we generate\nincorrect sentences by using our models. We used batch size 8 for AraT5. We set the learning rate to 5e-5. After 23\ntrain epochs, we fine-tuned by using the QALB-14-L1 and QALB-15-L1 training data for two epochs."}, {"title": "Experiments", "content": "In this section, we describe our evaluation metrics and experimental setup used to evaluate our error tagging model and\nsynthetic data generation model."}, {"title": "Evaluation Measures", "content": "Our tagging model is considered a multi-label classification task. A multi-label classification allows multiple labels\nto be assigned simultaneously to an output, whereas a single-label classification allows only one label to be assigned.\nThus, evaluating the performance of multi-label classification is more challenging and complicated [38] and requires a\ndifferent approach than evaluating single-label classification performance; multiple metrics are recommended. Several\nmulti-label classification evaluation measures are described and categorized in [38]. This work was evaluated using\nHamming loss, micro-precision, micro-recall, micro-F1, and micro-F0.5.\nThe precision of a classification prediction is the percentage of the correct positive classification predictions. A recall\nscore is defined as how many positive classes were correctly predicted, while an F1-score, which is the recall and\nprecision harmonic mean, is an evaluation measure most commonly used. F0.5 is calculated by averaging precision and\nrecall (given a threshold value). The F0.5 score places more emphasis on precision than recall, unlike the F1 score,\nwhich gives equal weight to both. False Positives should be weighted more heavily than False Negatives in cases with\nhigher precision. For precision, recall, F1-score, and F0.5 score, we apply the following definitions to several true\npositives TP, true negatives TN, false positives FP, and false negatives FN:\nPrecision = $\\frac{TP}{TP+FP}$ (1)\nRecall = $\\frac{\u03a4\u03a1}{TP+FN}$ (2)\nF1 - Score = $\\frac{2 \u00d7 TP}{2TP + FP+ FN}$ (3)\nF0.5 Score = 1.25 \u00d7 $\\frac{(Precision)(Recall)}{0.25 \u00d7 (Precision + Recall)}$ (4)\nMultiple labels can be evaluated via micro-averaging, macro-averaging, and Weighted-averaging [39]. Metrics for\nmacro-averaging are calculated independently for each label before averaging them all. By contrast, micro-averaging\naggregates the number of hits and misses before calculating the desired metric once [40]. Therefore, the final measure\ncalculation takes into account the disparate data distribution associated with each label as well as the weight assigned to\neach label. In a weighted average, all per-class scores are averaged, adjusting for the support that each class receives.\nAlthough micro-averaging is commonly used for evaluating performance when the data is unbalanced [41], the following\nare the definitions of micro-F1, micro-F0.5, micro-precision, and micro-recall:\nMicro-Precision = $\\frac{\\Sigma TP}{\\Sigma=1(TP + FP)}$ (5)\nMicro-Recall = $\\frac{\\Sigma \u03a4\u03a1}{\\Sigma=1(TP + FN\u00bf)}$ (6)\nMicro-F1 = 2. $\\frac{Micro-Precision Micro-Recall}{Micro-Precision + Micro-Recall}$ (7)\nMicro-F0.5 = (1 + 0.5\u00b2) $\\frac{(0.5\u00b2 Micro Precision Micro-RealRecall}{(0.5\u00b2 Micro Precision + Micro-Recall)}$ (8)\nThe Hamming loss is a measure of the percentage of incorrect predictions made across an entire set of labels. A model's\nperformance is higher when its hamming loss is lower. Hamming loss can be calculated as follows:\nHamming Loss = $\\frac{FP + FN}{FP + FN + TP + TN}$ (9)\nWhere: - FP is the number of false positives, - FN is the number of false negatives, - TP is the number of true positives,\n- TN is the number of true negatives.\nTo evaluate our synthetic data generation model and grammatical error correction model, we used MaxMatch [42] to\nevaluate the performance in a shared task by measuring word-level edits compared to golden target sentences and to"}, {"title": "Experiment Setup", "content": "In an error tagging model, we use the AraBERTv02, the CAMELBERT-MSA, the ARBERTv2, the MARBERTv2, and\nthe DeBERTa-v3-based architectures. The aim is to identify multiple tags (or labels) associated with a text sequence\ninput. We implemented the Hugging Face Transformers library's base version in our experiments. The Hugging\nFace Transformers library is a well-known, publicly accessible library [43]. All of the pre-trained model weights\nhave been included in the Hugging Face Transformers library. With such libraries, pre-trained models can be unified,\nand pretraining can be performed with as little computational resource usage as possible. Since we only have one\npre-trained model, DeBERTa-v3, 'Microsoft/debertav3-base', for multilingual DeBERTa models in Arabic, we use it in\nthe experiment. As a comparison, we employ Arabic MSA AraBERTv02, CAMELBERT-MSA, ARBERTv2, and the\nMARBERTv02 models named \"aubmindlab/bert-base-arabertv02\", \"CAMeL-Lab/bert-base-arabic-camelbert-msa\",\n\"UBC-NLP/ARBERTv2\" and \"UBC-NLP/MARBERTv2\".\nTo maximize training efficiency, several hyperparameters are set using the TrainingArguments API. In both training\nand evaluation, batch size is set to 8 samples per GPU. The learning rate is 2e-5, with a weight decay of 0.1. There\nis a checkpoint saved every 1000 steps, and the learning rate is evaluated every 1000 steps. A total of 120 epochs is\nrequired to ensure convergence. Training on GPUs can be sped up using mixed precision (fp16). Early stopping is\nincorporated with a patience of 20 steps and an improvement threshold of 0.001. Depending on the evaluation loss, up\nto three checkpoints are saved during the training process. Sigmoid activation and threshold are used in the model to\ngenerate predictions.\nFor handling the class imbalance in the dataset, we use a custom loss function (ResampleLoss) [37]. With this loss\nfunction, class importance is dynamically adjusted through a focal loss component. Weights are assigned based on\nclass frequencies and sample counts. The model optimizes performance across frequent and minority classes using\nbinary cross entropy (BCE), focal loss, and class-balanced loss. Focus loss is one of the most important components\nof this model. Focus loss underlines hard-to-classify data by modulating it with factors such as gamma (minimizing\nthe influence of well-classified data) and alpha (minimizing the influence of positive labels). A class-balanced loss\n(CB) also ensures that minority classes contribute meaningfully to training by applying a correction factor based on\nclass frequency. To stabilize predictions for imbalanced datasets, the function uses nonlinear functions, such as logit\nregularization, which further enhances optimization.\nPyTorch and Hugging face Transformers are used to implement the experiment within the Aziz Supercomputer\nExecution Environment, which leverages GPUs for efficient training. It also facilitates fine-tuning and testing with new\ndata by ensuring the reproducibility of results and models."}, {"title": "Experimental Results and Discussion"}]}