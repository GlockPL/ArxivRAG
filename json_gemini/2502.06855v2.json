{"title": "Self-Supervised Prompt Optimization", "authors": ["Jinyu Xiang", "Jiayi Zhang", "Zhaoyang Yu", "Fengwei Teng", "Jinhao Tu", "Xinbing Liang", "Sirui Hong", "Chenglin Wu", "Yuyu Luo"], "abstract": "Well-designed prompts are crucial for enhancing Large language models' (LLMs) reasoning capabilities while aligning their outputs with task requirements across diverse domains. However, manually designed prompts require expertise and iterative experimentation. While existing prompt optimization methods aim to automate this process, they rely heavily on external references such as ground truth or by humans, limiting their applicability in real-world scenarios where such data is unavailable or costly to obtain. To address this, we propose Self-Supervised Prompt Optimization (SPO), a cost-efficient framework that discovers effective prompts for both closed and open-ended tasks without requiring external reference. Motivated by the observations that prompt quality manifests directly in LLM outputs and LLMs can effectively assess adherence to task requirements, we derive evaluation and optimization signals purely from output comparisons. Specifically, SPO selects superior prompts through pairwise output comparisons evaluated by an LLM evaluator, followed by an LLM optimizer that aligns outputs with task requirements. Extensive experiments demonstrate that SPO outperforms state-of-the-art prompt optimization methods, achieving comparable or superior results with significantly lower costs (e.g., 1.1% to 5.6% of existing methods) and fewer samples (e.g., three samples). The code is available at https://github.com/geekan/MetaGPT.", "sections": [{"title": "1. Introduction", "content": "As large language models (LLMs) continue to advance, well-designed prompts have become critical for maximizing their reasoning capabilities (Wei et al., 2022; Zheng et al., 2024; Deng et al., 2023) and ensuring alignment with diverse task requirements (Hong et al., 2024b; Liu et al., 2024a; Zhang et al., 2024b; Hong et al., 2024a). However, creating effective prompts often requires substantial trial-and-error experimentation and deep task-specific knowledge.\nTo address this challenge, researchers have explored Prompt Optimization (PO) methods that use LLMs' own capabilities to automatically improve prompts. PO advances beyond traditional prompt engineering, by providing a more systematic and efficient approach to prompt design. As shown in Figure 1(a), these methods typically involve an iterative process of prompt optimization, execution, and evaluation. The design choices for these components significantly influence optimization effectiveness and efficiency. Existing approaches have demonstrated success with both numerical evaluation mechanisms (Wang et al., 2024e; Yang et al., 2024a; Fernando et al., 2024) and textual \"gradient\" opti-"}, {"title": "2. Preliminary", "content": null}, {"title": "2.1. Problem Definition", "content": "Prompt Optimization aims to automatically enhance the effectiveness of a prompt for a given task. Formally, let T = (Q, Gt) represent a task, where Q denotes the input question and Gt is the optional ground truth. The goal is to generate a task-specific prompt P* that maximizes performance on task T. This optimization objective can be formally expressed as:\nP* = arg max EP~D[$eval($exe(Q, Pt))],  (1)\nPEP\nwhere P represents the space of all possible prompts. As illustrated in Figure 1, this optimization process typically involves three fundamental functions: (1) Optimization function (opt): generates a revised prompt based on the candidate prompt; (2) Execution function (exe): applies the revised prompt with an LLM to produce outputs O, consisting of a reasoning path and a final answer; (3) Evaluation function (eval): assesses the quality of O and provides feedback F to guide further optimization, refining the candidate prompts iteratively.\nAmong these functions, the evaluation function plays a pivotal role as its output (feedback F) guides the assessment and improvement of prompts. We will discuss the evaluation framework for prompt optimization in Section 2.2."}, {"title": "2.2. Evaluation Framework in Prompt Optimization", "content": "This section outlines our evaluation framework for prompt optimization, covering three key components: evaluation sources, evaluation methods, and feedback types, as shown in Figure 3. We conclude by introducing our selected evaluation framework for SPO.\nEvaluation Sources As shown in Figure 3(a), two primary sources can be used for evaluation: LLM-generated outputs and task-specific ground truth. These sources provide the basis for assessing prompt performance.\nEvaluation Methods The evaluation method defines how the evaluation sources are assessed and the associated costs. Three common methods are used: (1) Benchmark relies on predefined metrics (Suzgun et al., 2023; Rein et al., 2023) or rules (Chen et al., 2024). (2) LLM-as-a-judge (Zheng et al., 2023) leverage LLMs capability to understand and assess outputs based on task requirements. (3) Human Feedback (Lin et al., 2024) provides the most comprehensive evaluation through direct human assessment of outputs.\nWhile Human Feedback offers the most thorough evaluation by capturing human preferences and task-specific needs, it incurs substantially higher costs than Benchmark or LLM-"}, {"title": "Feedback Types", "content": "Feedback produced by evaluation methods typically take three forms: (1) Numerical Feedback provides quantitative performance measures across the dataset. However, it requires substantial samples for stable evaluation and may overlook instance-specific details (Zhang et al., 2024a). (2) Textual Feedback offers rich, instance-specific guidance through analysis and suggestions, directly generating optimization signals (Y\u00fcksekg\u00f6n\u00fcl et al., 2024). (3) Ranking or Selection Feedback (Liu et al., 2024b) establishes relative quality ordering among outputs through either complete ranking or pairwise comparisons, providing clear optimization direction without requiring absolute quality measures.\nEvaluation Framework Building on the previous discussion on evaluation's sources, methods, and feedback types, the evaluation framework determines how sources are compared and assessed within the context of prompt optimization. Specifically, we derive two evaluation frameworks to generate feedback F for prompt optimization:\n(1) Output vs. Ground Truth (OvG): Feedback is generated by comparing outputs O with ground truth GT:\nfovG(Oi, Gi) = eval(exe(Qi, Tpi), Gi) (2)\nAlthough this approach allows for a direct quality assessment through an external reference, it requires well-defined ground truth, making it unsuitable for open-ended tasks where ground truth may not always be available or practical to define.\n(2) Output vs. Output (OvO): When ground truth is unavailable, we turn to direct output comparison. The core idea"}, {"title": "3. Self-Supervised Prompt Optimization", "content": "In this section, we first overview our method (Section 3.1) and then analyze its effectiveness (Section 3.2)."}, {"title": "3.1. An Overview of SPO", "content": "A core challenge in reference-free prompt optimization is how to construct effective evaluation and optimization signals. We propose Self-Supervised Prompt Optimization (SPO), a simple yet effective framework that retains the basic Optimize-Execute-Evaluate loop while enabling reference-free optimization by leveraging only model outputs as both evaluation sources and optimization guidance.\nAs shown in Algorithm 1, SPO operates through three key components and the corresponding prompts are shown in Appendix A.1:\nOptimization function (opt): Generates new prompts by analyzing the current best prompt and its corresponding outputs.\nExecution function (exe): Applies the generated prompts to obtain outputs.\nEvaluation function (eval): Uses an LLM to compare outputs and determine the superior prompt through pairwise comparisons.\nThis iterative process begins with a basic prompt template (e.g., Chain-of-Thought (Wei et al., 2022)) and a small question set sampled from the dataset. In each iteration, SPO generates new prompts, executes them, and performs pairwise evaluations of outputs to assess their adherence to task requirements."}, {"title": "3.2. Understanding the Effectiveness of SPO", "content": "The theoretical foundation of SPO is built upon two key observations:\nFirst, the outputs of LLMs inherently contain rich quality information that directly reflects prompt effectiveness, as evidenced by how step-by-step reasoning paths demonstrate the success of Chain-of-thought prompting (Wei et al., 2022). Second, LLMs exhibit human-like task comprehension, enabling them to assess answer quality and identify superior solutions based on task requirements. These complementary capabilities allow SPO to perform prompt evaluation and optimization without external references. These two aspects of utilizing model outputs work together to enable effective prompt optimization:\nOutput as Optimization Guidance In terms of opt design, unlike other methods that introduce explicit optimization signals (Fernando et al., 2024; Y\u00fcksekg\u00f6n\u00fcl et al., 2024; Pryzant et al., 2023), opt optimizes directly based on the prompt and its corresponding outputs. The optimization signal stems from the LLMs' inherent ability to assess output quality, while the optimization behavior is guided by its understanding of what constitutes superior solutions. There-"}, {"title": "4. Experiment", "content": null}, {"title": "4.1. Experimental Setup", "content": "Datasets We evaluated SPO on a diverse set of tasks, including both closed tasks and open-ended tasks, to comprehensively assess its effectiveness.\nFor closed tasks, we utilized five established benchmarks:"}, {"title": "Ablation Study", "content": "To evaluate the transferability of SPO across different optimization, evaluation, and execution"}, {"title": "Main Result of Open-ended Tasks", "content": "To validate SPO's capability in open-ended tasks, we selected three categories from MT-Bench: \u201cWriting"}, {"title": "4.3. Case Study", "content": "We present optimization results on additional open-ended tasks without datasets and SPO's optimization trajectories in the Appendix A.4. We also provide optimal prompt across five closed tasks discoverd by SPO in the supplementary material. Given that real-world applications often face challenges with limited datasets, we evaluate SPO's performance on tasks that lack conventional benchmarks. The experimental results, coupled with SPO's cost efficiency, demonstrate its practical value in real-world scenarios. Specifically, we demonstrate the optimization results after 10 iterations using Claude-3.5-Sonnet as the optimization model, GPT-40-mini as the evaluation model, and Llama-3-8B as the execution model across four tasks: Advertising"}, {"title": "5. Related Work", "content": null}, {"title": "5.1. Prompt Engineering", "content": "Research on effective prompting methods for large language models has primarily evolved along two main directions. The first focuses on task-agnostic prompting techniques that enhance LLMs' general capabilities. Notable examples include the chain-of-thought (Wei et al., 2022; Kojima et al., 2022) which improved reasoning across various tasks, techniques for enhancing single-shot reasoning (Deng et al., 2023; Zheng et al., 2024; Wang et al., 2024d), and methods for output format specification (Zhang et al., 2024a; He et al., 2024; Tam et al., 2024). These techniques, developed through human insights and extensive experimentation, provide essential optimization seeds for automated prompt optimization research.\nThe second direction addresses domain-specific prompting, where researchers have developed specialized techniques for tasks in code generation (Hong et al., 2024b; Ridnik et al., 2024; Shen et al., 2024a), data analysis (Hong et al., 2024a; Liu et al., 2024a; Li et al., 2024a), question answering (Wu et al., 2024b; Zhu et al., 2024; Yang et al., 2024b), decision-makings (Zhang et al., 2024b; Wang et al., 2024a), and other domains (Guo et al., 2024b; Ye et al., 2024; Shen et al., 2024b). However, as applications of LLMs expand to increasingly complex real-world scenarios, manually crafting effective prompts for each domain becomes impractical (Zhang et al., 2024a). This challenge has motivated research in prompt optimization, which aims to systematically develop effective domain-specific prompts rather than discovering general prompting principles."}, {"title": "5.2. Prompt Optimization", "content": "The design of evaluation frameworks is crucial in Prompt Optimization (PO), as it determines both optimization effectiveness and computational efficiency. The evolution of evaluation mechanisms in PO has progressed from simple evaluation feedback collection to sophisticated optimization signal generation (Chang et al., 2024). Existing PO methods can be categorized based on their evaluation sources and mechanisms.\nThe most common approach relies on ground truth as the evaluation source, utilizing benchmark-based numerical assessments (Zhou et al., 2023; Guo et al., 2024a; Yang et al., 2024a; Fernando et al., 2024; Wang et al., 2024e; Khat-"}, {"title": "6. Conclusion", "content": "This paper addresses a fundamental challenge in prompt optimization: reliance on external references that limits real-world applications. We introduce Self-Supervised Prompt Optimization (SPO), a framework that overcomes this reliance while achieving remarkable cost-efficiency at only $0.15 per dataset. Drawing inspiration from self-supervised learning, SPO innovatively constructs evaluation and optimization signals through pairwise comparisons of model outputs, enabling reference-free optimization without compromising effectiveness.\nOur comprehensive evaluation demonstrates SPO's superior performance across both closed and open-ended tasks, achieving state-of-the-art results while requiring only 1.1%-5.6% of existing methods' costs. The success on both standard benchmarks and diverse real-world applications validates SPO's effectiveness and generalization capabilities. By dramatically reducing both resource requirements and operational complexity, SPO represents a significant"}, {"title": "Impact Statement", "content": "SPO offers significant advancements in prompt engineering for LLMs, offering benefits such as democratized access, reduced costs, and improved performance across various tasks. However, it also carries risks, including potential bias amplification, misuse of harmful content generation, and over-reliance on LLMs."}, {"title": "A. Appendix", "content": null}, {"title": "A.1. Detailed Prompts of SPO", "content": "In this section, we present the Meta Prompt used for iteration. It should be noted that here we have only used the simplest and most straightforward Prompt. There is still room for improvement by optimizing the following Meta Prompt for specific domains."}, {"title": "A.2. Detailed Prompt Template of Iteration Start", "content": "This YAML file demonstrates the initial configuration for our approach to iterating on the BBH-navigate task. By configuring a simple initial Prompt and requirements, along with three specific questions, iterative optimization can be performed. It should be noted that the content shown here is the complete content of the file, and the content in the answer section is not the actual answer but serves as a reference for the thought process and correct output format."}, {"title": "A.3. Experiment Details", "content": null}, {"title": "A.3.1. TASKS AND DATA DETAILS", "content": "LIAR LIAR (Wang, 2017) is an English fake news detection dataset consisting of 4,000 statements, each accompanied by contextual information and lie labels. For our experiments, we sampled portions from the original dataset as test sets following Yan et al. (2024).\nBBH-Navigate BBH-Navigate (Suzgun et al., 2023) is a task from the BIG-bench Hard dataset, a subset of the BIG Bench dataset. This task focuses on navigation reasoning, requiring the model to determine whether an agent, after following a"}, {"title": "A.3.2. CONFIGURATION", "content": "In our experiments, we configured different optimization frameworks to align their optimization costs as much as possible. These frameworks generally allow setting some parameters to adjust optimization costs, including the number of iterations and the number of prompts generated per iteration.\nAPE APE employs a three-round iterative optimization process, selecting the top 10% (ratio=0.1) performing prompts from the current pool as elite prompts in each round. To maintain diversity and size of the prompt pool, variant sampling is used to mutate these elite prompts, keeping the total number of prompts at 50. Following the setting in original paper (Zhou et al., 2023), the optimization process does not incorporate specific sample execution results to guide LLM prompt optimization. Instead, performance scores are obtained by evaluating prompts on the entire training set.\nOPRO OPRO uses a 10-round iterative optimization process, generating 10 candidate prompts per round. OPRO evaluates prompt performance on the complete training set and filters based on evaluation scores. OPRO doesn't maintain a fixed-size prompt pool but directly generates new candidates based on the current best prompt in each round. The optimization direction is guided through performance evaluation on the full training data."}, {"title": "A.3.3. BASELINE PROMPT", "content": "In this section, we provide the Baseline Prompts for comparison. Note that for all Prompt Optimization work requiring initial iteration prompts, we consistently provide the COT Prompt shown below."}, {"title": "A.3.4. PROMPT OPTIMIZED BY SPO", "content": "In this section, we present the optimized prompts obtained from our main experiments, where Claude-3.5-Sonnet serves as the optimization model, and GPT-40-mini serves as both the evaluation and execution model."}, {"title": "A.4. Case Study", "content": null}, {"title": "A.4.1. OPTIMIZATION TRAJECTORY", "content": "We present the prompt optimization trajectory of GPT-40-mini on the BBH-navigate dataset, where it serves as the optimization model, evaluation model, and execution model. This includes whether each iteration was successful relative to the best prompt at that time, as well as the corresponding prompt content."}]}