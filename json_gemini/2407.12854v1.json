{"title": "Scaling Retrieval-Based Language Models with a Trillion-Token Datastore", "authors": ["Rulin Shao", "Jacqueline He", "Akari Asai", "Tim Dettmers", "Sewon Min", "Luke Zettlemoyer", "Weijia Shi", "Pang Wei Koh"], "abstract": "Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another dimension of scaling: the amount of data available at inference time. Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks. By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget. We carry out our study by constructing a 1.4 trillion-token datastore named MASSIVEDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in a computationally accessible manner. Finally, we analyze the effect of improving the retriever, datastore quality filtering, and other design choices on our observed scaling trends. Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To facilitate future research, we open-source our datastore and code at https://github.com/RulinShao/retrieval-scaling.", "sections": [{"title": "1 Introduction", "content": "The scaling of large language models (LMs) has driven tremendous performance gains across a variety of tasks (Brown et al., 2020; Kaplan et al., 2020; Muennighoff et al., 2023). Current scaling laws are primarily a function of the size of the pretraining data and the number of parameters (Hoffmann et al., 2022; Muennighoff et al., 2023; Gadre et al., 2024). In this paper, we consider another dimension of scaling: the amount of data in a datastore used at inference time by retrieval-based LMs, which can directly retrieve information from the datastore to use in context when generating output (Karpukhin et al., 2020; Guu et al., 2020; Izacard & Grave, 2020; Asai et al., 2024b).\nRetrieval-based LMs have a range of benefits such as improved factuality (Mallen et al., 2023), effective domain adaptation (Khandelwal et al., 2020), credit attribution (Gao et al., 2023), and parametric efficiency (Min et al., 2023b). However, most prior work in retrieval-based LMs use datastores constructed from a single data source (Karpukhin et al., 2020), such as Wikipedia, with sizes on the order of a few billion tokens. While there has been some work on larger datastores (Table 1), with the largest being RETRO (Borgeaud et al., 2022; Wang et al., 2024) in the trillion-token range, these studies use proprietary datastores and custom architectures with a limited evaluation suite. As such, it remains unknown how datastore scaling helps the currently dominant retrieval-in-context approaches on a broad categories of tasks.\nWe first construct MASSIVEDS, a massively multi-domain datastore comprising 1.4 trillion tokens of both general web data and domain specific data (\u00a73.1) that serves as the cornerstone for our scaling study. A key challenge in studying datastore scaling is the computational cost introduced by building datastores with all possible combinations of factors such as the datastore scale, data composition, random seed for subsampling, and different data preprocessing methods. To make our study accessible, we design an efficient datastore construction pipeline that reduces the compute needed by an order of magnitude while being equivalent to the standard pipeline (\u00a73.2).\nUsing the proposed pipeline, we systematically evaluate the effects of scaling MASSIVEDS on retrieval-based LMs with varying numbers of parameters and pretraining tokens (\u00a74). Beyond upstream language modeling, we also consider a suite of diverse downstream tasks, including general-knowledge question answering (QA), domain-specialized QA, and reasoning tasks. We find that, first, datastore scaling consistently improves both language modeling and some downstream tasks in a task-dependent manner (Figure 1 Left), much like the widely observed data and parameter scaling trends. In fact, on knowledge-intensive tasks, a small retrieval-based LM can outperform its larger LM-only counterparts. Second, since indexing a datastore is cheaper than training on the same amount of data, retrieval-based LMs enable better compute-optimal scaling trends, where they achieve superior performance than LM-only models at the same training cost (Figure 1 Right).\nThrough our analyses (\u00a75), we show that retrieval-based LMs are capable of automatically retrieving documents that are in-domain to the query, which allows them to reap the benefits of larger, broader datastores. In addition, data quality filters and improved retrieval methods can further enhance our observed scaling trends.\nOverall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To spur future research, we open-source MASSIVEDS (including the raw passages, the embedding, and the index) and all code (including our evaluation suite and pipeline for efficiently studying datastore scaling) at https://github.com/RulinShao/retrieval-scaling."}, {"title": "2 Related Work", "content": "Retrieval-based LMs. Unlike parametric LMs that only use data during training, retrieval-based LMs can access data through a datastore during inference (see Asai et al. (2024b) for a review). We focus on retrieve-in-context language models (RIC-LMs), which retrieves a small set of documents from the datastore and feeds a concatenation of them as an input to the LM (Ram et al., 2023; Shi et al., 2023). The RIC-LM approach is simple and allows the use of off-the-shelf retrievers and LMs, even with only black-box access.\nScaling the retrieval datastore. Prior work on retrieval-based LMs often focused on specific aspects of LMs such as factuality and attribution. In addition, they typically use limited-size, single-domain"}, {"title": "3 MASSIVEDS and our Datastore Scaling Pipeline", "content": ""}, {"title": "3.1 MASSIVEDS: A Trillion-Token Datastore With a Diverse Domain Composition", "content": "MASSIVEDS encompasses both domain-specific data and general web data (Table 2). Domain-specific data comes from specialized sources, and tends to be smaller but higher in quality. We cull from a mix of data-rich domains: books which span a variety of genres (Computer, 2023); open-access scientific papers (Lo et al., 2020; Soldaini &\nLo, 2023; Computer, 2023); encyclopedic articles (Karpukhin et al., 2020; Computer,\n2023); community questions and answers from StackExchange (Computer, 2023);\ncode from GitHub (Computer, 2023); mathematical webpages (Paster et al., 2023)\nand mathematical language (Welleck et al.,\n2021); biomedical articles (of Medicine,\n2023). On the other hand, general web data\nis sourced from Common Crawl snapshots at five time periods (07/2019, 05/2020, 04/2021, 05/2022,\n06/2023) and C4 (Raffel et al., 2020)."}, {"title": "3.2 Studying Datastore Scaling with the MASSIVEDS Pipeline", "content": "Studying datastore scaling requires constructing datastores of varying sizes and varying compositions from the raw text corpus. This involves the following operations: data filtering, including deduplication, decontamination, and quality filters (Soldaini et al., 2024); data subsampling, which randomly subselects a p-fraction of the text corpus to achieve the specified size; indexing, which embeds the data using an auxiliary model and builds a searchable index; document retrieval, which uses the index to find the top-k relevant documents for each test query\u00b9; and top-k evaluation, which uses the top-k documents per test query to augment the retrieval-based model. A naive approach is to run these operations in the aforementioned order for each datastore, and build separate datastores for all combinations of subsampled datastore sizes, random seeds, and other experimental variations. However, this naive approach is prohibitively computationally expensive at the trillion-token datastore scale because it repeats expensive operations, as shown in Figure 2 (top).\nTo make the datastore scaling study computationally feasible, we develop the MASSIVEDS pipeline (Figure 2 bottom). The key idea is to reorder the above operations such that the most expensive ones\u2014indexing and retrieval\u2014are run only once at the start and then shared across all subsequent datastore variants. Other operations with many variants\u2014such as subsampling, deduplication, and decontamination\u2014are performed as late as possible to minimize repeating subsequent steps. To enable this, we first retrieve a relatively large number (K\u226b k) of documents for each query and then apply the subsequent operations to these sets of retrieved documents, rather than the entire datastore. Altogether, this pipeline reduces compute requirements by more than an order of magnitude, enabling us to conduct the datastore scaling study on a modest compute budget. In Appendix A.5, we show that the results from the MASSIVEDS pipeline are equivalent to the results from the naive pipeline with high probability, where the randomness comes from the subsampling procedure. We provide more details on the steps in the MASSIVEDS pipeline in Appendix A and detailed configuration in Appendix B.1.\nNote on the number of tokens in the datastore. In our figures, we plot the datastore size (on the x-axis) by multiplying the total number of tokens in the raw data pool by the subsampling fraction p. A more accurate representation would be the number of tokens in the filtered data pool; however, we do not know the size of the filtered data pool as we apply data filtering on the retrieved documents instead of the raw data for computational efficiency. As a result, the number of tokens we plot on our x-axis is proportionally larger, i.e., if a pf fraction of the data is filtered out (0 < pf < 1), then the actual number of tokens should also be multiplied by pf. Since we plot datastore size on a log axis, this corresponds to a constant shift and does not change the scaling trends."}, {"title": "4 Datastore Scaling with Retrieval-Based Language Models", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Model details. Following prior work (Izacard et al., 2023; Liu et al., 2023; Ram et al., 2023; Shi et al., 2024; Xu et al., 2023; Asai et al., 2024a), we use CONTRIEVER-MSMARCO (Izacard et al., 2022), which represents every document in the datastore as a dense vector, as our retriever. We ablate the choice of retriever in Appendix E.1; we found that CONTRIEVER-MSMARCO performs on par with, or even better than, more recent larger retrievers. We augment input examples with retrieved documents at the granularity of 256-word chunks. We study datastore scaling performance with the LLAMA-2, LLAMA-3 (Touvron et al., 2023), PYTHIA (Biderman et al., 2023), and OLMO (Groeneveld et al., 2024) model families.\nEvaluation. We consider both language modeling and downstream tasks for evaluation. We evaluate language modeling perplexity on data from two domains: (1) general web data sampled from REDPAJAMA (Computer, 2023); (2) scientific paper data sampled from S2ORC (Lo et al., 2020). For downstream tasks, our evaluation encompasses general-knowledge, medical, math, and science domains including the following tasks. TriviaQA (TQA; Joshi et al. 2017) comprises trivia questions with answers sourced from Wikipedia and the web. Natural Questions (NQ; Kwiatkowski et al. 2019; Lee et al. 2019) comprises search engine queries and human-annotated answers from Wikipedia. Massive Multitask Language Understanding (MMLU; Hendrycks et al. 2021) comprises general-purpose, multi-task reasoning questions. MedQA (Jin et al., 2020) comprises medical multiple-choice questions sourced from professional medical exams.\nImplementation details. For evaluation with retrieval, we concatenate the top k = 3 documents in reverse order, so that higher-ranked documents are positioned closer to the query. For downstream tasks, we evaluate models via 5-shot prompting, and we prepend the retrieved documents before the few-shot examples, followed by the question. We do not apply reranking for our main experiments in Section 4; we study the effect of rerankers in Section 5.2. More details, including decontamination measures, are in Appendix B."}, {"title": "4.2 Datastore Scaling Results on Language Modeling and Downstream Tasks", "content": "Finding 1: Datastore scaling significantly helps language modeling. Figures 3(a) and (b) show perplexity curves as a function of datastore size on general web and scientific papers, respectively. Retrieval is strictly beneficial for language modeling: the LM-only baselines (denoted by dashed lines) show the highest perplexity across all models and evaluation datasets. Scaling up the datastore reduces perplexity without signs of saturation, suggesting that further scaling is likely to yield additional improvements. Further, datastore scaling enables small models to outperform their larger LM-only counterparts: when retrieving from MASSIVEDS at the largest scale, LLAMA-2 7B outperforms the LM-only performance of its larger LLAMA-2-13B counterpart. Interestingly, we find LLAMA-3 8B\nFinding 2: Datastore scaling improves performance on several downstream tasks, and the degree of improvement is task-dependent. Figure 3(c)-(f) show the performance on four downstream tasks as a function of datastore size. Datastore scaling brings major improvements to knowledge-intensive question answering tasks such as NQ and TQA, where retrieval-based LMs significantly outperform LM-only baselines across all scales, and performance monotonically increases with datastore scale. For instance, a LLAMA-2 7B model that retrieves from fewer than 100B tokens can outperform both its 13B LM-only counterpart and the more capable LM-only LLAMA-3 8B on TQA and NQ, indicating the effectiveness of storing knowledge in the datastore.\nOn MMLU, a multi-subject, reasoning-heavy benchmark, datastore scaling monotonically improves performance across all model scales. Results are more mixed for MedQA, where only the weaker LLAMA-2 7B benefits more from datastore scaling. For both tasks, datastore scaling does not help the smaller model do better than the larger model. We suspect that this is due to task difficulty and the lack of in-domain data sources: both MMLU and MedQA are more oriented toward reasoning rather than pure factual recall, which poses bigger challenges for both the retriever and the LM. Additionally, MASSIVEDS only contains a small subset of web data and medical papers which may not cover all necessary information to answer these questions. We defer to future work to explore better data sources for these tasks."}, {"title": "4.3 Compute-Optimal Scaling with Retrieval-Based Language Models", "content": "Next, we study performance as a function of total training-time compute and show that retrieval-based LMs achieve superior compute-optimal performance compared to LM-only models.\nUse of intermediate checkpoints. We use the intermediate checkpoints of PYTHIA and OLMO as an approximation of models trained on different numbers of tokens, as detailed in Appendix B.4. These intermediate checkpoints share the same learning rate scheduler, with a fixed maximum number of training steps that equals or exceeds the number of steps they have been actually trained for, and"}, {"title": "5 Analysis", "content": ""}, {"title": "5.1 Effects of Data Composition", "content": "Finding 6: MASSIVEDS matches or outperforms single-domain datastores. The default setting in prior work is to use a single-domain datastore that is in-distribution to the downstream task. In practice, however, it is often difficult to determine and curate a datastore that is perfectly in-distribution for a downstream task, and even if we can, it limits the generality of the retrieval-based model to that task.\nIn Table 3, we compare MASSIVEDS with single-domain datastores. MASSIVEDS significantly outperforms these in-domain datastores on language modeling, as well as TQA and MMLU, while matching performance on NQ and MedQA. In Figure 5, we show that the retriever tends to retrieve from the relevant domain even in the presence of out-of-domain data in the datastore: for NQ, it retrieves relatively more frequently from Wikipedia and web sources, whereas for MedQA, it retrieves more frequently from scientific papers from peS20 (Soldaini & Lo, 2023). Thus, the retriever can maintain robustness to out-of-domain data in the datastore; this aligns with similar findings on KNN-LM (Khandelwal et al., 2020), another type of retrieval-based LM, in Shao et al. (2023).\nOverall, these results show that retrieving from broad datastores like MASSIVEDS can simultaneously improve performance across multiple domains, paving the path towards general-purpose retrieval-based models."}, {"title": "5.2 Effects of Reranking", "content": "Retrieving the most relevant documents from a large-scale datastore remains a challenging problem. To study how improving the retrieval process impacts datastore scaling trends, we first retrieve 500 documents from CONTRIEVER, rerank them using a more computationally expensive model (Ram et al., 2023; Sachan et al., 2022), and take the final top-3 reranked documents for evaluation. Specifically, we use a cross-encoder reranker, which encodes a concatenation of a query and document and returns a similarity score (Nogueira & Cho, 2019). We choose MINI-LM-L12 V2, a BERT-based cross-encoder that is trained for passage ranking, following Izacard et al. (2022). Additionally, we"}, {"title": "5.3 Effects of Datastore Filtering", "content": "Data decontamination. Data decontamination is a crucial concern when evaluating LMs, especially in retrieval-based LMs that can retrieve the test data verbatim during inference (Borgeaud et al., 2022). By default (Section 4), we perform decontamination by filtering documents with 80+% 13-gram Jaccard similarity for downstream tasks and 32-gram longest sequence overlap for perplexity, which we call standard decontamination. Prior work such as RETRO (Borgeaud et al., 2022) only used 80+% 13-gram Jaccard similarity for decontamination. However, we find the additional 32-gram longest sequence overlap decontamination is critical for removing near-identical documents."}, {"title": "6 Limitations and Discussion", "content": "We conclude by discussing limitations and future directions. First, while our pipeline allows us to study datastore scaling efficiently, our experiments are still limited by our available compute. In particular, our compute-optimal scaling studies are limited to model families like OLMo and Pythia that release intermediate model checkpoints, since full pretraining runs exceed our budget constraints. Similarly, we conduct the full scaling study with a single retriever, as changing the retriever necessitates re-indexing the entire datastore. It remains unclear how changes in the size and architecture of the retriever affect datastore scaling trends.\nSecond, although MASSIVEDS is large in size, it might still lack high-quality data for improving performance on more complex, reasoning-heavy tasks such as MMLU and MedQA. Future work could study the effect of extending MASSIVEDS to more varied and higher quality data sources.\nLastly, our downstream evaluations are mostly on question-answering tasks whose outputs are either predefined choices or short form generations. We defer the evaluation on more tasks such as long-form generation and mathematical reasoning to future work.\nDespite these limitations, our research shows that increasing the scale of data available at inference time can improve model performance, at lower training cost, on language modeling and a variety of downstream tasks. We expect that future work on improving retrieval-based models with large-scale datastores will lead to even larger improvements: for example, our analysis suggests that further improving the retrieval process, either through better retrievers or rerankers, could have a significant impact."}, {"title": "A Datastore Construction Pipeline", "content": "The MASSIVEDS pipeline entails running the following operations in order (Figure 2):\n1. Distributed indexing. We split the documents from each domain into separate shards and construct an index for each shard. Distributing the index allows us to parallelize indexing and retrieval processes, and to study different combinations of data sources more easily.\n2. Distributed document retrieval. For each test query, we search for the top-K documents over each shard index in parallel. The searched results are first merged within each domain. We cache the per-domain searched results for data composition analysis.\n3. Domain merging. For a target combination of domains in the datastore, we merge the top-K documents from the target domains for that query. This leads to a merged pool of DK retrieved documents, where D is the number of target domains. From this pool, we re-select the top-K documents.\n4. Data filtering and reranking. We then apply the data filtering and reranking steps, as described above, to only the top-K documents (for each test query). This allows us to experiment with different data filters and rerankers without having to rerun indexing or retrieval; moreover, we only need to filter/rerank the retrieved results instead of the whole datastore, and we do not need to repeat these processes for different subsampling ratios.\n5. Data subsampling. We subsample the filtered and reranked documents for each test query. Specifically, for a particular subsampling ratio p, we select each document for inclusion i.i.d. with probability p. To try different subsamples (based on different random seeds), we only need to restart the pipeline from this step, and we can reuse computation from all previous steps.\n6. Evaluation. We use the top-k documents for each test query and prepend these documents in front of the query and few-shot examples for evaluation.\nIn practice, we position the reranking step after data subsampling to reduce the number of documents that require reranking. The commutativity of reranking and subsampling is demonstrated in Ap-pendix A.5. Furthermore, we collect the top-K' documents (where K' < K) from the deduplicated and decontaminated top-K set for reranking. We set K' = k when reranking is not applied. Next, we describe each step of our efficiency-oriented datastore construction pipeline in detail and demonstrate the equivalence between our MASSIVEDS pipeline and the naive pipeline. Below is a table of notations for easy reference."}, {"title": "A.1 Distributed Indexing", "content": "Our pipeline starts with raw data that is split into fixed-size documents. For each document, we run one forward pass over it with a retriever model $\\phi$ and save the final-layer representation as an embedding for that document. We store the embeddings of all documents in the datastore for similarity-based search. In practice, we split the documents into separate shards and embed each shard in parallel. As we use the uncompressed embedding for retrieval searches, our indexing does not require additional operations over the saved embeddings. We treat each shard of embeddings as one sharded index. This indexing step is executed only once over all data, while the subsequent steps are performed for each query at inference. For simplicity, we describe the subsequent steps for a single query in the sections below."}, {"title": "A.2 Distributed Document Retrieval", "content": "At inference, we first retrieve the top-K documents from each sharded index in parallel to accelerate the search process. Specifically, we convert the query into an embedding and compute the inner-product similarity scores between this query embedding and every document embedding. We then rank the documents from each shard based on these similarity scores and collect all top-K documents for subsequent merging."}, {"title": "A.3 Domain Merging", "content": "Assuming we have m sharded indices, we merge the m sets of top-K retrieved documents from all indices based on their similarity scores to obtain the final top-K documents. This process is formally defined below as m-sharded distributed element-wise top-K retrieval. We demonstrate that it is equivalent to directly retrieving the top-K documents from a single index built on all data.\nFormally, we introduce two definitions: element-wise top-K retrieval and m-shard element-wise top-K retrieval, the latter of which is what our pipeline uses.\nDefinition A.1 (Element-wise top-K retrieval). Assume a datastore of N documents: $\\mathcal{D}= \\{d_1,...,d_N\\}$. Given a query q, element-wise top-K retrieval uses a retriever $g_{\\theta}$ to compute the similarity score $s_i = g_{\\theta}(q, d_i)$ of the query and each document $d_i$ independently, and then returns the documents with the top-K highest retrieval scores.\nDefinition A.2 (m-shard distributed element-wise top-K retrieval). Consider a sharding strategy that splits the datastore into m shards $\\{\\mathcal{D}^1,......,\\mathcal{D}^m\\}$, such that each shard contains a disjoint subset of documents D (i.e., $\\mathcal{D}^1 \\cup \\mathcal{D}^2 \\cup \\cdot\\cdot\\cdot \\cup \\mathcal{D}^m = \\mathcal{D}$; $\\mathcal{D}^i \\cap \\mathcal{D}^j = \\varnothing$, when $i \\neq j$). In m-shard distributed element-wise retrieval, we fetch the top-K documents from each shard in parallel (if there are fewer than K documents in a shard, then all documents in the shard are returned). Then, we merge these mK documents and return the top-K highest-scoring documents from the merged pool.\nm-shard distributed element-wise top-K retrieval is equivalent to retrieving from a single unsharded index (m = 1).\nLemma A.1. Changing m, the number of shards used in distributed element-wise top-K retrieval, does not impact the final retrieved results.\nProof. Let the top-K documents obtained by the element-wise top-K retrieval and m-shard dis-tributed element-wise top-K retrieval be $\\mathcal{D}_K$ and $\\mathcal{D'}_K$, respectively. Since a document that is ranked in the top-K across all documents must be ranked in the top-K within any individual shard, we have $\\mathcal{D}_K \\subseteq \\mathcal{D'}_K$. Because $\\{\\mathcal{D}^1,.......,\\mathcal{D}^m\\}$ is a disjoint union of D, for any document d in $\\mathcal{D'}_K$, there are no more than K - 1 documents in the m shards, i.e., all documents, that have higher scores than d. Then we have $d \\in \\mathcal{D}_K$. Therefore, $\\mathcal{D'}_K \\subseteq \\mathcal{D}_K$. Finally, we have $\\mathcal{D}_K = \\mathcal{D'}_K$ for any choice of $m \\in \\mathbb{N}^+$.\nIn our experiments, we use an indexing method 5 that computes the similarity score between the query and every document, independent of other documents, i.e., element-wise retrieval. Therefore, our method for distributed search is equivalent to building and retrieving from a single non-distributed index."}, {"title": "A.4 Data Filtering and Reranking", "content": "Our pipeline includes two basic data processing steps: data deduplication and decontamination. These steps are applied post-hoc on the retrieved documents. Reranking, which is optional, is used to enhance the quality of retrieval. Detailed descriptions of deduplication, decontamination, and optional reranking are provided in this section."}, {"title": "A.4.1 Post Hoc Datastore Deduplication", "content": "Although REDPAJAMA (Computer, 2023), the pretraining corpora used to build MASSIVEDS, has already been deduplicated, we still noticed many duplicates in the retrieved results, particularly from the web domain. This is because Computer (2023) only performs local deduplication within each data shard; globally, many duplicates between shards remain.\nWe use INDEXFLATIP implemented in FAISS: https://github.com/facebookresearch/faiss/wiki/Faiss-indexes.Additional data quality filters, such as those from Dolma Soldaini et al. (2024), are discussed in our analysis section $5.3. These filters are applied at the same stage as deduplication."}, {"title": "A.4.2 Post Hoc Datastore Decontamination", "content": "Expanding the datastore to the Internet-scale incurs the risk of data contamination. Similar to global deduplication, running pre-decontamination on the entire datastore against every possible evaluation set is inconvenient and computationally expensive. Therefore, we implement a post-hoc decontamination method to remove contamination from the retrieved documents instead of the large datastore. For downstream tasks, we compare the 13-gram Jaccard similarity between each question and the retrieved document, and remove the documents with a high similarity score that is at least 80%. For language modeling (Figure 8), we adopt a stricter approach: we calculate both the 13-gram Jaccard similarity and the longest sub-sentence between the document and the answer, marking the document as contaminated if it either has at least 80% 13-gram similarity or contains a continuous 13-gram overlap with the answer. We split sentences into grams based on whitespace.\nWe show in Lemma A.2 that our method for post-deduplication and post-decontamination is equivalent to running deduplication and decontamination on the raw data prior to retrieval.\nLemma A.2. Running post hoc exact deduplication and decontamination over the top-K retrieved documents before taking the top-K' (where K' < K) documents is equivalent to retrieving the top-K' documents from a deduplicated and decontaminated datastore.\nProof. Given a datastore D, let the deduplicated datastore as D'. Denote the top-K documents from D as $\\mathcal{D}_K$ and the top-K' documents $\\mathcal{D}'$ as $\\mathcal{D'}_{K'}$. Then, denote the top-K' documents retrieved from $\\mathcal{D}_K$ as $\\mathcal{D'}_{K'}$. Since both $\\mathcal{D'}_{K'}$ and $\\mathcal{D}_K$ contain the top-K' ranked documents from all those retained after deduplication or decontamination, and since the removal of data is deterministic, we have $\\mathcal{D'}_{K'} = \\mathcal{D}_K$.\nFor approximate deduplication, there might be edge cases where the two documents are near duplicates but only one of those documents is retrieved in the top-K results. However, the property that none of the final top-k retrieved results are (near-)duplicates of each other will still hold."}, {"title": "A.4.3 Reranking", "content": "Reranking is optionally applied to enhance retrieval quality in our pipeline. Given the top-K retrieved documents, we apply a stronger retrieval model to embed each document and the query,"}, {"title": "A.5 Data Subsampling", "content": "To study how model behavior changes with datastore scale, we must subsample datastores of different sizes (i.e., different sampling ratios) and with different random seeds. We first describe a naive way of subsampling from a large datastore (Algorithm 1) and show that it is computationally expensive to scale the datastore in this manner. We then introduce our efficient datastore subsampling strategy (Algorithm 2) to make datastore scaling more affordable. We also provide theoretical justification to show that our method is equivalent with high probability to naive datastore subsampling, and additionally provide a computational comparison of the two approaches. Finally, we discuss the commutability of subsampling and other operations, demonstrating that users can flexibly reorder some of these operations in practice.\nFirst, we define a few functions used in our algorithm.\nFunction SUBSAMPLE(\\mathcal{D}, p, s). Given a data corpus D with N elements, we sample each element i.i.d. following a Bernoulli(p) distribution with probability p, and a random seed s set such that for any given (p, s), the same x will always either be sampled or not. The total number of sampled data follows Binomial(N, p).\nFunction SEARCH(q, D, k). Given a query q and a datastore D, this function returns the k documents from the datastore with the highest similarity to q.\nFunction GETTOP(D, k). This function takes in a descending ordered list D and returns the first k elements.\nA naive way of studying datastore scaling is to subsample from the raw data for every combination of subsampling ratio and random seed, as Algorithm 1 shows. However, this approach is computationally inefficient as it would entail repeatedly running subsampling over the raw data, building an index, and running retrieval search.\nTo affordably study datastore scaling trends, we propose an efficient way to subsample over the datastore. Instead of subsampling from the raw data, for each query, we first retrieve the top-K documents from the entire datastore, and then subsample from this set. We then take the top-k from"}, {"title": "Algorithm 1 Naive implementation of datastore scaling", "content": ""}, {"title": "Algorithm 2 Our efficient datastore scaling implementation", "content": ""}, {"title": "Lemma A.3. Subsampling from the retrieved top-K documents with probability p and then taking the top-k (k <K) from the subsampled documents (Algorithm 2) is equivalent with high probability to directly retrieving top-k documents from a datastore that is subsampled from the entire raw text datastore with probability p (Algorithm 1). The equivalence holds as long as there are enough k"}, {"title": "Lemma A.4. Independent element-level operations are commutable with each other. Set-level operations are not commutable.", "content": "Proof. Since the results of independent element-level operations are not impacted by their order of execution, they are commutative with each other. However, this does not hold true for set-level operations, which are order-sensitive.\nWe note that both merging and subsampling can be considered independent element-level operations if we regard the removed documents as being classified by either process to be masked out. As a results, operations such as data quality filters, data decontamination, reranking can be moved around before or after post-hoc merging, which made it possible for us to efficiently evaluate the effect of their variants by moving them to after retrieval and merging.\nProposition A.1. Our MASSIVEDS pipeline is equivalent to the naive pipeline, as shown in Figure 2, with high probability.\nProof. Lemma 1 shows that the distributed indexing and retrieval is equivalent to unsharded"}]}