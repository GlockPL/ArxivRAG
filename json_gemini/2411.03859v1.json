{"title": "UniTraj: Universal Human Trajectory Modeling from Billion-Scale Worldwide Traces", "authors": ["Yuanshao Zhu", "James Jianqiao Yu", "Xiangyu Zhao", "Xuetao Wei", "Yuxuan Liang"], "abstract": "Human trajectory modeling is essential for deciphering movement patterns and supporting advanced applications across various domains. However, existing methods are often tailored to specific tasks and regions, resulting in limitations related to task specificity, regional dependency, and data quality sensitivity. Addressing these challenges requires a universal human trajectory foundation model capable of generalizing and scaling across diverse tasks and geographic contexts. To this end, we propose UniTraj, a Universal human Trajectory foundation model that is task-adaptive, region-independent, and highly generalizable. To further enhance performance, we construct WorldTrace, the first large-scale, high-quality, globally distributed dataset sourced from open web platforms, encompassing 2.45 million trajectories with billions of points across 70 countries. Through multiple resampling and masking strategies designed for pre-training, UniTraj effectively overcomes geographic and task constraints, adapting to heterogeneous data quality. Extensive experiments across multiple trajectory analysis tasks and real-world datasets demonstrate that UniTraj consistently outperforms existing approaches in terms of scalability and adaptability. These results underscore the potential of UniTraj as a versatile, robust solution for a wide range of trajectory analysis applications, with WorldTrace serving as an ideal but non-exclusive foundation for training.", "sections": [{"title": "1 INTRODUCTION", "content": "Human trajectory data, which captures the movement paths of individuals or groups over time, has become increasingly significant in various domains such as transportation management [25], logistics optimization [13], and web-based services [43]. With the widespread adoption of GPS-enabled devices and the integration of positioning technologies into numerous applications, vast amounts of trajectory data are generated daily from vehicles and mobile devices connected to the Internet [18, 24, 34]. This type of data permits us an unprecedented opportunity to analyze movement patterns, traffic flow, and user mobility behaviors, supporting a range of applications from real-time traffic updates to location-based services and personalized content recommendations [3].\nTo effectively harness this wealth of data, robust human traj\u0435\u0441-tory modeling techniques are essential to extract meaningful insights. Modeling trajectories allows us to convert raw location data into actionable information, uncovering human mobility patterns across spatial and temporal dimensions to enable advanced applications in diverse fields [2]. From a task-oriented perspective, existing methods often utilize various statistical and machine learning techniques (e.g., CNNs and RNNs) to capture detailed spatio-temporal features [20]. These models are typically optimized for specific tasks, with algorithms and architectures tailored to address distinct challenges such as trajectory prediction, anomaly detection, and activity recognition [23]. On the data side, researchers employ a wide array of trajectory datasets gathered from sources like vehicles, mobile devices, and other GPS-enabled equipment [3, 25]. These datasets vary significantly in size, geographic coverage, and quality, providing essential support for model development and evaluation. Collectively, these task-specific modeling efforts and diverse data sources have propelled advancements in human trajectory analysis, deepening our understanding of mobility behavior.\nDespite these advancements, existing methods face significant limitations that impede their generalizability and practical applicability: Despite these advancements, existing methods face (i) Task Specificity: Current approaches are typically designed and optimized for specific tasks, lacking the flexibility to adapt across different applications without extensive modifications. This task-centric focus restricts their reusability across a range of trajectory-related problems, including prediction, classification, and anomaly detection. (ii) Regional Dependency: Many models are developed and trained on data from specific geographic regions, limiting their effectiveness when applied to trajectories from diverse locations. Variations in infrastructure, traffic patterns, and behaviors across regions mean that models confined on narrow geographic data often fail to capture the diversity essential for global trajectory datasets, thus struggling to generalize to new environments. (iii) Data Quality Sensitivity: Real-world trajectory data is inherently heterogeneous, with variability in sampling rates, noise levels, and occasional missing data due to differences in data collection criteria and device capabilities. Existing models are typically sensitive to these inconsistencies, leading to degraded performance when faced with noisy or incomplete data. This sensitivity requires extensive data preprocessing and cleaning, which may not always be practical, reducing the robustness of these models in real-world scenarios.\nWhat measures can be taken to overcome these limitations? Empirically, developing a task-adaptive, region-independent, and scalable foundation model for universal trajectory modeling is both an emerging necessity and a promising trend [41]. As shown in Figure 1, such a model can generalize across various tasks without requiring specialized models for each application, thereby enhancing scalability and efficiency. Additionally, a foundation model can effectively handle diverse data qualities, making it adaptable to real-world scenarios where data variability is the norm. However, constructing a universal trajectory foundation model presents two primary challenges:\n\u2022 Data preparation: Constructing a foundation model requires the collection and integration of vast amounts of high-quality trajectory data, covering different geographic regions, sampling rates, and user behaviors. However, most existing datasets are primarily held by a limited number of companies or organizations with proprietary rights or restrictive access policies, hindering widespread usage and collaborative research. Furthermore, the labor and financial costs associated with data collection make obtaining large-scale, high-quality trajectory datasets particularly difficult. As a result, available datasets (such as GeoLife [45] and Porto [26]) are often restricted to specific regions or cities, reducing their generalizability and constraining research aimed at broader, global applications.\n\u2022 Model Design: A universal trajectory foundation model must be equipped several capabilities that current approaches lack. First, the model should be capable of generalizing across diverse spatio-temporal contexts, enabling it to serve as a backbone that can be adapted to a wide range of tasks without extensive modifications. Second, it must maintain robust representation capabilities to handle data with varying qualities, demonstrating resilience to noise, missing values, and inconsistent sampling rates. Finally, the model should balance complexity and computational efficiency, avoiding overfitting to specific data patterns while remaining scalable for large datasets.\nWith these challenges in mind, we introduce WorldTrace, the first large-scale, high-quality, globally distributed trajectory dataset sourced from open platforms. Spanning 2.45 million trajectories with billions of points across 70 countries, WorldTrace overcomes the limitations of existing datasets by offering extensive geographic coverage, diverse sampling rates, and accessible data, thereby calling widespread use and collaboration. Meanwhile, we present UniTraj, a Universal human Trajectory foundation model designed to be task-adaptive, region-independent, and resilient to varying data quality. UniTraj can serve as a versatile backbone capable of supporting diverse trajectory analysis tasks without dependence on a specific dataset, though it achieves optimal performance when trained on high-quality, diverse data like WorldTrace. In addition, our approach employs advanced pre-training techniques, including multiple resampling and masking strategies, which enable UniTraj to capture complex spatio-temporal dependencies and adapt to heterogeneous data characteristics across regions and sampling frequencies. This design promotes robust generalization across tasks and regions, offering a scalable and efficient solution for a wide range of trajectory analysis applications.\nIn summary, the contributions of our research are as follows:\n\u2022 We construct the first large-scale, high-quality, globally distributed trajectory dataset, called WorldTrace. This dataset overcomes the limitations of existing datasets by offering accessible data for widespread use and collaboration, facilitating research with a broader global perspective, and supporting the development of universal trajectory models.\n\u2022 We propose UniTraj, a universal human trajectory foundation model that leverages advanced techniques such as multiple resampling and masking strategies. UniTraj can serve as a backbone to captures complex spatio-temporal dependencies and adapts to heterogeneous data characteristics across different regions and sampling rates.\n\u2022 We conduct extensive experiments across diverse trajectory analysis tasks and real-world datasets, demonstrating the scalability and adaptability of UniTraj. Additionally, we validate the unique advantages of WorldTrace, highlighting its potential as an ideal dataset for building robust and generalizable trajectory models."}, {"title": "2 PRELIMINARY", "content": "In this section, we introduce the fundamental concepts and problem statements pertinent to our work, followed by a background introduction about trajectory dataset and foundation models."}, {"title": "2.1 Problem Definition", "content": "Definition 1 (Human Trajectory). Trajectory refers to the sequential record of the movement of individuals or groups through space over time. Formally, a trajectory \u03c4t of length n is represented as a sequence of continuously sampled GPS points: \u03c4 = {P1, P2, .., pn}, where each point pi = (lngi, lati, ti) denotes the spatial coordinates (longitude and latitude) at timestamp t\u012f. In addition, the sampling interval between two consecutive points is defined as \u2206ti = ti-ti\u22121, for i = 2, ..., n. Note that the sampling intervals within or between trajectory data may be consistent or inconsistent.\nDefinition 2 (Trajectory Dataset). A trajectory dataset is a collection of multiple trajectories, each representing the movement of an object over time. Formally, a trajectory dataset is defined as: D = {T1, T2, ..., T|11|}, where |D| is the total number of trajectories within the dataset.\nProblem Statement (Universal Trajectory Modeling). The primary objective of this study is to develop a foundation model for"}, {"title": "2.2 Related Work", "content": "2.2.1 Trajectory Datasets. The availability of comprehensive trajectory datasets is fundamental for advancing research in trajectory-related analysis. Over the years, several datasets have been developed, each differing in source, geographic coverage, granularity, and data quality (Note that we only focus on GPS trajectory data, other types of disjoint point-of-interest sequences are not discussed in this paper) One of the most renowned datasets is GeoLife [44], collected over five years (from April 2007 to August 2012) by 182 users. This dataset has been instrumental in various research domains such as travel mode detection [5], location recommendation [7], and traffic flow analysis [19]. Despite its extensive application, GeoLife is limited by its coverage and participant diversity, capturing movement patterns from a relatively small population. Datasets like Porto [26], T-drive [37], and Electric Vehicle Data [32] are collected through GPS devices mounted on taxis. These datasets provide a broader picture of the activities of multiple individuals but typically offer low or uneven sampling rates, which limits detailed movement pattern analysis. As a synthetic dataset, SynMob offers a uniform sampling rate and an unlimited amount of data [46], but its quality and regions still depend on the original data. Proprietary datasets like GAIA Initiative by Didi Chuxing [9], Grab-Posisi [16], and Taxi-Shanghai have released a series of high-quality trajectory datasets [9]. But these datasets are often restricted access because of authority concerns and regulation limitations, limiting their availability for widespread research and collaboration.\nIn summary, while existing trajectory datasets have significantly contributed to urban mobility research, they are frequently constrained by limitations such as restricted geographic coverage, low sampling rates, limited participant diversity, accessibility issues, and data quality inconsistencies. These challenges highlight the need for alternative trajectory data sources that offer extensive coverage, high-quality data, and open accessibility. Such datasets would enhance the scope and quality of urban traffic analysis and support the development of more generalizable and robust trajectory models.\n2.2.2 Foundation Models. In recent years, foundation models have significantly advanced the fields natural language processing (NLP) and computer vision (CV), the development of foundation models specifically tailored for trajectory data remains largely underexplored. Trajectory data present unique challenges, such as irregular sampling intervals, spatial heterogeneity, and complex temporal dependencies, which are not fully addressed by existing time series or spatio-temporal models. In domains like NLP and CV, models such as BERT [8], GPT-3 [1], and Vision Transformers (ViT) [10] have shown that it is possible to learn powerful, generalizable representations through large-scale pretraining, enabling the models to be adapted for a variety of downstream tasks. Building upon these, researchers have begun exploring foundation models for time series and spatio-temporal data. In time series analysis, models like TST [40], TimeFM [6], and Moirai [35] employ Transformer architectures [31] to capture temporal dependencies and patterns in sequential data, facilitating applications in forecasting, anomaly detection, and classification. For spatio-temporal prediction, efforts have been made to develop models that handle data varying over both space and time, such as UniST [39] and ClimaX [27], which are traffic flow prediction and climate modeling. However, trajectory data introduces additional complexities, as models must not only capture spatio-temporal dependencies but also handle noise, missing data, and varying data quality. Although some efforts have been made towards trajectory-specific models, such as TrajGDM [4] and TrajFM [22], these models are typically task-specific or regionally focused, lacking the generalization capabilities and robustness seen in other domains. For example, while foundation models like MAE [14] and TimeFM [6] have demonstrated success in unsupervised learning for image and time series data, trajectory models require even more flexibility to transfer across regions and tasks. The existing approaches often fail to provide the backbone structure needed to serve diverse scenarios without customizing stand-alone models for each task or region. Therefore, there is a pressing need for trajectory foundation models that can unify various trajectory-related tasks under a single, pre-trained framework, similar to how foundation models in NLP and CV offer robust and transferable representations across tasks. Such models must generalize across tasks, exhibit strong representational capabilities despite data variability, and maintain computational efficiency to prevent overfitting while scaling across different applications and regions."}, {"title": "3 WORLDTRACE DATASET CONSTRUCTION", "content": "In this section, we introduce WorldTrace, a comprehensive and globally distributed trajectory dataset that overcomes the limitations of existing datasets. We detail the data acquisition process, describe the preprocessing steps, and present key statistics and analyses. As shown in Figure 2, WorldTrace provides extensive geographic coverage, encompassing trajectory data from 70 countries and spanning diverse environments and infrastructure types. This global distribution, visualized in Figure2(a), illustrates a dense representation in North America, East Asia, and parts of Europe, with trajectory counts exceeding 107 in the most represented regions. Figure 2(b) displays the top 10 countries by trajectory counts, with the United States, China, and Canada leading in data volume. This distribution highlights the diversity of movement patterns captured in the dataset, spanning both developed and emerging economic areas. Furthermore, Figure 2(c) shows the data density within the contiguous United States, demonstrating high-resolution coverage along major road networks and urban centers. Together, these figures emphasize the potential of WorldTrace to be suitable for the development of region-independent and universal trajectory foundation models."}, {"title": "3.1 Data Acquisition", "content": "The raw data for WorldTrace is sourced from the shared trajectory trace platform on OpenStreetMap (OSM) [28], which has hosted over 11 million GPS trajectories contributed by users worldwide since 2004. This platform\u00b9 provides a rich repository of movement data suitable for large-scale analysis. To ensure data quality and reliability, we specifically filtered for vehicle trajectory data uploaded between 2021 and 2023, focusing on tags that indicated motorized movement. By prioritizing recent and labeled data, we mitigated challenges related to variable data quality, device heterogeneity, and potential obsolescence, resulting in a more consistent and reliable dataset for developing a universal trajectory model. The raw data from OSM is provided in GPX (GPS Exchange Format) files in XML format, containing geographic coordinates (latitude and longitude), timestamps, and optional altitude information. This standardized format not only ensures uniformity in data attributes but also simplifies data parsing and preprocessing, as it captures temporal and spatial information in a structured manner. Prior to integration, we applied preprocessing to eliminate duplicate entries and anomalous data points, such as unrealistically faraway or erroneous coordinates, to enhance data consistency."}, {"title": "3.2 Data Preprocessing", "content": "During data acquisition, we encountered challenges related to inconsistent data formats and varying sampling rates. To standardize the data and ensure its suitability for modeling, we implemented a series of preprocessing steps:\n(1) Normalization: The raw trajectory data from the original system had a high sampling frequency of 10 Hz (10 points per second), resulting in data redundancy and increased storage requirements. To mitigate this, we resampled the trajectories to a standardized rate of one point per second. This adjustment reduced redundancy and optimized storage without sacrificing key movement details, ensuring that the temporal resolution remained sufficient to capture the dynamics of the trajectories.\n(2) Filtering: To improve dataset relevance and accuracy, we applied multiple filtering criteria. Trajectories with fewer than 32 points or covering distances under 100 meters were discarded, as short trips generally lack meaningful movement patterns and could introduce noise. Additionally, following standard practices [5], we removed trajectories with unrealistic speeds (e.g., over 120 km/h), which often indicate GPS errors or data anomalies. This filtering ensured that only meaningful and realistic trajectories were retained for analysis.\n(3) Calibration: Given that GPS signals can suffer from errors due to building obstructions, multipath effects, and receiver noise [15], we applied map-matching techniques [36] to align raw GPS points with underlying road networks. This calibration step corrected positioning errors, improving spatial accuracy and making the trajectories more reliable for analysis."}, {"title": "3.3 Data Analysis and Statistic", "content": "After acquiring and preprocessing the raw trajectory data, we conducted an in-depth analysis to examine the characteristics and quality of the WorldTrace dataset. This analysis offers critical insights into the diversity, quality, and geographic coverage, all essential for developing a universal trajectory foundation model. Figure 2 presents a global heatmap along with the distribution of trajectories across the top 10 contributing countries within WorldTrace, highlighting a wide geographical spread that includes both developed and emgering economies. This distribution ensures various movement patterns influenced by different transportation infrastructures, cultural behaviors, and environmental conditions. We also summarize the key statistical attributes of WorldTrace in Table 1. The dataset comprises approximately 2.45 million trajectories with around 8.8 billion raw GPS points, covering 70 countries across all inhabited continents. To maintain data consistency, we normalized the sampling interval to 1 second, and the dataset spans from August 2021 to December 2023. The average trajectory duration is around 6 minutes, covering an average distance of 5.73 kilometers with an average speed of 48.0 km/h. Additionally, trajectory lengths vary significantly, from as few as 32 points to over 600 points, with an average length of approximately 358 points per trajectory. To summarize, WorldTrace provides a robust resource for training and evaluating universal trajectory models. Its extensive coverage and comprehensive statistical properties make it suitable for developing models that generalize across tasks and geographic regions, thereby addressing the limitations of existing trajectory datasets.\nData Privacy and Copyright. All data collection adhered strictly to privacy regulations and ethical guidelines. Trajectories were"}, {"title": "4 UNIVERSAL TRAJECTORY MODELING", "content": "As illustrated in Figure 3, UniTraj consists of three main components: trajectory handling and model structure, and downstream task adaptation. The trajectory handling module includes resampling and masking strategies designed to manage varying sampling intervals and enhance the model's adaptability to different data qualities. The model structure employs a general encoder-decoder structure with tokenization and positional embedding to capture spatio-temporal dependencies. Finally, we detail the downstream task adaptation process, where pre-trained encoder of UniTraj serves as a backbone, allowing it to support multiple trajectory-related tasks with minimal additional training or fine-tuning."}, {"title": "4.1 Resampling Strategies", "content": "The sampling rate in trajectory data is crucial, as it determines the granularity of location information and the fidelity of captured motion patterns, both of which significantly impact modeling performance. However, real-world trajectories often have heterogeneous sampling rates due to differing data collection standards, device capabilities, and user behaviors. For instance, while the Porto dataset uses a 15-second sampling interval [26], the T-drive dataset averages an interval of 177 seconds [38]. Such variability introduces inconsistencies in temporal resolution, which can degrade model performance when a model trained on uniformly sampled data is applied to datasets with different sampling characteristics. This variability highlights the need for diverse temporal resolutions during training to enhance the generalization across practical scenarios. A model capable of handling different sampling rates is inherently more robust and better suited for real-world applications, where data heterogeneity is prevalent. Additionally, the diversity in trajectory lengths poses challenges in balancing data integrity and computational efficiency. Traditional uniform sampling methods struggle to handle both extremes effectively: long trajectories may suffer from data redundancy, increased computational costs, and potential overfitting, while short trajectories risk losing critical information if undersampled, compromising the model's ability to learn meaningful representations.\nTo address these challenges, we adopt two complementary resampling strategies: a dynamic trajectory resampling method and an intervals consistent resampling approach. These strategies serve two main purposes. First, they help manage data redundancy and computational load by adjusting the sampling rate according to trajectory length, ensuring efficiency. Second, they create diverse temporal intervals that improve the model's generalization capability across datasets with varying temporal characteristics, making it better suited to handle the diverse data conditions encountered in real-world applications.\n4.1.1 Dynamic Trajectory Resampling. We introduce a dynamic resampling strategy based on a logarithmic sampling ratio that adjusts the sampling rate according to the trajectory length. Specifically, the sampling ratio function R(n) is designed to decrease logarithmically as the trajectory length n increases:\n$R(n) =\\begin{cases}R_{min}, & n \\geq n_{max} \\\\ 1 - (1 - R_{min}) \\varphi(n), & n_{min} < n < N_{max} \\\\ 1, & n \\leq n_{min}\\end{cases}$ (2)\nwhere $R_{min}$ is the minimum sampling ratio, and $n_{min}$ and $n_{max}$ denotes the shortest and longest length thresholds, respectively. The normalization factor $\u03c6(n)$ is computed as follows:\n$\u03c6(n) = \\frac{ln(n - n_{min} + 1)}{ln(n_{max} - n_{min} + 1)}$ (3)\nThis formulation ensures that the sampling ratio decreases smoothly with increasing trajectory length, resulting in high sampling retention for shorter trajectories and a substantial reduction for longer ones. Specifically: For short trajectories (n \u2264 nmin): we set R(n) = 1, retaining all points to preserve detailed movement information. For long trajectories (n \u2265 nmin): we set R(n) = Rmin, significantly reducing data volume and avoiding redundancy in trajectories with many points. For intermediate trajectories (nmin \u2264 n \u2264 nmax): The sampling ratio decreases logarithmically decreases from 1 to Rmin, controlled by the normalization factor \u03c6(n).\nThis dynamic adjustment offers key benefits: it controls trajectory length, reduces computational costs by eliminating redundancy, and increases dataset diversity through varied time intervals. This variability exposes the model to a wider range of sampling frequencies during training, enhancing robustness and adaptability to real-world data inconsistencies.\n4.1.2 Interval Consistent Resampling. In addition to the dynamic resampling strategy, we employ Interval Consistent Resampling to ensure uniform temporal intervals between consecutive points within a trajectory. Specifically, this method resamples each trajectory at fixed time intervals, \u2206t, to create a standardized temporal structure across the dataset. Formally, given a trajectory \u03c4, we select points at intervals of \u2206t, yielding a resampled trajectory \u03c4' = Pk\u2081,\u00b7\u00b7\u00b7, Pkm, where the indices kj are defined as:\n$k_j = 1 + (j - 1) \\cdot \\Delta t, \\quad \\text{for} \\quad j = 1, 2, ..., m,$ (4)"}, {"title": "4.2 Masking Strategies", "content": "In the context of trajectory modeling, masking is essential in pre-training, as it encourages models to learn robust and generalizable representations. Inspired by self-supervised learning techniques in NLP and CV, masking entails concealing portions of the input data and requiring the model to predict the missing segments [8, 14]. This approach compels the model to capture both local and global dependencies within the data, thereby enhancing performance in downstream tasks.\nMasking serves several critical functions for trajectory data. First, it acts as data augmentation, introducing variability in the input without additional data collection. Second, it strengthens model robustness by enabling it to handle incomplete or noisy trajectories issues often encountered in real-world settings due to sensor errors or communication interruptions. Finally, masking supports task-agnostic pre-training, allowing the model to learn generalized features that can later be fine-tuned for various trajectory-related tasks, such as prediction and classification. To maximize the effectiveness and compatibility of the model, we employ four distinct masking strategies, as illustrated in Figure 3. Each strategy is tailored to capture different trajectory aspects, ensuring that the model learns both global and local patterns while effectively handling diverse real-world challenges.\nFormally, given a resampled trajectory \u03c4 = P1, P2,..., Pn, we define a general masking function M(t, r) that replaces a fraction r of the points with a mask token [MASK] at selected indices IC {1, 2, ..., n}, r = |I|/n. This process results in a masked trajectory:\n$\\tilde{\\tau} = M(\\tau, r) = \\{p_1, ..., [MASK]_{i \\in I},..., p_n\\}.$ (5)\nTo comprehensively capture various patterns within the trajectory data, we employ four distinct masking strategies:\n\u2022 Random Masking: In this strategy, we randomly select a subset of the points Irand to mask. By randomly masking points throughout the trajectory, the model is encouraged to learn both local and global dependencies, as the masked points are not confined to any particular position in the sequence:\n$I_{rand} \\sim Uniform(\\{1, 2, ..., n\\}).$ (6)\nRandom masking is effective for training the model to capture general spatio-temporal patterns across various parts of the trajectory, enhancing its robustness to missing data points.\n\u2022 Block Masking: Block masking targets consecutive points within the trajectory, simulating scenarios where continuous segments of data may be missing. In this approach, we define blocks of a specified size b and mask all points within each block. The starting indices of the blocks are chosen randomly, creating masked indices Iblock that represent contiguous sequences of points:\n$I_{block} = \\{k, k + 1, ..., k + b - 1\\}, \\quad \\text{for some} k.$ (7)\nBlock masking is particularly useful for teaching the model to handle longer-term dependencies, preparing it to reconstruct missing segments that may occur due to sensor failures or temporary communication losses.\n\u2022 Key Points Masking: Key points masking focuses on significant trajectory points, such as turns or points where speed or direction changes notably. We identify these key points using the Ramer-Douglas-Peucker (RDP) algorithm [11], which simplifies a trajectory by retaining points that are farthest from the line P1pn connecting the first and last points. The indices selected for masking, Ikey, are determined by:\n$I_{key} = \\{p_k | d_{max}(p_k, p_1 p_n) > \\epsilon\\},$ (8)\nwhere e is a predefined threshold, and the maximum distance $d_{max} = \\max \\{d(p_k, p_1 p_n) | 2 \\leq k \\leq n - 1\\}$ measures deviation from this line. As summarized in Algorithm 1, the RDP algorithm iteratively identifies the point pk that maximizes dmax = d(pk, P1Pn). If dmax > \u20ac, the corresponding point pk is treated as a key point and included in the mask set Ikey. This process is recursively applied to the trajectory segments $T_{left} = \\{p_1,..., p_k\\}$"}, {"title": "4.3 Model Structure", "content": "We adopt a flexible encoder-decoder architecture for UniTraj, designed to capture both local and global trajectory patterns. This structure enables effective reconstruction and prediction of trajectories, making UniTraj adaptable to trajectory-based applications.\n4.3.1 Trajectory Tokenizer. Effective trajectory modeling requires transforming raw spatial and temporal data into structured embeddings that capture both spatial relationships and temporal dynamics. Tokenizing trajectory data presents unique challenges due to varying trajectory lengths, the heterogeneous nature of spatial and temporal components, and the need to capture dynamic dependencies across time. To address these challenges, our trajectory tokenizer method converts spatial coordinates and timestamps into embeddings that retain individual significance and encode spatio-temporal relationships.\nWe use a two-part tokenizer that separately embeds spatial and temporal components and then combines them into a unified representation. For the spatial component, we normalize the trajectory to the origin by subtracting the starting point, (xi, Yi) = (Ing Ing\u2081, lati \u2013 lat\u2081), allowing the model to focus on relative movement patterns rather than absolute locations. Each normalized spatial coordinate is transformed into a d-dimensional space using a 1D convolutional or linear layer, resulting in the spatial embedding hs = Conv1D(xi, yi) \u2208 Rd. For the temporal component, we compute the time interval between consecutive points \u2206ti (with \u2206t\u2081 = 0), embedding it into the same d-dimensional space via a linear layer, producing the temporal embedding ht = Linear(\u2206t\u012f) \u2208 Rd. We then combine the spatial and temporal embeddings by summation, yielding a unified point embedding hi = hs + ht \u2208 Rd. This final embedding effectively captures both spatial and temporal dependencies, creating a comprehensive representation for each trajectory point that is fed into the model.\n4.3.2 Positional Embedding. In addition to encoding the spatial and temporal details of each trajectory point, it is essential to capture the relative positional relationships between points. These relationships enable the model to comprehend the movement sequence and the timing between points, both crucial for accurate trajectory modeling. To achieve this, we employ Rotary Position Encoding (ROPE) [30], which maintains the relative positional information between points by rotating the trajectory embedding vectors. Given the combined spatial-temporal embeddings hi for point i in the trajectory, RoPE applies a rotational transformation:\n$ROPE(h_i) = \\begin{pmatrix} cos \\theta_i & - sin \\theta_i \\\\ sin \\theta_i & cos \\theta_i \\end{pmatrix} \\begin{pmatrix} h^{(1)}_i\\\\ h^{(2)}_i \\end{pmatrix},$ (10)\nwhere $h^{(1)}_i$ and $h^{(2)}_i$ are the first and second halves of the embedding hi, and i is a rotation angle that varies proportionally with the position index i. Specifically, \u03b8\u00a1 is calculated as \u03b8\u2081 = $\\frac{i}{10000^{2k/d}}$, where k is the index of the embedding dimension, and d is the total dimension of the embedding.\nThe main advantage of RoPE is its ability to preserve relative positional information through rotational symmetry. This ensures that the relative distance and directional relationships between points are maintained, enabling the model to capture both local patterns (e.g., short-term movements) and global patterns (e.g., long-range directionality) within a trajectory. By encoding these relative positions, ROPE strengthens the model's capacity to understand movement dynamics across varying scales.\n4.3.3 Encoder-Decoder Architecture. The UniTraj employs an encoder-decoder architecture inspired by [14], tailored for trajectory data. Both the encoder and decoder are built using stacks of Transformer blocks [31], which utilize RoPE-powered self-attention mechanisms to capture dependencies within trajectory embeddings. This architecture leverages the encoder to learn rich trajectory representations from visible points and the decoder to reconstruct masked points.\nEncoder. The encoder processes the visible points in a trajectory, which are the unmasked points from the input sequence. The"}, {"title": "4.4 Training and Representing Extraction", "content": "Training Objective. The model is trained using a reconstruction objective, aiming to minimize the discrepancy between the predicted points and the original points ti at the masked positions. The loss function is defined as follows:\n$L = \\frac{1}{|I|} \\sum_{i \\in I} ||t_i - f_{\\theta, \\varphi}(\\tau)||_2^2 ,$ (12)\nwhere $f\u03b8, \u03c6(7)$ represents the encoder-decoder output, and i denotes the predicted coordinates with position. Optimizing this objective function enables the model to infer missing trajectory segments based on the observed data, allowing it to learn robust spatial-temporal dependencies essential for trajectory modeling.\nRepresentation Extraction. For inference and downstream task applications, we leverage only the encoder to extract trajectory representations as follows:\nz = E ({hi | i = 1, ..., m}) . (13)\nThe encoder outputs, represented by z, serve as rich trajectory embeddings, which can be directly utilized for various tasks. For tasks requiring fixed-dimensional representations, a pooling operation or token selection can be applied to the sequence of encoder outputs, providing compact and informative representations tailored for downstream applications."}, {"title": "5 EXPERIMENTS", "content": "In this section, we comprehensively evaluate the performance of our proposed UniTraj across six real-world trajectory datasets, focusing on three key dimensions. We first outline the experimental setup, detailing the evaluation dimensions, datasets, and implement details. Following this, we present results and analysis for each of the four downstream tasks, demonstrating UniTraj's generalization ability, adaptability, and robustness across diverse trajectory analysis scenarios."}, {"title": "5.1 Experimental Setups", "content": "5.1.1 Evaluation Dimensions. Our experimental analysis spans three key evaluation dimensions to comprehensively assess the capabilities of UniTraj and the WorldTrace dataset. First, the Task Applicability Analysis examines UniTraj's adaptability and generalizability across various trajectory-related tasks, including recovery, prediction, classification, and generation. This evaluation reveals robustness of UniTraj as a backbone architecture capable of supporting diverse geographic contexts and trajectory-based applications in real-world settings. Second, the Dataset Study investigates the efficacy of training on our proposed WorldTrace dataset compared to other real-world datasets. This analysis demonstrates that WorldTrace provides superior model performance, underscoring its potential as a foundational dataset for developing universal trajectory foundation models. Finally, the Model Study delves into the impact of architectural components, parameters, and design choices on overall performance and scalability, helping identify optimal configurations for processing large-scale trajectory data efficiently.\n5.1.2 Datasets. We evaluate the performance of the proposed model using six diverse real-world trajectory datasets. Each dataset represents different data collection scenarios, quality levels, motion patterns, and geographic regions, providing a comprehensive test of the capabilities of UniTraj.\n\u2022 WorldTrace: WorldTrace is our proposed large-scale, globally distributed dataset, which we describe in detail in Section 3. From the original dataset, we curated a high-quality subset of 1.1 million trajectories, which have been filtered to remove long stops and loops. Of this subset, 1 million trajectories are designated for model training combined with resampling or masking strategies, with the remaining 100,000 reserved for testing without any operation. To ensure consistency and enable independent zero-shot evaluations, the testing dataset is normalized to a sampling interval of 3 seconds per point.\n\u2022 Chengdu [9]: The Chengdu dataset comprises over one million urban mobility trajectories collected from taxis operating in Chengdu, China, reflecting daily commuting and transportation patterns in a densely urbanized area. It features dense, high-frequency (3-second for most trajectories) sampling points that provide detailed insights into active urban environments.\n\u2022 Xi'an [9]: Similar to Chengdu, the Xi'an dataset includes millions of taxi trajectories gathered in Xi'an, China, focusing on movement patterns within another densely populated Chinese city. The data, collected during November 2016, captures the traffic dynamics and urban mobility behaviors specific to this region.\n\u2022 GeoLife [45]: The GeoLife dataset is a widely used trajectory dataset collected over three years by 182 users, primarily in Beijing, China. It is mainly distinguished by a wide variety of travel modes, including walking, cycling and driving. With this data, we can study the trajectory movement patterns and behavioral habits of different travel modes. Besides, this dataset suffers from"}, {"title": "5.2 Task Applicability Analysis", "content": "5.2.1 Trajectory Recovery. The objective of the trajectory recovery task is to evaluate the representation learning capability of the model by reconstructing incomplete trajectories with partially missing points. This task holds real-world significance, as trajectory data often suffers from missing points due to irregular sampling rates or sensor issues. Effective trajectory recovery showcases the ability to capture underlying spatio-temporal dependencies, enhancing robustness across varied data scenarios. In this experiment, we randomly mask 50% of trajectory points and test the recovery performance. We evaluate UniTraj in both zero-shot (trained solely on WorldTrace) and fine-tuned settings (trained on WorldTrace and then fine-tuned on each respective dataset), aiming to understand its adaptability with and without task-specific training. Additionally, we compare UniTraj against a diverse range of baselines, including traditional deep learning models (Linear, DHTR [33", "31": "and DeepMove [12", "29": "and TrajFM [22"}]}