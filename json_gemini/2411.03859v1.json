[{"title": "UniTraj: Universal Human Trajectory Modeling from Billion-Scale Worldwide Traces", "authors": ["Yuanshao Zhu", "James Jianqiao Yu", "Xiangyu Zhao", "Xuetao Wei", "Yuxuan Liang"], "abstract": "Human trajectory modeling is essential for deciphering movement patterns and supporting advanced applications across various domains. However, existing methods are often tailored to specific tasks and regions, resulting in limitations related to task specificity, regional dependency, and data quality sensitivity. Addressing these challenges requires a universal human trajectory foundation model capable of generalizing and scaling across diverse tasks and geographic contexts. To this end, we propose UniTraj, a Universal human Trajectory foundation model that is task-adaptive, region-independent, and highly generalizable. To further enhance performance, we construct WorldTrace, the first large-scale, high-quality, globally distributed dataset sourced from open web platforms, encompassing 2.45 million trajectories with billions of points across 70 countries. Through multiple resampling and masking strategies designed for pre-training, UniTraj effectively overcomes geographic and task constraints, adapting to heterogeneous data quality. Extensive experiments across multiple trajectory analysis tasks and real-world datasets demonstrate that UniTraj consistently outperforms existing approaches in terms of scalability and adaptability. These results underscore the potential of UniTraj as a versatile, robust solution for a wide range of trajectory analysis applications, with WorldTrace serving as an ideal but non-exclusive foundation for training.", "sections": [{"title": "1 INTRODUCTION", "content": "Human trajectory data, which captures the movement paths of individuals or groups over time, has become increasingly significant in various domains such as transportation management [25], logistics optimization [13], and web-based services [43]. With the widespread adoption of GPS-enabled devices and the integration of positioning technologies into numerous applications, vast amounts of trajectory data are generated daily from vehicles and mobile devices connected to the Internet [18, 24, 34]. This type of data permits us an unprecedented opportunity to analyze movement patterns, traffic flow, and user mobility behaviors, supporting a range of applications from real-time traffic updates to location-based services and personalized content recommendations [3].\nTo effectively harness this wealth of data, robust human trajectory modeling techniques are essential to extract meaningful insights. Modeling trajectories allows us to convert raw location data into actionable information, uncovering human mobility patterns across spatial and temporal dimensions to enable advanced applications in diverse fields [2]. From a task-oriented perspective, existing methods often utilize various statistical and machine learning techniques (e.g., CNNs and RNNs) to capture detailed spatio-temporal features [20]. These models are typically optimized for specific tasks, with algorithms and architectures tailored to address distinct challenges such as trajectory prediction, anomaly detection, and activity recognition [23]. On the data side, researchers employ a wide array of trajectory datasets gathered from sources like vehicles, mobile devices, and other GPS-enabled equipment [3, 25]. These datasets vary significantly in size, geographic coverage, and quality, providing essential support for model development and evaluation. Collectively, these task-specific modeling efforts and diverse data sources have propelled advancements in human trajectory analysis, deepening our understanding of mobility behavior.\nDespite these advancements, existing methods face significant limitations that impede their generalizability and practical applicability: Despite these advancements, existing methods face (i) Task Specificity: Current approaches are typically designed and optimized for specific tasks, lacking the flexibility to adapt across different applications without extensive modifications. This task-centric focus restricts their reusability across a range of trajectory-related problems, including prediction, classification, and anomaly detection. (ii) Regional Dependency: Many models are developed and trained on data from specific geographic regions, limiting their effectiveness when applied to trajectories from diverse locations. Variations in infrastructure, traffic patterns, and behaviors across regions mean that models confined on narrow geographic data often fail to capture the diversity essential for global trajectory datasets, thus struggling to generalize to new environments. (iii) Data Quality Sensitivity: Real-world trajectory data is inherently heterogeneous, with variability in sampling rates, noise levels, and occasional missing data due to differences in data collection criteria and device capabilities. Existing models are typically sensitive to these inconsistencies, leading to degraded performance when faced with noisy or incomplete data. This sensitivity requires extensive data preprocessing and cleaning, which may not always be practical, reducing the robustness of these models in real-world scenarios.\nWhat measures can be taken to overcome these limitations? Empirically, developing a task-adaptive, region-independent, and scalable foundation model for universal trajectory modeling is both an emerging necessity and a promising trend [41]. As shown in Figure 1, such a model can generalize across various tasks without requiring specialized models for each application, thereby enhancing scalability and efficiency. Additionally, a foundation model can effectively handle diverse data qualities, making it adaptable to real-world scenarios where data variability is the norm. However, constructing a universal trajectory foundation model presents two primary challenges:\n\u2022 Data preparation: Constructing a foundation model requires the collection and integration of vast amounts of high-quality trajectory data, covering different geographic regions, sampling rates, and user behaviors. However, most existing datasets are primarily held by a limited number of companies or organizations with proprietary rights or restrictive access policies, hindering widespread usage and collaborative research. Furthermore, the labor and financial costs associated with data collection make obtaining large-scale, high-quality trajectory datasets particularly difficult. As a result, available datasets (such as GeoLife [45] and Porto [26]) are often restricted to specific regions or cities, reducing their generalizability and constraining research aimed at broader, global applications.\n\u2022 Model Design: A universal trajectory foundation model must be equipped several capabilities that current approaches lack. First, the model should be capable of generalizing across diverse spatio-temporal contexts, enabling it to serve as a backbone that can be adapted to a wide range of tasks without extensive modifications. Second, it must maintain robust representation capabilities to handle data with varying qualities, demonstrating resilience to noise, missing values, and inconsistent sampling rates. Finally, the model should balance complexity and computational efficiency, avoiding overfitting to specific data patterns while remaining scalable for large datasets.\nWith these challenges in mind, we introduce WorldTrace, the first large-scale, high-quality, globally distributed trajectory dataset sourced from open platforms. Spanning 2.45 million trajectories with billions of points across 70 countries, WorldTrace overcomes the limitations of existing datasets by offering extensive geographic coverage, diverse sampling rates, and accessible data, thereby calling widespread use and collaboration. Meanwhile, we present UniTraj, a Universal human Trajectory foundation model designed to be task-adaptive, region-independent, and resilient to varying data quality. UniTraj can serve as a versatile backbone capable of supporting diverse trajectory analysis tasks without dependence on a specific dataset, though it achieves optimal performance when trained on high-quality, diverse data like WorldTrace. In addition, our approach employs advanced pre-training techniques, including multiple resampling and masking strategies, which enable UniTraj to capture complex spatio-temporal dependencies and adapt to heterogeneous data characteristics across regions and sampling frequencies. This design promotes robust generalization across tasks and regions, offering a scalable and efficient solution for a wide range of trajectory analysis applications.\nIn summary, the contributions of our research are as follows:\n\u2022 We construct the first large-scale, high-quality, globally distributed trajectory dataset, called WorldTrace. This dataset overcomes the limitations of existing datasets by offering accessible data for widespread use and collaboration, facilitating research with a broader global perspective, and supporting the development of universal trajectory models.\n\u2022 We propose UniTraj, a universal human trajectory foundation model that leverages advanced techniques such as multiple resampling and masking strategies. UniTraj can serve as a backbone to captures complex spatio-temporal dependencies and adapts to heterogeneous data characteristics across different regions and sampling rates.\n\u2022 We conduct extensive experiments across diverse trajectory analysis tasks and real-world datasets, demonstrating the scalability and adaptability of UniTraj. Additionally, we validate the unique advantages of WorldTrace, highlighting its potential as an ideal dataset for building robust and generalizable trajectory models."}, {"title": "2 PRELIMINARY", "content": "In this section, we introduce the fundamental concepts and problem statements pertinent to our work, followed by a background introduction about trajectory dataset and foundation models."}, {"title": "2.1 Problem Definition", "content": "Definition 1 (Human Trajectory). Trajectory refers to the sequential record of the movement of individuals or groups through space over time. Formally, a trajectory \\(\\tau_t\\) of length n is represented as a sequence of continuously sampled GPS points: \\(\\tau = {P_1, P_2, .., p_n}\\), where each point \\(p_i = (lng_i, lat_i, t_i)\\) denotes the spatial coordinates (longitude and latitude) at timestamp \\(t_i\\). In addition, the sampling interval between two consecutive points is defined as \\(\\Delta t_i = t_i-t_{i-1}\\), for i = 2, ..., n. Note that the sampling intervals within or between trajectory data may be consistent or inconsistent.\nDefinition 2 (Trajectory Dataset). A trajectory dataset is a collection of multiple trajectories, each representing the movement of an object over time. Formally, a trajectory dataset is defined as: \\(D = {\\tau_1, \\tau_2, ..., \\tau_{|D|}}\\}, where |D| is the total number of trajectories within the dataset.\nProblem Statement (Universal Trajectory Modeling). The primary objective of this study is to develop a foundation model for universal human trajectory data that can adaptively operate across different applications and geographic regions while processing heterogeneous data sources. Formally, the problem is defined as follows: Given a set of trajectories \\(D = {\\tau_i}\\), where each trajectory \\(\\tau_i\\) is as defined in Definition 1, the goal is to learn a mapping function:\n\\[F: \\tau \\rightarrow h \\in \\mathbb{R}^d,\\]\nwhich maps a raw trajectory \\(\\tau_t\\) to a d-dimensional representation tensor h. The function F() should be capable of capturing the intrinsic spatial-temporal patterns inherent in the trajectories. These trajectory representations h are intended to support various analytical tasks across different regions and applications, such as trajectory classification, prediction, and anomaly detection. The model should generalize well to unseen data and be robust to the heterogeneity present in real-world trajectory datasets."}, {"title": "2.2 Related Work", "content": "2.2.1 Trajectory Datasets. The availability of comprehensive trajectory datasets is fundamental for advancing research in trajectory-related analysis. Over the years, several datasets have been developed, each differing in source, geographic coverage, granularity, and data quality (Note that we only focus on GPS trajectory data, other types of disjoint point-of-interest sequences are not discussed in this paper) One of the most renowned datasets is GeoLife [44], collected over five years (from April 2007 to August 2012) by 182 users. This dataset has been instrumental in various research domains such as travel mode detection [5], location recommendation [7], and traffic flow analysis [19]. Despite its extensive application, GeoLife is limited by its coverage and participant diversity, capturing movement patterns from a relatively small population. Datasets like Porto [26], T-drive [37], and Electric Vehicle Data [32] are collected through GPS devices mounted on taxis. These datasets provide a broader picture of the activities of multiple individuals but typically offer low or uneven sampling rates, which limits detailed movement pattern analysis. As a synthetic dataset, SynMob offers a uniform sampling rate and an unlimited amount of data [46], but its quality and regions still depend on the original data. Proprietary datasets like GAIA Initiative by Didi Chuxing [9], Grab-Posisi [16], and Taxi-Shanghai have released a series of high-quality trajectory datasets [9]. But these datasets are often restricted access because of authority concerns and regulation limitations, limiting their availability for widespread research and collaboration.\nIn summary, while existing trajectory datasets have significantly contributed to urban mobility research, they are frequently constrained by limitations such as restricted geographic coverage, low sampling rates, limited participant diversity, accessibility issues, and data quality inconsistencies. These challenges highlight the need for alternative trajectory data sources that offer extensive coverage, high-quality data, and open accessibility. Such datasets would enhance the scope and quality of urban traffic analysis and support the development of more generalizable and robust trajectory models.\n2.2.2 Foundation Models. In recent years, foundation models have significantly advanced the fields natural language processing (NLP) and computer vision (CV), the development of foundation models specifically tailored for trajectory data remains largely underexplored. Trajectory data present unique challenges, such as irregular sampling intervals, spatial heterogeneity, and complex temporal dependencies, which are not fully addressed by existing time series or spatio-temporal models. In domains like NLP and CV, models such as BERT [8], GPT-3 [1], and Vision Transformers (ViT) [10] have shown that it is possible to learn powerful, generalizable representations through large-scale pretraining, enabling the models to be adapted for a variety of downstream tasks. Building upon these, researchers have begun exploring foundation models for time series and spatio-temporal data. In time series analysis, models like TST [40], TimeFM [6], and Moirai [35] employ Transformer architectures [31] to capture temporal dependencies and patterns in sequential data, facilitating applications in forecasting, anomaly detection, and classification. For spatio-temporal prediction, efforts have been made to develop models that handle data varying over both space and time, such as UniST [39] and ClimaX [27], which are traffic flow prediction and climate modeling. However, trajectory data introduces additional complexities, as models must not only capture spatio-temporal dependencies but also handle noise, missing data, and varying data quality. Although some efforts have been made towards trajectory-specific models, such as TrajGDM [4] and TrajFM [22], these models are typically task-specific or regionally focused, lacking the generalization capabilities and robustness seen in other domains. For example, while foundation models like MAE [14] and TimeFM [6] have demonstrated success in unsupervised learning for image and time series data, trajectory models require even more flexibility to transfer across regions and tasks. The existing approaches often fail to provide the backbone structure needed to serve diverse scenarios without customizing stand-alone models for each task or region.\nTherefore, there is a pressing need for trajectory foundation models that can unify various trajectory-related tasks under a single, pre-trained framework, similar to how foundation models in NLP and CV offer robust and transferable representations across tasks. Such models must generalize across tasks, exhibit strong representational capabilities despite data variability, and maintain computational efficiency to prevent overfitting while scaling across different applications and regions."}, {"title": "3 WORLDTRACE DATASET CONSTRUCTION", "content": "In this section, we introduce WorldTrace, a comprehensive and globally distributed trajectory dataset that overcomes the limitations of existing datasets. We detail the data acquisition process, describe the preprocessing steps, and present key statistics and analyses. As shown in Figure 2, WorldTrace provides extensive geographic coverage, encompassing trajectory data from 70 countries and spanning diverse environments and infrastructure types. This global distribution, visualized in Figure2(a), illustrates a dense representation in North America, East Asia, and parts of Europe, with trajectory counts exceeding \\(10^7\\) in the most represented regions. Figure 2(b) displays the top 10 countries by trajectory counts, with the United States, China, and Canada leading in data volume. This distribution highlights the diversity of movement patterns captured in the dataset, spanning both developed and emerging economic areas. Furthermore, Figure 2(c) shows the data density within the contiguous United States, demonstrating high-resolution coverage along major road networks and urban centers. Together, these figures emphasize the potential of WorldTrace to be suitable for the development of region-independent and universal trajectory foundation models."}, {"title": "3.1 Data Acquisition", "content": "The raw data for WorldTrace is sourced from the shared trajectory trace platform on OpenStreetMap (OSM) [28], which has hosted over 11 million GPS trajectories contributed by users worldwide since 2004. This platform\u00b9 provides a rich repository of movement data suitable for large-scale analysis. To ensure data quality and reliability, we specifically filtered for vehicle trajectory data uploaded between 2021 and 2023, focusing on tags that indicated motorized movement. By prioritizing recent and labeled data, we mitigated challenges related to variable data quality, device heterogeneity, and potential obsolescence, resulting in a more consistent and reliable dataset for developing a universal trajectory model. The raw data from OSM is provided in GPX (GPS Exchange Format) files in XML format, containing geographic coordinates (latitude and longitude), timestamps, and optional altitude information. This standardized format not only ensures uniformity in data attributes but also simplifies data parsing and preprocessing, as it captures temporal and spatial information in a structured manner. Prior to integration, we applied preprocessing to eliminate duplicate entries and anomalous data points, such as unrealistically faraway or erroneous coordinates, to enhance data consistency."}, {"title": "3.2 Data Preprocessing", "content": "During data acquisition, we encountered challenges related to inconsistent data formats and varying sampling rates. To standardize the data and ensure its suitability for modeling, we implemented a series of preprocessing steps:\n(1) Normalization: The raw trajectory data from the original system had a high sampling frequency of 10 Hz (10 points per second), resulting in data redundancy and increased storage requirements. To mitigate this, we resampled the trajectories to a standardized rate of one point per second. This adjustment reduced redundancy and optimized storage without sacrificing key movement details, ensuring that the temporal resolution remained sufficient to capture the dynamics of the trajectories.\n(2) Filtering: To improve dataset relevance and accuracy, we applied multiple filtering criteria. Trajectories with fewer than 32 points or covering distances under 100 meters were discarded, as short trips generally lack meaningful movement patterns and could introduce noise. Additionally, following standard practices [5], we removed trajectories with unrealistic speeds (e.g., over 120 km/h), which often indicate GPS errors or data anomalies. This filtering ensured that only meaningful and realistic trajectories were retained for analysis.\n(3) Calibration: Given that GPS signals can suffer from errors due to building obstructions, multipath effects, and receiver noise [15], we applied map-matching techniques [36] to align raw GPS points with underlying road networks. This calibration step corrected positioning errors, improving spatial accuracy and making the trajectories more reliable for analysis."}, {"title": "3.3 Data Analysis and Statistic", "content": "After acquiring and preprocessing the raw trajectory data, we conducted an in-depth analysis to examine the characteristics and quality of the WorldTrace dataset. This analysis offers critical insights into the diversity, quality, and geographic coverage, all essential for developing a universal trajectory foundation model. Figure 2 presents a global heatmap along with the distribution of trajectories across the top 10 contributing countries within WorldTrace, highlighting a wide geographical spread that includes both developed and emgering economies. This distribution ensures various movement patterns influenced by different transportation infrastructures, cultural behaviors, and environmental conditions. We also summarize the key statistical attributes of WorldTrace in Table 1. The dataset comprises approximately 2.45 million trajectories with around 8.8 billion raw GPS points, covering 70 countries across all inhabited continents. To maintain data consistency, we normalized the sampling interval to 1 second, and the dataset spans from August 2021 to December 2023. The average trajectory duration is around 6 minutes, covering an average distance of 5.73 kilometers with an average speed of 48.0 km/h. Additionally, trajectory lengths vary significantly, from as few as 32 points to over 600 points, with an average length of approximately 358 points per trajectory. To summarize, WorldTrace provides a robust resource for training and evaluating universal trajectory models. Its extensive coverage and comprehensive statistical properties make it suitable for developing models that generalize across tasks and geographic regions, thereby addressing the limitations of existing trajectory datasets.\nData Privacy and Copyright. All data collection adhered strictly to privacy regulations and ethical guidelines. Trajectories were anonymized, and any personally identifiable information was excluded to protect user privacy. In addition, all raw data follows the Open Data Commons Open Database License (ODbL) license from OSM: http://opendatacommons.org/licenses/odbl/1.0/. We will share derived datasets under the same license terms to respect the data use policies of the community. Currently, we provide a data sample for reference 2."}, {"title": "4 UNIVERSAL TRAJECTORY MODELING", "content": "As illustrated in Figure 3, UniTraj consists of three main components: trajectory handling and model structure, and downstream task adaptation. The trajectory handling module includes resampling and masking strategies designed to manage varying sampling intervals and enhance the model's adaptability to different data qualities. The model structure employs a general encoder-decoder structure with tokenization and positional embedding to capture spatio-temporal dependencies. Finally, we detail the downstream task adaptation process, where pre-trained encoder of UniTraj serves as a backbone, allowing it to support multiple trajectory-related tasks with minimal additional training or fine-tuning."}, {"title": "4.1 Resampling Strategies", "content": "The sampling rate in trajectory data is crucial, as it determines the granularity of location information and the fidelity of captured motion patterns, both of which significantly impact modeling performance. However, real-world trajectories often have heterogeneous sampling rates due to differing data collection standards, device capabilities, and user behaviors. For instance, while the Porto dataset uses a 15-second sampling interval [26], the T-drive dataset averages an interval of 177 seconds [38]. Such variability introduces inconsistencies in temporal resolution, which can degrade model performance when a model trained on uniformly sampled data is applied to datasets with different sampling characteristics. This variability highlights the need for diverse temporal resolutions during training to enhance the generalization across practical scenarios. A model capable of handling different sampling rates is inherently more robust and better suited for real-world applications, where data heterogeneity is prevalent. Additionally, the diversity in trajectory lengths poses challenges in balancing data integrity and computational efficiency. Traditional uniform sampling methods struggle to handle both extremes effectively: long trajectories may suffer from data redundancy, increased computational costs, and potential overfitting, while short trajectories risk losing critical information if undersampled, compromising the model's ability to learn meaningful representations.\nTo address these challenges, we adopt two complementary resampling strategies: a dynamic trajectory resampling method and an intervals consistent resampling approach. These strategies serve two main purposes. First, they help manage data redundancy and computational load by adjusting the sampling rate according to trajectory length, ensuring efficiency. Second, they create diverse temporal intervals that improve the model's generalization capability across datasets with varying temporal characteristics, making it better suited to handle the diverse data conditions encountered in real-world applications.\n4.1.1 Dynamic Trajectory Resampling. We introduce a dynamic resampling strategy based on a logarithmic sampling ratio that adjusts the sampling rate according to the trajectory length. Specifically, the sampling ratio function R(n) is designed to decrease logarithmically as the trajectory length n increases:\n\\[R(n) = \\begin{cases} R_{min}, & n \\geq n_{max} \\\\ 1- (1 \u2013 R_{min}) \\varphi (n), & n_{min} < n < N_{max} \\\\ 1, & n \\leq n_{min} \\end{cases}\\]\nwhere \\(R_{min}\\) is the minimum sampling ratio, and \\(n_{min}\\) and \\(n_{max}\\) denotes the shortest and longest length thresholds, respectively. The normalization factor \\(\\varphi(n)\\) is computed as follows:\n\\[\\varphi(n) = \\frac{ln(n - n_{min} + 1)}{ln(n_{max} - n_{min} + 1)}\\]\nThis formulation ensures that the sampling ratio decreases smoothly with increasing trajectory length, resulting in high sampling retention for shorter trajectories and a substantial reduction for longer ones. Specifically: For short trajectories (\\(n \\leq n_{min}\\)): we set R(n) = 1, retaining all points to preserve detailed movement information. For long trajectories (\\(n \\geq n_{min}\\)): we set \\(R(n) = R_{min}\\), significantly reducing data volume and avoiding redundancy in trajectories with many points. For intermediate trajectories (\\(n_{min} \\leq n \\leq n_{max}\\)): The sampling ratio decreases logarithmically decreases from 1 to \\(R_{min}\\), controlled by the normalization factor (\\(\\varphi(n)\\)).\nThis dynamic adjustment offers key benefits: it controls trajectory length, reduces computational costs by eliminating redundancy, and increases dataset diversity through varied time intervals. This variability exposes the model to a wider range of sampling frequencies during training, enhancing robustness and adaptability to real-world data inconsistencies.\n4.1.2 Interval Consistent Resampling. In addition to the dynamic resampling strategy, we employ Interval Consistent Resampling to ensure uniform temporal intervals between consecutive points within a trajectory. Specifically, this method resamples each trajectory at fixed time intervals, \\(\\Delta t\\), to create a standardized temporal structure across the dataset. Formally, given a trajectory \\(\\tau\\), we select points at intervals of \\(\\Delta t\\), yielding a resampled trajectory \\(\\tau' = {P_{k_1},\u00b7\u00b7\u00b7, P_{k_m}}\\}, where the indices \\(k_j\\) are defined as:\n\\[k_j = 1 + (j \u2212 1) \\cdot \\Delta t, \\text{ for } j = 1, 2, ..., m,\\]\nwith m representing the number of points in the resampled trajectory and \\(\\Delta t\\) specifying the desired sampling interval in seconds. This approach enforces temporal uniformity, which simplifies the modeling process and enhances the model's capacity to learn consistent temporal patterns. By mitigating issues associated with irregular or inconsistent sampling in raw data, interval consistent resampling complements the dynamic resampling strategy: while dynamic resampling adjusts sampling based on trajectory length to control data volume and introduce variability, interval consistent resampling standardizes the temporal structure across trajectories."}, {"title": "4.2 Masking Strategies", "content": "In the context of trajectory modeling, masking is essential in pre-training, as it encourages models to learn robust and generalizable representations. Inspired by self-supervised learning techniques in NLP and CV, masking entails concealing portions of the input data and requiring the model to predict the missing segments [8, 14]. This approach compels the model to capture both local and global dependencies within the data, thereby enhancing performance in downstream tasks.\nMasking serves several critical functions for trajectory data. First, it acts as data augmentation, introducing variability in the input without additional data collection. Second, it strengthens model robustness by enabling it to handle incomplete or noisy trajectories issues often encountered in real-world settings due to sensor errors or communication interruptions. Finally, masking supports task-agnostic pre-training, allowing the model to learn generalized features that can later be fine-tuned for various trajectory-related tasks, such as prediction and classification. To maximize the effectiveness and compatibility of the model, we employ four distinct masking strategies, as illustrated in Figure 3. Each strategy is tailored to capture different trajectory aspects, ensuring that the model learns both global and local patterns while effectively handling diverse real-world challenges.\nFormally, given a resampled trajectory \\(\\tau = {P_1, P_2,..., P_n}\\), we define a general masking function M(t, r) that replaces a fraction r of the points with a mask token [MASK] at selected indices I \\(\\subset\\) {1, 2, ..., n}, r = |I|/n. This process results in a masked trajectory:\n\\[\\tilde{\\tau} = M(\\tau, r) = {p_1, ..., [MASK]_{i \\in I},..., p_n}.\\]\nTo comprehensively capture various patterns within the trajectory data, we employ four distinct masking strategies:\n\u2022 Random Masking: In this strategy, we randomly select a subset of the points \\(I_{rand}\\) to mask. By randomly masking points throughout the trajectory, the model is encouraged to learn both local and global dependencies, as the masked points are not confined to any particular position in the sequence:\n\\[I_{rand} \\thicksim Uniform({1, 2, . . ., n}).\\]\nRandom masking is effective for training the model to capture general spatio-temporal patterns across various parts of the trajectory, enhancing its robustness to missing data points.\n\u2022 Block Masking: Block masking targets consecutive points within the trajectory, simulating scenarios where continuous segments of data may be missing. In this approach, we define blocks of a specified size b and mask all points within each block. The starting indices of the blocks are chosen randomly, creating masked indices \\(I_{block}\\) that represent contiguous sequences of points:\n\\[I_{block} = {k, k + 1, ..., k + b \u2212 1}, \\text{ for some } k.\\]\nBlock masking is particularly useful for teaching the model to handle longer-term dependencies, preparing it to reconstruct missing segments that may occur due to sensor failures or temporary communication losses.\n\u2022 Key Points Masking: Key points masking focuses on significant trajectory points, such as turns or points where speed or direction changes notably. We identify these key points using the Ramer-Douglas-Peucker (RDP) algorithm [11], which simplifies a trajectory by retaining points that are farthest from the line \\(P_1P_n\\) connecting the first and last points. The indices selected for masking, \\(I_{key}\\), are determined by:\n\\[I_{key} = {P_k | d_{max} (P_k, P_1P_n) > \\epsilon},\\]\nwhere \\(\\epsilon\\) is a predefined threshold, and the maximum distance \\(d_{max} = max {d(p_k, P_1P_n) | 2 \\leq k \\leq n \u2212 1}\\) measures deviation from this line. As summarized in Algorithm 1, the RDP algorithm iteratively identifies the point \\(p_k\\) that maximizes \\(d_{max} = d(p_k, P_1P_n)\\). If \\(d_{max} > \\epsilon\\), the corresponding point \\(p_k\\) is treated as a key point and included in the mask set \\(I_{key}\\). This process is recursively applied to the trajectory segments \\(T_{left} = {p_1,..., p_k}\\) and \\(T_{right} = {p_k,..., P_n}\\), isolating critical points for masking. By focusing on these pivotal points, the model is challenged to reconstruct essential trajectory segments, reinforcing its understanding of key structural patterns within trajectories.\n\u2022 Last N Masking: This strategy masks the last N points of the trajectory, simulating scenarios where future points are unavailable and must be inferred from observed data. The masked indices, \\(I_{last}\\), comprise the last N points of the trajectory:\n\\[I_{last} = {n- N + 1, n \u2013 N + 2, ..., n}.\\]\nThis method is particularly useful for tasks such as trajectory prediction, where the model needs to predict future points based on the past observed data.\nBy using these four masking strategies, we can train the model to effectively predict missing parts of the trajectory while learning spatio-temporal dependencies. Each strategy introduces unique challenges, forcing the model to generalize different types of missing data scenarios, enhancing its robustness for downstream tasks."}, {"title": "4.3 Model Structure", "content": "We adopt a flexible encoder-decoder architecture for UniTraj", "30": "which maintains the relative positional information between points by rotating the trajectory embedding vectors. Given the combined spatial-temporal embeddings \\(h_i\\) for point i in the trajectory", "transformation": "n\\[ROPE(h_i) = \\begin{pmatrix"}, "cos \\theta_i & - sin \\theta_i \\\\ sin \\theta_i & cos \\theta_i \\end{pmatrix} \\begin{pmatrix} h_i^{(1)} \\\\ h_i^{(2)} \\end{pmatrix},\\"], "representations": "z_{enc"}, ["MASK"], ["Z_{dec} = Reorder \\begin{cases} Z_{i} = Z_{enc, j}, & \\text{if } i = Index(j), i \\notin I \\\\ [MASK"], {}]