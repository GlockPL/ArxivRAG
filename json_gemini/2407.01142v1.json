{"title": "Integrated feature analysis for deep learning interpretation and class activation maps", "authors": ["Yanli Li", "Tahereh Hassanzadeh", "Denis P. Shamonin", "Monique Reijnierse", "Annette H.M. van der Helm-van Mil", "Berend C. Stoel"], "abstract": "Understanding the decisions of deep learning (DL) models is essential for the acceptance of DL to risk-sensitive applications, such as security systems, industrial anomaly detection and medical imaging. Although methods, like class activation maps (CAMs), give a glimpse into the black box, they do miss some crucial information, thereby limiting its interpretability and merely providing the considered locations of objects. To provide more insight into the models and the influence of datasets, we propose an integrated feature analysis method, which consists of feature distribution analysis and feature decomposition, to look closer into the intermediate features extracted by DL models. This integrated feature analysis could provide information on overfitting, confounders, outliers in datasets, model redundancies and principal features extracted by the models, and provide distribution information to form a common intensity scale, which are missing in current CAM algorithms. The integrated feature analysis was applied to eight different datasets for general validation: photographs of handwritten digits, two datasets of natural images and five medical datasets, including skin photography, ultrasound, CT, X-rays and MRIs. The method was evaluated by calculating the consistency between the CAMs average class activation levels and the logits of the model. Based on the eight datasets, the rescaled CAMs achieved on average increases in consistency of around 23.1%, 51.6%, 15.5%, 22.7%, 32.9%, 10.7%, 64.2% and 17.1%, respectively. The correlation coefficients were all very close to 100%, proving the effectiveness of the distribution analysis in the integrated feature analysis. Moreover, based on the feature decomposition, 5%-25% of features could generate equally informative saliency maps and obtain the same model performances as using all features. This proves the reliability of the feature decomposition. As the proposed methods rely on very few assumptions, this is a step towards better model interpretation and a useful extension to existing CAM algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "PROVIDING insight into how deep learning (DL) models make decisions is a prerequisite before DL models can be applied in risk-sensitive areas. The potential risk of making decisions based on confounders is a non-trivial issue in risk-averse domains like healthcare [1], security systems [2] and other practices [3]. An intuitive idea to validate the reliability of deep learning models is to confirm that models are making decisions based on reasonable evidence and convincing features. Since there is no formal definition of interpreting DL models, current interpretation algorithms rely on some commonly agreed principles [4]. For instance, a dog should be classified based on the dog's face instead of the background or other objects in the image, or a bird should be distinguished from other animals because of its wings. Three principles are mainly followed in current studies: (1) analyzing the responses of or changes in the model's output to investigate the model, by assessing the individual contributions of samples in datasets [5], estimating the learning difficulty [6] or detecting misclassified samples [7]; (2) analyzing low-level features extracted by the models to investigate the model's feature extraction process [8]-[10]; and (3) analyzing the contribution of certain parts in the input image to the model's output, evaluating whether the contribution of these parts is consistent with human knowledge. Compared to some early efforts based on principles (1) and (2), methods based on principle (3) are more intuitive and easier for visual checks and understanding. As a typical representative of methods based on principle (3), class activation mapping and its variants, which generate class activation maps (CAMs), demonstrate high computational efficiency compared to other widely-used methods, such as LIME and its variants [11]\u2013[14], global interpretation [15]-[17] and other interpretation methods [18]-[23], becoming one of the most popular methods for investigating model reliability and interpretability. CAMs (also known as saliency maps, heatmaps or attention maps) provide information on the locations of objects considered by the given models, based on the assumption that weighted features could precisely describe the model outputs. The weighted features in CAM algorithms usually refer to the summed features from a certain layer of the models, weighted by the designed weights that are assumed to represent the \"importance\" of these features for the final output class. Fig. 1 presents a summary of the workflow and terms used in generating CAMs and the formulas for the most common variations of CAMs, including the original CAM [9], Grad CAM [24], Grad CAM++ [25], Layer CAM [26], xGrad CAM [27], Score CAM [28], Smooth Grad CAM++ [29], SS-CAM [30] and IS-CAM [31], and a flowchart of the shared process. These algorithms gave promising performances on complicated tasks by locating the target objects in the images with complex backgrounds and foregrounds [32] and developed the field of weakly supervised object detection [33]. Although these algorithms provide important insight, some important information is lost in the calculation process as shown in the red boxes in Fig. 1: (1) due to intensity scaling at a single-image level an individual intensity scale, real (unscaled) class activation levels are missing of each input image, which would be useful for the comparison with other input images, are missing due to intensity scaling at a single-image level an individual intensity scale; (2) pixels that provide negative contributions (lowering the confidence based on some reasonable hints) to the logits of the selected classes are dismissed by selecting only positive class activations to create CAMs in current methods; and (3) features that are very frequently activated for some classes over the whole dataset are treated the same as features that are hardly activated at all, since CAMs simply sum the weighted features together. As a result, the only information left in current CAMs is the considered location of the objects in one image, which can be misleading. Consequently, these CAMs may be inconsistent with respect to the corresponding logits and confidence of the model. Some images that did not contain target objects of the selected class still received higher class activations in the CAMs than those with the target objects (See Fig. 2(a)). Moreover, without ground truths or prior knowledge, it is impossible to determine from the CAMs when the model has high confidence in the output class or when the model is struggling to classify correctly (See Fig. 2(b)). Confounders may have falsely \u201ccontributed\" to the model outputs, yet they are invisible because negative class activations are discarded during the generation of CAMs. Furthermore, the lack of presenting more detailed information in CAMs could limit its applications to the fields that require scalar outputs rather than binary outputs, like the severity of diseases or lesions in some specific regions in medical imaging. Generally, current algorithms do not provide information on the working of a model, apart from the location of the target objects of the selected classes. Providing additional information about the models and the datasets requires methods to be developed, based on principles (2) and (3).\nBased on the same assumption of utilizing weighted features, we aimed to retrieve more information by providing more insight into the trained models and datasets, and solving the above problems of current algorithms. Specifically, we propose an integrated feature analysis that consists of a distribution analysis and a feature decomposition, focusing on the statistical information (e.g., peaks, valleys and spread of the weighted features) and the independence of each weighted feature (e.g., frequency of becoming activated over the whole dataset), respectively. Visualized and validated by modifying and improving the CAMs, the integrated feature analysis enabled us to fill the gaps between models and CAMs and improve the understanding of models and datasets similar to the method based on principles (2) and (3). The layout of this paper is as follows. First, we introduce two parts of the proposed integrated feature analysis: (1) distribution analysis and (2) feature decomposition, followed by the CAM generation process based on these two parts. Subsequently, we introduce the methodology to evaluate the proposed method. In the section on evaluation, some quantitative evaluation methods are introduced, including current evaluation metrics based on object localization and a new method, based on consistency that excludes the impact of model performances and prior knowledge involvements. We validated the integrated feature analysis by proving that distribution analysis could help to improve the consistency between the class activation levels of CAMs and the corresponding model confidence and logits, and feature decomposition contributes to finding the principal features that contain the main information for the model outputs. Following the methodology and evaluation, we briefly introduce the eight datasets from different subjects and modalities, used for validation. After the basic information, we present the overall performance of the proposed methods on these eight datasets. In the last two chapters, we discuss and summarize some applications that could improve the understanding of models and datasets, together with the advantages and limitations of the proposed method."}, {"title": "II. METHOD", "content": "The integrated feature analysis consists of two parts: (1) distribution analysis and (2) feature decomposition. The distribution analysis aims at determining the spread, standard deviation and percentiles of the class activation levels of weighted features, in order to compare these levels from one sample to another or to the average level over the whole dataset. The feature decomposition focuses on the value of each feature through a so-called \"importance matrix\u201d. Fig. 3 presents the process of the integrated feature analysis and the calculation of the importance matrix, including the path for distribution analysis before summing up the weighted features and the path for calculating a column of the importance matrix (with a shape of [number of features (F), number of output classes (C)]) for the selected class c."}, {"title": "A. Distribution analysis", "content": "The analysis of the feature distribution forms the basis for feature decomposition and standard CAM generation. The collected information enables a common intensity scale for the CAM generation (shown in Fig. 4(a)), which could improve the consistency between the CAMs and the models.\nThe CAM calculation with a common intensity scale can be described as: instead of scaling the products of weights and features based on the maximum and minimum of only the current input, the whole dataset is used to scale the weights and features (training set if time-available, validation set if time-limited) obtained during the distribution analysis a common intensity scaling compared to the individual intensity scaling in current methods. The revised saliency map generation is given by:\n$S-CAM = \\sigma_{p_{10}}(scale_{space} (weights \\cdot features))$  (1)"}, {"title": "B. Feature decomposition", "content": "The feature decomposition aims at determining the independence and importance of each feature, without considering the spatial information and the correlation with other features. We define the importance of a feature by the average contribution of this feature (measured through the sum of a weighted feature) to the model logits of the selected class over the whole dataset, as each feature represents a fixed feature extraction path after training the model (See Fig. 3, the \"Importance\" of features). To determine the principal features, we introduce an importance matrix, measuring the importance of each feature based on the above definition of importance. Importance matrix is a measurement with a shape of [number of features (F), number of classes (C)] for the analysis of each feature, in which each row is the average class activation of a certain feature for all classes on the whole dataset, and each column indicates the average class activation of all features for a specific selected class. The $IM_{f,c}$ refers to the average class activation level of the $f^{th}$ feature for the $c^{th}$ class on the whole dataset. Instead of summing up the F weighted features to generate the CAMs, we obtain the F values for each input, model and the selected class c, by calculating the sums of each separate feature and regard these fth value in the F values as the \"importance\" of the fth feature. As the model does not change and therefore the size of F does not change, the corresponding F values from different inputs can be accumulated. Finally, a vector with a shape of [F, 1(c)] is obtained, containing F values for each selected class that represents the average importance of each feature for each selected class on the whole dataset. By merging all vectors in the order of the features, the importance matrix of [F, C] can be obtained.\n$IM_{f,c} = \\sum_{n=0}^{N} ((weight_{n} \\cdot feature_{n})_{f})/N$ (3)\nEq. 3 defines a column of the importance matrix of the fth feature for a selected class c, where N represents the total number of samples in the datasets and n represents the nth sample, f refers to the fth feature out of all features from the target layer in the deep learning model, $weight_{n}$ and $feature_{f}$ refers to the weights and features designed by the CAM algorithms for a selected class c and fth feature out of all features. Generating importance matrices based on Eq. 3 is time-consuming if the models are used for datasets with a large number of classes, because the weights of CAMs are calculated based on a selected class, which requires to change the selected classes for calculation. For example, for ImageNet 2012 that contains 1000 classes to be selected, this process would require more than 1000 for-loops to complete the importance matrix for all classes and is unacceptable. To improve the efficiency for models with many classes, we therefore proposed to generate a unified importance matrix for all classes based on the true classes instead of generating importance matrices for each class. Therefore, the CAMs generated based on the gradients from output classes naturally have the highest class activation levels in the classification based on the Argmax function. For a model designed for a 1000-class classification task with 512 features at the selected layers of the model, this approach produces an importance matrix with a shape of [512, 1000] through only one loop.\n$IM_{f,c} = \\sum_{n=0}^{N} (\\frac{(weighth \\cdot feature) gt=c}{\\begin{cases}\nN\\\\0\\end{cases} \\quad num_{gt!=c} \\quad gt!=c}$ (4)\nEq. 4 presents the modified calculation process, where $g_t$ refers to the true classes of the inputs, $num_{gt!=c}$ represents the number of samples that belong to other classes, and others remain the same as the Eq. 1. In this way, for the classification tasks with large number of classes, one loop is enough to get the importance matrices for all classes. As shown in the Fig. 4 (b), a direct and visual application of the importance matrix is that a binarized thresholded importance matrix can be applied to the CAM generation process as a purposeful filter, to filter out the features that are not frequently activated for the selected class or generate CAMs based only on principal features (e.g., Top 5% features in the importance matrix). The revised CAM generation is given by:\n$FS - CAM = \\sigma (scale_{space} (im \\cdot weights \\cdot features))$ (5)\nwhere $i_m$ represents a mask based on the importance matrix, generated by the importance matrices being thresholded according to a specific purpose, while others remain the same. For example, when the importance matrices are thresholded to get the principal features, confounders or a single feature, a CAM based only on these selected features could be generated. Similarly to commonly scaled CAMs (indicated by prefix \u2018S-'), we indicate these \"Feature Selected\" CAMs computed from a mask by a prefix \"FS-\"For example, for S-Grad CAM, after the proposed masking, we call it \"FS-S-Grad CAM\". The performance of the FS-CAMs compared to the original CAMs demonstrates the contribution of the selected features out of all features, and therefore the FS-CAMs for the principal features are used for the validation of the feature decomposition based on the importance matrix."}, {"title": "III. EVALUATION", "content": "Since the proposed integrated feature analysis could contribute to improving the CAMs, we evaluated the proposed method by evaluating the corresponding CAMs compared to the baseline CAMs based on current class activation mapping algorithms. To evaluate the proposed methods, we introduce two metrics based on the CAMs and another metric based on the accuracy of models. The two metrics based on CAMs are based on the performance of CAMs in object localization and the consistency between the class activation levels of CAMs and models' logits. The accuracy-based metric evaluates the feature decomposition according to the changes in model accuracy when different features are masked."}, {"title": "A. Object localization", "content": "The evaluation based on object localization and its variants, the most widely-used evaluation metrics, originate from the previous work [25], through measuring how much are the confidences affected by the regions highlighted in CAMs, using so-called \u201caverage increase\u201d and \u201caverage drop (decrease)\". It starts by inputting the original image, and subsequently the original image is masked by the CAMs (thresholded and normalized) and fed into the model. This generates two output confidences (one from the original image and one from the masked image). Subsequently, the average increase and decrease (drop) in confidence due to the masking is calculated over the validation dataset. However, these evaluation metrics have a significant disadvantage: the changes can be caused by both the model and the model interpretation algorithms, yet they are used to evaluate the model interpretation algorithms only, without excluding the impact of model performances and prior knowledge involvements. Evaluating interpretation algorithms through the accuracy of locating the target object also requires certain assumptions: (1) a perfect model that has perfect performance and is getting output classes \"only\" based on the target objects \u2013 without any inference from the environment or other objects that may correlate to the output classes. This assumption is so strict that no model today has been proven to meet this requirement. In addition, this evaluation also requires the model interpretation algorithms to have no prior knowledge that would lead to perfect object localization without models involved. In fact, a randomly generated model without training could receive \u201cconvincing\" CAMs with an over-designed model interpretation algorithm according to [34] Due to the above reasons, in this work, the average increase and decrease are only applied to prove that principal features according to the feature decomposition are enough for model to get the output classes and generate CAMS to locate objects, as it is indeed a good application in the field of weakly supervised object detection.\""}, {"title": "B. Consistency with the model's logits", "content": "In the Introduction section, we showed that class activation mapping algorithms assume that the well-weighted features can explain the model decision. Moreover, the class activation mapping algorithms also rely on the regions containing objects of the selected class being more activated than the other regions or images without objects belonging to the selected classes. Based on these facts, good CAMs should directly reflect the confidences/logits of the models using the class activation values at the dataset level. Therefore, we propose to evaluate the effectiveness of CAMs by calculating the correlation between the total class activation levels of the CAMs and the corresponding model confidences, which are independent of the model performance and any prior knowledge. We use the correlation between logits and the sum of class activation levels in CAMs as the metric, using Pearson's correlation coefficient if they satisfy the bivariate normal distribution, and Spearman's rank correlation coefficient if they do not. Using logits instead of confidences may improve readability, as the activation functions (e.g. the SoftMax function) break the linear relationship between logits and confidences by rescaling according to the logits for other classes, and thus lead to a non-linear relationship between model logits and class activations."}, {"title": "C. Accuracy changes after feature masking", "content": "Using the difference in accuracy between the model with and without feature selection is a direct way to evaluate the contribution of the selected features. This masking method is different from the masking method used in the object localization evaluation and is applied to the feature instead of the input. Specifically, to validate the effectiveness and contribution of the selected features, we propose to block the forward paths of the unselected features during inference, obtain the logits based only on the selected features, and then take the accuracy changes as the evaluation metric. This could avoid the accuracy decreases caused by other factors, as masking the input can lead to the destruction of the input distribution and result in a significant decrease in accuracies. We used the accuracy changes with feature masking to evaluate the effectiveness of feature decomposition."}, {"title": "IV. MATERIALS", "content": "Eight datasets and eight models from different research fields are used for experiments, together with eight different CAM algorithms applied for comparison and backbones of the integrated feature analysis."}, {"title": "A. Datasets and models", "content": "The datasets and corresponding models used for method validation include:\nILSVRC2012 [35] for 1000-class natural objects, using standard ResNet18/34/50 and VGG11/13/16 as models, with top-1 accuracies ranging from 70% to 83%;\nCats & Dogs [36] for classifying cats and dogs, using standard ResNet18/34/50 and VGG11/13/16 as the models, achieving accuracies around 99.5% (ResNet34) and 98.6% (VGG16) on 10000 test images;\nMNIST [37] for classifying ten-class digits, using a model consisting of a simple two-layer multi-head self-attention block (from Transformers) with a multi-layer perceptron (fully connected layers), achieving an accuracy of over 99.9% on test digits;\nMRI (T1-weighted contrast-enhanced with fat suppression) from the ESMIRA project [38] for classification of rheumatoid arthritis (RA). This (non-public) dataset, containing over 6000 3D MRIs from 2000 subjects, includes three classes: early arthritis clinic (EAC), clinically suspect arthralgia (CSA) and healthy controls (ATL). The task is to discriminate between these classes. The model used is a 2D plus 3D U-net encoder with a multilayer perceptron that achieves an AUC of 83%;\nCropped CT scans from the LIDC/IDRI malignancy detection database [39], where the target objects are the malignant lesions. The model for this task achieved an AUC of 81% and is a copy of VGG11;\nX-rays from the RSNA pneumonia detection task [40], the target objects are the regions with signs of pneumonia. The model for this task achieved an AUC of 84% and is a copy of ResNet34;\nUltrasound for breast cancer classification [41], and the target objects are the cancer-related regions. The model for this task achieved an AUC of 76% and is a simple multilayer CNN described in the literature;\nSkin image from the SIIM-ISIC melanoma classification [42]; the target objects are the regions associated with malignant skin cancers. The model for this task achieved an AUC of 78% and is a copy of VGG11 (resulting in low resolution due to down-sampling).\nThe discussion about the types of models and input modalities can be found in the section of generalizability in the supplementary materials."}, {"title": "B. CAMs involved", "content": "The CAM algorithms involved in the experiment are Grad CAM [24], Grad CAM++ [25], xGrad CAM [27], Score CAM [28], Smooth Grad CAM++ [29], SS-CAM [30], IS-CAM [31] and pixel-wise Grad CAM (pixel-wise gradients with features). Since the branch of perturbation-based weights (Score CAM, SS-CAM, etc.) is unacceptably time-consuming for a broad quantitative evaluation, and because the purpose of this work is not to compare different CAM algorithms, the quantitative evaluation focuses on Grad CAM, Grad CAM++, xGrad CAM and pixel-wise Grad CAM. For the branch of perturbation-based weights, such as Score CAM, we present some visual results instead."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "This section contains three subsections, including (1) visual examples of the S-CAMs and FS-S-CAMs compared to some existing CAM algorithms; (2) the quantitative evaluation of the consistency improvement with the common intensity scaling based on the distribution analysis; and (3) the quantitative evaluation of the principal features selected by the importance matrix."}, {"title": "A. Visual checks of the S-CAMs and FS-S-CAMS", "content": "Fig. 6 to 8 briefly show some examples based on existing class activation mapping algorithms compared to the CAMs with the common intensity scaling based on the P90 and P10 from the distribution analysis. In the visual checks, we present the S-Grad CAM and FS-S-Grad CAM, which is based on the most basic CAM algorithm, to show the importance and effectiveness of the common intensity scaling and the results that it could exceed other CAM algorithms on matching the model confidences. Fig. 6 presents an example of the CAMs for a selected class of Carpodacus mexicanus (ImageNet2012) based on a standard ResNet34. From the perspective of matching the confidences, S-Grad CAM and FS-S-Grad CAM with common intensity scaling managed to show a decreasing trend on their CAMs as the confidences decreased, while other methods failed.\nFig. 7 presents an example of the CAMs for a selected class of digit \"7\" [37] based on a model with a two-layer multi-head self-attention block followed by a multilayer perceptron, and an example of the CAMs for a selected class of \"dogs\u201d [36] based on a standard VGG16. In the CAMs for digit \"7\", the red boxes indicate the regions that have negative contributions to the model's decision to give an output class of \"7\" according to S-Grad CAM. This results fits our intuition since there should be nothing around the left and bottom left regions around the digit \"7\" and are supposed to have some intensities appearing on the top regions. In the CAMs for the selected class \"dogs\", the S-Grad CAM did not only remove the random highlighted regions in the backgrounds when input was a cat or an undefined object. The S-Grad CAM gave class activation levels of less than the backgrounds for cats as they are the opposite class of dogs in this dataset, and gave overall \"background\" class activation levels to the undefined inputs.\nFig. 8 presents some examples of the CAMs with a selected class of lesion existence, inflammation areas or any signs of diseases (the opposite classes of normal/healthy) based on the five different modalities in medical imaging, using the models described in the Material section. These examples are shown to prove in medical images, that the same problem of inconsistency between class activations and model logits also exists in the original CAMs. In these applications, scalar outputs are more frequently required than in the previous datasets. In the medical domain scalar outputs (logits or confidences) should demonstrate the severity of diseases/symptoms or the chances of a developing disease. With a common intensity scale, medical images with higher model confidences on lesions, inflammation areas or diseases received more and higher class activations than those with lower model confidences."}, {"title": "B. Quantitative evaluation: the common intensity scaling", "content": "Fig. 9 presents some examples of the scatter plots that show the relation between the sum of class activations in CAMs and model logits. Table I provides more details with the correlation coefficients from each dataset, based on the original CAM algorithms and with the proposed common intensity scale. In Fig. 9, most CAMs received high (nearly 100%) correlation coefficients using the common intensity scale (especially for pixel-wise S-Grad CAM and S-XGrad CAM), however, the CAMs based on some class activation mapping algorithms may perform relatively worse than others. These errors originate from the weight definition, for example, for the scatter plots of MNIST-Grad CAM and MNIST-S-Grad CAM, the calculation of the weights of Grad CAM is based on the average of the gradients in the feature, which is not appropriate or MNIST dataset, as in this dataset, the digits appear more frequently in the center of the image."}, {"title": "C. Quantitative evaluation: the selected principal features", "content": "Table. II presents the accuracies from the models using all features and using only the selected principal features (top 5% to 25% based on the feature decomposition). Furthermore, Fig. 10and Table. 3 present the two different ways of evaluating the effectiveness of the selected principal features: the CAMs based only on these selected features could reach (1) the consistency with the logits of the models and (2) the performance in object localization with the models as good as the CAMs using all features. In all these approaches, the effectiveness of the feature decomposition is validated by evaluating the principal features selected based on the importance matrices. Based on the average class activations of each feature in importance matrices, we took the top 5% (top 25% ImageNet and top 20% for MNIST) features in the importance matrices as the principal features and used them to conduct the experiments."}, {"title": "VI. DISCUSSION", "content": "In this paper, we provide a model interpretation method called the integrated feature analysis, based on the same premise as the CAM algorithms. It is not a new class activation mapping algorithm or a new definition of weights for CAMs, but a different perspective of using existing weighted features. We proved the reliability of the method, based on the consistency between CAMs using common intensity scales and models, and the effectiveness of the principal features. The intensity scaling and feature selection could be applied to most CAM methods without increasing computation time during inference, serving as a useful functional supplement. On the other hand, the value of the integrated feature analysis is more than improving the CAM methods. In the following subsections, we discuss about the uses of the importance matrix, generalizability of the proposed method and the definition of importance. Some other topics (e.g. regression tasks, time costs, and evaluation metrics) can be found in supplementary materials."}, {"title": "A. Uses of the importance matrix", "content": "As the effectiveness of the feature decomposition has been proved by the experiments with principal features, the importance matrix indeed contains feature-wise information over the whole dataset and could be applied to the training set or the test set for different purposes. The importance matrix could provide useful information about both the model and dataset, such as:\nOverfitting. The difference in the importance matrices between the training and test sets indicates potential overfitting when the class activations of each feature in the training set differ significantly from those in the test set. For example, in Fig. 11 (a), (b), (d), (e), (f) and (g) we show the importance matrices from both the training and test sets. From the difference between the training and test sets, (a) and (b) indicate more severe overfitting, as they have features that are much more activated in the training sets than in the test sets, while the training and test sets share a very similar pattern in other line graphs.\nPrincipal features. If we select a single column in the importance matrix (thereby selecting a class), the values in each row represent the average class activation level of each feature for that selected class. Those features that are more activated (higher class activations) also have a greater influence on this selected model output class in general. Therefore, they can serve as the main features of this study. For example, in the line graphs in Fig. 11, except for (d) and (h), some features are more frequently activated with significant values, resulting in a large ratio of total activation values to average importance over the entire dataset. The values of these features indicate the potential of some features to emerge from many others.\nPotential confounders. The confounders in deep learning models usually refer to some objects or noise in the background of images or time series that could provide a \"shortcut\" for a model to be easily trained and make inferences based only on these non-targets. In the importance matrices, confounders could cause extremely high values for some specific features and relatively low values for all other features because they provide the \"shortcut\". These potential confounders can therefore be checked by generating CAMs using only the relevant features. As the models are copies of successful models on these public datasets, we did not find any significant signs of confounding in these datasets. Fig. 11 (f) may benefit from some potential confounders compared to other datasets, some visual checks on the CAMs are valuable for further investigation.\nSpecial cases and outliers. Special cases or outliers would show very different class activations of the features compared to the importance matrices of their corresponding classes. For example, in the blue boxes of Fig. 3, we present a simple example from (d) of the ESMIRA project, where the sample belongs to the class \"EAC\", but has a different picture of activated features and was classified as the opposite with confidences around 0.5.\nModel redundancies. The ratio of highly activated features out of all features in the importance matrices represents the ratio of influential features in the model, as the values are a combination of activation frequency and \"amplitude\". Therefore, the rarely activated features in all classes/outputs contribute very little to the model decision in most cases and could be removed by model pruning. For example, in Fig. 11 (b) and (f), compared to other importance matrices, they have more features that are rarely activated across the datasets, while some features are significantly activated. This phenomenon indicates that the models have some \"lazy\" features that contribute very little and can be pruned."}, {"title": "B. Generalizability of the feature analysis", "content": "Although the proposed methods are mostly applied to convolutional neural networks, the importance matrix and the common intensity scaling are not limited to these type of networks. They are also feasible for other deep learning models, because the original CAM algorithms are also feasible for different types of models [43]. We had applied the proposed method to the tiny transformer on MNIST to prove its generalizability. However, the integrated feature analysis faced a transition problem that requires further studies on the definition of features in transformers and recurrent neural networks and the spatial reorganization to generate CAMs in these models."}, {"title": "C. Definition of the importance", "content": "For the importance matrices in this paper, we took the average of the sum of class activations over the whole dataset as the \"importance\". However, the definition of the importance of each feature could be further investigated, as the standard deviation of the average of the weighted features could also be informative. Rarely activated features with massive class activation values in very few cases might obtain high values in the importance matrices based on the current definition. The definition of feature importance remains an open question, just as the definition of the weights for CAMs."}, {"title": "VII. CONCLUSION", "content": "In summary, we proposed the integrated feature analysis for model interpretation by collecting the distribution information and decomposing the weighted features. It helps to improve the consistency between CAMs and models by constructing a common intensity scale and to obtain more information (e.g., main features) about models and datasets by constructing the importance matrix. As the experiments validate its effectiveness, and consider the potential uses of the importance matrix in analyzing models and datasets, integrated feature analysis appears to be a current \"local-optimal\" step towards better model interpretation and more informative class activation maps."}]}