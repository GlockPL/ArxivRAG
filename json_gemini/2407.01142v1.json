{"title": "Integrated feature analysis for deep learning\ninterpretation and class activation maps", "authors": ["Yanli Li", "Tahereh Hassanzadeh", "Denis P. Shamonin", "Monique Reijnierse", "Annette H.M. van der Helm-van Mil", "Berend C. Stoel"], "abstract": "Understanding the decisions of deep learning (DL)\nmodels is essential for the acceptance of DL to risk-sensitive ap-\nplications, such as security systems, industrial anomaly detection\nand medical imaging. Although methods, like class activation\nmaps (CAMs), give a glimpse into the black box, they do miss\nsome crucial information, thereby limiting its interpretability and\nmerely providing the considered locations of objects. To provide\nmore insight into the models and the influence of datasets, we\npropose an integrated feature analysis method, which consists of\nfeature distribution analysis and feature decomposition, to look\ncloser into the intermediate features extracted by DL models.\nThis integrated feature analysis could provide information on\noverfitting, confounders, outliers in datasets, model redundancies\nand principal features extracted by the models, and provide\ndistribution information to form a common intensity scale, which\nare missing in current CAM algorithms. The integrated feature\nanalysis was applied to eight different datasets for general valida-\ntion: photographs of handwritten digits, two datasets of natural\nimages and five medical datasets, including skin photography,\nultrasound, CT, X-rays and MRIs. The method was evaluated\nby calculating the consistency between the CAMs average class\nactivation levels and the logits of the model. Based on the eight\ndatasets, the rescaled CAMs achieved on average increases in\nconsistency of around 23.1%, 51.6%, 15.5%, 22.7%, 32.9%,\n10.7%, 64.2% and 17.1%, respectively. The correlation coeffi-\ncients were all very close to 100%, proving the effectiveness of the\ndistribution analysis in the integrated feature analysis. Moreover,\nbased on the feature decomposition, 5%-25% of features could\ngenerate equally informative saliency maps and obtain the same\nmodel performances as using all features. This proves the\nreliability of the feature decomposition. As the proposed methods\nrely on very few assumptions, this is a step towards better model\ninterpretation and a useful extension to existing CAM algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "PROVIDING insight into how deep learning (DL) models\nmake decisions is a prerequisite before DL models can\nbe applied in risk-sensitive areas. The potential risk of making\ndecisions based on confounders is a non-trivial issue in risk-\naverse domains like healthcare [1], security systems [2] and\nother practices [3]. An intuitive idea to validate the reliability\nof deep learning models is to confirm that models are mak-\ning decisions based on reasonable evidence and convincing\nfeatures. Since there is no formal definition of interpreting\nDL models, current interpretation algorithms rely on some\ncommonly agreed principles [4]. For instance, a dog should be\nclassified based on the dog's face instead of the background or\nother objects in the image, or a bird should be distinguished\nfrom other animals because of its wings. Three principles\nare mainly followed in current studies: (1) analyzing the\nresponses of or changes in the model's output to investigate the\nmodel, by assessing the individual contributions of samples in\ndatasets [5], estimating the learning difficulty [6] or detecting\nmisclassified samples [7]; (2) analyzing low-level features\nextracted by the models to investigate the model's feature\nextraction process [8]-[10]; and (3) analyzing the contribution\nof certain parts in the input image to the model's output,\nevaluating whether the contribution of these parts is consistent\nwith human knowledge. Compared to some early efforts based\non principles (1) and (2), methods based on principle (3) are\nmore intuitive and easier for visual checks and understanding.\nAs a typical representative of methods based on principle (3),\nclass activation mapping and its variants, which generate class\nactivation maps (CAMs), demonstrate high computational\nefficiency compared to other widely-used methods, such as\nLIME and its variants [11]\u2013[14], global interpretation [15]-\n[17] and other interpretation methods [18]-[23], becoming\none of the most popular methods for investigating model\nreliability and interpretability. CAMs (also known as saliency\nmaps, heatmaps or attention maps) provide information on the\nlocations of objects considered by the given models, based on\nthe assumption that weighted features could precisely describe\nthe model outputs. The weighted features in CAM algorithms\nusually refer to the summed features from a certain layer of the\nmodels, weighted by the designed weights that are assumed\nto represent the \"importance\" of these features for the final\noutput class. These algorithms gave promising performances on\ncomplicated tasks by locating the target objects in the images\nwith complex backgrounds and foregrounds [32] and devel-\noped the field of weakly supervised object detection [33].\nAlthough these algorithms provide important insight, some\nimportant information is lost in the calculation process as"}, {"title": "II. METHOD", "content": "The integrated feature analysis consists of two parts: (1) distri-\nbution analysis and (2) feature decomposition. The distribution\nanalysis aims at determining the spread, standard deviation and\npercentiles of the class activation levels of weighted features,\nin order to compare these levels from one sample to another\nor to the average level over the whole dataset. The feature\ndecomposition focuses on the value of each feature through\na so-called \"importance matrix\u201d. The collected information enables a common intensity scale for the\nCAM generation, which could improve\nthe consistency between the CAMs and the models.\nThe CAM calculation with a common intensity scale can be\ndescribed as: instead of scaling the products of weights and\nfeatures based on the maximum and minimum of only the\ncurrent input, the whole dataset is used to scale the weights and\nfeatures (training set if time-available, validation set if time-\nlimited) obtained during the distribution analysis a common\nintensity scaling compared to the individual intensity scaling\nin current methods. The revised saliency map generation is\ngiven by:\n$S-CAM = \u03c3_{op10}(scale_{space}(weights\u00b7 features))$\nwhere $weights$ represents the weights designed by the CAM\nalgorithms, $features$ refers to the chosen features, $scalespace$\nis the spatial resizing function, and $P90/P10$ are the upper\nand lower limit for normalization. In this work, the $\u03c3_{p10}(\u00b7)$ is\ndesigned to be:\n$\u03c3_{p10}(x) = tanh(\u03b1x + \u03b2)$\nwhere $\u03b1$ and $\u03b2$ are constants given by $\\alpha = \\frac{tanh^{-1}0.9 - tanh^{-1}0.1}{P90-P10}$ and $\\beta = \\frac{P90tanh^{-1}0.9-P10tanh^{-1}0.1}{P90-P10}$. This scaling function with\na hyperbolic tangent function is to prevent information loss\noutside the interval P10-P90, as would be the case in classical\nmaximum-minimum normalization with clipping. At the same\ntime, the central curves and the domain of definition of this\nfunction also enables the common intensity scale to amplify\nthe differences in smaller class activation levels and adapt to\nexternal data input for more general uses. Since the proposed\nintensity scaling and preservation of negative values can be\nconsidered a functional extension to existing methods, we\npreferred not to call it as a new CAM method, but rather\nan intensity-scaled version of these methods, labeled with \u201cS-\". For example, for Grad CAM, after the proposed common\nintensity scaling, we call this \"S-Grad CAM\".\nThe feature decomposition aims at determining the indepen-\ndence and importance of each feature, without considering\nthe spatial information and the correlation with other fea-\ntures. We define the importance of a feature by the average\ncontribution of this feature (measured through the sum of a\nweighted feature) to the model logits of the selected class\nover the whole dataset, as each feature represents a fixed\nfeature extraction path after training the model (See The\nrevised CAM generation is given by:\n$FS \u2013 CAM = \u03c3(scalespace(im \u00b7 weights\u00b7 features))$\nwhere $im$ represents a mask based on the importance matrix,\ngenerated by the importance matrices being thresholded ac-\ncording to a specific purpose, while others remain the same.\nFor example, when the importance matrices are thresholded to\nget the principal features, confounders or a single feature, a\nCAM based only on these selected features could be generated.\nSimilarly to commonly scaled CAMs (indicated by prefix \u2018S-'),\nwe indicate these \"Feature Selected\" CAMs computed from\na mask by a prefix \"FS-\"For example, for S-Grad CAM, after\nthe proposed masking, we call it \"FS-S-Grad CAM\". The\nperformance of the FS-CAMs compared to the original CAMs\ndemonstrates the contribution of the selected features out of all\nfeatures, and therefore the FS-CAMs for the principal features\nare used for the validation of the feature decomposition based\non the importance matrix."}, {"title": "III. EVALUATION", "content": "Since the proposed integrated feature analysis could contribute\nto improving the CAMs, we evaluated the proposed method by"}, {"title": "IV. MATERIALS", "content": "Eight datasets and eight models from different research fields\nare used for experiments, together with eight different CAM\nalgorithms applied for comparison and backbones of the\nintegrated feature analysis."}, {"title": "A. Datasets and models", "content": "The datasets and corresponding models used for method\nvalidation include:\n\u2022 ILSVRC2012 [35] for 1000-class natural objects, using\nstandard ResNet18/34/50 and VGG11/13/16 as models,\nwith top-1 accuracies ranging from 70% to 83%;\n\u2022 Cats & Dogs [36] for classifying cats and dogs, using\nstandard ResNet18/34/50 and VGG11/13/16 as the mod-.\nels, achieving accuracies around 99.5% (ResNet34) and\n98.6% (VGG16) on 10000 test images;\n\u2022 MNIST [37] for classifying ten-class digits, using a\nmodel consisting of a simple two-layer multi-head self-\nattention block (from Transformers) with a multi-layer\nperceptron (fully connected layers), achieving an accu-\nracy of over 99.9% on test digits;\n\u2022 MRI (T1-weighted contrast-enhanced with fat suppres-\nsion) from the ESMIRA project [38] for classification\nof rheumatoid arthritis (RA). This (non-public) dataset,"}, {"title": "B. CAMs involved", "content": "The CAM algorithms involved in the experiment are Grad\nCAM [24], Grad CAM++ [25], xGrad CAM [27], Score CAM\n[28], Smooth Grad CAM++ [29], SS-CAM [30], IS-CAM [31]\nand pixel-wise Grad CAM (pixel-wise gradients with features).\nSince the branch of perturbation-based weights (Score CAM,\nSS-CAM, etc.) is unacceptably time-consuming for a broad\nquantitative evaluation, and because the purpose of this work\nis not to compare different CAM algorithms, the quantitative\nevaluation focuses on Grad CAM, Grad CAM++, xGrad CAM\nand pixel-wise Grad CAM. For the branch of perturbation-\nbased weights, such as Score CAM, we present some visual\nresults instead."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "This section contains three subsections, including (1) visual\nexamples of the S-CAMs and FS-S-CAMs compared to some\nexisting CAM algorithms; (2) the quantitative evaluation of the\nconsistency improvement with the common intensity scaling\nbased on the distribution analysis; and (3) the quantitative\nevaluation of the principal features selected by the importance\nmatrix."}, {"title": "A. Visual checks of the S-CAMs and FS-S-CAMS", "content": "Fig. 6 to 8 briefly show some examples based on existing\nclass activation mapping algorithms compared to the CAMs\nwith the common intensity scaling based on the P90 and\nP10 from the distribution analysis. In the visual checks,\nwe present the S-Grad CAM and FS-S-Grad CAM, which\nis based on the most basic CAM algorithm, to show the\nimportance and effectiveness of the common intensity scaling\nand the results that it could exceed other CAM algorithms on\nmatching the model confidences."}, {"title": "B. Quantitative evaluation: the common intensity scaling", "content": "Fig. 9 presents some examples of the scatter plots that show\nthe relation between the sum of class activations in CAMs and\nmodel logits. Table I provides more details with the correlation\ncoefficients from each dataset, based on the original CAM\nalgorithms and with the proposed common intensity scale.\nIn Fig. 9, most CAMs received high (nearly 100%) correlation\ncoefficients using the common intensity scale (especially for\npixel-wise S-Grad CAM and S-XGrad CAM), however, the\nCAMs based on some class activation mapping algorithms\nmay perform relatively worse than others. These errors orig-\ninate from the weight definition, for example, for the scatter\nplots of MNIST-Grad CAM and MNIST-S-Grad CAM, the\ncalculation of the weights of Grad CAM is based on the\naverage of the gradients in the feature, which is not appropriate\nor MNIST dataset, as in this dataset, the digits appear more\nfrequently in the center of the image."}, {"title": "C. Quantitative evaluation: the selected principal features", "content": "presents the accuracies from the models using all\nfeatures and using only the selected principal features (top\n5% to 25% based on the feature decomposition)."}, {"title": "VI. DISCUSSION", "content": "In this paper, we provide a model interpretation method called\nthe integrated feature analysis, based on the same premise as\nthe CAM algorithms. It is not a new class activation mapping\nalgorithm or a new definition of weights for CAMs, but a\ndifferent perspective of using existing weighted features. We\nproved the reliability of the method, based on the consistency\nbetween CAMs using common intensity scales and models,\nand the effectiveness of the principal features. The inten-\nsity scaling and feature selection could be applied to most\nCAM methods without increasing computation time during\ninference, serving as a useful functional supplement. On the\nother hand, the value of the integrated feature analysis is more\nthan improving the CAM methods. In the following subsec-\ntions, we discuss about the uses of the importance matrix,\ngeneralizability of the proposed method and the definition\nof importance. Some other topics (e.g. regression tasks, time\ncosts, and evaluation metrics) can be found in supplementary\nmaterials."}, {"title": "A. Uses of the importance matrix", "content": "As the effectiveness of the feature decomposition has been\nproved by the experiments with principal features, the impor-\ntance matrix indeed contains feature-wise information over the\nwhole dataset and could be applied to the training set or the\ntest set for different purposes. The importance matrix could\nprovide useful information about both the model and dataset,\nsuch as:\n\u2022 Overfitting. The difference in the importance matrices\nbetween the training and test sets indicates potential\noverfitting when the class activations of each feature in\nthe training set differ significantly from those in the test\nset. Potential confounders. The confounders in deep learning\nmodels usually refer to some objects or noise in the\nbackground of images or time series that could provide\na \"shortcut\" for a model to be easily trained and make\ninferences based only on these non-targets. The im-\nportance matrices, confounders could cause extremely\nhigh values for some specific features and relatively low\nvalues for all other features because they provide the\n\"shortcut\". These potential confounders can therefore be\nchecked by generating CAMs using only the relevant\nfeatures. Model redundancies. The ratio of highly activated fea-\ntures out of all features in the importance matrices\nrepresents the ratio of influential features in the model, as\nthe values are a combination of activation frequency and\n\"amplitude\". Therefore, the rarely activated features in all\nclasses/outputs contribute very little to the model decision\nin most cases and could be removed by model pruning."}, {"title": "B. Generalizability of the feature analysis", "content": "Although the proposed methods are mostly applied to convolu-\ntional neural networks, the importance matrix and the common\nintensity scaling are not limited to these type of networks.\nThey are also feasible for other deep learning models, because\nthe original CAM algorithms are also feasible for different\ntypes of models [43]. We had applied the proposed method to\nthe tiny transformer on MNIST to prove its generalizability.\nHowever, the integrated feature analysis faced a transition\nproblem that requires further studies on the definition of\nfeatures in transformers and recurrent neural networks and the\nspatial reorganization to generate CAMs in these models."}, {"title": "C. Definition of the importance", "content": "For the importance matrices in this paper, we took the average\nof the sum of class activations over the whole dataset as the\n\"importance\". However, the definition of the importance of\neach feature could be further investigated, as the standard\ndeviation of the average of the weighted features could also\nbe informative. Rarely activated features with massive class\nactivation values in very few cases might obtain high values\nin the importance matrices based on the current definition. The"}, {"title": "VII. CONCLUSION", "content": "In summary, we proposed the integrated feature analysis for\nmodel interpretation by collecting the distribution information\nand decomposing the weighted features. It helps to improve\nthe consistency between CAMs and models by constructing a\ncommon intensity scale and to obtain more information (e.g.,\nmain features) about models and datasets by constructing the\nimportance matrix. As the experiments validate its effective-.\nness, and consider the potential uses of the importance matrix\nin analyzing models and datasets, integrated feature analysis\nappears to be a current \"local-optimal\" step towards better\nmodel interpretation and more informative class activation\nmaps."}]}