{"title": "TRANSFORMERS HANDLE ENDOGENEITY IN IN-CONTEXT\nLINEAR REGRESSION", "authors": ["Haodong Liang", "Krishnakumar Balasubramanian", "Lifeng Lai"], "abstract": "We explore the capability of transformers to address endogeneity in in-context linear regression. Our\nmain finding is that transformers inherently possess a mechanism to handle endogeneity effectively\nusing instrumental variables (IV). First, we demonstrate that the transformer architecture can emulate\na gradient-based bi-level optimization procedure that converges to the widely used two-stage least\nsquares (2SLS) solution at an exponential rate. Next, we propose an in-context pretraining scheme\nand provide theoretical guarantees showing that the global minimizer of the pre-training loss achieves\na small excess loss. Our extensive experiments validate these theoretical findings, showing that the\ntrained transformer provides more robust and reliable in-context predictions and coefficient estimates\nthan the 2SLS method, in the presence of endogeneity.", "sections": [{"title": "Introduction", "content": "The transformer architecture [Vaswani et al., 2017] has demonstrated remarkable in-context learning (ICL) capabilities\nacross various domains, such as natural language processing [Devlin et al., 2019, Radford et al., 2019, Brown et al.,\n2020], computer vision [Dosovitskiy et al., 2021, Carion et al., 2020], and reinforcement learning [Lee et al., 2022,\nParisotto et al., 2019]. Self-attention mechanism, a core component of transformers, allows these models to capture\nlong-range dependencies in data, which is critical for success in these tasks. Despite their impressive performance, the\ntheoretical understanding of transformers remains limited, leaving important questions unanswered about their true\ncapabilities and the underlying mechanisms driving their exceptional results.\nRecent efforts to theoretically understand transformers' ICL capabilities have focused on their performance in fundamen-\ntal statistical tasks. Focusing on simple function classes, Garg et al. [2022] highlighted that transformers, when trained\non sufficiently large and diverse data from a specific function class, can generalize across most functions of that class\nwithout task-specific fine-tuning. Building on this, subsequent work by Bai et al. [2024] established that attention layers\nenable transformers to perform gradient descent, implementing algorithms like linear regression, logistic regression,\nand LASSO; see also Aky\u00fcrek et al. [2023], Von Oswald et al. [2023], Li et al. [2023], Fu et al. [2023], Ahn et al.\n[2024]. Furthermore Zhang et al. [2024a,b] showed that trained transformers' ICL abilities for linear regression tasks\nare theoretically robust under certain distributional shifts and characterized the corresponding sample complexities.\nExisting works on analyzing the ICL ability of transformers for linear regression tasks, however, ignore endogeneity\nand have mainly focused on the exogenous setup where the additive noise is uncorrelated with the explanatory variables.\nIgnoring endogeneity in linear regression leads to biased and inconsistent estimates, resulting from issues like omitted\nvariable bias, simultaneity, and measurement error, which can distort causal inferences and lead to incorrect policy\nconclusions [Hausman, 2001, Wooldridge, 2015, Angrist and Pischke, 2009, Greene, 2018]. Instrumental variable\n(IV) regression is a widely adopted method to handle endogeneity by utilizing instruments that are correlated with the\nendogenous variables but uncorrelated with the error term [Angrist and Krueger, 2001]. A naturally intriguing question\nthat therefore arises is:\nCan transformers leverage instrumental variables and provide reliable predictions\nand coefficient estimates, in the presence of endogeneity?"}, {"title": "Related works", "content": "In-context Learning. Initial works by Garg et al. [2022] and Bai et al. [2024] adopted the standard multi-layer\ntransformer architecture to conduct the experiments. Later, Giannou et al. [2023] and Yang et al. [2023] showed that a\nlooped architecture reduces the required depth of transformers and exhibits better efficiency in learning algorithms. Gao\net al. [2024] illustrated that the looped transformer architecture with extra pre-processing and post-processing layers\ncan achieve higher expressive power than a standard transformer with the same number of parameters. Apart from\nworks concerning the implementability of first-order gradient descent algorithms by transformers, other works have\nalso examined higher-order and non-parametric optimization methods. Specifically, Giannou et al. [2024] showed that\ntransformers can emulate Newton's method for logistic regression. Cheng et al. [2024] showed that transformers can\nimplement functional gradient descent and hence enable them to learn non-linear functions in-context. Relationship\nbetween in-context learning and Bayesian inference is also studied in Ye et al. [2024], Falck et al. [2024].\nNichani et al. [2024] illustrated how the transformers can learn the causal structure by encoding the latent causal graph\nin the first attention layer. Goel and Bartlett [2024] explored the representational power of transformer for learning linear\ndynamical systems. Makkuva et al. [2024a,b], Rajaraman et al. [2024], Edelman et al. [2024] considered ICL Markov\nchains with transformers, including both landscape and training dynamics analyses. To the best of our knowledge, we\nare not aware of prior works on handling endogeniety with transformers.\nInstrumental Variable Regression. IV regression has been widely studied in econometrics [Angrist and Krueger,\n2001, Angrist and Pischke, 2009]. Recent works in machine learning explored the optimization based approaches for\nthe IV regression problem. Singh et al. [2019] proposed the kernel IV regression to model non-linear relationship\nbetween variables. Muandet et al. [2020] proposed that a non-linear IV regression problem can be formulated as\na convex-concave saddle point problem. Della Vecchia and Basu [2023], Chen et al. [2024], Peixoto et al. [2024]\nproposed a stochastic optimization algorithm for IV regression.\nNotation: Throughout this paper, unless otherwise specified, lower-case letters denote random variables or samples,\nwhile upper-case letters represent datasets (collections of samples). Bolded letters indicate vectors or matrices, whereas\nunbolded letters indicate scalars. The notation $X_{:,i}$ refers to the i-th column, and $X_{i,:}$ refers to the i-th row of matrix\nX. By default, $\\lVert \\cdot \\rVert$ denotes the Euclidean norm for a vector, or the spectral norm for a matrix."}, {"title": "Endogeneity and Instrumental Variable Regression", "content": "Suppose we are interested in estimating the relationship between response variable $y \\in \\mathbb{R}$ and predictor variable $x \\in \\mathbb{R}^p$\nwith endogeneity. Given instruments $z \\in \\mathbb{R}^q$, we consider the model\n$$y = x^T\\beta + \\epsilon_1, \\text{ and } x = \\Theta z + \\epsilon_2,$$\nwhere $\\beta\\in \\mathbb{R}^p$, and $\\Theta \\in \\mathbb{R}^{q \\times p}$ are the true model parameters, $\\epsilon_1 \\in \\mathbb{R}$ and $\\epsilon_2 \\in \\mathbb{R}^p$ are (centered) random noise terms\nwith variance $\\sigma_1^2$ and covariance matrix $\\Sigma_2$, respectively. Further, $\\epsilon_2$ is an unobserved noise correlated with $\\epsilon_1$, leading\nto the correlation between $x$ and $\\epsilon_1$, which introduces confounding in the model between $x$ and $y$. Under this setting,\nthe standard ordinary least squares (OLS) estimator is a biased and inconsistent estimator of $\\beta$ (see Wooldridge [2015],\nChapter 9). To address this issue, instrumental variable (IV) regression is a widely used method to provide a consistent\nestimate for $\\beta$."}, {"title": "Related works", "content": "In-context Learning. Initial works by Garg et al. [2022] and Bai et al. [2024] adopted the standard multi-layer\ntransformer architecture to conduct the experiments. Later, Giannou et al. [2023] and Yang et al. [2023] showed that a\nlooped architecture reduces the required depth of transformers and exhibits better efficiency in learning algorithms. Gao\net al. [2024] illustrated that the looped transformer architecture with extra pre-processing and post-processing layers\ncan achieve higher expressive power than a standard transformer with the same number of parameters. Apart from\nworks concerning the implementability of first-order gradient descent algorithms by transformers, other works have\nalso examined higher-order and non-parametric optimization methods. Specifically, Giannou et al. [2024] showed that\ntransformers can emulate Newton's method for logistic regression. Cheng et al. [2024] showed that transformers can\nimplement functional gradient descent and hence enable them to learn non-linear functions in-context. Relationship\nbetween in-context learning and Bayesian inference is also studied in Ye et al. [2024], Falck et al. [2024].\nNichani et al. [2024] illustrated how the transformers can learn the causal structure by encoding the latent causal graph\nin the first attention layer. Goel and Bartlett [2024] explored the representational power of transformer for learning linear\ndynamical systems. Makkuva et al. [2024a,b], Rajaraman et al. [2024], Edelman et al. [2024] considered ICL Markov\nchains with transformers, including both landscape and training dynamics analyses. To the best of our knowledge, we\nare not aware of prior works on handling endogeniety with transformers.\nInstrumental Variable Regression. IV regression has been widely studied in econometrics [Angrist and Krueger,\n2001, Angrist and Pischke, 2009]. Recent works in machine learning explored the optimization based approaches for\nthe IV regression problem. Singh et al. [2019] proposed the kernel IV regression to model non-linear relationship\nbetween variables. Muandet et al. [2020] proposed that a non-linear IV regression problem can be formulated as\na convex-concave saddle point problem. Della Vecchia and Basu [2023], Chen et al. [2024], Peixoto et al. [2024]\nproposed a stochastic optimization algorithm for IV regression.\nNotation: Throughout this paper, unless otherwise specified, lower-case letters denote random variables or samples,\nwhile upper-case letters represent datasets (collections of samples). Bolded letters indicate vectors or matrices, whereas\nunbolded letters indicate scalars. The notation $X_{:,i}$ refers to the i-th column, and $X_{i,:}$ refers to the i-th row of matrix\nX. By default, $\\lVert \\cdot \\rVert$ denotes the Euclidean norm for a vector, or the spectral norm for a matrix."}, {"title": "Endogeneity and Instrumental Variable Regression", "content": "Suppose we are interested in estimating the relationship between response variable $y \\in \\mathbb{R}$ and predictor variable $x \\in \\mathbb{R}^p$\nwith endogeneity. Given instruments $z \\in \\mathbb{R}^q$, we consider the model\n$$y = x^T\\beta + \\epsilon_1, \\text{ and } x = \\Theta z + \\epsilon_2,$$$$\\hat{\\Theta} = (Z^TZ)^{-1}Z^TX.$$\nii. Second stage: Regress $Y$ on $Z\\hat{\\Theta}$ to obtain:\n$$\\hat{\\beta}_{2SLS} = (\\hat{\\Theta}^TZ^TZ\\hat{\\Theta})^{-1}\\hat{\\Theta}^TZ^TY.$$\nWe introduce the standard assumptions required to show the convergence rate of the above estimator.\nAssumption 1 (Instrumental variable). A random variable $z \\in \\mathbb{R}^q$ is a valid IV, if it satisfies the following conditions:\ni. Fully identification: $q > p$ (without loss of generality, we assume data $Z, X$ are full rank).\nii. Correlated to $x$: $\\text{Corr}(z, x) \\neq 0$.\niii. Conditional uncorrelated to $y$: $\\text{Corr}(z, \\epsilon_1) = 0$.\nIn particular, condition (i) above ensures the existence of unique solution for $\\hat{\\beta}_{2SLS}$. We refer to Stock and Watson\n2011, Chapter 12] for additional elaborate discussions on the above conditions. To derive non-asymptotic convergence\nrates, we further assume the following regularity conditions.\nAssumption 2 (Regularity conditions). We assume the following conditions hold:\ni. Bounded parameters: $\\lVert \\beta \\rVert \\le B_{\\beta}$, $\\lVert \\Theta \\rVert \\le B_{\\Theta}$.\nii. Population condition: $\\lVert z \\rVert \\le B_z$, $\\lVert x \\rVert \\le B_x$, $|\\epsilon_1| \\le B_{\\epsilon_1}$, almost surely.\niii. Sample condition: There exists constants $\\lambda_z$, $\\lambda_{\\tilde{x}}$, $\\delta_{z\\epsilon_2} > 0$, such that the following event holds with probability\nat least $1-\\xi$:\n$$A = \\left\\{ \\lambda_{\\min}\\left(\\frac{Z^TZ}{n}\\right) > \\lambda_z, \\lambda_{\\min}\\left(\\frac{X^T P_z X}{n}\\right) > \\lambda_{\\tilde{x}}, \\frac{\\lVert Z^T\\epsilon_2 \\rVert}{n} < \\delta_{z\\epsilon_2} \\right\\},$$where $\\lambda_{\\min}(\\cdot)$ denotes the smallest eigenvalue of a matrix, $P_z := Z(Z^TZ)^{-1}Z^T$ denotes the projection\nmatrix.\niv. Without loss of generality, we assume Assumption 1 holds under event $A$, and $\\mathbb{E}[\\epsilon_1|A] = O(\\sigma)$.\nThe almost sure boundedness condition in (ii) is required to invoke matrix Bernstein inequalities [Tropp, 2015] in\nthe analysis. We anticipate that this condition may be relaxed to sub-Gaussian or moment conditions by using more\nsophisticated matrix concentration results. We emphasize that condition (iii) is a mild condition in general, given some\ncommon assumptions on the population distributions. See Appendix C.1 for related analysis and empirical justifications.\nNow we establish the consistency of the 2SLS estimator under random design.\nTheorem 2.1 (Non-asymptotic error bound of 2SLS estimator). Given Assumptions 1 and 2, for any $\\varepsilon > 0$, we have:\n$$\\mathbb{P}(\\lVert \\hat{\\beta}_{2SLS} - \\beta \\rVert \\ge \\varepsilon \\mid A) \\le (q + 1) \\exp\\left(-\\frac{\\left(\\frac{\\varepsilon}{\\theta B_{\\Theta} + \\frac{\\varepsilon}{2}}\\right)^2}{\\frac{\\lambda_z^2}{\\nu(\\Omega_{Z\\epsilon_1}|A)} + \\frac{\\lambda_{\\tilde{x}}}{\\left(B_{\\Theta} + \\frac{\\varepsilon}{2}\\right)^2}B_{\\beta}^2} \\right),$$where $\\Omega_{Z\\epsilon_1} := \\sum_{i=1}^n Z_i\\epsilon_{1,i}$, and $\\nu(\\cdot)$ is the matrix variance statistic:\n$$\\nu(\\Omega) := \\max\\{\\lVert \\mathbb{E}(\\Omega)\\rVert, \\lVert \\mathbb{E}(\\Omega^T\\Omega) \\rVert\\}.$$Furthermore, consider clipping operation $\\text{clip}_{B_{\\beta}}(\\cdot) := \\begin{cases}\n    \\beta & \\text{if } \\lVert \\beta \\rVert \\le B_{\\beta} \\\\\n    \\frac{\\beta}{\\lVert \\beta \\rVert} B_{\\beta} & \\text{otherwise} \\\\\n\\end{cases}$, then the expectation of the estimation\nerror is bounded by:\n$$\\mathbb{E}\\left[\\lVert \\text{clip}_{B_{\\beta}}(\\hat{\\beta}_{2SLS}) - \\beta \\rVert^2 \\right] \\le O\\left(\\frac{C_{z,x}^2}{\\lambda_{\\tilde{x}}}\\right)^2 \\frac{\\xi B_{\\beta}^2}{n},$$where $C_{z,x} := \\frac{\\left(B_{\\Theta} + \\frac{\\varepsilon}{2}\\right)B_z}{\\lambda_z}$ is a constant depending on the population distributions of $z$ and $x$."}, {"title": "Transformers Handle Endogeniety", "content": "We note that although the consistency of the 2SLS estimator is a standard result in econometrics, most existing works\nfocus on the asymptotic properties of the estimator. Theorem 2.1 provides the first non-asymptotic bound for estimation\nerror $\\lVert \\hat{\\beta}_{2SLS} - \\beta \\rVert_2^2$, under random design. The detailed proof is provided in Appendix A.1.\n3 Transformers Handle Endogeniety\n3.1 Transformer Architecture\nDenote the input matrix as $H = [h_1,..., h_n] \\in \\mathbb{R}^{D \\times n}$, where each column corresponds to one sample vector.\nDefinition 3.1 (Attention layer). A self-attention layer with $M$ heads is denoted as $\\text{ATTN}_\\theta(\\cdot)$, with parameters\n$\\theta = \\{(Q_m, K_m, V_m)\\}_{m \\in [M]} \\subseteq \\mathbb{R}^{D \\times D}$. Given input $H$,\n$$H = \\text{ATTN}_\\theta(H) := H + \\frac{1}{M} \\sum_{m=1}^M (V_mH) \\times \\sigma((Q_mH)^T (K_mH)) \\in \\mathbb{R}^{D \\times n},$$or element-wise:\n$$h_i = \\left[\\text{ATTN}_\\theta(H)\\right]_i := h_i + \\sum_{m=1}^M \\frac{1}{n} \\sum_{j=1}^n \\sigma(\\langle Q_mh_i, K_mh_j \\rangle) \\cdot V_mh_j \\in \\mathbb{R}^{D},$$where $\\sigma(\\cdot)$ is the ReLU function.\nDefinition 3.2 (MLP layer). An MLP layer is denoted as $\\text{MLP}_\\theta(\\cdot)$, with parameters $\\theta = (W_1, W_2) \\in \\mathbb{R}^{D' \\times D \\times D \\times D'}$.\nGiven input $H$,\n$$H = \\text{MLP}_\\theta(H) := H + W_2\\sigma(W_1H),$$or element-wise:\n$$h_i = \\left[\\text{MLP}_\\theta(H)\\right]_i := h_i + W_2\\sigma(W_1h_i).$$Definition 3.3 (Transformer). An $L$-layer transformer is denoted as $\\text{TF}_\\theta(\\cdot)$, with parameters $\\theta = (\\theta_{\\text{ATTN}}^{(1:L)}, \\theta_{\\text{MLP}}^{(1:L)})$.\nGiven input $H = H^{(0)}$,\n$$H^{(l)} = \\text{MLP}_{\\theta_{\\text{MLP}}^{(l)}}\\left(\\text{ATTN}_{\\theta_{\\text{ATTN}}^{(l)}}(H^{(l-1)})\\right), l = 1,..., L.$$The output of this transformer is the final layer output: $H := H^{(L)} = \\text{TF}_\\theta(H^{(0)})$.\nDefinition 3.4 (Looped transformer). An $\\overline{L}$-looped transformer is a special transformer architecture, denoted as\n$\\text{LTF}_{\\overline{\\theta}, l}(\\cdot)$, with parameters $\\overline{\\theta} = (\\theta_{\\text{ATTN}}^{(1:\\overline{L})}, \\theta_{\\text{MLP}}^{(1:\\overline{L})})$. Given input $H = H^{(0)}$,\n$$H^{(l)} = \\text{TF}_\\theta(H^{(l-1)}), l = 1,..., \\overline{L}.$$The output of this looped transformer is the final loop output: $H := H^{(\\overline{L})} = \\text{LTF}_{\\overline{\\theta}, l}(H^{(0)})$.\nPrevious works (e.g., Bai et al. [2024], Zhang et al. [2024a]) have shown that transformers can perform in-context\nlinear regression by emulating gradient descent (GD) with in-context pretraining. However, these studies have two key\nlimitations. First, their analysis is based on single-level optimization algorithms, which is insufficient to demonstrate\nthat transformers can efficiently learn more complex algorithms like 2SLS (Definition 2.1). Second, most ICL-related\nresearch focuses on the predictive performance of transformers, paying little attention to their ability to provide accurate\ncoefficient estimates. We extend the current ICL framework by showing that transformers can implement a bi-level GD\nprocedure (see Section 3.2) with looped transformer architecture (Definition 3.4), allowing them to efficiently emulate\n2SLS and provide coefficient estimates that are at least as accurate as 2SLS in the presence of endogeneity (as in (1))."}, {"title": "Gradient descent based IV regression", "content": "We first introduce a gradient-based bi-level optimization procedure to obtain the 2SLS estimator in (2). Given the\ndataset $(Z, X, Y) = \\{(Z_i, X_i, Y_i)\\}_{i=1}^n$, the objective funtion of IV regression can be formulated as the following\nbi-level optimization problem:\n$$\\min_{\\beta} \\mathcal{L}(\\beta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^T \\beta)^2, \\text{ where } \\Theta := \\arg \\min_{\\Theta} \\frac{1}{n} \\sum_{j=1}^n (x_j - z_j^T \\Theta)^2.$$"}, {"title": "Transformers Can Efficiently Implement GD-2SLS", "content": "Consider the following gradient updates with learning rates $\\alpha, \\eta$:\n$$\\Theta^{(t+1)} = \\Theta^{(t)} - \\eta \\frac{1}{n} Z^T (Z\\Theta^{(t)} - X),$$$$\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\Theta^{(t) T} \\frac{1}{n} Z^T (Z\\Theta^{(t)} \\beta^{(t)} - Y).$$Note that the GD-2SLS updates in (8) are designed to solve (7). We now show that regardless the convergence of $\\Theta^{(t)}$,\nthe GD estimator $\\beta^{(t)}$ will always converge to the 2SLS estimator in (2) with exponential rate.\nTheorem 3.1 (Implementing 2SLS with gradient-based method). Given training data $(Z, X, Y) = \\{(Z_i, X_i, Y_i)\\}_{i=1}^n$.\nSuppose the learning rates $\\alpha, \\eta$ satisfy the following conditions:\n$$0 < \\alpha < \\frac{2}{\\sigma_{\\max}^2(Z \\Theta)} \\text{ and } 0 < \\eta < \\frac{2}{\\sigma_{\\max}^2(Z)},$$where $\\sigma_{\\max}(\\cdot)$ denotes the largest singular value of a matrix. Then, the GD updates in (8) converge to the 2SLS\nestimator at an exponential rate:\n$$\\lVert \\beta^{(t)} - \\beta_{2SLS} \\rVert \\le O(\\Lambda^t),$$where, with $\\rho(\\cdot)$ denoting the spectral radius of the matrix,\n$$\\Lambda := \\max\\{\\gamma(\\alpha), \\kappa(\\eta)\\}, \\quad \\gamma(\\alpha) := \\rho(I - \\alpha \\Theta^T Z^T Z \\Theta), \\quad \\kappa(\\eta) := \\rho(I - \\eta Z^T Z).$$To the best of our knowledge, Theorem 3.1 provides the first theoretical result demonstrating that 2SLS can be\nefficiently implemented using a gradient-based method, with an exponential convergence rate. We provide the proof in\nAppendix B.1 and present simulation results in Appendix C.2 to examine the convergence behavior of the optimization\nprocess."}, {"title": "Transformers Can Efficiently Implement GD-2SLS", "content": "The looped transformer architecture (Definition 3.4), as proposed by Giannou et al. [2023], introduces an efficient\napproach to learn iterative algorithms by cascading the same transformer block for multiple times. With the GD updates\nin (8), we will show that there exists a looped transformer architecuture that can efficiently learn the 2SLS estimator.\nWe emphasize here that although we can implement 2SLS by sequentially attaching two separate GD iterates (each\nhandling OLS for one stage), the overall convergence depends heavily on the convergence of the first stage estimate $\\Theta$.\nHence, significantly more number of layers are needed to ensure convergence. In addition, the advantage of looped\ntransformer architecture cannot be fully exploited with this approach.\nTheorem 3.2 (Implement a step of GD-2SLS with a transformer block). Suppose the embedded input matrix takes the\nform:\n$$H^{(2l)} = \\begin{bmatrix}\nz_1 & ... & z_n & z_{n+1} \\\\\nx_1 & ... & x_n & x_{n+1} \\\\\ny_1 & ... & y_n & 0 \\\\\n\\Theta^{(1)}_{:,1} & ... & \\Theta^{(1)}_{:,1} & \\Theta^{(1)}_{:,1} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\beta^{(1)}_{x_1} & ... & \\beta^{(1)}_{x_n} & \\beta^{(1)}_{x_{n+1}} \\\\\n1 & ... & 1 & 1 \\\\\n1 & ... & 1 & 1 \\\\\n1 & ... & 1 & 1 \\\\\n1 & ... & 1 & 1 \\\\\n1 & ... & 1 & 0 \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{D \\times (n+1)}.$$\nGiven $H^{(2l)}$, there exists a double-layer attention-only transformer block with parameters $\\theta = \\theta^{(2l+1:2l+2)} = \\{(Q_m^{(2l+1:2l+2)}, K_m^{(2l+1:2l+2)}, V_m^{(2l+1:2l+2)})\\}_{m \\in [M^{(2l+1:2l+2)}]} \\subseteq \\mathbb{R}^{D \\times D}$, where the number of heads $M^{(2l+1)} = 2p$,\n$M^{(2l+2)} = 2(p + 1)$ and embedding dimension $D = qp + 3p + q + 3$, that implements a 2SLS gradient update in (8)"}, {"title": "Pretraining and Excess Loss Bound", "content": "with any given learning rates $\\alpha, \\eta$:\n$$H^{2(l+1)} = \\text{TF}_{(\\theta^{(2l+1:2l+2)})}(H^{(2l)}) = \\begin{bmatrix}\nz_1 & ... & z_n & z_{n+1} \\\\\nx_1 & ... & x_n & x_{n+1} \\\\\ny_1 & ... & y_n & 0 \\\\\n\\Theta^{(l+1)}_{:,1} & ... & \\Theta^{(l+1)}_{:,1} & \\Theta^{(l+1)}_{:,1} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\beta^{(l+1)}_{x_1} & ... & \\beta^{(l+1)}_{x_n} & \\beta^{(l+1)}_{x_{n+1}} \\\\\n1 & ... & 1 & 1 \\\\\n1 & ... & 1 & 1 \\\\\n1 & ... & 1 & 1 \\\\\n1 & ... & 1 & 1 \\\\\n1 & ... & 1 & 0 \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{D \\times (n+1)}$$\nOur existence proof specifies an attention structure such that one layer updates only the first-stage estimate $\\Theta_{:,j}^{(l)}$ for\nall samples, followed by another layer to update the parameters $\\Theta_{:,j}^{(l)}$ and $\\beta_{:,j}^{(l)}$. Furthermore, as noted in the proof of\nTheorem 3.2 (ref. Appendix B.2), regardless of the initial values of $\\Theta_{:,j}^{(l)}$, $\\beta_{:,j}^{(l)}$ and $H_{:,j}^{(l)}$, the structures of the transformer\nblocks remain the same. This allows us to exploit the looped transformer architecture to significantly reduce the number\nof parameters and improve learning efficiency [Yang et al., 2023].\nBy cascading the transformer block $L$ times, with Theorem 3.1, one can show that transformers are able to mimic the\n2SLS estimator with exponential convergence rate, as described in the following corollary.\nCorollary 3.1 (Implementing GD-2SLS with looped transformer). For any $0 < \\varepsilon < 1$, given learning rates $\\alpha, \\eta$, and\n$\\Lambda \\in (0, 1)$, as defined in (9), there exists a transformer formulated as $\\text{TF}_\\theta(\\cdot) := \\text{TF}_{\\theta'}(\\text{LTF}_{\\overline{\\theta}, l}(\\cdot))$, which consists of an\n$\\overline{L}$-looped transformer $\\text{LTF}_{\\overline{\\theta}, l}$, with $\\overline{\\theta} = \\theta = \\{(Q_m^{(1:2)}, K_m^{(1:2)}, V_m^{(1:2)})\\}_{m \\in [M^{(1:2)}]} \\subseteq \\mathbb{R}^{D \\times D}$, $\\overline{L} = [O(\\frac{\\log \\Lambda}{\\log \\varepsilon})]$,\nand a final attention layer$^1$ $\\theta' = \\theta'_{\\text{ATTN}} = \\{(Q'_m, K'_m, V'_m)\\}_{m \\in [M']} \\subseteq \\mathbb{R}^{D \\times D}$, where $M^{(1)} = 2p, M^{(2)} = 2(p + 1)$,\n$M' = 2$, such that given embedded input $H^{(0)}$ taking the format in (10), the model output satisfies:\n$$|\\text{ready}(\\text{TF}_\\theta(H^{(0)})) - \\beta_{2SLS}^T x_{n+1}| \\le B_{\\epsilon},$$where $\\text{ready}(\\cdot)$ is a function that reads the prediction $\\hat{y}_{n+1}$ from the output of the transformer.\nWe emphasize here that our construction differs from the implementation of Bai et al. [2024, Theorem 4] for OLS in\nthe following aspects:\ni. We apply the square loss as defined in (7) to learn the 2SLS estimator, which simplifies the loss function's\nsum-of-ReLU representation.\nii. The dimension of the input embedding is $D = qp + 3p + q + 3$, where the extra dimensions store the vectorized\nparameters $\\Theta_{:,j}^{(l)}$, $\\beta_{:,j}^{(l)}$, and the first stage estimate $\\hat{x}_j^{(l)}$.\niii. We use a two-layer attention-only transformer block $\\theta$ to implement a 2SLS GD update (8), with the first layer\nto update the current first-stage estimate $\\hat{x}_j^{(l)}$, and the second layer to update the parameters $\\Theta_{:,j}^{(l)}$ and $\\beta_{:,j}^{(l)}$.\niv. For each transformer block, in the first layer, we equip 2 heads to update each dimension of $\\hat{x}_j^{(l)} \\in \\mathbb{R}^p$ for all\nsamples. In the second layer, we equip 2 heads to update each column of $\\Theta_{:,j}^{(l)} \\in \\mathbb{R}^{q \\times p}$ and $\\beta_{:,j}^{(l)} \\in \\mathbb{R}^p$.\nPretraining and Excess Loss Bound\nWith slightly abuse of notations, we denote the (formulated) training prompt as:\n$$H_k = \\begin{bmatrix}\nz_{1,k} & ... & z_{n,k} & z_{n+1,k} \\\\\nx_{1,k} & ... & x_{n,k} & x_{n+1,k} \\\\\ny_{1,k} & ... & y_{n,k} & 0 \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{(p+q+1) \\times (n+1)}, k = 1,..., N.$$\nNote that we denote each training prompt by the subscript $k = 1,..., N$, where $N$ is the total number of prompts. Each\ntraining prompt consists of $n$ labeled training samples $\\{(z_i, x_i, y_i)\\}_{i=1}^n$, and one unlabeled query sample $(z_{n+1}, x_{n+1})$.\nOur goal is to predict $y_{n+1}$ given the context provided by the prompt."}, {"title": "Extracting the regression coefficients", "content": "This layer updates the prediction $\\hat{y"}, {"n+1}": "beta_{:", "13": "nAlgorithm 2 Extracting the regression coefficients\nInput: Trained transformer model $\\text{TF"}, "theta$, input matrix $H$, perturbation $\\Delta$.\nOutput: Estimated coefficient $\\hat{\\beta}$.\nProcedure:\nCompute the output of the transformer model: $Y = \\text{TF}_\\theta(H)$.\nfor each dimension $k = 1,..., p$ do\nCopy $H_{\\Delta(k)} = H$. Set the k-th dimension of $x_{n+1}$ to be $(x_{n+1})_k + \\Delta$ for $H_{\\Delta(k)}$."]}