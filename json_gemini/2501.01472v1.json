{"title": "Augmented Contrastive Clustering with Uncertainty-Aware Prototyping for Time Series Test Time Adaptation", "authors": ["Peiliang Gong", "Mohamed Ragab", "Min Wu", "Zhenghua Chen", "Yongyi Su", "Xiaoli Li", "Daoqiang Zhang"], "abstract": "Test-time adaptation aims to adapt pre-trained deep neural networks using solely online unlabelled test data during inference. Although TTA has shown promise in visual applications, its potential in time series contexts remains largely unexplored. Existing TTA methods, originally designed for visual tasks, may not effectively handle the complex temporal dynamics of real-world time series data, resulting in suboptimal adaptation performance. To address this gap, we propose Augmented Contrastive Clustering with Uncertainty-aware Prototyping (ACCUP), a straightforward yet effective TTA method for time series data. Initially, our approach employs augmentation ensemble on the time series data to capture diverse temporal information and variations, incorporating uncertainty-aware prototypes to distill essential characteristics. Additionally, we introduce an entropy comparison scheme to selectively acquire more confident predictions, enhancing the reliability of pseudo labels. Furthermore, we utilize augmented contrastive clustering to enhance feature discriminability and mitigate error accumulation from noisy pseudo labels, promoting cohesive clustering within the same class while facilitating clear separation between different classes. Extensive experiments conducted on three real-world time series datasets and an additional visual dataset demonstrate the effectiveness and generalization potential of the proposed method, advancing the underexplored realm of TTA for time series data.", "sections": [{"title": "1 INTRODUCTION", "content": "Real-world time series applications frequently suffer from performance degradation due to domain shifts between training and test data. Unsupervised domain adaptation (UDA) methods [58] have emerged to mitigate this impact by leveraging unlabeled target data, aiming to align feature representations in the embedding space [4, 15, 19, 41, 42, 59, 60]. However, these approaches typically require access to source domain data during adaptation, which may be infeasible due to privacy and security concerns. Source-free domain adaptation (SFDA) offers an alternative by handling domain shifts without source data, demonstrating promising results by considering temporal dynamics [24, 43, 45, 62]. Nonetheless, SFDA struggles in real-world scenarios, particularly online time series applications where continuous data streams demand immediate inference. For instance, deploying a pre-trained fault diagnosis model for real-time monitoring on production lines using SFDA becomes impractical due to its requirement for pre-collecting all target data, rendering it unsuitable for on-the-fly diagnostics.\nTest-time adaptation (TTA) provides a more viable solution, enabling on-the-fly model adaptation with incoming data streams [29]. This approach circumvents the high training costs associated with SFDA and ensures the model remains responsive to new information, ideal for applications requiring immediate inference, as illustrated in Figure 1. Recent advancements in TTA for various vision applications have shown promising progress [2, 13, 20, 35, 49, 51, 68, 69]. Nevertheless, these approaches are primarily designed for visual tasks and may overlook the critical temporal characteristic inherent in real-world time series scenarios. Consequently, directly applying existing methods to time series data often yields suboptimal performance.\nTime series data present distinct challenges compared to other data types, particularly due to magnitude variability and inherent noise. Magnitude variability arises when the amplitude of time series data changes due to factors such as differing operational conditions, sensor calibrations, or individual differences. For instance, in human activity recognition, the same activity, such as walking, can produce varying magnitudes in accelerometer readings depending on the individual's walking speed, requiring the model to accurately classify the activity regardless of these amplitude variations. Additionally, real-world time series data are often noisy and non-stationarity, with external interferences or environmental factors obscuring true patterns and complicating accurate data interpretation. These challenges are particularly pronounced under the test-time adaptation settings, where the model must adapt instantaneously to the evolving temporal dynamics of the target data without access to the source domain. Therefore, our key question is how to effectively alleviate the negative impact of these temporal characteristic during the test time adaptation phase.\nTo overcome these challenges, we propose Augmented Contrastive Clustering with Uncertainty-aware Prototyping (ACCUP), a novel TTA method for time series data. Our solution introduces an uncertainty-aware prototypical ensemble module that enhances the reliability of model outputs. First, this module employs magnitude warping augmentation to generate amplitude variations in the time series data, enabling the model to learn temporal patterns that are invariant to these changes and enrich feature representations with more informative content. Uncertainty-aware prototypes are then calculated using entropy-minimized ensemble predictions from the augmentation-enhanced features. By focusing on these most confident and stable temporal patterns, it mitigates the effects of noise and distribution shifts. In addition, an entropy comparison scheme further ensures trustworthy predictions by filtering out instances with high uncertainty estimates.\nDespite its effectiveness, the inevitable occurrence of some noisy pseudo-labels can diminish model performance through error propagation. Using backpropagation with cross-entropy loss on noisy labels can mislead the training process, detrimentally affecting the model's adaptation [10, 50, 70]. To mitigate this, we further introduce an augmented contrastive clustering strategy to enhance feature discriminability and reduce the influence of noisy pseudo-labels by prioritizing reliable predictions. By maximizing mutual information between raw and augmented data, the model maintains temporal coherence and ensures consistent learned temporal dependencies across different views. In addition, augmented contrastive clustering also promotes tight grouping of instances within the same class and clear separation between different classes, ensuring that the temporal patterns unique to each class are preserved and accurately classified. This is particularly crucial for time series data, where subtle temporal variations often define class boundaries.\nThe main contributions of this paper can be summarized below:\n\u2022 To our knowledge, this is the first work to advance TTA for a broad spectrum of real-world time series applications, despite the existence of some domain-specific approaches.\n\u2022 We propose a novel uncertainty-aware prototypical ensemble module designed to prioritize confident temporal patterns and rectify the quality of pseudo-labels, thereby enhancing their trustworthiness.\n\u2022 We present an augmented contrastive clustering constraint to improve feature discriminability and mitigate error accumulation during model adaptation."}, {"title": "2 RELATED WORKS", "content": "UDA was developed to bridge the performance gap in deploying models across different domains without target annotations. Significant strides in UDA for time series tasks have emerged through two main strategies: discrepancy aligning-based methods and adversarial learning [42]. Discrepancy aligning aims to minimize statistical differences for domain alignment [4, 15, 34], while adversarial learning promotes domain-invariant features through adversarial training [19, 41, 59, 60]. For instance, RAINCOAT tackles feature and label shifts by aligning temporal and frequency features across domains [15], while SLARDA utilizes autoregressive adversarial training to align temporal dynamics [41]. Despite these methods showing promising results, they rely on access to source data during adaptation. In practical scenarios, however, obtaining source data can be challenging due to privacy concerns or storage limitations.\nSFDA was proposed to address this challenge [43, 45, 62]. Methods like MAPU utilize temporal imputation tasks to recover the original signal in feature space without source data [43]. However, SFDA requires pre-existing access to all target data, and the model needs iterative training on this data, hindering efficiency. Real-world time series applications demand immediate inference and adaptation upon data arrival, creating a need for more realistic adaptation scenarios. Some recent work has made preliminary attempts to adapt the model during testing of tasks such as automatic speech recognition or video-based action recognition [12, 21-23, 32]. Although these existing methods also consider data of a sequence nature, they are considered to be application-specific and may not generalize well across applications. In addition, video and time series, while sharing some commonalities, are usually treated as different modalities. Therefore, this motivates us to explore versatile TTA methods suitable for time series data, which is underexplored."}, {"title": "2.2 Test-Time Adaptation", "content": "Test-time adaptation (TTA) presents a realistic and challenging scenario by adapting pre-trained models to online target data streams during testing. One paradigm involves incorporating auxiliary self-supervised tasks during both training and testing, which may not be feasible or scalable in real-life scenarios [35, 51]. Another prevailing paradigm adapts the model solely during the testing phase using pre-trained source models and online unlabeled data [3, 6, 14, 40, 46, 52, 54]. For instance, normalization-based approaches either replace the training model's statistics with estimates from test data or adjust its normalization layer parameters [46, 49, 65]. Entropy-minimizing self-training enhances model generalization by minimizing entropy to adjust the trainable parameters [30, 38, 39, 54, 55]. Additionally, some methods enhance adaptation performance using strategies such as feature alignment or prototype learning [17, 18, 48, 56]. Although existing TTA methods show promise, their design primarily targets visual tasks, which may limit their effectiveness in handling the inherent noise and magnitude variability of real-world time series data. In contrast, our approach specifically addresses these limitations through an uncertainty-aware prototypical ensemble and an augmented contrastive clustering strategy, effectively boosting adaptation performance in this unique domain."}, {"title": "2.3 Prototypical Learning", "content": "Prototype learning represents a paradigm where classes or concepts are characterized by central or typical examples within the feature space. It has evolved from its early combination with nearest neighbor rules for classification tasks [7] to its recent prominence with the advent of deep learning. These methods have been successfully applied in various applications such as few-shot learning [47, 66, 67, 71] and continual learning [8, 16, 28, 44, 57, 72]. For instance, prototypical networks calculate class prototypes as mean embeddings of few-shot examples, simplifying classification by comparing new instances to these prototypes [47]. In continual learning, COPE [8] extends this concept by incorporating a high momentum-based update strategy for prototypes with each observed batch. Additionally, substantial research has focused on using prototype learning to create compact feature spaces for open-set recognition [33, 36, 61, 63, 64] and to enhance model interpretability [5, 37]. Recent advancements have explored dynamic prototype updates during testing [17, 18, 56], as exemplified by T3A [17], which updates prototypes using a support set comprising original classifier weights and online data features based on pseudo-labels. However, these methods often struggle to maintain prototype reliability in the presence of noisy data, particularly in time-series domains where temporal variations can introduce significant challenges."}, {"title": "3 METHODOLOGY", "content": "Given the pre-trained model with standard empirical risk minimization on the source $D_s = \\{ (X_s^i, y_s^i) \\}_{i=1}^{n_s}$, our goal is to adapt the source pre-trained model using unlabelled target data $D_t = \\{ X_t^i \\}_{i=1}^{n_t}$. Here, $X_s$ and $X_t$ denote univariate or multivariate time series of length L in the source and target domains, respectively, $y_s$ denotes the category labels corresponding to $X_s$, $n_s$ and $n_t$ denote the number of source and target samples, respectively. During testing, we initialize model $g = h_\\theta \\circ f_\\theta$ using the source pre-trained model parameters, where $f_\\theta$ denotes the feature encoder and $h_\\theta$ denotes the linear classifier. The output of model g for time series $X_i$ is written as $p_i = g(X_i) \\in \\mathbb{R}^C$, where C is the number of categories. This work aims to address the test-time adaptation problem, where the source-domain data is strictly inaccessible and the training process of the source-domain model is fixed. This assumption is more realistic in real-world application scenarios, taking into account data privacy concerns as well as the direct deployment of source-domain pre-trained models. In addition, we assume a difference across the marginal distributions, i.e., $P(X_S) \\neq P(X_T)$, while the conditional distributions are stable, i.e., $P(y_s|X_s) \\approx P(y_T|X_T)$."}, {"title": "3.2 Overview", "content": "Our proposed method, ACCUP, is designed for real-world time series adaptation. Figure 2 illustrates its main pipeline. The model includes two key modules: (1) uncertainty-aware prototypical ensemble first enhances predictions through magnitude warping augmentation and then refines them using uncertainty-aware prototypes. Finally, it generates reliable pseudo-labels based on an entropy comparison scheme, effectively addressing the magnitude variability and noise inherent in time series data; (2) the augmented contrastive clustering mitigates error accumulation of noisy pseudo-labels by simultaneously considering cross-view prediction consistency, sample cohesion within classes, and clear separation between classes. It ensures robustness against test-time variations, maintaining class conditional invariance despite the challenges posed by time series data. We will elaborate on each component in the next subsections. The complete algorithm is provided in the Appendix A.1."}, {"title": "3.3 Uncertainty-Aware Prototypical Ensemble", "content": "Time series often exhibit significant variability in amplitude due to various factors such as operational conditions or environmental influences. This variability can cause substantial discrepancies between training and test data, making it difficult for models to generalize and perform accurately on new data. To mitigate the challenges posed by this characteristic, we introduce an augmentation-ensemble strategy for refined temporal features and logits in the context of TTA. However, unlike standard augmentation during training, TTA involves fine-tuning unlabeled target data during inference. Aggressive augmentation might deviate significantly from the target distribution, leading to inconsistent predictions and hindering performance. It is essential to choose a moderate augmentation strategy so that the generated views can retain overall the intrinsic characteristics of the original time series while still introducing enough variability. To cater for this, we introduce magnitude warping augmentation to alleviate magnitude variability by integrating information from both raw and augmentated views [53]. Magnitude warping involves stretching or compressing the magnitude of the time series signal by convolving the data window with a smooth curve varying around one. This augmentation method performs elastic transformations that introduce nuanced variations while ensuring that the innate temporal dynamics of the time series data are preserved.\nGiven a batch of unlabelled test samples X, we use the magnitude warping to obtain the corresponding augmented views A. For each time series x \u2208 X and a \u2208 A, we can obtain the corresponding feature embeddings and predicted logits: $f_{raw} = f_\\theta(x)$, $p_{raw} = h_\\theta(f_{raw})$, $f_{aug} = f_\\theta(a)$, and $p_{aug} = h_\\theta(f_{aug})$, respectively. Thus augmentation-ensemble features and logits are then derived by averaging, generating more robust representations as follows,"}, {"title": "3.3.1 Uncertainty-Aware Prototypes", "content": "Time series data are typically noisy and subject to distribution shifts between training and testing phases. To prioritize more confident temporal patterns and mitigate the impact of noise, we present uncertainty-aware prototypes in this context by incorporating uncertainty estimates to weigh features based on their reliability, as shown in Figure 3. Prototypes often are less sensitive to outliers and treat categories equally, addressing class-imbalanced real-world time-series data. Specifically, since labeled data is not available in TTA, we compute pseudo-class prototypes using the memorized support set that stores the past test data and the corresponding logits. The memorized support set starts with the pre-trained source model's linear classifier weights. When a batch of target-domain data X arrives, we add the feature embeddings $f_{ens}$ and logits $p_{ens}$ of each sample $x \\in X$ obtained from the augmentation-ensemble module to the memorized support set. To ensure reliable prototypes, we filter out unreliable using Shannon entropy [31]. Specifically, the entropy of the predicted logits $p_{ens}$ is $H_{ens} = - \\sum_{c=1}^C \\sigma(p_{ens}^c) log \\sigma(p_{ens}^c)$, where $\\sigma$ denotes the softmax activation function. For each category, the K samples with the lowest entropy (most confident predictions) are retained. Each category's prototype is then computed as the centroid of the filtered feature embeddings. Finally, the uncertainty-aware prototype $\\mu_c$ for class c can be calculated as:"}, {"title": "3.3.2 Entropy Comparison Scheme", "content": "To rectify the quality of the pseudo-labels used for the augmented contrastive clustering, we further propose an entropy comparison scheme. This scheme selectively combines predictions from the augmented-ensemble module and the uncertainty-aware prototype classification based on their respective entropy values. For each sample x, the prediction with more credibility (i.e., lower entropy) is chosen as the final output logits, which denoted as,\n$p_{out} = p_{ens} \\cdot I [H_{ens} < H_{proto}] + p_{proto} \\cdot I [H_{ens} \\geq H_{proto}]$.\nHere, I [.] is the indicator function, and $H_{ens}$ and $H_{proto}$ are the Shannon entropies of $p_{ens}$ and $p_{proto}$, respectively. Subsequently, the pseudo-label for x is obtained as $\\hat{y} = arg max p_{out}$."}, {"title": "3.4 Augmented Contrastive Clustering", "content": "Although securing reliable pseudo-labels significantly boosts performance, self-training with cross-entropy remains sensitive to noisy labels, where even a small number can detrimentally harm performance [10, 50, 70]. To address this issue and enhance feature discrimination, we propose an augmented contrastive clustering to optimize our adaptation strategy. Instead of relying on cross-entropy which directly propagates labels, contrastive cluster is based on similarity rather than explicit label assignment, which inherently mitigates the risk of propagating erroneous labels. Even if some labels within a cluster are noisy, their influence is diluted as the model is primarily guided by the majority of correctly labeled examples in each cluster. Specifically, our module's design focuses on two key aspects. First, ensuring that predictions from both raw and augmented data have similar distributions is vital for maintaining the temporal coherence of time series data. By maximizing mutual information between similar temporal patterns across different views, the model can better capture the underlying temporal dynamics. Second, temporal data often require distinguishing between subtle temporal patterns, which can be obscured by noise. By promoting tight clustering within the same class and clear separation between different classes, this method ensures that the temporal dependencies are preserved and accurately classified.\nTo accomplish this, we initially combine the raw time series X with its augmented views A from the current test batch to form a unified set $C = X \\cup A$. With this combined set C, we identify positive and negative pairs for an anchor sample $x_i$ as $pos(i) = \\{x_j \\in C; y_j = y_i\\}$ and $neg(i) = \\{x_k \\in C;y_k \\neq y_i\\}$, respectively. Given the absence of true target labels, we use pseudo-labels generated by the uncertainty-aware prototype ensemble module to categorize the samples into positive and negative sets for clustering purposes. Our objective is to maximize mutual information among positive pairs while reducing their similarity with negative pairs. To ensure consistent labeling across views, both original and augmented versions of a sample are assigned the same pseudo-label. Finally, the augmented contrastive clustering constraint for sample $x_i$ can be defined as follows:\n$L'_{contrast} = - \\frac{1}{|pos(i)|} \\log (\\frac{\\exp(\\sigma(p_i, p_j)/\\tau)}{\\sum_{k\\in neg(i)} \\exp(\\sigma(p_i, p_k)/\\tau)})$.\nwhere $|pos(i)|$ denotes the cardinality of the positive sample set, $\\tau$ is the temperature parameter to adjust the contrastive scale, and $\\sigma(p_i, p_j)$ denotes cosine similarity score between the uncertainty-aware prototypical ensemble logits of two samples."}, {"title": "4 EXPERIMENT AND RESULTS", "content": "In this section, we systematically evaluate our method against state-of-the-art approaches across various time-series applications through rigorous experimentation. Further, we perform multiple ablation experiments to verify the effectiveness of the different components. Ultimately, we conduct a series of model analyses including parameter sensitivity analysis and visualization of t-SNE feature distributions. The experimental implementation details are presented in Appendix A.3."}, {"title": "4.1 Datasets", "content": "We evaluate the model performance on three commonly used time series datasets from three real-world scenarios, i.e., human activity recognition (UCIHAR) [1], machine fault diagnosis (MFD) [26], and sleep stage classification (SSC) [11]. The details of each dataset are presented in the Appendix A.4."}, {"title": "4.2 Baseline Methods", "content": "For evaluation, we compare our method with following established test-time adaptation approaches: BN [46], PL [25], TENT [54], SHOT [30], EATA [38], SAR [39], TTAC [48], NOTE [12], T3A [17], TAST [18], COTTA [55], ROTTA [65], and TSD [56]. Additionally, we include the method of source-only (i.e., Source) and target-supervised (i.e., Target) performance to establish baseline and upper bound on generalizability, respectively. The description of each comparison method is shown in Appendix A.5."}, {"title": "4.3 Comparative Experiments", "content": "We evaluate our model's efficacy on three real-world time-series datasets: UCIHAR, MFD, and SSC. The experimental results of the different approaches for five cross-domain scenarios for each dataset, as well as the average results across all the scenarios, are presented in Tables 1, 2 and 3, respectively. In addition, to further validate the versatility potential of the proposed model, we also conduct an additional comparative experiment on the visual dataset PACS [27]. Detailed results are provided in Appendix A.6."}, {"title": "4.3.1 Evaluation on UCIHAR Dataset", "content": "Table 1 presents the classification performance of various methods on the UCIHAR dataset across five cross-subject scenarios. The results indicate performance improvements for all methods when compared to the direct testing using the source domain pre-trained model. Methods trained using entropy minimization or pseudo-labeling (e.g. TENT, PL, SHOT, EATA, SAR) all achieve comparable classification performance, while the TAST method further enhances prediction reliability by introducing prototype prediction information from nearest neighbors. Distinctly, our method excelled in two scenarios and achieved an overall macro F1 score of 88.16%. This success likely stems from leveraging augmented contrastive clustering to effectively group similar samples while differentiating dissimilar ones. Moreover, the uncertainty-aware prototypical ensemble further enhances the quality of pseudo-labels."}, {"title": "4.3.2 Evaluation on MFD Dataset", "content": "Table 2 showcases our method's dominance on the MFD task, achieving an average macro F1 score of 95.60% outperforming other methods in four scenarios. It significantly improves upon the Source method by over 15% and surpasses the second-best approach by nearly 1%. Notably, most methods on MFD see substantial performance gains, likely due to the longer sequences offering richer information and the smaller number of categories facilitating improvement. Nevertheless, T3A and TAST exhibit relatively lower performance, possibly due to substantial distributional differences between the source and target domains in this dataset. The use of prototype classifications alone may not perfectly characterize the distributional properties of the target domains, resulting in marginal performance gains."}, {"title": "4.3.3 Evaluation on SSC Dataset", "content": "Table 3 highlights our method's superiority on the sleep staging task, achieving an average macro F1 score of 62.65% and excelling in four out of five scenarios. This outperforms the second-best method by 2.85% and improves upon the source pre-trained model by 12.27%, respectively. This can be attributed to the inherent characteristic of class imbalance in the SSC dataset, which tends to limit the performance improvement of other methods. In contrast, our method mitigates the degradation by introducing an uncertainty-aware prototype ensemble and augmented contrastive clustering, resulting in significantly enhanced adaptation performance."}, {"title": "4.4 Ablation Studies", "content": "In this section, we perform various ablation studies to assess the individual contributions within each module on three datasets. In addition, we also provide overall ablation study to quantify the effect of each module of proposed model in Appendix A.7."}, {"title": "4.4.1 Effectiveness of Uncertainty-Aware Prototypical Ensemble", "content": "To explore the impact of our uncertainty-aware prototypical ensemble, we evaluated different pseudo-labeling strategies, as shown in Table 4. Specifically, $P_{ensemble}$ denotes pseudo-labels predicted with augmentation-ensemble. $P_{prototypes}$ denotes pseudo-labels obtained with uncertainty-aware prototypes. $P_{averaged}$ denotes pseudo-labels obtained with averaged predictions using augmentation-ensemble and uncertainty-aware prototypes. While averaging prototypes and enhanced predictions can achieve competent performance, it may harm performance on certain datasets, leading to a reduction of around 3% due to its disregard for prediction uncertainty during averaging. In contrast, our method selects more reliable predictions rather than averaging both. Besides, our method outperformed other pseudo-labeling strategies on two of the three datasets and achieved the second-highest performance on the third. This demonstrates that our entropy comparison scheme significantly enhances the quality of pseudo-labels, resulting in superior outcomes compared to alternative approaches. This also suggested that, in general, data with longer sequences and fewer categories tend to benefit from classification-based logits due to richer discriminative information. Conversely, datasets with a larger number of classes and imbalanced class distributions may favour prototype-based prediction."}, {"title": "4.4.2 Effectiveness of Augmented Contrastive Clustering", "content": "We validate the effectiveness of augmented contrastive clustering through ablation experiments using various optimization objectives, as outlined in Table 5. Specifically, SE Loss refers to entropy minimization as the optimization objective, CE Loss indicates the use of cross-entropy loss, while TC Loss denotes the use of traditional contrastive learning. Our augmented contrastive clustering objective consistently outperforms the others across all three datasets, highlighting its efficacy in mitigating error accumulation even in the presence of a few noisy labels. In addition, in comparison to traditional contrastive learning, our approach, by focusing on cross-cluster contrasts, yields more robust representations that exhibit greater resilience to noisy label conditions."}, {"title": "4.4.3 Effectiveness of Different Augmentation Strategies", "content": "We investigated the impact of various data augmentation strategies on performance. Table 6 compares our employed magnitude warping augmentation method with other commonly used techniques [53]. Time series specific magnitude warping augmentation achieves the best performance across two out of three datasets. This success may be attributed to the elastic transformations introduced by this method, which preserve the inherent temporal dynamics of the time series data while introducing nuanced variations. Additionally, all augmentation methods resulted in improved classification performance compared to no augmentation, underscoring the effectiveness of employing appropriate weak augmentation strategies to enhance model features and bolster prediction robustness. We also conduct additional experiments to explore the effects of different augmentation combinations in the Appendix A.8."}, {"title": "4.5 Model Analysis", "content": "Here, we perform parameter sensitivity analysis and qualitative analysis. We also analyze the model performance when fine-tuning different layers of the feature encoder, as shown in Appendix A.9."}, {"title": "A.4 Descriptions of Datasets", "content": "The UCIHAR dataset [1] is designed specifically for tasks related to human activity recognition. Data collection involved the utilization of three distinct sensor types: accelerometer sensor, gyroscope sensor, and body sensor. Each sensor yielded three-dimensional readings, resulting in a total of 9 channels per sample, with 128 data points per sample. The dataset encompasses information gathered from 30 distinct users, with each user representing an individual domain. In our experimental design, we executed five cross-user experiments, wherein the model was trained on one user and subsequently tested on different users, facilitating an assessment of its cross-domain performance."}, {"title": "A.4.2 Machine Fault Diagnosis (MFD) Dataset", "content": "The MFD dataset [26], curated by Paderborn University, is intended for fault diagnosis applications, employing vibration signals to discern various types of incipient faults. The data collection encompasses four distinct working conditions, with each data sample comprising a solitary univariate channel containing 5120 data points, as established in previous studies [41, 43]. In our experimental framework, each working condition is treated as an individual domain, and we implement five diverse cross-condition scenarios to assess the domain adaptation performance."}, {"title": "A.4.3 Sleep Stage Classification (SSC) Dataset", "content": "The SSC task involves the classification of EEG signals into five distinct stages: Wake (W), Non-Rapid Eye Movement stages (N1, N2, N3), and Rapid Eye Movement (REM). This study employs the Sleep-EDF dataset [11], which encompasses EEG recordings obtained from 20 healthy subjects. Following the methodology of prior research [43], we focus on a single channel, specifically Fpz-Cz, and leverage data from 10 subjects to design five cross-domain experiments."}, {"title": "A.4.4 PACS Dataset", "content": "The PACS dataset [27] comprises 9991 RGB images of size 224x224, categorized into seven classes across four domains: Photo (P), Art painting (A), Cartoon (C), and Sketch (S). Following the established protocol in previous research [17, 56], we configure four cross-domain scenarios to evaluate domain adaptation performance, with each domain serving as the target domain in turn while the remaining domains constitute the source domains."}, {"title": "A.5 Descriptions of Baseline Methods", "content": "\u2022 Target indicates that the upper bound of domain adaptation, where training and testing are performed on the target domain only.\n\u2022 Source indicates that no adaptation is performed and only the source domain model is used to inference on the target domain.\n\u2022 BN [46] Test time normalization updates the statistics of the batch normalization module by using streamed data from the target domain.\n\u2022 PL [25] Pseudo Label facilitates the parameter updating of all normalization layers by minimizing the cross-entropy loss associated with the predicted pseudo labels.\n\u2022 TENT [54] Test time entropy minimization facilitates the parameter updating of all normalization layers by minimizing the entropy of the model predictions in the streamed data.\n\u2022 SHOT [30] Source hypothesis transfer involves learning the target-specific feature extraction module by leveraging both information maximization and self-supervised pseudo-labeling techniques.\n\u2022 EATA [38] Efficient anti-forgetting test-time adaptation employs a sample-efficient entropy minimization loss to exclude uninformative samples and introduces a regularization loss designed to preserve critical model weights throughout the adaptation process.\n\u2022 SAR [39] introduces a sharpness-aware and reliable entropy minimization method to stabilize the TTA process.\n\u2022 TTAC [48] Test-Time Anchored Clustering mitigates the KL divergence between the distributions of the source and target domains.\n\u2022 NOTE [12] Non-i.i.d. Test-Time Adaptation (NOTE) optionally updates batch normalization statistics when the distributional shift between a test instance and the source model's training distribution exceeds a predefined threshold.\n\u2022 T3A [17] Test time template adjuster creates a pseudo-prototype for each class using streamed target domain data and then classifies each sample based on its distance to the pseudoprototype."}, {"title": "A.6 Generalization on Vision Task", "content": "To assess the model's generalizability, we conducted additional experiments on the PACS dataset[27], a benchmark for visual domain adaptation. The more description of PACS dataset is presented in Appendix A.4.4. The model was adapted to the image domain by replacing the time series-specific feature encoder with ResNet18 and ResNet50 architectures, respectively. Data augmentation techniques commonly used in image processing (random cropping, flipping, and color jittering) were employed as counterparts to the time series-specific magnitude warping. To ensure fair comparison, all experimental settings were consistent with previous studies, and we directly quoted baseline results from prior research [56]. For source training, we used an Adam optimizer with a learning rate of 5e-5 and a batch size of 32. For test-time adaptation, we employed an Adam optimizer with a learning rate of 5e-5 and a batch size of 128. All experiments were conducted using three random seeds {0, 1, 2}, and we report the average accuracies in Tables 9 and 10. The results demonstrate that our proposed model consistently outperforms baseline approaches. Specifically, with the ResNet18 backbone, we achieved optimal performance in three of the four cross-domain scenarios, attaining the highest average accuracy of 88.33% approximately a 1% improvement over the second-best baseline. Using the ResNet50 backbone, we achieved optimal performance in three of the four cross-domain scenarios, with an overall classification performance exceeding 90%. These findings underscore the model's versatility potential and the effectiveness of our proposed uncertainty-aware prototypical ensemble approach in enhancing model adaptability beyond time series domains."}, {"title": "A.7 Ablation Study of Each Module", "content": "To assess the individual contributions of each model component, ablation study was conducted by removing specific modules. Table 11 presents classification performance with the following ablation configurations: a model without contrast learning (w/o Contrast), without entropy comparison (w/o entComp), without augmentation (w/o augmentation), and without prototype learning (w/o prototypes). Results indicate that removing different modules decreases model performance to varying degrees, with the relative contribution of modules differing slightly across datasets. For instance, the uncertainty-aware prototype significantly improves performance on UCIHAR and SSC by 1.34% and 4.29%, respectively. On the MFD dataset, augmentated contrastive clustering and entropy comparison strategies has a greater impact than other modules, improving performance by 2.56% and 1.18%, respectively. Notably, the w/o prototype model variant achieved optimal results on MFD. This may be attributed to the fact that for data with longer sequence lengths and fewer categories, classification-based prediction logits exhibit better discriminative properties due to richer information content. These findings underscore the importance of each component in achieving optimal model performance while highlighting the need for dataset-specific considerations in module selection and optimization."}, {"title": "A.8 Effectiveness of Different Augmentation Combination Strategies", "content": "We conduct additional experiments to explore augmentation combinations in Table 12. The results suggest that while combining augmentations can enhance performance, our method-whether used alone or in conjunction with baseline methods-consistently outperforms others. For instance, using our method with Jitter augmentation achieved an 88.48% F1-score on the UCIHAR dataset, whereas our method alone scored 95.60% on the MFD dataset. These results further indicate that the time series-specific amplitude and noise variations can be effectively mitigated through the use of a suitable augmentation strategy, enhancing the robustness of the model during the test adaptation phase."}, {"title": "A.9 Analysis of Fine-Tuing Different Layers of Feature Encoder", "content": "This study examines the impact of fine-tuning feature encoder parameters across various layers on model performance. Table 13 presents the mean and standard deviation of macro F1 scores achieved when fine-tuning different layers during adaptation. Fine-tuning all feature encoding layers yielded the best classification results, suggesting a significant improvement in the model's discriminative power through modifications across all layers. Interestingly, classification performance demonstrated resilience, showing only a slight decline when adjusting parameters for select layers, compared to fine-tuning all layers. This observation implies that competitive classification results can still be achieved by"}]}