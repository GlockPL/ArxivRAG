{"title": "Demystifying Reinforcement Learning in Production Scheduling via Explainable AI", "authors": ["Daniel Fischer", "Hannah M. H\u00fcsener", "Felix Grumbach", "Lukas Vollenkemper", "Arthur M\u00fcller", "Pascal Reusch"], "abstract": "Deep Reinforcement Learning (DRL) is a frequently employed technique to solve scheduling problems. Although DRL agents ace at delivering viable results in short computing times, their reasoning remains opaque. We conduct a case study where we systematically apply two explainable AI (XAI) frameworks, namely SHAP (DeepSHAP) and Captum (Input x Gradient), to describe the reasoning behind scheduling decisions of a specialized DRL agent in a flow production. We find that methods in the xAI literature lack falsifiability and consistent terminology, do not adequately consider domain-knowledge, the target audience or real-world scenarios, and typically provide simple input-output explanations rather than causal interpretations. To resolve this issue, we introduce a hypotheses-based workflow. This approach enables us to inspect whether explanations align with domain knowledge and match the reward hypotheses of the agent. We furthermore tackle the challenge of communicating these insights to third parties by tailoring hypotheses to the target audience, which can serve as interpretations of the agent's behavior after verification. Our proposed workflow emphasizes the repeated verification of explanations and may be applicable to various DRL-based scheduling use cases.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Background and key concepts", "content": "While the application of Artificial Intelligence (AI) is gaining more attention in many disciplines (Y. K. Dwivedi et al., 2021; Fast & Horvitz, 2017; Heuillet et al., 2021; R. Dwivedi et al., 2023), such as medicine or education (T.-C. T. Chen, 2023b), the public remains divided on its benefits and risks, and generally lacks a deep understanding of AI and machine learning (ML) in general (Bao et al., 2022; Gillespie et al., 2021; Liehner et al., 2023). Concerns exist regarding ethics, transparency and job displacement (Commission, 2024; Liehner et al., 2023), but these do not not stop AI researchers from pushing the boundaries of the field (Peters & Jandri\u0107, 2019).\nOne area where AI is becoming increasingly relevant is manufacturing and scheduling. Scheduling is defined as a decision-making process that \"deals with the allocation of resources to tasks over given time periods\u201d (Pinedo, 2012, p. 1). The allocation process can be divided into rules, such as a sequence in which machines process production jobs. Here, one way to tackle scheduling problems is Reinforcement Learning (RL). RL is a branch of ML where an agent interacts with an environment and is trained to behave optimally. Typically, RL problems are framed as Markov Decision Processes (MDPs). An MDP is defined as a tuple $(S, A, P, R, \\gamma)$. The set of states $S$ represents all possible situations in which the agent can find itself, where each state $s_t$ encodes information about the environment at a given time step $t$. The set of actions $A$ includes all possible actions the agent can take in any given state. The transition probability function $P$ defines the probability of moving from one state to another given a specific action, encapsulating the dynamics of the environment. The reward function $R$ assigns a numerical value $r_{t+1}$ to each action taken in a particular state, providing immediate feedback to the agent on the desirability of its actions. Lastly, the discount factor $\\gamma$ (ranging between 0 and 1) determines the importance of future rewards, balancing immediate and long-term gains in the agent's decision-making process (Sutton & Barto, 2018a). RL agents can function as a decision support tool to find near-optimal job sequences in short computing times, even for highly customized configurations. This capability is a significant advantage for automated planning systems, enabling near real-time (re-)scheduling (Grumbach et al., 2023).\nRecently, deep reinforcement learning (DRL) as a subcategory of RL has become a preferred method for tackling complex scheduling problems. (Kayhan"}, {"title": "2 Literature Review", "content": "We start by laying a common ground on RL in production and scheduling. Afterwards, we introduce xAI methods and workflows before we stress the research gap for falsifiable and systematic xAI approaches in production planning. In the following, we differentiate between frameworks (software tools which contain xAI methods) and the methods themselves that can be mathematically formulated. In accompanying literature, proposed xAI workflows are often found under the synonyms \"framework\", \"process\", \"process model\" or \"procedure\u201d."}, {"title": "2.1 Reinforcement Learning Applications in Production and Scheduling", "content": "Referring to the definition by Pinedo (2012), scheduling as is has to be optimized to meet the expectations of stakeholders. There are several possible dependent variables, such as lead times, resource utilization, or simply costs. One way to approach this problem is to solve it heuristically or analytically (e.g., with operations research methods). Although these methods are widely studied and applied, heuristics tend to find only local optima, while mathematical models are only suitable for smaller problems with a simple search space."}, {"title": "2.2 State of the art xAI frameworks and methods", "content": "Before diving into the xAI approaches, a common terminology has to be laid out as Palacio et al. (2021) propose in their framework for unifying xAI.\nFollowing this framework, the explanation is the description of information made to create understanding by humans. How explanations are created is defined by the xAI methods. Here, the methods need to specify what is needed as input and in return what output is produced. On the other hand, the interpretation is the meaning that is associated with the explanation. This also relates to why this meaning is assigned (e.g., a causal link or the context). (Palacio et al., 2021)\nOther researchers argue that in the field of AI, interpretability (\"interpretable AI\") refers to a lower complexity of the model so that its components can be analyzed and understood (e.g., small decision trees are interpretable) intrinsically. These types of algorithms are sometimes labelled 'transparent'. While xAI approaches can enhance model transparency already during development, for black-box models that are not intrinsically interpretable or transparent, explainers are oftentimes used. These additional algorithms can explain the"}, {"title": "2.2.1 The choice of explanation type", "content": "Various xAI methods can produce different explanation types for different use cases or needs (for an overview see Mohseni et al. (2021) or T.-C. T. Chen (2023b)). The choice what type to use is highly dependent on the target audience, which means that one has to consider the background knowledge of the explainee. An AI expert could be interested in detailed explanations for debugging, while laymen might only want to understand enough to decide whether or not they can rely on the AI system (e.g., via explanations or more transparency on the algorithm) (Mohseni et al., 2021; M. Y. Kim et al., 2021; Heuillet et al., 2021; R. Dwivedi et al., 2023).\nResearchers can use xAI to describe how the entire model works, for example by using global explanations (Mohseni et al., 2021; T.-C. T. Chen, 2023b). In xRL, there are approaches aimed at helping developers debugging their model by better understanding the states, actions and their outcomes (Dazeley et al., 2023). Quality control might also play a role. For example, Klar et al. (2024) use policy summarization (Wells & Bednarz, 2021), state value evaluation and perturbation methods to filter out random explanations. This way they ensure that random solutions are not influencing the outcome of the planning of a factory layout.\nExample-based explanations concentrate on explaining selected instances in the data, e.g., by using local explanations for describing a single action of the agent (T.-C. T. Chen, 2023b). However, single explanations are criticized for"}, {"title": "2.2.2 x\u0391\u0399 Methods", "content": "One of the ways to categorize xAI methods for deep learning is into intrinsic, perturbation- and gradient- or backpropagation-based (T.-C. T. Chen, 2023b; Kamath & Liu, 2021). We first introduce decision trees, a popular and intrinsically interpretable algorithm that is broadly used. Then, we outline SHAP as well as several perturbation- and gradient-based xAI methods.\nDecision Trees\nDecision trees are widely used and offer explainability out of the box (Kotsiantis, 2013). Starting with the work of Breiman, Friedman, Olshen, and Stone (1984) the foundation of modern decision trees has been laid. With his CART (classification and regression tree) the rules discovered by the algorithm are displayable in a transparent form. Decision trees are highly versatile, e.g. for regression and classification. In the original CART algorithm, on each split the information gain has to be maximized which relies on information entropy (Quinlan, 1986) and the concept of information content (Shannon, 1948).\nDecision trees have been used in a wide range of domains, including stock market (Wu et al., 2006), marketing (J. W. Kim et al., 2001), image classification (C.-C. Yang et al., 2003) and scheduling (Portoleau et al., 2024).\nSHAP (SHapley Additive exPlanation)\nSHAP is a framework introduced by Lundberg and Lee (2017). The authors show that multiple xAI methods such as Lime (Ribeiro et al., 2016) and DeepLIFT (Shrikumar et al., 2017) are additive feature attribution methods. Because of the complex nature of blackbox models, (local) explanation models"}, {"title": "2.2.3 xRL", "content": "Xiong et al. (2024) state that in xRL there a multiple different approaches and respective methods: One can explain the model logic, the reward function (by decomposition or shaping), the states and the tasks. Specifically for explaining the states, post-hoc methods such as SHAP may be used.\nMilani et al. (2024) propose to categorize xRL into methods for feature importance (e.g., local state-action relationships), training and MDP (f.e.,"}, {"title": "2.2.4 Frameworks for xAI workflow and design decisions", "content": "Asides from the methods themselves, we must also consider the procedure of generating xAI in a complex business setting. This includes the workflow, design choices as well as other aspects to look out for. We draw on frameworks from both the xAI community as well as other domains.\nLeavitt and Morcos (2020) point out that approaches of xAI, especially for deep learning, often rely on visualization or single examples and lack falsification via hypotheses, quantifiability and human as well as general verification of validity. T.-C. T. Chen (2023b) also highlight that xAI results need to be validated. This is something that should be kept in mind in the workflow and design of explanations, in order to ensure a scientific standard.\nMohseni et al. (2021) propose a framework for xAI design and evaluation. In their nested model, the outer layer focuses on the outcomes. When designing an xAI system, general considerations have to be made first: What is the goal, the target group and what is supposed to be explained? This also involves determining how to evaluate the xAI system to measure if the expectations were met. In the next layer, the process of explaining has to be decided on with the user in mind. This includes aspects like the explanation formats and the amount of details to include, while also evaluating the usefulness of the chosen explanations. The inner layer is the type of xAI method itself. When black-box models are explained with ad-hoc algorithms, fidelity to the original model plays a role. The trustworthiness of the underlying model should also be evaluated.\nIn their framework for design and analysis of xAI, M. Y. Kim et al. (2021) outline the historical development of explanations and point out that explanations should to be scientific or at least causal. Dazeley et al. (2023) also highlight the importance of causality and introduce the Causal xRL Framework to create causal explanations in RL. They draw on two existing theories. First, Dazeley et al. (2021) suggest that xRL can take place at different orders and that higher level explanations should be incorporated for acceptance of AI systems. Zero-order explanations only consider how an input resulted in an output (base case in xAI). First-order explanations that consider an agent's objective to maximise a reward signal are also relevant. Explaining the agent's intentionality (e.g., the objectives behind the behavior) might improve understanding of the agent, in comparison to only giving zero-order explanations (Dazeley et al., 2021, 2023). Secondly, due to the temporal sequence of transitions in RL, Dazeley et"}, {"title": "2.3 Research Gap", "content": "We did not find many established frameworks for xAI in scheduling. The amount of xRL methods suitable in real-world scenarios is limited (Bekkemoen, 2023; Heuillet et al., 2021). Also, many xAI methods we reviewed were suitable for debugging, but not for non-AI experts. Here, the questions arises which XAI methods may be applied to describe the decisions of a DRL agent in a real-world flow production context (RQ 1.1)? How can they systematically be implemented, applied, and validated (Chi & Liao, 2022)? Every use case poses different challenges for the developer to consider. Explanations may vary according to the questions which arise. Identifying the most suitable xAI method as well as weighing its (dis-)advantages is crucial (RQ 1.2).\nMany efforts in xRL only provide zero-order explanations, which are not comprehensive enough to create trust and acceptance in the AI system and only few approaches include utilizing an agent's objectives or dispositions to create casual explanations (Dazeley et al., 2021, 2023). Adding domain knowledge to XRL is important (Milani et al., 2024), but considering it in xAI approaches for manufacturing is rare T.-C. T. Chen (2023b). Here, the questions arise how domain knowledge as well as context can be included into an explanation. How can xAI-results be processed and presented to stakeholders using this knowledge (RQ 2.2)?\nIn the xAI community, researchers have criticized the lack of unified terminology (Palacio et al., 2021) as well as falsification and missing validity checks (Leavitt & Morcos, 2020; T.-C. T. Chen, 2023b). How can falsifiablity be ensured (RQ 1.1, 2.1)?\nWe fill this gap by developing a workflow based on hypotheses (RQ 2.1), utilizing the domain knowledge (RQ 2.2) of the users and knowledge of the agent's workings. The framework builds upon existing xAI methods and -frameworks and focuses on real world scheduling applications (RQ 1.1, 1.2)."}, {"title": "3 Approach", "content": "In the following section, we illustrate our real-world use case and the scheduling problem formulation, as well as the data set were are working with. Then, we state our XAI workflow with the aim to fill the research gap."}, {"title": "3.1 Preliminaries", "content": "To gain a deeper understanding of our use case, we now formulate the scheduling problem and the MDP to address it."}, {"title": "3.1.1 Real-world case and scheduling problem formulation", "content": "The systematic application of xAI techniques is examined in the context of a real-world use case at a large German manufacturer of household appliances. In previous publications, an extensive scheduling model, along with a specialized DRL agent, has been investigated, implemented and tuned (M\u00fcller, Grumbach, & Kattenstroth, 2024; M\u00fcller, Grumbach, & Sabatelli, 2024). The considered manufacturing process involves a two-stage flow production system. As visualized in Figure 1, the shopfloor consists of a pre-assembly stage (PAS) and a final assembly stage (FAS). In the PAS, a single station produces eight types of semi-finished products (so-called aggregates), which are then finished at one of four possible FAS stations. Between these two stages, there is a limited intermediate buffer where the aggregates are temporarily stored. According to M\u00fcller, Grumbach, and Sabatelli (2024), the key model components of the extended permutation flow shop can be defined semi-formally as outlined below. For a comprehensive mathematical description, refer to M\u00fcller, Grumbach, and Kattenstroth (2024).\n\u2022 Given a set of specific products to be finalized in the FAS, each based on a standardized aggregate of eight different types produced in the PAS.\n\u2022 All PAS and FAS stations can process only one product at a time and all processing times are deterministic.\n\u2022 Each aggregate must be finalized at exactly one of four FAS stations. In this context, each FAS has a predefined schedule containing the sequence of single products to be manufactured.\n\u2022 A FAS station can only start finishing the aggregate when it is available in the central buffer. If the next required aggregate is not available in the central buffer, the FAS station will incur idle times until it becomes available.\n\u2022 In the PAS, all aggregates required by the FAS that are not initially available in the central buffer must be produced.\n\u2022 Switching from one aggregate type to another may cause sequence-dependent setup efforts in the PAS."}, {"title": "3.1.2 Existing RL Approach", "content": "The real world setting stated above needs to be formalized. Several approaches are feasible here. For example, the environment could be modelled using operations research methods. Because of RL-agent's abilities to efficiently find local optima, we chose DRL. The scheduling problem is modelled as an MDP as presented below. For a detailed explanation, refer to (M\u00fcller, Grumbach, & Kattenstroth, 2024).\nState Space\nThe state space is encoded as a vector and comprises following elements:\n\u2022 next_24h_demand_agg for all aggregates: Specifies the demand of the related aggregate for the next 24 hours, taking into account the amount left in the buffer.\n\u2022 end_of_planning_period_demand for all aggregates: Specifies the demand of the related aggregate for the planning period left, taking into account the amount left in the buffer.\n\u2022 buffer_content_duration_agg for all aggregates: Specifies how long the amount of an aggregate type in the buffer will suffice to meet the demands of the FAS if this type is no longer produced in the PAS.\n\u2022 buffer_fill_level: Specifies the fill level of the buffer, i.e., the amount of all aggregates in relation to its total capacity.\n\u2022 last_agg_type_is for all aggregates: Specifies the last type of aggregates produced, represented in one-hot encoding.\nAction Space\nWe define the action space to be discrete. It consists of 8 actions representing the 8 different aggregate types with 0 centered start (e.g., action 7 = agent recommends to build aggregate 8). Each time the agent selects an action, a lot of 50 units of the corresponding aggregate type is produced.\nReward Function\nThe primary objective of the agent is to avoid idle times. Rather than penalizing idle times directly, we penalize criticality, which we define to be the ratio of next_24h_demand_agg to buffer_content_duration_agg. This has proven to be significantly more effective for training, as criticality provides a richer learning signal compared to idle times (M\u00fcller, Grumbach, & Kattenstroth,"}, {"title": "3.2 Domain-knowledge hypotheses to combine falsification, interpretation and communication of xAI", "content": "As we have pointed out, many efforts in xAI, and xRL specifically, provide zero-order explanations and only few include utilizing an agent's objectives or dispositions to create casual explanations (Dazeley et al., 2021, 2023; M. Y. Kim et al., 2021). Also, domain knowledge is not considered adequately in the context of manufacturing (RQ 2.2) (T.-C. T. Chen, 2023b). Thus, we want to utilize the reward function and pair it with domain knowledge (Mohseni et al., 2021; M. Y. Kim et al., 2021; Heuillet et al., 2021; R. Dwivedi et al., 2023; Milani et al., 2024) to create 'higher-order' explanations (Dazeley et al., 2023). Note that following Palacio et al. (2021) the (causal) context and the 'why'"}, {"title": "3.3 The choice of xAI methods for our use case", "content": "Regarding the choice of xAI methods, one has to consider the goal and the target audience (Mohseni et al., 2021; M. Y. Kim et al., 2021; Heuillet et al., 2021; R. Dwivedi et al., 2023). In our use case, we are neither interested in interpreting static components of the model, nor are we trying to create a self-explaining model as is a general goal in some xAI research (Mohseni et al.,"}, {"title": "3.4 The choice of a systematic workflow", "content": "Our approach uses the workflow by Tchuente et al. (2024), who provide a robust structure for guiding empirical investigations with xAI in business applications in general (holistic approach).\nThe workflow by Tchuente et al. (2024) is specifically adapted to xAI models and anticipates changes in the assumptions of the base model. They include a wide range of business parties in their process to mitigate reservations of sceptics."}, {"title": "3.4.1 Business question identification and importance scoring", "content": "The framework by Tchuente et al. (2024) starts with the identification of the business question and its importance (i.e. to the stakeholders). Important (target-) variables may be identified. In order to achieve this, several techniques may be used. It is important to verify that the results stems from a collaboration of many stakeholders to ensure a wide range of opinions and expertise is included in the questions to be answered. The process may be led by engineers or data scientists to ensure that the questions are in fact answerable by algorithms at hand and that the desired objectives are realistic. Lastly, the objectives can be enriched with the sources of the data to help speeding up the next phase. A proper example in this context could be the business question, how the lead time of an exemplary product can be reduced and how these results can be communicated to the stakeholders. Here, the target variable is the lead time. Independent variables could be production times on several machines, dummy variables indicating special product configurations etc. It is important to evaluate what persons may be involved and to include their expertise."}, {"title": "3.4.2 Data collection", "content": "After clarifying the desired objective and variables, the data collection starts. Incorporating Data Engineering here might help with the collection. It makes sense to store batch data at disk and to create a pipeline for data streams to"}, {"title": "3.4.3 Data preprocessing and feature engineering", "content": "Before we can make the data available to the model, we have to handle missing values (Tchuente et al., 2024). The pipeline used here must be transparent and documented to ensure that third parties can follow which data related decisions had been made (Chakraborty et al., 2017). These can highly influence the results of the model and explainer, i.e. if important structures in the data are deleted."}, {"title": "3.4.4 Fitting and validation of the model", "content": "More important than training the model and validating it, the process of choosing the right model and its communication is key (Nyawa et al., 2023). Tchuente et al. (2024) emphasize the process of evaluating which model is the most precise. This highly technical decisions may not be interesting to stakeholders at first sight, but may offer problem misconceptions or valuable insights for others. Communication is key even in this phase. At the end of this thought process, the adequate model or model ensemble is found. Explaining its limitations and interpretability is also crucial.\nAnother possibility is to explain a model which is already live. In this context, one may directly assess which explainers may be suitable."}, {"title": "3.4.5 Testing the model", "content": "This step includes testing the model on the test set. When splitting the dataset into training-, validation- and test set, stakeholders should ensure that the data parts are still representative."}, {"title": "3.4.6 Explanation of model outputs", "content": "In this step, the modeller is choosing the explanation method to be used. If only a whitebox model has been chosen in the latter step, this step can be skipped. There are several methods available for explaining the model results derived (see chapter xAI methods). The approach chosen has to reflect the objectives the stakeholders want to achieve and the data formats which are at hand. Several explainers only support tabular data or images. Orchestrating several explainers to provide more robust explanations may be beneficial. Doshi-Velez and Kim (2017) point out that the opinion of domain experts and practitioners might be useful here, because they can verify at first glance if results should be carried in the next phase. The formatting of the results should be comprehensible for every party involved and may be embellished for other applications."}, {"title": "3.4.7 Robustness checking", "content": "If the explanations hold at first glance, one must check them more thoroughly. Robustness in this context means that the explanations provide consistent results. Unfortunately, inconsistent results are possible (Slack et al., 2019) depending on the model explained. Consistency can be verified by double-checking the explanations of different approaches to see if they reach the same conclusions (Senoner et al., 2021). Additional strategies relying on measuring the ability of the explainer to emulate the behavior of the original model (fidelity, (Chi & Liao, 2022)) or using classic metrics like accuracy are possible."}, {"title": "3.4.8 Validation of explanations", "content": "If the results are robust, they have to be validated by domain experts. Validity can include the degree of applicability in practice. In the end result, the results should \"make sense\" to every party involved in the whole process. Because the assumptions made at the deployment of the model forfeit eventually (concept drift), it is important to iterate over the last two steps in a regular interval."}, {"title": "4 Results and Implications", "content": "We begin by examining the use case to develop hypotheses from a domain-knowledge perspective. These hypotheses are then tested using selected xAI methods. Additionally, we discuss the advantages and disadvantages of the chosen methods. Based on our findings, we propose an adapted workflow and outline the limitations of our process, offering directions for future research. The workflow stems from the approach by Tchuente et al. (2024) and is inspired by their phases Idea, Data and Context."}, {"title": "4.1 Initial exploration of the use case", "content": "To develop a basis for comprehensible and expertise-based hypotheses, an analysis must be carried out in the specific domain. For this, we analyze the data using an exploratory data analysis (Tukey, 1977). The chosen action of the agent can be considered the dependent variable in our dataset, while we treat the other variables as features."}, {"title": "4.1.1 Most produced aggregates", "content": "Aggregate 5 was produced the most, followed by aggregate 8, then aggregate 1. Aggregate 7 was produced the least (Fig 3). The most produced aggregate 5 appeared more than five times as often as the second most produced aggregate 8. The dataset is thus imbalanced regarding aggregate 5 (the dependent variable)."}, {"title": "4.1.2 Last aggregate type", "content": "The last aggregate type feature is a dummy encoded variable (0 or 1) indicating if the predecessor is the same aggregate as produced before. For instance, if last_agg_type_is_agg5 equals 1, the aggregate produced one step before was aggregate 5. It thus encodes the ordering of the aggregates (Fig. 4). Aggregate 5 has been produced first followed by short periods of aggregates 8 and 7. Aggregate 1 also breaks the production cycle of aggregate 5 at index 50. Aggregate 8 is being produced again a second time at the end."}, {"title": "4.1.3 Buffer fill level", "content": "Since the PAS and the FAS are connected via a limited buffer, the buffer acts as a bottleneck that constrains the material flow. The buffer fill level increases over time (Fig. 5). This is intuitive, since the agent causes aggregates to be produced and the PAS has a higher throughput than the FAS. This effect becomes particularly significant later in the planning period."}, {"title": "4.1.4 Buffer fill level for the specific aggregates", "content": "To gain knowledge about specific aggregates, the role of the buffer has to be examined for these aggregate types. The following plot shows the mean buffer content for all aggregates over the whole trajectory (Fig. 6)."}, {"title": "4.1.5 Demand", "content": "Demand is encoded in two features, next_24h_demand and end_of_planning_period_demand. The adjoining table summarizes these:\nIt is important to note that both demand types, next_24h_demand and end_of_planning_period_demand, show the demand left after considering the amount of demand the buffer can satisfy. Therefore, a logical conclusion is that if the buffer for an aggregate increases the demand should decrease. Since the 24h demand is more urgent in a manufacturing environment, those aggregates with a higher next_24h_demand should be more likely to be produced. In fact, we see in Table 1 that only those aggregates with a 24h demand were produced at all. Aggregate 5 with the highest mean demand (both types) was produced the most.\nThe conclusion to be derived from this is that net demand (after deducting the buffer) of an aggregate is positively associated with its production. However, if no aggregate is critical, the agent can continue to produce an aggregate with low demand to minimize setup efforts. Additionally, demand of other"}, {"title": "4.1.6 Criticality", "content": "Criticality $C$ is defined as:\n$C = \\frac{next\\_24h\\_demand}{buffer\\_content\\_duration}$ ,\nwhere buffer_content_duration > 0. Increasing criticality is linked to increasing demand and decreasing buffer content of an aggregate. Criticality is implicitly present in the data, but not explicitly as a feature. We assume that the NN was able to learn this non-linear relationship (Bengio et al., 2013)."}, {"title": "4.2 Generating hypotheses", "content": "The hypotheses to be derived using domain knowledge (RQ 1.1), descriptive statistics and the objectives of the agent can be subsumed under the two parts of the agent's reward function.\nCriticality: High criticality is characterized by high net demand and low buffer content, which is a state the agent tries to avoid, because it increases risk of idle times. Therefore, a high net demand and/or low buffer_content_duration of one of the produced aggregates in the data should be positively associated with production in the xAI methods and vice versa. However, if the demand of the successing aggregate is higher and/or the buffer is lower, the agent should switch to that aggregate and last_agg_type = 1, a higher demand and/or lower buffer of the current aggregate will speak against it being produced again. In Table 2, the criticality for aggregate 5 is zero (the demand is zero and the buffer is halfway full (0.522), but the criticality for aggregate 8 is greater than 1 (there is demand and the buffer content is low). Therefore, the agent switches from producing aggregate 5 to aggregate 8 in the second action.\nSetup efforts: The agent tries to minimize setup efforts, which is indicated by the last_agg_type dummy variable. If an aggregate was produced already (last_agg_type = 1), it should increase the probability that the same aggregate will be produced again. This however can lead to a trend that speaks against the hypothesis implied by criticality: The probability of producing an aggregate may increase, although the buffer of this aggregate is high and the demand is low. This can happen, when no other aggregate is critical; therefore, the agent sticks to the production of one aggregate to minimize setup efforts. If we find the reversed trend and last_agg_type = 1 decreases the probability that the aggregate is produced again, this should correspond with high"}, {"title": "4.3 Applying hypotheses", "content": ""}, {"title": "4.3.1 Aggregate 1", "content": "Aggregate 1 was produced the third most (Fig. 4). In the order of production, it predecessor and successor aggregate was aggregate 5 which was the most produced aggregate.\nRegarding DeepSHAP, we can see that if the last aggregate produced was already aggregate 1, it is more likely that this aggregate will be produced again (Fig. 18) and vice versa. This aligns with the setup efforts hypothesis. Moreover, a lower buffer content is positively associated with production of the aggregate and vice versa, which matches the criticality hypothesis. If the last aggregate type was aggregate 5, aggregate 1 is more likely to be produced. This aligns with aggregate 1 only being produced following aggregate 5 in the course of production. The other variables have SHAP values around zero.\nRegarding Input X Gradient (Table 5), it can be seen that increasing buffer_content_duration_agg1 mitigates the production of aggregate 1. This can be taken one step further by observing that a higher buffer content for aggregate 5 supports the production of aggregate 1. This is in line with the criticality hypothesis, because aggregate 5 is less critical based on a full buffer. In the result, aggregate 1 is produced more. If the last aggregate which was produced is aggregate 1 as well, it has a positive impact on the renewed production of this aggregate. Here the setup-efforts hypothesis is confirmed and matches the data that shows that aggregate 1 was produced continuously without a machine retrofitting. If the end_of_planning_period_demand of aggregate 5 or 8 were increasing, aggregate 1 had slightly less attribution which matches the criticality hypothesis. The demand at the end of the planning period for both aggregate 5 and 8 favoured less production of aggregate 1, but this effect was really low. Both hypotheses categories are thus supported by Input X Gradient for aggregate 1."}]}