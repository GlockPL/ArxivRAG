{"title": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code Completion", "authors": ["Siyuan Jiang", "Jia Li", "He Zong", "Huanyu Liu", "Hao Zhu", "Shukai Hu", "Erlu Li", "Jiazheng Ding", "Yu Han", "Wei Ning", "Ge Li"], "abstract": "Large Language Models (LLMs) have been widely used in code completion, and researchers are focusing on scaling up LLMs to improve their accuracy. However, larger LLMs will increase the response time of code completion and decrease the developers' productivity. In this paper, we propose a lightweight and effective LLM for code completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B achieves higher code completion accuracy while having smaller scales (i.e., 7 billion parameters). We attribute the superiority of aiXcoder-7B to three key factors: Multi-objective training. We employ three training objectives, one of which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers the syntax structures in code and effectively improves the performance of LLMs for code. Diverse data sampling strategies. They consider inter-file relationships and enhance the capability of LLMs in understanding cross-file contexts. Extensive high-quality data. We establish a rigorous data collection pipeline and consume a total of 1.2 trillion unique tokens for training aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a broad distribution of code. We evaluate aiXcoder-7B in five popular code completion benchmarks and a new benchmark collected by this paper. The results show that aiXcoder-7B outperforms the latest six LLMs with similar sizes and even surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B), positioning aiXcoder-7B as a lightweight and effective LLM for academia and industry. Finally, we summarize three valuable insights for helping practitioners train the next generations of LLMs for code. aiXcoder-7B has been open-souced and gained significant attention [1]. As of the submission date, aiXcoder-7B has received 2,193 GitHub Stars.\nIndex Terms\u2014Code Completion, Large Language Model", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have been widely used in code completion [2]\u2013[5], i.e., predicting the subsequent code based on the previous context. For example, GitHub Copilot [6], an LLM-based code completion tool, is regularly utilized by developers from over 10,000 organizations. Nowadays, researchers often improve the accuracy of LLMs by scaling up LLMs, e.g., CodeLlama-70B [2]. However, larger LLMS will increase the response time of code completion, which is a critical factor for developer experience and productivity. Thus, it is necessary to train lightweight LLMs that maintain high code completion accuracy while having smaller scales. Recognizing the above research gap, we present aiXcoder- 7B, a lightweight and powerful LLM for code completion. aiXcoder-7B contains 7 billion parameters, ensuring a high inference speed while achieving superior code completion accuracy. In our later experiments, aiXcoder-7B outperforms the latest LLMs with similar sizes in six code comple- tion benchmarks and even surpasses larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B). aiXcoder-7B effec- tively balances model size and performance, providing a better foundational model for both academia and industry.\nCompared to previous LLMs, we attribute the superiority of aiXcoder-7B to the following three key factors:\n\u2022 Multi-objective training. Previous LLMs mainly use Next- Token Prediction (NTP) as the training objective, which only covers limited code completion scenarios. To address this limitation, we propose multi-objective training, including NTP, Fill-In-the-Middle (FIM), and Structured Fill-In- the-Middle (SFIM). NTP simulates the scenario where developers write a new file from top to bottom, and FIM models the scenario of developers modifying existing code. Because FIM mainly trains models to predict incomplete and irregular code snippets, we further propose SFIM. It parses the code into a syntax tree and mines a relatively complete code span based on a tree node. aiXcoder-7B is trained to predict the code span based on its surrounding context. The three objectives help aiXcoder-7B learn a comprehensive code completion ability across a wider range of code completion scenarios. Details of the multi-objective training are in Section III-C.\nA diverse data sampling algorithm. A code repository of- ten contains multiple code files. Previous studies [2], [4], [5] typically randomly sample files for training, failing to lever- age the relationships and contextual information between files. We propose four new sampling strategies: sampling based on file content similarity, sampling based on file\n\u2022\npath similarity, sampling based on inter-file dependency, and random sampling. The first three strategies simulate common cross-file code completion scenarios, such as code completion augmented by similar code and cross-file API completion, helping aiXcoder-7B better understand and uti- lize dependencies across files. The fourth strategy, random sampling, is to simulate other potential code completion scenarios. Through these diverse sampling strategies, we enhance aiXcoder-7B's understanding capability of cross- file contexts within a repository. Details of our data sampling algorithm are in Section III-B.\nExtensive high-quality data. LLMs are inherently data- driven, and their performance is significantly influenced by the quantity and quality of the training data. We establish a rigorous data collection pipeline, including data crawl- ing, data cleaning, deduplication, code quality checks, and sensitive information removal. We leverage this pipeline to collect a substantial amount of high-quality training data. We continuously feed the training data into aiXcoder- 7B, consuming a total of 1.2 trillion unique tokens. This vast volume of data enables aiXcoder-7B to learn a broad distribution of code data, allowing it to perform exception- ally well across different code completion scenarios. More details of our data collection pipeline are in Section II.\nWe assess the effectiveness of aiXcoder-7B in three code completion tasks, including Fill-In-the-Middle (FIM), cross-file code completion, and Natural language to Code (NL2Code). We experiment with six code completion bench- marks, five of which are popular public datasets and one is FIM-Eval collected by this paper. FIM-Eval is a benchmark for FIM, consisting of 16,136 samples and covering four languages (i.e., Java, Python, C++, JavaScript). FIM-Eval additionally labels the types of code to be completed, including 13 types, e.g., function signatures and comments. Then, we compare aiXcoder-7B to 10 recently released LLMs (from 7B to 34B) on these benchmarks and yield the following insights:\naiXcoder-7B substantially outperforms LLMs with similar sizes and even surpasses larger LLMs in six benchmarks. For example, in a popular benchmark - HumanEval, aiXcoder-7B achieves a Pass@1 score of 54.9%, outperforming CodeLlama- 34B (i.e., 48.2%) and StarCoder2-15B (i.e., 46.3%). The improvements show that aiXcoder-7B achieves higher code completion accuracy while having smaller scales. Based on our FIM-Eval, we analyze the performance of aiXcoder-7B in completing different types of code. aiXcoder-7B outperforms LLMs with similar sizes on most types (max: 13, min: 8). The results show the strong generalization ability of aiXcoder-7B in code completion. \u272a We show that existing LLMs are prone to generate longer code in FIM, while the code generated by aiXcoder-7B is closer in length to human-written reference code. The result shows that the code generated by aiXcoder- 7B is more concise and closer to the human coding style.\nInsights of training LLMs for code. Based on our prac- tices in aiXcoder-7B, we summarize three valuable insights, including scaling up training data and introducing the inter-\n\u2022\n\u2022"}, {"title": "II. DATA COLLECTION PIPELINE", "content": "This section presents the process of collecting the pre- training data of aiXcoder-7B. Figure 1 shows an overview of our data collection pipeline, consisting of five stages: data crawl (Section II-A), data cleaning (Section II-B), data deduplication (Section II-C), code quality checking (II-D), and sensitive and personally identifiable information removal (Section II-E). Through this pipeline, we collect and clean 2.8TB of natural language data and 3.5TB of source code data. Figure 2 visualizes the distributions of the top 10 programming languages in the code data. Next, we describe the details of the data collection pipeline in the following sections.\nA. Data Crawling\nThe pre-training data of aiXcoder-7B consists of two parts: natural language data and source code data.\nNatural Language Data. We collect natural language data from two public datasets: WuDaoCorpora [7] and RefineWeb [8], driven by two key motivations. First, these datasets are highly diverse, covering a wide range of domains and lan- guages. They include a broad spectrum of natural language text from the internet, such as social media conversations, books, and technical papers, and cover two mainstream lan- guages, i.e., English and Chinese. Second, both datasets have been thoroughly cleaned and deduplicated in previous studies, which significantly reduces the preprocessing workload and allows us to focus on processing code data. Finally, we collect 2.8TB of natural language data for pre-training.\nSource Code Data. The raw source code data comes from two sources: one is the open-source dataset - The Stack v1.2, and the other is the code data we crawled ourselves.\nThe Stack v1.2 [9] is a comprehensive dataset comprising approximately 6TB of permissively licensed source code sourced from public GitHub repositories, spanning 358 programming languages, with notable representation from\n\u2022"}, {"title": "B. Data Cleaning", "content": "In this stage, we clean the collected data by removing invalid or low-quality data. Because the natural language data has already undergone rigorous cleaning, we focus on cleaning the source code data. Our cleaning process comprises two steps: repository-level cleaning and file-level cleaning. Below, we provide a detailed explanation of each step.\nRepository-level Cleaning. Our goal is to remove repos- itories with impressive licenses and low-quality repositories. To achieve this goal, our cleaning is performed in three steps:\nCollecting permissive licenses. We build a list of permissive licenses based on the Blue Oak Council\u00b9 and previous work [9]. This list includes various permissive licenses with minimal restrictions on software copying, modification, and redistribution. Only repositories with licenses from this list are retained for pre-training.\nIdentifying repositories' licenses. GHArchive provides li- cense information when repository owners explicitly set the code license through the web interface. We first extract each repository's license from GHArchive. If a license is not listed in GHArchive, we leverage the go-license-detector\u00b2 to identify the most likely license.\nRemoving repositories with impressive licenses. After iden- tifying the licenses, we exclude repositories whose licenses do not appear on the permissive license list.\nRemoving low-quality repositories. We score the repositories from different aspects, including the number of stars, the number of git commits, and the number of test files. Then, we sort the repositories in descending order based on their scores and remove the lowest 10%.\nFile-level Cleaning. Next, we filter out low-quality files in repositories. Specifically, we empirically design some rules to filter out low-quality documents: Trivial files, including empty files, corrupted files, non-text files, and auto-generated files. Too long files. Too long files typically contain wordy or repetitive content and are not suitable as training data. If a line in a file exceeds 1000 characters, the total number of lines in the file exceeds 10,000, or the size of the file exceeds 1MB, we consider it a long file.\n\u2022\n\u2022\n\u2022"}, {"title": "C. Data Deduplication", "content": "Previous work [8] has shown that data deduplication can significantly improve the performance of trained models. It is particularly necessary for code data, where code reuse leads to a large amount of duplicate content. Therefore, in this stage, we eliminate duplicate code files within the repositories. Our deduplication process consists of two steps:\nExact deduplication. We extract file contents, find the files with exactly the same content, and keep only one copy.\nNear deduplication. Exact deduplication is too strict and may cause false positives. Thus, we further perform near deduplication. We compute the MinHash [10] with 256 permutations of all files and use Locality Sensitive Hashing [11] to find clusters of duplicates. We further reduce the clusters by ensuring that each file in the original cluster is similar to at least one other file in the reduced cluster. We consider two files near-duplicate when their Jaccard similarity exceeds 0.85.\n\u2022\n\u2022"}, {"title": "D. Code Quality Checking", "content": "In this stage, we use code analysis tools to assess the quality of code data and filter out low-quality code. Low-quality code often contains syntax errors, code defects, vulnerabilities, and misleading models that generate unreliable code. Specifically, we use the following tools to assess the quality of code:\nSyntax Parser. Syntax correctness is one of the basic principles that source code should satisfy. We use a public syntax parser tree-sitter\u00b3 to parse all code files and delete files that fail to parse or time out.\nSonarQube. SonarQube is an open-source tool for the in- spection of code quality. It can detect code defects, vulnerabil- ities, code smells, and technical debt in various programming languages. We use SonarQube to identify problematic code files and delete them.\n\u2022\n\u2022"}, {"title": "E. Sensitive Information Removal", "content": "In this section, we remove the sensitive information in the pre-training data, e.g., texts involving sensitive topics and personally identifiable information (PII). We remove this information in two steps:\nMatch-based filter. We manually build a list of sensitive words, which covers a broad range of sensitive topics (e.g., politics). Then, we scan all pre-training data and delete the data containing sensitive words.\nModel-based filter. Following previous work [4], we use a Named Entity Recognition (NER) model to identify PII in the data. Specifically, we reuse a trained NER model in previous work [4], which can identify six categories of PII, including emails, names, IP addresses, usernames, passwords, and keys. Then, we replace the detected PII entities with the follow- ing special tokens: <EMAIL>, <NAME>, <IP_ADDRESS>,\n\u2022\n\u2022"}, {"title": "III. MODEL TRAINING", "content": "In this section, we describe the pre-training procedure of aiXcoder-7B, including model architecture, data sampling algorithm, and training objectives.\nA. Model Architecture\naiXcoder-7B is built upon an auto-regressive dense Trans- former architecture [12]. aiXcoder-7B consists of 32 Trans- former decoder layers, with a hidden state size of 4096 and an intermediate size of 14464. More details are in our open-sourced repository [1]. Our tokenizer is trained with Senten- cePiece [13] upon 500GB of training data. The vocabulary size is 49,512. We adopt Rotary Positional Encodings (ROPE) [14] to enhance the representation of positional information in sequences, following [2], [4]. RoPE allows for a more flexible encoding of position than absolute position encoding. Additionally, we implement Grouped Query Attention (GQA) [15], which enhances the efficiency of the attention mechanism by grouping queries, allowing for a more scalable attention computation. We maintain a balanced design in our attention heads, with a configuration of 32 query attention heads and 8 key-value attention heads."}, {"title": "B. Data Sampling Algorithm", "content": "Through the pipeline in Section II, we collect extensive code repositories and natural language articles. We randomly shuffle these repositories and articles and iterate through them. If a natural language article is sampled, we process it into training sequences based on the training objectives (Section III-C).\nIf a code repository is sampled, we design an algorithm for sampling files from the repository, as described in Algorithm 1. The algorithm contains four strategies: sampling based on file content similarity, sampling based on file path similarity, sam- pling based on inter-file dependencies, and random sampling. The first three strategies simulate common cross-file code completion scenarios, such as code completion augmented by similar code and cross-file API completion, helping aiXcoder- 7B better understand and utilize dependencies across files. The fourth strategy, random sampling, is to simulate other potential code completion scenarios. For each repository, the probability of selecting each of the first three strategies is 30%, and the probability of selecting the last strategy is 10%. These sampled files are further converted into training sequences based on the training objectives (Section III-C)."}, {"title": "C. Training Objectives", "content": "The training objectives of aiXcoder-7B consist of the Next- Token Prediction (NTP) and Structured Fill-In-the-Middle (SFIM), detailed as follows.\nNext-Token Prediction (NTP). It is similar to code com- pletion, training models to predict the subsequent token based on the provided context. Given a code file or natural language article x = {xo,x1,...,x\u0131}, NTP trains models predict the next token xi based on previous tokens {x<i}. The objective is to minimize the following loss function:\n$los_{NTP} = - \\sum_{i=0}^{l-1}logp (X_i | X_{t<i})$\nFill-In-the-Middle (FIM) [16]. The motivation behind this training objective is that human developers frequently modify existing code, e.g., inserting new code snippets. Thus, FIM trains models to predict the middle content based on the preceding and following context. Specifically, given a code file or natural language article x = {xo,...,x}, we ran- domly select a span of contiguous tokens as the middle = {xi,...,xj}, using the content above the span as the prefix = {xo,...,xi-1} and the content below as the suffix = {Xj+1,...,x}. We employ two distinct modes to construct the training sequence: PSM (i.e., [prefix; suffix; middle]) or SPM (i.e., [suffix; prefix; middle]). [;] means the con- catenation of multiple strings using special tokens. Previous work [3] has found that models work best when PSM and SPM account for 50% each. Thus, we choose the probability of each mode being 50%.\nFinally, we feed the training sequence into aiXcoder-7B and minimize the following loss function:\n$los_{SFIM} = log p ([prefix; suffix; middle]) \\\\ log p ([suffix; prefix; middle])$"}, {"title": "Structured Fill-In-the-Middle (SFIM)", "content": "As shown in Fig- ure 3, FIM randomly selects spans and trains models to predict incomplete and irregular code snippets (e.g., or i in range(2). However, developers often expect models to complete the current code into a complete snippet, such as a completed code line or loop block, instead of suggesting an incomplete code snippet. To address this, we propose SFIM, which trains aiXcoder-7B to predict complete code snippets, enabling aiXcoder-7B to align with the practical needs of developers. Given a code file, SFIM uses a new strategy for selecting spans: randomly select a function from the file and parse the function into a syntax tree; randomly choose a non-root, non-leaf node from the tree and locate the code snippet corresponding to this node; within this code snippet, randomly select a span, where the start position is randomly determined, but the end position must be the end of a code line. As shown in Figure 3, SFIM selects an If node and mines a relatively complete code snippet (i.e., i%5==0:) as the span. Subsequently, we follow the FIM and convert the select span into a training sequence in the format of PSM or SPM. Based on preliminary experiments, we set the probability of selecting PSM to 30% and SPM to 70%. We input the training sequence into aiXcoder-7B and minimize the following loss function:\n$los_{SSFIM} = - log p ([prefix; suffix; middle]) \\\\ log p ([suffix; prefix; middle])$\nMulti-objective Training. We optimize the above three ob- jectives alternately. Given a code repository, we choose SFIM with a probability of 70% and FIM and NTP with a probability of 15%, respectively. Given a natural language article, we choose FIM and NTP with a probability of 50%, respectively. We determine these probabilities based on previous work [3]\u2013 [5] and our preliminary experiments.\nD. Training Details\nWe leverage Megatron to train aiXcoder-7B. The training process is conducted on 128 A100 40GB GPUs, consuming a total of 1.2 trillion unique tokens. The hyper-parameters used during training are shown in Table I."}, {"title": "IV. STUDY DESIGN", "content": "We design a large-scale study to evaluate the effectiveness of aiXcoder-7B. This section presents the details of our study, including research questions, benchmarks, compared baselines, and evaluation metrics."}, {"title": "A. Research Questions", "content": "Our study aims to answer the following Research Questions (RQs). They evaluate aiXcoder-7B in three code completion tasks, including Natural Language to Code (NL2Code), Fill- In-the-Middle (FIM), and cross-file code completion.\nRQ1: How does aiXcoder-7B perform on NL2Code task compared to existing LLMs? NL2Code is the task of completing the source code based on a natural language description or function signature.\nRQ2: How does aiXcoder-7B perform on Fill-In-the- Middle task compared to existing LLMs? FIM simulate scenarios where developers modify existing code by predicting the missing middle portion using bidirectional contexts.\nRQ3: How does aiXcoder-7B perform on Cross-File Code Completion compared to existing LLMs? This task requires completing code by using relevant context from other files within the current repository.\nIn the RQs, we apply aiXcoder-7B on 6 benchmarks totally. These benchmarks cover 6 programming languages. To show the superiority of aiXcoder-7B, we also select 10 popular LLMs as baselines for comparison. Then, we report the execution-based and text-based metrics (Section IV-D) of completed programs."}, {"title": "B. Compared Models", "content": "Note that our aiXcoder-7B was open-sourced in March 2024. Thus, we select LLMs released before March 2024 for comparison. Specifically, we select 10 popular LLMs for comparison, and they can be divided into two groups:\nLLMs with similar sizes. The first group contains six popular LLMs, which have similar sizes to aiXcoder-7B.\nCodeGen2.5-7B [17], released by Salesforce, is a 7B pa- rameter model specialized in code generation and under- standing, trained on a diverse set of programming languages.\nCodeGeex2-7B [18], developed by Zhipu AI, is a 7B parameter model designed for code completion and bug fixing, leveraging a large corpus of code data.\nCodeLlama-7B [2], an open-source model by Meta AI, is a 7B parameter architecture fine-tuned on a vast collection of code and natural language data based on Llama2 [19].\n\u2022\n\u2022\n\u2022"}, {"title": "C. Benchmarks", "content": "NL2Code Benchmarks. Following previous studies [2]- [4], we select three popular NL2Code benchmarks in our experiments, detailed as follows.\nHumanEval [21] and MBPP [22] consist of 164 and 974 Python programming problems. Each problem includes a function signature, a detailed docstring, and several test cases. LLMs are required to complete the function body based on the signature and docstring. The generated code is checked by executing test cases, being considered correct only if all tests pass.\nMultiPL-E [23] is the multilingual version of HumanEval, covering multiple programming languages, e.g., C++, Java, and JavaScript.\nFIM Benchmarks. Code is rarely composed in a straight- forward left-to-right sequence. Simulating when a developer modifies existing code, FIM refers to the task of completing missing a middle code snippet leveraging bidirectional con- texts.\nSantacoder-data [24] is a popular FIM benchmarks con- sisting of 4,792 samples. It is built from MultiPL-E [23] and requires LLMs to predict a single line of code based on the preceding and following context.\nFIM-Eval is a large-scale FIM benchmark collected by this paper. We construct FIM-Eval from some real-world repositories, which are excluded from the training data of aiXcoder-7B. We extract 13 types of code snippets from these repositories and randomly mine spans from these code snippets. These 13 types of code snippets encompass common code completion scenarios, including method sig- natures, method bodies, single-line statements, methods with comments, empty code blocks, specific positions within a\n\u2022\n\u2022\n\u2022"}, {"title": "D. Evaluation Metrics", "content": "We describe the evaluation metrics used in different code completion tasks.\nNL2Code. NL2Code benchmarks provide test cases for evaluation. Thus, we execute test cases to check the correct- ness of the generated code and report Pass@k [21]. Specif- ically, we generate n \u2265 k code snippets per testing sample, count the number of correct code snippets c \u2264 n that pass all test cases, and calculate the Pass@k:\n$Pass@k := E \\frac{\\binom{n-c}{k}}{\\binom{n}{k}}$\nFIM and cross-file code completion. We consider the LLMs' completions as predictions and the human-written completions as references. We compare the predictions to references and compute the following metric:\nBLEU [26] measures the n-gram similarity between predic- tions and references. n is empirically set to 4.\nCodeBLEU [27] is a variant of BLEU for code. It considers not only the n-gram similarity but also the syntax and data flow similarity.\n\u2022\n\u2022"}, {"title": "A. RQ1: Performance on NL2Code", "content": "Following recent work on LLMs [3], [5], we use greedy decoding and report Pass@1. Table II shows the results of different LLMs on NL2Code benchmarks. From Table II, we draw the following observations:\nCompared to LLMs of similar sizes, our aiXcoder-7B achieves the current best results, outperforming the top- performing model DeepSeekCoder-7B by an average of 9.8%. Moreover, it significantly surpasses CodeGen2.5-7B with a 31% absolute advantage.\naiXcoder-7B even surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B), achieving a lead of 13.1% over CodeLlama-34B, which is nearly five times larger, and 13.7% over StarCoder2-15B on average.\nAcross languages like Java, Python, C++, and JavaScript, our aiXcoder-7B shows strong performance. It surpasses DeepSeekCoder-7B by 16.1% in JavaScript and exceeds by 5.5% in Python.\n\u2022\n\u2022\n\u2022"}, {"title": "B. RQ2: Performance on Fill-In-the-Middle (FIM)", "content": "Generally, FIM closely mirrors how developers modify existing code, making it an ideal method for evaluating models in real-world programming scenarios.\nBased on the experimental findings outlined in Table III, aiXcoder-7B demonstrates the highest overall performance on SantaCoder-data, achieving the best results in Python, JavaScript, and Java among the models tested.\nTable V shows the average generation performance on FIM- Eval. Figure 4 shows the performance of LLMs in predicting different types of code. Based on the results, we obtain the following observations:\nIn real-world programming, aiXcoder-7B performs well in FIM. When evaluated on Java, C++, and JavaScript in FIM-Eval, aiXcoder-7B surpasses DeepSeekCoder-7B by an average of 5.2, 6.7, and 6.4 in FIM metrics for these three languages, highlighting its multilingual versatility. It is highest in C++, exceeding StarCoder2-7B by 11.8.\naiXcoder-7B offers no clear edge over DeepSeekCoder- 7B in Python, likely due to lower training data propor- tion. When calculating CodeBLEU in FIM-Eval, aiXcoder- 7B's score of 63.0 is slightly lower than DeepSeekCoder- 7B's score of 63.4. In several aspects, such as method body top/mid and if statement, it falls behind by up to 10%, indicating the need for a better understanding of method initiations and conditional branches. Moreover, in the SantaCoder-data benchmark, aiXcoder-7B's 83.0% EM of Java is 5.1% lower than the best score of 88.1%. This will be rectified by boosting Python and Java data in training.\n\u2022\n\u2022"}, {"title": "C. RQ3: Performance on Cross-File Code Completion", "content": "Another important capability of LLMs is the ability to understand code context across files, as developers often need to consider information from other files within the current project when writing code. In Table IV, we fix the context length for all LLMs at 16K and format the input using the PSM pattern in FIM. All LLMs employ the greedy search to generate code.\nWe design three experimental settings: Base Setting. As a baseline, LLMs complete based solely on the current file without cross-fire context. Retrieval BM25. Based on the current file context in the base settings, It additionally uses BM25 to match repository code fragments. The top 5 matches, capped at 512 tokens, are added to the prompt, along with formatted class definitions from other files. Retrieval w/Ref. In this setting, we make use of not only the in-file context (as in Retrieval BM25 setting) but also the reference to retrieve the cross-file context. We prepend the retrieved context to the in-file context to construct the prompt for this setting.\n\u2022"}, {"title": "VI. DISCUSSION", "content": "A. Comparison in the Length of Code\nWe propose a novel evaluation perspective in FIM, i.e., comparing the code length between human-written reference code and code generated by LLMs. It is essential not only to ensure that the completed code is functionally correct but also that its length is consistent with what a human programmer would produce.\nTo gain insights into this aspect, we evaluate LLMs' per- formance using FIM-Eval (Section IV-C), which includes a variety of scenarios. Additionally, we present the code length ratio, which is calculated as the ratio of the number of tokens in the prediction to the number of tokens in the ground truth code. Based on the experimental results in Table VI below, we observe that existing LLMs tend to over-generate, producing code that is substantially longer than necessary. Too long code will increase the burden on users and reduce maintainability."}, {"title": "B. Insights of Training LLMs for Code", "content": "Based on our practices in aiXcoder-7B, we summarize the following insights to help practitioners train the next generations of LLMs for code.\nScaling up training data can continuously improve the performance of LLMs. Although the scaling law [29] provides a relationship between model size and the amount of training data, we discover that further scaling up training data is necessary. Even if the training loss of LLMs is already small, continually training with new data still can improve the performance of models. Similar phenomena have also been found in other works [30].\nExploiting the relationships between code files during training can enhance the LLMs' ability to understand cross-file context. In practical applications, LLMs often need to predict code based on cross-file context [25]. Thus, during the training process, we should organize files based on their relationships and train LLMs to understand and utilize the cross-file context. For example, we sample files based on the similarity of their content, which is closer to retrieval-augmented code completion scenarios.\nIncorporating the code structures into the training objectives can improve the performance of LLMs. The code is highly structured. However, previous LLMs [2]\u2013[4] view the code into plain text, ignoring the underlying code structures. This paper first incorporates code structures into the training objectives of LLMs and proposes SFIM. SFIM constructs code spans based on syntax trees of code and trains LLMs to generate accurate and concise code. The results in Section V show the effectiveness of our SFIM. This inspires practitioners to explore new training objectives to model valuable code structures, e.g., control flows and data flows.\n\u2022\n\u2022\n\u2022"}, {"title": "C. Threats to Validity", "content": "We summarize two main threats to this paper.\nData Leakage. A critical threat when training LLMs is the potential inclusion of evaluation data within the training set, which can undermine the reliability of evaluation outcomes. To address this threat, we exclude any data related to our evaluation datasets during data collection. Additionally, the FIM-Eval dataset we constructed and used in our experiments was further emphasized to ensure its independence from the training data. While we cannot guarantee the absence of data leakage in other models due to lack of access, our benchmarks demonstrate that aiXcoder-7B outperforms them reliably.\nThe selection of hyper-parameters. Another threat to the validity of our study lies in the selection of hyperparameters and rules used during the training of aiXcoder-7B, including model architecture hyperparameters, thresholds for data clean- ing, data deduplication parameters, code quality assessment criteria, and sensitive information removal strategies. We se- lected these based on our preliminary experiments and prior empirical knowledge. We did not conduct a comprehensive hy- perparameter search due to the substantial computational costs involved in pre-training, potentially resulting in suboptimal configurations. However, this does not affect our contributions, as future improvements in hyperparameter optimization or heuristic rules can be easily integrated into our framework.\n\u2022\n\u2022"}, {"title": "VII. RELATED WORK", "content": "This section provides an overview of the evolution of LLMS for code. We categorize LLMs into closed-source and open-source models.\nClosed-Source LLMs for Code. One of the earliest no- table breakthroughs is Codex [21", "31": ".", "32": "and Anthropic contributed with Claude3 [33"}]}