{"title": "Personalizing Multimodal Large Language Models for Image Captioning: An Experimental Analysis", "authors": ["Davide Bucciarelli", "Nicholas Moratelli", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "abstract": "The task of image captioning demands an algorithm to generate natural language descriptions of visual inputs. Recent advancements have seen a convergence between image captioning research and the development of Large Language Models (LLMs) and Multimodal LLMs like GPT-4V and Gemini which extend the capabilities of text-only LLMs to multiple modalities. This paper investigates whether Multimodal LLMs can supplant traditional image captioning networks by evaluating their performance on various image description benchmarks. We explore both the zero-shot capabilities of these models and their adaptability to different semantic domains through fine-tuning methods, including prompt learning, prefix tuning, and low-rank adaptation. Our results demonstrate that while Multimodal LLMs achieve impressive zero-shot performance, fine-tuning for specific domains while maintaining their generalization capabilities intact remains challenging. We discuss the implications of these findings for future research in image captioning and the development of more adaptable Multimodal LLMs.", "sections": [{"title": "1 Introduction", "content": "The task of image captioning requires an algorithm to describe a visual input in natural language. Over the last years, researchers have made remarkable progress in developing approaches specifically devoted to image description, with the aim of increasing visual encoding capabilities [5,63], finding proper architectures and multimodal connectors [11,36], and improving linguistic fluency, relevance, and adherence to a desired description style [21,31,48]. These advancements have not only enhanced the ability of models to generate accurate and contextually appropriate captions but have also contributed to bridging the gap between visual understanding and language generation.\nGiven the inherent multimodal nature of the task, the evolution of image captioning research has many times intersected that of Large Language Models (LLMs) [31, 46, 52] and, more recently, it is crossing that of Multimodal LLMs"}, {"title": "2 Related Work", "content": "Standard Image Captioning. Early efforts in image captioning primarily focused on detecting key objects within a scene to populate predefined templates [58, 67]. Subsequent research evolved to employ RNN-based encoder-decoder architectures, where visual input was encoded using a CNN and then exploited to condition the generation process through an RNN [30,63]. These methodologies were further refined with the introduction of attention-based strategies, which applied attention mechanisms to either spatial regions [5] or semantic graphs [66]. Recently, Transformer-based architectures have emerged as the standard in image captioning [19,20,28], often in combination with CLIP-based [51] visual features which demonstrate increased semantics leading to better performance [10,11,36]. Despite being a well-established task in literature, it has historically struggled with generalization and tends to produce very literal captions. In this regard, recent approaches have proposed fine-tuning strategies guided by open-vocabulary metrics [26,54,55] to enhance the descriptive capacity of the models [18,31,48].\nImage Captioning with Multimodal LLMs. In the last year, MLLMs have become predominant in performing a wide range of vision-and-language tasks including visual dialogue, image description, and visual question answering [13]. Almost all existing MLLMs, indeed, adopt large-scale architectures to tackle the challenge of bridging visual and language modalities, connecting a pre-trained LLM with a large-scale visual encoder (i.e. typically CLIP or its variants).\nMLLMs can be categorized considering the type of multimodal connections they employ. Following the widely-used LLaVA model family [41-43], the prevalent strategy in this domain involves using an MLP [65,70] or a single linear layer [16,39] to establish multimodal connections. Several variations have been introduced, such as LLaMA-Adapter [23] that proposes an alternative attention mechanism with zero gating, and the approach introduced by Cha et al. [15] that replaces linear layers with convolutions. Another significant category of models is built upon Q-Former architecture introduced in [34]. On this line, mPLUG-Owl [68] streamlines Q-Former by incorporating a visual abstractor component that condenses visual information into distinct trainable tokens. Similarly, Qwen-VL [8] employs a single-layer cross-attention module with learnable queries to compress visual features. Other approaches integrate dense cross-attention blocks within the existing pre-trained layers of the LLM [3, 7]. This method is often used in conjunction with a Perceiver model [29], reducing the number of visual tokens before their integration into the language model.\nDespite their rapid evolution, the performance analysis of MLLMs in image captioning remains significantly under-explored. Only a few MLLMs are directly trained and evaluated on this task using standard benchmarks, while others treat image description as an inherent capability. On a different line, some recent"}, {"title": "3 Proposed Method", "content": ""}, {"title": "3.1 Preliminaries", "content": "An MLLM usually takes as input a multimodal input, comprising both image and text, and generates a textual output in an autoregressive manner. Formally, the architecture is trained to model a probability distribution p(wt|I, wo, w1, ..., wt\u22121, \u03b8), where \u03b8 denotes the parameters of the model, I represents an input image, and wo, .., wt\u22121 denotes the textual prompt. The textual prompt usually includes a pre-defined system-level prompt and a question related to the input image, given by the user. Clearly, a standard MLLM can only rely on the user prompt, the input image, and the knowledge stored in its internal parameters (i.e. \u03b8) to accommodate requests.\nIn the rest of the paper, we employ LLaVA [43] as our reference MLLM. LLaVA exploits the capabilities of a pre-trained LLM (i.e. Vicuna [17]) and a pre-trained visual model (i.e. a CLIP-based visual encoder [51]), which are interconnected through an MLP adapter, in charge of converting CLIP features to dense input tokens. For an input image I, therefore, LLaVA utilizes a pre-trained CLIP visual encoder Ev, extracts a dense grid of visual features Z = E(I), which is then projected via a learnable MLP to produce a sequence of dense embedding tokens vo, v1, ..., vN . Finally, these are prepended to the system prompt, and the full sequence of visual and textual tokens is then given as input to the LLM component of the model."}, {"title": "3.2 Personalization Strategies", "content": "To adapt MLLMs to specific description styles and semantic domains, we investigate several parameter-efficient fine-tuning (PEFT) techniques.\nPrompt Learning. To adapt the MLLM to perform classical image captioning, the most straightforward option is to enrich the input context by injecting learnable vectors into its embedding. This is usually done by adding new embedding vectors to an existing prompt, which are initialized from scratch and trained through stochastic gradient descent. In our preliminary experiments, however, we found it beneficial to fine-tune the user prompt and the system prompt embeddings, rather than injecting new embeddings which might be more complex to initialize. Formally, the distribution of the MLLM is conditioned on visual tokens, a system prompt, and a trainable user prompt, leading to\np(wt|\u03c5\u03bf, \u03c51, ..., \u03c5N, eo, e1, ..., el, WO, W1, ..., Wt-1), (1)\nwhere eo, ..., el, represents the trainable embeddings of the user prompt. The set of trainable parameters \u03b8*, in this case, is simply \u03b8* = {eo, ..., el}. Differently from the standard formulation of MLLMs, by fine-tuning a portion of the input context, we allow the model to generate more specific answers.\nPrefix Tuning. Differently from the previous case, in this case we add a sequence of learnable embeddings to every layer of the Transformer decoder of the MLLM. While this formulation does not allow a straightforward meaningful initialization of the embeddings, like in the case of prompt learning, it comes with the advantage of injecting trainable knowledge at different layers of the architecture, which might increase the degree of adaptation of the model. Formally,"}, {"title": "4.4 Computational Analysis", "content": "Finally, in Table 4 we present a computational and energy consumption analysis for the fine-tuning strategies under consideration on LLaVA-v1.5. For each PEFT strategy, we report the number of trainable parameters along with the energy consumed during training, measured in Kilowatt-hours (kWh). Energy consumption is detailed both for the entire training process on the COCO training split (i.e. four epochs in our experiments) and for the epochs up to the best checkpoint, selected based on validation loss. As it can be seen, prompt learning and prefix tuning are the least computationally demanding strategies. Furthermore, while training with LoRA or DORA consumes a similar amount of energy as full fine-tuning when considering all epochs, DORA generally converges in fewer iterations, leading to lower overall energy consumption."}, {"title": "5 Conclusion", "content": "This paper has explored the intersection of image captioning and the rapidly evolving landscape of Multimodal LLMs, assessing their potential as effective"}]}