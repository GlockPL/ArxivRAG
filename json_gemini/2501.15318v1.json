{"title": "A Post-Processing-Based Fair Federated Learning Framework", "authors": ["Yi Zhou", "Naman Goel"], "abstract": "Federated Learning (FL) allows collaborative model training among\ndistributed parties without pooling local datasets at a central server.\nHowever, the distributed nature of FL poses challenges in training\nfair federated learning models. The existing techniques are often\nlimited in offering fairness flexibility to clients and performance.\nWe formally define and empirically analyze a simple and intuitive\npost-processing-based framework to improve group fairness in FL\nsystems. This framework can be divided into two stages: a standard\nFL training stage followed by a completely decentralized local de-\nbiasing stage. In the first stage, a global model is trained without\nfairness constraints using a standard federated learning algorithm\n(e.g. FedAvg). In the second stage, each client applies fairness post-\nprocessing on the global model using their respective local dataset.\nThis allows for customized fairness improvements based on clients'\ndesired and context-guided fairness requirements. We demonstrate\ntwo well-established post-processing techniques in this framework:\nmodel output post-processing and final layer fine-tuning. We evalu-\nate the framework against three common baselines on four different\ndatasets, including tabular, signal, and image data, each with vary-\ning levels of data heterogeneity across clients. Our work shows\nthat this framework not only simplifies fairness implementation in\nFL but also provides significant fairness improvements with min-\nimal accuracy loss or even accuracy gain, across data modalities\nand machine learning methods, being especially effective in more\nheterogeneous settings.", "sections": [{"title": "1 Introduction", "content": "In Federated learning (FL), multiple parties (clients) collaboratively\ntrain a model without sharing their local data [34]. In addition\nto offering a way to use distributed data and compute for ma-\nchine learning, federated learning is also investigated as a potential\nprivacy-enhancing technology [56]. As a result, FL has attracted\nmuch attention [54], with proposed applications in domains includ-\ning healthcare, mobile devices and industrial engineering [24].\nFair machine learning refer to a set of techniques to make ma-\nchine learning models produce more fair outputs [5]. For example,\nin some application contexts, the output of a binary classifier, that\nhas been trained with historical data using machine learning, may\nneed to satisfy certain fairness properties like conditional indepen-\ndence from attributes like gender and race [17, 52].\nThe literature in fair machine learning focuses mostly on cen-\ntralized training setting, i.e. when all data used for training the\nmodel is located centrally. There has been recent work on under-\nstanding the challenges of fairness in federated learning setting as\nwell. Federated learning inspired techniques train a global model\nwith fairness constraints when data is distributed across multiple\nlocations [2, 11, 53, 55]. Although these methods can be effective in\nimproving fairness in FL, they also have a few limitations. Firstly,\nthese methods often lack fairness decentralization. Clients are re-\nquired to agree on (a) the same fairness definition and the same\nfairness measurement, and (b) the same fairness requirements in-\ncluding the choice of sensitive attributes and protected groups, the\nlevel of fairness-accuracy trade-off, etc. The requirement is problem-\natic for several reasons. One is that clients may have different needs\ndepending on their specific use cases. Another reason is that clients\ncould have very different data distributions due to data heterogene-\nity in FL. A single fairness constraint may not be suitable or work\neffectively on all clients, which may produce unpredictable model\nbehavior on certain clients. Secondly, many of these approaches\nrequire clients to use their statistics of sensitive attributes during\ndistributed communication, which may raise additional privacy-\nrelated concerns. Thirdly, imposing additional fairness-related con-\nstraints during global model training may also increase the network\ncommunication costs and slow down the training process, reducing\nthe overall training efficiency.\nTo address these limitations, we explore a very simple post-\nprocessing-based fair FL framework. In this framework, all clients\nfirst collaborate to perform global FL training without any fair-\nness constraints. Once the global training is finished, each client\nperforms local fairness or debiasing post-processing on its own\ndataset, tailoring the debiasing process towards its specific data\ndistribution and requirements. This method has a high flexibility\nand decentralization as it allows clients to apply different fairness\nconstraints based on their specific needs, providing customized\ndebiasing strategies. It also addresses data heterogeneity since the\ndebiasing is performed locally, it adapts fairness to the local data\ndistribution. Moreover, no information about sensitive attributes\nis required during the global training stage, thus offering better\nsensitive data protection. This also improves training efficiency as\nthere is no additional network communication cost for debiasing.\nWe demonstrate how post-processing debiasing methods from\nprior work can be integrated in this framework. In our experiments,\nwe use two post-processing methods, but other methods can also be\nsimilarly integrated. One is model output post-processing [17]. This\nmethod addresses fairness by solving a linear program and com-\nputing a derived predictor. It does not need to change the original\nmodel weights and instead post-processes the model output. The"}, {"title": "2 Related Work", "content": "In this section, we discuss prior work that are most closely related\nto this paper. For readers interested in a more comprehensive back-\nground in federated learning and machine learning fairness, we\nrefer to recent surveys (such as [35, 54]).\nFederated Learning. In federated learning (FL) [34], different par-\nties which are often called \"clients\" can collaborate with each other\nvia a \"server\" without sharing their training data. During each\nglobal training round of FL, each client computes gradient updates\non its local dataset, and then, instead of sharing their data directly,\nthey only share the updates with the server. The server aggregates\nthe updates from all the clients to compute an updated global model\nweights, and sends the new global modal to all clients for them to\ncompute a new round of updates. This procedure repeats for a few\nglobal rounds until the global model reaches a certain performance\nor converges. Note that this process involves not only computation\ncosts (for computing updates and aggregation), but also network\ncommunication costs (for sharing updates). Communication is often\na bottleneck in distributed computation systems.\nOne of the other major challenges in FL is that of data hetero-\ngeneity across clients [36]. In many real-world use cases of FL,\ndifferent clients follow very different data distributions. For exam-\nple, in healthcare domain, different healthcare institutions might\nwant to utilize all of their data to train a more accurate model for"}, {"title": "3 Preliminaries", "content": "We consider the following problem setting. There are K clients\nand one server participating in the FL training. Each client Ck\n(k\u2208 {1, .., K}) has a local dataset Dk. And all local datasets together\nform the global set D = UkDk. The distributions of Dk may differ for\ndifferent k, simulating heterogeneous data setting across different\nclients.\nOur framework can be divided into mainly two stages, the train-\ning stage and the debiasing stage. Only the training stage requires\nmessage exchange between clients and the server, and the debiasing\nstage is performed fully locally, which means only communication\nrounds required for the basic FL framework are needed in our\nframework and there are no extra communication costs.\nThe training stage of this framework follows the training proce-\ndure of standard FL setting in FedAvg [34]. The general objective\nfunction in FedAvg can be written as follows (Equation 1):\n$f(\\theta) = \\sum_{k=1}^{K} w_k l_k (\\theta)$\nwhere \\theta denotes the model parameters, lk denotes the local objec-\ntive function on client k. The local objective function is fairness\nunaware, e.g. a vanilla loss function like cross-entropy loss. Mini-\nmizing function f (\\theta) finds model parameter \\theta that minimizes the\nweighted average of the local model losses across all clients.\nIn the debiasing stage (which comes after the first stage is fin-\nished), our framework sends the global model to each client for\nthem to evaluate the model locally on their dataset Dk, and apply\ndifferent post-processing debiasing methods based on their local\nfairness constraint Fk. In our experiments, we use Equalized Odds\n(EOD) as fairness metrics, but note that due to the decoupling of\nglobal model training and debiasing in our framework, different\nclients are not required to follow the same fairness definition or\nconstraint. Clients can adjust to different fairness metrics, differ-\nent levels of fairness-accuracy trade-off, even different sensitive\nattributes based on their specific requirements. Clients can also\nuse different post-process debiasing methods methods. Before dis-\ncussing how we integrate post-process debiasing in this framework,\nwe discuss EOD metric more formally.\nEqualized Odds (EOD) [17] defines fairness as groups with dif-\nferent sensitive attributes having the same true positive rate (TPR)\nand false positive rate (FPR). For evaluating and comparing EOD"}, {"title": "4 Post-Processing-Based Fair Federated Leaning\nFramework Details", "content": "In this section, we present the post-processing-based framework\nwith two example post-processing debiasing approaches: output\npost-processing method for any binary classifier [17] and final\nlayer fine-tuning method for deep neural networks [31]. For both\napproaches, the global training process is based on the basic FedAvg\ntraining without fairness constraints as discussed above in Section\n3. After the global training phase has finished, clients perform local\npost-processing methods on their copy of the global model."}, {"title": "4.1 FL Model Output Fairness Post-Processing", "content": "In Algorithm 1, we present the pseudo-code of our framework with\nFL model output fairness post-processing.\nAlgorithm 1 starts by general FL training in lines 1-8 [34]. At\nthe termination of the global for loop, each client k \u2208 1,..K has\nreceived a trained FL model with weights \u03c9\u03c4. In line 10, predictions\nYk are computed using the global model wy on the local dataset\nDk for each client Ck (k = 1,2,...K). In line 11, each client com-\nputes a derived predictor pk based on the prediction Yk and the\nlocal dataset Dk. The method of obtaining the derived predictor\nis adopted from Hardt et al. [17]. Before discussing the method of\nobtaining derived predictor, we stress that that each client com-\nputes a derived predictor separately based on their local context and\nrequirements. For brevity, we do not distinguish between clients\nin the pseudo-code of Algorithm 1 (beyond the distinction in their\ndatasets Dk). In principle, each client can also use different fairness\ndefinitions and post-processing methods in lines 10-11 and if a"}, {"title": "4.2 FL Model Final Layer Fairness Fine-Tuning", "content": "In Algorithm 2, we present the pseudo-code of our framework with\nFL model final layer fairness fine-tuning.\nLines 1-8 are the same as Algorithm 1 since this is the global\nFedAvg model training stage. Each client receives a copy of the\nglobal model with weights \u03c9\u03c4 at the end of this stage. In lines 9-15,\nwe show the decoupled debiasing stage that is executed indepen-\ndently for each client. As was the case in the model output fairness\npost-processing method discussed previously, clients can decide\ntheir fairness definitions, metrics etc or skip fairness enforcement\ndepending on their local context. The distinction between clients is"}, {"title": "5 Experiment Settings", "content": "Next, we discuss the experiments settings for our empirical analysis\nof the the strengths and limitations of the framework using different\nreal-world datasets. We also compare performance compare with\ndifferent baselines."}, {"title": "5.1 Datasets", "content": "We first introduce the datasets used in our empirical analysis. We\nuse two tabular datasets that are widely used in fair machine learn-\ning literature, namely Adult [6] and ProPublica COMPAS [33]. In\naddition, we use two other datasets that are both larger in size and\nmore complex in structure compared with the tabular datasets. One\nof them is an ECG signal dataset called PTB-XL [47], and the other\nis a chest X-ray image dataset called NIH Chest X-Ray [49].\nAdult Dataset. Adult dataset [6] is a popular dataset for individ-\nual's annual income prediction, and it is widely used in fairness\nevaluation of machine learning algorithms. The target label in the\nAdult dataset is \"income\" which is divided into two classes (\"<=50K\"\nand \">50K\") for binary classification task. We use \"sex\" as the sensi-\ntive attribute with \"male\" as 1 and \"female\" as 0 in our experiments.\nWe use one-hot encoding for all categorical features and apply"}, {"title": "5.2 Data Split", "content": "To simulate the data distributions across different clients, we first\npartition the whole dataset to form local datasets without over-\nlap. Then for each client, we split the local dataset into a training\nset and a test set, and only the training set is used during model\ndevelopment."}, {"title": "Clients Split", "content": "We use a 4-client setup for all the datasets [45]. To generate hetero-\ngeneity among clients, we use Dirichlet Distribution to sample data\npoints, which is a commonly used in fair FL literature [15, 45, 50].\nWe sample pi Dir(a) for each client i \u2208 {1, .., 4}, where a is a\nparameter controlling the level of data heterogeneity. Smaller a\nprovides a more heterogeneous distribution, and as a \u2192 \u221e, the\ndata distribution gets close to homogeneous and can be considered\nas random split [15].\nTo simulate different levels of data heterogeneity from extremely\nimbalanced to (very close to) random split, we start with a = 0.5, 1,\n5, 100, 500 in Section 7.1. After comparing a method's performance\nunder these different settings, we restrict to a = 0.5, 5, 500 for the\nfollowing experiments since these appeared to be three broadly\ndistinguishable heterogeneity levels.\nInitially, we also considered a = 0.1 as one of the heterogeneity\nsettings. However, we found that due to the small dataset size and\nimbalanced label split, there would be missing labels within some\ngroups with a = 0.1. For example, Table 5 shows an example data\ndistribution on the COMPAS dataset generated from the Dirichlet\ndistribution Dir(a) with a = 0.1. We can see that the data partition\nis very imbalanced with only one sample or no samples for certain\ngroups. This creates problems in our experiments (e.g. inability to\neven measure fairness on some clients). Therefore, we consider a =\n0.5 as the highest level of data heterogeneity for our experiment\nsetting. Similarly, we observed the data heterogeneity with a =\n500 to be very small and quite close to random splits. Thus we\nwill consider a = 500 as the lowest level of heterogeneity in the\nexperiments."}, {"title": "Train/Test Split", "content": "We split train and test sets on each client independently. On each\nclient, we use 80% samples as training set and 20% as test set. The\nglobal model is trained using all local training sets in the first stage\n(i.e. federated training). During the fairness post-processing and\nfine-tuning stage, we use local training set at the respective client."}, {"title": "5.3 Baselines", "content": "FedAvg: FedAvg refers to the originally proposed FL training\nframework without any fairness considerations [34]. Each client\ncomputes its local update and sends it to the server for aggregation\nand global model update.\nFairFed: As discussed in Section 2, FairFed [15] is based on the\nFedAvg framework but it uses a fairness-aware aggregation.\nFairFed + Fair Representation (FairFed/FR): . Fair Representation\n[51] is a pre-processing method which debias the input features by\nremoving their correlations with sensitive attributes. We include\nFairFed combined with FairRep [51] as one of our baselines, for\nthe reason that the authors of the FairFed paper [15] presented\nthe performance of FairFed combined with local pre-processing.\nUnder their experimental setting, FairFed works well with Fair\nRepresentation with heterogeneous data distributions too."}, {"title": "5.4 Evaluation Metrics", "content": "We now describe the evaluation metrics we used in experiments for\nperformance comparisons between different methods, following\nconventions in the literature.\nAccuracy. We use the local (client-level) test accuracy as one of\nthe main performance evaluation metrics on the tabular datasets\nAdult and COMPAS. It can be calculated using Equation 5:\n$Accuracy = \\frac{Number \\ of \\ correct \\ predictions}{Number \\ of \\ total \\ samples} = \\frac{TP + TN}{(TP+FN+FP+TN)}$\nBalanced Accuracy. For PTB-XL and NIH Chest X-Ray, we use\nbalanced accuracy [9] instead of accuracy for model performance\nevaluation. The reason is that health related datasets are often very\nimbalanced with labels for disease prediction or other classification\ntasks. In most cases, within a given dataset only a minority of\npatients is diagnosed with the specific disease, and the prediction\naccuracy can be high even if the model just predicts every sample\nas \"No disease\". Balanced accuracy addresses this issue by including\nboth sensitivity (TPR) and specificity (TNR) (Equation 6), providing\na more reliable measurement for imbalanced datasets. It is widely\nused in health ML literature such as [19, 28].\n$Balanced \\ Accuracy \\ (BA) = \\frac{Sensitivity \\ (TPR) + Specificity \\ (TNR)}{2}$"}, {"title": "6 Model Development", "content": "Adult and COMPAS tabular datasets are generally evaluated with\nsimple architectures in the literature [52]. We use a simple one-\nlayer model to train on the Adult and COMPAS datasets for our\nmethods and all the baselines. The activation function ReLU [4] is\nused in the model. We use stochastic gradient descent (SGD) [42]\nas the optimizer and Binary Cross Entropy Loss (BCELoss) [30] as\nthe loss function when training on Adult and COMPAS.\nFor the ECG dataset PTB-XL, we used a ResNet-based model\n[27] with residual blocks [18] to handle the uni-dimensional ECG\nsignals. The model is composed of one convolutional layer and five\nresidual blocks. Each residual block consists of two convolutional\nlayers by batch normalization [21], activation function ReLU [4]\nand Dropout regularizer [46]. We use Adam optimizer [22] with\nweighted mean square error loss function for the ECG dataset.\nIn the case of NIH Chest X-Ray dataset, we initialize our model\nwith the pre-trained model MobileNetV2 [44]. The MobileNetV2\nmodel is trained on Imagenet. Prior works such as [41] have shown\nthat utilizing pre-trained models (including MobileNet) for initial-ization is useful for classification tasks on X-Ray datasets.\nCode Availability. To supplement the above information, we also\nprovide our implementation/code in the supplementary material.\nCode will be available on GitHub https://github.com/Yi-Zhou-01/\nfairpostprocess-fedml. As we will discuss in the next subsection,"}, {"title": "6.2 Hyperparameters", "content": "The hyperparameters we used for methods on different datasets can\nbe found in Table 6. Note that although we only listed three methods\nin the table (FedAvg, FairFed and FT), all the methods used in the\nexperiments are covered. The FL training stage of our methods\nPP (output post-processing) and FT (fine-tuning) are the same as\nFedAvg, and PP does not require hyperparameter tuning. FairFed/FR\nuses the same parameters as FairFed since Fair Representation is a\npre-processing method and does not require extra hyperparameter\ntuning."}, {"title": "6.3 Libraries and Computational Resources", "content": "We use Python as the programming language and PyTorch [39] for\nthe machine learning model development and experiments. We also\nuse IBM AIF 360 [7] for the output post-processing approach and\nfor fairness evaluation. Besides, libraries including sklearn, numpy,\npandas, Pillow [1], and H5py are also used in the experiments for\ndata processing and model training. For training on signal and\nimage datasets, we use NVIDIA Tesla V100 for GPU acceleration."}, {"title": "7 Results and Discussion", "content": "In this section, we first explore the impact of data heterogeneity\nlevel on the performance of different methods. We partition the\nCOMPAS dataset across four clients using Dirichlet distribution\nDir(a) with a = 0.5, 1, 5, 100, 500, and evaluate the performance of\ndifferent methods. The third row in Figure 3 shows the distribution\nof data labels across clients under different heterogeneity settings.\nAs expected, lower values of a (e.g., a = 0.5 and a = 1) result in\nmore imbalanced data partitions both in terms of label distribution\nand dataset size. In contrast, higher values (a = 100 and a = 500)\ngenerate more balanced splits, with a = 500 closely simulating an\nidentically distributed setting, mimicking random data splits across"}, {"title": "7.2 Performance on COMPAS Dataset", "content": "In Table 7, we present the client-wise performance of our framework\nwith output post-processing (PP) method compared to the baselines,\nunder varying data heterogeneity. In general, our framework with\nPP shows a significant improvement in fairness at all clients as well\nas in the weighted average of local fairness under all heterogeneity\nsettings. In contrast, the FairFed and FairFed/FR baselines show\nlower fairness improvements. The performance of the baselines in\nis also less consistent, which suggests the sensitivity of FairFed-based\napproaches to experimental conditions. However, the improvement\nin fairness in PP comes at a cost of relatively more drop in accuracy.\nPerformance under extreme heterogeneity. Under extreme hetero-\ngeneity setting (a = 0.5), our framework with PP, outperforms\nall the baselines in fairness improvement across clients and also\nshow a significant improvement in weighted averaged EOD (~79%\nimprovement over FedAvg).\nPerformance under medium to low heterogeneity. When the data\npartitions are less heterogeneous (with a = 5 and a = 500), PP\nbecomes slightly more effective in improving fairness with an im-\nprovement of ~ 85% for a = 500 over FedAvg.\nTraining cost. Table 7 shows that our framework with PP have the\nsame number of communication rounds as the basic FL framework\nFedAvg. This is because the FL training stage of our method is\nadopted from FedAvg and the debiasing stage is performed locally\nand does not need message exchange.\nThe FairFed-based baselines also happen to have the same num-\nber of communication rounds as FedAvg for each global round\nof training in this dataset, but as we will see in the case of other\ndatasets, this is not always guaranteed. The communication cost per\nglobal round is also in theory higher in baselines because fairness\nrelated information is also exchanged in these rounds.\nWe finally note the total training time of baselines and our frame-\nwork. We report the time after rounding and do not show decimal\npoints for brevity. As we can see that our framework with PP has\nalmost the same time as the FedAvg (i.e. fairness comes in our\nframework with PP at almost no time cost). On the other hand, the\ntime is higher for FairFed based baselines. Due to computational"}, {"title": "7.3 Performance on Adult Dataset", "content": "Compared with the COMPAS dataset, the Adult dataset is larger\nin size but more imbalanced in terms of label and sensitive at-\ntribute distribution with only 3.6% of samples having label = 1 and\nsensitive_attribute = 0 (Table 1). Table 9 shows a label distribution\nwith a = 0.5 on Adult dataset for clients.\nAs shown in Table 8, our framework with PP shows its effective-ness in improving fairness under all heterogeneity and outperforms\nall baselines on Adult too.\nPerformance under different heterogeneity level. Compared with\nthe baselines, PP achieves the highest fairness improvement under\nall heterogeneity settings with some accuracy decrease. FairFed\nprovides a much smaller EOD improvement while maintaining a\nsimilar accuracy level as FedAvg. FairFed/FR generally appears to\nperform worse in terms of fairness-accuracy trade-off.\nTraining cost. A larger difference in training time between our\nmethods and FairFed-based baselines can be found in this exper-iment compared with the COMPAS experiment, mainly because\nof the larger dataset size of Adult dataset. The time spent on the\npost-processing stage in PP is still negligible resulting in almost\nthe same running time after rounding as FedAvg. FT method is also\nefficient with less than half of the training time of FairFed method.\nIn contrast, FairFed takes two to three times the running time as\nit needs to calculate a new global fairness metric and metric gap\nbetween global and local fairness metrics. In addition, we note that\nour method require a smaller number of communication rounds. As\ndiscussed previously recall that communication cost per commu-nication round too is smaller for our method compared to FairFed\nbased baselines."}, {"title": "7.4 Performance on PTB-XL ECG Dataset", "content": "Table 10 presents the performance comparison of different methods\non PTB-XL ECG dataset under different heterogeneity settings.\nNote that, for both PTB-XL ECG and NIH-Chest X-Ray datasets,\nFairFed/FR baseline will not be included in the results. The Fair Lin-ear Representation (FR) approach is based on correlations between\ndata features and sensitive attributes and, to the best of our knowl-edge, does not apply to signal or image datasets [51]. Therefore, we\nwill only use FairFed and FedAvg as our baselines for the following\nexperiments. On the other hand, as promised in Section 7.1, now\nwe will also include the results for final-layer fine-tuning method\n(FT) with our framework in addition to results with the PP method.\nModel Output Post-Processing (PP). Our framework with model\noutput post-processing (PP) for fairness is still very effective on\nmore complex datasets and models and outperforms all the base-lines in fairness improvement. We can see from Table 10 that the\neffectiveness of PP is not influenced much by the change of data"}, {"title": "7.6 Summary of Observed Strengths and\nLimitations", "content": "Model Output Post-Processing: In the analyses of our framework\nwith PP (i.e. model output post-processing) method, we observed\nseveral of its strengths. Firstly, this method shows its effectiveness\nwith significant fairness improvement across all datasets under\nvarious heterogeneity settings. It is especially effective when the\noriginal model or data contains relatively large EOD or when local\ndatasets are highly heterogeneous. Secondly, this method is very\nefficient to apply because it is very fast to compute the derived\npredictor and requires no hyperparameter tuning. Additionally, it\nrequires minimal computational resources on local clients, making\nit a low-cost solution (i.e. no GPUs required). Thirdly, the derived\npredictor generated by PP is human-interpretable (i.e. clients can\nsee how predictions are changed for fairness). On the negative\nside, we observed that PP comes with drop in accuracy in nearly\nall cases. While a drop in accuracy is commonly observed in the\nfairness literature and may often be due to data characteristics and\nassumptions [13], but it also shows the importance of considering\napplication and context in employing any fairness intervention. As\nwe discussed in Section 5.1, application grounded discussions with\nethical, legal, domain experts and various stakeholders must be\ntaken into account to select the right fairness intervention.\nFinal Layer Fine-Tuning: In the analysis of our framework with\nFT (i.e. fair final layer fine-tuning) method, we found it to be also\neffective in improving model fairness. It improves the EOD under\nall heterogeneity settings for most datasets. Similar to PP, this ap-\nproach tends to achieve better fairness improvement with more\nheterogeneous data across clients. In contrast to PP, fine-tuning\nalso provides a better model accuracy under more heterogeneous\nsettings on most datasets. Notably, in cases of extreme data het-erogeneity, fine-tuning often improves both fairness and accuracy\nsimultaneously. While it does not always deliver as large a fairness\nboost as PP, it delivers fairness improvements with minimal or no\nimpact on model performance.\nWhile not as computationally inexpensive as PP, FT is still ef-ficient because only the last layer of the model is updated, while\nthe other layers are kept fixed. This ensures that even with large\nmodels like the deep neural network (DNN) we used in the NIH\nChest X-Ray experiments, the time required in this method is still\nbetter compared to other fairness baselines. Moreover, since the\nfine-tuning procedure is performed fully locally, there are no addi-tional communication costs. Local clients can also tune their local\nmodel to suit their specific fairness requirements by tweaking pa-rameters such as aft and the number of local training rounds. This\nprovides a more flexible fairness option for clients compared to the\nPP approach.\nHowever, this dependence on the fine-tuning parameter aft also\nmeans that achieving the optimal performance requires tuning\nthe hyperparameters, which adds to the cost and complexity of\napplying it, especially on large datasets with increasing complexity."}, {"title": "8 Conclusions and Future Work", "content": "In this work, we formally defined a simple post-processing-based\nfair federated learning (FL) framework, aiming to improve group\nfairness for each of the clients. This framework relies on well-established FL training procedure and fairness post-processing ap-proaches commonly used in centralized machine learning, allowing\neach client to independently apply fairness methods on their local\ndata. We demonstrated the framework with two different fairness\npost-processing techniques: model output post-processing and final\nlayer fine-tuning. Through comprehensive experiments on four dif-ferent datasets (tabular, ECG, X-Ray) and with varying degrees of\nclient data heterogeneity, we analyze the strengths and limitations\nof this framework. The framework decentralizes fairness enforce-ment by providing the clients with a computationally efficient way\nof obtaining fairer FL models with the flexibility of choosing differ-ent fairness definitions and requirements guided by local context\nand application needs.\nFuture work can consider other group fairness definitions that\nwere not covered in our work and individual fairness [14]. Sim-ilarly, settings beyond binary classification can be considered in\nfuture work (e.g. non-binary classification tasks or bias in large\ngenerative Al models etc). In our experiments, we only consid-ered heterogeneity across clients due to their local datasets. This\ncan be supplemented in future work by considering other kinds\nof differences (for e.g. different clients optimally selecting hyper-parameters for final layer fine-tuning method for their local context\nand datasets)."}]}