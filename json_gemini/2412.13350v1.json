{"title": "A Novel Machine Learning Classifier Based on Genetic Algorithms and Data Importance Reformatting", "authors": ["A. K. Alkhayyat", "N. M. Hewahi"], "abstract": "In this paper, a novel classification algorithm that is based on Data Importance (DI) reformatting and Genetic Algorithms (GA) named GADIC is proposed to overcome the issues related to the nature of data which may hinder the performance of the Machine Learning (ML) classifiers. GADIC comprises three phases which are data reformatting phase which depends on DI concept, training phase where GA is applied on the reformatted training dataset, and testing phase where the instances of the reformatted testing dataset are being averaged based on similar instances in the training dataset. GADIC is an approach that utilizes the exiting ML classifiers with involvement of data reformatting, using GA to tune the inputs, and averaging the similar instances to the unknown instance. The averaging of the instances becomes the unknown instance to be classified in the stage of testing. GADIC has been tested on five existing ML classifiers which are Support Vector Machine (SVM), K-Nearest Neighbour (KNN), Logistic Regression (LR), Decision Tree (DT), and Na\u00efve Bayes (NB). All were evaluated using seven open-source UCI ML repository and Kaggle datasets which are Cleveland heart disease, Indian liver patient, Pima Indian diabetes, employee future prediction, telecom churn prediction, bank customer churn, and tech students. In terms of accuracy, the results showed that, with the exception of approximately 1% decrease in the accuracy of NB classifier in Cleveland heart disease dataset, GADIC significantly enhanced the performance of most ML classifiers using various datasets. In addition, KNN with GADIC showed the greatest performance gain when compared with other ML classifiers with GADIC followed by SVM while LR had the lowest improvement. The lowest average improvement that GADIC could achieve is 5.96%, whereas the maximum average improvement reached 16.79%.", "sections": [{"title": "1. Introduction", "content": "ML has enabled industries to be more efficient and cost-effective. With the exponential growth of data, a need has arisen for ML algorithms with better prediction capabilities. This may be accomplished by applying optimization methods (OM), which can improve the efficiency of the used ML algorithms. This can help diverse fields to accurately predict the occurrence of certain conditions that are of paramount value to them.\nBecause to the nature of the data, some classifiers often perform poorly and achieve low performance. Data scientists may improve the performance of ML algorithms by altering the hyperparameters or carrying out additional pre- processing steps. However, there are times when the algorithm's performance remains low, suggesting that the problem lies within the data itself.\nIn this paper, a novel classification algorithm named (GADIC) is proposed that is based on GA and DI reformatting for the ultimate goal of enhancing the performance of the classifiers. GADIC uses existing classifiers to measure its performance. The main approach relies on altering the data representation in accordance with the DI technique presented in Hewahi (2019) and then applying GA as an OM to tune and adjust the data. The data reformatting is performed based on the influence of certain values within the attributes in affecting the produced output. Applying GA in both training and testing phases on the reformatted data to tune it and avoid any loss is a key step to enhance the performance of the classifiers.\nGADIC is anticipated to be capable of overcoming some of data related impediments that might prevent the classifier from performing well, hence, elevating the performance of the classifier."}, {"title": "2. Background", "content": "The three core elements of GADIC are ML classifiers, GA, and data reformatting."}, {"title": "2.1. Classifiers", "content": "Under this section we briefly present the ML algorithms used in our GADIC proposed approach."}, {"title": "SVM", "content": "SVM is a supervised method that its decision function is an optimal hyperplane that separates classes of the observations based on their features. Accordingly, the best hyperplane is the one that maximizes the margin between classes by utilizing the support vector points. Recently, the community of ML draw more attention to SVM due to its exceptional generalization capability and discriminative power as it is able to handle high-dimensional data (Cervantes et al., 2020; Pisner & Schnyer, 2020). The advantage of SVM is that it can handle both semi-structured and structured data by using a kernel function to transform the input space into a higher dimensional space. However, SVM can be computationally expensive especially with large datasets due to the extended training time. Furthermore, it has the difficulty of selecting the right kernel function (Ray, 2019)."}, {"title": "KNN", "content": "KNN is a non-parametric algorithm which essentially reflects the number (k) of neighbours in the training data that are closest to a specific testing point. The most crucial step in KNN is choosing the proper k value. The nearest neighbours are selected based on Euclidean Distance Function. Eventually, the label of the given testing point is determined by a majority vote of the chosen k nearest neighbours Despite being a simple and flexible algorithm, KNN has a high computation cost and is sensitive to the selection of k value as it is the only parameter (Ray, 2019; Taunk et al., 2019; Cheng et al., 2014)."}, {"title": "LR", "content": "LR is a statistical method for binary and multiclass classification problems in ML as a sort of regression (Connelly, 2020; Maalouf, 2011). It is used to discover the relationship between a dependent variable and one or more independent variables in addition to predict a binary outcome based on those independent variables. The fundamental notion of LR is that it provides probability estimations that range from 0 to 1 (Maalouf, 2011). The main drawback of this algorithm is its assumption of linearity between input and output. Yet, it is easy to implement, train, and interpret (Ray, 2019; Maalouf, 2011)."}, {"title": "DT", "content": "DT is a tree-based classification algorithm in which data is recursively splitting based on a specific threshold. Starting with a root node that represents the complete dataset, DT splits the data into smaller subsets based on the features and their values until final leaves are reached. In the final leaves, each subset consists only of instances of a single class (Jijo & Abdulazeez, 2021; Ray, 2019; Priyam et al., 2013). DT is renowned for its ease in implementation and handling quantitative, qualitative, and missing data. Nonetheless, DT has flaws such as instability and difficulty in controlling the tree size (Jijo & Abdulazeez, 2021)."}, {"title": "NB", "content": "NB is a probabilistic method that implements Bayes Theorem to estimate the probabilities of the dataset's values. This is done by calculating the frequency of their occurrence. The assumption that the attributes are independent in NB is problematic. However, it is known for its scalability, ability of handling missing values and multi-class problems, as well as its suitability to handle high dimensional data (Peling et al., 2017; Taheri & Mammadov, 2013). Despite that, NB's simplicity may result in lower performance compared to other models that have been adequately trained and tuned (Ray, 2019)."}, {"title": "2.2.GA", "content": "GA lies in the family of evolutionary algorithms, a type of meta-heuristic optimization algorithm, which are computational search methods that draws their inspiration from natural selection (Vi\u00e9 et al., 2021; Savio & Chakraborty, 2019; Dahiya & Sangwan, 2018). The Darwinian theory of \"Survival of the fittest\" is the cornerstone of the GA (Katoch et al., 2021; Savio & Chakraborty, 2019). GA operations rely on randomness to set the values of their parameters (Alam et al., 2020).\nGA is carried out through a series of standardized processes. In this method, the problem space is abstracted as a population of individuals, and it attempts to discover which individual is the most fit by constructing generations repeatedly. The fitness function, which identifies the criterion for rating prospective populations and choosing them for inclusion in the following generation, determines the optimal solution to a given problem (Haldurai et al., 2016; Haldar et al., 2014).\nSelection, crossover, and mutation are the three main genetic operators that are consecutively applied with specific probabilities to every individual during each generation until a predefined termination criterion is met (Haldurai et al., 2016). Based on the fitness values of the individuals in the population, the selection operator determines which individuals of the population will be selected for reproduction in the following generation. The crossover operator involves exchanging genes between two selected individuals (by selection operator) to generate new offspring with genes from both parents, while mutation operator randomly alters some of the genes in an individual.\nGA has demonstrated its effectiveness as a robust OM. This, together with its flexibility, simplicity in comprehending, capability of finding the global optimum solution, and ability of solving complex optimization problems that are difficult to solve using other OM, contributes to its increasing recognition and adoption (Haldurai et al., 2016; Tabassum & Mathew, 2014). With concentrating on bio-inspired operators, GA is widely employed to provide high quality solutions to optimization and search problems. GA can settle problems in the real world by replicating this process (Alam et al., 2020; Dahiya & Sangwan, 2018).\nGA, though, encounter significant challenges that have constrained its uses. One thing is its high computational cost. For satisfactory results, GA may need to run for a lengthy time. Additionally, a larger population or complex problems typically cause a considerable slowdown of GA. Another thing is the difficulty in determining the suitable parameter's settings i.e. selecting the initial population, fitness function, and degree of crossover and mutation (Katoch et al., 2021; Vi\u00e9 et al., 2021)."}, {"title": "2.3.Data Reformatting", "content": "The data reformatting method employed in this paper was adopted from research conducted by Hewahi (2019). According to Hewahi (2019), the method is formulated on how frequently a specific attribute value is replicated across all dataset instances. Based on potential changes in the attribute's values across the dataset, the importance of each input for each attribute is determined.\nThe essence of the method is that an attribute's relevance in influencing the output generated decreases in proportion to how small its values changes. Likewise, the more an attribute's values change across the dataset, the more important it is to the process of producing a certain outcome. The importance of each input is calculated using equations established by Hewahi (2019). All data inputs in the dataset are eventually replaced with their estimated importance values."}, {"title": "3. Related Work", "content": ""}, {"title": "3.1.Optimization Methods", "content": "The optimization of ML algorithms can be carried out in a variety of methods, including Feature selection (FS), Parameter Optimization (PO), regularization, model selection, and ensemble methods (Brownlee, 2021; Bains, 2020; Brownlee, 2020). A summary of the reviewed GA-based ML algorithm optimizations is presented in Table 1."}, {"title": "3.2.Data Reformatting Techniques", "content": "A new encoding approach based on existing encoding techniques was proposed by Tabassum et al. (2020) to handle categorical variables in a dataset. The developed approach, known as \"one-hot-frequency\", combines one-hot and frequency encoding methods. A number of ML algorithms' classification performance was improved by the proposed approach. The best performance, though, came from RF followed by DT and ANN.\nA strategy for image quality enhancement was introduced by Hung et al. (2021). This method can enhance image features by incorporating technologies such as noise reduction, sharpening, and color channel adjustment. This method was combined with Deep Learning (DL) model to improve the classification performance.\nHewahi (2019) proposed a new method for data reformatting in which all the dataset inputs are eventually replaced based on their importance in affecting the produced output. The breast cancer, iris, and diabetes datasets from the UCI ML repository were subjected to the proposed data reformatting technique. Employing this technique in conjunction with NN pruning algorithm has yielded classification performance that is superior to that of other NN pruning algorithms in most of the tested datasets."}, {"title": "4. The Proposed Approach: GADIC Algorithm", "content": "GADIC comprises three phases which are data reformatting phase which depends on DI concept, training phase where GA is applied on the reformatted training dataset, and testing phase where the instances of the reformatted testing dataset are being averaged based on similar instances in the training dataset. GADIC is tested on five ML classifiers namely, SVM, KNN, LR, DT, and NB. Accuracy and f1-score are utilized to evaluate the effectiveness of GADIC and compare the results. \nThe steps of GADIC are illustrated in Figure 1. The GADIC algorithm is given below."}, {"title": "Algorithm: GADIC Algorithm", "content": "Start\n# Data reformatting phase\nInput of phase: random sample size s, number of repetitions w, number of intervals pti\nFor (i in range (attributes in the dataset))\nSelect s\nIf (datatype = integer)\nCalculate proportion of possible values $r_{im}$ using Eq. 1\nCalculate $\u03b2_i$ using Eq. 2\nElseif (datatype = float)\nCalculate size of interval $int_i$ using Eq. 5\nCalculate proportion $r_{im}$ using Eq. 6\nCalculate $\u03b2_i$ using Eq. 2\nRepeat w times to find average$\u03b2_i$ using Eq. 3\nCompute importance $I_{iy}$ using Eq. 4\nReplace attribute values with their $I_{iy}$\nOutput of phase: reformatted dataset based on data reformatting phase\n# Data splitting\nSplit the reformatted dataset into training and testing datasets\n# Training phase\nInput of phase: termination generation, crossover probability cp, crossover point, mutation probability mp, mutation rate, mutation value, training dataset\nSet count = 0\nSet initial population = training dataset\nWhile (count < termination generation)\nSelect 2 instances as parents from population randomly\nApply crossover on parents based on cp\nApply mutation on offspring obtained from crossover based on mp\nCompute fitness function for the new offspring\nIf (fitness = True)\nReplace parents with offspring in the initial population\nElse\nDon't replace parents with offspring in the initial population\nCount+=1\nEnd while\nOutput of phase: final training dataset\n#Testing phase\nInput of phase: number of closest rows n, the final training dataset\nFor (i in range (instances in the testing dataset))\nFind n closet instances in the training dataset using Euclidean distance function\nFor (i in range (attributes in the testing dataset))\nCalculate average attribute value of the retrieved closest instances using Eq. 8\nApply ML classifier on the averaged instances\nAssess the performance\nOutput of phase: performance measures results\nEnd"}, {"title": "4.1.Data Reformatting Phase", "content": "In this phase, based on Hewahi (2019), the importance of each attribute input in producing certain output in a dataset will be calculated. Changing factor $\u03b2$ for each attribute will be used to calculate the importance of each attribute input. Each input in that attribute will be replaced by its importance value. All steps will be carried out for each attribute independently in the dataset in order to compute the importance of its inputs and replace their values in accordance with their importance. The main steps of the data reformatting phase are as follow:\n1. Select a random sample of the given dataset where s is the number of randomly selected instances.\n2. In the case where the attribute has categorical or integer datatypes, the following steps will be followed:\na. Each attribute i has pi possible values. For each possible value in the attribute i, the proportion of every attribute input a\u2081 is calculated as shown in Equation (1):\n$r_{im} = \\frac{v_{im}}{s}$, (1)\nwhere,\nvim is the number of m-equivalent values in the random sample for $a_i$.\nb. All rim values calculated by Equation (1) is then appended to a set $R_i$ that will contain all proportion values of $a_i$.\n$R_i = \\{r_{im1}, r_{im2}, r_{im,3},...., r_{imp} \\}$\nc. Then, the set R\u2081 is utilized to calculate the changing factor $\u03b2_i$ for each attribute i using Equation (2):\n$\u03b2_i = \\frac{maxR_i-minR_i}{2}$, (2)\nwhere,\nmax$R_i$ is the maximum value in $R_i$,\nmin$R_i$ is the minimum value in $R_i$.\nd. To reduce the risk of bias in the random selection process, the step of the random selection of instances and the subsequent stages are"}, {"title": "4.2.GA", "content": "In this phase, the training dataset will be subjected to GA operators in order to fine- tune the data. The whole training dataset constitutes the population in GADIC. Also, the fitness function is built to find the attribute values that will yield to the best performance. The steps in this phase will be repeated until the termination condition specified at the beginning is met. The sequence of the steps of GA phase in GADIC is illustrated in Figure 2.\nThe steps of GA phase in the proposed algorithm are as follow:\n1. Set a termination condition for GA to stop and return the final results. In the proposed algorithm, the termination condition is satisfied when a number of generations g is reached. This number is specified before applying any GA operators.\n2. Select two instances randomly from the training set as parents to perform the GA operators on them.\n3. Apply single point crossover on the selected parents. This type of crossover is carried out based on two main parameters:\na. Crossover probability which is a parameter that regulates the possibility that crossover will happen between the parents. It is more probable that crossover between the parents will take place if the crossover probability is set to a high number and vice versa.\nb. Crossover point which is a location selected randomly in the instance. The two parents will be cut at that point and the data to the right of this location are swapped between the two parents to produce two offspring.\nIn this proposed algorithm, the crossover will be applied only on the independent attributes. The class labels will remain untouched. The crossover is performed at the selected crossover point only if it satisfies the crossover probability criteria.\n4. Apply mutation on the offspring produced in the previous step. The mutation is carried out based on three main parameters:\na. Mutation probability which is a parameter that regulates the possibility that an instance will be subjected to the mutation process. The probability should ideally be set to a small number so that only a very small portion of the population mutates in each generation.\nb. Genes to be mutated in the offspring. A mutation rate that is specified at the beginning determines how many genes will be mutated while the locations of these genes are selected randomly.\nc. Mutation value which is a value chosen at random between 0 and 0.1. This value will be added to the values of genes that were selected in step b.\nIt is worth mentioning that the mutation is performed only if it satisfies the mutation probability criteria.\n5. Apply the fitness function to the produced training set after crossover and mutation operators. The fitness function in the proposed algorithm is designed to assess whether applying GA operators on the reformatted dataset will yield to an increase in the performance or not. Only if the fitness function returns a True value, the offspring will replace their parents in the initial dataset. Otherwise, they will be discarded. The fitness function process is as follows:\na. At the first generation, the fitness function will return True if the model accuracy after the crossover and mutation operators is higher than the initial model accuracy. If not, it will return False.\nb. At the subsequent generations, the fitness function will return True if the model accuracy after the crossover and mutation operators in that generation is better than the previous generation's model accuracy. Otherwise, it will return False."}, {"title": "4.3. Averaging Each Instance in the Testing Set", "content": "In this phase, the attributes values of each instance in the testing test is averaged using the attributes values of their similar instances in the training dataset. The following are the steps of the testing phase. These steps will be repeated for each instance in the testing dataset.\n1. For instance n in the testing set, find the h instances in the training set that are closest to n, where h is the chosen number of the similar instances from the training set. The value of h will be selected based on trial and error to find the best value for it. Euclidean Distance function will be used to find the h similar instances from the training set to instance n. Such distance is calculated using Equation (7):\n$d_{i,j} = \\sqrt{\\sum_{p=1}^{k}(X_{ip} - X_{jp})^2}$, (7)\nwhere,\np=1, , k are the attributes,\nXip is the input value of attribute p in instance i,\nXjp is the input value of attribute p in instance j.\n2. Calculate the average value for each attribute in the similar instances independently. Equation (8) is used to find the average avgAtti:\n$avgAtt_i = \\frac{v_1+v_2+...+v_h}{h}$ (8)\nwhere,\nv is the input value of the specified attribute in the selected instances.\n3. The average values avgAtt\u2081 obtained in the prior step for each attribute will be used in place of the values of instance n in the testing set."}, {"title": "5. Experiments", "content": "The datasets used in this paper were obtained from Kaggle and UCI ML repository. GADIC experiments were run on seven datasets which are Cleveland heart disease, Indian liver patient, Pima Indian diabetes, employee future prediction, telecom churn prediction, bank customer churn, and tech students.\nThe chosen datasets for the experiments ranged in size from a few hundred to 20,000 instances. All of them are classification datasets with either binary or multiclass class labels. Input values of various datatypes are present in these datasets. With the exception of the tech students dataset, which includes six class labels, all datasets have binary class labels. The only datasets with balanced or almost equal classes are Cleveland heart disease and tech students. Imbalances exist in the other five datasets.\nThe pre-processing steps employed in this paper include the elimination of irrelevant attributes, the removal of missing and duplicate data, and the application of encoding techniques on the categorical attributes.\nExtensive experiments were conducted throughout every single phase to find the best parameters to be employed in each phase of GADIC. Python libraries were used in Jupyter notebook to conduct all experiments and compare the results.\nIn the data reformatting phase, the optimal parameter values for the datasets of sizes up to 5,000 records were 50 for the random sample size, 10 for the number of repetitions, and 4 for the number of intervals. While, for larger datasets, the parameter values were raised to 100 for the random sample size, 50 for the number of repetitions, and 10 for the number of intervals. As for data splitting of the datasets, 70/30 split produced the optimum performance across most datasets, hence it was used for all experiments.\nIn the training phase, the termination condition was set to the completion of 25,000 generations. Table 2 displays the parameter values for crossover operator that led to the best performance along with the number of generations it took to achieve that result. As for mutation operator, based on the recommendation by Datta (2023) and Savio and Chakraborty (2019) that the mutation probability should be set to a low number, all mutation parameters were set at low values. The mutation rate was set to maximum 0.2 randomly, the mutation probability was set to 0.3, and the mutation value was set to a random value between 0 and 0.1 for all experiments.\nIn the testing phase, the number of similar instances (n) to be used in the Euclidean distance function is the only parameter in this phase. For all datasets and classifiers, the n values that produced the highest performance in the experiments fell between 3 and 7."}, {"title": "6. Results", "content": "To determine the effectiveness of GADIC, the datasets were first evaluated using the conventional ML classifiers, and then GADIC was tested on the same datasets to compare the results. The performance was evaluated using accuracy and f1-score measures. Table 3 and Table 4 show the difference in accuracy before and after applying GADIC on the datasets."}, {"title": "7. Conclusion", "content": "A novel classification algorithm, namely GADIC, that is based on GA and DI reformatting was proposed. GADIC was intended to be able to overcome some data-related issues that could prohibit the classifiers from improving their performance and hence increasing the performance of the classifiers. GADIC was tested on five ML classifiers using seven open-source datasets to assess its performance. The results demonstrated that GADIC improved the performance of the majority of the tested classifiers using various datasets in terms of size and type.\nKNN had the most performance improvement when applying GADIC with an average increase of 16.79%, followed by SVM with 9.03% average increase. The other three classifiers (LR, DT, NB) had an average increase of 5.96%, 6.4% and 6.75% respectively, with LR showing the least improvement which is still considered to be an important achievement. The improvement in the models' performance with GADIC indicates that the nature of data is a factor influencing the performance of the classifiers.\nFuture studies should consider testing the GADIC approach on larger datasets, gauging its potential for enhancing performance of other ML and DL models, and refining the proposed algorithm by experimenting with and integrating different parameters."}]}