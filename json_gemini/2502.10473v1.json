{"title": "Portfolio Beam Search: Diverse Transformer Decoding for Offline Reinforcement Learning Using Financial Algorithmic Approaches", "authors": ["Dan Elbaz", "Oren Salzman"], "abstract": "Offline Reinforcement Learning (RL) algorithms learn a policy using a fixed training dataset, which is then deployed online to interact with the environment. Transformers, a standard choice for modeling time-series data, are gaining popularity in offline RL. In this context, Beam Search (BS), an approximate inference algorithm, is the go-to decoding method. In offline RL, the restricted dataset induces uncertainty as the agent may encounter unfamiliar sequences of states and actions during execution that were not covered in the training data. In this context, BS lacks two important properties essential for offline RL: It does not account for the aforementioned uncertainty, and its greedy left-right search approach often results in sequences with minimal variations, failing to explore potentially better alternatives.\nTo address these limitations, we propose Portfolio Beam Search (PBS), a simple-yet-effective alternative to BS that balances exploration and exploitation within a Transformer model during decoding. We draw inspiration from financial economics and apply these principles to develop an uncertainty-aware diversification mechanism, which we integrate into a sequential decoding algorithm at inference time. We empirically demonstrate the effectiveness of PBS on the D4RL locomotion benchmark, where it achieves higher returns and significantly reduces outcome variability.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) is concerned with an agent learning how to take actions in an environment to maximize the total reward it obtains. An RL agent typically learns by trial and error, involving online interaction with the environment [Sutton and Barto, 1998]. However, such online interaction may incur undesirable costs, time or risks. This is relevant to many fields such as healthcare, autonomous driving, and recommendation systems [Gottesman et al., 2018; Wang et al., 2018; Yu et al., 2019; Sallab et al., 2017; Strehl et al., 2010; Covington et al., 2016].\nIn contrast, offline RL allows agents to avoid undesirable online interactions by using pre-collected data (batch) from other agents or human demonstrations. While this approach mitigates risks associated with online exploration, it also introduces significant challenges. In offline RL, there are no guarantees regarding the quality of the batch data, as it may have been collected by an agent following a suboptimal policy. Therefore, the goal for the learned policy is to outperform the policy that generated the data. Consequently, it is necessary to execute a different sequence of actions than those stored in the batch, leading to encounters with unfamiliar state-action sequences.\nThis situation introduces subjective or epistemic\nuncertainty uncertainty arising from a lack of knowledge or statistical evidence due to the limited size of the training batch or due to its quality. This uncertainty can result in erroneous value estimations, increasing variability in the results, which decreases stability and reliability. This phenomenon, known as distributional shift [Kumar et al., 2020a], is one of the central challenges in offline RL."}, {"title": "Offline RL with Transformers", "content": "Transformers are a deep-learning architecture originally proposed for natural language processing (NLP) tasks [Vaswani et al., 2017]. They have demonstrated powerful expressive capabilities, leading to their adoption in various domains where processing sequential information is crucial [Radford et al., 2023; Liu et al., 2023]. Recently, Transformer models have been successfully applied to various RL benchmarks, such as language instructions for vision tasks [Yao et al., 2023], the Atari games suite [Reed et al., 2022], and continuous control locomotion tasks [Wu et al., 2024].\nWhile earlier studies employed traditional RL algorithms, merely substituting recurrent networks with Transformer architecture [Parisotto and Salakhutdinov, 2021; Kumar et al., 2020d], our work builds on a more recent approach wherein Transformers are used as trajectory models. Pioneered by Decision Transformer (DT) [Chen et al., 2021] and Trajectory Transformer (TT) [Janner et al., 2021], these techniques utilize Transformers to learn dynamics and reward structures, framing offline RL as a conditional sequence-modeling problem. With trajectory models at our disposal, we can simulate potential future scenarios to formulate plans, a principle known as model-based planning [Sutton and Barto, 1998]."}, {"title": null, "content": "Our work targets real-world applications, particularly locomotion problems involving high-dimensional continuous domains. As a concrete example, consider the task of bipedal locomotion. This problem supports multiple solutions, as it can encompass various motion styles using different postures, stride patterns, or gaits. Such problems pose significant challenges for offline RL, as they are often sensitive to even minor variations, especially when such variations are not explicitly included in the offline data batch. Moreover, in such problems, the vast number of possible trajectories makes exhaustive model exploration impractical. In fact, similar to NLP tasks, the number of possible sequences grows exponentially with sequence length, making exact inference NP-hard. Thus, designing algorithms that can efficiently explore these overwhelmingly large domains remains a significant challenge.\nAs finding an exact solution to these sequential-decision problems is typically intractable, approximate inference algorithms such as Beam Search (BS) are typically used. BS is a heuristic-search algorithm that explores the search space in a greedy left-right fashion retaining only the top b scoring candidates sequences. In offline RL, BS is repurposed as a planning or decoding algorithm that integrates reward signals, evaluating candidate trajectories based on their cumulative return. Despite the success of this approach in some offline RL tasks, naively modifying BS to to consider only the maximum predicted returns has several disadvantages that may be detremental in certain applications. Here, we highlight the two most critical ones:\n(i) Ignoring the Distributional Shift: When used as a planning algorithm, BS is not designed to be robust against the distributional shift typical in offline RL. Pursuing the maximal reward trajectory without accounting for uncertainty can result in suboptimal outcomes\n(ii) Insufficient Diversity: Greedy approaches, such as BS, tend to follow the highest rewarding trajectories, which can result in limited exploration and a lack of diversity in decoded solutions. In locomotion problems, identifying various motion modalities, including suboptimal ones, can be advantageous. While a cluster of similar motions might fail if the predicted trajectories prove infeasible, a diverse portfolio of behaviors can adapt to new situations by employing alternative strategies."}, {"title": "Overview and Contributions", "content": "To address the shortcomings of BS when decoding Transformers for offline RL, we propose Portfolio Beam Search (PBS), an uncertainty-aware strategy to decode a set of diverse sequences.\nOur decoding approach is inspired by the finance sector, where investors demand a reward for bearing risk when making investment decisions. This approach was formalized by Harry Markowitz [1952] who introduced this risk-expected return relationship in the mean-variance model, for which he was awarded a Nobel Prize in economics. The mean-variance model addresses the portfolio-optimization problem (formally defined in Sec. 2.3) by identifying the optimal allocation of wealth across different assets to achieve a diversified portfolio by taking into account both the mean (expected return) and the variance (risk).\nBuilding on portfolio theory, our primary technical contribution is the application of these principles to develop PBS, a Transformer decoding algorithm for offline RL. PBS mitigates the adverse effects of the distributional-shift by balancing the desire to maximize rewards with the risk associated with incorrect actions. In the context of offline RL, we treat each candidate trajectory as an asset. When selecting action sequences to explore, we consider our limited computation budget as a form of wealth to be allocated across these trajectories. To optimize this allocation, we solve a convex portfolio-optimization problem and explore trajectories with probabilities proportional to their allocated \u201cwealth\u201d.\nTo demonstrate the effectiveness of PBS, we present results from continuous control tasks using the widely adopted D4RL offline RL benchmark [Fu et al., 2020]. Our method consistently outperforms other Transformer-based offline RL techniques, while maintaining comparable memory requirements and a small run-time overhead. Additionally, and crucial for the offline RL setting, our algorithm exhibits significant stability, substantially reducing result variance compared to other Transformer decoding methods."}, {"title": "2 Preliminaries", "content": null}, {"title": "2.1 Offline RL", "content": "A Markov decision process (MDP) is a tuple $\\mathcal{M}$ = {$\\mathcal{S}$, $\\mathcal{A}$, r, P, $\\rho_0$, $\\gamma$} where $\\mathcal{S}$ and $\\mathcal{A}$ are sets of states and actions, r:$\\mathcal{S}$\u00d7$\\mathcal{A}$ \u2192 R is the reward function, P represents the system's dynamics, describing how the environment changes in response to the agent's actions. It is a conditional probability distribution of the form $P(s_{t+1}|s_t, a_t)$, which indicates the probability of transitioning to the next state $s_{t+1}$ given the current state $s_t$ and the applied action $a_t$. $\\rho_0$ defines the initial state distribution and $\\gamma$\u2208 (0,1] is a scalar discount factor that penalizes future rewards. A policy \u03c0 is a probability function $\\pi(a_t|s_t)$, representing the probability of taking action $a_t$ at state $s_t$. Simply put, an agent makes a decision $a$ \u2208 $\\mathcal{A}$ based on the current state $s_t$ \u2208 $\\mathcal{S}$. The environment responds to the agent's action by transitioning to the next state $s_{t+1}$ \u2208 $\\mathcal{S}$ and providing a reward $r_t$ \u2208 R. The goal is to find an optimal policy that maximizes the expected sum of discounted rewards.\nIn the offline RL problem, we are provided with a static, pre-collected dataset containing transitions from trajectory rollouts $\\mathcal{D}$ = {($s_t$, $a_t$, $s_{t+1}$, $r_t$)$_i$}, where i is the sample index of experiences. The actions are collected from some (potentially unknown) behavior policy $\\pi_B$ (for example, $\\pi_B$ can be human demonstrations, random robot explorations, or both) and the next states and rewards are determined by the dynamics. The goal is to use $\\mathcal{D}$ to learn, in an offline phase, a policy \u03c0 for the underlying, unknown MDP $\\mathcal{M}$, to be executed in the environment an online phase."}, {"title": "2.2 Transformers for Offline RL: a Sequence-modeling Approach", "content": "Our work extends Trajectory Transformers (TT) [Janner et al., 2021] and Decision Transformers (DT) [Chen et al.,"}, {"title": "3 Method: Portfolio Beam Search (PBS)", "content": "There is nothing wrong with a 'know nothing' investor who realizes it. The problem is when you are a 'know nothing' investor, but you think you know something.\"\nThis quote, associated to Warren Buffett [Goodman, 2018] aligns with core idea of our decoding method, Portfolio Beam Search (PBS), for Transformer decoding in offline RL.\nSimilar to other sampling-based variants of BS, PBS is a best-first search method that retains at most b trajectories at each time step. However, it differs in its definition of 'best'. PBS determines which trajectories to retain by considering both the expectation and the uncertainty due to distributional shift. Additionally, it assesses the similarity between candidate trajectories to promote diversity in the decoded solutions. To achieve this, PBS strategically allocates computational resources by solving a portfolio optimization problem. However, unlike standard portfolio optimization, in our context, the budget represents computational effort rather than monetary value, and the assets are the candidate trajectories.\nTo introduce PBS, we revisit Alg. 1 which presented BS as a meta-algorithm. Recall that this meta-algorithm incorporates two functions: score and prune and outputs a set of b approximated best trajectories. We now describe PBS via a description of these two functions."}, {"title": "3.1 score()", "content": "The score () function (Alg.1, Line 6), is the core of PBS. It assigns a relative score to each of the N candidate trajectories available in C at time step t. Specifically, we assign a relative score, $w_n$, to each candidate trajectory, $c_n$, by solving a portfolio-optimization problem (Eq. (1)) that encompasses all candidate trajectories C (where N = |C| \u2265 b). The solution to this optimization problem, represented by $W_1$,..., $W_N$, determines the proportion of computational time allocated to exploring each candidate trajectory.\nTo solve this portfolio-optimization problem, we need to construct two key components: a vector of expected returns"}, {"title": "3.2 prune()", "content": "Given coefficients $W_1$,...,$W_N$, computed using the aforementioned score() function, our prune () function samples b trajectories (with repetition), where the nth trajectory is sampled with probability $w_n$. We underline that more promising trajectories should receive greater computational investment (larger $w_n$). Consequently, these trajectories may be sampled multiple times, biasing the search towards these more promising trajectories. In such cases, our effective beam width is smaller than b (but never larger)."}, {"title": "4 Related Work", "content": "Offline Reinforcement Learning: In model-free offline RL methods, the agent learns a policy or value function directly from the dataset. To address the distributional shift, these algorithms typically penalize the values of out-of-distribution state-action or constrain the policy closed to the behavior policy [Fujimoto et al., 2019; Kumar et al., 2019; Siegel et al., 2020; Wu et al., 2019; Peng et al., 2019]. Additionally, uncertainty quantification techniques are used to stabilize Q-functions [Fujimoto et al., 2019; Wu et al., 2019; Agarwal et al., 2020; Jaques et al., 2019]. On the other side, in model-based RL methods, the offline data is used to train predictive models of the environment that can then be used for planning or policy learning [Argenson and Dulac-Arnold, 2020; Kidambi et al., 2020]. The benefits of discovering diverse solutions have been demonstrated in literature pertaining to both online and offline RL [Kumar et al., 2020c; Osa and Harada, 2024; Kumar et al., 2020b]. However, most these methods tend to require the policy to be pessimistic, while our algorithm does not involve this constraint. Besides, our algorithm is built upon the sequence-generation framework, which is different from the above methods.\nTransformers as Trajectory Models in Offline RL: Decision Transformer (DT) [Chen et al., 2021] predicts actions by feeding target rewards and previous states, rewards, and actions. Trajectory Transformer (TT) [Janner et al., 2021] further brings out the capability of the sequence model by repurposing beam search as a planner. The Bootstrapped Trajectory Transformer (BooT) [Wang et al., 2022] improves on TT by self-generating additional offline data from the model to enhance its training process. The Elastic Decision Transformer (EDT) [Wu et al., 2023] augments DT with a maximum in-support return training objective, enabling it to dynamically adjust history length to better combine sub-optimal trajectories. The Q-learning Decision Transformer (QDT) [Yamagata et al., 2023] uses dynamic programming to relabel the return-to-go in the training data and trains the DT with the relabeled data. However, these studies primarily rely on advanced and often complex training procedures. In contrast, our approach focuses exclusively on modifying the inference algorithm, leaving the training process unchanged. This distinction is significant because our method is complementary and orthogonal to training-based approaches, and can potentially be combined with them to achieve additional improvements.\nTransformer Decoding Methods: In recent years, the rise of large Transformer-based language models has driven the development of better decoding methods. Many of these methods are sampling-based variants of BS [Holtzman et al., 2019; Fan et al., 2018]. A notable example of diverse decoding is Diverse Beam Search [Vijayakumar et al., 2018], an alternative to BS that decodes a list of diverse outputs by optimizing a diversity-augmented objective. However, these methods are primarily ad-hoc and designed to produce fluent text in large Transformer-based language models trained in supervised-learning settings, where the goal is to find the most likely output sequence from the model. In offline RL, the batch also contains suboptimal behaviors, so the objective must incorporate reward signals."}, {"title": "5 Experiments and Results", "content": "To evaluate PBS, we empirically compare it with three methods based on the Trajectory Transformer: the original Trajectory Transformer (TT) [Janner et al., 2021]. the Bootstrapped Trajectory Transformer (BooT) [Wang et al., 2022] and Elastic Decision Transformer (EDT) [Wu et al., 2023]. Additionally, we add comparisons with Decision Transformer (DT) [Chen et al., 2021] and an extension of the Q-learning Decision Transformer (QDT) [Yamagata et al., 2023]\nWe evaluate our algorithm on locomotion control tasks using the OpenAI Gym MuJoCo physics engine [Todorov et al., 2012]. To ensure that our performance improvements are solely due to the improved Transformer-decoding algorithms, we used the same hyperparameters (including beam width, planning horizon, vocabulary size, and context size) and trained models with identical network weights as those provided in the original TT, which were trained on D4RL offline dataset for locomotion tasks [Fu et al., 2020].\nD4RL consists of various datasets designed for benchmarking offline RL. The \"medium\" dataset comes from a policy reaching about a third of expert performance. The \"medium-replay\", sourced from this policy's replay buffer, and the \"medium-expert\" dataset is generated by mixing samples generated by the medium policy and samples generated by an expert policy.\nWe present Gaussian distributions illustrating the mean and variance of the baseline algorithms results in Fig. 3. Detailed results can be found in the Appendix, Table 1. The results indicate that the proposed PBS consistently outperforms the baseline methods TT, DT, and their variants across the majority of datasets. Notably, PBS reduces the standard deviation by more than 60% compared to the TT network, which shares the same weights, while also achieving a higher mean. BooT achieved higher means on some benchmarks; however, its overall average score is lower, and its average standard deviation is more than twice as high. These findings highlight the effectiveness of our approach in addressing distribution shift. Consequently, PBS demonstrates more stable behavior, which is crucial in high-stakes offline RL settings where errors can have significant consequences.\nFor all datasets and environments in our reported results (Table 1), except for HalfCheetah-medium-expert, we set 8 = 1, giving equal weights to the expected return (\u03bc) and the covariance matrix (\u03a3) (terms (1) and (2) in Eq. (1)). Additionally, we set x = 0.1 and the discount factor y = 0.99. HalfCheetah-medium-expert is the only dataset where our performance is worse than the original BS variant, which achieves a remarkably low standard deviation. Interestingly, adopting a more aggressive approach can be advantageous in this case. We observed improved results for more risk-tolerant settings (\u03b1 = \u03b4 = 0.01). This risk-tolerant setting results in numerous negligible weights in the portfolio, leading to the exclusion of most trajectories from the beam. Consequently, this creates an almost greedy decoding with small beam width (C| \u2264 10) for most time steps. However, for this dataset, our performance still lags the original expectation-maximizing baseline. Although further adapting these parameters for each dataset could potentially enhance performance,"}, {"title": "6 Conclusion", "content": "In this work, we addressed the question of how we can modify the decoding algorithms to tackle extremely challenging offline RL problems such as locomotion problems. To this end, we introduced PBS, a decoding algorithm that adapts tools from a different domain (economics) and proposes a structured approach to risk control, pinpointing where these tools can be effectively utilized in the decoding process.\nPBS leverages the model's ability to generalize to states outside the static dataset support. However, it remains cautious when drifting to states where the model cannot provide confident predictions based on the dataset. Additionally it encourage diversity which is important in problems where multiple solutions exist. This dual consideration allows for a robust selection of trajectories, a crucial attribute in offline RL, where failures can lead to significant costs or dangerous consequences. While many studies focus on enhancing Transformer training procedures, we emphasize that PBS is exclusively an inference-time algorithm. Consequently, it operates independently of these methods and can be integrated with advanced training techniques to further boost performance."}], "equations": ["Maximize\\ \\ w^T\\mu - \\delta w^T\\Sigma w + \\alpha w^T w,\\tag{1}", "subject\\ to\\ \\sum_{i=1}^N w_i=1\\ \\ and\\ \\ w_i\\geq 0.\\tag{1}", "E[r_t] = \\sum_{i=1}^V p_i v_i,\\tag{2}", "Var[r_t] = \\sum_{i=1}^V p_i (v_i \u2013 E[r_t])^2.\\tag{2}", "\u03bc_\u03b7 = \\sum_{t=1}^{H-1} \u03b3^t E[r_t] + \u03b3^{H-1}E[\u0154_t^f].\\tag{3}", "\u03a3= USU,\\tag{4}", "\u03c3_\u03b7 = \\sum_{t=1}^{H-1} \u03b3^{-2t}Var[r_t] + \u03b3^{-2T}Var[\u0154_t^f].\\tag{5}"], "latex": ["\\mathcal{M}", "\\mathcal{S}", "\\mathcal{A}", "r", "P", "\\rho_0", "\\gamma", "\\mathcal{S}", "\\mathcal{A}", "r", "\\mathcal{S}", "\\mathcal{A}", "R", "\\pi", "\\pi", "s_t", "\\mathcal{A}", "s_t", "\\mathcal{S}", "s_{t+1}", "\\mathcal{S}", "r_t", "\\mathcal{D}", "s_t", "a_t", "s_{t+1}", "r_t", "\\pi_B", "\\mathcal{M}", "w^T\\mu - \\delta w^T\\Sigma w + \\alpha w^T w", "\\sum_{i=1}^N w_i=1", "w_i\\geq 0", "\\sum_{i=1}^V p_i v_i", "\\sum_{i=1}^V p_i (v_i \u2013 E[r_t])^2", "\\sum_{t=1}^{H-1} \u03b3^t E[r_t] + \u03b3^{H-1}E[\u0154_t^f]", "\u03a3", "USU", "\\sum_{t=1}^{H-1} \u03b3^{-2t}Var[r_t] + \u03b3^{-2T}Var[\u0154_t^f]"]}