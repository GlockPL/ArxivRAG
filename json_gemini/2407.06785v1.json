{"title": "Towards physics-informed neural networks for landslide prediction", "authors": ["Ashok Dahal", "Luigi Lombardo"], "abstract": "For decades, solutions to regional scale landslide prediction have mostly relied on data-\ndriven models, by definition, disconnected from the physics of the failure mechanism. The\nsuccess and spread of such tools came from the ability to exploit proxy variables rather than\nexplicit geotechnical ones, as the latter are prohibitive to acquire over broad landscapes. Our\nwork implements a Physics Informed Neural Network (PINN) approach, thereby adding to\na standard data-driven architecture, an intermediate constraint to solve for the permanent\ndeformation typical of Newmark slope stability methods. This translates into a neural net-\nwork tasked with explicitly retrieving geotechnical parameters from common proxy variables\nand then minimize a loss function with respect to the available coseismic landside inventory.\nThe results are very promising, because our model not only produces excellent predictive\nperformance in the form of standard susceptibility output, but in the process, also gener-\nates maps of the expected geotechnical properties at a regional scale. Such architecture is\ntherefore framed to tackle coseismic landslide prediction, something that, if confirmed in\nother studies, could open up towards PINN-based near-real-time predictions. To stimulate\nrepeatability and reproducibility of the same experiment, we are openly sharing data and\ncodes at the following GitHub repository: https://github.com/ashokdahal/PINN.git.", "sections": [{"title": "Introduction", "content": "Landslides represent a major cascading geological hazard primarily triggered by two earth\nsystem processes: solid earth and hydrological processes. Landslides induced by solid earth\nprocesses are typically caused by earthquakes, volcanic activity, and other geophysical phe-\nnomena, while the hydrological process predominantly triggers landslides through changes in\ngroundwater levels and surface water saturation (C.D.C., 2019). In both instances, landslides\noccur due to increased stress, or \u201cdriving force\u201d, on a potential failure plane, with these forces\nbeing commonly referred to as triggering forces (Newmark, 1965; Jibson, 1993; Fan et al.,\n2019). The global distribution of landslides results in considerable loss of infrastructure and\nhuman lives (Petley, 2012; Fidan et al., 2024). Moreover, this trend is expected to escalate\ndue to the increased frequency of hydrologically induced landslides under climate change\nscenarios (Gariano and Guzzetti, 2016; Dahal et al., 2024a) and heightened exposure due to\ndevelopment activities in areas susceptible to solid Earth-induced landslides (Reichenbach\net al., 2018; Wang et al., 2024b). Therefore, the accurate and reliable modelling of landslides\nis a crucial and ongoing area of research.\nIn the case of earthquake-induced landslides, the slope potentially fails due to seismic\nloading in an otherwise stable slope (Gorum and Carranza, 2015). Generally, earthquake-\ninduced landslide hazard modelling involves understanding and estimating the required seis-\nmic loading conditions for a slope to fail (Jibson, 2011). This approach can be broadly\ncategorized into two main types: physically based modelling and data-driven modelling\n(Fan et al., 2019). Physically-based modelling solves the forward physical systems given the\ngeotechnical properties of the material involved and the triggering force (Memon, 2018). In\ncontrast, data-driven modelling employs statistical and machine learning-based models to\nsimulate landslides, utilizing explicit and/or latent explanatory variables influencing land-\nslide occurrence (Amato et al., 2023).\nPhysics-based landslide modelling has been a well-established approach since the incep-\ntion of soil mechanics (Terzaghi, 1950), and it is primarily based on force balance problems"}, {"title": "", "content": "in physics. For earthquake-induced landslides, Jibson (2011) categorizes these models into\nthree main types: pseudo-static, stress deformation, and permanent deformation methods.\nThe pseudo-static method, as described by (Terzaghi, 1950), models seismic loading as\na permanent body force acting on a static limit-equilibrium force diagram. When the force\ndue to seismic loading exceeds the resisting force, the slope begins to fail. This intuitive\nand simplified method has a long history of use, but it only determines whether a slope is\nstable or failed without estimating the consequences of instability or the likelihood of failure\n(Jibson, 2011).\nThe stress deformation method is based on the finite element method developed by\nClough (1990). This approach calculates stress changes due to reduced soil stiffness and\nresulting deformation due to seismic loading within a finite element mesh (Memon, 2018).\nAlthough this method is computationally intensive and requires detailed geotechnical pa-\nrameters, it provides a naturally evolving failure plane and the most realistic representation\nof slope failure (Jibson, 2011).\nThe permanent displacement method, initially developed by Newmark (1965) and later\nsimplified by Jibson (1993), falls between the stress deformation and pseudo-static methods\nin terms of complexity and data requirements (Jibson, 2011). This method models a landslide\nas a rigid block on an inclined plane with a known critical seismic acceleration. The slope\nbegins to displace when seismic perturbation exceeds this critical acceleration. Once a certain\ndisplacement threshold is reached, the slope is considered to have failed. This method is fairly\naccurate if the slope geometry, soil properties, and ground motion are known (Jibson, 1993;\nHsieh and Lee, 2011).\nAll of the physics-based methods require spatially varying and detailed geotechnical pa-\nrameters such as soil cohesion, internal friction angle, density, thickness, etc. (Fan et al.,\n2019). This is possible for single or multiple slopes but is challenging for regional-scale\nresearch and applications.\nData-driven methods for landslide modelling are mainly categorized into statistical and"}, {"title": "", "content": "machine-learning approaches (Dahal and Lombardo, 2023). Statistical methods determine\nlandslides' probability, frequency or intensity based on environmental variables, which may\nbe expressed explicitly or at a latent level (Lombardo et al., 2019). Common statistical\nfunctions used in landslide modelling include bivariate models, logistic regression, general-\nized linear models, generalized additive models, and advanced models such as Cox processes\n(Atkinson and Massari, 1998; Brenning, 2008; Yalcin, 2008; Steger et al., 2016a; Lombardo\net al., 2020; Yadav et al., 2023). Despite their differing mathematical formulations, these\nmodels share a common principle: identifying statistically significant relationships between\nindependent and dependent variables. The dependent variables represent landslides' occur-\nrence (susceptibility), frequency, or intensity (e.g., volume, area, impact pressure) (Dahal\net al., 2024c,a). To define the statistical function that estimates landslides, statistical models\nare fitted using maximization or minimization techniques, aiming to produce outputs closely\nmatching observations for a given set of input variables (Taylor and Diggle, 2014). Once\nfitted, the model's predictive capability is validated using an independent dataset to ensure\nits reliability for future estimations (Steger et al., 2016b).\nMachine learning methods follow the same principle as the statistical ones (Jackson,\n1988).\nThe main difference between the statistical and machine learning methods is that the\nletter do not assume a definite functional form, and the model has a potentially highly\nnon-linear representation (Dahal et al., 2024a). Moreover, the optimization of deep learning\ntechniques predominantly minimizes the loss function (Yang et al., 2019). The architecture\nand how the model operates are technical details that have been improved throughout the\npast decades, improving the given models' performance capability. The most common ma-\nchine (and deep) learning models in landslide hazard modelling are artificial neural networks\n(ANN) (Gomez and Kavzoglu, 2005), recurrent neural networks (RNN) (Fang et al., 2023),\ntransformers (Dahal et al., 2024b), and convolutional neural networks (CNN) (Dahal et al.,\n2024c)."}, {"title": "", "content": "The main criticism of data-driven methods is that they do not respect the physical\nmechanism of the process they are modelling, and often, the neural networks cannot perform\nwell outside of their calibration domain due to the excessive training on a specific data set\nor the unknown physics behind the retrieve functional dependence (Dahal and Lombardo,\n2023). Similarly, the physics-based approach is more difficult to constrain in the larger\nspatial domain due to the complexity of obtaining dynamic geotechnical parameters (Fan\net al., 2019). As such, the geotechnical properties of the material are also influenced by\nlocal climatological conditions such as prolonged precipitation or drought (Kramer, 1996).\nTherefore, there is a need for a modelling framework for landslides that can infer the landslide\ngiven latent variables while respecting the process they are tasked to solve. Currently, only\ntwo contributions have been published with a focus on physics-informed neural networks\n(PINN; Liu et al., 2023; Moeineddin et al., 2023). This class of models is essentially placed\nin between physics and data-driven ones, with a neural network being the core solver, but\ntasked with learning the physics of the failure mechanism rather than \u201cblindly\u201d matching\nlandslide presence/absence data to a set of predictors. The first of the two contributions (Liu\net al., 2023) makes use of the scoop3d model (Reid et al., 2015) to identify no-landslide (0 in\nBernoulli distribution). This is how the authors justify their model to be physics-informed.\nThis is somewhat a true statement, but also does not fully satisfies the requirement of a\nPINN, as per definition in physics or computer science (Raissi et al., 2019). In fact, the\nmodel proposed by Liu et al. (2023) constrains the data in a physical manner, but the\nmodel does not formally incorporate any physical law. What the authors assume is that by\ngenerating landslide occurrence / non-occurrence locations through scoop3d, then a neural\nnetwork should learn the physics behind it. So, even if the attempt is certainly in the right\ndirection, it still misses an important component in the standard definition of PINNs.\nThe second contribution in the topic was recently published by Moeineddin et al. (2023).\nTheir work integrates the creeping mechanism of landslides and correctly integrates the\nphysics in the modelling framework. In this sense, they satisfy the PINN definition re-"}, {"title": "", "content": "quirements. However, their approach is suitable for solving partial differential equations\nexclusively at specific locations. These are single slopes where geotechnical data is available.\nTherefore, the use of the PINN as per their proposed method, cannot be extended to other\nareas. In turn, this implies that the advantage of traditional data-driven models is still there,\nunless one defines a generalizable PINN.\nThe aim of this manuscript is precisely to work towards a generalizable PINN, capable\nof respecting the physics of the landslide genesis, but also of being extended across large\nlandscapes. We will do so, by treating geotechnical parameters as the latent covariates of\nan architecture tasked with retreaving those as an intermediate step and make use of them\nto minimize the difference between landslide presence/absence locations and the estimated\nfactor of safety.\nIn the remainder of the manuscript, we present the data and area where we tested our\nPINN (Section 2), followed by a methodological overview (Section 3), which we wrote with\na strong mathematical formalism to offer an engineering geological blueprint for those that\nmay want to replicate the same model in the future. The manuscript then presents the\nresults in Section 4, discusses them in Section 5 and offers our vision for future developments\nin Section 6."}, {"title": "Data and test site overview", "content": "To train and evaluate our proposed PINN, we choose the same dataset freely distributed by\nDahal and Lombardo (2023). The study area was affected in 2015 by the 7.8 Mw Gorkha\nearthquake, Nepal, resulting in tens of thousand coseismic landslides (see Figure 1). These\nhave been mapped by Roback et al. (2017) in an openly accessible inventory. Because of the\ncombined use of manual mapping and very high-resolution optical images, the This inventory\nhas been reported to be among the best available in the global repository made by (Schmitt\net al., 2017) and (Tanya\u015f et al., 2017), both in terms of quality (Tanya\u015f and Lombardo, 2019)"}, {"title": "", "content": "and completeness (Tanyas and Lombardo, 2020).\nThe Nepalese landscape where this study is conducted is essentially the roughest land-\nscape on earth, due to the presence of the Himalayan range, being responsible for slopes\nranging from 5\u00b0 to 80\u00b0 in steepness. This steepness is associated with high mountain relief,\nhosting variations in elevation ranging from \u2248 500 meters to \u2248 5000 meters.\nNotably, because the Gorkha earthquake occurred during the Nepalese dry season, the\ncontribution of the groundwater to the shallow failures (the dominant mechanism reported\nby Roback and co-authors) is assumed to play a negligible role (Regmi et al., 2016).\nIn this experiment, most of the outcropping lithologies belong to the Siwalik formations,\nfollowed by the Himal group and various river formations such as the Seti and Sarung\nKhola formations (Dahal, 2012). The Siwalik Formation is primarily constituted by Molasse\ndeposits of the Himalayas, comprising sandstones, mudstones, shales, and conglomerates\n(Upreti, 2001). The river formations, mainly found in the middle Himalayas, consist of"}, {"title": "", "content": "a mixture of schist, granite, gneiss, phyllite, and quartzite (Upreti, 2001; Dahal, 2012).\nFinally, the upper Himalayan region is where most of the tectonic compression is affected\nthe lithological records, with metamorphic rocks encompassing schist, gneiss, migmatites,\nand marbles (Upreti, 2001). The geological map we used in this experiment was obtained\nfrom Wandrey (1998), thus partitioning the study area into nine different geological classes.\nOur mapping unit of choice corresponds to slope units (SUs, hereafter). These are essen-\ntially half sub-basins considered to be homogenous in terms of their overall slope exposition\n(Alvioli et al., 2016). Their use mainly appeared in lanslide studies as part of data-driven\nmodels (e.g., Tanya\u015f et al., 2019), with only one example to date where they have supported\nphysics-based slope stability models (Dom\u00e8nech et al., 2020), although exclusively during\nthe post-processing phase.\nOur main objective is to make use of SUs to represent the \u201cinfinite-slope\" problem (Xiao\net al., 2016; Xi et al., 2024).\nWe recall here that for regional studies, SUs are typically considered an ideal partition\nbecause they geographically approximate the hillslope response to a failure (Alvioli et al.,\n2020). Analogous considerations can be made in the framework of our proposed PINN. In\nfact, for a physics-informed data-driven approach, the same geomorphological considerations\nare valid, with the added geotechnical value of the homogeneity ensured by SUs being useful\nto express the inclined slope assumption predominantly held in pseudo-static and permanent\ndeformation analyses (Newmark, 1965).\nWhen defining our mapping units, we explored the potential use of the SU delineation\nmade by (Alvioli et al., 2022). However, these are SUs generated to partition the whole Hi-\nmalaya, thus making them too coarse for our modeling purposes. For this reason, we opted\nto use the same delineation appeared in Dahal and Lombardo (2023). These had a much\nsmaller initial target size, were selected among several possible versions generated through\npermutation of the r.slopeunits paramerization Alvioli et al. (see, 2016) and ultimately val-\nidated through visual inspection."}, {"title": "", "content": "Each of the SU served as the reference spatial scale at which we aggregated the input\npredictors as well as the landslide stable/unstable labels. For clarity, below we quickly\nsummarize the reasoning behind the choice of our predictor set:\ni. Eastness: The eastness shows how east direction the mountain slope faces. If it is\nfacing completely towards the east, then the value will result in 1 and 0 for the contrary.\nIt represents how much sunlight a particular slope gets, which can be a latent variable\nfor landslide occurrence (Olaya, 2009).\nii. Northness: Similar to eastness, northness provides information on the degree of north-\nness of the mountain slope. This is also used for the exact same reason as eastness\n(Olaya, 2009).\niii. Horizontal Curvature: The horizontal curvature provides on the overall horizontal\n\"bending\" of a given slope unit with respect to a imaginary horizontal tangent line.\nThis information is a proxy for the 3-dimensional geometry of a sloped unit (Hengl\nand Reuter, 2008).\niv. Vertical Curvature: Vertical curvature provides information on general vertical\n\"bending\" of the slope with respect to a given vertical plumbline (Hengl and Reuter,\n2008).\nv. Slope: Slope provides information regarding the slope unit's steepness and is crucial\ninformation for any landslide hazard information as the acting gravity on a rigid body\nalways acts with respect to the slope reference frame. Therefore, the slope is a first-\ndegree control of the occurrence of landslides (Hengl and Reuter, 2008; Lombardo et al.,\n2019).\nvi. Precipitation:To inform the model about the relative wetness of the given slope, we\nincluded the total precipitation at a local slope unit for the three months before the"}, {"title": "", "content": "main event. This information provides an overview of the relative wetness of the soil\nand failure surface (Moore et al., 1991).\nvii. NDVI: NDVI is a proxy for the root strength of vegetation present in the study area\nand it provides an overall quality and presence of the vegetation on a particular slope\nunit.\nviii. PGA: The peak ground acceleration is another first-order control of the earthquake-\ninduced landslides as it provides information on the perturbation to an otherwise stable\nslope due to the ground motion (Dahal et al., 2023).\nix. Sand, Silt, and Clay Content: This information provides the average quantity of\nsand, silt, and clay available on a given slope unit's top 2 meters of soil block. This\ninformation is important for a shallow landslide as properties of the material on the\ntop 2 meters often represent the failure plane (Hengl et al., 2017).\nx. Bulk Density: Similar to soil content, bulk density or the oven-dry weight of soil\nprovides information on the material composition of the failure plane as well as the\nfailure material (Hengl et al., 2017).\nxi. Geology: Similar to ground motion and the slope, geology is another first-order control\nof the earthquake-induced landslides as it provides information on the overall type of\ngeological structure of the slope unit and often has certain categories of the geological\nunits have a higher correlation to landslide occurrence and even controlling the type\nof feasible landslides (Dahal, 2006; Wandrey, 1998)."}, {"title": "Methods", "content": "Landslide susceptibility can be defined as the landslide occurrence probability $L(s,t) \\in 0,1$\nin a specific location and time $L(s,t) = 1$ (Fell et al., 2008; van Westen et al., 2008). When\nassessing how prone a given landscape is to generate slope failures, the use of historical"}, {"title": "", "content": "(Maharaj, 1993) or event-based (Lombardo and Tanyas, 2020) landslide inventories often\ncomes with neglecting the temporal component of the landslide genesis. Therefore, the\nsusceptibility is spatially defined, Xs leading to a simplified notation as compared to the\none presented above, $L(s) = 1$.\nWe recall once more that susceptibility is usually obtained by means of either physics-\nbased or data-driven methods (Fan et al., 2019). In the first case, the susceptibility\n$p_p(s) \\in [0,1]$ is defined as the probability such that the displacement D at any loca-\ntion s exceeds a minimum displacement threshold $d_p$ (or velocity in some cases) $p_p(s) =$\n$Pr(D(s) \\geq d_m | G(s) = x(s))$ for given geotechnical conditions $G(s)$ (Huang et al., 2020a,b).\nIn many cases, the variables $G(s)$ cannot be gathered across regional sales, thus physics-based\nmethods employed for such scales simplify those variables by assuming constant geotechnical\nproperties over the study domain Xs (Fan et al., 2019).\nIn this overarching theme, data-driven models offer valid alternative solutions. As sta-\ntistical models can directly infer the probability of a random process conditioning it to a set\nof predictors, they directly estimate the probability of landslide occurrence $L(s) = 1$ given\nthe environmental conditions $X(s)$, including topographic, geologic and other influencing\nfactors (Dahal et al., 2024a). Given the dichotomous nature of $L(s) = 1$, the Bernoulli prob-\nability distribution is well suited to address this classification task, with standard techniques\ndenoting the probability distribution as $L(s) \\sim Ber(p_d(s))$. Therefore, the probability of\nlandslide occurrence $p_d(s) \\in [0, 1]$ can be written as $p_d(s) = Pr{L(s) = 1 | X(s) = x(s)}$.\nGiven the two very different systems described above, the main problem with the data-\ndriven method is that $X (s)$ does not often represent the actual geotechnical conditions at\nwhich the slope failure occurs, and the model itself also does not respect the physical mecha-\nnism behind the landslide process (Dahal and Lombardo, 2023). Therefore, the data-driven\napproach is ideally used as a first-order estimation of landslide hazard, from which the most\nsusceptible locations can be selected and further analyses using physics-based approaches.\nHowever, there are two major issues in this sequence. Firstly, further analyses may miss"}, {"title": "", "content": "hazardous locations if the data-driven approach misclassifies some highly susceptible loca-\ntions, or is prone to type II errors. Secondly, such ideal implementation is useful only in a\npreemptive manner and cease to be practical during emergencies. In situations where strong\nearthquakes induces widespread landsliding, what can be achieved is confined to a near-real-\ntime landslide susceptibility assessment (Nowicki Jessee et al., 2018), and detailed analyses\nare prohibitive because of the urgency in disaster response (Mon et al., 2018).\nTo solve these problems, we propose to combine the best of both worlds in a single model\nthat offers regional scale predictions, geotechnical meaningful results and a near-real-time\npredisposition, once pre-trained.\nTo do so, we estimate geotechnical parameters $G(s)$ given environmental variables $X(s)$\nwith some sort of highly non-linear functional approximation using deep learning function\nF. Then, we use those parameters through a physics-based landslide estimation technique to\nproduce a landslide susceptibility values. Therefore the approximated geotechnical parame-\nters can be defined as $G(s) = F(X(s)) | X(s) = x(s)$.\nIn this study, we opted to define a simple architecture where the neural network function\nF is a ANN with 16 neural network blocks. Each block consists of a dense neural network\nof 64 neurons except for the last block, a batch normalization layer (Ioffe and Szegedy,\n2015), a dropout layer with a dropout ratio of 0.3 (Srivastava et al., 2014), and a rectified\nlinear unit (ReLU) activation function (Yarotsky, 2017; Nair and Hinton, 2010). The neural\nnetwork itself contains the trainable weights and biases, which are initialized using random\nnormal initialization (Thimm and Fiesler, 1995). The batch normalization and dropout\nlayers prevent the model from overfitting by regularizing the distribution of each batch and\nrandomly deactivating 30% of neurons. The ReLU activation changes the data flow to\nconvert the model into a non-linear one. The last block of the neural network consists of a\nvariable number of neurons, which are equal to the number of geotechnical parameters that\nwe wish to estimate, which in this case is two.\nNotably, which types and how many geotechnical parameters $G(s)$ entails depends on"}, {"title": "", "content": "our assumptions and the underlying physical mechanism we want to solve. Once $G(s)$\nis estimated with the function F, it is then passed, together with required environmental\nconditions such as terrain slope, through a \u201cLandslide-Physics\" layer in the model, which\nrespects the physics of the failure mechanism.\nWith this general framework in mind, we define our model as follows. First, we se-\nlect a suitable physical mechanism for landslide estimation in earthquake-induced land-\nslides. In this case, we opt for a simplified permanent deformation method (i.e., Newmarks'\nmethod) (Jibson, 2007; Gallen et al., 2015). This method is our choice of physical model\nfor the \u201cLandslide-Physics\" layer. This layer defines landslide susceptibility as a function\nof permanent displacement due to seismic perturbation (Newmark, 1965). Let us assume\nthat without any seismic loading, the slope has a \u201cstandard\u201d factor of safety based on its\ngeotechnical properties and geometry. The geotechnical properties that affect slope stability\nare the cohesion of the material C, the thickness of the failure block t, the density of the\nrock and soil material $rho_r$, the density of water $p_w$, and internal friction angle $\\varphi$. The safety\nfactor is also largely controlled by the slope angle of failure plane a, the ratio of saturated\nthickness to total thickness m, and gravitational acceleration g. Gallen et al. (2015, 2017)\ndefines the factor of safety based on those properties as:\n$FS = \\frac{C}{\\tm (\\prg) \\sin(a)} + \\frac{ \\frac{\\tan(\\varphi)}{\\tan(a)} - m \\frac{(p_wg) \\tan(\\varphi)}{(p_rg) \\tan(a)}}{1}$\t(1)\nBecause the Gorkha earthquake occurred in the dry season in the Himalayan range,\nwe assume that the water table was below the failure surface, rendering m = 0 (Gallen\net al., 2017). This assumption simplifies the FS equation. Moreover, following Gallen\net al. (2017), we assume the material's average and constant density over the study area\nwith $p_r = 2300kgm^{-3}$. The gravitational constant is $g = 9.8ms^{-2}$. The factor of safety\nprovides the slope's strength to resist the driving force; this can be converted to represent\nthe minimum acceleration required to move the sliding block on an inclined plane. This"}, {"title": "", "content": "acceleration term is called critical acceleration $a_c$ (Jibson, 1993), formulated as:\n$a_c = (FS - 1)g\\sina$\t(2)\nNow, by adding seismic loading in terms of peak ground acceleration $a_p$, we obtain an\nempirical model for the estimation of Newmark displacement of sliding block D(s) (Jibson,\n2007; Gallen et al., 2015, 2017)shown as:\n$\\log D(s) = 0.215+\\log \\left(1 - \\frac{a_c}{a_p}\\right)^{2.341} - \\left(\\frac{a_c}{a_p}\\right)^{-1.488} \\pm0.51$\t(3)\nThis whole process (equations, 1,2, and 3 are combined in a layer representing function\n$\\psi$ which takes the input of slope $\\sigma(s) \\in X(s)$ and peak ground acceleration $a_p(s) \\in X(s)$\nfrom environmental variables as well the estimated geotechnical parameters $G(s)$. The\ngeotechnical parameters are estimated using the neural network model F are the ratio of\ncohesion per thickness of failure mass $C/t_m$ and the internal friction angle $\\varphi$. The function\n$\\psi$ returns the exponential of the permanent displacement term $\\exp(D(s))$. Mathematically,\nit can be represented as $D(s) = \\psi(G(s), \\sigma(s), a_p(s))$. As we do not know the intermediate\ndeformation of our landslide mass and we only know if the slope has failed or not, we\ndefine a commonly accepted threshold of 5 cm as the displacement threshold above which\nthe slope starts to move. However, this still does not provide the probability of failure\n(or susceptibility) but rather a dichotomous representation of the process. Therefore, we\nmodified the ANN sigmoid functions to be centered around 5 cm threshold, such that it\nprovides an physics-integrated data-driven susceptibility $p(s) \\in [0, 1]$ as:\n$p(s) = c(D(s)) = \\frac{1}{1 + e^{(5-D(s))}}$\t(4)\nSpecifically, equation 4 is similar to sigmoid activation with a centre around 5 cm and\nprovides higher probabilities for the cases $D(s) \\gg 5$cm and lower probabilities for $D(s) \\ll$"}, {"title": "", "content": "5cm. This function can be graphically represented in Figure 2.\nThis overall physics-integrated landslide susceptibility model can be written in the form\n$p(s) = \\varsigma(\\psi(F(X(s)), \\sigma(s), a_p(s))) | X(s) = x(s), \\sigma(s) \\in x(s), a_p(s) \\in x(s)$. This can be\nfurther simplified to denote the physics integrated model as $\\Theta(X(s)) | X(s) = x(s)$.\nWe recall at this point that any neural network architecture relies on a loss function\nto converge to the best solution. In this sense, to train the trainable parts of the model\n$\\Theta(X(s))$, we can use a normal binary cross entropy loss function, because our model output\nis a pseudo-probability $p(s) \\in [0,1]$ (Good, 1963; Akaike, 1998). To train our model, we\nhave used a mini-batch optimisation technique where network parameters are updated with\neach randomly selected batch $B_N C 1,...,n$ of size $N < n$, where n is the total number of\nbatches (Li et al., 2014). Let the $l_i$ be the observed L(s) value for each slope unit during the"}, {"title": "", "content": "earthquake event, and $p_i(s_i)$ the predicted probability for $i^{th}$ variable. This can be written\nas per equation 5.\n$Loss =  -\\sum_{i \\in B_N}[l_i \\log(p_i(s_i)) + (1 - l_i) \\log(1 - p_i(s_i))]$\t(5)\nWith the above loss function and defined model, we trained our model using 70% of the\noverall slope units. Out of the remaining 30% of the dataset, half of it was used to validate\nthe model, while the remaining half was used to test the it. The Adam optimizer (Kingma\nand Ba, 2014) is selected for updating the model weights and biases (optimization) because\nof its robustness in training deep neural network models. We started the training process\nwith $1e^{-3}$ learning rate, which was decreased by 90% at each 10000 minibatch step. The\nbatch size used for the training was 1024, meaning that 1024 randomly selected training\ndatasets were passed to the model at each training step. The model performance was tested\nusing the area under the traditional receiver operating characteristics curve (ROC) curve as\nwell as the balanced accuracy metric and F1-score (Thibos et al., 1979; Goutte and Gaussier,\n2005). Moreover, to validate the usability of the model, we performed 10-fold cross-validation\nwhere nine near-equal random subsets of data are iteratively used to train the model while\nthe complementary 10% is used for testing. Especially for binary classification models, a\ncross-validation procedure purely based on random data extraction is known to potentially\nproduce overly optimistic results (Brenning, 2012). This happens because a random sampling\nessentially leave the spatial pattern in the data undisturbed, which in turn is translated in\ncross-validation performance results close to the ones obtained by fitting the model to the\nwhole dataset. For this reason, the literature on the topic suggests to include spatial-cross\nvalidations, to complete the prediction performance overview (Pohjankukka et al., 2017).\nThis is the case because a spatial cross-validation draws data not randomly but rather on\nthe basis of their position, ensuring that any underlying spatial coherence is broken. This"}, {"title": "", "content": "produces an unbiased overview of model performance.\nFor the reasons above, we included a 10-fold spatial cross-validation as part of our anal-\nyses. In other words, we subdivided our SU dataset into ten spatial clusters, nine of which\nwere iteratively selected for training, leaving the complentary one for testing. To visual-\nize the ten SU clusters, see Figure 3 in Dahal and Lombbardo (2023), as we used the same\narrangement.\nAside from the performance, the core of this contribution is in its geotechnical explain-\nability. Therefore, it is important to showcase not only the probabilistic output p(s) (i.e.,\nthe susceptibility map) but also an overview of the estimated parameters G(s), and how\nconfidently our model retrieves their values per SU. In the way we defined our PINN, these\nparameters are the ratio of cohesion and landslide thickness as well as the internal friction\nangle. Additionally, we can also extract the critical acceleration ratio to understand how\nmuch seismic excitation is required to initiate a slope failure. To obtain the geotechnical\nparameters (C/tm and <5) and their uncertainty, we bootstrapped the entire dataset 50 times\nby randomly picking 50% of the data each time to train the model and test on the remaining\ndataset. We predicted the geotechnical parameters using each bootstrap model and esti-\nmated the median, 5th percentile and 95th percentile of the dataset. As a result, we can offer\na statistical summary of the geotechnical parameter distribution for each SU."}, {"title": "Results", "content": "In this section, we present the results obtained from performance evaluation with spatial and\nrandom cross-validation, the intermediate geotechnical parameters, and landslide suscepti-\nbility.\nIn general, our PINN produced excellent performance scores (see the classification pro-\nposed by Hosmer and Lemeshow, 2000) with an area under the curve (AUC) of 0.87. \u03a4\u03bf\ncomplete the performance evaluation, we also computed balanced accuracy and F1-score,"}, {"title": "", "content": "these returning values of 0.79 and 0.78, respectively. This shows that the reference model,\nbuilt by using the whole dataset, can accurately differentiate stable and unstable SUs. Fig-\nure 3 completes the performance assessment by presenting the two cross-validation schemes.\nThe outcome of the random cross-validation metrics are shown in Figure 3, with panel (a)\npresenting the result in a confusion map (Titti et al., 2022) and panel (c) showing the corre-\nsponding ROC curves and AUC values. What stands out is that the model is able to predict\nmost of the landslide locations in unseen datasets during 10-fold random cross-validation.\nExcept for a few dark red and dark blue locations in the confusion map, most of the area\nshows a good fit with the observed dataset. Focusing on false positives and negatives, these\nare mostly present at the transition between SU clusters that failed and those that did not\nfail. This translates in a near-outstanding classification performance, the K-fold random\ncross-validation producing consisten AUC values within the range of [0.86-0.90].\nIn an analogous manner, Figure 3 also reports the confusion map produced through\nspatial cross-validation in panel (b), as well as the corresponding performance in panel (d).\nEven in this case, most of the misclassified SUs are present at the transition between\nfailed and unfailed clusters of SU. Looking at the two confusion maps, if the overall patterns\nseem to agree, the spatial cross-validation produces more fluctuating performance, with AUC\nvalues ranging between [0.69-0.89].\nInterestingly, because we used the same data and cross-validation structure as the one\npresented in (Dahal and Lombardo, 2023), we are able to compare the results between this\ncontribution and the previous, based on a standard ANN.\nWhen comparing the confusion map obtained through a random cross-validation, the\npresent contribution and the previous seem to produce similar predictive patterns. It is im-\nportant to recall that deep learning architectures are tasked with maximizing the predictive\nperformance. Therefore, it is not surprising that a model not being informed by the under-\nlying physics, can still perform well. In the end, this is why deep learning is so common\nnowadays."}, {"title": "", "content": "The most interesting aspect emerges when looking at the current confusion map and\nthat of Dahal and Lombardo (see Figure 3; 2023). In fact, the previous model, blind to any\nphysical law, produced a large cluster of false positives in the south east of the study area.\nOn the contrary, the map produce by our PINN does not present the same issue.\nAnalogous considerations can be made examining the predictive performance, with the\ncurrent spatial-cross validation producing a worst performance quantified in a 0.69 AUC,\nwhereas the previous standard neural network returned a 0.58 for the same cross-validation\nfold.\nWe would like to stress the importance of this last observations regarding standard and\nphysics-informed neural networks, and refer the reader to Section 5 where we will further\ndiscuss this topic.\nWhile data-driven models can often represent only performance evaluations and the final\nproduct, our framework also provides the capability to depict the estimated geotechnical\nparameters. Here, we present the bootstrapped cohesion per unit thickness of the failure\nbody c/t and the internal friction angle <5 in Figures 4 and 5, respectively. The values of c/t\nand o appears to be saturated in both 5th and 95th percentile. This is due to both percentiles\nbeing at the far ends of distribution, thus potentially being unsuited to represent the bulk of\nthe distribution. Even though there are fewer spatial variations in those percentiles, the range\nof both parameters is within a commonly accepted and observed range of physical quantities\nGallen et al. (2017). This implies that the model is behaving according to the boundary\nconditions dictated by the physics of the process at hand. Looking carefully at the median,\nthe c/t has a higher range of values along the upper Himalayas. This corresponds to the\nnorthernost region of the study area, a sector mostly hosting rock materials. Conversely, c/t\nvalues progressively decrease in the middle and lower Himalayas where the thickness of soil\nlayer draping over bedrock materials becomes larger. Interestingly, some of the catchments\nshow very low c/t values, which might indirectly indicate a larger thickness of the landslide\nbody, or a deeper potential sliding surface."}, {"title": "", "content": "Overall, the spatial patterns of c/t appeared to be quite saturated, irrespective of the\nquantile of interest. This is likely because by taking the ratio of c over t, the latter term\nshrinks of the parameter distribution, allowing for a smaller spatial variability. This is not the\ncase for <5, as shown in Figure 5. There, a much higher spatial variability of the friction angle\nis retrieved, with reasonable patterns associated to the upper, middle and lower Himalayas.\nSpecifically, the upper section of the Himalayan belt shows higher internal friction angles,\nwherease SUs where landslides occurred are associated to lower internal friction angles.\nTo better understand the predicted results, we present the critical acceleration and the\nobtained susceptibility map through the main model in Figure 6. The critical acceleration\nrepresents how much ground motion is required for a certain slope to fail, and it shows a"}, {"title": "", "content": "mostly random distribution of high and low critical acceleration values, depending on their\ngeotechnical parameters and slope angle. These slopes exhibit some level of spatial pattern\nthat matches the low susceptibility SU, but aside from this, no visible pattern arises. Further\ndiscussions on the estimated critical acceleration will be provided in Section 5.\nAs for the susceptibility map itself, we can observe that the Middle Himalayan region\nwhere most of the landslides occurred are assigned with high susceptibility, and as we move\nsouthward, the susceptibility gradually decreases with few SU clusters appearing with higher\nlandslide susceptibility."}, {"title": "Discussion", "content": "The model we developed may constitute a valuable alternative for landslide hazard mod-\nelling, for it offers a series of advantages with respect to traditional data-driven solutions.\nThe first element of strength, as the PINN definition implies, resides in the ability of our\nmodel to follow the physics of the coseismic failure process. Consequently, as the second\nelement of strength, our PINN treats geotechnical properties as latent predictors, thus ex-\ntracting them from data. On a more generic level, the weakness of many physically-based"}, {"title": "", "content": "methods is precisely their need for explicit geotechnical information. In turn, they are either\nconstrained to small geographic areas or potentially lead to lower prediction capabilities in\nlarge geographic areas. This is the case because acquiring geotechnical data is prohibitive\nwhen considering regional scales. Therefore, whenever applied to address regional slope sta-\nbility problems, these models make assumptions that limit the variability of the required\nparameters. For instance, they assume a single value to be characteristic of the whole study\narea under consideration (Fan et al., 2019).\nOn the other side of the spectrum, data-driven models usually cannot work at the slope\nscale or at a scale involving few catchments. This happens because landslide inventories\nwith numerous records are typically obtained over broader landscapes, unless some extreme\ntrigger locally produced such conditions (e.g., G\u00f6r\u00fcm et al., 2023; Santangelo et al., 2023). In\nother words, even if formally there is no minimum requirement in terms of landslide numbers\nto support any data-driven susceptibility assessment, it is also true that a dataset containing\njust tens of landslides cannot support the estimation of any meaningful statistical relation\nbetween stable/unstable slopes and the given predictor set.\nThis is why data-driven susceptibility models are mostly successful when the involved\ngeographic scales span over regional (e.g., Wan, 2009; Frattini et al., 2010; Lin et al., 2018),\nnational (e.g., Trigila et al., 2013; Lima et al., 2017; Wang et al., 2022) and global (e.g.,\nJia et al., 2021; Felsberg et al., 2022; Tang et al., 2023) territories. However, despite the\npredictive power they ensure, physically-consistent results are not achieved (Glade et al.,\n2005). To date, the best one can do in this sense is to interpret the estimated regression\ncoefficients (Steger et al., 2022, 2024) for statistical solutions or to interpret the predictor\nimportance (Di Napoli et al., 2020; Meena et al., 2022) and more recently the SHAP (Collini\net al., 2022; Wang et al., 2024a) values for machine learning tools.\nWe thus stress that our modelling approach consitutes a first step towards combining\nphysically based and data-driven models to obtain physically consistent, generalizable and\nregional scale landslide susceptibility results."}, {"title": "", "content": "Here, we have combined the permanent deformation approach proposed by Newmark\n(1965) into a deep learning model capable of estimating its parameters. Similar to adding\nground motion and seismic loading to the pseudo-static method of Terzaghi (1950), such that\nthe seismic perturbations may lead to a failure condition, our approach adds the physical\nconstraint and seismic perturbations in data-driven landslide occurrence modelling. Another\nproblem our model tackles is that, even in the case where one can get access to a reliable\ngeotechnical characterization of a regional landscape (to our knowledge such example exists\nin Northridge, US; Dreyfus et al., 2013), the corresponding soil or rock sample are collected\nand tested in laboratory conditions. This implies that the geotechnical characterization\nis valid for a specific location and time. However, these may be subjected to changes in\nresponse to precipitation, vegetation, anthropic influence, etc. In turn, this requires physics-\nbased models to be updated with new data or to be re-calibrated (Clarke and Burbank, 2011;\nTownsend et al., 2020; Singeisen et al., 2022). To some extent, our model is still affected\nby the same problem. However, the level of spatial details at which it retrieves the required\ngeotechnical parameters corresponds to each individual SU, making it a tool that may serve\nthe purpose of inferring a basic geotechnical characterization of a regional landscape. At an\naggregated SU level, the geotechnical changes one may expect at the scale of a single sample\nshould be less prominent. Therefore, our architecture should be, at least theoretically, less\nsensitive to changes, and if this issue would exist at all, it definitely provides a much cheaper\nway to generate a regional geotechnical characterization, as compared to a systematic field\nsurvey campaign.\nIt is important to keep in mind that such advantages may come with limitations when\ncomparing PINNs to standard deep learning architectures. In fact, PINNs add an additional\nand intermediate constraint in the form of physical laws. In general, this translates into\nlower prediction performance compared to a neural network who's free to converge to an\noptimal prediction function solely on the basis of the provided environmental variables. This\nis the generic assumption one can find in the technical literature on PINNS (see, Cuomo"}, {"title": "", "content": "et al., 2022).\nBecause we used the same data as Dahal and Lombardo (2023), we have the chance to use\nour previous work as a benchmark and verify this common hypothesis on PINNs versus pure\nperformance-oriented architectures. Interestingly, in the present case and for both spatial\nand random cross-validation, we can see that the model performs similarly to the previous\nexperiment, if not better.\nFor instance, we can observe that for the 10-fold random cross-validation case (Figure 3),\nthe misclassified landslide locations are present at the boundaries between the SU with and\nwithout landslides.\nThis might be because, due to the same environmental conditions, our model estimates\nsimilar geotechnical parameters. Also, the peak ground motion data is spatially smooth.\nThus, the nearby SUs may appear to be exposed to similar ground motion characteristics.\nThis is a difficult issue to address due to the combination of the physically constrained\napproach and the available predictor data. Therefore, we observe that misclassification is\nmainly located near the transition between failed and stable slope units.\nAs for the case of spatial cross-validation, our PINN performed much better than the\nprevious model without physics (Dahal and Lombardo, 2023). This is clearly visible in\nthe southeastern sector of the study area, where our PINN essentially does not produce\nfalse predictions, whereas the previously published architecture erroneously classified the\nlandscape. We interpret these results in terms of spatial transferability. In other words,\nwhen data is insufficient or performance-oriented models predict over new data, our PINN\nprovides more reliable estimates because they still need to obey the underlying laws of\nphysics.\nLooking into the generated geotechnical characterization of the study area, the estimated\nc/t and the retrieve large spatial variations (in the median maps), as one would expect in\nnature or as opposed to generalized single-value assumptions.\nThe median value range shows a geotechnical valid range of parameters with a spatial"}, {"title": "", "content": "variation where the higher Himalayan rock formations and the Siwalik regions have a higher\n4, whereas the locations in the middle Himalayas with more dense soil composition have\nlower internal friction angle. A similar observation can also be made with c/t, but as it is\nnormalized to possible failure thickness, no distinct variation appears as shallow landslides\nusually occur in the low cohesion zone, and higher cohesive materials often have higher\nlandslide depth.\nAs for the 5th percentile, both parameters present very small spatial variation, and the\nmodel parameters almost become constant, as constant values also can explain the landslide\ndensity as done by Gallen et al. (2015), this is still a realistic estimation.\nIn 95th percentile, we can observe that the o is saturated and has higher values through-\nout the study area with some exceptions and c/t also has the same elevated value range.\nThis shows that the bootstrapped geotechnical properties show a reasonable value range in\nthe median range and can be used as default, but we can also perform a full uncertainty\npropagation with all possible estimated values of geotechnical parameters.\nThe products obtained from the model for critical acceleration and landslide susceptibility\nhave interesting properties. The susceptibility product has a very similar spatial distribution\nto the landslide inventory we obtained. In this case, the susceptibility range has a very\ndistinct range for locations with and without landslides, which shows the model's confidence\nin estimating the landslide susceptibility. One of the most interesting observations is that\ncritical acceleration, being a function of geotechnical parameters and slope, has a random-\nlooking distribution but shows similarities with slope units without landslides in regions\nwhere many landslides are present. Critical acceleration is the amount of seismic excitation\nrequired for a slope to fail, and this can vary depending on the slope's properties. However,\nwhen the higher ground motion exceeds the critical acceleration, it eventually causes higher\nsusceptibility and, eventually, higher landslide density.\nUsing physics-integrated landslide hazard modelling could open up different possibilities\nfor data-driven modelling as it is physically constrained. This framework can be used to"}, {"title": "", "content": "model landslides under climate extremes and different hazard scenarios similar to Dahal et al.\n(2024a). Specifically, because AI-based data-driven models lack generalizability outside of\nthe trained domain, but we have shown that integrating physics in the model can improve\nit. Therefore, one can use it both for rainfall-induced and earthquake-induced landslides in\ndifferent climatological and geophysical scenarios (Cuomo et al., 2022; Maleki et al., 2022).\nMoreover, having a mathematically and physically consistent model with the natural process\ncan provide explainability, where one can explore intermediate products to understand if the\nmodel is behaving correctly or not. This human-on-the-loop approach is important for\nlandslide susceptibility modelling in near real-time scenarios, as it is very important to have\ncorrect predictions to act on them (Dahal and Lombardo, 2023).\nEven though our model has provided a new framework for landslide hazard modelling,\nit still has some limitations and requires future research. The mathematical/physical frame-\nwork that we used is based on permanent deformation analysis, and it is still a simplification\nof the process and assumes a consistent failure plane with a constant slope (Jibson, 2011).\nIn nature, this is often unrealistic and requires simplification of slope by using slope units.\nMoreover, the failure plane is often assumed in this case. The stress-deformation approach,\nhowever, can naturally generate a failure plane given the stress and strain scenarios (Griffiths\nand Lane, 1999). However, it requires stress parameters and a refined mesh-based approach\nfor landslide solution where each mesh element transfers its stress-strain properties to the\nnext element (Memon, 2018). This is quite a common approach in physics-based modelling,\nbut due to the lack of failure propagation on shallow landslide inventory, which occurs within\nvery few seconds, this research could not use the stress-deformation approach. However, it\nmay be suitable for slow-moving landslides on a regional scale, where geotechnical param-\neters are estimated using a neural network, and then the solution for stress deformation is\ndone via a finite-element-based approach (Moeineddin et al., 2023).\nThe landslides are often driven by a complex physical relationship between different earth\nsystem processes, and data-driven models are often incapable of modelling those processes"}, {"title": "", "content": "sequentially or simultaneously (Fan et al., 2019). One of the main processes that control the\nlandslide is a cascading hazard process, and it has many underlying physical mechanisms\nthat can influence slope failures. This is mostly solved using the physics-based modelling\napproach (van den Bout et al., 2021; Pudasaini and Krautblatter, 2021). However, it has the\nsame limitations as we explained before and therefore, using a multi-process physics-based\napproach to solve landslide hazards would generate a more realistic cascading model.\nAnother limitation that we have faced in this work is that the ground motion data we\nhave used comes from an empirical relationship (Wald et al., 2022), and the permanent\ndeformation model that we have used is also of an empirical nature (Jibson, 2007). This is\neasier regarding modelling in near real-time, as the first ground motion estimation is available\nwithin an hour of the major earthquake. We could further improve the quality of model\nestimation using full ground motion waveform as it fully respects Newmark (1965) approach\nand can be used in regional scales with full waveform ground motion simulations (Dahal\net al., 2023). The model structure that is required in this case would need to be changed\nfrom a simple artificial neural network model to a more complex recurrent (Medsker et al.,\n2001) or transformer (Jaderberg et al., 2015) model."}, {"title": "Concluding remarks", "content": "Physics-informed modelling approaches are the future of scientific modelling in any system\nscience, for they are robust and can combine the benefits of data-driven and physics-based\ntools. In landslide science, data-driven and physics-based approaches have dominated two\ndifferent and complementary aspects of hazard assessment procedures, with the former being\nthe most sought solution at regional scales and the former being the standard for local ones.\nBridging the gap between the two by combining their strengths can provide a valid alternative\nfor landslide susceptibility and hazard modelling.\nOur modelling framework integrates the permanent deformation approach as part of a"}, {"title": "", "content": "deep-learning architecture. This avenue uses latent variables to learn the geotechnical param-\neters and use them to estimate the physically constrained landslide occurrence probability.\nOur results show that the estimated values of geotechnical data lie in the observable and\nreasonable range, allowing for their variation over space. This leads to excellent model per-\nformance, reached while ensuring a clear physical explainability. In the immediate future,\nthis framework can be extended as part of near-real-time prediction tools.\nEven though current physical constraints fit the observed data well and show potential\napplications, this framework can be further improved by including a stress deformation\napproach in the landslide initiation. A better inventory with intermediate deformation and\na stress-strain condition is required for this. Moreover, further improvement on this work\ncould be made by including a multi-physical earth system process, which would provide a\nmore general solution to the landslide susceptibility modelling."}]}