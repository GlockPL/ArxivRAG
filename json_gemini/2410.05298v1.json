{"title": "How Do LARGE LANGUAGE MODELS UNDERSTAND GRAPH PATTERNS? A BENCHMARK FOR GRAPH PATTERN COMPREHENSION", "authors": ["Xinnan Dai", "Haohao Qu", "Yifei Shen", "Bohang Zhang", "Qihao Wen", "Wenqi Fan", "Dongsheng Li", "Jiliang Tang", "Caihua Shan"], "abstract": "Benchmarking the capabilities and limitations of large language models (LLMs) in graph-related tasks is becoming an increasingly popular and crucial area of research. Recent studies have shown that LLMs exhibit a preliminary ability to understand graph structures and node features. However, the potential of LLMs in graph pattern mining remains largely unexplored. This is a key component in fields such as computational chemistry, biology, and social network analysis. To bridge this gap, this work introduces a comprehensive benchmark to assess LLMs' capabilities in graph pattern tasks. We have developed a benchmark that evaluates whether LLMs can understand graph patterns based on either terminological or topological descriptions. Additionally, our benchmark tests the LLMs' capacity to autonomously discover graph patterns from data. The benchmark encompasses both synthetic and real datasets, and a variety of models, with a total of 11 tasks and 7 models. Our experimental framework is designed for easy expansion to accommodate new models and datasets. Our findings reveal that: (1) LLMs have preliminary abilities to understand graph patterns, with O1-mini outperforming in the majority of tasks; (2) Formatting input data to align with the knowledge acquired during pretraining can enhance performance; (3) The strategies employed by LLMs may differ from those used in conventional algorithms.", "sections": [{"title": "INTRODUCTION", "content": "Originally trained on textual data, LLMs have demonstrated remarkable success in various tasks, such as reading comprehension and text reasoning (Achiam et al., 2023; Touvron et al., 2023). To evaluate whether LLMs can adapt the text understanding ability across graphs, several studies have investigated this at both the feature and structural levels (Zhao et al., 2023; Chai et al., 2023). Specifically, LLMs have been shown to enhance node features in social networks (Ye et al., 2023; Huang et al., 2024). Additionally, graph structure understanding tasks, such as shortest path and connectivity, have also been evaluated (Guo et al., 2023; Wang et al., 2024).\nGraph patterns, a key aspect of graphs, have yet to be thoroughly explored. Mining graph patterns play a critical role in numerous real-world applications. For instance, they aid in uncovering new insights within biological protein-protein interaction networks (Hu et al., 2005; Tran et al., 2013), identifying key molecular structures in chemistry (Murray & Rees, 2009), and detecting fraudulent activities in transaction networks (Cheng et al., 2020), and so on. In addition, these patterns represent fundamental transferable structures among graphs, such as community groups for friendship recommendations in social networks (Wu et al., 2022) and functional groups for molecular property prediction (Agarwal et al., 2023). Therefore, it is of great interest to investigate whether LLMs possess the capability to comprehend these patterns and effectively apply them to a variety of graph mining and learning tasks."}, {"title": "BENCHMARK SETTINGS", "content": "Primitive graph patterns. We select 9 primitive graph patterns with varying numbers of nodes, edges and connection types, commonly used in network analysis (Cheng et al., 2008; Ying et al., 2019; Li et al., 2021). Specifically, we include 5 undirected graph patterns: triangle, tailed-triangle (T-triangle), square, diamond, and house. For directed graph patterns, we have V-structure (V-S),"}, {"title": "TERMINOLOGY-BASED PATTERNS", "content": "In this section, we first introduce a pattern translation task to assess the alignment between LLMs' comprehension and human understanding of terminology-based graph patterns. Then, we design graph modification and pattern detection tasks to evaluate LLMs' ability to either modify the input graph to include specific patterns, or identify all patterns present in the input graph."}, {"title": "PATTERN TRANSLATION", "content": "We study whether LLMs have an accurate understanding of terminology-based patterns. Given the terminology of a primitive graph pattern, we prompt LLMs to create a graph including this pattern. Besides, we also require that the resulting graph is constrained by a specified number of nodes and pattern occurrences. One prompt example is \u201cGenerate a graph that includes only one triangle, with a total of 20 nodes. Each node should be connected.\" For evaluation, we use accuracy (ACC) to assess whether the output meets the requirement. In addition, to evaluate the creativity of LLMs, we measure diversity of generated graphs by comparing each pair to ascertain their uniqueness. The diversity score (DIV) is defined as DIV = $\\frac{\\text{#Different pairs}}{\\text{#All pairs}}$\nWe repeat experiments 50 times for each pattern and LLM, and present the results in Table 2. Most LLMs effectively understand primitive graph patterns, demonstrating their ability to translate terminology-based descriptions into graph structures. Among the models tested, O1-mini stands out with the highest ACC and DIV on average. LLMs perform badly in T-triangle and V-S patterns because LLMs prefer to add one extra edge to these patterns to make them into other unintended patterns. In addition, LLMs show biased outputs, resulting in low DIV scores for simple patterns. For instance, when processing triangle patterns, GPT-40 often outputs a simple graph containing a triangle with a chain."}, {"title": "GRAPH MODIFICATION", "content": "In graph modification, we construct a dataset of input graphs, each including a specific primitive pattern, and then prompt LLMs to transform one pattern into another. Specifically, we define the following modifications: Square to House (S \u2192 H), Square to Diamond (S \u2192 D), Diamond to Square (D \u2192 S), and FFL to FBL (F \u2192 B). They contain edit operations like adding several edges, removing one edge, or changing the direction of an edge. A prompt example is \"Modify the input graph to include a house pattern. The input graph G is . The number of modified edges should be minimized.\" We evaluate if LLMs modify graphs to include the target pattern by the success rate.\nThe results are presented in Table 3. We observe that O1-mini stands out in its ability to edit diverse terminology-based patterns. Additionally, the scale of the input graphs generally doesn't have a major impact. This is because LLMs generally prioritize high-degree nodes and their neighbors to form the pattern. In larger graphs, LLMs tend to identify more regions for potential edits. These regions are local and invariant to the graph size."}, {"title": "PATTERN DETECTION", "content": "In the pattern detection task, we prompt LLMs to detect specific primitive graph patterns in the input graph. We randomly generate thousands of non-isomorphic input graphs, with three graph scales: small (5\u201315 nodes), medium (15\u201325 nodes), and large (25-35 nodes). The example prompt is \"Identify the occurrence of the given pattern in the input graph. The pattern is a diamond, defined as a 4-node motif with five edges. The input graph is .\" For each pattern, we calculate the F1 score to evaluate LLM performance in terms of precision and recall.\nThe results for small and medium scales are shown in Figure 1, with the full results provided in Appendix Table 16. Overall, the performance of most LLMs declines as the pattern becomes more complex and the input graph grows larger. This is because LLMs must examine every pair of nodes to determine whether they align with the pattern. Notably, O1-mini is less affected by the graph scale, as its step-by-step approach may help mitigate this issue."}, {"title": "TOPOLOGY-BASED PATTERN", "content": "In this section, we first perform pattern isomorphic mapping to examine LLMs' understanding of topology-based graph patterns. Subsequently, we conduct graph modification and pattern detection tasks to further evaluate LLMs' capabilities with various patterns and graph scales."}, {"title": "PATTERN ISOMORPHIC MAPPING", "content": "In a topology-based description, isomorphic graphs can appear different due to the distinct naming of nodes. Thus, LLMs may mistakenly recognize isomorphic graphs as distinct entities. In this task, we test whether LLMs find a one-to-one correspondence (bijection) between the node sets of two isomorphic graphs. We reuse the graph datasets in the terminology-based pattern detection task. For each graph, the original node IDs range from 0 to 35. We shuffle and relabel the node IDs to span from 100 to 135 to create the isomorphic graphs. Each sample is guaranteed to have at least one valid mapping solution. We employ accuracy as the evaluation metric, measuring how effectively LLMs map node IDs correctly between two graphs.\nThe results, presented in Table 4, show that most LLMs perform well on small datasets, but their performance degrades as the graph scale increases. However, GPT-40 and Claude still maintain high accuracy using the edge list format. Consequently, we select medium- and large-scale graphs for further analysis. Upon manually reviewing the output text from GPT-40, Claude, and O1-mini, we found that there exist two algorithms to help LLMs reason whether the graphs are isomorphic. One approach involves first calculating node degrees and then mapping nodes with matching degrees, while the other compares the edge list directly. Due to the LLMs' inherent difficulty with counting, results based on degree counting tend to be less reliable. As shown in Figure 2, O1-mini uses the degree counting approach with the proportion 89%. Only about 30% of the outputs from Claude and GPT-40 use degree counting, with the majority relying on edge comparison. As LLMs are not good at counting, the overall accuracy of Claude and GPT-40 is significantly higher than that of 01-mini."}, {"title": "GRAPH MODIFICATION", "content": "We use the same setting as that in the terminology-based pattern detection, except the descriptions are now replaced with topology-based descriptions. Note that since the graph nodes are represented by numerical IDs, we use uppercase letters for pattern nodes to prevent confusion for LLMs. The results are shown in Table 5. Similar to the terminology-based case, O1-mini still delivers the best performance, and the graph scale has little impact on the outcome. However, the average accuracy for topology-based descriptions is lower than that for terminology-based ones, likely due to increased hallucinations. For instance, when modifying diamond graphs to include square patterns using the edge list format, the presence of squares within diamond structures prevents LLMs from exhibiting any change."}, {"title": "PATTERN DETECTION", "content": "We use the same setting as that in the terminology-based pattern detection, except the pattern descriptions are now topology-based descriptions. The results are shown in Figure 3, with full results provided in Appendix Table 17. The performance of LLMs is also influenced by their capabilities, graph scales, and the complexity of patterns. For instance, while O1-mini achieves a high F1 score in detecting tailed-triangle, square and diamond patterns, other models only reach an F1 score of 20%. Moreover, although O1-mini performs well on small-scale graphs, it fails on medium-scale datasets. Among the patterns, the house pattern is particularly difficult to detect, as it is the most complex pattern in our setting."}, {"title": "DATA-DRIVEN PATTERNS", "content": "In this section, we focus on three classic data-driven patterns that are useful in many areas: densely connected subgraphs, frequent subgraphs within graphs, and discriminative patterns based on labels. By examining densely connected subgraphs, such as k-cores, we can identify communities and make friendship recommendations in social networks. Frequent subgraphs help uncover common structural motifs, which is valuable in chemistry and biology. Discriminative patterns enable us to distinguish between different classes within the data, aiding in classification tasks."}, {"title": "DENSE SUBGRAPH MINING (K-CORE)", "content": "K-core is a commonly used densely connected subgraph, defined as a maximal subgraph containing nodes of degree k or more. In this task, we prompt LLMs to leverage the k-core algorithm (Batagelj & Zaversnik, 2003) to identify 3-core in the input graph. We compute Precision as the metric by judging whether the identified nodes belong to a 3-core and present the results in Table 6.\nOn average, LLMs achieve over 60% precision across various graph scales, demonstrating their ability to execute the algorithm effectively. Notably, LLMs obtain better performance on large-scale graphs than on smaller ones, likely because most nodes in large graphs have degrees greater than 3. LLMs tend to make errors when node degrees are close to 3 but become increasingly accurate as node degrees increase. In Figure 4, we select GPT-40 and O1-mini to analyze the relationship between node degrees and precision scores. Nodes with a degree of 3 have 50% precision, while those with degrees above 5 reach nearly 100% precision."}, {"title": "FREQUENT SUBGRAPH EXTRACTION", "content": "Mining frequent subgraphs is an important task on graphs, defined as finding subgraphs that appear frequently in a graph dataset given a frequency threshold. In this task, we set a 100 turns evaluation."}, {"title": "DISCRIMINATIVE PATTERN LEARNING", "content": "This task is to extract and learn important patterns from input graphs to discriminate labels. The labels, which typically represent categories or outcomes, act as a guide for discovering the most significant patterns for the task. We implement a two-step process to instruct LLMs, as illustrated in Figure 6. We first sample an equal number of graphs from each label to form a dataset. Then, we prompt LLMs to identify discriminative patterns that differentiate one label from another. This process is repeated multiple times, and all extracted patterns are retained. We further filter these patterns: for each pattern, we ensure that over 90% of graphs with the corresponding label contain the identified pattern, while 90% of graphs with other labels do not. The remaining are regarded as discriminative patterns. We use two metrics to evaluate the discriminative patterns. We introduce an extra classification task to measure the effectiveness, where we ask LLMs to predict the labels of new graphs based on the discriminative patterns and compute the prediction accuracy. In addition, we calculate the discriminative pattern ratio as D.P. = $\\frac{\\text{#Discriminative patterns}}{\\text{#Extracted patterns}}$, which assesses the efficiency of the extraction process."}, {"title": "EVALUATION ON REAL-WORLD GRAPHS", "content": "In this section, we gather a wide range of real-world datasets across diverse domains, including molecules, social networks, bioinformatics, and computer vision. Different from synthetic dataset, the real-world graphs have node attributes and potential noise in the graph structures, which makes the graph pattern tasks more challenging. More details of datasets and prompts are provided in the appendix C. In this evaluation, we focus on pattern detection and discriminative pattern learning.\nMolecule Pattern Detection. Followed by Section 3.3 and Section 4.3, we perform pattern detection in molecular graphs, where the goal is to ask LLMs to detect the target pattern in the given molecule. We evaluate three datasets: Benzene, Alkane-Carbonyl (R-CO), and Fluoride-Carbonyl (F-CO), where the target patterns are hexagons, alkane groups, and fluoride groups, respectively. We compare the terminology-based with topology-based patterns, and also assess a combined approach, referred to as Both.\nTable 9 illustrates that terminology-based descriptions outperform topology-based ones for both the Benzene and R-CO datasets. However, combining both descriptions does not enhance performance, as the final F1 scores are slightly lower than using terminology alone. This indicates that in real-world applications, LLMs may rely on internal knowledge, such as retrieving the name of patterns, to understand molecular structures.\nDiscriminative Pattern Learning and Classification. Based on Section 5.3, we conduct the 2-step process of discriminative pattern learning and classification on real-world datasets. We use hundreds of samples for discrimination and test about 50 samples in classification. Further details can be found in Appendix Table 13.\nTable 10 presents the results for both binary and multi-label classification accuracy on the molecular (MUTAG, OGB-MOL, OGB-BBBP), social network (IMDB-BINARY, IMDB-MULTI), bioinformatics (ENZYYMES), and computer vision (FINGERPRINT) datasets. Notably, we highlight the most discriminative patterns extracted by GPT-4, which provide meaningful insights into the given dataset. For instance, in the MUTAG dataset, NO2 groups are marked as positively influencing mutagenicity (Agarwal et al., 2023). Due to the current high cost of LLMs, we performed limited sampling during the discrimination process. We believe that increasing the sampling size in this stage would improve the overall classification score."}, {"title": "DISCUSSIONS", "content": "In this section, we endeavor to provide intuitions and insights on LLMs' ability in graph pattern tasks. We first provide our theoretical intuitions from the perspective of expressiveness and then summarize key insights from our observations."}, {"title": "THEORETICAL INTUITIONS", "content": "In this subsection, we discuss the theoretical intuitions of LLMs' capabilities in graph pattern recognition from an expressiveness standpoint, often used in analyzing Transformers (Feng et al., 2024; Li et al., 2024; de Luca & Fountoulakis, 2024). We demonstrate that Transformers, given graph inputs, can be configured to perform graph pattern tasks. The LOCAL framework Angluin (1980) is a distributed computing paradigm, implemented by the message passing among the nodes. Many graph pattern algorithms such as pattern detection can be efficiently implemented by LOCAL (Drucker et al., 2014; Korhonen & Rybicki, 2017). Intuitively if Transformers can simulate any LOCAL algorithms, it has the capability for graph pattern tasks. We build the following theorem for this intuition.\nTheorem 1. (Informal) For any LOCAL algorithm A, there exists a Transformer with edge list as input that can simulate A.\nThe detailed proof is given in Appendix B. The proof intuition is that the attention mechanism is a message-passing between the tokens and each token is executed in a parallel manner. Consequently, this mechanism enables Transformers to simulate the message passing between nodes. The above theorem suggests the existence of such weights."}, {"title": "EMPIRICAL INSIGHTS", "content": "LLMs may employ strategies differently from traditional algorithms. Previous theoretical works (Feng et al., 2024; Wu et al., 2024; Li et al., 2024; de Luca & Fountoulakis, 2024) demonstrate that LLMs can perform graph-related tasks by simulating traditional graph algorithms (with chain-of-thought). However, in many cases, the LLMs pretrained by language could use strategies differently from traditional algorithms. First, while frequent subgraph extraction is computationally more challenging than k-core detection, LLMs perform better on the former. This is because LLMs struggle with simple degree counting and fail to execute specific algorithms effectively. Second, the format of the adjacency list and edge list can be easily converted to each other in linear time, but the adjacency list is often better than the edge list in experiments. We hypothesize that the adjacency list format provides a more concise representation of a node's edges, which aligns well with the locality of attention mechanisms in LLMs (Xiao et al., 2023). Third, in the pattern isomorphic mapping task, O1-mini prefers the degree counting approach with the proportion 89% while only about 30% of the outputs from Claude and GPT-40 use degree counting, as shown in Figure 2.\nInput format that aligns with the pretrained knowledge improves the performance. Our experiments demonstrate that LLMs have basic knowledge about graph patterns and they help LLMs in graph pattern tasks. First, in Section 3.1, LLMs can translate the terminologies to the topology descriptions, which shows that LLMs have the basic knowledge about graph patterns. Second, comparing the results in Section 3.3 and those in Section 4.3, we find that terminology-based graph pattern detection is usually better than topology-based ones. It indicates that with terminology as input, LLMs make use of their internal knowledge to improve performance. Third, comparing the results in Section 3 and Section 4, we find that the terminology-based description is often better than topology-based. In most cases, the adjacency list input is better than the edge list input.\nO1-mini often gives the best results, but not always. In most tasks, O1-mini has the best performance, especially when the graph size is large. This is not surprising because it has a much longer chain-of-thought. For example, in the pattern detection task, which requires LLMs to search node combinations one by one, O1-mini holds a clear advantage. However, it falls back in the discriminative pattern learning by labels in Section 5.3. The reason is that O1-mini strictly searches for patterns that exactly meet the data requirements, without allowing for fuzzy matching. This results in smaller discriminative patterns being found. Consequently, when O1-mini attempts graph classification, the patterns struggle to generalize, leading to lower classification accuracy."}, {"title": "RELATED WORK", "content": "Large Language Models for Graphs. Here we introduce previous work using LLMs based on the types of graph tasks they focused on. The first category is to solve graph algorithm tasks, such as connectivity, degree counting, cycle check, shortest path, and Hamilton path. NLGraph (Wang et al., 2024) and Talk like a graph (Fatemi et al., 2023) translated graph structures into natural language, and leveraged techniques like few-shot prompting or chain-of-thought (CoT) to feed these descriptions directly into LLMs for inference. Since exact computational algorithms can be employed to obtain an accurate reasoning process and results, GraphInstruct (Luo et al., 2024) built the instruction-tuning dataset and fine-tuned LLMs by Lora. Moreover, Graphwiz (Chen et al., 2024a) extended the scale and diversity of the dataset and applied Direct Preference Optimization (DPO) loss to enhance the performance. Recently, GraphToken (Perozzi et al., 2024) utilized a GNN-like encoder to process the input graph and train this encoder with a frozen LLM.\nThe second category encompasses graph learning tasks, which typically include node classification, link prediction, and graph classification (Chen et al., 2024c). Unlike graph algorithm tasks relying on deterministic rules for inference, graph learning tasks involve learning the relationship between graphs and labels using provided training examples. Graph-LLM (Chai et al., 2023) and GraphText (Zhao et al., 2023) designed prompts that convert graph structure, node/edge attributes, and label information into natural language, enabling LLMs to make predictions without additional training. In contrast, other approaches incorporate projectors or encoders before LLMs and construct datasets for tuning. LLaGA (Chen et al., 2024b) and MuseGraph (Tan et al., 2024) reorganized the center node and its neighborhood information as a structure-aware textual description, which is then tokenized by a projector and fed into LLMs to predict node labels in citation graphs. GraphGPT (Tang et al., 2024) utilized a GNN encoder to tokenize the input graph before processing it with the LLM to address node classification in text-attributed graphs. MolCA (Liu et al., 2023) and InstructMol (Cao et al., 2023) used similar GNN encoders for molecular structures to predict their properties.\nGraph Pattern Discovery. Graph patterns (also known as substructures, graphlets, motifs and subgraphs) have been extensively studied before the era of LLMs due to their crucial role in real-world applications, particularly in chemistry (Murray & Rees, 2009), biology (Hu et al., 2005), and social science (Wu et al., 2022). Numerous exact algorithms have been proposed for pattern discovery. For example, the algorithms designed to identify and count dense pre-defined patterns, such as k-core, k-cycle, and k-clique, were introduced in Milo et al. (2002); Zhang & Parthasarathy (2012). These methods were further extended to attributed graphs to capture more realistic and complex patterns (Hu et al., 2019). Frequent subgraph mining algorithms were developed to efficiently search for possible patterns (Yan et al., 2008; Elseidy et al., 2014), aiming to reduce the search space as much as possible.\nIn recent years, deep learning, particularly graph neural networks (GNNs), has become prominent in this area. Many studies (Chen et al., 2020; Zhang et al., 2023; Luo et al., 2022) investigated the theoretical guarantees of specific GNNs capable of identifying certain substructures, and developed subgraph-aware GNNs with strong expressiveness to improve the accuracy of graph learning tasks.\nAs previously mentioned, few studies have addressed graph pattern discovery using LLMs, despite their importance for many downstream tasks, which still require further exploration."}, {"title": "CONCLUSION", "content": "We investigate how LLMs understand graph patterns. While recent studies demonstrate that LLMs have shown success in various graph-related tasks, the comprehension of graph patterns remains unexplored. To bridge this gap, we propose a benchmark featuring both synthetic and real-world data, spanning 11 subtasks evaluated across 7 LLMs. Using known patterns defined through terminology-based and topology-based descriptions, we conduct tasks including pattern translation, isomorphic mapping, graph modification, and pattern detection. For unknown patterns, we cover classic tasks such as densely connected subgraph detection, frequent pattern mining and discriminative pattern learning using labeled graph data. The real-world datasets are gathered to further assess LLMs' ability to handle the attributes and noise in the pattern. Our results show that LLMs possess a preliminary capability to understand graph patterns and perform tasks related to them effectively."}]}