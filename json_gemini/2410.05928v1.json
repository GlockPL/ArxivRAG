{"title": "Beyond Captioning: Task-Specific Prompting for Improved VLM Performance in Mathematical Reasoning", "authors": ["Ayush Singh", "Mansi Gupta", "Shivank Garg", "Abhinav Kumar", "Vansh Agrawal"], "abstract": "Vision-Language Models (VLMs) have transformed tasks requiring visual and reasoning abilities, such as image retrieval and Visual Question Answering (VQA). Despite their success, VLMs face significant challenges with tasks involving geometric reasoning, algebraic problem-solving, and counting. These limitations stem from difficulties in effectively integrating multiple modalities and accurately interpreting geometry-related tasks [1]. Various works claim that introducing a captioning pipeline before VQA tasks enhances performance [2]. We incorporated this pipeline for tasks involving geometry, algebra, and counting and found that captioning results are not generalizable specifically with larger VLMs primarily trained on downstream QnA tasks showing random performance on math-related challenges. However, we present a promising alternative: task-based prompting, enriching the prompt with task-specific guidance. This approach shows promise and proves more effective than direct captioning methods for math-heavy problems.", "sections": [{"title": "1 Introduction", "content": "With the rise of Large Language Models, which demonstrate the ability to understand and generate text for tasks beyond their explicit training, Vision-Language Models have extended these capabilities to multimodal tasks involving images and text [3]. These models excel in tasks like Visual Question Answering (VQA), image captioning, and object segmentation [4]. However, recent studies reveal that VLMs struggle with simple, low-level visual tasks that humans easily solve, highlighting a need to enhance their visual reasoning and understanding [5].\nSome works try to improve VLMs' reasoning abilities by fine-tuning the VLMs [6], [7], [8]. Additionally, some research on VLMs focuses on improving their question-answering abilities through a two-step process: captioning followed by question-answering. This method takes advantage of VLMs' pre-training in text generation, as many tasks require generating text descriptions. The main challenge lies in the model's ability to effectively combine and interpret multimodal information, understanding visual and textual inputs while capturing their interactions. One area where VLMs consistently underperform is counting [9], primarily due to the scarcity of training data that accurately labels object counts, especially as the number of objects increases. While captioning has improved performance in some tasks [2], we hypothesize that these improvements are not generalizable and depend on various factors, which we aim to explore through our experiments. Further, captioning fails to capture all the attributes of the image, which is especially crucial in mathematical tasks.\nTo address these limitations, we introduce prompting techniques designed to enhance the models' reasoning capabilities. we specifically constructed prompts based solely on the question, excluding any direct information about the answer. These approach-based prompts were tested in both direct"}, {"title": "1.1 Background and Related work", "content": "Several studies have examined the reasoning and comprehension abilities of Vision-Language Models in various tasks requiring spatial understanding and reasoning capabilities. These studies have demonstrated that multimodal language models rely less on visual information and perform better when they are given adequate textual cues.[10], [11]. While methods like few-shot prompting [12] have been shown to improve the performance of VLMs [13], these models continue to struggle with mathematical tasks, particularly counting, leading some to describe them as \u201cblind\u201d to numbers [5]. Recent research suggests that much of the reasoning performed by VLMs may stem more from the phrasing of the questions rather than the images themselves. This is evident in tasks that heavily rely on visual information, such as counting nested squares or identifying line intersections, where VLMs consistently underperform. Datasets like Math Vision [14] and Count Bench[9] have been developed specifically to test these visual reasoning abilities.\nTo enhance Visual Questioning (VQA) performance, various techniques have been proposed, including the use of question-driven image captions, which are subsequently fed into language models. These approaches have demonstrated potential to enhance outcomes in specific tasks, such as direct image-based QnA. [2]. However, whether such captioning-based techniques can reliably enhance VLM performance on math-related tasks has been explored in our work."}, {"title": "2 Method", "content": "We assessed the Vision-Language Models on a range of geometry-related tasks. We take tasks from four datasets containing different questions to ensure our tests' robustness and generalization. These datasets contain a variety of tasks related to geometry, counting, algebra, and mathematical reasoning. We use a diverse set of VLMs in our experiments to assess the generalizability of our approach. One closed-source large model was taken, Gemini-1.5-Flash, and three open-sourced smaller models, LLaVa, Florence-2, and Phi 3.5 Vision Instruct, were chosen. Such a range of models was chosen to ensure variation in size, from smaller ones with fewer parameters to larger, more complex models. They were tested across eight distinct tasks, divided into two main categories:\n1. Question-Answering (QnA): Using a classical zero-shot approach, each model was directly queried with questions related to images from the datasets.\n2. Captioning: We generated captions for the image using the base model. After generating the captions, we fed them back into the LLM, and QnA was performed on the generated caption using the LLM.\nWe further tested the impact of incorporating additional information and context into the prompts. Specifically, we provided explicit guidance to solve the problem, which was generated using an"}, {"title": "3 Experiments and Results", "content": "We chose diverse models and techniques to prove and test our hypothesis. We chose four datasets, Geo170k, CountBench, Blind, and MathVision, containing various tasks related to geometry, reasoning, algebra, and counting. Also, we split the MathVision dataset into three subparts: mainly vision-based, geometry-based, and mathematics-based . The approach-based prompting improves overall results for both Direct VQA and Caption-based QA . Further, we observe a drop in performance when prompted with the adversarial approach and an overall increase in performance compared to baseline when prompted with the random approach. We also observe that the performance of the models varies across different datasets. Models perform best on the CountBench dataset, which focuses on counting tasks, while they perform poorly on the MathVision Dataset due to the complexity of the tasks. Further, on the MathVision dataset, we observe better performance on visual-based tasks as compared to mathematics-related tasks. Also, performance on geometry related tasks is observed to be relatively poor."}, {"title": "4 Conclusion", "content": "The results of our study align with our initial hypothesis: VLMs exhibit significant limitations when it comes to mathematical tasks, particularly those involving numbers and counting. While"}, {"title": "5 Limitations and Future Work", "content": "Testing the reasoning capabilities of multimodal models is a broad area of research, and we propose ways to improve the generalizability of these models in our research. Due to resource constraints, we couldn't experiment with many mainstream and large-scale models, especially state-of-the-art Vision-Language Models (VLMs). With more computational resources and funding, future work could focus on scalability and robustness across a wider range of model architectures such as Claude Sonnet[16], GPT 4[17], etc.\nAdditionally, advanced methods, such as sophisticated prompt engineering techniques or incorporating domain-specific knowledge-could enhance the models' ability to generate more accurate and contextually relevant captions. Moreover, replacing the QA model with interpretable alternatives could offer greater transparency and insights into the decision-making process, thus shedding light on the model's reasoning and performance in a more understandable way."}, {"title": "A Prompting", "content": "To enrich the prompts used in our experiments, we employed Gemini 1.5 Flash. The question was presented to the model without an accompanying image. The model was tasked with generating responses under three different conditions:\n1. Approach-Based: The model was asked to provide an approach for solving the question.\n2. Adversarial: The model was prompted to generate a misleading or incorrect approach to solving the question. Although inaccurate, the response needed to be plausible.\n3. Random: The model was asked to generate a random string.\nIn the Captioning-based experiments, we utilized the LLaMA 3.1 Instruct 8B\u00b9 model to generate keywords based solely on the question. These keywords provided initial guidance for the captioning task. For the zero-shot captioning, we instructed the respective model being tested to generate a caption for the image using the keywords provided. Similarly, for the rest of the captioning tasks, we asked the QnA base model to generate image captions, using both keywords and additional hints generated in the same manner as described above (Approach-Based, Adversarial, Random), following which the answer was generated by passing the caption to another LLM(Llama 3.1 Instruct).\nNote: For Florence-2 captions were generated using <DETAILED CAPTION>, and the approach was passed onto the detailed caption. For other models, the approach was passed during the caption generation stage. Additionally, the Florence-2 direct checkpoint was unable to perform QnA-related tasks, so we used Florence-2 DocQnA for QnA-related tasks."}, {"title": "B Experiment Details", "content": "Certain experimental details worth mentioning for the respective models used are:\n\u2022 LLaVa: For LLaVa, we used the GrokAPI 2 to access the model.\n\u2022 Gemini-1.5-Flash: For Gemini, we used Google AI studio\u00b3 to access the model.\n\u2022 Florence-2:45 For Florence-2, we used the open-sourced model available on huggingface .\n\u2022 Phi 3.5 Vision Instruct: we used the open-sourced model available on huggingface."}, {"title": "C Datasets", "content": "The following datasets were used for our experiments:\n\u2022 Math Vision: The Math Vision dataset is a curated collection of 3,040 high-quality mathematical problems with visual contexts from real math competitions. For our experiments, we broadly divided the dataset into three categories:\n- Visual Based: This was originally split into Area, Angles, and Length-related tasks.\n- Geometry Based: This was originally split into categories: Analytical Geometry, Combinatorial Geometry, Transformation Geometry, Descriptive Geometry and Solid Geometry.\n- General Mathematics: This was originally split into categories: Graph Theory, Logic, Algebra, Combinatorics, Statistics, and Arithmetic.\n\u2022 Blind: The Blind dataset consisted of images and question-answer pairs about visual tasks. We used a subset of 150 images per task. The tasks include counting the number of"}, {"title": "D The Random Approach", "content": "When using the random approach, we observed that performance often surpassed the baseline, with significant improvements in some cases. We hypothesize that the model tends to disregard random information, but in doing so, it becomes more cautious and focused on providing a correct response. This contrasts with the baseline, where such behavior is less apparent. While these findings are promising, there is still potential for further research to understand and refine this approach fully."}, {"title": "E Task Wise Keywords", "content": ""}]}