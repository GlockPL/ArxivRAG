{"title": "Taming the Tail: Leveraging Asymmetric Loss and Pad\u00e9 Approximation to Overcome Medical Image Long-Tailed Class Imbalance", "authors": ["Pankhi Kashyap", "Pavni Tandon", "Sunny Gupta", "Abhishek Tiwari", "Ritwik Kulkarni", "Kshitij Sharad Jadhav"], "abstract": "Long-tailed problems in healthcare emerge from data imbalance due to variability in the prevalence and representation of different medical conditions, warranting the requirement of precise and dependable classification methods. Traditional loss functions such as cross-entropy and binary cross-entropy are often inadequate due to their inability to address the imbalances between the classes with high representation and the classes with low representation found in medical image datasets. We introduce a novel polynomial loss function based on Pad\u00e9 approximation, designed specifically to overcome the challenges associated with long-tailed classification. This approach incorporates asymmetric sampling techniques to better classify under-represented classes. We conducted extensive evaluations on three publicly available medical datasets and a proprietary medical dataset. Our implementation of the proposed loss function is open-sourced in the public repository: https://github.com/ipankhi/ALPA.", "sections": [{"title": "Introduction", "content": "Medical image classification is a crucial component in the development of effective diagnostic as well as prognostic tools [28]. The utility of these tools often relies on the ability to manage and interpret large volumes of medical imaging data. However, a pervasive challenge encountered in these datasets is the prevalence of a long-tailed distribution\u2014a scenario where the majority of data samples belong to a few dominant classes, while the remaining classes have significantly fewer samples [23]. This imbalance poses significant challenges in training accurate classifiers, as conventional machine learning algorithms often struggle to learn from classes with limited samples [16]. The existence of long tails in medical image datasets can be attributed to several factors, such as the rarity of certain medical conditions or diseases leading to a limited number of samples available for those classes [27]. As a result, these classes have few positive examples, making them challenging to detect and classify accurately. Furthermore, data collection in medical imaging is often biased towards common and easily accessible conditions, resulting in an uneven representation of different classes [9], [12]. The challenges posed by long-tailed class distributions in medical image classification have thus prompted researchers to explore various solutions.\nIn their survey \"Deep Long-Tailed Learning,\" Zhang et al grouped existing solutions into three main categories: class re-balancing, information augmentation, and module improvement [38]. These were further classified into nine sub-categories; Re-sampling methods, such as oversampling and undersampling, involve altering the class distribution in the training set [6], [21]. Class-sensitive learning methods, like re-weighting [19], [8],[34] and re-margining [5] aim to re-balance training loss values for different classes promoting equitable learning, while logit adjustment techniques [25] aim to re-calibrate the output probabilities of the classifier to account for the imbalanced class distribution. Transfer learning aims to enhance model training on a target domain by transferring knowledge from a source domain [10] and data augmentation techniques diversify datasets by either applying transformations directly to existing data or by utilizing generative AI methods, such as Generative Adversarial Networks (GANs) and Diffusion models, to create new samples [30], [33], [32]. Representation learning methods aim to learn more discriminative feature representations that can better separate different classes [15], [37], while classifier design involves optimizing the architecture and parameters of the classifier to improve its performance on long-tailed datasets by transferring geometric structures from head classes to tail classes [20]. Decoupled training techniques decouple the training of the classifier into two stages: a representation learning stage and a classifier learning stage [14]. Finally, ensemble learning methods combine multiple classifiers, each trained on different subsets of the data or with different techniques, to improve classification performance [40], [18].\nLoss functions play a crucial role in guiding model training. Class-sensitive loss functions are designed to mitigate the adverse effects of class imbalance by adjusting the contribution of each class to the overall loss calculation. These loss functions aim to ensure that the model does not disproportionately prioritize majority classes over minority ones during training. By doing so, they help alleviate the challenges associated with skewed class distributions and improve the model's ability to generalize across all classes. Focal loss, introduced by [19], is a classic strategy to mitigate long-tailedness in classification tasks by dynamically adjusting the weighting of different examples during training to focus more on hard-to-classify samples. Similarly, class-balanced loss [8] assigns weights to different classes inversely proportional to their frequencies. Asymmetric loss [2] and asymmetric polynomial loss [11] are variants of loss functions designed to penalize misclassifications of minority classes more heavily than majority classes."}, {"title": "Our contribution", "content": "Polynomial expansions allow for the modeling of higher-order interactions between variables that linear models typically miss, thus providing a more nuanced and detailed depiction of data behaviors. Additionally, this method can be particularly useful in healthcare image analysis domains where capturing non-linear patterns is essential for predicting outcomes with high accuracy. By incorporating polynomial terms, models can approximate a wider range of functions, thereby adapting more effectively to the underlying complexities of the dataset [17].\nThe Pad\u00e9 approximation [35] is a mathematical technique that approximates a function through a ratio of two polynomials rather than relying solely on polynomial expansions. In earlier works, learnable activation functions based on the Pad\u00e9 approximation have shown promising performance [26], [3]. This method is particularly effective in modeling functions with singularities and provides a more accurate approximation over certain intervals. By applying the Pad\u00e9 approximation to the BCE loss function, we aim to achieve a more precise representation of the loss landscape, enabling our model to adjust more effectively to the true distribution of training data. Asymmetric focusing addresses the imbalance between the positive and negative classes by applying different weights to the loss contributions of each class. This technique is crucial in long-tail scenarios, where the minority class requires greater emphasis to ensure sufficient model sensitivity towards less frequent conditions.\nIn our research,\n\u2022 We introduce a novel approach to address the challenge of long-tailed medical image classification by proposing a Pad\u00e9 expansion-based polynomial loss function.\n\u2022 Furthermore, by implementing an asymmetric focus, this loss function demonstrates enhanced classification performance for under-represented classes compared with other loss function-driven techniques in long-tailed problems.\n\u2022 We rigorously tested the efficacy of our method (Asymmetric Loss with Pad\u00e9 Approximation [ALPA]) across three publicly available medical image datasets in addition to a proprietary medical image dataset."}, {"title": "Related Work", "content": "The development of loss functions tailored for imbalanced datasets has been a focal point of research. The standard cross entropy loss is a commonly used loss function for classification tasks, defined as:\n$L_{CE} = -\\frac{1}{K}\\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)$,\n$L_{CE} = \\sum_{i=1}^{K}(1-y_i) \\log(1 - \\hat{y}_i)$,\nwhere K is the number of classes, and $y_i$ and $\\hat{y}_i$ represent the ground-truth and estimated probabilities for class i respectively. However, when dealing with imbalanced datasets, the cross entropy loss (Equation 1) treats all class samples equally and does not consider the imbalanced distribution. Thus, it tends to prioritize majority classes, leading to suboptimal performance on minority classes. Lin et al [19] proposed Focal Loss (Equation 2) as a modification, which dynamically adjusts the loss weights based on the predicted probabilities. This enables Focal Loss to down-weigh the loss assigned to well-classified examples and focus more on difficult-to-classify instances. It is formulated as follows:\n$L_{Focal} = \\alpha_+ (1 - \\hat{y})\\log(\\hat{y})$\n$L_{Focal} = \\alpha_-\\hat{y}\\log(1 - \\hat{y})$\nwhere $\\alpha_+$ and $\\alpha_-$ are the balancing factors for positive and negative losses, respectively, and $\\gamma$ is the focusing parameter. Notably, setting $\\gamma = 0$ yields the binary cross-entropy loss. However, Focal Loss uses the same focusing parameter $\\gamma$ for both positive and negative losses. This can lead to suboptimal performance, especially in scenarios where the tail classes require different treatment compared to the head classes.\nThe Asymmetric Loss [2] introduces an asymmetric weighting scheme to alleviate the weaknesses of the Focal Loss. Equation 3 assigns different focusing parameters for positive and negative losses, allowing for separate optimization of the training of positive and negative samples. It is defined as:\n$L_{ASL} = (1 - \\hat{y})^{\\gamma_+} \\log(\\hat{y})$\n$L_{ASL} = \\hat{y}^{\\gamma_-}\\log(1 - \\hat{y})$\nwhere $\\gamma_+$ and $\\gamma_-$ are the focusing parameters for positive and negative losses respectively.\nThe Class-Balanced (CB) Loss [8] is another technique aimed at mitigating the challenges posed by class imbalance in training datasets and is formulated as follows:\n$L_{CB} = \\frac{1}{K}\\sum_{k=1}^{K} \\frac{1 - \\beta^{\\gamma}}{1 - \\beta^{\\gamma} y_k} y_k \\log(\\hat{y}_k)$\nwhere $\\gamma$ is the focusing parameter and $\\beta$ is a hyperparameter controlling the balance between the effective number of samples for each class and the average effective number of samples. Unlike traditional loss functions, CB loss (Equation 4) introduces a mechanism to dynamically adjust the weights of different classes during the training process. This adjustment is based on the effective number of samples for each class, thereby ensuring that minority classes receive higher weights compared to majority classes. In [13] Jamal et al shows that class-balanced loss can underperform due to the domain gap between head and tail classes. Similarly, the Label-Distribution-Aware Margin (LDAM) Loss [5] is a loss function designed to enhance the discriminative power of deep neural networks by explicitly maximizing the margins between different classes. Unlike traditional loss functions like cross-entropy, LDAM loss focuses on optimizing the margins between classes in the feature space, thereby promoting better class separation and improved generalization performance. However, negative eigenvalues can persist in the LDAM loss landscape for tail classes due to insufficient data representation, leading to directions of negative curvature [31], making it inefficient for achieving effective generalization on tail classes."}, {"title": "METHOD", "content": ""}, {"title": "Pad\u00e9 approximants for BCE loss", "content": "The BCE loss can be decomposed into C-independent binary classification subproblems:\n$L_{BCE} = \\frac{1}{C} \\sum_{i=0}^{C} (y_i L^+ +(1 - y_i)L^-)$,   $\\sum y_i \\in {1,0}$\nwhere $L^+ = - \\log(\\hat{y}_i)$ is for the positive class, and $L^- = -\\log(1-\\hat{y}_i)$ is for the negative class. Here, $\\hat{y}_i$ is the prediction probability after the sigmoid function. We first define $L_{BCE}$ in Pad\u00e9 approximant form. For positive classes where $y_i = 1$, we set the polynomial expansion point to be 1; for negative classes where $y_i = 0$, we set the expansion point to 0. Thus, Pad\u00e9 approximants for the positive and negative classes for a single sample are:\n$L_{Pad\u00e9}^+ \\approx \\frac{a_0+\\sum_{m=1}^{M} a_m \\hat{y}^m}{1+\\sum_{n=1}^{N} b_n \\hat{y}^n}$ ,\n$L_{Pad\u00e9}^- \\approx \\frac{c_0+\\sum_{m=1}^{M} c_m (1 - \\hat{y})^m}{1+\\sum_{n=1}^{N} d_n (1 - \\hat{y})^n}$ ,\nwhere $\\hat{y}$ represents the prediction probability of a single sample, while M and N represent the orders of the numerator and denominator polynomials, respectively, and $a_0, a_m, b_n, c_0, c_m$, and $d_n$ are coefficients of Pad\u00e9 approximants."}, {"title": "Derivation of the coefficients", "content": "The conventional Pad\u00e9 approximation of order m/n tends to reproduce the Taylor expansion of order [35] m + n, and the coefficients are found by setting\n$\\frac{P(x)}{Q(x)} = A(x)$\nwhere P(x) is a numerator polynomial of order m, Q(x) is the denominator polynomial of order n of Pad\u00e9 approximant, and A(x) is the Taylor expansion of order m + n.\nIn terms of the Taylor Series Expansion, $L^+$ and $L^-$ are:\n$L_{Taylor}^+ = \\sum_{k=1}^{\\infty} \\frac{(-1)^{k+1} (\\hat{y}-1)^k}{k}$,\n$L_{Taylor}^- = - \\sum_{k=1}^{\\infty} \\frac{\\hat{y}^k}{k}$\nIn line with previous research that highlights the effectiveness of the first-degree polynomial [17], we adopt the first-order Pad\u00e9 approximation for our loss function. This approach sets both the numerator's and the denominator's orders to one, and for deriving the coefficients, we equate them with the respective Taylor series expansion of the second order.\nThe first order of 6 would be:\n$L_{Pad\u00e9}^+ \\approx \\frac{a_0+a_1 \\hat{y}}{1+b_1 \\hat{y}}$,\n$L_{Pad\u00e9}^- \\approx \\frac{c_0+c_1 (1-\\hat{y})}{1+d_1 (1-\\hat{y})}$\nExpanding 8 up to second order:\n$L_{Taylor}^+ \\approx (\\hat{y} - 1) - \\frac{1}{2}(\\hat{y}-1)^2$\n$L_{Taylor}^- \\approx - \\hat{y}- \\frac{\\hat{y}^2}{2}$\nBy equating 9 and 10, we obtain the values of the coefficients $a_0 = \u22121.5$, $a_1 = 1.5$, and $b_1 = 0$, and the coefficients $c_0 = \u22121$, $c_1 = 1$, and $d_1 = 0."}, {"title": "Addition of asymmetric focusing mechanism and balancing factors", "content": "Allowing separate optimization of the positive and negative samples, we add balancing factors and asymmetric focusing mechanism from 3. Our proposed asymmetric loss based on Pad\u00e9 approximation becomes,\n$L_{ALPA} = \\sum_{i=1}^{N} [\\alpha y_i(1-\\hat{y}_i)^{ \\gamma_{pos}} L_{Pad\u00e9}^+ + \\beta (1-y_i) \\hat{y}_i^{ \\gamma_{neg}} L_{Pad\u00e9}^-]W_i$\nwhere N is the number of labels, $\\hat{y}_i$ is the predicted probability and $y_i$ is the binary target label for the i-th sample, $\\alpha$ and $\\beta$ are balancing parameters, $\\gamma_{pos}$ and $\\gamma_{neg}$ are focusing parameters, $L_{Pad\u00e9}^+$ and $L_{Pad\u00e9}^-$ are the Pad\u00e9 Approximation forms for positive and negative predictions, respectively. $W_i$ is the weight for the i-th sample, calculated as $(1 - p_{ti})$, where $p_{ti}$ is the predicted probability adjusted for the target label such that $p_{ti} = y_i\\hat{y}_i + (1-y_i)(1 - \\hat{y}_i)$, and $\\gamma$ is the summation of focusing parameters, with $\\gamma_{pos}$ applied for positive targets and $\\gamma_{neg}$ for negative targets.\nWe studied the effects of hyperparameters $\\alpha$, $\\beta$, $\\gamma_{pos}$ and $\\gamma_{neg}$ on the loss function and evaluated the loss function using the best-performing combination of values on the datasets used in this study."}, {"title": "Gradient Analysis", "content": "Gradients play a pivotal role in the training process, guiding the adjustments of network weights with respect to the input logit z. In this section, following the work of [2], we provide a comprehensive analysis of the loss gradients of ALPA compared to established loss functions such as Cross Entropy, Focal Loss, and Asymmetric Loss.\nFor ALPA, we have $L_{ALPA} = (\\hat{y}_i)^{ \\gamma_{neg}+1}$, thus, the negative gradient equation for the ALPA function is given by:\n$\\frac{dL_{ALPA}}{dz} = \\frac{dL_{ALPA}}{d \\hat{y}_i}  \\frac{d\\hat{y}_i}{dz} = (\\hat{y}_i)^{\\gamma_{neg}+1}. (1 - \\hat{y}_i)  (\\gamma_{neg} + 1)$\nwhere $\\hat{y}_i = \\frac{1}{1+e^{-z}}$ represents the predicted probability for the input logit z and $\\gamma_{neg}$ is the focusing parameter for negative targets.\nThe results of the gradient analysis are shown in Figure 1.\nWe observe that the gradient for ALPA increases moderately as the probability $\\hat{y}_i$ approaches 1. This suggests that our proposed loss function provides a consistent learning signal across the probability spectrum. It neither penalizes very harshly for misclassifications (when $\\hat{y}_i$ is low) nor relaxes too much when the classification is correct (when $\\hat{y}_i$ is high). Thus, ALPA appears to be a good choice for consistent learning across all probabilities. By focusing on harder examples and not over-penalizing the correctly classified ones, it achieves better generalization compared to other losses."}, {"title": "Experimental Setup", "content": ""}, {"title": "Datasets", "content": "The APTOS 2019 BD dataset [4] includes data from individuals diagnosed with varying levels of Diabetic Retinopathy (DR), categorized into five classes: No DR, Mild DR, Moderate DR, Severe DR, and Proliferative DR. The DermMNIST dataset [36] comprises 450x600 pixel images of various skin diseases classified into seven categories: Melanoma, Melanocytic Nevus, Basal Cell Carcinoma, Actinic Keratosis, Benign Keratosis, Dermatofibroma, and Vascular Lesion. The BoneMarrow dataset [24] contains expertly annotated cells from bone marrow smears of 945 patients, classified into 17 types including Basophil (BAS), Blast (BLA), Erythroblast (EBO), and more. The Oraiclebio dataset, which remains proprietary, includes 3,643 images of oral regions featuring 52 classes of precancerous and cancerous lesions.\nDetails of these datasets are summarized in Table 1, where the imbalance ratio, defined as $N_{max}/N_{min}$ (with N representing the sample count per class), illustrates the significance of the long-tailed distribution. For experimentation, each dataset was split 80-20 into training and testing sets, and a 5-fold cross-validation strategy was used during training to enhance model reliability."}, {"title": "Implementation", "content": "We use ConvNeXT-B [22] as the backbone for the proposed loss. We resize the input images as 256 x 256 and exploit the data augmentation schemes following the previous work [1, 7]. We train our networks using the Adam optimizer with 0.9 momentum and 0.001 weight decay. The batch size is 128, and the initial learning rate is set to 1e-4. Our networks are trained on PyTorch version 2.2.1 with RTX A6000 GPUs. We use accuracy, balanced accuracy and F1-score as evaluation metrics for this study."}, {"title": "Results", "content": "In this section, we present experimental results validating the effectiveness of the proposed ALPA function. We first analyze the impact of hyperparameters on the loss function and then compare ALPA with state-of-the-art loss functions like Asymmetric Loss, Focal Loss and Cross Entropy."}, {"title": "Effect of the hyperparameters", "content": "To evaluate the effect of hyperparameters, we experimented as follows:\n\u2022 Loss v1: Hyperparameters were randomly set as $\\alpha = 1$, $\\beta = 1$, $\\gamma_{pos} = 0$, and $\\gamma_{neg} = 4$. This is indicated as Loss v1 in Table 2.\n\u2022 Loss v2 ($L_{ALPA}$): Using random search, hyperparameters were optimized within the ranges $\\alpha$ and $\\beta$ (0.5 to 2), and $\\gamma_{pos}$ and $\\gamma_{neg}$ (0 to 5). Final values were $\\alpha = 0.875$, $\\beta = 1.625$, $\\gamma_{pos} = 0$, and $\\gamma_{neg} = 4$. This is indicated as Loss v2 in Table 2.\n\u2022 Loss v3: Incorporating Hill Loss [39] following [29], we added $\\lambda - \\hat{y}_i$ to $L^-$ ($\\lambda = 1.5$), optimizing via random search to $\\alpha = 1.25$, $\\beta = 2$, $\\gamma_{pos} = 3$, and $\\gamma_{neg} = 2$. This is indicated as Loss v3 in Table 2.\nResults on the APTOS2019 dataset for these settings are shown in Table 2. We focused on detecting crucial cases like Proliferative DR and examined the performance of underrepresented classes to proceed with Loss v2. From here on, Loss v2 is referred to as $L_{ALPA}$."}, {"title": "Comparison with existing methods", "content": "We compare our proposed loss function with state-of-the-art methods such as ASL, Focal Loss, LDAM, and CE on the datasets listed in Section 5.1. Results on the publicly available datasets are presented in Tables 3, 4, and 5, while the results for LDAM loss functions can be found in the supplementary materials. ALPA consistently excels in classes with fewer samples while maintaining competitive accuracy in classes with higher representation. In terms of balanced accuracy, ALPA surpasses all other loss functions across the three public datasets."}, {"title": "Conclusion", "content": "In this study, we present a Pad\u00e9 approximation-based loss function with asymmetric focusing, tailored for multi-class classification tasks with long-tailed distributions. Our proposed loss function demonstrates competitive and superior performance on long-tailed datasets when benchmarked against previous state-of-the-art approaches. We believe that our findings can serve as a valuable resource for future research, offering a foundation for further development and integration into new studies."}, {"title": "Future work", "content": "The learning process of the model is intrinsically tied to data representation. Modifying the loss function alone, however, may have limited potential for performance improvement. To enhance class-wise accuracy, integrating loss functions with data augmentation strategies and data generation pipelines presents a promising approach. Data augmentation artificially expands the training dataset by creating modified versions of existing images, while data generation pipelines synthesize entirely new samples. These methods can help balance class representation and bolster model robustness. A more thorough exploration of these techniques in future work could offer substantial benefits in addressing class imbalance."}]}