{"title": "CORE TOKENSETS FOR DATA-EFFICIENT SEQUENTIAL TRAINING OF TRANSFORMERS", "authors": ["Subarnaduti Paul", "Manuel Brack", "Patrick Schramowski", "Kristian Kersting", "Martin Mundt"], "abstract": "Deep networks are frequently tuned to novel tasks and continue learning from ongoing data streams. Such sequential training requires consolidation of new and past information, a challenge predominantly addressed by retaining the most important data points - formally known as coresets. Traditionally, these coresets consist of entire samples, such as images or sentences. However, recent transformer architectures operate on tokens, leading to the famous assertion that an image is worth 16x16 words. Intuitively, not all of these tokens are equally informative or memorable. Going beyond coresets, we thus propose to construct a deeper-level data summary on the level of tokens. Our respectively named core tokensets both select the most informative data points and leverage feature attribution to store only their most relevant features. We demonstrate that core tokensets yield significant performance retention in incremental image classification, open-ended visual question answering, and continual image captioning with significantly reduced memory. In fact, we empirically find that a core tokenset of 1% of the data performs comparably to at least a twice as large and up to 10 times larger coreset.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning models are rarely deployed in static environments and instead need to be continuously adjusted to their ever-changing surroundings. As distribution shifts occur over time, we require models to retain their performance on previous settings while accommodating new examples (Hadsell et al., 2020; Kudithipudi et al., 2022; Mundt et al., 2023). Naively, re-training a model from scratch for every change in the environment quickly becomes unfeasible for complex architectures with billions of parameters (Touvron et al., 2023; Brown et al., 2020). Consequently, research has focused on identifying small, representative subsets of the training data (Lopez-Paz & Ranzato, 2017), which can later be replayed continuously to avoid (catastrophic) forgetting (McCloskey & Cohen, 1989) of previously acquired information (Graham et al., 2021; Hassani et al., 2021; Xu et al., 2021). Formally, these (weighted) subsets can be described as coresets; data summaries that approximate the original loss function with a negligible loss in performance (Bachem et al., 2015; Bl\u00f6mer et al., 2016; Trapp et al., 2022). However, whereas multiple successful coreset selection techniques were proposed (Mirzasoleiman et al., 2020; Killamsetty et al., 2021a), we argue that modern architecture advances now allow for a yet to be leveraged level of data summarization. In particular, (vision) transformers now conveniently define each data input as an ordered set of tokens (Vaswani et al., 2017; Dosovitskiy et al., 2020). In turn, we posit that it is not only a subset of the data instances that allows effective summarization but also only a handful of features within every data point that may be relevant to generalizing the task at hand. We refer to the latter as core tokens and, respectively, the subset of most meaningful tokens of important data instances as core tokensets.\nCore tokensets build on the general principle of coresets, where we aim to identify the tokens that approximate the loss within a negligible error margin of the full dataset. To this end, we demonstrate that the attribution maps calculated using the attention scores across the transformer layers, as a"}, {"title": "2 PRELIMINARIES AND RELATED WORK", "content": "Coresets: Conventional dataset summaries are constructed as instance-wise (weighted) subsets of the entire training data. These coresets are comprised of the most informative instances with an aim to approximate the full dataset with a negligible difference in the cost function. That is, we wish to find the subset $C_r$ of $D$, assuming an additively, into non-negative functions decomposable, cost function for our given dataset $D$ (Bachem et al., 2015; 2017), such that for any solution (parameters) $Q$:\n$|cost(D, Q) \u2013 cost(C_r, Q)| \u2264 \u0454\u00b7 cost(D,Q)$, where $e > 0$ (1)\nIn practice, coresets were primarily designed to demonstrate dataset approximation capabilities for traditional machine learning models such as K-means (Bl\u00f6mer et al., 2016; Feldman et al., 2007; Har-Peled & Kushal, 2005), SVM (Dong et al., 2005), and logistic regression (Huggins et al., 2016), with very few works aimed at deep neural networks (DNNs). For the latter, Killamsetty et al."}, {"title": "3 CORE TOKENSETS: SUMMARIZING DATASETS WITH INFORMATIVE TOKENS", "content": "As transformers are now capable of processing data inputs as a sequence of multiple tokens, we now present a novel way to summarize datasets even more efficiently than traditional coresets.\nDefinition: A Core Tokenset $C_t$ is a subset of tokens ($C_T$) of a dataset $D$, such that a solution $Q$ found on the core tokenset approximates the cost of the full dataset within negligible error margin: $|cost(T, Q) - cost(C_t, Q)| \u2264 \u0454 cost(T, Q)$.\nThis definition is a modification of core set equation 1. Instead of being based on data instances, it is now based on tokens and we can thus leverage the unique architectural properties of modern transformers to process inputs at a sub-data point level. In order to identify the core tokens, we associate each input token $x$ with a binary importance variable $v_t$:\n$C_t := {(x, t)|v_t = 1,0 <= t < |z_0|} s.t.|C_t| <T$ (3)\nNote that, each stored core token remains associated with its position $t$, preserved in the form of integer indexes of the extracted tokens. This will later be used to preserve original structure in continued training. The importance variable will typically be regulated by the relevance of the token to its current task, but we can also think of straightforward selection methods. For instance, a naive (random) selection can be realized by sampling the importance variable from a Bernoulli distribution: $v_t ~ Bernoulli(r)$. Here, r naturally maps to our key hyperparameter, i.e. the retention rate that reflects the percentage of tokens retained from T.\nWhereas there exist well-defined strategies to determine coresets, methods to select core tokens are yet to be explored. Even though random sampling is a feasible baseline in forming memory (Hayes et al., 2021), randomly selecting core tokens can easily lead to storing redundant tokens. In particular, in images, the object of interest often only occupies tiny regions of an input sample; storing the background alone is unlikely to approximate the true loss well. As a remedy, we propose to leverage attribution techniques and propose to investigate the attention maps for token relevance to construct our set of core tokens. First, each token is assigned a relevance score, determining its contribution"}, {"title": "4 SEQUENTIAL TRAINING WITH CORE TOKENSETS", "content": "When we train our transformer sequentially over time, we optimize our model simultaneously on newly arriving data of the novel task and the memory buffer data as modeled through the core tokenset. The training loss $L$ is thus:\n$L = E_{(x,y)} L(f(x), y) + E_{(x,y)\u2208C_t} L(f(x), y)$ (5)\nwhere $L(...)$ denotes an arbitrary loss (e.g. the cross-entropy), $f$ denotes the model predictions $f = z(g(x))$ and $g(x)$ is the shared encoder model. Similarly, $f(x)$ denotes the predictions made on the core tokenset memory buffer. Before we proceed to experimentally corroborate our core tokensets, let us first consider two further imperative aspects: 1.) how to make transformers susceptible to the partial inputs (subset of tokens) stored in our data summary, and 2.) provide an intuition for the trade-offs and interplay between storing a traditional core set and a subset of tokens."}, {"title": "4.1 MAKING TRANSFORMERS SUSCEPTIBLE TO PARTIAL INPUTS", "content": "During regular (pre-)training, ViTs only observe complete inputs of T tokens. If we now feed them with a subset of tokens, i.e. drop a subset of the concatenated patches, this results in an input sequence that strongly deviates from the expected input structure. This, unfortunately, immediately leads to unexpected model behavior and performance degradation. In fact, in Fig. 2a we report the latter, in an experiment where we transition from training on complete samples during initial training to fine-tuning on partial inputs with core tokensets. Specifically, a pre-trained ImageNet (Deng et al., 2009) Vit-B/16 model is continuously fine-tuned to 20 random classes first on all data and then on core tokenset with different retention. For simplicity, we randomly select the core tokens. We can observe a considerable drop in performance of roughly 15% in prediction accuracy at 30% retention.\nIf one were to draw premature conclusions, one would posit that core tokens are ineffective. How- ever, this is not the case and our initial deteriorated model performance is truly due to the model encountering unexpected input. As a remedy, we demonstrate that we need only make the model amenable to observing partial inputs. Whereas we could conceive several notions to achieve the latter, an effective simple strategy is to ensure that the model is always prompted with a complete sequence of T tokens for core tokens. To that extent, the input value of left out tokens is set to 0. Crucially, this setup preserves the input structure on which the model was pre-trained, thus ensuring that spatial relations of patches are represented correctly. To emulate the respective sparsity of core tokensets, we additionally randomly zero-out tokens of full data samples according to the expected retention rate,"}, {"title": "4.2 INTUITION ON EFFECTIVENESS: CORESETS VS. CORE TOKENSETS", "content": "Our definition of core tokens naturally raises the question of whether it is more beneficial to store subsets of tokens or subsets of data points, or whether to combine the latter. To provide intuition for this question, we conceive two practical variants of data summarization. The first strives to identify the most relevant subset of tokens for every data point and use this as a data summary. We will refer to this idea as only extracting core tokens. In the second, we would instead attempt to find the most important subset of data points and respectively their most influential tokens. We will refer to the latter when we proceed to speak of core tokensets in the remainder of the paper's experiments. In other words, core tokenset follow earlier Fig. 1, where we first build a coreset and then subsequently identify the core tokens for the selected samples, whereas only selecting core tokens can be seen as an ablation omitting the core set step.\nIn Fig. 4 we evaluate these two strategies, only core tokens and core tokensets, against traditional core sets for our pre-trained ViT-B model on 20 random ImageNet classes with varying retention rates (10-100%). We can observe that for all retention rates, training on core tokens outperforms traditional core sets. However, core tokensets, identifying the most important tokens for the most important data points, yield substantial further performance improvement. We believe this to be fairly intuitive. For instance, consider a budget where we store only 1% of the samples in our memory buffer; coreset techniques, most evidently, will throw away 99% of the total data instances. Meanwhile, for core tokens, it will be possible to store 1% of information for every single item of dataset. In contrast, for core tokensets, the combination allows us to select more instances than the coreset technique by focusing on a few core tokens per sample. To be precise, we can store 10% of the samples (> 1%"}, {"title": "4.3 EXPERIMENTAL DETAILS", "content": "Datasets and Models. We use ImageNet100 (Deng et al., 2009) for classification, which we sequentially segment into five tasks with 20 unique classes each. For visual question answering, we train on three separate visual question answering datasets in sequence, namely VQA 2.0 (Antol et al., 2015), CLEVR (Johnson et al., 2017), and Visual Genome (Krishna et al., 2017). For the image captioning experiments, we rely on MS COCO (Lin et al., 2014), which we segment into five equally sized tasks based on the object classes. We start with a pre-trained VIT-B/16 trained on ImageNet21k as our base model (Wightman, 2019) for image classification to focus our investigation on data summary efficiency and knowledge retention. Lastly, we use a pre-trained BLIP model (Li et al., 2022) as initialization for both VQA and image captioning tasks. More details for datasets and training set-ups are provided in Appendix A.3. Depending on the complexity of the task, we have trained our models on 8-12 Nvidia A100 GPUs.\nStrategies. We evaluate two recent state-of-the-art coreset approaches, namely CRAIG (Mirza-soleiman et al., 2020) and GradMatch (Killamsetty et al., 2021a), both relying on gradients for their data selection. In addition, we consider four distinct feature attribution methods to derive attention maps, based on which core tokens are selected. Rollout considers the information flow from the input layer to deeper ones (Abnar & Zuidema, 2022). In contrast, Grad-Cam relies on the gradients for each input token with respect to the ground-truth output. Going a step further, gradients with layer-wise relevance propagation (Grad-LRP) (Chefer et al., 2021b;a) considers the respective relevance of each layer individually. Finally, AtMan (Deiseroth et al., 2023) calculates the influence of each token by perturbing the attention scores. We provide detailed descriptions for each strategy in Appendix A.1.\nEvaluation Metrics. In all incremental setups, we report the corresponding metrics averaged over all the tasks. We report averaged accuracy for image classification and VQA. However, we note that we do not train VQA as a classification task. Instead, we open-endedly generate answers in natural language. Consequently, an answer is counted as correct if the generated text is the same as the ground-truth label. For image captioning, we report BLEU-4 (Papineni et al., 2002) and ROUGE (Lin, 2004), which measure the similarity between the model-generated text and the reference text."}, {"title": "4.4 SEQUENTIAL IMAGE CLASSIFICATION", "content": "In our sequential classification task, we specifically evaluate the informativeness of our memory buffer in preserving the old information by comparing two variants of coreset strategies with four ways of selecting core tokens. Tab. 1 compares the methods as a function of the stored proportion"}, {"title": "4.5 OPEN-ENDED VISUAL QUESTIONING ANSWERING", "content": "We now move on to a larger-scale, multi-modal setup, where we finetune BLIP over 2 million incremental VQA pairs. In Tab. 2, we report results over the considered selection strategies at different retention rates. While GradMatch remains the best coreset selection strategy, GradLRP now takes the lead over ATMAN for identification of core tokens. Respectively, our VQA core tokenset combines GradMatch and GradLRP. Again, core tokensets consistently outperform all other methods across all retention rates. Similar to our image classification table, to offer better visualization, we have color-coded specific cells of coreset and core tokensets to highlight their comparable performances at contrasting retention rates. Again core tokensets can be observed to either perform much better or be significantly more memory efficient. For instance, should we wish to achieve a performance of at least 75%, we would have to resort to a GradMatch-based core set that extracts a 30% subset of the dataset. In contrast, core tokens already surpass our example performance criterion when retaining only 20% of the data. Once more, this effect gets amplified as we store less data, up to an astonishing performance at 1% stored data that outperforms a ten times larger core set. Consequently, the findings of our VQA experiments are consistent with those observed image classification."}, {"title": "4.6 IMAGE CAPTIONING", "content": "Finally, we conduct experiments on sequential image captioning tasks, comparing core tokensets (CTS) based on the various selection strategies at an aggressive retention rate of 10%. We have chosen this low retention rate as it already demonstrates results that are reasonably close to training on the full dataset, making it obsolete to investigate higher percentages. Instead, we report both BLEU-4 and ROUGE scores as a function of tasks over time in Fig. 6. Again, we report GradMatch as the favorable coreset selection strategy and combine it with different core token selection strategies. In full consistency with all prior findings, all CTS variants outperform the traditional coreset, whereas the GradLRP-based CTS takes the lead and is once more closely followed by Atman. The ordering of the methods is the same for both BLEU and ROUGE scores, with both demonstrating a marginal drop in performance when only 10% of the data is stored with a CTS."}, {"title": "5 DISCUSSION", "content": "We have thoroughly evaluated core tokensets across three different sequential tasks. Despite the different nature of these tasks, our empirical findings provide consistent insights.\nThe first crucial insight lies in the efficacy when comparing retention of data subsets, i.e. traditional coresets, vs. core tokens, i.e. storing the relevant subset of tokens of each data point. If these methods were to be viewed as separate competitors, then it appears to be safer to reside on the token level. Our perhaps obvious hypothesis here is that intuitively, one loses less information when throwing away an informative token, in contrast to throwing away an entire data point. As an example, if the memory size is chosen to be e.g. 10%, it is more likely that a high amount of relevant information is"}, {"title": "6 CONCLUSION", "content": "We have introduced core tokensets for data summarization tailored to the unique architectural properties of transformers. Building on the notion of coresets, we have demonstrated that each data sample can be sufficiently summarized with only a subset of core tokens. Consequently, we have seen the applicability of feature attribution scores in determining the relevance of each input token on the target task. Our empirical investigation in three distinct scenarios has demonstrated that core tokens are a cost-effective way to effectively preserve the stability of a sequentially trained model. With respect to future work, we envision the development of single-stage core tokenset selection strategies, where the relevant set of samples and their informative tokens are identified concurrently."}, {"title": "A APPENDIX", "content": "A.1 DETAILS OF ATTRIBUTION STRATEGIES\nIn this section, we detail the different selection strategies that ultimately influence the informative-ness of the core tokensets. Our code is available at https://github.com/paulsubarna/\nCore-Tokenset.git\nGradCam: Inspired by the idea proposed in the context of convolutional networks (Selvaraju et al., 2017), we implement a gradient-based feature attribution map for transformers. In particular, we only consider the last layer of the transformer right before the classification head to compute the gradients of the attention heads. The corresponding feature map will obtain a shape of $R^{T\u00d7D}$ where T denotes the number of the tokens and D is the embedding dimension. As a note, GradCam is purely a class-specific approach, where we only consider the gradients with respect to the [CLS] token and set the rest of the elements in the first dimension to zero.\nRollout: Unlike Gradcam, this strategy is purely attention-based (A). The feature relevance map S is usually determined by multiplying the attention maps of all the blocks (b \u2208 B) across the transformer layers (Abnar & Zuidema, 2022). It is thereby:\n$S = A^{(1)}. A^{(2)}. A^{(3)}. A^{(B)}$ (6)\nGradLRP: GradLRP is the second gradient-based approach that amends some of GradCam's limitations. As the name suggests, it combines the gradients of the individual attention maps with the layer-wise propagation score calculated from the target token to the input sample. Given a transformer model with B blocks, we first calculate the attention map ($A^b$) for each block ($b \u2208 B$) comprising h attention heads. We then consider the Hadamard product of the gradients of the attention map ($A^b$) and the layer-wise relevance score (LR) (Bach et al., 2015) concerning a target class. The final attribution map $A^b$ is achieved by calculating the mean (Eh) of the product across h attention heads in addition to an identity matrix to account for skip connections in transformers. The final influence matrix $S_{TT}$ for a set of tokens T is then defined by multiplying all the attention head scores, where each row consists of the relevance score of each token compared to the other token.\n$\u0100^b = I + E_h(\u2207A \u2299 LR); S_{(TXT)} = A^{(1)}. A^{(2)}. A^{(3)}. A^{(B)}$ (7)\nAtman: This approach follows a different route, where the relevance of a token is determined by leveraging the perturbation technique. In particular, we perturb the pre-softmax attention score H ($eR^{hxtxd}$) for each token by a factor f and determine their influence over the final target loss. A subtle advantage over its gradient counterpart is that perturbation-based approaches only require forward passes without calculating or storing gradients. Across all heads h and the transformer blocks, we can compute the modified pre-softmax attention scores $H^h$ as:\n$H^h_{*,*,*} = H^h_{*,*,*} ((1 \u2212 f) + f(1 \u2212 f_{k,*}) where f_{k,*} {St,k} if 0 <= St,k <= 1{0}{otherwise, }$ (8)\nHere, 1 denotes the matrix containing only ones $[1]^{t\u00d7t}$ whereas the $s_{t,k}$ denotes the cosine similarity matrix for the T tokens. Consistent across all the heads, we manipulate the attention scores of token t by a factor of f. Due to the nature of the image sample and the correlation between a number of patches, we also manipulate the scores of the tokens, which are correlated with the token t. For that, we compute the similarity matrix consisting of the similarity scores for each token compared to other tokens and consequently select column t to perturb individual tokens. We compute the difference between the loss function with the perturbed token and the unchanged token to determine the token relevance.\nThe highest I score contributes to the largest deviation in the loss function, highlighting the most relevant token.\n$target ~ L_{target} (z, 0-z_t) \u2013 \u00a3_{target} (z, 0)$ (9)\nHere, e denotes the set of model parameters whereas the $0_{zt}$ gives the weight parameters with the perturbed token. Ultimately, we consider the row of the most relevant token from the cosine similarity matrix to construct our core tokenset."}, {"title": "A.2 VISUALIZATION OF ATTENTION MAP", "content": "In this subsection, we visualize the token relevance map computed using different feature attribution techniques to select the core tokens. To provide better intuition, we gradually reduce the retention rate from 60% to 20% and visualize which of the core token selection strategies were able to preserve the target object in consideration. Referring to Fig 7, we observe that random selection, as speculated, fails to capture the relevant information within a sample with the gradual decline in the retention rate. Out of all feature attribution techniques, we notice that GradLRP successfully identifies the target object in consideration, in this case, the elephant, even at a lower retention rate of 20%. Perturbation techniques perform well to preserve the core information but also capture a few background patches owing to the closely related patches present in image samples. In the case of Rollout, the closest to the GradLRP approach, we observe a similar trend in terms of establishing a token relevance map. Lastly, Gradcam seems to offer only a marginal improvement over random selection."}, {"title": "A.3 TRAINING AND DATASET DETAILS", "content": "Sequential image classification: In this particular task, we have initialized our image transformer with a Vit-B/16 (Wightman, 2019) model pretrained on ImageNet21k. To mimic a sequential learning task, we have created a sequence of 5 sub-tasks, where each sub-task consists of instances from 20 random ImageNet classes. We then fine-tune our models on each sub-task for 40 epochs with a mini-batch size of 128 using Adam as an optimizer with \u03b21 = 0.9, \u03b22 = 0.999. At the end of each sub-task, we report the model's accuracy on the current task and the previously encountered sub-tasks.\nSequential multimodal training: We have empirically evaluated our proposed approach on two distinct tasks: visual-question answering and image captioning. In the context of this work, we have explored the base model of BLIP, with VIT-B/16 being the image encoder (Li et al., 2022) pre-trained on 14M images, including two human-annotated datasets (COCO and Visual Genome), and three web datasets (Conceptual Captions, Conceptual 12M, SBU captions). We trained on 8 Nvidia-A100 GPU nodes.\nVisual-question answering (VQA): To formulate a sequential learning task, we incrementally fine-tune our pre-trained BLIP in the order: VQA-v2 (Antol et al., 2015) \u2192 CLEVR (Johnson et al., 2017) \u2192 VG (Antol et al., 2015). Each sub-task is trained for 20 epochs with a mini-batch size of 32 and Adam as our optimizer with a weight decay of 0.05. The learning rate is warmed up to 3e-4."}, {"title": "A.4 ADDITIONAL RESULTS", "content": "A.4.1 VISUAL QUESTION ANSWERING\nTo complement the main body, we report the plasticity and stability of the vision-language model BLIP while being trained sequentially on our VQA task, as reported in Fig. 8. For each scenario, a task versus task confusion matrix is shown. The diagonal from the top left to the lower right represents the evaluation performance on a task when it is trained. Respectively, the upper right-hand triangle shows forward transfer, i.e. when the model is evaluated on an unseen task after being trained on a specific prior dataset. Finally, the lower left triangle quantifies backward transfer, where the model is re-evaluated on an older dataset after being trained on a new dataset.\nNot surprisingly, all methods show similarly limited generalization capabilities to unseen tasks, as evident from the upper right triangle. Intuitively, any technique to store a memory of the past, whether core sets or core tokensets, does not affect this generalization capability. The observation primarily underpins that the chosen tasks and pre-trained model are meaningfully picked for continual learning.\nNaturally, the lower triangle is most relevant to evaluating core tokenset selection, as it indicates forgetting between tasks. Here, cumulative training serves as an upper bound. On the contrary, finetuning without replay suffers from catastrophic forgetting, thus marking the lower bound. We can observe that all feature attribution methods improve upon random core token selection. However, a well-picked selection strategy significantly reduces forgetting. For example, when using our core tokenset based on GradLRP, we achieve over 75% accuracy on VQA after three tasks. This is roughly 95% of the originally achieved accuracy when training the task (79.8%). In direct comparison, GradCam ends up at 69%, barely more than the performance of random selection at 68%.\nA.4.2 SEQUENTIAL IMAGE CLASSIFICATION\nTo complement the main body, we further show the evolution across tasks of different feature attribution techniques for core token selection, namely GradLRP, Atman, GradCam, and Rollout."}]}