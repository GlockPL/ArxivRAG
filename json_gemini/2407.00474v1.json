{"title": "MH-pFLGB: Model Heterogeneous personalized Federated Learning via Global Bypass for Medical Image Analysis", "authors": ["Luyuan Xie", "Manqing Lin", "ChenMing Xu", "Tianyu Luan", "Zhipeng Zeng", "Wenjun Qian", "Cong Li", "Yuejian Fang", "Qingni Shen", "Zhonghai Wu"], "abstract": "In the evolving application of medical artificial intelligence, federated learning is notable for its ability to protect training data privacy. Federated learning facilitates collaborative model development without the need to share local data from healthcare institutions. Yet, the statistical and system heterogeneity among these institutions poses substantial challenges, which affects the effectiveness of federated learning and hampers the exchange of information between clients. To address these issues, we introduce a novel approach, MH-pFLGB, which employs a global bypass strategy to mitigate the reliance on public datasets and navigate the complexities of non-IID data distributions. Our method enhances traditional federated learning by integrating a global bypass model, which would share the information among the clients, but also serves as part of the network to enhance the performance on each client. Additionally, MH-pFLGB provides a feature fusion module to better combine the local and global features. We validate MH-pFLGB's effectiveness and adaptability through extensive testing on different medical tasks, demonstrating superior performance compared to existing state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "In the field of medical images, federated learning [20] has emerged as a key technique for its ability to protect the privacy of training datasets. This approach allows for the collaborative development of a unified global model, eliminating the need to directly share local data from individual healthcare facilities. However, the application of federated learning in healthcare faces challenges such as statistical heterogeneity [?], due to the diverse and non-uniformly distributed (non-IID) data across different institutions, and system heterogeneity [?], due to the unique architecture of local models by each institution. These challenges compromise the efficiency of federated learning and hinder the seamless exchange of information between client models. Addressing the issues of statistical and system heterogeneity presents a critical and impactful challenge in the application of federated learning within healthcare facilities.\nPrevious works only focused on statistical heterogeneity and proposed personalized federated learning methods [5,24,2,27,14,1,15]. Compared to traditional single model settings [20,10,13], personalized federated learning allows each client to learn their own model, effectively alleviating the problem of statistic heterogeneity. However, these methods still require models with the same structure for each client. Recent works including FedMD [12], FedDF [16], DS-pFL [9] and KT-pFL [30] tackle statistic and system heterogeneity in federated learning by sharing soft predictions among clients. These approaches have advanced the field by addressing heterogeneity issue, but depend heavily on public datasets for generating these soft predictions. However, collecting the medical dataset for public usage would involve a certain level of privacy requirements and complex censoring processes. Besides, the extensive size of public datasets would largely increase the computational cost, thus limiting the application of these techniques. All of these problems would significantly raise the cost of deploying those methods.\nTo eliminate the reliance on public datasets, we propose a global bypass strategy to address the challenges of heterogeneous models under the distribution of non-IID data. Unlike traditional approaches that rely on soft prediction generated from public datasets, our method adds a global bypass model to the local clients to share the information among the clients and help the local clients. In each client, the global bypass would not only learn the information from local data, but also help the previous local network to make its prediction. In the server, we aggregate the global bypass to share the information among each client. Additionally, we design the global bypass to be small so the computational cost is less than what would be required for local training on a public dataset.\nSpecifically, we propose framework Model Heterogeneous personalized Federated Learning via Global Bypass (MH-pFLGB). Our global bypass consists of a body and a head module. The body is a light-weighted encoder for feature extraction and the head is a small module designed to fit the outputs of different tasks. To better fuse the information from the local model and global bypass model, we designed a fusion module named features weighted fusion to fusion the features from the body of the local and global model. The fusion is based on allowing models to learn how to better select weights for global and local features. This design would better utilize global knowledge and integrate it with local features, so that it can improve the performance of local models for each client.\nOur contributions are summarized as follows:\nWe introduce a novel personalized federated learning approach for dealing with heterogeneous models named MH-pFLGB. This approach leverages a global bypass mechanism that obviates the need for public medical datasets, thereby reducing the additional burdens associated with local training."}, {"title": "2 Methods", "content": null}, {"title": "2.1 Pipeline", "content": "The pipeline of MH-pFLGB is shown in Fig. 1 (a). Each local client consists of an architecture heterogeneous local model and a global bypass model that shares the same network architecture among other clients. The local body extracts personalized features from local clients, while the global body shares learned parameters among clients. Both local and global models are divided into a body model to extract features, and a head model to generate the network output using the features. Our training process consists of 3 steps: a. Local model training, b. Global bypass model training, and c. Global aggregation. We will explain the details of those steps in the following.\nLocal model training. In the local model training stage, the local model learns from both the local dataset and the global insights provided by the bypass model. At this stage, we freeze the global bypass model and only train the local model. For client i, its local model training loss function $L_{loc, i}$ is:\n$L_{loc, i} = \\lambda_{loc}L_{loc}(y_i, \\hat{y}_i) + \\lambda_{loc}L_{loc}(y_i, \\tilde{y}_i), \\qquad (1)$\nwhere $\\hat{y}_i$ and $\\tilde{y}_i$ are the predictions from the local head and global head. $L_{loc}$ and $L_{loc}$ represent the loss functions for the local and global model output, respectively. $\\lambda_{loc}$ and $\\lambda_{loc}$ are their corresponding weights. $y_i$ is the label of input data $x_i$. Note that, even though the global bypass is fixed, we still calculate the loss function on its output to maintain generalizability when training the local model.\nGlobal bypass model training. During the global model training phase, we freeze the local model and fine-tune the global bypass model. This enables the body of the global model to learn the information from each client. The loss function $L_{glob,i}$ is represented as:\n$L_{glob,i} = \\lambda_{glob}L_{glob}(y_i, \\hat{y}_i) + \\lambda_{glob}L_{glob}(y_i, \\tilde{y}_i). \\qquad (2)$\n$L_{glob}$ and $L_{glob}$ represent the loss function for training the global and local model at the global training stage, respectively. $\\lambda_{glob}$ and $\\lambda_{glob}$ are their corresponding weights. Other variables are defined the same as in eq.(1). Similar to the local training stage, local loss function $L_{glob}$ is designed to avoid client drift.\nGlobal Aggregation. As the global model is uploaded to the server, the global aggregation process aggregates the model parameters, with distinct processes for both the body and head of the global model. This aggregation employs weight averaging, as outlined in [20]. Finally, the aggregated model is downloaded and distributed for the next round of training.\nAt the inference stage, we fuse global and local features, and the fused features output prediction results through the local head. The global head only participates in the training stage and does not participate in the inference stage. This is similar to adding a regularization term during local training, effectively preventing overfitting of the local model."}, {"title": "2.2 Features Weighted Fusion", "content": "In order to better fuse global and local features, we propose a new feature method named Feature Weighted Fusion. As shown in Fig. 1(b), the feature from global body $x_g$ ensures that the dimension is the same as local client feature $x_l$ through upsampling or downsampling as\n$x_g = f_{up}(x_g) \\text{ or } f_{down}(x_g), \\qquad (3)$\nwhere $f_{up}(\\cdot)$ and $f_{down}(\\cdot)$ respectively represent upsampling or downsampling operations. Specifically, in the classification task, we use $1 \\times 1$ convolution for both upsampling and downsampling. In the segmentation task, we adopt deconvolution for upsampling and convolution for downsampling, along with a global average pooling operation on the results. Having $\\hat{x}_g$, a Softmax operator is applied on $\\hat{x}_g$ and $x_l$'s channel-wise digits:\n$a_i = \\frac{exp(x_{g,i})}{exp(x_{g,i}) + exp(x_{l,i})} \\qquad b_i = \\frac{exp(x_{l,i})}{exp(x_{g,i}) + exp(x_{l,i})}, \\qquad 0 < i < C, \\qquad (4)$\nwhere $C$ is the the dimension of $\\hat{x}_g$ and $x_l$. $a$ and $b$ are the calculated weights. The local fusion feature $x_{lf}$ is obtained by multiplying and adding the corresponding weights and features.\n$x_{lf,i} = a_ix_{g,i} + b_ix_{l,i}, \\qquad 0 < i < C, \\qquad (5)$\nwhere $x_{lf,i}$ is the i-th element of $x_{lf}$. $x_{lf}$ obtains the global fusion feature $x_{gf}$ for global head through downsampling or upsampling as:\n$x_{gf} = f_{down}(x_{lf}) \\text{ or } f_{up}(x_{lf}). \\qquad (6)$"}, {"title": "3 Experiments Setup", "content": "Task and Dataset. We verify the effectiveness of MH-pFLGB on 3 non-IID tasks. For medical image classification (different resolution) task, our experiments are conducted on the Breast Cancer Histopathological Image Database (BreaKHis) [26]. We perform x2\u2193, x4\u2193, and x8\u2193 downsampling on the high-resolution images [36]. Each resolution of medical images is treated as a client. In this task, we employed ResNet {17,11,8,5 11,8, 5} as the local model of each client, respectively. For medical image classification (different label distributions) task, we employ 2 datasets, including a breast cancer classification dataset BreaKHis (color images) and an Optical Coherence Tomography (OCT) disease classification dataset OCT2017 (grayscale images) [11]. We"}, {"title": "4 Results and Discussion", "content": null}, {"title": "4.1 Medical Image Classification (Different Resolutions)", "content": "In this task, we employ the ResNet family model to train breast cancer medical images under different resolutions. The higher the image resolution of this client, the deeper"}, {"title": "4.2 Medical Image Classification (Different Label Distributions)", "content": "In Table 2, the experimental results for the medical image classification task with different label distributions, where each client uses heterogeneous models, show that MH-pFLGB achieves the optimal results. This demonstrates that, compared to heterogeneous federated learning methods based on soft predictions, the global bypass model approach of MH-pFLGB has advantages. It can more effectively utilize knowledge from other clients to guide local client learning. Compared to only local training, MH-pFLGB enhances the local performance of each heterogeneous model. This indicates that our proposed feature weighted fusion method fuses global and local features well, thereby improving the performance of local models."}, {"title": "4.3 Medical Image Segmentation", "content": "We validate the effectiveness of MH-pFLGB in medical image segmentation tasks. Table 3 presents the results of previous federated learning frameworks in the segmentation task, demonstrating that MH-pFLGB achieves the best outcomes. This indicates that our framework can effectively fuse global features and local heterogeneous model features from various clients, thus performing well in various downstream tasks. Meanwhile,"}, {"title": "4.4 Ablation Experiments", "content": "To verify the effectiveness of MH-pFLGB's key components, we conduct a comparative analysis by removing each of the three elements (global head, global body, and feature-weighted fusion) during breast cancer classification tasks with different label distributions and segmentation tasks, as shown in Table 4. The experimental results indicate that more parameter sharing is beneficial for MH-pFLGB, and features weighted fusion effectively improves the performance of local heterogeneous models."}, {"title": "4.5 GFLOPS and Parameters", "content": "We compare the GFLOPS and parameter of the global bypass model with local heterogeneous models in three tasks. The results of Table 5 show that the GFLOPS and parameters of the global bypass model are much smaller than those of the local heterogeneous model on all the tasks."}, {"title": "5 Conclusion", "content": "MH-pFLGB can effectively solve the problems of statistic heterogeneity and system heterogeneity faced in federated learning. MH-pFLGB, based on the global bypass model paradigm, offers a solution to these issues. MH-pFLGB introduces a lightweight global bypass model in each client and designs a feature weighted fusion to fuse local and global knowledge. These can enable local heterogeneous models to capture information from other clients well under statistic heterogeneity. Numerous experiments have demonstrated that our method outperforms existing federated learning frameworks with heterogeneous models in multiple tasks."}]}