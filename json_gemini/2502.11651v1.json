{"title": "MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease Progression", "authors": ["Linjie Mu", "Zhongzhen Huang", "Shengqian Qin", "Yakun Zhu", "Shaoting Zhang", "Xiaofan Zhang"], "abstract": "Large vision-language models (LVLMs) have\nshown great promise in medical applications,\nparticularly in visual question answering (Med-\nVQA) and diagnosis from medical images.\nHowever, existing datasets and models often\nfail to consider critical aspects of medical di-\nagnostics, such as the integration of historical\nrecords and the analysis of disease progression\nover time. In this paper, we introduce MMXU\n(Multimodal and MultiX-ray Understanding),\na novel dataset for MedVQA that focuses on\nidentifying changes in specific regions between\ntwo patient visits. Unlike previous datasets\nthat primarily address single-image questions,\nMMXU enables multi-image questions, incor-\nporating both current and historical patient\ndata. We demonstrate the limitations of cur-\nrent LVLMs in identifying disease progression\non MMXU-test, even those that perform well\non traditional benchmarks. To address this, we\npropose a MedRecord-Augmented Generation\n(MAG) approach, incorporating both global\nand regional historical records. Our experi-\nments show that integrating historical records\nsignificantly enhances diagnostic accuracy by\nat least 20%, bridging the gap between cur-\nrent LVLMs and human expert performance.\nAdditionally, we fine-tune models with MAG\non MMXU-dev, which demonstrates notable\nimprovements. We hope this work could il-\nluminate the avenue of advancing the use of\nLVLMs in medical diagnostics by emphasizing\nthe importance of historical context in interpret-\ning medical images. Our dataset is released at\nhttps://github.com/linjiemu/MMXU.", "sections": [{"title": "1 Introduction", "content": "Stemming from the ever-increasing number of\nparameters and large-scale training corpus, large\nvision-language models (LVLMs) (Zhu et al., 2023;\nBai et al., 2023; Liu et al., 2023; Achiam et al.,\n2023; Lu et al., 2024; Chen et al., 2024b) have\ndemonstrated remarkable capabilities in general vi-"}, {"title": "2 Related Work", "content": "Large Vision-Language Models Large Vision-\nLanguage Models, which integrate vision encoders,\nconnectors, and large language models to enhance\ncross-modal understanding, have emerged as pow-\nerful frameworks that combine visual and textual\ninformation for a wide range of tasks. These mod-\nels can be systematically categorized based on the\ntype of connector. The first category comprises\napproaches utilizing cross-attention-based connec-\ntors, such as Flamingo (Alayrac et al., 2022) and\nCogVLM (Wang et al., 2023; Hong et al., 2024),\nwhich exploit attention mechanisms to facilitate\nthe exchange of information between the vision\nencoder and the language model. The second cate-\ngory includes methods that employ query-based\nconnectors, such as BLIP-2 (Li et al., 2023a),\nInstruct-BLIP (Dai et al., 2023), mPLUG-owl2 (Ye\net al., 2024), and Qwen-VL (Bai et al., 2023),\nwherein queries are leveraged to orchestrate the\ninteraction between visual and textual modalities,\nthereby enhancing the alignment and coherence of\nvisual and linguistic representations. Furthermore,\nprojection-based connector methods, exemplified\nby LLaVA (Liu et al., 2023), Mini-GPT4 (Zhu et al.,\n2023), DeepSeek-VL (Lu et al., 2024), and Mini-\nGemini (Li et al., 2024b), project visual data into a\nshared embedding space, thereby fostering seam-\nless integration with textual information. These in-\nnovations offer a range of solutions for cross-modal\nunderstanding, driving the potential applications of"}, {"title": "3 Dataset Construction", "content": "In this section, we outline the pipeline of construct-\ning MMXU, as shown in Figure 2. The process\nstarts with the Chest ImaGenome dataset (Wu et al.,\n2021), which includes the silver_dataset section\ncontaining annotations for 243,310 images from\n63,945 patients. These annotations cover bounding\nboxes for 29 anatomical regions, along with cor-\nresponding region-level report phrases, labeled at-\ntributes, and relationships. The entire method con-\nsists of four distinct phases: (1) Comparative Sen-\ntences Extraction (\u00a73.1), (2) Comparative Targets\nSelection (\u00a73.2), (3) QA pairs Generation (\u00a73.3),\nand (4) Post-Processing (\u00a73.4)."}, {"title": "3.1 Comparative Sentences Extraction", "content": "In the first stage, our objective is to identify and\nextract sentences that contain comparative infor-\nmation, forming the foundation for generating\nquestion-answer pairs in subsequent stages. For\nexample, the sentence \u201cPreviously seen ill-defined\nperibronchial lower lobe opacity seen on lateral\nview has resolved,\u201d along with its associated rela-\ntionships label \u201ccomparisonlyeslimproved\u201d explic-\nitly indicates a comparison with prior conditions,\nhighlighting the resolution of the lower lobe opacity\nand thus signaling an improvement in the patient's\ncondition. In total, we extract 232,247 compara-\ntive sentences, encompassing 22,770 patients and\n102,606 reports. A more detailed example of such\na sentence is provided in Appendix C.1."}, {"title": "3.2 Comparative Targets Selection", "content": "At this stage, we categorize comparative sentences\ninto three groups based on disease progression:\n\u201cWorsen\u201d (108,734), \u201cImproved\u201d (91,084), and \u201cNo\nChange\" (264,676), with the majority concentrated\nin the \u201cNo Change\u201d category. Given that the preva-\nlence of medical conditions often follows a long-\ntail distribution (Wu et al., 2024b), it is essential to"}, {"title": "3.3 QA pairs Generation", "content": "In the third stage, we employ GPT-40 as the genera-\ntor, leveraging comparative sentences, relationship\nlabels, and reports from two visits as foundational\ndata to guide the model in producing question-\nanswer pairs. Our observations suggest that a sin-\ngle sentence can encapsulate multiple changes. For\ninstance, the sentence \u201cBilateral pleural effusions\nare again seen, and atelectasis is present\u201d concur-\nrently describes alterations in both \u201cpleural effu-\nsions\u201d and \u201catelectasis\u201d. To fully harness this data,\nwe direct GPT-40 to generate up to three distinct\nquestion-answer pairs, ensuring maximum diver-\nsity. Additionally, to facilitate the future validation\nof data accuracy, we require the model to provide\njustifications for the generated pairs.\nIn total, we have established six rules to guide\nGPT-40 generating QA pairs that capture changes\nin the same area across the two reports. Appendix\nC.2 presents our prompt template and a detailed\nexample during the QA generation process."}, {"title": "3.4 Post-Processing", "content": "Finally, we conduct post-processing to further en-\nsure the quality of generated QA-pairs. Since im-\nbalanced answer options can mislead the model\nduring training and result in unfair evaluations dur-\ning testing, we first balance the distribution of four\nanswer choices in the QA pairs generated by GPT-\n40. Moreover, we employ GPT-40 to answer the\ngenerated questions with the prompt detailed in Fig-\nure 3, which contains the information used during\nquestion generation. Questions that were answered\nincorrectly are considered either excessively diffi-\ncult or erroneous and are removed. As illustrated in\nTable 2, 95.1% of the questions were answered cor-\nrectly. The final candidate set comprises 121,800\nQA pairs, including 47,000 instances of \"Worsen\",\n41,000 instances of \u201cImproved\" and 35,000 in-\nstances of \"No Change\"."}, {"title": "4 Benchmark Results", "content": "To ensure the professionalism and accuracy of\nMMXU-test benchmark, we recruited a panel of 5\nboard-certified chest radiology experts to assess it.\nFollowing that, we evaluated the performance of\nseveral prominent open-source and closed-source\nLVLMs capable of supporting multi-image VQA\non the MMXU-test benchmark. Since all the ques-\ntions from our benchmark are single-choice, we\nuse accuracy as the metric."}, {"title": "4.1 Evaluation Models", "content": "Prominent medical large vision-language mod-\nels (LVLMs), such as LLaVA-Med-v1 (Li et al.,\n2024a), LLaVA-Med-v1.5 (Li et al., 2024a),\nMiniGPT-Med (Alkhaldi et al., 2024), and\nXrayGPT (Thawkar et al., 2023), have predomi-\nnantly been trained on single-image visual ques-\ntion answering (VQA) tasks. This limitation poses\nchallenges in accurately evaluating our dataset's\nperformance on these models. Therefore, all the\nmodels used in this paper belong to the general\ndomain, as listed below:\nOpen-source LVLMs: Qwen2-VL 2B& 7B (Bai\net al., 2023), DeepSeek-VL 1.3B& 7B (Lu et al.,\n2024), InternVL2 1B&2B& 4B&8B (Chen et al.,\n2024b), IDEFICS2 8B (Lauren\u00e7on et al., 2024)\nand Llama3.2-Vision 11B (Touvron et al., 2023)\nClosed-source LVLMs: GPT-40 (Achiam et al.,\n2023) and Claude-3-5-sonnet"}, {"title": "4.2 Human Expert Evaluation", "content": "To evaluate the quality of the MMXU-test bench-\nmark, we conduct the human expert evaluation with\nfive radiologists. The data from the MMXU-test\nbenchmark was randomly divided into five parts,\ncontaining 500, 500, 500, 750, and 750 questions,\nrespectively. We ensured that the three question\ncategories were distributed as evenly as possible\nwithin each subset. The evaluation results are pre-\nsented in Table 3. Except for Expert 3, all experts\nachieved an accuracy rate of at least 96.0%, with\nan overall accuracy reaching 95.3%. These find-\nings demonstrate that the MMXU-test benchmark\nis both highly professional and well-structured."}, {"title": "4.3 Model Performance", "content": "We evaluated the performance of several well-\nknown open-source and closed-source models\non two benchmarks: VQA-RAD, which focuses\non single-image visual question answering, and\nMMXU-test, which emphasizes multi-image dif-\nference analysis. The experimental results are pre-\nsented in Table 4. For open-ended questions in\nVQA-RAD (Lau et al., 2018), we utilized GPT-\n4o to compare the model-generated answers with\nthe ground truth, incorporating a certain level of\ntolerance to assess correctness (He et al., 2024).\nComparing the performance of these models on\nsingle-image and multi-image benchmarks, it is\nclear that the accuracy of the same model on\noverall accuracy on MMXU-test is lower than\nits VQA-RAD closed-ended questions. For exam-\nple, with the Qwen2-VL 7B model, the accuracy on\nVQA-RAD closed questions reaches an impressive\n74.5%. However, its performance on MMXU-test\ndrops significantly to just 45.8%. This discrep-\nancy highlights the gap between current MedVQA\nbenchmarks and the demands of real-world sce-\nnarios, suggesting that models that perform well\non public benchmarks may not be effective in sup-\nporting clinical diagnosis. Furthermore, as shown\nin Table 3, even the best-performing model lags\nbehind human experts by nearly 40%. This high-\nlights the existing limitations of these models in\nmulti-image MedVQA tasks.\nAdditionally, we can observe that almost all\nopen-source models exhibit significant accuracy\ndiscrepancies across the three types of questions\nin the MMXU-test, suggesting that these models\nhave certain biases when interpreting disease pro-\ngression. This bias is particularly noticeable in\nsmaller models, such as Qwen2-VL 2B, which\nshows an accuracy of 71.2% on the \u201cWorsen\u201d ques-\ntion, while its accuracy on the other two question\ntypes is below 30%. Clearly, this model tends\nto favor results associated with disease deteriora-\ntion. Additionally, such biases are less apparent\nin closed-source models. As the model size in-\ncreases, the overall accuracy improves, and the\nbias of progression gradually diminishes. Despite\nall models being general-domain models, their re-\nsults still exhibit noticeable differences."}, {"title": "5 MedRecord-Augmented Generation", "content": "Through prior experiments, we pinpointed that cur-\nrent LVLMs face significant challenges in identi-\nfying disease progression. To address this chal-\nlenge, we propose a novel approach, MedRecord-\nAugmented Generation (MAG). In routine diag-\nnostic practice, physicians often rely on a patient's\nhistorical records to inform their analysis and diag-\nnosis of current conditions. To replicate this pro-\ncess, we integrate historical records directly into\nthe prompt as contextual information. Our study\ninvestigates the effectiveness of global reports de-\nrived from historical images, alongside regional\nreports related to the specified questions, which we\ncategorize as global and regional historical records,\nrespectively."}, {"title": "5.1 Effectiveness of MAG", "content": "Firstly,\nwe conducted experiments with\nMedRecord-augmented generation tests on both\nthe open-source models InternVL2 8B, Qwen2-VL\n7B, and the closed-source model GPT-40. The\noutcomes of these experiments are summarized\nin Table 5. It is clear that providing historical\nrecords significantly improves the models'\noverall accuracy. With historical records, the\nopen-source models achieved accuracy comparable\nto that of closed-source commercial models.\nExcept for GPT-40, using medical records led to\nimprovements across all three types of questions,\nindicating that these models could understand and\nreason from historical records instead of merely\nrepeating past information. Moreover, it can be\nobserved that regional historical records offer\ngreater improvements than global records.\nThis may be attributed to the models' limited\nability to analyze contextual and region-specific\ndetails, making it difficult for them to identify the\nmost relevant information. By providing precise,\nregion-specific information, the models are better\nable to extract valuable information."}, {"title": "5.2 MAG Fine-tuning on MMXU-dev", "content": "Furthermore, we assess the efficacy of MMXU-\ndev dataset and our proposed MAG method by\nfine-tuning the InternVL2 8B. The model was fine-\ntuned using 20%, 40%, 60%, 80%, and 100% of the\nMMXU-dev dataset. We evaluate the performance\non MMXU-test benchmark. The corresponding\nfine-tuning results are presented in Table 6.\nWe observed that even with just 20% of the\nMMXU-dev, the model achieved substantial im-\nprovements in accuracy across all three problem"}, {"title": "5.3 Case Study", "content": "In Figure 4, we present two answer samples pro-\nvided by GPT-40, InternVL2 8B, Qwen2-VL 7B,\nand fine-tuned InternVL2 8B, evaluated under vari-\nous historical record strategies. Case 1 illustrates\na successful scenario where all models adjusted\ntheir responses accurately when regional historical\nrecords were incorporated. However, the zero-shot\nInternVL model misinterpreted critical informa-"}, {"title": "6 Conclusion", "content": "In this paper, we introduce MMXU, a dataset de-\nsigned for multi-modal and multi-X-ray under-\nstanding in MedVQA. First, we propose a bench-\nmark, MMXU-test, and invite five chest X-ray ex-\nperts to evaluate the performance. Then, we con-\nduct evaluations using several well-known open-\nsource and closed-source large vision-language\nmodels (LVLMs) that support multi-image VQA.\nThe experimental results indicate that even the\nbest-performing models exhibit a significant per-"}, {"title": "Limitation", "content": "Although we have carefully designed our MMXU-\ntest benchmark and established the MMXU-dev\ndataset, there are still some limitations: 1) Our\ndataset is based on MIMIC-CXR, a chest X-ray\ndataset, which somewhat limits its generalization\nwhen applied to other datasets. 2) Most cur-\nrent medical LVLMs, which have extensive med-\nical knowledge, only support single-image VQA,\nthus restricting our evaluation to general-domain\nLVLMs capable of handling multiple images. 3)\nWe have proposed the MAG method, inspired by\nclinical scenarios, to validate the effectiveness of\nhistorical historical records in enhancing LVLMs\u2019\nmedical responses. Although we aim to replicate\nclinical scenarios as closely as possible, the scarcity\nof data means we can only use previous reports as\nhistorical records for research purposes."}, {"title": "A. Pilot Studies on Medical Diff-VQA", "content": ""}, {"title": "A.1 Motivation", "content": "In the clinical diagnostic process, medical experts\nintegrate a patient's medical historical records with\ncurrent medical evidence to guide their diagnos-\ntic decisions. To explore whether this approach\ncan enhance the diagnostic capabilities of large\nvision-language models in medical settings, we\nfine-tuned and evaluated InternVL2 8B and Qwen2-\nVL 8B on the publicly available Medical-Diff-VQA\ndataset (Hu et al., 2023), incorporating historical\nmedical records.\nThe Medical-Diff-VQA dataset comprises seven\ncategories of questions: abnormality, location, type,\nlevel, view, presence, and difference. However,\nwe excluded the view questions, as they do not re-\nquire historical information. Therefore, we selected\nsix categories of questions from the Medical-Diff-\nVQA dataset for our study: abnormality, location,\ntype, level, presence, and difference. The distribu-\ntion of the training and test sets for each category is\ndetailed in Table 7. For each category, two distinct\ntemplates were used as model inputs: one incor-\nporating historical records and the other devoid of\nadditional information, as illustrated in Figure 6\nand Figure 7. These templates were employed to\nfine-tune and evaluate the model performance. It is\nworth noting that not all questions are associated\nwith historical information; some are constructed\nbased on records from a patient's initial visit, seen\nin Figure 5(a). In such instances, we directly em-\nployed the template without additional information\nas the model input.\nFor historical records, we utilized the report in\nMIMIC-CXR dataset, which contains data from\n65,079 subjects, each representing an individual pa-\ntient. A subject may have multiple studies, where\neach study corresponds to a patient's visit and in-\ncludes multiple chest X-ray (CXR) images along\nwith an associated medical report. As shown in\nFigure 5(b), the blue line shows the average L1\ndistance variation between the Chexpert (Ye et al.,\n2020) labels of different studies within the same\nsubject. The red dashed line indicates the aver-\nage L1 distance between patients who meet certain\nconditions in terms of the number of studies. It is\nevident that as the study order increases, the differ-\nences between studies grow significantly. Further-\nmore, under the same conditions, the differences\nbetween patients are much greater than those within\na single patient. Based on these observations, we"}, {"title": "A.2 Experimental", "content": "Experimental Setting We benchmarked our ap-\nproach against the DRax method (Nisar et al.,\n2025), employing distinct evaluation metrics: to-\nken recall for open-ended questions and accuracy\nfor close-ended ones. The models utilized in the\nexperiments were InternVL2 8B and Qwen2-VL\n8B. During the fine-tuning process, the LoRA rank\nwas set to 16, the learning rate to 4e-5, and a cosine\nlearning rate scheduling strategy was implemented."}, {"title": "C.1 Comparative Sentences Extraction", "content": "Figure 10 presents a detailed example of a compar-\native sentence extracted during the dataset construc-\ntion process. The example includes the compara-\ntive sentence itself, the specific regions it pertains\nto, and the two corresponding images. Additionally,\nthe bounding boxes of the relevant regions within\nthe images, along with the comparative relation-\nship, are illustrated. This detailed data structure is\ncrucial for building a robust dataset that facilitates\naccurate comparisons during QA generation."}, {"title": "C.2 QA Pairs Generation", "content": "The version of GPT-40 we utilized is gpt-40-2024-\n08-06. We employ the prompt template shown in\nFigure 11, where the \"explanatory information\"\nis derived from the details associated with the re-\nlationship label. Figure 16 illustrates a detailed\nexample of the input prompt and output content\nfor the question-answer pair model using GPT-40.\nWe provided six specific rules and supplied GPT-"}, {"title": "E More Case Study", "content": "Figure 15 shows two challenging examples from\nthe \"Worsen\" category in MMXU. In CASE 1, none\nof the models without fine-tuning were able to cor-\nrectly answer the question, and only the fine-tuned\nmodel using the MAG method provided the correct\nanswer. This may be due to the difficulty of the\nquestion itself, as well as the challenge of extract-\ning useful information to answer the question from\nthe historical records.In CASE 2, only GPT-4o was\nable to answer the question correctly after provid-\ning historical records. For other models, since R-\nMRec reiterated previously unchanged views, all\nincorrectly chose option A (no change), indicating\nthat these models failed to correctly understand the\nchange and simply repeated the previous informa-\ntion."}]}