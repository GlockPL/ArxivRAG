{"title": "ED-VIT: Splitting Vision Transformer for Distributed Inference on Edge Devices", "authors": ["Xiang Liu", "Yijun Song", "Xia Li", "Yifei Sun", "Huiying Lan", "Zemin Liu", "Linshan Jiang", "Jialin Li"], "abstract": "Deep learning models are increasingly deployed on resource-constrained edge devices for real-time data analytics. In recent years, Vision Transformer models and their variants have demonstrated outstanding performance across various computer vision tasks. However, their high computational demands and inference latency pose significant challenges for model deployment on resource-constraint edge devices. To address this issue, we propose a novel Vision Transformer splitting framework, ED-ViT, designed to execute complex models across multiple edge devices efficiently. Specifically, we partition Vision Transformer models into several sub-models, where each sub-model is tailored to handle a specific subset of data classes. To further minimize computation overhead and inference latency, we introduce a class-wise pruning technique that reduces the size of each sub-model. We conduct extensive experiments on five datasets with three model structures, demonstrating that our approach significantly reduces inference latency on edge devices and achieves a model size reduction of up to 28.9 times and 34.1 times, respectively, while maintaining test accuracy comparable to the original Vision Transformer. Additionally, we compare ED-ViT with two state-of-the-art methods that deploy CNN and SNN models on edge devices, evaluating accuracy, inference time, and overall model size. Our comprehensive evaluation underscores the effectiveness of the proposed ED-ViT framework.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep learning models have been increasingly deployed on resource-constrained edge devices to meet the growing demand for real-time data analytics in industrial systems [5, 6, 62] and have demonstrated remarkable capabilities in various applications such as video image analysis and speech recognition. Convolutional Neural Networks (CNNs) [27] like VGGNet [40] and ResNet [20], as well as Spike Neural Networks (SNNs) [10], have achieved satisfactory performance in many edge computing scenarios. As the field progresses, researchers are exploring the deployment of more complex structured models on edge devices to further improve performance. Transformer architecture [49], which has revolutionized natural language processing (NLP) tasks, has inspired similar advancements in computer vision. Vision Transformer (ViT) models [1] and their variants have shown outstanding"}, {"title": "2 Related Works", "content": "Deploying Vision Transformer models in resource-constrained environments poses significant challenges due to their intensive computational and memory demands. These approaches address Vision Transformer resource limitations via pruning, encompassing both local and global strategies as follows."}, {"title": "2.1 General Vision Transformer Compression", "content": "Local pruning techniques focus on removing redundant components within specific layers of the model. For instance, PVT [52] and its successor PVTv2 [51] introduce a pyramid hierarchical structure to transformer backbones, achieving high accuracy with reduced computation. Zhu et al. [69] applies sparsity regularization during training and subsequently prunes the dimensions of linear projections, targeting less significant parameters. Xia et al. [57] prunes multi-head self-attention (MHSA) and feed-forward networks (FFN), which are often redundant components. Liang et al. [31], Liu et al. [32] proposes network pruning to eliminate complexity and model sizes by reducing tokens. Other noteworthy contributions include DToP [44], which enables early token exits for semantic segmentation tasks. Conversely, global pruning techniques adopt a comprehensive perspective by evaluating and pruning the overall significance of neurons or layers across the entire network. SAVIT [68] purposes structure-aware Vision Transformer pruning via collaborative optimization. For instance, CP-ViT [42] systematically assesses the importance of head and attention layers for the purpose of pruning, while Evo-ViT [58] identifies and preserves significant tokens, thereby discarding those of lesser importance. Moreover, the Skip-attention approach [50] facilitates the omission of entire self-attention layers, thereby exemplifying a global pruning methodology. X-pruner [64] employs explainability-aware masks to inform its pruning decisions, thereby advancing a more informed global pruning strategy. In addition, UP-ViT [63] introduces a unified pruning framework that leverages KL divergence to guide the decision-making process for pruning, while LORS [30] optimizes parameter usage by sharing the majority of parameters across stacked modules, thereby necessitating fewer unique parameters per module.\nAmong existing pruning methods, UP-ViT [63] has the closest resemblance to our approach. However, it is important to note that these techniques cannot be directly applied to edge devices: they often suffer from poor performance when the pruning ratio is high or incur high computation overhead when the pruning ratio is"}, {"title": "2.2 Vision Transformer on Edge Devices", "content": "There are several methods focused on deploying Vision Transformer on-edge devices, which can be classified into three major categories.\nArchitecture and Hierarchy Restructuring: HVT [37] compresses sequential resolutions using hierarchical pooling, reducing computational cost and enhancing model scalability. LeViT [18] is a hybrid model that combines the strengths of CNNs and transformers. For image classification tasks, it utilizes the hierarchical structure of LeNet [27] to optimize the balance between accuracy and efficiency, and uses average pooling in the feature map stage.\nEncoder Block Enhancements: ViL [67] introduces a multi-scale vision longformer that lessens computational and memory complexity when encoding high-resolution images. Poolformer [65] deliberately replaces the attention module in transformers with a simple pooling layer. LiteViT [59] introduces a compact transformer backbone with two new lightweight self-attention modules (self-attention and recursive atrous self-attention) to mitigate performance loss. Dual-ViT [61] reduces feature map resolution, consisting of two dual-block and two merge-block stages. MaxViT [46] divides attention into local and global components and decomposes it into a sparse form with window and grid attention. Slide-Transformer [36] proposes a slide attention module to address the problem that computational complexity increases quadratically with the attention modules.\nIntegrated Approaches: Some methods integrate both of the above approaches. CeiT [66] combines Transformer and CNN strengths to overcome the shortcomings of each, incorporating an image-to-tokens module, locally-enhanced feedforward layers, and layer-wise class token attention. CoAtNet [8] combines depth-wise convolutions and simplifies traditional self-attention by relative attention, enhancing efficiency by stacking convolutions and attention layers.\nHowever, they fail to link pruning with specific classes, which limits their methods when both high performance and low memory usage are required."}, {"title": "2.3 Split Learning", "content": "Current works that combine Vision Transformer and split learning primarily focus on federated learning, addressing data privacy and efficient collaboration in multi-client environments [2, 34, 35], where the inner structure of a large model is split across smaller devices and later fused [43]. However, these approaches do not target the deployment of Vision Transformer on edge devices.\nTraditional machine learning model splitting generally involves partitioning a large model into multiple smaller sub-models that can be executed collaboratively on resource-constrained devices, providing a promising technique for deploying models on edge devices. Splitnet [23] clusters classes into groups, partitioning a deep neural network into tree-structed sub-networks. Bakhtiarnia et al. [3] dynamically partitions models based on the communication channel's state. Nnfacet [5, 6] splits large CNNs into lightweight class-specific sub-models to accommodate device memory and energy constraints, with the sub-models being fused later. Yu et al. [62] follows a similar approach to split deep SNNs across edge devices. Distredge [21] uses deep reinforcement learning to compute the optimal partition for CNN models.\nTo the best of our knowledge, our work presents the first exploration of Vision Transformer model partitioning for edge deployment, marking a significant contribution to this field. Drawing inspiration from previous studies [5, 6, 62], our framework, ED-ViT, introduces an innovative approach to decompose a multi-class ViT model into several class-specific sub-models, each performing a subset of classification. Unlike relying on channel-wise pruning, ED-ViT employs advanced pruning techniques specifically designed for the unique architecture of Vision Transformers."}, {"title": "3 Problem Formulation", "content": "The structures of three representative Vision Transformer models, ViT-Small, ViT-Base, and ViT-Large, are presented in Table 1. The number of operations is commonly used to estimate computational energy consumption at the hardware level. In Vision Transformer models, almost all floating-point operations (FLOPs) are multiply-accumulate (MAC) operations."}, {"title": "4 Methodology", "content": "This section describes the design of the ED-ViT framework proposed to solve the optimization problem outlined in Eq. 1. We first explain the main workflow of ED-ViT and then provide detailed descriptions of the four key steps involved."}, {"title": "4.1 Design Overview", "content": "As illustrated in Figure 1, ED-ViT leverages the unique characteristics of Vision Transformer and the collaboration of multiple edge devices. The framework involves N concurrent edge devices for distributed inference alongside a lightweight MLP aggregation to derive the final classification results. Initially, the original Vision Transformer is trained on the entire dataset to achieve high test accuracy for the classification task. The ED-ViT framework is composed of four main components: model splitting, pruning, assignment, and fusion. During model splitting, the Vision Transformer model is divided into sub-models, each responsible for a subset of classes. To reduce computation overhead, these sub-models are further pruned using model pruning techniques. Subsequently, the sub-models are assigned to the appropriate edge devices, taking the optimization problem into consideration. Finally, the aggregation server fuses the outputs from the edge devices to produce the final inference results. The specific details of each component are provided below."}, {"title": "4.2 Model Splitting", "content": "In the original Vision Transformer, different heads contribute to learning and inferring from the samples. However, for certain classes, maintaining all the connections between the heads can be redundant. As a result, ED-ViT prunes these connections and reconstructs the heads, with more retained heads leading to more parameters and connections being preserved. As illustrated in Algorithm 1, each Vision Transformer sub-model undergoes pruning based on a head number threshold and its associated categories, following a relatively equitable workload distribution. Subsequently, a greedy search mechanism is used to identify the most suitable edge device model assignment plan for deploying a particular sub-model, considering both energy and memory constraints. If the total memory size exceeds the budget or no suitable plan is found, an iterative approach is applied to adjust the number of heads for the sub-model with the biggest memory size to be pruned, repeating the allocation process until all sub-models are successfully assigned to edge devices. The pruning and the greedy assignment methods are shown as Algorithm 2 and Algorithm 3, located in Section 4.3 and Section 4.4, respectively."}, {"title": "4.3 Model Pruning", "content": "We believe that reducing the computational burden of Vision Transformer will significantly contribute to lowering inference latency in distributed edge device settings. We focus on the original ViT architecture [1], chosen for its simplicity and well-defined design space, focusing on redistributing the dimensionality across different blocks to achieve a more balanced tradeoff between computational efficiency and accuracy, as shown in Figure 2.\nAnalysis of Prunable Parameters: The main prunable components in a ViT block, as illustrated in Figure 2, are:\n\u2022 Residual Connection Channels (Red, d): The channels across the shortcut connections within the transformer blocks.\n\u2022 Heads in MHSA (Green, h): The dimensions of the query, key, value projections ($d_q, d_k, d_v$)\n\u2022 Feed-Forward Network (FFN) Hidden Dimensions (Blue, c): The dimension c of the hidden layer used for expanding and reducing.\nPruning Process: As illustrated in Figure 2, The pruning process is carried out in stages, with each stage focusing on one of the prunable components. We compute the KL-Divergence between\n$d_q = d_k = d_v = d/h$"}, {"title": "4.4 Model Assignment", "content": "To address the optimization problem expressed in Eq. 1, we propose a greedy search algorithm for assigning Vision Transformer sub-models to edge devices. As shown in Algorithm3, the sub-models are first sorted based on their energy consumption (computation overhead). ED-ViT assigns the most computation-intensive sub-model first based on their model sizes, which is proportional to the computation overhead as in Section 3. The algorithm then iteratively assigns the remaining sub-models to maximize the system's available energy. Initially, the device with the highest computational power is selected. If the remaining memory and energy can accommodate the sub-model, we update the device's available memory and energy. Otherwise, if the sub-model exceeds the device's memory capacity, the memory-exhausted device is removed from the set. If no devices remain, it indicates that the current pruning results prevent deployment of all sub-models. In this case, the algorithm terminates, and the ED-ViT framework re-prunes the"}, {"title": "4.5 Model Fusion", "content": "In the result fusion phase, each sub-model on the edge devices processes inputs and extracts corresponding features. The server aggregates the generated features through concatenation and feeds them into an MLP to produce the final prediction. Notably, the MLP for result fusion requires training only once after all sub-models have been trained.\nIn our paper, we utilize a tower-structured MLP to process the concatenated tensors received from the various edge devices. Specifically, each transmitted tensor from a device is integrated using a $N \\times d \\times s \\rightarrow A \\times N \\times d \\times s \\rightarrow numcls$ MLP structure, where A is the shrinking hyperparameter and the default value is 0.5, numcls is the number of classes. By utilizing a compact MLP model, we effectively fuse the distributed inference results from the sub-models while consuming only a minimal amount of computational resources."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Experiments settings", "content": "Datesets. Considering the versatile applicability of the framework across various scenarios, we select three computer vision datasets (CIFAR-10 [26], MNIST [29], and Caltech256 [19]) and two audio recognition datasets (GTZAN [47] and Speech Command [54]) to construct the classification tasks for our experiments. We choose these datasets based on the real-world tasks as explained in Appendix A. For all the computer vision datasets, we resize the sample to 224 \u00d7 224 x 3 to support various datasets and downstream tasks via a similar data structure without loss of generality; for the audio recognition datasets, we resize the sample to 224 \u00d7 224 \u00d7 1 with the same aim. The details of the datasets and the preprocessing methods are shown in Appendix C.\nImplementation Details: All models are implemented using Pytorch [38]. During the training process, we use the Adam optimizer [12] with a decaying learning rate initialized to 1e-4, and we set the batch size to 256. For the computer vision task, the original Vision Transformer model is pre-trained on the ImageNet dataset [9], followed by fine-tuning the task-specific data for 10 epochs. For the audio recognition task, Vision Transformer is pre-trained on the AudioSet dataset [15] and then fine-tuned on the task data for about 20 epochs. All the experimental results are averaged over five trial runs. Each trial is conducted on an NVIDIA GeForce RTX 4090 GPU and 1 to 10 Raspberry Pi-4B devices, which serve as the edge devices for evaluating the execution time of processing a single sample on a specific sub-model."}, {"title": "5.2 Experiments on Computer Vision Datasets", "content": "We evaluate our approach using CIFAR-10, MNIST, and Caltech image datasets. The original model size is 327.38 MB. Figure 3 shows the accuracy, inference latency, and memory usage of the ViT-Base model under the ED-ViT framework, with 1 to 10 edge devices. With only one edge device, we apply model compression by pruning Vision Transformer without decomposition. All experiments are conducted with a total memory budget of 180MB across devices, ensuring fair comparisons.\nThe results demonstrate that as the number of edge devices increases, the accuracy remains largely consistent and yields strong performance. For CIFAR-10, accuracies are consistently above 85%; for MNIST, they are above 91%; and for Caltech, they exceed 90%. In most cases, the variance in final fusion prediction accuracy is less than one percentage point. The inclusion of more sub-models illustrates the feasibility of deploying larger-scale models without significant accuracy loss. As the number of edge devices increases, the inference latency decreases, as each sub-model is responsible for fewer classes and contains fewer parameters. Notably, the latency for the original model is 36.94 seconds on the CIFAR-10 dataset, which is 28.9 times the smallest latency (1.28s) and 3.84 times the highest latency (9.63s). Our ED-ViT could make multiple edge devices work collaboratively to maintain accuracy while lowering the storage burden and inference time as the number of edge devices increases. The results for other datasets show a similar trend as the model structures are the same.\nIn terms of total memory usage, ED-ViT provides effective splitting and assignment strategies. As the number of edge devices increases, the memory size of each sub-model decreases, reducing computation overhead, and demonstrating that many complex model designs and computational operations are redundant for problem-solving. In the 10-edge device setting, the model size on the CIFAR-10 dataset is reduced to just 9.6MB, achieving a size reduction of up to 34.1 times compared to the original model."}, {"title": "5.3 Experiments on Audio Recognition Datasets", "content": "We use the GTZAN and Speech Command audio datasets to evaluate the performance of our framework. The original model sizes of Vision Transformer for GTZAN and Speech Command is 325.88MB. Figure 4 presents the accuracy, inference latency, and total memory size of the ViT-Base model as implemented by the ED-ViT framework, similar to the experiments with the computer vision datasets. We still set the memory budget to 180MB.\nThe results show that as the number of edge devices increases, ED-ViT is able to maintain the accuracy, delivering robust performance. For GTZAN, accuracies are consistently above 84%, and for"}, {"title": "5.4 Experiments on Different Vision Transformer Model Structures", "content": "We also select two complex datasets (CIFAR-10 and Caltech) to test different Vision Transformer structures for low-power video analytics tasks. The original model sizes of ViT-Small and ViT-Large are 82.71MB, 1,157MB, respectively. Figure 5 presents the accuracy, inference latency, and total memory size of the ViT-Small and ViT-Large models as implemented by the ED-ViT framework, similar to Figure 3. We increase the total memory size limit for ViT-Large to 600MB and decrease the limit for ViT-Small to 50MB.\nThe results show that as the number of devices increases, the accuracy remains relatively consistent, again showing robust performance. For ViT-Small, the accuracy is over 76.5% on the CIFAR-10 dataset and over 77.39% on Caltech across all settings; for ViT-Large, the accuracy is over 86% on the CIFAR-10 dataset and over 90.48% on Caltech in all settings. In most cases, the accuracy fluctuation for the final fusion prediction remains within a variance of less than one percentage point. The accuracy for ViT-Small is lower than that of ViT-Base, while ViT-Large achieves higher accuracy than ViT-Base, corresponding to the difference in parameter counts. Generally, the more parameters, the better the accuracy. As the number of edge devices increases, the latency decreases for both settings, similar to ViT-Base. The latency for ViT-Small is lower than that of ViT-Base, as ViT-Small requires less computational power, while the latency for ViT-Large is higher due to its larger size. In terms of memory size, in the 10-edge device setting, for each model on the CIFAR-10 dataset, the size for ViT-Small is 2.58MB, achieving a reduction of up to 32.06 times compared to the original model. Similarly, for ViT-Large, the size is 18.73MB, which also achieves a 61.77-fold reduction compared to the original model size. Note that for the ViT-Small on the CIFAR-10 and Caltech, the input size and the output size are the same; thus, their latency and total memory size on the edge devices are also the same. Similar results are observed across both datasets for ViT-Small and ViT-Large."}, {"title": "5.5 Comparison with Baseline Methods: Splitting CNN and SNN", "content": "Vision Transformer achieves better accuracy compared to traditional CNN and SNN models. However, the performance of these models on edge devices has not been directly compared before. Nn-facet [5, 6] proposes a method to split CNNs across multiple edge devices, employing a filter pruning technique [22], which differs from our approach. Yu et al. [62] utilize the convolutional spiking neural network (CSNN) [10] to transform CNNs into SNNs, using a similar strategy. Both methods focus on VGGNet [40] networks and are channel-wise methods. In our experiments, the baseline model for these methods is VGGNet-16 in their papers, which also has a memory size similar to ViT-Base and achieves the best original results for comparison. We follow the hyper-parameters in their papers to get the results."}, {"title": "5.6 Experiments on Effects for Retraining", "content": "As we quantify model accuracy, we perform an ablation study to assess the impact of retraining. The results are shown in Table 3. The first line shows the results of the original ED-ViT. The second line shows the results from averaging the softmax output of sub-models without the fusion MLP. The third line shows the results based on the retraining of the overall models (sub-models and MLP together) for the fusion stage. When using only one device, the result is the same as the original ED-ViT as the training process remains unchanged in this scenario. Different from the work on splitting SNN and CNN, which are based on channel-wise methods and only get about 0.1% improvement in performance when retaining the overall models, our method is shown to have a great potential to improve performance (up to 6.15%). However, in the practical setting, it may be hard to retrain the sub-models with MLP."}, {"title": "5.7 Supplementary Experiments", "content": "Experiments of FLOPs computation, case studies for heterogeneous sub-models, case studies for heterogeneous edge devices, experiments with Transformer following the method extending ED-VIT to Transformer as Appendix D, and experiments with t-SNE visualization can be found in Appendix E."}, {"title": "6 Conclusion", "content": "In this study, we are the first to propose a novel model-partitioning framework aimed at deploying Vision Transformer on edge devices. The formulation and resolution of the problem offer a viable solution, ED-ViT, which decomposes the Vision Transformer model into smaller sub-models and leverages the state-of-the-art pruning method to streamline the complex network architecture. ED-ViT not only preserves the essential structure of the original model but also enables more efficient inference, maintaining high system accuracy within the memory and energy constraints of edge devices. Extensive experiments have been conducted on five datasets, three ViT architectures, and two baseline methods, using three evaluation metrics of accuracy, inference latency, and total memory size. The results demonstrate that ED-ViT significantly reduces overall energy consumption and inference latency on edge devices while maintaining high inference accuracy. Our ED-ViT shows great potential for deployment on edge devices and for future integration with other horizontal methods to achieve better performance."}, {"title": "A Background", "content": "This section describes the applications that utilize concurrent edge devices."}, {"title": "In-Situ Low-Power Video Analytics", "content": "Recently, edge devices, such as wireless smart cameras, have already been widely employed for video analytics, including traffic control and industrial activity monitoring. Low-power edge devices can be deployed ad hoc whenever needed. Vision Transformer is an ideal deep learning model for these tasks due to its high accuracy. However, a complex Vision Transformer may not fit within the limited memory and resource-constrained edge devices. One potential solution is to split the original complex Vision Transformer into multiple small class-specific sub-models, which can be loaded into edge devices to process the consecutive image frames sequentially, incurring only a few frame delays. The outputs from the sub-models are then combined to get the final real-time video analytics results, as illustrated in Figure 7."}, {"title": "Audio Recognition", "content": "In smart homes, an increasing number of concurrent microphones on edge devices are used for collaborative keyword spotting and speech recognition. In some factories, deep learning models monitor machine conditions by detecting sound spectrograms. Beyond the main focus of low-power video analytics, Vision Transformer could also be employed to analyze audio data, demonstrating the generalization capability of the ED-ViT framework."}, {"title": "B Pruning Method Details", "content": null}, {"title": "B.1 Analysis of Prunable Parameters", "content": "The main prunable components in a ViT block, as illustrated in Figure 2, are:\n(1) Residual Connection Channels (Red, d): The channels across the shortcut connections (residuals) within the transformer blocks can be pruned to reduce dimensionality. These channels, denoted in red in the figure, are critical for maintaining information flow between layers, but may contain redundant channels that can be pruned without significantly affecting the model performance. Pruning these channels can help streamline the computational flow across transformer blocks.\n(2) Multi-Head Self-Attention (MHSA) Heads (Green, h): In the MHSA module, the input tokens are projected into queries, keys, and values, each of which is split across h attention heads. Each head independently computes self-attention, which is then concatenated and followed by a fully connected layer that restores"}, {"title": "the original dimension of d", "content": "The dimensions of the query, key, and value projections ($d_q$, $d_k$, and $d_v$), can be pruned to reduce the computational cost associated with the attention mechanism, as shown in green in the figure.\n(3) Feed-Forward Network (FFN) Hidden Dimensions (Blue, c): The FFN consists of two fully connected layers, where the hidden dimension c is typically set to four times the embedding dimension d, i.e., $c = 4d$. The first layer expands the input to the hidden dimension c, and the second layer reduces it back to d, ensuring the output dimension matches the input of the next block $R^{P \\times d}$. As the hidden dimension c contributes significantly to the overall computational cost of the FFN, it becomes a critical target for pruning, as indicated in blue in the figure. By reducing c, we can effectively decrease the complexity of the FFN while retaining its functionality.\nThese three components \u2014 residual connection channels (d), MHSA heads (h), and FFN hidden dimensions (c)\u2014are the primary targets for pruning in our approach. By pruning redundant parameters, we aim to reduce the model's computational cost without sacrificing its core functionality and accuracy."}, {"title": "B.2 Pruning Process", "content": "The pruning process is carried out in stages, with each stage focusing on one of the prunable components. As illustrated in Figure 2, the yellow sections represent the parameters being pruned at each stage, while the gray sections indicate parameters that were pruned in previous stages. The pruning factor s controls the degree of reduction in the parameters, and we calculate the importance of each channel or head using KL-Divergence.\nTo evaluate the importance of each component, we compute the KL-Divergence between the output distributions of the original model and the pruned model, as follows:\n$D_{KL}(P \\| Q) = \\sum P(i) \\log \\frac{P(i)}{Q(i)}$\nwhere P(i) represents the output distribution of the original model, and Q(i) represents the distribution after pruning. This divergence helps to identify which channels, heads, or neurons contribute the least to the model's overall performance, making them ideal candidates for pruning.\nStage 1: Pruning Residual Connections: In the first stage, we focus on pruning the channels in the residual connections (shown in red). Using KL-Divergence, we identify and prune the channels that contribute the least, reducing the dimensionality from d to s \u00d7 d. This helps to streamline the flow of information between layers without significantly affecting model performance.\nStage 2: Pruning Multi-Head Self-Attention (MHSA): In the second stage, instead of directly removing entire heads in the MHSA module, we prune the least important dimensions within the query, key, and value projections ($d_q$, $d_k$, and $d_v$) across multiple heads. Using KL-Divergence, we identify and remove the less significant dimensions within each head, and then merge the remaining important dimensions from different heads into new, consolidated heads. This process effectively reduces the total number of heads to s \u00d7 h, without entirely discarding any head, thus maintaining a balanced representation of the attention mechanism while reducing its complexity. The dimensionality of the projections is scaled"}]}