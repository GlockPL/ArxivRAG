{"title": "LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena", "authors": ["Stefan Pasch"], "abstract": "LLM safety and ethical alignment are widely discussed, but the impact of content moderation on user satisfaction remains underexplored. To address this, we analyze nearly 50,000 Chatbot Arena response- pairs using a novel fine-tuned ROBERTa model, that we trained on hand-labeled data to disentangle refusals due to ethical concerns from other refusals due to technical disabilities or lack of information. Our findings reveal a significant refusal penalty on content moderation, with users choosing ethical-based refusals roughly one-fourth as often as their preferred LLM response compared to standard responses. However, the context and phrasing play critical roles: refusals on highly sensitive prompts, such as illegal content, achieve higher win rates than less sensitive ethical concerns, and longer responses closely aligned with the prompt perform better. These results emphasize the need for nuanced moderation strategies that balance ethical safeguards with user satisfaction. Moreover, we find that the refusal penalty is notably lower in evaluations using the LLM-as-a-Judge method, highlighting discrepancies between user and automated assessments.", "sections": [{"title": "Introduction", "content": "With the rise of large language models (LLMs) such as ChatGPT, the topics of LLM alignment and safety have garnered significant attention across academia (Xie et al., 2024), industry (OpenAI, 2023, Antropic, 2023), and regulatory bodies (European Data Protection Board, 2024). LLMs, while demonstrating remarkable capabilities, face challenges due to their black-box nature, which makes controlling their outputs inherently difficult. Consequently, these models may generate illegal, unethical, or inappropriate content, raising concerns about their safe deployment (Weidinger et al., 2021).\nIn response, various methods have been developed to minimize harmful or sensitive outputs, including model guardrails (Dong et al., 2024) and detoxification techniques (Welbl et al., 2021). Other approaches include fine-tuning models with ethically curated datasets (Raza et al., 2024) and reinforcement learning from human feedback (RLHF) to encourage safer and more aligned responses (Ouyang et al., 2022). While these techniques address compliance, they often focus on preventing harmful outputs without considering how refusals to answer specific prompts influence user perceptions.\nUnderstanding user perceptions of content moderation due to ethical concerns is crucial because it directly impacts the adoption and acceptance of LLMs in real-world applications. While users generally agree on the need for ethical safeguards (Kieslich et al. 2021), overly restrictive or poorly phrased refusals may be perceived as unhelpful, undermining the user experience. This tension between safety and helpfulness represents a critical gap in existing research.\nTo address this gap, we leverage data from Chatbot Arena, a platform where users compare"}, {"title": "Related Work", "content": "Much of the existing work on content moderation focuses on classifying sensitive or toxic content, encompassing outputs flagged as harmful, inappropriate, or offensive. Efforts in this domain have been strengthened by open datasets such as Jigsaw's Toxic Comment Classification Challenge (Jigsaw, 2019) and RealToxicity Prompts (Gehman et al., 2020), which enable systematic evaluation of LLMs' ability to generate or resist toxic outputs. Tools like Detoxify (Hanu, 2020) provide classifiers to evaluate text on dimensions such as threats, obscenity, or insults,"}, {"title": "Response Refusals and User Satisfaction", "content": "Naturally, users prefer capable LLMs, so we should expect that refusals due to technical limitations are negatively associated with user satisfaction, as they signal a lack of capability, which can frustrate users seeking assistance (Kim et al., 2023). However, for refusals based on ethical concerns this association is less clear.\nOn the one hand, ethical refusals demonstrate adherence to societal norms, ensuring safety and alignment with widely accepted ethical guidelines (Kieslich et al., 2021). Similarly, refusals, when well-articulated, enhance trust by transparently communicating limitations or ethical concerns (Cheong, 2024). On the other hand, ethical refusals, may be perceived as overly restrictive or unhelpful, particularly if they fail to provide detailed explanations or alternatives (Weidinger et al., 2021). Such refusals may also invoke concerns around censorship or the suppression of uncomfortable content (Matusevych et al., 2024). Moreover, for users already skeptical of Al moderation, ethical refusals might reinforce mistrust in automated systems, especially if users perceive the model as overly biased, politically motivated, or lacking nuance (Duenser & Douglas, 2023).\nThe effect of user refusals may also depend on the phrasing of refusals. Overly generic or abrupt refusals can feel dismissive or unhelpful, eroding trust and satisfaction. For example, research in conversational AI (Denny et al. 2021) highlights that users prefer responses that offer detailed explanations or alternative suggestions, even when refusing a prompt. Additionally, studies in human- computer interaction suggest that users value transparency and effort in Al communication (Cheong 2024). Hence, ethical refusals that clearly articulate the reasoning behind the refusal, provide constructive alternatives, or acknowledge user concerns should be perceived as more trustworthy and aligned with user expectations. In contrast, vague or formulaic refusals may frustrate users, particularly when they feel their needs are dismissed without sufficient justification."}, {"title": "Response Refusals and LLM-as-a-Judge", "content": "Similarly to user satisfaction, it is also not clear how refusals affect judgments by LLM-as-a-Judge, a common framework for automated evaluation (Zheng et al., 2023). Recent findings reveal that the LLM-as-a-Judge method exhibits several biases in its evaluation process, some of which may specifically affect its assessment of refusals (Ye et al., 2024): (i) Preference for content without emotional elements: LLM judges tend to favor responses that lack emotional elements or subjective framing. This suggests that ethical and judgment refusals, which aim to maintain neutrality and avoid subjectivity, may align well with LLM-as-Judge preferences. (ii) Preference for longer responses: LLM judges favor longer responses, even when these are less clear, accurate, or high-quality compared to shorter alternatives. This bias implies that short refusals, particularly"}, {"title": "Methodology", "content": "To investigate the impact of content moderation by large language models on user satisfaction, we leverage data from Chatbot Arena, a widely used benchmarking platform for conversational AI models (Chiang et al., 2024). The dataset consists of 57,477 conversation pairs, with each pair comprising responses from two distinct models to a shared user prompt. Users are tasked with selecting their preferred response or declaring a tie if neither model is favored. This user feedback serves as the indicator of user satisfaction.\nA significant majority (approximately 86%) of all interactions in the dataset are single-turn conversations, meaning they consist of a single user prompt followed by two model responses. These one-turn interactions offer a clear, isolated view of user preferences, minimizing the potential influence of follow-up clarifications or context from the user. To maintain the interpretability of user decisions, we exclude multi-turn conversations (those involving multiple user- model interactions) from our analysis. This choice ensures that user preferences are directly linked to the initial model responses rather than follow-up exchanges, which could introduce complexities like jailbreaking attempts or context refinement. After applying this filtering criterion, our final dataset consists of 49,938 conversation pairs, each with a single user prompt and two corresponding model responses."}, {"title": "Labeling", "content": "To the best of our knowledge, no existing classifier distinguishes between the different types of refusals exhibited by large language models (LLMs). While frameworks like Sorry-Bench (Xie et al. 2024) provide valuable insights into LLM refusal behavior, they primarily focus on pre- defined questions designed to trigger ethical concerns. This allows for a targeted evaluation of how well models adhere to ethical guidelines. However, it does not capture refusals that arise organically from general, user-generated prompts, nor does it distinguish between refusals driven by ethical concerns from a general lack of capability.\nTo address this gap, we created a novel labeled dataset through a systematic and rigorous hand-labeling process. The use of hand-labeling for complex annotation tasks is well-supported in the NLP literature, particularly when the categories to be labeled are nuanced and context-dependent. Similar approaches have been used in tasks such as opinion and emotion classification (Wiebe et al., 2005), sentiment detection (Mohammad et al., 2016), or culture classification (Koch & Pasch, 2023) where clear distinctions between classes require human expertise.\nGiven that most responses in the Chatbot Arena dataset are standard and do not involve refusals, we applied a pre-filtering strategy to focus on responses where refusals were more likely to occur. Specifically, we searched for key phrases in model responses that commonly signal a refusal or disclaimer, such as \u201cI'm sorry,\u201d \u201cI cannot,\u201d or \u201cas an Al model\". This process identified roughly 6,700 conversation pairs in which at least one of the two model responses contained a refusal-indicating phrase. Importantly, for each of these flagged pairs, we labeled the responses from both models, even if only one of the models contained the key phrase. Similar pre-filtering strategies have been successfully used in offensive language detection"}, {"title": "Training Transformer Model for LLM- Refusal", "content": "A common approach for training models for domain- or task-specific text classification is to fine-tune transformer-based models, such as BERT or RoBERTa. These models are pre-trained on large corpora of general text but fine-tuning them on a specific task allows them to better capture the nuances and context of the target domain. Fine- tuning requires training the model on task-specific labeled data while using transfer learning to retain the general language understanding from the pre- trained weights. This approach is widely used for classification tasks in NLP and, such as sentiment analysis (Gonz\u00e1lez-Carvajal & Garrido-Merch\u00e1n, 2020), culture classification (Koch & Pasch 2023), or sustainability analyses in finance (Pasch & Ehnes 2022).\nTo train a model for our LLM-Refusal classification task, we used the labeled dataset of 1,750 questions and 3,500 model responses, split into training, validation, and test sets following a 70-10-20 split, a widely adopted practice in supervised machine learning. The split was conducted at the prompt/question level, ensuring that both responses from the two models corresponding to the same prompt were allocated to the same set.\nFor the classification task, we combined the question and model response into a single input sequence, separated by distinct markers. To accommodate the 512-token limit imposed by BERT and ROBERTa, we truncated the question to the first 200 characters, ensuring that a sufficient portion of the response was always included in the input. This approach maintains the essential context of the prompt while prioritizing the model's ability to process the full response.\nTo ensure consistent and reliable performance, we adopted a standardized training configuration for the transformer-based model. Our approach followed established best practices for large-scale transformer training, focusing on stability, convergence, and handling class imbalance (Sun et al., 2019; Pasch & Cutura, 2024). The model was trained for 12 epochs with a learning rate of le-5, a batch size of 8, and weight decay of 0.01 to prevent overfitting. Training was further stabilized using the AdamW optimizer, known for its ability to combine adaptive learning rates with weight decay (Zheng et al., 2020).\nTo ensure smooth convergence, we employed a cosine learning rate scheduler with a 10% warmup phase, where the learning rate increased linearly during the initial training steps before decaying according to a cosine function. This scheduling strategy promotes stable learning in early stages and smooth convergence thereafter. (H\u00e4gele et al. 2024)\nModel evaluation was conducted at regular intervals, with validation every 3 epochs. The best checkpoint was selected based on the highest weighted F1 score, ensuring an optimal balance between precision and recall."}, {"title": "LLM-as-a-Judge", "content": "In addition to evaluating user response preferences from the Chatbot Arena, we employ the LLM-as-a- Judge method (Zheng et al., 2023) to assess model performance.\nFor this analysis, we adopt the pairwise comparison method, where the LLM is presented with the user prompt along with the two model responses and tasked with selecting the better response or declaring a tie if neither response is superior. To ensure methodological consistency, we use the pairwise comparison prompt introduced by Zheng et al. (2023). Based on recent findings, we employ GPT-40 as the evaluation model, as it has been shown to exhibit high alignment with user preferences in LLM-based comparisons (Raju et al., 2024)."}, {"title": "Results", "content": "When examining user decisions, we observe a clear refusal penalty. Normal responses, which neither refuse nor disclaim, achieve a win rate of 36%, with the remaining outcomes split between losses (33%) and ties (31%). In contrast, responses involving refusals perform significantly worse, indicating a strong user preference against refusal behaviors.\nAmong refusal categories, Refusal Unethical responses exhibit the lowest win rates for user decisions. These responses achieve a win rate of only 8%, with a win/loss ratio of 0.16, highlighting the strong aversion to refusals for ethical reasons. Refusal Capability responses fare better, with win rates of 16%, but still fall well below the performance of Normal responses."}, {"title": "Response Refusal for Ethical Reasons and Win Rates", "content": "Given the low win rates for responses that refuse to answer prompts due to ethical reasons, we further investigate factors influencing these"}, {"title": "Discussion", "content": "Our findings reveal a clear refusal penalty in user decisions, where responses that refuse to answer prompts - particularly for ethical reasons - are rated significantly lower than responses that attempt to engage with the prompt. While refusals"}, {"title": "Implications for Benchmarks and Evaluation Frameworks", "content": "Chatbot Arena, as a widely used benchmarking platform, offers valuable insights into model performance and user preferences. However, while the platform claims to use the OpenAI Moderation API to flag sensitive content, our analysis suggests that a significant proportion of prompts that invoke ethical refusals are not flagged by this system. For example, many refusals occur for content like lighthearted jokes or mildly inappropriate queries, which fall outside the moderation API's current detection capabilities. This discrepancy implies that Chatbot Arena benchmarks might favor models with less stringent moderation policies, as they would generate fewer refusals for similar prompts. Whether this bias toward reduced moderation is desirable depends on the intended use case of the evaluated models - whether prioritizing user satisfaction or ethical safeguards.\nFurthermore, our results highlight the differences between user decisions and LLM-as-a- Judge evaluations. LLM-as-a-Judge exhibits a stronger preference for refusals, particularly for flagged prompts, reflecting its inherent alignment with training objectives that emphasize adherence to moderation policies. For instance, while users penalize refusals for sexual content heavily, LLM- as-a-Judge rates such refusals favorably. This discrepancy adds to the growing discussion on the biases of automated evaluators and their divergence from human-centric metrics. As with Chatbot Arena benchmarks, it is unclear whether favoring alignment-compliant responses is inherently good or bad, but these findings call for careful interpretation when relying on automated evaluation frameworks to assess model performance especially related to sensitive content."}, {"title": "Importance of Refusal Phrasing and Moderation Calibration", "content": "Finally, the phrasing of refusals plays a pivotal role in shaping user perceptions. Generic refusals are more likely to be penalized than refusals that provide detailed explanations, acknowledge user intent, or offer constructive alternatives. This suggests that refining refusal phrasing is a critical area for improving user satisfaction while maintaining alignment.\nMoreover, our findings highlight the need for careful calibration of content moderation. The discrepancies between flagged and unflagged content, as well as varying win rates across different content categories, suggest room for improvement in defining which content should invoke an ethical refusal. Developing moderation systems that better capture nuanced or borderline cases - such as lighthearted but potentially sensitive queries - can help align model behavior with both user expectations and safety standards."}, {"title": "Limitations", "content": "This study has several limitations that warrant consideration. First, the categorization of refusals into broad categories - unethical and capabilities - may oversimplify the complex motivations behind model refusals. More refined subcategories could provide deeper insights into user perceptions and model behaviors."}]}