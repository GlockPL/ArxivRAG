{"title": "CTRLSYNTH: CONTROLLABLE IMAGE TEXT SYNTHESIS FOR DATA-EFFICIENT MULTIMODAL LEARNING", "authors": ["Qingqing Cao", "Mahyar Najibi", "Sachin Mehta"], "abstract": "Pretraining robust vision or multimodal foundation models (e.g., CLIP) relies on large-scale datasets that may be noisy, potentially misaligned, and have long-tail distributions. Previous works have shown promising results in augmenting datasets by generating synthetic samples. However, they only support domain-specific ad hoc use cases (e.g., either image or text only, but not both), and are limited in data diversity due to a lack of fine-grained control over the synthesis process. In this paper, we design a controllable image-text synthesis pipeline, CtrlSynth, for data-efficient and robust multimodal learning. The key idea is to decompose the visual semantics of an image into basic elements, apply user-specified control policies (e.g., remove, add, or replace operations), and recompose them to synthesize images or texts. The decompose and recompose feature in CtrlSynth allows users to control data synthesis in a fine-grained manner by defining customized control policies to manipulate the basic elements. CtrlSynth leverages the capabilities of pretrained foundation models such as large language models or diffusion models to reason and recompose basic elements such that synthetic samples are natural and composed in diverse ways. CtrlSynth is a closed-loop, training-free, and modular framework, making it easy to support different pretrained models. With extensive experiments on 31 datasets spanning different vision and vision-language tasks, we show that CtrlSynth substantially improves zero-shot classification, image-text retrieval, and compositional reasoning performance of CLIP models.", "sections": [{"title": "INTRODUCTION", "content": "High-quality large-scale datasets have driven the success of large foundational AI models (Radford et al., 2021; Rombach et al., 2022; Touvron et al., 2023). Collecting and annotating datasets at large-scale is challenging and costly. One solution is to crawl data from the web; however, web data is noisy (Lai et al., 2024; Kang et al., 2023), has long-tail distributions (Udandarao et al., 2024), and often causes privacy or copyright issues (Schuhmann et al., 2022). Synthetic data presents a viable and complementary alternative to overcome these challenges, as it allows for precise control over data generation and customization to meet specific requirements. A large body of work has focused on improving the quality of synthetic data for image and text data, from the generation of high-quality images (Dunlap et al., 2023; Islam et al., 2024) to the improvement of synthetic captions (Lai et al., 2024; Fan et al., 2023). While these works have shown that synthetic data successfully improves model performance for various vision or vision-language tasks, their synthetic pipeline is often ad hoc and tailored to specific purposes such as training better CLIP models or improving domain-specific vision models (e.g., DiffuseMix uses diffusion models to augment images and improves accuracy on image classification tasks Islam et al., 2024). These data synthesis works also lack explicit fine-grained control over the generated texts or images, which are important for tasks with long-tail distribution (e.g., augmenting tail class samples) or enforcing safety requirements (e.g., mitigating biased or sensitive content generation Schramowski et al., 2023).\nIn this work, we aim to systematically control the synthetic pipeline for generating image-text data while accommodating different use cases (e.g., improving long-tail task performance, enhancing compositional reasoning of CLIP models, etc.). Our intuition is that large foundation models are already pretrained on a wide range of data and contain general knowledge about concepts, objects, and"}, {"title": "RELATED WORK", "content": "Data-Efficient Vision-Language Representation Learning. Contrastive Language-Image Pretraining (CLIP) (Radford et al., 2021) has popularized visual representation learning from image-text pairs due to its strong zero-shot transfer capabilities. Many recent works have focused on improving the data efficiency of training CLIP models. SLIP (Mu et al., 2022) brings self-supervised learning into a multitask learning framework to improve CLIP performance. FLIP (Li et al., 2023c) masks out image patches during CLIP training, improving training efficiency and zero-shot accuracy over baselines. CLIPA (Li et al., 2023b;a) further improves over FLIP ideas and reduces the number of image text tokens by block and syntax masking for CLIP training and it significantly reduces the training costs of CLIP models. LiT (Zhai et al., 2022) freezes the image encoder in CLIP models and achieves\nImage-text Data Augmentation. Much recent work aims to improve the caption quality of image-text pairs. For example, VeCLIP (Lai et al., 2024), LaCLIP (Fan et al., 2023), and ReCap (Li et al., 2024) leverage LLMs to synthesize new captions that are more informative and contain rich descriptions about the image. The key difference of CtrlSynth is that we provide more diverse and high-quality captions that outperform prior works (we will show in Table 5 and Table 6). This is because CtrlSynth breaks down the image semantics to allow more fine-grained control and recombination using LLM. Another line of work uses text-to-image models like diffusion models to generate synthetic images and augment downstream vision tasks. ALIA (Dunlap et al., 2023) uses language to guide the image editing process and provides domain-specific diversity to augment the image samples. DiffuseMix (Islam et al., 2024) augments image samples using diffusion models to blend original and generated images. EDA (Trabucco et al., 2023) generates variations of real images using diffusion models to maintain the semantics while augmenting image samples. These semantic image augmentation methods provide strong performance improvements on various vision datasets. Our CtrlSynth instead unifies the image and text synthesis via a closed-loop pipeline, it provides more flexibility and diverse synthetic samples while allowing more fine-grained control over the sample generation process."}, {"title": "CTRLSYNTH", "content": "CtrlSynth leverages semantic knowledge and reasoning skills of pretrained foundation models (e.g., large language and diffusion models) to generate diverse synthetic data samples in a controlled manner. Specifically, CtrlSynth consists of three foundation models: (1) a vision tagging model, (2) a large language model, and (3) a text-to-image model; plus the two text and image controllers. For a given real (1a in Figure 1) or synthetic (1c) input image, a vision tagging model (2a) extracts visual tags (e.g., objects, attributes, and their relationships) (le). These tags describe the image's visual concepts and semantic contexts. The text controller (3a) takes the image tags and user-defined control policies as inputs and generates instructions for synthesizing new text. An example control policy is to edit the tags or optionally add the text (1b) associated with the image. A large language model ( 2b) then follows the instructions and generates the synthetic text (1d). The image controller (3b) operates on the given input text and applies user-defined image control policies to output instructions for image synthesis. An example policy is to specify the style for generating artistic, cinematic, or realistic images. A text-to-image model (2c) takes an image synthesis instruction provided by the image controller as an input and produces a synthetic image as an output (1c).\nKEY COMPONENTS\nVision Tagging Model. The goal of a vision tagging model (VTM) is to extract the basic visual elements (or tags) of an image, including all objects or entities, attributes (e.g., color, shape, and size), and visual relations (e.g., interaction between objects).\nVTM, as a key component in CtrlSynth, can be a combination of an advanced captioning model (Xiao et al., 2024) that generates comprehensive image descriptions and an LLM that extracts the visual tags from the captions to decompose the visual semantics of an image into a set of fine-grained visual concepts. Appendix A.4 includes more details about this hybrid VTM. These fine-grained visual"}, {"title": "IMAGE TEXT SYNTHESIS IN CTRLSYNTH", "content": "CtrlSynth is a modular and closed-loop system by design and supports diverse image and text synthesis configurations. In this section, we first introduce different synthesis paths in CtrlSynth and then describe how the closed-loop feature allows CtrlSynth to filter out low-quality samples.\nFlexible and diverse synthesis paths. A data synthesis path (SP) starts and ends with a data node (rounded box in Figure 1). We define the following synthesis paths:\nSP(1): 1a \u2192 2a \u2192 1e \u2192 3a \u2192 2b \u2192 1d. This path (Figure 4a) means CtrlSynth generates a new text that describes the original image. The synthetic text 1d may not align with the semantics in the original image since the LLM can create new combinations of the visual tags and add information that does not exist in the image. Such new information provides useful semantic augmentation over the original image while containing similar visual concepts.\nSP(2): 1a \u2192 2a \u2192 1e \u2192 3a \u2192 2b \u2192 1d. This path (Figure 4b) is similar to the previous path but a key difference is that it constrains the synthetic text to be faithful to an original text. We can consider it as using the VTM and LLM to synthesize an improved text over the original one. We will show later in Section 4.5 that text samples generated from this path outperform previous works (Lai et al., 2024; Fan et al., 2023) that rewrite noisy captions. We include the example prompts to reflect the control policies in Appendix A.1.\nSP(3): 1a \u2192 2a \u2192 1e \u2192 3a \u2192 2b \u2192 1d \u2192 3b \u2192 2c \u2192 1c. This path (Figure 4c) provides both synthetic text (1d) and image (1c) samples. 1c can be an effective image sample that augments the original image (1a) or can be paired with (1d) to augment the original image-text pair (1a and 1b).\nSP(4): 1b \u2192 3b \u2192 2c \u2192 1c. This path (Figure 4d) bypasses the language model and the original text is directly fed to the image controller and then generates a synthetic image (1c). The image sample could be a strong augmentation sample to the original image if the original text has a comprehensive and high-quality description.\nNote that CtrlSynth supports more synthesis paths that are not listed above. For example, one can start with original text and use LLM to add creative elements and generate synthetic text and further use it to generate an image, i.e. 1b \u2192 3a \u2192 2b \u2192 1d \u2192 3b \u2192 2c \u2192 1c. Another category of examples includes starting with synthetic texts or images and creating more synthetic samples.\nSelf-filtering for better synthetic data. Synthetic samples often suffer from degraded quality especially when running at large scale. Synthetic systems often rely on heuristics or rule-based filtering techniques to filter out bad-quality samples. Because CtrlSynth pipeline is closed-loop, it implicitly provides self-filtering functionality. To check the quality of the synthetic text, we can detect if the synthetic text (1d) contains the visual tags (le), to filter out potentially misaligned or lower quality synthetic text samples, we define that at least some ratio pf of the visual tags exist. For the synthetic image, we run it through the VTM again and output the visual tags, then we do the same check against the starting node text (1b or 1d). Later in Section 4.4, we will show that self-filtering improves the synthetic samples."}, {"title": "EXPERIMENTS", "content": "SETUP\nTasks and Datasets. We adopt the CLIP (Radford et al., 2021) model architecture for multimodal representation learning. For pretraining CLIP models, we use two public image-text datasets: CC3M (Sharma et al., 2018) and CC12M (Changpinyo et al., 2021). To evaluate the representation quality of pretrained CLIP models, we measure the zero-shot performance on classification, retrieval, and compositional reasoning tasks. For image classification, we use 25 common vision datasets, including five ImageNet (Deng et al., 2009; Recht et al., 2019) variants and the tasks from the VTAB benchmark (Zhai et al., 2020). We list the detailed dataset information in Appendix A.2. We use COCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015) for image-to-text and text-to-image retrieval tasks and report the metrics in recall@1. SugarCrepe (Hsieh et al., 2023) is a recent benchmark that measures the compositional understanding of vision-language models, we report the zero-shot accuracy numbers. Additionally, to study the effects of CtrlSynth on long-tail tasks, we evaluate the task accuracy of Places-LT and ImageNet-LT datasets (Liu et al., 2019) by augmenting the tail classes with CtrlSynth synthetic data.\nTraining and Baselines. Note that CtrlSynth itself does not require any training. We conduct pretraining experiments on CLIP models to evaluate the quality of synthetic data. We use ViT-B/16 (Dosovitskiy et al., 2020) architecture for the CLIP vision backbone. For a fair comparison, we train all models for the same number of iterations on the original dataset (baseline) and the dataset mixed with CtrlSynth augmented samples. We use CtrlSynth-cap to denote the original image and synthetic text pair (1a, 1d) from synthesis path SP(1). CtrlSynth-img stands for the synthetic image and original text pair (16, 1c) from synthesis path SP(4). CtrlSynth-capimg means the synthetic image and text pair (1d, 1c) from synthesis path SP(3). We define CtrlSynth-mix as taking one image-text pair from CtrlSynth-cap and another from CtrlSynth-capimg. We do not take CtrlSynth-img image-text pairs since we found the original texts are noisy and thus a substantial portion of synthetic images are bad quality. We refer CtrlSynth-mix as the default setting unless otherwise stated. We list detailed information in Appendix A.3.\nCtrlSynth Models. For the VTM, we adopt a hybrid approach by default, we combine the tags from a captioning plus tag extraction pipeline and an advanced multi-label image classifier. We use a recent vision foundation model called Florence-large (Xiao et al., 2024) to generate detailed image descriptions and then extract the objects, attributes, and relations using the Qwen2-7B-Instruct (Yang et al., 2024a) LLM. Then we use an accurate image classifier, the huge variant of CatLIP (Mehta et al., 2024b), to output multiple high-confidence objects and attributes. We show later in Section 4.5 that this hybrid VTM provides the best visual tags compared with using individual approach alone. For the LLM, we use Mistral-NeMo-instruct model (AI, 2024) by default due to its strong instruction-following capability. We choose the stable-diffusion-xl-base-1.0 (Podell et al., 2024) for the text-to-image model by default. We describe the detailed setup in Appendix A.4. In Section 4.5, we study different pretrained models for each of the three modules in CtrlSynth."}, {"title": "MAIN RESULTS", "content": "Image Classification Evaluation. We conduct the zero-shot evaluation for image classification tasks. Table 1 shows the results across 20 commonly used vision datasets and Table 2 shows the results of 6 ImageNet-related datasets. Notably, CtrlSynth outperforms the baseline consistently by 2.5% to 9.4% for the CLIP models trained on the CC3M and CC12M datasets. We observe that CtrlSynth significantly improves the zero-shot performance (by over 7.7%) by augmenting smaller datasets like CC3M, while the performance gains become smaller on larger datasets like CC12M.\nImage-Text Retrieval Evaluation. We evaluate the zero-shot image-text retrieval performance for our CtrlSynth and baseline CLIP models and present the recall@1 results in Table 3. CtrlSynth substantially improves the text-to-image and image-to-text retrieval recall by up to 24% and 36% for the Flickr dataset, and overall improves recall by 23.4% on average for CC3M models. CtrlSynth also brings over 9% retrieval gains for CC12M models on average. The improvements show that data samples from CtrlSynth have better coverage of visual concepts.\nCompositional Reasoning Results. A key strength in CtrlSynth is the inclusion of visual tags that contain objects, attributes and relations from an image. To understand how the fine-grained visual"}, {"title": "PERFORMANCE ON LONG-TAIL TASKS.", "content": "Real-world data often have long-tail distributions. Much recent research (Shi et al., 2024; Liu et al., 2019) has focused on developing new learning methods for long-tail recognition tasks. Data"}, {"title": "ANALYSIS", "content": "Data-Efficiency of CtrlSynth in Training CLIP.\nTo study the data efficiency of CtrlSynth samples, we plot the top1 zero-shot accuracy of the ImageNet validation set in Section 4.4 for the baseline and CtrlSynth CLIP models trained on CC3M. CtrlSynth reaches the 20% accuracy with 40% fewer iterations than the baseline, indicating that using CtrlSynth samples is more data-efficient.\nStatistics and visualization of CtrlSynth Samples.\nIn this section, we provide the statistics for the synthetic samples from CtrlSynth. Figure 6 shows examples of CtrlSynth images and texts compared with the original real samples. We observe that the text samples from CtrlSynth are usually longer and contain"}, {"title": "ABLATION STUDY", "content": "In this section, we evaluate the effectiveness of visual tags, the impact of using different pretrained models in the CtrlSynth pipeline, and mixing and filtering effects for CtrlSynth samples. We use the same text and image control policy described in Section 3.2 for all settings. We experiment with CC3M dataset for CLIP pretraining and report the accuracy on the SugarCrepe benchmark, zero-shot accuracy of common downstream vision tasks (same tasks in Table 1), and top1 accuracy on the ImageNet 1k validation set.\nDifferent Pretrained Models. We choose an alternate LLM and a different text-to-image model to understand how different pretrained models affect the quality of synthetic samples. CtrlSynth"}, {"title": "CONCLUSION", "content": "Synthetic data emerges as a viable solution to address challenges in curating high-quality samples from noisy, misaligned, and long-tail web data. However, existing data synthesis pipelines are rigid and the generation process is hard to control and thus being tailored for ad hoc use cases. We develop CtrlSynth, a new image-text synthesis pipeline that allows users to control the data generation in a fine-grained way. CtrlSynth decomposes the semantics of images and texts into basic elements and uses pretrained foundation models to recompose them based on specified control policies. This way, CtrlSynth provides flexible and diverse image-text samples. Synthetic samples from CtrlSynth improve the long-tail task performance by a large margin. They also significantly boost the zero-shot and compositional capability of CLIP models and enable data-efficient multimodal learning."}, {"title": "APPENDIX", "content": "CONTROL POLICIES\nText Prompt Templates. We provide example control policies for text synthesis as predefined prompt templates, the first five templates do not include original text:\nCreate a detailed and high-quality caption using phrases that represent the entities or objects, their unique attributes, and the visual relationships in the scene depicted. Phrases: {phrases}.\nCompose a rich and immersive caption by incorporating a set of phrases that illustrate the entities or objects, their defining attributes, and the interconnections presented within the image. Phrases: {phrases}.\nFormulate an articulate and informative caption by using a series of phrases that outline the entities, their attributes, and their visual relationships depicted in an image. Phrases: {phrases}.\nUsing a set of phrases that highlight the entities, attributes, and their visual associations in an image, craft a detailed and expressive caption. Phrases: {phrases}.\nConstruct a comprehensive and expressive caption by integrating phrases that detail the entities, their features, and the spatial or thematic relationships in an image. Phrases: {phrases}.\nThe following five templates include the original text, which is useful for maintaining the original meaning:\nCreate a comprehensive caption that faithfully represents the objects, attributes, and their relationships contained within the provided sentence and phrases. Given sentence: {caption}. Given phrases: {phrases}. If the original caption specifies particular give phrases, maintain their integrity while using the phrases to enhance the description.\nWrite a faithful caption by integrating the given phrases with the original sentence. Given sentence: {caption}. Given phrases: {phrases}. Ensure any objects or specific nouns from the original caption are preserved while elaborating on the visual relationships and attributes provided in the phrases to create a more detailed depiction.\nProvide a faithful and informative image caption using a given sentence and few phrases. Sentence: {caption}, phrases: {phrases}. Consider the initial sentence as a base for the overall context and ensure that specific objects or nouns such as numbers, car models, animals, etc., are preserved in the new caption. Integrate the given phrases, which describe entities, attributes, or visual relationships, to enrich and elaborate on the original meaning. Maintain fidelity to the original content while enhancing descriptive quality.\nMake a detailed caption based on the given phrases and a given sentence. Given phrases: {phrases}. Given sentence: {caption}. The sentence serves as a foundation, while the phrases elaborate on elements depicted in the image, like objects, their characteristics, and interactions. Preserve any pivotal information concerning objects, attributes, and their relations present in the sentence.\nWrite a new faithful and high-quality caption based on the given phrases and a given sentence. The given sentence is the original caption and the phrases are entities or objects, attributes, and their visual relationships in an image. Given sentence: {caption}. Given phrases: {phrases}. If the sentence contains objects or nouns (e.g. digits, car models, planes, pets, animals, etc.), the new caption should be faithful and keep this information. Otherwise, use the phrases to create the new caption.\nImage Prompt Templates. We provide five image prompt templates:\nreal: \"a real photo. {prompt}. 35mm photograph, film, bokeh, professional, 4k, highly detailed\",\nnocap: \"a real photo showing {prompt}. highly detailed\""}, {"title": "DATASETS DETAILS", "content": "Evaluation Datasets. We list the vision datasets for evaluation in Table 9."}, {"title": "TRAINING DETAILS", "content": "Pretraining Hyper-parameters. We pretrain the CLIP for the same number of iterations for both the baseline and CtrlSynth. For example, suppose we train for E epochs, if the original dataset has N samples, CtrlSynth has generated N' samples (N' <= N due to filtering), then the total samples are , we train CtrlSynth models for epochs. This guarantees that the baseline and CtrlSynth CLIP models have seen the same number of data samples.\nTable 10 lists the hyper-parameters used for pretraining on CC3M and CC12m. We use AdamW (Loshchilov & Hutter, 2018) with default $\u00df$ values as an optimizer and binary cross-entropy loss as an objective function. We use cosine learning rate schedule (Loshchilov & Hutter, 2022). We use the CoreNet library (Mehta et al., 2024a; 2022) for all pretraining experiments. We adapt the LIFT codebase (Shi et al., 2024) for fine-tuning long-tail tasks, main modifications include adding support for iteration-based training and data loader for multiple datasets."}, {"title": "CTRLSYNTH INFERENCE DETAILS", "content": "VTM. We use a hybrid tagging model consisting of two stages. We first run the ViT-Huge variant of CatLIP (Mehta et al., 2024b) for each image and output top20 classes based on the sigmoid score of prediction logits, then we convert the class indices to actual word labels. The vocabulary size of CatLIP is 24320. Most of the vocabulary words are nouns and single-word attributes. We then run the Florence-large (Xiao et al., 2024) for each image to extract detailed captions using the task prompt <MORE_DETAILED_CAPTION>. After that, we run Qwen2-7B-Instruct (Yang et al., 2024a) to extract objects, attributes, and relations from the Florence captions. We then merge the objects field with CatLIP-predicted labels. The extraction instruction contains a 2-shot example and we list the prompt template below:\nFor a given image caption, identify all the attributes, objects or entities, and visual relationships or actions that are phrases. The phrases should only come from the caption. Separate the phrases by comma without formatting. Output three lines: attributes: phrases\nobjects: phrases\nrelations: phrases\nExamples:\ncaption: The image is a close-up portrait of a middle-aged man wearing a white cowboy hat. He appears to be in his late 60s or early 70s, with gray hair and a serious expression on his face. He is wearing a dark suit jacket and a light blue collared shirt. The background is a clear blue sky with trees visible in the distance. The man is looking off to the side with a slight smile on his lips.\nattributes: close-up, middle-aged, white cowboy hat, gray hair, serious expression, light blue\nobjects: portrait, man, hat, face, dark suit jacket, shirt, blue sky, trees, lips\nrelations: wearing a, visible in the distance, looking off to the side, slight smile on his lips\ncaption: The image shows a female singer performing on a stage. She is standing on a set of stairs with her legs spread apart and holding a microphone in her hand. The stage is lit up with red and blue lights and there is a large circular screen in the background. The singer is wearing a black and white patterned outfit with high heels. She appears to be in the middle of a song or performance.\nattributes: female singer, stage, set of stairs, red and blue lights, large circular screen, black and white patterned outfit, high heels\nobjects: female singer, stage, set of stairs, legs, microphone, screen, outfit, high heels, song, performance\nrelations: performing on a stage, standing on, her legs spread apart, holding, lit up, background, wearing, in the middle of a song\ncaption: {caption}"}, {"title": "MORE ANALYSIS DETAILS", "content": "CtrlSynth Samples. For CC3M, the original dataset has 2.8 million image-caption pairs, CtrlSynth-cap contains 2.6 million captions, CtrlSynth-img contains 2.4 million images, and CtrlSynth-mix contains 5.1 million image-caption pairs. Original CC12M has 11.3 million image-caption samples, CtrlSynth-cap consists of 10.2 million captions, CtrlSynth-img contains 9.5 million images, and CtrlSynth-mix has 19.7 million image-caption pairs.\nCtrlSynth Synthetic Texts. We plot the number of words for synthetic texts generated by CtrlSynth and compare them with original real texts in Figure 8."}]}