{"title": "HUMOS: Human Motion Model Conditioned on Body Shape", "authors": ["Shashank Tripathi", "Omid Taheri", "Christoph Lassner", "Michael Black", "Daniel Holden", "Carsten Stoll"], "abstract": "Generating realistic human motion is crucial for many computer vision and graphics applications. The rich diversity of human body shapes and sizes significantly influences how people move. However, existing motion models typically overlook these differences, using a normalized, average body instead. This results in a homogenization of motion across human bodies, with motions not aligning with their physical attributes, thus limiting diversity. To address this, we propose a novel approach to learn a generative motion model conditioned on body shape. We demonstrate that it is possible to learn such a model from unpaired training data using cycle consistency, intuitive physics, and stability constraints that model the correlation between identity and movement. The", "sections": [{"title": "1 Introduction", "content": "Modeling virtual humans that move and interact realistically with 3D environments is extremely important for interactive entertainment, AR/VR and simulation technology, with numerous applications in crowd simulation, gaming and robotics. There has been rapid progress in training models that generate human motion either unconditionally or conditioned on text or previous motions. Existing state-of-the-art human motion models [18,55,56,69] are trained on datasets like AMASS [49], but they typically ignore body shape and proportions. However, variations in muscle mass distribution and body proportions contribute to a person's distinct movement patterns. People with different body types will generally move differently when prompted to perform the same motion. We argue that to achieve physical realism and motion diversity, it is critical to condition generated human motions on body shape.\nTo address this problem we adopt a novel approach called HUMOS, that enhances traditional data-driven motion generation methods and uses a transformer-based conditional Variational Auto-Encoder (c-VAE) trained to generate human motion conditioned on identity features such as a subject's body shape and sex. We take inspiration from a recent 3D human pose and shape estimation method, IPMAN [70], to propose new dynamic intuitive physics (IP) terms that are simple, fully differentiable, and compatible with parametric body models like SMPL [47]. Since IPMAN's IP terms only apply to static 3D poses, they are not suited for dynamic human motion modeling. We go beyond this by proposing general IP terms that are effective for dynamic human motions involving a sequence of poses. We show that these dynamic IP terms are critical to effectively train our model without paired data of differently-shaped people performing the same action.\nSpecifically, we propose differentiable physics terms that improve the realism of generated motions by addressing common issues like foot sliding, ground penetration, and unrealistic floating effects. Our key contribution here is a dynamic stability term, that models the interaction between a body's Center of Mass (COM), Center of Pressure (CoP), and the Zero Moment Point (ZMP). Dynamic stability is a biomechanical concept, frequently employed for ensuring balance in humanoid robots [38], but has also been shown to hold true for human gait [57]. This approach ensures our generated motions are not only visually convincing but also more closely adhere to principles of biomechanics, making them suitable for a wide range of applications in realistic motion generation.\nOne of the key applications our model enables is retargeting of animation between characters with different identities. Existing methods typically generate human motions for a canonical body and then use a second character retargeting step to transfer the generated motions to the target body. Since classical"}, {"title": "2 Related Work", "content": "We categorize related work into several broad areas: previous works which transfer motion between different skeleton proportions or topologies, works which use physics simulation as a prior to constrain human motion generation, and works which synthesize or generate motion, often conditioned on various different desired parameters.\nMotion Transfer: Most industry methods of transferring motion between two characters assume that characters have either the same skeletal topology (but may differ in bone lengths) or a manual mapping between the two is provided. Motion is then largely transferred directly, without taking into account the further shape or identity difference between the characters. Simple heuristics such as inverse kinematics are used to remove any artifacts [20, 51, 62]. There have been several attempts to apply human motion data to entirely different characters or creatures [1, 76, 85], yet they tend to require paired data to function effectively which can be difficult to obtain at a large scale. Recent methods using Deep Learning have been developed which can transfer motion between different topologies [2, 43] without paired data, or even between entirely new mesh shapes [17]. Similarly, techniques have been developed which can re-target motion from other sources or data representations such as 2D videos [3], or can take into account physical constraints such as floor contacts [13,72]. However, these"}, {"title": "3 Method", "content": "Our goal is to learn an identity-conditioned motion model capable of generating 1) realistic, 2) physically-plausible and 3) dynamically-stable human motions. Specifically, we represent a 3D human motion as a sequence of poses $P_{1:T} = P_1,..., P_T$, where T denotes the number of frames. We follow prior work [8,9,55,56] and represent each pose P by the 3D SMPL [47] vertex mesh, $V(\\theta,\\beta,G)$. We choose a mesh-based representation as any physical analysis of human motion requires accurate modeling of skin-level surface contact. SMPL is a convenient choice as it parameterizes the 3D mesh into disentangled pose, $\\theta$, body shape, $\\beta$ and gender, G parameters, allowing explicit and independent control over the gender, body shape and pose. For ease of notation, we combine the body shape and gender into a single identity parameter, $I = (\\beta,G)$, and use it as the conditioning signal in our motion model. Given the target identity, $I_t$ and an arbitrary duration T, we generate a sequence consisting of the global root joint position, $x_t \\in \\mathbb{R}^3$ and the root-relative joint rotations, $r_t \\in \\mathbb{R}^{J\\times 6}$ in the 6D rotation representation [86], where J = 23 is the number of SMPL joints and one global rotation. Although SMPL uses parent-relative rotations defined on the kinematic chain, we empirically observe that using root-relative joint rotations results in more stable gradients and faster convergence. Consequently, we convert the SMPL parent-relative pose parameters, to root-relative rotations to construct our motion features. Additionally, we process all sequences by removing the z-component of the first-frame root orientation $r_1$ and the horizontal root translation, $x_1$ and $x_1$. Doing this canonicalizes all sequences to start at the origin with the same forward facing direction and makes network training easier. Please refer to Sup. Mat. for a detailed description of our motion representation."}, {"title": "3.1 HUMOS model architecture", "content": "HUMOS is designed as a conditional Variation Auto-Encoder (c-VAE) network that generates sequential motion features in a non-autoregressive manner where we output motion features for T consecutive frame in one-shot. Our choice of non-autoregressive training is driven by the observation that while auto-regressive approaches are effective in generating simple motions like walking, running, etc., one-shot approaches yield better motion diversity [60]. Consequently, most text-to-motion approaches employ non-autoregressive generative modeling since they focus on generating diverse motions conditioned on text. Following this trend, we use a non-autoregressive training paradigm to output diverse motions conditioned on body shape. We use a Transformer [71] architecture to obtain spatio-temporal embeddings from the input motion features. Next, we describe our motion encoder followed by the motion decoder (see Fig. 2 for an overview)."}, {"title": "3.2 Self-supervised shape-conditioned training", "content": "Training a shape-conditioned c-VAE in a fully supervised manner requires paired identity and motion data. While datasets like AMASS [49] contain a large collection of 3D motion capture sequences, often containing diverse motions per-"}, {"title": "3.3 Intuitive-physics (IP) terms", "content": "Our motion encoder aggregates spatio-temporal features over successive frames to learn a shape-agnostic latent embedding by disentangling the motion \"style\""}, {"title": "3.4 Dynamic stability term", "content": "In the real world, motion is the result of internal muscular forces and external forces acting on the body and the surrounding scene. Human bodies are typically stable, i.e. they have the ability to control their body position and momentum during movement without falling over.\nTripathi et al. [70] successfully use the notion of static stability in 3D human pose and shape estimation to output physically-plausible and biomechanically stable poses from RGB images. In static poses, a body is considered stable if the gravity-projection of the center of mass (CoM) onto the ground is within the base of support (BoS). The base of support is defined as the convex hull of all points in contact with the ground. Since the base of support requires a convex hull computation that is not easily differentiable, [70] minimize the distance between an estimated center of pressure (CoP) and the projected CoM instead, and use it as a proxy static stability loss or energy term which is minimized during training and optimization. However, this static treatment of stability is only applicable to static poses. Humans are highly dynamic by nature and we need a general treatment of stability analysis that extends to all scenarios involving human movement and locomotion.\nDynamic stability extends this concept to bodies in motion. We follow the concept of zero-moment point (ZMP) [73], which has been widely used in robotics and biomechanics [41,57]. Assuming flat ground, the ZMP is the point on the ground where the horizontal component of the moment of ground reaction force is zero. If this point lies within the base of support, the ZMP is equivalent to the center of pressure and the motion is considered dynamically stable (see Sup. Mat. video for an example in human gait)."}, {"title": "3.5 Latent embedding losses", "content": "To enable motion generation at inference time, we regularize the distributions of the latent embedding spaces, $Z_{M_A} = \\mathcal{N}(\\mu_A, \\Sigma_A)$ and $Z_{M_{A\\rightarrow B}} = \\mathcal{N}(\\mu_{A\\rightarrow B}, \\Sigma_{A\\rightarrow B})$ to be similar to the normal distribution $\\psi = \\mathcal{N}(0, I)$ by minimizing the Kullback-Leibler (KL) divergence via\n$L_{KL} = KL(z_{M_A},\\psi) + KL(z_{M_{A\\rightarrow B}},\\psi)$"}, {"title": "4 Experiments", "content": "We first discuss our data processing and implementation details in (Sec. 4.1). Next, we introduce baselines and the evaluation metrics (Sec. 4.2) used in our comparisons. Then, we discuss quantitative, perceptual and qualitative comparisons of our method with baselines (Sec. 4.3) and present an ablation study (Sec. 4.4)."}, {"title": "4.1 Data and implementation details", "content": "For training, we use the AMASS dataset which contains 480 unique gender identities out of which 274 are male and 206 are female with diverse body shapes and sizes. Please refer to Sup. Mat. for a full analysis on the diversity and distribution of body shape $\\beta$ parameters in AMASS. We first subsample raw SMPL-H sequences from AMASS [49] to 20 fps following Guo et al. [32]. We augment the data by mirroring sequences left-to-right. We exclude sequences where the feet are more than 20 cm above the ground and where the lowest vertex across all frames is not grounded. This is to ensure ground support as it is an essential component for dynamic stability. We observed that normalizing sequences for consistent facing and start position in the first frame helped with training. We then extract input features by converting the root orientation and joint rotations to 6D form [86] and concatenate root translation, betas and gender. In addition, we augment the AMASS dataset by applying left-right flip augmentation to the pose parameters, effectively doubling the amount of training data. We show a step-by-step visualization of our data-processing pipeline in Sup. Mat.1\nWe train our models for 1300 epochs with the AdamW [39,48] optimizer using a fixed learning rate 10-5 and a batch size of 60. Both our encoder and decoder consist of 6 transformer layers. We train with sequence length T = 200 frames on"}, {"title": "4.2 Baselines and evaluation metrics", "content": "We focus on the task of shape-conditioned motion reconstruction for evaluating the performance of HUMOS. Since no existing baseline directly addresses shape-conditioned motion reconstruction, we create new baselines by combining a state-of-the-art motion generation model, TEMOS [56] and retargeting its output motions to a target body shape using 1) na\u00efve retargeting and 2) using the commercial retargeting system, Rokoko [62]. For these experiments, we reconstruct the same motions from the AMASS test-split for both the TEMOS baselines and HUMOS.\nTEMOS generates motions for a canonicalized mean-shape SMPL body by directly regressing the pose and global root-joint translation. For a fair comparison, we use its \"unconditional\" variant which does not require any text inputs. For obtaining motions for a target body, we do simple retargeting where we randomly sample identities from AMASS and na\u00efvely copy the target $\\beta$B and gender $G$B parameters to the motions obtained from TEMOS. We call this baseline, \"TEMOS-Simple\". Intuitively, na\u00efvely copying a new identity to a neutral mean-shape body motion will result in ground peneration, floating and foot sliding artefacts. We address ground penetration by translating the whole motion sequence such that the lowest vertex in the sequence is on the ground. We refer to this baseline as \"TEMOS-Simple-G\". For another strong baseline, we use the Rokoko retargeting system to retarget TEMOS generated motions to the target body. We call this baseline \"TEMOS-Rokoko\", with \"TEMOS-Rokoko-G\" being the variant where we ground the Rokoko output sequences as described above. For consistency in evaluation, given an input motion, we sample the same target identities across our method and all baselines.\nEvaluation Metrics. For evaluating the physical plausibility of generated motions, we use the physics-based metrics suggested by Yuan et al. [80]. The Penetrate metric measures ground penetration by computing the distance (in cm) between the ground and the lowest body mesh vertex below the ground. Float measuring the amount of unsupported floating by computing the distance (in cm) between the ground and the lowest body mesh vertex above the ground. Skate measures foot skating by computing the percentage of adjacent frames where the foot joints in contact with the ground have a non-zero average velocity. We also report two metrics for measuring dynamic stability. Dyn. Stability computes the percentage of frames where the ZMP is outside the base of support. The BoSDist metric measures the distance of the ZMP to the closest edge of the BoS convex hull, if the ZMP lies outside the BoS, indicating the pose is dynamically unstable. For details, please refer to Sup. Mat."}, {"title": "4.3 Comparison to baselines", "content": "Quantitative. We summarize our main results in Tab. 1. As we lack ground-truth motions for the target body shape, we rely on physics and stability metrics to compare our method with baselines. Our method substantially outperforms on all metrics except on Penetrate. As expected, the lowest ground penetration is observed for TEMOS-Simple-G and TEMOS-Rokoko-G as both were specifically altered to ground the sequence. However, this comes at the cost of increasing the Float metric. In contrast, HUMOS simultaneously improves both Penetrate and Float indicating that the network learns to modify body pose (e.g. foot tilt) in addition to learning the correct global translation for grounding the motion. HUMOS also improves over baselines on foot skating, achieving a ~ 7.3% Skate compared to 20% and 27% for the TEMOS-Rokoko and TEMOS-Simple baselines. HUMOS's motions are also dynamically stable in 71.9% of all frames, a significant improvement of 16% over the closest baseline. Even for dynamically unstable poses, when the ZMP is outside the BoS, it is close to the BoS edge as indicated by the low BoSDict metric for our method.\nQualitative. We provide additional qualitative comparisons with baselines in Fig. 3. Each row represents the same pair of source motion and target body across all methods. We highlight physical plausibility issues such as foot-skate, ground penetration and floating in red. The green highlighted region points to"}, {"title": "4.4 Ablation Study", "content": "We evaluate the importance of our key contributions, $L_{cycle}$, $L_{physics}$ and $L_{dyn}$ in Tab. 3. As shown, $L_{cycle}$ alone achieves a significant ~ 33% improvement in"}, {"title": "5 Conclusion", "content": "In this paper we presented a method for shape-conditioned motion generation that used a set of physically inspired constraints to allow for self-supervised disentanglement of character motion and identity. This allows for motion generation and retargetting of a higher quality than previous methods both qualitatively and quantitatively.\nIn terms of limitations, although our method represents an improvement over previous work there are still motion artefacts introduced by the model. Additionally, the differences in the style of motion produced by characters of very different body shapes remain subtle. This may be due to the limited shape diversity in the training set. In the future it would be interesting to examine how this data distribution affects the diversity and generalization capabilities of the model. We also do not take into account self-penetrations that may arise during shape-conditioned motion generation. Addressing this would be another promising direction for future work. While human motion is influenced by both body shape and individual motion style, we only consider body shape. Motion style includes factors like emotional state, physiological impediments, societal influences, and personal biases, which are not annotated in existing mocap datasets. With style-specific annotations, it would be useful to extend HUMOS to include style attributes as additional conditioning signals."}, {"title": "Appendix", "content": "In the supplementary materials, we provide a detailed derivation of the ZMP and the dynamic stability term (Appendix A), analyze the effect of body shape on motion (Appendix B), provide additional qualitative results (Appendix C), ablations for the latent embedding losses (Appendix D), a discussion on AMASS shape diversity (Appendix E), and finally additional implementation details (Appendix F).\nVideo. Our research focuses on humans in motion with diverse body shapes and sizes, making motion a critical aspect of our results. Given the difficulty of conveying motion quality through a static document, we strongly recommend that readers view the provided supplemental video for an in-depth overview of our methodology and findings."}, {"title": "A Detailed derivation for the Zero Moment Point (ZMP) and the dynamic stability term", "content": "Before we compute the ZMP, we first compute the body Center of Mass (COM) by adapting the CoM formulation of Tripathi et al. [70] to dynamic humans. For every sequence, we use their body part segmentation and the differentiable \"close-translate-fill\" [70] to compute per-part volumes $VP_i$ by splitting the mesh in the first frame into 10 parts. Using the per-part volumes, the CoM is calculated for time instance t, as a volume weighted-average of $N_v$ = 6890 mesh vertex points.\n$G_t = \\frac{\\sum_{i=1}^{N_v} VP_{vi} V_{it}}{\\sum_{i=1}^{N_v} VP_{vi}}$\nThe acceleration of the CoM, $a_{G_t}$, is obtained using the central difference as,\naGt = \\frac{G_{t+1}-2G_t + G_{t-1}}{\\Delta t^2}\nWith $a_{G_t}$, the force of inertia, $F_{gi}$, is computed as\n$F_{gi} = mg - ma_G$\nwhere m is the body mass. The moment around the projected CoM, $C_m$, is\nM_G = \\vec{C_m G} \\times mg - \\vec{C_m G} \\times ma_G - \\vec{H_{\\zeta}}$\nwhere $\\vec{C_m G}$ is the vector joining the projected CoM, $C_m$ with the actual CoM, G and $H_{\\zeta}$ is the rate of change of angular momentum at the CoM. For $H_{\\zeta}$, we equally distribute the total m to point masses at the vertices of the body mesh proportional to the volume of the body part they are part of. The per-vertex mass and acceleration is\nm_{vi} = \\frac{VP_{vi}}{\\sum_{i=1}^{N_v} VP_{vi}} m, a_{vi} = \\frac{V_{it+1} - 2V_{it} + V_{it-1}}{\\Delta t^2}"}, {"title": "B Effect of body shape on motion", "content": "In Fig. S.1 (left), we assess the diversity of HUMOS generated motions across 100 $\\beta$ parameters obtained by interpolating between a short male and a tall male body. We report the maximum right knee joint angle ($|\\theta|$) for the same walk sequence shown in the Sup. Mat. (SM) video (05:34). The graph illustrates that taller people bend their knees less for the same walking motion, indicating body parameters affect movement. Similarly, in Fig. S.1 (center), we plot the right hand joint velocity across six different identities in the same walk sequence [SM video (05:34)]. The joint velocities differ across subjects in corresponding frames implying diversity induced by body shape variation. In Fig. S.1 (right), we qualitatively show the same frame of the jumping jack sequence [SM video (05:47)] where the different arm positions indicate motion diversity."}, {"title": "C Additional Qualitative Results", "content": "We include additional comparisons with baselines in Fig. S.2. For video results, we recommend watching the supplementary video."}]}