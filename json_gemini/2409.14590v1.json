{"title": "Explainable AI needs formal notions of explanation correctness", "authors": ["Stefan Haufe", "Rick Wilming", "Benedict Clark", "Rustam Zhumagambetov", "Danny Panknin", "Ahc\u00e8ne Boubekki"], "abstract": "The use of machine learning (ML) in critical domains such as medicine poses risks and requires regulation. One requirement is that decisions of ML systems in high-risk applications should be human-understandable. The field of \"explainable artificial intelligence\" (XAI) seemingly addresses this need. However, in its current form, XAI is unfit to provide quality control for ML; it itself needs scrutiny. Popular XAI methods cannot reliably answer important questions about ML models, their training data, or a given test input. We recapitulate results demonstrating that popular XAI methods systematically attribute importance to input features that are independent of the prediction target. This limits their utility for purposes such as model and data (in)validation, model improvement, and scientific discovery. We argue that the fundamental reason for this limitation is that current XAI methods do not address well-defined problems and are not evaluated against objective criteria of explanation correctness. Researchers should formally define the problems they intend to solve first and then design methods accordingly. This will lead to notions of explanation correctness that can be theoretically verified and objective metrics of explanation performance that can be assessed using ground-truth data.", "sections": [{"title": "Introduction", "content": "The use of machine learning (ML) holds great promise in many fields including high-risk domains such as medicine. Regulations like the European AI Act demand that \"high-risk AI systems shall be designed and developed [...] to enable deployers to interpret the system's output and use it appropriately.\" [19]. This need for \"human-understandable\" descriptions of the functions implemented by individual ML models is seemingly addressed by the field of \"explainable artificial intelligence\" (XAI). However, the formal basis of XAI is underdeveloped. Consequently, the possibility of using XAI for ML quality assurance is currently strongly limited."}, {"title": "Purported purposes of \u03a7\u0391\u0399", "content": "The popularity of XAI tools rests on their promise to provide insight into the properties of ML models, their training data, a given test input submitted to the model, and/or the interplay between these. A particularly popular class of XAI methods are those that attribute an \"importance\" score to each feature of a given test input. It has been argued that such feature attribution methods can be used for:\nModel (in)validation: It is often of interest to know which features of a dataset or single sample an AI system \"bases\" its decision on. This would then be used to \"validate\" a model. In mammographic data analysis, a radiologist would likely trust a cancer diagnosis made by an AI if told that the decision was based on a patch of tissue they themselves identify as cancerous. Conversely, if the XAI method assigns high \"importance\" to features that are known not to be"}, {"title": "Existing feature attribution methods do not serve important purposes", "content": "It is indisputable that the purposes of XAI presented in Section 2 are of high relevance. However, for these, it is assumed, explicitly or implicitly, that \u03a7\u0391\u0399 methods have the following capabilities.\nAssertions underlying the uses of \u03a7\u0391\u0399."}, {"title": "Two minimal examples of classification problems", "content": "In the following, non-scalar values are highlighted in bold and we denote random variables by upper-case symbols, e.g. Z, whereas lower-case analogs, e.g. z, represent their respective realizations.\nExample A: In Haufe et al. [23], the two-dimensional classification problem X = aZ + H, Y = Z is introduced, with a = (1,0), Z ~ Rademacher(1/2), and H ~ N(0, 2) with covariance \u2211 = ($\\begin{smallmatrix} s_1^2 & c s_1 s_2 \\ c s_1 s_2 & s_2^2 \\end{smallmatrix}$), where $s_1$ and $s_2$ are non-negative standard deviations, and c\u2208 [-1,1] is a correlation. In this example, only feature $X_1$ is correlated with the classification target $Y = Z$ through $a_1 = 1$. In contrast, $X_2$ is independent of Y since $a_2 = 0$. Both features are correlated through the superposition of additive noise H with covariance \u03a3. A depiction of data generated under the model is provided in Figure 1 (a/b). For $c\u2260 0$, the Bayes-optimal bivariate linear classification model $f_{w,b}(x) = w^\\top x+b$ can reduce the contribution of H from $X_1$ using information contained in $X_2$, and thereby estimate y as $\\hat y = f_{w,b}(x)$ more precisely as what would be possible using $X_1$ alone [23]. To this end, it needs to put non-zero weight $w_2 = -ac s_1/s_2$ on $X_2$, where $a = (1 + (c s_1/s_2)^2)^{-\\frac{1}{2}}$ and $||w||_2 = 1$. This shows that linear models can assign arbitrarily high weights on features, like $X_2$, that have no statistical association with Y.\nExample B: An even simpler example is given by the generative model $X_1 = Y \u2013 X_2$, where the suppressor $X_2$ and the target Y are independent [23]. Here the Bayes-optimal linear model with weights $w_1 = w_2 = 1$ completely removes the nuisance term $X_2$ from $X_1$ to recover Y, yielding a model output that is statistically independent of $X_2$. Such examples question the notion of a model \"using a feature\" or \"basing its decision on a feature\"."}, {"title": "Suppressor variables", "content": "Features like $X_2$ in Examples A and B, which improve predictions without being predictive themselves, are called suppressor variables in causal terminology [16]. Causal diagrams of the generative models in both examples are provided in"}, {"title": "Existing feature attribution methods attribute importance to variables unrelated to target", "content": "Recent theoretical and empirical research has shown that various popular feature attribution methods consistently assign importance to suppressor variables [23, 34, 65, 67, 15]. We call these methods suppressor attributors. Kindermans et al. [34] showed analytically that the importance scores returned by gradient-based techniques [9], LRP [8], and DTD [40] reduce to the weight vector w in case of linear models. Thus, these methods are suppressor attributors. In Wilming et al. [67], the latter was shown also for Shapley Values [52] and their approximations such as SHAP [37, 1], as well as for LIME [46], integrated gradients [57], and counterfactual explanations [61]. A list of suppressor attributing methods is provided in Table 1."}, {"title": "Existing feature attribution methods violate common assertions", "content": "Since suppressor variables have no statistical or causal association with the target variable, suppressor attributors violate assertion A1, which has implications regarding their expected utility for the purposes introduced in Section 2. Sup-pressor features may often not coincide with prior expectations of an expert. Therefore, suppressor attributors cannot be used in a straight-forward way to validate models or models' decisions using expert knowledge as insinuated by Ribeiro et al. [46]. Moreover, since it cannot be concluded that the highlighted features are part of previously unknown interactions or are causally related to the output, these methods cannot be reliably used to facilitate scientific discoveries or to invalidate models. For example, high importance on a protected attribute does not necessarily mean that the method \"uses\u201d this attribute for prediction. The model may also just remove variance related to that attribute from other, in-formative, variables. Finally, a prerequisite for identifying confounding variables causally influencing both in- and outputs of a model is to be able to recognize features with a statistical association to the target in the first place. The inability"}, {"title": "Structural limitations of current X\u0391\u0399 research", "content": "The results presented above have been established through joint theoretical anal-yses of data-generating processes, ML models, and feature attribution methods as well as through simulations using synthetic data with known ground-truth explanations. These techniques are not currently part of the standard toolkit for assessing the quality of explanations and XAI methods, pointing out the following fundamental structural limitations in the way the field assesses itself.\nLack of formal problem definitions The current XAI terminology uses the term \"explanation\" indiscriminately in different contexts. This lack of differentiation gives rise to equivocality of evaluation framework and is reflective of a deeper absence of well-defined problems for XAI to solve. Even though XAI methods are frequently proposed to serve purposes such as those listed in Section 2, it is rarely stated what concrete types of conclusions can be drawn from the explanations provided by any particular method, and under which assumptions each conclusion is valid. Instead, various popular XAI methods are purely algorithmically defined without reference to a formal problem or a cost function to be minimized, leading to a logical loop where the method defines the problem it solves. In their work, Ribeiro et al. [46] do not define what the correct features for LIME to highlight would be - the algorithm itself is considered to be the definition of feature importance.\nExisting theory spares out notions of explanation correctness Existing theoretical work has postulated axioms that are desirable for XAI methods to fulfill. For example, according to Sundararajan et al. [57], a method satisfies sensitivity, if a) for every input and baseline that differ in one feature but have different predictions, the differing feature is given non-zero importance, and if also b) the importance of a variable is always zero if the function implemented by the deep network does not depend (mathematically) on it. Axioms like this encode meaningful sanity checks but do not provide a notion of correctness or utility-for-purpose of an explanation. Several authors have proposed to close this gap by describing criteria for the \"faithfulness\" or \"fidelity\" of XAI methods. The descriptions of these concepts, however, vastly differ and they are often not formulated in mathematically stringent form [see, 22, 28].\nXAI methods are mostly ignorant of the data distribution and causal structure With few exceptions, XAI methods are applied post-hoc to model weights or outputs only. However, a model's behavior cannot be meaningfully interpreted without knowledge of the correlation or causal structure of its training data [23, 64, 31, 67]. The same model weights that cancel out a suppressor in Examples A and B (see Section 3.1) would have a completely different function, and hence interpretation, when applied to independent features.\nThe simplifying assumption of independent features is inherent to most \u03a7\u0391\u0399 methods, be it explicit or implicit. Assuming independence is thereby in line"}, {"title": "Towards XAI for quality assurance", "content": "Apart from the points raised here, XAI methods have been criticized in many further ways [e.g., 21, 56, 63]. For example, the low robustness and consistency of XAI explanations has been noted [7]. It has also been pointed out that XAI methods can be manipulated to yield arbitrary explanations [18]. In image prediction tasks, XAI explanations are frequently observed to resemble results of simple edge detection filters [e.g., 2, 33, 15]. Many XAI methods come in multiple variants, and the criteria for choosing methods and their hyperparameters are often not well justified or documented. Nevertheless, XAI is a fast-developing field, and we see individual points of criticism being addressed. The fundamental limitation of the field - the lack of formal specifications of XAI problems - however, renders efforts to improve secondary quality indicators, such as attempts to make explanations more robust, more consistent, or more aligned with subjective human judgment, premature. Such efforts will become relevant again once methods that are proven to solve well-posed \"explainability problems\" in the first place are available. To enable the development of such new methods, the current paradigm of algorithm-driven development needs to be revised. We propose that a new scientific process of XAI development should proceed in six steps: 1. Assessing the use-case-specific information needs of users and stakeholders. 2. Defining the formal requirements and the XAI problems that address these information needs. 3. Designing suitable methods to solve the concrete XAI problems. 4. Performing theoretical analyses w.r.t. the adherence to the formal requirements. 5. Performing empirical validation using appropriate ground-truth benchmarks. 6. Improving the methods concerning further desiderata.\nFormalizing XAI It is unreasonable to call a mapping from input features to real numbers an \"explanation\" without endowing these numbers with a well-defined formal interpretation [e.g. 41]. If this is not precisely stated, the ability of an XAI method to answer relevant questions cannot be objectively assessed, and it is not possible to use the method for systematic quality control. Relevant questions may relate to properties of a given ML model, its training data, a given test input, or combinations of these, and may differ between use cases. Additionally, different stakeholders, such as ML developers, users (e.g., physicians or patients), and regulators, have different information needs. Formalizing use-case and stakeholder-specific questions will lead to distinct XAI problems to be addressed by distinct tailored XAI methodologies. Such a formal framework will also provide provable and testable corresponding notions of explanation"}, {"title": "Outlook", "content": "Just as ML in general, the field of XAI is fast-paced with clever novel method-ological developments and empirical validation approaches being introduced each year. Building blocks and frameworks addressing some of the problems introduced here with theoretical rigour are already available, for example for the purposes of algorithmic discourse and confounder detection [29, 32]. The systematic formalization and scrutinization of the field of XAI is a wider effort that will make it possible to objectively assess the ability of approaches to solve specific XAI problems. This may lead to XAI-based workflows that can indeed"}, {"title": "Conclusion", "content": "Theoretical and empirical analyses of simple data-generating models have shown that popular XAI methods can systematically fail to answer important questions about data and ML models. The root causes of this result lie in the current research practices of the field, which is algorithm- instead of problem-driven. The main technical limitation, causing false interpretations of XAI outputs, is the explicit or implicit assumption of feature independence. Researchers should formally define the specific problems that XAI should solve and design methods accordingly. Synthetic data with ground-truth explanations can play an important role in (in)validating XAI methods."}]}