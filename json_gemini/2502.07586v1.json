{"title": "We Can't Understand AI Using our Existing Vocabulary", "authors": ["John Hewitt", "Robert Geirhos", "Been Kim"], "abstract": "This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they're reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a \"length neologism\" enables controlling LLM response length, while a \u201cdiversity neologism\u201d allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better.", "sections": [{"title": "1. Introduction", "content": "\"Die Grenzen meiner Sprache bedeuten die Grenzen meiner Welt\" (The limits of my language are the limits of my world) Ludwig Wittgenstein\nAs researchers interested in understanding and controlling language model-based AI systems, we often search for human-like concepts in machines-e.g., by analyzing a machine's activation patterns. Examples of such concepts include human-like linguistic structure (e.g. Lakretz et al., 2019; Hewitt & Manning, 2019), or notions of safety or truth (Burns et al., 2023). Often, the goal of this search is to help specify human concepts to machines\u2014that is, to control them, e.g., through supervised probes, or prompts, or RLHF (Ouyang et al., 2022). Taken together, understanding and control are a communication problem: communicating concepts between humans and machines. Within this communication problem, understanding and control are often two sides of the same coin: the purpose of communicating with machines is typically to make machines do what we want them to do (control), while achieving a better understanding should directly translate into better communication.\nThis communication problem is hard because-and we take this as a premise-humans and machines conceptualize the world differently, at many levels of abstraction (as expressed by Figure 1). A machine's notion of sentiment is different from a human notion of sentiment. Likewise for high-quality code, or topic. In Kim (2022) for example, reproduced in Figure 2, there is a space M of machine concepts and a space H of human concepts, and many things are either in $H - M$: concepts humans have but machines do not, or $M - H$: concepts machines have but humans do not. In fact; even for things seemingly in $H\u2229 M$, we expect that careful inspection would show that the seemingly similar concepts actually differ between humans and machines.\nOur position is that progress in this communication problem\u2014thus, progress in interpretability is best achieved by striving to define new words (neologisms) that mean a human concept (when interpreted by a machine) or a machine concept (when interpreted by a human).\nWhat does introducing neologisms offer? The perspective provides clarity in what level of abstraction to attempt to bridge this communication gap. Successful words in a language strike useful levels of abstraction: they're not too exacting and low-level, like a word for the exact placement of the chairs at my table in relation to me. Such words would be too rarely used to be successful. This is alike to attempting for a full, exact, mechanistic understanding of a neural network: words corresponding to such exactness must necessarily not apply commonly, because the world (and networks modeling the world) are too complex to be concisely described at that level. At the same time, most successful words are not too high-level (only a few such words, like thing, exist)\u2014they're discriminative enough of communicative intent to be informative in conversation. Erring too high-level is alike to only doing behavioral testing of a network; the level of abstraction is that of an input-output map (e.g., logit output). Such evaluation is useful, but it gives us insufficient richness to specify our goals and understand future behavior.\nThe next useful property that neologism learning gives us is participation in language. Language is how we understand other humans, and we define new words when our differences in conceptualization lead to the need to concisely communicate new concepts. Likewise, defining new words in our communication problem with machines, we can plug these words into existing language and leverage the expressive compositional structure thereof.\nFinally, the neologism framing helps us combat confirmation bias and anthropomorphism (e.g. Buckner, 2019). As human researchers, we have a bias towards seeing human-like things in artificial networks; we want to see high-level human concepts appearing in networks. We want to see exciting unsupervised structure. Even something as simple as a \u201csentiment neuron\" (OpenAI, 2017), if given its own new word, reminds us that this sentiment-like concept of the machine is likely dissimilar from what we call sentiment in ways that another human's notion of sentiment might not be. In Section 3.1, we argue that this dissimilarity will only increase as machines become more capable.\""}, {"title": "2. Understanding AI requires neologisms", "content": "In this section, we present our argument that effective understanding and communication with AI systems requires us to form neologisms. We start with problems faced by researchers attempting to understand AI systems, and then present the solutions provided by the neologism framing."}, {"title": "2.1. Problems in Understanding AI", "content": "Understanding (and controlling) AI systems, as a joint problem of science and engineering, forces researchers to make bets on what kinds of properties to attempt to discover in AI systems, and what kinds of tools to build. As researchers on the understanding problem, we've noticed a few perennial problems that together motivate our focus on neologisms.\nThe Conceptualization Difference Problem. Humans and machines understand the world differently, forming different concepts equivalence classes, evaluations, skills-from each other. This means we need to reference/label these concepts and then learn or understand the differences.\nLet's take an example. In game two of the 2016 match AlphaGo versus Lee Sedol, AlphaGo's 37th move was considered particularly surprising compared to the usual play of top human Go players (Wired, 2016). AlphaGo would go on to win that game, and the match. Intuitively, AlphaGo may have had a general concept-in this case, a pattern it recognized in the board state\u2014that motivated this move, which humans had not thought of. The general concept behind this move is yet to be understood (M \u2013 \u041d).\nIf machines understand the world differently, can humans learn useful aspects of their thinking? Testing this idea in chess, Schut et al. (2023) developed a process for discovering superhuman chess concepts in AlphaZero (Silver et al., 2017), and another process for teaching those concepts to grandmasters-humans at the frontier of human knowledge to expand what they know. Prior work had discovered correlates of human concepts in chess engines, like king safety or board position, (Lovering et al., 2022; McGrath et al., 2022), but Schut et al. (2023) focused specifically on new yet teachable (generalizable) concepts. These concepts are successfully taught to four top\u00b9 grandmasters, showing that the concepts were alien, but learnable.\nThe Abstraction Problem. Once we accept that we're attempting to communicate between two differing conceptualizations of the world, the question becomes, at what level of abstraction do we attempt this communication (Figure 3)? Low abstraction would suggest attempting exactingly precise concepts. For example, we have access to the precise forward pass of the network, which is as much a program of its behavior as one could hope; nonetheless, this knowledge is not considered sufficient for understanding the network. High abstraction, on the other hand, would suggest attempting to build very broad statements, like AlphaZero doesn't mind giving up material for a positional advantage. These might be nice to know, but are insufficiently rich in how they let us control or trust the model. There is no one right level of abstraction at which to tackle the understanding problem, but it is key to hit a good balance, as we'll argue.\nThe Confirmation Bias Problem. As humans and as researchers, we have a bias towards finding exciting, seemingly human-like properties in models. Considerable ink has been spilled on the pitfalls of such biases, e.g., in probing (Hewitt & Liang, 2019), saliency maps (Adebayo et al., 2018; Bilodeau et al., 2024), as well as interpretability as a field (Lipton, 2017; Doshi-Velez & Kim, 2017). When we're looking for interesting concepts, attempting to control them, even at a useful level of abstraction, we still fall victim to our own biases when evaluating whether those concepts, like safety or sentiment, really line up with our own. Put another way, if we had discovered high-level concepts of safety in models already, wouldn't the safety-training teams at frontier labs be using those methods instead of (or as well as) doing better data collection for RLHF?"}, {"title": "2.2. How Neologisms Help", "content": "We now argue how the problems we've discussed can be ameliorated by framing understanding AI as a communication problem in which we form neologisms to reference human or machine concepts.\nLet's go back to the example of chess and AlphaZero. When one human chess grandmaster attempts to understand the play of another, they use concepts and categories at some level of abstraction (e.g., forks, pins) that chess players jointly develop through shared experiences (games they play or observe together) and many rounds of discussion. Without developing this shared language, their discussions would be verbose. When we as humans attempt to understand AlphaZero, we should expect to have to develop such a language of concepts ourselves. Developing neologisms is a first key step in developing this shared language such that it enables efficient communication of complex concepts. Those of us who speak more than one language are familiar with the difficulty and verboseness of expressing certain thoughts in a language that simply does not have a word for the same concept, like the Dutch \u201cGezelligheid\" (a sense of warmth, coziness and sometimes friendship in a social context), or the Korean \"Jeong\" (affection that only develops over time, sometimes love and hate affection, expressed through experience-translating it as \"affection\" simply does not do the concept justice)."}, {"title": "Neologisms concisely reference new learnable concepts.", "content": "Differences in conceptualization introduce two problems: 1) we cannot yet reference new concepts concisely 2) we do not yet understand each concept. Neologisms provide a solution to the first problem. A neologism is a successful new word; neologisms are formed when there are complex concepts that would be onerously verbose to reference otherwise-i.e., by explaining the concept in a paragraph or a book each time. Given that machines and humans have differences in conceptualization, developing new words to reference those concepts to each other is a natural solution. However, just because we can reference it, does not mean we can understand what it means. An important element for a successful neologism is proximity; in Vygotsky's education theory, proximity references a concept being in \"the space between what a learner can do without assistance and what a learner can do with adult guidance or in collaboration with capable peers\" (Vygotsky, 1978); neologisms are formed to help reference things that are reachable but outside our current understanding. For example, Schut et al. (2023) targeted teaching chess champions whose proximity zone has better chance of capturing AlphaZero's superhuman strategies."}, {"title": "Neologisms moderate useful abstraction.", "content": "Natural languages are living, ever-changing things, and as such, potential new words crop up regularly. The words that survive to become neologisms strike a useful level of abstraction. Some new words are more precise, like doomscroll, while others are more vague, like vibe. A word that struck the right balance between the right level of abstractness and usefulness is a case of successful neologism, thus, interpretability. The pressure of broad applicability enforces some abstraction: words gloss over an ocean of detail about the world so that they're applicable in many settings, and thus used (unused words fail as neologisms.) The pressure of informativeness presses down on the amount of abstraction: a word that references all things would be uninformative."}, {"title": "Neologisms lessen confirmation bias.", "content": "There is power to giving a new label to a thing instead of referring to it by a known label: it encourages us to believe that the new thing is by default different and unknown. In the case of the OpenAI \"sentiment neuron,\" researchers discovered a single activation in a network that correlated reasonably well with sentiment on the Stanford Sentiment Treebank (Socher et al., 2013). At first glance this is not a bad label, but as interpretability researchers, labeling its concept with a new word $sentiment\u2122$ reminds us that it is probably not like sentiment in systematic ways that remain to be interpreted. We haven't found a human concept; we've found a machine concept that has some overlap with human concepts."}, {"title": "Neologisms enable compositionality.", "content": "The beauty of human language is in the ability to concisely build an infinitude of meanings from a finite symbol vocabulary. And due to the complexity of the world, there is an infinitude of concepts to understand about language models, not a finite set of features. Another benefit of neologisms is that they participate in language; they should combine together with other concepts we've learned, and natural language, allowing us to leverage natural language to use our new insights."}, {"title": "Neologisms provide a human interface for control.", "content": "Many interpretability techniques are repurposed for control, typically by working with internal representations (e.g., sparse autoencoders (Cunningham et al., 2023), or probing (Zou et al., 2023),). Neologisms enable these controls by integrating into humans' natural way of communication - language. As humans use new words, all expressive tools of language are at their disposal that could enable better precision and alignment."}, {"title": "3. Alternative views and rebuttals", "content": "This section describes views that are opposed to our position, along with rebuttals. We follow it with a broader discussion of related interpretability work."}, {"title": "3.1. Automatic convergence: scale will solve the communication problem", "content": "Position. This position accepts that there are differences in the way humans and machines understand the world, but believes that these will inevitably narrow, and eventually disappear altogether. The argument is based on the observation that the shared human-machine space ($M\u2229 H$ in Figure 2) has grown rapidly over recent years, from models that were barely able to produce a coherent paragraph to today's LLMs that excel at writing poetry, code and email drafts. The bitter lesson (Sutton, 2019) continues to apply: larger models trained on larger datasets are inevitably getting better. Extrapolating this trend, the \u201cautomatic convergence\" argument posits that the remaining gap between human and machine understanding, represented by the non-overlapping areas in Figure 2, will gradually shrink and eventually vanish as a result of scaling models.\nRebuttal. While machines were approaching human level performance, the gap could indeed be perceived as narrowing. However, the gap will not close: as a simple example, humans will not be able to reason over adversarial examples that will likely continue to exist. We are yet to find how to reason over why change in one meaningless word in a prompt cause drastic changes in response (sometimes critical mistakes e.g., jailbreaking) (Branch et al., 2022). Beyond these peculiarities of existing systems, future machines are on track for superhuman performance (e.g, AlphaFold,"}, {"title": "3.2. We already have all the words needed to communicate.", "content": "Position. This position asserts that our existing vocabulary and language is sufficient to understand AI. We don't always introduce a new word when we extend our knowledge; sometimes it's just a longer description combining things we already know. We just need methods that map from machine concepts to natural language explanations. While there may be debate around whether explanations should be faithful (ideal but hard to verify) or just appear plausible or helpful (after all, we also accept post-hoc explanations from humans without being able to check whether they are true), this position essentially states \u201cwhy invent new words when our existing vocabulary must be sufficient for explanations. We can't understand what we can't describe.\"\nRebuttal. We agree that in lieu of new words for new concepts, it may be possible to derive a natural language description for any concept-albeit a potentially long and cumbersome one. However, a lack of a concise word stifles communication; imagine not being able to use the word \"house\", and instead having to describe what you mean every time you'd like to refer to the concept. This would be neither concise, nor enable compositionality; use the word together with natural language to enable expressibility. For example, having crisp words like \"house\u201d enable us to combine them in novel ways (houseboat, courthouse). According to Wittgenstein, \"the limits of my language are the limits of my world\" (Wittgenstein, 1922) \u2013 and if we're interested in communicating with the world of machines, leveraging the expressibility of natural language and beyond would be necessary for flexible and effective communication."}, {"title": "3.3. We just need an exhaustive map of explanations", "content": "Position. The lack of rigor in interpreting a model is the fundamental problem. If we can understand the exact low-level circuits, creating a comprehensive map of model features along with their function, this 'explanation map' would solve interpretability and explain the entire decision-making process of models with great accuracy.\nRebuttal. Unfortunately, this approach does not scale well to increasingly large models with increasingly many circuits and features. Even if it did, an important lesson from neuroscience is that even mapping out the entire connectome of a system (as done for the worm c. elegans, cf. Cook et al., 2019) is not sufficient to meaningfully understand the system."}, {"title": "3.4. We don't need abstraction", "content": "Position. In terms of understanding machines, the gold standard is a precise, exact mechanistic understanding with as little unexplained abstraction as possible-alike to \u201creverse engineering\" neural networks (Olah, 2022).\nRebuttal. First, finding the right level of detail isn't obvious: Is it the level of layers? Circuits? Individual units, and their receptive and projective fields? The code that specifies a network, or the code it compiles to? The silicon it runs on? The atoms that create the silicon? Presumably, we can all agree that the level of individual atoms would be a ridiculous level of detail for analyzing machine intelligence; nonetheless this goes to show that in science, abstraction is often an advantage, not a drawback (cf. Borges et al., 2002). Secondly, even if there was a universally accepted \"right level of detail\", in terms of human-machine communication this would still be a one way street, seeking to identify human concepts in machines. In contrast, by creating new words to communicate concepts between humans and machines, this enables a compositional understanding where one concept can be re-used for a different purpose, and combined with others. Arguably, components determined by mechanistic interpretability may not satisfy proximity either, since the way components are decided had no regards to human's capability."}, {"title": "4. How neologisms fit into other interpretability work", "content": "We here discuss connections to a range of techniques and perspectives within existing interpretability work.\nFeature attribution methods. Feature attribution methods (e.g. Sundararajan et al., 2017; Lundberg & Lee, 2017; Selvaraju et al., 2017; Shrikumar et al., 2017; Smilkov et al., 2017) are widely used methods in interpretability. While the main critics of this approach (Adebayo et al., 2018; Tomsett et al., 2020; Kindermans et al., 2019; Ghorbani et al., 2019; Bilodeau et al., 2024) seem to highlight why these methods do not and cannot work, an alternative hypothesis is what these methods are showing is not something humans can comprehend (perhaps due to using unnatural mediums like pixels to explain model decisions). For example, the fact that humans or quantitative metrics that we defined cannot distinguish between feature attributions from a trained network vs. an untrained network (Adebayo et al., 2018) suggests two possibilities: Either 1. that they are truly the same or 2. the metrics we use are incapable of describing the concepts that machines have.\nConcept discovery. Finding new concepts from models has been a well-studied problem (Bau et al., 2017; Ghorbani et al., 2019; Fel et al., 2023; Lang et al., 2021; Rane et al., 2023; Schut et al., 2023). While many attempt to name certain machine concepts, there is no systematic thinking on developing a new word with reusability in mind, let alone being able to use them in composition with natural language. Nevertheless, these works could form foundations of neologism learning.\nFaithfulness and evaluation. Faithfulness-whether an explanation truly reflects a model's concepts has long been a point of discussion in interpretability in the context of evaluating explanations (Lipton, 2017; Doshi-Velez & Kim, 2017). Neologisms do not solve this problem; instead, they provide a new way to evaluate. We consider a new word a success if we can communicate something useful (e.g., control) using the new word, and the machine possibly uses the same word to communicate something in return. For example, If a new word $good_m$ defines how machines understand 'good' answers, humans learn ways to use this towards their goal. If $good_m$ is aligned with good except for length, we can prompt 'give me $good_m$ answers but make them short'. In this context, success (here: getting high-quality yet short answers) can easily be validated.\nProbing and representation engineering. Probing-training a simple readout function from neural activity to a property of interest-was most recently independently introduced in machine learning by Alain & Bengio (2016); Ettinger et al. (2016); Shi et al. (2016), though the methods were directly inspired by similar methods in neuroscience, which have a long history. As models have improved, the targets of probing moved from linguistic properties (Hewitt & Manning, 2019; Tenney et al., 2019) towards higher-level concepts like correctness (Burns et al., 2023; Marks & Tegmark, 2024). Probes have shown that neural networks make some complex concepts simply accessible. Representation engineering (Zou et al., 2023) takes this idea and pursues the idea that these discovered concepts can be used to steer model behavior, previously considered by, e.g., (Eisape et al., 2022). Probing and representation engineering as tools allow for a range of levels of abstraction and can be tools for communication of already known concepts ($M\u2229 H$), though the fundamental question in this case becomes when and where to apply probes to communicate with machines."}, {"title": "5. A proof of concept: Neologism Embedding Learning", "content": "While our position in favor of neologisms is independent of specific implementations, one may ask how to implement this idea. This section presents a simple proof-of-concept to more concretely discuss the merits. Our method, neologism embedding learning (Figure 4), involves three steps:\n1. Defining a new vocabulary element in the tokenizer of a language model (and corresponding embedding matrix)."}, {"title": "5.1. Method", "content": "Let $p_\u03b8$ be a neural language model parameterized by \u03b8. Let V be a finite vocabulary, with corresponding word embeddings $E \u2208 R^{d\u00d7|V|}$, where $E \u2208 \u03b8$, and d is the dimesionality of representations in $p_\u03b8$. We add a new word w to the vocabulary, such that $w \\notin V$, and our new vocabulary is $V' = V\u222a \\{w\\}$. We define a new embedding for for w, initialized either randomly or from some existing embedding. We'll refer to this new $R^d$ embedding of w as $E_w$. By construction, no naturalistic data contains the new word w.\nWe take preference data wherein the prompt contains the new word w. More precisely, we assume a dataset $D = \\{(x_i, y_{i,c}, y_{i,r})\\}_{i=1}^n$, where each $x_i$ is a string over $V'$ that is guaranteed to include w, and $y_{i,c}$ is the chosen response, a string over V, and likewise $y_{i,r}$ is the rejected response.\nPulling an example from below, a sample prompt from such data might be Give me a recipe for tiramisu. Ensureh that the length of the response is at least 600 words. We've labelled the new word Ensureh for legibility, but recall that its embedding is new, and not tied to the English word ensure. Instead, it refers to a specific human meaning (thus h) of ensuring length. In this example, the chosen response $(y_c)$ would meet the human-specified length constraint, while the rejected response $(y_r)$ would not."}, {"title": "5.2. Merits and related methods", "content": "Neologism embedding learning combines the lightweight finetuning method soft prompting (Lester et al., 2021) with the flexibility of prompting. Soft prompting involves learning an embedding or embeddings that are prepended to all inputs without aiming to being interpretable (Bailey et al., 2023). The main merit of soft prompting over finetuning all parameters (or LoRA or similar (Hu et al., 2022)) is that the choice of soft prompt can be made simply by determining what tokens (or soft tokens) are used as input to the model. Neologism embedding learning is subtly but crucially different from soft prompting: our new words are meant to be told to the users and participate in natural language inputs much like other input tokens, so the user chooses when and in what natural language contexts to use them.\nAs such, when a user decides not to use a new word w in their prompt, they get a guarantee that they sample from the original, unchanged model (Figure 4.) Users can also use them in composition with other new words; we envision understanding machine concepts will require a variety of ways to combine new words."}, {"title": "5.3. Experiment: Length Neologism (H \u2192 M)", "content": "Controlling the length of a response is a surprisingly difficult problem even for the frontier models when the desired length is very different from the model's prior over response lengths for the instruction. In these experiments, we define a preference dataset D by taking broad-domain instructions from LIMA (Zhou et al., 2023), and constructing responses that do and do not obey a specified length constraint; in particular, one of two: Ensure that the response is between 400-600 words and between 600-1000 words. For our neologism word ensure, we replace the word ensure in that prompt. When we test a base Gemma model (Mesnard et al., 2024) on held out instructions with one of the two length constraints (Figure 5), we find that the base model never meets the length constraints for the instructions we used (Figure 5.) However, many more responses meet the length constraints when prompted with ensure."}, {"title": "5.4. Experiment: Diversity Neologism (H \u2192 M)", "content": "One LLM 'control knob' that a human user might want to have access to is the diversity or variation across several responses. Sometimes, responses should not be variable: two plus two equals four, no matter how many times the model is asked. On the other hand, sometimes variability is desired: for example, when we want a model to try multiple attempts to get the right answer or do a creative task (e.g., write a poem). Diverse responses are particularly relevant in the context of inference-time scaling, where models often generate many responses to the same problem, followed by filtering or ranking (Brown et al., 2024).\nOne expensive heuristic for generating more diverse outputs is conditioning on existing responses, and asking for a new one. We leverage this heuristic to train a neologism for this human concept by using a high quality teacher model (Gemini 1.5 Pro, Georgiev et al., 2024) to condition on up to four responses for each instruction, and request a subsequent new (different) response. Our preference data contains instructions from the LIMA dataset (Zhou et al., 2023), each appended with an additional phrase. Instead of asking give me your kth response, we prompt with diversen me your kth response. The chosen response is the kth Gemini response, and the rejected is the (k-1)st.\nTo test this neologism in a proof-of-concept setting, a model is asked to guess an integer between 1 and 9. We then as-"}, {"title": "5.5. Experiment: A Model's Preferences (M \u2192 H)", "content": "How can we learn a concept from a language model using almost just the methods we've discussed so far? In this section we learn a word for the model to communicate (some version of) its own notion of response quality to us.\nIn these experiments, we learn a neologism w that makes a model generate responses that it would itself deem as \"good\". Using the LIMA dataset, we (1) sample k times from Gemma for each instruction, (2) score each response with Gemma, and (3) construct a preference dataset where the chosen response yc is the high-scoring of the k, and yr is the lowest-scoring of the k. We then learn goodm, using the neologism embedding learning method, where the natural language we add to each LIMA instruction is Give me a response you think is good, where good is our new word."}, {"title": "6. Conclusion", "content": "Solving the communication problem between two dissimilar intelligent entities-communicating human concepts to machines, and machine concepts to humans-requires new language. Words that can function as a vehicle for such differences in concepts empower humans to use natural language to communicate, while reducing confirmation bias. Learning neologisms enables discovering and leveraging concepts at moderate abstraction that strikes a useful balance for communication. Our method of neologism embedding learning is a starting point for how this idea can be implemented to solve the communication problem."}, {"title": "Impact Statement", "content": "This article presents a perspective related to understanding and controlling AI systems through neologisms. As an interpretability tool, neologisms could benefit two purposes:"}, {"title": "A. Methods", "content": "A.1. Preference Loss\nFor our preference loss function L, we use a variant of DPO (Rafailov et al., 2024) called APO-up (D'Oosterlinck et al., 2024). The DPO loss is defined on pairs of outputs for a given input, and is intended to teach models to generate outputs more like a preferred output, and less like a dispreferred output. D'Oosterlinck et al. (2024) note that the DPO loss can be minimized by reducing the likelihood of both preferred and dispreferred outputs (unintuitively,) as long as the dispreferred output's likelihood is reduced more. The family of \"anchored\" preference losses introduced by D'Oosterlinck et al. (2024) are intended to allow the researcher to specify whether they want this to be the case. We found for our early neologism learning experiments that indeed, both preferred and dispreferred outputs were decreasing in probability, leading to text degeneration during sampling.\nThe variant we use, APO-up, simply adds a term to the DPO loss that corresponds to a saturating benefit to increasing the likelihood of the preferred output relartive to its initial likelihood (in this view, DPO gives a saturating benefit to increasing the likelihood-ratio of preferred over dispreferred, again relative to the original likelihood ratio.) We found that this greatly improved training stability.\nRecall that x is an input sequence, yc a chosen output sequence, yr a rejected output sequence, and pe a language model parameterized by the parameters we're optimizing over, \u03b8. Let \u03b8o be the initial value of the parameters before any optimization. Let \u1e9e be a hyperparameter constant. The loss is as follows:\n$\\mathcal{L}(x, y_c, y_r) = -\\log \\sigma \\bigg(\\beta \\log \\frac{P_\u03b8(y_c | x)}{P_{\u03b8_o}(y_c | x)} + \\beta \\log \\frac{P_o(y_r | x)}{P_{\u03b8_o}(y_r | x)} \\bigg) - \\log \\sigma \\bigg(\\beta \\log \\frac{P_\u03b8(y_c | x)}{P_{\u03b8_o}(y_c | x)} \\bigg)$"}, {"title": "B. Experimental Details", "content": "In all experiments, we use a Gemma 2B model (Mesnard et al., 2024) and the Adafactor optimizer (Shazeer & Stern, 2018). Through early exploration, we determined a learning rate of 0.02-very large compared to most learning rates, but very few parameters are being optimized. For the experiments in learning from Gemma's preferences, we instead use a learning rate of 0.001. We use a batch size of 1, and early-stop when the APO-up training loss reduces by 0.2. During all generation, we enforce that the new token is not generated by the model by replacing its logit with -\u221e. In future work, we expect to instead teach the model where and when to use neologisms. For the \u1e9e hyperparameter in APO-up, we use 0.2. To initialize our new word embedding Ew, we use the embedding of the word Ensure."}, {"title": "B.1. Length neologism", "content": "For our preference data, we used 700 instructions from the LIMA dataset (Zhou et al., 2023). Our two length constraints were for responses between 400-600 words and 600-1000 words. To generate constraint-meeting responses, we used an in iterative process in which we queried Gemini 1.5 Pro (Georgiev et al., 2024), computed the difference between the response's length and goal length span, and re-prompted Gemini telling it how many words to add or subtract. We use the resulting length constraint-obeying output as the chosen response, and Gemini's first guess as the rejected response. In Table 2, we give example generations from baseline Gemma (prompted with a length constraint) and from using our neologism in the same length constraint. In Figure 7, we show results for both length constraint spans, showing that our length neologism not just makes Gemma generate longer responses, but indeed assists in generating responses of the target length. Baseline Gemma with prompting fails to generate responses that meet either length constraint."}, {"title": "B.2. Diversity neologism", "content": "The following prompt was used to elicit guesses from the model: \u201cYour task is to select an integer between 1"}]}