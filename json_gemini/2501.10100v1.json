{"title": "Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics", "authors": ["Chenhao Li", "Andreas Krause", "Marco Hutter"], "abstract": "Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The proposed method employs a dual-autoregressive mechanism and self-supervised training to achieve reliable long-horizon predictions without relying on domain-specific inductive biases, ensuring adaptability across diverse robotic tasks. We further propose a policy optimization framework that leverages world models for efficient training in imagined environments and seamless deployment in real-world systems. Through extensive experiments, our approach consistently outperforms state-of-the-art methods, demonstrating superior autoregressive prediction accuracy, robustness to noise, and generalization across manipulation and locomotion tasks. Notably, policies trained with our method are successfully deployed on ANYmal D hardware in a zero-shot transfer, achieving robust performance with minimal sim-to-real performance loss. This work advances model-based reinforcement learning by addressing the challenges of long-horizon prediction, error accumulation, and sim-to-real transfer. By providing a scalable and robust framework, the introduced methods pave the way for adaptive and efficient robotic systems in real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Robotic systems have achieved remarkable advancements in recent years, driven by progress in reinforcement learning (RL) [1, 2] and control theory [3, 4]. High-fidelity simulators have played a pivotal role in these developments, enabling safe and efficient policy training [4, 5, 6]. However, the transfer of policies from simulation to real-world environments remains a significant challenge due to inevitable inaccuracies in modeling real-world dynamics [7, 8, 9]. These inaccuracies lead to performance degradation when deploying learned policies on physical hardware, a problem commonly referred to as the sim-to-real gap.\nA prevalent limitation in many approaches is the lack of adaptation and learning once the policy is deployed on the real system [10, 11, 12]. This results in underutilization of the valuable data generated during real-world interactions. Robotic systems operating in dynamic and uncertain environments require the ability to continually adapt their behavior to new conditions. The inability to exploit real-world experience for further learning restricts the system's robustness and limits its ability to handle evolving scenarios effectively. Truly intelligent robotic systems should operate efficiently and reliably using limited data, adapting to real-world conditions in a scalable manner [13, 14]. While model-free RL algorithms such as Proximal Policy Optimization (PPO) [2] and Soft Actor-Critic (SAC) [1] have demonstrated impressive results in simulation, their high interaction requirements make them impractical for real-world robotics. Sample-efficient methods are therefore essential for leveraging the information in real-world data without extensive environment interactions [15, 16].\nA promising solution is the use of predictive models of the environment, commonly referred to as world models[17, 18]. World models simulate environment dynamics to enable planning and policy optimization, often referred to as learning in imagination [19]. These models have shown success across diverse robotic domains, including manipulation [20, 21], navigation [14], and locomotion [13]. However, developing reliable and generalizable world models poses unique challenges due to the complexity of real-world dynamics, including nonlinearities, stochasticity, and partial observability [22, 23].\nExisting approaches often incorporate domain-specific inductive biases, such as structured state representations or hand-designed network architectures [24, 25, 26], to improve model fidelity. While effective, these methods are limited in their scalability and adaptability to novel environments or tasks. In contrast, a general framework for learning world models without domain-specific assumptions has the potential to enhance generalization and applicability across a wide range of robotic systems and scenarios.\nIn this work, we present a novel approach for learning world models that emphasizes robustness and accuracy over long-horizon predictions. Our method is designed to operate without handcrafted representations or specialized architectural biases, enabling broad applicability to diverse robotic tasks. To evaluate the utility of these learned models, we further propose a policy optimization method using PPO and demonstrate successful deployment in both simulated and real-world environments.\nTo the best of our knowledge, this is the first framework to reliably train policies on a learned neural network simulator without any domain-specific knowledge and deploy them on physical hardware with minimal performance loss.\nOur contributions are summarized as follows:\n\u2022 We introduce a novel network architecture and training framework that enables the learning of reliable world models capable of long autoregressive rollouts, a critical property for downstream planning and control.\n\u2022 We provide a comprehensive evaluation suite spanning diverse robotic tasks to benchmark our method. Comparative experiments with existing world model frameworks demonstrate the effectiveness of our approach.\n\u2022 We propose an efficient policy optimization framework that leverages the learned world models for continuous control and generalizes effectively to real-world scenarios.\nBy addressing the challenges associated with learning world models, this work contributes toward bridging the gap between data-driven modeling and real-world deployment. The proposed framework enhances the scalability, adaptability, and robustness of robotic systems, paving the way for broader adoption of model-based reinforcement learning in real-world applications."}, {"title": "II. RELATED WORK", "content": "World models have emerged as a cornerstone in robotics for capturing system dynamics and enabling efficient planning and control through simulated trajectories. Early works like PILCO [27] demonstrated exceptional sample efficiency by leveraging Gaussian processes, but their applicability to high-dimensional, nonlinear systems has been limited by scalability constraints. The advent of deep neural networks expanded the horizons of dynamics modeling, allowing complex robotic systems to be effectively modeled and controlled. For example, neural network dynamics models have been integrated with Model Predictive Control (MPC) to enhance adaptability in real-world settings [13]. A prominent application of world models is in robotic control, where dynamics models are used to describe real-world dynamics for policy optimization [28]. Extensions to vision-based tasks have been realized through visual foresight techniques [21, 29, 20], which learn visual dynamics for planning in high-dimensional sensory spaces. Similar ideas are applied to train RL agents in such world models aiming to fully replicate real environment interactions [17, 30]. These approaches underline the versatility of world models in tasks requiring rich perceptual inputs.\nTo improve the generalization of black-box neural network-based world models beyond the training distribution, many works incorporate known physics principles or state structures into model design, addressing potential limitations in control performance. Examples include foot-placement dynamics [24], object invariance [25], granular media interactions [31], frequency domain parameterization [26], rigid body dynamics [23], and semi-structured Lagrangian dynamics models [32]. While these methods demonstrate impressive results, they often require strong domain knowledge and carefully crafted inductive biases, which can restrict their scalability and adaptability to diverse robotic applications. Latent-space dynamics models offer an alternative by abstracting the state space into compact representations, enabling efficient long-horizon planning. Deep Planning Network (PlaNet) [18] and its successor Dreamer [33, 14, 34] exemplify this trend, achieving state-of-the-art performance in continuous control and visual navigation tasks. These frameworks have been extended to real-world robotics [22, 35], demonstrating their potential in both simulation and hardware deployment."}, {"title": "B. Model-Based Reinforcement Learning", "content": "Model-Based Reinforcement Learning (MBRL) has emerged as a powerful approach to address the limitations of model-free reinforcement learning, particularly in scenarios where sample efficiency and safety are critical. Unlike model-free methods, which learn policies directly from interactions with the environment, MBRL leverages a learned model of the environment to simulate interactions, enabling more efficient and safer policy learning. One of the pioneering methods in MBRL is Probabilistic Ensembles with Trajectory Sampling (PETS), which uses an ensemble of probabilistic neural networks to model the environment dynamics [15]. This ensemble approach captures the uncertainty in the environment, allowing for more robust decision-making. Building on the idea of latent-space modeling, PlaNet leverages a latent dynamics model to plan directly in a learned latent space [18]. This approach enables long-horizon planning and is particularly effective in environments where direct planning in the raw state space would be computationally prohibitive. Similar ideas were also seen in previous attempts [36, 37]. Dreamer extends the concept by incorporating an actor-critic framework into the latent dynamics model, enabling the simultaneous learning of both the dynamics model and the policy [33, 14, 34]. Variations on the architectural design also see success in improving generation capabilities of such latent dynamics models with autoregressive transformer [38] and the stochastic nature of variational autoencoders [39]. Recent advancements in this area include TD-MPC and TD-MPC2, which integrate model-based learning with MPC to achieve high-performance control in dynamic environments. TD-MPC leverages temporal difference learning to refine the model and improve the accuracy of long-term predictions [40, 41], while TD-MPC2 extends this approach by incorporating more sophisticated planning strategies to handle environments with high variability [42].\nRecognizing the strengths of both model-based and model-free methods, several hybrid approaches have been developed to combine the sample efficiency of MBRL with the robustness of model-free reinforcement learning. One notable example is Model-Based Policy Optimization (MBPO), which uses a model-based approach for planning and policy optimization but refines the policy using model-free updates [16]. It emphasizes selectively relying on the learned model when its predictions are accurate, thus mitigating the negative effects of model inaccuracies. Building on similar principles, Model-based Offline Policy Optimization (MOPO) extends the framework to the offline setting, where learning is conducted entirely from previously collected data without further environment interaction [43]. MOPO introduces uncertainty penalization in the learned model's predictions to ensure conservative policy updates, effectively reducing the risk of overfitting to model errors. Model-Based Meta-Policy-Optimization (MB-MPO) advances MBPO by incorporating meta-learning techniques to improve the generalization of learned policies across different tasks [44]. This method trains a meta-policy that can quickly adapt to new environments by leveraging a shared model, enhancing the applicability of MBPO in multi-task settings. In contrast to using zeroth-order model-free reinforcement learning for policy optimization, first-order gradient-based optimization is used to improve policy learning [45, 46]. These methods scale up the world models to capture intricate details of environment dynamics and directly apply gradient-based optimization to improve policy learning. This allows for more efficient and precise policy updates, particularly in complex, high-dimensional environments, where accurate gradient information is crucial for performance."}, {"title": "III. APPROACH", "content": "We formulate the problem by modeling the environment as a Partially Observable Markov Decision Process (POMDP) [47], defined by the tuple (S, A, O, T, R, \u039f, \u03b3), where S, A, and O denote the state, action, and observation spaces, respectively. The transition kernel T : S \u00d7 A \u2192 S captures the environment dynamics p (st+1 | St, at), while the reward function R:S\u00d7A\u00d7S \u2192 R maps transitions to scalar rewards. Observations Ot \u2208 O are emitted according to probabilities p (ot | st), governed by the observation function O : S \u2192 0. The agent seeks to learn a policy \u03c0\u03bf : \u039f \u2192 A that maximizes the expected discounted return \u0395\u03c0\u03bf [\u03a3\u03c4\u22650 Vrt], where rt is the reward at time t and \u03b3\u2208 [0, 1] is the discount factor.\nWorld models [17] approximate the environment dynamics by learning p(St+1 | St,at). These models facilitate policy optimization by enabling simulated environment interactions in imagination [19]. Training typically involves three iterative steps: (1) collect data from real environment interactions; (2) train the world model using the collected data; and (3) optimize the policy within the simulated environment produced by the world model.\nDespite the success of existing frameworks in achieving tasks in simplified settings, their application to complex low-level robotic control remains a significant challenge. To address this gap, we propose Robotic World Model (RWM), a novel framework for learning robust world models in partially observable and dynamically complex environments. RWM builds on the core concept of world models but introduces architectural and training innovations that enable reliable long-horizon predictions, even in stochastic and partially observable settings. By incorporating historical context and autoregressive training, RWM addresses challenges such as error accumulation and partially observable and discontinuous dynamics, which are critical in real-world robotics applications."}, {"title": "B. Self-supervised Autoregressive Training", "content": "To address the inherent complexity of partially observable environments, we propose a self-supervised autoregressive training framework as the backbone of RWM. This framework trains the world model po to predict future observations by leveraging both historical observation-action sequences and its own predictions, ensuring robustness over extended rollouts.\nThe input to the world model consists of a sequence of observation-action pairs spanning M historical steps. At each time step t, the model predicts the distribution of the next observation_p(Ot+1 | Ot\u2212M+1:t, at\u2212M+1:t). Predictions are generated autoregressively: at each step, the predicted observation 0+1 is appended to the history and combined with the next action at+1 to serve as input for subsequent predictions. This process is repeated over a prediction horizon of N steps, producing a sequence of future predictions. The predicted observation k steps ahead can thus be written as\nOt+k ~ P\u00a2 (\u00b7 | Ot\u2212M+k:t, Ot+1:t+k-1, at-M+k:t+k\u22121).   (1)\nA similar process is also applied to predict privileged information c, such as contacts, providing an additional learning objective that implicitly embeds critical information for accurate long-term predictions. Such a training scheme introduces the model to the distribution it will encounter at test time, reducing the mismatch between training and inference distributions. Overall, the model is optimized by minimizing the multi-step prediction error:\nL = 1/N \u03a3k=1 \u03b1k[Lo (Ot+k, Ot+k) + Lc (Ct+k,Ct+k)].  (2)\nwhere Lo and Le quantify the discrepancy between predicted and true observations and privileged information, and a denotes a decay factor. This autoregressive training objective encourages the hidden states to encode representations that support accurate and reliable long-horizon predictions.\nTraining data is constructed by sliding a window of size M+N over collected trajectories, providing sufficient historical context for prediction targets. To improve gradient propagation through autoregressive predictions, we apply reparameterization tricks to enable effective end-to-end optimization. By incorporating historical observations, RWM captures unobservable dynamics, addressing the challenges of partially observable and potentially discontinuous environments. The autoregressive training mitigates error accumulation, a common issue in long-horizon predictions, and eliminates the need for handcrafted representations or domain-specific inductive biases, enhancing generalization across diverse tasks. Specifically, teacher-forcing can be viewed as a special case of autoregressive training with forecast horizon N 1, which boosts training with higher parallelization.\nWhile the proposed autoregressive training framework can be applied to any network architecture, RWM utilizes a GRU-based architecture for its ability to maintain long-term historical context while operating on low-dimensional inputs. The network predicts the mean and standard deviation of a Gaussian distribution describing the next observation. Our framework introduces a dual-autoregressive mechanism:\n\u2022 Inner autoregression updates GRU hidden states autore-gressively after each historical step within the context horizon M.\n\u2022 Outer autoregression feeds predicted observations from the forecast horizon N back into the network.\nThis architecture ensures robustness to long-term dependencies and transitions, making RWM suitable for complex robotics applications."}, {"title": "C. Policy Optimization on Learned World Models", "content": "Policy optimization in RWM is conducted using the learned world model, following a framework inspired by Model-Based Policy Optimization (MBPO) [16] and the Dyna algorithm [49]. During imagination, the actions are generated recursively by the policy \u03c0\u03b8 conditioned on the observations predicted by the world model po, which is further conditioned on the previous predictions. The actions at time t + k can thus be written as\nat+k ~ \u03c0\u03bf (\u00b7 | Ot+k), (3)\nwhere of+k is drawn autoregressively from the distribution according to Eq. 1. The approach combines model-based imagination with model-free reinforcement learning to achieve efficient and robust policy optimization."}, {"title": "IV. EXPERIMENTS", "content": "We validate RWM through a comprehensive set of experiments across diverse robotic systems, environments, network architectures. The experiments are designed to assess the accuracy and robustness of RWM, evaluate its architectural and training design choices, and demonstrate its effectiveness across diverse robotic tasks and in real-world deployment combined with MBPO-PPO. We start the analysis by looking into the autoregressive prediction accuracy and robustness of the world model on ANYmal D learned with simulation data induced by a velocity tracking policy. We then compare various network architectures and the error induced across diverse robotic environments and tasks to demonstrate the generality of RWM. And finally, we learn a policy in RWM with the proposed MBPO-PPO and demonstrate the applicability and robustness of the method on an ANYmalD hardware."}, {"title": "A. Autoregressive Trajectory Prediction", "content": "The capability of a world model to maintain high fidelity during autoregressive rollouts is critical for effective planning and policy optimization. To evaluate this aspect, we analyze the autoregressive prediction performance of RWM using trajectories collected from ANYmalD hardware. The control frequency of the robot is at 50 Hz. The model is trained with history horizon M = 32 and forecast horizon N = 8. The autoregressive trajectory predictions by RWM are visualized in Fig. 5.\nAt time t = 32, the world model initiates predictions based on historical observations. From this point onward, future observations are predicted autoregressively using its prior outputs, simulating the deployment scenario in downstream applications. In Fig. 5, the predicted observations are compared against the ground truth trajectories.\nThe results demonstrate that RWM exhibits a remarkable alignment between predicted and ground truth trajectories across all observed variables. This consistency persists over extended rollouts, showcasing the model's ability to mitigate compounding errors a critical challenge in long-horizon predictions. Despite the accumulated shifts at the far end of the prediction, the variables maintain close fidelity to the actual trajectories, with minimal deviations, even in stochastic and dynamically complex environments.\nThe proposed GRU-based autoregressive mechanism demonstrates superior predictive accuracy. The historical context encoded in the hidden states allows RWM to effectively capture partially observable dynamics, a capability evident in the accurate long-term predictions. This performance is further attributed to the dual-autoregressive mechanism introduced in Sec. III-B, which stabilizes predictions despite the short forecast horizon employed during training. This robust predictive capability has profound implications for downstream tasks. Both environments are initialized in the same state and propagated using the same policy to ensure a fair evaluation of the world model's predictive accuracy.\nThe visualization highlights the ability of RWM to maintain consistency in trajectory predictions over long horizons, even beyond the training forecast horizon. This robustness is pivotal for stable policy learning and deployment, as discussed further in Sec. IV-E. The results underscore the capability of RWM to produce accurate long-horizon predictions, validating its applicability to real-world robotic tasks."}, {"title": "B. Robustness under Noise", "content": "A critical challenge in training world models is their ability to generalize under noisy conditions, particularly when predictions rely on autoregressive rollouts. Even small deviations from the training distribution can cascade into untrained regions, causing the model to hallucinate future trajectories. These issues are magnified in autoregressive settings due to the compounding nature of prediction errors. To assess the robustness of RWM, we analyze its performance under Gaussian noise perturbations applied to both observations and actions. We compare the results with an MLP-based baseline also trained autoregressively with the same history and forecast horizon, as shown in Fig. 6, where yellow curves denote the relative prediction error e for RWM, and grey curves represent the MLP baseline.\nThe results indicate a clear advantage of RWM over the MLP baseline across all noise levels. As forecast steps increase, the relative prediction error of the MLP model grows significantly, diverging more rapidly than RWM. In contrast, RWM demonstrates superior stability, maintaining lower prediction errors even under high noise levels. This robustness can be attributed to the dual-autoregressive mechanism introduced in Sec. III-B, which ensures stability in long-horizon predictions. The inner autoregression updates the GRU hidden states during the historical context window, while the outer autoregression feeds back predictions during the forecast horizon. This design minimizes the accumulation of errors by continually refining the state representation toward long-term predictions, even in the presence of noisy inputs."}, {"title": "C. Dual-autoregressive Mechanism", "content": "The choice of history horizon M and forecast horizon N plays a critical role in the training and performance of RWM. The history horizon M governs the amount of historical information the model utilizes to infer the current state and predict future observations. The forecast horizon N defines the extent of supervised forward prediction steps during training. These two hyperparameters directly impact the autoregressive prediction accuracy and training time, as shown in Fig. 7.\nThe heatmap on the left in Fig. 7 shows the relative autoregressive prediction error e under different combinations of M and N. Models trained with a longer history horizon M consistently exhibit lower prediction errors, demonstrating the importance of providing sufficient historical context to capture the underlying dynamics. However, the influence of M plateaus beyond a certain point, indicating diminishing returns for very large history horizons. Forecast horizon N, on the other hand, plays a decisive role in improving long-term prediction accuracy. Increasing N during training leads to better performance in autoregressive rollouts, as it encourages the model to learn representations robust to compounding errors over extended prediction horizons. This improvement comes at the cost of increased training time, as shown in the heatmap on the right. Larger N values require sequential computation during training due to the autoregressive nature of the process, significantly lengthening the training duration.\nInterestingly, when the forecast horizon N = 1 (teacher-forcing), training can be highly parallelized, resulting in minimal training time. However, this setting leads to poor autoregressive performance, as the model lacks exposure to long-horizon prediction during training and fails to effectively handle compounding errors. From the results, an optimal trade-off emerges: moderate values of M and N balance prediction accuracy and training efficiency. For instance, a history horizon of M = 32 and forecast horizon of N = 8 achieve strong autoregressive performance with manageable training time. These settings ensure sufficient historical context while training the model for robust long-term predictions. Overall, the results highlight the critical interplay between history and forecast horizons in autoregressive training. While extending both M and N improves accuracy, practical considerations of computational cost necessitate careful tuning of these hyperparameters to achieve optimal performance."}, {"title": "D. Generality across Robotic Environments", "content": "To assess the generality and robustness of RWM across a diverse range of robotic environments, we compare its performance with several baseline methods, including MLP, recurrent state-space model (RSSM) [18, 33, 14, 34], and transformer-based architectures [48, 50]. These baselines represent widely adopted approaches in dynamics modeling and policy optimization. All models are given the same context during training and evaluation. The results highlight the superiority of RWM trained with autoregressive training (RWM-AR), which consistently achieves the lowest prediction errors across all environments. The performance gap between RWM-AR and the baselines is especially pronounced in complex and dynamic tasks, such as velocity tracking for legged robots, where accurate long-horizon predictions are critical for effective control. The comparison also reveals that RWM-AR significantly outperforms its teacher-forcing counterpart (RWM-TF), underscoring the importance of autoregressive training in mitigating compounding prediction errors over long rollouts. Note that the baselines are trained using teacher forcing as they are traditionally implemented. However, the proposed autoregressive training framework is architecture-agnostic and can also be applied to baseline models. When trained with autoregressive training, RSSM achieves a performance comparable to the proposed GRU-based architecture. Nevertheless, we opt for the GRU-based model due to its simplicity and computational efficiency. On the other hand, training transformer architectures with autoregressive training does not scale effectively, as the multi-step gradient propagation in autoregressive forecasting leads to GPU memory constraints, limiting their practicality for this approach. These results demonstrate that RWM, when combined with autoregressive training, achieves robust and generalizable performance across diverse robotic tasks. Its ability to consistently outperform baseline architectures highlights the effectiveness of its design in addressing the challenges of long-horizon prediction and error accumulation."}, {"title": "E. Policy Learning and Hardware Transfer", "content": "Using MBPO-PPO, we train a goal-conditioned velocity tracking policy for ANYmal D leveraging RWM. At the beginning of each imagination rollout, a target velocity is sampled alongside the initial observation retrieved from the replay buffer D. The policy selects an action based on the sampled observation and velocity command. RWM then predicts the next observation based on the observation-action pair, and the reward is computed from this transition. This process continues recursively over the rollout horizon, with the collected imagination trajectories being used to update the policy. After each iteration, the updated policy interacts with the real environment, collecting additional data for the replay buffer, which is subsequently used to refine RWM.\nNote that learning policies with long imagination rollouts from a learned world model is especially challenging due to the inherent compounding errors in the autoregressive nature of the predictions arising from two key factors: The world model's reliance on previous predictions to infer future observations, which introduces drift over time. And the policy's dependence on predicted observations to compute actions, amplifying errors in the generated trajectories."}, {"title": "V. LIMITATIONS", "content": "The policy learned with RWM surpasses existing MBRL methods in both robustness and generalization. However, it still falls short of the performance achieved by well-tuned model-free RL methods trained on high-fidelity simulators. Model-free RL, being a more mature and extensively optimized paradigm, excels in settings where unlimited interaction with near-perfect simulators is possible. In contrast, the strengths of MBRL are more pronounced in scenarios where accurate or efficient simulation is infeasible, making it an indispensable tool for enabling intelligent agents to eventually learn and adapt in complex, real-world environments.\nIn this work, the world model is pre-trained using simulation data prior to policy optimization, reducing instability during training. However, training from scratch remains challenging as policies can exploit model inaccuracies during exploration, leading to inefficiency and instability. In addition, the need for additional interaction with the environment to fine-tune the world model highlights areas for further refinement. Nevertheless, enabling safe and effective online learning directly on hardware remains challenging. Current training in simulation avoids potential hardware damage, but incorporating safety constraints and robust uncertainty estimates will be critical for deploying RWM and MBPO-PPO in real-world, lifelong learning scenarios. These limitations underscore the trade-offs inherent in MBRL frameworks, balancing data efficiency, safety, and performance while addressing the complexities of real-world robotic systems."}, {"title": "VI. CONCLUSION", "content": "In this work, we present RWM, a robust and scalable framework for learning world models tailored to complex robotic tasks. Leveraging a dual-autoregressive mechanism, RWM effectively addresses key challenges such as compounding errors, partial observability, and stochastic dynamics. By incorporating historical context and self-supervised training over long prediction horizons, RWM achieves superior accuracy and robustness without relying on domain-specific inductive biases, enabling generalization across diverse tasks.\nThrough extensive experiments, we demonstrate that RWM consistently outperforms state-of-the-art approaches like RSSM and transformer-based architectures in autoregressive prediction accuracy across diverse robotic environments. Its ability to maintain stability under noise and dynamically complex scenarios further underscores its robustness and applicability to real-world tasks.\nBuilding on RWM, we propose MBPO-PPO, a policy optimization framework that leverages long world model rollout fidelity. Policies trained using MBPO-PPO demonstrate superior performance in simulation and transfer seamlessly to hardware, as evidenced by zero-shot deployment on the ANYmal D robot. Unlike baseline methods such as SHAC and Dreamer, which struggle with unstable dynamics and compounding errors, MBPO-PPO reliably optimizes policies that generalize effectively to physical systems.\nThis work advances the field of model-based reinforcement learning by providing a generalizable, efficient, and scalable framework for learning and deploying world models. The results highlight RWM 's potential to enable adaptive, robust, and high-performing robotic systems, setting a foundation for broader adoption of model-based approaches in real-world applications."}, {"title": "APPENDIX", "content": "A. Task Representation \n1) Observation and action spaces: The observation space for the ANYmal world model is composed of base linear and angular velocities v, w in the robot frame, measurement of the gravity vector in the robot frame g, joint positions q, velocities q and torques \u03c4 as in Table S1.\nThe privileged information is used to provide an additional learning objective that implicitly embeds critical information for accurate long-term predictions. The space is composed of knee and foot contacts as in Table S2.\nThe action space is composed of joint position targets as in Table S3.\nThe observation space for the ANYmal velocity tracking policy is composed of base linear and angular velocities v, w in the robot frame, measurement of the gravity vector in the robot frame g, velocity command c, joint positions q and velocities q as in Table S4.\n2) Reward functions: The total reward is sum of the following terms with weights detailed in Table S5.\nLinear velocity tracking x, y \nr_vxy = w_vxy * e^(-||c_vxy - v_xy||^2/\u03c3_vxy)\nwhere \u03c3\u03c5\u03b1\u03bd = 0.25 denotes a temperature factor, Czy and Uxy denote the commanded and current base linear velocity.\nAngular velocity tracking\nr_wz = w_wz * e^(-||c_z - w_z||^2/\u03c3_wz),\nwhere \u03c3\u03c9\u03c0 = 0.25 denotes a temperature factor, cz and wz denote the commanded and current base angular velocity.\nLinear velocity z \nr_vz = w_vz * ||v_z||^2,\nwhere vz denotes the base vertical velocity.\nAngular velocity x, y \nr_wxy = w_wxy * ||w_xy||^2,\nwhere Wxy denotes the current base roll and pitch velocity.\nJoint torque \nr_\u03c4 = w_g * ||\u03c4||^2,\nwhere \u03c4 denotes the joint torques.\nJoint acceleration \nr_\u00e4 = w_\u00e4 * ||\u00e4||^2,\nwhere \u00e4 denotes the joint acceleration.\nAction rate \nr_a = w_a * ||a' - a||^2,\nwhere a' and a denote the previous and current actions.\nFeet air time \nr_fa = w_fa * t_fa,\nwhere tfa denotes the sum of the time for which the feet are in the air.\nUndesired contacts \nr_c = w_c * c_u,\nwhere cu denotes the counts of the undesired knee contacts.\nFlat orientation \nr_g = w_g * g_xy\nwhere gxy denotes the xy-components of the projected gravity."}, {"title": "B. Network Architecture", "content": "1) RWM: The robotic world model consists of a GRU base and MLP heads predicting the mean and standard deviation of the next observation and privileged information such as contacts, as detailed in Table S6.\n2) Baselines: The network architectures of the baselines are detailed in Table S7.\n3) MBPO-PPO: The network architectures of the policy and the value function used in MBPO-PPO are detailed in Table S8."}, {"title": "C. Training Parameters", "content": "The learning networks and algorithm are implemented in PyTorch 2.4.0 with CUDA 12.6 and trained on an NVIDIA RTX 4090 GPU.\n1) RWM: The training information of RWM is summarized in Table S9.\n2) MBPO-PPO: The training information of MBPO-PPO is summarized in Table S10."}]}