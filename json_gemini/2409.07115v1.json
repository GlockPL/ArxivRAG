{"title": "ATTENTION DOWN-SAMPLING TRANSFORMER, RELATIVE RANKING AND SELF-CONSISTENCY FOR BLIND IMAGE QUALITY ASSESSMENT", "authors": ["Mohammed Alsaafin", "Musab Alsheikh", "Saeed Anwar", "Muhammad Usman"], "abstract": "The no-reference image quality assessment is a challeng-ing domain that addresses estimating image quality withoutthe original reference. We introduce an improved mecha-nism to extract local and non-local information from im-ages via different transformer encoders and CNNs. Theutilization of Transformer encoders aims to mitigate local-ity bias and generate a non-local representation by sequen-tially processing CNN features, which inherently capturelocal visual structures. Establishing a stronger connectionbetween subjective and objective assessments is achievedthrough sorting within batches of images based on rela-tive distance information. A self-consistency approach toself-supervision is presented, explicitly addressing the degra-dation of no-reference image quality assessment (NR-IQA)models under equivariant transformations. Our approach en-sures model robustness by maintaining consistency betweenan image and its horizontally flipped equivalent. Throughempirical evaluation of five popular image quality assess-ment datasets, the proposed model outperforms alternativealgorithms in the context of no-reference image quality as-sessment datasets, especially on smaller datasets.", "sections": [{"title": "1. INTRODUCTION", "content": "Understanding image quality is essential for many applications; however, it may be difficult since we periodically needan ideal reference image. We can address this issue usingNon-Reference Image Quality Assessment (NR-IQA). Theobjective is to develop techniques that can independentlyassess the image's quality without needing the original im-age. The importance of NR-IQA arises from its wide rangeof applications, including surveillance systems [1], medicalimaging [2], content delivery networks [3], image & videocompression [4], etc. It is vital in these domains to assessquality without the original reference image. NR-IQA advances imaging technology and improves user experience.Existing NR-IQA methods focus on developing novel algo-rithms to handle the problem of evaluating image quality. TestTime Adaptation technique for Image Quality Assessment(TTAIQA) [5], Quality-aware Pre-Trained (QPT) [6] modelsthrough self-supervised learning, the Language-Image Qual-ity Evaluator (LIQE), the data-efficient image quality trans-former (DEIQT) [7] represents strides in this field and manymethods that leverage CNNs. However, shortcomings persist,particularly the limitation imposed by the scarcity of labeleddata, hindering the effectiveness of deep learning models andcapturing only local features via CNNs while disregarding thenonlocal features of the image that transformers can capture.Popular datasets like the largest NR-IQA dataset, FLIVE,fall short compared to those in other domains, impeding therobust training of NR-IQA models.\nOur main contribution is to develop an enhanced NR-IQAmodel to elevate its performance based on established metricsby leveraging the transformer architecture to capture nonlo-cal features and CNNs to capture local features. We seekto assess the performance of our improved model againstexisting NR-IQA methods. We test our methods using themost popular image quality datasets like LIVE, TID2013,CSIQ, LIVE-C and KonIQ10K. We'll utilize metrics likeSpearman's Rank-Order Correlation Coefficient (SRCC) andPearson's Linear Correlation Coefficient (PLCC) to calculatehow well our model works. The main goal of this study isto get better-performing models using famous performancemetrics."}, {"title": "2. RELATED WORKS", "content": "Image quality assessment, or IQA, is an integral part of computer vision and image processing and is widely utilized in social networking, online content sharing, and photography.This review is divided into two parts. The first discusses the recently released papers and focuses on developing creative methods for assessing NR-IRQA. The second part focuseson papers that use convolutional neural networks (CNNs) andtransformers for quality assessment.\nZhao et al. [6] provide a unique method for blind im-age assessment that uses self-supervised learning to get overthe absence of labeled data. They propose a customized pre-text task and a quality-aware contrastive loss, expanding im-age distortion simulations to enhance NR-IQA. Their Quality-conscious Pre-trained models beat current techniques on sev-eral BIQA benchmark datasets, demonstrating increased sen-sitivity for image quality. Distribution changes between train-ing and testing situations in blind IQA significantly impactinference performance. Based on this, Subhadeep et al. [5]propose test-time adaptation by incorporating quality-relevantauxiliary activities at the batch and sample levels. Employingeven a small batch of test data substantially improves modelperformance, surpassing existing SOTA approaches.\nA multitask approach [7] for blind image quality evalu-ation, leveraging auxiliary information from other tasks andautomating model parameter sharing and weighting for lossfunctions, benefiting from scene categorization and distortiontype identification tasks. The mentioned method outperformsexisting approaches across multiple IQA datasets, enhancingresilience and aligning better with quality annotations. It alsooffers insights into the creation of next-generation NR-IQAmodels.\nRecently, a distinctive Mixture of Expert [8] introducesblind image quality assessment. This approach trains two sep-arate encoders in an unsupervised setting to capture high-levelcontent and low-level image characteristics. A linear regres-sion model is developed to evaluate image quality by leverag-ing the synergy between these features. The authors showcasetheir technique's superiority over others across various exten-sive IQA databases, adeptly handling genuine and syntheticdistortions. They highlight the significant impact of content-aware image representations, especially in an unsupervisedcontext, on enhancing non-reference IQA performance.\nThe introduction of transformers [9] as novel networkarchitecture defines a novel attention mechanism to drawglobal dependencies between I/O while eliminating the needfor recurrence and convolutions. Furthermore, You et al. [10]were among the first papers to explore the transformer appli-cation in Image Quality (TRIQ) assessment. Building uponthe original Transformer encoder in Vision Transformer, theypresented a shallow Transformer encoder atop a convolu-tional neural network (CNN)-extracted feature map. Theirarchitecture accommodates images of varying resolutions,employing adaptive positional embedding. They systemati-cally evaluated different Transformer configurations acrossmultiple publicly available image quality databases. Their re-search outcomes highlight the efficacy of the proposed TRIQframework.\nFor blind image quality assessment, Zhang et al. [11] pre-sented a unique deep bilinear model that can handle real andartificial distortions. Two CNNs designed for various distor-tion conditions make up their model. One CNN is pre-trainedon a sizable dataset for visual distortion classification to han-dle synthetic distortions, using a previously trained CNN toclassify images for real distortions. The two CNNs' featureswere bilinearly pooled to provide a cohesive interpretation forimage quality estimate. The performance of the entire modelis improved by fine-tuning it on target subject-rated datasetsusing a variation of stochastic gradient descent. Numeroustests confirm the model's exceptional performance on artifi-cial and real image databases to approach the issue of no ref-erence IQA as a learning-to-rank problem in which trainingutilizes the ranking data.\nCNNs are good at translating images but struggle with ro-tations. H-Nets [12], an innovative solution to the problem ofimagining translation and rotation impacting computer visiontasks differently, can accomplish 360-degree rotation equiv-ariance and patch-wise translation using circular harmonicsinstead of standard CNN filters. Each receptive field patchreceives the maximum responsiveness and orientation fromthis special construction. Deep feature mappings inside thenetwork may encode complicated rotational invariants thanksto H-Nets' parameter-efficient representation with constantcomputational complexity. Their approach is easily incorpo-rated into contemporary systems such as batch normalizationand deep supervision. H-Nets compete well on benchmarkproblems and produce cutting-edge classification scores onrotated-MNIST, demonstrating their effectiveness. Their re-search indicates that the data remains vulnerable to equiv-ariant transformations even with various augmentation tech-niques to increase CNN generalization.\nThe field of blind image quality assessment has seen sig-nificant advancements recently, where each article addressescritical challenges in unique ways. The approaches lever-age self-supervised learning, test-time adaptation, multitasklearning, and unsupervised feature extraction to improve theprecision and resilience of IQA models, making them bettersuited for real-world applications. Collectively, many stud-ies contribute to the ongoing development of SOTA NR-IQAmodels, benefiting a wide range of industries and applica-tions. Our primary goal in this paper centers around devel-oping an advanced NR-IQA model that leverages transform-ers and CNNs to assess the quality of images with the pri-"}, {"title": "3. METHODOLOGY", "content": "In the proposed Attention Down-Sampling Transformer, Rel-ative ranking and self-consistency abbreviated as ADTRSmodel for NR-IQA, we adopt the relative ranking and self-consistency mechanisms inspired by TRES [13] but employ acompletely different transformer architecture to evaluate thequality of images without reference standards. Our methodbegins with an input image from which a series of CNN layersextract crucial features representing varying complexities andscales. These features are then normalized and subjected todropout to ensure the model's generalizability across diverseimage sets.\nAs depicted in Figure 2, the workflow progresses by con-catenating extracted features to create a cohesive feature set.The Transformer encoder employs self-attention to empha-size essential data elements, leveraging attention mechanisms toprioritize relevant information. Subsequently, the en-coder's output is aggregated through a fully connected layer,facilitating dimensionality reduction and regression analysis.By incorporating self-consistency mechanisms, the modelensures the reliability of its predictions. In the final stage ofthe ADTRS model, it produces both absolute quality scoresand relative rankings, enabling comprehensive image qualityassessment and facilitating meaningful comparisons."}, {"title": "3.1. Feature Extraction", "content": "For an input image $I$ defined in the space $R^{3 \\times m \\times n}$ withdimensions $m$ and $n$ symbolizing the width and height, re-spectively, the objective is to evaluate its perceptual QS.A CNN, represented by $f_{\\theta}$ with learnable parameters $\\theta$,is utilized to extract features $F_i$ from the $i^{th}$ block, where$F_i \\in R^{b \\times c_i \\times m_i \\times n_i}$ captures the feature maps with batch size$b$, and $c_i, m_i$, and $n_i$ denote the dimensions of the channels,width, and height of the extracted features, correspondingly.\nIn neural network architectures, specifically in the contextof deep learning and CNNs, a series of pre-processing stepsare often applied to the extracted features from different lay-ers. These steps include normalization, pooling, and dropout,each serving distinct purposes to enhance the network's per-formance and generalization. Normalization is employed toaddress variations in feature scales among different layers.Standardizing the features to have zero mean and unit vari-ance or scaling them to a specific range ensures that they con-tribute more uniformly to the learning process. This step iscrucial because features with different scales might dominatethe learning, hindering the network's ability to converge ef-fectively.\nPooling layers play a pivotal role in down-sampling thespatial dimensions of feature maps. By selecting the maxi-mum or average values within specific regions, pooling re-duces the computational complexity of the network, makingit more efficient. Additionally, pooling contributes to thenetwork's robustness by detecting invariant features and han-dling spatial variations in the input data. As a regularizationtechnique, dropout addresses the risk of over-fitting by ran-domly deactivating a fraction of neurons during training. Bypreventing the network from relying too heavily on specificneurons, dropout encourages learning more robust featuresand improves the model's ability to generalize to unseen data.During testing, all neurons are reinstated to ensure the full uti-lization of the trained network. These pre-processing steps:\nnormalization, pooling, and dropout-collectively constitutea comprehensive strategy for enhancing the efficiency, gen-eralization, and robustness of neural networks, particularly inthe complex tasks associated with deep learning architectures.\n(Eq. 1) is used to normalize the feature vector $F_i$ using theEuclidean norm. The $L_2$ pooling is defined by (Eq. 2).\n$F_i = \\frac{F_i}{\\max(\\lVert F_i \\rVert_2, \\epsilon)}$\n$\\mathcal{P}(x) = \\sqrt{g * (x \\otimes x)}$\nwhere $\\otimes$ denotes the point-wise product, and the blurring kernel $g(\\cdot)$ is implemented via a Hamming window that approximately applies the Nyquist criterion.\nThe extracted features will be concatenated after goingthrough the normalization, pooling and dropout layers."}, {"title": "3.2. Transformer Encoder", "content": "We adopt the encoder architecture from the literature toprocess multi-scale features $F_i$ from the CNN [14]. Thesefeatures are sequenced and fed into the Transformer en-coder [15], where a multi-head, multi-layer self-attentionmechanism, depicted in Figure 2, is employed to model thedependencies across the feature maps. The Transformer's ar-chitecture can learn complicated feature relationships withoutany built-in inductive bias.\nAt the heart of our model lies the Transformer EncoderLayer paired with the Self-attention mechanism, essential foranalyzing image characteristics through sophisticated spatialrecognition. Initially, the model processes image features thatdistill the essence of the visual input and integrates positionalencoding to inject spatial context into these features. Follow-ing this, the multi-head self-attention framework comes intoplay, dissecting and assessing input segments by generatingattention scores through queries, keys, and values, all createdvia adaptive linear transformations.\nAfter the self-attention phase, the outputs are fused andnormalized in the Add & Norm step, a measure that stabilizes the learning process and embeds residual connections, vital for the architecture's depth and efficacy. A subsequent Feed-Forward Network (FFN) executes additional linear transfor-mations, punctuated by ReLU activation, to polish the fea-ture set further. The depth of this encoding process is rep-resented by 'Nx', reflecting the iterations of the transforma-tion sequence. The depicted data flow in Figure 3, especially the loopback arrows, emphasizes the Transformer's residual connections, ensuring a seamless and continuous informationstream within the model's architecture.\nThe self-attention mechanism within the Transformer model, as depicted in Figure 4, commences with dual linear blocks that reformulate the input data into intermediate states, integral for the ensuing attention calculations. This mechanism further employs the SoftMax function to normalize attention scores calculated from the dot products of queries and keys. This normalization facilitates a targeted distribution of attention across different data segments. Following this, the output from the SoftMax stage is combined with the outputs of a third linear block, symbolizing information integration within the attention process. This synthesis is not terminal but instead feeds back iteratively into the SoftMax stage, highlighting the recursive nature of the self-attention mechanism. This iterative loop, crucial for refining attention over successive cycles, is visually represented in Figure 4.\nMulti-Head Attention: The multi-head attention mechanism, pivotal in our model, involves transforming input features into query, key, and value vectors. These vectors are then processed through the attention mechanism as shown in the following equations:\n$MultiHead (Q', K', V') = Concat (h_1, ..., h_h) W^O,$\n$h_i = Attention (Q_i, K_i, V_i),$\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V,$"}, {"title": "3.3. Feature Fusion and Quality Estimation", "content": "We utilize fully connected layers to combine the extracted features from convolutional and self-attention mechanisms. This fusion is instrumental in predicting the image's quality (as visualized in Figure 2). Formulated as follows, the model is trained to minimize the regression loss as:\n$L_{Q, B} = \\frac{1}{N} \\sum_{i=1}^N \\lVert q_i - s_i \\rVert,$\nwhere $q_i$ represents the predicted quality score for the $i^{th}$ im-age, while $s_i$ represents ground truth quality score."}, {"title": "3.4. Relative Ranking Incorporation", "content": "The regression loss effectively handles quality prediction; itoverlooks ranking and correlation among images. We aimto account for the relative ranking within batches, focusingon extreme cases due to computational constraints. In im-age batch B, $q_{a_{max}}, q_{a'_{max}}, q_{a_{min}}$, and $q_{a'_{min}}$ represent pre-dicted qualities for the highest, second highest, lowest, andsecond lowest subjective quality scores, respectively. Utiliz-ing triplet loss with $d(x, y) = |x - y|$, we aim for constraintslike $d(q_{a_{max}}, q_{a'_{max}}) + margin_1 \\leq d(q_{a_{max}}, q_{a_{min}})$. Similarly,we desire $d(q_{a_{min}}, q_{a'_{min}}) + margin_2 \\leq d(q_{a_{max}}, q_{a_{min}})$. Em-pirically selecting margin values is challenging due to datasetvariations. For perfect predictions, $margin_1$ is bounded by$s_{q_{a_{max}}} - s_{q_{a_{min}}}$, serving as an upper-bound during training,where $margin_1 = s_{q_{a_{max}}} - s_{q_{a_{min}}}$, where $s_{q_{a_{max}}}$ signifies thesubjective quality score associated with the image having thepredicted quality score $q_{a_{max}}$. We can do a similar process formargin, which is bounded by $s_{q_{a_{max}}} - s_{q_{a_{min}}}$.\n$L_{RR, B} = L_{triplet}(q_{a_{max}}, q_{a'_{max}},q_{a_{min}}) + L_{triplet}(q_{a_{min}}, q_{a'_{min}},q_{a_{max}})\n= max \\{0, d (q_{a_{max}}, q_{a'_{max}}) - d (q_{a_{max}}, q_{a_{min}} ) + margin_1\\}\n+ max \\{0, d (q_{a_{min}}, q_{a'_{min}}) - d (q_{a_{max}}, q_{a_{min}} ) + margin_2\\}$"}, {"title": "3.5. Self-Consistency Mechanism", "content": "For the last part of the methodology, we advocate leverag-ing the model's uncertainty in both the original input imageand its equivariant transformation during the training pro-cess. To enhance the robustness of the model, we exploitself-consistency by establishing a self-supervisory signal be-tween each image and its equivariant transformation. Fora given input $I$, denote the output logits from the Convo-lutional and Transformer layers as $\\phi_{\\theta,conv}(I)$ and $\\phi_{\\psi,atten} (I)$,respectively, where $\\phi_{\\theta,conv}$ and $\\phi_{\\psi,atten}$ represent the CNN andTransformer with learnable parameters $\\theta$ and $\\psi$, respectively.Our model utilizes these outputs to predict image quality.Given that human subjective scores remain consistent for thehorizontally flipped version of the input image, we anticipate$\\$\\phi_{\\theta,conv} (I) = \\phi_{\\theta,conv} (T(I)) and $\\phi_{\\psi,atten} (I)= $\\phi_{\\psi,atten} (T(I)),where $T$ signifies the horizontal flipping transformation. Con-sequently, by incorporating our consistency loss, the networklearns to fortify its representation learning autonomously,eliminating the need for additional labels or external supervision. We aim to minimize the self-consistency loss\n$L_{SC} = \\lVert \\phi_{\\theta, conv}(I) \u2013 \\phi_{\\theta,conv}(T(I)) \\rVert + \\lVert \\phi_{\\psi,atten} (I) \u2013 \\phi_{\\psi,atten}(T(I)) \\rVert + O_1 \\lVert L_{RR,B} - L_{RR,T(B)} \\rVert,$\nwhere $\\tau(B)$ signifies the equivariant transformation on imagebatch B."}, {"title": "3.6. Composite Loss Function", "content": "The overall training process involves the minimization of acomposite loss function (Eq. 9), which encompasses qualityloss (Eq. 6), relative ranking loss (Eq. 7), and self-consistencyloss (Eq. 8). These losses are balanced by coefficients $O_1, O_2$,and $O_3$ to optimize the training outcome effectively. Our pro-posed model aims to provide a comprehensive and reliableNR-IQA solution by incorporating these elements. The effec-tiveness of this approach is demonstrated in the experimentalresults section.\n$L_{CLF} = L_Q + O_2L_{RR} + O_3L_{SC}$"}, {"title": "4. EXPERIMENTS", "content": "We assess our proposed ADTRS model's performance on fivewidely recognized IQA datasets, displayed in Table 1 (amongthe distortions, three were synthetically generated while twooccurred authentically). We utilize two standard performancemetrics, PLCC and SROCC, among various metrics availablein IQA evaluation. PLCC (Pearson Linear Correlation Coef-ficient) evaluates the correlation between algorithmic resultsand human eye subjective scores, reflecting the algorithm'saccuracy. On the other hand, SROCC (Spearman Rank-Ordered Correlation Coefficient) measures the monotonicityof the algorithm's predictions. Both metrics range from 0 to1, with higher values indicating superior performance.\nImplementation Details: We used an NVIDIA RTX2060 GPU and PyTorch to train our model for training andtesting. We augmented the horizontal and vertical dimensions of 138 randomly chosen patches, each measuring 224 by 224 pixels, from each image, by accepted IQA training protocols. The quality scores of the original image were carried over tothese patches. With a weight decay of $5 \\times 10^{-4}$ across a maximum of five epochs, we trained by minimizing the composite loss function over the training set, changing the learning rate from $2 \\times 10^{-5}$ and decreasing it by a factor of 5 after each epoch. A total of 138 patches, each measuring 224 $224$, were chosen randomly from the test picture during testing, and the final quality score was calculated by averaging their projected values. ResNet50 [28] served as the CNN backbone for our model, which was seeded using Imagenet weights. We set the hyperparameters $O_1, O_2$, and $O_3$ to 0.5, 0.05, and 1 ac- cordingly, using the Transformer architecture with 4 encoder layers, a hidden layer dimensionality of 64 ($d = 16$), and"}, {"title": "5. CONCLUSION", "content": "Our study presents an enhanced NR-IQA algorithm that efficiently merges CNNs and Transformer features, exploiting both local and non-local image characteristics for a comprehensive representation. We have incorporated a relative ranking loss function to capture essential ranking information among images, thereby augmenting the discriminative power of our model. By utilizing equivariant image transformations for self-supervision, we have bolstered the robustness of our approach. The performance of our method across five distinct IQA datasets underscores its robustness and adaptability. Our proposed algorithm outperforms all other algorithms on smaller and synthetic datasets while performing exceptionally well on larger datasets compared to different state-of-the-art algorithms. The results unequivocally establish the robustness and precision of our proposed method compared to the TRES model in accurately assessing image quality, highlighting its significant potential for diverse applications in image analysis and assessment."}]}