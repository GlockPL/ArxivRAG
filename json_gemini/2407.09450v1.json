{"title": "HUMAN-LIKE EPISODIC MEMORY FOR INFINITE CONTEXT LLMS", "authors": ["Zafeirios Fountas", "Martin A Benfeghoul", "Adnan Oomerjee", "Fenia Christopoulou", "Gerasimos Lampouras", "Haitham Bou-Ammar", "Jun Wang"], "abstract": "Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs, enabling them to effectively handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an on-line fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient and human-like access to relevant information. Experiments on the LongBench dataset demonstrate EM-LLM's superior performance, outperforming the state-of-the-art InfLLM model with an overall relative improvement of 4.3% across various tasks, including a 33% improvement on the PassageRetrieval task. Furthermore, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting a bridge between this artificial system and its biological counterpart. This work not only advances LLM capabilities in processing extended contexts but also provides a computational framework for exploring human memory mechanisms, opening new avenues for interdisciplinary research in AI and cognitive science.", "sections": [{"title": "1 INTRODUCTION", "content": "For contemporary pre-trained large language models (LLMs), the context window serves as the primary mechanism to incorporate domain-specific, private, or common up-to-date information. However, despite their remarkable and ever-expanding capabilities, LLMs still exhibit significant limitations when tasked with processing extensive contexts (Liu et al., 2024a). These limitations stem from inherent challenges in Transformer-based architectures. Recent studies have shown that Transformers struggle with extrapolating to contexts longer than their training window size (Kazemnejad et al., 2024). On top of this, employing softmax attention over extended token sequences requires substantial computational resources for each token generation, and the resulting attention embeddings risk becoming excessively noisy and losing their distinctiveness (Tworkowski et al., 2023).\nTo mitigate those challenges, recent works have focused on retrieval-based methods, either in the form of in-context augmentation (e.g., RAG-based techniques (Lewis et al., 2020; Gao et al., 2024)) or via retrieval of previously-inferred key-value pairs (KV) within individual attention heads (Wu et al., 2022; Tworkowski et al., 2023; Bertsch et al., 2023). Notably, state-of-the-art performance is achieved when KV pairs are initially organised into non-overlapping segments and then retrieved together as one block of sequential tokens (Xiao et al., 2024a). While such techniques present interesting avenues of research, results still indicate a significant gap between the performance of LLMs in short- vs long-context tasks, even when existing long-context architectures are employed (Liu et al., 2024a).\nThis work tackles the above challenges and attempts to bridge this performance gap by taking inspiration from the algorithmic interpretation of episodic memory in the human brain \u2013 the memory system responsible for encoding, storing, and retrieving personal experiences and events. The brain makes sense of its continuous experience in the real world by segmenting it into discrete episodic events (Clewett et al., 2019; Zacks, 2020), which are organised in a"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 LONG-CONTEXT IN LLMS", "content": "Recently, several approaches have been proposed to extend the context window of Transformer-based models. Those include methods that address the limited representational capacity of softmax attention, and its quadratic computational and memory cost (Katharopoulos et al., 2020; Munkhdalai et al., 2024). Other methods target the poor extrapolation of typical positional encodings to out-of-distribution (OOD) context lengths (Kazemnejad et al., 2024). The latter is evident in most widely used methods, including the original absolute positional encodings (Vaswani et al., 2017) and the more recent relative positional encodings, such as the popular Rotary Positional Embeddings (RoPE) (Su et al., 2024). To address this, some approaches propose scaling of the rotation angles (Chen et al., 2024a) or the base constant (Xiong et al., 2023; Liu et al., 2024b; Peng et al., 2024; Ding et al., 2024). Others, scale positions without affecting the embedding function (Press et al., 2021; Chen et al., 2023; Jin et al., 2024), exploring alternative strategies such as KERPLE (Chi et al., 2022) and FIRE (Li et al., 2024) or adopt mechanisms from certain LMs like T5 (Raffel et al., 2020).\nConcerning computational efficiency and diluted attention, successful approaches propose methods for general improve- ments to the efficiency of Transformers through optimised computations (Dao, 2024; Han et al., 2024a; Aminabadi et al., 2022; Kwon et al., 2023; Liu et al., 2024c; Brandon et al., 2023) or compression techniques (Nawrot et al., 2024; Zhang et al., 2023), as well as training methods tailored for long-context scenarios (Zhu et al., 2024; Chen et al., 2024b)."}, {"title": "2.2 NEURAL MODELS OF EPISODIC MEMORY AND EVENT COGNITION", "content": "The concept of episodic memory, central to our approach, has been extensively studied in both theoretical neuroscience and machine learning. Neural models of episodic memory capture human behaviour and neuroimaging data, providing insights into how the brain processes and stores experiences and suggesting links between memory, efficient representa- tions and navigation of physical and conceptual spaces (Gershman et al., 2012; Benna and Fusi, 2021). In machine learning, episodic memory-inspired approaches have yielded significant improvements across various domains. For instance, episodic control has enhanced reinforcement learning agents' performance and learning speed (Blundell et al., 2016; Pritzel et al., 2017; Coda-Forno et al., 2024). In addition, models of memory construction and consolidation have been successful in alleviating catastrophic forgetting in neural networks (Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2019; Buzzega et al., 2020; Prabhu et al., 2020), including LLMs (Das et al., 2024), and appear to explain key features of human memory, such as imagination and future thinking (Spens and Burgess, 2024).\nThese models have revealed key aspects of episodic memory, particularly in describing how experiences are segmented into events, and when new memories are encoded and retrieved (Lu et al., 2022). Surprise plays a critical role in this process, triggering event boundaries and memory formation (Fountas et al., 2022; Kumar et al., 2023). This event-based structure is deeply intertwined with our perception of time (Roseboom et al., 2019; Sherman et al., 2022), highlighting the interdependence of memory and temporal cognition. This insight has helped generative models for video (Zakharov et al., 2022a;b) and reinforcement learning (Zakharov et al., 2021) to capture temporal dynamics more accurately. In terms of memory retrieval, studies in human free recall have shown a distinctive increased likelihood of retrieving items encoded close together in time (temporal contiguity) and in succession (temporal asymmetry) Recently, it was shown that attention heads in transformer-based LLMs that are associated with in-context learning, already exhibit the same dynamic retrieval behaviour (Ji-An et al., 2024) including both contiguity and asymmetry effects. Therefore, transformers have the inherent ability to act as episodic memory retrieval models, if provided with the right information within their context window. Our work leverages these concepts of surprise-based event segmentation and LLMs' inherent temporal contiguity and asymmetry effects to enable a new generation of Infinite Context-Length LLMs, capable of processing and understanding information over vastly extended timescales."}, {"title": "3 EM-LLM: LLM WITH EPISODIC MEMORY", "content": ""}, {"title": "3.1 ARCHITECTURE", "content": "EM-LLM is designed to be applied directly to pre-trained LLMs, enabling them to handle context lengths significantly larger than their original training length. Our architecture divides the context into three distinct groups: initial tokens, evicted tokens, and local context. This structure, while incorporating insights from recent work on token block retrieval (Xiao et al., 2024a), introduces novel elements inspired by human episodic memory.\nThe local context represents the most recent tokens, maximising information about the current task, and fits within the typical context window of the underlying LLM. This group utilises full softmax attention and plays a role similar to the focus of attention in cognitive models of working memory, holding the most immediately relevant information for the current task (Cowan, 2001). The evicted tokens typically comprise the majority of past tokens in a long-context scenario, extending far beyond the LLM's original training length. These tokens are managed by our proposed memory model functioning similarly to short-term episodic memory in the brain. Finally, following previous work, we also maintain a group of 128 initial tokens in the LLM context. These act as attention sinks and help recover the performance of window attention, as first observed by Xiao et al. (2024b); Han et al. (2024b) and later adopted by Xiao et al. (2024a). For retrieved tokens, which are therefore discontinuous and outside the local context, we assign a fixed position embedding as in Raffel et al. (2020); Xiao et al. (2024a). This architecture enables EM-LLM to effectively process and utilise information from positions outside its pre-trained local context window, while maintaining the underlying LLM's performance characteristics."}, {"title": "3.2 \u039c\u0395\u039cORY FORMATION VIA SURPRISE", "content": "In the context of LLMs, we define episodic memory as the organised, event-based collection of past key-value pairs, analogous to the latent representations of personal experiences in human memory. Just as unexpected or novel information plays a crucial role in human memory formation, we posit that analogous indicators of novelty in LLMs can serve as an effective proxy for identifying significant \u201cevents\u201d within the model's experience. In Bayesian terms, surprise is quantified by the negative log-likelihood of observing the current, ground-truth token given the previous tokens in an auto-regressive model, with high values indicating the unpredictability or novelty of each new token within the context according to the model, i.e., it is \"surprised\" by the next token. Following work on cognitive"}, {"title": "3.3 BOUNDARY REFINEMENT", "content": "While surprise-based segmentation provides an effective initial estimate of event boundaries, we make the key observa- tion that the utility of elements within an event during memory recall depends on their likelihood of being utilised by the current query. Therefore, we theorise that memory recall will be most efficient with high intra-event similarity between keys while maintaining low inter-event similarity. For instance, see the similarity of groups in Fig. 1. To further ensure this, we introduce a boundary refinement step which looks to optimise this objective. Such an objective is typically optimised in the context of graph-clustering, hence we will express this refinement process in a graph-theoretic manner. To achieve this, we treat the similarity matrix between all keys of an attention head h within the local context window for tokens \\(x_1, x_2, ..., x_n\\) as an adjacency matrix. We define the adjacency matrix \\(A^h\\) as\n\\[A^h_{ij} = \\text{sim}(K^h_i, K^h_j),\\]\nwhere \\(K^h_i\\) and \\(K^h_j\\) are the key vectors corresponding to tokens \\(x_i\\) and \\(x_j\\), respectively. The similarity function measures the closeness of two key vectors; in our implementation, we use dot product similarity \\(K^h_i \\cdot K^h_j\\) due to its effectiveness in capturing semantic relationships in high-dimensional spaces (Vaswani et al., 2017) and to align with the mechanism of self-attention in Transformers.\nTo evaluate the quality of potential boundaries, we define a metric function \\(f(A, B) : \\mathbb{R}^{n \\times n} \\times \\{1, ..., n\\}^k \\rightarrow \\mathbb{R}\\). This function quantifies the cohesion within events and separation between events based on the graph structure represented by the similarity matrix A and event boundaries B. We experimented with two widely-accepted graph-clustering metrics: modularity and conductance (Miasnikof et al., 2018). Modularity (Newman and Girvan, 2004) provides a measure of the quality of a particular division of a network into communities, with higher values indicating higher edge density in the identified cluster when compared to the density of edges expected in a random cluster. As our edge weights represent the similarity between two tokens, we seek to maximise this metric. Modularity is defined as:\n\\[f_M(A^h, B) = \\frac{1}{4m} \\sum_{i,j} \\bigg[A^h_{ij} - \\frac{\\sum_j A^h_{ij} \\sum_i A^h_{ij}}{2m} \\bigg] \\delta(c_i, c_j)\\]\nwhere m is the total edge weight in the graph, \\(c_i\\) is the community (episodic event) to which node i is assigned, and \\(\\delta\\) is the Kronecker delta function. Conductance, on the other hand, measures the fraction of total weighted edges cut by a given community boundary, and is defined as:\n\\[f_C(A^h, B) = \\min_{S \\subset V} \\frac{\\sum_{i \\in S, j \\notin S} A^h_{ij}}{\\min(\\text{vol}(S), \\text{vol}(V \\setminus S))}, \\text{ with vol}(S) = \\sum_{i \\in S, j \\in S} A^h_{ij}, \\text{vol}(V \\setminus S) = \\sum_{i \\notin S, j \\notin S} A^h_{ij}\\]\nwhere \\(S = \\{b_i, b_i + 1, ..., b_{i+1}\\}\\) is a subset of all nodes \\(V = \\{b_1, b_1 + 1, ..., b_k\\}\\) in the induced graph, with \\(b_i \\in B\\). Lower conductance values indicate better community structure. Our boundary refinement algorithm iteratively adjusts the initial surprise-based boundaries to optimise these metric functions. While our best results were achieved using modularity, we also include comparisons with conductance-based boundary refinement to provide a comprehensive analysis. The overall process can be summarized in Algorithm 1.\nThis algorithm first identifies initial boundaries based on the surprise threshold T, then refines these boundaries by finding the optimal position \u00df between each pair of consecutive initial boundaries (\u03b1, \u03b2) that optimises the chosen"}, {"title": "3.4 \u039c\u0395\u039cORY RETRIEVAL", "content": "When inferring a new token, a number of episodic events are selected and become a part of the (extended) context window of the underlying LLM. Our memory retrieval process employs a two-stage mechanism to select relevant episodic events for the LLM's context window (Fig.2C). First, we retrieve \\(k_s\\) events using k-NN search based on dot product similarity between the current query and representative tokens of each event. These representatives, selected as per Xiao et al. (2024a), are the most influential tokens within each event. For large memory stores, we utilise approximate k-NN (Douze et al., 2024) to maintain efficiency. These \\(k_s\\) events, retrieved based on their similarity to the current query, form a part of the LLM's context window that we refer to as the similarity buffer.\nThe second stage of our retrieval process introduces another buffer, which we refer to as the contiguity buffer, designed to maintain temporal context. Implemented as a queue of size \\(k_c\\), this buffer promotes temporal relationships in retrieval. When an event is retrieved, we also enqueue its neighboring events (within \\(\\pm n\\) positions in the original sequence) into this buffer. This mechanism enables the LLM's \u201cinduction\u201d attention heads to exhibit the contiguity and asymmetry effects discussed in Section 2.2. The queue structure allows for a natural decay of temporal context as new events are processed, with older or repeated events being dequeued as new ones are added. In total, \\(k = k_s + k_c + 2\\) events are added to the context window, striking a balance between relevance and temporal relationships in a manner analogous to human episodic memory retrieval."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 PERFORMANCE OF EM-LLM ON LONG-CONTEXT TASKS", "content": "As previously mentioned, InfLLM is, at the time of writing, considered to achieve state-of-the-art performance on long-context benchmarks (\u221e-Bench, LongBench), as well as being the only method which uses group-based k-NN retrieval in LLMs on such benchmarks. We, therefore, employ this model as our baseline for comparison with our own methods on short context windows (4K+2K, as in Xiao et al., 2024a).\nResults on the LongBench dataset (Table 1) show that our method is able to improve on InfLLM in all but one task, with the best method achieving an overall increase in performance of 1.8 percentage points (a relative improvement of 4.3%). Note that the table shows the best single method in terms of overall performance for each ablation. Looking at individual task performance across all experiments, we are able to beat InfLLM in all tasks (see Appendix A.1). Interestingly, we see an especially large jump in performance on the PassageRetrieval task across all ablations, with up to a 33% improvement on InfLLM. This task requires the model to identify the original paragraph from a summary, a challenging task that tests the model's ability to accurately recall a wide range of detailed information from a large context concurrently. The substantial improvement on this task highlights the effectiveness of our event segmentation method in enhancing long-term memory recall and retrieval accuracy in LLMs. Additionally, our method achieves"}, {"title": "4.2 HUMANS AND LLM SURPRISE CLUSTER SIMILAR TOKENS TOGETHER", "content": "As mentioned in Section 3.2, we employ modularity and conductance as two refinement objectives in our boundary refinement algorithm, due to their qualities in assessing the intra- and inter-event similarities between individual tokens. We will now use such metrics to compare various event segmentation methods, including human event segmentation data. Additionally, we introduce one further, simple metric for this experiment: the ratio between intra- and inter-community similarity (I/IS), calculated for each head and community S as follows:\n\\[\\text{intra} = \\sum_{i \\in S, j \\in S} A_{ij}, \\quad \\text{inter} = \\sum_{i \\in S, j \\notin S} A_{ij}, \\quad \\text{I/IS} = \\frac{\\text{intra}}{\\text{inter}}\\]\nKumar et al. (2023) found strong correlations between human-perceived events and prediction errors across 3 short podcasts (7-30 minutes on average), when processing the corresponding transcript with an LLM. Taking advantage of such human data and results from previous works on this dataset we compare the segmentation quality and correlation with human data for each of our methods (3) using our similarity metrics.\nAs shown in Fig. 3A, human-perceived events achieve significantly higher scores in similarity metrics compared to fixed or random events, suggesting that surprise is indeed an important factor for humans in their own perception of events. Furthermore, surprise-only segmentation (S) achieves very similar results to humans, while the addition of our refinement algorithm (SM, SC, FM, FC) significantly improves performance. Fig. 3B further shows that surprise-based methods (S, SM, SC), consistently identify event boundaries that are closest to those perceived by humans."}, {"title": "4.3 COMPARING SEGMENTATION METHODS", "content": "Looking at Table 2, it is clear that surprise-based segmentation with refinement (SM, SC) provides the best results in terms of event similarity metrics, regardless of the base LLM used. While the surprise-only method (S) achieves some good results, we observe that refinement is especially adept to improving this performance with regards to our metrics, as it is directly optimising for such an objective. Interestingly however, the fixed-based refinement methods (FM, FC) do not reach the same performance as their surprise-based counterparts, further showing that the initial segmentation with a surprise threshold is crucial to achieving the best possible balance in intra-/inter-similarity with our methods."}, {"title": "4.4 SIMILARITY, CONTIGUITY, RECENCY AND TEMPORAL ORDER", "content": "As demonstrated in Tables 1 and 2, along with Fig. 3, each of our ablations show various positive improvements on InfLLM (also see Appendix A.1). As mentioned in Section 4.3, refinement has a strong positive impact in improving our similarity metrics. This is seen to translate well to model performance in Table 1, achieving the best performance in a third of the tasks, as well as agreeing with human data (Fig. 3). The effects of contiguity are also clearly demonstrated in this table, with the addition of our contiguity buffer achieving the best performance on three tasks, and the second-best overall score. Furthermore, these methods are shown to be generally complementary, achieving the best overall performance when combined.\nHowever, the fact that certain tasks still appear to benefit more from either surprise-only, refinement, or contiguity, is an interesting result. This is likely due to the nature of the tasks and the varying importance of contiguity across these tasks. For instance, in Supplementary Fig. 5, the MultiNews task scores higher than our baseline only for a ratio of 70% contiguity to similarity buffers. Where contiguity is not crucial, adding such a buffer to our context window also reduces the size of the similarity buffer, and therefore provides potentially less directly relevant events. This is compatible with our own findings that a contiguity buffer that is as big or smaller than the similarity buffer yields the best results, suggesting that the similarity buffer, is still the most crucial part of our approach. This is especially the"}, {"title": "5 DISCUSSION", "content": "Human studies The surprise-based segmentation and boundary refinement processes in EM-LLM mirror key aspects of human event perception and memory formation. Our approach aligns with theories proposing that humans segment continuous experiences into discrete events based on prediction errors or moments of surprise (Zacks et al., 2007; Fountas et al., 2022). This segmentation process is crucial for organising and later retrieving episodic memories efficiently. Indeed, significant correlations have been found between human event segmentation and prediction errors in both LLMs (Kumar et al., 2023) and video models (Fountas et al., 2022; Mariola et al., 2022). Our results add to this growing body of evidence, demonstrating that LLM-based surprise can serve as a proxy for human event segmentation, in multiple levels of hierarchical abstraction, and that the resulting event structure in EM-LLM's attention heads correlates strongly with human-perceived events. This finding creates a more direct, low-level connection between LLM mechanisms and human cognitive processes. Furthermore, our model's use of both similarity-based and temporally contiguous retrieval mechanisms parallels human memory retrieval patterns, allowing for the expression of robust phenomena found in human memory research (Howard and Kahana, 2002).\nFurthermore, our model's use of both similarity-based and temporally contiguous retrieval mechanisms parallels human memory retrieval patterns. The temporal contiguity effect, where items experienced close together in time are often recalled together, is a robust phenomenon in human memory research Further experiments could deepen our understanding of the connections between EM-LLM and human episodic memory. Following one potential direction is to test whether the timing of the event boundaries or the degree of modularity per level that our method produces is closer on average to the human consensus, than individual human subjects. Second, we can explore the level at which different ratios of the contiguity buffer allow the human biases presented in Fig. 2A and the analysis in Ji-An et al. (2024) to be more easily reproduced. Finally, we could investigate how skewing event recall based on recency and originally-recorded surprise affects model performance and to what extent it produces biased behaviour found in studies of free recall.\nIn addition, the architecture of EM-LLM, with its differentiated context handling described in Section 3.1, invites further interesting comparisons to cognitive models of human memory beyond episodic. The group of tokens forming the local context, which hold the most recent and task-relevant information, share characteristics with the concept of working memory. For instance, Baddeley (2003)'s influential model of working memory, which posits a limited-capacity system for temporary information storage and manipulation, bears similarities to our local context functionality. Yet, the analogy is not perfect. Our broader context window, including both local context and retrieved memories, might be more accurately compared to Ericsson and Kintsch (1995)'s concept of long-term working memory, which proposes a mechanism for rapid access to relevant information in long-term memory, extending beyond the traditional capacity limits of working memory. Alternatively, our architecture aligns well with Cowan (2001)'s embedded-processes model, where our local context could be likened to the limited-capacity \"focus of attention\" within working memory, while the full context window parallels the activated portion of long-term memory. Future work could explore these analogies more deeply, providing a flexible test-bed for rapidly exploring hypotheses about human memory, and potentially informing debates about capacity limits in working memory. Additionally, inspired by the multi-component nature of Baddeley's model, one might explore the integration of modality-specific buffers within EM-LLM to enhance its performance on multi-modal tasks.\nMachine learning In refining event boundaries, we utilized modularity and conductance as metrics for evaluating community structure in the similarity graph of attention keys. While effective in our experiments, we acknowledge that numerous other methods for graph clustering and sequence segmentation could potentially be applied Our choice was motivated by their established theoretical foundations and computational efficiency, though comparative studies suggest performance can vary based on network characteristics Interestingly, our surprise-based initial boundary detection shares similarities with Bayesian online change-point detection suggesting potential avenues for integrating time series analysis techniques into LLM context processing. Future work could explore whether more sophisticated segmentation or clustering algorithms could improve EM-LLM's performance, particularly for extremely long contexts or streaming data scenarios. Such investigations could enhance our model and contribute to understanding how information is structured and processed in LLMs, bridging the gap between traditional sequence analysis and LLM context processing.\nLooking ahead, several more avenues for future research emerge from this work. One promising direction is to extend our surprise-based segmentation and boundary refinement processes to operate at each layer of the Transformer independently. This could lead to more nuanced and hierarchical representations of episodic memories, following"}, {"title": "6 CONCLUSION", "content": "In this work, we introduced EM-LLM, a novel and flexible architecture that integrates key aspects of human episodic memory and event cognition into transformer-based language models. Our approach enables LLMs to effectively process and utilise information from vastly extended contexts, far beyond their original training lengths. By combining surprise-based event segmentation with graph-theoretic boundary refinement, and a two-stage memory retrieval process, EM-LLM demonstrates superior performance on long-context tasks compared to state-of-the-art models. Crucially, our method requires no pre-training and can be readily applied to existing LLMs, offering a promising path towards virtually infinite context windows. This capability has the potential to revolutionise how we interact with LLMs, enabling continuous, personalized interactions over extended periods. Furthermore, the flexibility of our framework suggests it could serve as a viable alternative to traditional retrieval-augmented generation (RAG) techniques, especially when combined with efficient compression methods to reduce the memory requirements for the model's KV cache.\nIn conclusion, EM-LLM represents a significant step forward in the development of language models with extended context-processing capabilities. By bridging insights from cognitive science with machine learning, our approach not only enhances the performance of LLMs on long-context tasks but also provides a scalable computational framework for testing hypotheses about human memory. We hope this study will inspire the community to expand research on the intersection between LLMs and human memory mechanisms."}, {"title": "A.2 COMPLEXITY ANALYSIS OF EM-LLM ALGORITHM", "content": "Here, we provide a detailed analysis of the computational complexity of our Algorithm 1, focusing on the boundary refinement step and the calculation of modularity and conductance metrics.\nBoundary Refinement Step The boundary refinement step involves finding the optimal position \u00df between each pair of consecutive initial boundaries (\u03b1, \u03b2) that optimizes the chosen metric function f. This step has the following components:\nIteration over initial boundaries: O(k), where k is the number of initial boundaries. For each pair of boundaries, we compute the metric function f for all positions between a and \u03b2. In the worst case, this is O(n) operations per boundary pair.\nTherefore, the overall complexity of this step is O(kn).\nMetric Function Computation The metric functions (modularity or conductance) are computed at the level of individual memory units. For a memory unit of size m:\n\u2022 Modularity: The naive computation involves summing over all pairs of nodes within the unit, resulting in a worst-case complexity of O(m\u00b2). However, in practice, the similarity graph is often sparse, meaning many node pairs have negligible similarity. Leveraging this sparsity, more efficient implementations can achieve O(l) complexity, where l is the number of non-zero similarity edges within the unit (Newman, 2004). Typically, l is much smaller than m\u00b2, especially for larger units, leading to significant computational savings.\n\u2022 Conductance: This requires computing the sum of edge weights crossing the boundary and the total volume of the unit, which can be done in O(m) time.\nGiven that m is typically much smaller than n and varies for each unit, we can consider the average unit size m and average number of non-zero similarity edges 1. The total complexity for computing metrics across all units is then O(kl) for modularity (which in the worst case is O(km\u00b2), but typically much lower) or O(km) for conductance.\nOverall Complexity Combining the boundary refinement step and metric computation, the overall complexity is:\nFor modularity: O(kn + km\u00b2) For conductance: O(kn + km)\nSince typically m < n, the dominant term in both cases is O(kn). Therefore, we express the overall complexity of our algorithm as O(kn)."}, {"title": "A.3 ANALYSIS OF HUMAN DATA", "content": "The human data released as part of Kumar et al. (2023) used Gaussian smoothing on the average signal across participants to define a probability distribution of likely event boundary positions with respect to timestamps in the podcast. In order to calculate our similarity metrics, as shown in Fig. 3A, we need to express this data in terms of discrete event positions with respect to tokens in the transcript. For fair comparison, we therefore identified human-annotated positions by selecting as many of the most likely positions in the distribution as our initial surprise-based event segmentation had identified in the transcript. In the same process used by Kumar et al. (2023), we then used their provided word onset times to translate these timestamps to token positions, allowing us to calculate our similarity metrics.\nIn Fig. 3B, we use Wasserstein distance in order to compare the relative positions of event boundaries between human annotations and those found by our own methods. Wasserstein distance is a versatile metric used to compare two probability distributions We used such a metric to better capture the uncertainty present in the human data, and found it to give more meaningful results than standard correlation or discrete distance metrics, which showed very little differences between methods. In order to calculate such a metric, we therefore need to convert our own discrete boundary positions to a distribution across token positions. We did so by defining a Mixture of Gaussians (MoG), with each Gaussian corresponding to a single position. Note that, for fair comparison with human data, we apply the same process to the discrete version of the human-annotated positions described above, and use this for comparison."}, {"title": "A.4 APPROXIMATE EQUIVALENCE OF K-NEAREST NEIGHBOURS AND SOFTMAX ATTENTION", "content": "Here we will attempt to show that using a k-NN retrieval in a key-value cache as part of the attention mechanism in transformers is an approximation of applying softmax attention over the entire sequence of tokens."}, {"title": "Lemma 1: Dominance of k-NN Subset", "content": "If K' consists of the k keys with the highest dot products q \u00b7 ki, then:\n\\[\\sum_{j \\in K'} \\exp(q \\cdot k_j \\cdot d^{-\\frac{1}{2}}) \\geq \\alpha \\sum_{j=1}^{n} \\exp(q \\cdot k_j \\cdot d^{-\\frac{1}{2}})\\]\nfor some a \u2248 1, typically very close to 1."}, {"title": "Lemma 2: Approximation of Output Vector", "content": "Given the dominance of K' as shown in Lemma 1, the approximate output u' effectively represents the full output u:\n\\[||u' - u|| \\leq \\epsilon\\]\nwhere e is a small error term."}]}