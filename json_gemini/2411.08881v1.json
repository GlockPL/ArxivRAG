{"title": "Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics", "authors": ["Jos\u00e9 Antonio Siqueira de Cerqueira", "Mamia Agbese", "Rebekah Rousi", "Nannan Xi", "Juho Hamari", "Pekka Abrahamsson"], "abstract": "AI-based systems, including Large Language Models (LLMs), impact millions by supporting diverse tasks but face issues like misinformation, bias, and misuse. Ethical AI development is crucial as new technologies and concerns emerge, but objective, practical ethical guidance remains debated. This study examines LLMs in developing ethical AI systems, assessing how trustworthiness-enhancing techniques affect ethical AI output generation. Using the Design Science Research (DSR) method, we identify techniques for LLM trustworthiness: multi-agents, distinct roles, structured communication, and multiple rounds of debate. We design the multi-agent prototype LLM-BMAS, where agents engage in structured discussions on real-world ethical AI issues from the AI Incident Database. The prototype's performance is evaluated through thematic analysis, hierarchical clustering, ablation studies, and source code execution. Our system generates around 2,000 lines per run, compared to only 80 lines in the ablation study. Discussions reveal terms like bias detection, transparency, accountability, user consent, GDPR compliance, fairness evaluation, and EU AI Act compliance, showing LLM-BMAS's ability to generate thorough source code and documentation addressing often-overlooked ethical AI issues. However, practical challenges in source code integration and dependency management may limit smooth system adoption by practitioners. This study aims to shed light on enhancing trustworthiness in LLMs to support practitioners in developing ethical AI-based systems.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) is emerging as a transformative force, reshaping industries, economies and everyday life. Despite its rapid development and adoption,"}, {"title": "2 Background and Related Work", "content": "LLMs are becoming ubiquitous and have significant impact on decision-making processes and human interactions [20,33]. LLMs are advanced AI algorithms capable of generating, interpreting, and predicting text based on vast amounts of data they have been trained on [4]. Of these LLMs, Generative Pre-trained Transformer (GPT) LLMs, such as ChatGPT, have showcased unprecedented proficiency in the human language, coding, logic, reasoning, and other associated natural language tasks [33]. They excel at guiding complex conversations and are used in countless endeavours such as software engineering (SE) as in AI for Software Engineering (AI4SE) [22] - and qualitative research [26].\nWithin the AI4SE field, the capabilities of LLMs are being explored in the software development, maintenance, and evolution [28,30,26,22], namely LLM4SE. Accordingly, they find applications across various stages of the software development life cycle, including requirement analysis, software design, code implementation, testing, refactoring, defect detection, and maintenance [30]. Albeit the notable use of LLM in SE, to the best of our knowledge there are no studies that focus on the use of LLM in the ethical AI development. In qualitative research, LLMs are also gaining prominence, particularly as a supplement to tasks traditionally performed by humans, like analysing qualitative data [12,32,24]. Specifically, in the coding process - where qualitative data is organised and interpreted by assigning codes or labels to text segments or different data forms - LLMs have demonstrated significant utility [36,32].\nHowever, several concerns arise, especially related with trustworthiness, as more practitioners are relying on LLMs to perform their task [22]. Several studies are drawing attention to possible problems in adopting LLMs for SE, in particular when syntactically correct but non-functional code is produced, which affects the reliability and effectiveness of LLM-based code generation [16]. Pearce et al. [29] used GitHub Copilot to produce 1,689 programs, and found that 40% of them have security vulnerabilities. Liu et al. [21] systematically analysed ChatGPT code generation reliability identifying quality issues as many of the programs generated provided wrong output or contained compilation or runtime error.\nEffective AI4SE should be trustworthy and synergistic with the practitioner's workflow, otherwise \"such AI4SE solutions risk becoming obstacles rather than facilitators\" [22]. Currently, there is a growing need for context-dependent empirical studies that explore how trust in LLMs affects their adoption in software engineering tasks [22]. Furthermore, there remains a significant gap in research focused on the practical operationalisation of AI ethics, particularly through the application of LLMs. To date, no studies have attempted to explore the development of ethical AI-based systems using LLMs."}, {"title": "2.1 Large Language Models", "content": "LLMs have been pre-trained on vast amounts of text data, often scraped from the internet, allowing them to learn patterns and structures inherent in human language [14]. One of the key features of LLMs is their ability to generate contextually relevant and apparently coherent text across a wide range of topics [33]. These models can perform tasks such as language translation, text summarization, question answering, and creative writing [14]. They achieve this by leveraging the vast amounts of data they have been trained on to predict the next word or sequence of words in each context [10]. Some well-known examples of LLMs include GPT (Generative Pre-trained Transformer) models developed by OpenAI [27], particularly GPT-3.5 and GPT-4."}, {"title": "2.2 Trustworthiness in Large Language Models", "content": "Recent advancements and use of LLMs have raised concerns about their ethical implications [40,19,35,20,37]. Issues like information hallucination, biases, and the need for factual correctness are critical in determining the trustworthiness"}, {"title": "2.3 Techniques to improve trustworthiness in LLM4SE", "content": "Hong et al. [15] propose MetaGPT to simulate a group of agents as software company. Worth noting the use of specialized roles (Product Manager, Architect, Project Manager, Engineer, and QA Engineer), workflow for the agents (all agents work in a sequential fashion), and a structure communication interface. An interesting point regarding the specialized roles, is the ability for agents to have skills, i.e., perform actions, such as being able to run code and search on the web. The authors argue that the incorporation of aforementioned techniques can considerably enhance code generation, as well as mitigating hallucinations risks.\nQian et al. [31] developed ChatDev, a chat-based software development framework using LLMs to enhance communication and collaboration across various roles, aimed at reducing hallucination risks like bugs and missing dependencies. Though, it does generate different outputs for the same inputs and carries risks due to untested code.\nMeanwhile, Wu et al. [41] from Microsoft introduced AutoGen, a multi-agent LLM framework enhancing coding productivity by structuring tasks among spe-"}, {"title": "2.4 AI Ethics", "content": "Despite the recent academic and industry interest, AI ethics is a long-standing area of research, in which diverse incidents have recently raised public concerns about its use and development [11]. Consequently, in the past few years a variety of principles and guidelines have emerged, from various sources (academia, industry, civil society) to frame and define what is ethical AI [17]. Ryan and Stahl [34] provide 11 ethical principles along with ethical issues related to AI: 1) Transparency, 2) Justice and Fairness, 3) Non-maleficence, 4) Responsibility, 5) Privacy, 6) Beneficence, 7) Freedom and Autonomy, 8) Trust, 9) Sustainability, 10) Dignity, 11) Solidarity. These ethical principles serve as the basis of our analysis.\nThe European Parliament is currently putting into force the world's first regulation on AI [5], which highlights the fact that the plethora of guidelines available are in fact soft law, with no legally binding nor real consequences [3]. Nevertheless, it is reminiscent of the abstract principles on how AI ethics is mainly approached [11,17,6]. Thus, the obstacles in operationalising ethical principles in AI-based systems stem from the subjectivity necessary to interpret and put into practice highly abstract principles by the practitioners [3]. Despite its importance, it has often been a task commonly taken as an afterthought [39]. As opposed to other efforts available in the literature, we aim to study the use of LLM-based multi-agent systems in the development process of AI-based systems, taking into consideration the operationalization of ethical principles from the first stage of the development process onwards. The use of LLM agents in developing ethically-aligned AI systems introduces significant complexity and novelty, making their ethical implications a compelling area of study. LLMs face unique challenges in producing accurate, unbiased text and source code while adhering to ethical guidelines. Their dynamic and often unpredictable outputs in multi-agent interactions provide a valuable opportunity to study the application of ethical principles in real-world settings, although there is a noted lack of literature on the operationalization of AI ethics through LLM agents."}, {"title": "2.5 LLMs for Qualitative Analysis", "content": "Qualitative analysis is another opportune expanding area where LLMs can be applied [7,8,24], particularly in qualitative research within the field of software engineering [1,32]. Thematic analysis is labour-intensive and time-consuming for humans, and are prone to mistakes and bias [2]. Therefore, LLMs present an opportunity to improve the accuracy and efficiency of this method. In thematic analysis, coders require multiple rounds of discussion to achieve consensus and resolve ambiguities for an in-depth understanding of the data [2].\nThus, LLMs can tackle the challenges of traditional qualitative research, such as the time-intensive nature of data analysis, limited generalizability, consistency issues, and personal subjective biases [1,12]. Hamilton et al. [12] argue that both humans and LLMs have identified unique and overlapping themes from interview data, indicating the potential for recognizing patterns and themes in qualitative data."}, {"title": "3 Methodology", "content": "In this study, we employed an adapted DSR [13] approach to explore and enhance trustworthiness in LLMs for software engineering tasks in four phases: 1) exploration, 2) prototyping, 3) evaluation, and 4) communication of results. The fourth phase will be approached in Section 4."}, {"title": "3.1 Exploring", "content": "Initially, we explored existing literature and found the need and motivation to improve trustworthiness in AI4SE, presented in Section 2.2, and most notably in the work of David Lo [22]. Furthermore, we identified techniques that have been recently proposed in the literature to improve trustworthiness in AI systems, particularly in the context of software engineering, i.e., AI4SE. In sum, the identified techniques described in Section 2.3 provide an LLM-based multi-agent systems approach, characterised by: a) a combination of agents, b) distinct roles (specialized agents), c) structured conversation, d) multiple rounds of debate.\nThese techniques were then synthesised and integrated into the development of a prototype system designed to assist in software engineering tasks, in particular in the creation of ethically-aligned AI-based systems."}, {"title": "3.2 Prototype: LLM-BMAS", "content": "From our findings, we devised a prototype that is an LLM-Based multi-agent system (henceforth, called LLM-BMAS). The LLM-BMAS is a Python script that consists of three agents making use of GPT-40-mini model through OpenAI API. They have different specialized roles and their conversation is structured in the same way. Specifically, there are two senior Python developers and one AI ethicist. As a team, they receive an input - a project description (PD)- that they should work on collaboratively to implement, through five rounds of discussion."}, {"title": "3.3 Evaluation", "content": "To evaluate the effectiveness of the prototype, we applied a multi-faceted evaluation strategy. First, we conducted a thematic analysis using an LLM to automatically identify and categorize key themes within the generated output. This provided insights into the model's alignment with ethical and functional requirements. Second, we employed hierarchical clustering to organize and group-related text segments, supporting and validating the findings of the thematic analysis by uncovering underlying patterns and relationships in the data. Then, both thematic analysis results and hierarchical clustering for each project description is compared and analysed. Finally, an ablation study is conducted following similar steps. Figure 2 presents a visualisation for the evaluation methodology devised.\nAn ablation study is useful to validate research results and consists in assessing performance of an AI system by removing components [23,25]. In this case, we will remove the LLM-BMAS prototype and use as a baseline only the interaction with OpenAI ChatGPT user interface.\nAs discussed, each run of the prototype produces a different result for the same input, thus, we run the system three different times to obtain distinct output (yielding the raw output, e.g., 011, 012, 013 for the first project description)."}, {"title": "4 Results and Discussion", "content": "In this Section we present for each project description provided its corresponding results and discussion, finally, an overview discussion is presented. All project descriptions are derived from real AI incidents available at https://incidentdatabase.ai/.\nAll data used in this experiment, including agent prompts, custom instructions, thematic analysis performed, are available at here."}, {"title": "4.1 Project Description 1", "content": "The first project description derived from this incident:\nIt is worth highlighting that in the project description there is a need for referencing the EU AI Act, however, by using GPT-40-mini, that was trained up until October 2023, this Act was not yet available, only being published on June 2024. In future work, we will introduce this document as a vector to be embedded and accessed by an open-source LLM. Moreover, according to David Lo [22], guaranteeing that AI4SE solution will comply with legislation is one way to enhance trustworthiness in AI4SE, since practitioners need to avoid conflict with laws.\nRaw data obtained vary from 1712 lines to 1976 lines of discussion plus source code. The thematic analysis in Table 1 was obtained from the merge of the three thematic analyses attained from each output. In Figure 3, 5 classes were devised, however, classes such as 1, 3, 4 and 5 are related to technical implementation. Nevertheless, it is possible to understand that themes 1 2 and 3 are closely related to the second class, where ethical, EU, act appear. Moreover with class number 3, w.r.t functions as evaluate_bias and bias_score.\nIn all cases, the source codes produced were spread around the text. As mentioned, this can be almost 2000 lines long. This makes the source code difficult to differentiate, and sometimes there are pieces that need to be glued together from different rounds of conversation. After trying to run the source code available in O1_1, it was found that some packages were needed to be installed manually. Furthermore, deprecated packages were being used: 'sklearn' PyPI package is deprecated, use 'scikit-learn'. This results in extra effort by the practitioners, impacting on their workflow.\nIn regards to the ablation study, only 78 lines of text were obtained, while no code was produced. On the other hand, three themes were devised from the thematic analysis: Ethical AI Development, Technical Implementation and Fairness,"}, {"title": "4.2 Project Description 2", "content": "The second project description derived from this incident:\nIn this PD, there is no reference to EU AI Act directly. However, it is worth pinpointing that it is available in the prompt of the third agent, the AI Ethicist. Raw data obtained vary from 1414 lines to 2178 lines of text data. The discussion here progresses in a similar way to the previous one.\nAs the relationship between the thematic analysis in Table 2 and the hierarchical clustering produced in Figure 4, it is possible to highlight the class number 1 with theme number 2, Ethical Compliance and AI Responsibility. Moreover, class number 2 is closely related to theme Security and Collaboration, with words such as authentication and logger.\nSimilarly to the previous PD, no source code was produced in the ablation study. This yielded 78 lines of text and themes such as Technological Framework, Ethics and Compliance, User Experience and Support, Strategic Objective. While the study with the proposed prototype provides a more fruitful outcome, yielding almost 2000 lines of text, the ablation study provides a generic text that can only serve as study material.\nIn relation to the source code produced, on top of the overhead to developers encountered in the previous PD, in O2_2 it is perceived that our prototype created modules, as in from your_module import VideoAuthenticator, ImageAuthenticator, AudioAuthenticator. Again, this poses a problem to practitioners when working with this tool, due to the fact that they will need to go through all the text and manually search for the source code that he/she needs to copy and paste somewhere else, test it, install dependencies, and manage modules."}, {"title": "4.3 Project Description 3", "content": "The third project description derived from this incident:"}, {"title": "5 Threats to Validity", "content": "Different limitations appear from this study, including reliance on proprietary LLMs, which may change unpredictably. Reproducibility is a concern due to the non-deterministic outputs of LLMs. Since our methodology relies mainly on LLM to assess the outputs of the prototype, there is a potential risk of circular reasoning, where an AI is validating another AI's trustworthiness. This could introduce bias, as both systems may share underlying flaws or limitations that go undetected. Moreover, thematic analysis and hierarchical clustering, though useful in"}, {"title": "6 Final Remarks", "content": "This study aimed to explore trustworthiness in AI4SE, specifically in the use of LLM for software engineering (LLM4SE), in the context of ethically aligned AI-based systems development. To the best of our knowledge, there are no studies in the literature that address practical AI ethics using LLM. Using the Design Science Research method, we identified the motivation and different techniques to improve trustworthiness in AI4SE, proposed a prototype - an LLM-based multi-agent system (LLM-BMAS) -, and evaluated it using thematic analysis, hierarchical clustering, ablation study, and source code execution. The techniques discovered to improve trustworthiness are a combination of agents, different roles (specialised agents), structured conversations and multiple rounds of debate. The prototype was tested with three distinct project descriptions derived from real AI incidents extracted from the AI Incident Database. For each project description, LLM-BMAS produced about 2000 lines of text, encompassing source code and documentation to implement the AI-based systems."}]}