{"title": "Modality Mirror: Improving Audio Classification in Modality Heterogeneity Federated Learning with Multimodal Distillation", "authors": ["Tiantian Feng", "Tuo Zhang", "Salman Avestimehr", "Shrikanth S. Narayanan"], "abstract": "Multimodal Federated Learning frequently encounters challenges of client modality heterogeneity, leading to undesiredperformances for secondary modality in multimodal learning.It is particularly prevalent in audiovisual learning, with audio isoften assumed to be the weaker modality in recognition tasks.To address this challenge, we introduce ModalityMirrorto improve audio model performance by leveraging knowledgedistillation from an audiovisual federated learning model.ModalityMirror involves two phases: a modality-wiseFL stage to aggregate uni-modal encoders; and a federatedknowledge distillation stage on multi-modality clients to trainan unimodal student model. Our results demonstrate thatModalityMirror significantly improves the audio classification compared to the state-of-the-art FL methods such asHarmony, particularly in audiovisual FL facing video missing.Our approach unlocks the potential for exploiting the diversemodality spectrum inherent in multi-modal FL.\nIndex Terms: speech recognition, federated learning, multimodal learning, model distillation", "sections": [{"title": "1. Introduction", "content": "Multi-modal sensing systems are increasingly important in arange of real-world applications, such as human activity recog-nition, health monitoring, and emotion recognition [1, 2, 3, 4,5, 5]. While these multimodal systems provide promises in en-riching human experiences, they have to address one fundamen-tal challenge: achieving robust recognition without compromis-ing data privacy, especially when data is collected in sensitiveenvironments such as hospitals, schools, and homes. Specif-ically, Federated Learning (FL) has been recently introducedas a privacy-preserving machine learning paradigm. It enablesmodel training on a central server using shared model param-eters from edge devices, eliminating the need to transfer localdata [6].\nDespite its potential to protect privacy, FL frequently facescritical challenges in achieving competitive performances whenencountering device and data heterogeneity. Devices in FL sys-tems vary widely in their capabilities and the modalities theycan collect. A significant aspect of this heterogeneity is thevariance in data modalities: some devices learn rich informationby processing multi-modal data, while others are constrained touse single-modal data. This variation introduces a much lessexplored challenge in FL, known as modality heterogeneity.Specifically, this challenge may frequently occur in applicationsinvolving audio-visual recognition. Notably, visual data, suchas videos or images, carry sensitive information about a per-\nson, including facial geometric, body shapes, and other biomet-ric data. This prevents many service providers from collecting,storing, and processing visual information, even in FL.\nFurthermore, particularly within audiovisual recognition,the dynamics between dominant and weak modalities play acritical role in recognition performance. Typically, visual input is considered the dominant modality in audiovisual recogni-tion, heavily influencing model training due to its rich, detailedcontent. In contrast, the audio modality is often recognizedas a weaker role in audiovisual recognition. This phenomenoncould be be more significant in FL with heterogeneous modal-ities described earlier, with some devices of multi-modal dataand others of audio data, leading to decreased audio classifi-cation. Existing literature in addressing modality heterogene-ity [7, 8] has primarily focused on increasing the utilizationof single-modality local data for multi-modality training. How-ever, such methods typically do not address the limitations ofsingle-modality clients, who cannot practically employ multi-modal models. Therefore, our research specifically aims to im-prove the classification performance of clients limited to audiodata, the conventionally weaker modality in audio-visual recog-nition, within FL with heterogeneous modalities.\nIn response, we introduce ModalityMirror, a novelFL paradigm to tackle modality heterogeneity in FL in-volving audiovisual recognition. The primary object ofModalityMirror is to enhance audio classification throughknowledge distillation from the audiovisual model trained byFL. The ModalityMirror involves two distinct trainingstages: a modality-wise FL stage to learn the audiovisual model;a federated knowledge distillation occurs among multi-modalityclients to train an audio student model. Empirical resultsdemonstrate that ModalityMirror yields significant im-provements in accuracy, especially in the audio-modality mod-els when compared to recent state-of-the-art FL frameworkssuch as Harmony [7]. Furthermore, ablation studies reveal thatModalityMirror is particularly effective at enhancing theaudio model's performance on labels where the audio modalityalone provides insufficient information for classification."}, {"title": "2. Related Study", "content": "Status Quo and Their Limitations. In the domain ofmulti-modal federated learning (FL), frameworks such asCreamFL [9] and Harmony [7] have emerged as significantcontributions, albeit with notable limitations in real-world sce-narios involving missing modalities. CreamFL utilizes a con-trastive representation-level ensemble to construct a compre-hensive server model from heterogeneous multi-modal clientdata, but its reliance on a quality-dependent public datasetfor server-side training and aggregation poses practical chal-"}, {"title": "3. Method", "content": "Here, we describe the problem formulation of our multi-modalFL with modality heterogeneity. Specifically, we refer to mul-timodal clients as data-rich clients. First, we denote client sizeas N in FL. Given our focus on audio-visual recognition, wedefine the audio and visual modalities as A and V, respec-tively. Moreover, as we emphasize studying the visual modalitymissing in modality heterogeneity FL, we introduce r as theratio of clients with missing visual modality, leading to the to-tal number of audio-only modality clients as $N_A = rN$. Simultaneously, the number of multi-modal clients is denoted as$N_M = (1-r)N$. For each audio modality client $p \\in [N_A]$, theassociated audio modality dataset is defined as $D_p^A = \\{x_i, y_i\\}$,where $i \\in N$. Additionally, each audio modality client $p \\in[N_A]$ is assumed to adopt the audio model $\\theta_p^A$ trained on $D_p^A$.Similarly, for each multi-modal client $q \\in [N_M]$, the multimodal data is represented as $D_q^M = \\{x_i^A, x_i^V, y_i\\}$, and the as-sociated model is denoted as $\\theta_q^M = \\{\\theta_q^{A},\\theta_q^{V} \\}$, where $q \\in [N_M]$."}, {"title": "3.1. Problem Formulation", "content": "Here, we describe the problem formulation of our multi-modalFL with modality heterogeneity. Specifically, we refer to mul-timodal clients as data-rich clients. First, we denote client sizeas N in FL. Given our focus on audio-visual recognition, wedefine the audio and visual modalities as A and V, respec-tively. Moreover, as we emphasize studying the visual modalitymissing in modality heterogeneity FL, we introduce r as theratio of clients with missing visual modality, leading to the to-tal number of audio-only modality clients as $N_A = rN$. Simultaneously, the number of multi-modal clients is denoted as$N_M = (1-r)N$. For each audio modality client $p \\in [N_A]$, theassociated audio modality dataset is defined as $D_p^A = \\{x_i, y_i\\}$,where $i \\in N$. Additionally, each audio modality client $p \\in[N_A]$ is assumed to adopt the audio model $\\theta_p^A$ trained on $D_p^A$.Similarly, for each multi-modal client $q \\in [N_M]$, the multimodal data is represented as $D_q^M = \\{x_i^A, x_i^V, y_i\\}$, and the as-sociated model is denoted as $\\theta_q^M = \\{\\theta_q^{A},\\theta_q^{V} \\}$, where $q \\in [N_M]$."}, {"title": "3.2. ModalityMirror For Audio-visual Recognition", "content": "Our proposed ModalityMirror is motivated by two prin-cipal objectives: first, to mitigate the challenge of modal-ity heterogeneity within FL; second, to enhance the perfor-mance of devices that support only a weaker data modality,such as the audio modality in this work, by leveraging theknowledge from multimodal clients within the FL training.Consequently, we present a two-phase framework comprisingmodality-aware Federated Learning followed by Federated Dis-tillation. The distillation training aims to decouple multi-modaland single-modal training processes. Figure 1 illustrates theModalityMirror system architecture. In the following, wedescribe the details of each training stage."}, {"title": "3.2.1. Overview", "content": "Our proposed ModalityMirror is motivated by two prin-cipal objectives: first, to mitigate the challenge of modal-ity heterogeneity within FL; second, to enhance the perfor-mance of devices that support only a weaker data modality,such as the audio modality in this work, by leveraging theknowledge from multimodal clients within the FL training.Consequently, we present a two-phase framework comprisingmodality-aware Federated Learning followed by Federated Dis-tillation. The distillation training aims to decouple multi-modaland single-modal training processes. Figure 1 illustrates theModalityMirror system architecture. In the following, wedescribe the details of each training stage."}, {"title": "3.2.2. Modality-Aware Federated Learning.", "content": "We present the modality-aware Federated Learning in Algo-rithm 1, which enables collaborative training across nodes withdistinct modalities. In this phase, nodes that support multi-modal data trains a multi-modal model locally. In contrast,nodes constrained to a single modality, like the audio modality,focus on training a model specific to their available modality.In this phase, the server applies a modality-aware aggregationto mitigate the disparities introduced by modality heterogeneity.\nThis modality-aware FL is practical, given that most mul-timodal models, including audio-visual recognition models,adopt the dual-encoder architecture with each encoder tied to aspecific modality. The embeddings from the modality-specificencoder are concatenated and fed into the classification layersfor audio-visual recognition. This embedding-level fusion allows the server to execute a nuanced aggregation process, accommodating nodes with distinct modalities by selectively aggregating encoder weights to refine the global model."}, {"title": "3.2.3. Federated Distillation", "content": "In the second phase, our objective is to enhance the perfor-mance of audio-only clients by leveraging knowledge fromdata-rich (multi-modal) clients. Specifically, we introduce Fed-erated Distillation training to unify model knowledge acquiredfrom clients with diverse modalities. This process particularlybenefits audio-modality nodes by enabling them to distill andlearn multimodal knowledge from the audio-visual model ini-tially trained by all nodes. The complete Federated Distillationalgorithm is presented in Algorithm 2.\nIn practice, data-rich clients perform the distillation processlocally, training an audio-modality model that distills knowl-edge from the audio-visual model. The distillation loss usedin the local training is the KL divergence $KL(p^A||p^M)$ be-tween the multimodal output $p^M$ and the audio model output$p^A$. Here, we introduce the temperature value $T$ to adjustthe entropy of the predicted softmax probabilities by $p^A \\proptoexp(\\frac{log(p^A)}{T})$ and $p^M \\propto exp(\\frac{log(p^M)}{T})$. This distilled audiomodality model is then applied with federated aggregation, re-sulting in an audio-modality model that incorporates the col-lective knowledge of both audio and audio-visual data sources.This federated distillation phase thus ensures that all participat-ing nodes, regardless of their data modality, benefit from the en-riched learning landscape offered by the multi-modality model."}, {"title": "4. Datasets and Experimental Details", "content": "In this work, we select two popular multimedia action recog-nition datasets, UCF101 [10] and ActivityNet [11], to assessthe ModalityMirror framework for audio-visual recogni-tion. We refer the reader to [12] for detailed label distribu-tion within these datasets. Motivated by studies presented in[12, 13], we extract the video frame in the middle of the videoas the visual modality. We conduct 3-fold cross-validation fol-lowing the standard fold splits in training the UCF101 dataset.Increasingly, we apply the standard train/validation/test split inexperimenting with the ActivityNet dataset. In line with [1], wepartition both datasets into 100 distinct data silos using a Dirich-let distribution with $\u03b1=0.1$, simulating an extreme non-IIDsetting. More detailed about datasets are listed in Appendix B."}, {"title": "4.1. Audio-visual Datasets", "content": "In this work, we select two popular multimedia action recog-nition datasets, UCF101 [10] and ActivityNet [11], to assessthe ModalityMirror framework for audio-visual recogni-tion. We refer the reader to [12] for detailed label distribu-tion within these datasets. Motivated by studies presented in[12, 13], we extract the video frame in the middle of the videoas the visual modality. We conduct 3-fold cross-validation fol-lowing the standard fold splits in training the UCF101 dataset.Increasingly, we apply the standard train/validation/test split inexperimenting with the ActivityNet dataset. In line with [1], wepartition both datasets into 100 distinct data silos using a Dirich-let distribution with $\u03b1=0.1$, simulating an extreme non-IIDsetting. More detailed about datasets are listed in Appendix B."}, {"title": "4.2. Model Details", "content": "We utilize pre-trained ResNet-18 [14] and SSAST [15] mod-els as the backbones for modeling visual and audio modali-"}, {"title": "4.3. Evaluation Baselines", "content": "Harmony [7] is the first selected baseline, achieving state-of-the-art performance in heterogeneous multi-modal FL.Harmony adopts a two-stage framework. Initially, all partic-ipating nodes collectively train two distinct unimodal encoderswithin the FL paradigm. In the second stage, only multi-modalnodes engage in the collaborative training of the multimodalmodel, with the initialization of feature encoders using themodel weights refined during the first stage, followed by finetuning. The unimodal encoders trained in the first stage wouldbe used for the single-modality clients.\nUniFL [16]: Motivated by Harmony, we include one naiveheterogeneous multi-modal FL baseline, UniFL [16], in com-parison. UniFL represents a scenario in which nodes sharingidentical data modalities collaboratively learn a model. For ex-ample, nodes with audio modality would collaboratively trainan audio model, while nodes with multi-modalities are learningan audio-visual model collectively.\nMultiFL [17]: Apart from UniFL, we experiment withMultiFL described in [17]. In MultiFL, all nodes participatein the training, and the server returns the unimodal encoders byaveraging the uni-modal and multi-modal nodes."}, {"title": "4.4. Evaluation Metrics", "content": "As this paper concentrates on the performance of clients whoonly hold weaker (audio) modalities for training, we use the top-1 test accuracy of the audio-modality model of UCF101 as ourevaluation metric. Due to relatively low performance on Activi-tyNet, we report the top-5 test accuracy. We ran the experimentswith three distinct seeds and reported the average performanceand variance on both datasets."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. End-to-End Performance", "content": "We begin by comparing the end-to-end performance ofModalityMirror on benchmark datasets. For consistency,all experiments conducted in Table 1 follow a uniform setting"}, {"title": "5.2. Impact of Video Modality Missing Rate", "content": "In order to investigate the impact of video modality missingrate on ModalityMirror, we test ModalityMirror withvideo modality missing rate ranging from 10% to 90% on theUCF101 dataset. As the results shown in Figure 3, the perfor-mance of ModalityMirror increases as the video missingrate decreases, indicating that a positive correlation between thecompleteness of video data and the model's performance."}, {"title": "5.3. Understanding the Advantages of ModalityMirror", "content": "To elucidate the benefits of ModalityMirror, we ana-lyze the F1 score performance differential for individual labels between ModalityMirror and Harmony across bothdatasets under a 30% video modality missing scenario. Figure 2 highlights the top 10 labels exhibiting significant relative performance changes. We make two key observations:(1) Through federated distillation from multi-modality mod-els, ModalityMirror notably enhances the audio model'sperformance on labels where the audio modality alone provides limited information. For instance, in the UCF101 dataset,Harmony achieves an F1 score of 9.12 for the bowling classand 0.0 for Tug of War in ActivityNet, reflecting the insuffi-ciency of audio cues in these videos for accurate classification.The integration of knowledge from the image encoder via modeldistillation allows the audio model to better classify these challenging categories. While this approach occasionally reducesperformance on labels predominantly reliant on audio, such asplaying flute and playing accordion, the overall enhancement inperformance, as evidenced in Table 1, demonstrates a net bene-fit. (2) ModalityMirror significantly reduces classificationambiguity in audio-based tasks. For certain labels, the distinc-tion between audio features is minimal; for instance, Playingcongas and Playing drums in ActivityNet exhibit highly simi-lar audio profiles. Consequently, Harmony attains F1 scoresof 33.02 and 19.42 for these categories, respectively. In con-trast, by leveraging insights from the multi-modality model,ModalityMirror is able to more effectively differentiatethese two labels, enhancing their performance to F1scores of 53.10 and 26.24, respectively."}, {"title": "6. Conclusion and Limitation", "content": "We propose ModalityMirror, a distillation-based FLframework aiming to improve audio classification in modalityheterogeneity FL. Our experimental results showcase the com-petitive performance of ModalityMirror when compared tostate-of-the-art FL methods. Moreover, through detailed abla-tion studies, we demonstrate that ModalityMirror notablyenhances the audio model's performance on labels where theaudio modality alone provides limited information.\nLimitation and future work. The empirical results indicate that ModalityMirror may occasionally reduce performanceon labels associated with strong acoustic characteristics. We"}, {"title": "A. Problem Formulation", "content": "Without loss of generality, we denote our multi-modal FL withmodality heterogeneity with a client size of N. Specifically, wealso refer to multimodal clients as data-rich clients. Given ourspecific focus on audio-visual applications, we define the audioand visual modalities as A and V, respectively. Moreover, aswe emphasize studying the visual modality missing in modal-ity heterogeneity FL, we introduce r as the ratio of clients withmissing visual modality, leading to the total number of audio-only modality clients as $N_A = rN$. Simultaneously, the number of multi-modal clients is denoted as $N_M = (1 - r)N$.\nFor each audio modality client $p \\in [N_A]$, the associated audiomodality dataset is defined as $D_p^A = \\{x_i, y_i\\}$, where $i \\in N$.Additionally, each audio modality client $p \\in [N_A]$ is assumedto adopt the audio model $\\theta_p^A$ trained on $D_p^A$. Similarly, for eachmulti-modal client $q \\in [N_M]$, the multi-modal data is representedas $D_q^M = \\{x_i^A, x_i^V, y_i\\}$, and the associated model isdenoted as $\\theta_q^M = \\{\\theta_q^{A},\\theta_q^{V} \\}$, where $q \\in [N_M]$. In modality het-erogeneity FL, we aim to obtain the global multi-modal model$\\{\\theta^M\\}$. The clients cooperate in training parameters $\\{\\theta^A_p\\}$ and $\\{\\theta^V_q\\}$with the aim of solving the optimization problem formulated asfollows:\n\nmin f(\\theta_p^A) := \\sum w_p F_p(\\theta_p^A) (1)\np\\in [N_A]\n\nmin f(\\theta_q^M) := \\sum w_q F_q(\\theta_q^{A},\\theta_q^{V}) (2)\nq\\in [N_M]\nwhere $F_p(.)$ and $F_q(.)$ represents the local objective of clientpand q, respectively. $w_p$ and $w_q$ denotes the aggregationweight of client p and q, satisfying $w_p \\geq 0$, $w_q > 0$ and$\\sum_{p\\in [N_A]} w_p = 1$, $\\sum_{q\\in [N_M]} w_q = 1$."}, {"title": "B. Audio-visual Datasets", "content": "UCF101 dataset focuses on human action recognition, includ-ing videos from 101 sport-related action categories. The ma-jority of the video is sourced from YouTube, leading to a totalof more than 10k videos. However, we identified that videoswith only 51 categories include both video and audio modali-ties. This leads to fewer than 7,000 videos for the experiments.As the video duration in this dataset varies from a few secondsto over 30 seconds, we decided to constrain the input audio duration to 5 seconds. In line with [1], we partition this datasetinto 100 distinct data silos using a Dirichlet distribution witha = 0.1, aiming to replicate an extreme non-IID setting.\nActivityNet is a similar dataset to UCF101 to study human action recognition but includes human actions in a broader spec-trum. The complete dataset has 203 unique human actionclasses for everyday life, while video data with only 200 human actions are presented with both audio and visual modalities. This reduces the total data to 18,976 instances. We adoptthe same partition strategy as with UCF101, dividing ActivityNet into 100 independent data silos via a Dirichlet distributionwith $\u03b1=0.1$ to mirror an extreme non-IID scenario."}]}