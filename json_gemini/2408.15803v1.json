{"title": "ModalityMirror: Improving Audio Classification in Modality Heterogeneity Federated Learning with Multimodal Distillation", "authors": ["Tiantian Feng", "Tuo Zhang", "Salman Avestimehr", "Shrikanth S. Narayanan"], "abstract": "Multimodal Federated Learning frequently encounters challenges of client modality heterogeneity, leading to undesired performances for secondary modality in multimodal learning. It is particularly prevalent in audiovisual learning, with audio is often assumed to be the weaker modality in recognition tasks. To address this challenge, we introduce ModalityMirror to improve audio model performance by leveraging knowledge distillation from an audiovisual federated learning model. ModalityMirror involves two phases: a modality-wise FL stage to aggregate uni-modal encoders; and a federated knowledge distillation stage on multi-modality clients to train an unimodal student model. Our results demonstrate that ModalityMirror significantly improves the audio classification compared to the state-of-the-art FL methods such as Harmony, particularly in audiovisual FL facing video missing. Our approach unlocks the potential for exploiting the diverse modality spectrum inherent in multi-modal FL.", "sections": [{"title": "1. Introduction", "content": "Multi-modal sensing systems are increasingly important in a range of real-world applications, such as human activity recognition, health monitoring, and emotion recognition [1, 2, 3, 4, 5, 5]. While these multimodal systems provide promises in enriching human experiences, they have to address one fundamental challenge: achieving robust recognition without compromising data privacy, especially when data is collected in sensitive environments such as hospitals, schools, and homes. Specifically, Federated Learning (FL) has been recently introduced as a privacy-preserving machine learning paradigm. It enables model training on a central server using shared model parameters from edge devices, eliminating the need to transfer local data [6].\nDespite its potential to protect privacy, FL frequently faces critical challenges in achieving competitive performances when encountering device and data heterogeneity. Devices in FL systems vary widely in their capabilities and the modalities they can collect. A significant aspect of this heterogeneity is the variance in data modalities: some devices learn rich information by processing multi-modal data, while others are constrained to use single-modal data. This variation introduces a much less explored challenge in FL, known as modality heterogeneity. Specifically, this challenge may frequently occur in applications involving audio-visual recognition. Notably, visual data, such as videos or images, carry sensitive information about a person, including facial geometric, body shapes, and other biometric data. This prevents many service providers from collecting, storing, and processing visual information, even in FL.\nFurthermore, particularly within audiovisual recognition, the dynamics between dominant and weak modalities play a critical role in recognition performance. Typically, visual input is considered the dominant modality in audiovisual recognition, heavily influencing model training due to its rich, detailed content. In contrast, the audio modality is often recognized as a weaker role in audiovisual recognition. This phenomenon could be be more significant in FL with heterogeneous modalities described earlier, with some devices of multi-modal data and others of audio data, leading to decreased audio classification. Existing literature in addressing modality heterogeneity [7, 8] has primarily focused on increasing the utilization of single-modality local data for multi-modality training. However, such methods typically do not address the limitations of single-modality clients, who cannot practically employ multi-modal models. Therefore, our research specifically aims to improve the classification performance of clients limited to audio data, the conventionally weaker modality in audio-visual recognition, within FL with heterogeneous modalities.\nIn response, we introduce ModalityMirror, a novel FL paradigm to tackle modality heterogeneity in FL involving audiovisual recognition. The primary object of ModalityMirror is to enhance audio classification through knowledge distillation from the audiovisual model trained by FL. The ModalityMirror involves two distinct training stages: a modality-wise FL stage to learn the audiovisual model; a federated knowledge distillation occurs among multi-modality clients to train an audio student model. Empirical results demonstrate that ModalityMirror yields significant improvements in accuracy, especially in the audio-modality models when compared to recent state-of-the-art FL frameworks such as Harmony [7]. Furthermore, ablation studies reveal that ModalityMirror is particularly effective at enhancing the audio model's performance on labels where the audio modality alone provides insufficient information for classification."}, {"title": "2. Related Study", "content": "Status Quo and Their Limitations. In the domain of multi-modal federated learning (FL), frameworks such as CreamFL [9] and Harmony [7] have emerged as significant contributions, albeit with notable limitations in real-world scenarios involving missing modalities. CreamFL utilizes a contrastive representation-level ensemble to construct a comprehensive server model from heterogeneous multi-modal client data, but its reliance on a quality-dependent public dataset for server-side training and aggregation poses practical challenges in diverse settings where such datasets may be unavailable. Harmony introduces a two-stage framework involving modality-specific federated learning and federated fusion, effectively segmenting multi-modal network training. However, it inadequately addresses missing modalities and predominantly outputs multi-modality models, thereby excluding single-modality clients and limiting its practical applicability."}, {"title": "3. Method", "content": "Here, we describe the problem formulation of our multi-modal FL with modality heterogeneity. Specifically, we refer to multimodal clients as data-rich clients. First, we denote client size as N in FL. Given our focus on audio-visual recognition, we define the audio and visual modalities as A and V, respectively. Moreover, as we emphasize studying the visual modality missing in modality heterogeneity FL, we introduce r as the ratio of clients with missing visual modality, leading to the total number of audio-only modality clients as $N_A = rN$. Simultaneously, the number of multi-modal clients is denoted as $N_M = (1-r)N$. For each audio modality client $p \\in [N_A]$, the associated audio modality dataset is defined as $D^A_p = \\{x_i, y_i\\}$, where $i \\in \\mathbb{N}$. Additionally, each audio modality client $p \\in [N_A]$ is assumed to adopt the audio model $\\theta^A_p$ trained on $D^A_p$. Similarly, for each multi-modal client $q \\in [N_M]$, the multi-modal data is represented as $D^M_q = \\{x^A_i, x^V_i, y_i\\}$, and the associated model is denoted as $\\theta^M_q = \\{\\theta^A_q, \\theta^V_q\\}$, where $q \\in [N_M]$. In modality heterogeneity FL, we aim to obtain the global multi-modal model $\\theta^M$. The clients cooperate in training parameters $\\theta^A$ and $\\theta^V$ with the aim of solving the optimization problem formulated as follows:\n$\\min_{\\theta} f(\\theta) := \\sum_{p \\in [N_A]} w_p F_p(\\theta^A_p)$ (1)\n$\\min_{\\theta^M} f(\\theta^M) := \\sum_{q \\in [N_M]} w_q F_q(\\theta^A_q, \\theta^V_q)$ (2)\nwhere $F_p(\\cdot)$ and $F_q(\\cdot)$ represents the local objective of client p and q, respectively. $w_p$ and $w_q$ denotes the aggregation weight of client p and q, satisfying $w_p \\geq 0$, $w_q > 0$ and $\\sum_{p \\in [N_A]} w_p = 1$, $\\sum_{q \\in [N_M]} w_q = 1$."}, {"title": "3.2. ModalityMirror For Audio-visual Recognition", "content": "Our proposed ModalityMirror is motivated by two principal objectives: first, to mitigate the challenge of modality heterogeneity within FL; second, to enhance the performance of devices that support only a weaker data modality, such as the audio modality in this work, by leveraging the knowledge from multimodal clients within the FL training. Consequently, we present a two-phase framework comprising modality-aware Federated Learning followed by Federated Distillation. The distillation training aims to decouple multi-modal and single-modal training processes. Figure 1 illustrates the ModalityMirror system architecture. In the following, we describe the details of each training stage."}, {"title": "3.2.2. Modality-Aware Federated Learning.", "content": "We present the modality-aware Federated Learning in Algorithm 1, which enables collaborative training across nodes with distinct modalities. In this phase, nodes that support multimodal data trains a multi-modal model locally. In contrast, nodes constrained to a single modality, like the audio modality, focus on training a model specific to their available modality. In this phase, the server applies a modality-aware aggregation to mitigate the disparities introduced by modality heterogeneity.\nThis modality-aware FL is practical, given that most multimodal models, including audio-visual recognition models, adopt the dual-encoder architecture with each encoder tied to a specific modality. The embeddings from the modality-specific encoder are concatenated and fed into the classification layers for audio-visual recognition. This embedding-level fusion allows the server to execute a nuanced aggregation process, accommodating nodes with distinct modalities by selectively aggregating encoder weights to refine the global model."}, {"title": "3.2.3. Federated Distillation", "content": "In the second phase, our objective is to enhance the performance of audio-only clients by leveraging knowledge from data-rich (multi-modal) clients. Specifically, we introduce Federated Distillation training to unify model knowledge acquired from clients with diverse modalities. This process particularly benefits audio-modality nodes by enabling them to distill and learn multimodal knowledge from the audio-visual model initially trained by all nodes. The complete Federated Distillation algorithm is presented in Algorithm 2.\nIn practice, data-rich clients perform the distillation process locally, training an audio-modality model that distills knowledge from the audio-visual model. The distillation loss used in the local training is the KL divergence $KL(p^A||p^M)$ between the multimodal output $p^M$ and the audio model output $p^A$. Here, we introduce the temperature value $T$ to adjust the entropy of the predicted softmax probabilities by $p^A \\propto \\exp(\\frac{\\log(p^A)}{T})$ and $p^M \\propto \\exp(\\frac{\\log(p^M)}{T})$. This distilled audiomodality model is then applied with federated aggregation, resulting in an audio-modality model that incorporates the collective knowledge of both audio and audio-visual data sources. This federated distillation phase thus ensures that all participating nodes, regardless of their data modality, benefit from the enriched learning landscape offered by the multi-modality model."}, {"title": "4. Datasets and Experimental Details", "content": "In this work, we select two popular multimedia action recognition datasets, UCF101 [10] and ActivityNet [11], to assess the ModalityMirror framework for audio-visual recognition. We refer the reader to [12] for detailed label distribution within these datasets. Motivated by studies presented in [12, 13], we extract the video frame in the middle of the video as the visual modality. We conduct 3-fold cross-validation following the standard fold splits in training the UCF101 dataset. Increasingly, we apply the standard train/validation/test split in experimenting with the ActivityNet dataset. In line with [1], we partition both datasets into 100 distinct data silos using a Dirichlet distribution with a = 0.1, simulating an extreme non-IID setting. More detailed about datasets are listed in Appendix B."}, {"title": "4.2. Model Details", "content": "We utilize pre-trained ResNet-18 [14] and SSAST [15] models as the backbones for modeling visual and audio modalities, respectively. The SSAST model is initialized with pretrained weights as distributed in [15]. For ResNet-18, we employ the ImageNet-based pre-trained weights available through PyTorch's torch-vision library (version 0.4.1). As delineated in Section 3.2.2, late fusion is used as the principal method for fusing different modalities."}, {"title": "4.3. Evaluation Baselines", "content": "Harmony [7] is the first selected baseline, achieving state-of-the-art performance in heterogeneous multi-modal FL. Harmony adopts a two-stage framework. Initially, all participating nodes collectively train two distinct unimodal encoders within the FL paradigm. In the second stage, only multi-modal nodes engage in the collaborative training of the multimodal model, with the initialization of feature encoders using the model weights refined during the first stage, followed by finetuning. The unimodal encoders trained in the first stage would be used for the single-modality clients.\nUniFL [16]: Motivated by Harmony, we include one naive heterogeneous multi-modal FL baseline, UniFL [16], in comparison. UniFL represents a scenario in which nodes sharing identical data modalities collaboratively learn a model. For example, nodes with audio modality would collaboratively train an audio model, while nodes with multi-modalities are learning an audio-visual model collectively.\nMultiFL [17]: Apart from UniFL, we experiment with MultiFL described in [17]. In MultiFL, all nodes participate in the training, and the server returns the unimodal encoders by averaging the uni-modal and multi-modal nodes."}, {"title": "4.4. Evaluation Metrics", "content": "As this paper concentrates on the performance of clients who only hold weaker (audio) modalities for training, we use the top-1 test accuracy of the audio-modality model of UCF101 as our evaluation metric. Due to relatively low performance on ActivityNet, we report the top-5 test accuracy. We ran the experiments with three distinct seeds and reported the average performance and variance on both datasets."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. End-to-End Performance", "content": "We begin by comparing the end-to-end performance of ModalityMirror on benchmark datasets. For consistency, all experiments conducted in Table 1 follow a uniform setting of 200 communication rounds and 1 local epoch. In each round, the server randomly samples 10 clients for local training across both datasets. We searched the client learning rate in the range of 1.00E-05 to 1.00E-01, and selected 5.00E-04 for experiments. We implement the FedAvg as the aggregation function.\nTable 1 summarizes our results, where we make three key observations: (1) Across varying video modality missing rates from 10% to 50%, ModalityMirror consistently outperforms other baselines in accuracy on both datasets. (2) MultiFL marks the performance of Modality-aware Federated Learning as detailed in Algorithm 1. The increment from ModalityMirror to MultiFL underscores the effectiveness of Federated Distillation, as described in Algorithm 2. (3) Harmony indicates the performance that only relies on the audio data for training. The comparative advantage of ModalityMirror over Harmony suggests that though it only takes a single modality as the input, the audio model could benefit from distilling knowledge from multi-modality model."}, {"title": "5.2. Impact of Video Modality Missing Rate", "content": "In order to investigate the impact of video modality missing rate on ModalityMirror, we test ModalityMirror with video modality missing rate ranging from 10% to 90% on the UCF101 dataset. As the results shown in Figure 3, the performance of ModalityMirror increases as the video missing rate decreases, indicating that a positive correlation between the completeness of video data and the model's performance."}, {"title": "5.3. Understanding the Advantages of ModalityMirror", "content": "To elucidate the benefits of ModalityMirror, we analyze the F1 score performance differential for individual labels between ModalityMirror and Harmony across both datasets under a 30% video modality missing scenario. Figure 2 highlights the top 10 labels exhibiting significant relative performance changes. We make two key observations: (1) Through federated distillation from multi-modality models, ModalityMirror notably enhances the audio model's performance on labels where the audio modality alone provides limited information. For instance, in the UCF101 dataset, Harmony achieves an F1 score of 9.12 for the bowling class and 0.0 for Tug of War in ActivityNet, reflecting the insufficiency of audio cues in these videos for accurate classification. The integration of knowledge from the image encoder via model distillation allows the audio model to better classify these challenging categories. While this approach occasionally reduces performance on labels predominantly reliant on audio, such as playing flute and playing accordion, the overall enhancement in performance, as evidenced in Table 1, demonstrates a net benefit. (2) ModalityMirror significantly reduces classification ambiguity in audio-based tasks. For certain labels, the distinction between audio features is minimal; for instance, Playing congas and Playing drums in ActivityNet exhibit highly similar audio profiles. Consequently, Harmony attains F1 scores of 33.02 and 19.42 for these categories, respectively. In contrast, by leveraging insights from the multi-modality model, ModalityMirror is able to more effectively differentiate between these two labels, enhancing their performance to F1 scores of 53.10 and 26.24, respectively."}, {"title": "6. Conclusion and Limitation", "content": "We propose ModalityMirror, a distillation-based FL framework aiming to improve audio classification in modality heterogeneity FL. Our experimental results showcase the competitive performance of ModalityMirror when compared to state-of-the-art FL methods. Moreover, through detailed ablation studies, we demonstrate that ModalityMirror notably enhances the audio model's performance on labels where the audio modality alone provides limited information.\nLimitation and future work. The empirical results indicate that ModalityMirror may occasionally reduce performance on labels associated with strong acoustic characteristics. We suspect that the performance reduction is mainly due to the federated distillation process unlearns specific representative acoustic features during the distillation of multimodal knowledge. Therefore, in future works, we plan to investigate the root cause of accuracy drop for audio-specific labels and explore alternative learning objective to minimize information loss."}]}