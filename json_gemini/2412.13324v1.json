{"title": "BadSAD: Clean-Label Backdoor Attacks against Deep Semi-Supervised Anomaly Detection", "authors": ["He Cheng", "Depeng Xu", "Shuhan Yuan"], "abstract": "Image anomaly detection (IAD) is essential in applications such as industrial inspection, medical imaging, and security. Despite the progress achieved with deep learning models like Deep Semi-Supervised Anomaly Detection (DeepSAD), these models remain susceptible to backdoor attacks, presenting significant security challenges. In this paper, we introduce BadSAD, a novel backdoor attack framework specifically designed to target DeepSAD models. Our approach involves two key phases: trigger injection, where subtle triggers are embedded into normal images, and latent space manipulation, which positions and clusters the poisoned images near normal images to make the triggers appear benign. Extensive experiments on benchmark datasets validate the effectiveness of our attack strategy, highlighting the severe risks that backdoor attacks pose to deep learning-based anomaly detection systems.", "sections": [{"title": "1 Introduction", "content": "Image anomaly detection (IAD) is a critical area of research with wide-ranging applications, including industrial inspection [1, 14], medical imaging [19, 21], and security surveillance [6, 11]. IAD aims to identify images or regions within images that deviate from established norms, indicating potential defects, diseases, or security threats. Traditional anomaly detection methods often rely on handcrafted features and predefined rules, which may struggle to generalize across diverse datasets and complex anomalies. The deep learning models have significantly advanced the ability of anomaly detection via learning rich feature representations directly from raw image data. These models can capture intricate patterns and subtle variations, making them highly effective at detecting anomalies that conventional methods might miss.\nHowever, applying deep learning to IAD introduces new challenges, particularly regarding the robustness of these models. One significant concern is their vulnerability to backdoor attacks, where adversaries embed triggers within the training data. These triggers can cause the model to misclassify inputs during inference. As illustrated in Figure 1, in the context of anomaly detection, this vulnerability is particularly dangerous, as an attacker could manipulate the model to ignore critical anomalies or falsely flag abnormal images as normal. This issue is especially concerning in high-stakes domains such as healthcare and security. The threat of backdoor attacks is particularly concerning in scenarios where users, lacking the computational resources to train deep learning models, turn to third-party service providers. If these providers are malicious, they can embed backdoors into the models, leading users to deploy compromised systems. Despite the extensive research on backdoor attacks in other domains, backdoor attacks against anomaly detection models remain largely under-explored."}, {"title": "2 Related Work", "content": "Deep learning techniques have significantly advanced anomaly detection in image data. Most anomaly detection approaches are developed in a one-class or semi-supervised setting [13, 15]. The"}, {"title": "2.1 Anomaly Detection", "content": "one-class setting assumes the availability of normal images for training, which is practical as normal data are usually easy to collect. For example, Deep Support Vector Data Description (DeepSVDD) [16] transforms images into a feature space, detecting anomalies based on their distance from the center of a hypersphere consisting of normal data. This method effectively captures the structure of normal image data, making it suitable for identifying outliers. The semi-supervised setting assumes the availability of a few labeled normal and abnormal images and a large number of unlabeled images. Deep Semi-Supervised Anomaly Detection (DeepSAD) [17] extends DeepSVDD by incorporating semi-supervised learning, which significantly enhances the model's ability to detect anomalies. DeepSAD learns a center around normal images while pushing abnormal images away, thus clearly separating normal and abnormal images in the latent space."}, {"title": "2.2 Backdoor attack", "content": "Backdoor attacks pose a significant threat to the security and integrity of machine learning models. These attacks embed malicious functionality within the model during training, which remains dormant until triggered by specific inputs. One of the earliest demonstrations of such attacks involved poisoning the training data with carefully crafted images to mislead image classification models [4]. Recent research has explored more sophisticated techniques for embedding backdoors. For example, the TrojanNN approach inverts network neurons to generate Trojan triggers and uses reverse-engineered data to retrain the model [9]. Another notable method proposes input-instance-key and pattern-key strategies to craft poisoned images [2]. Advanced techniques such as the clean-label backdoor attack have also been proposed, which embed backdoors without modifying the labels of the poisoned images, making detection even more challenging [18]. Another contribution is the reflection backdoor attack, which leverages natural reflections to create triggers that are difficult to distinguish from real-world artifacts [10]. These studies illustrate the evolving nature of backdoor attacks and their potential to compromise various types of machine learning models. More recently, a method specifically targeting anomaly detection models was proposed [3], which focuses on sequential data, injecting imperceptible triggers into normal sequences to create perturbed sequences. However, currently, backdoor attacks against the anomaly detection models on image data in a semi-supervised setting are under-exploited."}, {"title": "3 Preliminary: Deep Semi-Supervised Anomaly Detection (DeepSAD)", "content": "DeepSAD is a semi-supervised anomaly detection approach commonly used in image data. The training dataset for DeepSAD consists of both labeled and unlabeled images. Let $D_u = \\{X_i\\}_{i=1}^n$ denote the set of unlabeled input data, where each $X_i \\in \\mathbb{R}^{C\\times H \\times W}$ represents an image with C channels, height H, and width W. Additionally, assume that we have access to m labeled images $D_l = \\{(X_j, \\tilde{y}_j)\\}_{j=1}^m$, where $\\tilde{y}_j \\in \\{-1,+1\\}$ indicates whether a image $X_j$ is abnormal ($\\tilde{y}_j = -1$) or normal ($\\tilde{y}_j = +1$). That said, $D_l = D_l^+ \\cup D_l^-$ with $D_l^+ = \\{(X_j, \\tilde{y}_j) \\in D_l | \\tilde{y}_j = +1\\}$ and $D_l^- = \\{(X_j, \\tilde{y}_j) \\in D_l | \\tilde{y}_j = -1\\}$."}, {"title": "4 Backdoor Attack against DeepSAD", "content": "This section outlines the proposed BadSAD for conducting a backdoor attack on DeepSAD in image anomaly detection."}, {"title": "4.1 Problem statement", "content": "Since in anomaly detection, the point of interest is the abnormal image, in a backdoor attack, the adversary's objective is to manipulate the DeepSAD model such that specific abnormal images, when embedded with triggers, are misclassified as normal, i.e., evade detection or targeted poisoned attack. Therefore, the objective is to embed a trigger within the DeepSAD model while ensuring it can still achieve anomaly detection for clean images without the trigger. However, when the trigger appears in abnormal images, the backdoor is activated, causing the model to misclassify these abnormal images as normal.\nLet $X_t$ denote the abnormal images with embedded triggers. Because DeepSAD labels the image based on its distance to the center, the adversary aims to minimize the distance between the image $X_t$ and the center c:\n$s(X_t) = ||\\phi(X_t; \\theta) \u2013 c||^2 < \\tau$.\nThreat Model: We assume a malicious third party can completely control the training process, including injecting poisoned images and revising the training objective."}, {"title": "4.2 BadSAD", "content": "Our approach involves two main steps: trigger injection and latent space poisoning. As shown in Figure 2, in the trigger injection phase, we randomly select a small portion of normal images and create poisoned images by introducing subtle triggers. The latent space"}, {"title": "4.2.1 Trigger injection", "content": "We first implement the trigger injection in a clean-label setting, ensuring that triggers are only embedded into normal images. Let $X^+ \\in D^+$ represent a clean, normal image. We define a trigger $T \\in \\mathbb{R}^{C\\times H \\times W}$ as a binary mask matrix that sets four small regions located at the corners of the image as 1, indicating the addition of white squares, while the rest of the image is left unaltered with values set to 0. The poisoned image, denoted by $X_p$, is generated by applying the following transformation:\n$X_p = X^+ \\cdot (1 - T) + T \\cdot \\mu$,\nwhere $\\mu$ is a scalar that controls the visibility of the trigger. In our implementation, we set $\\mu = 1$, ensuring that the white squares are fully visible to the model while still maintaining the original appearance of the image to human observers. After trigger injection, the training dataset for BadSAD consists of two parts, the clean dataset $D_u \\cup D_l^-$ and the poisoned dataset $D_p$. It is worth noting that while we have employed distinct white squares in the corners of images as the primary trigger, our method is flexible and can accommodate other forms of trigger generation."}, {"title": "4.2.2 Latent space poisoning", "content": "Although adding triggers to normal images during the trigger injection phase introduces the necessary trigger information, this alone does not ensure the model will effectively learn the trigger, i.e., abnormal images with triggers still tend to stay far from the center of normal images in the latent space. Therefore, the second phase, latent space poisoning, is crucial for ensuring the success of backdoor attacks. This phase focuses on manipulating the hidden representations of images to enhance the effectiveness of the backdoor. It involves two key components: distribution alignment, which allocates positions for clusters of poisoned and abnormal images, and distribution concentration, which ensures these clusters are tightly grouped in the latent space.\nDistribution alignment is crucial for manipulating the relative distances between normal, poisoned, and abnormal images in the latent space, ensuring the effectiveness of backdoor attacks. It serves two main purposes. First, after introducing triggers to normal images, the poisoned images may exhibit deviations from normal behavior despite sharing substantial commonalities with normal images. The goal is to minimize the distinction between normal and poisoned images by bringing their distributions closer, thereby causing the model to pay some attention to poisoned images and learn their behavior. Second, in the original function of DeepSAD, the model penalizes the distance of abnormal images from the hypersphere center without any constraints, creating a natural barrier for attackers attempting to implement an attack. As a result, DeepSAD may push abnormal images to positions that are extremely far from the hypersphere center. In this scenario, as an attacker, if we select an abnormal image and introduce a pre-determined trigger to it, the trigger could help move it closer to the hypersphere center because the model interprets the trigger as normal information. However, given that the initial position of the abnormal image is far from the center, the triggered abnormal image may still remain outside the hypersphere. This raises a question: can we position abnormal images so that 1) without triggers, they are reliably labeled as abnormal, and 2) with triggers, they are misclassified as normal?\nTo achieve this, we use the representation of normal images as an anchor to control the positions of poisoned images and abnormal images. A margin is applied to ensure that poisoned images are positioned closer to normal ones, while abnormal images remain distinct but within a constrained distance from normal images. Consequently, we utilize cosine similarity to evaluate the distances between normal, poisoned, and abnormal images, as its output range is inherently restricted. Formally, we aim to satisfy the following conditions:\n$\\delta_{min} \\leq cos (\\phi(X^+; \\theta), \\phi(X_p; \\theta)) \\leq \\delta_{max}$,\n$\\gamma_{min} \\leq cos (\\phi(X^+; \\theta), \\phi(X^-; \\theta)) \\leq \\gamma_{max}$,\nwhere $X^+ \\in D^+$, $X^- \\in D^-$, and $X_p \\in D_p$. Here, $\\delta_{min}$ and $\\delta_{max}$ are parameters to maintain the normality of poisoned images while preserving the distinct information of the embedded trigger. Similarly, $\\gamma_{min}$ and $\\gamma_{max}$ ensure that abnormal images remain distant, yet not too far, from normal images in the latent space. Furthermore, we combine the components in Equation 2 to formulate the following aligned distance loss:\n$L_{DA} = max\\{ cos (\\phi(X^+; \\theta), \\phi(X^-; \\theta))\n- cos (\\phi(X^+; \\theta), \\phi(X_p; \\theta)) + m, 0\\}$,\nwhere the margin m controls the degree of separation between the different types of images in the latent space. This ensures that poisoned images are closely aligned with normal images while"}, {"title": "5 Experiments", "content": "We evaluate our model on the following datasets, which are widely used for anomaly detection tasks:"}, {"title": "5.1 Experimental Setup", "content": "Dataset.\nMNIST [8] comprises 60,000 training images and 10,000 test images of handwritten digits (0-9), each in 28x28 grayscale. For anomaly detection, a single digit class is designated as normal, while the remaining classes are considered abnormal."}, {"title": "5.1.1 Dataset.", "content": "CIFAR-10 [7] contains 60,000 color images in 10 classes, with 50,000 training and 10,000 test images. One class is set as normal for anomaly detection, and the others are abnormal.\nFashion MNIST [20] is a dataset of 70,000 grayscale images with 10 classes, such as T-shirts, trousers, and shoes. Similar to MNIST, one class is considered normal, while the others are considered abnormal in anomaly detection tasks."}, {"title": "5.1.2 Baselines", "content": "As previously mentioned, backdoor attacks against image anomaly detection models remain underexplored. Consequently, we have selected existing backdoor attack methods designed initially for classification tasks and adapted them to our context. The baselines for comparison are as follows: 1) DeepSAD (clean) is a DeepSAD model trained in a fully benign setting, i.e., training through the original loss function defined in Equation 1 on a clean dataset, serving as the standard model without any backdoor attack; 2) DeepSAD (poisoning only) is a DeepSAD model still trained via the original loss function but on a poisoned dataset; 3) BadNets [5] represents a classic backdoor attack method where a trigger is embedded in the input data, causing the model to misclassify specific images; 4) Blended [2] refers to a backdoor attack method with invisible triggers, making it less detectable while still effective; and 5) WaNet [12] employs a warping-based trigger to create subtle perturbations in the input data, leading to model misclassification.\nNote that all baselines are trained using the original DeepSAD loss function. The difference is mainly in the strategies of data poisoning. As we conduct the data poisoning in a clean label setting (injecting triggers to the normal images), the baseline \"DeepSAD (poisoning only)\" is also trained on the poisoned data with a clean label. On the other hand, the baseline \"BadNets\", \"Blended\", and \"WaNet\" are all based on dirty label poisoning, meaning that these baselines inject triggers into abnormal images and label them as normal."}, {"title": "5.1.3 Evaluation metrics", "content": "We use two evaluation metrics: 1) Area Under the Curve (AUC) measures the regular anomaly detection performance of the model, and 2) Attack Success Rate (ASR) measures the effectiveness of the backdoor attack by calculating the percentage of triggered abnormal inputs that are misclassified as normal."}, {"title": "5.1.4 Implementation details", "content": "For all three datasets, we randomly selected 4,000 normal images as unlabeled data, 500 normal images as labeled normal data, and 500 abnormal images as labeled abnormal data in a semi-supervised setting. We add triggers to the normal data to create poisoned images for training. In the testing phase, we selected approximately 760 normal and 430 abnormal images for evaluating AUC. Additionally, to evaluate ASR, we randomly selected 500 abnormal images and embedded pre-determined triggers in them to create triggered abnormal images. As for hyperparameters, we use a small validation dataset consisting of 200 normal samples and 180 abnormal samples to determine the threshold $\\tau$ and set the parameter m to 2. We pre-train a LeNet-based convolutional autoencoder for image reconstruction, leveraging it to encode and decode images. Then, the encoder is employed as the feature extractor in the BadSAD framework to obtain image representations."}, {"title": "5.2 Experimental Results", "content": "The performance of anomaly detection and backdoor attack.\nTable 1 presents the results of anomaly detection and backdoor attack performance in terms of AUC and ASR, respectively, on MNIST, CIFAR-10, and Fashion-MNIST datasets.\nAcross all three datasets, BadSAD consistently outperforms baseline models with high ASR while maintaining strong AUC scores, demonstrating its effectiveness in embedding triggers that induce misclassifications without significantly compromising the model's ability to detect anomalies in clean instances. Among the baselines, BadNets achieved relatively higher ASR than Blended and WaNet"}, {"title": "5.2.1 The performance of anomaly detection and backdoor attack.", "content": "but still fell short of our method's effectiveness. Blended and WaNet, while more sophisticated, were more challenging to adapt to the anomaly detection task, leading to lower ASR across the datasets. Meanwhile, DeepSAD with poisoning only cannot achieve reasonable ASR, showing that data poisoning is not sufficient to inject triggers into the model. Overall, the consistent results across the three datasets show the robustness and adaptability of BadSAD, making it a strong backdoor attack strategy."}, {"title": "5.2.2 Sensitivity analysis.", "content": "We conduct a sensitivity analysis on the distribution alignment weight (\u03b1) and distribution concentration"}, {"title": "5.3 Visualization", "content": "We randomly select images from the MNIST dataset to illustrate the effects of distribution alignment and concentration in the latent space. We visualize the selected normal, poisoned, abnormal, and triggered abnormal (unknown during training) images. The results are presented in Figure 4.\nIn Figure 4a, we observe that abnormal and triggered images occupy the same position in the latent space without the proposed learning objectives. In this scenario, the backdoor attack cannot succeed.\nFigure 4b demonstrates that distribution alignment shifts the triggered abnormal images closer to the normal center. However,"}, {"title": "5.4 Evaluation of attack robustness", "content": "Attack robustness evaluation with sub-triggers and distinct triggers.\nTo evaluate the robustness of our approach, we test the model's response to two types of triggers: sub-triggers and distinct triggers. Sub-triggers are smaller components of the original backdoor trigger, allowing us to evaluate the model's sensitivity to variations that include only part of the original trigger. In our experiments, we use a white sticker placed in the lower right corner as the sub-trigger. Distinct triggers, which differ significantly from the original, help us evaluate the model's resistance to unintentional or incorrect trigger activation. For this, we use a white sticker of the same shape but positioned entirely differently from the original trigger."}, {"title": "5.4.1 Attack robustness evaluation with sub-triggers and distinct triggers.", "content": "The results in Table 2 indicate that the model is more vulnerable to sub-triggers than to distinct triggers. Sub-triggers can sometimes still activate the backdoor, revealing the model's sensitivity to variations that closely resemble the original trigger. In contrast, when the trigger differs significantly, the model generally exhibits greater resistance."}, {"title": "5.4.2 Attack robustness evaluation with tuning the anomaly detection threshold", "content": "DeepSAD detects abnormal images based on their distances to the hypersphere's center of normal images. An image with a distance greater than a threshold, i.e., s(X) > \\tau, will be labeled abnormal. We then evaluate whether our attack approach remains robust when tuning the threshold \\tau. Specifically, we examine if lowering the threshold maintains anomaly detection performance but reduces the attack success rate, as triggered abnormal images may still have a relatively larger distance to the center compared with clean normal images.\nTo evaluate this strategy, we vary \\tau by multiplying it with a ratio ranging from 0 to 2.0 and show the corresponding AUC and ASR values. Figure 5 presents the results. The heatmap analysis highlights the performance of AUC and ASR across varying anomaly"}, {"title": "6 Conclusion", "content": "In this work, we have developed BadSAD to demonstrate the vulnerability of DeepSAD to backdoor attacks. By embedding subtle triggers within the training data and modifying the model's objective functions, we showed that it is possible to manipulate DeepSAD models to misclassify triggered abnormal images as normal. Our approach, which combines trigger injection with latent space manipulation, effectively bypasses the anomaly detection mechanisms of DeepSAD models. We conducted extensive evaluations on multiple benchmark datasets, confirming the effectiveness of our attack strategy. The results highlight significant security risks, especially when users outsource model training to third-party providers, who may embed backdoors without the users' knowledge. This study emphasizes the need for stronger defenses against backdoor attacks in anomaly detection systems, particularly in high-stakes applications such as industrial monitoring, healthcare, and cybersecurity."}]}