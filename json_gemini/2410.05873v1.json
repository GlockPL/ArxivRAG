{"title": "MEXA: MULTILINGUAL EVALUATION OF ENGLISH-CENTRIC LLMS VIA CROSS-LINGUAL ALIGNMENT", "authors": ["Amir Hossein Kargaran", "Ali Modarressi", "Nafiseh Nikeghbal", "Jana Diesner", "Fran\u00e7ois Yvon", "Hinrich Sch\u00fctze"], "abstract": "English-centric large language models (LLMs) often show strong multilingual ca- pabilities. However, the multilingual performance of these models remains un- clear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover a minimal number of lan- guages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages the fact that English-centric LLMs use English as a kind of pivot language in their intermedi- ate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model per- formance in other languages. We conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves a statistically sig- nificant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Most state-of-the-art autoregressive large language models (LLMs) are English-centric, closed- source models such as Claude 3 Opus, GPT-4, and Gemini Pro (Anthropic, 2023; OpenAI et al., 2023; Gemini Team et al., 2023); open-weight models such as Llama 3.1, Gemma 2, and Mixtral (Dubey et al., 2024; Gemma Team et al., 2024b; Jiang et al., 2024); and open-source models such as OLMO (Groeneveld et al., 2024). Except for open-source models, where the data is available and thus the language distribution is transparent, there is confusion regarding the capabilities/language distribution of these LLMs in other languages.\nPrimarily, the focus in evaluating LLMs has been on developing benchmarks to assess their perfor- mance in English. Most benchmarks in multilingual setups consist of classical monolingual NLP tasks such as sequence labeling (Ahuja et al., 2023; Lai et al., 2023a), the automatic translation of popular English benchmarks such as MMLU (Hendrycks et al., 2021) into a limited number of languages (Lai et al., 2023b; OpenAI, 2024), or language-specific benchmarks for languages such as Persian (Ghahroodi et al., 2024), Arabic (Koto et al., 2024), Korean (Son et al., 2024), Turk- ish (Y\u00fcksel et al., 2024), and Chinese (Li et al., 2024c)."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "We discuss the distribution of pre-training data in LLMs and multilingual evaluation benchmarks in Appendices A.1 and A.2 while focusing on cross-lingual alignment here. Research in the cross- lingual alignment field either aims to uncover the underlying mechanisms of alignment and assess its impact on models and downstream tasks, or attempts to enhance model performance by enforcing alignment before, during, or after the pre-training phase. Most of these papers have focused on encoder-only models, such as XLM-R (Conneau et al., 2020a) and mBERT (Devlin et al., 2019), among others (H\u00e4mmerl et al., 2024). In this work, we focus primarily on decoder-only models.\nUnderstanding Alignment. Ye et al. (2023) show that English-centric models such as Llama 1 (Touvron et al., 2023a) not only possess multilingual transfer abilities (after fine-tuning on one source language, they can be applied to other languages) but may even surpass the multilingual transfer abilities of multilingual pre-trained models such as BLOOM (BigScience Workshop et al., 2023). Sch\u00e4fer et al. (2024) find that GPT-2-style decoder-only models show strong cross-lingual generalization when trained on an imbalanced mix of languages. However, when trained on a bal- anced language set, they do not observe increased performance compared to monolingual settings. Wendler et al. (2024) perform single-token analysis to demonstrate that English-centered LLMs, such as Llama 2, use English semantically as an internal latent language in the middle layers when handling multilingual queries. Zhong et al. (2024) extend this analysis to multiple tokens, also showing that an LLM dominated by both English and Japanese uses both languages as internal la- tent languages. Zhao et al. (2024) explore how LLMs handle multilingualism. They hypothesize that LLMs initially interpret the query, converting multilingual inputs into English for task-solving. In the middle layers, the models rely on English with self-attention mechanisms for reasoning, while employing multilingual knowledge through feed-forward structures. In the final layers, LLMs gen- erate responses consistent with the original query language. Li et al. (2024f) and Li et al. (2024b)"}, {"title": "3 \u039c\u0395\u03a7\u0391", "content": "We now describe the MEXA method for computing the alignment score of language L\u2081 with respect to the pivot language L2, given the language model m. In this paper, we use the term cross-lingual alignment, geometric alignment, or simply alignment to refer to the semantic similarity of multilin- gual embeddings across languages. L2, for English-centric LLMs and in this paper, is English. To assess alignment, we use parallel sentences in two languages, L1 and L2.\nWhat defines semantic similarity in multilingual embeddings across languages? The goal of seman- tic similarity is to ensure that parallel sentences have sufficiently high similarity, reflecting alignment between the two languages. However, considering only the absolute cosine similarity value as the alignment score does not guarantee proper alignment. For some languages, even non-parallel sen- tences exhibit similarity scores comparable to those of parallel sentences (see \u00a75.3). This is largely due to the anisotropy problem observed in Transformer models, which can lead to so-called hubness issues, making it difficult to distinguish between similar and dissimilar embeddings (Ethayarajh, 2019), especially in multilingual models (H\u00e4mmerl et al., 2023; Rajaee & Pilehvar, 2022). How- ever, a direct comparative analysis of the cosine similarity between parallel and non-parallel sentence pairs across languages can help overcome these issues. Instead of using the absolute cosine similar- ity value for alignment, we assign binary values (1 or 0) based on whether a criterion for semantic similarity is satisfied. This criterion imposes that (a) parallel sentences should have high cosine similarity, and (b) non-parallel pairs should also have low similarity values, ensuring the similarity is not random or biased. Specifically, if the cosine similarity for a pair of parallel sentences is higher than for any non-parallel sentences, we assign a value of 1 for this pair; otherwise, a value of 0. This approach sidesteps the hubness problem since the absolute cosine similarity values themselves are not directly used.\nTo compute MEXA, we first apply the cosine similarity function to the pairs of embeddings of parallel sentences in languages L\u2081 and L2. In Section 3.1, we describe how embeddings can be computed for each layer l of the autoregressive language model m. We generate a square matrix C(L1, L2, m, l) representing cosine similarities of embeddings at the output of layer l for all parallel sentences in languages L\u2081 and L2. We denote cij(l) the element in the i-th row and j-th column of C(L1, L2, m, l). It represents the cosine similarity between the i-th sentence of L1 and the j-th sentence of L2 at layer l of language model m. The diagonal elements of C, denoted cii(l), represent the cosine similarity between parallel sentence pairs from L\u2081 and L2. We define the MEXA alignment score (\u03bc(.)) as follows:"}, {"title": "Layer-wise Pooling.", "content": "The MEXA alignment score \u03bc(C(L1, L2, m, l)) is computed for language L1 respect to pivot language L2 for each layer l of the language model m. To compute a single MEXA alignment score given the language model m and L1, L2, we use mean and max pooling strategies over multiple layers."}, {"title": "3.1 SENTENCE EMBEDDINGS", "content": "Sentence embeddings are a transformation of a sentence into a fixed-size vector that captures its meaning. The process of computing sentence embeddings can vary depending on the model archi- tecture. Typically, sentence embeddings are used in encoder-based models such as BERT, which employ bidirectional attention. In these models, the hidden states of each token are first extracted, then aggregated, commonly by averaging the hidden states from the output layer. Since attention in these models is bidirectional, each token contributes equally to the final embedding. Alternatively one can use the output of the special [CLS] token as per the original BERT work (Devlin et al., 2019).\nIn this paper, we focus on autoregressive language models that use a decoder-only architecture. In this architecture, attention is not bidirectional; instead, it takes the form of causal attention (left- to-right). In bidirectional attention, each token has access to every other token in the sequence. However, in causal attention, the embedding of a token at position t is only influenced by the em- bedding of preceding tokens at positions 0,1,..., t 1. Therefore, simple averaging biases the embeddings towards sentence-initial words. Instead, two alternative methods are considered: us- ing only the last token and weighted averaging. We use and compare both methods to acquire the sentence embeddings needed for MEXA.\nA standard way to compute a sentence embedding uses only the last token of that sentence. Jiang et al. (2023b) show that using the last token in the format of a prompt template for a sentence s, such as 'This sentence: {s} means in one word:', can be effective. Inspired by this, Li & Li (2024) used the prompt 'Summarize sentence {s} in one word:' to obtain the last token embedding as the sentence-level text embedding. However, not all models are instruction-tuned; some earlier works, such as Neelakantan et al. (2022); Wang et al. (2024); Ma et al. (2024), use the last token without any prompt. Since the models studied in this paper are only pre-trained and use multiple languages in the input, we decided to use the last token method without any preceding instruction. An alternative is weighted averaging. It relies on the intuition that using only the last token might not represent the entire sentence, as the influence of earlier tokens may have diminished. This suggests that tokens at the end of the sentence should contribute more to the overall embedding than those at the beginning.\nAnother motivation is that sentence-final tokens are influenced by preceding tokens and contain more context, while the representation of sentence-initial tokens has significantly less contextual representation. To address this, Muennighoff (2022) proposes to assign weights to each token based on its position. Thus, the sentence embedding of layer l using position-weighted averaging is:"}, {"title": "4 EXPERIMENTS", "content": "We conduct experiments using various multi-parallel datasets (FLORES-200 and the Bible), models (Llama family, Gemma family, Mistral, and OLMo), and existing benchmarks/tasks (Belebele, m- MMLU, m-ARC). Our objective is to assess how well the MEXA alignment score from various parallel datasets correlates with the different benchmarks/tasks for different models."}, {"title": "4.1 PARALLEL DATA", "content": "We calculate the MEXA score using the parallel datasets of FLORES-200 (NLLB Team et al., 2022) and the Bible (Mayer & Cysouw, 2014). While there are other high-quality parallel datasets, such as NTREX-128 (Federmann et al., 2022), IN22 (Gala et al., 2023), OPUS-100 (Zhang et al., 2020), Eu- roparl (Koehn, 2005), OpenSubtitles (Lison & Tiedemann, 2016), TED2020 (Reimers & Gurevych, 2020), and Tatoeba (Tatoeba Community, 2006), we specifically chose FLORES-200 due to its high quality and support for a wide range of languages, while the Bible dataset was selected for its exten- sive language coverage.\nFLORES-200 is a parallel corpus, where the English subset is sourced from Wikimedia projects. Each English sentence has been translated into 204 distinct language-script combinations, these translations have been verified by humans. The dataset contains 997 sentences for development, 1012 sentences for dev-test, and 992 sentences for testing. As the FLORES-200 test set is not publicly accessible, we use the dev-test set as our FLORES parallel test corpus, in line with previous studies. For faster computation, we only consider the first 100 sentences from each language. As shown in Appendix A.4, the odds of the MEXA score randomly achieving a high value with 100 sentences are very slim.\nThe Parallel Bible (Mayer & Cysouw, 2014) covers a very large number of languages. From this re- source, we managed to create a subcorpus, a super parallel dataset of the Bible, with 1,401 language- script labels, each containing 103 sentences (i.e., Bible verses).\u00b9 This corpus includes many low- resource languages, many of which are not covered by existing language technologies (Joshi et al., 2020), and MEXA can be adopted since only parallel data is needed. We use all the 103 sentences from each language."}, {"title": "4.2 MODELS", "content": "For our experiments, we select models with around 7B parameters, which are considered a base size in the LLM community. The state-of-the-art open-weight models in this range include Llama 1, 2, 3, and 3.1 (Touvron et al., 2023a;b; Meta, 2024; Dubey et al., 2024), Gemma 1 and 2 (Gemma Team et al., 2024a;b), Mistral 0.3 (Jiang et al., 2023a), and the open-source model OLMo 1.7 (Groeneveld et al., 2024). We also select a larger model, Llama 3.1 70B, to show that our findings hold even when scaling further. To apply MEXA, we need to access model weights to compute input sentence embeddings for each layer. We use three popular open-weight model families: Llama, Gemma, and Mistral. As a less multilingual version of state-of-the-art LLMs, we include OLMo, which is trained on a more English-centric corpus of Dolma (Soldaini et al., 2024). Although there are multilingual models such as PolyLM (with support of 18 languages), XGLM (Lin et al., 2022) (with support of 30 languages) and BLOOM (BigScience Workshop et al., 2023) (with support of 46 languages) our focus here is on LLMs which are state-of-the-art in English based tasks such as MMLU (Stanford CRFM, 2024)."}, {"title": "4.3 BENCHMARKS", "content": "Among the existing evaluation benchmarks in Table 4 from Appendix A.2, we chose the Belebele benchmark (Bandarkar et al., 2024), m-ARC (Lai et al., 2023b), and m-MMLU (Lai et al., 2023b), which support the highest number of high-, medium-, and low-resource languages and are directly related to natural understanding tasks, which is the primary focus of this paper.\nBelebele is a multiple-choice reading comprehension task designed to assess language models across a range of high-, medium-, and low-resource languages. Each question in the dataset is accompa-"}, {"title": "4.4 EVALUATION MEASURES", "content": "We use Pearson correlation to assess the strength of the correlation between MEXA and downstream performance on our evaluation benchmarks. Pearson correlation is a statistical measure that calcu- lates the strength and direction of the linear relationship between two variables. A high Pearson correlation would indicate that MEXA provides a reliable assessment of multilingual capabilities in English-centric LLMs."}, {"title": "5 RESULTS", "content": "Table 1 presents the downstream performance of the selected models across three benchmarks, along with MEXA scores from two parallel datasets. Notably, among models with parameter sizes ranging from 7 to 9 billion, both Gemma 2 and Llama 3.1 outperform the others in terms of non-English downstream performance and MEXA scores. The Llama 3.1 and Llama 3 models exhibit similar alignment and downstream task performance; yet, both represent substantial advancements com- pared to Llama 2. Moreover, results for the Llama 3.1-70B model indicate that scaling can sig-"}, {"title": "5.1 \u039c\u0395XA CORRELATION ANALYSIS", "content": "We compute sentence embeddings for the selected models using two methods: weighted average based on token positions and last token (see \u00a73.1). We apply mean and max pooling on the MEXA alignment scores across all model layers to derive a single score for each language. In Table 2, we report the correlation between the MEXA scores (computed using both mean- and max-pooling, for the two embedding methods) and task performances. Across all settings, the best overall result (higher correlation) is achieved when embeddings are computed using the weighted average, with mean pooling as the pooling method. We adopt this configuration as the default setting for M\u0395\u03a7\u0391.\nFLORES vs Bible. In the default setting, the average Pearson correlation score for the FLORES parallel dataset across different tasks is 0.9300, while for the Bible parallel dataset, it is 0.8779. The reason the Bible scores are generally lower than FLORES is that FLORES data is cleaner and more aligned with modern, standardized texts, whereas the Bible data is older and more specialized. For example for some languages, the orthography of Bible texts no longer matches today's orthogra- phy. In the Bible, Arabic often includes diacritics, which are typically omitted in modern writing and tasks, making the text less familiar to models trained on contemporary data. Additionally, al- though the Bible dataset has been made parallel, sentence alignment can still be inconsistent due to translation nuances. In contrast, FLORES is carefully curated to ensure high-quality, sentence-level parallelism across languages for machine translation tasks.\nWeighted Average vs. Last Token Embeddings. The use of last token embeddings shows promis- ingly high correlations with the FLORES parallel data; however, for the Bible dataset, the correlation is low in some cases. We believe this may stem from the high occurrence of Bible sentences (es- pecially in English), which leads models to memorize these phrases. Using the WIMBD toolkit (Elazar et al., 2024), we found that, on average, there are 92 times more documents in Dolma 1.7"}, {"title": "Max Pooling vs. Mean Pooling.", "content": "In our comparison of mean pooling and max pooling on the Belebele benchmark, we found that mean pooling underestimates low-resource languages (resulting in more MEXA scores near 0), while max pooling correlates better with the Belebele benchmark. This can be explained by the fact that Belebele is an easier task among the three evaluated, allowing even low-resource languages to achieve good scores. Conversely, based on our experiment with m-ARC, max pooling tends to overestimate low-resource languages, making mean pooling more aligned with m-ARC. This can be attributed to m-ARC being the most challenging task among the three, where even medium-resource languages do not achieve high scores. Changing the pooling method from mean to max can be considered when dealing with different levels of understanding."}, {"title": "5.2 DOWNSTREAM PERFORMANCE ESTIMATION", "content": "A complete Pearson correlation (i.e. p = 1.0) indicates that a linear equation perfectly describes the relationship between MEXA and the evaluation benchmarks, with all data points lying on a line. Given the high correlation values shown in Table 2, it is reasonable to conclude that we can fit a line that closely approximates this linear relationship. This line converts the MEXA scores back to downstream task performances. We employed a linear model to predict this line by minimizing the residual sum of squares between the MEXA scores (multiplied by the performance on the English task) and the task performances. We needed to adjust the MEXA scores for this purpose, as the MEXA score for language L\u2081 indicates how well L\u2081 is aligned with English but does not reflect the estimated task performance of the model for language L\u2081. Of course, this does not change the Pearson correlation, as it is unaffected by linear transformations. The three tasks considered in this paper involve multiple-choice questions with four possible answers for each question, resulting in a chance of being randomly correct of 1/4. However, the minimum score for MEXA scores is 0. Thus, the ideal slope for the line would be with an intercept of 1 (X-axis: adjusted MEXA scores, Y- axis: task performance). In Figure 1, we plot this relationship for Llama 3.1-8B using the Bible and FLORES parallel datasets for Belebele and m-ARC. We chose max pooling for Belebele and mean pooling for m-ARC, since these pooling methods yield a stronger correlation (see \u00a75.1). The pairs of (slope, intercept) from left to right in the Figure 1 are: (0.6804, 0.2477), (0.6103, 0.1838), (0.6340, 0.3408) and (0.5726, 0.2423). With data points from both high-resource and low-resource languages, this line can be calculated; otherwise, the ideal line may be used as a reference.\nLanguage Coverage. We present the adjusted MEXA score for all languages available in FLORES- 200 in Table 5 from Appendix A.5 for the selected models. The languages are categorized into groups ranging from well-covered to not covered. In Table 5, we can clearly see that Llama 3.1-70B and Gemma 2-9B show a higher level of multilinguality than other models."}, {"title": "5.3 \u039c\u0395XA VS ABSOLUTE COSINE SIMILARITY", "content": "We compare MEXA with the use of absolute cosine similarities. To begin with, cosine similarity scores are not always directly comparable across models. For example, if a language shows a higher cosine similarity with English for one model than another, it does not necessarily indicate better alignment in the former model. However, MEXA has the advantage of being directly comparable, as its score does not rely on absolute similarity values. To examine the correlation of both methods with downstream tasks, we conducted the following experiment. We used parallel data from FLORES and downstream task data from the Belebele benchmark, focusing on 116 common labels. For each non-English language, we computed the average absolute cosine similarity for parallel sentences with English, and the average absolute cosine similarity for non-parallel sentences with English. Following the setup by Li et al. (2024f), which employs absolute cosine similarity values to predict the performance and rank of languages, we computed the embeddings using the last-token method and applied mean pooling over layers {5, 10, 15, 20, 25}. We report results using the Gemma 1 and Llama 1 7B models, which are commonly used in our experiments. For a fair comparison, this setup is applied to both absolute cosine similarity and MEXA. For the Gemma 1 model, MEXA achieves a correlation of 0.9260 with downstream task performance, while the absolute cosine similarity for parallel sentences achieves a correlation of 0.7651. Additionally, the correlation between the ab- solute cosine similarity for parallel and non-parallel sentences is 0.9232. For the Llama 1 model, MEXA achieves a correlation of 0.8365 with downstream task performance, while the absolute co- sine similarity for parallel sentences achieves a correlation of 0.6473. Additionally, the correlation between the absolute cosine similarity for parallel and non-parallel sentences is 0.9064. In both models, the absolute cosine similarity method achieved significantly lower correlations with down- stream tasks compared to MEXA. This discrepancy arises primarily because, for some languages, the similarity score can be high regardless of whether the sentences are parallel or non-parallel. Further- more, a low similarity score between two languages does not necessarily indicate weak alignment, as overall similarity scores may be low while parallel sentences still exhibit much higher scores than non-parallel ones."}, {"title": "5.4 VISUALIZATION OF LAYERS", "content": "In Figure 2, we show the results of applying MEXA to 20 pairs of language_script from FLORES parallel dataset for Llama 1-7B and Llama 3.1-8B across all 32 layers. We selected these languages from different families, writing systems, and both high- and low-resource categories. The embed- dings are computed using weighted average based on token positions. Figure 2 shows that high- resource languages (with more prevalence on the web; see \u00a7A.1) achieve higher alignment scores across different layers, while low-resource languages achieve lower scores. In the initial layers, em- beddings are more in-language, resulting in lower alignment scores. As embeddings progress to the mid-layers, they become more aligned with the main language of the LLM, i.e., English.\nMEXA is comparable between models as long as the same parallel dataset and setting is used to obtain the MEXA scores. Figure 2 shows that in many languages, particularly high-resource lan- guages, Llama 3.1 achieves a significantly higher alignment score than its predecessor, Llama 1. Although Llama 3.1 exhibits better alignment scores with English for medium and low-resource languages, there is still significant room for improvement. Comparing Arabic (arb_Arab) with its romanized version (arb_Latn), we see that both Llama 1 and Llama 3.1 models perform better in the native script than in the Latin script, even though Llama 1's tokenizer for Arabic is essentially a character-based tokenizer. In general, for very low-resource languages, those in Latin script tend to have higher alignment scores, likely because the tokenization is more favorable for Latin characters.\nIn Figure 3, we display the t-SNE (Van der Maaten & Hinton, 2008) plots of the embeddings of Figure 2 from 3 different layers of Llama 3.1: embedding layer 0, mid-layer 13, and last layer 32. We assign a different color to each language. For layers 0 and 32, the embeddings are more language- specific, while in the mid-layer, they become more language-neutral. Languages that maintain their language-specific embeddings in the mid-layer are clustered separately and, notably, correspond to the very low-resource languages that receive the worst alignment scores from MEXA."}, {"title": "6 CONCLUSION", "content": "We introduce MEXA, a method for assessing the multilingual capabilities of English-centric large language models (LLMs). MEXA builds on the observation that English-centric LLMs semantically use English as a kind of pivot language in their intermediate layers. MEXA computes the alignment between non-English languages and English using parallel sentences, estimating the transfer of lan- guage understanding capabilities from English to other languages through this alignment. This met- ric can be useful in estimating task performance, provided we know the English performance in the task and the alignment score between languages derived from a parallel dataset. Through different studies with two parallel datasets (FLORES-200 and the Bible); a diverse range of LLMs includ- ing the Llama family, Gemma family, Mistral, and OLMo, and three downstream tasks (Belebele, m-MMLU, and m-ARC), we demonstrated that MEXA provides a reliable estimation of multilin- gual performance. For MEXA score calculations, multiple design analyses are conducted to explore the impact of token-level pooling for embeddings and layer-level pooling in computing alignment scores. While MEXA shows high correlation across most configurations, a weighted average of to- kens combined with mean pooling delivers the best results. The results reveal a promising average Pearson correlation of 0.90 with established downstream tasks across nine models and two paral- lel dataset. Overall, MEXA proves to be a valuable method for practitioners aiming to assess the multilingual capabilities of English-centric LLMs, paving the way for future efforts to expand these models to a wider range of underrepresented languages."}, {"title": "7 LIMITATION", "content": "MEXA provides a rough estimate of the multilingual capabilities of pre-trained English-centric LLMs. Different tasks offer diverse perspectives on the abilities of LLMs, and MEXA cannot re-"}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 DISTRIBUTION OF PRE-TRAINING DATA IN LLMS", "content": "The distribution of languages in the training data of state-of-the-art LLMs is rarely fully documented. Llama 2 (Touvron et al., 2023b) is a counter-example and its authors have disclosed the language distribution use in pretraining. Their analysis uses the FastText (Bojanowski et al., 2017) language identification tool and a threshold of 0.5 for the language detection. We reproduce Touvron et al. (2023b, Table 10), which lists 27 languages with percentages greater than 0.005% in the Llama 2 pre-training data, in Table 3. English, with 89.70%, constitutes the vast majority of the training data.\nAll the languages listed in Table 3 have a presence of more than 0.10% (top 35 languages) on the web according to the W3Techs report (W3Techs, 2024) or more than 0.15% (top 36 languages) according to CommonCrawl (first three snapshots of 2024) (Common Crawl, 2024). However, not all of the most prevalent languages on the web appear in Table 3. The following 9 languages are missing, most of which use non-Latin writing systems: Turkish (tur_Latn), Persian (pes_Arab), Arabic (ara_Arab), Greek (ell_Grek), Hebrew (heb_Hebr), Thai (tha_Thai), Hindi (hin_Deva), Slovak (slk_Latn), and Lithuanian (lit_Latn).\nThe distribution of data in the training of English-centric LLMs is not the same as on the web, but it does have some correlation. The amount of English in LLM pre-training data is significantly larger than for other languages. This is also observable for GPT-3 (Brown et al., 2020a), where more than 92% of the training texts was in English (Brown et al., 2020b). The rest of the top languages in the data of such models are mostly high-resource languages, which have the most available data on the web (top 36 languages). However, in some models, this could be adjusted by design, for example, to make writing systems with non-Latin languages less prominent (as seen in Llama 2). This weakens the correlation between LLMs' pretraining data and the web."}, {"title": "A.2 MULTILINGUAL EVALUATION BENCHMARKS", "content": "Multilingual evaluation methods and the development of benchmarks not only facilitate the assess- ment of diverse language representations in LLMs but also help in monitoring cross-lingual gen- eralization, to assess the effect of quantization across multiple languages (Marchisio et al., 2024), the development of language-specific models (Tejaswi et al., 2024), and the optimization of safety preferences (Li et al., 2024e), among others. In Table 4, we list benchmarks with the largest lan- guage coverage. This list includes benchmarks referenced by MEGA (Ahuja et al., 2023), MEGA- VERSE (Ahuja et al., 2024), xP3 (Muennighoff et al., 2023), the Aya collection (Singh et al., 2024), the lm-evaluation-harness framework (Gao et al., 2023; Biderman et al., 2024), and inter alia. These datasets comprise a mix of translated datasets, some human-translated or verified by native speakers such as AfriXNLI (Adelani et al., 2024) and some relying only on machine translation Lai et al. (2023b). Additionally, there are datasets created independently for each language, such as XLSum (Hasan et al., 2021), where the data is not parallel and the size of the data varies between languages. Despite the efforts reflected in Table 4, the community is still lacking highly multilingual bench- marks for tasks such as natural language understanding or text generation."}, {"title": "A.3 SEMANTIC SIMILARITY IN MULTILINGUAL EMBEDDINGS", "content": "There are other ways to compute similarity between languages, such as Representational Similarity Analysis (RSA) (Chrupa\u0142a & Alishahi, 2019) and Central Kernel Alignment (CKA) (Kornblith et al., 2019). RSA involves first computing the cosine similarity for sentence embeddings within each lan- guage, then correlating these in-language similarities with those in other languages. CKA, another metric, is adopted by Conneau et al. (2020b) and Muller et al. (2021). Conneau et al. (2020b) show that the CKA similarity is highly correlated with sentence retrieval scores for four languages. In this paper, our focus is not on finding different ways to calculate similarity between languages, but on how helpful a properly defined alignment score can be in estimating the multilingual capabilities of LLMs across multiple languages."}, {"title": "A.4 ROBUSTNESS OF MEXA", "content": "We show that the MEXA alignment score (\u03bc(.)) is very robust, and the odds of this score randomly achieving a high value are very slim. Recall that \u03bc(C(L1, L2, m, l)) measures the fraction of diag- onal elements in matrix C(L1, L2, m, l) that have the maximum value in their respective rows and columns. If this condition is met k times out of n diagonal elements, then \u03bc(C(L"}]}