{"title": "STRATEGIST: LEARNING STRATEGIC SKILLS BY LLMS VIA BI-LEVEL TREE SEARCH", "authors": ["Jonathan Light", "Min Cai", "Weiqin Chen", "Guanzhi Wang", "Xiusi Chen", "Wei Cheng", "Yisong Yue", "Ziniu Hu"], "abstract": "In this paper, we propose a new method STRATEGIST that utilizes LLMs to acquire new skills for playing multi-agent games through a self-improvement process. Our method gathers quality feedback through self-play simulations with Monte Carlo tree search and LLM-based reflection, which can then be used to learn high-level strategic skills such as how to evaluate states that guide the low-level execution. We showcase how our method can be used in both action planning and dialogue generation in the context of games, achieving good performance on both tasks. Specifically, we demonstrate that our method can help train agents with better performance than both traditional reinforcement learning-based approaches and other LLM-based skill learning approaches in games including the Game of Pure Strategy (GOPS) and The Resistance: Avalon.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent studies have demonstrated how Large Language Models (LLMs) can be utilized to learn skills for improved decision-making in interactive environments (Wang et al., 2023; 2022). However, learning skills in adversarial environments with multiple agents presents a significant challenge for LLMs, as it requires accounting for the responses of other players or environment to their actions. In these environments, it is hard to determine the optimal policy, since strategies that are effective against one opponent's policy may not work for another, and the opponents can adapt to our current strategy. Moreover, the complexity of these environments means that we need to smartly search across a large policy space to find the optimal policy. Hence, existing LLM self-improvement methods that rely only on feedback from the LLM often have trouble distinguishing what improvements help and end up in local optima strategies that can be exploited.\nIn this paper, we study how to (1) acquire and analyze feedback in these noisy environments and (2) make sure that the improvements actually help against different opponents. We propose a method, referred to as STRATEGIST, that learns an effective policy, evaluates the policy in the absence of the real environment at a low level, and finds such a policy efficiently. In other words, we are also learning how to learn!"}, {"title": "2 METHODOLOGY", "content": null}, {"title": "2.1 STRATEGY LEARNING IN DECISION MAKING SETTING", "content": "The general framework for STRATEGIST is shown in Figure 2 with pseudo-code 1. Our improvement process contains two improvement steps in each improvement cycle \u2013 the (1) reflection and idea generation step and (2) the strategy improvement step. During the idea generation step we prompt the LLM to reflect on simulated self-play feedback from previously evaluated strategies and generate possible improvement ideas to the strategies and add them to the idea queue. During the strategy improvement step, we select a strategy from the strategy tree and an improvement idea from the idea queue and prompt the LLM to improve the strategy using the improvement idea. The improved strategy is then evaluated via self-play simulations, and we use the feedback and reward signals from the simulation to help guide future improvements.\nThe general goal in our decision-making setting is to learn a good policy function in a sequential decision-making setting (generally formulated as a partially observable Markov decision game (POMDG)), which can be done by improving strategies associated with the policy function. We describe in more detail what a strategy looks like, how we derive a policy function from a strategy, and how to acquire feedback for the strategy for both dialogue generation and action generation here.\nProblem definition. Given state space $S$ and action space $A$, a policy function $\\phi$ in policy space $\\Pi$ is a mapping $\\phi : S \\rightarrow \\Delta A$ where we allow $\\phi$ to output a probability distribution over the actions $(\\Delta A)$. An environment $E = (S, A,N,T, R, A, \\Phi_e)$ defines the state space $S$, the action space $A$, a set of actors $N$, a transition function $T : S \\times A \\rightarrow S$, a reward function $R : S \\times A \\rightarrow R^{|N|}$ that specifies intermediate rewards for each actor, an action function $A : S \\rightarrow N, P(A)$ that specifies which actor may take what legal actions at some state where $P$ is power set, and $\\phi_e$, the policy function for the"}, {"title": "2.2 STRATEGIST FOR HIGH-LEVEL STRATEGY LEARNING", "content": "Our main methodological contribution lies in the development of a self-improvement method STRATEGIST that utilizes LLMs to learn new skills without direct supervision, where feedback comes from simulated self-play. We further use modular search to improve the sample efficiency. The method generalizes to different settings, and we show different ways to use our method in the next sections.\nThe framework behind the STRATEGIST is shown in Figure 2, with pseudo-code 1. The skill coach maintains a strategy library of the strategies it has generated so far, starting from some seed strategies, along with the performance score of the strategy and raw feedback on how the strategy performed in practice. It also maintains a queue of improvement ideas, ways that we can improve strategies, along with how much the idea improves strategies on average (score). We keep track of which strategies evolved from which and what improvement idea it used, which forms a tree structure. There are two alternating improvement loops that we run the reflection and idea generation step, and the strategy improvement step. We run these two loops for a fixed number of improvement cycles. When improving, we use an adaptive selection policy such as upper confidence bound (UCB) or best first search (BFS) to select strategies and ideas to improve upon."}, {"title": "2.3 SELF-IMPROVEMENT FEEDBACK FOR NON-DIALOGUE ACTIONS", "content": "For non-dialogue actions, while the action spaces $A$ and state spaces $S$ themselves are usually discrete and finite, the number of possible functions $\\Phi$ from state space to action space is very large. Most LLM-agents query the LLM directly with the state information for next actions in decision-making environments (Yao et al., 2023; Shinn et al., 2024; Zhao et al., 2024). However, we found this method to be costly since the LLM needs be queried for every move, and a game of Avalon usually has at least 20 moves per player. This becomes even costlier when we use look-ahead search to improve the planning capabilities of the agent and does also needs to query the LLM for every look-ahead future actions and states. Traditionally reinforcement learning tackles the problem of having a large policy space by parameterizing the policy and optimizing the parameters instead, starting from a random initialization of the parameters, thus reducing the search space. Building on this approach, instead of querying the LLM every time for an action, we instead parameterize the policy as a python function, and we have the LLM search over the space of such functions instead.\nWith its inherent world model, the LLM can help us search and optimize over the policy space more effectively. Given the rules of the game in natural language form that a human can understand, the LLM can quickly generate policies that seem reasonable. In natural language form, it is often easier to describe the value of a state versus describing the optimal action to take in a given state. Humans often mentally go through the process of comparing different states (outcomes) when making decisions. Thus, we opt to learn a value function, i.e. $\\sigma := v : S \\rightarrow R^{|N|}$, an approach that is often used in reinforcement learning to help stabilize the training process. It is easy to convert from a value function to a policy function since we can simple take the action that leads to the best state, i.e. $\\Phi_i(s) = argmax_{a \\in A} Q(s, a) = argmax_{a \\in A} R_i (s, a) + v_i(s')|s' = T(s,a)$.\nWe call it a value heuristics function because, given the little data that we improve on, it is an inaccurate estimate of the true value function. Hence, to resolve these inaccuracies, we additionally enhance the policy function with MCTS, used commonly in other AI agents such as Alpha-go and"}, {"title": "2.4 SELF-IMPROVEMENT FEEDBACK FOR DIALOGUE GENERATION", "content": "Dialogue generation presents another distinct challenge for self-improvement. In dialogue generation, both the action spac A, i.e. the number of possible sequences of words one could generate for any discussion round, the state space S, i.e. the number of possible input dialogue from previous rounds, are huge. This means that the number of possible dialogue generation policies $\\Phi$ is massive, and traditional parameter training approaches have great difficulty optimizing across this space. We solve this problem by learning a high-level strategy guide that the agent uses when speaking. Specifically, the strategy guide formalizes a process to think about how to generate dialogue, given the situation. This is implemented as a question and answer worksheet. The agent is first instructed to answer all the questions in the strategy guide before using it as a prompt to generate dialogue."}, {"title": "3 EXPERIMENTS", "content": "We demonstrate the effectiveness of our self-improvement process through experiments against different improvement benchmarks. We tested our method on (1) GOPS, a two-player zero-sum card game (see B for rules) and (2) Avalon, a five or more player team-based discussion game (see A for rules). For Avalon dialogue generation, we specifically benchmark on the Merlin role and Assassin roles, representing both Good and Evil sides. Experiments were run with gpt-3.5-turbo-0125, with more ablation studies in Appendix J. Details on how we implemented policies from learned strategies and acquired feedback are described in Section 2.1, 2.3, and 2.4, and Appendix D, E, and F."}, {"title": "3.1 DIFFERENT LLM IMPROVEMENT METHODS", "content": "We demonstrate the effectiveness of our strategy improvement method by benchmarking it against four other skill-improvement methods. Line search (Madaan et al., 2024) always reflects and improves upon the latest improved strategy. Greedy search (Ma et al., 2023) selects the best strategy from the last generation of improved strategies to improve upon each improvement cycle. Best first search (Yao et al., 2024) improves upon the k best strategies generated in any iteration of each improvement cycle. Best first search with thought asks the LLM to improve upon the thoughts used to generate the k best strategies before improving the strategy itself. STRATEGIST is our method that uses an additional idea queue Q and an idea generation step to guide the improvement process. More details can be found in Appendix M.\nOur results are shown in Table 1, where the method of collecting feedback (simulational self-play) is the same but we vary the improvement method. The number of new strategies generated by each method is also held constant. The gameplay scores of playing the strategies generated by each method against each other on GOPS is shown in Figure 3 right. Even when we control for the number of output tokens generated by the LLM, we see that our method still achieves higher performance as shown in Figure 4. We believe that the reason why we see higher performance in our method is because (1) the idea queue helps test which incremental improvements are helpful and guide the search process and (2) our strategy and idea selection policy help us explore the strategy space more efficiently and escape local maxima as shown in Figure 3 left."}, {"title": "3.2 LLM-IMPROVEMENT VS. REINFORCEMENT LEARNING (RL) TRAINING", "content": "We demonstrate the effectiveness of our method against traditional RL-based approaches to learning a good policy. Specifically, we show that our method is able to learn a value heuristic function more efficiently than deep RL, the approach taken by AlphaGo and MuZero (Silver et al., 2017; Schrittwieser et al., 2020). We know that given enough training data, training time, and a large"}, {"title": "3.3 FEEDBACK QUALITY AND REWARD SIGNAL", "content": "Recent works tend to focus on either using another LLM to critique the generations and thus provide feedback (Madaan et al., 2024; Shinn et al., 2024), observations from real environment interactions (Nottingham et al., 2024), or some combination of the two (Wang et al., 2023). Since our method learns how to internally simulate the opponents by learning a strategy and thus gain insights into the opponent's policy, we are able to acquire better quality feedback than either approach. We benchmark our feedback acquisition me- thod against (1) using a LLM-critic and (2) trajectory feedback from interactions against a fixed opponent policy. Our results are shown in table 3, where our method achieves better performance on both action planning in GOPS and dialogue generation."}, {"title": "3.4 STRATEGIST VS. RULE-BASED BASELINES", "content": "Previous results from AVALONBENCH (Light et al., 2023) show that the rule-based bots with deliberate human guided design serves as a strong baseline, and pure ReAct-like decision making is far from competitive. We follow AVALONBENCH (Light et al., 2023) to play games under two settings with and without discussion. Each model will play 10 games against the rule-based baselines. As is shown in Table 4, STRATEGIST has great advantages over ReAct-like LLM in winrate and assassination accuracy. However, it does not outperform the rule based methods at deduction, which use Bayesian methods to deduce player identities."}, {"title": "4 RELATED WORK", "content": "LLMs for text agents. Large language models (LLMs) have demonstrated significant emergent capabilities, such as zero-shot prompting and complex reasoning (Bommasani et al., 2021; Brown et al., 2020; Raffel et al., 2020; Wei et al., 2022a; Chowdhery et al., 2022; Chung et al., 2022). They also possess extensive world knowledge (Yu et al., 2023a), which has spurred increasing efforts to use LLMs for decision-making in text agents (Wang et al., 2024). One notable paradigm is ReAct (Yao et al., 2023), which employs an observation-reasoning-acting loop for agent planning with LLMs. Building on ReAct, Reflexion (Shinn et al., 2024) incorporates self-reflection to enhance reasoning capabilities. Other works in this domain have utilized feedback (Wang et al., 2023; Huang et al., 2022), memory (Park et al., 2023), and tool use (Schick et al., 2024; Cai et al., 2023) to further enhance agent performance. Our proposed method, STRATEGIST, integrates these components to design an agent capable of systematic analysis and strategic decision-making. Typical prompting techniques for text agents include Chain-of-Thought (Wei et al., 2022b), Tree-of-Thought (Yao et al., 2024), and Graph-of-Thought (Besta et al., 2024). While these techniques are effective for high-level reasoning, they are insufficient for complex games that require feedback signals for self-improvement. STRATEGIST adopts a bi-level tree search approach, enabling both high-level planning and low-level agent self-play for providing feedback.\nSkill learning with LLMs. Recent works have explored the possibly of LLMs learning skills through learning a textual short and long term memory (Shinn et al., 2024; Majumder et al., 2023), or textual insights extracted from the memories (Zhao et al., 2024). Due to the length of trajectories in our game setting and the numerical nature of the data, it is difficult to learn textual memories, so we learn high level strategies instead. We also explore how to acquire simulational self-play feedback in multiagent settings. Using LLMs to learn a functional reward model has also been applied to great success on single-agent robotic tasks (Ma et al., 2023; Yu et al., 2023b). We build upon their work by introducing a new improvement method that can help learn a better reward model, and exploring how function learing can be applied to multiagent settings with simulated feedback.\nAI in strategy games. Al has been applied to great success in board games. AlphaGo and MuZero demonstrated the power of combining MCTS, deep learning, and feedback generation using self-play in games such as Go, Chess, and Shogi Silver et al. (2017); Schrittwieser et al. (2020). Language models can also be trained on human in-game discussion data and integrated with another separately trained action planner to play board games with dialogue (FAIR). We build upon the AI for games"}, {"title": "5 LIMITATIONS AND CONCLUSION", "content": "While our method performs better on average, individual runs can have high variance. Since the performance of an agent in multi-agent adversarial game settings is highly dependent on opponents' policies, feedback from these environments tend to be highly noisy, with noise increasing with the number of players. This is especially true when learning Avalon heuristics, where the performance depends on the policies of 5 other players, teammates and opponents. We believe that running more game simulations with different opponent policies can help reduce this feedback noise. We also acknowledge the inherent noisiness in LLM generations and how that can impact our results. We tried to reduce this noise by (1) using the same seed functions when benchmarking the different LLM improvement methods and (2) collecting generated strategies from multiple runs. We also did not test our method on other non-adversarial environments such as question answering and text-based worlds. However, given the strong performance of our method in adversarial multi-agent settings, we believe that similar performance will be observed in single agent, non-adversarial settings.\nIn conclusion, we have presented STRATEGIST, a generalizable non-parametric self-improvement framework that learns and improves skills. Given the rules of the game, our method is able to learn good strategies to play the game through self-play without task-specific prompting or human generated policy data. The performance of STRATEGIST suggests that incorporating better guidance, whether this be through modular high-level search or low-level simulated self-play feedback, into LLM-improvement processes can greatly enhance the improvement process."}, {"title": "A RESISTANCE: AVALON GAME DESCRIPTION", "content": "We describe the game in more detail here. There are four phases in the game where players need to make decisions: (1) team selection phase, (2) voting phase, (3) quest phase, and (4) assassination phase. The game alternates between the first three phases until the end condition is reached, at which point we move on to the assassination phase. Each phase also contains discussion where players can challenge others, defend themselves, and negotiate. A flowchart of the game is presented in Figure 8, and an Avalon Rule Prompt is included in Section A.4."}, {"title": "A.1 ROLES", "content": "There are four basic roles in Resistance Avalon: Servant of Arthur, Minion of Mordred, Merlin, and Assassin. The Servant is a basic good character who does not know the identity of any of the other players. The Minion is a base evil character who knows who is good and evil but does not know the specific roles of each player. Merlin is a unique good character who knows who is good and evil. The Assassin is a unique evil character who knows who is good and evil, and in addition, has the ability to assassinate a character at the end of the game. If that character is Merlin, the evil team wins.\nGood players will always outnumber evil players. Hence, evil players must pretend to be good in order to be voted in on teams (and thus sabotage missions). SERVANTS will thus need to sniff out the evil players through their actions and dialogue. MERLIN is usually the only good player with additional information, so they will need to discreetly guide the SERVANTS in the right direction. Servants also need to protect MERLIN, so a common strategy is for SERVANTS to pretend to have hidden information so that evil players will think that they are MERLIN. Evil players will be trying to sniff out MERLIN at the same time, so deduction skills are required for all roles."}, {"title": "A.2 ACTIONS FOR EACH PHASE", "content": "Depending on the phase team selection, voting, quest, and assassination, players may conduct different actions. We detail the specific actions that players can take in each of these phases below.\nDuring the team selection phase, only the current leader has to make a choice. Leadership passes around the players sequentially in a loop. The action space of team selection for the leader consists of all subsets of the players with size equal to the mission team size. The mission team size is different"}, {"title": "A.3 DISCUSSION", "content": "Group discussion occurs between the quest and selection phases, as well as right before the as- sassination phase. Players may not communicate during any other time. All conversations are public, and there is no private communication. Typically players may discuss in any format of their choosing as long as only one person is speaking at a time. Some examples of formats include a"}, {"title": "A.4 GAME ENDING AND ASSASSINATION", "content": "In classic RESISTANCE, a good team wins immediately if three missions are successful. In RESIS- TANCE AVALON, there is an additional assassination phase if three missions are successful. During the assassination phase, the ASSASSIN player chooses one player to assassinate. If that player is MERLIN, then evil wins. Otherwise good wins.\nBefore they assassinate a player, the ASSASSIN player can and is encouraged to discuss with the other players (mostly their teammates). good players are also welcome to join in on this discussion to mislead the evil players, though it rarely helps. Players can discuss in a format of their choosing, though there is usually a time limit on how long players can discuss before reaching a decision."}, {"title": "B GAME OF PURE STRATEGY (GOPS) GAME DESCRIPTION", "content": "Game of Pure Strategy (GOPS) is a card game for two or more players with a standard deck of card, which is commonly used as an example of multi-stage move game in artificial intelligence (Wikipedia contributors (2023)). In our experiments we play 5 or 6 card GOPS. Specifically, the score cards are {1, 2, ....n} and each player starts with a hand of cards {1, 2, ....n} where n is the number of cards and rounds. The GOPS rules prompt is included in this section below."}, {"title": "C IMPROVEMENT PROCESS IMPLEMENTATION DETAILS", "content": null}, {"title": "D AVALON AGENT IMPLEMENTATION DETAILS", "content": "We describe in detail how we implement our model below and as shown in figure 9. Unless otherwise specified, the word 'action' will refer to non-dialogue actions. Note that we do not conduct search over raw dialogue space since that is not very computationally feasible. Instead, we search over intended actions and condition our dialogue on that.\nSpecifically, the language component consists of a dialogue analyzer and a dialogue generator, while the moves component consist of the action planner. Whenever the agent needs to speak, they first analyze what was said so far in the current discussion round using the dialogue analyzer. The dialogue analyzer, with the help of an LLM, updates the internal beliefs of the agent. For example, in Avalon, internal beliefs might include the probability that the agent assigns to each other player of being Evil and of being Merlin. These beliefs are then passed to the action planner, which uses them to figure out the best next move, i.e. the action intent. The action intent is then passed to the dialogue generator, which generates dialogue with the help of an LLM. When the agent needs to take a move, we run through the same process except that the agent takes the action intent as the move and no dialogue is generated."}, {"title": "D.1 DIALOGUE ANALYZER (DISCRIMINATOR)", "content": "The dialogue analyzer $f_{ana}$ takes as input $I$ information set (partial information) of the current state for the player, $d_t$ the discussion so far this round, and $b$ some prior beliefs about the hidden state of the game, and returns $\\hat{b}$, the updated beliefs, and $\\hat{\\Pi}_t$, the predicted joint action policy of the all the players (i.e. the action intent) for the next action step $t$. Recall that simultaneous games can be expanded as partial information games, where the simultaneous moves are treated as hidden information. Hence, we are essentially predicting a distribution over the hidden states $s$ given the information set $I$ using the dialogue analyzer.\n$\\hat{b}, \\hat{\\Pi}_t = f_{ana}(I, d_t, b)$"}, {"title": "D.2 ACTION PLANNER", "content": "Given $\\hat{b}$ the belief prior, $\\hat{\\Pi}_t$ the predicted joint action policy for all players, and $s$ the representation of the current state, the action generation model $f_{act}$ generates a probability distribution over possible actions $\\pi^i_t$ for the main player $i$ that is the best response to $\\hat{\\Pi}_t$. We do so by using search techniques to look ahead and find the best response.\n$\\pi^i_t = f_{act}(\\hat{b}, \\hat{\\Pi}_t, I)$"}, {"title": "D.3 DIALOGUE GENERATION", "content": "The dialogue generator $f_{gen}$ takes as input $I$ some representation of the current information set and $\\hat{a}^i_t$, the intended best response action, and outputs dialogue $d$.\n$d = f_{gen}(I, \\hat{a}^i_t)$"}, {"title": "E VALUE HEURISTIC IMPLEMENTATION DETAILS", "content": "The MCTS search process is depicted in Figure 10, where we simulate a trajectory from the hidden state we are at until we reach some unexpanded state s. The probability of transitioning to a state during simulations is computed assuming that each player samples from their optimal actions according to their PUCT (polynomial upper confidence trees) values (and $\\phi_e$ be for the environment actor) (Schrittwieser et al., 2020). Since in some environments players may only be able to observe information sets, when computing the PUCT values we average over all expanded states in that information set. Moreover, the initial hidden state can be sampled according to a prior (or empirical prior) over the states in the information set that the player observed. Then, using our value heuristic, we compute the values of each of the next hidden states. We then backpropogate our new values back up the simulated trajectory, updating the intermediate states. After running a few MCTS simulations (roll-outs) like the one we described, the planner then outputs the action which leads to the highest value next state. We show our information set PUCT formula below, where N(s, a) is the number of times we took action a at state s during MCTS rollouts, P(s, a) is the prior prior probability of selecting action a from state s, C is the exploration constant, $Q_{emp}$ is the empirical average of MCTS roll-out outcomes, Q(s, a) is the prior computed by our value heuristic, $\\alpha$ controls how much weight be put on the prior (often $\\alpha=1$), and $\\pi_B$ is the distribution across hidden states in the information set given our beliefs B, some parametrization of B. Since $\\pi_B$ is often hard to compute, we can simply set $\\pi_B(S|I)=\\frac{\\sum_{i}N(s,b)}{\\sum_{t}N(s',b)}$ to be the empirical roll-out distribution, given that we sample initial states $s_0 \\sim \\pi_B(s_0|I)$ according to our beliefs. For example, in Avalon, we can sample the hidden roles according to our beliefs B using Metropolis-Hastings for the initial state $s_0$.\n$Q(s, a) = \\frac{N(s, a) \\cdot Q_{emp}(s, a) + \\alpha Q(s, a)}{N(s, a) + \\alpha}  (1)$\n$PUCT(I, a) = \\sum_{s \\in I} \\pi_B(S|I) [Q(s,a) + C \\cdot P(s, a) \\cdot  \\sqrt{\\\\frac{\\sum_{i}N(s,b)}{1+N(s, a) }}] (2)$"}, {"title": "F DIALOGUE GUIDE IMPROVEMENT EVALUATION IMPLEMENTATION DETAILS", "content": "We provide more details on our dialogue improvement evaluation process here and as shown in figure 11. The improvement method (skill coach) remains the same as we described before.\nWe first generate a synthetic dataset by simulating a game of Avalon with initial dialogue and move policies $\\pi_0$. Given the dialogue guide $\\sigma$ we want to evaluate, we then sample 'scenarios' from the dataset. A scenario consists of a game state, intended action, and private information in the simulated trajectory. We create an Avalon agent like the one we described in D for each player in the game, initialized with their corresponding private information. The Avalon agent is then asked to generate dialogue using the dialogue guide $\\sigma$.\nUsing this new generated dialogue, we then simulate the next round of dialogue analysis for each Avalon agent. This produces analysis scores based on how likely they think the player is to be Merlin $Z_{merlin}$, and how likely they think the player is to be Evil $z_{evil}$, where $Z_{merlin}, Z_{evil} \\in [-2, 2]$. For evaluating Merlin, we get the average $Z_{merlin}$ scores from the Evil players, $\\overline{Z}_{merlin}$, along with the average $Z_{evil}$ scores from the Good players $\\overline{Z}_{evil}$. We then take the minimum of these two as the feedback score $z = min{\\overline{Z}_{evil}, \\overline{Z}_{merlin}}$. This is because Merlin wants to both minimize the probability of being detected by the Evil players, and also minimize the probability of being identified as Evil by the Good players."}, {"title": "G VALUE HEURISTIC LLM PROMPT AND OUTPUT EXAMPLES", "content": null}, {"title": "G.1 SYSTEM PROMPTS", "content": "System prompt are guidelines for LLM to generate outputs align with the intended goals. In our case, the goal is to generate a function that evaluates the value of a state in a game under low cost."}, {"title": "G.2 IDEA GENERATION EXAMPLES", "content": "The idea generation prompt included system prompt, game rules, previous guide and feedback reflections. Following those four components, we construct the format and an example of ideas to guide the generation of LLM."}, {"title": "G.3 STRATEGY IMPLEMENTATION EXAMPLES", "content": "During the strategy implementation step, we first select a strategy and an idea from the libraries using the adaptive selection policy. Then, we prompt the LLM to implement the idea on the strategy, generating a new improved strategy.\nIn this section, we showcase two examples. Each example illustrates strategies before and after enhancements made by the LLM. This comparison highlights the effectiveness of our strategy implementation. The improved parts by LLM are highlighted in yellow."}, {"title": "H DIALOGUE GUIDE LLM PROMPT AND OUTPUT EXAMPLES", "content": "This sections shows the system prompts of dialogue guidance on LLM and several examples, including system prompts, idea generation prompts, and strategy implementation examples."}, {"title": "H.1 SYSTEM PROMPTS", "content": "Below is the Dialogue guide system prompt."}, {"title": "H.2 IDEA GENERATION EXAMPLES", "content": "The following is the selected dialouge guide speaking as the Merlin role.\nA feedback example is provided for better understanding of dialogue guidance to teach LLM how to role play Merlin."}, {"title": "H.3 STRATEGY IMPLEMENTATION EXAMPLES", "content": "Recall the strategy implementation mentioned in the paper before. Here, we implement strategy improvements on dialogue guide. The improved dialogue guide is highlighted."}, {"title": "J OTHER ABLATION STUDIES", "content": null}, {"title": "J.0.1 BASE LLM MODEL", "content": "We show the performance of different base models in Figure 12."}, {"title": "J.0.2 SEARCH BUDGET", "content": "How does the effectiveness of the search+LLM agent scale with regards to the search budget? Does having a larger search process help achieve better performance?"}, {"title": "K DETAILS ON LEARNING THE VALUE HEURISTIC VIA REINFORCEMENT LEARNING", "content": "We employ Monte-Carlo based RL approach (Sutton & Barto (2018)) to train a value heuristic for both five-player Avalon and five-card GOPS games. To do so, we construct a MSE loss in each episode for training the value function, i.e.,\n$\\underset{\\theta}{argmin} \\sum_i \\sum_{t=0}^{NT} (V_i(s_t) \u2013 Score^i(s_t))^2$\nwhere N represents the number of actors, $V_i(s_t), i = 1,2,\u2026,N$ denotes the value function for each actor, and T is the time horizon. Notice that $s_t$ and $Score^i(s_t)$ denote the state at time step t and the corresponding cumulative reward for each actor, i.e., $\\sum_t R_i(s_t, a_t)$. It is worth pointing that $Score^i(s_t)$ (the cumulative reward starting from $s_t$) is the unbiased estimate of the value function $V_i(s_t)$.\nFor both Avalon and GOPS games, the value function $V_i(s_t)$ is predicted by a neural network. We then train the value function network by minimizing the aforementioned loss function over episodes. In Avalon, we consider 20 evolutions (epochs) for the training process. At the end of each evolution, 30 batch runs (episodes) are generated and used to train the value function network, i.e., a total of 600 episodes for training. In GOPS, we train by 20 evolutions as well while considering 60 batch runs each (1200 episodes in total). We evaluate the final performance over 10 episodes in both games. The neural network is constructed by a multilayer perceptron (MLP) with 2 hidden layers. We select a hidden layer size of 128 * 128 for Avalon and that of 64 * 64 for GOPS. Likewise, the chosen learning rates are 5e - 4 and 8e \u2013 4, respectively. The value function is expected to predict the score for each player in the game, e.g., two for GOPS and number of players for Avalon. All experimental hyper-parameters are summarized in Table 6."}, {"title": "L EXPERIMENTAL COMP"}]}