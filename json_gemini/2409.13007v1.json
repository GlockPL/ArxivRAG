{"title": "iCost: A Novel Instance Complexity Based Cost-Sensitive Learning Framework for Imbalanced Classification", "authors": ["Asif Newaz", "Asif Ur Rahman Adib", "Taskeed Jabid"], "abstract": "Class imbalance in data presents significant chal- lenges for classification tasks. It is fairly common and requires careful handling to obtain desirable performance. Traditional classification algorithms become biased toward the majority class. One way to alleviate the scenario is to make the classifiers cost- sensitive. This is achieved by assigning a higher misclassification cost to minority-class instances. One issue with this implemen- tation is that all the minority-class instances are treated equally, and assigned with the same penalty value. However, the learning difficulties of all the instances are not the same. Instances that are located near the decision boundary are harder to classify, whereas those further away are easier. Without taking into consideration the instance complexity and naively weighting all the minority-class samples uniformly, results in an unwarranted bias and consequently, a higher number of misclassifications of the majority-class instances. This is undesirable and to overcome the situation, we propose a novel instance complexity-based cost- sensitive approach in this study. We first categorize all the minority-class instances based on their difficulty level and then the instances are penalized accordingly. This ensures a more equitable instance weighting and prevents excessive penalization. The performance of the proposed approach is tested on 66 imbalanced datasets against the traditional cost-sensitive learning frameworks and a significant improvement in performance is noticeable, demonstrating the effectiveness of our method.", "sections": [{"title": "I. INTRODUCTION", "content": "When the class distribution in the dataset is uneven, with one class (the majority class) significantly outnumbering the other (the minority class), it is referred to as imbalanced data [1]. This type of data is frequently encountered in different applications such as medical diagnosis, fraud detection, spam detection, etc. [2]. The class imbalance can pose significant challenges for standard machine learning (ML) algorithms, which typically assume that the classes are balanced. Con- sequently, ML classifiers produce biased performance towards the majority class. Imbalanced learning is a critical area in ML that requires specialized techniques to ensure that models are effective and fair, especially in applications where the cost of misclassifying minority instances can be dire. As such, the imbalanced domain has caught a lot of attention from researchers and different approaches have been proposed to address the issue [3]. The techniques can be broadly classified into two categories: data-level approach and algorithmic-level approach.\nIn the data-level approach, the original class distribution in the data is modified by adding new synthetic minority- class instances or eliminating samples from the majority class. The goal is to balance the class distribution in the data. Recent research suggests it is even more important to reduce the class overlapping in the process to obtain better performance [4]. On the other hand, in the algorithmic-level approach, the original classification algorithm is modified to adapt to the imbalanced domain scenario. This is achieved by changing the cost function to handle the class imbalance directly [5]. Higher misclassification costs are assigned to the minority class instances to make the algorithm more sensitive to those errors. During training, the model learns by trying to reduce the overall misclassification cost. Assigning higher weight to the minority-class misclassifications shifts the bias from the majority class. This way, the algorithm is made cost-sensitive (CS). This approach is classifier-dependent as different algorithms use different learning procedures. Both of these approaches perform almost equally well and are extensively used in different real-world applications [6]\u2013[8].\nThis study is focused on cost-sensitive learning. Here, a spe- cific penalty is added to the misclassifications of the minority-"}, {"title": "II. RELATED WORKS", "content": "A lot of different techniques have been proposed over the years to deal with imbalanced data [2]. However, very few of them address data-intrinsic characteristics [11], [12]. Several data difficulty factors have been identified which include class overlapping, small disjunct, noisy samples, etc [13]. It has been suggested that the overlapping between the classes, not the imbalance, is primarily responsible for complicating the learning task [14]. Recent literature emphasizes the impor- tance of considering data difficulty factors to develop more robust methods for learning from imbalanced datasets [13]. Stefanowski proposed several approaches to tackle these data difficulty factors [15]. However, the study only considers data resampling techniques. In this study, we are primarily focused on CS approaches.\nDifferent classifiers such as SVM, ANN, and DTs are adapted to the CS framework [16], [17]. Their implementation is available in the popular sklearn library. In different medical datasets, class imbalance is prevalent and CS approaches are found to be quite successful in handling such scenarios [18]. Mohosheu et al. performed a detailed efficacy analysis of the performance of CS approaches as compared to data resampling techniques in their study [19]. The authors reported that the traditional CS approaches outperform undersampling and ensemble methods but cannot surpass popular oversam- pling techniques such as SMOTE or ADASYN. Hybridization between sampling and CS approaches is also possible and has been proposed by Newaz et al. [20]. The authors first resampled the data by generating synthetic samples using SMOTE and then used a weighted XGBoost classifier for training. The authors suggested that combining the techniques together allows for lower misclassification costs to be applied while also reducing the number of synthetic samples to be generated. Consequently, overfitting is reduced and better prediction performance can be obtained.\nGan et al. proposed a sample distribution probability-based CS framework in their article [21]. Roychoudhuri et al. adapted the CS algorithm for time-series classification [22]. Zhou et al. extended the CS framework for multiclass imbalanced scenarios [23]. Other variations of CS approaches include MetaCost [24], a meta-learning algorithm that converts any given classifier into a CS classifier. The idea of example- dependent cost has also been proposed in previous literature [25], [26]. For instance, in credit scoring, a borrower's credit risk is determined based on different factors which include their credit history and financial behaviors. These factors"}, {"title": "III. METHODOLOGY", "content": "In this section, we discuss our proposed algorithm in detail. At first, we identify the K-nearest neighbors of each minority class sample. A k-value of 5 was utilized in this study. The nearest neighbors are computed using the Euclidean distance. Next, each minority class instance is categorized as follows:\n\u2022 Pure: Number of neighboring samples belonging to the majority class = 0\n\u2022 Safe: Number of neighboring samples belonging to the majority class = 1 or 2\n\u2022 Border: Number of neighboring samples belonging to the majority class > 2\nThis has been illustrated in Fig. 2. Samples that are cat- egorized as 'pure' are completely surrounded by instances of the same class. These samples are easy to classify and are usually located far from the decision boundary. Hence, a comparatively much smaller misclassification cost should be enough to correctly identify these samples. Assigning a higher weight might worsen the scenario, resulting in a higher number of misclassifications of the majority class instances. 'Safe' samples have 1 or 2 neighboring opposite- class instances. These samples should be handled carefully due to the risk of misclassification. Assigning too small a weight may be insufficient, while too large a weight could reverse the situation. On the other hand, border samples are surrounded by majority-class instances. These samples would be misclassified by the K-nearest neighbor classification rule. Therefore, a higher weight is necessary to prioritize these samples over the neighboring samples from the opposite class.\nIn our implementation, different weights are assigned to different categories of samples based on their difficulty level. We employed a grid-search technique to determine the ap- propriate cost for different categories. The values varied from one dataset to another. Based on the experiments conducted"}, {"title": "Algorithm: Instance complexity-based Cost-sensitive learning (iCost)", "content": "Inputs:\n\u2022 data: Input dataset (Pandas DataFrame)\n\u2022 classifier: 'SVM', 'LR', or 'DT' (default = 'SVM'). The algorithm inherits from Sklearn's SVC, LogisticRe- gression, and DecisionTreeClassifier implementations, re- spectively.\n\u2022 type: 'org', 'ins', 'nap', or 'gen' (default = 'ins'). 'org' refers to the original implementation of the CS classifier. 'ins' refers to the instance categorization criteria we pro- posed. 'nap' refers to the instance categorization criteria proposed by Napierala et al. [29]. 'gen' provides the general categorization mentioned earlier.\n\u2022 k: The number of neighbors to be considered for catego- rization of the minority-class instances (default = 5).\n\u2022 cost-factor: The misclassification cost to be assigned. It can be an integer or a list/dictionary. This input parameter is related to the 'type' parameter. For type = 'org', the cost-factor value must be an integer. For other types, the cost-factor value can be both an integer and a list/dictionary. For all cases, the default value is set as the IR of the dataset.\nOutput: Instance-level weighted classifier fitted on the given input training data.\nProcedure:\n\u2022 If type = 'org', the algorithm assigns a weight equal to the cost factor to all the minority-class instances without any other consideration. This is the original CS implementation of the algorithms. If the cost factor value is 1, the algorithm will work as a standard ED classifier.\n\u2022 If type = 'ins' or 'nap', the algorithm categorizes the minority-class instances into three or four categories, respectively. For 'gen', minority-class instances are cate- gorized into k+1 categories.\n\u2022 In the case of 'ins', the user can provide an integer or an array/dictionary with three elements as the input values for the cost factor. If it is an integer, then a penalty equal to the integer is assigned to the 'border' samples. For the 'safe' samples, half of the integer value is assigned as the penalty factor. For the 'pure' samples, misclassification cost = 1.2 is fixed. If the input is an array, values are directly assigned to border, safe, and pure samples, in that order. In the case of a dictionary, key-value pairs can be used to directly state the cost values for each pair.\n\u2022 Same thing goes for type = 'nap'. If the input value for the cost factor is an integer, then the weights are assigned to the minority class samples in the following way: outlier = cost factor, rare = 0.75 * cost factor, border = 0.5 * cost factor, and safe = 0.25 * cost factor. The user can also directly assign weights using an array or dictionary with four elements.\n\u2022 For 'gen', the user can assign weights using an array of k+1 elements. In the case of integer input or default sce- nario (weight=IR), the weight is equally divided between the samples from 1 to IR proportionally based on their grade.\n\u2022 The appropriate values for the misclassification cost are dependent on the dataset. A value equal to the IR of the dataset is set as default, similar to the implementation of the sklearn library. The above-mentioned approach of assigning costs to different categories of samples provides a decent improvement in performance. However, it can be further optimized using different search algorithms [30].\n\u2022 Any misclassifications of the majority class samples are assigned a weight of 1. Therefore, assigning a weight lower than 1 to any minority class instance can result in poor sensitivity in imbalanced classification tasks. Since minority-class samples are usually more important to classify correctly, a conditional statement is kept to ensure that the minimum weight assigned to any minority-class instance should not be lower than 1.\nExample:\n\u2022 iCost(data, classifier = 'LR', type = 'gen', cost-factor = [2, 2, 2, 10, 10, 10])\nThis will apply an instance complexity-based cost- sensitive Logistic Regression (LR) classifier on the given data. Here, g0, g1, and g2 graded minority-class samples"}, {"title": "IV. EXPERIMENT", "content": "A. Datasets\nThe performance of the proposed algorithm has been evaluated on 66 imbalanced datasets with varying degrees of imbalance to ensure the generalizability of the proposed approach. The datasets are collected from KEEL and the UCI data repository [31]. All the datasets are publicly available and with no missing entries. A summary of the datasets is provided in Table II.\nB. Experimental Setup\nTo ensure proper validation and avoid data leakage, the data was first split into training and testing folds. The algorithms are applied only to the training set and the performance is evaluated on the testing set. A repeated stratified K-fold cross-validation strategy (5 folds, 10 repeats) was adopted to ensure better generalization. The average of the results from all different testing folds is considered as the performance measure.\nWe experimented with 3 different classification algorithms in this study: LR, SVM, and Decision Tree (DT). The default parameters of the Sklearn library were utilized to implement these classifiers. In the case of the proposed iCost algorithm, only the 'cost-factor' parameter was tuned using the grid- search technique. We used type='org' and 'ins' to obtain results for the traditional CS classifiers and our proposed modified CS approach, respectively. The parameter setting for the grid-search implementation is provided in Table III. MCC score was utilized as the scoring criteria.\nC. Performance Metrics\nAssessing the performance of different techniques on skewed data can be challenging [1]. Eight different perfor- mance metrics were computed for evaluation in this study: MCC, ROC-AUC, G-mean, F1-score, sensitivity, specificity, precision, and accuracy. ML algorithms often excel at pre- dicting instances from the majority class but tend to perform poorly on the minority class. Consequently, traditional perfor- mance metrics like accuracy can be misleading because they do not account for the distribution of classes. For instance, if 95% of the data belongs to one class and 5% to another, a model that always predicts the majority class will have 95% accuracy but will be useless for predicting the minority class. Sensitivity and specificity are two class-specific metrics that manifest the performance accuracy of the minority and majority classes, respectively. They only show the performance of a particular class. As a result, it is difficult to apprehend the entire performance spectrum from these measures.\nComposite metrics are more suitable for performance mea- surements on imbalanced data. The g-mean score exhibits a broader picture by providing the geometric mean of sensitivity and specificity. F1-score provides the harmonic mean between"}, {"title": "V. RESULTS AND DISCUSSION", "content": "In this section, we present and discuss the results we have obtained during the experiment. We have measured the performance of 3 different classifiers on 66 imbalanced datasets using 8 different metrics. All these measures cannot be included here due to space constraints. They are provided in separate supplementary files. We mainly considered the MCC, ROC-AUC, G-mean, and F1-score for comparison.\nA. Performance from traditional approaches\nStandard classification algorithms do not perform well on imbalanced datasets. The performance gets even worse with higher imbalances and class overlapping [34]. In many cases, the g-mean score was found to be 0 (sensitivity=0, speci- ficity=100), indicating a clear bias towards the majority class. Cost-sensitive learning (CSL) provides a significant improve- ment in performance as can be observed from Fig 3. Fig. 3 shows the average G-mean scores obtained on 66 datasets. Among the classifiers, CS-SVM provided the highest G-mean score. DT is found to be less sensitive to CS approaches. Improvement in performance was the maximum in the LR classifier. The average results from other metrics are provided in Table IV, Table V, and Table VI.\nB. Performance comparison of the proposed iCost algorithm with the standard CS approach\nIn this study, we propose a modification to the original im- plementation of CSL by introducing an instance complexity- based CS framework. In our approach, different misclassifica- tion costs are assigned to minority-class instances depending on their difficulty level. The samples located near the border are highly penalized compared to the samples that are away from the decision boundary or surrounded by instances of the same class. This ensures that the safe samples do not over- shadow other majority-class instances and create unwarranted bias. Thus, our proposed approach provides a more plausible CSL framework where instances are weighted according to their complexity, rather than indiscriminately. This modifica- tion provides quite an improvement in performance."}, {"title": "VI. CONCLUSION", "content": "In this study, a modified Cost-sensitive learning framework is proposed where data difficulty factors are taken into con- sideration while penalizing the instances. In the traditional framework, all the instances are weighted equally. This is impractical and can bias the prediction toward the minority- class, leading to an increased amount of false positives. Uniform instance weighting overfits the data by unusually deforming the decision boundary. This does not fare well dur-"}]}