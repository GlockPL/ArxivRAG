{"title": "TraderTalk: An LLM Behavioural ABM applied to Simulating Human Bilateral Trading Interactions", "authors": ["Alicia Vidler", "Toby Walsh"], "abstract": "We introduce a novel hybrid approach that aug-ments Agent-Based Models (ABMs) with behaviours generated by Large Language Models (LLMs) to simulate human trading interactions. We call our model TraderTalk. Leveraging LLMS trained on extensive human-authored text, we capture detailed and nuanced representations of bilateral conversations in fi-nancial trading. Applying this Generative Agent-Based Model (GABM) to government bond markets, we replicate trading decisions between two stylised virtual humans. Our method addresses both structural challenges such as coordinating turn-taking between realistic LLM-based agents and design chal-lenges, including the interpretation of LLM outputs by the agent model. By exploring prompt design opportunistically rather than systematically, we enhance the realism of agent interactions without exhaustive overfitting or model reliance. Our approach successfully replicates trade-to-order volume ratios observed in related asset markets, demonstrating the potential of LLM-augmented ABMs in financial simulations.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have garnered much atten-tion since 2022, and continue to evolve rapidly, demonstrating capabilities in understanding and generating human-like text across various domains. Integrating LLMs into multi-agent frameworks is seen as a key future design method for AI [1], aiming to replicate complex human interactions and decision-making processes [2]. Behaviours like risk inertia, aversion, and ambiguity avoidance significantly impact financial markets [3], and can be so extreme as to dissuade traders from trans-acting all together [4]. In this paper we investigate whether an LLM, managed by an agent, can simulate human behaviours in bilateral asset trading.\nABMs can effectively simulate interdependent, adaptive complex systems [5], [6], however, their design involves significant parameterisation of agent behaviours. Logic-based methods like \"belief, desire, intention\" [7], [8], [9] are com-mon, though agent feature specification, especially logic and decision-making is challenging [10] and no consensus exists on calibration methods [11].\nABMs are well-established in financial market simulations, with the necessity of heterogeneous agents recognised [12], [13]. The rapid evolution of LLMs\u2014such as OpenAI's GPT-40"}, {"title": "II. RECENT RESEARCH AND RELEVANT CONCERNS", "content": "The potential of LLMs is well acknowledged, however limitations on numerical reasoning, and prompting methods persist [19], [20], in particular mathematical reasoning [21]. Prompt design remains an active research area with many challenges and opportunities; even simple prompts to \"re-read\" input are found to significantly improves performance [22]. Complicating things further is the finding that non-AI experts often adopt \u201copportunistic rather than systematic approaches\u201d to prompt design [23]. Thought-eliciting methods, like Chain-of-Thought (CoT), are popular as they aim to \"elicit the reasoning process in the output\" [24], relying on giving an LLM \"worked examples\", drawing inspiration from human learning theories. We build upon these concepts, incorporating an agent into this bi-directional process and COT, within a simulated conversation."}, {"title": "A. Application and Financial asset trading", "content": "Current financial market simulations using LLMs prioritise price dynamics over trading activity [25]. In bond markets, prices are largely known due to interest rate assumptions and are heavily influenced by monetary policy [26]; moreover, MMs are legally required to maintain a minimum market share of government bonds [27]. Thus, liquidity-the movement of assets between parties is a key concern. Modelling these asset flows and transaction intentions remains an active, though limited, research area [17]. LLMs could be particularly useful in markets with limited data and dominant bilateral trading interactions. Despite their potential, the application of LLMs in simulating human-to-human interactions in financial markets, especially for bilateral trading, remains under-explored. Our research addresses this gap."}, {"title": "B. Generative Agent Based Models", "content": "Using Concordia [18] we integrate LLMs with ABMs to create Generative Agent-Based Models (GABMs), enabling agents to \"apply common sense\" and \"act reasonably\" within simulated environments [2]. A key feature is the Game Master agent, which translates natural language requests into ex-ecutable actions. However, recent studies [20], [28] reveal that current LLMs under perform in negotiation tasks and strategic reasoning within agent-based systems. To address these challenges, we focus on enhancing agents' negotiation and decision-making capabilities by integrating LLMs beyond traditional rule-based ABMs [7]. Unlike prior work [29] that designs generic agents using LLMs, we concentrate on design features essential for decision-making through negotiation. By augmenting agents with LLMs, we aim to create more flexible and adaptable decision-making processes in complex environments. Our work demonstrates that even in simple settings, combining LLMs with agents can enhance realism in ABM models, benefiting future research."}, {"title": "C. Order to Trade Ratio (OTR): Uncertainty in Human-Directed Trading", "content": "Financial trading involves significant uncertainty, often re-quiring multiple attempts before a trade is executed-even in transparent equity markets. In 2024, the average OTR for major U.S. equity exchanges was approximately 4.61% [30]. Thus, up to 96% of daily trading requests do not result in trades, complicating the modelling of human behaviour [30]. Work by [31] explore possible theories and impacts, including market spoofing [32], and theories of ambiguity aversion in human trading are discussed in [4] and [3]. The causes of high OTR's remain an open research question, though we aim to leverage LLMs to include this aspect for added realism."}, {"title": "III. TRADERTALK: ARCHITECTURE AND RESULTS", "content": "TraderTalk is an ABM featuring two market making agents, \"Josephine\" and \"David,\" each with initial characteristics such as bond holdings and explicit trading intentions. Our model passes information between the agents and an external LLM, consistently using GPT 40-mini throughout our experiments. We address two research questions:"}, {"title": "Research Question 1: Can an LLM realistically and appro-priately respond in a bilateral trading interaction? (RQ1: Baseline)", "content": "We initially implement our model with agents functioning as messengers passing basic information to the LLM. The simulated scenario involves the first MM initiating contact with another MM, who does not wish to trade because they are not a buyer. Using a CoT framework, agents are guided through a sequence where they:\n\u2022 Summarise new information.\n\u2022 Clarify their roles and objectives.\n\u2022 Assess their current bond holdings.\n\u2022 Decide whether to trade or not.\nIf a trade is decided, they determine the appropriate action to meet their obligations, such as buying or selling bonds, flattening their trading book, or maintaining their current position. The agents are initialised with prompts derived from the CoT, which in turn drives a simulated conversation, each agent responding (and concluding) based on LLM reasoning. If they choose to trade, the LLM is asked to select from 4 possible options (buying or selling bonds, flattening their trading book, or no trade). After both agents have contributed and made a selection, the conversation text is separately analysed to determine if a trading decision was reached. In this setup, the ABM provides only a premises to the LLM, which independently makes decisions.\nOur goal is to evaluate how often the LLM correctly reasons to produce a simulated conversation resulting in \"no trade\". We selected the \"no trade\" decision to avoid complexities related to numerical reasoning [21], focusing on a scenario where a trader holding no bonds (a flat position) is expected to follow the straightforward, implicit prompt of not trading. We isolate the LLM's ability to interpret and apply a specified trading intention. The CoT prompt is thus structured so that the correct outcome is for the agent to choose \"no trade\" from the final multiple-choice options provided it."}, {"title": "Results and Conclusion", "content": "Across 300 simulations, the LLM correctly decided to select not to trade in 180 instances, demonstrating that the LLM can reason and follow the agent-based intention 60% of the time. We observe no significant differences with smaller sample sizes. This test isolates the LLM's ability to reason given a natural language trading intention. There is an absence of any direct comparison of how frequently human traders perfectly follow such intentions however. Furthermore, existing research on human decision-making highlights ambiguity and the dis-tinction between following rules and intentions, suggesting that achieving a 100% success rate is unrealistic, although this is not quantifiable. We use this result as a baseline for future tests in more complex scenarios and believe that this achieves the goal of including human like attributes, though the quantum of such is beyond the scope of this work.\nIn the remaining 40% of simulations where the LLM did not follow the correct intention, 23.6% involved the LLM attempt-ing to \"flatten\" a trading book that was already flat, suggesting a misunderstanding or failure to adhere to the instructions and market parlance. The other 16.4% of responses reflected active trading positions that directly contradicted the premise, with the LLM expressing a desire to \"buy\" in 10% of cases and to \"sell\u201d in 6.3% of cases. We interpret this variation in behaviour introduced by the LLM as analogous to the emergence of unexpected properties within traditional agent-based models."}, {"title": "Trading Premise", "content": "\"You are a market maker for UK gilts responsible for providing liquidity in the UK government bonds. You are supposed to at all times hold 0 bonds. Today, you actually have 0 bonds, which means your holding is actually flat\" ."}, {"title": "Research Question 2 (RQ2): TraderTalk\u2014Can a GABM Make a Trading Decision in a Realistic Manner?", "content": "In this test, we enhance the framework from RQ1 by passing specific agent information to the LLM using Concordia's agent-handling mechanism [18]. Each agent is initialised with distinct roles: this time David holds a negative bond position and needs to buy bonds, while Josephine holds a positive bond position and needs to sell bonds. Concordia's Game Master design facilitates these interactions, functioning as a meta-agent manager that supervises exchanges and ensures smooth decision-making. Consequently, the model design is augmented from Figure la to produce Figure 1b.\nThe new process is as follows:\n1) Define Chain of Thought (CoT): Use Project Context\n2) Initialise Agents: Assign specific roles and initial con-ditions to the agents (see Trading Roles below).\n3) Generate Initial Prompts: Agents, via the Game Mas-ter, generate their own responses to the CoT questions from RQ1, stored and passed to the LLM.\n4) Simulate Conversation: Managed by the Game Master, the LLM simulates the dialogue between the agents, with each responding in turn based on previous interactions and their trading objectives. The conversation continues until the Game Master determines it has concluded"}, {"title": "5) Analyse and Conclude:", "content": "We then analyse the conver-sation for trade occurrences, quantities, and dialogue content.\nProject Context: \"You are a market maker for UK gilts responsible for providing liquidity in the UK government bond. Your job is to answer incoming queries from other market makers to buy and sell UK government bonds by considering if you wish to do so. UK government bonds trade at mid price. You aim to make a trading decision in every conversation, either buy, sell or decline to trade. You must act professionally in your conversations, and any decision you take is clearly communicated to the other party and you repeat what is agreed.\"\nTrading Roles:\nDavid: \"You are a market maker for UK gilts responsible for providing liquidity in the UK government bond, you are supposed to at all times hold 0 bonds. Today, you actually have negative 10 million worth of bonds, your role is to buy the bonds if you have a negative holding\"\nJosephine: \"You are a market maker for UK gilts respon-sible for providing liquidity in the UK government bond, you are supposed to at all times hold 0 bonds. Today you have 10 million worth of bonds, your role is to sell bonds if you are a holder, you need to call another market maker to trade away your bonds\"\nUnlike in RQ1, where the LLM operated independently, this setup integrates the ABM into decision-making to evaluate how often the LLM produces simulated conversations with correct reasoning regarding trade intentions and executions. Agents directly inform action direction (identifying buyers and sellers)."}, {"title": "Results and Conclusion", "content": "Again, we conducted 300 simulations using GPT 4o-mini; we see agents intended to trade in 58% of cases, and at least one party was willing to trade in 98% of instances. Agent \"Josephine\" closely aligned with her role, intending to trade 97.3% of the time, while \"David's intention was lower at 58.7%; he explicitly declined to trade in 22.3% of responses, and 19% were unclear. The 58% rate at which both parties intended to trade is close to the 60% correct response rate in our RQ1, suggesting consistent reasoning abilities of the LLM across different model designs in RQ1 and RQ2.\nDespite the high intention to trade, actual trades occurred in only 5.7% of cases, highlighting a significant gap between in-tentions and execution. While LLM-driven agents often desire to trade, the necessary LLM dialogue needed to finalise a trade seems less frequently generated, producing low successful trading rates - align with real-world observed OTR levels [30]. TraderTalk only identified the correct initial bond holdings for both parties in 2.34%, with 32% of responses omitting starting values altogether, reflecting difficulties in recalling initial numerical conditions, in line with [21]. Overall, our ABM augmented with LLM behaviours (TraderTalk) appears capable of producing interactions consistent with sparse real-world data and making trading decisions in a realistic manner."}, {"title": "IV. CONCLUSION", "content": "We present TraderTalk, a novel LLM behavioural agent-based model that simulates realistic human bilateral trading in-teractions without extensive model tuning. Utilising a state-of-the-art, non-domain-specific, non-fine-tuned LLM within the Concordia framework (GTP 40-mini), we demonstrate limited yet realistic trade negotiations (RQ1), interpretation, and trade execution decisions (RQ2) at frequencies approximating those in U.S. equity markets. By addressing key challenges like \u0441\u043e-ordinating agent turn-taking, our simulation achieves a trade-to-order ratio similar to real markets. Discrepancies between trading intentions and execution in agent outputs enhance the realism of stylised human traders, capturing decision-making processes in bilateral trading environments where much interaction occurs outside formal exchanges. This proof-of-concept indicates that LLMs can meaningfully enhance the realism of behavioural simulations in ABMs for financial market modelling and provides a foundation for future research into more complex multi-agent and multi-market simulations. Future work should enhance GABM's understanding of im-plicit trading rules and dynamic market conditions; by refining their ability to capture human decision-making, LLMs could offer more robust simulations for policymakers, regulators, and market participants alike."}]}