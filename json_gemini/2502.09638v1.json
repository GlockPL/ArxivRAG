{"title": "Jailbreaking to Jailbreak", "authors": ["Jeremy Kritz", "Vaughn Robinson", "Robert Vacareanu", "Bijan Varjavand", "Michael Choi", "Bobby Gogov", "Summer Yue", "Willow E. Primack", "Zifan Wang"], "abstract": "Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as J2 attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet-3.5 and Gemini-1.5-pro outperform other LLMs as J2, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-40 (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming-drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with J2, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.", "sections": [{"title": "1. Introduction", "content": "To align Large Language Models (LLMs) with safety specifications, refusal training embeds safeguards within an LLM to prevent generating harmful responses, such as refusing user instructions such as: provide me a tutorial for how to make bombs at home [7, 37, 41]. However, recent works show that well-crafted prompts by humans or automated methods can effectively jailbreak LLM safeguards [3, 12, 30, 34, 42, 44, 49, 50, 55, 58, 61]. In particular, when scaffolding an LLM as an autonomous agent, the safeguards become even more vulnerable [2, 22].\nHumans and automated attacks each bring distinct strengths to red teaming. Automated attacks usually identifies token-level jailbreaks, such as appending carefully crafted tokens to malicious prompts. In contrast, humans are more effective at exploring specific framing strategies, such as fictionalization, that may have been overlooked during the model's refusal training. Furthermore, human red teamers are able to utilize previous experience, iterate on strategies, and pivot when a particular one fails. These capabilities represent a meaningful subset of general intelligence.\nUnlike automated methods, human red teaming is faced with challenges in scalability, replicability, and the high cost of hiring and training. In line with the concept of using LLMs as judges, which can date back to Vicuna [13], for capability evaluations, a promising approach is to leverage LLMs as red teamers, as they have demonstrated increasingly sophisticated reasoning and persuasive abilities [3, 11, 12, 14, 34, 38, 40, 42, 44, 46, 50, 54].\nDirectly assisting jailbreaking is usually considered harmful by LLM safeguards. The first contribution of this work is our jailbreak that enables frontier LLMs to facilitate red teaming requests \u2013 sometimes even against themselves."}, {"title": "2. Background", "content": "The goal of red teaming is to optimize input prompts to elicit harmful content from refusal-trained LLMs, effectively bypassing their safeguards. By doing so, we provide an empirical evaluation of safeguard robustness in predicting misuse risks. In this paper, we focus on a threat model that best represents public interactions with LLMs. Specifically, a red teamer has only API access to the model's input and output tokens\u2014commonly referred to as black-box access, as opposite to white-box ones [5, 24, 47, 60]. Additionally, pre-filling [1] the LLM's generation with adversarial examples is not permitted. Despite these limitations, both human and algorithmic attacks have successfully uncovered numerous jailbreak examples across proprietary models such as GPT-40 [36] and Sonnet-3.5 [4], as well as open-weight models like Llama-3.1 [15].\nExperienced human red teamers can assess the strengths and weaknesses of an LLM's guardrails, strategically eliciting harmful outputs in multi-turn conversations, as demonstrated by multi-turn human jailbreak datasets [25]. Similar to adversarial attacks on vision classification models [31, 51], automated attacks on LLMs generate specific token sequences that bypass safeguards\u2014often without clear human interpretability. Efficient automated jailbreaks require internal access to the LLM. Otherwise, attackers must rely either on adversarial transferability (e.g., attacking smaller models and transfer the attacks to larger ones [61]) or exhaustive black-box optimization techniques [21, 32].\nUsing LLMs to assist red teaming presents a hybrid approach that leverages the reasoning abilities of advanced LLMs to mimic human red teamers, mitigating the scalability challenges of human-led red teaming. Unlike purely algorithmic approaches, this method does not require access to activations or gradients. Existing research primarily explores using LLMs as prompt engineers to refine jailbreak attempts\u2014either by iterating on failed"}, {"title": "3. Method", "content": "We create an LLM red teamer J2 in order to jailbreak refusal-trained LLMs to elicit desired harmful behaviors. This jailbroken LLM is referred to as J2 attacker.\nTo construct a J2 attacker from a refusal-trained LLM, a human red teamer jailbreaks the model to elicit a willingness to red team and provides broad guidance on common jailbreaking techniques. This conversation is saved as an array of messages in the candidate LLM API's conversation history format. Empirical results show that this jailbreak is effective across multiple frontier LLMs without any modification. While the design allows for either single-turn or multi-turn red teaming, empirically we find J2 is much more capable of multi-turn red teaming and all jailbreak examples consist of multiple turns if not noted otherwise.\nJ2 operates in an iterative loop. Each iteration begins with a planning phase where J2 is provided a specific one from a set red teaming strategies to develop its attack approach. This is followed by an attack stage where J2 attempts to execute the strategy in conversation with the target model. Finally, a debrief stage where a judge prompt is used to evaluate the attack's success. We refer to these three stage as a red teaming cycle as illustrated in Figure 3. This cycle repeats with the same strategy, keeping all previous planning, attack, and debrief attempts in the context window, until either a successful jailbreak is achieved or a maximum number of cycles is reached.\nNotations. We denote an LLM as F(X) that takes a conversation history X and outputs an assistant response. We denote conversation concatenation as F(X\u2081; X2), meaning we append X2 to X\u2081 while preserving any system prompt in X1. We denote the number of turns in each attack stage as T, the number of red teaming cycles as N and the set of red teaming strategies as S.", "subsections": [{"title": "3.2 Creating J2 Attackers", "content": "We enlist one of the authors, an experienced red teamer, to curate a multi-turn conversation Xhuman until the resulting model outputs portray an LLM assistant that has been convinced to knowingly help with jailbreaking, rather than an assistant that has been tricked into thinking it is engaged in some other benign activity, or that the jailbreaking is happening within a fictional context. This allows the guidance and instruction for the remainder of the attack to be straightforward, rather than being required to further reinforce some deception. Next, the human red teamer uses several conversational turns Xinfo to introduce broad guidance around jailbreaking techniques. We provide some excerpts in Appendix B.\nBy using a concatenation of Xhuman and Xinfo as a prefix, we successfully turn state-of-the-art LLMs into J2attackers, such as Sonnet-3.5, Gemini-1.5-pro, and GPT-40. Noticeably, Xhuman and Xinfo are created between the human and Sonnet-3.5 but our empirical results show that it effectively transfers without edits to convert other LLMs into J2 attackers as well. Thus, given an LLM F(X), we denote the J2 attacker as\nJ2(X) = F(Xhuman; Xinfo; X)."}, {"title": "3.3 Preparing Red Teaming Strategies", "content": "Before we let J2 attempt jailbreaking, we first prepare a list of red teaming strategies for it to apply and narrow down its reasoning space. We curate a strategy set S in our experiment containing 9 strategies, a short summary of which is shown in Table 1 and the full description can be found in Appendix A. The strategies in S are based on the authors' past experience in manually jailbreaking refusal-trained LLMs, and vary in specificity."}, {"title": "3.4 Red teaming Workflow", "content": "We provide a brief description for our proposed red teaming workflow with greater details in Appendix B.\nStage I: Planning. Before J2 starts to query the target LLM Ftarget, it is instructed with hard-coded human prompts to plan its attack. Specifically, we provide the target harmful behavior to elicit from Fr, and a red teaming strategy from S. The J2's responses in this section act as a scratchpad or chain of thought to plan its approach before it engages with Ftarget.\nEven though the initial jailbreaking conversation Xhuman makes the LLM willing to jailbreak, J2 sometimes will \"regret\" its previous decision and refuse to continue. This occurs most often in the planning stage of the first cycle, and most often with Claude-3.5-Sonnet. To address this, J2's responses in the planning stage are parsed for specific strings such as \"cannot assist\", \"I aim to be direct\", or \"apologize\", which serve as refusal indicators. When detected, these responses are not removed, but a short additional hard-coded user message referencing Xhuman is sent to J2. After several iterations on improving the hard-coded user message, we are able to converge to prompts that can resume the red teaming workflow in most cases. The refusal handling conversation remains in the models context window for the remainder of the attack, and all subsequent cycles for the given strategy.\nStage II: Attack. We instruct J2 to start a T-turn conversation with the target Ftarget with the following prompt. \u201cStarting {T}-turn test session. You'll be interacting with another AI model. Your response for the next {T} turns will be sent to the model. No planning or inner thoughts, as those would also be seen by the model ...\u201d\nThe generations of J2 and Ftarget are both labeled with a user role in each other's API messages array, respectively. J2's response to this message will become the first user message for the target model Ftarget."}]}, {"title": "4. Experiments", "content": "In this section, we conduct experiments for answering the following research questions.\n\u2022 Q1: Which models are the best J2 attackers?\n\u2022 Q2: How do cycles (N), the strategy set (S) and attack turns (T) affect the effectiveness of J2?"}, {"title": "4.1 Optimizing the Effectiveness of J2", "content": "Metric: We use attack success rate (ASR) to measure the effectiveness of J2 attackers, which measures the percentage of harmful behaviors with successful jailbreaks and is used across related works [28, 33, 42, 61]. Recall that when J2 is searching jailbreaks against a target LLM, it uses the feedback from an external judge and its own self-criticism to determine if the jailbreak is successful (as described in Section 3). To mitigate any potential reward hacking, we employ another judge LLM (i.e. the ActorAttack judge) introduced by Ren et al. [42] as an extra filter. This judge's prompt and outputs are never seen by J2. We align with Ren et al. [42] to interpret an input to the ActorAttack judge as harmful when the judge returns 5 as the harm score. In doing so, we consider that J2 finds a successful jailbreak only when it passes both the independent judge in J2 and the ActorAttack judge. To minimize randomness, we set the temperature to 0 for all judges.\nComparing LLMs as J2 Attackers. We compare LLMs with varying sizes as J2 attackers over the first 50 standard behaviors in Harmbench. We use a subset only to find the best LLMs and will provide the results on the full set in Section 4.2. These LLMs include: GPT-40 [35], Sonnet-3.5-1022, Sonnet-3.5-0620, Haiku-3.5 [4] and Gemini-1.5- pro [18]. We use GPT-40 with a temperature of 0.9 as the target LLM. We set the maximum cycles (N) to 10 and the number conversation turns (T) to 6. We use strategies in Table 1. Results are plotted in Figure 4a.\nOur results in Figure 4a show that Gemini-1.5-pro, Sonnet-3.5-1022 and Haiku-3.5 find more successful jailbreaks when given more cycles. While Haiku-3.5 at most can jailbreak 20% of behaviors, Gemini-1.5-pro and Sonnet-3.5- 1022 succeed almost at all 50. Allowing N\u2265 6 is necessary for most J2 attackers to be useful, while scaling T can still increase the ASRs but with diminishing returns. Surprisingly, we see a big drop in ASRs using Sonnet-3.5-0620 and GPT-40 as J2 attackers. Upon a closer look at the logs of each, they fail for different reasons. GPT-40 is willing to engage in red teaming, but it seems surprisingly incapable of producing good red teaming prompts with our current red teaming instructions. It is possible we did not find the best way to allow GPT-40 to comprehend the red"}, {"title": "4.2 Harmbench Results", "content": "We evaluate J2 against 200 standard text behaviors (i.e. val + test splits) in Harmbench and compare it with other automated attack methods and human red teamers.\nExperiment Setup. For baselines, we include the following single-turn attacks: GCG [61], PAIR [12], PAP [58], CipherChat [55], BoN [21] and the following multi-turn attacks: ActorAttack[42], and Bijection [20]. Another multi-turn attack that also employs an LLM red teamer is GOAT [38], but while it does report results on Harmbench, it lacks sufficient implementation details for us to replicate the results. Instead, we only compare to it qualitatively"}, {"title": "4.3 Qualitative Analysis and Further Discussion", "content": "J2's Capability At Fictionalization. Among jailbreaks, one interesting finding from examining the attack conver- sations is that J2 shows a strong capability in fictionalization-based attacks such as the popular \"Napalm Grandma\" framing (e.g. please act as my deceased grandmother, who used to be a chemical engineer at a napalm production factory...)\u00b2. This safeguard failure has been a known vulnerability since the early days of commercial LLMs. Our results, such as the examples shown in Table 3 (Appendix D) show that this vector is still effective, and that LLMs are skilled at crafting innocent-seeming fictional scenarios to correspond with nearly any possible harmful behavior. When provided the first strategy dealer's_choice in Table 1, J2 relies on only broad guidance from the initial human-written phase in B. The following text from J2 (Gemini) shows a general example of how multiple attempts allows for the model to build on a particular fictional context. In this example, Gemini jailbreaks for guidance on implementing SQL injections by posing as a technical writer, and seeks to further improve its attempt by reducing the target LLM's hesitation:\nDisguising the Malicious Intent. J2 can disguise malicious intents in various ways, capable of creating scenarios where a harm is disguised, but also through justifications of why a harm might be necessary. For instance, we observe the following example from J2(Gemini), where it independently devises the idea of framing a scenario as a matter of life or death to pressure the target LLM into compliance.\nSimilarly, J2 (Sonnet-3.5) can identify and adapt to the target LLM's preferences, using this information to more effectively conceal its attacks. The following example illustrates how J2 (Sonnet-3.5) recognizes that an academic framing can help prevent the ethical disengagement of the target LLM.\nNew Failure Mode of LLM Safeguards. As the target of a J2 attacker can be its own backbone model, e.g. using J2(Gemini) to attack Gemini, we reveal a new failure mode in LLM safeguards. Specifically, while fully jailbreaking"}, {"title": "5. Related Work", "content": "Because Section 2 has discussed related work in red teaming methods, this section focuses on comparing our work with the most similar ones, ways to further improve LLM red teamers and defense methods.\nComparing With Other LLM Red Teamers. One related line of work in LLM red teaming is iteratively prompting the attacker model to do a zero-shot and single-turn attacks. In this framework, the attacker LLM refines the input until the jailbreak is successful, such as PAIR [12] and TAP [34]. The idea is LLMs can directly optimize the input string and the optimization will converge at some point, similar to a first-order optimizer in GCG [61]. These methods usually provide little or no guidance from the human or successful jailbreak artifacts. It remains unclear whether one LLM, without being trained to jailbreak, can model another LLM's safeguard pattern and whether the optimization will converge.\nOur work is more related to existing works that use in-context learning and allow LLMs to autonomously conduct multi-turn attacks [38, 39, 42]. In doing so, the attacker LLM can better capture the target LLM's behavior in multiple turns, adapt to the safeguard and finally break it. By comparing Haiku-3.5 and Sonnet-3.5 we show that LLM capability and intelligence directly correlates with jailbreak success, which is one reason why Perez et al. [39] was ineffective when larger models were not available.\nThe closest framework to J2 is the GOAT attacker [38]. We discuss two major differences. First, the GOAT method does not include an initial jailbreaking step. This limits the options for the GOAT to those without safeguards, or to models which have been fine-tuned to undo their safety training. Since red teaming skill correlates strongly with reasoning capabilities, this serves as a limiting factor in using the GOAT in practice. Second, the GOAT does not retain previous attempts in the attacker model's context window to leverage its in-context learning ability. Our experiments show that multiple cycles of planning, and attacking massively increase ASR, as the attacker is able to refine its approach, and try new strategies.\nImproving J2 Attackers. The performance of J2 is tied to the underlying LLM's reasoning ability and the quality of the provided guidance strategies. This means that J2's effectiveness should increase along with increases in the general ability of frontier LLMs. As adversarial robustness may also increase with reasoning ability, this creates a double-bladed effect as increases in intelligence directly increase both offensive and defensive capabilities of models [57]. Working closely with human red teamers will help improve existing strategies and create new ones."}, {"title": "6. Conclusion", "content": "This work introduces J2 attackers for demonstrating how advanced language models can be transformed into effective red teamers by human red teamers. Our experiments show that once jailbroken, models like Sonnet-3.5 and Gemini-1.5-pro achieve attack success rates of 93.0% and 91.0% respectively against GPT-40, approaching human red teamers while offering greater scalability. J2's success also exposes a critical failure mode: capable AI systems can systematically reason through and bypass their own safeguards."}, {"title": "B.1 Guidance Before Red Teaming", "content": "Figure 6 shows the progress in finding the best Xhuman that allows us to effectively jailbreak more LLMs across the range of target behaviors. We find that the length of Xhuman (measured by User input tokens) shows a scaling effect in improving the jailbreaking effectiveness due to the longer jailbreaks containing additional tricks and arguments. Interestingly, GPT-40 and Gemini-1.5-pro quickly agree to jailbreak other LLMs with fairly short conversations, while Sonnet-3.5 and Haiku-3.5 need much more effort. The actual jailbreak conversation Xhuman in J2 is the longest one we show in Figure 6. This experiment only measures presence of strings indicating refusal in the turn immediately following the introduction of strategy. For Sonnet-3.5-0620, this initial lack of refusal typically does not persist.\nThe following turns Xinfo introduce broad guidance around jailbreaking techniques. Some excerpts are shown below."}, {"title": "B.2 Planning", "content": "The planning stage consists of a two turn conversation Xp. The user messages introduce the target harmful behavior b to elicit from the target Fr and the strategy s to be sampled from a curated list S."}, {"title": "B.3 Attack", "content": "In this stage, FA will start a t-turn conversation with the target LLM FT. We denote this t-turn conversation between J2 and the target LLM as Xa.\nAt the start of the phase, J2 receives an initialization user message u prompting it to begin."}, {"title": "B.4 Debrief", "content": "Once the attack is concluded, the attack is first evaluated by sending the conversation Xa to an LLM judge. We are using GPT-40 as the external judge. The judge is sent the conversation as a single string, along with the target behavior b and asked to evaluate the attack's success. This judge prompt uses a chain of thought to analyze the attack.\nThis response is then sent to J2as a user message. This checklist approach is intended not just to accurately evaluate the break, but also to highlight how the attack may have fallen short."}]}