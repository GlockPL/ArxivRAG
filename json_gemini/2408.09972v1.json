{"title": "Edge-Cloud Collaborative Motion Planning for Autonomous Driving with Large Language Models", "authors": ["Jiao Chen", "Suyan Dai", "Fangfang Chen", "Zuohong Lv", "Jianhua Tang"], "abstract": "Integrating large language models (LLMs) into autonomous driving enhances personalization and adaptability in open-world scenarios. However, traditional edge computing models still face significant challenges in processing complex driving data, particularly regarding real-time performance and system efficiency. To address these challenges, this study introduces EC-Drive, a novel edge-cloud collaborative autonomous driving system with data drift detection capabilities. EC-Drive utilizes drift detection algorithms to selectively upload critical data, including new obstacles and traffic pattern changes, to the cloud for processing by GPT-4, while routine data is efficiently managed by smaller LLMs on edge devices. This approach not only reduces inference latency but also improves system efficiency by optimizing communication resource use. Experimental validation confirms the system's robust processing capabilities and practical applicability in real-world driving conditions, demonstrating the effectiveness of this edge-cloud collaboration framework. Our data and system demonstration will be released at https://sites.google.com/view/ec-drive.", "sections": [{"title": "I. INTRODUCTION", "content": "As intelligent transportation and autonomous driving technologies rapidly advance, the motion planning system, as a critical component, faces increasingly complex environments and diverse challenges. Traditional motion planning methods often rely on fixed algorithms and models, making it difficult to fully address the dynamic changes in traffic conditions and the personalized needs of drivers [1].\nIntegrating large language models (LLMs) into autonomous vehicles not only enables artificial intelligence systems to control the driving process but also significantly enhances the system's personalization and adaptability. By understanding natural language commands, LLMs can dynamically adjust driving strategies to meet the personalized preferences of drivers or passengers, thereby improving the overall driving experience. Moreover, the integration of LLMs allows autonomous systems to better handle complex and dynamic open-world scenarios, making them more flexible in addressing diverse driving tasks.\nThe Transformer, originally designed for sequential data, has achieved state-of-the-art performance in natural language processing, driving the development of LLMs [2], [3]. These models pretrain Transformer architectures (encoder, encoder-decoder, and decoder) on vast corpora to capture extensive language statistics. Pretrained LLMs can be fine-tuned for specialized downstream tasks. The Vision Transformer (ViT) [4] applies the Transformer to image tasks, converting images into sequences of patches that the Transformer can process. CLIP [5], a multimodal model that matches textual descriptions with images, demonstrates strong transfer capabilities in many image classification tasks. Utilizing pretrained LLMs as a framework for multimodal tasks leverages their text generation capabilities, which is crucial for the question-answering tasks in our research. However, despite their impressive performance in many tasks, deploying these large models with typically over a billion parameters for real-time applications remains challenging.\nAlthough autonomous driving systems primarily rely on visual features, incorporating linguistic features can enhance system interpretability and aid in identifying new traffic situations. This advantage has sparked interest in integrating multimodal data to train language models as autonomous driving agents. DriveGPT4 [6] employs LLaMA as the backbone LLM, with CLIP as the visual encoder, using traffic scene videos and prompt texts as inputs to generate responses and low-level vehicle control signals. DriveMLM [7] utilizes multi-view images, LiDAR point clouds, traffic rules, and user instructions from a real simulator to perform closed-loop driving. This multimodal model is constructed with LLaMA and ViT as the image processor. GPT-Driver [8] reframes motion planning as a language modeling task, using GPT-3.5 to represent the planner's inputs and outputs as language tokens."}, {"title": "II. RELATED WORKS", "content": "This section reviews motion planning methods and the practical application of LLMs in autonomous driving, focusing on their strengths and challenges in complex traffic environments.\nAutonomous driving utilizes various motion planning strategies for efficient vehicle navigation. (1) Rule-based method: This approach generates paths based on predefined rules that account for environmental constraints like road geometry and traffic signals [10]. While simple and efficient, it is rigid and struggles to adapt to unexpected changes. (2) Optimization-based method: Optimization algorithms compute optimal trajectories by minimizing a cost function considering factors such as time, energy, safety, and comfort [1]. Though precise, these methods are computationally intensive and may not suit real-time decision-making. (3) Learning-based method: This approach uses machine learning to adapt to dynamic environments by learning from past data [11]. Deep neural networks and reinforcement learning provide adaptability but require significant data and resources, often struggling with rare or novel scenarios.\nLarge models (LMs) based on the Transformer, such as Large Language Models [2], [3], vision models [4], [5], time series models [12], [13], and multimodal models [14], have gained widespread attention due to their unique advantages. With billions to trillions of parameters, these models accumulate extensive knowledge through pre-trained on large datasets, significantly advancing the automation and diversification of data processing while reducing reliance on human expertise. Such capabilities have attracted broad interest in the industrial sector, fostering numerous studies targeting industrial intelligence.\nThe collaboration between large and small language models garners considerable attention. Inspired by dual-process cognitive theory, various methods can be integrated into a unified framework. Research indicates that the essential difference between large and small models lies in the control of uncertainty in next token predictions during the decoding process, and it highlights that collaborative interactions between models are most critical at the beginning of the generation process [9].\nIn recent years, significant progress has been made in the application of LLMs in the field of autonomous driving. Utilizing LLMs to enhance decision-making processes in autonomous vehicles has the potential to transform their operational methods. This approach offers personalized assistance, facilitates continuous learning, and improves decision intelligence [15]. PlanAgent [16] is a multimodal large language model-based autonomous motion planning agent system that enhances environmental understanding through Bird's Eye View (BEV) and lane-graph-based textual descriptions. It introduces a hierarchical Chain of Thought (CoT) [17] to guide the MLLM in generating planner code. Hu et al. [18] propose an LLM-driven collaborative driving framework for multiple vehicles, featuring lifelong learning capabilities. It allows different driving agents to communicate with each other, facilitating collaborative driving in complex traffic scenarios. DiLu [19] is the first framework to leverage knowledge-driven capabilities in autonomous driving decision-making. It combines reasoning and reflection modules, enhancing the capabilities of LLMs, enabling them to apply knowledge and perform causal reasoning in the autonomous driving domain. TrafficGPT [20] reveals the application potential of large language models in the smart transportation domain. These models possess the capability to view and process traffic data, providing profound decision support for urban traffic system management. Additionally, they assist in human decision-making during traffic control, demonstrating their practicality and efficacy in traffic management."}, {"title": "III. EDGE-CLOUD COLLABORATIVE MOTION PLANNING FOR AUTONOMOUS DRIVING", "content": "In this section, we elaborate on the methodologies and technologies employed in the EC-Drive system, emphasizing the use of edge and cloud models as well as the collaborative process between them. This approach ensures efficient and safe decision-making, even in complex driving environments.\nIn edge-cloud collaborative intelligent driving systems, we deploy small-scale LLMs on edge devices for real-time motion planning and large-scale LLMs on the cloud to provide efficient support. Edge devices, when processing real-time driving data, may encounter distribution shifts or decreased model confidence due to natural variations (such as changes in lighting or weather) or sensor degradation, which can affect model performance.\nTwo primary scenarios necessitate the request for support from large models in the cloud:\n(1) When the vehicle encounters new or previously unseen objects or situations, increasing decision-making complexity, and the edge model may be insufficient for accurate inference.\n(2) For instance, visual obstructions or lighting variations may reduce the accuracy and reliability of edge model predictions. Under such circumstances, leveraging large models in the cloud for deeper analysis can enhance system performance and safety.\nThe proposed system architecture, illustrated in Fig. 2, integrates edge and cloud components to enhance the overall performance of autonomous driving systems. The vehicle employs small-scale LLMs, fine-tuned using instruction-based approaches as shown in Fig. 3, to manage routine driving tasks and process real-time sensor data for immediate decision-making.\nWe employ LLaMA-Adapter [24], a parameter-efficient tuning mechanism based on the LLaMA language model. LLaMA-Adapter is specifically designed for scenarios where computational resources are constrained, such as autonomous driving. It introduces small, zero-initialized attention modules, which are fine-tuned to adapt to new tasks without modifying the entire pre-trained model. This approach minimizes the additional computational overhead, making it ideal for real-time motion planning on edge devices. The model processes real-time sensor data, including text, vision and LiDAR inputs, to make preliminary driving decisions under normal conditions. Pre-print, manuscript submitted to IEEE.\nIn the cloud, large-scale LLMs such as GPT-4 offer advanced computational power for handling more complex and dynamic driving scenarios. Real-time data from various onboard sensors, including cameras, LiDAR, and radar, is collected and preprocessed to extract pertinent features. This preprocessing converts the raw sensor data into a structured format that is amenable to model inference. The processed data is then input into the edge model for initial inference, facilitating efficient and timely driving decisions under varying conditions.\nInspired by [25], we utilize the Alibi Detect library [26] to monitor edge model performance. If anomalies or low-confidence predictions are detected, the system flags those instances and uploads the data to the cloud. The cloud model then performs detailed inference to generate optimized decisions, which are integrated with the edge model's outputs to update the vehicle's driving plan, ensuring safe and efficient operation."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we present experimental investigations into the real-time operational capabilities of autonomous driving systems using LLMs under different computational paradigms: Edge, Cloud, and Edge-Cloud Collaborative scenarios. Each subsection details distinct approaches and methodologies\u2014ranging from handling in-vehicle data processing at the edge to leveraging cloud computational power for intensive data analysis and decision-making. This comparative study aims to highlight the efficiency, scalability, and reliability of each model under varied driving conditions and their implications on autonomous driving technologies.\nWe transcribe the current driving scene into descriptive text, including the current speed, acceleration, position of the ego vehicle, and information about surrounding vehicles. For example, the ego vehicle is traveling in the rightmost lane of a four-lane road at a speed of 25.0 m/s, with an acceleration of 0.0 m/s\u00b2, and its lane position is 361.18 m. The information for other vehicles includes their speed, acceleration, and relative position, such as vehicle 496 in the left lane, ahead by 372.81 m, traveling at a speed of 21.2 m/s, with an acceleration of 0.2 m/s\u00b2.\nThe scene description is embedded into vectors and input into the LLaMA-Adapter. Using CoT techniques, LLaMA-Adapter generates sequential reasoning logic and performs step-by-step logical reasoning. For instance, it first assesses whether the vehicle can accelerate. If not, it evaluates the safety of maintaining the current speed. If necessary, it further evaluates the possibility and safety of changing lanes.\nAs shown in Fig. 2, the system decodes the final decision from the LLM's response and translates it into corresponding vehicle actions, following the outlined process.\nAs shown in Fig 4, we demonstrate how the LLM performs step-by-step logical reasoning and decision-making in a complex traffic environment.\nAs depicted in Fig. 5, edge models face significant challenges in real incremental scenarios. Through the identification module, the system selectively uploads data to the cloud-based foundational model, powered by GPT-4, for queries, thereby enhancing motion planning performance. The inference process of the cloud model in real scenarios encompasses three critical stages: perception, prediction, and planning. These stages are essential for ensuring the model's efficient response.\nThis project utilizes data collected by autonomous vehicles at the Guangzhou International Campus of South China University of Technology as the testing benchmark. The dataset comprises images captured from the perspective of autonomous vehicles, with a lower camera angle that aligns closely with practical autonomous driving applications such as delivery and patrol. Fig. 6 illustrates the inference outcomes of different models in the same scenario. In most cases, edge models (LLaMA-Adapter [24]) demonstrate performance comparable to cloud models (GPT-4 [2]), where invoking cloud models offers limited improvement to the driving task and may lead to resource wastage and unnecessary delays.\nAlthough the edge model is capable of making quick inferences in most cases, the cloud model demonstrates extremely high accuracy when dealing with complex scenarios. For instance, in pedestrian recognition and complex road planning (as shown in the second case of Fig. 6), the cloud model can correct the inference errors made by the edge model, thereby enhancing the overall safety and reliability of the system.\nInspired by [27], we evaluate LLMs based on three metrics. Gaze: Assessing the accuracy of LLMs in identifying areas of focus during the driving process. Scene Understanding: Evaluating the precision of LLMs in describing elements present in the current driving scene. Logic: Analyzing the correctness of the reasoning employed by LLMs in making driving decisions.\nTab. I presents the inference results of models of varying sizes within the dataset. The performance of cloud-based LLMs significantly surpasses that of edge-based small-scale models: As shown in the table, cloud-based LLMs (such as GPT-4 and GPT-40) achieve higher scores across all three metrics (Gaze, Scene Understanding, and Logic) compared to edge-based small-scale models. Specifically, GPT-4 scores 87.1 in Gaze and 88.9 in Scene Understanding, significantly outperforming the highest scores of edge-based models, which are 66.8 and 59.4, respectively.\nEdge-based small-scale LLMs exhibit advantages in specific scenarios: Despite the superior overall performance of cloud-based LLMs, edge-based small-scale models demonstrate significant benefits in environments with limited computational resources or where low-latency responses are required. For instance, edge-based models such as Phi-2-2.7B and TinyLlama-1.1B provide relatively stable performance under constrained resources."}, {"title": "V. CONCLUSION", "content": "This study extensively investigates the application performance of LLMs in autonomous driving systems, leveraging edge computing, cloud computing, and edge-cloud collaborative processing. In the edge computing environment, the system swiftly processes real-time driving data, utilizing CoT for logical inference and decision-making. The cloud model exhibits exceptional perception, prediction, and planning capabilities when handling complex driving scenarios. Notably, the edge-cloud collaboration selectively uploads critical data to the cloud, not only enhancing inference speed and conserving communication resources but also significantly reducing system latency. This collaboration also markedly improves the edge model's understanding of incremental and complex scenarios, thereby enhancing overall system performance in motion planning. The experimental results validate the effectiveness and efficiency of the model in practical applications. These findings provide crucial theoretical and practical guidance for the future development of autonomous driving technologies."}]}