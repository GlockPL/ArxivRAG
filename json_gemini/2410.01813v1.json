{"title": "Privacy-Preserving SAM Quantization for Efficient Edge Intelligence in Healthcare", "authors": ["Zhikai Li", "Jing Zhang", "Qingyi Gu"], "abstract": "The disparity in healthcare personnel expertise and medical resources across different regions of the world is a pressing social issue. Artificial intelligence technology offers new opportunities to alleviate this issue by empowering diagnostic and treatment capabilities in underdeveloped areas. Segment Anything Model (SAM), which excels in intelligent image segmentation, has demonstrated exceptional performance in medical monitoring and assisted diagnosis. Unfortunately, the huge computational and storage overhead of SAM poses significant challenges for deployment on resource-limited edge devices, especially in underdeveloped regions with limited equipment and computing power. Quantization is an effective solution for model compression; however, traditional methods rely heavily on original data for calibration, which raises widespread concerns about medical data privacy and security. In this paper, we propose a data-free quantization framework for SAM, called DFQ-SAM, which learns and calibrates quantization parameters without any original data, thus effectively preserving data privacy during model compression. Specifically, we propose pseudo-positive label evolution for segmentation, combined with patch similarity, to fully leverage the semantic and distribution priors in pre-trained models, which facilitates high-quality data synthesis as a substitute for real data. Furthermore, we introduce scale reparameterization to ensure the accuracy of low-bit quantization. We perform extensive segmentation experiments on datasets of various modalities such as CT and MRI, and DFQ-SAM consistently provides significant performance on low-bit quantization, e.g., 4-bit quantization results in only a 2.01% accuracy decrease in abdominal organ segmentation on AbdomenCT1K dataset. DFQ-SAM decouples the model deployment from real data, eliminating the need for data transfer in cloud-edge collaboration, thereby protecting sensitive data from potential attacks. By offloading complex medical analysis tasks to local nodes and employing privacy-preserving model compression, it enables secure, fast, and personalized healthcare services at the edge. This enhances system efficiency and optimizes resource allocation, and thus facilitating the pervasive application of artificial intelligence in worldwide healthcare, especially in remote or resource-limited regions.", "sections": [{"title": "1 INTRODUCTION", "content": "THE severe imbalance in healthcare personnel expertise and medical resources worldwide, particularly between developed and underdeveloped regions, has become an urgent social issue [1], [2], [3]. This disparity prevents patients in underdeveloped regions from receiving timely and effective medical services. For instance, over 70% of the global cancer burden occurs in low- and middle-income settings, yet most patients with malignancies lack access to the resources and systems available in high-income countries [4]. Artificial intelligence technology, especially Segment Anything Model (SAM) that excels in intelligent image segmentation [5], [6], [7], offers unprecedented opportuni- ties to address this issue. SAM demonstrates exceptional performance in medical monitoring and assisted diagnosis, significantly enhancing diagnostic accuracy and treatment outcomes, thereby improving the overall healthcare stan- dards in underdeveloped regions.\nDespite the outstanding performance, SAM's high compu- tational and storage requirements pose significant challenges for its deployment and application [8], [9]. This is particularly problematic in underdeveloped regions, where economic constraints limit the capabilities of medical equipment and computing power, which further exacerbates the inequality in healthcare resources, preventing those most in need of technological support from benefiting. Therefore, to facili- tate the downward mobilization of high-quality healthcare resources and ensure convenient access, we are committed to exploring effective model compression techniques that allow SAM to be deployed on low-cost edge devices, serving as dedicated local nodes for each demand source.\nModel quantization, which reduces the representation precision of model parameters, is a practical solution for model compression [10], [11]. Note that considering the diversity in hardware and medical tasks across local nodes, such as different generations of GPUs and CPUs, and various image modalities like CT and MRI, it's essential that each node performs quantization locally with settings tailored to its specific conditions, rather than relying on a unified approach in the cloud. However, traditional quantization methods require access to the original training data from the cloud to identify parameter distributions for quantization calibration [12], [13], [14]. This dependency on original data poses significant risks to data privacy and security, especially in the data-sensitive medical field [15], [16], [17]. To this end, data-free quantization (DFQ), a.k.a. zero-shot quantization, is increasingly gaining attention [18], [19], [20]. Its goal is to utilize the prior information of the pre-trained model to synthesize data in reverse and then use them for calibration, as shown in Fig. 1. Despite some progress, previous DFQ efforts focus on convolutional neural networks (CNNs) for classification or detection tasks, with little exploration of SAM for segmentation tasks, leaving it an open issue.\nFor image segmentation with SAM, the dual complexity of its model structure and task presents greater challenges in designing the DFQ algorithm. In terms of model structure, the image encoder in SAM utilizes a Transformer architecture [21]. Most previous works have been limited to CNNs, relying on BatchNorm statistics, which are not applicable to Transformers. Additionally, the DFQ algorithm requires pseudo-labels to guide data synthesis, but segmentation task labels must account for mask size, category, and relative position, making it challenging to specify them appropriately.\nTo address the above issues, we propose DFQ-SAM, an accurate and practical data-free quantization framework for SAM. Specifically, we propose an evolutionary strategy for pseudo-positive labels, which iteratively samples model outputs to reverse-update the preset labels. This approach ensures that the labels are fully aligned with the response preferences of the pre-trained model, thereby guaranteeing their rationality and effectiveness. We also leverage patch similarity to further exploit the prior knowledge within the Transformer model, enhancing the quality of synthesized data. Moreover, to achieve accurate quantization calibration, we introduce the scale reparameterization technique, which significantly improves the performance of low-bit quantiza- tion. DFQ-SAM exhibits strong generalization across various image modalities, and the experimental results show that it consistently achieves impressive performance on datasets of multiple modalities such as CT and MRI. For instance, when performing 4-bit quantization, DFQ-SAM achieves an 8x reduction in model size and a 64x reduction in computational complexity (i.e., BOPs), while only incurring a 2.01% drop in accuracy on AbdomenCT1K dataset.\nNote that this work is an extension of our previous works, PSAQ-ViT [22], [23] and RepQ-ViT [24], which lay a strong theoretical foundation and provide early validation. PSAQ-ViT introduces patch similarity, making the first successful attempt at data-free quantization for Transformers, and RepQ-ViT is a powerful baseline of low-bit quantization calibration in the community. Building on these achievements, DFQ-SAM introduces a novel segmentation label evolution strategy tailored for SAM, resulting in a promising and practical data-free quantization framework.\nTo the best of our knowledge, it is the first data-free quan- tization approach for SAM. DFQ-SAM has the potential to accelerate the widespread adoption of SAM-based intelligent healthcare through low-cost, multi-node distributed edge deployment, as shown in Fig. 2. In this system, a large central model is maintained in the cloud, where it is continuously updated and upgraded, and periodically distributed to edge nodes. At the edge, the model is quantized according to the hardware specifications and used to deliver medical services. Thanks to data-free quantization, only the pre- trained model needs to be transmitted between the cloud and the edge, eliminating the need for any training data. This not only greatly reduces bandwidth requirements but, more importantly, ensures robust protection of data privacy and security. Therefore, DFQ-SAM effectively addresses data privacy concerns in the compression and deployment of SAM, facilitating its widespread and secure application in real-world scenarios. In the context of the current imbalance in healthcare resource distribution, this technology helps underdeveloped regions equally benefit from intelligent healthcare, making a significant impact on improving medi- cal conditions in these areas."}, {"title": "2 RELATED WORKS", "content": "Medical SAM Medical image segmentation, which works on separating different tissues, organs, or pathological areas within complex medical images, can provide clear diagnostic insights and treatment guidance, and thus plays a crucial role in clinical practice [25], [26]. In recent years, a series of notable deep learning-based models, such as UNet++ [27] and TransUNet [28], have emerged. However, these models are typically designed for specific image modalities or anatomical regions and lack generalizability. In contrast, SAM stands out due to its versatility, zero-shot transfer capabilities, and interactive features [29], resulting in a signif- icant improvement in segmentation performance. To enhance the adaptability of SAM to medical images, MedSAM [5] and SAM-Med 2D [6] collect large-scale medical image datasets to fine-tune the model, significantly improving SAM's performance on medical image segmentation. There are also works that extend them to applications with higher image resolutions [7] and 3D scenarios [30].\nHowever, SAM's high model complexity makes it chal- lenging to deploy and execute on resource-constrained edge devices, particularly in the medical field where high concurrency and real-time processing are essential. FastSAM [31] and MobileSAM [9] optimize model efficiency from an architectural design perspective, but there is still a large gap with practical implementation. Consequently, model compression techniques for SAM are critically desired to enable real-time intelligent medical analysis at the edge.\nData-free Quantization Quantization, which uses reduced numerical precision to represent parameters, is an effec- tive approach for model compression [10], [32]. In the quantization process, parameter calibration relies on the numerical distribution, which traditionally requires the input of original training data to obtain this distribution [13], [33]. This approach raises data privacy and security concerns, particularly in the medical field where access to data is typically restricted, rendering traditional methods impractical [15], [16]. To address these challenges, various efforts have begun to explore the emerging field of data-free quantization [17], [18], [20]. It focuses on leveraging the prior knowledge embedded in pre-trained models to synthesize data for calibration, and the quality of synthesized data is crucial to quantization performance [23].\nThe quality of synthesized data depends on its consis- tency with the original training data in both distribution and semantics. Earlier approaches achieve distribution consis- tency through BatchNorm statistics [15], [18] and promote semantic consistency using preset pseudo-labels [16], [34]. However, they suffer from great limitations on model struc- tures and tasks. On one hand, there is no BatchNorm layer in Transformers, resulting in unavailable statistical information; on the other hand, the presetting of labels for segmentation tasks is unexplored. Therefore, how to realize data-free quantization of SAM for image segmentation remains an open issue. In this paper, we first employ patch similarity as a substitute for BatchNorm statistics to capture the prior distribution in Transformers, as proposed in our preliminary work PSAQ-ViT [22], [23]. Building on it, we propose the pseudo-positive label evolution strategy to generate and iteratively update the segmentation labels, thus achieving accurate data-free quantization of SAM."}, {"title": "3 METHODS", "content": ""}, {"title": "3.1 Preliminaries: Quantization", "content": "Model quantization discretizes the full-precision parameters in the pre-trained model and represents them as integer values. In the following, we introduce the discretization process through the uniform quantizer, which is the simplest and most commonly used quantizer:\n$\\text{Quant} : x = \\text{clip} \\left(\\left[\\frac{x}{s} + z\\right], 0, 2^b - 1\\right), \\\\ \\text{DeQuant} : \\hat{x} = s \\cdot (x' - z) \\approx x,$\nwhere $x \\in \\mathbb{R}$ and $x' \\in \\mathbb{Z}$ represent the floating-point and quantized values, respectively. The operator $[\\cdot]$ refers to the rounding function, and $b \\in \\mathbb{N}$ is the bit-width used for quantization. The de-quantized value $\\hat{x}$ serves as an approximation to $x$. Notably, $s \\in \\mathbb{R}^+$ is the quantization scale, and $z \\in \\mathbb{Z}$ is the zero-point. Both of them are essential parameters for quantization and they are determined by the numerical distribution of $x$ as follows:\n$s = \\frac{\\max(x) - \\min(x)}{2^b - 1}, z = -\\frac{\\min(x)}{s}$\nFormally, Eq. 2 is referred to as the quantization cali- bration procedure. It is evident that accurately capturing the parameter distributions to obtain precise values for $s$ and $z$ is crucial. Traditionally, acquiring the distribution of activation values requires inputting the original training data to extract feature maps. However, this approach raises privacy and security concerns, particularly in sensitive fields like healthcare. Thus, in this paper, we are dedicated to exploring how to perform quantization calibration without accessing the original data."}, {"title": "3.2 Data Synthesis from Pre-trained SAM", "content": "The quality of synthesized data from pre-trained models is measured in two ways: semantic matching and distributional matching. We design SAM-specific methods to facilitate both matching, as described in detail below.\nSemantic Matching: Pseudo-positive Label Evolution To enhance the semantic information of synthesized images, we need to predefine the labels as a guide and utilize the pre-trained model as a discriminator to optimize the images by facilitating the alignment of the model outputs with the labels. For instance, in the classification task, if the category label \"dog\" is preset, the image is optimized so that when it is input into the model, the output predicts \"dog\" with progressively higher confidence, thereby infusing the image with clear and distinct \"dog\" semantics. As a result, the semantics of the final synthesized images are strongly correlated with the predefined labels. However, creating labels with reasonable semantics for segmentation tasks is challenging, as it requires consideration of the mask's size, category, and relative position.\nTherefore, instead of direct manual predefinition, we propose the pseudo-positive label evolution strategy, which continuously samples the model's output in iterations and progressively superimposes high-confidence (i.e., pseudo- positive) predicted masks into the labels. This capitalizes on the output preferences of the pre-trained model, allowing the pseudo-labels to be fully aligned with the semantics learned by the model. Interestingly, the concept of label evolution is also utilized in DIODE [34] for object detection. However, in this work, the segmentation task and the zero- shot capabilities of SAM create a stark contrast, posing greater challenges for mask sampling: (i) Masks are pixel- level, with irregular shapes and connectivity; (ii) SAM's outputs do not include category information.\nTo address the above challenges, we measure confidence by calculating the average category score within the mask region, with the category scores obtained from an auxiliary model (e.g., TransUNet [28]). Formally, the computation process is as follows:\n$M_c^{*(t)} = \\arg \\max_c \\frac{1}{|M_c|} \\sum_{(h,w) \\in M_c} S_{h,w,c}$\nwhere $M_c$ is the predicted mask of category $c$, with region provided by SAM and category provided by the auxiliary model, and $|M_c|$ is the number of pixels in the mask. $S_{h,w,c}$ is the confidence score that the pixel $(h,w)$ belongs to the category $c$. $M_c^*$ is the pseudo-positive response obtained at the $t$-th iteration. To ensure the saliency of sampling, we continuously filter them in real time according to confidence scores and connected component sizes during the iteration process. Eventually, we stack the filtered masks and obtain the pseudo-labels as follows:\n$M_{GT} = \\{M_c^{(j)} | \\max_{(h,w) \\in M_c^{(j)}} (S_{h,w,c^*}) > \\epsilon_1, |M_c^{(j)}| > \\epsilon_2 \\}$\nwhere $\\epsilon_1$ and $\\epsilon_2$ are the filter thresholds for confidence and size of the mask, respectively. Given the pseudo-labels, we define Semantic Matching Loss to enhance the semantic content of synthesized images as follows:\n$L_{SM} = L_{Mask} + \\alpha \\cdot L_{Class}$\n$L_{Mask} = 1 - \\frac{|M_{Pred} \\cap M_{GT}|}{|M_{Pred} \\cup M_{GT}|}$\n$L_{Class} = -\\frac{1}{\\sum_{M_c \\in M_{GT}} |M_c|} \\sum_{M_c \\in M_{GT}} \\sum_{(h,w) \\in M_c} \\log (S_{h,w,c^*})$"}, {"title": "3.3 Quantization Calibration: Scale Reparameterization", "content": "With the synthesized data, it is also crucial to utilize them for accurate quantization calibration. However, the Transformer structure in SAM poses significant challenges for low-bit quantization due to the extreme parameter distributions of its unique components. In particular, LayerNorm activa- tions exhibit severe inter-channel variations, so that sharing a unified quantization scale across all channels leads to crashing quantization performance. Therefore, we propose a novel quantization strategy, which first computes a unique quantization scale for each channel, and then converts them into a unified scale through mathematical equivalent trans- formations. This can significantly improve the quantization accuracy while ensuring hardware adaptability.\nSpecifically, we perform an independent quantization calibration for each channel to obtain the quantization scale $s$ and zero-point $z$. Here, both $s$ and $z$ are $D$-dimensional vectors, and $D$ is the number of channels in LayerNorm acti- vations. Then, we define the reparameterization procedures $\\bar{s} = \\mathbb{E}[s]$ and $\\bar{z} = \\mathbb{E}[z]$, where $\\bar{s}$ and $\\bar{z}$ are the values to be shared by all channels. To hold the model output constant after reparameterization, we need to adjust the affine factors $\\beta$ and $\\gamma$ in LayerNorm and the next layer's weights $W^{qkv}$ and $b^{qkv}$ as follows:\n$\\beta' = (\\beta + s \\cdot r_2)/r_1$\n$\\gamma' = \\gamma/r_1$\n$W^{qkv}_{:,j} = r_1 W^{qkv}_{:,j}$\n$b^{qkv} = b^{qkv} - (s \\cdot r_2) W^{qkv}_{:,j}$\nwhere $r_1 = s/\\bar{s}$ and $r_2 = z - \\bar{z}$ are the variation factors, $j \\in \\{1,\\dots, D'\\}$, and $D'$ is the number of channels in $W^{qkv}$. Eq. 10 can realize equivalent transformations from $s$ and $z$ to $\\bar{s}$ and $\\bar{z}$ while maintaining the model output the same. Note that the equivalent transformations are proposed in our preliminary work RepQ-ViT [24], and the details of the derivation are in Appendix A.2."}, {"title": "3.4 The Overall Quantization Pipeline", "content": "The quantization process of DFQ-SAM is divided into two stages: (i) image synthesis, and (ii) quantization calibration. The whole pipeline is summarized in Algorithm 1. Each stage is described in detail below.\nImage Synthesis Our goal is to fully exploit the prior information in the pre-trained SAM, and thus facilitate the matching of synthesized images with real images from both semantic and distributional aspects. Note that $L_{SM}$ and $L_{DM}$ are specifically designed with careful consideration of the Transformer model architecture and the image seg- mentation task, thus resulting in significant performance. Specifically, we take the image as the optimization objective and update it by calculating the following loss function:\n$L_{IS} = L_{SM} + \\beta \\cdot L_{DM}$\nwhere $\\beta$ is the balance coefficient.\nQuantization Calibration After obtaining the synthesized images, we utilize them instead of the real dataset to calibrate the quantization parameters. In this stage, we employ scale reparameterization presented in Eq. 10 to ensure the accuracy of low-bit quantization. Note that we adopt post-training quantization, which does not require any resource-intensive training or fine-tuning, allowing the compression process to be achieved easily and quickly."}, {"title": "4 EXPERIMENTS", "content": "We conduct exhaustive experiments to demonstrate the effec- tiveness and advantages of DFQ-SAM. The implementation details, including the models, datasets, comparison methods, and settings, are presented in Appendix B.1. In the following, we will discuss the experimental results in detail."}, {"title": "4.1 Analysis of Synthesized Images", "content": "Fig. 5 shows the visualization results of synthesized images (256\u00d7256 pixels). It should be emphasized that only the pre-trained model is required to synthesize these images, without any other information, especially the original data or any external prior. In terms of brightness and contrast, we can observe that the synthesized images are well-aligned with the distribution of real images, demonstrating that the image synthesis process has successfully captured the fundamental visual priors in the pre-trained model. Additionally, from a semantic perspective, the synthesized images accurately reflect the anatomical structures and relative positions of various organs, aligning well with the characteristics of real images. This alignment in both aspects plays a crucial role in downstream quantization calibration."}, {"title": "4.2 Efficiency Evaluation of Quantized Models", "content": "We demonstrate the effect of inference acceleration by deploying the 4-bit quantized model on different NVIDIA GPUs, including Titan RTX, RTX 3090, and RTX A6000. In this implementation, the kernel of the quantized computation is built on CUTLASS [35], which is a high-performance CUDA library specifically designed for matrix multiplication. The acceleration results are presented in Fig. 6, indicating a 3.43~3.59\u00d7 speedup compared to the full-precision baseline. Therefore, the quantized SAM consistently delivers signifi- cantly enhanced inference efficiency across various hardware platforms, exhibiting strong compatibility and versatility. This ensures real-time and efficient medical services at the edge, which is particularly crucial in distributed systems with diverse hardware environments at local nodes, thereby promoting the widespread deployment and application of intelligent healthcare."}, {"title": "4.3 Accuracy Evaluation of Quantized Models", "content": "Here, we conduct extensive experiments on a variety set of datasets to demonstrate the accuracy advantages of the proposed DFQ-SAM. Notably, to highlight the generalization and robustness of the algorithm, the adopted datasets are from diverse modalities, including CT, MRI, PET, Ultrasound, Xray, Dermoscopy, and Endoscopy. The evaluation metric is the predicted IoU of the masks. The accuracy results are presented and discussed in detail below."}, {"title": "4.4 Ablation Studies", "content": ""}, {"title": "Effectiveness of Each Component", "content": "Here, we verify the effectiveness of each component in DFQ-SAM, and the results are shown in Fig. 8. The initial baseline is established by optimizing Gaussian noise with random labels. Building on this, the sequential addition of the proposed modules, including label evolution, patch similarity, and scale repa- rameterization, further enhances performance at each step. For instance, on AbdomenCT1k dataset, the initial accuracy is 68.73, and the improvements by the three components are 1.42, 2.46, and 1.71, respectively, resulting in a final accuracy of 74.32. Therefore, each component plays a crucial role in improving the overall performance."}, {"title": "Grid Search for Hyperparameters", "content": "We also assess the impact of the balance hyperparameters $\\alpha$ and $\\beta$ through a grid search and determine the optimal values. The results are shown in Fig. 9. We observe that the quantization perfor- mance is robust to changes in hyperparameters, showing only minor variations, such as between 0.732 and 0.744. More importantly, they reach the optimal value at the same point (0.5, 0.05) across different datasets."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose DFQ-SAM, a model quantization framework for medical SAM without requiring any original training data, thus preserving data privacy in model com- pression and deployment. DFQ-SAM effectively leverages the prior knowledge from the pre-trained model to invert the synthesized data, aligning it with real data in both semantic and distributional aspects. It then employs an advanced quantization calibration strategy, achieving near lossless 4- bit model compression. To the best of our knowledge, this is the first successful attempt at data-free quantization for SAM. Exhaustive experiments demonstrate that DFQ-SAM exhibits encouraging performance on a range of datasets with multiple modalities, even surpassing the real-data- driven approach. With the proposed framework, the model complexity of medical SAM is significantly reduced, while data privacy is well protected in cloud-side collaboration, which facilitates the provision of safe, reliable, and fast intelligent medical services on resource-constrained devices.\nThis helps to actively promote the downward sinking of high-quality medical resources, ensuring more equitable access to healthcare in remote and underdeveloped areas, which is crucial for addressing the current imbalance in the distribution of medical resources."}, {"title": "APPENDIX A OUR PRELIMINARY EFFORTS FOR DFQ-SAM", "content": "Our preliminary efforts, PSAQ-ViT [22], [23] and RepQ-ViT [24], have laid a strong foundation for the success of DFQ- SAM, allowing us to accomplish accurate low-bit model quantization without any original data. Here, we present their ideas and details."}, {"title": "A.1 PSAQ-VIT: Patch Similarity Metric", "content": "PSAQ-ViT is the first successful attempt within the com- munity to quantize Transformers without any original data, aiming to utilize the inherent prior information embedded in the unique properties of Transformer models to synthesize data. In the absence of an absolute value metric such as BatchNorm statistics, the approach focus on assessing the general difference in model inference when the input is Gaus- sian noise versus real images, and anticipates developing a relative value metric accordingly to optimize the noise by reducing the above difference.\nAs noted by [21], the self-attention mechanism in Trans- formers is designed to extract key information during training, such as distinguishing the foreground from the background, which helps the model make accurate decisions. During the inference phase of a pre-trained model, real images generate varying responses between foreground and background patches, resulting in diverse patch similarities (i.e., differences in response similarity across patches). In contrast, Gaussian noise inputs, which do not clearly sep- arate foreground from background, lead to more uniform responses.\nTherefore, PSAQ-ViT treats the diversity of patch similar- ities as the key difference and quantifies this diversity using the differential entropy of patch responses in self-attention mechanism. By using this measure as an optimization criterion, the approach gradually transforms the image from Gaussian noise to one with increasingly diverse responses, thereby closely approximating a real image.\nThe patch similarity, its differential entropy, and the loss are presented in Eqs. 7, 8, and 9, respectively."}, {"title": "A.2 RepQ-ViT: Scale Reparameterization", "content": "RepQ-ViT enables accurate low-bit (e.g., 4-bit) quantization calibration without the need for cumbersome and resource- intensive training or fine-tuning. It achieves this by ele- gantly addressing the challenge of quantizing components with extreme distributions. Specifically, it proposes a novel quantization-inference decoupling paradigm, where the former employs complex quantizers to maintain the data distribution as much as possible, and the latter employs simplified quantizers to perform fast inference. And impor- tantly, the two can be equivalently transformed by scale reparameterization.\nThe proposed paradigm is remarkably effective for com- ponents with extreme distributions, especially LayerNorm activations with severe inter-channel variance. RepQ-ViT first performs channel-wise quantization for LayerNorm activa- tions in the quantization process, i.e., each channel has a unique quantization scale, and then equivalently transforms to layer-wise quantization in the inference process, i.e., all channels share a unified quantization scale. In the following we describe the derivation of the equivalent transformations.\nGiven LayerNorm activations X, when performing channel-wise quantization, the quantization scale s \u2208 RD and zero-point z \u2208 ZD are obtained. The objective is to reparameterize them into scalars $\\bar{s} \\in \\mathbb{R}^1$ and $\\bar{z} \\in \\mathbb{Z}^1$ to accommodate layer-wise quantization. Here, s and z are pre-specified and set to their respective mean values, i.e., $\\bar{s} = \\mathbb{E}[s]$, $\\bar{z} = \\mathbb{E}[z]$. Introducing the variation factors $r_1 = s/\\bar{s}$ and $r_2 = z - \\bar{z}$, the following equations are established:\n$\\bar{z} = \\frac{[\\min(X_{:,d})]_{1 < d < D} + s \\cdot r_2}{s}$\n$\\bar{s} = \\frac{[\\max(X_{:,d}) - \\min(X_{:,d})]_{1 < d < D}/r_1}{2^b-1}$\nEq. 12 indicates that adding $s \\cdot r_2$ to each channel of X results in $\\bar{z}$, while Eq. 13 shows that dividing each channel of X by $r_1$ achieves $\\bar{s}$. These adjustments can be implemented by modifying the affine factors in LayerNorm layer as follows:\n$\\beta' = \\frac{\\beta + s \\cdot r_2}{r_1}, \\qquad \\gamma' = \\frac{\\gamma}{r_1}$\nThe above procedure results in a distribution shift of activations, i.e., $X'_{n,:} = (X_{n,:} + s \\cdot r_2)/r_1$. This distribution shift can be corrected by applying inverse compensation to the weights of the subsequent layer. Specifically, using equivalent transformations, we obtain:\n$X_{n,:} W^{qkv} + b^{qkv} = \\frac{r_1}{r_1}(X_{n,:} W^{qkv}) + (b^{qkv} - (s \\cdot r_2) W^{qkv})$\nwhich suggests that to ensure the next layer's outputs remain consistent, we need to adjust the weights as follows:\n$W^{qkv}_{:,j} = r_1 W^{qkv}_{:,j}$\n$b^{qkv} = b^{qkv} - (s \\cdot r_2) W^{qkv}_{:,j}$\nCombining Eqs. 14 and 16, the parameters to be trans- formed are presented in Eq. 10."}, {"title": "APPENDIX B MORE EXPERIMENTAL DETAILS AND RESULTS", "content": ""}, {"title": "B.1 Implementation Details", "content": "Models and datasets The pre-trained model we adopt is SAM-Med2D [6], which achieves impressive image segmen- tation performance in the medical domain. To demonstrate generalizability, we adopt a variety of datasets from multiple modalities, as follows:\n*   AbdomenCT1k [36]: Abdominal CT organ segmentation, including the liver, kidneys, spleen, and pancreas. The test dataset includes 3,000 images with 3,361 labels.\n*   FLARE [37]: Abdominal CT organ segmentation, in- cluding 13 regions such as the liver, spleen, pancreas, right kidney, left kidney, and stomach. The test dataset includes 479 images with 2,480 labels.\n*   Synapse [38]: Abdominal CT organ segmentation, in- cluding 9 regions such as the aorta, gallbladder, spleen, left kidney, and right kidney. The test dataset includes 175 images with 672 labels.\n*   ACDC [39]: Cardiac MRI organ segmentation, including delineation of the left ventricular cavity, myocardium, and right ventricle. The test dataset includes 423 images with 481 labels.\n*   MSD [40]: MRI organ segmentation, including the heart and prostate. The test dataset includes 190 images with 193 labels.\n*   ATLAS [41]: Liver MRI organ segmentation, including the liver and liver tumors. The test dataset includes 773 images with 1,169 labels.\n*   AutoPET [42]: PET lesion segmentation, including ma- lignant lymphoma, melanoma, and non-small cell lung cancer. The test dataset includes 521 images with 533 labels.\n*   ISIC [43] [44]: Dermoscopy melanoma segmentation. The test dataset includes 330 images with 330 labels.\n*   EndoVis [45]: Endoscopy abdominal organ segmentation. The test dataset includes 492 images with 648 labels.\n*   SA-Med2D-20M [6]: The large medical image dataset collected in SAM-Med 2D, and we use ultrasound, X- ray, and Endoscopy datasets (prefixed with SA). SA- Ultrasound is the segmentation of nerves and breasts, including 198 images with 198 labels; SA-Xray is the chest X-ray segmentation for pneumothorax and pul- monary embolism, including 555 images with 580 labels; SA-Endoscopy is the colonoscopy polyp segmentation, including 126 images with 127 labels.\nComparison methods First, DFQ-SAM is compared to the full-precision baseline, which is the original pre-trained model without any post-processing. Then, since there is no data-free quantization work for SAM to date, we have to construct the comparison methods on our own under equivalent conditions, as follows:\n    *   Real Data: Sampling real data from the original dataset for quantization calibration.\n    *   Gaussian Data: Directly using random Gaussian noise for quantization calibration.\nNote that to ensure a fair comparison, they differ from DFQ-SAM only in calibration data, while remaining identical in other aspects, such as the calibration strategy.\nExperimental settings All experiments in this work are implemented in Pytorch. To accommodate the hardware and deployment toolchains, we apply channel-wise quantization for weights and layer-wise quantization for activations in the inference process, where scale reparameterization is applied to post-LayerNorm activations in all blocks. The quantization bit precision is set to 4, both for weights and activations. We synthesize one image for each dataset to perform quantization calibration, with a resolution of 256x256 pixels, which is aligned with the training set of SAM- Med2D. In the data synthesis phase, we adopt the Adam [46] optimizer. Although further tuning might improve accuracy, we standardized the process by setting the total number of iterations to 1500, with label evolution occurring simultane- ously during the first 500 iterations. The hyperparameters $\\alpha$ and $\\beta$ are set to 0.5 and 0.05, respectively, after a grid search. All data synthesis and accuracy evaluation experiments are done on a single NVIDIA RTX A6000 GPU."}, {"title": "B.2 Visualization of Label Evolution", "content": "Fig 10 provides more illustrations of pseudo-label evolution and synthesized image updating (256x256 pixels). In the case of different samples, both labels and images co-evolve and gradually learn more semantic information, resulting in a win-win situation."}, {"title": "B.3 Qualitative Evaluation of Quantization Results", "content": "Fig 11 provides more visualizations of segmentation results on different datasets (256\u00d7256 pixels). As observed, the synthesized data from our DFQ-SAM offers significantly better quantization calibration than Gaussian noise. Fur- thermore, the 4-bit quantized model produced by DFQ- SAM achieves performance comparable to the full-precision baseline, enabling almost lossless compression."}]}