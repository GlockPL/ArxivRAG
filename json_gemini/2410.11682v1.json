{"title": "SURFHEAD: AFFINE RIG BLENDING FOR GEOMETRICALLY ACCURATE 2D GAUSSIAN SURFEL HEAD AVATARS", "authors": ["Jaeseong Lee", "Taewoong Kang", "Marcel C. B\u00fchler", "Min-Jung Kim", "Sungwon Hwang", "Junha Hyung", "Hyojin Jang", "Jaegul Choo"], "abstract": "Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render unseen poses due to their reliance on similarity transformations, which cannot handle stretch and shear transforms essential for detailed deformations of geometry. To address this, we propose SurFhead, a novel method that reconstructs riggable head geometry from RGB videos using 2D Gaussian surfels, which offer well-defined geometric properties, such as precise depth from fixed ray intersections and normals derived from their surface orientation, making them advantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of both normals and images, even in extreme poses, by leveraging classical mesh-based deformation transfer and affine transformation interpolation. SurFhead introduces precise geometric deformation and blends surfels through polar decomposition of transformations, including those affecting normals. Our key contribution lies in bridging classical graphics techniques, such as mesh-based deformation, with modern Gaussian primitives, achieving state-of-the-art geometry reconstruction and rendering quality. Unlike previous avatar rendering approaches, SurFhead enables efficient reconstruction driven by Gaussian primitives while preserving high-fidelity geometry.", "sections": [{"title": "1 INTRODUCTION", "content": "The construction of personalized head avatars has seen rapid advancements in both research and industry. Among the most notable developments in this field is the Codec Avatar family (Ma et al., 2021; Saito et al., 2024), which aims to reconstruct highly detailed, movie-quality head avatars using high-cost data captured from head-mounted cameras or studios. This approach has spurred significant research efforts to bridge the gap between high-cost and low-cost capture systems by utilizing only using RGB video setups. Neural Radiance Fields (NeRFs) (Mildenhall et al., 2021) have further accelerated these efforts with their topology-agnostic representations. As a result, numerous NeRF-based methods (Gafni et al., 2021; Athar et al., 2022; Zielonka et al., 2023b) for constructing head avatars from RGB videos have emerged, demonstrating potentials of improving high-cost systems (Ma et al., 2021; Yang et al., 2023; Saito et al., 2024).\nRecently, 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) has been used to create photo-realistic head avatars. However, there has been no attempt to create geometrically accurate head avatars within the 3DGS framework. While other representation-based methods (Bharadwaj et al., 2023; Zheng et al., 2022; 2023; Grassal et al., 2022) demonstrate plausible geometry using implicit or explicit representations, they still suffer from suboptimal results; over-smoothed geometry from implicit neural representations or inherently limited explicit representations such as 3D Morphable Face Model (3DMFM) learned meshes or inflexible points."}, {"title": "2 PROPOSED METHOD", "content": "This section highlights the key technical contributions of our work. We introduce a novel representation for geometrically accurate 2D Gaussian primitive-based head avatars, alongside a geometrically precise Jacobian deformation gradient and corresponding principled normal deformation (Sec. 2.2). Our proposed Jacobian Blend Skinning (JBS) approach (Sec. 2.3) allows for the interpolation of affine transformations, effectively addressing the discontinuities caused by piece-wise deformation in meshes. Additionally, we tackle the concavity (hollow-illusion) issue observed in the cornea by regularizing the modern 3DMFM model, FLAME (Li et al., 2017)'s eyeball model, to better manage high specularity through the use of Anisotropic Spherical Gaussians (ASGs) (Xu et al., 2013) (Sec. 2.4). We begin with preliminary on Gaussian Splatting and GaussianAvatars in Sec. 2.1."}, {"title": "2.1 PRELIMINARY", "content": "The two key building blocks of our approach are 2D Gaussian Splatting (2DGS) (Huang et al., 2024) and GaussianAvatars (Qian et al., 2024). The former provides intricate geometric properties, such as depth and normal. The latter employs a mesh-based rule-governed rigging strategy, allowing primitives to be rigged directly according to mesh triangles."}, {"title": "2.1.1 GAUSSIAN SPLATTING", "content": "3D Gaussian Splatting (Kerbl et al., 2023) (3DGS) reconstructs a scene with anisotropic 3D Gaussian primitives. Each Gaussian is defined by a positive semi-definite covariance matrix $\\Sigma$ that is centered at a position $\\mu$. The covariance $\\Sigma$ is decomposed as $\\Sigma = RSST RT$, where R is a rotation matrix and S is a diagonal scaling matrix. This covariance matrix is used when estimating each Gaussians's function value at 3D point x such as $G(x) = exp(-\\frac{1}{2}(x - \\mu)^{T}\\Sigma^{-1}(x - \\mu))$. Besides these geometrical properties, each Gaussian has appearance properties opacity a and color c. To render the scene, the final color C is computed by alpha-blending Gaussians after projecting them to the image plane: $C = \\sum_{i=1}^n C_i \\alpha_i G_{proj}(x) \\prod_{i=1}^{i-1}(1 - \\alpha_i G_{proj}(x))$. $G_{proj}$ is given by evaluating the function value of 2D projection of the 3D Gaussian in image space by EWA volume splatting algorithm (Zwicker et al., 2001).\nAn extension to 3DGS, 2DGS (Huang et al., 2024), modifies 3D primitives to 2D \"flat\" surfels embedded in 3D space for reconstructing high-fidelity geometry. 2D surfels have some advantages"}, {"title": "2.1.2 GAUSSIANAVATARS", "content": "GaussianAvatars (Qian et al., 2024) (GA) is an efficient head avatars model with FLAME (Li et al., 2017) mesh binding inheritance. Namely, each Gaussian has a single parent triangle in a canonical (local) space. When they are rendered in a deformed space, each Gaussian is transformed with their parent's current state such as relative rotation matrix $R_p$, relative triangle's area volume as isotropic scale $s_p$, and relative barycenter position of triangle $T_p$ in the world space. Namely,\n$R = R_p R_c, \\mu = s_p R_p \\mu_c + T_p, S = S_p S_c$, (1)\nwhere $R_c$ is canonical rotations, $\\mu_c$ is canonical position, and $S_c$ is canonical scaling in parent's triangle. Although this deformation approach is frequently utilized in certain head avatar research (Qian et al., 2024; Shao et al., 2024), we emphasize that it falls short in capturing the stretch and shear deformations that are essential for accurately extrapolating extreme expressions."}, {"title": "2.2 AFFINE TRANSFORMATION OF 2D SURFELS", "content": "Our goal is to reconstruct geometrically accurate head avatars using the 2D surfel splatting regime, which employs a normal consistency energy (Huang et al., 2024). This approach implies that high-fidelity normals originate from high-fidelity depths. Previous methods only adopt rigid deformation to maintain the positive semi-definite (PSD) property of Gaussian. However, as shown in Fig. 2a, using only rigid deformation for rigging can result in over- or under-occupying phenomena and deformation distortion.\nLocal Geometry Descriptor. Let us consider a triangle in the canonical space as the matrix formed by its vertices $v_i \\in \\mathbb{R}^3$, and its edge matrix as $E = [v_1 - v_0, v_2 - v_0, v_3 - v_0] \\in \\mathbb{R}^{3\\times3}$, where $v_3 := v_1 + (1 - \\alpha)(v_2 - v_0)$. $\\tilde{E}$ is the deformed version with the deformed triangle's vertices $\\tilde{v_i}$ following Eq. 1. Note that in GaussianAvatars, the edge matrix is defined with a normalized base, height, and normal direction. This constructs an orthogonal matrix and approximates the shear or stretching-related deformations with isotropic scaling from the average of the base and height length. Please find a detailed derivation in GaussianAvatars (Qian et al., 2024).\nJacobian Deformation Gradient for Surface. The deformation gradient J is defined as $\\tilde{E}E^{-1}$ following (Sumner & Popovi\u0107, 2004). We introduce affine rigging with 2D surfels. The new pa- rameterization is $\\Sigma^{1/2} = J R_c S_c$, instead of $s_p R_p R_c S_c$, where $\\Sigma = \\Sigma^{1/2}(\\Sigma^{1/2})^T$. We note that the Jacobian deformation gradient J can correct inaccuracies in deformation, such as the lack of stretch or shear awareness, which can lead to incorrect surface reconstruction. It is also important to keep in mind that the Gaussian primitives retain their physical meaning only when the positive semi-definite"}, {"title": "2.3 JACOBIAN BLEND SKINNING (JBS)", "content": "Only with the local affine transformations might introduce discontinuities among adjacent triangles. This subsection introduces an improved technique to alleviate such discontinuities: Jacobian Blend Skinning. Specifically, we blend the local deformation with adjacent triangles' deformations as done in previous arts (Zielonka et al., 2023b; Shao et al., 2024).\nDegenerate Solution of Linear Blend Skinning in Matrix Space. Linear Blend Skinning (LBS) (Badler & Morris, 1982) is widely used for dynamic modeling of the human body and head (Li et al., 2017; Loper et al., 2023), but it struggles with the non-linearity of rotation matrix interpolation in SO(3), causing distortions in rotational transformations. Our toy experiments (Fig. 2b) show how element-wise linear interpolation distorts shapes. Previous works (Zielonka et al., 2023b; Shao et al., 2024) also adopt this sub-optimal approach to handle local deformations. Techniques like matrix-logarithm interpolation or quaternion's SLERP can address this but are limited to rotations.\nIntroducing Jacobian Blend Skinning (JBS). The Jacobian Blend Skinning (JBS) algorithm overcomes the limitations of LBS by focusing on the interpolation of Jacobian gradients, which encapsulate not only rotations but also shear and stretch, both residing in the broader GL(3) (General Linear) field. The key to JBS is leveraging Polar Decomposition (PD) (Shoemake & Duff, 1992) to break down transformations into two meaningful components: rotation (orthogonal matrix U) and stretch/shear (symmetric positive semi-definite matrix P). This decomposition is unique and coordinate-independent (proof in Appendix A.2.1), making it geometrically sound for interpolation. By blending the rotation in matrix-logarithm space and the stretch/shear in linear matrix space, JBS ensures that the resulting transformations remain geometrically valid, avoiding the distortions seen with LBS (Fig. 2b).\nJBS is applied to the Gaussian deformations. As aforementioned in Eq. 1, each Gaussian has its own parents' deformation parameters which correspond to the current expression and pose, such as isotropic scale $s_p$ and rotation $R_p$. These parameters compose the deformed square root covariance and position of Gaussian, $\\Sigma^{1/2} = s_p R_p R_c S_c$ and $\\mu = s_p R_p \\mu_c + T_p$, each. We replace the deformation of covariance term $s_p R_p$ to the blended Jacobian, $\\mathcal{J}$, which is computed by blending adjacent triangles' deformations.\nThe blending itself is performed separately for the rotational ($U_b$) and stretch/shear ($P_b$) compo- nents. For rotations, the blending weights are applied in the matrix-logarithm space $\\mathfrak{so}(3)$, followed by an exponential mapping back into the SO(3) space. For stretch/shear, the weights are applied directly in the linear matrix space, ensuring positive semi-definiteness, which avoids geometrical distortions. Mathematically, this Jacobian Blend Skinning is defined as:\n$\\mathcal{J}_b := JBS(\\mathcal{J}, w) = exp(\\sum_{i \\in adj.} w_i log(U_i)). \\sum_{i \\in adj.} w_i P_i$. (2)\n$\\qquad U_b \\qquad P_b$"}, {"title": "2.4 RESOLVING HOLLOW-ILLUSION IN EYEBALLS", "content": "Many studies (Park et al., 2021; Li et al., 2022; 2024) have witnessed that volumetric approaches often fail to produce a good approximation of the eyeball geometry and yield hollow eyes. We also have empirically found that the cornea of the human eye often appears as concave geometry without proper constriction. Since the cornea exhibits high specularity due to its multiple membranes (Dua et al., 2013), the limited representation capability Spherical Harmonics (SHs) can distort the geometry, unnaturally struggling to satisfy the photometric losses. This deceptive phenomenon is harmful to achieving accurate geometric representation as can be seen in the inset.\nTo address the issue of concave geometry around the eyeball, we eliminate the geometrical gradient on the eyeball-bound Gaussians, excluding Gaussians' cloning and splitting. We use the FLAME eyeball mesh as an approximation for the eyeball geometry. Additionally, we regularize the opacity of the eyes to approach 1, following the energy $\\mathcal{L}_{eye}$ in Sec. 3. Although this improves the geometry, SHs still fall short of capturing eye specularity.\nIn mitigation, we employ Anisotropic Spherical Gaussians (ASGs) (Xu et al., 2013). Since the cornea and sclera often reflect their light environment, ASGs excel at capturing sharp reflections. Implementation details of ASGs can be found in Appendix A.3. To preserve computational efficiency, we leverage common knowledge from casual data capture. First, eyeballs aren't affected by light from the back of the head, so we limit the sampling range to the frontal hemisphere of the world space, reducing ASGs by 50% compared to previous methods (Han & Xiang, 2023). Second, given that captured data is often in environments with ample white light, representing specular reflections as a monochrome intensity channel is sufficient."}, {"title": "3 TRAINING STRATEGY", "content": "We supervise the rendered images with photometric loss $\\mathcal{L}_{photo}$ which is a combination of L1 term $\\mathcal{L}_{1}$ and a D-SSIM term $\\mathcal{L}_{ssim}$ following 3DGS (Kerbl et al., 2023). Moreover, since we aim to reconstruct high-fidelity geometry, we followed the geometric energies in 2DGS (Huang et al., 2024), depth-distortion $\\mathcal{L}_{depth}$ and normal consistency energies $\\mathcal{L}_{normal}$. Toward the better alignment between Gaussians and their parent triangles, we utilize two regularization energy terms $\\mathcal{L}_{scaling}$ and $\\mathcal{L}_{position}$ from GA (Qian et al., 2024).\nEyeball Regularization. We have empirically found that the highly specular eyeball region, especially the cornea, tends to yield overly transparent Gaussians to satisfy the photometric losses. This leads to an incorrectly shaped, concave cornea, although the genuine surface should be roughly convex (Li et al., 2022). To alleviate this issue, we constrain the cornea regions to be opaque by regularizing the opacity of the respective Gaussians: $\\mathcal{L}_{eye} = \\sum_{i \\in \\mathcal{E}}(1 - \\alpha)^2$, where the $\\mathcal{E}$ de- notes the set of Gaussian in the eyeball region. Since we utilize the 3DMFM parametric mesh, this"}, {"title": "3.2 ADAPTIVE DENSITY CONTROL", "content": "We enable adaptive density control with binding inheritance (Qian et al., 2024). Moreover, to avoid degenerate transparent solutions of eyeballs, we stop the gradient of eyeball-bound Gaussians' rotation $R_e$ and position $\\mu_e$ by blocking the proliferation, initialized identity and zero. We describe the proliferation of Gaussians, so-called Adaptive Density Control (ADC) (Kerbl et al., 2023) below.\nOcclusion Gradient Amplification. The original adaptive density control (ADC) strategy (Kerbl et al., 2023) relies on view-space gradients. Occluded regions such as the lower tooth tend to suffer from less frequent updates. To remedy this, we amplify the view-space gradients of the tooth by 20\u00d7. This amplifier is a hyper-parameter that depends on the extent of the occluded parts shown."}, {"title": "4 EXPERIMENTS", "content": "As shown in Fig. 3, the input to our pipeline is a tracked RGB video. For head tracking, we adopt the same preprocessing approach used in GaussianAvatars (Qian et al., 2024) (GA) for the multiview RGB video dataset, NeRSemble (Kirschstein et al., 2023). Furthermore, since the strength of our method lies in accurately reconstructing the geometry of dynamic head avatars, we validate our approach using a synthetic dataset FaceTalk (Zheng et al., 2022) that includes ground truth normal."}, {"title": "4.1 EVALUATION PROTOCOL", "content": "For evaluation, we train our model with five baselines: IMAvatar (Zheng et al., 2022), FLARE (Bharadwaj et al., 2023), PointAvatar (Zheng et al., 2023), SplattingAvatar (Shao et al., 2024), and GaussianAvatars (Qian et al., 2024). IMAvatar employs a neural occupancy field, FLARE uses mesh-based avatars with intrinsic material decomposition, and PointAvatar adopts a point-based explicit model. GaussianAvatars and SplattingAvatars, utilizing mesh-binding inheritance, represent 3D Gaussian splatting. We exclude 3D Gaussian splatting models for the synthetic dataset due to the lack of surface normals and omit IMAvatar for the real dataset due to training instability."}, {"title": "4.1.2 DATASETS", "content": "Monocular Synthetic Dataset (FaceTalk (Zheng et al., 2022)). FaceTalk, rendered from the FLAME (Li et al., 2017) model, includes diverse expressions and poses at a resolution of 512 \u00d7 512. With ground truth normals available, we assess geometry fidelity. For experiments, we selected five identities, using 49 sequences for training and 3 with extreme poses for testing.\nMulti-view Real Dataset (NeRSemble (Kirschstein et al., 2023)). NeRSemble, a real human head dataset with 16 cameras, follows the protocol of GaussianAvatars, using 11 video sequences (four emotions, six expressions, and one free performance). One sequence is reserved for testing, and the free performance sequence is used for cross-identity reenactment. To assess generalization for extreme poses, we manually constructed a new held-out set (detailed in Appendix A.3). LPIPS (Zhang et al., 2018) is excluded for efficiency but included in Tab. 2 only for fair comparison."}, {"title": "4.1.3 METRICS AND TASKS", "content": "To evaluate self-reenactment and dynamic novel-view synthesis, we use PSNR, SSIM, perceptual LPIPS (Zhang et al., 2018), and normal cosine similarity (NCS). As the FaceTalk dataset (Zheng et al., 2022) lacks multi-view data, we only evaluate NVS on the NeRSemble dataset (Kirschstein et al., 2023). For NCS on NeRSemble, we use pseudo-ground truth normals from the Sapiens model (Khirodkar et al., 2024). Alongside quantitative evaluations, we report qualitative cross-reenactment results in Fig. 5. Each table highlights Best and Second best scores."}, {"title": "4.2 HEAD AVATAR RECONSTRUCTION AND REENACTMENT", "content": "Synthetic Dataset. Fig. 4 presents qualitative comparisons on FaceTalk synthetic dataset. IMAvatar (Zheng et al., 2022) demonstrates plausible rendering and geometry, but it often misses geometrical details with over- smoothing, particularly in the ears and nasal line, and displays concave artifacts in the pupil region. Furthermore, this method is quite slow, as training requires numerical searches to locate the surface, making it approximately 200x slower than PointAvatar (Zheng et al., 2023). While PointAvatar offers faster perfor- mance, it suffers from dotted noise in geome- try due to its fixed, isotropic point size, mak- ing it challenging to represent extreme poses. FLARE (Bharadwaj et al., 2023) also shows mangled rendering and geometry, particularly in cases of extreme facial expressions, as it involves decomposing intrinsic material from a single environment, an ill-posed problem. This trend is reflected in the quantitatives in Tab. 1.\nReal Dataset. For the NeRSemble real dataset, Fig. 5 shows qualitative comparisons. FLARE (Bharadwaj et al., 2023)'s mesh involving unstable remeshing technique leads both over-smoothed rendering and normal with inferior quality. PointAvatar (Zheng et al., 2023) excels in fine detailed normals such as wrinkles, owing to their drastic pruning strategy to avoid the ambiguity of normal volumetric rendering, but they show salt-and-pepper artifacts from inflexible point represen- tation. We also regard these two baselines' degradations stem from their inherent representations intractability of their personalized blendshape space. SplattingAvatar (SA) (Shao et al., 2024) and GaussianAvatars (GA) (Qian et al., 2024), which utilize 3DGS (Kerbl et al., 2023), aim for high- quality renderings. Note that the normal from SA and GA is derived from the shortest axis of the Gaussians, a method commonly used in recent relighting research (Jiang et al., 2024b). SA lacks explicit regularization of Gaussian positions, allowing them to drift far from their parent triangles, leading to inferior normal quality and floating artifacts. In both quantitative (Tab. 2) and qualitative evaluations, GA produces results comparable to ours, but its coarse deformation strategy, similarity transformation, does not account for triangle stretching and deformation discontinuity, resulting in semi-transparent and blob-like artifacts during extreme pose extrapolation. Additionally, SA and GA's normal quality suffer due to the absence of geometry-level optimizations. Otherwise, thanks to our design of capturing high-fidelity geometry and accurate deformations, ours show superior qual- ity of normal with detailed geometry. Not only the geometry, ours outperforms other state-of-the-art methods in terms of rendering quality. The qualitative observation is also proved in Tab. 2. Notably, GA shows better PSNR and LPIPS than ours, owing to 3DGS (Kerbl et al., 2023)'s representations. This phenomenon in rendering quality gap is also reported in 2DGS (Huang et al., 2024) in view of the absence of representation dimensionality. Although this numerical scores, as can be seen in red boxes Fig. 5, ours result is more robust than GA on extreme expression scenarios. However, ours with LPIPS achieves the best LPIPS with preserving other metrics. Ours scores best in the NCS which indicate its high fidelity for dynamic geometry reconstruction with a large margin. In summary, ours outperform other baselines in terms of reconstruction capability with appearance and geometry, both."}, {"title": "4.3 ABLATION STUDY", "content": "Jacobian and JBS help intricate rigging.\nWe further validate the effectiveness of our method's Jacobian and Jacobian Blend Skinning (JBS) in Tab. 3 and Fig. 6a. Here, \"Vanilla\" refers to the combination of 2DGS (Huang et al., 2024) and Gaussian Avatars (Qian et al., 2024), with eyeball convexity regularization and ASGs as described in Sec. 2.4.\nWhen incorporating the Jacobian deformation gradient, we observe a noticeable improvement in normal quality, particularly in reducing ar- tifacts near the jaw and nasal lines, resulting in more coherent normal representations."}, {"title": "5 CONCLUSION AND DISCUSSION", "content": "SurFhead introduces a novel method for reconstructing dynamic head avatars that strikes a balance between photorealism and geometrically accurate rigging. By integrating Jacobian deformation with detailed normal adjustments and Jacobian Blend Skinning (JBS), our approach enables precise control over both appearance and geometry. As a result, SurFhead surpasses existing state-of-the-art dynamic head models, excelling in accurate geometry reconstruction, pose and expression extrapola- tion, and demonstrating its applicability in areas such as relighting (see Appendix A.5) and dynamic mesh reconstruction by leveraging the efficient, rule-governed rigging regime of 3DMFM meshes.\nHowever, there is still room for improvement in the near future. First, similar to other 3DMFM and Gaussian-based methods, certain challenges persist, particularly regarding the bounded representation of the expression space in 3DMFM. Additionally, expressions that are grossly exaggerated and fall outside the span of 3DMFM's expression space, even cannot be head tracked in the prepro- cessing stage. Furthermore, elements like the tongue and individual hair strands are still missing in modern 3DMFMs. Besides, one of the strengths of our rule-governed deformation from 3DMFM meshes is efficiency. Therefore, we also discuss the improvement room for computational effi- ciency of polar decomposition, which, unlike other Gaussian primitive-based methods relying on black-box learning, is deterministically governed in SurFhead. These further discussions on these limitations are in Appendix A.5."}, {"title": "A.1 RELATED WORK", "content": "Static to Dynamic Radiance Field Reconstruction. NeRF (Mildenhall et al., 2021) and NV (Lombardi et al., 2019) have opened the era of photorealistic renderings with undertaking novel-view synthesis task. Optimization and rendering efficiency could be improved by hash encoding (M\u00fcller et al., 2022) and tensor decomposition (Chen et al., 2022). Later, Mixture of Volumetric Prim- itives (Lombardi et al., 2021) proposes efficient methodology with surface-aligned cuboid prim- itives which boost only ray marching around surfaces. Few years later, 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) utilizes anisotropic 3D Gaussians by rasterizing them without expensive ray-marching like previous strategies. We leverage the 3DGS, benefiting from the expressiveness, and efficiency.\nReconstructing and Animating Head Avatars. Existing approaches for reconstructing and animat- ing avatars mainly differ in two fundamental aspects: implicit or explicit models. Implicit models reconstruct the face by neural radiance field in combination with volumetric rendering or using im- plicit surface functions. (Gao et al., 2022), (Zielonka et al., 2023b), (Zheng et al., 2022), (Xu et al., 2023) With explicit models (Grassal et al., 2022), (Zheng et al., 2023), (Khakhulin et al., 2022), the seminal work of 3DMFM uses principal component analysis (PCA) to model facial appearance and geometry on a low-dimensional linear subspace. 3DMFM and its variants have been widely applied in optimization-based and deep learning-based head avatar creation. Recently, due to its efficient rendering and topological flexibility, there are many works utilizing 3D Gaussian Splatting (Kerbl et al., 2023). These works can be further categorized based on how they define and use Gaussians. Some approaches bind Gaussians directly to the mesh (Shao et al., 2024; Qian et al., 2024), while others map them onto UV coordinates (Xiang et al., 2024). In some cases, Gaussians are extracted as features (Giebenhain et al., 2024), (Xu et al., 2024), or a neural parametric model replaces the traditional 3DMFM (Giebenhain et al., 2024). However, in most cases, the deformation is often handled by the mesh itself with little additional consideration."}, {"title": "A.2 PROOFS", "content": ""}, {"title": "A.2.1 POLAR DECOMPOSITION'S UNIQUENESS", "content": "Lemma 1. There exists a unique polar decomposition of an arbitrary invertible matrix A such that\n$A = UP$,\nwhere U is an orthogonal matrix and P is a positive semidefinite symmetric matrix.\nProof (Lemma 1). Suppose that there are two polar decompositions of the matrix A:\n$A = U_1P_1 = U_2P_2$,\nwhere $U_1, U_2$ are orthogonal matrices and $P_1, P_2$ are positive semidefinite symmetric matrices. Now, multiply both sides of $U_1P_1 = U_2P_2$ by $U_1^T$ on the left:\n$P_1 = U_1^T U_2 P_2$.\nLet $Q = U_1^T U_2$, so that\n$P_1 = QP_2$.\nSince Q is an orthogonal matrix and both $P_1$ and $P_2$ are positive semidefinite symmetric matrices, we conclude that Q = I, the identity matrix. Hence,\n$U_1^T U_2 = I \\qquad U_1 = U_2$.\nThus, $P_1 = P_2$, and therefore the polar decomposition $A = UP$ is unique."}, {"title": "A.2.2 PRESERVATION OF POSITIVE SEMIDEFINITE WITH JACOBIAN GRADIENTS", "content": "Lemma 2. Consider an arbitrary matrix M. The matrix multiplication $MM^T$ is positive semidefinite (PSD) because for any vector x,\n$x^T MM^T x > 0 \\Leftrightarrow ||M^T x||^2 \\geq 0$.\nThe matrix $\\Sigma$ also follows this form. Therefore, $\\Sigma$ retains its positive semidefinite property after the affine transformation."}, {"title": "A.3 IMPLEMENTATION DETAILS", "content": "Optimization Specifications. We use Adam (Kingma & Ba, 2014) optimizer for learnable Gaussian parameters and translation, joint rotations, and expression parameters of FLAME (Li et al., 2017). We set the learning rates for all parameters same as with GA (Qian et al., 2024), except for the blending weights w. For the w, we set the learning rate as 1e-3. We train for 300,000 iterations, and exponentially decay the learning rate for the $\\mu$ until the final iteration, where it reaches 0.01\u00d7 the initial.\nFinally, the entire energies are defined as:\n$\\mathcal{L} = \\mathcal{L}_{photo} + \\lambda_{depth}\\mathcal{L}_{depth} + \\lambda_{normal}\\mathcal{L}_{normal} + \\lambda_{eye} \\mathcal{L}_{eye}$. (5)\nThe energy balances are $\\lambda_{depth}$ = 100, $\\lambda_{normal}$ = 0.05, and $\\lambda_{eye}$ = 0.1.\nCalculation Jacobian with Covariance. Since there is no support for precomputation of covariance with original 2DGS (Huang et al., 2024) rasterizer implementation we calculate the Jacobian deformation gradient J combining with 3DGS's original covariance as NVIDIA CUDA kernel implementation.\nColor Change with Geometrical Transformations.\nMost previous dynamic head avatars almost neglect the color change with deformations (Zielonka et al., 2023b; Qian et al., 2024; Shao et al., 2024; Lombardi et al., 2021). When the bases of SHs do not rotate, they just query the same direction with facing identical side of Gaussians in any deformed space. This phenomenon is illustrated in the inset of the upper right branch. To mitigate this, we suggest a simple solution like the lower right of the in- set, we inversely rotate the view direction d with the inverse of Gaussian's blended rotation part $U_b$. Namely, we rewrite the rotated input view direc- tion: $d_{rot} = U_b^T d$.\nAnisotropic Spherical Gaussians for Eyeballs' specularity. Comparing with Spherical Gaussians (SGs), ASGs (Xu et al., 2013) have been demonstrated to effectively represent anisotropic scene with a relatively small number.\nASGs are theoretically defined as:\n$ASG(v|[x, y, z], [\\lambda, \\mu], \\xi) = \\xi \\cdot max(v \\cdot z, 0) \\cdot e^{-\\lambda (v \\cdot x)^2 - \\mu (v \\cdot y)^2}$, (6)\nwhere z is lobe-axis, x and y are each tangent and bi-tangent of z, {\u03bb,\u03bc} \u2208 \\mathbb{R}+ are sharpness parameters, v is the unit direction function input, and the $max$ term implies the smooth term.\nGiven that most specular BRDFs feature a lobe aligned with a specific reflection direction, we cal- culate the reflection vector to serve as the input direction: $\\omega_o = 2(d_{rot} \\cdot n)n - d_{rot}$, where $d_{rot}$ is the rotated input view direction, n is a normal direction computed for each Gaussian, and wo is the reflect direction. Lastly, since a simple summation of ASG output limits its representative ability, we utilize tiny two-hidden layered MLP F to obtain the final specular color, following (Yang et al., 2024):\n$\\mathbf{C}_s(\\omega_o; \\mathcal{F}) = \\mathcal{F}\\left(\\left\\{ASG\\left(\\omega_o | [\\omega_x^i, \\omega_y^i, \\omega_z^i], [\\lambda_i, \\mu_i], \\xi_i\\right), \\gamma(d_{rot}), n \\cdot d_{rot}\\right\\}\\right)$, (7)\nwhere indicates the concatenation operation, $\\gamma$ denotes the positional encoding, and N is the number of basis SGs."}, {"title": "A.4 ADDITIONAL ABLATION STUDY", "content": "Adaptive Density Control (ADC) Amplification on Teeth. We posted qualitatives with the oc- clusion amplification on the teeth part in Fig. 8. This technique is very simple yet efficient with boosting Gaussian proliferation to overlooked parts in terms of optimization."}]}