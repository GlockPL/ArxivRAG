{"title": "NotaGen: Advancing Musicality in Symbolic Music Generation with\nLarge Language Model Training Paradigms", "authors": ["Yashan Wang", "Shangda Wu", "Jianhuai Hu", "Xingjian Du", "Yueqi Peng", "Yongxin Huang", "Shuai Fan", "Xiaobing Li", "Feng Yu", "Maosong Sun"], "abstract": "We introduce NotaGen, a symbolic music genera-\ntion model aiming to explore the potential of pro-\nducing high-quality classical sheet music. Inspired\nby the success of Large Language Models (LLMs),\nNotaGen adopts pre-training, fine-tuning, and rein-\nforcement learning paradigms (henceforth referred\nto as the LLM training paradigms). It is pre-\ntrained on 1.6M pieces of music, and then fine-\ntuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-\ninstrumentation\" prompts. For reinforcement\nlearning, we propose the CLaMP-DPO method,\nwhich further enhances generation quality and con-\ntrollability without requiring human annotations or\npredefined rewards. Our experiments demonstrate\nthe efficacy of CLaMP-DPO in symbolic music\ngeneration models with different architectures and\nencoding schemes. Furthermore, subjective A/B\ntests show that NotaGen outperforms baseline mod-\nels against human compositions, greatly advancing\nmusical aesthetics in symbolic music generation.", "sections": [{"title": "Introduction", "content": "The pursuit of musicality is a core objective in music gener-\nation research, as it fundamentally shapes how we perceive\nand experience musical compositions. Symbolic music ab-\nstracts music into discrete symbols such as notes and beats,\nwith MIDI and sheet music being the two predominant for-\nmats. These formats can efficiently model melody, harmony,\ninstrumentation, etc., all of which are crucial for musicality.\nTraining tokenized representations with language model\narchitectures, such as Transformers [Vaswani et al., 2017],\nhas emerged as a powerful paradigm for symbolic music gen-\neration [Huang et al., 2018; Casini and Sturm, 2022]. How-\never, several challenges persist. First, the scarcity of high-\nquality music data [Hentschel et al., 2023] hinders the abil-\nity of deep learning models to generate sophisticated com-"}, {"title": "Related Works", "content": ""}, {"title": "Sheet Music Generation", "content": "Sheet music generation has been widely studied, with a fo-\ncus on encoding methods and composition modeling. Score\nTransformer [Suzuki, 2021] introduces a tokenized represen-\ntation for sheet music and applies it to piano music gener-\nation. Measure by Measure [Yan and Duan, 2024] models\nsheet music as grids of part-wise bars and employs hierar-\nchical architectures for generation. Compared to the intricate\nrepresentations used by the models above, ABC notation, a\ncomprehensive text-based sheet music representation, simpli-\nfies encoding and facilitates composition modeling, gaining\nincreasing adoption in recent research. The following models"}, {"title": "Pre-training in Symbolic Music Generation", "content": "The success of pre-training in NLP has inspired the ap-\nplication of this technique in symbolic music generation.\nLakhNES [Donahue et al., 2019] enhances chiptune music\ngeneration by pre-training on the Lakh MIDI Dataset [Raf-\nfel, 2016]. MuseBERT [Wang and Xia, 2021] adopts masked\nlanguage modeling [Devlin et al., 2019], while MelodyGLM\n[Wu et al., 2023d] implements auto-regressive blank infilling\n[Du et al., 2021] for generation. MelodyT5 [Wu et al., 2024a]\nleverages multi-task learning [Raffel et al., 2020]. These\nstudies highlight the effectiveness of pre-training in enhanc-\ning music generation performance."}, {"title": "Reinforcement Learning in Music Generation", "content": "Reinforcement learning has long been recognized as a\npromising approach for enhancing the musicality of music\ngeneration models. It has been successfully applied in RL\nTuner [Jaques et al., 2017] for melody generation, RL-Duet\n[Jiang et al., 2020] for online duet accompaniment, RL-\nChord [Ji et al., 2023] for melody harmonization, and [Guo et\nal., 2022] for multi-track music generation. However, these\nmethods either base their rewards on music theory, which lim-\nits flexibility, or tailor them to specific music styles, hindering\ntheir generalization to a broader range of music generation\ntasks. To tackle this problem, MusicRL [Cideron et al., 2024]\nadopts the RLHF method with extensive human feedback to\nalign the generated compositions with human preference."}, {"title": "NotaGen", "content": ""}, {"title": "Data Representation", "content": "ABC Notation sheets consist of two parts: the tune header,\nwhich contains metadata such as tempo, time signature,\nkey, and instrumentation; the tune body, where the musi-\ncal content for each voice is recorded. We adopt a modi-\nfied version-interleaved ABC notation [Wu et al., 2024b;\nQu et al., 2024]. In this format, different voices of the same\nbar are rearranged into a single line and differentiated using\nvoice indicators \u201c[V:]\u201d. This ensures alignment of duration\nand musical content across voices. Furthermore, we remove\nbars with full rests (containing only \u201cz\u201d or \u201cx\u201d notes), reduc-\ning the length to 80.7% on average, while increasing infor-\nmation density.\nWe employ stream-based training and inference methods to\nenable long musical piece generation. We annotate the cur-\nrent and countdown bar indices before each tune body line\nusing the label \u201c[r:]\u201d. During training, we randomly segment\nthe tune body and concatenated it with the tune header for\nlonger pieces; during inference, we enforce the generation to\nstart from scratch using the bar annotations. If the piece is\nincomplete within the current context length, we concatenate\nthe generated tune header with the second half of the tune\nbody and continue generating until the final bar."}, {"title": "Model Architecture", "content": "NotaGen utilizes bar-stream patching [Wang et al., 2024]\nand the Tunesformer architecture [Wu et al., 2023b]. Build-\ning upon bar patching [Wu et al., 2023c], bar-stream patch-\ning divides the tune header lines and bars into fixed-length\npaches (padded when necessary), striking a balance between\nmusicality of generation and computational efficiency among\nsheet music tokenization methods.\nNotaGen consists of two hierarchical GPT-2 decoders\n[Radford et al., 2019]: a patch-level decoder and a character-\nlevel decoder. Each patch is flattened by concatenating one-\nhot character vectors and then passed through a linear layer\nto obtain the patch embedding. The patch-level decoder cap-\ntures the temporal relationships among patches, and its final\nhidden states are passed to the character-level decoder, which\nauto-regressively predicts the characters of the next patch."}, {"title": "Training Paradigms", "content": ""}, {"title": "Pre-training", "content": "Pre-training enables NotaGen to capture fundamental musi-\ncal structures and patterns through next-token prediction on a\nlarge, diverse dataset spanning various genres and instrumen-\ntations.\nWe cleaned the data rigorously to ensure the quality: pieces\nwith fewer than 8 bars were excluded, and deduplication was\nperformed based on MD5 checksums and edit distances be-\ntween sheet texts. We also preprocessed the text annotations,\nretaining music-related content such as tempo and expres-\nsion hints, while removing irrelevant content like lyrics and\nbackground information. This resulted in a final pre-training\ndataset of 1.6M music sheets.\nAll music sheets were transposed to 15 keys (including F,\nGb, C, Cb) for data augmentation. During training, a ran-\ndomly selected key was used for each piece in every epoch."}, {"title": "Fine-tuning", "content": "NotaGen was fine-tuned on high-quality classical music sheet\ndata to further enhance musicality in generation. Spanning\nfrom the intricate contrapuntal orchestra suites of the Baroque\nperiod to the melodious and harmonically nuanced piano\npieces of the Romantic era, classical music encompasses a\ndiverse array of compositional styles and instrumentations,\nall characterized by exceptional musicality.\nThus, we curated a fine-tuning dataset comprising 8,948\nclassical music sheets, from DCML corpora [Neuwirth et\nal., 2018; Hentschel et al., 2021a; Hentschel et al., 2021b;\nHentschel et al., 2023], OpenScore String Quartet Corpus\nGotham et al., 2023], OpenScore Lieder Corpus [Gotham\nand Jonas, 2022], ATEPP [Zhang et al., 2022], KernScores\nSapp, 2005], and internal resources, as listed in Table 1.\nSheets with more than 16 staves were excluded due to gen-\neration complexity. Each work was assigned with three la-\nbels: period, composer and instrumentation. The data distri-\nbution is provided in supplementary materials, and the details\nof each label are explained as follows:"}, {"title": "Reinforcement Learning", "content": "To refine both the musicality and the prompt controllability\nof the fine-tuned NotaGen, we present CLaMP-DPO. This\nmethod builds upon the principles of Reinforcement Learn-\ning from AI Feedback (RLAIF) [Lee et al., 2024] and im-\nplements Direct Preference Optimization (DPO) [Rafailov et\nal., 2024]. In CLaMP-DPO, CLAMP 2 serves as the evaluator\nwithin the DPO framework, distinguishing between chosen\nand rejected musical outputs to optimize NotaGen.\nCLaMP 2 is a multimodal symbolic music information re-\ntrieval model supporting both ABC notation and MIDI for-\nmats. Leveraging contrastive learning, CLAMP 2 extracts\nsemantic features that encapsulate global musical proper-\nties. These features encompass comprehensive musical infor-\nmation, including style, instrumentation, and compositional\ncomplexity. Meanwhile, they are consistent with human sub-\njective perceptions, as validated by [Retkowski et al., 2024].\nIn the context of music generation, the objective is to produce\npieces which closely resemble the ground truth. Accordingly,\nit is critical to ensure the alignment of the semantic features\nbetween the generated pieces and authentic references.\nWe introduce the CLaMP 2 Score to quantify the similar-\nity among pieces. To elaborate, we denote Pas the set of\nprompts for NotaGen. For each prompt $p \\in P$, $Y_p$ represents\nthe corresponding set of ground truth with an average seman-\ntic feature $\\overline{Z_p}$. Similarly, each prompt p has a generated set\n$X_p$, where each piece $x_p$ is associated with a semantic fea-\nture $Z_{xp}$."}, {"title": "", "content": "The CLaMP 2 score $c$ for a generated piece $x_p$ is defined\nas the cosine similarity between $Z_{xp}$ and $\\overline{Z_p}$:\n$$c_{x_p} = \\frac{Z_{x_p}\\overline{Z_p}}{||Z_{x_p}|| \\|\\overline{Z_p}\\|}$$\nOur goal is to maximize the average, $c_{x_p}$ over $X_p$, thereby\nensuring the music generated for prompt p aligns semanti-\ncally with the ground truth. It is achieved by employing the\nDPO algorithm to improve $c_{x_p}$.\nThe DPO algorithm optimizes a language model based on\npreference data, which consists of paired chosen and rejected\nexamples under the same prompts. It eliminates the need of\nexplicit reward modeling. In the proposed CLAMP-DPO al-\ngorithm, the fine-tuned model first generates data across the\nprompt set $P$. For each generated set $X_p$, the pieces $x_p \\in X_p$\nare sorted according to $c_{x_p}$, with the top 10% selected as cho-\nsen set $X_p^w$ and the bottom 10% as rejected set $X_p^l$. Ad-\nditional criteria, such as syntax error checks or the exclusion\nof ground-truth plagiarism, can be applied to refine these sets.\nFinally, the chosen and rejected pairs ($X_p^w, X_p^l$) are randomly\nselected and combined into preference data for optimization.\nGiven a prompt p, an auto-regressive language model pre-\ndicts the next token based on its policy $\\pi_\\theta$, where $\\theta$ represents\nthe model parameters. The probability of generating a chosen\ndata $x_p^w$ is $\\pi_\\theta(x_p^w|p)$, and that of generating a rejected data\n$x_p^l$ is $\\pi_\\theta(x_p^l|p)$. To prevent excessive drift from the initial\nmodel that generates the preference data and ensure diversity\nin the generated content, the initial model policy, i.e., the ref-\nerence model policy $\\pi_{ref}$ is introduced and kept frozen during\noptimization. The objective function to be minimized in DPO\nis given by:\n$$L_{DPO}(\\pi_\\theta;\\pi_{ref}) = -E_{(p,x_p^w,x_p^l)~D} \\bigg[log \\sigma \\bigg( \\beta log \\frac{\\pi_\\theta(x_p^w|p)}{\\pi_{ref}(x_p^w|p)}\n- \\beta log \\frac{\\pi_\\theta(x_p^l|p)}{\\pi_{ref}(x_p^l|p)} \\bigg) \\bigg],$$\nwhere $\\sigma$ is the sigmoid function, D is the preference dataset,\nand $\\beta$ is the hyperparameter that controls the deviation be-\ntween $\\pi_\\theta$ and $\\pi_{ref}$.\nThe optimization process increases the relative log prob-\nability of chosen data over rejected data. However, we ob-\nserved a decrease in $\\pi_\\theta(x_p^w|p)$, leading to degraded outputs.\nTo mitigate this issue, we adopt the DPO-Positive (DPOP)\nobjective function [Pal et al., 2024], which incorporates a\npenalty term to stabilize $\\pi_\\theta(x_p^w|p)$:\n$$L_{DPOP}(\\pi_\\theta;\\pi_{ref}) = -E_{(p,x_p^w,x_p^l)~D} \\bigg[log \\sigma \\bigg( \\beta log \\frac{\\pi_\\theta(x_p^w|p)}{\\pi_{ref}(x_p^w|p)}\n- \\beta log \\frac{\\pi_\\theta(x_p^l|p)}{\\pi_{ref}(x_p^l|p)} \\bigg) - \\beta \\lambda \\cdot max \\bigg(0,log \\frac{\\pi_\\theta(x_p^w|p)}{\\pi_{ref}(x_p^w|p)} \\bigg) \\bigg],$$\nwhere the hyperparameter $\\lambda$ controls the impact of penalty."}, {"title": "Experiments", "content": ""}, {"title": "Settings", "content": "The experiments are divided into two parts. The first part\nassesses CLaMP-DPO's ability to improve the controllabil-\nity and musicality of symbolic music models. The second\npart compares the musicality of NotaGen with baseline mod-\nels. Along with the pre-trained NotaGen, we selected two\npre-trained symbolic music generation models as baselines:\nMuPT [Qu et al., 2024] and Music Event Transformer (MET)\n[SkyTNT, 2024]. All models adopt language model archi-"}, {"title": "Ablation Studies on CLaMP-DPO", "content": "This experiment evaluates the impact of the proposed\nCLaMP-DPO algorithm in enhancing the controllability and\nmusicality of generated music for NotaGen, MuPT, and MET.\nIn the objective assessment, we selected several metrics for\nboth the fine-tuned models (denoted as K = 0) and the mod-\nels after K iterations of CLaMP-DPO optimization. We also\nassessed a subset of these metrics on the fine-tuning datasets\n(sheet-GT and MIDI-GT) for reference. The metrics are as\nfollows:"}, {"title": "Comparative Evaluations", "content": "This experiment compares the musicality of three models af-\nter the LLM training paradigm. For baseline comparison,\nwe constructed the reference set using human-authored musi-\ncal pieces from the fine-tuning dataset, which represent pro-\nfessional compositional standards. The subjective A/B tests\nwere organized into three groups, each containing the gener-\nated results of a model and the ground truth. For comparison\ninvolving MET, all data were converted to MIDI to eliminate\nformat-based bias."}, {"title": "Limitations and Challenges", "content": "While NotaGen shows promising advancements in symbolic\nmusic generation, limitations and challenges still warrant dis-\ncussion.\nWe once introduced a post-training stage between pre-\ntraining and fine-tuning, refining the model with classical-\nstyle subset of the pre-training dataset. While it accelerated\nthe fine-tuning convergence and improved ACS for NotaGen,\nthe impact was less pronounced on MuPT and MET."}, {"title": "Conclusions", "content": "In this work, we present NotaGen, a symbolic music gen-\neration model designed to advance the musicality of gener-\nated outputs through a comprehensive LLM-inspired training\nparadigm. By integrating pre-training, fine-tuning, and re-\ninforcement learning with the proposed CLAMP-DPO algo-\nrithm, NotaGen demonstrates superior performance in gener-\nating compositions that align with both the music style spec-\nified by prompts and human-perceived musicality. Our ex-\nperiments validate two key findings: (1) CLAMP-DPO effi-\ncaciously enhances controllability and musicality across di-\nverse symbolic music models, regardless of their modality,\narchitectures, or encoding schemes, without requiring human\nannotations or predefined rewards; (2) NotaGen outperforms\nbaseline models in subjective evaluations, achieving the high-\nest voting rate against human-composed ground truth.\nNotaGen establishes the viability of adapting LLM train-\ning paradigms to symbolic music generation, while address-\ning domain-specific challenges, including data scarcity and\ndemand for high-quality music outputs. Future work could\nextend this framework with CLaMP-DPO to broader musical\ngenres such as jazz, pop, and ethnic music; as well as explor-\ning its compatibility with emerging music generation models."}, {"title": "MIDI Event Transformer", "content": "MIDI Event Transformer (MET) is a pre-trained music gen-\neration model. It encodes MIDI events into token sequences\nand utilizes hierarchical transformer decoders for generation."}, {"title": "MIDI Encoding", "content": "In the MIDI encoding process, each MIDI event is trans-\nformed into a token sequence si. This sequence begins with\nthe token representing the event type ei, followed by tokens\ncorresponding to the event parameters p, as shown below:\n$$s_i = \\{e_i, p_i^1, p_i^2, ..., p_i^n\\}$$\nThe types of events and their corresponding parameters\nare listed in Table 3. Among these, BOS (beginning-of\nsequence) and EOS (end-of-sequence) events are used to\nmark the start and end of a musical piece, respectively. Pa-\nrameters such as channel (16 values), pitch (128 values), ve-\nlocity (128 values), controller (128 values), program (128 val-\nues), controller value (128 values) all comply with the MIDI\nstandard. The details of additional parameters are as follows:"}, {"title": "Model Architecture", "content": "The MIDI Event Transformer is comprised of two hierarchi-\ncal decoders: an event-level decoder and a token-level de-\ncoder, both of which are based on the Llama architecture.\nInitially, each event token sequence is processed through a\ntoken embedding layer, where individual token embeddings\nare aggregated to produce a dense vector representation. The\nevent-level decoder focuses on modeling temporal dependen-\ncies across high-level events, thereby capturing their sequen-\ntial relationships. The output hidden states from the event-\nlevel decoder are subsequently passed into the token-level\ndecoder, which generates the detailed token sequence in an\nauto-regressive manner."}, {"title": "Distribution of the Fine-tuning Dataset", "content": "We utilized CLaMP 2 to extract the semantic features of all\npieces, and the distribution of each label group is shown in\nFigure 6 (for composers, only eight composers with most\npieces in the dataset are shown)."}, {"title": "Prompt Set in Experiments", "content": "The full list of prompt set P in the reinforcement learning\nstage of our experiments is listed in Table 4."}]}