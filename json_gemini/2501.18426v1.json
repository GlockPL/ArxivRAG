{"title": "Guaranteed confidence-band enclosures for PDE surrogates", "authors": ["Ander Gray", "Vignesh Gopakumar", "Sylvain Rousseau", "S\u00e9bastien Destercke"], "abstract": "We propose a method for obtaining statistically guaranteed confidence bands for functional machine learning techniques: surrogate models which map between function spaces, motivated by the need build reliable PDE emulators. The method constructs nested confidence sets on a low-dimensional representation (an SVD) of the surrogate model's prediction error, and then maps these sets to the prediction space using set-propagation techniques. The result are conformal-like coverage guaranteed prediction sets for functional surrogate models. We use zonotopes as basis of the set construction, due to their well studied set-propagation and verification properties. The method is model agnostic and can thus be applied to complex Sci-ML models, including Neural Operators, but also in simpler settings. We also elicit a technique to capture the truncation error of the SVD, ensuring the guarantees of the method.", "sections": [{"title": "Introduction", "content": "One struggles to find an engineering or scientific discipline which has remained untouched from the rapid progression in machine learning (ML) and artificial intelligence (AI). Scientific machine learning (Sci-ML), with tailored architectures for physics-based problems, have in particular driven major advancements. Neural Operators (NOs) (Li et al., 2020), neural networks which can learn between function spaces, have received attention due to their efficient surrogacy of partial differential equation (PDE) solvers (Azizzadenesheli et al., 2024), and have seen application in complex problems including weather modelling (Kurth et al., 2023) and plasma physics (Gopakumar et al., 2024).\nHowever, the wide application of AI methods continues despite widely-shared concerns, as adumbrated by Brundage et al. (2020), Dalrymple et al. (2024), and many others, about the reliability and soundness of these methods. This is an opinion that we share: that methods for AI reliability are underdeveloped in comparison to its progression and wide application.\nAlthough multiple methods exist for uncertainty analysis in AI, little R&D effort has gone into approaches which can provide quantitative safety guarantees on the predictions of AI systems. Bayesian machine learning methods, such as Bayesian neural networks, Gaussian processes, Monte Carlo drop-out, and deep ensembles, are powerful methods which can equip predictions with distributional uncertainties. However, they do not attempt to provide statistical guarantees. It has also recently been shown by Balch et al. (2019) that Bayesian posteriors (even exact solutions) can suffer from False Confidence, meaning that by their nature (additivity) they can produce unsafe results, including in practical/real-world risk calculations, as they demonstrate. Although the advancements in Bayesian machine learning methods has improved AI reliability, for the above reasons alternatives should perhaps be sought for safety-critical systems.\nProblem statement Presented with a pre-trained model $f: \\mathcal{X} \\rightarrow \\mathcal{F}$, which maps from Euclidean space $\\mathcal{X} \\in \\mathbb{R}^m$ to a space of functions $\\mathcal{F} \\in \\mathbb{F}$, and some additional (calibration) data unseen by the model $\\mathcal{Z} = (Z_1,..., Z_n)$ where $Z_i = (X_i, F_i)$, construct a prediction set $C \\subset \\mathbb{F}$ (a band of functions) guaranteed to enclose a next unseen observation $F_{n+1}$ with a user prescribed confidence level: $P(F_{n+1} \\in C^\\alpha) \\geq 1 - \\alpha$. The space $\\mathcal{F}$ has been discretised by a prior analyst $\\mathcal{F}_\\mathbb{l} = [F_\\mathbb{l}(y_1), ..., F_\\mathbb{l}(y_\\mathbb{l})] \\in \\mathbb{R}^\\mathbb{l}$, but rather finely $\\mathbb{l} \\gg 1$.\n1.1 Summary of the method\nWe briefly outline our strategy.\n1. Compute f's error with respect to the calibration data $e_i = F_i - f(X_i)$,\n2. Perform a dimension reduction (e.g. an SVD) of e, and project it to a lower dimensional space,"}, {"title": "Related work", "content": "We mention three distinct prior methods for producing guaranteed multivariate prediction sets, however we would not be surprised if there are other methods we have missed, since uncertainty quantification in Sci-ML is quite a timely problem.\nCopula-based conformal prediction : Messoudi et al. (2021) suggest a method to combine univariate prediction sets obtained from conformal prediction using copulas, powerful aggregation functions used to decompose multivariate distributions into their marginals and dependencies. This work was further extended by Sun and Yu (2023) to time series. We note however that using copulas to model dependence in high dimensions is challenging, and their proposition may require more advanced techniques, such as vine-copulas, to be applicable to functional surrogates.\nSupremum-based conformal prediction : a quite straightforward way to combine univariate conformal predictors is by taking the supremum over a collection of non-conformity scores. Diquigiovanni et al. (2022) take this idea forward by proposing that the scores of each dimension can be normalised or modulated by some function $\\sigma(t)$:\n$s(x,y) = \\underset{i\\in1,..., N}{sup} \\underset{t \\in T}{sup} \\Big|\\frac{Y_i(t) - \\mu(t)}{\\sigma(t)}\\Big|,$ \nin some prediction region T of interest, with $\\mu(t)$ being the estimated data mean. They suggest that\n$\\sigma(t) = \\sqrt{\\frac{\\Sigma_{i=1}^N (Y_i(t) - \\mu(t))^2}{N-1}}$\nis taken to be the estimated data standard deviation. Their method is quite easily adapted to surrogate modelling, by replacing the $\\mu(t)$ with a trained predictor $f(x_i)(t)$ in both of above expressions, with $\\sigma(t)$ now giving the standard deviation of f's prediction error. Although this method is simple to apply, we find it can at times give quite wide prediction sets.\nElliptical-set conformal prediction : Messoudi et al. (2022) propose a multi-target (multivariate) non-conformity score\n$s(x,y) = \\sqrt{(y - f(x))^T\\hat{\\Sigma}^{-1}(y - f(x))},$\nwhere $\\hat{\\Sigma}$ is sample covariance of the surrogate's prediction error. This non-conformity has a known analytical sublevel-set\n$C_\\alpha = {y \\in \\mathbb{R}^n | s(x, y) \\leq q(x)},$\nwhich is an ellipsoid centred at f(x), and eccentricity related to $\\hat{\\Sigma}$. They further show their method extends to 'normalised' conformal prediction, where the ellipsoid changes depending on input x."}, {"title": "Contributions", "content": "Our contribution is most similar to the last of the above methods, where we predict a multivariate set equipped with a guaranteed $\\alpha$-level frequentist performance. Our method diverges as we do not use a non-conformity scoring function, but construct the prediction sets directly on a processed calibration data set.\n*   A compact representation of probabilistic uncertainty based zonotopes,\n*   Methods to calibrate this representation from data, forming a probabilistic bound on the unknown training distribution,\n*   An application of this technique to functional surrogate models, giving guaranteed field enclosures as predictions,\n*   A method to account for the dimension reduction truncation error, as required by the framework."}, {"title": "Conformal prediction and consonant belief functions", "content": "In this section we briefly describe inductive conformal prediction, for calibrating guaranteed prediction sets, particularly used for ML models (but not exclusively, as this paper exemplifies), and the related idea of belief functions, which we find to be a useful theory for performing computation using the calibrated sets.\nInductive conformal prediction (ICP) is a computationally efficient version of conformal prediction (Papadopoulos et al., 2002) for computing a set of possible predictions $C^\\alpha : \\mathcal{X} \\rightarrow$ {subsets of $\\mathcal{Y}$} of an underlying machine learning model $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$. The prediction set is equipped with the following probabilistic inequality\n$P(Y_{n+1} \\in C^\\alpha(X_{n+1})) \\geq 1 - \\alpha.$\nThat is, the probability that the next unobserved prediction $Y_{n+1}$ is in computed set $C^\\alpha(X_{n+1})$ is bounded by $1 - \\alpha$, where $\\alpha \\in [0,1]$ is a user-defined error rate. Additionally, Equation 1 can be guaranteed if the data is exchangeable, i.e. the data used to build $C^\\alpha$ can be replaced with the future unobserved samples $Y_{n+1}$ without changing their underlying distributions.\nMost relevant for this paper is how $C^\\alpha$ is constructed, and how inequality 1 is guaranteed. In ICP, one compares the predictions of a pre-trained model f to a calibration dataset $\\mathcal{Z} = (Z_1, Z_2,..., Z_n)$ where $Z_i = (X_i, Y_i)$, which is yet unseen by f. A non-conformity scores: $\\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ is used to compare f's predictions to $Z_i$, with large values of s indicating a large disagreement between the prediction and the ground truth. A common score used in regression is $s(x,y) = |f(x) - y|$. When applied to the calibration data $a_i = s(X_i, Y_i)$ gives an ordering of $Z_i$ in terms of how well f performs. Moreover, the probability that another (exchangeable) sample $a_{n+1} = s(X_{n+1}, Y_{n+1})$ performs at least as well as previous $a_i$ scores is\n$\\mathbb{P}(\\frac{1}{n} \\underset{i=1}{\\overset{n}{\\sum}} I[a_{n+1} \\leq a_i] \\leq E \\leq \\alpha,$\nwhere I is the indicator function that $A_{n+1} \\leq a_i$. Therefore, taking a (conservative) quantile of the sorted scores $q^\\alpha = a_{[(1-\\alpha)n]}$, where $[x]$ denotes the ceiling function, and computing the set of f's possible values giving at least this score $C^\\alpha = {y \\in \\mathcal{Y} | s(x,y) \\leq q(x)}$ gives a set satisfying 1. Note that the prediction regions form a nested family of sets w.r.t $\\alpha$: $C^{\\alpha_1} \\subset C^{\\alpha_2}$ for $0 \\leq \\alpha_1 \\leq \\alpha_2 \\leq 1$.\nOur framework Computing the level-set of a complicated s(x,y) function may be challenging, so often we opt for quite simple non-conformity scores with known level-sets. A challenge which is exacerbated for multivariate problems, somewhat limiting application. We therefore take a different approach, and directly construct $C^\\alpha$ sets using a parametric nested family of sets which we fit to the calibration data (Gupta et al., 2022). This allows us to design prediction sets tailored to our particular data, and well-\nsuited for multivariate problems. Using an interpretation of $C^\\alpha$ as belief functions, we can perform additional computations (for example linear and non-linear transformations) on $C^\\alpha$, while still maintaining the guarantee 1. This framework however comes with its own challenges. In some sense we are doing the reverse of conformal prediction, where our challenge is not to compute the level-set $C^\\alpha = {y \\in \\mathcal{Y} | s(x, y) \\leq q(x)}$, rather we begin with $C^\\alpha$ and need to find the membership values $a_i = sup{\\alpha \\in [0,1] | Z_i \\in C^\\alpha}$ of the calibration data. Depending on what set-representation is used, computing the membership values can be a costly operation.\nBelief functions Cella and Martin (2022) make a useful connection between conformal prediction and belief functions (Shafer, 1976), a generalisation of the Bayesian theory of probability where one can make set-valued probabilistic statements, such as inequality 1 given by conformal prediction.\nBelief functions, also called random-sets or Dempster-Shafer structures, are set-valued random variables whose statistical properties (such their cdf, moments, sample realisations, and probability measure) are set-valued. Belief functions form a bound on a collection of partially unknown random variables, often used in robust risk analysis. In particular, the conformal nested prediction sets $C^\\alpha$ can be related a consonant belief function, and under this framework transformations $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ of the imprecise random variable are quite simply:\n$C^\\mathcal{Y} = f(C^\\mathcal{X}),$\nthat is, for each $\\alpha$-level, a single set-propagation of $C_x$ through f is required to determine $C_y$, maintaining the same $\\alpha$-confidence level. This is comparatively simpler than other representations, which we use to transform fitted $C_x$ through computations (SVDs in particular)."}, {"title": "Zonotopic confidence sets", "content": "We require a base set-representation to construct the prediction sets $C^\\alpha$. For this, we chose a class of convex sets known as zonotopes, for some favorable properties, such as being closed under linear transformations and under Minkowski sums. They generalise intervals, boxes, hyper-rectangles, and all their rotations, and may be represented on the computer in a compact manner. An n-dimensional zonotope is completely characterised by a vector in $\\mathbb{R}^n$ (centre), and a collection of p \u2208 N vectors in $\\mathbb{R}^n$ (generators).\nDefinition 1 (Zonotope) Given a centre $c_z \\in \\mathbb{R}^n$ and $p \\in \\mathbb{N}$ generator vectors in a generator matrix"}, {"title": "Fitting zonotopic confidence sets", "content": "In this section we discuss various methods to fit $Z^\\alpha_{p_z}$ to a dataset $X_i \\sim F_x$, that is to find a data-enclosing zonotope $X_i \\in Z$ for all samples, and an estimated point $p_z$ with a high data-depth.\nFitting a rotated hyperrectangle for Z A simple but effective method is to enclose $X_i$ is using rotated hyperrectangle, with generators $G_z$ along the principal components of the data.\nvectors. The singular values provide a natural ranking of the contribution of each eigenvector on the variance of X. An enclosing hyperrectangle can be easily found by computing the data min and max over each dimension of U. Upon converting this hyperrectangle to a zonotope representation, the zonotope $Z_u = (C_u, G_u)$ can be projected back to the original space by $Z_x = (V\\Sigma c_u, V\\Sigma G_u)$.\nThis yields a zonotope whose generators are aligned with the eigenvectors of the dataset. Although simple, quick to compute, and scalable, an obvious downside to this method is that it only yields zonotopes with generators as the same number of dimensions of X.\nOverapproximating a convex hull for Z A second method to fit Z is by first computing the convex hull $C_H$ of $X_i$, giving a bounding polytope of the data. An enclosing zonotope $Z \\supset C_H$ can then be computed. This yields a zonotope with half the number of generators as there are bounding half-spaces of $C_H$. We note that the overapproximation of a convex hull with a zonotope requires a conversion between the vertex representation (v-rep) of polytope to the half-space representation (h-rep), where the v-rep corresponds to a vector of extreme points, and the h-rep is a vector of bounding half-spaces. The conversion from v-rep to h-rep (and vice-versa) can be an expensive operation.\nAdditionally, computing $C_H$ can be expensive for higher dimensions or for large quantities of data. For these situations, we recommend the first method, which scales very favorably.\nEuclidean data-depth for $p_z$ A fast and simple, but potentially inaccurate, method is to take $p_z$ to be the data point $X_i$ with the greatest Euclidean depth w.r.t the sample mean $\\mu$\n$p_z = arg \\underset{X_i}{max} (1+d_e(X_i))^{-1}$\nwhere $d_e$ is the Euclidean distance of x to $\\mu$\n$d_e(x) = \\sqrt{(x - \\mu)^T(x - \\mu)}.$"}, {"title": "Calibrating zonotopic confidence sets", "content": "Calibration with a known distribution For mostly illustrative purposes, we begin by showing how $Z^\\alpha_{p_z}$ can be calibrated from a known multivariate distribution, whose cdf $F_x$ or density $f_x$ are available. This could perhaps be useful for some applications, but the next method (calibrating from sample data) is likely to find wider use. Given a zonotope Z, ideally containing the range of $f_x$ or otherwise some large probability region ($\\mathbb{P}_x(Z) \\approx 1$) if X is unbounded, and a $p_z$ ideally near the mode, the structure can be calibrated by integrating the density $f_x$ in the sets $Z^\\alpha_{p_z}$\n$s(\\alpha) = 1 - \\int_{Z^\\alpha_{p_z}} f_x(x)dx.$\nThe condition\n$\\mathbb{P}(X \\in Z(\\alpha)) \\geq 1-\\alpha$\nholds straightaway.\nCalibration from sample data Our method is similar to conformal prediction, as described in section 2, however without a trained regressor f and our data {$X_1, X_2, ..., X_n$} is multidimensional. We also have a parametric set family $Z^\\alpha_{p_z}$ in contrast to a non-conformity scoring function. However, the set-membership of the data $a_i = sup{\\alpha \\in [0,1] | X_i \\in Z^\\alpha_{p_z}}$ can be used to rank or score the data. Under the assumption of exchangeability, the probability of another $X_{n+1}$ having a membership score as extreme as the $a_i$'s is equation 2. This directly leads to the (conservative) quantile $s(\\epsilon) = a_{[(en)]}$ with sorted a producing a set with the property\n$\\mathbb{P}(X_{n+1} \\in Z(\\epsilon)) \\geq 1 - \\epsilon,$\nwhere $\\epsilon \\in [0, 1]$ is a user defined confidence value.\nWe note that computing the exact membership score $a_i = sup{\\alpha \\in [0,1] | X_i \\in Z^\\alpha_{p_z} }$ may be challenging, as the set must be varied with $\\alpha$ until $X_i \\notin Z^\\alpha_{p_z}$, which could be relatively well performed using mathematical optimisation for certain set representations. We however suggest a simple method, where $\\alpha$ uniformly discretised in a grid in A = {0,0.111, . . ., 0.999,1}, and computing\n$a_i = max{\\alpha \\in A | X_i \\in Z(\\alpha)}.$\nAlthough the exact supremum isn't found, this still gives conservative results, as two collection of scores $a_i \\leq a_j$ will yield a lower bound on probabilistic bound on 3. I.e. larger confidence set $Z^\\alpha_{a_i} \\subseteq Z^\\alpha_{a_j}$. We suggested that the discretisation of the $\\alpha$ values is as least a large as the data set, to avoid multiple repetitions of the same $a_i$ values as much as possible. Also note that like in conformal prediction, the maximum number of unique prediction sets that can be obtained is the number of calibration data points provided. Figure 6 gives three examples of calibrating $Z^\\alpha_{p_z}$ using this method with different data lengths.\nSome potential alternative frameworks exist for reliably calibrating $Z^\\alpha_{p_z}$, including inferential models"}, {"title": "Functional confidence-bands", "content": "Given a pre-trained model $f : \\mathcal{X} \\rightarrow \\mathcal{F}$, which maps from Euclidean space $\\mathcal{X} \\in \\mathbb{R}^m$ (space of PDE inputs) to a space of functions $\\mathcal{F} \\in \\mathbb{F}$ (PDE solutions), discretised on a fine grid $\\mathcal{F}_\\mathbb{l} = [F_\\mathbb{l}(y_1), ..., F_\\mathbb{l}(y_\\mathbb{l})] \\in \\mathbb{R}^\\mathbb{l}$, we can compute the error of f's error on some unseen data {($X_1, F_1$), ..., ($X_n, F_n$)} with $e_i = F_i - f(X_i)$. We may then reduce the dimension of $e_i$ using an SVD: $e = UC V^T$, where the singular values indicate the variance contribution of each eigenvector to the variance of $e_i$. We may truncate the dimensions of the data, capturing some high (often 99%) of the overall variance, and projecting the data $e_i$ onto these remaining dimensions, a low dimension representation of f's error: $U_i$. Depending on the size of these dimensions, one of the above fitting methods may be applied to find $Z_z$. We find the 'rotated hyperrectangle' method and the Mahalanobis depth scale well to high dimensions, with the convex hull method being tighter for smaller dimensions. $Z^\\alpha_{p_z}$ may then be calibrated, giving $\\mathbb{P}(U_{n+1} \\in Z^\\alpha) \\geq 1 - \\alpha$. Mapping $Z^\\alpha_{p_z}$ through the linear transformation back to $\\mathbb{R}^\\mathbb{l}$ is straightforward for the underlying zonotope, and for the probabilistic part $\\alpha$, propagate each zonotope level-set independently $R^\\alpha = f(Z^\\alpha_{p_z})$, i.e. perform one transformation for each confidence-level, with the $\\alpha$-guaranteed being retained by the set. Note we may perform this since we compute a multivariate confidence set, this would not be the case had each dimension been fit independently.\nThese nested sets can be added to the prediction of f, yielding the desired functional prediction set\n$C = f(X_{n+1}) + R^\\alpha.$\nHowever, the resulting confidence regions are not strictly a guaranteed bound on ($X_{n+1}, F_{n+1}$), since we have truncated some data variance during the fitting. Although these dimensions weakly effect the variance of $F_i$, we cannot strictly claim a guaranteed bound. We therefore propose a quite simple method to account for the uncertainty in these dimensions, without including them in the expensive set calibration."}, {"title": "Bounding truncation error", "content": "By taking a bounding box of $F_i$'s projection onto the truncated eigenvectors, one can obtain an estimate of the range of uncertainty in these dimensions, that is, the range of Z had we included these dimensions. We capture this error with a simple bounding hyperrectangle E found using the data minimum and maximum in each dimension. Upon taking the Cartesian product $R^\\alpha = Z^\\alpha_{p_z} \\times E$, one obtains a zonotope in high dimensions which contracts in the important directions as $\\alpha$ varies, but remains constant in the truncated dimensions. The $\\alpha$-guarantee remains unchanged due to this operation because: 1) the number of data points remains unchanged (only their dimension), and 2) the data is totally bounded (and remains so as $\\alpha$ changes) in the extra dimensions. Indeed, there is a slight performance-loss when performing this (our probabilistic bound becomes looser), however this is quite minor as these extra dimensions don't affect $F_i$ dramatically. The Cartesian product between two zonotopes $Z_x \\subset \\mathbb{R}^n$ and $Z_y \\subset \\mathbb{R}^m$ produces an additional zonotope $Z_{x \\times y} \\subset \\mathbb{R}^{n \\times m}$, whose centre and generator are a simple concatenation of the centres and generators of $Z_x$ and $Z_y$: $C_{x \\times y} = (C_x||C_y)$ and $G_{x \\times y} = (G_x||G_y)$."}, {"title": "PDE surrogate example", "content": "We use LazySets.jl from Forets and Schilling (2021) for the set construction, and NeuralOperators.jl for the base Sci-ML model. Otherwise, a supplementary code repository will be released for those wishing to reproduce this papers results.\nTo demonstrate the methodology, we construct functional confidence bands on a Fourier neural operator (FNO) for the Burger's equation, trained on data supplementary data provided from Li et al. (2020), which was built by sampling a numerical PDE solver."}, {"title": "Relationships to other ideas", "content": "It is fairly straightforward to extend the above to other set-representations, for example Taylor-models or polynomial zonotopes, with examples shown in Figure 8. However, a compact representation, like our proposition 1 is challenging for more complex set representations. The below sets were constructed by first constructing a $Z^\\alpha_{p_z} \\subseteq [-1,1]^p$, and mapping this through the corresponding Taylor polynomial. Although this allows us to compute level-sets (and therefore memberships 4), an efficient method to fit these sets to data would need to be proposed.\nThis more sophisticated set-representation could allow us to perform a more aggressive non-linear dimension reduction. A potentially interesting application of this work would be to probabilistically fit from data the set-representations used in dynamical system verification."}, {"title": "Relationship to conformal prediction", "content": "The proposed method is similar to conformal prediction, but where we directly solve for a valid prediction set, instead of taking a level-set of a non-conformity scoring function. Indeed, the membership function of $Z^\\alpha_{p_z}$ is the non-conformity score in conformal prediction. If one could find an efficient (perhaps analytical) expression for this membership function, in terms of $\\alpha$ and the underlying set-representation, then the calibration procedure would be made much more efficient (corresponding to a simple function evaluation), without the need to check the set-membership of $Z^\\alpha_{p_z}$ in 4, which is a costly part of the proposition. This would also additionally greatly simplify the implementation.\nWe therefore share many of the advantages and disadvantages of conformal prediction, including finite-sample guarantees and being model agnostic. Although we show example of regression problems, we believe our method could apply to multivariate classification problems."}, {"title": "Limitations", "content": "Our method requires the same assumption as conformal prediction, namely exchangeability, we therefore suffer similar limitations. The guarantee is lost if the dataset is not exchangeable (for example if it changes over time). We also give only marginal coverage (averaged for all samples of $X_i$), rather than the stronger conditional coverage (conditioned for particular x), which is more desirable for surrogate modelling. Finally, we mention that our method would greatly benefit for refined methods for fitting set-representations to data. Although the routines we proposed are simple, we do not doubt that more sophisticated methods"}, {"title": "Further detail about proposition 1.", "content": "Here we provide additional detail about the proposed nested zonotope family $Z^\\alpha_{p_z}$, and a proof that the family is a nested.\nA.1 Intuition behind the nested family\nFigure 9 gives a visual description of how the nested sets $Z^\\alpha_{p_z}$ are constructed. Given a zonotope Z and a point $p_z \\in Z$, the centres of the parametric zonotopes move along the line defined by $c_z(1-\\alpha)+\\alpha p_z$, and additionally the generators are contracted by $(1 - \\alpha)$. Thus, the zonotopes are translated toward $p_z$ as $\\alpha$ increases, and their size reduces. Figure 9 shows an example of a parametric zonotope being constructed for $\\alpha = 0.5$. Note that all the sets in $Z^\\alpha_{p_z}$ have the same shape, and only differ by a translation and a contraction. This is partially because all generators are scaled by the same magnitude. Note: $c_z$ does not need to be inside all zonotopes, however the point $p_z$ is inside all zonotopes. Indeed, it is the only point that is in all zonotopes (given $Z^{\\alpha=1}_{p_z} = {p_z}$).\nand its generators scaled by 0.5.\nA.2 Proof of proposition 1.\nHere we provide a detailed proof of proposition 1. (nested zonotopic sets), that $Z^{\\alpha_1}_{p_z} \\subseteq Z^{\\alpha_2}_{p_z}$ for any $0 \\leq \\alpha_1 \\leq \\alpha_2 \\leq 1$. We remind you that $Z^\\alpha_{p_z}$ is defined as\n$Z^\\alpha_{p_z} = { x \\in \\mathbb{R} | x = c_z(1-\\alpha) + \\alpha p_z + \\sum_{i=1}^{p} \\xi_i g_i (1-\\alpha), \\xi_i \\in [-1, 1] },$\nfor some point $p_z \\in Z$ and $\\alpha \\in [0,1]$. The proof is based on the following rational:\n1. Take two sets L and K composed as the intersection of n sets $L = \\bigcap_{i=1}^n A_i$ and $K = \\bigcap_{i=1}^n B_i$, if each $B_i \\subseteq A_i$, then K \\subseteq L."}]}