{"title": "Aquila: A Hierarchically Aligned Visual-Language Model for Enhanced Remote Sensing Image Comprehension", "authors": ["Kaixuan Lu", "Ruiqian Zhang", "Xiao Huang", "Yuxing Xie"], "abstract": "Recently, large vision language models (VLMs) have made significant strides in visual-language capabilities through visual instruction tuning, showing great promise in the field of remote sensing image interpretation. However, existing remote sensing vision language models (RSVLMs) often fall short in capturing the complex characteristics of remote sensing scenes, as they typically rely on low-resolution, single-scale visual features, and simplistic methods to map visual features to language features. In this paper, we present Aquila, an advanced visual-language foundation model designed to enable richer visual feature representation and more precise visual-language feature alignment for remote sensing images. Our approach introduces a learnable Hierarchical Spatial Feature Integration (SFI) module that supports high-resolution image inputs (e.g., 1024\u00d71024) and aggregates multi-scale visual features, allowing for the detailed representation of complex visual information. Additionally, the SFI module is repeatedly integrated into the layers of the large language model (LLM) to achieve deep visual-language feature alignment, without compromising the model's performance in natural language processing tasks. These innovations\u2014capturing detailed visual effects through higher resolution and multi-scale input, and enhancing feature alignment\u2014significantly improve the model's ability to learn from image-text data. We validate the effectiveness of Aquila through extensive quantitative experiments and qualitative analyses, demonstrating its superior performance.", "sections": [{"title": "1. Instruction", "content": "Remote sensing technology, which captures Earth observation data from airborne or satellite platforms, has been widely applied in diverse domains, including agriculture, environmental monitoring, conservation, urban planning, and disaster response. With advancements in artificial intelligence (AI), remote sensing image analysis has become increasingly precise and automated. However, current models predominantly rely on visual inputs, such as those used in target detection, semantic segmentation, and change detection within remote sensing [1-3]. In recent years, the rapid development of large language models (LLMs) has led to significant advancements in natural language processing tasks, enabling human-like dialogue [4-6]. Building on the success of LLMs, vision language models (VLMs) [7-9] have extended the capabilities of traditional visual-only models by incorporating text inputs. These models align visual features with the representation space of language models through visual instruction tuning, facilitating end-to-end integration. VLMs excel in processing multimodal data, demonstrating superior capabilities to earlier models, and are gaining significant attention. Harnessing the potential of VLMs to bridge visual and linguistic data, while developing large-scale models for specific vertical domains, has opened new frontiers in remote sensing applications. This advancement represents a pivotal step in the AI-driven transformation of remote sensing technologies.\nIn recent years, several notable remote sensing VLMs (RSVLMs) frameworks have emerged within the field of remote sensing. To address the modality gap, some studies have focused on constructing image-text pairs embedded with domain-specific remote sensing knowledge [10-11]. These efforts aim to enhance model comprehension across various remote sensing scenarios, such as image captioning and visual question answering (VQA). Examples include datasets developed for instruction-tuning, such as"}, {"title": "2. Related work", "content": ""}, {"title": "2.1 Large vision language model", "content": "Building on the strong zero-shot and reasoning capabilities of LLMs [24-30], substantial advancements have been achieved in the development of Large VLMs within the field of computer science. Some VLM studies capitalize on the inherent zero-shot and few-shot learning abilities of pre-trained LLMs to interpret user intent and subsequently invoke external multimodal models. For instance, MiniGPT4 [31] enhances multimodal functionality by integrating a visual module with LLMs, while MM-React[32] introduces a protocol that enables LLMs to decide when to engage visual experts for multimodal tasks. Additionally, Visual ChatGPT[33] implements a prompt manager that instructs ChatGPT on which specialized tools to employ, facilitating the use of 22 different expert models. Lastly, GPT4Tools[34] generates an instruction-following dataset by prompting a high-level teacher model (ChatGPT4) with multimodal contexts, followed by fine-tuning the model using Low-Rank Adaptation (LoRA) [35] to enable expert model invocation.\nWhile these models offer versatile multimodal capabilities, their performance is fundamentally constrained by the limitations of both the LLMs and the external models on which they depend. Another branch of research seeks to improve visual and language processing by aligning the output features of visual encoders with the feature space of language models, thereby enabling end-to-end multimodal integration in VLMs. For instance, Flamingo [36] introduced the Perceiver Resampler, which processes a variable number of input images from the visual encoder and produces a fixed number of visual outputs, enhancing the representation of visual features. BLIP2 [37] employs a trainable Q-Former module to extract a fixed number of output features from the image encoder, effectively bridging the gap between the frozen image encoder and the frozen LLM."}, {"title": "2.2 Vision language model for remote sensing", "content": "The application of VLMs in remote sensing is gaining significant momentum, broadly classified into two main categories: contrastive-based VLMs and generative-based VLMs. Contrastive-based VLMs are designed to align representations of similar (positive) image-text pairs while differentiating those of dissimilar (negative) pairs. Notable examples include RemoteCLIP [43] and GeoRSCLIP [44], which leverage large-scale remote sensing image-text datasets and employ contrastive training methods based on the CLIP model, enabling tasks such as scene classification and image-text retrieval.\nConversely, generative-based VLMs focus on learning latent representations that facilitate bidirectional mapping between images and text tokens, allowing the generation of comprehensive image descriptions or detailed textual outputs. For instance, RSGPT [13], the first generative RSVLM in remote sensing, builds upon the InstructBLIP model and utilizes constructed instruction datasets derived from remote sensing scenes to support tasks such as image description and visual question answering. GeoChat [45],"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1 Approach Overview", "content": "The model consists of three fundamental components: the Aquila-CLIP ConvNext (A-CCN) Vision Encoder, the Hierarchical Spatial Feature Integration (SFI) module, and the Multi-layer Deep Alignment (MDA)-LLM, which is based on a pre-trained LLM (Llama-3 [50]) and integrates the SFI module."}, {"title": "3.2 Aquila-CLIP ConvNext (A-CCN) Vision Encoder", "content": "Most current VLMs utilize the ViT-CLIP model as the vision encoder. However, the input image resolution is typically constrained to lower dimensions, such as 336\u00d7336, due to the computational demands imposed by the global attention mechanism in the ViT architecture. This limitation hinders the ability to capture fine-grained details within the image. Furthermore, the ViT architecture lacks the capability to extract multi-scale visual features, which are essential for comprehensive image understanding.\nTo overcome these challenges, we propose the A-CCN Vision Encoder, which utilizes a convolutional CLIP model, such as CLIP-ConvNext, to support higher-resolution inputs and incorporate multi-scale feature aggregation, following an eagle-eye approach to capture both broad context and fine details. Compared to ViT-based CLIP models, CNN-based architectures not only maintain high performance with higher-resolution inputs but also offer more efficient training and faster inference. Additionally, the multi-scale feature maps generated by the CLIP-ConvNext vision encoder facilitate effective feature aggregation in downstream tasks. In our implementation, we employ the pre-trained CLIP ConvNeXt-XXL model as the vision encoder, which supports image inputs at resolutions up to 1024\u00d71024. In comparison, previous studies only support input image size of 448\u00d7448 pixels [51]. We also remove the final layer of the encoder, which is specifically designed for classification-based contrastive learning, to better suit our feature extraction objectives.\nUnlike previous methods that only output the final layer of the vision encoder, the A-CCN Vision Encoder outputs multi-scale features from the four stages of CLIP-ConvNeXt. To map these features into the same space as the text features from word embeddings, we use four projectors. Each projector is a two-layer MLP. For each scale of features $f_i \\in R^{C \\times H \\times W}$, we first reshape it to $f_i \\in R^{(H \\times W) \\times C}$, where H/W at the four"}, {"title": "3.3 Hierarchical Spatial Feature Integration (SFI)", "content": "In remote sensing imagery, the same object may exhibit varying characteristics at different GSDs, underscoring the importance of multi-scale features for comprehensively interpreting remote sensing scenes. We investigate the potential of multi-scale features derived from a vision encoder to leverage their distinctive representations, thereby enhancing the performance of Large VLM. A common approach involves interpolating multi-scale features to a uniform size and concatenating them along the channel dimension. While effective, this method can result in information loss, particularly when high-resolution features are downsampled to lower resolutions. To address this limitation and fully exploit multi-scale feature information, we propose a more advanced strategy, i.e., SFI, which utilizes cross-attention mechanisms and introduces a set of learnable query features that effectively aggregate information from multi-scale features while preserving the spatial structure unique to each scale.\nTo capture cross-contextual information from multi-scale features, we initialize a learnable feature as the query feature, $X_q \\in R^{L^2 \\times C}$, with multi-scale visual features serving as the key and value features, $X_s \\in R^{L_s^2 \\times C}$, where L represents the height/width of the learnable query feature. When flattened, it forms the query feature token length $L^2$, and C is the hidden dimension. To ensure consistency across spatial regions, we maintain a proportional relationship between L and $L_s$ (e.g., the size of the i-th scale feature is $k_sL \\times k_sL \\times C$, where $k_s$ is a positive integer). Since the multi-scale features from the encoder are well-aligned spatially, we aim to maintain spatial structure and learn more"}, {"title": "3.4 Multi-layer Deep Alignment (MDA)", "content": "As illustrated at the bottom of Figure 1, we utilize the vision encoder and SFI to obtain image-level embeddings. For the textual data, we tokenize the text sequence using the pre-trained LLM (Llama-3) tokenizer and project the tokens into text embeddings. Given that the SFI model represents complex and abundant image semantics from multi-scale features using a single token, when both the visual and text tokens are input into the LLM, the visual information may degrade as the number of decoder layers increases. To mitigate this issue, we introduce a Multi-layer Deep Alignment (MDA) strategy, incorporating multiple SFI operations within the LLM layers to form the innovative MDA-LLM architecture. This enables the LLM to retain access to uncompressed visual data across layers, preserving the depth and integrity of visual information and ensuring robust visual-textual representation throughout the decoding process."}, {"title": "3.5 Training", "content": "Drawing inspiration from LlaVA, our model implements a two-stage training framework consisting of pre-training and instruction fine-tuning. Both phases leverage next-token prediction loss for supervised learning, wherein the model predicts the subsequent token in a given input text sequence. The initial stage focuses on aligning image features with word embeddings using simple image-text pairs, while the second stage refines the model's capacity to process complex scenarios through instruction fine-tuning.\n\u2022 Stage 1 (Image-Text Alignment Pre-training): In the first stage, we initialize the A-CCN vision encoder with pre-trained CLIP-ConvNeXt-XXL weights and the LLM with pre-trained Llama-3 weights. During this stage, only the SFI module parameters"}, {"title": "4. Experiments", "content": "To verify the superior performance and generalization of Aquila, we evaluate our model on a series of vision-language benchmarks. These benchmarks cover a comprehensive range in the multimodal remote sensing domain, primarily including tasks such as image captioning and visual question answering."}, {"title": "4.1 Implement Details", "content": ""}, {"title": "4.1.1 Datasets", "content": "The image-text data utilized for training are entirely publicly available and comprise two primary components. The first component is used for alignment pre-training and includes datasets such as CapERA [52], UCM [53], Sydney [53], NWPU [54], RSICD [55], RSITMD [56], RSVQA-HR [57], RSVQA-LR [57], and WHU_RS19 [58]. We integrated these datasets following the LLaVA data format, resulting in approximately 1 million"}, {"title": "4.1.2 Summary of the Evaluation Benchmarks", "content": "This section provides a comprehensive overview of the evaluation benchmarks employed in our experiments, along with their corresponding metrics (Table 1). For the image captioning task, we selected four widely used datasets:\n\u2022 RSICD [55]: A dataset specifically designed for remote sensing image captioning tasks. The images are standardized to 224\u00d7224 pixels and cover various resolutions.\n\u2022 Sydney [53] and UCM [53]: Datasets utilized for high-resolution remote sensing image descriptions, providing detailed imagery for more precise captioning.\n\u2022 FIT-RSFG-Captions [47]: A dataset dedicated to fine-grained remote sensing image descriptions, focusing on detailed and nuanced captioning of remote sensing imagery.\nFor the VQA task, we chose three datasets:\n\u2022 RSVQA-LR [57]: A VQA dataset tailored for low-resolution remote sensing images, testing the model's ability to infer information from less detailed visuals."}, {"title": "4.1.3 Training", "content": "Our training methodology employs pre-trained weights from a vision encoder (CLIP-ConvNeXt) and a large language model (Llama3) and consists of two stages. In the first stage, we train on a dataset of 1 million image-text pairs for one epoch, freezing both the vision encoder and the LLM. In the second stage, we fine-tune the LLM using the Low-Rank Adaptation (LoRA) method on a dataset of 1.8 million instruction image-text pairs for one epoch, while keeping the vision encoder frozen. To ensure high image resolution, input images are resized to 1024\u00d71024 pixels in both stages. We use the AdamW optimizer and adopt a cosine annealing scheduler for the learning rate. All training is conducted with a global batch size of 32 on four NVIDIA A800 GPUs. The settings and hyperparameters applied in alignment pretraining and instruction finetuning are listed in Table 2."}, {"title": "4.2 Results", "content": "We present our results on tasks including image captioning and visual question answering (VQA). For image captioning, we employ the BiLingual Evaluation Understudy (BLEU) [59] metric with unigram precision (BLEU-1). For VQA, we report the average accuracy across all questions."}, {"title": "4.2.1 Image Captioning.", "content": "This task involves the automatic generation of natural language descriptions of image content, requiring the model to both comprehend the image and articulate its content fluently. We assessed our model's image captioning capabilities using the RSICD, Sydney Caption, RSITMD, and FIT_RSFG-Captions benchmarks [47]. As shown in Table 3, our model Aquila achieves superior performance on three simple description datasets, exceeding RSGPT by 4.28%, 1.16%, and 2.13% in the BLEU-1 metric, respectively. For the complex description dataset FIT_RSFG-Captions, we outperform the state-of-the-art method SkySenseGPT by 7.77%."}, {"title": "4.2.2 Visual Question Answering (VQA).", "content": "VQA tasks require models to generate correct answers based on input images and accompanying questions, necessitating not only a thorough understanding of the image content but also the ability to infer accurate answers from visual cues in response to specific queries. We evaluated our model's performance on three benchmarks: RSVQA-LR, RSVQA-HR, and FIT_RSFG-VQA [47]. As presented in Table 4, our model Aquila demonstrates significant improvements over the nearest competing methods, surpassing them by an average margin of 1.41%. These results underscore the effectiveness of our approach."}, {"title": "4.3 Ablation Study", "content": "To validate the effectiveness of the key modules in our model design, we conducted the following ablation experiments."}, {"title": "4.3.1 SFI vs. Concat", "content": "We employed the SFI module to perform multi-scale feature aggregation. The aggregated features encompass both the global information of multi-scale features and the refined details of their sub-regions. To assess the effectiveness of SFI, we replaced it with a concatenation module (Concat) as the baseline. Specifically, we constructed a convolutional module to aggregate the four multi-scale features into a single feature, serving as the final output of the visual encoder. As shown in Table 5, without performing deep alignment, SFI outperforms Concat in image captioning and VQA tasks by 5.62% and 6.85%, respectively. These experimental results indicate that the SFI module, based on a cross-attention mechanism corresponding to local regions, achieves superior feature fusion."}, {"title": "4.3.1 Multi-layer Deep Alignment(MDA)", "content": "The deep align module facilitates comprehensive alignment between visual and textual features. To evaluate its impact on model performance, we conducted an ablation study by removing the deep alignment module and directly inputting the visual and textual features into the LLM. As shown in Table 5, the absence of deep alignment resulted in performance decreases of 2.55% and 4.64% in image captioning and VQA tasks, respectively, demonstrating the effectiveness of deeply aligning visual and textual features. Furthermore, the combined use of the SFI module and MDA yielded the best results, confirming the efficacy of aggregating and aligning multi-scale visual information."}, {"title": "4.4 Demonstrations", "content": "The demonstrations provided in Figures 2, 3, and 4 offer some insights into Aquila's capabilities in remote sensing image comprehension and VQA. Through these examples, we can highlight Aquila's distinct advantages over existing models, such as GPT-4V, especially when interpreting high-resolution, multi-scale remote sensing imagery.\nOne of the key advantages of Aquila lies in its SFI module, which allows the model to extract and integrate multi-scale visual features efficiently. This capability is crucial in remote sensing applications, where a single feature can exhibit different"}, {"title": "5. Conclusion", "content": "In this paper, we introduced Aquila, an advanced visual-language model tailored to enhance remote sensing image understanding by effectively integrating high-resolution and multi-scale visual features with language representations. Addressing the limitations of existing RSVLMs\u2014which often rely on low-resolution inputs and simplistic visual-language mappings\u2014Aquila leverages a novel SFI module that supports high-resolution images up to 1024\u00d71024 pixels and aggregates multi-scale visual data, capturing fine-grained details essential for interpreting complex remote sensing scenes.\nTo bridge the gap between visual and linguistic modalities, we developed an MDA strategy that repeatedly integrates the SFI module within LLM layers. This deep alignment ensures a more precise and robust fusion of visual and language features without compromising the LLM's performance in natural language processing tasks. Through extensive experiments on various benchmarks\u2014including image captioning and VQA\u2014Aquila demonstrated superior performance compared to state-of-the-art models. Our ablation studies further validated the effectiveness of the SFI module and the importance of deep visual-language feature fusion. The collaborative synergy of the SFI and MDA modules enables Aquila to achieve a finer-grained semantic understanding and provides detailed interpretations of key image attributes, spatial relationships, and complex scene descriptions. Notably, when compared to large multimodal models like GPT-4V, Aquila exhibits competitive performance, particularly in focusing on textual information and capturing intricate image details.\nIn general, the proposed Aquila represents a significant advancement in AI-driven remote sensing technologies, paving the way for more accurate, detailed, and context-sensitive analyses of remote sensing data. Future work may explore optimizing"}]}