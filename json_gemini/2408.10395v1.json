{"title": "Evaluating Image-Based Face and Eye Tracking with Event Cameras", "authors": ["Khadija Iddrisu", "Waseem Shariff", "Noel E. O'Connor", "Joseph Lemley", "Suzanne Little"], "abstract": "Event Cameras, also known as Neuromorphic sensors, capture changes in local light intensity at the pixel level, producing asynchronously generated data termed \u201cevents\u201d. This distinct data format mitigates common issues observed in conventional cameras, like undersampling when capturing fast-moving objects, thereby preserving critical information that might otherwise be lost. However, leveraging this data often necessitates the development of specialized, handcrafted event representations that can integrate seamlessly with conventional Convolutional Neural Networks (CNNs), considering the unique attributes of event data. In this study, We evaluate event-based Face and Eye tracking. The core objective of our study is to showcase the viability of integrating conventional algorithms with event-based data, transformed into a frame format, while preserving the unique benefits of event cameras. To validate our approach, we constructed a frame-based event dataset by simulating events between RGB frames derived from the publicly accessible Helen Dataset. We assess its utility for face and eye detection tasks through the application of GR-YOLO a pioneering technique derived from YOLOv3. This evaluation includes a comparative analysis with results derived from training the dataset with YOLOv8. Subsequently, the trained models were tested on real event streams from various iterations of Prophesee's event cameras and further evaluated on the Faces in Event Stream (FES) benchmark dataset. The models trained on our dataset shows a good prediction performance across all the datasets obtained for validation with the best results of a mean Average precision score of 0.91. Additionally, The models trained demonstrated robust performance on real event camera data under varying light conditions.", "sections": [{"title": "1 Introduction", "content": "Face and Eye Tracking are critical tasks in Computer Vision, with substantial applications in Healthcare, In-cabin Monitoring [35], Attention Estimation [17,39] and Human-Computer Interactions [12]. This tracking technology is pivotal in detecting signs of fatigue, distraction, or impairment, necessitating a continuous stream of visual data, which is often unfeasible with traditional frame-based cameras. Other challenges include addressing scale variations as faces move closer or further from the camera, managing temporal dependencies between consecutive frames, accurately detecting faces under occlusions caused by rapid movements, and accounting for motion-induced shape deformations [5,48].\nEvent cameras (ECs) on the other hand, respond to changes in brightness to produce a continuous stream of data of asynchronous nature called events. ECs capture high-speed motion with very low latency and minimal motion blur. Events in an EC are represented by a stream of variable data points, each indicating a change in intensity at a specific pixel location at a given time. An event is therefore represented as a tuple, (x,y,t,p), where x, y are the spatial coordinates, t is the timestamp, and p is the polarity. The polarity is used to indicate changes in pixel intensity. That is, for p \u2208 (0,1), 0 indicates a decreasing change while 1 indicates an increasing change. ECs offer many advantages over traditional cameras including High Dynamic Range (140 dB versus 60 dB), High Temporal Resolution and Low Latency allowing them to capture and process visual information in real-time, with minimal delay making them ideal for applications that require fast, yet accurate visual feedback.\nHowever, utilizing ECs for such tasks introduces unique challenges due to the distinct nature of events. It is a common practice in the domain of event-based vision to develop specialized algorithms with hand-crafted event representations to accommodate the use of this data. However, it is important to bridge the gap between this novel data type and the established paradigms of computer vision, which predominantly utilize Convolutional Neural Networks (CNNs) that process standard video frames [33]. In our work, we aim to highlight methods of representing events in a format accepted by existing deep learning approaches, specifically an image-based representation while preserving the unique advantages of event cameras. We do this by optimizing the frames generated during motion simulation from static images by maximizing the number of events produced and event-frame accumulation with Temporal Binary Representation (TBR) [21]. This study is inspired from the work of Ryan et al. [36]. However, our methodology diverges from the previous approach in three fundamental aspects:\n1. We generate motion from the Helen Dataset by simulating planar motion from images placed in front of a camera in 6-Degrees of Freedom (DOF), as opposed to cropping an image and shifting the crop along the x and y axes.\n2. Beyond employing an event simulator to convert RGB videos into events, we accumulate events into binary frames and aggregate these frames into a single frame. This process enhances the density and quality of the simulated frames derived from the original RGB frames, enriching better simulation of events."}, {"title": "3. Finally, we carry out a detailed comparison, examining the performance of state-of-the-art models such as YOLOv8 and contrasting these results with those achieved by the GR-YOLO model [36]. This evaluation is performed using simulated event datasets and real event datasets, thereby affirming the effectiveness and relevance of our proposed methods.\nThrough this comparison, we aim to demonstrate the improvements our approach introduces to the domain and verify its usefulness in improving the adaptability and efficiency of deep learning models for processing the distinct data produced by event cameras. The paper is structured as follows: Literature Review, Dataset, Event Representation, Network Architecture, Training, Results and concluding remarks. We demonstrate the effectiveness of the proposed methodologies and show that training on these datasets generalizes well to real-world examples. An overview of the proposed methodology is illustrated in Fig. 1.", "content": null}, {"title": "2 Literature Review", "content": null}, {"title": "2.1 Face Detection and Tracking", "content": "Face tracking as a Facial Analysis task is critical, finding applications in neuroscience [11,47], automotive systems [15] etc. The distinctive attributes of ECs, such as their low latency, facilitate the immediate reporting of scene changes with minimal delay. These characteristics among others have been demonstrated to offer promising applications in face analysis such as gaze estimation [26,35], face pose alignment [37], yawn detection [23], emotion recognition [3,6] etc. However, these tasks have not achieved widespread attention within the domain of event-based vision. This is primarily due to the absence of event-based datasets that are readily applicable for face tracking.\nStudies have revealed different approaches to the use of ECs for face tracking [5]. The first works on ECs for face tracking leveraged different representations and algorithms that could fit these representations. Barua et al. [4], utilised a patch-based sparse dictionary generated from event data with the K-SVD algorithm, reconstructed high-intensity images from these streams and employed"}, {"title": "2.2 Eye tracking", "content": "Prior research in eye tracking primarily relied on conventional cameras to identify and monitor eye movements, aiding in tasks such as activity recognition and attention assessment [29,40]. However, recent attention has shifted towards ECs due to their asynchronous data presentation, eliminating the need for fixed frame rates. ECs offer low latency, enabling immediate event reporting with high temporal precision, making them particularly suitable for tracking eyes in scenarios involving rapid motion.\nFoundational works by Angelopoulos et al. [1] used traditional image processing and statistical methods for segmenting eye regions effectively. Building on this, Feng et al. [18] introduced an innovative Auto ROI algorithm that dynamically predicts eye ROIs to enhance tracking efficiency. Further, Li et al. [25] adopted an event-driven approach, processing event streams into frames and employing a low-latency CNN with an event-based ROI system for accurate pupil detection.\nIn contrast, a series of studies leveraged neural networks to address the unique challenges posed by sparse event data. Chen et al. [13] proposed a Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for precise pupil tracking. Bonazzi et al. [10] utilized a Spiking Neural Network (SNN)"}, {"title": "3 Dataset", "content": "The availability of datasets in event-based vision, particularly for face and eye tracking, remains limited. Most research utilizes locally sourced or simulated datasets [8,38]. Publicly available options like the Neuromorphic Event-based Facial Expression Recognition dataset [7] offer RGB and event streams for facial expression recognition but lack face and eye location annotations of the event stream. Directly applicable datasets for face and eye tracking are scarce, with the recent Faces in Event Stream (FES) dataset [9] being a notable exception. Despite providing event streams and bounding boxes, FES lacks eye annotations, necessitating manual annotation efforts.\nA promising solution involves using event simulators. For instance, Ryan et al. [36] developed the Neuro-morphic Helen synthetic dataset by converting the Helen Facial Landmarks Dataset, which comprises 2,300 internet-sourced images with landmark annotations, into an event format. This conversion involved simulating camera motion across the images and applying random augmentations before feeding the resulting videos into an event simulator that transforms RGB videos into events [20]. A main advantage in the use of the Helen dataset for this task is the different facial features present in the dataset that enables us to train a model robust to a wide variety of facial features and occlusions such as glasses, headwear, etc."}, {"title": "4 Event Representation", "content": "A typical characteristic of events is the structural differences between the data produced and that used as input for Convolutional Neural Networks (CNNs). For object tracking there is a need to transform event data into a representation"}, {"title": "5 Network Architecture", "content": "This section outlines the network architectures employed in the study to assess the efficacy of traditional Face and Eye tracking techniques compared to the state-of-the-art YOLOv8 based method."}, {"title": "5.1 GR-YOLOV3", "content": "The research methodology closely aligns with the experimental framework proposed by Ryan et al. [36] and utilizes the custom GR-YOLO architecture. A notable deviation in our approach is eschewing voxel grids, typically employed in discrete time bins, and instead leveraging frame-based representations. This decision simplifies the model's interface with existing architectures and enhances adaptability and applicability in real-world scenarios. The GR-YOLO architecture, shown in Tab. 1, integrates the YOLOv3 Tiny model with the addition of a Gated Recurrent Unit (GRU). The network comprises several key layers, each contributing uniquely to the effectiveness of the model.\nThe GRU is particularly beneficial in scenarios where sequences of frames are involved. By retaining information from previous frames, the GRU enables the model to understand motion and changes over time, leading to more accurate and consistent tracking of faces and eyes. This backward propagation of information significantly enhances the model's performance compared to the Tiny-YOLOv3 model without a GRU. YOLO Heads are responsible for predicting bounding boxes and class probabilities for detected objects. Having two YOLO heads implies that the model can process different scales of detection simultaneously, improving its accuracy and robustness in detecting objects of various sizes."}, {"title": "5.2 YOLOv8", "content": "YOLOv8 [22] offers enhanced accuracy and speed, making it suitable for real-time applications. YOLOv8 is characterized by its Fully Convolutional Architecture, which allows for efficient processing of images. As there is no published paper for this model, inference and understanding of the research methodology are achieved mainly by the published code. One of the critical improvements in YOLOv8 over its predecessors is its enhanced backbone network. This backbone is responsible for feature extraction and is more adept at handling small and occluded objects, which is a common challenge in face and eye tracking scenarios."}, {"title": "5.3 Training", "content": "In our study, we utilised the event frames generated from the methodology reviewed in Sec. 4 to train the baseline model (GR-YOLO) and YOLOv8. The dataset, comprising 2,330 accumulated frames, was divided into a training and validation set with a ratio of 80:20 respectively. The training process was implemented in PyTorch and trained on an NVIDIA GeForce RTX 2080 Ti GPU. An AdamW optimizer was utilised in both models to achieve the highest performance with a learning rate of 1 \u00d7 10-3 and a weight decay of 1 \u00d7 10-3. With trained using the Mean Squared Error (MSE) as the loss function, with the loss being calculated across both detection layers of the GR-YOLO algorithm."}, {"title": "6 Experiments and Evaluation", "content": "In this section, we present the results for both our baseline model and YOLOv8. To assess the efficacy of our trained models on our dataset and to demonstrate that the Frame-Based Representation of events from TBR effectively generalizes to real event camera datasets, we will conduct a comprehensive evaluation. This assessment will include both quantitative and qualitative analysis of the performance of our models when applied to real event camera datasets."}, {"title": "6.1 Quantitative Results: Synthetic Data Evaluation", "content": "We assess the effectiveness and suitability of our dataset for face and eye tracking by comparing the outcomes of testing three iterations of YOLO models on the test set of our synthetic data. This comparison will be against the performance metrics reported by Ryan et al. [36] when they applied GR-YOLO to their synthetic dataset. A more appropriate and direct comparison would have entailed evaluating the performance of our trained models on the synthetic datasets created by the authors. This direct comparison would allow us to evaluate the difference in performance of our proposed method of data generation as well as our method of event representation. However, this was not feasible due to our lack of access to their dataset. Our analysis, detailed in Tab. 2, evaluates our frame-based methodology for face and eye tracking with ECs against the voxel grid approach used in the referenced study. This comparison allows us to critically examine the relative strengths of each approach using the results from the respective synthetic datasets\nRyan et al. [36] demonstrated a robust performance with the top mean Average Precision (mAP) closely followed by YOLOv8. It is also observed that GR-YOLO still results in significant performance when compared to YOLOV3 without GRU, suggesting that this adaptation of YOLOv3 is applicable to other representations aside from voxel grids. The lowest error is observed in YOLOv8, which is close to the results obtained by Ryan et al. YOLOv3 has the highest MSE with 1.33, which indicates significant inaccuracies in its predictions. The average recall metric, which evaluates the model's ability to detect all relevant"}, {"title": "instances, indicates a superior capacity to identify relevant objects in YOLOv8. The recall rates for GR-YOLOv3 and YOLOv3 are lower. Despite the absence of the F1-Score data, the prevailing metrics suggest that Ryan et al.'s [36] model surpasses the others in terms of precision, accuracy, and recall. This is quite comprehensive as voxel grids offer a superior representation. This could also be a result of other factors such as the type of augmentations used during synthetic data generation which we did not have information on.\nNonetheless, the frame-based approach we presented demonstrates excellent performance, with YOLOv8 leading in terms of error minimization and localization accuracy. Consequently, we have successfully demonstrated that face and eye tracking can be achieved using a frame-based representation of events. This method generates robust results, making it a preferable choice for applications requiring high reliability and low computational demand. Additionally, it addresses the issues of under-sampling present in traditional RGB cameras for this task.", "content": null}, {"title": "6.2 Quantitative Results: Evaluation on Real Event Camera Data", "content": "In the subsequent phases of evaluation, we verify the suitability of the models trained on our synthetic dataset against diverse data datasets gathered by different models of real ECs. To begin with, we test the trained models against the largest and publicly available event-based dataset for face tracking, the recent Faces in Event Stream (FES) dataset [9]. This dataset serves as the most comparable benchmark in event-based vision, comprising 1.6 million event streams captured with a Prophesee PPS3MVCD EC, incorporating face bounding box annotations. This data was obtained by sending a formal request to the authors and subsequently, we were given the permission to access and utilise the data for this study. Given that our focus includes eye tracking, we manually annotated the eyes of selected test subjects within FES to facilitate a balanced comparison.\nAdditionally, we evaluate the performance of our models using naturalistic driving data recorded by Ryan et al. [36]. The dataset, which was not publicly accessible, was obtained for our study by requesting its use as a benchmark. Likewise, permission to utilize this data for our research was granted by the authors. The direct comparison with this data enables us to rigorously evaluate our frame-based methodology for face and eye tracking with ECs against the Voxel Grid strategy employed in the cited study."}, {"title": "In our final phase of evaluation, we collected sample data using an EVK4 EC, which captured a spectrum of head movements from minimal to extremely rapid. The task of manually annotating bounding box labels for faces and eyes in each frame for the last 2 datasets, though time-consuming, was imperative. This detailed annotation process was critical not only for deriving precise results but also for illustrating the effectiveness of our models across diverse datasets and with varying event cameras. All datasets employed in our testing phase were excluded from the training to ensure an unbiased evaluation of the performance of our models.", "content": null}, {"title": "Tab. 3 contains the validation outcomes of our baseline model (GR-YOLO) and YOLOv8 across the test datasets employing the same evaluation metrics for a consistent and comparative analysis. This facilitates a clear understanding of each model's effectiveness and efficiency in real-world application scenarios. All the datasets employed in our testing phase were excluded from the training to ensure an unbiased evaluation of the performance of our models.\nThe GR-YOLO model exhibits a robust performance in face detection, particularly when validated with the dataset of Ryan et al. [36]. This was anticipated as the motion simulating approach employed was based upon the operation of a prophesee EC. Therefore, the data obtained from the prophesee EC should yield highly accurate prediction outcomes. The FES dataset shows a decrease in validation results across all metrics. Though it shows good performance in face mAP, this is half what is shown in eye mAP. As the FES dataset is more focused on detecting faces than eyes, some event streams do not highlight the eyes. Therefore, frames from these events were not annotated during manual annotation to avoid assuming the positions of eyes. Thus lower results in face mAP are to be expected, which affects the overall performance of other metrics.\nGenerally across all datasets tested, the efficacy of GR-YOLO in eye detection exhibits a greater variability, accompanied by a significantly lower mAP. This highlights a challenge often encountered when using YOLOv3 for detecting smaller or more detailed objects like eyes. This is more apparent in ECs where the motion is relatively low, and as such, only a few events or no events are generated in eye regions. The overall performance measured by mAP across all datasets is satisfactory, indicating a comprehensive capability to detect faces and eyes from diverse sensors. Precision and recall metrics also support this conclu-", "content": null}, {"title": "sion. This demonstrates that GR-YOLO not only accurately identifies objects, but also performs consistently across datasets.\nIn contrast, YOLOv8 shows a noticeable improvement in performance across all metrics and datasets compared with GR-YOLOv3. The mAP for faces remains high, with a slight improvement in the FES Dataset and consistent performance in Ryan et al. The eye detection capability significantly improved, as evidenced by the increase in mAP for eyes in the FES Dataset. This enhancement indicates that YOLOv8 better handles the intricacies of eye detection across different sensors. The overall metric evaluation across all objects also sees an increase highlighting the superior general object detection capability of YOLOv8 over GR-YOLOv3 and underscoring the model's accuracy and consistency in object detection across varied conditions.", "content": null}, {"title": "6.3 Qualitative Results", "content": "The test videos used to record results qualitatively in this section include:\n1. Test video 1 recorded with Prophesee EVK4 in the lab with rapid head movement. Example frames shown in Fig. 4a and Fig. 5a\n2. Test video 2 from test subjects (Subject 0 test 1) used by [36], exhibiting slight head movements with another fast-moving object within the field of view. Example frames shown in Fig. 4b and Fig. 5b\n3. Test video 3 of subject 3 from FES dataset raw event streams with the subject wearing a nose mask. Example frames shown in Fig. 4c and Fig. 5c\nTest video 1 indicates performance in the presence of rapid motion, test video 2 indicates performance in different head positions while test video 3 indicates performance in the presence of occlusions. We use green boxes to indicate ground truth while purple boxes and red boxes in represent predictions of our models with confidence scores respectively."}, {"title": "7 Conclusions", "content": "This study uses a frame-based representation of events for face and eye tracking. The research addresses the challenge of under-sampling inherent in traditional RGB cameras and explores the advantages of using event cameras for face and eye tracking. By converting EC data into a format amenable to conventional deep learning algorithms, the study highlighted the need for bridging the gap between the distinctive data format of ECS and the established paradigms of deep learning. A testament to our methodological innovations is the successful generation of an event-based counterpart to the publicly available Helen Dataset, while optimizing the number of event frames produced during the simulation and thereby, enriching the resources available for face and eye tracking research. The efficacy of this dataset for face and eye tracking tasks is rigorously evaluated, and the findings are compared with those obtained using a voxel-based representation. The presented results affirm the dataset's reliability and its applicability to real-world event camera data. Our findings not only validate the dataset's utility but also highlight the distinct advantages of frame-based approaches, including its computational efficiency and accessibility.\nLooking forward, the application of event-based vision technology and event cameras holds potential for wide-ranging areas, including sports analytics, blink and saccade detection, emotion recognition, gaze tracking, etc. The potential for applications in scenarios requiring adaptable lighting conditions, decrease of blur and adaptable resolution opens up new avenues for research and development."}]}