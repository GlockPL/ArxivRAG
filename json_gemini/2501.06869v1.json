{"title": "A Foundational Generative Model for Breast Ultrasound Image Analysis", "authors": ["Haojun Yu", "Youcheng Li", "Nan Zhang", "Zihan Niu", "Xuantong Gong", "Yanwen Luo", "Haotian Ye", "Siyu He", "Quanlin Wu", "Wangyan Qin", "Mengyuan Zhou", "Jie Han", "Jia Tao", "Ziwei Zhao", "Di Dai", "Di He", "Dong Wang", "Binghui Tang", "Ling Huo", "James Zou", "Qingli Zhu", "Yong Wang", "Liwei Wang"], "abstract": "Foundational models have emerged as powerful tools for addressing various tasks in clinical settings. However, their potential development to breast ultrasound analysis remains untapped. In this paper, we present BUSGen, the first foundational generative model specifically designed for breast ultrasound image analysis. Pretrained on over 3.5 million breast ultrasound images, BUSGen has acquired extensive knowledge of breast structures, pathological features, and clinical variations. With few-shot adaptation, BUSGen can generate repositories of realistic and informative task-specific data, facilitating the development of models for a wide range of downstream tasks. Extensive experiments highlight BUSGen's exceptional adaptability, significantly exceeding real-data-trained foundational models in breast cancer screening, diagnosis, and prognosis. In breast cancer early diagnosis, our approach outperformed all board-certified radiologists (n=9), achieving an average sensitivity improvement of 16.5% (P-value<0.0001). Additionally, we characterized the scaling effect of using generated data which was as effective as the collected real-world data for training diagnostic models. Moreover, extensive experiments demonstrated that our approach improved the generalization ability of downstream models. Importantly, BUSGen protected patient privacy by enabling fully de-identified data sharing, making progress forward in secure medical data utilization. An online demo of BUSGen is available at https://aibus.bio.", "sections": [{"title": "1 Main", "content": "Breast cancer is one of the most prevalent cancers, and remains a significant health threat for females worldwide [1-3]. Ultrasound is an essential imaging modality to reduce breast cancer-related mortality, which is widely adopted [4, 5] for its non-invasiveness, convenience, and higher sensitivity compared to mammography, especially in patients with dense breasts or younger ages [6-10]. However, the accurate interpretation of breast ultrasound images is challenging and time-consuming due to the complexity of biological structures, the subtlety of pathological semantics, and variability among clinical scenarios [11-15]. Even experienced clinicians could make mistakes in interpreting the images, potentially delaying early diagnosis, hindering timely intervene to tumor progression, and compromising treatment plans.\nRecent breakthroughs in deep learning have achieved success in medical domains, such as analysis of chest X-ray images [16, 17], pathological images [18], and retinal images [19, 20], based on large scale high-quality datasets. However, breast ultrasound analysis remains under-explored by deep learning models, as publicly available breast ultrasound images are limited [21-23] due to the privacy and legal constraints [24, 25]. Therefore, the deep learning models for breast ultrasound analysis face challenges such as over-fitting [26-28], lack of generalization ability [29], learning inefficiency of rare conditions [25], and lack of data transparency [25, 30]. Meanwhile, deep learning is undergoing a paradigm shift from task-specific schemas to foundational models [18, 20, 31-37], which leverage knowledge transfer from extensive and diverse pretraining data to support broad downstream tasks. Consequently, a foundational generative model is essential for enabling precision breast ultrasound analysis.\nIn this paper, we introduce BUSGen (Breast UltraSound Generative model), the first foundational generative model designed for breast ultrasound image analysis, which significantly improves a wide range of essential tasks associated with breast cancer screening [38-40], diagnosis [41-43], and prognosis [44-47] (Fig. 1). The generative capabilities of BUSGen are rendered by the component of a powerful diffusion model [48], pretrained on an extensive dataset encompassing diverse sources, regions, and subpopulations. As shown in Fig. 1a, this dataset includes over 3.5 million images and 3,749 lesions spanning more than 30 pathological subtypes. This pretraining allows"}, {"title": "2 Results", "content": "We developed BUSGen, a foundational generative model for breast ultrasound analysis, pretrained on extensive datasets and designed to generate unlimited, informative, and task-specific data to enhance the performance of deep learning approaches. As shown in Fig. 1, the ability of this foundational generative model was empowered by two key parts in our frameworks: pretraining and adaptation.\nPretraining data. We collected an extensive dataset from multiple institutions, including comprehensive and specialized cancer centers across various regions. The"}, {"title": "2.1 The BUSGen pretraining and adaptation framework", "content": ""}, {"title": "2.2 BUSGen generates realistic data while protecting privacy", "content": "To verify the fidelity of generated data, we conducted a Visual Turing Test [60], involving board-certified radiologists (n=3) with clinical experience of 3, 4, and 11 years, respectively. We presented 100 images, consisting of 50 generated and 50 real images and asked readers to distinguish \"fake\" images generated by BUSGen from real images. Fig. 2a shows the predictions and accuracy of each reader's predictions as well as their corresponding experience. These results show that approximately 50%-75% of the generated images were mistakenly identified as real by the radiologists, indicating the biological structures and semantic features of more than half of the generated images were realistic for radiologists.\nAs BUSGen provides breast ultrasound images with high fidelity, a reasonable concern is patient privacy leakage when releasing this foundational generative model because diffusion models tend to exactly replicate images in their training sets [61]. We attempted to make progress in patient privacy protection and took advantage of the recent advances in sampling strategy (CPSampling [57]) to protect the privacy of patients in our pretraining data. To illustrate the effectiveness of privacy protection, we visualize the similarity scores of BUSGen-generated samples and their nearest neighbor in the training data in Fig. 2b. In addition, we present two image pairs with the highest similarity scores in Fig. 2c. These results demonstrate that BUSGen will not exactly replicate any image in the training data, paving a new way to completely de-privacy data sharing."}, {"title": "2.3 Improving breast cancer screening", "content": "Breast cancer screening aims to detect breast cancer in a normal population without severe symptoms [62, 63] where ultrasound is widely used for breast cancer screening [6], because of its high sensitivity to breast nodules, especially in the dense breasts [7-10]. However, precise detection of small lesions and accurately interpreting screening-detected lesions are challenging in clinical practice [64]. We adapted BUSGen to these two screening tasks to evaluate its efficacy in breast cancer screening.\nThe lesion detection task [21, 65] aims to precisely identify whether the breast ultrasound images contain lesions and locate all lesions using bounding boxes. As shown in Fig. 3a, we evaluated the detection performance on small lesions which are defined as lesions with smallest 30% relative areas (n=16,896; 4,822 with lesions and 12,074 without lesions). As shown in Fig. 3b, for small lesion detection, our BUS-DM achieved an Average Precision ($AP_{small}$) of 0.702 (95% CI 0.681-0.720), while the baseline model achieved an $AP_{small}$ of 0.657 (95% CI 0.637-0.679; P-value=0.0017). As shown in Fig. 3c, on the whole test set (n=28,150; 16,076 with lesions and 12,074 without lesions), our BUS-DM achieved an AP of 0.934 (95% CI 0.931-0.938); while the baseline model achieved an AP of 0.912 (95% CI 0.907-0.917; P-value<0.0001).\nFurthermore, to facilitate accurate clinical triage, we conducted experiments in interpreting opportunistic screening-detected lesions. These lesions were detected from patients who had no symptoms of breast cancer, such as palpable breast masses, nipple discharge, severe breast pain, or changes in breast appearance, as illustrated in Fig. 3d. In this task, our BUS-DM achieved an AUC of 0.913 (95% CI 0.873-0.946) which significantly exceeded Baseline-CLIP (AUC: 0.870; 95% CI 0.823-0.912; P-value=0.0074), as shown in Fig. 3e. These results show the high adaptivity of BUSGen in breast cancer screening."}, {"title": "2.4 Facilitating generalizable model for breast cancer early diagnosis", "content": "Early diagnosis of breast cancer can significantly reduce mortality. Here, we conducted an experiment to distinguish an important early-stage breast cancer (ductal carcinoma in situ) from benign lesions. Ductal carcinoma in situ (DCIS) is a pathological subtype of non-invasive early-stage breast cancer where all cancer cells are confined within"}, {"title": "2.5 Enhancing diagnosis with scalable generated data", "content": "Diagnosis is an essential task in breast ultrasound analysis, requiring to classify the benignity or malignancy of lesions. As pathological features were already learned by BUSGen during the pretraining process, we directly sampled data without adaptation for training diagnostic models. First, we explored the critical question: when scaling up the generated data, to what extent could data generation replace data collection? In Fig. 4e, we showcased the scaling effect [71] of real and generated data where we trained diagnostic models on various data scales and then evaluated their test losses and AUC scores. Restricted by computational resources, we scaled up generated data to 1 million samples. We highlight that our generated data were as effective as real collected data and continuously improved the diagnostic performance when scaling up, which could not be achieved in previous works [50, 72, 73]. We compared BUS-DM with an in-house NYU-AI which was developed with 288,767 real collected data. Trained on 1 million generated images, our BUS-DM achieved comparable results (AUC: 0.929; 95% CI 0.907-0.950) to the NYU-AI (AUC: 0.927; 95% CI: 0.907-0.959) on an open-source BUSI test set, and significantly outperformed the Baseline-CLIP (AUC: 0.893; 95% CI 0.865-0.918; P-value=0.0006), as illustrated in Fig. 4e.\nFurther, we evaluated the efficacy of our approach on in-house test sets. In the internal evaluation (Fig. 4f), our BUS-DM achieved an AUC of 0.953 (95% CI 0.935-0.967)"}, {"title": "2.6 Facilitating prognostic indicator prediction", "content": "Prognosis aims to predict the final outcomes of patients with breast cancer. It is difficult to directly predict the final outcomes [75], as many potential influencing factors are not well-defined or not available [76]. Instead, we conducted experiments to predict indicators that are highly related to prognosis. We adapted BUSGen to two prognostic indicator prediction tasks in the few-shot settings and evaluated the performance of BUS-DMs trained on datasets augmented by generated data.\nWe conducted experiments on predicting triple-negative breast cancer (TNBC), which is a molecular subtype of breast cancer that tends to be more aggressive and resistant to common treatments with a high recurrence rate and poor prognosis than non-TNBC [77, 78]. The \"gold standard\u201d molecular subtype labels are determined by immunohistochemistry (IHC) and in situ hybridization (ISH) confirmed biomarkers (ER-a, PgR and HER2). As shown in Fig. 5a, the BUS-DM achieved an AUC of 0.803 (95% CI 0.740-0.880) and outperformed the Baseline-CLIP (AUC: 0.723; 95% CI 0.648-0.795; P-value=0.0046).\nAdditionally, we conducted experiments to predict axillary lymph node (ALN) status (positive or negative for ALN metastasis) which plays an essential role in treatment planning for breast cancer [79], being the most significant prognostic indicator for early-stage patients [80]. Pre-operative prediction of ALN metastasis from ultrasound images could pave the way for optimized clinical decision-making. The \"gold standard\" ALN status labels are determined by pathology after sentinel ALN biopsy or dissection. As shown in Fig. 5b, the BUS-DM achieved an AUC of 0.895 (95% CI 0.841-0.960) while the Baseline-CLIP achieved an AUC of 0.807 (95% CI 0.723-0.890; P-value=0.0118).\nIn Fig. 5c and d, we provides the t-SNE plots of embeddings produced by different models. The plots show that BUS-DM distinguished TNBC from non-TNBC better than Baseline-CLIP, and BUS-DM separated ALN-positive from ALN-negative lesions more effectively than Baseline-CLIP. For quantitative evaluation of clustering results, we employed metrics including normalized mutual information (NMI), adjusted Rand index (ARI) and silhouette coefficient (SIL) (Supplementary Information Section 8). The BUS-DM had higher NMI, ARI and SIL scores, showing its higher generalization ability than the Baseline-CLIP.\nIn an attempt to understand the decision-making process of BUS-DMs, we conducted a qualitative analysis of saliency maps. These saliency maps, obtained by GradCAM [81] indicated the important regions for BUS-DM to identify the prognostic indicators. Fig. 5e shows the saliency map of identifying a TNBC lesion. The upper left part of the lesion margin is highlighted by BUS-DM in predicting TNBC. In Fig. 5f, we show the saliency map of predicting a lesion with positive ALN status for metastasis. Instead of focusing on the lesion structure, we found BUS-DM paid more attention to the surrounding glandular tissues of the lesion. These intriguing findings"}, {"title": "3 Discussion", "content": "Data scarcity restricts the development of deep learning models in various breast ultrasound analysis tasks. At the same time, deep learning is undergoing a paradigm shift with the rise of foundation models [35-37] trained on large-scale data that can be adapted to a wide range of downstream tasks. This paradigm can be particularly advantageous for medical domains where access to high-quality annotated data is limited. In this paper, we introduce BUSGen, the first foundational generative model in breast ultrasound image analysis, capable of learning features and patterns from over 3.5 million images and generating data tailored for various downstream tasks with the guidance of a few examples. These generated data can serve as unlimited resources to train downstream models.\nWe demonstrate the high adaptivity of BUSGen to improve a wide range of downstream tasks: lesion detection, opportunistic screening-detected lesion analysis, early-stage cancer diagnosis, lesion diagnosis, molecular subtype prediction, and metastasis prediction. Moreover, for the first time, we characterized the scaling effect of generated data in the medical domain and show that generated data can be as effective as real collected data in diagnostic tasks. Additionally, we find that BUSGen improves the generalization ability of downstream models, which is also reported in recent works [51]. Finally, we make progress in patient privacy protection with generated data, and expect this de-privacy approach could encourage data sharing in more medical tasks.\nOur study could potentially facilitate various downstream applications in real-world clinical practice. First, our BUS-DM significantly outperforms human radiologists on DCIS, an important early-stage cancer subtype. This makes it suitable for integration into the workflow of breast ultrasound diagnosis to facilitate early identification of breast cancer. Additionally, the BUS-DM for lesion diagnosis could re-evaluate a huge volume of retrospective breast ultrasound cases deposited in hospitals that have not undergone biopsies or surgeries, identifying potential false negatives and prompting further examinations. Finally, the BUS-DM for ALN metastasis prediction could be used before ALN biopsy or dissection, making a way for optimized clinical decision-making. These improvements might contribute to better treatment outcomes and reduced mortality rates.\nIn the future, we will pretrain the BUSGen model on a more comprehensive dataset, covering breast ultrasound images under various drug treatment and post-operative conditions, to facilitate more tasks, such as prediction of the efficacy of neoadjuvant therapy response, recurrence rate and disease-free survival time. Additionally, we will conduct experiments to explore more efficient and effective human-AI interactions of BUS-DMs in real-world clinical settings. Finally, we expect the paradigm of foundational generative models to be extended to a wider range of clinical tasks across more imaging modalities, such as computed tomography and magnetic resonance imaging."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Ethical approval", "content": "Our study was approved by the institutional review board of the Peking University Cancer Hospital & Institute (ID: 2024YJZ41). The study was not interventional and was performed under guidelines approved by the institutional review board. Informed consent was waived since the study presents no more than minimal risk. All datasets processed for this research were de-identified before being transferred to study investigators."}, {"title": "4.2 Pretraining data collection, processing, and annotation", "content": ""}, {"title": "4.2.1 Data collection", "content": "For pretraining the foundational generative model, we collected scanning videos of 7,965 breast ultrasound examinations of 5,985 patients from two institutions, Peking University Cancer Hospital & Institute (PKUCH) and Nanchang People's Hospital (NPH). These exams were conducted between January 2020 and March 2021. We randomly sampled an internal test set for lesion diagnosis (detailed in Section 4.4.3) and kept others in the pretraining set where we sampled by patients to prevent information leakage. This pretraining dataset covered a broad range of conditions in breast ultrasound analysis. The collected data covered patients aged 11 to 84, with lesions encompassing 34 pathological subtypes and 4 molecular subtypes."}, {"title": "4.2.2 Data processing", "content": "We removed low-quality data where clinical information is incomplete or lesions could not be clearly visualized. After data processing, quality control and data sampling, we kept 4,636 patients (1,589 normal patients and 3,047 abnormal patients with 3,749 lesions). In total, the pretraining dataset contained 5,907 breast ultrasound examinations with scanning videos (2,157 normal videos and 3,750 videos with lesions) and 3,518,495 images (1,130,843 normal images and 2,387,652 images with lesions). Note that these massive images might contain redundant temporal information."}, {"title": "4.2.3 Data annotation", "content": "To incorporate more information in the pretraining process, we extracted rich \"gold standard\" annotations by clinical experts from ultrasound reports. Based on pathological reports, 1,387 out of 3,749 lesions were attached with biopsy- or surgery-confirmed pathology labels (694 benign and 693 malignant), encompassing 1,454 videos (733 benign and 721 malignant) and 931,525 images (404,220 benign and 527,305 malignant). Additionally, clinical experts annotated bounding boxes to indicate lesion areas based on surgery or ultrasound reports, and they identified the device types of ultrasound scanners (9 manufacturers with 18 device types) from the ultrasound reports."}, {"title": "4.3 BUSGen pretraining and adaptation framework", "content": ""}, {"title": "4.3.1 BUSGen pretraining", "content": "We developed BUSGen as a conditional Denoising Diffusion Probabilistic Model (diffusion model) [48, 52] with lightweight U-Net architecture. The pretraining process of diffusion models enabled it to learn the data distribution P(x) of breast ultrasound images. Specifically, diffusion models learn to gradually denoise a random Gaussian sample $x \\sim N(0, I)$ to a realistic image $x_o \\sim P(x)$. This is achieved via learning the reverse process $P(x_{t-1}|x_t)$ of a Markov Chain of length T. Diffusion models can be interpreted as a denoising autoencoder $\\epsilon_{\\theta}(x_t,t)$, which estimates the noise $\\epsilon$ in $x_t$ at each step. The learning objective is simplified to:\n$L = E_{x,\\epsilon,t} [||\\epsilon - \\epsilon_{\\theta}(x,t)||_2]$"}, {"title": "4.3.2 BUSGen adaptation", "content": "In adaptation of BUSGen to downstream tasks, we designed several algorithms to ensure the high quality of generated data, even in the case of limited downstream data. To preserve the domain knowledge acquired during the pretraining process, we froze BUSGen parameters and fine-tuned only the additional lightweight parameters designed as low-rank adapters (LoRA). To prevent overfitting, we apply strong conventional data augmentations, such as random crop, color jittering, and random flip. Additionally, we incorporate a novel device type data augmentation, transforming each image to all device types [82]. This image-to-image translation task was performed using CycleGAN [83] models, where each CycleGAN model was responsible for transferring images from any source device to the target device. For each real image, 18 augmented images from various device types were generated while preserving the same \"content\".\nWe employed classifier-free guidance [53] to achieve better control of the generated images with input conditions. This algorithm could be formulated as:\n$\\epsilon_{\\rho}(x_t, t, c) = (1 + w)\\epsilon_{\\theta}(x_t, t, c) \u2013 w\\epsilon_{\\theta}(x_t, t)$"}, {"title": "4.4 Downstream methods", "content": "The datasets used for the downstream tasks were collected from two external hospitals: Peking Union Medical College Hospital (PUMCH) and the Cancer Hospital of the Chinese Academy of Medical Sciences (CICAMS). However, an exception applies to the internal test set for the diagnostic task, which was collected from PKUCH and NPH (introduced in Section 4.2.1). We conducted extensive experiments to verify the adaptivity of BUSGen. Specifically, we fine-tined BUSGen on various downstream tasks, and sampled generated images for training the BUS-DMs for various tasks. To select the hyperparameters, we adopted 5-fold cross-validation on downstream training sets and repeated each experiment 5 times for robustness."}, {"title": "4.4.1 Breast cancer screening", "content": "We conducted two tasks critical in breast cancer screening: (1) lesion detection and (2) clinical triage of opportunistic screening-detected lesions. For the first task, the training set of lesion detection contained 67,911 images (38,425 with lesion boxes and 29,486 without lesions); and the test set contained 28,150 images (16,076 with lesion boxes and 12,074 without lesions). We directly produced generated images with lesion box conditions as BUSGen gained this ability during pretraining, and these box conditions were regarded as pseudo-labels in lesion detection. We used Mask R-CNN [84] models with ViT-L backbones as the detectors. We trained Baseline-CLIP on the real data and developed BUS-DM on the same dataset enhanced by 20,000 generated images. For the second task, the opportunistic screening-detected lesions were collected from two external institutions. We divided the collected data evenly into training and test sets: the training set contained 265 lesions (154 benign and 111 malignant), and the test set contained 266 lesions (152 benign and 114 malignant). We adapted BUSGen to this task and generated 10,000 training data. For this task, we developed the BUS-DM using only generated data. We first pretrained BUS-DM on 1 million generated training data mentioned in Section 2.5, and then trained BUS-DM with these task-specific 10,000 generated data for the opportunistic screening task."}, {"title": "4.4.2 Breast cancer early diagnosis", "content": "We adapted BUSGen using 34 biopsy-confirmed DCIS lesions and trained the binary classifier using the generated DCIS and benign data. For this task, we developed the BUS-DM using only generated data with the same pipeline mentioned in Section 4.4.1. The test set contained 296 lesions (63 benign and 133 DCIS) where the DCIS"}, {"title": "4.4.3 Lesion diagnosis", "content": "For lesion diagnosis, the internal test set contained 579 lesions (274 benign and 305 malignant) with \"gold standard\" labels. For external evaluation, we prospectively collected 227 lesions (63 benign and 164 malignant) from 225 consecutive patients who underwent breast ultrasound examinations and biopsies between October 2022 and March 2023. Additionally, we included a public Breast Ultrasound Images (BUSI) dataset [22] collected from an institution in Egypt (437 benign, 210 malignant, and 133 negative lesions). We trained Baseline-CLIP on the real data and developed BUS-DM with only generated data. To evaluate the scaling effect, we generated 1 million images using BUSGen, and randomly sampled images in different data scales to train BUS-DMs where we repeatedly sampled 5 times at each data scale for robust evaluation."}, {"title": "4.4.4 Prognostic indicator prediction", "content": "For molecular subtype classification, the data were collected from two external institutions. The \"gold standard\" labels were extracted by clinical experts based on IHC and ISH reports. We split all collected data into training and test sets. The training set contained 423 lesions (363 non-TNBC and 60 TNBC) and the test set contained 426 lesions (369 non-TNBC and 57 TNBC). For ALN status classification, the data were also collected from two external institutions. The \"gold standard\" labels were assigned by clinical experts based on ALN biopsy or dissection results, with negative ALN status defined as the absence of metastasis in any ALN. We split all collected data into training and test sets. The training set contained 110 lesions (77 ALN-negative and 33 ALN-positive) and the test set contained 117 lesions (80 ALN-negative and 37 ALN-positive). We trained Baseline-CLIP on the real data and developed BUS-DM with the training set enhanced by 10,000 samples generated by the BUSGen model, which had been adapted to these tasks, respectively."}, {"title": "4.5 Statistical analysis", "content": "We estimate the 95% confidence intervals by 1,000 bootstrap replications. We calculate the two-sided P-values for significance comparisons of sensitivity and specificity using permutation tests with 10,000 permutations. The P-values of AUC are calculated using DeLong's test [85, 86]."}, {"title": "4.6 Implementation details", "content": "We implemented the project based on the following packages: Python (3.9), OpenCV (4.9.0.80), Pandas (2.2.1), Numpy (1.26.4), and Pillow (10.3.0). Additionally, the deep learning model is implemented using PyTorch (1.10.1) and Torchvision (0.11.2). Evaluation metrics are calculated using Sklearn (1.4.1). We conducted the experiments using computational resources from seven GPU clusters. Four of these clusters were equipped with eight NVIDIA RTX 3090 GPUs each, while the remaining three featured eight NVIDIA RTX 4090 GPUs each. The sampling process costs ~2.14 seconds for generating one image using one RTX 4090 GPU. Therefore, the computational cost of generating 1 million images is ~600 GPU hours.\nData Availability. The BUSI dataset used in this study is publicly available at https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset. We release a repository of the BUSI dataset with device augmentation. However, due to respective Institutional Review Boards' restrictions and to protect patient privacy, the BUS-3.5M and the downstream datasets used in this study cannot be made publicly available. De-identified data may be made available by the corresponding authors for research purposes upon reasonable request.\nCode Availability. The pretraining and adaptation code for BUSGen and an online API will be released. Also, we provide an online demo of BUSGen at https://aibus.bio."}]}