{"title": "Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures", "authors": ["Akhila Yerukola", "Saadia Gabriel", "Nanyun Peng", "Maarten Sap"], "abstract": "Gestures are an integral part of non-verbal communication, with meanings that vary across cultures, and misinterpretations that can have serious social and diplomatic consequences. As AI systems become more integrated into global applications, ensuring they do not inadvertently perpetuate cultural offenses is critical. To this end, we introduce Multi-Cultural Set of Inappropriate Gestures and Nonverbal Signs (MC-SIGNS), a dataset of 288 gesture-country pairs annotated for offensiveness, cultural significance, and contextual factors across 25 gestures and 85 countries. Through systematic evaluation using MC-SIGNS, we uncover critical limitations: text-to-image (T2I) systems exhibit strong US-centric biases, performing better at detecting offensive gestures in US contexts than in non-US ones; large language models (LLMs) tend to over-flag gestures as offensive; and vision-language models (VLMs) default to US-based interpretations when responding to universal concepts like wishing someone luck, frequently suggesting culturally inappropriate gestures. These findings highlight the urgent need for culturally-aware AI safety mechanisms to ensure equitable global deployment of AI technologies.", "sections": [{"title": "Introduction", "content": "Gestures, along with body postures and facial expressions, are integral to non-verbal communication and play a critical role in conveying beliefs, emotions, and intentions (Efron, 1941; Knapp, 1978; Kendon, 1997; Burgoon et al., 2011). While non-verbal communication is universal, its interpretations significantly vary across cultures, often leading to misunderstandings (Kirch, 1979; Matsumoto and Hwang, 2012, 2016).\nWith AI systems increasingly deployed globally across various domains, understanding cultural nuances in gesture usage becomes crucial. Companies such as AdCreative.ai and QuickAds integrate AI into advertising to tailor promotional materials for different cultural contexts, while travel platforms like TripAdvisor provide (often unverified) culturally specific recommendations, including local etiquette and customs. However, as these systems engage with diverse audiences, the risk of generating culturally offensive content poses challenges not only in terms of harm and exclusion but also in reputational damage and business liability (Wenzel and Kaufman, 2024; Ryan et al., 2024).\nDespite these real-world risks, current AI safety efforts primarily target explicit threats such as violence and sexual content (Han et al., 2024; Deng and Chen, 2023; Riccio et al., 2024), with relatively less attention on cultural sensitivities. Large language models (LLMs) and vision-language models (VLMs) are increasingly studied for their knowledge of cultural norms and artifacts like food and clothing (Yin et al., 2021; Romero et al., 2024; Rao et al., 2024), while text-to-image (T2I) models have prioritized geographical diversity, realism, and faithfulness (Hall et al., 2023, 2024; Kannen et al., 2024). However, the extent to which these models handle cultural nuances in nonverbal communication largely remains unexplored.\nTo bridge this critical gap, we study culturally contextualized safety guardrails of AI systems through the lens of emblematic or conventional gestures - gestures that convey a single distinct message, typically independent of speech, but whose meaning can vary across communities. We introduce MC-SIGNS, a novel dataset capturing cultural interpretations of 288 gesture-country pairs spanning 25 common gestures and 85 countries (\u00a72). Annotators from respective regions provide insights on: (1) the gesture's regional level of offensiveness (from not offensive to hateful), (2) its cultural significance, and (3) situational factors such as social setting and audience that influence its interpretation within that region. This dataset serves as a test bed for evaluating and improving cultural safety of AI systems in real-world applications.\nUsing our MC-SIGNS dataset, we aim to answer the following research questions:\nRQ1: Can models (LLMs, VLMs) accurately detect and (for T2I systems) reject culturally offensive gestures?\nRQ2: Are models culturally competent when interpreting universal concepts described by"}, {"title": "MC-SIGNS: Dataset Construction", "content": "We curate MC-SIGNS, a dataset focused on identifying and documenting gestures that may be considered offensive or inappropriate across different regions. We employ two approaches to collect data: (1) identifying offensive gestures across different regions using documented online sources (\u00a72.1), and (2) identifying regions where gestures considered offensive in the US are not offensive elsewhere, using LLM-generated suggestions (\u00a72.2). All gesture-country pairs are human validated (\u00a72.4).\nWe manually curated a set of 25 emblematic gestures by consolidating information from numerous travel advisory boards, cultural exchange programs, workplace etiquette resources, and existing anthropological studies. These sources documented the countries where each gesture is considered offensive, resulting in 181 distinct culturally sensitive gestures-country pairs across 76 countries. We use these country boundaries as proxies for culture, despite their limitations, following similar existing work in computational studies (Wilson et al., 2016; Jha et al., 2023; Romero et al., 2024).\nFor each gesture, we extract the canonical name from its corresponding Wikipedia page title and collect all alternate names mentioned on the page, including those in English and other languages. We also record the physical description provided on Wikipedia to ensure annotators can fully understand each gesture, even if a specific name is unfamiliar. To further support annotation, we collect two images per gesture (50 total) from Wikipedia, Wikimedia, and CC-BY-4.0 licensed sources, cropping each to focus on the gesture.\nTo investigate potential western-centric biases in Al systems (Bender et al., 2021; Prabhakaran et al., 2022), we collected offensiveness interpretations of all 25 gestures from USA and Canada.\nTo complement our initial set focused on gestures considered offensive across different regions, we leveraged LLMs (GPT-4 and Claude 3.5 Sonnet) to identify countries where gestures offensive in USA might be culturally acceptable elsewhere. We used LLMs for such suggestions due to inherent reporting biases in human-curated sources, which predominantly document where gestures are unacceptable rather than explicitly listing where they are acceptable. Unsurprisingly, LLMs had low precision in suggesting such regions; however, this still helped identify regions where these gestures are not offensive, as well as additional countries where they are offensive, thus enriching our dataset.\nOur final set comprises of 288 gesture-country pairs (43 from USA and Canada, and 64 from LLMs) spanning across 25 gestures and 85 countries. We collect annotations for all of these pairs.\nSince collecting country-level annotations for each of the 85 countries would be prohibitively complex, we define cultural in-groups using the United Nations geoscheme's 22 geographical subregions. This grouping provides finer granularity than continent-level, but more practical than country level. Within each in-group, we select annotators exclusively from countries represented in our dataset, ensuring cultural relevance while maintaining practical scalability. Our final set spans 18 of these subregions.\nFor each gesture-country pair, annotators were presented with the gesture name, alternate names, physical description, country name and 2 images of the gesture. The annotators provided:\nAn Offensiveness label (Hateful, Offensive, Rude, Not Offensive, or Unsure)\nConfidence rating on a 5-point Likert scale\nFree-text cultural meaning of the gesture\nSpecific contexts or scenarios where the gesture is considered offensive or appropriate"}, {"title": "Dataset Characteristics", "content": "Our final dataset comprises 288 gesture-country pairs spanning across 25 gestures and 85 countries, with an average of 4.89 annotations per pair, yielding a total of 1,408 annotations. The most severe harm types identified are gender-based harassment (sexual harassment 7.64%, infidelity 3.47%) and discriminatory content (antisemitism 2.43%, homophobia 2.08%, white supremacy 1.04%, ableism 0.69%). The dataset also includes hostile behavior (11.11%) and obscene gestures (9.38%). Please refer to Appendix B for more examples, inter-annotator agreement, offensiveness ratings and confidence score distributions.\nThresholding Since interpretations of offensiveness are known to be subjective (Prabhakaran et al.,"}, {"title": "Experimental Setup", "content": "To showcase one of the use cases of our MC-SIGNS dataset, we conduct investigations focused on cross-cultural gesture understanding in AI systems, specifically T2I models, LLMs, and VLMs."}, {"title": "Evaluation Strategies", "content": "Motivated by real-world applications of AI systems, we employ two types of evaluation strategies to assess models' ability to interpret gestures across cultural contexts:\nExplicit Mention Here, we evaluate whether models correctly interpret gestures when referenced directly \u2013 via specific gesture names, physical descriptions, or images, depending on the model type. This setting is motivated by cross-cultural applications such as marketing and advertising, where an accurate understanding of gestures across countries is crucial. For instance, when generating advertising content for Turkey featuring a group of people showing an \u201cOK\u201d gesture, models should be able to recognize its potential homophobic connotations and flag the request.\nWe test this through:\nCountry Prompt: Prompts explicitly specify the country and the gesture.\nCountry + Scene Prompt: Provide additional context via specific usage scenarios, involving certain demographic attributes, and scene descriptions.\nTo generate gesture-specific scene descriptions, we aggregated annotator-provided meanings and context descriptions. With this, we use GPT-4 to generate scenarios in the template 'A {demographic} person showing {gesture} in {country} in {scene}', prioritizing hateful/offensive/rude human-annotated contexts for offensive gestures and appropriate contexts for non-offensive ones. The first author manually verified and edited all generations.\nImplicit Mention Here, we test whether models default to US-centric interpretations when gestures are referenced through their neutral or positive US meanings. This setting is motivated by AI applications in travel and education, where gestures meant to communicate universal values may vary across cultures. For instance, while wishing good luck is universal, the gesture used varies across cultures; if a user asks how to wish someone good luck in Vietnam, a model should avoid suggesting US-centric gestures (e.g., fingers crossed) that may carry unintended negative connotations. We apply this evaluation to the subset of n = 10/25 gestures in the MC-SIGNS that carry benign interpretations in US contexts."}, {"title": "Model-Specific Design Considerations", "content": "Prompt Details The following prompt designs are employed for each model type:\nT2I systems: Explicit prompts include the canonical and alternate gesture names.\nLLMs: Explicit prompts specify the gesture's canonical name, alternate names, and physical description. We evaluate two settings: (1) single-turn prompts, and (2) a two-turn Chain-of-Thought setup (Wei et al., 2022) getting meaning in first-turn, and then offensiveness classification in the second.\nVLMs: Explicit and Implicit prompts have no gesture details in the textual inputs. Instead, the manually scraped images of gestures are used as visual inputs.\nEach prompt design under each type of model has two rephrases to ensure robustness of evaluation.\nExplicit Mention Evaluation Metrics We measure model understanding of gesture offensiveness through complementary metrics. For T2I systems, we examine rejection rates \u2013 the proportion of generation requests blocked by safety systems. For LLMs and VLMs, models classify gestures into four categories (Hateful, Offensive, Rude, Not Offensive), which we then map to 'Generally Offensive' and 'Not Offensive'.\nAcross all three models, we measure Recall (true positive rate; TPR) (correct identification of offensive gestures) and Specificity (true negative rate; TNR) (correct identification of non-offensive gestures). A culturally safe system should have high scores on both these measures.\nImplicit Mention Evaluation Metrics For T2I systems, we measure the error rate, i.e., the proportion of generated images that depict US-specific gesture interpretations in regions where they are offensive. For instance, we prompt the model to generate a gesture for a given intent (e.g., \u201cwishing someone luck in Vietnam\") and count it as an\""}, {"title": "Results and Analysis", "content": "For each research question, we evaluate T2I systems, LLMs and VLMs.\nRQ1: Do models accurately detect culturally offensive gestures across different regions?"}, {"title": "Related Work and Discussion", "content": "Nonverbal Behavior across Cultures Nonverbal behavior encompasses gestures, facial expressions, posture, proxemics (space use), haptics (touch), and vocalics (tone, pitch) (Knapp, 1972; Matsumoto et al., 2013)\u2013all of which vary significantly across cultures. In contact cultures like Latin America and the Middle East, people engage in closer proximity interactions than in Northern America or Northern Europe (Hall, 1963; Sorokowska et al., 2017); direct eye contact is encouraged in Western countries like France but considered disrespectful in parts of Asia, such as Japan (Argyle et al., 1994). Gestures, in particular, pose a high risk of misinterpretation. They can be broadly classified into emblematic gestures- also known as symbolic gestures-which have distinct, culture-dependent meanings (Matsumoto and Hwang, 2012), and co-verbal gestures (or speech illustrators), which accompany speech and follow more universal patterns (McNeill, 1992). Unlike co-verbal gestures, emblematic gestures function independently and are especially prone to cross-cultural misinterpretation (Matsumoto and Hwang, 2013; Kendon, 2004). Our work focuses solely on emblematic gestures.\nCultural Unawareness as a Safety Concern Current AI safety research primarily focuses on explicit threats like violence and NSFW content (Rando et al., 2022; Schramowski et al., 2022; Yang et al., 2023; Liu et al., 2023), employing strategies such as safety training (Huang et al., 2023; Shen et al., 2023), red-teaming (Ganguli et al., 2022; Liu et al., 2024c; Ge et al., 2023), safety modules (Touvron et al., 2023; Liu et al., 2024d), and risk taxonomies (Wang et al., 2023; Brahman et al., 2024; Vidgen et al., 2024). However, they often overlook cultural contexts (Sambasivan et al., 2021), as demonstrated by our findings of widespread cultural unawareness in current AI systems.\nWestern-Centric Biases in AI Systems Al systems exhibit Western-centric biases (Bender et al., 2021; Masoud et al., 2023; Prabhakaran et al., 2022), favoring Western perspectives while misinterpreting or underrepresenting non-Western cultural elements (Bhatt et al., 2022; Zhou et al., 2022; Basu et al., 2023). Our results align with these observations \u2013 all evaluated models show better detection of US-offensive gestures compared to those offensive in other cultures. These skews likely stem from biased training data (Ferrara, 2023; Suresh and Guttag, 2021) and problematic AI development practices (Mehrabi et al., 2021; Belenguer, 2022). Potential mitigation strategies include finetuning on culturally-specific datasets (Dwivedi et al., 2023; Li et al., 2024), and increased participation of local experts in model development (Kirk et al., 2024).\nContextual Reasoning for Cultural Norms Visual interpretation of cultural norms, particularly non-verbal gestures, presents unique challenges compared to traditional offensive content detection. While both language (Gehman et al., 2020; Jain et al., 2024) and visual (Arora et al., 2023; Shidaganti et al., 2023) safety systems rely on large-scale curated datasets, gesture interpretation requires nuanced cultural understanding. Recent work suggests contextual information can improve offensive content detection (Zhou et al., 2023; Yerukola et al., 2024). However, our Country+Scene evaluation reveals that additional scene context had no effect on LLMs and actually degraded T2I and VLM performance, highlighting fundamental limitations in current cross-modal contextual reasoning approaches."}, {"title": "Conclusion", "content": "We introduce MC-SIGNS, a novel dataset of 288 gesture-country pairs spanning 25 gestures and 85 countries, enabling systematic evaluation of AI systems' cultural awareness. Our assessment of T21 systems, LLMs, and VLMs reveals critical gaps: over-flagging of offensive content, poor utilization of scene descriptions, resorting to US-centric interpretation of universal concepts, and better awareness of US-offensive gestures than non-US ones. These findings highlight the need for cultural sensitivity in AI safety frameworks as these systems increasingly serve global audiences."}, {"title": "Limitations", "content": "Despite introducing the first dataset for evaluating non-verbal communication through gestures across different regions, there are certain limitations:\nLimited Gesture Coverage MC-SIGNS includes 25 gestures but does not account for interpretations specific to sign languages, such as American Sign Language (ASL), nor does it comprehensively"}, {"title": "Ethical Considerations", "content": "This work advocates for culturally inclusive and context-aware safety in AI systems, considering these ethical factors:\nRisks in Annotation Recent work has shown that exposure of potentially offensive content can be harmful to the annotators (Roberts, 2016). To mitigate these risks, we restricted each annotator to only 5-7 annotations, offered fair compensation at $15/hour, and obtained informed consent before participation. Only essential demographic information was collected, and our annotation study is also supervised by an Institutional Review Board (IRB).\nHarm Prevention and Intended Use While documenting offensive content carries inherent risks, such as the potential for misuse or the misrepresentation of cultural practices, we are committed to minimizing these risks. We believe the benefits of improving AI systems' cultural awareness and safety outweigh the potential harms (Larimore et al., 2021; Ipsos, 2016). The research is intended to contribute to the development of AI systems that are less likely to inadvertently cause cultural offense or misinterpretations. We explicitly do not endorse the use of the data for harmful purposes, including generating offensive content, exploiting cultural differences for malicious intents, or developing biased and discriminatory AI technologies."}]}