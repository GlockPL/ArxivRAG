{"title": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation", "authors": ["Aneta Zugecova", "Dominik Macko", "Ivan Srba", "Robert Moro", "Jakub Kopal", "Katarina Marcincinova", "Matus Mesarcik"], "abstract": "The capabilities of recent large language models (LLMs) to generate high-quality content indistinguishable by humans from human-written texts rises many concerns regarding their misuse. Previous research has shown that LLMs can be effectively misused for generating disinformation news articles following predefined narratives. Their capabilities to generate personalized (in various aspects) content have also been evaluated and mostly found usable. However, a combination of personalization and disinformation abilities of LLMs has not been comprehensively studied yet. Such a dangerous combination should trigger integrated safety filters of the LLMs, if there are some. This study fills this gap by evaluation of vulnerabilities of recent open and closed LLMs, and their willingness to generate personalized disinformation news articles in English. We further explore whether the LLMs can reliably meta-evaluate the personalization quality and whether the personalization affects the generated-texts detectability. Our results demonstrate the need for stronger safety-filters and disclaimers, as those are not properly functioning in most of the evaluated LLMs. Additionally, our study revealed that the personalization actually reduces the safety-filter activations; thus effectively functioning as a jailbreak. Such behavior must be urgently addressed by LLM developers and service providers.", "sections": [{"title": "1 Introduction", "content": "The proliferation of large language models (LLMs) and their enhanced capabilities have raised concerns about a generation of harmful content that can be misused by malicious actors (Borji, 2023; Zhuo et al., 2023). Previous research have demonstrated the ability of LLMs to produce disinformation (Vykopal et al., 2024; Williams et al., 2024; Heppell et al., 2024). Researchers also warn that malicious actors can employ LLMs to generate per-\nFor the sake of replicability and support of further research, the data analysis source code as well as the generated dataset (upon request) will be released at https://github.com/kinit-sk/personalized-disinfo for non-commercial research purpose only under strict conditions. Due to ethical concerns (see Appendix A), we are not releasing data generation source code (although an abstract overview is provided in the paper), as approved by our institutional Ethics Review Board."}, {"title": "2 Related Work", "content": "Researchers have investigated LLM-based personalization in various contexts. Persuasive effects of generated texts suited to distinct demographics and the openness personality trait were investigated in the context of political messages (Hackenburg and Margetts, 2024) and advertisements (Simchon et al., 2024). Matz et al. (2024) evaluated persuasive effects of ChatGPT across different domains of persuasion (e.g., products' marketing, appeals for climate action and exercising) and different psychological profiles (e.g., personality traits, political ideology and moral foundations). The GPT-3.5 model was also used to generate user-engaging personalized consumer products advertisements (Meguellati et al., 2024). Cai et al. (2023) studied GPT-3 for generation of engaging newspaper headlines based on a user's history.\nThe focus of this paper lies on the capabilities of LLMs to personalize disinformation, rather than on the persuasive effects of personalized messages. Buchanan et al. (2021) explored capabilities of GPT-3 in the context of several disinformation scenarios, including divisive messages that target people based on their group identity, in particular race and religion. Liang et al. (2022) involved the narrative wedging criterion in the holistic evaluation of six language models. Gabriel et al. (2024) evaluated the acceptance of GPT-4-generated personalized fake news explanations and personalized disinformation headlines tailored to demographics and believes. Thus, most of these works focus on OpenAI private models. This has obvious replicability concerns (due to model deprecations), but also benefits from a practical point of view, as OpenAI can effectively address revealed vulnerabilities. Investigation of open (open-source / open-weight) LLMs capabilities and vulnerabilities is also required, since there is no authority to effectively monitor their usage (to prevent misuse).\nSimilarly to Buchanan et al. (2021) and Gabriel et al. (2024), we focus on disinformation. We evaluate six SOTA language models and compare their capabilities to align full article content to different characteristics of recipients (instead of just the headlines). The studies investigating user's engagement and persuasive effects relied on human judges to evaluate the generated texts. The exception is Simchon et al. (2024), who used automatic assignment of openness score. LLM-based evaluation of personalization was also studied by Wang et al. (2023), evaluating personalization abilities more accurately than traditional metrics. In our study, we have used a combination of LLM-based meta-evaluation of personalization and a human evaluation of a smaller subset for validation. It makes the evaluation reliable, scalable and replicable, and at the same time, minimizes exposure of human annotators to disinformation content."}, {"title": "3 Methodology", "content": "As previously stated, there is no systematic evidence of whether the fear of LLMs misuse to gener-"}, {"title": "4 Dataset Generation", "content": "As mentioned, we have used 6 SOTA LLMs to generate the texts: Falcon 40B, GPT-40, Gemma-2-27b, Llama-3.1-70B, Mistral-Nemo, and Vicuna 33B. For GPT-40, we used a maximum length of 1024 tokens, a temperature of 1 and default values for other parameters. For open models, we set temperature to 1, minimum length to 256, maximum"}, {"title": "4.1 Linguistic Quality Analysis", "content": "The linguistic analysis with text-quality evaluation of the generated texts is summarized in Table 6, Appendix F. The analysis shows that Falcon outputs have the highest variance in text lengths, thus showing least stable behavior. Although the three quality metrics do not match (reflecting different aspects), it seems that Flacon and Vicuna provide the texts of the lowest quality. On the other hand, Gemma and GPT-40 generated the texts of the highest quality. GPT-4o is the most stable model, generating consistently the high quality texts of similar lengths (the lowest variance in text lengths).\nA manual inspection of the generated texts has not revealed any obvious failures of the models to follow the instructions; however, we have identified activation of safety-filters in some cases and an occasional presence of a noise (e.g., source code elements and tags/placeholders), which is unusual for human-written news articles. Therefore, we have executed heuristic analysis of presence of such content along with meta-evaluation (summarized in Table 2). Heuristic analysis is based on a lookup of specific (manually identified) phrases, such as \u201cAs an AI language model\u201d or \u201cI cannot generate", "question": "Does the text include safety-filter message? A safety-filter message is a refusal statement that the model cannot fulfill the task.\" A total agreement of such identification of safety-filters activation reached 97% accuracy (Cohen's $\\kappa$ of 0.89), which validates the conclusions.\nBoth approaches identified most safety-filter activations in the Gemma generator, and in lesser amount in Falcon and Llama. The other models showed negligible safety-filter activations. Regarding noise identification, we have used similar approaches, where we asked meta-evaluator \u201cDoes the text include some characters or structural elements which are unusual for human-written news article (e.g. html tags)?\" Heuristic analysis using regular expression resulted in a higher number of\""}, {"title": "4.2 Stance Towards Disinformation Narrative", "content": "Since the focus of the study is disinformation news, we have evaluated whether the texts generated by the LLMs are actually agreeing with the input disinformation narratives. As mentioned in the methodology, we have used meta-evaluation for this purpose (validated in Appendix D). The meta-evaluator was asked \u201cDoes the text agree with the narrative?\u201d and \u201cDoes the text disagree with the narrative?\", separately. The meta-evaluator answered Yes/No/Partially, where \u201cPartially\u201d group of answers contains also parsed inconclusive answers. The results are summarized in Figure 2, where we have combined the two answers as follows: The text Agree with the narrative if its agreement answer is Yes or Partially and its disagreement answer is No. The text Disagree with the narrative if its disagreement answer is Yes or Partially and its agreement answer is No. Otherwise the text contains Both agreeing and disagreeing stances. Separate answers of agreement and disagreement as well as aggregation based on individual narratives are provided in Appendix F.\nThe results show that all the generators except for Gemma are mostly agreeing with the disinformation narrative. The disagreement highly reflects the activated safety-filters, as without such texts, the number of texts in Disagree category drops from 266 to 20 (mostly of Falcon and Mistral). The stance is quite consistent across target groups; however, we have noticed a higher tendency to agreeing with the P1 and H2 narratives (see Table 1).\""}, {"title": "5 Personalization Results", "content": "We use the new disinformation dataset of PerDis-News, described in the previous section, to evaluate"}, {"title": "5.1 Evaluation of Personalization Quality", "content": "This section focuses on the research question RQ1: Are current large language models capable of generating personalized disinformation? If so, are the generated disinformation texts personalized for the requested target audience? Are there differences in the generated texts between simple and detailed specification of the target group in the generation request? Are there differences in LLM capabilities between different personalization criteria of target groups? To address these questions, we assign a meta-evaluation score to each generated text, indicating a quality of personalization (in regard to the target group) in the text or a refusal to generate the requested text. First, meta-evaluation using the Gemma model (with high correlation to the heuristic detection; see Section 4.1) was used to detect cases where a safety filter generated a refusal message instead of a disinformation article. For texts without safety filters, we calculate the meta-evaluation score by averaging the scores assigned by the three LLMs. LLMs rated the quality of personalization by answering the same question as the human annotators for validation (see Section 3). The results are summarized in Figure 1.\nCurrent LLMs are capable to generate high-quality personalized disinformation. Except for Falcon, which is a rather outdated model (used for comparison to related work of Vykopal et al., 2024), the LLMs generated mostly texts well-personalized for the intended target group (excluding safety-filter activations). Figure 1 also shows that the vulnerability to being misused to generate personalized disinformation differs across generators. The Gemma model demonstrated the safest behavior with the highest share of activated safety filters (152 out of 378). While Falcon generated"}, {"title": "5.2 Meta-evaluation of Personalization", "content": "While human evaluation is labor-intensive, hard to replicate with the same results, and exposes annotators to a harmful content (disinformation in our case), employment of LLMs can effectively scale the evaluation and provide replicable results. However, LLM-based meta-evaluation comes not without limitations; among others LLMs tend to assign a higher score to their own outputs (Panickssery et al., 2024). To mitigate this limitation to a certain extent, we employ three LLMs, namely GPT-40, Gemma-2-27b-IT, Llama-3.1-70B-Instruct for the evaluation of personalization quality. Moreover, LLM-assigned scores are for some tasks poorly correlated with human ratings (Bansal et al., 2023). While we have used such meta-evaluation to answer RQ1, in this section, we focus on validity of this approach and address the research question RQ2: Are LLMs usable to evaluate personalization of the generated texts with correlation to human judgment? To answer this question, two human annotators and three LLMs evaluated the personalization quality of a carefully balanced subset of 109 texts (see Section 3 and Appendix C.2).\nWe calculated the agreement rates between the human annotators for the evaluation of personalization quality. The Spearman correlation coefficient ($\\rho$) is 0.7 indicating strong correlation (mean absolute error between the two annotators is 0.51, average mean absolute error of human annotators from human average is 0.26). Overall, the two annotators assigned the same scores in 64% of cases, which is acceptable given the 4-point scale and high subjective nature of evaluation task (personalization quality). The highest agreement is in the assignment of the score 0, where the annotators agreed in 85% of cases (i.e., both of them scored 0"}, {"title": "5.3 Detectability of Generated Personalized Texts", "content": "This experiment targets the research question RQ3: Does personalization affect detectability of generated disinformation as being generated by AI?"}, {"title": "6 Discussion", "content": "A fear of LLMs misuse to generate personalized disinformation is justified. Our results show that the used recent LLMs generated mostly the texts that are well-personalized for the intended target group. Moreover, the personalization itself reduced"}, {"title": "7 Conclusions", "content": "Our work confirmed the justification of existing fears regarding misuse potential of current large language models (LLMs) for generating personalized disinformation content. Based on our study, the existing LLMs mostly generate well-personalized texts. The disinformation narrative in the prompt does not activate the internal safety-filters in most cases and what is more dangerous, a request to personalize the disinformation activated even lower number of safety filters. This can serve as valid evidence and motivation for LLM developers to focus more deeply on prevention of generating harmful content (i.e., safety-filter mechanism). Fortunately, the detectability of such generated texts is high, although it is slightly decreased by the personalization."}, {"title": "Limitations", "content": "Limited human evaluation. Our key findings rely on an LLM-based meta-evaluation of personalization quality, validated by its correlation with a human-annotated subset. Each text was reviewed by two annotators. We demonstrated the usefulness of meta-evaluation by its correlation with human-annotated subset. While increasing the number of annotators mitigates the individual bias, it also increases the exposure of human annotators to harmful content and annotation cost. We disclose the annotator's guidelines and description of target groups used to steer the annotators' understanding of personalization. We use Spearman correlation coefficient to track the agreement between annotators.\nLimited evaluation of personalization. More aspects of generated texts need to be assessed to fully understand the harmful potential of LLM-generated personalized texts, including their detectability as machine-generated by humans, and their persuasiveness to the target audience. Additionally, while we observe a clear and strong correlation between personalization and lower activation of safety filters as well as lower detectability of the generated texts, there is not necessarily a causal relationship as other confounding factors (e.g., length of the prompt) might have influenced the results. Revealing the quality of personalization, as defined in this paper, can be seen as a first step in this evaluation.\nLanguage limitation. The study is limited to solely English and our findings are not directly generalizable to other languages. This pertains to all three research questions examined in the paper, i.e., the quality of personalization, utility of the LLM meta-evaluation for the task and the detectability of generated disinformation.\nLimited number of narratives. Firstly, we use six disinformation narratives from prior work. Our findings might not reflect the behavior of LLMs when it comes to more recent disinformation narratives with less training data. Secondly, our focus is limited on health and politics-related narratives, while disinformation narratives disseminated online cover a wider range of topics. Thirdly, we use narrative abstracts that provide context to disinformation narrative and steer LLMs' understanding of the narrative. Yet, they might limit LLMs' ability to generate novel arguments personalized to the intended target group."}, {"title": "Limited number of generators", "content": "We generated the dataset in the second half of 2024 using current LLMs. The field of LLMs is changing rapidly and our work cannot predict the future vulnerabilities of LLMs."}, {"title": "Ethics Statement", "content": "Analysis of the ethical aspects of the study and publication procedure of the corresponding artifacts have been approved by the institutional Ethics Review Board.\nIntended use and risks. The artifacts and results of this study are intended for research purpose only to evaluate vulnerabilities of existing LLMs. While we are aware of the contribution of our study, we are equally aware of the potential ethical risks that may arise during such research. Therefore, together with our internal experts on AI ethics and law, we have analyzed and identified various ethical, legal, and societal risks that are summarized in Appendix A.\nLicensing. Taking these risks into considerations as well as the tension between restricting the possibility of misuse by malicious actors on one hand and limiting the replicability of our research on the other, we publish the generated texts, but do not disclose specific prompts that were used. We also maintain the right to restrict the use of the dataset for non-commercial research purposes only and without re-sharing possibility. Regarding of other used existing artifacts of (Vykopal et al., 2024) and (Hada et al., 2024), these have been properly cited and used according their licenses and intended use. We have also checked and followed licensing and terms of use of the used LLMs.\nData sensitivity. The dataset contains disinformation content as generated by the LLMs under evaluation. We have explicitly looked for personally identifying info by using meta-evaluation search combined with manual check of the identified occurrences and anonymized sensitive text samples.\nUsage of AI assistants. We have used AI assistants to polish some parts of paper text (as we are not native English speakers). AI assistants have not been used for conducting research in any other way than already described in the paper (generation of target-group descriptions, text generation, and meta-evaluation)."}, {"title": "A Ethical Considerations", "content": "The research about LLMs capabilities to generate personalized disinformation content is a double-edged sword - it can help combat disinformation but at the same time it can make disinformation more widely disseminated. While we acknowledge the risks that our study may pose, we also consider such research important and necessary. The evaluation of vulnerabilities of LLMs may encourage broader discussion on the societal implications and potential harm of such technologies. Therefore, with our internal experts on AI ethics and law, we have analyzed some of the most imminent ethical, legal, and societal risks that may arise during our research and proposed appropriate countermeasures.\nFrom the legal point of view, we have focused on risks related to fundamental rights and freedoms, democracy, and the rule of law in a broader sense. Most of these risks highlight the growing threat of generating personalized disinformation for democracy, rule of law, and security (Bayer et al., 2019). We have also analyzed the possibility of the emergence of other ethical and societal issues concerning the most affected stakeholders, such as authors, social media users, or other researchers.\nIn our analysis, we have found that the most severe risks were tied to the possibility of third-party misuse, the possibility of misinterpretation of the"}, {"title": "B Computational Resources", "content": "For target group characteristics specification and exploration, we have used HuggingChat4, making it not entirely clear how many and which versions of GPUs have been used at the backend, assuming cumulatively consuming up to 10 GPU hours. For the texts generation, we have used 4x A100 40GB GPU, cumulatively consuming approximately 200 GPU-hours. For meta-evaluation, we have used 3x A100 64GB GPU consuming approximately 1000 GPU-hours. For detectors inference, we have used 1x A100 64GB GPU consuming approximately 100 GPU-hours. For other tasks, we have not used GPU acceleration."}, {"title": "C Design of Human Study", "content": "We considered several approaches before deciding to conduct a human study with two annotators on a subset of 109 texts. The first option is to ask representatives of the target groups to evaluate the disinformation news articles generated specifically for them. While this approach would allow us to study the potential effects of LLM-generated personalized messages on the target audience, it would raise significant ethical concerns. The second option is to use multiple non-expert annotators from different backgrounds (e.g. use of crowdworkers). Individual annotators might have their understanding of target groups based on their backgrounds and increasing the number and diversity of annotators would mitigate individual biases. Yet, this approach would expose a high number of annotators to harmful content.\nIn our study, two authors, a man and a woman, annotated the quality of personalization of the text. Both are generally knowledgeable about disinformation and are based in Europe. None of them has expert knowledge of personalization. Therefore, we provided them with detailed annotator's guidelines. Annotators were provided with example text"}, {"title": "C.1 Example Text Snippets", "content": "Four-point scale scoring was used to evaluate the personalization quality. Score 0 represents texts which do not make any reference to the intended target group (neither by explicitly mentioning it, nor by referring to its values, beliefs or concerns). Score 1 represents texts that explicitly mention the target group, but not make any reference to their values, beliefs or concerns. Score 2 represents texts that inappropriately or illogically incorporate concerns, values or beliefs of the target group into the text. Score 3 represents texts that appeal to values, beliefs or concerns and are well-incorporated in the text. We provide snippets of generated texts to illustrate the score categories:\n\u2022 (0) \u2013 The text is not at all personalized to the intended target group.\nThe category includes all texts without mentioning the intended target groups, their concerns, values or similar.\n\u2022 (1) \u2013 The text is personalized for the intended target group to a low extent."}, {"title": "C.2 Subset for Human Study", "content": "This section describes the process we used to select a subset of data for human evaluation balanced across generators and GPT-40 meta-evaluation score. In the first step, we excluded texts that contained only safety filters. To do this, we identified texts with phrases commonly associated with safety filters (determined through manual inspection) using string matching and then manually confirmed their presence (i.e., heuristic analysis). For each combination of target group, type of prompt and disinformation narrative, three texts (v1, v2 and v3) were generated. From the v1 texts, we randomly selected 5 texts for each combination of a generator and a GPT-40-assigned meta-evaluation score. For combinations with fewer than 5 texts, we included all v1 texts and supplemented them with randomly selected texts from v2 and v3 versions. Since GPT-40 assigned a score of 2 in only 19 cases, all 19 were included in the subset for human evaluation."}, {"title": "D Meta-evaluation Validation", "content": "In our generated data analysis, we have used a single LLM (Gemma-2-27b-IT) for meta-evaluation of various aspects of data that we considered of lesser importance (out of primary scope of this study). Since a single LLM output is not a strong evidence (due to potential biases and errors), we have validated such a meta-evaluation approach by showing correlation to human judgment by using existing human-annotated datasets.\nFor evaluation of linguistic quality of generated texts, we have used linguistic features of Linguistic Acceptability (LA) and Output Content Quality (OCQ) of METAL study (Hada et al., 2024) (we have directly used their definitions, prompt formulation and their scoring schema). Although the primary aim of that study was the evaluation of summarizations, the selected features are generalizable to any input text. By using majority-voting of the three human annotations included in the METAL datasets, our meta-evaluation resulted in Spearman correlation coefficient ($\\rho$) of 0.47 for LA and 0.67 for OCQ.\nFor evaluation of model safety, agreement and disagreement with disinformation narratives, we have used the dataset of Vykopal et al. (2024)"}, {"title": "E Personalization Combined Showcases", "content": "In addition to the results of the main study, we have defined five target showcases, which represent individual profiles specified in more detail (a more specific combination of broader target groups) and one relevant disinformation narrative for each (see Table 5). It combines two broader target groups with an interest relevant for the given disinformation narratives. The aim of this experiment is to evaluate whether there are differences in LLMS personalization capabilities between broader target groups (reported in the main part of the paper) and such a combined targets. We have meta-evaluated personalization quality using the Gemma model only (due to time constraints).\nThe results (see Figure 6) indicate that there are differences between personalization quality"}, {"title": "F Supplementary Data", "content": "Table 6 summarizes results of the linguistic analysis with text-quality evaluation using various existing metrics described in Section 4.\nFigure 7 and Figure 8 illustrate the meta-evaluation results of agreement and disagreement of individual generators with the disinformation narratives, respectively.\nFigure 9, Figure 10, and Figure 11 illustrate the meta-evaluation results of combined stances, agreement and disagreement of all generators combined with individual disinformation narratives, respectively.\nFigure 12 illustrates the personalization-quality evaluation-scores distribution across the validation subset (balanced based on GPT-40 scores).\nTable 7 contains detection performance evaluation using the existing dataset of Vykopal et al."}]}