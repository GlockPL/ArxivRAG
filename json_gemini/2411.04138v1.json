{"title": "NetworkGym: Reinforcement Learning Environments\nfor Multi-Access Traffic Management in Network\nSimulation", "authors": ["Momin Haider", "Arpit Gupta", "Ming Yin", "Menglei Zhang", "Jing Zhu", "Yu-Xiang Wang"], "abstract": "Mobile devices such as smartphones, laptops, and tablets can often connect to\nmultiple access networks (e.g., Wi-Fi, LTE, and 5G) simultaneously. Recent ad-\nvancements facilitate seamless integration of these connections below the transport\nlayer, enhancing the experience for apps that lack inherent multi-path support.\nThis optimization hinges on dynamically determining the traffic distribution across\nnetworks for each device, a process referred to as multi-access traffic splitting.\nThis paper introduces NetworkGym, a high-fidelity network environment simulator\nthat facilitates generating multiple network traffic flows and multi-access traffic\nsplitting. This simulator facilitates training and evaluating different RL-based\nsolutions for the multi-access traffic splitting problem. Our initial explorations\ndemonstrate that the majority of existing state-of-the-art offline RL algorithms\n(e.g. CQL) fail to outperform certain hand-crafted heuristic policies on average.\nThis illustrates the urgent need to evaluate offline RL algorithms against a broader\nrange of benchmarks, rather than relying solely on popular ones such as D4RL.\nWe also propose an extension to the TD3+BC algorithm, named Pessimistic TD3\n(PTD3), and demonstrate that it outperforms many state-of-the-art offline RL algo-\nrithms. PTD3's behavioral constraint mechanism, which relies on value-function\npessimism, is theoretically motivated and relatively simple to implement.", "sections": [{"title": "Introduction", "content": "There exists a general lack of standardized benchmarks for reinforcement learning (RL) in the\ndomain of computer networking. Whereas RL has shown promise in addressing various challenges in\ncomputer networking, such as congestion control, routing, and resource allocation, the field lacks\nwidely accepted benchmarks that would facilitate systematic evaluation and comparison of different\nRL approaches. Hence, we propose NetworkGym, a high-fidelity, end-to-end, full-stack network\nSimulation-as-a-Service framework that leverages open-source network simulation tools, such as\nns-3 Henderson et al. [2008]. Furthermore, NetworkGym offers a closed-loop machine learning (ML)\nalgorithm development and training pipeline via open-source gym-like APIs. The components of\nNetworkGym achieve the following objectives:"}, {"title": "Related Work", "content": "RL-based Network Optimization. RL has been used for network optimization in a variety of\ncontexts Yang et al. [2024], Mao et al. [2017], Jay et al. [2019], Jamil et al. [2022], Zhang et al.\n[2023], Xia et al. [2022], Gilad et al. [2019], Boyan and Littman [1993], Wei et al. [2022], He et al.\n[2017], Liang et al. [2019], Sadeghi et al. [2017]. For example, Yang et al. Yang et al. [2024] use\noffline RL on a mixture of datasets from different behavior policies to maximize throughput via\nradio resource management. Additionally, Mao et al. Mao et al. [2017] construct a system that can\ngenerate adaptive bitrate algorithms to maximize user quality of experience by training a deep RL\nmodel on client video player observations. Jay et al. Jay et al. [2019] employ deep RL to solve the\ncongestion control problem, whereas Jamil et al. Jamil et al. [2022] use deep RL to dynamically\ndetermine the optimal number of TCP streams in parallel to maximize throughput while avoiding\nnetwork congestion. Despite the existing works, our use of offline RL for multi-access traffic splitting\nis novel and the first of its kind.\nRL Benchmarks. A wide variety of online and offline RL benchmarks have been proposed in\nthe research community in order to properly evaluate the performance and generalization of RL\nalgorithms. Popular online RL benchmarks include the OpenAI Gym Brockman et al. [2016], Atari\n2600 games bel [2013], and Mujoco Todorov et al. [2012]. These sets of environments offer a diverse\nselection of tasks to choose from, mostly involving classic control, continuous control of multi-joint\nbodies, or video game playing with high-dimensional input spaces. Common offline RL benchmarks\ninclude D4RL Fu et al. [2020] and RL Unplugged Gulcehre et al. [2020], which provide similar\nenvironments to those in the referenced online RL benchmarks. Recent efforts have been made to\nconsolidate these offline RL benchmarks and have also reinforced the finding that the success of"}, {"title": "Problem Setup", "content": "Markov Decision processes. Let $(S, A, R, p, \\gamma)$ define a Markov decision process (MDP) where $S$ is\nthe state space, $A$ is the action space, $R : S \\times A \\rightarrow R$ is the scalar reward function, $p : S \\times A \\rightarrow \\Delta_S$\nis the transition dynamics model where $\\Delta_S$ is a set of probability distributions over $S$, and $\\gamma\\in [0, 1]$\nis the discount factor. $S$ and $A$ can both potentially be infinite or continuous. Typically, an RL agent\nchooses actions via a deterministic policy $\\mu : S \\rightarrow A$ or a stochastic policy $\\pi : S \\rightarrow \\Delta_A$ where $\\Delta_A$ is\na set of probability distributions over $A$. The goal of an RL agent is to find a policy that maximizes the\nexpected discounted return $E_{\\pi} [\\sum_{t=0} ^\\infty \\gamma^t r_t | s_0 = s]$ from the starting state distribution. We denote the\nstate-action value function with respect to policy $\\pi$ as $Q^\\pi(s, a) = E_{\\pi} [\\sum_{t=0} ^\\infty \\gamma^t r_t | s_0 = s, a_0 = a]$.\nOffline RL. In the offline RL setting, we assume that the agent does not have the ability to interact\nwith the environment and instead has access to an offline dataset $D = \\{(s_k, a_k, r_k, s'_k)\\}_{k=0}^{K-1}$ collected\nby some unknown data-generating process (for example, a collection of different behavior policies).\nThis makes the offline RL setting more challenging than the online RL setting. An online RL agent\nthat overestimates the Q-values at specific state-action pairs can quickly adapt after being punished\nfor taking those actions in the environment, but an offline RL agent does not have the ability to\ninteract with the environment. This leads to the resulting problem of distribution shift in offline RL,\nwhich occurs due to extrapolation error in the Q-function approximators on state-action pairs that are\npoorly represented by those in the offline dataset."}, {"title": "Experiments", "content": "Experimental Setup. We test PTD3 and other state-of-the-art offline RL algorithms on a simplified\nconfiguration of the NetworkGym multi-access traffic splitting environment. The relevant environ-\nment configuration file is included in Appendix D. At initialization of each environment, four UEs\nare randomly stationed 1.5 meters above the x-axis between x = 0 and x = 80 meters. From there,\nthey begin to bounce back and forth in the x-direction at 1 m/s for the entire duration of an episode.\nThe Wi-Fi access points are stationed at (x, z) = (30m, 3m) and (x, z) = (50m, 3m), respectively\nwhile the LTE base station lies at (x, z) = (40m, 3m). Figure 2 illustrates this environment setting.\nAlthough this setup is deceptively simple and unrealistic due to the relative locations between UEs\nand access points as well as the degenerate movement of the UEs, it provides a simple enough testing\nground for offline RL on the GMA traffic splitting protocol while still containing some amount of\ndynamic behavior and resource competition between UEs.\nSince the multi-access gateway is connected to all four UEs, the gateway can send traffic splitting\ncommand messages to each of the UEs in a centralized manner via the GMA protocol while taking\ninto account network information across all UEs. Therefore, we represent the state as a 14 \u00d7 4 matrix\nwhere we have 14 network measurement values for each user from the previous time interval and we\nrepresent the action as a 1 \u00d7 4 row-vector, where each element represents the desired traffic splitting\nratio during the next time interval for a specific user. Although the traffic splitting ratio for each"}, {"title": "Discussion", "content": "Offline RL Algorithm Performance. First, we note that of the 7 off-the-shelf offline RL algorithms\ntested in our NetworkGym environment setting, only 2 of them were able to significantly outperform\nthe average performance of the heuristic baseline algorithms. Furthermore, in the case of both of\nthese algorithms, they were only able to do so when we disabled state normalization based on the\noffline dataset, a feature that is included by default when training these offline algorithms. Therefore,\nusing the default hyperparameters for every tested off-the-shelf offline RL algorithm, none of these\nalgorithms could significantly outperform the heuristic baseline algorithms on average. Furthermore,\nin the case of a few of these algorithms, such as EDAC and LB-SAC, the performance across\ndifferent datasets is erratic, resulting in a significantly lower average performance overall, compared\nto the heuristic baseline algorithms. While these algorithms are known to exhibit state-of-the-art\nperformance on D4RL-like tasks, it has been noted that the performance of these algorithms in\npractice is unstable across environments of varying characteristics Tarasov et al. [2024]. These\nfindings strongly suggest that it would be imprudent to deploy such algorithms trained on a similar\ntask into the real world, even if they were trained on datasets collected from real interactions.\nSince our implementation of PTD3 is, on average, able to significantly outperform not only the\nheuristic baseline policies, but also several existing state-of-the-art offline RL algorithms, this\nsuggests that the poor performance across existing algorithms is not due to a lack of coverage across\ndatasets, but rather the lack of diversity and breadth of testing environments for these algorithms.\nWhile the D4RL benchmark has become a standard for assessing offline RL performance, it is\nessential to recognize its limitations. Algorithms that are touted as state-of-the-art based on their\nperformance on D4RL may not generalize well to other, perhaps more complex or varied, scenarios.\nWe have shown that many advanced offline RL algorithms have the potential to fail catastrophically\nwhen deployed in different contexts or faced with unfamiliar environments. Therefore, to ensure\nrobustness and reliability, it is crucial to test offline RL algorithms across a wider array of datasets\nand environments. This broader testing approach helps to uncover potential weaknesses and provides\na more comprehensive understanding of an algorithm's capabilities and limitations. We hope that by\nexpanding the scope of testing beyond popular benchmarks like D4RL and RL Unplugged, researchers\nand practitioners can better gauge the true potential and practicality of their offline RL solutions.\nBehavioral Cloning vs. State-Action Value Function Pessimism. In testing PTD3 (\\alpha = 1.0)\non all three offline datasets with varying values of \\beta, we note that while the performance on the\nsystem_default dataset improves up to a point as \\beta increases then drops off, the performance on\nthe utility_logistic dataset improves substantially even up to values as high as \\beta = 300.0. In"}, {"title": "Conclusion", "content": "In this work, we present NetworkGym, a high-fidelity gym-like network environment simulator that\nfacilitates multi-access traffic splitting. NetworkGym seamlessly aids in training and evaluating\nonline and offline RL algorithms on the multi-access traffic splitting task in simulation. In our\nsimulated experiments, we demonstrate that existing state-of-the-art offline RL algorithms fail to\nsignificantly outperform heuristic policies on this task. This highlights the critical need for a broader\nrange of benchmarks across multiple domains for offline RL algorithm evaluation. On the other hand,\nour proposed PTD3 algorithm significantly outperforms not only heuristic policies, but also many\nstate-of-the-art offline RL algorithms trained on heuristic-generated datasets. These findings pave\nthe way for more effective offline RL algorithms and demonstrate the potential of PTD3 as a strong\ncontender among existing solutions. Future research should consider evaluating offline RL algorithms\non networking-specific tasks alongside other benchmarks to foster the development of more robust\nand versatile solutions."}]}