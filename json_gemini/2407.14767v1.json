{"title": "I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case Study on Text-to-SQL Generation", "authors": ["Cheng-Kuang Wu", "Zhi Rui Tam", "Chao-Chung Wu", "Chieh-Yen Lin", "Hung-yi Lee", "Yun-Nung Chen"], "abstract": "In this study, we explore the proactive ability of LLMs to seek user support, using text-to-SQL generation as a case study. We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help and examine their performance with varying levels of information availability. Our experiments reveal that without external feedback, many LLMs struggle to recognize their need for additional support. Our findings highlight the importance of external signals and provide insights for future research on improving support-seeking strategies.", "sections": [{"title": "1 Introduction", "content": "The impressive instruction-following (Wei et al., 2021) abilities of large language models (LLMs) have enabled their out-of-the-box usage to solve problems. However, these models generate hallucinated content (Rawte et al., 2023) or incorrect predictions in their efforts to fulfill user instructions, which undermines their reliability.\nWhen LLMs generate incorrect outputs for a given instruction, the issue can be examined from multiple perspectives. One is that the model simply lacks the competence to satisfy the instruction, suggesting a straightforward solution: enhancing the model's capabilities, which is the focus of most previous research. Another is that the model could actually solve the task with additional support. For instance, Pourreza and Rafiei (2023) found that models often fail due to underspecified natural language queries. Similarly, Li et al. (2024) showed that while GPT-4 struggles initially, its performance can improve by up to 20.01% with human-annotated external knowledge. In such cases, models should proactively seek help rather than attempting to satisfy instructions with insufficient information."}, {"title": "2 Formulation for Seeking Support", "content": ""}, {"title": "2.1 General Setup", "content": "Consider an LLM $f$ parameterized by $\\theta$, along with a prompt template $p(\\cdot)$. Given a natural language instruction $x$, we use $z$ to represent support, which should enhance the LLM's ability to fulfill $x$. Formally, $\\hat{y}_z = f(p(x, z) | \\theta)$ is more likely to satisfy $x$ compared to $\\hat{y} = f(p(x) | \\theta)$. We denote the \"ask for support\" signal emitted by the LLM as $\\hat{a}$, defined as a confidence score in the range [0, 1], where 1 indicates an absolute need for support. A threshold $\\tau$ is then used to determine whether to request $z$. In practice, $\\hat{a}$ could also be a natural language request specifying the type of support needed by the LLM, which we leave for future work."}, {"title": "2.2 Evaluation", "content": "To measure the trade-off between performance improvement from user support and user burden, we need 2-dimensional evaluation. One dimension is the user burden (B), which we define as the proportion of instances where the LLM ask for support:\n$B = \\frac{N_{ask}}{N}$\nwhere $N_{ask}$ is the number of instances where the LLM asks for support, and $N$ denotes the total number of instances in the test set. The other dimension is the performance improvement ($\\Delta$, Delta):\n$\\Delta = \\frac{1}{N_{ask}}\\sum_{i=1}^{N_{ask}} (h(y_i, \\hat{y}_{i,z}) - h(y_i, \\hat{y}_i))$\nwhere h(\u00b7) is the evaluation function of a given task, which takes ground truth $y_i$ and model output $\\hat{y}_i$ as arguments ($\\hat{y}_{i,z}$ is an output with the help of z). Inspired by the idea behind the ROC curve (Majnik and Bosni\u0107, 2013), we illustrate this trade-off with a graph, where the performance curve is plotted by adjusting the threshold $\u03c4$ from high to low along the x-axis. We refer to this curve as Delta-Burden Curve (DBC) (see the leftmost subplot of Figure 2)."}, {"title": "2.3 Methods for Seeking Support", "content": "We design a prompt template $p_{ask}(\\cdot)$ to enable LLMs to request support by $\\hat{a} = s(f(p_{ask}(w) | \\theta))$. Here, $w$ represents the textual information that the LLM $f$ uses to determine whether it needs to seek support, and $s$ is the scoring function that converts the probability distribution of output tokens into a confidence score $\\hat{a} \\in [0, 1]$. We propose methods with varying compositions of $w$ to explore the information LLMs require to achieve better trade-off under DBC. Note that $p_{ask}$ remains the same across all methods to minimize prompt engineering. An overview of these methods is shown in Figure 1.\nDirect Ask (DA): $w = (db, x)$, composed of database schema $db$ and user data requirement $x$.\nWrite then Ask (WA): $w = (db, x, \\hat{y})$, where the LLM generates the SQL code $\\hat{y} = f(p(db, x) | \\theta)$ first and then use this self-generated output as the additional information in $w$.\nExecute then Ask (EA): $w = (db, x, \\hat{y}, r)$, where the execution results $r$ is returned by the database by executing LLM-generated SQL $\\hat{y}$."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Dataset", "content": "We use BIRD (Li et al., 2024), which includes human-annotated external knowledge that serves as $z$. For example, $z$ might be domain-specific knowledge, such as how to calculate financial indicators from database values. The instruction $x$ represents the users' data requirements, paired with the ground truth SQL $y$. It uses Execution Accuracy (EX) as the evaluation metric, where $h(y_i, \\hat{y}_i)$ is defined as $1(\\hat{r_i} = r_i)$. Here, $r_i$ is the SQL execution result of $y_i$, and $\\hat{r_i}$ is the execution result of $\\hat{y}_i$. Simply put, EX is the proportion of testing instances where $r_i$ and $\\hat{r_i}$ are identical."}, {"title": "3.2 Implementation", "content": "For open-weight LLMs, we use WizardCoder-34B (Luo et al., 2023), Llama-3-70b-chat, DeepSeek-Coder-33B (Guo et al., 2024), and Mixtral-8x22B (Jiang et al., 2024) for diversity of different LLM families. For closed-source LLMs, we use gpt-3.5-turbo-0125, gpt-4-turbo-2024-04-09, and gpt-4o-2024-05-13 (OpenAI, 2023). The prompt $p_{ask}(w)$ (included in Appendix A) instructs the model to output a single token Yes/No to indicate whether it needs support. We define the scoring function s as the softmax of Yes over log probabilities of Yes and No to derive $\\hat{a} \\in [0, 1]$."}, {"title": "4 Main Results", "content": "Using the formulation in Section 2.2, we quantify the performance of different methods with the Area Under Delta-Burden Curve (AUDBC) in Table 1. Visualized DBCs are available in the leftmost subplots in Figure 2. Note that AUDBC should only be compared between methods under the same LLM, as it is normalized to the range of [0, 1] by dividing the area under the curve by the maximum square area, which depends on the scale of $\\Delta EX$ and differs across LLMs, as shown in Table 2.\nThere are three major findings: (1) Execution then Ask consistently improves the performance-burden trade-off for LLMs, although Llama-3-70b-chat fails to outperform the random baseline. (2) The leftmost four LLMs in Table 1 do not surpass the random baseline without the assistance of $r$, indicating that many current LLMs still struggle to determine the need for support based on $x$ and $\\hat{y}$ alone. (3) Despite this, the rightmost three LLMs outperform the random baseline with the Write then Ask $(x, \\hat{y})$ or even Direct Ask $(x)$ methods. Nevertheless, the inclusion of $r$ remains beneficial for further enhancing the trade-off between performance improvement and user burden. Practical implications of the third point include the potential for cost savings by trading off the execution of $\\hat{y}$ to obtain $r$ in certain resource-constrained scenarios."}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Analysis on the Delta-Burden Curves", "content": "The Delta Burden Curves (DBCs) plotted in Figure 2 quantify the following practical question: Under the same user burden, which method can achieve more performance boost? To further analyze how this performance boost is achieved, we decompose the concept into two abilities:\n1.  The ability to ask for support when the LLM cannot satisfy the instruction originally.\n2.  The ability to utilize support effectively to flip the incorrect output to the correct output.\n1.  For the first ability, we introduce the following metrics inspired by the precision-recall trade-off: Precision of Asking for Support ($P_{ask}$) When the LLM asks for support, it should be the case that the LLM cannot satisfy the instruction originally, or it would cause unnecessary user burden:\n$P_{ask} = \\frac{\\#(Ask for Support \\& Originally Wrong)}{\\# Ask for Support}$\nRecall of Asking for Support ($R_{ask}$) When the LLM is not able to satisfy the instruction originally, it should identify this need and ask for support:\n$R_{ask} = \\frac{\\#(Ask for Support \\& Originally Wrong)}{\\# Originally Wrong}$\nPR Curve of Asking for Support Similar to how DBC is plotted, one can also adjust the threshold $\\tau \\in [0, 1]$ from high to low along the x-axis to plot the Precision-Recall Curve of Asking for Support.\n2.  For the second ability, we introduce Flip Rate: Flip Rate: This metric is calculated as the proportion of instances where the LLM's initially incorrect answers were corrected after receiving support, divided by the total number of instances where support was requested. Formally, it is defined as:\n$FR= \\frac{1}{N_{ask}}\\sum_{i=1}^{N} (h(y_i, \\hat{y}_{i,z}) - h(y_i, \\hat{y}_i))$"}, {"title": "5.2 LLMs without Access to Log Probabilities", "content": "Given that not all LLMs provide access to token log probabilities, we discuss how our method can be adapted for these \u201cblack-box\" models. We modify the prompt template $p_{ask}$ to $p_{verb}$, which instructs the LLM to output the verbalized confidence score $\\hat{a}$ directly by specifying the range and meaning of $\\hat{a} \\in [0, 1]$ in $p_{verb}$ (attached in Appendix A.2). In addition to the seven LLMs mentioned in Section 3.2, we also include two black-box models: gemini-1.0-pro-001 and claude-3-haiku-20240307. The results, shown in Table 3, indicate that using verbalized confidence scores generally degrades performance for most LLMs. However, it remains a promising alternative for black-box LLMs such as Gemini and Claude to surpass the random baseline."}, {"title": "6 Related Work", "content": "The ability of LLMs to identify the need for support relies on their well-calibratedness (Kadavath et al., 2022), which refers to their capacity to recognize uncertainty. Previous studies focus on enhancing the calibration of predictions (Xiao et al., 2022; Kuhn et al., 2023), or using verbalized token probabilities to achieve better calibration (Tian et al., 2023). Our work extends this line of research by exploring how LLMs can effectively seek user support by leveraging their well-calibrated property. The major distinction between this and existing calibration studies lies in extending the focus from identifying the uncertainty to utilizing support."}, {"title": "7 Conclusion", "content": "We propose a framework for LLMs to seek support, and evaluate methods on Text-to-SQL generation. Our findings suggest the importance of external signals, such as SQL execution results, in helping LLMs better manage performance-burden trade-off. We further decompose DBC into the ability of identify the need for support and the ability to utilize the support. Future works may explore a broader range of tasks or develop methods to improve both the identification and utilization of support."}, {"title": "8 Limitations", "content": ""}, {"title": "8.1 Task Coverage", "content": "The scope of our experiments is limited to the Text-to-SQL task. While this task provides a useful case study for evaluating LLMs' ability to seek and utilize support, it does not encompass the full range of potential applications for LLMs. Future work should extend the evaluation to a broader set of tasks to ensure the generalizability of our findings."}, {"title": "8.2 Types of Support", "content": "In this study, we primarily focus on a single type of support: human-annotated external knowledge. However, there are many other types of support that LLMs might require. Future works could explore how LLMs can request and utilize these various forms of support to enhance their performance."}, {"title": "8.3 Dependence on External Feedback", "content": "Our findings indicate that LLMs significantly benefit from external signals, such as SQL execution results. However, this reliance on external feedback may not always be feasible in practical applications, where immediate execution or access to external data might be limited. Developing methods that enable LLMs to better manage without such feedback remains an important area for future exploration."}, {"title": "A Prompt Templates", "content": "We include the prompt templates used in this work."}, {"title": "A.1 Prompt for Seeking Support", "content": "The prompt template $p_{ask}(w)$ used to instruct LLMs for seeking support is as follows:\nYou are currently doing the text-to-SQL task. Based on the information provided ({items}), you have to determine whether additional hints are required for you to generate the SQL correctly to answer the user's question. You should only ask for additional hints when you actually need them, since you will also be evaluated based on the number of times you ask for hints, which would be provided by the user.\ninformation provided (enclosed by triple backticks):\n666\n{information}\n666\nAnswer a single word Yes if you need hints (since the information provided is not enough to generate SQL correctly). Answer a single word No if hints are not required (since you are already confident to generate SQL). Do you need additional hints? Answer (Yes / No):"}, {"title": "A.2 Prompt for Seeking Support (Verbalized)", "content": "The prompt template for generating verbalized probabilities in LLMs without access to token log probabilities (e.g., Gemini and Claude families):\nYou are currently doing the text-to-SQL task. Based on the information provided ({items}), you have to determine whether additional hints are required for you to generate the SQL correctly to answer the user's question. You should only ask for additional hints when you actually need them, since you will also be evaluated based on the number of times you ask for hints, which would be provided by the user.\ninformation provided (enclosed by triple backticks):\n666\n{information}\n666\nDo you need additional hints? Provide the precise probability that you need hints (closer to 0 means you don't need hints, closer to 1 means you need hints).\nGive ONLY the precise probability to five decimal places (format: 0.abcde, where abcde can be different digits), no other words or explanations are needed.\nThe prompt template is similar to the original template shown in A.1, except that the last few sentences are modified."}, {"title": "A.3 Prompt for Generating SQL Code", "content": "The prompt template $p(\\cdot)$ for converting user data requirement $x$ into SQL code is as follows:\n{db_schema}\nUsing valid SQLite, answer the following questions for the tables provided above.\nQuestion: {question}\nNow, generate the correct SQL code directly in the format of \u201csql\\n<your_SQL_code>\\n\u201c:\nIf user support $z$ is provided (i.e., when LLMs ask for support), the prompt template is slightly modified as follows:\n{db_schema}\nExternal Knowledge: {support}\nUsing valid SQLite, answer the following questions for the tables provided above. You can use the provided External Knowledge to help you generate valid and correct SQLite.\nQuestion: {question}\nNow, generate the correct SQL code directly in the format of \u201csql\\n<your_SQL_code>\\n\u201c:"}, {"title": "B Performance Curves", "content": "We present visualizations of all performance curves in Table 3, 4, 5, 6, 7, 8, and 9."}]}