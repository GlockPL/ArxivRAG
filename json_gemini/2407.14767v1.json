{"title": "I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case Study on Text-to-SQL Generation", "authors": ["Cheng-Kuang Wu", "Zhi Rui Tam", "Chao-Chung Wu", "Chieh-Yen Lin", "Hung-yi Lee", "Yun-Nung Chen"], "abstract": "In this study, we explore the proactive ability of LLMs to seek user support, using text-to-SQL generation as a case study. We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help and examine their performance with varying levels of information availability. Our experiments reveal that without external feedback, many LLMs struggle to recognize their need for additional support. Our findings highlight the importance of external signals and provide insights for future research on improving support-seeking strategies.", "sections": [{"title": "1 Introduction", "content": "The impressive instruction-following (Wei et al., 2021) abilities of large language models (LLMs) have enabled their out-of-the-box usage to solve problems. However, these models generate hallucinated content (Rawte et al., 2023) or incorrect predictions in their efforts to fulfill user instructions, which undermines their reliability.\nWhen LLMs generate incorrect outputs for a given instruction, the issue can be examined from multiple perspectives. One is that the model simply lacks the competence to satisfy the instruction, suggesting a straightforward solution: enhancing the model's capabilities, which is the focus of most previous research. Another is that the model could actually solve the task with additional support. For instance, Pourreza and Rafiei (2023) found that models often fail due to underspecified natural language queries. Similarly, Li et al. (2024) showed that while GPT-4 struggles initially, its performance can improve by up to 20.01% with human-annotated external knowledge. In such cases, models should proactively seek help rather than attempting to satisfy instructions with insufficient information.\nMotivated by these considerations, we aim to investigate whether LLMs can identify when to ask for user support. Since providing such support requires additional effort from users, there is an inherent trade-off between \u201cLLM performance improvement from user support\" and \"user burden\". Therefore, we seek to answer the following research questions: RQ1: How can we design evaluation metrics to quantify this trade-off? RQ2: How effectively do LLMs manage this trade-off, and what strategies are effective in improving it?\nIn this work, we focus on the text-to-SQL task as a case study to empirically investigate the aforementioned research questions. We chose the text-to-SQL task for several reasons: (1) Its promising applicability, empowering lay users to retrieve data with natural language queries. (2) The inherent ambiguity in some natural language queries, leading to uncertainty in the generation of SQL code (Pourreza and Rafiei, 2023), making it suitable for scenarios where additional user support is beneficial. (3) There exists a large-scale BIRD dataset (Li et al., 2024) with human-annotated external knowledge, providing a valuable source of user support for our empirical investigation."}, {"title": "2 Formulation for Seeking Support", "content": "2.1 General Setup\nConsider an LLM f parameterized by \u03b8, along with a prompt template p(\u00b7). Given a natural language instruction x, we use z to represent support, which should enhance the LLM's ability to fulfill x. Formally, \\(\\hat{y}_{z} = f(p(x, z) | \\theta)\\) is more likely to satisfy x compared to \\(\\hat{y} = f(p(x) | \\theta)\\). We denote the \"ask for support\" signal emitted by the LLM as \u00e2, defined as a confidence score in the range [0, 1], where 1 indicates an absolute need for support. A threshold \u03c4 is then used to determine whether to request z. In practice, \u00e2 could also be a natural language request specifying the type of support needed by the LLM, which we leave for future work.\n2.2 Evaluation\nTo measure the trade-off between performance improvement from user support and user burden, we need 2-dimensional evaluation. One dimension is the user burden (B), which we define as the proportion of instances where the LLM ask for support:\n\\(B = \\frac{N_{ask}}{N}\\)\nwhere \\(N_{ask}\\) is the number of instances where the LLM asks for support, and N denotes the total number of instances in the test set. The other dimension is the performance improvement (\u0394, Delta):\n\\(\u0394 = \\frac{1}{N_{ask}} \\sum_{i=1}^{N_{ask}} (h(y_i, \\hat{y}_{i,z}) \u2013 h(y_i, \\hat{y}_i))\\)\nwhere h(\u00b7) is the evaluation function of a given task, which takes ground truth yi and model output \\(\\hat{y}_i\\) as arguments (\\(\\hat{y}_{i,z}\\) is an output with the help of z). Inspired by the idea behind the ROC curve (Majnik and Bosni\u0107, 2013), we illustrate this trade-off with a graph, where the performance curve is plotted by adjusting the threshold \u03c4 from high to low along the x-axis. We refer to this curve as Delta-Burden Curve (DBC) (see the leftmost subplot of Figure 2).\n2.3 Methods for Seeking Support\nWe design a prompt template \\(p_{ask}(\u00b7)\\) to enable LLMs to request support by \\(\\hat{a} = s(f(p_{ask}(w) | \\theta))\\). Here, w represents the textual information that the LLM f uses to determine whether it needs to seek support, and s is the scoring function that converts the probability distribution of output tokens into a confidence score \\(\\hat{a} \\in [0, 1]\\). We propose methods with varying compositions of w to explore the information LLMs require to achieve better trade-off under DBC. Note that \\(p_{ask}\\) remains the same across all methods to minimize prompt engineering. An overview of these methods is shown in Figure 1.\nDirect Ask (DA): w = (db, x), composed of database schema db and user data requirement x.\nWrite then Ask (WA): w = (db, x, \\(\\hat{y}\\)), where the LLM generates the SQL code \\(\\hat{y} = f(p(db, x) | \\theta)\\) first and then use this self-generated output as the additional information in w.\nExecute then Ask (EA): w = (db, x, \\(\\hat{y}\\), r), where the execution results \u00ee is returned by the database by executing LLM-generated SQL \\(\\hat{y}\\)."}, {"title": "3 Experiments", "content": "3.1 Dataset\nWe use BIRD (Li et al., 2024), which includes human-annotated external knowledge that serves as z. For example, z might be domain-specific knowledge, such as how to calculate financial indicators from database values. The instruction x represents the users' data requirements, paired with the ground truth SQL y. It uses Execution Accuracy (EX) as the evaluation metric, where h(yi, \\(\\hat{y}_i\\)) is defined as \\(1(r_i = \\hat{r}_i)\\). Here, ri is the SQL execution result of yi, and \\(\\hat{r}_i\\) is the execution result of \\(\\hat{y}_i\\). Simply put, EX is the proportion of testing instances where ri and \\(\\hat{r}_i\\) are identical.\n3.2 Implementation\nFor open-weight LLMs, we use WizardCoder-34B (Luo et al., 2023), Llama-3-70b-chat, DeepSeek-Coder-33B (Guo et al., 2024), and Mixtral-8x22B (Jiang et al., 2024) for diversity of different LLM families. For closed-source LLMs, we use gpt-3.5-turbo-0125, gpt-4-turbo-2024-04-09, and gpt-4o-2024-05-13 (OpenAI, 2023). The prompt \\(p_{ask}(w)\\) (included in Appendix A) instructs the model to output a single token Yes/No to indicate whether it needs support. We define the scoring function s as the softmax of Yes over log probabilities of Yes and No to derive \\(\\hat{a} \\in [0, 1]\\)."}, {"title": "4 Main Results", "content": "Using the formulation in Section 2.2, we quantify the performance of different methods with the Area Under Delta-Burden Curve (AUDBC) in Table 1. Visualized DBCs are available in the leftmost subplots in Figure 2. Note that AUDBC should only be compared between methods under the same LLM, as it is normalized to the range of [0, 1] by dividing the area under the curve by the maximum square area, which depends on the scale of \u0394EX and differs across LLMs, as shown in Table 2.\nThere are three major findings: (1) Execution then Ask consistently improves the performance-burden trade-off for LLMs, although Llama-3-70b-chat fails to outperform the random baseline. (2) The leftmost four LLMs in Table 1 do not surpass the random baseline without the assistance of r, indicating that many current LLMs still struggle to determine the need for support based on x and \\(\\hat{y}\\) alone. (3) Despite this, the rightmost three LLMs outperform the random baseline with the Write then Ask (x, \\(\\hat{y}\\)) or even Direct Ask (x) methods. Nevertheless, the inclusion of r remains beneficial for further enhancing the trade-off between performance improvement and user burden. Practical implications of the third point include the potential for cost savings by trading off the execution of \\(\\hat{y}\\) to obtain r in certain resource-constrained scenarios."}, {"title": "5 Discussion", "content": "5.1 Analysis on the Delta-Burden Curves\nThe Delta-Burden Curves (DBCs) plotted in Figure 2 quantify the following practical question: Under the same user burden, which method can achieve more performance boost? To further analyze how this performance boost is achieved, we decompose the concept into two abilities:\n1. The ability to ask for support when the LLM cannot satisfy the instruction originally.\n2. The ability to utilize support effectively to flip the incorrect output to the correct output.\n1. For the first ability, we introduce the following metrics inspired by the precision-recall trade-off: Precision of Asking for Support (\\(P_{ask}\\)) When the LLM asks for support, it should be the case that the LLM cannot satisfy the instruction originally, or it would cause unnecessary user burden:\n\\(P_{ask} = \\frac{\\#(Ask for Support \\& Originally Wrong)}{\\# Ask for Support}\\)\nRecall of Asking for Support (\\(R_{ask}\\)) When the LLM is not able to satisfy the instruction originally, it should identify this need and ask for support:\n\\(R_{ask} = \\frac{\\#(Ask for Support \\& Originally Wrong)}{\\# Originally Wrong}\\)\nPR Curve of Asking for Support Similar to how DBC is plotted, one can also adjust the threshold \\(\\tau \\in [0, 1]\\) from high to low along the x-axis to plot the Precision-Recall Curve of Asking for Support.\n2. For the second ability, we introduce Flip Rate: Flip Rate: This metric is calculated as the proportion of instances where the LLM's initially incorrect answers were corrected after receiving support, divided by the total number of instances where support was requested. Formally, it is defined as:\n\\(FR = \\frac{1}{N_{ask}} \\sum_{i=1}^{N_{ask}} (h(y_i, \\hat{y}_{i,z}) \u2013 h(y_i, \\hat{y}_i))\\)\n5.2 LLMs without Access to Log Probabilities\nGiven that not all LLMs provide access to token log probabilities, we discuss how our method can be adapted for these \u201cblack-box\" models. We modify the prompt template \\(p_{ask}\\) to \\(p_{verb}\\), which instructs the LLM to output the verbalized confidence score \\(\\hat{a}\\) directly by specifying the range and meaning of \\(\\hat{a} \\in [0, 1]\\) in \\(p_{verb}\\) (attached in Appendix A.2). In addition to the seven LLMs mentioned in Section 3.2, we also include two black-box models: gemini-1.0-pro-001 and claude-3-haiku-20240307. The results, shown in Table 3, indicate that using verbalized confidence scores generally degrades performance for most LLMs. However, it remains a promising alternative for black-box LLMs such as Gemini and Claude to surpass the random baseline."}, {"title": "6 Related Work", "content": "The ability of LLMs to identify the need for support relies on their well-calibratedness (Kadavath et al., 2022), which refers to their capacity to recognize uncertainty. Previous studies focus on enhancing the calibration of predictions (Xiao et al., 2022; Kuhn et al., 2023), or using verbalized token probabilities to achieve better calibration (Tian et al., 2023). Our work extends this line of research by exploring how LLMs can effectively seek user support by leveraging their well-calibrated property. The major distinction between this and existing calibration studies lies in extending the focus from identifying the uncertainty to utilizing support."}, {"title": "7 Conclusion", "content": "We propose a framework for LLMs to seek support, and evaluate methods on Text-to-SQL generation. Our findings suggest the importance of external signals, such as SQL execution results, in helping LLMs better manage performance-burden trade-off. We further decompose DBC into the ability of identify the need for support and the ability to utilize the support. Future works may explore a broader range of tasks or develop methods to improve both the identification and utilization of support."}, {"title": "8 Limitations", "content": "8.1 Task Coverage\nThe scope of our experiments is limited to the Text-to-SQL task. While this task provides a useful case study for evaluating LLMs' ability to seek and utilize support, it does not encompass the full range of potential applications for LLMs. Future work should extend the evaluation to a broader set of tasks to ensure the generalizability of our findings.\n8.2 Types of Support\nIn this study, we primarily focus on a single type of support: human-annotated external knowledge. However, there are many other types of support that LLMs might require. Future works could explore how LLMs can request and utilize these various forms of support to enhance their performance.\n8.3 Dependence on External Feedback\nOur findings indicate that LLMs significantly benefit from external signals, such as SQL execution results. However, this reliance on external feedback may not always be feasible in practical applications, where immediate execution or access to external data might be limited. Developing methods that enable LLMs to better manage without such feedback remains an important area for future exploration."}, {"title": "A Prompt Templates", "content": "We include the prompt templates used in this work.\nA.1 Prompt for Seeking Support\nThe prompt template \\(p_{ask}(w)\\) used to instruct LLMs for seeking support is as follows:\nYou are currently doing the text-to-SQL task. Based on the information provided (\\{items\\}), you have to determine whether additional hints are required for you to generate the SQL correctly to answer the user's question. You should only ask for additional hints when you actually need them, since you will also be evaluated based on the number of times you ask for hints, which would be provided by the user.\ninformation provided (enclosed by triple backticks):\nAnswer a single word Yes if you need hints (since the information provided is not enough to generate SQL correctly). Answer a single word No if hints are not required (since you are already confident to generate SQL). Do you need additional hints? Answer (Yes / No):\nIn this template, the actual contents of \\{items\\} and \\{information\\} depend on the method used. The contents are summarized in Table 4. For example, w = (db,x,y,r) in Execute then Ask (EA), so \\{items\\} will be filled with the four item names and \\{information\\} will be replaced by actual information of the four items. Similarly for Write then Ask (w = (db, x, \\(\\hat{y}\\))) and Direct Ask (w = (db, x)).\nA.2 Prompt for Seeking Support (Verbalized)\nThe prompt template for generating verbalized probabilities in LLMs without access to token log probabilities (e.g., Gemini and Claude families):\nYou are currently doing the text-to-SQL task. Based on the information provided (\\{items\\}), you have to determine whether additional hints are required for you to generate the SQL correctly to answer the user's question. You should only ask for additional hints when you actually need them, since you will also be evaluated based on the number of times you ask for hints, which would be provided by the user.\ninformation provided (enclosed by triple backticks):\nDo you need additional hints? Provide the precise probability that you need hints (closer to 0 means you don't need hints, closer to 1 means you need hints).\nGive ONLY the precise probability to five decimal places (format: 0.abcde, where abcde can be different digits), no other words or explanations are needed.\nThe prompt template is similar to the original template shown in A.1, except that the last few sentences are modified.\nA.3 Prompt for Generating SQL Code\nThe prompt template p(\u00b7) for converting user data requirement x into SQL code is as follows:\n\\{db_schema\\}\nUsing valid SQLite, answer the following questions for the tables provided above.\nQuestion: \\{question\\}\nNow, generate the correct SQL code directly in the format of \u201csql\\\\n<your_SQL_code>\\\\n\u201c\u2018:\nIf user support z is provided (i.e., when LLMs ask for support), the prompt template is slightly modified as follows:\n\\{db_schema\\}\nExternal Knowledge: \\{support\\}\nUsing valid SQLite, answer the following questions for the tables provided above. You can use the provided External Knowledge to help you generate valid and correct SQLite.\nQuestion: \\{question\\}\nNow, generate the correct SQL code directly in the format of \u201csql\\\\n<your_SQL_code>\\\\n\u201c\u201c:\nIn these two templates, \\{db_schema\\} is db, \\{question\\} is user data requirement x, and \\{support\\} is user support z, which is human-annotated external knowledge in BIRD (Li et al., 2024)."}]}