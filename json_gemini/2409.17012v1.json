{"title": "AI-Driven Risk-Aware Scheduling for Active Debris Removal Missions", "authors": ["Antoine Poupon", "Hugo de Rohan Willner", "Pierre Nikitits", "Adam Abdin"], "abstract": "The proliferation of debris in Low Earth Orbit (LEO) represents a significant threat to space sustainability and spacecraft safety. Active Debris Removal (ADR) has emerged as a promising approach to address this issue, utilising Orbital Transfer Vehicles (OTVs) to facilitate debris deorbiting, thereby reducing future collision risks. However, ADR missions are substantially complex, necessitating accurate planning to make the missions economically viable and technically effective. Moreover, these servicing missions require a high level of autonomous capability to plan under evolving orbital conditions and changing mission requirements. In this paper, an autonomous decision-planning model based on Deep Reinforcement Learning (DRL) is developed to train an OTV to plan optimal debris removal sequencing. It is shown that using the proposed framework, the agent can find optimal mission plans and learn to update the planning autonomously to include risk handling of debris with high collision risk.", "sections": [{"title": "1 Introduction", "content": "In recent years, Low Earth Orbit (LEO) has become increasingly crowded with debris originating from spacecraft fragmentation at the end of their operational life cycle. Collisions of such debris with space assets can often lead to catastrophic failure. One example is the Cosmos 2251-Iridium 33, which led to the destruction of the Iridium 33 satellite and added more than 2000 pieces of debris to LEO [1]. In addition, collisions between objects generate new space debris, increasing the likelihood of further collisions, in what is known as the Kessler effect [2]. Although international post-mission disposal policies are being developed, to this date, they are not sufficient to handle this growing problem [3].\nTo mitigate debris growth, studies recommend a removal rate of five heavy debris per year [4]. Many approaches have been proposed to remove or \"deorbit\" these debris. A promising method is Active Debris Removal (ADR). ADR missions are performed by an Orbital Transfer Vehicle (OTV), which can visit and de-orbit multiple debris during a single mission. Due to high costs, ADR missions should be able to maximise debris removal per mission at minimal cost. However, finding the optimal planning for de-orbiting several debris is non-trivial and requires adapted tools to find the most adequate plans. In addition, these missions require a high level of autonomous planning capabilities for the robotic spacecraft to be able to adapt to changing orbital conditions and mission requirements rapidly and effectively.\nThe ADR mission planning problem can be formulated as a Cost-Constrained Traveling Salesman Problem (CCTSP) [5], a variant of the Traveling Salesman Problem (TSP) [6]. Key constraints are the mission duration and the propellant available to the OTV. As the OTV transfers from one debris to the next, it consumes both time and propellant. Each debris is given a value, allowing the mission designer to prioritise for a certain goal, such as debris size [7] or collision risk. The objective is, therefore, to optimise the debris sequence for maximum mission value under these constraints.\nTraditional optimisation models of ADR missions focus on minimising the mission cost using static methods. However, recent AI advancements, particularly in reinforcement learning (RL), allow the consideration of a dynamic approach. RL allows sequential decision-making and adaptability to new information [8]. Applying RL to ADR mission planning is relevant since OTVs receive ongoing monitoring data and may need to update plans accordingly, ensuring their capacity to operate autonomously.\nResearch in ADR mission planning varies mainly in transfer strategy and optimisation approach. Ion thrusters for Low-Thrust transfers and chemical propulsion for High-Thrust transfers have been studied using various static optimisation methods [9][10]. RL frameworks, such as Deep Q-Network (DQN) [11],"}, {"title": "2 Methods", "content": null}, {"title": "2.1 General framework for ADR missions", "content": "In an ADR mission, a robotic OTV transfers from one debris to another, captures it, and deorbits each debris until it exceeds its mission time or allotted propellant. Given that the OTV's resources only allow it to visit a subset of debris within a larger debris set, the goal is to find the optimal sequence of debris to visit to maximise mission value under technical and economic constraints. In this work, the mission value is associated with the debris collision risk.\nUsing the Keplerian reference frame, and assuming circular orbits, a transfer from one debris to the next is represented as: (a\u2081, i\u2081, \u03c9\u2081, \u03bd\u2081) \u2192 (a2, 12, W2, V2). where (a), (i), (w) and (v) represent the semi-major axis, inclination, argument of periapsis and true anomaly, respectively. Transfer strategies can be a sequence of individual manoeuvres (e.g., (a\u2081) \u2192 (a2), (i\u2081) \u2192 (12)), or combined manoeuvres (e.g., (a\u2081, i\u2081) \u2192 (\u04302, 12))."}, {"title": "2.2 Reinforcement Learning formulation", "content": "The ADR Mission planning problem can be formulated as a Markov Decision Process (MDP) to be solved using RL. The main components of the proposed MDP framework are shown in this section."}, {"title": "2.2.1 State", "content": "Inspired by [7], in this formulation, the state is represented as a vector shown in Table (1). Element 1 is the number of debris left to remove. Element 2 is the total fuel left for the OTV, calculated as the remaining possible (AV). Element 3 is the total mission time left (\u0394Tleft). Element 4 is the current debris to be removed. Element 5 is a binary flag that describes the \"removed or not\" status of each debris. Element 6 is the collision risk associated with each debris. Each piece of debris is assigned a collision risk level ranging from 1 to 10, indicating its deorbiting priority, which can change during the mission. All possible states (s) construct the state space (S). Both the length of the Removal Flags and collision risk lists are equal to the amount of debris considered for deorbiting (N)."}, {"title": "2.2.2 Action", "content": "At each step, the OTV captures debris. The agent's action is to select the next debris to deorbit. The action space (A) is therefore constructed of all actions a = d, where {d \u2208 N+ | 1 \u2264 d \u2264 N} is the order of debris in the list (N). It is assumed that the time of arriving at the target debris is also the time of leaving for the next one (ignoring the time of the de-orbiting maneuver) as it is negligible compared to orbital transfer times. Therefore, this action formation without a waiting procedure is applicable."}, {"title": "2.2.3 State Transition", "content": "Unlike traditional RL methods in a control context where the state transition is associated with a time step, the formulation proposed in this work associates the state transition with a removal step. Hence, given a state and action, the next state is computed using the state transition function described in Eq.(1)\ns'[Ndebris left] = s[Ndebris left] - 1\ns'[AVleft] = s[AVleft] - AV;\ns'[ATleft] = s[ATleft] \u2013 \u0394\u03a4;\ns'[CurrentLocation] = a\ns'[RemovalFlags][a] = 1\ns'[CollisionRisk] = RandRisk(s'[RemovalFlags])\nAfter taking an action, the state transition updates the number of debris left, the fuel and time left for the rest of the mission, the current location of the OTV, the binary flag of the last deorbited debris, and the collision risk of all debris.\nTo validate the proposed framework on a case study, a simple risk setting function \u201cRandRisk\u201d is used. It randomly assigns a high collision risk to available debris according to the logic shown in Algorithm (1).\nWhere (Rprio) is the reward the agent will receive for deorbiting a high-risk debris."}, {"title": "2.2.4 Reward", "content": "The reward function is designed to ensure that the OTV minimises the overall risk in the system by deorbiting debris according to their risk level. To do so, a deterministic reward function is used, as described in Eq. 2.\nr(s, a) = \n{\ns[CollisionRisk][a] if (s) is a non-terminal state\n0 if (s) is a terminal state\n\nThe agent receives a positive reward proportional to the collision risk of the debris removed, and no reward if no debris is removed. Moreover, since RiskLevel is reset at every removal step (i), the agent only receives a high reward if it takes the action (a) that deorbits the high-risk debris immediately after receiving the risk information in state (s)."}, {"title": "2.2.5 Terminal States", "content": "The terminal states, which represent the end of a training episode, are used in the algorithm in two different scenarios. First, if the agent takes an action that results in a state where it has exceeded its allocated resources (AVmax) or (\u2206Tmax), and second, if the agent takes an action that is impossible (e.g., revisiting debris already deorbited). These terminal states are coupled with a reward of 0."}, {"title": "2.2.6 Orbital Transfer Simulation", "content": "A high-thrust transfer simulator is developed to calculate the orbital transfer times and fuel consumption of the OTV during the learning process. It takes the action selected by the agent (i.e., the next debris to visit) and outputs the time of the maneuver and the fuel cost. The simulator is based on the use of 3 sequential maneuvers to transfer from the current debris to the next."}, {"title": "2.3 DQN Agent", "content": "A Deep Q-Network architecture is used to train the agent, given its ability to handle the continuous state-space of the MDP model described in Sec. (2.2.1).\nThe network takes as input the state vector described in (2.2.1) and outputs Q-values, which represent the estimated value of deorbiting each debris given the current state. These values guide the agent's decision-making process during the mission. The function approximation network learns the optimal Q-values based on the reward sampled from the environment at each step, by minimising the difference between the Q-value prediction and the Q-target described in Eqs. (3) and (4).\nQ-value prediction: y\u2081 = \n{\nRi if episode terminates at step i+1\n| R\u00a1 + y maxa' Q(si+1,a';0target) otherwise\n\nQ-value loss: (vi \u2013 Q(si, a; 0))2\nwhere (i) is the removal step and (si), (ai) and (R\u2081) are respectively the state, the action and the reward at removal step (i).\nThe neural network used in this paper has a straight-forward architecture consisting of two hidden layers with ReLU activation functions. As discussed in [11], a Fixed Q-Target approach was used to stabilise the learning. This is done by implementing separate Value and Target networks, parameterised by (Ovalue) and (Otarget), respectively.\nFinally, since sampling from the environment is computationally expensive, a Replay_Buffer is used to make a more efficient use of experiences, and reduce the correlation between experiences."}, {"title": "3 Results", "content": "To benchmark the agent on realistic data, the Iridium-33 dataset was used, containing the coordinates of 320 debris resulting from collisions between the satellites Cosmos 2251 and Iridium 33. Hyperparameter tuning of the agent was undertaken on this dataset using Bayesian Optimisation [13]."}, {"title": "3.1 Validation of the DRL results", "content": "To validate the RL algorithm's effectiveness, an exhaustive search is performed on a small subset of debris, and used as a baseline for comparison with the RL agent's mission. The full depth search results are known to be optimal, hence this allows the comparison of the theoretical maximum reward and the reward received by the RL agent.\nGiven the available resources (AVmax) or (ATmax), and a debris dataset, a full-depth search algorithm is designed to determine the theoretical maximum sequence length. Since the computational cost of the exhaustive search increases exponentially, this full-depth algorithm can only be used on small datasets for validation. To further scrutinise the RL agent, (AVmax) is chosen so that the maximal solution subset is of size one, i.e., only a single debris sequence can achieve the maximum reward. Finally, since the aim is to validate the maximal sequence length, the environment is set without considering collision risks.\nThe validation methodology is the following:\n(1) Using a debris list length of N = 10. Compute all of the possible subsets of size Nsubset = 5, i.e., all possible sequences of debris of length 5.\n(2) Using the orbital transfer simulator, compute the (AVtotal) for each subset of debris visited.\n(3) Find the subset that used the minimum (AVtotal), such that min(AVtotal) = Vo optimal.\n(4) Run the RL algorithm on the same N = 10 debris, with a constraint of AVmax = AVoptimal\n(5) Verify that the agent converges to the optimal sequence.\nFollowing this methodology, agents are trained over different initiating seeds and the results of the DRL obtained solutions are compared to the guaranteed optimal solutions found via exhaustive search. The agent learning progress over training episodes is shown in Fig. (1)."}, {"title": "3.2 Case Study Results", "content": "To evaluate the agent's performance, two scenarios are tested. In both, the agent receives a high reward for deorbiting high-risk debris. In the first scenario, the agent does not have access to collision risk information and thus receives the high reward only if it happens to select and deorbit high-risk debris by chance. This serves as a baseline for comparison with the second scenario, where the agent has direct access to collision risk levels."}, {"title": "4 Discussion", "content": "In this work, an RL-based autonomous planning algorithm for OTV is developed for deorbiting the maximum amount of debris in a single mission, considering a limited amount of fuel and mission time. The results show that this algorithm is capable of finding optimal sequence of deorbiting debris. Moreover, it is shown that using the proposed framework, the agent can learn to update the planning mid-mission based on collision risk estimations, with the aim of reducing the overall risk in the system."}]}