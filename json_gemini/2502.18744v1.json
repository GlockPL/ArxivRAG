{"title": "Like Father, Like Son: Kinship-Aware Preference Mapping (KARMA) for Automatic Alignment in Large Language Models", "authors": ["Jeesu Jung", "Chanjun Park", "Sangkeun Jung"], "abstract": "Recent advancements in Large Language Model (LLM) alignment have sought to mitigate the cost of human annotations by leveraging pretrained models to generate preference data. However, existing methods often compare responses from models with substantially different capabilities, yielding superficial distinctions that fail to provide meaningful guidance on what constitutes a superior response. To address this limitation, we propose Kinship-Aware pReference MApping (KARMA), a novel framework that systematically pairs responses from models with comparable competencies. By constraining preference comparisons to outputs of similar complexity and quality, KARMA enhances the informativeness of preference data and improves the granularity of alignment signals. Empirical evaluations demonstrate that our kinship-aware approach leads to more consistent and interpretable alignment outcomes, ultimately facilitating a more principled and reliable pathway for aligning LLM behavior with human preferences.", "sections": [{"title": "1 Introduction", "content": "Aligning Large Language Models (LLMs) with human preferences is a fundamental challenge in artificial intelligence, where the effectiveness of alignment hinges on the quality and specificity of the preference data used to guide model outputs (Shen et al.). Early alignment methodologies primarily relied on human-annotated datasets with binary preference labels, wherein explicit human judgments directed models toward generating more desirable responses (Ouyang et al., 2022a). While these methods have played a critical role in improving LLM behavior, their dependence on extensive human labor imposes significant scalability constraints.\nTo address this limitation, recent research has explored automated preference data generation techniques, such as Reinforcement Learning from AI Feedback (RLAIF) (Lee et al.). These methods leverage pretrained models to automatically generate preference pairs, significantly reducing the need for human annotation. However, the quality of the resulting preference data remains a major concern. If responses within a preference pair exhibit substantial disparities in quality, complexity, or style, the preference signal may become trivial, capturing only superficial contrasts rather than informative distinctions that refine model behavior.\nWe define this issue as the Response Quality Gap, which refers to the disparity in coherence, complexity, and reasoning quality between candidate responses. A large Response Quality Gap weakens preference signals, leading to misaligned or less informative training objectives. This problem is particularly pronounced in large-scale, automatically generated datasets, where manually curating response pairs to minimize the Response Quality Gap is infeasible due to the massive volume of data.\nA promising solution lies in considering the origin of the responses being compared-specifically, the LLM that generated them. Since a model's"}, {"title": "2 Kinship-Aware pReference MApping (KARMA) Framework", "content": "Recent approaches to LLM alignment have reduced the reliance on human annotation through automated preference data generation. However, these methods often neglect the foundational influence of the Origin Model. For example, a model trained using Llama's methodology is going to produce outputs that fall within the distribution of Llama's training data. Just as a son inherits traits from his father, a model's quality directly influences the quality of its outputs. when preference pairs are derived from models with markedly disparate capabilities, the resulting comparisons yield only trivial distinctions.\nTo address this, the proposed Kinship-Aware pReference MApping (KARMA) framework evaluates kinship based on the performance of origin"}, {"title": "2.1 Definition of Kinship", "content": "In the context of binarized alignment data mapping, the similarity between the responses serves as a fundamental determinant of mapping efficacy. When data pairs are closely aligned, they provide more nuanced and meaningful guidance signals for alignment tuning. Building upon this principle of response similarity, KARMA introduces the concept of kinship among models.\nThe kinship is measured by evaluating the capabilities of origin models. We measure the benchmarks as the origin model capability. We define a set of n candidate origin models, ${M_1, M_2,..., M_n}$, each evaluated according to multiple dimensions of competency. Let $B = {B_1, B_2,..., B_m }$ be a set of m benchmark tasks. Each benchmark $B_k$ provides an evaluation metric $E_{B_k}(M_i)$ that quantifies the performance of model $M_i$. This evaluation may include measures of factual correctness, reasoning, and adherence to instructions.\nFor each pair of models $(M_i, M_j)$, we define a kinship score $S_{ij}$, which reflects the similarity of their capabilities:\n$S_{ij} = f(\\{E_{B_k}(M_i), E_{B_k}(M_j)\\}_{k=1}^m),$   (1)\nwhere $f(\u00b7)$ is a similarity function. Higher values of $S_{ij}$ indicate closer competency alignment between $M_i$ and $M_j$. The exact functional form of $f(\u00b7)$ can vary depending on the chosen benchmarks and may incorporate multiple factors (denoted collectively as $I_F$, $R$, and $K$) to minimize bias and ensure comprehensive evaluation."}, {"title": "2.2 Stage 1: Benchmark Evaluation for Model Competency", "content": "To identify which models are suitable candidates for kinship-based pairing, we employ a series of benchmarks that meet three criteria: (1) they are widely validated in prior research, (2) they cover both general knowledge and domain-specific tasks, and (3) they provide well-defined quantitative metrics. \nEvaluating models on these benchmarks yields performance scores that serve as the basis for computing $S_{ij}$. The degree of kinship among models varies with the choice of benchmarks. To avoid relying solely on log-probabilities or overly simplistic metrics, we adopt prompt-based evaluations that require each model to produce explicit responses. \nThe prompt formats are adapted from templates in MMLU-pro (Wang et al., 2024) and FLAN (Longpre et al., 2023), ensuring compatibility with each model's input structure and recommended usage patterns."}, {"title": "2.3 Stage 2: Kinship-Based Response Mapping", "content": "Once we identify a subset of models whose pairwise kinship scores exceed a predetermined threshold $\u03c4$ (e.g., $S_{ij} > \u03c4$), we set $\u03c4$ at 0.1, as cosine similarity values below this threshold indicate a lack of meaningful similarity between models. we focus on collecting responses from these closely matched models. Kinship serves as an indicator of model's ability and similarity. While various criteria can be applied to measure this, we utilize the"}, {"title": "2.4 Stage 3: Automatic Preference Integration with Alignment Processes", "content": "To integrate kinship-based mapped data into existing alignment tuning, the data requires a preference-setting process. Accordingly, we construct our kinship-based preference estimation on the principle that higher-performing models tend to produce more reliable outputs.\nTo quantify this, we calculate a normalized benchmark score that aggregates multiple benchmark performances into a single value. For a model $M_i$, this score is computed as:\n$S_{pref} (M_i) = \\frac{1}{m} \\sum_{j=1}^m \\frac{b_j - min(B_j)}{max(B_j) \u2013 min(B_j)},$  (2)\nFor a pair of kinship-aligned models $(M_i, M_j)$, we obtain a binary preference label:\n$Pref(M_i, M_j) =  \\begin{cases}   1, \\text{ if } S_{pref} (M_i) > S_{pref} (M_j),\\\\   0 \\text{ otherwise.}  \\end{cases}$ (3)\nOur data maintains the Response Quality Gap through kinship-based model comparisons and can establish preferences via Pref$(M_i, M_j)$ without additional annotation. This provides high-quality training signals, which can be leveraged through reinforcement learning or supervised fine-tuning. In contrast to existing approaches (e.g., RLAIF (Lee et al.)), KARMA's focus on model kinship produces preference pairs that yield more nuanced insights. This shift in perspective-from relying on comparisons of models with vastly different capabilities to leveraging finely matched pairs-provides a more parsimonious and theoretically grounded method for constructing alignment-relevant preference data."}, {"title": "3 Experiments", "content": "In this paper, we propose the KARMA framework. To validate the functionality of this framework and the characteristics of Alignment Tuning, we conducted experiments to address the following Research Questions (RQs):"}, {"title": "3.1 Alignment Dataset", "content": "For preference binarization, we utilized the Ultrafeedback dataset (Cui et al., 2023), which comprises responses generated by a diverse set of language models. The dataset includes outputs from both commercial and open-source models, ensuring a broad representation of model capabilities. The commercial models incorporated in the dataset include GPT-4(OpenAI, 2023), GPT-3.5 Turbo(Ouyang et al., 2022a), and Bard(Waisberg et al., 2024). Additionally, several models from the Llama family were included, such as Llama-2 (7B, 13B, and 70B)-chat(Touvron et al., 2023), UltraLM-13B(Cui et al., 2024), WizardLM (7B, 13B, and 70B)(Xu et al., 2023), Vicuna-33B(Zheng et al., 2024), and Alpaca-7B(Taori et al., 2023). Beyond the Llama-based architectures, the dataset also features responses from other notable models, including Falcon-40B-instruct(Almazrouei et al., 2023), MPT-30B-chat(Team, 2023), StarChat-Beta(Tunstall et al., 2023), and Pythia-12B(Biderman et al., 2023).\nAlthough the Ultrafeedback dataset contains results from UltraLM-65B(Cui et al., 2024), its performance could not be accurately assessed. To maintain the reliability of our evaluation, we excluded these results from the final dataset composition."}, {"title": "3.2 Models", "content": "To assess the effectiveness of the newly binarized dataset constructed using the KARMA framework, we fine-tuned and evaluated multiple instruction-following models. The selected models for evaluation include Llama-3.1-(3B, 8B)-Instruct(Dubey et al., 2024), representing Llama-based architectures. Additionally, we included Qwen2.5-(3B, 8B)-Instruct(Yang et al., 2024), enabling a comparative"}, {"title": "3.3 Training Algorithm for Alignment Tuning", "content": "We tested two different learning method for alignment tuning using the KARMA Framework:\nSupervised Fine-Tuning (SFT)\nDirect Preference Optimization (DPO)(Rafailov et al., 2024)"}, {"title": "4 Experimental Results", "content": null}, {"title": "4.1 RQ1: Dataset Reconstruction using KARMA", "content": "The proposed KARMA framework enables the application of a unified Total Ranking map across the entire dataset, facilitating a structured and consistent preference mapping process. Utilizing this reconstructed preference dataset, we conducted model training while ensuring that each critic's binarized response was systematically incorporated. The effectiveness of this reconstructed dataset was assessed by evaluating the trained models on standardized benchmarks, including MMLU and IFeval. \nFor evaluation, we employed prediction-based assessment methodologies across all benchmark tasks. Specifically, for ARC, MMLU, and MMLU-Pro, we adapted the MMLU-Pro evaluation framework, modifying only the multiple-choice options to align with our dataset. For IFeval, we leveraged its native evaluation framework to ensure consistency in assessment.\nTo analyze the relationships between models, we computed kinship scores by normalizing evaluation results across six benchmark tasks. Figure 4 presents a Principal Component Analysis (PCA)(Ma\u0107kiewicz and Ratajczak, 1993) visualization of these relationships, illustrating distinct clustering patterns among models. Generally, smaller-scale models tend to cluster in the first quadrant, models with closer kinship relationships in the second quadrant, while Llama-based models and models exceeding 10B parameters are predominantly distributed in the third and fourth quadrants. This distribution underscores the critical role of model"}, {"title": "4.2 RQ2: Performance of KARMA Binarization", "content": "To assess the effectiveness of KARMA binarization, we examined whether model performance can serve as an indicator of data quality. KARMA performs binarization in two sequential steps: first, it defines preference pairs based on kinship between models, and second, it ranks these pairs according to their relative performance. Through this process, we aimed to determine whether response similarity-estimated through model similarity-plays a more significant role in preference modeling than simply prioritizing higher-performing models by consistently positioning them as dominant within each pair."}, {"title": "4.3 RQ3: Model Size and Kinship", "content": "The impact of different kinship approaches varies significantly with model size, influencing how models learn from preference data. To investigate this relationship, we conducted experiments using models from the same family but with different parameter counts, specifically comparing small models (3B parameters) and larger models (7-8B param-"}, {"title": "4.4 RQ4: Model Capability and Kinship Patterns", "content": "Models exhibit distinct similarity patterns based on their capabilities, with these patterns varying across different evaluation metrics. Notably, model kinship is influenced by factors such as model scale and training methodology, leading to variations in clustering behavior across different capability dimensions.\nAnalysis Framework To systematically investigate these relationships, we analyzed model similarities across three key capability dimensions:\nKnowledge-Based Tasks: Unlike reasoning capability, clustering in instruction-following tasks is more strongly aligned with model families rather than size. This indicates that training methodology and architectural choices exert a greater influence on instruction adherence.\nReasoning Capability: Models tend to cluster primarily based on parameter count, suggesting that model size plays a dominant role in shaping reasoning performance.\nInstruction-Following (IF) Tasks: A hierarchical influence pattern emerges, where:\nAt the initial hierarchy, the model family is the primary determinant.\nAt the higher hierarchy, the model size becomes a stronger predictor of performance."}, {"title": "5 Conclusion", "content": "This paper introduces KARMA, a kinship-based framework for generating high-quality preference data for language model alignment without requiring instance-wise annotation. KARMA achieves performance comparable to RLAIF while offering greater computational efficiency and surpassing baselines on MMLU-Pro and IFeval, demonstrating the effectiveness of kinship-aware preference mapping. Our findings show that model size significantly affects the optimal kinship strategy. Smaller models (3B) benefit from prioritizing response quality, while larger models (7-8B) perform better when leveraging response similarity. Additionally, kinship patterns vary across tasks, with reasoning clustering by model size, instruction-following by model family, and knowledge-based tasks exhibiting a hierarchical structure. These results highlight the need for size-adaptive preference learning strategies and demonstrate that high-quality preference data can be generated without costly manual annotation. Future work will extend KARMA to cross-family model comparisons and further explore the relationship between model kinship and human preference judgments."}, {"title": "6 Related Work and Background", "content": null}, {"title": "6.1 Alignment Tuning", "content": "Alignment tuning has become a central focus in enhancing LLMs to meet user expectations and ethical standards(Kumar et al., 2024). Various preference-based learning techniques, particularly reinforcement learning methods such as Proximal Policy Optimization (PPO)(Schulman et al., 2017), Direct Preference Optimization (DPO)(Rafailov et al., 2024), and Optimized Reinforcement Preference Optimization (ORPO)(Hong et al., 2024), have been developed to facilitate this tuning process. These methods rely on preference datasets, which typically contain pairs of responses generated by models based on given instructions, with each pair ranked according to human or model-based evaluations. Prominent alignment datasets"}, {"title": "6.2 Limitations in Existing Preference Data Approaches", "content": "Effective preference data construction requires a clear, rigorous set of criteria to ensure alignment quality across generated response pairs. Common criteria, including Reasoning, Truthfulness, and Instruction-Following, guide the selection of data that aligns with key ethical and functional standards(Cui et al., 2023; Bai et al., 2022). High-performing models, capable of producing responses that meet these standards, are often evaluated using benchmarks like ARC(Clark et al., 2018), MMLU(Hendrycks et al., 2020), and the Instruction-Following eval(Zhou et al., 2023) suite, which assess a model's factual accuracy, reasoning ability, and compliance with instructions. While these benchmarks provide a foundation for selecting responses that promote alignment goals, existing methods typically overlook the impact of variation in model capabilities on alignment consistency.\nThe KARMA Framework addresses this limitation by introducing a kinship-aware approach that emphasizes model compatibility in preference data selection. By curating preference pairs based on model kinship-aligning models with similar core competencies in knowledge, reasoning, and instruction-following-the KARMA Framework enhances alignment coherence and ensures stable, high-quality data. Further, its curriculum-based selection process sequences preference data from simpler to more complex distinctions, optimizing the alignment process for improved consistency and scalability."}, {"title": "Limitation", "content": "In this paper, we demonstrated that KARMA can achieve performance comparable to existing RLAIF without requiring annotations for SFT and DPO. However, due to resource and time constraints, we were unable to validate our approach across a broader range of alignment tuning techniques. Further evaluation is needed for methods.\nAdditionally, our evaluation primarily focused on fundamental abilities such as Reasoning, Knowledge, and Instruction-Following. However, we did not assess KARMA's performance on other important values, including factuality and ethical standard. Future work should incorporate evaluations reflecting these aspects."}]}