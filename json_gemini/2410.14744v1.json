{"title": "Eliciting Uncertainty in Chain-of-Thought to Mitigate Bias against Forecasting Harmful User Behaviors", "authors": ["Anthony Sicilia", "Malihe Alikhani"], "abstract": "Conversation forecasting tasks a model with predicting the outcome of an unfolding conversation. For instance, it can be applied in social media moderation to predict harmful user behaviors before they occur, allowing for preventative interventions. While large language models (LLMs) have recently been proposed as an effective tool for conversation forecasting, it's unclear what biases they may have, especially against forecasting the (potentially harmful) outcomes we request them to predict during moderation. This paper explores to what extent model uncertainty can be used as a tool to mitigate potential biases. Specifically, we ask three primary research questions: 1) how does LLM forecasting accuracy change when we ask models to represent their uncertainty; 2) how does LLM bias change when we ask models to represent their uncertainty; 3) how can we use uncertainty representations to reduce or completely mitigate biases without many training data points. We address these questions for 5 open-source language models tested on 2 datasets designed to evaluate conversation forecasting for social media moderation.", "sections": [{"title": "1 Introduction", "content": "Conversation forecasting \u2013 where a model predicts the outcome of a partial conversation \u2013 is useful across many domains, e.g., see research on negotiation dynamics (Sokolova et al., 2008), mental health monitoring (Cao et al., 2019a), and social media moderation (Zhang et al., 2018). For instance, in online moderation, the forecasting task may be to predict whether a harmful behavior (like digital bullying) will eventually occur in an unfolding conversation, allowing moderators to intervene to prevent these behaviors. Recently, Sicilia et al. (2024) demonstrate pre-trained language models are relatively effective conversation forecasters, setting themselves apart because they do not require copious amounts of domain-specific training data prior to inference time. Yet, it remains unclear what biases these systems may hold, especially in digital media contexts, where they are specifically asked to predict outcomes that may be harmful to the parties involved (see Figure 1).\nIndeed, the data used in common instruction-tuning algorithms \u2013 e.g., RLHF (Ouyang et al., 2022) and DPO (Rafailov et al., 2024) \u2013 are designed to align language models with human values, and subsequently, avoid any propagation of harm. Meanwhile, the motivating tasks of this paper draw a fine line between propagation and prediction. Surely, \"predicting\" a harmful outcome is not \"speaking into existence\" but it's unclear whether this distinction is lost on \"aligned\" language models. Or, if it is not lost, whether underlying data bias (i.e., against harmful outcomes) predisposes language models to propagate this bias when forecasting harmful outcomes.\nWhile the role of alignment mechanisms in producing model bias is difficult to confirm,\u00b9 our own empirical results, and those of previous work (Sicilia et al., 2024), indicate current language models are indeed biased against predicting harmful out-"}, {"title": "2 Background", "content": "We work within the conversation forecasting framework established by Sicilia et al. (2024), wherein the model is tasked with predicting a conversation's outcome. For instance, it may need to predict whether a personal attack will occur (or not). Since the conversation provides only a limited glimpse into the underlying reality, unknown factors like future developments or unobservable mental states introduce an element of randomness, making it challenging to determine the outcome with certainty based solely on the available information."}, {"title": "2.1 Conversation Forecasting Setup", "content": "Task For a set of natural language tokens T, we assume observation of a partial multi-party dialogue $D \\in T^*$ consisting of K turns. Following Sicilia et al. (2024), the length K is a uniform random number between 2 and the full dialogue length, simulating the \u201cpartial\" property of the dialogue.\u00b3 These conversations appear unfinished to the model, but in reality, have an eventual ground-truth outcome $\u039f \\in \\{0,1\\}$, indicating whether a personal attack occurs or does not occur. The task of the model is to predict O given D \u2013 that is, to predict whether a personal attack will occur given the partial conversation.\nMetrics Sicilia et al. (2024) evaluate the quality of a model's uncertainty estimates when conversation forecasting (i.e., using a metric called the Brier score). We focus on different evaluation metrics, selected to properly answer our distinct research questions. Given a model prediction \u00d4 for O, we evaluate the model using the accuracy of the prediction: $E[\u00d4 = 0]$. Besides accuracy, we also report the F1 score to capture both precision and recall. To measure the bias of the predictions, we report the statistical bias: E[\u00d4 \u2013 O], which is traditional measure of systematic error in an estimator. Specifically, this captures the average trend of the model's errors: whether it over-estimates (bias is positive) or under-estimates (bias is negative) on average. This type of bias is seemingly different from common quantitative notions of social bias in a model's outputs; e.g., see Gallegos et al. (2024). In reality, this (older) measure of bias is a special case of accuracy parity (Zhao and Gordon, 2022) where the group trait of interest, or \u201cprotected attribute,\" is the occurrence of a personal attack.\nCorpora We consider two corpora in this work:\n1. (wiki) a corpus of conversations from Wikipedia's talk page, proposed by Zhang et al. (2018), in which authors discuss edits to Wikipedia articles; and\n2. (reddit) a corpus of conversations from the subreddit ChangeMyView, proposed by Chang et al. (2019), in which redditors try to"}, {"title": "2.2 Other Related Work", "content": "Conversation Forecasting As noted, Zhang et al. (2018) and Chang et al. (2019) provide early investigations and data for forecasting personal attacks during dialogue to proactively moderate online forums. Using the same data, Kementchedjhieva and S\u00f8gaard (2021); Altarawneh et al. (2023) propose new models, capitalizing on temporal and social aspects of dialogue. Meanwhile, forecasting of other conversation outcomes includes task-success (Walker et al., 2000; Reitter and Moore, 2007), mental health codes (Cao et al., 2019b), emotions (Wang et al., 2020; Matero and Schwartz, 2020), situated actions (Lei et al., 2020), and financial events (Koval et al., 2023). Among these, our work is uniquely positioned by its focus on the relationship between uncertainty and bias when using modern language models for this task. Broadly, studying how language models perform at this task is an important research direction because they promise a pipeline that requires very limited labeled data relative to other, previous directions of study. At the same time, these pre-trained models may have unknown biases, calling for the direction of study proposed in the current paper.\nUncertainty Estimation with LMs Modern \"aligned\" language models have been shown to be capable at representing uncertainty in their responses to factual queries, even with minimal supervision (Kadavath et al., 2022). Meanwhile, uncertainty has also been well studied in models without alignment to human preferences (Desai and Durrett, 2020; Jiang et al., 2021; Dan and Roth, 2021; Kong et al., 2020; Zhang et al., 2021; Li et al., 2022). Unlike existing work, ours is interested in how fine-tuning for alignment to human preferences might bias the model against predicting adverse outcomes. As far as how we extract uncertainty estimates from the language model, our work is most in line with that of Lin et al. (2022); Mielke et al. (2022); Tian et al. (2023) who all suggest \"direct forecasts\" or uncertainty estimates directly specified in the sampled tokens of the model. These estimates are considered best out-of-the-box for the types of models we study (Sicilia et al., 2024)."}, {"title": "3 Methods", "content": "Here, we describe prompts used to elicit conversation forecasts. A full example is in the Appendix."}, {"title": "3.1 Forecasting with Language Models", "content": "Traditional CoT Classification To predict conversation outcomes with language models, we simply provide the language model with the partial conversation segment and prompt the language model to predict the outcome. There are some key components to precisely detail our strategy.\n1. Role Play: As part of the system prompt, we give the language model a \u201cname\u201d and \u201cskill set\" to direct the language model to mimic a task expert. This is a common prompt engineering technique. We use a similar role description as (Sicilia et al., 2024), emphasizing skills like Theory of Mind and the ability to predict actions/thoughts of different interlocutors.\n2. Output Format: To conclude the system prompt, we direct the model to use an easy-to-parse format; e.g., ANSWER = 1 for O = 1.\n3. Context: To start the user prompt, we explain the context of the conversation; e.g., \"The speakers are discussing edits to a Wikipedia article.\" We then provide context for predicting this specific instance. These include the partial conversation segment (delimited using special token sequences) and the question of interest. Specifically, we ask \"Will a personal attack occur at the end of the conversation?\".\n4. Chain of Thought: We conclude the user prompt with a chain-of-thought trigger phrase. Specifically, we use \u201cLet's think step by step, but keep your answer concise (less than 100 words).\u201d This encourages the model to output reasoning for it's answer and has been shown to improve performance (Kojima et al., 2022).\nUncertainty-Aware CoT Classification We use largely the same prompting strategy as traditional classification. Instead of asking for an answer directly, we instruct the model to report it's answer on a 10 point Likert scale where 1 indicates \"not likely at all\" and 10 indicates \u201calmost certainly.\""}, {"title": "3.2 Semi-Automated Topic Analysis", "content": "Method One aspect we explore empirically is the relationship between a model's forecasting bias and the topic of the conversation. This can give us a more fine-grained view of how a model is biased in the context of social media moderation. We use a semi-automated pipeline to predict topics using a large language model. Specifically, we use Meta's Llama 3.1 405B. Our strategy is as follows:\n1. Prompt the language model to provide a noun phrase describing the topic of each instance.\n2. Prompt (the same model) to collect the list of sub-topics into higher-level categories.\n3. Iterate step two if the model misses any sub-topics. This process is accelerated with a programmatic check on the model outputs. We re-prompted (in the same conversation context) to tell the model which noun phrases were left out of the current category list.\n4. Manually inspect the final model-generated categories. To improve the categories, we re-organize, combine, and remove small categories (less than 10 instances).\n5. Ask the model to analyze it's own (author adjusted) categories and provide descriptions.\nTopics This process only worked well for the reddit corpus (as manually evaluated by the authors based on diversity and correctness). It produced the following categories (and descriptions):\n\u2022 Social Issues: \"This category encompasses a wide range of topics related to social justice, equality, and human rights. It includes discussions on discrimination, feminism, LGBTQ+ rights, racism, and other forms of social inequality. Sub-topics also explore issues related to family and relationships, such as marriage, child abuse, and parental leave.\"\n\u2022 Politics and Law: \u201cThis category delves into the realm of governance, policy-making, and the legal system. It covers topics such as gun control, immigration, free speech, and electoral politics, as well as issues related to national security, terrorism, and international relations. Sub-topics also examine the role of government, the judicial system, and the relationship between citizens and the state.\"\n\u2022 Economics: \"This category focuses on the production, distribution, and exchange of goods and services. It includes discussions on trade deficits, minimum wage, labor unions, and regulation, as well as emerging topics like cryptocurrency and digital goods. Sub-topics also touch on social welfare and the economic aspects of family relationships, such as alimony and child support.\"\n\u2022 Health: \"This category explores topics related to physical and mental well-being, including vaccination, mental health, and substance use. It also covers issues related to healthcare policy, medical"}, {"title": "4 Experiments", "content": "In general, we use Hoeffding's Inequality to test statistical significance at level a = 0.05. It provides a versatile (albeit, conservative) confidence interval with limited assumptions, making it applicable to accuracy (ACC) and statistical bias (SB)."}, {"title": "4.1 Uncertainty and Forecasting Performance", "content": "RQ1: How does uncertainty-aware inference impact the forecasting performance of language models?\nA: Some language models, especially those that perform poorly initially, benefit from considering uncertainty.\nForecasting Accuracy Results Table 1 shows forecast accuracy across models and datasets with and without the uncertainty-aware prompt strategy. For 3 out of 5 models, the uncertainty-aware strategy leads to improved performance on average. Average increases in accuracy range from 1% up to 5.25%, which on our dataset corresponds to about 3 to 13 more correct predictions, respectively. The Llama 3.1 series (8B and 70B) are the only models which do no benefit from the uncertainty-aware strategy. For the 8B model, performance is unchanged (averaged across datasets). For the 70B model, performance is reduced by nearly 4%. For both datasets, the uncertainty-aware strategy lead to improved performance (on average). Average increases are near 1% for the Wikipedia corpus and the Reddit corpus. The only statistically significant improvement in performance comes when we apply the uncertainty-aware strategy to Mixtral.\nForecasting F1 Results Table 2 shows F1 scores for forecasts across models and corpora. When considering precision and recall of inferences (F1 is their harmonic mean), we find results are largely consistent with those reported for accuracy. Three of five models show improvement, meanwhile both datasets show improvement. Relative performance of models is also consistent: Qwen2 does worst, is improved by the Mistral models, and further improved by the Llama 3.1 series.\nDiscussion Findings indicate that considering uncertainty in the LM forecast either has little impact (on average) or a slight positive one, for certain models. One observation is that the best performing models (the Llama 3 series) are either unaf-"}, {"title": "4.2 Uncertainty and Forecasting Bias", "content": "RQ2: How does uncertainty-aware inference impact forecasting bias?\nA: While some language models consistently under-predict the occurrence of personal attacks, considering uncertainty is able to partially reduce this bias.\nForecasting Bias Results Table 3 shows statistical bias of language model forecasts with and without consideration of uncertainty at inference-time. Uncertain inferences reduce bias for three out of five models. Again, the Llama 3.1 series are the only models that do not show any benefit. In this case, bias is relatively consistent with/without uncertainty (unlike the drop in accuracy for the 70B model observed in Table 1). Bias was often negative, indicating that models typically under-predict the occurrence of a personal attack; i.e., on average, they predict no personal attack when an attack does in fact occur. Only the Llama 3.1 series showed any sign of positive bias (specifically, on the Reddit corpus). Reductions in bias range from 0.05 up to 0.09. In our context, this means use of uncertainty corrected 5 out of 100 or 9 out of 100 false negatives, respectively. For some models (Mixtral and Qwen2), this reduction is statistically significant. Both datasets also experience reduction in bias on average, with 3 out of 100 and 7 out of 100 less false negatives for the Wikipedia corpus and Reddit corpus, respectively. These reductions were not statistically significant.\nInteractions Between Topic and Bias Figure 3 shows the relationship between bias and different topics identified using the method from \u00a7 3 applied to the Reddit corpus. We limit consideration to the Mixtral models and Qwen2, since these models exhibited consistent negative bias (i.e., systematic under-prediction of personal attacks). For traditional inference without uncertainty (traditional CoT), bias is most prominent on Reddit conversations about \u201cEthics and Morality\" followed by conversation about \"Economics\" or \"Tech and Entertainment.\" When uncertainty is considered during inference (uncertain CoT), bias is reduced for all topics. One of the biggest reductions occurs for the \"Economics\" topic. For both forecasting methods, the topics with the lowest overall bias are \"Culture and Identity\" and \"Politics and Law.\"\nDiscussion Findings indicate that most language models exhibit negative statistical bias (systematic under-prediction) when forecasting personal attacks. This lends evidence to our over-arching hypothesis - that AI alignment mechanisms can bias language models against predicting harmful outcomes since under-prediction of a personal attack is indeed a harmful outcome. Of course, it is difficult to confirm this idea without transparent access to training data and methods (for alignment) as well instruction-tuned models, which are guaranteed to be \"un-aligned along the dimensions of interest. In any case, findings also indicate that uncertainty-aware inference with language models is able to reduce negative bias. As before, the impact of uncertainty-aware inference is not consistent across models: the more biased models experience the greatest degrees of bias reduction. For two models, this reduction was even statistically significant. We hypothesize the disparity across models again may be due to a saturation effect, as models which are not consistently biased do not have consistent patterns of \u201creasoning\u201d that can be modified by consideration of uncertainty. We also observe that bias is not uniform across topics, nor is bias reduction (by uncertain CoT). We do not find any consistent properties among topics, which cause more/less bias. Yet, if our overarching hypothesis is correct \u2013 that AI alignment is a cause of bias - then this non-uniformity may be related to the types/amounts of data used during alignment."}, {"title": "4.3 More Benefits of Uncertainty: Scaling", "content": "RQ3: Can post-hoc scaling of uncertainty estimates further mitigate bias without impacting accuracy?\nA: Yes. Scaling consistently produces the least biased and most accurate forecasts.\nForecasting Accuracy Results Table 4 shows F1 scores for language model forecasts with and without post-hoc scaling of uncertainty estimates. Note, this implies we use the uncertain CoT strategy, since scaling is not possible with traditional CoT. Scaling improves F1 scores by almost 20 pts (out of 100) for Mistral models and more than 30 pts for Qwen2. The Llama 3.1 series remain as the \"odd-models-out\u201d with their high performance being maintained after the application of scaling. All datasets also show substantial improvements in F1 score after application of scaling.\nForecasting Bias Results Table 5 shows statistical bias with and without post-hoc scaling. Scaling is able to reduce the magnitude of bias for all models, including three (out of five) statistically significant reductions (i.e., all models except the Llama 3.1 series). Average reduction in bias across datasets is also consistent with statistically significant reduction on the Wikipedia corpus. From Figure 3, we more easily see that scaling tends to lead to slight positive bias (less in magnitude then the original negative bias).\nInteraction Between Forecasting Bias and Accuracy Figure 2 shows bias and F1 score simultaneously via a scatter plot, for all models/data, organized by prompt strategy and use of scaling. Reductions in bias generally correlate with improved accuracy (an apparent quadratic relationship). Use of all proposed methods (uncertainty-aware CoT with scaling) creates a unique cluster of data points with near 0 bias and high F1 score.\nDiscussion Findings show that using a small amount of data for post-hoc scaling consistently improves both F1 score and bias by a relatively large magnitude. We remark, this is a benefit of using uncertainty estimates to make predictions, since post-hoc scaling is not possible for traditional CoT classification. One interesting point is that\nthe Llama 3.1 series remains relatively unaffected by any of our modifications. Again, we believe this to be an effect of saturated (high) performance out-of-the-box. We can understand why scaling works from a mathematical perspective. In particular, the parameter \u03b2 acts to remove systematic biases from the latent score \u017d in Eq. (1). If latent scores are typically higher than they should be (i.e., leading to higher forecast confidence, and thus, over-prediction), the MLE optimization uses\n$\\overline{Z} \\leftarrow log P/(1 - P)$\n$\\overline{Z} \\leftarrow \\frac{\\overline{Z}}{\\tau} - \\beta$\n$P_{new} + 1/(1 + exp(-\\overline{Z}))$."}, {"content": "a \u03b2 to lower these latent scores systematically across all predictions. We hypothesize the reason this correction sometimes leads to positive bias is from over-fitting to the small data sample used for MLE."}, {"title": "5 Conclusions", "content": "This paper studies three research questions about the interaction between uncertainty estimation and forecast bias for social media moderation using language models. Briefly, our findings show how asking language models to represent their uncertainty when forecasting personal attacks can reduce bias and increase accuracy, especially if a small amount of data is available to fine-tune these inferences.\nOne interesting point, which we are unable to address, is the root cause of the biases observed. We speculate this is a result of alignment mechanisms biasing language models against predicting the harmful outcomes we wish to forecast (i.e., personal attacks). Yet, more transparency in language model training is needed to investigate this issue."}, {"title": "Limitations", "content": "As noted in our conclusions, some key hypotheses of our work remain under-explored. Specifically, the cause of observed biases in the language models we study. Working with open-source language models that have closed-source training pipelines makes this a difficult research question to definitely handle. On the other hand, the research questions we do answer may also have limited interpretation outside of the contexts in which we study them; i.e., the specific models and datasets explored in \u00a7 4. A compounding issue of our analysis is the relatively small test sets we explore (200 instances, due to paper budget) which limited the statistical power of our study, as highlighted by the relatively few statistical significant results."}, {"title": "Ethics Statement", "content": "While the focus of this work is on analyzing (and mitigating) the bias of the language models we study, we emphasize that models which employ our proposed techniques still incur some bias. This can have direct, negative impact on users if these models are used for social media moderation in a automated pipeline without appropriate human checks. Even with human checks, if these models are used for decision-making, they may influence their human users in unknown ways, which can have unknown (and vast) negative impacts on online communities where they are deployed. Not to mention, we have only explored a very small subset of the potential biases these pre-trained models can possibly have. Other (social) biases may also exist in these models, which our methods are not explicitly designed to counteract and which can also have negative impacts on (vast) numbers of users if used for semi-automated decision-making. These caveats should be carefully considered and studied before systems like the language models we study are used for any automated moderation decisions.\nOne additional issue is the broader of role content moderation on the internet, and how decisions in content moderation can broadly impact online discourse. The question of who makes moderation decisions, how these decisions are made, and whether moderation should occur at all are each important issues of social debate, which we do not address in this paper. Tacitly, the datasets we study make some claim about what behaviors should be allowed (or not allowed) on online forums, as an-"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Forecasting System Prompt Example", "content": "You are TheoryOfMindGPT, an expert language model at using your theory-of-mind capabilities to predict the beliefs and actions of others in human conversations. You will be given an unfinished conversation between two speakers. Put yourself in the mindset of the speakers and try to reason about the requested conversation outcome. Use the keyword \"ANSWER\" to report your prediction for the outcome of interest. Report your answer on a scale from 1 to 10 with 1 indicating \"not likely at all\" and 10 indicating \"almost certainly\". For example, \"ANSWER = 7\" would mean you think the outcome is fairly likely."}, {"title": "A.2 Forecasting User Prompt Example", "content": "In the following conversation segment, the speakers are negotiating how to allocate available resources among themselves.\n[SEGMENT START]\nSpeaker 0: Hello how are you?\nSpeaker 1: Hello! I am doing well. How about you?\nSpeaker 0: I'm doing well. I'm trying to prepare for this camping trip.\nSpeaker 1: Me too.\nSpeaker 0: What are you looking for?...\n[SEGMENT END]\nNow, fast-forward to the end of the conversation. Will both speakers be satisfied at the end of the conversation? Let's think step by step, but keep your answer concise (less than 100 words)."}, {"title": "A.3 Topic Model System Prompt", "content": "You are TopicClassifierGPT, an expert language model at assigning topics to conversations across the internet. Try to categorize the topic of the conversation using only one or two words, so that your categories can be automatically grouped and analyzed later. Topics should be nouns or noun phrases that provide an answer to the question: \"What are the speakers discussing?\" Use the keyword \"ANSWER\" to report your predicted category. For example, \"ANSWER = Religion\" could be used for a conversation that is broadly about religion."}, {"title": "A.4 Topic Model User Prompt", "content": "In the following conversation segment,\n{same as forecasting prompt}\n[SEGMENT END]\nWhat is the topic of the conversation?"}, {"title": "A.5 Topics", "content": "\u2022 \"Social Issues\": [ \"homophobia\", \"transgenderism\", \"transgender issues\", \"transgender rights\", \"lgbt rights\", \"islamophobia\", \"racism\", \"sexism\", \"discrimination\", \"feminism\", \"social justice\", \"equal pay\", \"body image\", \"objectification\", \"rape\", \"sexual assault\", \"hate speech\", \"slurs\", \"marriage pressure\", \"alimony\", \"child support\", \"parental leave\", \"child abuse\", \"bullying\", \"polygamy\" ],\n\u2022 \"Politics and Law\": [ \"politics\", \"gun control\", \"immigration ban\", \"judicial bias\", \"free speech\", \"affirmative action\", \"abortion\", \"censorship\", \"media bias\", \"socialism\", \"communism vs capitalism\", \"electoral college\", \"government\", \"nationalism\", \"patriotism\", \"travel ban\", \"us-saudi relations\", \"terrorism\", \"military draft\", \"war\", \"nuclear power\", \"capital punishment\", \"self-defense\", \"gun ownership\", \"gun rights\", \"gun regulation\", \"gun violence\", \"dueling laws\", \"prison\", \"corporal punishment\", \"death penalty\", \"military spending\", \"immigration\", \"don't ask don't tell (dadt)\", \"immigration enforcement\", \"immigration policy\" ],\n\u2022 \"Economics\": [ \"economics\", \"cryptocurrency\", \"digital goods\", \"trade deficits\", \"minimum wage\", \"labor unions\", \"regulation\", \"social welfare\", \"alimony\", \"child support\"],\n\u2022 \"Health\": [ \"mental health\", \"vaccination\", \"vaccines\", \"cannabis\", \"marijuana\", \"opium trade\", \"prostitution\", \"sexting laws\", \"necrophilia\", \"veganism\", \"vegetarianism\", \"gmos\"],\n\u2022 \"Culture and ID\": [ \"cultural identity\", \"feminist terminology\", \"islam\", \"indigenous rights\", \"israeli-palestinian conflict\", \"israel\", \"jordan peterson\", \"hillary clinton"}]}