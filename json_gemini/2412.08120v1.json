{"title": "Dense Depth from Event Focal Stack", "authors": ["Kenta Horikawa", "Mariko Isogawa", "Hideo Saito", "Shohei Mori"], "abstract": "We propose a method for dense depth estimation from an event stream generated when sweeping the focal plane of the driving lens attached to an event camera. In this method, a depth map is inferred from an \u201cevent focal stack\" composed of the event stream using a convolutional neural network trained with synthesized event focal stacks. The synthesized event stream is created from a focal stack gen-erated by Blender for any arbitrary 3D scene. This allows for training on scenes with diverse structures. Additionally, we explored methods to eliminate the domain gap be-tween real event streams and synthetic event streams. Our method demonstrates superior performance over a depth-from-defocus method in the image domain on synthetic and real datasets.", "sections": [{"title": "1. Introduction", "content": "Depth estimation enables various applications, including collision avoidance in autonomous vehicles [17] and 3D in-teraction in mixed reality [7]. Approaches to this fundamen-tal task are often phrased as \u201cdepth from X,\u201d where X can be substituted by stereo [1], defocus [2], and even sounds [13]. One of the most feasible solutions is to use a monocular camera to analyze active motions or disparities from one frame to another. However, this approach easily fails in ex-treme conditions such as fast motions and over and under exposures [26]. As mobile measurement devices, conven-tional cameras yet consume electrical power and memory resources too much, limiting further practical applications.\nEvent-based cameras, or event cameras, collect tempo-rally and spatially occasional responses (i.e., events) and, thus, can break some of the limitations of the cumulative nature of conventional cameras [9]. They report the posi-tions, time, and polarity of intensity change at an extremely high frequency. This allows the device to report tens of millions of high-dynamic range responses with significantly less electricity. Consequently, depth from events can lead to an efficient and robust depth estimation [4, 10, 15].\nTherefore, inspired by the previous framework that uti-lizes RGB image focal stacks, a series of images captured at different focus distances, to infer depth and create all-in-focus images [16], we propose a method for dense depth estimation using an \"event focal stack,\" which consists of a sparse set of events corresponding to in- and out-of-focus regions, driven by active lens control. Contrary to the ex-isting approaches that use events from optical flows [15] or estimates depths at exact event locations [4, 10], we design a network that derives a complete depth map.\nTo tackle this task, this paper implements an event fo-cal stack simulator for arbitrary scenes using 3D com-puter graphics software, and event simulators [20, 22]. One straightforward approach is to train solely on synthetic data generated by this simulator. However, our analysis revealed a significant domain gap between the events generated by the simulator and real-world events. To mitigate this gap, we introduce fine-tuning using real data.\nIn summary, we make the following contributions: (1) We propose a deep dense depth estimation method using an event focal stack from lens defocus. To this end, (2) we pro-pose a framework that directly maps an event focal stack to a dense inverse depth image. For this framework, (3) we created our own synthetic and real-world datasets, consist-ing of paired event focal stacks and inverse depth images, to perform training and inference. (4) We also extensively investigated the domain gap between synthetic events from existing simulators and physical events, revealing the limi-tations of the simulators in our task. Based on the findings, we introduced a fine-tuning technique to effectively bridge this domain gap, thereby enhancing the model applicability to real-world scenarios."}, {"title": "2. Related Work", "content": "This section reviews image- and event-based depth esti-mation methods, event simulators, and their challenges."}, {"title": "2.1. Depth from Defocus", "content": "Depth from Defocus (DfD), pioneered by Pentland [2] is a technique for estimating the scene depth by analyzing the amount of blur (defocus) in images. When an image is captured, objects at different distances appear with varying degrees of sharpness or blur. DfD leverages this informa-tion, typically using multiple images with different focus settings, to compute the distance of objects from the cam-era. However, model-based DfD is susceptible to changes in lighting conditions and the scattering of light on surfaces.\nRecent data-driven approaches resolve this issue using a deep neural network trained on a synthetic dataset since defocus analysis is independent of the image domain [16]. The Focus on Defocus framework introduces image focal stacks, a series of images captured at different focus dis-tances, to infer depth and all-in-focus images [16]. A typi-cal U-Net can also learn soft 3D reconstruction, a method that can flexibly perform 3D reconstruction even with miss-ing or noisy data like event, from synthetic focal stacks [14]. However, networks trained in the color domain suffer from extreme conditions such as under and over-exposures and motion blur. Although networks can be trained on images that simulate extreme conditions, optical behaviors are of-ten difficult to simulate appropriately [23].\nNote that recent approaches are powerful enough to es-timate a depth map from a single-shot image [5, 8, 21]. Nonetheless, all these approaches rely on the color domain and are weak under extreme conditions [23]."}, {"title": "2.2. Depth from Events", "content": "Depth-from-event approaches take advantage of the event nature, such as high temporal resolution, high dy-namic range response, and low power consumption. Ex-isting works have explored events from motion and used recurrent networks to handle temporal events [15, 24]. The idea of depth from event focal stack is new, and only a few attempts exist. These approaches estimate a single focus distance for auto-focus [11] and a sparse depth [4]\u00b9.\nInstead, we estimate a complete depth map from sparsely observed events. We collect all events from a lens focus sweep that travels across the volume of interest and voxelize them as an event focal stack. Inspired by the image from events [12], we train our network to infer a dense depth map from the stack instead of an intensity image."}, {"title": "3. Proposed Method", "content": "Figure 1 shows our framework. Given a sequence of event points $e_k = (x_i, y_i, t_i, p_i)_{i \\in [0,N-1]}$ that includes N events, our framework obtains event focal stack, $V\\in \\mathbb{R}^{W\\times H\\times B}$ by segmenting $e_k$ into B time bins of event voxel grids composed of the event frames of size W (width) by H (height). Here, x, y indicates 2D location, t and p represent the timestamp when the event occurred and the polar-ity information indicating whether the brightness increased or decreased, respectively. Event focal stack V is fed into the depth map generation network to generate depth map $D_{pred} \\in \\mathbb{R}^{W\\times H}$, which is our output.\nFor training, our method utilizes RGB image focal stacks from a simulation of the imaging process in virtual scenes using 3D computer graphics software, followed by event"}, {"title": "3.1. Constructing Event Focal Stack", "content": "Event points e form a 3D point cloud with high mem-ory demands. We generate an event focal stack to reduce computational complexity by quantizing the event points along the time axis. One of the straightforward ways is to segment the event points into fixed time windows and record the number of events occurred at each pixels within that window on a single frame. However, this approach re-sults in the loss of a significant amount of temporal informa-tion. Inspired by the task of generating gray-scaled images from event data [12], we generate time-weighted event vox-els [27] to address this issue.\nTo generate V of B \u226a N from the observed events $e_i$, we followed the voxelization method of Alex et al. [27]. This 'voxelization' process is outlined as follows:\nNormalize timestamps into the size, B (0 < t < B).\nA polarity value, $p_i$, is linearly weighted depending on the distance to the two closest bins, $(b_k, b_{k+1})$.\nStore the values to the corresponding voxel location, $(x_i, y_i, b_k)$ and $(x_i, y_i, b_{k+1})$\nAfter collecting all events, voxel data is normalized by the min-max values in each dimension.\nWe found the practical best bin size is B = 5 (Tables 1 and 2), consistent with the work for a similar task [12]."}, {"title": "3.2. Depth Map Generation Network", "content": "Following existing gray scale video reconstruction ap-proach [12], we use U-Net [18] like architecture for a dense depth map estimation. To apply [12] to our task, we remove the recurrent functionality and modified the model to gener-ate a single dense depth map. Here, we treat inverse depth image as the output, in order to enhance the resolution of the near-field depth.\nThe input of the network is an event focal stack, which consists of B = 5 event frames (each frame has a size of 256 x 256). It is fed into the encoder, which has four 2D convolutional layers, to the feature map with a size of 512 \u00d7 16 \u00d7 16. Then, it is fed into an intermediate layer containing two convolutional layers, where it is trans-formed into 512 \u00d7 16 \u00d7 16 feature map, and then passed through a four-layer decoder. In each layer of the decoder, by adding the input features to the corresponding encoder features through skip connections, the location information of the events is conveyed. Finally, the output layer converts the 32 \u00d7 256 \u00d7 256 feature maps into 1 \u00d7 256 \u00d7 256 single-channel gray-scale depth image.\nWith the variable @ that contains all trainable parameters, the training objective uses Mean Squared Error (MSE) loss\n$L(\\Theta) = \\frac{1}{HW} \\sum_{x=1}^{H} \\sum_{y=1}^{W} (D_{gt}(x, y) \u2013 D_{pred}(x, y))^2$. (1)\nFor real-world data, we utilize pseudo Dgt generated by Depth Anything [25]."}, {"title": "3.3. Lens Breathing Correction", "content": "To develop a method that works well not only on syn-thetic data but also in real scenes, we train the network on a large synthetic dataset and fine-tuning it with a small amount of real data. Unlike synthetic data with perfect op-tics, real event data is affected by slight distortions due to lens breathing during the focus sweep (Figure 2). Lens breathing is a phenomenon that involves varying fields of view as the lens focus distance changes. It is challenging to mitigate this effect optically for physical cameras. These distortions can be a major factor in the domain gap be-tween synthetic and real data. Simply fine-tuning the model with raw real-captured data is insufficient to cope with our method effectively.\nWe geometrically correct lens breathing. Specifically, we calculate homography matrices $H[k] \\in \\mathbb{R}^{3\\times 3}$ from a reference plane in focus to the others at different focuses. For accurate homography calculations, we use a circular checkerboard, which is more robust against lens defocus blur than a typical square grid pattern. We recorded 330 images of a defocus-robust circular checkerboard at fixed distances of up to two meters. Using the temporally closest homography matrix, we warped event coordinates, $(x_i, y_i)$."}, {"title": "4. Experimental Settings", "content": "We create our synthetic and real datasets since no dataset is publicly available for this task. Our strategy is to config-ure the best-performing network and voxelization approach and then evaluate the applicability to the real dataset."}, {"title": "4.1. Datasets", "content": "Synthetic Dataset. To generate synthetic data for a wide variety of scenes, we rendered scenes of Thingi10K objects [19] with random positions and materials using Blender [3]. We set the camera parameters in Blender based on the lens parameters used in the real-world environment, with a fo-cal length of 16mm, sensor size of 4.81mm (height) by 6.4mm (width), and an F-number of 2.3. However, it should be noted that these parameters in Blender do not faithfully replicate the actual camera. After rendering image focal stacks of 600 focus distances, we simulated event data using"}, {"title": "4.2. Baseline Method", "content": "We compare our approach with a depth-from-defocus method for the color domain, Focus on Defocus [16]. For a fair comparison, we trained the Focus on Defocus network on our synthetic dataset and used the same bin size (B = 5). Although this network accepts three-channel color images, our real dataset consists of only gray-scale images. For better applicability to real scenes, we converted our origi-nal color images into three-channel gray-scale images and trained the network."}, {"title": "4.3. Evaluation Metrics", "content": "We calculated mean absolute error (MAE) [1/m] (\u2193), and root mean squared error (RMSE) [1/m] (\u2193), for quan-titative metrics:\n\u039c\u0391\u0395 = \u03a3\u03a3|Dpred - Dgt|/(WH)\nRMSE = $\\sqrt{\\frac{1}{WH} \\Sigma \\Sigma (D_{pred} - D_{gt})^2}$.\nMAE and RMSE quantify the average magnitude of errors between prediction and ground truth, and RMSE is more sensitive to outliers. We also discuss qualitative results."}, {"title": "4.4. Implementation Details", "content": "For both the baseline and our methods, we trained for 200 epochs using the Adam optimizer [6] with a learning rate of 1.0 \u00d7 10-4. We randomly rotated the data in W \u00d7 H dimensions for data augmentation. We performed the same evaluations 10 times to mean out the influence of the random seeds for training."}, {"title": "4.5. Parameter Validations", "content": "There are multiple ways to determine the event focal stack (such as differences in bin size and event polarity). Furthermore, by comparing the events generated by event simulators (ESIM, DVS-Voltmeter) with real events, we aim to select and evaluate suitable simulator for this task.\nBin size. The bin size, B, alters the temporal resolution of events in an event focal stack, V, which can affect the overall performance. Although the prior work [12] found B = 5 for their empirical best, the parameter can be task-dependent. Therefore, using our synthetic dataset, we eval-uated our network with differently sized bins of event focal stacks and their impacts on performance. We measured the errors under different bin sizes of {1, 2, 5, 10, 100}."}, {"title": "5. Experiments and Discussion", "content": "Synthetic dataset. Table 5 summarizes the quantitative results. Our method shows superior results to our baseline, Focus on Defocus [16], regardless of the event simulators. Ours with ESIM and DVS-Voltmeter reduced the error by approximately 1.2 and 2.0 times compared to the baseline, respectively. With DVS-Voltmeter, ours performed the best.\nFigure 6 shows estimated depth maps and correspond-ing difference images of the baseline and ours with DVS-Voltmeter in three example scenes. Contrary to the baseline, which loses sharp edges and smooth surfaces, ours keeps the scene structures and is less affected by the texture de-tails. The baseline shows high errors over the image space, and ours successfully suppresses them.\nThe results indicate that our event focal stack can pre-serve denser edge information in a smoother focal sweep, and our simple network can interpret the captured informa-tion into a depth map. On the other hand, an image focal stack has rather sparser or discretized information than ours.\nReal dataset. Figure 7 (left) shows depth maps of the baseline and ours in three real scenes under office lighting conditions. The baseline method and ours without the real-scene fine-tuning tend to show front-back reversed depths. Ours even shows some holes. With fine-tuning, ours can mitigate both the reversed depth and holes and present rea-sonable overall depths, overcoming the domain gap be-tween real and simulated events. However, details such as thin structures and edges are more ambiguous than results in the synthetic dataset. From these results, it can be inferred that ours with fine-tuning is superior to image-based method and there is a domain gap between the synthetic event data used for training and the real-captured event data. Also, they shows fine-tuning with real event is effective method to overcome this problem. Compared to ours with ESIM and DVS-Voltmeter, ESIM can present sharper and smoother scene structures and appears to be a superior choice. This draws the same conclusion as in Section 4.5.\nFigure 7 (right) demonstrates the robustness of our ap-proach under extreme conditions (i.e., low-light condi-tions). Since intensity images capture almost no infor-mation, the baseline method fails to estimate any reason-able depth maps. Ours relies on events sensitive to subtle changes in the imaging sensor and can still grasp the depth information. Same as in the office lighting conditions, with-out fine-tuning, ours shows holes and inconsistent depths.\nLimitation. Our method shows inferior results on texture-less surfaces since it lacks events of depth clues. Also, our method is designed for static scenes. If dynamic content appears, events on the content must be spatially aligned. Furthermore, the performance of our imaging depends on the focus sweep mechanisms. While we used a mechanical lens, a focus-tunable lens can provide a faster focus sweep.\nThe susceptibility of image- and event-based methods in different environments can vary. Different image blur shapes can have a negligible impact on the former because defocus blurs spread evenly. Event-based methods focus-ing on brightness changes are more affected by the position"}, {"title": "6. Conclusion", "content": "In this paper, towards robust method even under low-light conditions, we proposed dense depth estimation from an event focal stack. To this end, we proposed a frame-work and training strategies, including fine-tuning for real-scene datasets. We validated event focal stack data structure and configurations for the best performance. We compared ours with the baseline method using an image focal stack for dense depth map estimation. We identified the domain gap between real and virtual events from lens focus sweeping in our task. Although the image blurs by defocus have fewer domain shifts between real and virtual worlds, events from such blurs are strongly dependent on event simulators. The quantitative and qualitative results showcased that ours, af-ter real-dataset fine-tuning, is more robust. Our future work includes improving algorithms and simulators to further re-duce the domain gap between synthetic and real-world data."}]}