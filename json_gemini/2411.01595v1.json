{"title": "RS-MoE: Mixture of Experts for Remote Sensing Image Captioning and Visual Question Answering", "authors": ["Hui Lin", "Danfeng Hong", "Shuhang Ge", "Chuyao Luo", "Kai Jiang", "Hao Jin", "Congcong Wen"], "abstract": "Remote Sensing Image Captioning (RSIC) presents unique challenges and plays a critical role in applications such as environmental monitoring, urban planning, and disaster management. Traditional RSIC methods often struggle to produce rich and diverse descriptions. Recently, with significant advancements in Vision-Language Models (VLMs), efforts have emerged to integrate these models into the remote sensing domain and to introduce richly descriptive datasets specifically designed to enhance VLM training. However, most current RSIC models generally apply only fine-tuning to these datasets without developing models tailored to the unique characteristics of remote sensing imagery. This paper proposes RS-MoE, a first Mixture of Expert based VLM specifically customized for remote sensing domain. Unlike traditional MoE models, the core of RS-MoE is the MoE Block, which incorporates a novel Instruction Router and multiple lightweight Large Language Models (LLMs) as expert models. The Instruction Router is designed to generate specific prompts tailored for each corresponding LLM, guiding them to focus on distinct aspects of the RSIC task. This design not only allows each expert LLM to concentrate on a specific subset of the task, thereby enhancing the specificity and accuracy of the generated captions, but also improves the scalability of the model by facilitating parallel processing of sub-tasks. Additionally, we present a two-stage training strategy for tuning our RS-MoE model to prevent performance degradation due to sparsity. We fine-tuned our model on the RSICap dataset using our proposed training strategy. Experimental results on the RSICap dataset, along with evaluations on other traditional datasets where no additional fine-tuning was applied, demonstrate that our model achieves state-of-the-art performance in generating precise and contextually relevant captions. Notably, our RS-MoE-1B variant achieves performance comparable to 13B VLMs, demonstrating the efficiency of our model design. Moreover, our model demonstrates promising generalization capabilities by consistently achieving state-of-the-art performance on the Remote Sensing Visual Question Answering (RSVQA) task.", "sections": [{"title": "I. INTRODUCTION", "content": "Image captioning integrates computer vision and natural language processing to automatically generate descriptive text for images, effectively bridging the information gap between visual and linguistic domains. Unlike captioning of common natural images, remote sensing image captioning (RSIC) poses greater challenges due to the inherent characteristics of remote sensing images, which cover larger geographic areas and more diverse geographical objects. Consequently, RSIC tasks demand not only the description of geographic object information but also a comprehensive understanding of the relationships and interactions among various geographic objects.\nEarly studies of RSIC focused on traditional methodologies, including template-based [1] and retrieval-based methods [2], [3]. However, these methods cannot generate rich and varied descriptive sentences. Recent studies [4], [5], [6], [7] commonly adopt the encoder-decoder architecture, which divides the RSIC task into an image encoding phase that extracts semantic features from the input image, and a sequence modeling phase that uses the extracted features to generate text and sentences. Depending on the specific models employed, encoder-decoder approaches can be further categorized into CNN-based encoders with RNN/LSTM decoders, and CNN-based encoders with Transformer decoders. Although these methods have achieved satisfying performance on the RSIC task, they typically generate only one or two simple sentences for captioning, limiting their practical application. This limitation typically arises due to two principal reasons: the simplicity and repetitiveness of the sentences in the datasets used for training models; and the relatively limited ability of these models to extract semantic features and generate complex descriptions.\nThe Large Language Models (LLMs) and Vision Language Models (VLMs) have recently achieved significant success across multiple fields, including computer vision [8], natural language processing [9], and robotics [10]. Particularly, VLMs [11], [12], [13] have effectively narrowed the gap between visual images and textual natural language by advancing the understanding of intermodal relationships, reaching a level of visual comprehension comparable to human capabilities. Therefore, some researchers [14], [15], [16], [17], [18], [19], [20] have recently shifted their focus to applying VLMs to remote sensing visual interpretation tasks. Specifically, for RSIC tasks, the RSGPT [15] introduced the human-annotated RSICap dataset, which provides high-quality and"}, {"title": "II. RELATED WORK", "content": "Remote Sensing Image Captioning (RSIC) poses a challenging task within the fields of computer vision and specialized linguistic modeling, aiming to generate descriptive narratives for remote sensing images by focusing on their visual attributes. Early methodologies predominantly employed template-based [1] and retrieval-based techniques [2], [3]. Template-based strategies involve creating fixed sentence structures and populating them with relevant information, such as attributes and categories displayed within the image. Conversely, retrieval-based strategies develop large-scale databases containing images and their corresponding textual descriptions. These strategies retrieve the most similar image from the database and provide the corresponding description.\nRecent studies [4], [5], [6], [7] have proposed methods based on the Encoder-Decoder framework for achieving RSIC tasks. This architecture typically incorporates an image encoder to extract semantic features from images, which are then interpreted by a sequential model to produce textual annotations. Depending on the model used for the encoder,"}, {"title": "III. METHODS", "content": "Let $I \\in \\mathbb{R}^{W\\times H\\times 3}$ be a remote sensing image, where $W$ and $H$ denote the width and height of the image, respectively, and the 3 represents the RGB color channels. The objective of RSIC is to generate a descriptive caption $S = \\{s_1,\\dots,s_n\\}$, where each $s_i$ is a sentence that effectively summarizes the content of the image. Consider $O = \\{o_1, o_2,...,o_m\\}$ as the set of visible geographic objects within $I$, where each $o_j$ represents an object identified in the remote sensing image. The relationships among these objects can be denoted as $R$, where $R \\subseteq O\\times O$ represents a set of tuples $(o_i, o_k)$, indicating a significant interaction or spatial relationship between object $o_i$ and object $o_k$. An effective captioning model for RSIC should not only identify the objects in $O$ but also interpret the relationships in $R$."}, {"title": "IV. EXPERIMENTS", "content": "We trained our model on 3 NVIDIA A100 GPUs, each with 80GB of memory. We selected three distinct versions of LLMs for use in our MoE model: Llama-3.2-1B [27], Llama-3.2-3B [27], and Vicuna-7B [28]. Each LLM within the MoE Block was individually fine-tuned for only 5 epochs. During the training process, the learning rate was initially set to le-4, and a warm-up phase for the learning rate was applied for one epoch. We used the AdamW optimizer with parameters set to $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, and a weight decay of 0.05, alongside adopting a cosine decay schedule for the reduction of the learning rate."}, {"title": "V. DISCUSSION", "content": "In this section, we conduct a comprehensive analysis of the effectiveness and generalizability of the proposed RS-MoE model from two perspectives. Firstly, we verify the effectiveness of our model and training strategy through three specific ablation studies. Specifically, the first ablation study investigates the impact of varying the number of LLMs within the MoE block. The second study assesses the effects of using different types of LLMs on model performance. The third ablation study focuses on evaluating the effectiveness of our two-stage training strategy. Subsequently, we extend the application of our model to the Remote Sensing Visual Question Answering (RSVQA) task to assess its broader applicability and generalization capabilities across diverse contexts."}, {"title": "VI. CONCLUSION", "content": "This paper introduces RS-MoE, a pioneering Mixture of Experts (MoE)-based Vision-Language Model (VLM) specifically developed for remote sensing image captioning. RS-MoE consists of three core modules: the Image Encoder, VLM Encoder, and MoE Block. At the heart of the framework, the MoE Block leverages an Instruction Router to dynamically generate task-specific prompts, guiding each Large Language Model (LLM) in subsequent layers to focus on distinct aspects of captioning\u2014theme comprehension, object identification, and relationship interpretation. This design allows the model to capture intricate semantic features unique to remote sensing imagery, enabling the generation of detailed, contextually accurate captions. Additionally, we propose a two-stage training strategy to address performance degradation from sparsity issues inherent in MoE-based methods and enhance training efficiency. Our extensive experiments show that, even after fine-tuning on only one dataset, RS-MoE achieves state-of-the-art performance across four RSIC datasets and demonstrates robust generalization capabilities on an RSVQA"}]}