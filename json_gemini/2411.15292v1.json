{"title": "Influence functions and regularity tangents for efficient active learning", "authors": ["Frederik Eaton"], "abstract": "In this paper we describe an efficient method for providing a regression model with a sense of curiosity about its data. In the field of machine learning, our framework for representing curiosity is called active learning, which means automatically choosing data points for which to query labels in the semisupervised setting. The methods we propose are based on computing a \"regularity tangent\" vector that can be calculated (with only a constant slow-down) together with the model's parameter vector during training. We then take the inner product of this tangent vector with the gradient vector of the model's loss at a given data point to obtain a measure of the influence of that point on the complexity of the model. There is only a single regularity tangent vector, of the same dimension as the parameter vector. Thus, in the proposed technique, once training is complete, evaluating our \"curiosity\" about a potential query data point can be done as quickly as calculating the model's loss gradient at that point. The new vector only doubles the amount of storage required by the model. We show that the quantity computed by our technique is an example of an \"influence function\", and that it measures the expected squared change in model complexity incurred by up-weighting a given data point. We propose a number of ways for using this quantity to choose new training data for a model in the framework of active learning.", "sections": [{"title": "Background", "content": "The sub-discipline of computer science called machine learning is concerned with modeling observations of any kind, whether \"real-world\" or simulated. It is used in many important applications today, including everything from AI language models to search engines to geo-graphical models. The input to a machine learning problem typically consists of a sequence of \"exchangeable\" observations, which is to say that the ordering of the observations is con-sidered irrelevant. A typical approach to modeling these observations entails searching for values for some numerical (i.e. floating point) variables or parameters controlling the pre-dictions of a probabilistic symbolic model of the input, such that the predicted probability of the set of observations is maximized. Equivalently, a parameter vector may be sought that minimizes some additive measure of model error. This is called regression, and it seems fair to say that most machine learning applications are based on some regression formalism.\nThe input to a regression problem often consists of data with human-generated labels, which are usually relatively more expensive to procure than the unlabeled data points. It has long been recognized that machine learning models could be trained more efficiently by giving them a way to choose unlabeled data points for which to query new labels, given that not all data points are equally informative to the model. We propose a new method for solving this problem, the primary strength of which is efficiency, as the method can be used with even the very largest models, and does not add to the time complexity of regression. The aforementioned problem of finding data points for which to query labels is called active learning. There have been attempts to produce efficient active learning query selectors using influence functions, which measure the derivatives of some computed quantity such as model parameters with respect to the infinitesimal up-weighting of a data point. We show that our proposed method is an example of the use of an influence function, which in our case estimates the effect of a candidate data point on some measure of model complexity (called the regularizer, which is often already a part of the model). We also show that an established algorithm for computing influence functions called LiSSA [1, 2] is a special case of an algorithm we propose for our method, called stochastic gradient descent with forward-mode automatic differentiation, or SGDF.\nThere is a good deal of published research which touches on some of the ideas in this paper, yet although the algorithm we propose is very simple, we have not discovered where it has been proposed before. The rest of this section gives a review of some relevant background in the published Machine Learning literature, which might make more sense after reading sections 2 and 3."}, {"title": "Definitions and preliminaries", "content": null}, {"title": "Regression and regularization", "content": "Consider the problem of least squares linear regression, also called ordinary least squares, which is to minimize the function\n$f(\\theta) = \\frac{1}{n} \\sum_i (\\theta^T x_i - y_i)^2$ (1)\nwhere $\\theta$ is a vector of parameters in $\\mathbb{R}^p$ and ${z_i = (x_i, y_i)|i = 1 ... n}$ is a set of data points in $\\mathbb{R}^p \\times \\mathbb{R}$. The function $f$ is called the empirical risk. The response variables $y_i$ may also be vector-valued, in which case the square loss terms $(\\theta^T x_i - y_i)^2$ became $L_2$ norms $|\\theta^T x_i - y_i||^2$ where $\\theta$ is now a matrix. The normalization coefficient $\\frac{1}{n}$ makes the objective an average of squared differences, and this is often desirable when thinking about the training"}, {"title": "The Bayesian interpretation and cross-validation", "content": "The Bayesian interpretation of a regression problem can be formulated by taking the expo-nent of the negative of the empirical risk function, which is then treated as a probability. Ideally, this results in a maximum likelihood estimation (MLE) optimization problem, which refers to maximizing the likelihood of some parameters $\\theta$ of a model given some data\n$\\theta^* = argmax_\\theta P(z|\\theta) = argmax_\\theta \\prod_j P(z_j|\\theta)$ (6)\nMLE has the nice property of being invariant to changes in the scale or representation of its parameters $\\theta$. However, sometimes we need to place a prior on $\\theta$, which might correspond for example to the use of a regularizer term in the regression problem:\n$\\theta^* = argmax_\\theta \\int P(z|\\theta)P(\\theta|s)d\\theta$ (7)\nThis is an example of a maximum a posteriori (MAP) optimization problem because the parameter vector $\\theta$ now has a probability distribution attached to it. If we change the"}, {"title": "Gradient descent and stochastic gradient descent", "content": "Optimization of the empirical risk function $f$ can be done using matrix arithmetic in ordi-nary least squares regression, but with more general loss functions it is usually performed using gradient descent:\n$\\theta_{t+1} = \\theta_t - \\eta_t \\frac{\\partial f}{\\partial \\theta} (\\theta_t)$ (26)\nwhere $\\eta_t$ is a step size that typically varies from one iteration to the next according to some schedule heuristic. Many algorithms make small adjustments to $\\eta$ by tracking the movement of $\\theta$ during the algorithm (as in Adam or Adagrad [17]).\nFor problems with large numbers of data points, the gradient descent updates can be modified to only consider one point, or a small batch of points, at a time. This is called stochastic gradient descent (SGD):\n$\\theta_{t+1} = \\theta_t - \\eta_t \\frac{\\partial L}{\\partial \\theta} (z_{ut}, \\theta_t)$ (27)"}, {"title": "Influence functions", "content": "Machine learning engineers may be interested in estimating the sensitivity of the optimum $\\theta^*$ to small changes in the objective function $f$, for example if one data point is given slightly more weight (Koh, Liang 2017):\n$f(\\theta,\\kappa, \\epsilon) = \\sum_i L(z_i,\\theta) + \\epsilon L(z,\\theta)$ (30)\nWe can use calculus to derive a general formula $\\frac{d \\theta^*}{dt}$ relating changes in the location of an extremum $\\theta^*$ to changes in a second function parameter $t$. Suppose we have a function $f(\\theta,t)$ which we wish to minimize with respect to $\\theta$:\n$\\theta^*(t) = argmin_\\theta f (\\theta, t)$ (31)\nWe are interested in knowing $\\frac{d \\theta^*(t)}{dt}$, in other words how much the optimal $\\theta$ will change when we vary the \"hyperparameter\" $t$. Under certain smoothness assumptions, we have, from the stationary property of the optimum,\n$\\frac{\\partial}{\\partial \\theta} f(\\theta^*,t) = 0.$ (32)\nTaking the derivative with respect to $t$ and applying the chain rule, we get\n$\\frac{d}{dt} (\\frac{\\partial}{\\partial \\theta} f(\\theta^*,t)) = \\frac{\\partial^2 f}{\\partial \\theta^2} (\\frac{\\partial \\theta^*}{\\partial t}) + \\frac{\\partial^2 f}{\\partial \\theta \\partial t} (\\theta^*,t) = 0$ (33)"}, {"title": "For cross-validation", "content": "Here is another example of how we can apply these influence function formulae to the analysis of a machine learning task. We can approximate the generalization error, which is"}, {"title": "In active learning", "content": "Influence functions can also theoretically be used for active learning by helping us identify new data points to request labels for [10, 12]. Here, active learning refers to the semisu-pervised learning task of querying a human or other source of data to provide labels for"}, {"title": "Observations", "content": null}, {"title": "Regularity tangents", "content": "We have now, we hope, laid enough of a foundation to introduce and motivate our influence-based selection criterion for active learning. Rather than using the change in optimal parameter values $\\theta^*$ induced by the up-weighting of a given data point, we can look at how $\\theta^*$ changes when we adjust the regularity hyperparameter $s$:\n$\\frac{d \\theta^*(s)}{ds} = -(\\frac{\\partial^2 f}{\\partial \\theta^2})^{-1} (\\frac{\\partial^2 f}{\\partial s \\partial \\theta}) = -H^{-1}\\rho$ (59)\nwhere the \u201ccomplexity gradient\" $\\rho = \\frac{\\partial}{\\partial \\theta} (\\frac{\\partial}{\\partial s} R(s, \\theta))$ depends only on the regulariza-tion term of the objective $f$ due to the partial with respect to $s$. (R is from equation 5.) For $L_2$ regularization, $\\rho$ will be a vector proportional to $\\theta^*$.\nThis quantity has the advantage of being global over the data set, so that it only has to be calculated once. Computing $-H^{-1}\\rho = \\frac{d \\theta}{ds}$ will allow us to quickly calculate for any given data point $z$ the quantity\n$\\frac{d L(z, \\theta^*)}{ds} = (\\frac{\\partial L(z, \\theta^*)}{\\partial \\theta})^T \\frac{d \\theta^*}{ds}$ (60)"}, {"title": "Regularity tangent derived query heuristics", "content": "This section explores additional query heuristics aside from the \"squared loss derivative\" heuristic introduced in section 3.1, which may also make use of the regularity tangent. The body of this section has been removed from this draft for reasons that we cannot disclose."}, {"title": "Influence functions and regularity tangents in an example regression problem", "content": "This section presents some plots illustrating the calculation of the squared loss derivative (SLD) query heuristic for a simple polynomial regression problem with degree 5 (i.e. having six parameters, which are the coefficients of the polynomial).\nThe responses are calculated as6\n$y(x) = F(x; \\theta) = power(x)^T \\theta$ (72)\nwhere power(x) = (1, x,..., x5) is a 6-element vector containing powers of x (or a 6-column matrix, if x is a vector, with each row corresponding to one data point). If x is a vector of unlabeled data points, and y contains the labels, the optimal $\\theta$ may be calculated as\n$\\theta = (X^T X + sI)^{-1} X^T y$ (73)"}, {"title": "Regularity in a multi-user setting", "content": "This section describes the hierarchical use of regularization in a setting where individual users are attempting to train models that exchange information by inheriting from a shared template model containing all of the user interactions. This setting is important because multiple users deserve multiple models, and therefore multiple different regularities; and yet it may be desirable to combine information learned from each user so that interactions are not wasted. Particularly when a user is new to the system, we would like the first few interaction cycles to benefit from knowledge that had been gained from other users, so that training the model doesn't have to restart from scratch each time.\nA simple version of this idea might use the same regularization coefficient for each user, but to use a regularizer that penalizes the deviation of $\\theta$ from some global parameter vector"}, {"title": "Computing influence functions", "content": "We have been talking about influence functions and other quantities which, according to the implicit function theorem, are constructed by multiplying the negative inverse Hes-sian matrix with one or more gradient vectors. We have so far set aside the question of calculating the Hessian H or inverse Hessian H-1. Although we have been motivated by efficiency concerns to propose approximations based on differential calculus to estimate the effects of adding or removing a data point from the corpus, so as to avoid the need for retraining the model after each addition or subtraction, these approximations have re-sulted in formulae involving the Hessian which can itself be intractable to compute. If the number of parameters $p$ is very large then the Hessian, which has $p^2$ entries, may even be impossible to store in memory, let alone invert. This is the case for many commonly-used models in machine learning. Some modern models are so large that even the parameter updates of Stochastic Gradient Descent would be prohibitively expensive but for the fact that the gradient vectors $\\frac{\\partial L}{\\partial \\theta}$ are designed to be sparse through the use of normally-zero activation functions like the \"ramp function\" max(x,0) in the network that defines the model. However, we can show that it is possible to calculate influence functions efficiently even in the case of such very large models, with the same time complexity as training the model.\nTo explain this, some familiarity with automatic differentiation is useful. Automatic differentiation is a family of methods based on the chain rule of calculus for computing the derivatives of programs. These methods generally fall into two classes: reverse-mode au-tomatic differentiation, also knows as back-propagation; and forward-mode automatic dif-ferentiation, which may be implemented using \"dual numbers\". Reverse-mode automatic differentiation computes the derivative of a single output variable with respect to multiple input variables, while forward-mode automatic differentiation computes the derivative of"}, {"title": "Equivalence of SGDF and LiSSA", "content": "Our \"SGDF\" algorithm computes $\\frac{d \\theta^*(s)}{ds}$, which as we have seen can also be written as $\\frac{d \\theta^*(s)}{ds} = -(\\frac{\\partial^2 f}{\\partial \\theta^2})^{-1} \\frac{\\partial^2 f}{\\partial s \\partial \\theta} = -H^{-1}v$ where $H = \\frac{\\partial^2 f}{\\partial \\theta^2}$ is the Hessian of $f$ and $v = \\frac{\\partial^2 f}{\\partial s \\partial \\theta}$ is a gradient of the regularizer term. It is straightforward to combine the two SGDF updates defined above, in equations 104 and 108, so that the regularizer is taken into account with every update. We show that the resulting algorithm generalizes an existing algorithm called LiSSA proposed by Agarwal in 2017 [1]. The purpose of LiSSA is to compute inverse-Hessian vector products $H^{-1}v$ from an objective function $f$ that can be written as an average of loss functions calculated at different data points, i.e. $f(\\theta) = \\sum_i L(z_{u_i}, \\theta)$. The LiSSA algorithm requires that we have the ability to compute Hessian-vector products for the Hessians of the loss function evaluated at random data points, which as we have said can be done easily using standard automatic differentiation libraries. The LiSSA algorithm is based on the observation that\n$v H^{-1} = v \\frac{I}{I - (I - H)} = v \\sum_{k=0}^{\\infty} (I - H)^k$ (112)\n$= ((((...)(I - H) + v) (I - H) + v) (I - H) + v$ (113)\nwhere the expansion on the second line is an application of a well-known trick for computing polynomials without exponentiation.\nIf $h_t$ is our current approximation to $H^{-1}v$ then this gives the update\n$h_{t+1} = v + (I - H)h_t$ (114)\nSince H is an average of loss Hessians at each data point, we can approximate it stochas-tically by substituting the loss Hessian at a random data point. If we do this at every update, we get Agarwal's \"LiSSA-sample\u201d algorithm, which is usually called LiSSA:\n$h_{t+1} = v + (I - l_t)h_t$ (115)\nwhere $l_t$ is defined as the loss Hessian $\\frac{\\partial^2 L(z_{u_t}, \\theta^* )}{\\partial \\theta^2}$ where $u$ is the (possibly repeating) sequence of random data point indices used for each update. [2, 1]"}, {"title": "SGDF and hyperparameter optimization", "content": "In section 2.4.1 we proposed using influence functions to approximate the LOOCV estimate of the regularized model's generalization error on a set of data points. In this section we return to the model selection theme and explore some possible uses for regularity tangents outside of active learning.\nWe return to the discussion of regularity adjoints, exploring some possible alternative uses outside of active learning. As we have shown, our loss derivative $\\frac{d L(z, \\theta^*(s))}{ds}$ is an in-fluence function in the up-weighting sense, because it is theoretically equal to $Z_{up,reg}$, or the change in regularizer $R(s, \\theta)$ when up-weighting a point z's loss by $\\epsilon$; or in other words $\\frac{d R_s(\\theta^*(\\epsilon))}{d\\theta}$. We now point out that our influence function can also be used to optimize the regularity hyperparameter s during cross-validation. In fact this can be done simultane-ously with SGD(F). Simply divide the data set into two groups, a training set and a test set. Pick a data point z from the training set and use $\\frac{\\partial L(z, \\theta)}{\\partial \\theta}$ and the gradient of the regularizer to update the parameters $\\theta$ (with the current step-size in SGD, keeping track of $\\frac{d \\theta}{d s} = 0$). Then, pick a random data point z from the test set and use the (scalar-valued) \"gradient\" $\\frac{d L(z, \\theta)}{d s}$, calculated as $\\frac{\\partial L}{\\partial \\theta} \\frac{d \\theta}{d s}$, to similarly update the regularity s.\nWe might hope this algorithm to produce a converged parameter vector $\\theta^*$ minimizing the loss on the training set over $\\theta$, simultaneously with a converged regularity s* such that $\\theta^*(s^*)$ maximizes with respect to s the loss on the test set when using parameters $\\theta^*(s)$. This is the standard goal when using cross-validation to optimize a hyperparameter such as s, and it can be achieved using some of the techniques we have presented for calculating $\\frac{d \\theta}{d s}$ with only a constant slow-down to the SGD algorithm per update.\nFurthermore, it should be possible to modify this algorithm by moving points back and forth between the test and training sets, as long as this rearrangement is done sufficiently infrequently that $\\theta^*$ can converge to its new value when the training set changes, but not so infrequently that s* shows excessive variation when the test set changes. Presumably, meeting both criteria requires using widely separate step sizes to updates and $\\theta$. It might also make sense to do these updates in batches, for example following each training batch with a test batch. One might hope that the resulting algorithm would be something like k-fold cross-validation, averaging over multiple test/training splits of the data.\nWe have not yet tested this idea but it is interesting to think about a simple gradient update algorithm like SGD that is capable of performing the same regularity hyperparam-eter optimizations as a methodology based on cross-validation, but on a continuous basis during a uniform training phase, keeping only a single copy of the parameter vector and its regularity tangent in storage, along with all the data points. It is also possible that this modified algorithm would be inefficient due to the need to update s sufficiently slowly.\nIn either case it seems interesting to ask if the existence of a continuous algorithm for jointly optimizing $\\theta$ and s could lead to any insights about the linear algebra behind what is happening in regularized cross-validated regression, whether in the original or the modified (with migration of points between training and test sets) algorithm, and what this could teach us about model regularity. It is tempting to recommend the use of influence"}, {"title": "Second-order optimization methods", "content": "We have been concentrating on gradient descent and its derivatives because they are cur-rently used to train the largest machine learning models. However, optimization methods that use second-order derivative information can be much more efficient, and are mostly applicable to systems where the training data is small enough to fit into one \"batch\", so that one may forego stochastic optimization when training the model. Such situations may in fact be common in interactive settings, as the entire history of a single user's interac-tion with an application, or at least the salient parts of it, is likely to be small enough to fit into one training batch on commodity CPU hardware. Where the Hessian is small enough to be inverted, Newton's Method may be used for training the model. This includes the \"Iteratively Reweighted Least Squares\u201d (IRWLS) algorithm which applies to a class of regression problems called \u201cGeneralized Linear Models\" (GLMs) and is equivalent to Newton's Method. In this complexity domain, since we can invert H, we may of course compute the trained regularity tangent directly as $\\frac{\\partial \\theta^*}{\\partial s} = -H^{-1}\\rho$ (equation 59)."}, {"title": "Conclusion", "content": "Our goal has been to explore ways in which a standard machine learning regression model can be made to express a notion of curiosity. We have done this through the framework of Active Learning, in which a model is made to select unlabeled data points whose (labeled)"}]}