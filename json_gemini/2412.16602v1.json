{"title": "V\"Mean\"ba: Visual State Space Models only need 1 hidden dimension", "authors": ["Tien-Yu Chi", "Hung-Yueh Chiang", "Chi-Chih Chang", "Ning-Chi Huang", "Kai-Chiang Wu"], "abstract": "Vision transformers dominate image processing tasks due to their superior performance. However, the quadratic complexity of self-attention limits the scalability of these systems and their deployment on resource-constrained devices. State Space Models (SSMs) have emerged as a solution by introducing a linear recurrence mechanism, which reduces the complexity of sequence modeling from quadratic to linear. Recently, SSMs have been extended to high-resolution vision tasks. Nonetheless, the linear recurrence mechanism struggles to fully utilize matrix multiplication units on modern hardware, resulting in a computational bottleneck. We address this issue by introducing VMeanba, a training-free compression method that eliminates the channel dimension in SSMs using mean operations. Our key observation is that the output activations of SSM blocks exhibit low variances across channels. Our VMeanba leverages this property to optimize computation by averaging activation maps across the channel to reduce the computational overhead without compromising accuracy. Evaluations on image classification and semantic segmentation tasks demonstrate that VMeanba achieves up to a 1.12x speedup with less than a 3% accuracy loss. When combined with 40% unstructured pruning, the accuracy drop remains under 3%.", "sections": [{"title": "Introduction", "content": "Computer vision has advanced significantly due to deep learning and the availability of large-scale datasets. Convolutional Neural Networks (CNNs) have become foundational for tasks such as image classification [11, 22, 9] and object detection [6, 5, 21]. However, CNNs struggle to capture long-range dependencies. Vision Transformers (ViTs) [3, 18, 24] which utilize self-attention mechanisms, effectively address this limitation but suffer from high computational costs due to quadratic complexity. To mitigate these costs, research has focused on reducing ViT complexity [25, 1, 18, 17, 15], applying model compression techniques [19, 14, 30, 27, 24, 13], and exploring alternative architectures like RWKV and State Space Models (SSMs) [20, 8, 4, 7].\nState Space Models (SSMs) have recently garnered attention in computer vision as efficient and effective alternatives to Vision Transformers (ViTs), demonstrating competitive performance across"}, {"title": "Methods", "content": "2.1 Distribution Analysis of VMamba\nWe conduct an in-depth investigation into the characteristics of each layer's output within the Mamba block of VMamba. The output is denoted as \\(Y_{layer} \\in \\mathbb{R}^{B\\times D\\times L}\\), where B is the batch size, D is the inner channel dimension utilized by the scan algorithm within the Mamba block, and L is 4x of the feature map size HW. Our analysis revealed that for each \\(Y_{layer}\\), the distribution of values across the inner channel dimension is remarkably consistent across different data points, as illustrated in figure 3. This observation raised a critical question: Is the full dimensionality D necessary for each \\(Y_{layer}\\)?"}, {"title": "VMeanba", "content": "Building on the findings from section 2.1, we indroduce a new model inference efficiency optimization method called VMeanba, which computes \\(I_{basis}\\) for each Mamba block using mean operators. We further design a pipeline to select which layers in the model will undergo this optimization.\nVMeanba block. The \\(I_{basis}\\) is derived by having a transform function T that maps the original inputs (A, But, C) to reduced dimension inputs. After processing by the original Mamba block, the output is recovered using an inverse transform function T-1. This entire process can be expressed as equation (2).\n\\[Y_{layer} = T^{-1}(Mamba(T(\\bar{A}, B_{ut}, C)))\\]\nIn this process, T is defined as the mean operator applied along the inner channel dimension axis, and T-\u00b9 is defined as the broadcast operator. While the mean transform may lead to a loss of information, it significantly reduces the dimensionality of the inputs from D to 1, with our experiments demonstrating that model performance is maintained. The computational complexity analysis is provided in B.\nLayer Selection. We developed a pipeline to replace K Mamba blocks with VMeanba blocks. We treat the choices of layers as a hyperparameter, determined using the validation set. Specifically, we calculate the layer impact score Slayer for each layer, and select the layers with the K smallest scores to apply the VMeanba optimization. The impact score is defined by equation (3):\n\\[S_{layer} = Acc(OriginalModel) \u2013 Acc(VMeanba on layer)\\]\nwhere Acc represents the model accuracy on the validation set. The algorithm for this process is detailed in C."}, {"title": "Experiments", "content": "We apply the proposed VMeanba method to two different tasks: image classification and semantic segmentation. The details experiment setup and more experiments are provided in appendix D, E"}, {"title": "Results on image classification and semantic segmentation", "content": "Accuracy versus K Analysis. We applied the VMeanba method to VMamba backbone models for both image classification and semantic segmentation tasks, varying the parameter K, as shown in Figure 4. Our results indicate that the model accuracy remains largely unaffected when an appropriate K value is chosen. However, there is a trade-off exists: increasing K reduces inference time but leads to a more pronounced accuracy decline, as indicated by the arrows in the figure. Striking an optimal balance between accuracy and K is essential. For example, selecting K = 10 for the base model in image classification and semantic segmentation appears reasonable. In cases where accuracy drop is deemed unacceptable, one could still opt for a larger K and retrain the model to recover performance. Since this study focuses on a training-free approach, retraining strategies are left for future work."}, {"title": "Combined with Other Optimization Techniques", "content": "We demonstrated that our VMeanba method can be seamlessly integrated with other optimization techniques to enhance model efficiency. Specifically, we explored the effectiveness of combining VMeanba with unstructured pruning on the VMamba base model for the image classification task using value K = 8. The results are summarized in Table 1. Pruning was applied to weight of linear layer or convolution 2D layer using the 11 norm, with a consistent pruning ratio of 40%. Our findings indicate that the VMeanba method is orthogonal to pruning, as it enhances efficiency while maintaining comparable accuracy, demonstrating that the two techniques can be combined without interference."}, {"title": "Conclusion", "content": "In this work, we introduced VMeanba, a novel, training-free model compression technique that reduces the inference time of the Mamba block in VMamba by applying a mean operation to reduce the dimensionality of input channel tensors in the associate scan operation. Our experimental results demonstrate that VMeanba enhances inference speed and throughput while maintaining competitive accuracy in VMamba.\nThis work contributes to the field by introducing a practical method for improving VMamba's efficiency and suggests future exploration of the dimensionality of input channel tensors and the kernel fusion of the discretization and selective scan operations to improve GPU utilization. Additionally, we envision extending VMeanba to other computer vision tasks to evaluate its broader applicability and scalability."}, {"title": "Preliminaries", "content": "In this section, we introduce some preliminaries of the State Space Model, SSM [10], and two recently proposed methods using SSM, mainly selective state space model (Mamba)[7] and VMamba[16]\nState Space Model (SSM). The SSM is a mathematical model that represents the evolution of a system over time. The model is specified as a set of equations that relate the state of the system to the observations at each time step. The most general form of the SSM is called continuous-time linear dynamical system, which is defined as equation (4).\n\\[\\begin{aligned}h'(t) &= A(t)h(t) + B(t)u(t) \\\\y(t) &= C(t)h(t) + D(t)u(t)\\end{aligned}\\]\nh(t) \u2208 Rn is the state variable at time step t\u2208 R, or usually called hidden variable in recent machine learning literature, u(t) \u2208 Rm is the input, y(t) \u2208 Rp is the output, and A(t) \u2208 Rn\u00d7n, B(t) \u2208 Rnxm, C(t) \u2208 Rp\u00d7m, D(t) \u2208 RP\u00d7m are the system matrices at each time step. Note that in the following context, we treat u(t) and y(t) as scalars, i.e., m = p = 1. The above continuous-time linear dynamical system can lead to a linear time-invariant (LTI) system when the system matrices A(t), B(t), C(t), D(t) are all time-invariant. This LTI SSM then can be written as equation (5). It can be discretized into a discrete-time linear dynamical system, which is defined as equation (6). One of the frequent ways for this transformation utilized in the literature related to SSM is zero-order hold (ZOH) discretization, which is defined as equation (7). Besides, it can further written as a convolution form (8).\n\\[\\begin{aligned}h'(t) &= Ah(t) + Bu(t) \\\\y(t) &= Ch(t) + Du(t)\\end{aligned}\\]\n\\[\\begin{aligned}h_t &= \\bar{A}h_{t-1} + Bu_t \\\\Y_t &= Ch_t + Du_t\\end{aligned}\\]\n\\[\\begin{aligned}\\bar{A} &= exp(\\Delta A) \\\\\\bar{B} &= (\\Delta A)^{-1} exp(\\Delta A \u2013 \u0399)\\Delta B \\\\K &= (CB,C\\bar{A}B, ..., C\\bar{A}^\\bar{n}B, ...)\\\\y &= x * K\\end{aligned}\\]\nSelective State Space Model (Mamba). Mamba is the discrete-time linear dynamical system with a timescale parameter \u25b3 that transforms the continuous variables A, B to discrete variables A, B. In addition to discretization, Mamba also relax the time-invariant constraint of the system matrices by introducing selection mechanism, which simply makes several parameters A, B, C to be time-varying by functions s of the input u. Specifically defined as equation (9).\n\\[\\begin{aligned}s_B(u) &= Linear_n(u) \\\\s_C(u) &= Linear_n(u) \\\\s_\\triangle(u) &= Broadcast_D(Linear_1(u)) \\\\\\triangle &= softplus(Parameter + s_\\triangle(u))\\end{aligned}\\]\nThe Lineard is a parameterized linear projection to dimension d, and the T\u2206 = softplus. As the selection mechanism loses the equivalence to convolution form (7), to avoid the sequential recurrence, Mamba further incorporates a work-efficient parallel algorithm, associate scan, into its GPU kernel implementation to facilitate parallel computation of the system.\nVMamba The original Mamba block is designed for 1-dimensional input and output, which is not suitable for computer vision tasks. VMamba proposed a new module called 2D-Selective-Scan (SS2D) for adapting Mamba to 2D input and output. The SS2D module is composed of three steps: cross-scan, selective scan (Mamba block), and cross merge. The cross-scan unfold the input feature map along four directions, forming 4 sets of 1D sequences. Then the selective scan processes each 1D sequence in parallel. The cross-merge finally merges the 4 sets of 1D sequences back to 2D feature map. The cross-scan and cross-merge are called Cross Scan Module (CSM) together, and by this way, the model can have a global receptive field. VMamba further stack multiple SS2D blocks in a layer, and then stack layers to form the whole model."}, {"title": "Computation Complexity", "content": "Complexity of SSM The computational complexity of the associated scan operation in Mamba block, measured in floating-point operations (FLOPs), is derived from processing a sequence of length L, which requires 2L operations. Furthermore, the input to the scan operation incurs an additional cost of 3 FLOPs, leading to a total of 3 \u00d7 2BLD, where B is the batch size, L is the sequence length, and D is the inner dimension.\nIn the context of the SSM system, computations involve multiplications for But and Cht, which amount to 2BLD, and additions for Cht and D, totaling BLD FLOPs. Consequently, the overall FLOPs for the SSM system is 3BLD. The total FLOPs for the Mamba block, therefore, aggregate to 3 \u00d7 2BLD + 3BLD.\nComplexity of reduced SSM The reduction in FLOPs can be achieved by employing the Ibasis, which consists of 9BLd FLOPs, and additional FLOPs for the reduce operation and broadcast operation. The total reduction in FLOPs is summarized by the equation (10):\n\\[FLOP_{original}^{Mamba} = FLOP_{scan} + FLOP_{SSM}\\]\n\\[FLOP_{reduction} =  3 \u00d7 2BLD + 3BLD\\]\n\\[FLOP_{reduced} = 9BLd + FLOP_{reduce\\_op} + FLOP_{broadcast}\\]\nComplexity of VMeanba The mean operator contribute only BLD+BL FLOPs, and the broadcast operator is just a memory operation. The reduced FLOPs is then B(10 + D)L FLOPs, comparing to the original 9BDL FLOPs, we achieve 89% FLOPs reduction (10 << D)."}, {"title": "Algorithm", "content": "Algorithm 1 VMeanba Layer Selection Pipeline\nInput: Model, Dval, K, CalculateScore\nOutput: layersToApply\n1: Scores \u2190 []\n2: for layer in Layers do\n3:\ns\u2190 CalculateScore(layer, Model, Dval)\n4:\nScores Scores + s\n5: end for\n6: Layers \u2190 Sort(Slayer)\n7: layersToApply \u2190 Layers[: K]\n8: return layersToApply"}, {"title": "Experiments Setup", "content": "Datasets. The datasets we use for our VMeanba experiments are the ImageNet-1k dataset [2] for image classification and the ADE20k dataset [28] for semantic segmentation. We only use the validation set of them for the experiments. The ImageNet-1k dataset contains 50k validation images from 1k classes, and the ADE20k dataset contains 2k images for validation, with pixel-level annotations.\nModels. We use the VMamba pre-trained backbone models [16] for both tasks. The backbone models is first trained on the ImageNet-1k training dataset. It is then used as the pre-trained backbone models for downstream task. The segmentation task use the UperNet [26] on top of the VMamba pre-trained backbone models, and trained on the ADE20k training dataset. The VMamba backbone models have three different versions: tiny, small, and base. There are two mainly differences between these versions: the number of layers and the dimension of the L and D in the SS2D block. All of the backbone models have four layers and the tiny version is stack as [2, 2, 8, 2], while the other two versions are stack as [2, 2, 20, 2]. The dimension of the L and D is different across two tasks, both of"}, {"title": "More Experiment results", "content": "them remain the same inside each layer. However, the dimension of the D grows by a factor of 2, and the dimension of the L scale down by a factor of 4 along the layers.\nKernel Implementation. The original CUDA kernel for the Mamba block includes both the discretization and scan operations, dividing the GPU multiprocessor into a 2D grid blocks based on the batch size and inner dimension. In this configuration, multiple threads within the block handle the scan operation. However, since the discretization process is not the focus of this study, and the original approach of dividing the inner dimension across blocks is not compatible with our VMeanba method, we developed a new CUDA kernel. This new kernel exclusively handles the scan operation, with the discretization process executed outside the kernel. All experiments conducted in this paper are based on this optimized kernel. Future work includes integrating the discretization and scan operations into a single kernel for further optimization.\nAdditional Information. The evaluation metric for the image classification task is top-1 accuracy, while for the semantic segmentation task, we utilized all pixel accuracy (aAcc). The batch size for the image classification task is set to 128, whereas for the semantic segmentation task, it is limited to 1 due to the dynamic input size present in the validation set. All experiments were conducted on a single NVIDIA RTX A6000 GPU with 48GB of memory. The profiling was performed using NVTX API, Nvidia Nsight Systems, and Nvidia Nsight Compute tools."}]}