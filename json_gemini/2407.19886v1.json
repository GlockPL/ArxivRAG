{"title": "A Unified Graph Transformer for Overcoming Isolations in Multi-modal Recommendation", "authors": ["Zixuan Yi", "Iadh Ounis"], "abstract": "With the rapid development of online multimedia services, especially in e-commerce platforms, there is a pressing need for personalised recommender systems that can effectively encode the diverse multi-modal content associated with each item. However, we argue that existing multi-modal recommender systems typically use isolated processes for both feature extraction and modality encoding. Such isolated processes can harm the recommendation performance. Firstly, an isolated extraction process underestimates the importance of effective feature extraction in multi-modal recommendations, potentially incorporating non-relevant information, which is harmful to item representations. Second, an isolated modality encoding process produces disjoint embeddings for item modalities due to the individual processing of each modality, which leads to a suboptimal fusion of user/item representations for an effective user preferences prediction. We hypothesise that the use of a unified model for addressing both aforementioned isolated processes will enable the consistent extraction and cohesive fusion of joint multi-modal features, thereby enhancing the effectiveness of multi-modal recommender systems. In this paper, we propose a novel model, called Unified multi-modal Graph Transformer (UGT), which firstly leverages a multi-way transformer to extract aligned multi-modal features from raw data for top-k recommendation. Subsequently, we build a unified graph neural network in our UGT model to jointly fuse the multi-modal user/item representations derived from the output of the multi-way transformer. Using the graph transformer architecture of our UGT model, we show that the UGT model achieves significant effectiveness gains, especially when jointly optimised with the commonly used recommendation losses. Our extensive experiments conducted on three benchmark datasets demonstrate the superiority of our proposed UGT model over nine existing state-of-the-art recommendation approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "In specific domains such as fashion [35], music [22], and micro-video recommendation [40], the effectiveness of recommender systems can be improved by using multi-modal data sources that the users usually interact with (e.g., product images and descriptions, users' reviews, audio tracks). Indeed, several previous studies [20, 32, 43] have proposed multi-modal recommender systems that leverage multi-modal (i.e., audio, visual, textual) content data to augment the representation of items, so as to tackle the problem of data sparsity in the user-item matrix.\nRecent research [4, 20] has shown the importance and benefits of coherently using multi-modal data in enhancing multi-modal recommender systems. Multi-modal feature extraction, which aims to derive meaningful patterns and information from multiple types of data, is fundamental to the initial stage of any multi-modal recommendation pipeline. Indeed, effectively performing such feature extraction is critical for ensuring high-quality recommendations [20, 43]. On the other hand, multi-modal fusion is also crucial for creating user/item representations from various contextual data, including the items' images and descriptions. Such a fusion typically occurs after feature extraction. In fact, most existing multi-modal recommendation models primarily function as a fusion component, designed to merge the extracted features into a joint representation of the items [43]. Figure 1 illustrates how multi-modal recommendation typically consists of a pipeline of workflows ranging from data ingestion and feature extraction, to multi-modal fusion and optimisation. Within this pipeline, two important problems emerge, namely, the use of (i) an isolated feature extraction process (c.f. Isolation 1 in the figure) and (ii) an isolated modality encoding process (c.f. Isolation 2). These two problems impede the system from exploring the full potential of the multi-modal data by creating isolated processes, as they ignore the collaborative power of unifying various modalities and leveraging an integrated architecture that effectively combines the extraction process with the fusion process. The 'Isolation 1' problem refers to the challenges arising from employing an isolated feature extraction process within each workflow of the multi-modal recommendation system. Specifically, the pre-trained feature extractors function independently alongside the subsequent fusion component. This isolated extraction process potentially leads to the incorporation of non-relevant information to the subsequent fusion component, thereby impeding the extraction of more effective multi-modal features. On the other hand, as indicated by the 'Isolation 2' problem in Figure 1, the existing recommender systems process each modality separately before finally fusing them with simple concatenations [34, 40]. Such an isolated modality encoding process misses opportunities to jointly optimise the user/item embeddings resulting from the different modalities.\nTo address these isolated processes for both feature extraction (Isolation 1) and modality modelling (Isolation 2), we introduce a Unified Graph Transformer (UGT) model for top-k multi-modal recommendation. UGT combines a multi-way transformer with a newly designed unified Graph Neural Network (GNN) in a novel cascading graph transformer architecture. We use the multi-way transformer as the extraction component and integrate its output in the unified GNN that serves as the fusion component, so as to address the 'Isolation 1' problem. This new unified GNN within our proposed UGT architecture allows to address the 'Isolation 2' problem by uniformly aggregating the users/items' neighbour information and their corresponding multi-modal features, thereby enriching the multi-modal fusion for better user/item representations. The integration of the multi-way transformer and the unified GNN enables a simultaneous optimisation of the extraction of effective multi-modal features and their subsequent fusion, thereby solving the isolated feature extraction process. Our work is the first to use a multi-way transformer as an extraction component in multi-modal recommendation, thereby going beyond its current use in the literature for encoding various modalities (e.g., images, text) within a shared transformer block [1] in Image-Text retrieval or Visual Question Answering (VQA) [6, 7, 31]. Indeed, we instead apply the multi-way transformer in a recommendation setting to transform heterogeneous data into a unified latent space, thereby producing aligned embeddings from each modality. We then use our proposed unified GNN component to seamlessly fuse the resulting aligned multi-modal embeddings, a significant departure from the existing models that process modalities separately.\nIn summary, our contributions in this paper are three-folds: (1) We propose a novel Unified Graph Transformer (UGT) model to resolve the isolated feature extraction problem in multi-modal recommendation. To the best of our knowledge, UGT is the first end-to-end multi-modal recommendation model that directly facilitates multi-modal recommendation from raw data. Our UGT model not only extracts the items' features and integrates them with the user-item interactions but also simultaneously optimises extraction and fusion under a unified architecture; (2) We introduce a new unified GNN as the fusion component to uniformly fuse the user-item interactions and their multi-modal features in a unified manner. This unified approach addresses the problem of isolated modality encoding in the existing multi-modal recommender systems, so as to enhance the recommendation performance by ensuring a more effective modelling of the target users from the items' images and descriptions; (3) We conduct extensive experiments on three public datasets to demonstrate the effectiveness of our UGT model in comparison to nine existing state-of-the-art multi-modal recommender systems. Moreover, we provide visualisations and measure the average Mean Square Error between the visual and textual embeddings to show that UGT successfully aligns these embeddings to a unified and tighter semantic space."}, {"title": "2 RELATED WORK", "content": "In this section, we position our work in relation to related methods and techniques in the literature, namely multi-modal recommendation (Section 2.1) and graph transformers (Section 2.2)."}, {"title": "2.1 Multi-modal Recommendation", "content": "Existing multi-modal recommendation models incorporate multi-modal data sources for items into the traditional Collaborative Filtering (CF) paradigm through the application of deep or graph learning techniques alongside different feature extraction methods [20, 43]. All such models capitalise on the items' multi-modalities to enhance the recommendation performance. For instance, VBPR [10] enhanced the BPR method [25] by modelling the users' preferences in relation to visual information and extracted visual features from the items' images through a pre-trained Deep Convolutional Neural Network (CNN) [5]. Moreover, MMGCN [34] used Graph Convolutional Networks (GCN) to propagate modality-specific embeddings and to capture the users' preferences related to these modalities. For feature extraction, MMGCN employed a variant of the Residual Network model (ResNet50 [9]) for visual features, Sentence2Vector [16] for textual features, and VGGish [8] for acoustic features. MMGCL [40], SLMRec [28] and BM3 [45] followed the same feature extraction methods as MMGCN. However, MMGCL proposed a self-supervised graph learning model that leverages modality-based edge dropout and modality masking to capture the complex user preferences. On the other hand, SLMRec constructed fine and coarse spaces to align features across modalities, then merely applied a contrastive learning loss for multi-modal recommendation. BM3 bootstrapped the user/item node embeddings through a dropout augmentation and reconstructed the user-item graph for multi-modal recommendation. In a different line of work, LATTICE [42] built item-item graphs from multi-modal features and uncovered latent semantic structures between items. Later, FREEDOM [44] further improved LATTICE by pruning popular edges on the item-item graphs. Both LATTICE and FREEDOM used Deep CNN [5] and Sentence-Transformer [24] for visual and textual feature extraction, respectively. While each multi-modal recommendation model leverages features extracted from pre-trained extraction models and employs specific strategies like Graph Neural Network (GNN) for multi-modal fusion, these recommendation models primarily rely on pre-trained extraction models for feature extraction. As a result, the use of pre-trained models for extraction isolates the extraction step from the entire process of multi-modal recommendation. This isolation occurs because the extraction models are not tailored or fine-tuned for the multi-modal recommendation task, thus potentially incorporating non-relevant information that could harm the item representations in a multi-modal recommendation context. In contrast to previous multi-modal recommendation models that use separate extraction models, in this paper, we propose instead a novel unified graph transformer model that seamlessly integrates the extraction and fusion processes by combining a multi-way transformer with a unified GNN in a cascading architecture, thereby overcoming the isolated extraction process (Isolation 1) in multi-modal recommendation."}, {"title": "2.2 Graph Transformers", "content": "The integration of transformer architectures into graph neural networks has been proposed as a promising solution to enhance existing Graph Neural Networks (GNNs) [38, 41]. In the recommendation domain, taking advantage of because of its contextualised information modelling, the transformer network recently drew much research attention from the recommendation research community to explore the relationships among items. For instance, prior works [2, 19] leveraged GNNs to model the item embeddings within a user's sequence of items, subsequently integrating them into transformer-based recommendation models (e.g. SASRec [13], Bert4Rec [27]) in order to better model user behaviour sequences. Moreover, existing approaches [3, 17, 36, 37, 39, 46] also integrated the vanilla transformer with a GNN to exploit the global user-item relationships. Such approaches enhance the recommendation performance by going beyond merely aggregating local neighbours to encompassing a global context. Different from these sequential and general recommendation models, the recent graph transformers in multi-modal recommendation [21, 33] explicitly modelled the correlation between the item representations by jointly extracting the items' images and descriptions as node features. Specifically, PMGT [21] constructed an item-item graph from pre-extracted multi-modal features and used a transformer network to model the item embeddings with their corresponding neighbours. LightGT [33] proposed a simplified graph transformer architecture, featuring a single-head attention to distil each type of pre-extracted multi-modal features. Different from the aforementioned multi-modal graph transformers that process each modality individually, we introduce a new unified GNN as a fusion component. This fusion component is closely paired with a multi-way transformer in our UGT architecture so as to effectively fuse the extracted multi-modal features from the multi-way transformer. In particular, we incorporate a novel attentive-fusion component within the unified GNN to uniformly fuse the resulting multi-modal features from the multi-way transformer, thereby addressing the problem of isolated modality encoding (Isolation 2) in multi-modal recommendation."}, {"title": "3 MODEL ARCHITECTURE", "content": "In this section, we describe our UGT model for top-k multi-modal recommendation. We first introduce the architecture of our UGT model in Section 3.2, which is illustrated in Figure 2. Section 3.3 presents the unified multi-modal encoding process of our proposed multi-way transformer. Then, in Section 3.4, we introduce the unified GNN, which explicitly fuses multi-modal features and user-item interactions into final user/item representations. In Section 3.5, we describe the optimisation of UGT with its corresponding losses."}, {"title": "3.1 Preliminaries", "content": "Let \\(U = \\{u\\}\\) denote the user set and \\(I = \\{i\\}\\) denote the item set. The input ID embeddings of user \\(u\\) and item \\(i\\) are \\(E_{id} \\in \\mathbb{R}^{d \\times (|U|+|I|)}\\). \\(d\\) is the dimension of the user/item embedding. Then, we denote each item modality feature as \\(E_{i,m} \\in \\mathbb{R}^{d_m \\times |I|}\\), where \\(d_m\\) is the dimension of that modality's features, \\(m\\in M\\) is a modality, and \\(M\\) is the set of modalities. In this paper, we mainly consider visual and textual modalities because the datasets used only contain these two types of raw data, hence \\(M = \\{v, t\\}\\). The user's historical behaviour data is denoted by \\(R \\in \\{0, 1\\}^{|U|\\times |I|}\\), where each entry \\(R_{u,i} = 1\\) if user \\(u\\) clicked item \\(i\\), otherwise \\(R_{u,i} = 0\\). The historical interaction data \\(R\\) can be seen as a sparse behaviour graph \\(G = \\{V, \\mathcal{E}\\}\\), where \\(V = \\{U \\cup I\\}\\) denotes the set of nodes and \\(\\mathcal{E} = \\{(u, i) | u \\in U, i \\in I, R_{ui} = 1\\}\\) denotes the set of edges. The purpose of the multi-modal recommendation model is to accurately predict the users' preferences by ranking items for each user according to the predicted preference scores \\(\\hat{y}_{ui}\\)."}, {"title": "3.2 Model Overview", "content": "Figure 2 shows the model architecture of UGT, our proposed graph transformer model, which is composed of two main components: an extraction component (i.e., a multi-way transformer) and a fusion component (i.e., a unified GNN). Given an item image, an item text and an item ID, we follow the common practice in the computer vision domain [15] by splitting the item image into patches, flattening these patches, and linearly projecting them to form patch embeddings. Similarly, we obtain the word embeddings from an item's description [23]. Our UGT model then uses the patch embeddings from the image and the word embeddings as inputs. Specifically, we use a shared multi-head self-attention component to extract the items' respective modality embeddings. After the shared self-attention component, we employ two specialised feed-forward networks, known as modality experts, corresponding to the two modalities available in our datasets - visual and textual. These feed-forward networks produce the corresponding multi-modal item embeddings.\nIn our UGT model, the shared self-attention component in the multi-way transformer is designed to align different modalities, while the modality experts are used to capture information unique to each modality. Consequently, the multi-way transformer acts as a unified feature extractor, returning the extracted item's visual and textual embeddings before their combination with the user/item ID embeddings in a later new unified GNN component. We design this integration between the extraction and fusion components to address the isolated extraction problem in exiting multi-modal recommender systems. As illustrated in Figure 2, once the extracted multi-modal item features have been obtained, our model's unified GNN component jointly combines these features with their ID embeddings to produce user/item embeddings based on the user-item interaction graph. The unified GNN is needed in order to obtain a smoother user/item embedding by aggregating the multi-modal features from the neighbours. This aggregation includes both the user-item interactions and the semantic information, which is derived from the multi-modal features processed by the multi-way transformer, into the final user/item representations. As shown in Figure 2, within the unified GNN, we incorporate an attentive-fusion component to fuse the jointly-learned and aggregated embeddings from different modalities, in order to enhance the multi-modal user/item representations. The improved joint multi-modal features in our UGT model are designed to overcome the limitations of modelling each modality separately (Isolation 2) in multi-modal recommendation."}, {"title": "3.3 Unified Multi-modal Encoding", "content": "As discussed in Section 2.1, we propose a multi-way transformer paired with a fusion component to alleviate the isolated extraction problem (Isolation 1) in existing multi-modal recommender systems. Specifically, we use this multi-way transformer to encode the raw images and textual descriptions of items, replacing the feed-forward network with modality experts in a standard Transformer [30]. Using the modality experts allows to capture modality-specific information, with each modality expert specifically handling different item modalities. As described in Section 3.2, the use of Multi-Head Self-Attention (MHSA), shared across modalities, allows the alignment of the visual and textual features. Hence, for a given item, the embedding is defined as follows:\n\\(h_i^{(l)} = LN(h_i^{(l-1)} + MHSA(h_i^{(l-1)}, h_i^{(l-1)}, m_i) + FFN(h_i^{(l-1)}, m_i))\\)  (1)\nwhere \\(h_i^{(l)}\\) is the hidden state of the i-th item in the l-th layer, \\(LN\\) and \\(MHSA\\) are the layer normalisation operation and the multi-head self-attention mechanism [30], respectively, and \\(m_i\\) is the corresponding multi-modal embedding of the item. \\(FFN\\) is the feed-forward network. Hence, the multi-way transformer chooses the right expert among multiple modality experts to handle the input based on both the modality's type and the transformer layer's position. Note that our UGT model can extract features from any number of modalities, not just two as per our used datasets. The multi-way transformer, combined with the fusion component shown in Figure 2, is designed to unify the extraction and fusion processes, addressing the problem of isolated extraction (Isolation 1)."}, {"title": "3.4 Unified Multi-modal Fusion", "content": "In Section 2.2, we argued for the need for a unified approach to process the extracted multi-modal features for an effective fusion, instead of handling each modality individually. Hence, we introduce our new unified Graph Neural Network (GNN), which seamlessly fuses the image and text embeddings in a joint manner, closely integrated with the multi-way transformer. The propagation function of our unified GNN is defined as follows:\n\\(x_i^{(lg)} = (1 + \\epsilon) x_i^{(lg-1)} + (x_{i-vt}^{(lg)} + x_{u-vt}^{(lg)} + x_{u-id}^{(lg)})\\) (2)\nwhere \\(x_i\\) denotes the item representations in the lg-th graph convolution layer; \\(x_{i-vt}\\) and \\(x_{u-vt}\\) denote the joint multi-modal embeddings of the item and its neighbours; \\(x_{u-id}\\) denotes the ID embeddings of the item's neighbours; \\(\\epsilon\\) is a scaling factor that controls the contribution of an item's self-connection with its associated joint multi-modal features. The user embeddings are similarly calculated. To obtain the user/item ID embeddings from the neighbours, we use LightGCN [11] to propagate the ID embeddings of the users and items over the user-item interaction graph. The propagation function of the ID embeddings is defined as follows:\n\\(x_{i-id}^{(lg)} = \\sum_{i\\in N_u} \\frac{x_{i-id}^{(lg-1)}}{\\sqrt{|N_u| |N_i|}}\\) (3)\nwhere \\(N_i\\) and \\(N_u\\) denote the set of neighbours for user \\(u\\) and item \\(i\\), respectively, while \\(|N_u|\\) and \\(|N_i|\\) represent the size of \\(N_u\\) and \\(N_i\\).\nA key addition to our UGT model is to propose an attentive-fusion method to enhance the joint embeddings based on the extracted multi-modal features. Given the obtained visual and textual embeddings, \\(h_{i-v}\\) and \\(h_{i-v}\\), derived from Equation (1), we combine the multi-modal embeddings with an attentive-fusion approach to obtain a unified joint embedding, as follows:\n\\(x_{i-vt}^{(lg)} = a h_i^{(lg)} || (1 - a) h_i^{(lg)}\\) (4)\nwhere \\(a\\) is a learned parameter for the attentive-fusion and \\(||\\) represents the concatenation operation. As a consequence, this fusion approach is designed to uniformly enhance the resulting user/item embeddings by weighting the importance of each modality, in contrast to the simple concatenation operation used in previous works [33, 40]. Our UGT model uses the enhanced joint multi-modal features to address the problem of isolated modality encoding (Isolation 2) in multi-modal recommendation."}, {"title": "3.5 Model Optimisation", "content": "To ensure the effective extraction and multi-modal fusion of the visual and textual modalities, our UGT model employs the Image-Text Contrastive (ITC) loss to train the model in a joint embedding space where the similarity between an item's image and its corresponding text description is maximised, while minimising the similarity between mismatched image-text pairs [18]. The ITC loss is defined as follows:\n\\(L_{ITC} = - \\frac{1}{N} \\sum_{z=1}^{N} log \\frac{p_{u2t}(z)}{\\sum_{z'=1}^{N} p_{u2t}(z')} - \\frac{1}{N} \\sum_{z=1}^{N} log \\frac{p_{t2v}(z)}{\\sum_{z'=1}^{N} p_{t2v}(z')}\\) (5)\nwhere \\(N\\) is the batch size, while \\(p_{u2t}(z)\\) and \\(p_{t2v}(z)\\) are the softmax-normalised image-to-text and text-to-image similarities of the z-th pair, respectively. As a result, this ITC loss simultaneously optimises our UGT model during training so as to extract and fuse more effective multi-modal features from raw data.\nFollowing [40, 42], we also implement a multi-task training strategy in our UGT model that simultaneously optimises the commonly used pairwise ranking loss in recommendation, specifically the Bayesian Personalised Ranking (BPR) [26] loss, denoted as \\(L_{BPR}\\), along with the ITC loss \\(L_{ITC}\\):\n\\(L = L_{BPR} + \\lambda_c L_{ITC} + \\lambda ||\\Theta||^2\\), (6)\nwhere \\(L_{BPR} = \\sum_{(u,i,j) \\in D_s} log \\sigma(y_{ui} - x_u^T x_i)\\)\nwhere \\(\\lambda_c\\) and \\(\\lambda\\) are hyper-parameters that control the strengths of the ITC loss and the L2 regularisation, respectively, \\(\\Theta\\) is the set of model parameters; \\(x_u\\) is the user embedding, \\(x_i\\) denotes the positive item embedding and \\(y_{ui}\\) is the ground truth value, \\(D_s = \\{(u, i, j)|(u,i) \\in R^+, (u, j) \\in R^-\\} \\) is the set of the training data, \\(R^+\\) indicates the observed interactions and \\(R^-\\) indicates the unobserved interactions, while \\(\\sigma(\\cdot)\\) is the sigmoid function. Hence, we optimise both the multi-way transformer and the unified GNN using the loss functions above. These loss functions prevent the incorporation of non-relevant information that could be harmful to the learned user/item embeddings, further supporting our UGT model in addressing the isolated extraction problem (Isolation 1) in a unified manner. By applying Equation (1), we extract multi-modal item embeddings from each modality using the multi-way transformer. Next, we fuse the extracted visual and textual item embeddings using the attentive fusion as detailed in Equation (4). We then integrate the resulting joint item embeddings with the user/item ID embeddings and the user-item interactions within our unified GNN, as detailed in Equation (2), so as to obtain the final user/item embeddings. We use Equation (6) to optimise both the multi-way transformer and the unified GNN within our UGT model, facilitating a unified approach to address the isolated extraction problem (Isolation 1). In addition, by incorporating an attentive-fusion component, the unified GNN uniformly fuses the multi-modal features, thereby tackling the problem of isolated modality encoding (Isolation 2) in multi-modal recommendation."}, {"title": "4 EXPERIMENTS", "content": "We conduct experiments to validate the effectiveness of our UGT model on three public datasets, in comparison to nine existing state-of-the-art multi-modal recommendation models. To examine and analyse the effectiveness of our UGT model, we conduct experiments to answer the following four research questions:\nRQ1: How does our proposed UGT model perform compared with existing multi-modal recommendation models?\nRQ2: How do the two components of UGT, namely the unified GNN and the multi-way transformer as well as the UGT's loss function affect the performance of the model?\nRQ3: How do different parameters (i.e. \\(\\epsilon\\), \\(\\lambda_c\\)) in our UGT model affect its performance?\nRQ4: Does the UGT model exihibit a better alignment of modalities"}, {"title": "4.1 Experimental Settings", "content": "4.1.1 Datasets. In order to evaluate the effectiveness of our UGT model in the top-k multi-modal recommendation task, we conduct experiments on three commonly used Amazon Review datasets\u00b2, namely Sports and Outdoors (Sports for short), Clothing, Shoes and Jewelry (Clothing for short), and Baby. Table 1 shows the statistics of the used datasets. We choose these datasets due to their comprehensive coverage of user-item interactions and their abundant multi-modal data, including the items' image URLs and textual descriptions. These datasets are also unique in providing extensive raw data, unlike other multi-modal recommendation datasets such as TikTok\u00b3 and Kwai\u2074, which do not provide raw data in relation to the various modalities. In addition, these datasets have been widely evaluated by several existing state-of-the-art baselines, such as [33, 42, 45], further confirming their suitability for our analysis.\n4.1.2 Evaluation Protocols. Following the evaluation setting in [42, 43], we randomly split the datasets into training, validation, and testing sets using an 8:1:1 ratio. Both our UGT model and the used baselines have their hyper-parameters optimised using a grid search on the validation set, and refined by the Adam [14] optimiser. For the other hyper-parameters unique to our UGT model (\\(\\epsilon\\) and \\(\\lambda_c\\)), we tune the parameters as follows: \\(\\epsilon \\in \\{0, 0.1, 0.2, ..., 1.0\\}\\) and \\(\\lambda_c\\in \\{0, 0.1, 0.2, ..., 1.0\\}\\). We use two commonly used evaluation metrics, namely Recall@K and NDCG@K, to examine the top-K recommendation performance. We set K to 10 and 20 [42], and report the average performance achieved for all users in the testing set. We also examine the statistical significance of UGT's improved performance over the used baselines using the paired t-test with the Holm-Bonferroni correction for p < 0.05. We apply an early-stopping strategy that terminates the training of both UGT and the baseline models if no decrease in validation loss is observed over 50 epochs. All used baselines and our UGT model are implemented with PyTorch and were ran on a GPU A6000 with 48GB memory.\n4.1.3 Baselines. To examine the effectiveness of our UGT model, we perform a comparative analysis between the baselines, which use isolated extraction methods and isolated modality fusion methods, and our proposed unified UGT model. To ensure a fair comparison, we employ the widely-used CNN and Sentence-Transformer as modality-specific extractors in our baselines [43]. These extractors are used to derive item features from the raw images and descriptions within the used datasets. In doing so, we ensure that both the CNN and Sentence-Transformer, as well as our multi-way transformer, have a similar number of parameters (170 million vs. 167"}, {"title": "4.2 Performance Comparison (RQ1)", "content": "Table 2 compares the performance of our UGT model to that of nine multi-modal recommendation baselines. In the table, the top and second-best results are highlighted in bold and underlined, respectively. We also evaluate the statistical significance of the difference in performance between our UGT model and that of the used baselines according to the paired t-test with the Holm-Bonferroni correction. From the results in Table 2, we observe the following:\n\u2022 For all three datasets, UGT outperforms all the baseline models on all metrics by a large margin, and the differences are statistically significant in all cases. In particular, our UGT model significantly outperforms the strongest baseline, LightGT, in terms of Recall@20, showing improvements of 6.60%, 13.97% and 6.41% over the three used datasets, respectively. These significant improvements demonstrate the effectiveness of our graph transformer architecture in comparison to using an independent multi-modal recommendation model alongside the corresponding pre-trained feature extractors. We attribute this improved performance to the combined effect of the multi-way transformer and the unified GNN, along with the used losses (i.e., BPR loss, ITC loss). These components enable a simultaneous optimisation for extracting effective multi-modal features, which are then fused to enhance the final user/item embeddings.\n\u2022 In Table 2, the comparison of the graph transformer models (PGMT, LightGT, UGT) with the GNN-based models (MMGCN, MMGCL, SLMRec, LATTICE, BM3, FREEDOM), shows that, with the exception of PGMT, the graph transformer models generally exhibit a performance on par with the GNN-based models. This result indicates that the application of the transformer network in the GNN-based models is more suitable for the extraction or distillation of multi-modal features, in comparison to using the transformer for aggregation. Similar observations have been made in other tasks such as VQA, as reported in [12].\nHence, in answer to RQ1, we conclude that our UGT model effectively leverages the graph transformer architecture, consisting of multi-way transformer paired with a unified GNN, to significantly enhance the multi-modal recommendation performance in comparison to 9 strong baselines from the literature. UGT indeed offers a comprehensive end-to-end solution that effectively and seamlessly combines the extraction and fusion components, thereby addressing the issue of isolated feature extraction (Isolation 1) in multi-modal recommender systems."}, {"title": "4.3 Ablation Study (RQ2)", "content": "In this section, we evaluate how each component of the UGT model, the multi-way transformer (referred to as Trans) and the unified GNN (UGNN), influences the recommendation performance. Recall that the unified GNN component incorporates an attentive-fusion (Attn-Fuse) as an internal component (see Fig. 2). Specifically, to evaluate the impact on effectiveness, we replace the multi-way transformer (Trans) and the unified GNN (UGNN) components with a CNN + Sentence Transformer [43] extractor and LightGCNs [11] respectively, which are commonly used in the baseline models. In addition, we simplify the attentive-fusion (Attn-Fuse) component by changing its attentive concatenation to a straightforward concatenation of the extracted multi-modal features so as to assess the effectiveness of the attentive-fusion component. Finally, we evaluate the effect of contrastive learning (CL) within the UGT's loss function by removing the contrastive loss (c.f. Equation (6)). Table 3 presents the performance outcomes of UGT's variants in multi-modal recommendation across the three datasets we used. We observe the following:\n\u2022 Comparing UGT to its variant without the attentive concatenation (UGT w/o Attn-Fuse), we note a marked decrease in effectiveness of UGT across all three datasets. This result indicates the importance of our attentive-fusion component in effectively fusing the multi-modal features into the final user/item embeddings.\n\u2022 The ablation of the unified GNN (UGNN) and its replacement with a LightGCN multi-stream processing on each modality 5. This observation emphasises the essential role of applying a unified processing on the multi-modal inputs, in contrast to the traditional method that processes each modality input separately.\n\u2022 Table 3 also shows that replacing our multi-way transformer (Trans) component with a CNN + Sentence Transformer extractor markedly degrades the UGT's recommendation performance. This result highlights the advantage of replacing the pre-trained extractors, typically used in the existing methods, with our multi-way transformer, thereby enabling the simultaneous optimisation of the extraction process and the generation of the user/item embeddings for effective multi-modal recommendation.\n\u2022 The UGT model, which uses both the ITC and BPR losses, significantly outperforms its variant without contrastive learning (i.e., UGT w/o CL). This suggests that the contrastive loss, as introduced in Sec 3.5, is beneficial for multi-modal recommendation. Indeed, CL, simultaneously provides large gradients to optimise both the extraction (multi-way transformer) and fusion (UGNN) components in our UGT model, thereby improving the recommendation performance.\nHence, in answer to RQ2, we conclude that UGT successfully uses each of its key components as well as its loss function to provide an effective unified approach for learning effective user/item representations in multi-modal recommendation. Specifically, our unified GNN (UGNN) component effectively fuses the multi-modal features using an attentive-fusion (Attn-Fuse) method, thereby addressing the problem of isolated modality encoding (Isolation 2) in the existing multi-modal recommenders. Furthermore, when we replace the CNN + Sentence Transformer extractor with our multi-way transformer (Trans) component and pair it with the unified GNN, we observe an improvement in recommendation performance, thereby addressing the isolated feature extraction problem (Isolation 1). Finally, the contrastive loss (CL) effectively optimises both the unified GNN (UGNN) and the multi-way transformer (Trans) components, thereby improving the recommendation performance."}, {"title": "4.4 Hyper-parameter Study (RQ3)", "content": "We now study the sensitivity of our UGT model to the hyper-parameters. We focus on Recall@10 for reporting the recommendation performance", "namely": "i) the scaling factor \\(\\epsilon\\), which controls the influence of an item's intrinsic multi-modal features in the graph convolution operations; and (ii) the contrastive factor \\(\\"}]}