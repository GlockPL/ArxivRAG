{"title": "EFFICIENT PERSONALIZED TEXT-TO-IMAGE GENERATION BY LEVERAGING TEXTUAL SUBSPACE", "authors": ["Shian Du", "Qi Qian", "Xiaotian Cheng", "Henglu Wei", "Yi Xu", "Xiangyang Ji"], "abstract": "Personalized text-to-image generation has attracted unprecedented attention in the recent few years due to its unique capability of generating highly-personalized images via using the input concept dataset and novel textual prompt. However, previous methods solely focus on the performance of the reconstruction task, degrading its ability to combine with different textual prompt. Besides, optimizing in the high-dimensional embedding space usually leads to unnecessary time-consuming training process and slow convergence. To address these issues, we propose an efficient method to explore the target embedding in a textual subspace, drawing inspiration from the self-expressiveness property. Additionally, we propose an efficient selection strategy for determining the basis vectors of the textual subspace. The experimental evaluations demonstrate that the learned embedding can not only faithfully reconstruct input image, but also significantly improves its alignment with novel input textual prompt. Furthermore, we observe that optimizing in the textual subspace leads to an significant improvement of the robustness to the initial word, relaxing the constraint that requires users to input the most relevant initial word. Our method opens the door to more efficient representation learning for personalized text-to-image generation.", "sections": [{"title": "INTRODUCTION", "content": "An important human ability is to abstract multiple visible concepts and naturally integrate them with known visual content using a powerful imagination (Ding et al., 2022; Li et al., 2022; Zhou et al., 2022; Gao et al., 2021; Skantze & Willemsen, 2022; Kumar et al., 2022; Cohen et al., 2022). Recently, a method for rapid personalized generation using pre-trained text-to-image model has been attracting public attention (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022). It allows users to represent the input image as a \"concept\" by parameterizing a word embedding or fine-tuning the parameters of the pre-trained model and combining it with other texts. The idea of parameterizing a \"concept\" not only allows the model to reconstruct the training data faithfully, but also facilitates a large number of applications of personalized generation, such as text-guided synthesis (Rombach et al., 2022b), style transfer (Zhang et al., 2022), object composition (Liu et al., 2022), etc.\nAs the use of personalized generation becomes more widespread, a number of issues have arisen that need to be addressed. The problems are two-fold: first, previous methods such as Textual Inversion (Gal et al., 2022) choose to optimize directly in high-dimensional embedding space, which leads to inefficient and time-consuming optimization process. Second, previous methods only target the reconstruction of input images, degrading the ability to combine the learned embedding with different textual prompt, which makes it difficult for users to use the input prompt to guide the"}, {"title": "BACKGROUND AND RELATED WORK", "content": "In this section, we give the background and related work about deep generative models and its extensions. We first introduce the diffusion models, a class of deep generative models, then we briefly discuss text-to-image synthesis and personalized generation."}, {"title": "2.1 DIFFUSION MODELS", "content": "The goal of deep generative models, such as Flow-based Models (Dinh et al., 2016; Du et al., 2022), VAE (Burgess et al., 2018), GAN (Goodfellow et al., 2020) and Diffusion Models (Dhariwal & Nichol, 2021), is to approximate an unknown data distribution by explicitly or implicitly parameterizing a model distribution using the training data.\nAs a class of deep generative models which has been shown to produce high-quality images, Diffusion Models (Dhariwal & Nichol, 2021; Ho et al., 2020; Nichol & Dhariwal, 2021; Song et al., 2020) synthesize data via an iterative denoising process. As suggested in (Ho et al., 2020), a reweighted variational bound can be utilized as a simplified training objective and the sampling process aims to predict the corresponding noise added in forward process. The denoising objective is finally realized as a mean squared error:\n$L_{original} = E_{x_0,c,\\epsilon,t}[||\\epsilon - \\epsilon_\\theta(a_t x_0 + \\sigma_t \\epsilon, c)||_2]$,\nwhere $x_0$ are training data with conditions $c$, $t \\sim U(0, 1)$, $\\epsilon \\sim N(0, I)$ is the gaussian noise sampled in the forward process, $a_t$, $\\sigma_t$ are pre-defined scalar functions of time step $t$, $\\epsilon_\\theta$ is the parameterized reverse process with trainable parameter $\\theta$.\nRecently, an introduced class of Diffusion Models named Latent Diffusion Models (Rombach et al., 2022a) raises the interest of the community, which leverages a pre-trained autoencoder to map the images from pixel space to a more efficient latent space that significantly accelerates training and reduces the memory. Latent Diffusion Models consist of two core models. Firstly, a pre-trained autoencoder is extracted, which consists of an encoder $\\varepsilon$ and a decoder $D$. The encoder $\\varepsilon$ maps the images $x_0 \\sim p(x)$ into a latent code $z_0$ in a low-dimensional latent space. The decoder $D$ learns to map it back to the pixel space, such that $D(\\varepsilon(x)) \\approx x$. Secondly, a diffusion model is trained on this latent space. The denoising objective now becomes\n$L_{latent} = E_{z_0,c,\\epsilon,t}[||\\epsilon - \\epsilon_\\theta(a_t z_0 + \\sigma_t \\epsilon, c) ||_2]$,\nwhere $z_0 = \\varepsilon(x_0)$ is the latent code encoded by the pre-trained encoder $\\varepsilon$.\nFor conditional synthesis, in order to improve sample quality while reducing diversity, classifier guidance (Dhariwal & Nichol, 2021) is proposed to use gradients from a pre-trained model $p(c|z_t)$, where $z_t := a_tz_0 + \\sigma_t\\epsilon$. Classifier-free guidance (Ho & Salimans, 2022) is an alternative approach that avoids this pre-trained model by instead jointly training a single diffusion model on conditional and unconditional objectives via randomly dropping $c$ during training with probability $1 - w$, where $w$ is the guidance scale offering a tradeoff between sample quality and diversity. The modified predictive model is shown as follows: $\\hat{\\epsilon}_\\theta(z_t, c) = w\\epsilon_\\theta(z_t, c) + (1 - w)\\epsilon_\\theta(z_t, \\phi)$, where $\\phi = c(\\text{`` ''})$ is the embedding of a null text and $c_\\theta$ is the pre-trained text encoder such as BERT (Devlin et al., 2018) and CLIP (Radford et al., 2021)."}, {"title": "2.2 TEXT-TO-IMAGE SYNTHESIS", "content": "Recent large-scale text-to-image models such as Stable-Diffusion (Rombach et al., 2022a), GLIDE (Nichol et al., 2021) and Imagen (Saharia et al., 2022) have demonstrated unprecedented semantic generation. We implement our method based on Stable-Diffusion, which is a publicly available 1.4 bil- lion parameters text-to-image diffusion model pre-trained on the LAION-400M dataset (Schuhmann et al., 2021). Here c is the processed text condition.\nTypical text encoder models include three steps to process the input text. Firstly, a textual prompt is input by the users and split by a tokenizer to transform each word or sub-word into a token, which is an index in a pre-defined language vocabulary. Secondly, each token is mapped to a unique text embedding vector, which can be retrieved through an index-based lookup (Gal et al., 2022). The embedding vector is then concatenated and transformed by the CLIP text encoder to obtain the text condition c."}, {"title": "2.3 PERSONALIZED GENERATION", "content": "As the demand for personalized generation continues to grow, personalized generation has become a prominent factor in the field of machine learning, such as recommendation systems (Amat et al., 2018) and language models (Cattiau, 2022). Within the vision community, adapting models to a specific object or style is gradually becoming a target of interest. Users often wish to input personalized real images to parameterize a \u201cconcept\" from them and combine it with large amounts of textual prompt to create new combination of images.\nA recent proposed method Textual Inversion (Gal et al., 2022) choose the text embedding space as the location of the \u201cconcept\". It intervenes in the embedding process and uses a learned embedding $v$ to represent the concept, in essence \u201cinjecting\u201d the concept into the language vocabulary. Specifically, it defines a placeholder string $S$ (such as \u201cA photo of *\u201d) as the textual prompt, where \u201c*\u201d is the pseudo-word corresponding to the target embedding $v$ it wishes to learn. The embedding matrix $y \\in \\mathbb{R}^{N \\times d}$ can be obtained by concatenating $v$ with other frozen embeddings (such as the corresponding embeddings of \"a\", \"photo\" and \"of\u201d in the example), where $N$ is the number of words in the placeholder strings and $d$ is the dimension of the embedding space. The above process is defined as (Gal et al., 2022): $y \\leftarrow \\text{Combine}(S, v)$.\nThe optimization goal is defined as: $\\arg \\min_v E_{z_0,\\epsilon,t} [||\\epsilon - \\epsilon_\\theta(a_t z_0 + \\sigma_t \\epsilon, C_\\theta(y))||_2]$, where $z_0$, $\\epsilon$ and $t$ are defined in Equation (1) and (2). Please note that $y$ is a function of $v$. Although Textual Inversion can extract a single concept formed by 3-5 images and reconstruct it faithfully, it can not be combined with textual prompt flexibly since it solely considers the performance of the reconstruction task during optimization. Also, it searches the target embedding in the high-dimensional embedding space, which is time-consuming and difficult to converge.\nTo address the issues above, we observe that the pre-trained embeddings in the vocabulary is expressive enough to represent any introduced concept. Therefore, we explicitly define a projection matrix to efficiently optimize the target embedding in a low-dimensional tetxual subspace, which speeds up convergence and better preserves the text similarity of the learned embedding."}, {"title": "3 THE PROPOSED BATEX METHOD", "content": "In this section, we introduce the proposed BaTex method. Following the definition of Stable Diffusion (Rombach et al., 2022a), $\\mathbb{E} = \\mathbb{R}^d$ is the word embedding space with dimension $d$ and $V$ is the word vocabulary defined by the CLIP text model (Radford et al., 2021). The words in vocabulary V corresponds to a set of pre-trained vectors ${v_i}_{i=1}^{|V|}$, where $|V|$ is the cardinality of set V."}, {"title": "3.1 \u039f\u03a1\u03a4\u0399MIZATION PROBLEM", "content": "We first state that any vector in the embedding space E can be represented by the embeddings in the vocabulary V, as shown in the following theorem whose proof can be found in Appendix \u0412.\nTheorem 1 Any vector $v$ in word embedding space E can be represented by a linear combination of the embeddings in vocabulary V.\nAs stated in Theorem 1, any vector in E can be represented by a linear combination of the embeddings in the vocabulary V. Now we define the weight vector $w = (w_1,..., w_{|V|})^T$ with each component corresponding to a embedding in V, and the embedding $v = \\sum w_i v_i$. Since the users input an initial embedding $u \\in V$, we wish the start point in E to be the same as $u$ in our algorithm. Thus, we initialize the weights as:\n$w^0 := (w_1,..., w_{|V|}) = (0, . . ., 0, \\underset{i=i_u}{1}, 0, ..., 0)^T,$\nwhere $i_u$ denotes the index corresponding to $u$. Then the embedding $v$ to be learned can now be initialized as:\n$v^0 = w_1 v_1 + \u2026 + w_u + ... + w_{|V|} v_{|V|}$.\nSubsequently, it can be combined with the placeholder string to form the embedding matrix $y$ as stated in Section 2.3. The reconstruction task can be formulated as the following optimization problem:\n$\\arg \\min_{v \\in E} L_{rec} := E_{z_0,\\epsilon,t}[||\\epsilon - \\epsilon_\\theta(a_t z_0 + \\sigma_t \\epsilon, C_\\theta(y))||_2]$.\nwhere $z_0$, $\\epsilon$, $t$, $a_t$ and $\\sigma_t$ are detailed in Equation (1) and (2), $C_\\theta$ is the text encoder defined in Section 2.1, $y$ is a function of $v$. To solve problem (5), we iteratively update the weight vector $w$ by using gradient descent with initial point $w^0$, so that the embedding $v$ is also updated.\nWhile it is expressive enough to represent any concept in the embedding space E, most weights and vectors are unnecessary since the rank $r(A_V) = d$ as stated in Theorem 1, where $A_V = [v_1, . . ., v_{|V|}] \\in \\mathbb{R}^{d \\times |V|}$. Thus, we only need at most $d$ vectors to optimize with $u$ included, which corresponds to selecting $d$ linearly-independent vectors $v_{i_1}$, $v_{i_2}$, ..., $u$, ..., $v_{i_d}$ from vocabulary V."}, {"title": "3.2 TEXTUAL SUBSPACE", "content": "As detailed in Section 3.1, any vector in the embedding space E can be obtained using $d$ vectors $v_{i_1}$, $v_{i_2}$, ..., $u$, ., $v_{i_d}$. However, optimizing the embedding $v$ by solving problem (5) solely target the reconstruction of input images, leading to low text similarity (Gal et al., 2022). Besides, solving problem (5) requires to search in the whole high-dimensional embedding space E, which results in time-consuming training process and slow convergence.\nIt is natural to construct a textual subspace with high text similarity, in which the searched embedding is able to capture the details of the input image. To this end, vectors with high semantic relevance to $u$ should be included. As suggested in (Mikolov et al., 2013a;b; Le & Mikolov, 2014; Goldberg &"}, {"title": "3.3 CONCEPT GENERATION", "content": "Given the chosen embeddings $v_{i_1},..., v_{i_M}$, once the corresponding learned weights ${w_i}_{i=1}^M$ are obtained by $\\arg \\min L_{rec}$ with $S = \\text{span}(v_{i_1},..., v_{i_M})$, the learned embedding $v^*$ can be formed as:\n$v^* = w_1 v_{i_1} + w_2 v_{i_2} + \u00b7\u00b7\u00b7 + w_M v_{i_M}$.\nThen it can be combined with any input textual prompt S as:\n$y \\leftarrow \\text{Combine}(S, v^*)$,\nwhere the $\\text{Combine}$ operator is defined in Section 2.3. Subsequently, the target images $\\hat{x}$ are generated using the pre-trained diffusion network $u_\\theta$:\n$\\hat{x} \\leftarrow u_\\theta(y)$.\nDetails of BaTex are shown in Algorithm 2.\nFinally, we derive that for single-step optimization scenario, the difference of the embedding update between Textual Inversion and BaTex corresponds to a matrix transformation with rank $d_1$, which is the dimension of the textual subspace. The formal theorem is presented as follows, and its proof is included in Appendix C.\nTheorem 2 For single-step optimization, let $v_1 = u + \\Delta v_1$ and $v_2 = u + \\Delta v_2$ be the updated embedding of Textual Inversion and BaTex respectively, where $u$ is the initial embedding. Then there exists a matrix $B_V \\in \\mathbb{R}^{d \\times d}$ with rank $d_1$, such that\n$\\Delta v_2 = B_V \\Delta v_1$,\nwhere $d_1$ is the dimension of the textual subspace ($d_1 < d$).\nIt can be seen from Theorem 2 that BaTex actually defines a transformation from $\\mathbb{R}^d$ to $\\mathbb{R}^{d_1}$ using the selection strategy stated in Algorithm 1, which intuitively benefits for optimization process since $B_V$ is formed by the pre-trained embeddings, showing that BaTex explicitly extracts more information from the vocabulary V."}, {"title": "4 EXPERIMENTS", "content": "In this subsection, we present the experimental settings, and more details can be found in Appendix E.\nWe compare the proposed BaTex with Textual Inversion (TI) (Gal et al., 2022), the original method for personalized generation which lies in the category of \u201cEmbedding Optimization\u201d. To analyze the quality of learned embeddings, we follow the most commonly used metrics in TI and measure the performance by computing the CLIP-space scores (Hessel et al., 2021).\nWe also compare with two \"Model Optimization\u201d methods, DreamBooth (DB) (Ruiz et al., 2022) and Custom Diffusion (CD) (Kumari et al., 2022). DB finetunes all the parameters in Diffusion Models, resulting in the ability of mimicing the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. However, it finetunes a large amount of model parameters, which leads to overfitting (Ramasesh et al., 2022). CD compares the effect of model parameters and chooses to optimize the parameters in the cross-attention layers. While it provides an efficient method to finetune the model parameters, it requires to prepare a regularized dataset (extracted from LAION-400M dataset (Schuhmann et al., 2021)) to mitigate overfitting, which is time-consuming and hinders its scalability to on-site application. A detailed comparison of method ability can be seen in Table 1."}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "In this subsection, we present the experimental settings, and more details can be found in Appendix E.\nWe compare the proposed BaTex with Textual Inversion (TI) (Gal et al., 2022), the original method for personalized generation which lies in the category of \u201cEmbedding Optimization\u201d. To analyze the quality of learned embeddings, we follow the most commonly used metrics in TI and measure the performance by computing the CLIP-space scores (Hessel et al., 2021).\nWe also compare with two \"Model Optimization\u201d methods, DreamBooth (DB) (Ruiz et al., 2022) and Custom Diffusion (CD) (Kumari et al., 2022). DB finetunes all the parameters in Diffusion Models, resulting in the ability of mimicing the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. However, it finetunes a large amount of model parameters, which leads to overfitting (Ramasesh et al., 2022). CD compares the effect of model parameters and chooses to optimize the parameters in the cross-attention layers. While it provides an efficient method to finetune the model parameters, it requires to prepare a regularized dataset (extracted from LAION-400M dataset (Schuhmann et al., 2021)) to mitigate overfitting, which is time-consuming and hinders its scalability to on-site application. A detailed comparison of method ability can be seen in Table 1."}, {"title": "4.2 QUALITATIVE COMPARISON", "content": "We first show that learning in a textual subspace significantly improves the text similarity of learned embedding while retaining the ability to reconstruct the input image. The results of text-guided synthesis are shown in Figure 2. As can be seen, for complex input textual prompt with additional text conditions, our method completely captures the input concept and naturally combines it with known concepts.\nAdditionally, We show the effectiveness of our method by composing two concepts together and introducing additional text conditions (shown in bolded text). The results are shown in Figure 3. It can be seen that BaTex not only allows for the lossless combination of two distinct concepts, but also faithfully generates the additional text conditions."}, {"title": "4.3 QUANTITATIVE COMPARISON", "content": "The results of image-image and text-image alignment scores compared with previous works are shown in Table 2. As can be seen, when compared with TI by text-image alignment score, BaTex substantially outperforms it (0.76 to 0.70) while maintaining non-degrading image reconstruc- tion effect (0.74 to 0.74). For \u201cModel Optimization\u201d category, BaTex is competitive in both metrics, while their methods perform poorly in one of them due to overfitting. Additional results can be found in Appendix F."}, {"title": "4.4 USER STUDY", "content": "Following Textual Inversion, we have conducted a human evaluation with two test phases of image- image and text-image alignments. We collected a total of 160 responses to each phase. The results are presented in Table 3, showing that the human evaluation results align with the CLIP scores."}, {"title": "5 DISCUSSION", "content": "In this section, we discuss the effects of proposed BaTex. Since the dimension of the textual subspace highly affects the search space of the target embedding, we perform an ablation study on the dimension"}, {"title": "6 CONCLUSION", "content": "We have proposed BaTex, a novel method for efficiently learning arbitrary concept in a textual subspace. Through a rank-based selection strategy, BaTex determines the textual subspace using the information from the vocabulary, which is time-efficient and better preserves the text similarity of the learned embedding. On the theoretical side, we demonstrate that the selected embeddings can be combined to produce arbitrary vectors in the embedding space and the proposed method is equivalent to applying a projection matrix to the update of embedding. We experimentally demonstrate the efficiency and robustness of the proposed BaTex. Future improvements include introducing sparse optimization algorithm to automatically choose the dimension of textual subspace, and combining with \"Model Optimization\" methods to improve its image-image alignment score."}]}