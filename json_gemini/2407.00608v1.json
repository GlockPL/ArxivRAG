{"title": "EFFICIENT PERSONALIZED TEXT-TO-IMAGE GENERATION BY LEVERAGING TEXTUAL SUBSPACE", "authors": ["Shian Du", "Qi Qian", "Yi Xu", "Xiaotian Cheng", "Henglu Wei", "Xiangyang Ji"], "abstract": "Personalized text-to-image generation has attracted unprecedented attention in the recent few years due to its unique capability of generating highly-personalized images via using the input concept dataset and novel textual prompt. However, previous methods solely focus on the performance of the reconstruction task, degrading its ability to combine with different textual prompt. Besides, optimizing in the high-dimensional embedding space usually leads to unnecessary time-consuming training process and slow convergence. To address these issues, we propose an efficient method to explore the target embedding in a textual subspace, drawing inspiration from the self-expressiveness property. Additionally, we propose an efficient selection strategy for determining the basis vectors of the textual subspace. The experimental evaluations demonstrate that the learned embedding can not only faithfully reconstruct input image, but also significantly improves its alignment with novel input textual prompt. Furthermore, we observe that optimizing in the textual subspace leads to an significant improvement of the robustness to the initial word, relaxing the constraint that requires users to input the most relevant initial word. Our method opens the door to more efficient representation learning for personalized text-to-image generation.", "sections": [{"title": "INTRODUCTION", "content": "An important human ability is to abstract multiple visible concepts and naturally integrate them with known visual content using a powerful imagination (Ding et al., 2022; Li et al., 2022; Zhou et al., 2022; Gao et al., 2021; Skantze & Willemsen, 2022; Kumar et al., 2022; Cohen et al., 2022). Recently, a method for rapid personalized generation using pre-trained text-to-image model has been attracting public attention (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022). It allows users to represent the input image as a \"concept\" by parameterizing a word embedding or fine-tuning the parameters of the pre-trained model and combining it with other texts. The idea of parameterizing a \"concept\" not only allows the model to reconstruct the training data faithfully, but also facilitates a large number of applications of personalized generation, such as text-guided synthesis (Rombach et al., 2022b), style transfer (Zhang et al., 2022), object composition (Liu et al., 2022), etc.\nAs the use of personalized generation becomes more widespread, a number of issues have arisen that need to be addressed. The problems are two-fold: first, previous methods such as Textual Inversion (Gal et al., 2022) choose to optimize directly in high-dimensional embedding space, which leads to inefficient and time-consuming optimization process. Second, previous methods only target the reconstruction of input images, degrading the ability to combine the learned embedding with different textual prompt, which makes it difficult for users to use the input prompt to guide the"}, {"title": "BACKGROUND AND RELATED WORK", "content": "In this section, we give the background and related work about deep generative models and its extensions. We first introduce the diffusion models, a class of deep generative models, then we briefly discuss text-to-image synthesis and personalized generation."}, {"title": "2.1 DIFFUSION MODELS", "content": "The goal of deep generative models, such as Flow-based Models (Dinh et al., 2016; Du et al., 2022), VAE (Burgess et al., 2018), GAN (Goodfellow et al., 2020) and Diffusion Models (Dhariwal & Nichol, 2021), is to approximate an unknown data distribution by explicitly or implicitly parameterizing a model distribution using the training data.\nAs a class of deep generative models which has been shown to produce high-quality images, Diffusion Models (Dhariwal & Nichol, 2021; Ho et al., 2020; Nichol & Dhariwal, 2021; Song et al., 2020) synthesize data via an iterative denoising process. As suggested in (Ho et al., 2020), a reweighted variational bound can be utilized as a simplified training objective and the sampling process aims to predict the corresponding noise added in forward process. The denoising objective is finally realized as a mean squared error:\n$Loriginal = E_{x0,c,\\epsilon,t}[||\\epsilon \u2013 \\epsilon_{\\theta}(a_t x_0 + \\sigma_t \\epsilon, c)||^2]$,\nwhere $x_0$ are training data with conditions c, $t \\sim U(0, 1)$, $\\epsilon \\sim N(0, I)$ is the gaussian noise sampled in the forward process, $a_t$, $\\sigma_t$ are pre-defined scalar functions of time step t, $\\epsilon_{\\theta}$ is the parameterized reverse process with trainable parameter $\\theta$.\nRecently, an introduced class of Diffusion Models named Latent Diffusion Models (Rombach et al., 2022a) raises the interest of the community, which leverages a pre-trained autoencoder to map the images from pixel space to a more efficient latent space that significantly accelerates training and reduces the memory. Latent Diffusion Models consist of two core models. Firstly, a pre-trained autoencoder is extracted, which consists of an encoder $\\varepsilon$ and a decoder $\\mathcal{D}$. The encoder $\\varepsilon$ maps the images $x_0 \\sim p(x)$ into a latent code $z_0$ in a low-dimensional latent space. The decoder $\\mathcal{D}$ learns to map it back to the pixel space, such that $\\mathcal{D}(\\varepsilon(x)) \\approx x$. Secondly, a diffusion model is trained on this latent space. The denoising objective now becomes\n$L_{latent} = E_{z_0,c,\\epsilon,t}[||\\epsilon \u2013 \\epsilon_{\\theta}(a_t z_0 + \\sigma_t \\epsilon, c) ||^2]$,\nwhere $z_0 = \\varepsilon(x_0)$ is the latent code encoded by the pre-trained encoder $\\varepsilon$.\nFor conditional synthesis, in order to improve sample quality while reducing diversity, classifier guidance (Dhariwal & Nichol, 2021) is proposed to use gradients from a pre-trained model $p(c|z_t)$, where $z_t := a_t z_0 + \\sigma_t \\epsilon$. Classifier-free guidance (Ho & Salimans, 2022) is an alternative approach that avoids this pre-trained model by instead jointly training a single diffusion model on conditional and unconditional objectives via randomly dropping c during training with probability 1 - w, where w is the guidance scale offering a tradeoff between sample quality and diversity. The modified predictive model is shown as follows: $\\hat{\\epsilon_{\\theta}}(z_t, c) = w \\epsilon_{\\theta}(z_t, c) + (1 \u2212 w)\\epsilon_{\\theta}(z_t, \u03c6)$, where $\u03c6 = c_{\\theta}("}, {"title": "2.2 TEXT-TO-IMAGE SYNTHESIS", "content": "Recent large-scale text-to-image models such as Stable-Diffusion (Rombach et al., 2022a), GLIDE (Nichol et al., 2021) and Imagen (Saharia et al., 2022) have demonstrated unprecedented semantic generation. We implement our method based on Stable-Diffusion, which is a publicly available 1.4 bil-lion parameters text-to-image diffusion model pre-trained on the LAION-400M dataset (Schuhmann et al., 2021). Here c is the processed text condition.\nTypical text encoder models include three steps to process the input text. Firstly, a textual prompt is input by the users and split by a tokenizer to transform each word or sub-word into a token, which is an index in a pre-defined language vocabulary. Secondly, each token is mapped to a unique text embedding vector, which can be retrieved through an index-based lookup (Gal et al., 2022). The embedding vector is then concatenated and transformed by the CLIP text encoder to obtain the text condition c."}, {"title": "2.3 PERSONALIZED GENERATION", "content": "As the demand for personalized generation continues to grow, personalized generation has become a prominent factor in the field of machine learning, such as recommendation systems (Amat et al., 2018) and language models (Cattiau, 2022). Within the vision community, adapting models to a specific object or style is gradually becoming a target of interest. Users often wish to input personalized real images to parameterize a \u201cconcept\" from them and combine it with large amounts of textual prompt to create new combination of images.\nA recent proposed method Textual Inversion (Gal et al., 2022) choose the text embedding space as the location of the \u201cconcept\". It intervenes in the embedding process and uses a learned embedding v to represent the concept, in essence \u201cinjecting\u201d the concept into the language vocabulary. Specifically, it defines a placeholder string S (such as \u201cA photo of *\") as the textual prompt, where \u201c*\u201d is the pseudo-word corresponding to the target embedding v it wishes to learn. The embedding matrix $y \\in \\mathbb{R}^{N \\times d}$ can be obtained by concatenating v with other frozen embeddings (such as the corresponding embeddings of \"a\", \"photo\" and \"of\u201d in the example), where N is the number of words in the placeholder string and d is the dimension of the embedding space. The above process is defined as (Gal et al., 2022): $y \\leftarrow Combine(S, v)$.\nThe optimization goal is defined as: $arg min E_{z0,\\epsilon,t} [||\\epsilon \u2013 \\epsilon_{\\theta}(a_t z_0 + \\sigma_t \\epsilon, C_{\\theta}(y))||^2]$, where $z_0, \\epsilon$ and t are defined in Equation (1) and (2). Please note that y is a function of v. Although Textual Inversion can extract a single concept formed by 3-5 images and reconstruct it faithfully, it can not be combined with textual prompt flexibly since it solely considers the performance of the reconstruction task during optimization. Also, it searches the target embedding in the high-dimensional embedding space, which is time-consuming and difficult to converge.\nTo address the issues above, we observe that the pre-trained embeddings in the vocabulary is expressive enough to represent any introduced concept. Therefore, we explicitly define a projection matrix to efficiently optimize the target embedding in a low-dimensional tetxual subspace, which speeds up convergence and better preserves the text similarity of the learned embedding.\""}, {"title": "3 THE PROPOSED BATEX METHOD", "content": "In this section, we introduce the proposed BaTex method. Following the definition of Stable Diffusion (Rombach et al., 2022a), $\\mathcal{E} = \\mathbb{R}^d$ is the word embedding space with dimension d and V is the word vocabulary defined by the CLIP text model (Radford et al., 2021). The words in vocabulary V corresponds to a set of pre-trained vectors {$v_i$}, where |V| is the cardinality of set V."}, {"title": "3.1 \u039f\u03a1\u03a4\u0399\u039cIZATION PROBLEM", "content": "We first state that any vector in the embedding space E can be represented by the embeddings in the vocabulary V, as shown in the following theorem whose proof can be found in Appendix \u0412.\nTheorem 1 Any vector v in word embedding space E can be represented by a linear combination of the embeddings in vocabulary V.\nAs stated in Theorem 1, any vector in E can be represented by a linear combination of the embeddings in the vocabulary V. Now we define the weight vector $w = (w_1,..., w_{|V|})^T$ with each component corresponding to a embedding in V, and the embedding $v = \\sum w_i v_i$. Since the users input an initial embedding u \u2208 V, we wish the start point in E to be the same as u in our algorithm. Thus, we initialize the weights as:\n$w^\u00b0 := (w_1,..., w_{|V|}) = (0, . . ., 0,1|_{i=i_u}, 0, ..., 0)^T$,\nwhere $i_u$ denotes the index corresponding to u. Then the embedding v to be learned can now be initialized as:\n$v^\u00b0 = w_1v_1 + \u2026 + w_u + ... + w_{|V|}v_{|V|}$.\nSubsequently, it can be combined with the placeholder string to form the embedding matrix y as stated in Section 2.3. The reconstruction task can be formulated as the following optimization problem:\n$arg min L_{rec} := E_{z0,\\epsilon,t}[||\\epsilon \u2013 \\epsilon_{\\theta}(a_t z_0 + \\sigma_t \\epsilon, C_{\\theta}(y))||^2]$.\nwhere $z_0, \\epsilon, t, a_t$ and $\\sigma_t$ are detailed in Equation (1) and (2), $C_{\\theta}$ is the text encoder defined in Section 2.1, y is a function of v. To solve problem (5), we iteratively update the weight vector w by using gradient descent with initial point $w^\u00b0$, so that the embedding v is also updated.\nWhile it is expressive enough to represent any concept in the embedding space E, most weights and vectors are unnecessary since the rank r(Av) = d as stated in Theorem 1, where Av = [V1,\u2026\u2026\u2026, V|V|] \u2208 Rd\u00d7|V|. Thus, we only need at most d vectors to optimize with u included, which corresponds to selecting d linearly-independent vectors $V_{i_1}, V_{i_2}, ..., u, ..., V_{i_d}$ from vocabulary V."}, {"title": "3.2 TEXTUAL SUBSPACE", "content": "As detailed in Section 3.1, any vector in the embedding space E can be obtained using d vectors $V_{i1}, V_{i2},..., u, ., V_{id}$. However, optimizing the embedding v by solving problem (5) solely target the reconstruction of input images, leading to low text similarity (Gal et al., 2022). Besides, solving problem (5) requires to search in the whole high-dimensional embedding space E, which results in time-consuming training process and slow convergence.\nIt is natural to construct a textual subspace with high text similarity, in which the searched embedding is able to capture the details of the input image. To this end, vectors with high semantic relevance to u should be included. As suggested in (Mikolov et al., 2013a;b; Le & Mikolov, 2014; Goldberg &"}, {"title": "3.3 CONCEPT GENERATION", "content": "Given the chosen embeddings $v_{i_1},..., v_{i_M}$, once the corresponding learned weights {$w_i$}$_{i=1}^M$ are obtained by arg min $L_{rec}$ with $\\mathcal{S}$ = span($v_{i_1},..., v_{i_M}$), the learned embedding $v^\u2217$ can be formed as:\n$v^\u2217 = w_1v_{i_1} + w_2 v_{i_2} + \u00b7\u00b7\u00b7 + w_Mv_{i_M}$.\nThen it can be combined with any input textual prompt $\\mathcal{S}$ as:\ny \u2190 Combine(S, v\u2217),\nwhere the Combine operator is defined in Section 2.3. Subsequently, the target images \u00ee are generated using the pre-trained diffusion network $u_\\theta$:\n$x \u2190 u_\\theta$(y)."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETTINGS\nIn this subsection, we present the experimental settings, and more details can be found in Appendix E.\nWe compare the proposed BaTex with Textual Inversion (TI) (Gal et al., 2022), the original method for personalized generation which lies in the category of \u201cEmbedding Optimization\u201d. To analyze the quality of learned embeddings, we follow the most commonly used metrics in TI and measure the performance by computing the CLIP-space scores (Hessel et al., 2021).\nWe also compare with two \"Model Optimization\u201d methods, DreamBooth (DB) (Ruiz et al., 2022) and Custom Diffusion (CD) (Kumari et al., 2022). DB finetunes all the parameters in Diffusion Models, resulting in the ability of mimicing the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. However, it finetunes a large amount of model parameters, which leads to overfitting (Ramasesh et al., 2022). CD compares the effect of model parameters and chooses to optimize the parameters in the cross-attention layers. While it provides an efficient method to finetune the model parameters, it requires to prepare a regularized dataset (extracted from LAION-400M dataset (Schuhmann et al., 2021)) to mitigate overfitting, which is time-consuming and hinders its scalability to on-site application. A detailed comparison of method ability can be seen in Table 1."}, {"title": "4.2 QUALITATIVE COMPARISON", "content": "We first show that learning in a textual subspace significantly improves the text similarity of learned embedding while retaining the ability to reconstruct the input image. The results of text-guided synthesis are shown in Figure 2. As can be seen, for complex input textual prompt with additional text conditions, our method completely captures the input concept and naturally combines it with known concepts.\nAdditionally, We show the effectiveness of our method by composing two concepts together and introducing additional text conditions (shown in bolded text). The results are shown in Figure 3. It can be seen that BaTex not only allows for the lossless combination of two distinct concepts, but also faithfully generates the additional text conditions."}, {"title": "4.3 QUANTITATIVE COMPARISON", "content": "The results of image-image and text-image alignment scores compared with previous works are shown in Table 2. As can be seen, when compared with TI by text-image alignment score, BaTex substantially outperforms it (0.76 to 0.70) while maintaining non-degrading image reconstruc-tion effect (0.74 to 0.74). For \u201cModel Optimization\u201d category, BaTex is competitive in both metrics, while their methods perform poorly in one of them due to overfitting. Additional results can be found in Appendix F."}, {"title": "4.4 USER STUDY", "content": "Following Textual Inversion, we have conducted a human evaluation with two test phases of image-image and text-image alignments. We collected a total of 160 responses to each phase. The results are presented in Table 3, showing that the human evaluation results align with the CLIP scores."}, {"title": "5 DISCUSSION", "content": "In this section, we discuss the effects of proposed BaTex. Since the dimension of the textual subspace highly affects the search space of the target embedding, we perform an ablation study on the dimension"}, {"title": "6 CONCLUSION", "content": "We have proposed BaTex, a novel method for efficiently learning arbitrary concept in a textual subspace. Through a rank-based selection strategy, BaTex determines the textual subspace using the information from the vocabulary, which is time-efficient and better preserves the text similarity of the learned embedding. On the theoretical side, we demonstrate that the selected embeddings can be combined to produce arbitrary vectors in the embedding space and the proposed method is equivalent to applying a projection matrix to the update of embedding. We experimentally demonstrate the efficiency and robustness of the proposed BaTex. Future improvements include introducing sparse optimization algorithm to automatically choose the dimension of textual subspace, and combining with \"Model Optimization\" methods to improve its image-image alignment score."}, {"title": "A LIMITATIONS", "content": "Since BaTex requires to form a textual subspace to search the target embedding, the dimension M has to be defined. In BaTex, M has been treated as a hyperparameter and we have performed a detailed comparison in Section 5 to choose an appropriate value. However, in practice, M should be determined automatically to avoid artificially tuning. In Section 6, we have given some directions for improvement to address this issue."}, {"title": "B PROOF OF THEOREM 1", "content": "Theorem 3 Any vector v in word embedding space E can be represented by a linear combination of the embeddings in vocabulary V.\nProof 1 For all embeddings $v_1,...,v_{|V|} \\in E$ in V, it forms a matrix $A_V = [v_1,\u2026\u2026,v_{|V|}] \\in $\\mathbb{R}^{d \\times |V|}$. It can be numerically computed that the rank of $A_V$ is r(AV) = d, demonstrating that any d linearly-independent vectors in V can be formed as a basis of the word embedding space E. Since E = $\\mathbb{R}^d$, we can derive that if we select a subset of d linearly-independent vectors $(v_{i_1}, . . . v_{i_d})$ from V, it forms a basis of E. Then any vector v \u2208 E can be expressed as a linear combination of the subset $(v_{i_1}, . . . v_{i_d})$ as: v = $w_{i_1}v_{i_1}$ + \u00b7\u00b7\u00b7 + $w_{i_d}v_{i_d}$, where $w_{i_1},..., w_{i_d}$ are the weights of the basis vectors."}, {"title": "CPROOF OF THEOREM 2", "content": "Theorem 4 For single-step optimization, let $v_1 = u + \\Delta v_1$ and $v_2^\\prime = u + \\Delta v_2$ be the updated embedding of Textual Inversion and BaTex respectively, where u is the initial embedding. Then there exists a matrix $B_V \u2208 $\\mathbb{R}^{d \\times d}$ with rank $d_1$, such that $\\Delta v_2 = B_V \\Delta v_1$, where $d_1$ is the dimension of the textual subspace ($d_1 < d$).\nProof 2 For BaTex, we have $u = V_M w^\\degree$, where $V_M \u2208 $\\mathbb{R}^{d \\times M}$ and $w^\\degree \u2208 $\\mathbb{R}^M$ are the selected embedding matrix and initialized weights respectively, M is the number of embeddings selected. We also have r($V_M$) = $d_1$. For single-step optimization, let $w^\u2217 = w^\\degree + \\Delta w$ be the updated weights, then $ \\Delta w = \\nabla w L_{rec}$, where $ L_{rec}$ is the objective of the reconstruction task. Now we have $v = V_M w^\u2217$ = $V_M$($w^\\degree$ + \\Delta w) = $u + \\Delta v_2$. So $ \\Delta v_2$ = $V_M \\Delta w$. Using the chain rule, we know that $ \\nabla w L_{rec}$ = $ \\nabla V \\nabla L_{rec}$, which draws a conclusion that $ \\Delta v_2$ = $V_M \\nabla V L_{rec}$. Also, we know that for single-step optimization, $ \\Delta v_1$ = $ \\nabla v L_{rec}$(Here we omit the learning rate and other hyperparameters). Now we have $ \\Delta v_2$ = $V_M V^\\prime \\Delta v_1$. Set $B_V = V_M V^\\prime$, we have r($B_V$) = r($V_M V^\\prime$) = r($V_M$) = $d_1$."}, {"title": "D SOCIETAL IMPACT", "content": "With the gradual increase in the capacity of multimodal models in recent years, training a large model from scratch is no longer possible for most people. Our method allows users to combine private images with arbitrary text for customized generation. Our method is time and parameter efficient, allowing users to take advantage of a large number of pre-trained parameters, promising to increase social productivity in image generation. While being expressive and efficient, it might increase the potential danger in misusing it to generate fake or illegal data. Possible solutions include enhancing the detection ability of diffusion model (Corvi et al., 2022) and constructing safer vocabulary of Text model (Devlin et al., 2018)."}, {"title": "E ADDITIONAL EXPERIMENTAL SETTINGS", "content": "Datasets Following the existing experimental settings, we conduct experiments on several concept datasets, including datasets from Textual Inversion (Gal et al., 2022) and Custom Diffusion (Kumari"}, {"title": "F ADDITIONAL RESULTS", "content": "F.1 QUANTITATIVE COMPARISON\nWe perform quantitative comparison on five additional datasets. Results are shown in Table 5. As can be seen, when compared with TI by text-image alignment score, BaTex substantially outperforms it (0.75 to 0.66) while maintaining non-degrading image reconstruction effect (0.76 to 0.76). For \"Model Optimization\u201d category, BaTex is competitive in both metrics, while their methods perform poorly in one of them due to overfitting."}, {"title": "F.2 ROBUSTNESS AGAINST INITIAL WORD", "content": "In practice, it is often difficult for users to give the most suitable initial word for complex concepts at once, posing a great challenge to the training of previous models. We show in Table 6 that when initializing by a less semantic-related word, BaTex outperforms all three previous models by a huge margin in terms of text-image alignment score. The robustness comes from the fact that BaTex selects M embeddings by the initial word, which contains almost all synonyms or semantically similar words of the initial word.\nWe additionally test the robustness against initial word on Dog dataset and replace the initial word \"dog\" by \"animal\". Results are shown in Table 7. It can be seen that when the text similarity of other three previous methods significantly decreases, BaTex obtains a much higher score."}, {"title": "F.3 TEXT-GUIDED SYNTHESIS", "content": "We show additional text-guided synthesis results on Figure 4 and 5. As can be seen, for complex input textual prompt with additional text conditions, our method completely captures the input concept and naturally combines it with known concepts.\nAlso, qualitative results of more object concepts are shown in Figure 6. The results match the conclusion in Section 4.2."}, {"title": "F.4 VECTOR DISTANCE", "content": "We compare the text-image alignment score of three type of vector distance: dot product, cosine similarity and L2 distance in Figure 7. As can be seen, dot product and cosine similarity both out-perform L2 in text-image alignment score and convergence speed. Compared with cosine similarity, the results of dot product are slightly better. We conjecture that the reason is dot product taking"}, {"title": "F.5 TOP ONE VECTOR", "content": "Simply using the top one vector would lead to worse reconstruction results since only a scalar cannot fully store the details of the reference images. We have also added qualitative results using the top one vector in Figure 8, showing that using only one vector results in unsatisfying results."}, {"title": "F.6 Two SIMILAR OBJECTS", "content": "We have performed additional multi-concept generation for similar objects and the results are shown in Figure 9. The results show the appearance of object neglect (in the right-most column). We also find that BaTex can generate reasonable images for most cases of two similar objects (see the results of other three columns)."}, {"title": "F.7 TWO DIFFERENT OBJECTS", "content": "In addition, we have provided additional experiments about composition of different objects in Figure 10, showing that BaTex successfully composites different objects."}, {"title": "F.9 ADDITIONAL COMPARISON", "content": "We have also compared BaTex with three concurrent works: SVDiff12 (Han et al., 2023), XTI13 (Voynov et al., 2023) and NeTI14 (Alaluf et al., 2023). Quantitative results are shown in Table 8. It can be seen that BaTex achieves superior results over all three concurrent works. Also, we have shown qualitative comparison results in Figure 12, which demonstrates the effectiveness of our method over state-of-the-art methods.\nAlso, We perform qualitative comparison with encoder-based method (Wei et al., 2023) using example \u201ccat\u201d. Results have been shown in Figure 13. It can be seen that although encoder-based method finetunes much faster (it only needs one forward step), it lacks text-to-image alignment ability in some cases (e.g., miss \u201csleeping\u201d in the second case)."}, {"title": "F.10 WEIGHT VISUALIZATION", "content": "To clarify the usage of basis vectors, we have shown the visualization of both learned weights and corresponding basis vectors for example \u201ccat\u201d in Figure 14. Due to page limit, we only showcase the first 10 basis vectors. As can be seen, starting from the initial word \u201ccat\u201d, BaTex learns a reasonable combination to obtain the target vector while TI tends to search around the initial embedding using only the reconstruction loss, which results in low text-to-image alignment."}, {"title": "F.11 HUMAN FACE RESULTS", "content": "Human face domain is very challenging for personalization of image diffusion models, which contains more perceptible details than other domains. Therefore, we have performed test on \u201clecun\u201d example used in (Gal et al., 2023). Results have been shown in Figure 15. It can be seen that BaTex generates results following the textual prompt, while successfully reconstructing the input human face image."}, {"title": "G REPRODUCIBILITY STATEMENT", "content": "We have provided main part of our code in the supplementary material. Please refer there for details of our method."}]}