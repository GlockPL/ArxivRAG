{"title": "Fully automatic extraction of morphological traits from the Web: utopia or reality?", "authors": ["Diego Marcos", "Robert van de Vlasakker", "Ioannis N. Athanasiadis", "Pierre Bonnet", "Herv\u00e9 Goeau", "Alexis Joly", "W. Daniel Kissling", "C\u00e9sar Leblanc", "Andr\u00e9 S.J. van Proosdij", "Konstantinos P. Panousis"], "abstract": "Premise: Plant morphological traits, their observable characteristics, are fundamental to understand the role played by each species within their ecosystem. However, compiling trait information for even a moderate number of species is a demanding task that may take experts years to accomplish. At the same time, massive amounts of information about species descriptions is available online in the form of text, although the lack of structure makes this source of data impossible to use at scale.\nMethod: To overcome this, we propose to leverage recent advances in large language models (LLMs) and devise a mechanism for gathering and processing information on plant traits in the form of unstructured textual descriptions, without manual curation.\nResults: We evaluate our approach by automatically replicating three manually created species-trait matrices. Our method managed to find values for over half of all species-trait pairs, with an F1-score of over 75%.\nDiscussion: Our results suggest that large-scale creation of structured trait databases from unstructured online text is currently feasible thanks to the information extraction capabilities of LLMs, being limited by the availability of textual descriptions covering all the traits of interest.\nKeywords: Automatic trait extraction; Large language models; Morphological trait matrices; Natural language processing.", "sections": [{"title": "INTRODUCTION", "content": "Traits are observable characteristics of organisms that can be used to answer a variety of questions about their ecology, evolution, and even usefulness to humans. Morphological traits in particular, i.e., those that correspond to the anatomical appearance of the organisms, such as the number and color of flower petals, the size and shape of the fruits or the leaf arrangement, are the main cues that humans have been using to identify species since the advent of taxonomy. However, the sheer number of known species, the variety of morphological traits and the complexity of trait-based descriptions make it extremely challenging to design a comprehensive framework for trait-based descriptions that would be suitable across taxonomic groups. Within this context, recent efforts advocate for a standard vocabulary to make trait databases cross-compatible [Schneider et al., 2019] and an open science initiative to leverage the collective effort of the community [Gallagher et al., 2020]. Nonetheless, this complexity has resulted in most existing databases of traits being limited either in terms of geographic [Falster et al., 2021] or taxonomic scope [Kissling et al., 2019]. Moreover, large community efforts such as TRY [Kattge et al., 2011], BIEN [Maitner et al., 2018] or TraitBank [Caldwell and Hart, 2014], which aim at covering all plant species, are far from being comprehensive or representative [Kattge et al., 2020], even if they have amassed millions of contributed trait measurements. For instance, in TRY version 6, 27 of the 30 species with the highest number of traits are from Western Europe, and 3 from North America, showcasing a common imbalance in which relatively less data is available for species from biodiverse"}, {"title": "METHODS", "content": "We propose a novel framework that only requires three inputs: (i) a list of species of interest, (ii) a list of traits of interest and, for each trait, (iii) a list with all the possible values each trait is allowed to take. The output is a species-trait table that indicates, for each species, which trait values pertain to it. Specifically, the workflow (see Figure 1) can be divided in the following steps:\na) Textual data harvesting: A search engine API is used to retrieve URLs that are relevant to the species name and downloads the text content therein.\nb) Description detection: In order to filter out irrelevant text, a binary classification NLP model is used to detect description sentences within the retrieved text.\nc) Trait information extraction: An LLM is then used to detect all possible categorical trait values within the descriptive text."}, {"title": "Species-trait datasets for evaluation", "content": "In order to be able to evaluate the automatic trait extraction workflow, we fix the species, traits and trait values to those found in three manually created species-trait matrices. Specifically, we used the following databases:\n\u2022 Caribbean: 42 woody species in the Dutch Caribbean, created in the context of this work. It contains 24 traits, with an average of 8.5 possible values per trait (minimum of 2 and a maximum of 22).\n\u2022 West Africa [Bonnet et al., 2005]: 361 species of trees in the West African savanna. We consider all 23 traits, averaging 5.8 possible values per trait (minimum of 2 and a maximum of 10)."}, {"title": "Textual data harvesting and description detection", "content": "Textual data harvesting\nGiven each species of interest, we used the Google Search API and submitted a query with the binary scientific name of the species, in quotes, to make sure that the search engine only returns websites containing the exact species name. The first 20 returned URLs are then visited and the text scrapped; only HTML sites are considered in this work. We double-check that the species name is present in the HTML header, in order to filter out web pages that are not specifically dedicated to it. Since the text obtained in this way is unstructured and a large part of it does not correspond to morphological descriptions, we select the sentences most likely to be part of a description by using a custom text classifier, described in the following section. Refer to the Appendix for a list of the most frequent Internet domains contributing to the harvested text.\nDescription detection\nWe start by formulating an approach for distinguishing between descriptive and non-descriptive sentences in the form of an NLP binary classification task, aimed at filtering out all the text from the retrieved websites that does not describe the morphology of the species. For instance, in the English Wikipedia page referring to Hedera helix, the sentence \"The fruit are purple-black to orange-yellow berries\", would be considered descriptive because it explicitly describes morphological traits, and indeed stems from the \"Description\" section. We are interested in such sentences from which trait values can potentially be extracted. On the other hand, the sentence \u201cOnce ivy is established it is very difficult to control or eradicate\", from the \"Control and eradication\" section, does not explicitly describe any morphological traits, and is thus considered non-descriptive. Given a sentence, we need an automated approach to determine if said sentence is descriptive or not. Such a model can be trained without the"}, {"title": "Training the classifier.", "content": "We can then proceed with training a description detector. For this task, we need a model that is able to assign a binary label (description versus non-description) to a piece of text of arbitrary length. The most straight forward approach for this is to use a text encoder model, that can convert a text sequence of any size (up to some maximum allowed length) into a vector of fixed length. Any machine learning classifier can then be trained, in a supervised manner, using this vector representation as input. For our description sentence classification model, we turn to a distilled version of BERT (Bidirectional Encoder Representation from Transformers) [Devlin et al., 2018], a widely used NLP model for obtaining vector representations of sentences, and specifically to DistillBERT [Sanh et al., 2020]. This decision was motivated by the balance between complexity and performance that DistillBERT exhibits. This variant comprises 40% less parameters than the original BERT model, leading to 60% faster computations, while still yielding 97% performance on general language understanding. Both BERT and DistillBERT have been trained on a large corpus of English text, and pre-trained model weights are freely available. Within this context, we augment the base DistillBERT model by introducing: (i) a dropout layer for regularization purposes and (ii) two fully connected layers: the first layer takes the output vector of DistillBERT, of size 768, and outputs a vector of size 512, while the second layer takes this output vector and yields an output of size 2, which are the logits for our binary classification task.\nTo prepare the collected text for fine-tuning the model, the first step is to split it into discrete tokens, which are either syllables or entire words, for which we use the tokenizer of [Wolf et al., 2020]. The conventional BERT architecture can accommodate up to 512 tokens at once, corresponding to approximately 400 words. This means that text spans (e.g., a paragraph or a sentence) longer than 512 tokens need to be truncated to length 512 in order to be compatible with DistillBERT. In this work, we randomly split the text into text spans with a minimum of 10 and a maximum of 512 tokens. This works as data augmentation and forces the model to capture the characteristics of descriptive sentences, even when not seeing the whole sentence, potentially providing robustness when exposed to"}, {"title": "Noise robust loss function.", "content": "Due to the use of automatically obtained labels, it is likely that the resulting dataset will contain inconsistencies, as is common when dealing with unstructured data [Kumar et al., 2020]; after all, not all text within the description section of a Wikipedia article consists of descriptive sentences, and some descriptive sentences may occur outside of it, typically in the introductory sections. There also exists a chance that some section headers may be missed through this process, further increasing the amount of noise in the final dataset. We can mitigate the effects of this potential inconsistency by turning to classification losses that are designed for robustness against noisy labels. Following [Marcos et al., 2022], we chose the \"soft bootstrap\" consistency objective of [Reed et al., 2015]. The underlying principle is that the labels are \"diluted\" by the model's current prediction, thus reducing the impact on the loss of data points in which the model confidently disagrees with the label. Specifically, the loss is computed as:\n$L_{SoftLoss}(q, t) = \\sum_{k=1}^{K}[\\beta t_k + (1 - \\beta)q_k] log q_k$ (1)\nwhere q are the predicted class probabilities, t are the observed noisy labels and \u03b2 is a balancing factor between the current prediction and the target. In this way, we can use the current state of the model to dynamically adapt the prediction targets, allowing the model to pay less attention to inconsistent labels. As the model improves its predictions over time, it becomes more coherent, allowing for assessment of the consistency of the noisy labels. We set \u03b2 = 0.20 in a similar fashion to previous works [Zhang et al., 2020, Marcos et al., 2022].\nFor fine-tuning the model, we keep the DistillBERT parameters frozen and train only the added classification head. We use the Adam optimizer to minimize Eq. (1) with a learning rate of 3\u00b710-5, a batch size of 32 and gradient clipping with a norm of 1.0. The model is fine-tuned for a total of 35 epochs."}, {"title": "Trait information extraction", "content": "Information extraction with a generative LLM\nThe next step towards information extraction for species descriptions involves extracting relevant information from the obtained text snippets into a structured form. To this end, we leverage the recent advances in LLMs, which have empirically demonstrated to capture relational knowledge in the training data that can be extracted via natural language queries, also known as prompts [Ouyang et al., 2022]. These models have been shown to perform well on relatively generic tasks, such as common sense knowledge [Davison et al., 2019] or general knowledge [Petroni et al., 2019].\nAlthough it is possible to directly query an LLM with a question, without providing any additional information, one should be aware about their tendency of providing responses that look legitimate, but that are completely unfounded, known as hallucinations [Zhang et al., 2023]. This is even more prominent in specialized domains, including botany, that are characterized by long-tailed distributions in which a few elements are very abundant and many are extremely rare. This leads to LLMs being unreliable for this majority of uncommon elements. To mitigate this issue, we turn the task into information extraction from text via search engine retrieval [Lewis et al., 2020], which we achieve by feeding the LLM a piece of descriptive text, obtained via the harvesting approach detailed in the \u201cTextual data harvesting and description detection\u201d section, along with questions referring to a predetermined set of traits and possible trait values. At the same time, we give the LLM the option to explicitly state if the requested information is not available (NA) in the given text, mitigating potential"}, {"title": "Prompt design", "content": "The considered species-trait datasets comprise mostly categorical traits; these can be encoded in a binary form and expressed as multiple choice textual questions to engineer discrete prompts. In this binary encoding context, we are interested in discovering which trait values should be \u201c1\u201d or \u201c0\u201d for any given set of species and trait value by exploiting the information from the retrieved description sentences. In Table 1, such an encoding of the categorical traits \"Life form\u201d and \u201cPhyllotaxis\" of the Caribbean dataset is depicted."}, {"title": "Evaluation metrics", "content": "Evaluation of the automatic trait extraction. In order to evaluate the responses of the LLM, we compare them to the species-trait matrices manually curated by expert botanists. We report the proportion of traits for which a value was found, i.e., the coverage rate, along with the precision, recall and F1 score computed for the found traits. The precision is the proportion of predicted positives (that is, all the trait values for which the model predicts 1) that turns out to be correct according to the manual dataset. The recall is the proportion of positives in the manual dataset that are retrieved by the approach. In order to combine these two complementary metrics, the F\u2081 score consists of the geometrical average of precision and recall.\nEvaluation of the false negative rate. Even though the described evaluation process allows for assessing whether the detected traits are correct, according to manually created species-trait matrices, it does not allow for quantifying the false negative rate of the LLM, i.e., whether all traits described in the text are effectively extracted. In this context, we need to assess whether the false negatives arise due to the LLM extraction process or due to the fact the information is simply not present in the harvested text. To mitigate this issue, we perform an additional evaluation of the trait extraction process by asking botanists whether a certain trait value can be inferred by using a specific piece of text and using this information as ground truth. Specifically, we first randomly selected a trait from one of the species-trait datasets. We then selected a random species from the same dataset and picked a text snippet with a low distance in the DistillBERT embedding space to the name of the trait, in order to increase the number of relevant text-trait pairs. This allowed us to generate 1216 text-trait pairs which we then shared with the botanists. According to the botanists, 298 out of the 1216 snippets contained relevant information about the trait of interest. To assess the capacity of the LLM extraction process in this setting, we construct the corresponding prompts with the same pairs of sentences and traits as the ones presented to the botanists. The prompt used was of the same structure as the one shown in Figure 2. This allows us to investigate whether the LLM behaves in an excessively conservative manner, preferring to return empty results rather than make a mistake, or has a tendency to hallucinate"}, {"title": "RESULTS", "content": "Descriptive text classification\nThe descriptive/non-descriptive dataset creation process described in the \"Descriptive text classification\" section, resulted in approximately 1.45 million sentences; 1.1 million sentences corresponding to non-descriptive text and 356k corresponding to descriptive text. The obtained results are shown in Table 2. Therein, we observe that, within the in-domain validation set, our description classification model reaches very high precision for both classes, i.e., \"Description\" and \"Non-Description\", with F1 scores of 0.96 and 0.99 respectively. However, the recall in the test set drops substantially for the descriptive class, from 0.95 to 0.55, albeit not for the non-descriptive class, which is 0.98 in the test set."}, {"title": "Descriptive text harvesting", "content": "Having trained and validated our descriptive sentence detector, we can now consider any potential source of textual information to extract species descriptions towards a downstream task. The text harvesting step returned description text for the majority of species, but not for all. Specifically, we obtained text for 40/42 species in the Caribbean dataset, 358/361 for the West Africa dataset and 248/333 for Palms. On average, we obtained 35, 36.8 and 43.5 descriptive sentences per species for each dataset respectively. Refer to the Appendix for a list with the Internet domains that contributed the most descriptive sentences."}, {"title": "Automatic trait extraction", "content": "Comparison to manually curated trait data\nThe results in Table 3 show the evaluation of the final trait prediction results. The coverage ranges between 55% and 56%, meaning that over half of traits are assigned a value with the described method. The F1 scores range between 73%, in the Palms dataset, and 78% in the West Africa dataset, with the recall being remarkably constant, between 77% and 78%, and the precision varying between 70% in Palms and 80% in West Africa.\nThe per-trait F\u2081 scores and coverages are displayed in Figure 5. Here we see large variations in both aspects. Some commonly found traits, such as life form and seed color in the Caribbean dataset or plant type and leaf shape in the West African dataset, have been retrieved for well above 80% of species, while trunk and root in the latter are only found for around 10% of species. Also, large variations in terms of F\u2081 accuracy can be observed across traits in all datasets. We see a tendency towards higher accuracy in traits for which fewer values are allowed. For instance, life form in Caribbean has only two possible values, and the F1 stands at over 95%. On the other hand, fruit color in Palms, has around 70% F1 for 12 possible values.\nIn order to visualize the most typical mistakes the model commits on this multi-label task, in which more than one trait value is allowed per trait, we show two co-occurrence matrices side by side; one corresponding to the co-occurrences found within the annotated data (that is, which trait values are simultaneously present in a species) and a second one with the co-occurrences between the annotations and our predictions (which values are predicted for species that are annotated with a certain value).\nIn addition, we can observe that the committed confusions are often reasonable. For instance, if we look at the leaf position trait, we see that our approach has returned opposite when the manual annotations stated alternate-opposite, opposite, whorls of 3 and opposite, whorls of 3, alternate.\nEvaluation of the false negative rate\nIn this section, we compare the ability of the LLM to predict \"NA\" in cases where no information about the desired trait can be found by comparing its responses to those of expert botanists on the same sentences. This allows us to estimate whether the coverage rate actually corresponds to the actual data availability in the harvested text. The confusion matrix in Figure 7 shows that the LLM,"}, {"title": "Automatic trait extraction", "content": "Our quantitative results show that the proposed pipeline is able to return a value for over half of the trait values in the three considered species-trait matrices, with an average F\u2081-score of over 0.75. In addition, the inspection of the errors it commits suggests that they tend to be relatively reasonable mistakes, with similar trait values being typically confused for one another. The results on the false negative rate evaluation show that, in general, the LLM is well-balanced and has no strong tendency towards either hallucinating nor ignoring information. The fact that by using a single trait per query results in very similar performance is a sign that this behavior does not depend on the number of simultaneously queried traits. This means that the low average coverage rate, of about 55%, can probably be blamed on a lack of information in the harvested dataset, rather than on the LLM being unable to pick up the information. A focus on improving the amount of textual information would, therefore, be the best way of further improving the trait coverage. Nonetheless, we have directly used the trait and trait value names as they were proposed by the original authors of the species-trait datasets. It is likely that these specific formulations are not the best possible for our task and can thus be optimized for prompting, such as by including descriptions of the traits and their possible values. In addition, the results using Mixtral-8x22B show that, at the time of publication, it would be possible to reproduce the results in this paper, and scale the approach to new species, using a model with openly available weights."}, {"title": "Limitations", "content": "This study focused on a relatively small number of plant species, approximately 700 in total, for which manually curated trait data was available for evaluation. Although we attempted to mitigate geographic bias toward Europe and North America by exclusively considering tropical species, the study was limited to woody plants. This constraint may affect the generalizability of our findings to the global flora. We also observed that our approach successfully filled in only a little over half of the traits, with up to 25% of species in the Palms dataset failing to yield any text during the web crawling phase. The primary limitation of our method lies in its reliance on species and traits that are textually documented online. As a result, the approach is more suited to retrieving morphological traits, which are more frequently described in online content. To address this limitation, the procedure could be enhanced by incorporating less stringent filtering during text harvesting. This could be further improved by implementing compatibility with JavaScript-based websites and PDF documents and enable the inclusion of non-English text, as many botanical descriptions are available in local languages. Expanding the language scope would be particularly feasible for languages with a significant online presence and a history of botanical use, such as French, Spanish, and Portuguese. Broadening the range of structured online resources used to train the description detector is essential for this multilingual expansion. Although modern LLMs are multilingual, the most significant challenge lies in extending our descriptive text detection approach to multiple languages. This could be addressed by training on structured websites in various languages or using translations of English descriptive texts. Finally, our study focused on categorical traits, though we believe the approach could be adapted for other types of trait formulations, such as numerical values, with modifications to the LLM prompt. We plan to explore this possibility in future work."}, {"title": "Concluding remarks", "content": "We develop and evaluate a pipeline that leverages recent advances in large language models to extract trait information for any set of species from unstructured online text. Unlike other recent approaches that require species-trait information for training, such as [Domazetoski et al., 2023] and [Folk et al., 2023], ours does not require any manual annotations for training. The only manual effort required is the initial creation of the list of traits and the possible trait values along with the list of species name to be examined; this means that the trait extraction can be effortlessly scaled to new sets of species without the need for previous knowledge on species-trait relations. These results point towards the potential of this type of methodology for leveraging the large amounts of unstructured text data available online on species descriptions. Although in this work we limited the list of traits to those"}]}