{"title": "AAPM: Large Language Model Agent-based Asset Pricing Models", "authors": ["Junyan Cheng", "Peter Chin"], "abstract": "In this study, we propose a novel asset pricing approach, LLM Agent-based Asset Pricing Models (AAPM), which fuses qualitative discretionary investment analysis from LLM agents and quantitative manual financial economic factors to predict excess asset returns. The experimental results show that our approach outperforms machine learning-based asset pricing baselines in portfolio optimization and asset pricing errors. Specifically, the Sharpe ratio and average |\u03b1| for anomaly portfolios improved significantly by 9.6% and 10.8% respectively. In addition, we conducted extensive ablation studies on our model and analysis of the data to reveal further insights into the proposed method.", "sections": [{"title": "Introduction", "content": "The pricing of financial assets, such as stocks, has been a focal point in empirical financial economics research. It has a significant impact on social good by moving towards Pareto efficiency in capital allocation. Current asset pricing methods rely on carefully crafting manual macroeconomic indicators or company-specific factors as predictors of future excess returns (Fama and French, 1992, 2015). Despite its great success in the real-world market, they have been challenged by the Efficient Market Hypothesis (EMH) that manual factors will ultimately lose their predictive power in an efficient market when these predictors are fully discovered and used by market participants.\nDue to this rationale, linguistic data, which are the primary sources of traditional discretionary investing, become essential. This is because the dynamics of society and the market are largely driven by the information flow of language. This is also evident in the real financial world, where discretionary portfolio management remains significant today (Abis, 2020). Such investment decisions are mainly shaped by the manager's experience and intuition, as they evaluate assets and determine their value based on information from news, investigations, reports, etc., instead of depending on quantitative models.\nThis phenomenon highlights two key points. First, qualitative discretionary analysis can uncover valuable pricing insights that are absent in economic indicators or market data. Second, even with the integration of current NLP and semantic analysis methods, quantitative factor models have not fully captured these insights. Achieving the synergy between both remains a complex yet appealing objective (Cao et al., 2021). Nonetheless, leveraging linguistic information is complicated as it requires financial reasoning and long-term memory of tracking events and company impressions to interpret. Furthermore, suboptimal interactions in model design between linguistic and manual factors can end up as noise (Bybee et al., 2023).\nIn this study, we introduce a novel asset pricing approach, LLM Agent-based Asset Pricing Models (AAPM), which fuses discretionary investment analysis simulated by an LLM agent and quantitative factor-based methods. AAPM employs the LLM agent to iteratively analyze the latest news, supported by a memory of previous analysis reports and a knowledge base comprising books, encyclopedias, and journals. The embedding of analysis reports is merged with manual factors to predict future excess asset returns. Besides offering a performance edge, our method also provides enhanced interpretability through generated analysis reports. We evaluate our approach with a dataset consisting of two years of news and approximately 70 years of economic and market data. The experimental results show that our approach surpasses machine learning-based asset pricing baselines, achieving a 9.6% increase in the Sharpe ratio and a 10.8% improvement in the average |\u03b1| for asset pricing errors in character-section portfolios. Our primary contributions are summarized as follows:"}, {"title": "Related Work", "content": "Asset pricing aims to search for the fair price of financial assets, such as securities. Sharpe (1964) introduced the groundbreaking Capital Asset Pricing Model (CAPM), which breaks down the expected return of an asset into a linear function of the market return. Various extensions of the CAPM have been developed. Merton (1973) incorporated wealth as a state variable, while Lucas Jr (1978) considered consumption risk as a pricing factor. The single-factor CAPM was later expanded into multi-factor models. Fama and French (1992) proposed the Fama-French 3-factor (FF3) model, which explains returns by size, leverage, book-to-market equity, and earnings-price ratios. They later revised it to a 5-factor model (Fama and French, 2015). Furthermore, Carhart (1997) identified momentum as an additional factor. Ross (1976) formulated the Arbitrage Pricing Theory (APT), which considers asset pricing as an equilibrium in the absence of arbitrage opportunities. The Stochastic Discount Factor (SDF) calculates the price by discounting future cash flows using a stochastic pricing kernel (Cochrane, 2009)."}, {"title": "Financial Machine Learning", "content": "The application of machine learning techniques has been introduced to explore the non-linear interactions among the growing \"factor zoo\" (Feng et al., 2020). Instrumented Principal Component Analysis (IPCA) was developed by Kelly et al. (2020) to estimate latent factors and their loadings from data. Gu et al. (2020) introduced a deep neural network to model interactions. Gu et al. (2021) proposed a conditional autoencoder that considers latent factors and asset characteristics as covariates. Chen et al. (2024) utilized Generative Adversarial Networks to train a neural SDF based on the methods of moments. Additionally, Bybee et al. (2021) conducted an analysis of the Wall Street Journals (WSJ) to gauge the state of the economy. Based on this analysis, Bybee et al. (2023) further suggested using Latent Dirichlet Allocation (LDA) to analyze monthly news topics from WSJ as pricing factors. Recent NLP methods (Xu and Cohen, 2018; Xie et al., 2022) have been employed to forecast stock movements, in contrast to asset pricing, they do not aim to find interpretable factors that explain anomalies in excess asset returns. Our LLM-based approach offers an alternative interpretation through analysis reports."}, {"title": "Large Language Model Agents", "content": "LLM agents possess powerful emergent capabilities, such as reasoning, planning, and tool-using (Achiam et al., 2023). The core of LLM agent programming lies in prompting, which employs contextual hinting text to regulate the output of LLM (Liu et al., 2023). Several prompting strategies have been proposed. Chain-of-Thoughts (CoT) (Wei et al., 2022) encourages the agent to reason in a step-by-step manner. Yao et al. (2022) introduced the ReAct prompt, enabling the agent to refine its output based on the results of previous attempts. It allows the agent to use external tools, such as databases and search engines. Memory is another crucial component of LLM agents. Hu et al. (2023) introduced databases as symbolic memories. Packer et al. (2023) stores dialogues in both long- and short-term memory, analogously to operating systems. Cheng and Chin (2024) developed an agent capable of making \u201cinvestment\u201d decisions on social science time series based on input news, reports, etc., and knowledge base, as well as the Internet. We focus on using the agent to simulate discretionary investment decision-making to synergize qualitative and quantitative asset pricing."}, {"title": "Method", "content": "Given a state vector Vr,a at a time point \u03c4\u2208 {0,1,2,...}, which represents the current status of the market, society, and an asset a, an asset pricing model predicts the excess returns r\u03c4+1,a of the asset at the subsequent time point, expressed as P(r\u03c4+1,a|Vr,a). In our study, each time point corresponds to one day. In traditional factor-based methods, the state Vr,a \u2208 RNF is a vector composed of NF factors that are manually derived from economic indicators, market data, asset characteristics, etc. For instance, the market excess return, the performance disparity between small and large firms, and the difference between high and low book-to-market companies in the Fama-French 3-factor model (Fama and French, 1992). Recently, Bybee et al. (2021) demonstrated that a collection of business news can serve as an alternative representation of macroeconomic conditions, while Bybee et al. (2023) employs LDA to extract news characteristics as economic predictors for pricing. Building on this idea, we use the average embedding of analysis reports that mine values from the news as a proxy for the society, economic, and market states."}, {"title": "Discretionary Analysis with LLM Agent", "content": "The agent utilizes the latest news xt at time t (e.g., a WSJ article published at 9:32 AM on 6 June 2020), along with a note nt on macroeconomics and market trends, to generate an analysis report Rt. The note nt is initalized with a macroeconomics summary n0 produced by GPT-3.5-Turbo-1106 (Brown et al., 2020), the LLM used in our study, prior to its knowledge cut-off date dk. It offers necessary macroscopic context on economic and societal trends not directly available from the news or the memory. The note is then iteratively updated to nt with the new analysis report Rt to keep the context up-to-date, and we also prompt the agent to document investment ideas and market thoughts in the notes to provide a short-term background such as the trends on the market, long-term research oppurtunities to watch. To ensure the note is continuously updated without missing information while preventing information leakage, the dataset in our study starts from dk + 1, immediately following the knowledge cut-off date.\nThe analysis process begins with generating a refined news item x't that summarizes key information from the raw input xt. This step helps control the input length and standardizes the format and style. The refined news x't and the note nt are then combined to form an input It for the agent. The agent will determine if the news contains investment information: if not, it will be skipped; otherwise, an initial analysis report R't will be created. The report undergoes iterative refinement over N rounds. In each round i, the report Ri-1 is used to query an external memory Mt, a vector database initialized with the SocioDojo knowledge base (Cheng and Chin, 2024), which includes textbooks, encyclopedias, and academic journals in fields such as economics, finance, business, politics, and sociology. We use BGE (Xiao et al., 2023) as the embedding model fe, which maps text to a vector e \u2208 Rdemb for querying the memory. This choice is based on the MTEB leaderboard (Muennighoff et al., 2022), where we selected the best retrieval model considering performance, model size, and embedding vector length. In each round i, the top-K most relevant items {mki}Ki=1\u2282Mt are retrieved and provided to the agent along with the report Ri-1 to produce the refined report Ri. The report RN generated after the N-th round is used as the final analysis report Rt for the news xt and to update the note as n't. Then it is inserted into the memory Mt for future reference and pricing, updating the memory to M't."}, {"title": "Hybrid Asset Pricing Network", "content": "We use the embedding model fe to transform each report Rt into an embedding et, where ti represents the timestamp of the i-th news on day d. The average embedding of the analysis reports on a given day d is calculated as ed = \u03a3tei/Nd. According to Bybee et al. (2023), a single day's news is insufficient to fully capture the broader economic and market conditions. Therefore, we employ a sliding window of Lw to derive a smoothed daily embedding sd using the average embeddings of the most recent L = min(Lw,d) days {ed-L+1,ed-L+2, ..., ed} as follows:\nsd=\u03a3Li=1\u03ba(L,i)ed\u2212L+i/L\nwhere \u03ba(L,i) is an exponential decay kernel defined as \u03ba(L,i)=e\u2212\u03b7i/\u03a3Li=1e\u2212\u03b7i. The decay coefficient is denoted as 0 < \u03b7 < 1. We form a raw hybrid state hd,a = [sd; vd,a] by concatenating the smoothed daily state sd with a vector vd,a \u2208 RNF"}, {"title": "Experiment", "content": "We conduct experiments to assess the asset pricing efficacy of the proposed AAPM. The experimental setup is detailed in Section 4.1. Subsequently, we present the outcomes of the portfolio optimization experiments in Section 4.2 and the asset pricing error in Section 4.3. An extensive ablation study of our method is provided in Section 4.4. Furthermore, we explore the predictive capabilities of refined news on economic indicators and stock movements in Appendix C."}, {"title": "Experiment Setting", "content": "We build a dataset comprising two years of WSJ articles spanning from September 29, 2021, to September 29, 2023, following the knowledge cut-off of the version of GPT we used. This approach mitigates potential information leaks while maintaining continuity in note n. Besides the LLM filtering described in Section 3.1, we also manually excluded articles on unrelated topics like travel, lifestyle, and puzzles, based on their WSJ categories. Visualizations of our news dataset can be found in Appendix B. The daily asset returns are sourced from CRSP, while daily risk-free returns and market returns are obtained from Kenneth French's data library.\nWe construct financial economic factors following Jensen et al. (2023). In line with Chen et al. (2024), we duplicate the values from the previous time step for factors that are not updated in the current step to handle discrepancies in the update frequencies of the factors. Additionally, we imputed the missing data values using the cross-sectional median. The data split remained consistent across all our experiments: the initial 9 months of data were utilized as the training set, the following 3 months served as the validation set, and the last 1 year was reserved for testing.\nWe select five recent asset pricing baselines from highly reputed financial economics journals, validated under current empirical finance standards, as indicated by Jensen et al. (2023), to assess our approach: NN (Gu et al., 2020) introduced a deep neural network for asset pricing; IPCA (Kelly et al., 2020) developed an instrumental PCA to identify hidden factors and loadings; CA (Gu et al., 2021) proposed to use a conditional autoencoder; NF (Bybee et al., 2023) employs LDA for the WSJ news as hidden factors similar to ours; and CPZ"}, {"title": "Portfolio Optimization", "content": "We begin by testing the Sharpe ratio for portfolios built on the predicted returns of individual assets. The Sharpe ratio (SR) (Sharpe, 1998) quantifies the risk-adjusted performance of a portfolio as Sp=(rp\u2212rf)/\u03c3, where rf stands for the risk-free return, rp represents the portfolio return and \u03c3 indicates the standard deviation. Furthermore, we evaluate the maximum drawdown, which is the largest decrease in the total value of the portfolio up to time T, expressed as MDD(T) = max\u03c4\u2208(0,T)[maxt\u2208(0,\u03c4)X(t)\u2212X(\u03c4)]. Here, X(\u03c4) is the highest value and X(t) is the lowest value of the portfolio within the time interval (0, \u03c4).\nWe evaluate three prevalent methods for portfolio construction. The Tangency Portfolio (TP), where the asset weights are calculated as wt = Et[R+1RT+1]\u22121Et[R+1], with R+1 denoting the predicted excess returns of all assets. Provides a theoretical portfolio in an ideal market without trading frictions. Next, we examine the more practical long-short decile portfolios, which involve ranking assets by their expected returns, going long on"}, {"title": "Asset Pricing Error", "content": "We further analyze the asset pricing errors of the proposed method. Following Bybee et al. (2023), we chose 78 anomaly portfolios as test assets. These portfolios were constructed using 78 characteristics, including typical anomaly characteristics such as idiosyncratic volatility, accruals, short-term reversal, and others, as identified by Gu et al. (2020). We applied multiple metrics. The average absolute alpha avg |\u03b1| is computed by dividing the expected value of the estimated error term et,i by the square root of the average squared returns E[Rt,i] for all quantile-sorted portfolios. This normalization was performed to account for variations"}, {"title": "Ablation Study", "content": "We conduct ablation studies to examine the influence of various components in our approach. Initially, we evaluate the performance of different modules in our agent design in Section 4.4.2, followed by an examination of the depth and width of the analysis, which are controlled by N and K respectively, in Section 4.4.2."}, {"title": "Agent Architecture Design", "content": "We analyze our architecture in a reverse manner, beginning with a \"Naive\" agent that generates the analysis report directly from the refined news without any supplementary information or iterative analysis, while the pricing network solely uses the daily embeddings as input. We then incrementally add components to develop stronger baselines until arriving at our method. The results are shown in Table 3, and the baseline illustrations are provided in Appendix D.\nFurthermore, we contrast these methods with the news-based asset pricing baseline NF (Bybee et al., 2023), along with an NF model incorporating manual factors, akin to our full model. It is important to highlight that NF employed WSJ news over a period of 33 years, whereas we utilized only 2 years of news data.\nOwing to the analytical capabilities and feature extraction proficiency of LLMs, the \u201cNaive\u201d baseline enhances the SR by 2.2% with comparable pricing errors to NF. Incorporating external memory further boosts the SR by 4.3% and decreases the average |\u03b1| by 5.7% over \u201cNaive\", highlighting the significance of additional contextual information when interpreting business news. Moreover, asset embedding contributes to a 2.1% increase in SR and a 2.3% reduction in average |\u03b1| by introducing asset-specific loadings.\nBy combining both, the \u201cMemory\u201d baseline enhances the SR of \u201cNaive\u201d by 6.0% and decreases the average |\u03b1| by 8.0% with a lower t value. Incorporating the manual factors, the SR saw a slight increase of 1.3%, while the average |\u03b1| decreased by 2.5%. In comparison, the performance of NF declined after the introduction of manual factors, which is consistent with the findings of Bybee et al. (2023), where the inclusion of Fama-French factors reduces the SR, which may due to suboptimal interactions between factors and news features.\nAfter pretraining the pricing network with historical factor data, the performance of the \u201cHybrid\" baseline saw a notable enhancement of 5.0% in SR and a 9.9% reduction in the average |\u03b1| when compared to the \u201cMemory\u201d baseline. This demonstrates the synergy between manual factors and LLM-generated reports, resulting in a successful non-linear interaction. The improvements from our iterative refinement and long-term notes over the \"Hybrid\" baseline are 3.8% and 1.3% in SR, 4.1% and a slight negative -1.4% in average |\u03b1|, respectively with a lower t value and a similar level of MDD. These enhancements collectively yield 4.8% and 9.6% gains in SR and average |\u03b1| respectively in our full method compared to a \u201cHybrid\u201d baseline, underscoring the effectiveness of our agent architecture design."}, {"title": "Analysis Depth and Width", "content": "We further investigate the depth of the analysis, which is controlled by the number of iterations N to refine the analysis report, and the width, which is determined by K, the amount of relevant information to check. The results are shown in Figure 5. We keep one variable constant and test the other. We observe that the agent benefits from more rounds of analysis and a broader range of relevant information overall with a sharp decline in marginal gain after a certain point around K \u00d7 N = 15, likely due to the sufficiency of the provided information. Thus, we test an extreme case where N = 1 and K = 15, resulting in the SR dropping to 3.12. This indicates that iterative refinement is necessary, as items retrieved in different rounds of refinement provide diverse information as the query evolves. In contrast, a single retrieval leads to items falling into the same topic, with the value of additional items decreasing rapidly and potentially introducing noise."}, {"title": "Discussion", "content": "Our proposed approach presents a promising method to fuse qualitative discretionary investment with quantitative factor-based strategies through the use of LLM agents. Nonetheless, there is still much to investigate regarding additional capabilities of LLM agents that could further enhance asset pricing power. Firstly, internet access and a broader range of information sources, including those available in SocioDojo, may enable the agent to generate more in-depth analyses, as discretionary investment relies on information beyond just news or domain knowledge. Secondly, employing specialized financial LLMs like FinGPT (Yang et al., 2023) could further improve the agent's financial analytical capabilities. Finally, it is crucial to consider multimodal information, such as diagrams and figures, which are frequently presented in financial documents."}, {"title": "Conclusion", "content": "In this research, we introduced AAPM, a model that combines qualitative analysis from the LLM agent with quantitative factors in asset pricing. AAPM surpassed established asset pricing methods in multiple evaluations, including portfolio optimization and asset pricing error. Additionally, we performed an in-depth analysis of each component in our agent design. We believe that our study can improve the comprehension of the interaction between discretionary investment and quantitative factor-based models, toward a society with increased economic efficiency."}, {"title": "Limitations", "content": "Our experiments only focus on the US market and English news, which may potentially impact model performance in lower-resources languages. In order to exclude the information leak, we can only apply news data after September 2021 which restricts our study to a 2 years period after this time, however, we use a large test split where half of the dataset was applied as the test set to best evaluate how well the proposed method can be generalized beyond the training period. Finally, public information in the stock market includes not only news, but also reports, reports from social networks, academic journals, opinions from experts, etc.; we do not cover these information nor consider multimodal inputs as discussed in Section 5."}, {"title": "Ethics Statement", "content": "We do not identify any ethical concerns in our approach. Our study does not involve any human participation. Furthermore, the application area of our method is not directly related to humans, reducing the risk of abuse or misuse. In fact, considering a wider range of information, our method has the potential to enhance market efficiency, resulting in economic benefits for society."}, {"title": "Hyperparam Search", "content": "For our approach, we conduct hyperparameter searches using Weights & Biases Sweep (Biewald, 2020). Table 4 shows the distribution of empirically significant parameters used for our hyperparameter search. Here, U(a, b) signifies a uniform distribution between a and b, while Ulog(a, b,r) indicates a logarithmic uniform distribution with base r between a and b. The evaluation criteria of our method are based on the Sharpe ratio of an equal-weight long-short portfolio.\nWe conducted our experiments on our clusters, the major workload has the following configuration:\n\u2022 2 x Intel Xeon Silver 4410Y Processor with 12-Core 2.0GHz 30 MB Cache\n\u2022 512GB 4800MHz DDR5 RAM\n\u2022 2 \u00d7 NVIDIA L40 Ada GPUs (no NVLink)\nWe employed PyTorch Lightning (Falcon and The PyTorch Lightning team, 2019) for parallel training."}, {"title": "News as Financial Economic Predictor", "content": "To explore the predictive capability of business news on financial and economic dynamics, we conduct an experiment using refined news features to forecast the economic indicators in Appendix C.1 and market movements in Appendix C.2. We embed the refined news directly and use the daily averaged embeddings of the refined news as predictors in our experiments."}, {"title": "Economic Indicators", "content": "We assess the capability of news features to forecast the daily percentage changes in typical and most popular macroeconomic indicators in different topics sourced from the FRED database. These indicators encompass the stock market (SP500), the market yield on U.S. Treasury Securities at a 10-Year constant maturity (DGS10), Moody's seasoned Baa corporate bond minus the federal funds rate (BAAFF), the 10-year breakeven inflation rate (T10YIE), Brent crude oil prices (DCOIL-BRENTEU), and the 30-year fixed-rate conforming mortgage index (OBMMIC30YF). The findings are illustrated in Figure 9. The forecasted results exhibit a high degree of accuracy, as evidenced by the high R2 score. This implies that news provides valuable insights for predicting macroeconomic indicators."}, {"title": "Stock Price Predictor", "content": "We further investigate the predictive power of news features to the price movements of individual stocks. We chose 8 typical stocks that has been frequently mentioned in the new from our analysis in Appendix 8, and used refined news features as predictors to estimate their daily percentage price changes. The results are displayed in Figure 10, with the corresponding R2 scores given in brackets. We note high accuracy and R2 scores for all selected stocks, suggesting that news can significantly help in forecasting stock prices. However, it is important to recognize that due to non-stationarity and the risk of overfitting, stock price prediction cannot be directly applicable as asset pricing (Kelly et al., 2023), but it provides insights into the value of the news in the pricing of individual stock."}, {"title": "Illustration of the Ablation Baselines", "content": "We progressively developed three baselines, starting with a naive agent, followed by a memory agent enhanced with an external vector base, and culminating in a hybrid agent that incorporated manual"}, {"title": "Prompts", "content": "In this section, we will present the prompts utilized by the agent, covering the refinement of the raw news input, the iterative refinement of the analysis report, the initial macroeconomic note, and the updating of notes."}, {"title": "News refinement", "content": "This refinement of the raw news input discussed in Section 3.1 is achieved through the following prompt:"}, {"title": "Iterative analysis", "content": "In the first iteration, the analysis begins with the following prompt:"}, {"title": "Update of note", "content": "The macroeconomic and market trend note is updated with the new analysis report by the following prompt:"}, {"title": "Initial Macroeconomic Summary", "content": "We use the LLM to summary the macroeconomic status before the beginning time of the dataset, the summarization generated by GPT-3.5-Turbo-1106 is as follows:\nBy September 2021, the global macroeconomic landscape was heavily influenced by the ongoing impacts of the COVID-19 pandemic. Many countries were in various stages of recovery, grappling with challenges such as disrupted supply chains, inflationary pressures, and shifts in employment patterns. Key points include: 1. **Economic Recovery**: Different regions experienced uneven recovery, with some economies bouncing back faster due to successful vaccination campaigns and substantial fiscal stimuli. For instance, the U.S. and China showed signs of robust economic rebound, whereas many European countries were still struggling with economic output below pre-pandemic levels. 2. **Inflation Concerns**: Rising inflation became a significant concern in many countries, partly due to supply chain disruptions and increased demand as economies reopened. This led to higher prices for commodities, goods, and services. 3. **Monetary Policy**: Central banks, including the U.S. Federal Reserve and the European Central Bank, maintained accommodative monetary policies, with low interest rates to support economic growth. However, there was growing discourse about when and how to start tapering these measures. 4. **Employment Fluctuations**: While some sectors and countries saw a rapid recovery in employment levels, others faced ongoing job losses, highlighting the pandemic's uneven impact across different industries. 5. **Supply Chain Disruptions**: Global supply chains were strained, impacting everything from consumer electronics to automobile manufacturing, leading to shortages and delays. 6. **Shifts in Consumer Behavior**: The pandemic accelerated trends like online shopping and remote working, reshaping economic activities and consumer behaviors in lasting ways. Overall, the state of global macroeconomics by September 2021 was defined by recovery efforts amidst ongoing challenges, with significant variability between different countries and regions."}]}