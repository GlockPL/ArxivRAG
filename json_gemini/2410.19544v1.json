{"title": "PMM-Net: Single-stage Multi-agent Trajectory Prediction with Patching-based Embedding and Explicit Modal Modulation", "authors": ["Liu Huajian", "Dong Wei", "Fan Kunpeng", "Wang Chao", "Gao Yongzhuo"], "abstract": "Analyzing and forecasting trajectories of agents like pedestrians plays a pivotal role for embodied intelligent applications. The inherent indeterminacy of human behavior and complex social interaction among a rich variety of agents make this task more challenging than common time-series forecasting. In this letter, we aim to explore a distinct formulation for multi-agent trajectory prediction framework. Specifically, we proposed a patching-based temporal feature extraction module and a graph-based social feature extraction module, enabling effective feature extraction and cross-scenario generalization. Moreover, we reassess the role of social interaction and present a novel method based on explicit modality modulation to integrate temporal and social features, thereby constructing an efficient single-stage inference pipeline. Results on public benchmark datasets demonstrate the superior performance of our model compared with the state-of-the-art methods. The code is available at: github.com/TIB-K330/pmm-net.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-agent trajectory prediction (MATP) problem aims to predict the future trajectories for one or multiple interacting agents conditioned on their past movements. It serves as a critical component for autonomous navigation tasks in dynamic environments, providing essential perception infor-mation for planners during decision-making and obstacle avoidance [1]. Compared to conventional time-series fore-casting (TSF) problem, MATP places greater emphasis on the accuracy of short-term predictions rather than long-term trends, while also requiring high real-time performance. In addition, MATP differs from TSF in two key challenges: handling complex interactions among agents, and addressing the inherent indeterminacy of human behavior, which leads to the multi-modal nature of future states.\nExisting mainstream approaches have commonly adopted Transformer-based architectures that are widely used in NLP domain [2], leveraging their remarkable capability of ex-tracting semantic correlations among elements in sequences. However, recent studies [3] have demonstrated that such architectures are redundant and inefficient for time-series forecasting. This is because self-attention is permutation-invariant, requiring positional encoding or other methods to preserve temporal information. The semantic information contained in individual trajectory points is too sparse, leading to a loss of temporal relationships between tokens in deeper network layers, which ultimately results in performance degradation of the entire model. Therefore, it is necessary to explore more effective feature extraction methods with lower computational complexity.\nFor multi-modal prediction, existing approaches often rely on generative models or employ goal-conditioned frame-works. Generative models represent multi-modality using a latent variable, with typical examples including gener-ative adversarial networks (GANs), conditional variational auto-encoders (CVAEs), and more recent frameworks based on probabilistic diffusion models (DPMs). Goal-conditioned methods usually adopt a two-stage training framework, pre-dicting candidate endpoints then guiding the the regression of the future trajectory. However, generative models are often inefficient and struggle to meet the high real-time demands of autonomous robotic tasks. On the other hand, the two-stage architecture used in goal-conditioned approaches can lead to unstable training or produce unnatural trajectories.\nTo address these critical gaps in MATP problem, we present a novel approach in this letter. Inspired by recent advances in TSF [4], we propose a patching-based tempo-ral feature extraction module. By incorporating sub-series-level patches, this module enhances locality and captures comprehensive semantic information that is unavailable at the point level. In addition, we design a graph-based social feature extraction module that ensures translational and rota-tional invariance under series-level normalization, improving the model's generalization ability across different scenarios. Furthermore, we construct a novel single-stage framework through a cross-attention-based modality modulation using social information, which decouples temporal and social features, thereby reducing the overall computational com-plexity. Extensive experiments demonstrate that our method accurately predicts plausible future trajectories with multi-modality, achieving state-of-the-art results on Stanford Drone and ETH/UCY datasets.\nThe main contributions of this paper can be summarized as follows: (1) We propose a sliding-window patching-based input embedding scheme that addresses the limitation of semantic correlation loss caused by directly embedding 2D points as tokens, enabling effective temporal feature extraction for agents; (2) We propose an novel social feature extraction method that utilizes an inverted attention mecha-nism among agents, achieving low computational complexity while ensuring translational and rotational invariance of the scene; (3) We design a distinct single-stage framework named PMM-Net that employs a cross-attention-based ex-plicit modality modulation to achieve multi-modal prediction for MATP, while more effectively supporting subsequent decision-making and planning tasks."}, {"title": "II. RELATED WORKS", "content": "For general learning-based TSF tasks including MATP, the essence lies in extracting temporal features through ob-serving historical sequences, thereby constructing an implicit model. Specifically, MATP task requires modeling social interactions by aggregating the influences of neighboring agents.\nSimilar to sequence modeling, com-mon MATP approaches often directly adopt RNNs such as LSTM and GRU to capture the temporal features of observed trajectories. However, these methods struggle to effectively model long-term temporal correlations. Specially, [5] uses a temporal convolutional network to extract temporal features, which can process spatio-temporal interaction features in par-allel and reduce computational complexity. [6] models and forecasts trajectories from the spectral domain, employing the Discrete Fourier Transform to obtain trajectory spectrums to capture agents' detailed motion preferences across differ-ent frequency scales. [7], [8] exploits an external addressable memory to explicitly store representative instances in the training set. By recalling related instances from memory instead of summarizing the entire history of observed inputs into a single internal latent state, these methods overcome the information loss issue of RNN.\nWith the invention of Transformers and positional encod-ing, many works start to adopt self-attention mechanism for sequence modeling due to their strong ability to capture long-range dependencies. However, recent studies indicate that directly using the self-attention mechanism does not necessarily improve, and may even degrade, the performance of TSF. While works in TSF, such as [4], [9], have explored various modifications to mitigate the performance degrada-tion of Transformer-based methods in temporal prediction, no comprehensive solution for temporal feature extraction in MATP has yet been developed.\nSocial interaction is a unique type of implicit information in MATP. Early works often adopted or improved upon the social pooling mechanism proposed by Alahi et al. to aggregate the hidden states of neigh-boring agents within a certain distance threshold. Grid-based methods, such as [10], were subsequently proposed to explore additional simple rules to enhance their capacity. Specially, [11] introduced a grid-based approach using polar coordinates which combines the interpretability of model-based approaches with data-driven enhancements, reducing the reliance on complex network structures and high-quality training data. However, these pooling-based methods assume that relative distance between agents is the key factor in interaction modeling, which is not always accurate. Besides, SMEMO [8], building on [7], exploits the external memory as a shared workspace to reason about social interactions between multiple agents.\nDue to inherent structural similarities, spatio-temporal graphs are widely used to model social scenarios. A rep-resentative example is [10], which extracts edge features from neighboring agents through element-wise summation and LSTM, followed by an additive attention mechanism to aggregate the edge features connected to the same node, constructing a social influence vector. Agentformer [12] designs a special agent-aware attention mechanism, allowing direct feature interaction across time and agents. Methods such as [13], [14], [5], [15], [16], [17] directly employ GAT [18] or its variants to simulate interactions as the edges between different nodes. However, these methods rely on refining temporal features, and the stacking of spatio-temporal attention significantly impacts the model's com-putational efficiency, while limiting the use of generalized normalization methods that could effectively enhance model performance.\nThe multi-modal prediction task is non-trivial, as a single input can correspond to multiple outputs. To address this issue, existing methods can generally be categorized into two major types: generative methods, which represent the multi-ple possible future trajectories as a probability distribution, and another type that predicts several possible goals, using them to guide the regression of the trajectories.\nFor generative methods, early pioneering work Social-GAN [19] employed GANs to generate multi-modal trajec-tories. However, GANs are difficult to train and can suffer from mode collapse. Approaches such as [10], [12], [14] explicitly handle multi-modality by leveraging the CVAE latent variable framework, but this approach can sometimes produce unnatural or unrealistic results. The diffusion model, a recent advanced generative framework inspired by non-equilibrium thermodynamics, was employed by Gu et al. in MID [20] to model the indeterminacy of human behavior. Following MID, LED [15] incorporated a leapfrog initializer to skip many denoising steps to reduce inference time.\nFor goal-conditioned approaches, TNT [21] was the first to adopt candidate goal prediction to assist a deterministic model in generating diverse outputs in parallel. PCCSNet [22] utilizes a deep clustering process to obtain multiple modality representations. [13] employs similarity search to match observed trajectories with training data stored in an expert repository, thereby obtaining target candidates. SGNet [23] estimates and utilizes goals at multiple temporal scales to better capture the potentially varying intentions of pedestrians. SRGAT [16] further considers the mutual influences among multi-modal target points for different agents. However, as explained earlier, generative methods are inefficient, and goal-conditioned methods that employ two-stage architectures require additional hyper-parameter tuning, which can lead to an unstable training process."}, {"title": "III. METHODOLOGY", "content": "The multi-pedestrian trajectory prediction problem can be formulated as following: given N historical trajectories $X$ in a 2D scenario $W \\subset \\mathbb{R}^2$ of length $T' \\in \\mathbb{N}^*$, the goal is to generate plausible K-modal future trajectories of a\nspecific length $T \\in \\mathbb{N}^*$ for each pedestrian based on all prior information. Each input historical trajectory can be denoted as $X^i = [p^i_{-T'+1}, p^i_{-T'+2},..., p^i_0]$, $\\forall i \\in \\{1, 2,..., N\\}$, and the corresponding predicted future trajectories can be written as $Y^i = [p^i_1, p^i_2, ..., p^i_T]$. In this research, we focus solely on uni-modal input, meaning we consider only the trajectory information within the same scene and ignore map or other environmental context."}, {"title": "B. Patching-based Temporal Feature Extraction", "content": "The original uni-modal input for the trajectory prediction problem of a specific agent i is a two-channel sequence of 2D position coordinates $X^i \\in \\mathbb{R}^{T'\\times2}$. Certain existing methods [16] use techniques like differencing to obtain pseudo-velocity $v$ or pseudo-acceleration $\\bar{a}$ as input for data augmentation, but this method has significant limitations in terms of scenario generalization. Our patching-based mod-ule further delves into this concept by utilizing a learnable approach to capture high-dimensional kinematic information.\nAs illustrated Fig. 1, for the two series channels $p_x$ and $p_y \\in \\mathbb{R}^{T'}$, we applay two channel-independent sliding-window MLPs of length P = 3 with ReLU activation to map them into high-dimensional latent state spaces, denoted as $Z_x$ and $Z_y \\in \\mathbb{R}^{(T'-2)\\times F}$.We then use a gated fusion, as shown in (1), to merge the two channels, resulting in T' - 2 input tokens, denoted as $Z\\in \\mathbb{R}^{(T'-2)\\times F}$.\n$\\sigma$ = Sigmoid($W[Z_x || Z_y] + b)$\n(1)\nwhere the operator pair (W, b) denotes a trainable linear projection with bias and operator $||$ denotes concatenation. Based on the aforementioned embedding method, we design an encoder that combines self-attention with GRU to extract temporal feature between tokens Z. Compared to a pure Transformer-based architecture, the proposed method has ad-vantages in few-shot settings. We use a vanilla 3-layer multi-head Transformer encoder with additive positional encoding, which we won't elaborate on here. Its output, Z', retains the same dimensions as Z. Finally, the GRU [24] is employed to enhance the temporal relationships between the tokens $z_t \\in Z'$ as:\n$\\tilde{z}_t, c_t = GRU(z_t, c_{t-1})$,\n(2)\nwhere $c_t$ denotes the hidden state output by the GRU at timestamp t. We concatenate all the outputs $\\tilde{z}_t$ to form the agent's final temporal features, denoted as $Z$."}, {"title": "C. Graph-based Social Interaction Representation", "content": "To effectively aggregate the social interaction information of neighboring agents in a scene while ensuring rotational and translational invariance, we propose a novel directed graph-based scene normalization method along with a graph neural network (GNN) based feature extraction module. For\nthe historical trajectories of all agents in the same scene $X$, we translate them to have t = 0 as the origin, while defining the features of directed edges based on the relative positional relationships between agents. Specifically, for a central agent i, the edge features in the graph, pointing toward this agent, represent the relative positional relationships between neighboring agents $j\\in N_i$ and i. These relationships are defined using a polar coordinate system, where the direction of the pseudo-velocity vector $v_i$ of the central agent serves as the zero-angle reference, as follows:\n$\\begin{cases}\nd_{ij} = (p_0^j - p_0^i, p'_0 - p_0)\\\\\ncos\\theta_{ij} = \\frac{(d_{ij}, v_i)}{||d_{ij}||||v_i||}\n\\end{cases}$\n(3)\nTo ensure numerical stability, we use the cosine of the angle $\\theta_{ij}$ instead of the angle itself. Further, we employ an MLP with ReLU activation to extract high-dimensional features for the edges, which is defined as follows:\ne_{ij} = MLP(d_{ij} || cos \\theta_{ij})\n(4)\nInspired by [25], we use an inverted channel-independent encoding method to extract node state features, as shown in Fig. 2. Specifically, the two channels of the node's historical trajectory, $p^i_x$ and $p^i_y$, are first linearly projected to higher dimensions, then fused via a gated mechanism to obtain the node feature $h_i \\in \\mathbb{R}^S$. We then apply a GNN to extract features from the directed graph. Based on [18], we define the message passing function of the GNN as follows:\n$\\begin{aligned}\nu_{ij} &= LeakyReLU(W[h_i || h_j || e_{ij}] + b) \\\\\na_{ij} &= Softmax_j(\\nu_{ij}) = \\frac{exp(\\nu_{ij})}{\\sum_{k \\in N_i} exp(\\nu_{ik})}\\\\\nh_i' &= PRELU(\\sum_{j\\in N_i}a_{ij} [Wh_j + b])\n\\end{aligned}$\n(5)"}, {"title": "D. Multi-modal Linear Projection with Social Refinement", "content": "Inspired by [26], we propose a novel single-stage frame-work to handle the multi-modal prediction requirement in MATP, while introducing a new perspective on the role of social information. By employing multi-modal linear pro-jections, we map the temporal feature of agents into a fixed number K of modalities, and instead of predicting end-points or Gaussian probability distributions of future trajectories, as in existing approaches, we assign a discrete probability score to each trajectory through an independent explicit scoring head. Formally, we have:\n$f_k = MLPk(Z_i), k \\in \\{1, ..., K\\} := K,$\n(6)\nwhere MLPk represents the k-th modality-specific MLP with dedicated parameters. $f_k \\in \\mathbb{R}^{H}$ serves as the high-dimensional latent variable for each potential modality.\nAs illustrated in Fig. 3, our framework can be loosely regarded as a reversed encoder-decoder architecture. The social feature $h_i$, described in Section III-C, is treated as the output of the encoder, while $\\{f_k |\\forall k \\in K\\}$ serves as the input to the decoder. We use a shared-weight MLP with a PRELU activation function to map $f_k$ into the same dimension as $h_i$, denoted as $f_k'$. Different from the standard Transformer used in Section III-B, in the modality modulation module,"}, {"title": "E. Training Objective", "content": "The proposed framework incorporates two parallel loss components concerning trajectory prediction and classifica-tion to capture multi-modal futures. Given the ground truth trajectory $\\{p_t^i\\}_{t=1}^T$ of a specific agent i, the trajectory loss $L_{traj}$ can be defined by calculating the mean of the per-step difference only for the most accurate trajectory over K predicted modalities $Y_k$:\n$L_{traj} = \\frac{1}{T}\\sum_{t=1}^{T} min_{k=1}^{K} ||p_t^i - \\hat{p_t}^k||^2$\n(10)\nClassification loss $L_{cls}$ is the Binary Cross Entropy Loss applied to the assigned probabilities $P_i$ of K trajectories, where the ground truth probability of the closest trajectory is set to 1 and the others to 0:\n$L_{cls} = L_{BCE} (P_i, y_i)$,\n(11)"}, {"title": "IV. EXPERIMENTS", "content": "We summarize the test results on the ETH/UCY dataset in Table I, highlighting the best performance in bold-red and the second best in blue. Results reproduced based on the open-source code are denoted by \u2020. It can be intuitively observed that our proposed model outperforms all baseline methods in most cases, achieving the highest average ADE20/FDE20 and the best or second best to the best performance across the all five scenarios. Specifically, compared with current SOTA method SRGAT, which adopts a goal-conditioned multi-modal prediction approach, the proposed method reduces the average ADE20/FDE20 from 0.19/0.31 to 0.17/0.28, achieving 10.5%/9.6% improvement. Compared with current SOTA method LED, which utilizes a probabilistic model for multi-modality, the proposed method reduces the aver-age ADE20/FDE20 from 0.21/0.33 to 0.17/0.28, achieving 19.0%/15.2% improvement.\nWe summarize the test results on the SDD dataset in Table II with the best performance highlighted in bold. \"T\" denotes the method only using the trajectory position information, and \"T + I\" denotes the method using both position and visual image information. Specifically, com-pared with current SOTA method that uses only historical trajectories as input, SIM, the proposed method reduces the average ADE20/FDE20 from 7.40/11.39 to 6.30/10.34, achieving 14.9%/9.2% improvement. Compared with current SOTA method that uses additional RGB maps as con-text information, V2-Net-SC, the proposed method reduces\nwith limited training data. In contrast, overly complex networks, while appearing fancy, do not provide significant performance improvements and can actu-ally reduce the model's efficiency. Our ablation experiments indicate that, in addition to the two proposed submodules, linear projection plays a crucial role in the overall per-formance of the framework, surpassing all decoder-based structures, including GRU, LSTM, and Transformer-based models. This finding aligns with recent conclusions from related work [33]. Furthermore, the performance of gen-erative decoder-based methods generally lags behind that of contemporaneous goal-conditioned methods and exhibits"}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "In this letter, we present PMM-Net, a novel single-stage framework to address MATP problem for robotic applica-tions. Within this framework, we design an efficient patching-based temporal feature extraction module and a graph-based social feature extraction module. We reassess the role of social interaction in the MATP problem and innovatively devise a method based on explicit modality modulation to integrate temporal and social features, achieving effective feature extraction and cross-scenario generalization while ensuring low computational complexity. Experimental results demonstrate the superiority of our method which achieves state-of-the-art performance on both the SDD and ETH/UCY benchmarks while satisfying real-time inference needs for embodied intelligent agents.\nWe attribute the effectiveness of the temporal feature ex-traction module to the ability of local patches to capture high-order kinematic information of trajectory points through im-plicit differentiation, thereby enhancing the semantics in each embedded token. This enhancement enables subsequent self-attention layers to effectively capture temporal dependencies. In the social feature extraction module, we utilize a channel-independent series-global representation for the historical trajectories of neighboring agents, which can also be viewed as a special patch with a length equal to that of the historical trajectory, embedding the normalized historical trajectories into variate tokens. Although this approach results in a slight decrease in performance, it effectively reduces computational complexity in scenarios with a high number of interacting pedestrians, ensuring real-time performance.\nOur experimental results demonstrate that effective feature extraction is a sufficient condition for achieving good pre-diction performance with limited training data. In contrast, overly complex networks, while appearing fancy, do not provide significant performance improvements and can actu-ally reduce the model's efficiency. Our ablation experiments indicate that, in addition to the two proposed submodules, linear projection plays a crucial role in the overall per-formance of the framework, surpassing all decoder-based structures, including GRU, LSTM, and Transformer-based models. This finding aligns with recent conclusions from related work [33]. Furthermore, the performance of gen-erative decoder-based methods generally lags behind that of contemporaneous goal-conditioned methods and exhibits"}]}