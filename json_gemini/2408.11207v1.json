{"title": "Quantum Inverse Contextual Vision Transformers (Q-ICVT): A New Frontier in 3D Object Detection for AVs", "authors": ["Sanjay Bhargav Dharavath", "Tanmoy Dam", "Supriyo Chakraborty", "Prithwiraj Roy", "Aniruddha Maiti"], "abstract": "The field of autonomous vehicles (AVs) predominantly leverages multi-modal integration of LiDAR and camera data to achieve better performance compared to using a single modality. However, the fusion process encounters challenges in detecting distant objects due to the disparity between the high resolution of cameras and the sparse data from LiDAR. Insufficient integration of global perspectives with local-level details results in sub-optimal fusion performance. To address this issue, we have developed an innovative two-stage fusion process called Quantum Inverse Contextual Vision Transformers (Q-ICVT). This approach leverages adiabatic computing in quantum concepts to create a novel reversible vision transformer known as the Global Adiabatic Transformer (GAT). GAT aggregates sparse LiDAR features with semantic features in dense images for cross-modal integration in a global form. Additionally, the Sparse Expert of Local Fusion (SELF) module maps the sparse LiDAR 3D proposals and encodes position information of the raw point cloud onto the dense camera feature space using a gating point fusion approach. Our experiments show that Q-ICVT achieves an mAPH of 82.54 for L2 difficulties on the Waymo dataset, improving by 1.88% over current state-of-the-art fusion methods. We also analyze GAT and SELF in ablation studies to highlight the impact of Q-ICVT. Our code is available at Q-ICVT.", "sections": [{"title": "RELATED WORK", "content": "LiDAR based 3D OD: LiDAR point clouds, which are typically unordered collections of data points, can be divided into three main types: voxel-based, point-based, and point-voxel fusion methods. Voxel-based techniques, as indicated in studies like [3, 8, 16], convert point cloud data into voxels and then use deep sparse convolution layers to extract features. Point-based methods, referenced in [3, 40], involve using stacked Multi-Layer Perceptrons (MLPs) to process raw point cloud data and obtain point-level features. Unlike earlier studies, recent works [10, 45] have introduced hybrid methods that combine point and voxel-based representations for more comprehensive feature extraction.\nMulti-modal Integration for 3D OD:Integrating monocular vision with LiDAR point clouds enhances 3D object detection [2, 18, 41, 43, 44]. Monocular systems infer 3D bounding boxes from 2D images but lack depth information [43], addressed by estimating pixel-level depth [43]. Recognizing objects in 2D images often precedes analyzing point cloud data [27, 32, 47], typically using a two-step, object-centered fusion approach [17, 32]. Mid-level fusion strategies, like local-global fusion methods [7, 21] and other approaches [31, 56], combine 2D and 3D data by transferring information across their respective backbones. However, optimal alignment between camera and LiDAR features remains a major challenge [21]. Additionally, maintaining matching camera characteristics becomes complex when numerous LiDAR points are combined within a single 3D voxel [7]. To address this challenge, we have introduced a novel two-stage fusion method, known as Q-ICVT. Our main contributions are defined as follows:\n\u2022 We introduce an adiabatic computing-inspired transformer, GAT, to align sparse voxelized features with dense image features in a global context.\n\u2022 We develop the sparse attention of gating experts, SELF, to achieve local fusion between RoI features and dense image features."}, {"title": "METHODOLOGY", "content": "LiDAR and Image Feature Estimation Let's define the multi-modal input-output sequences as {(Ij, Lj), (I(j\u22121), L(j-1)), ...} for simplicity. Each input sequence at the j-th step consists of two types of data: LiDAR, referred to as Lj, and a camera image, represented by x \u2208 Ij \u2208 RH\u00d7W\u00d73. The raw LiDAR point cloud for the j-th input is denoted as Lj \u2192 Qaw, with Qaw = (Up, Vp, Wp, Gp)Np=1 where N represents the total number of points. Our objective is to design a robust local-global fusion integration to obtain adequate performance. The input point cloud data from the j \u2013 th position (Q) is converted into a voxelized representation with the coordinates U \u00d7 V \u00d7 W \u00d7 Cv; this representation is represented by the symbol Gv. The calculation of voxel features involves the mean value of point-wise features applied to non-empty voxels [7, 21]. The farthest point sampling (FPS) technique is employed for determining critical points [38]. This results in the generation of K crucial points (G), where K is defined as 4096 for WOD. Then, the average of attributes from each point within the voxel is computed to characterize non-empty voxels, such as three-dimensional coordinates and reflectance values. Following this, a sequence of 3 \u00d7 3 \u00d7 3 3D sparse convolutions [39] are performed on the feature volumes of the point cloud. This results in the downsampling of spatial resolutions by 1x, 2x, 4x, and 8x, respectively. Following that, hierarchical intra-voxel regions (RoI) are obtained using a region proposal network [7, 21, 39] to generate initial bounding box proposals. Consequently, the sparse LiDAR feature is represented as Gv \u2208 RHvxWv\u00d7Cv. Similarly, dense semantic image features G1 \u2208 RH1\u00d7W1\u00d7C1 are obtained using a 2D detector [25, 35].\n2.1 Adiabatic computing in GAT\nIn the context of vision transformers [13, 55], the main goal is to utilize the reversibility of adiabatic processes [4, 20, 33, 34] to improve the global context and effectively handle the interaction between two modalities: LiDAR point cloud (Gv \u2208 RHv\u00d7Wv\u00d7Cv) and image data (GI \u2208 RH1\u00d7W1\u00d7C1). In order to assess the reversibility process, both forward and backward transformations were conducted between two distinct feature modalities. One modality was represented in a sparse form using LiDAR, while the other modality was represented in a dense form retrieved from the input image.\nLet us consider a reversible block in a transformer layer, as illustrated in Figure 1. The GAT module is capable of effectively handling sparse and dense modalities through forward and reverse mechanisms [12]. To attain this objective, we first consider the forward transition through linear projection data as a global query [13] with key-value pair matching. These forward mechanism transformer blocks help retain not only the focus on the central forward points around the voxelized sparse feature (Gy), but also identify affine matrices within the dense image (G1). Unlike existing methods [21, 28], the global spatial resolution around the voxelized LiDAR point feature (Gv) does not guarantee a comprehensive sense of the dense image feature (G1). Therefore, we also introduce another reversible transformer block for query-key-value matching of the voxelized LiDAR feature [29], which will be considered as the backward transition of the matrix. Therefore, the forward transformation block in GAT is defined as follows:\nYf = F(G1)\t\t\t(1)\nwhere F\u2208 RH1\u00d7W1\u00d7C1 is the forward transformer block [14]. Here, Q, K, V \u2208 G\u2081 and we use layer normalization in between the query-key-value matching [13]. Similarly, the backward transformation for the voxelized LiDAR feature is represented as follows:\nY\u2081 = F\u00af\u00b9(Gv)\t\t\t(2)\nwhere F-1 \u2208 RHv\u00d7Wv\u00d7Cv is the backward transformer block [14]. Here, Q, K, V \u2208 Gy, and we use layer normalization similar to the forward path [29]. To accumulate the dimension matching, we"}, {"title": "Sparse Expert of Local Fusion (SELF)", "content": "To achieve local-level fusion, we introduce Sparse Expert fusion for voxelized LiDAR data. A region proposal network [7, 21, 39] generates initial bounding box suggestions (P = {P1, P2, . . .,Pn}) based on multi-level voxel features (Gy). The multi-level voxel features of RoI are defined by GL. We adopt the Mixture of Experts (MoE) [36, 37] model, known for capturing long dependencies in heterogeneous sequence datasets [6, 11, 50], for sparse RoI LiDAR features (GL) and dense image features (G1). This robust framework leverages specialized sub-models or \"experts,\" each optimized for different input subsets. We extend two separate gating networks (A gating network determines the weights for each expert), GNL (.) for LiDAR and GN\u2081(.) for image data, to determine the weights for each gating networks. For a given LiDAR input G\u2081 and an image input Gr, the outputs of both modalities are computed as:\nYL =\n\u03a3i=1\nN GNLI(GL) ELi(GL)\n(4)\nYI =\n\u03a3i=1\nN GNI(G1) Eli(G1)\n(5)\nwhere, YL and y\u0131 are each gating mechanism outputs for each modalities passing through by selecting by expert networks. N numbers of experts for each modalities. Therefore, the gating network(GN) is defined as follows,\nGN(x) = Softmax(TopK(Hw(x), k))\t\t(6)\nwhere, Hw(x) is the individual gating function that can be represented by the parametric weight w, Hw(x) = \u03a8(Softplus (x\u00b7 \u03b4)), where 8 is the noise [24].\nE(.) is an expert network function that will choose top K values. Therefore, selecting TopK experts from E(.) is represented as follows [5],\n{EL/I,k}k=1 =\n[\nEL/I,j; if j \u2208 arg max1:K{GNL/I,j}N\nj=1\n=\t1\notherwise\n(7)\nThese dual-gating mechanism (Equ. 7 and Equ. 6)allows the model to independently assess and integrate the specific characteristics of each modality. By doing so, the model can more effectively capture the complementary information provided by LiDAR and image data. The final fused output y is then derived by combining YL and y\u2081 through a subsequent fusion network F:\ny = F(YL, YI)\t\t\t(8)"}, {"title": "EXPERIMENTAL VALIDATION", "content": "3.1 Dataset Details\nWOD achieves excellent performance in 3D object detection benchmarks, thanks to its extensive dataset consisting of more than 200,000 frames, 1,150 sequences, and a combination of LiDAR, images [42]. The dataset comprises 798 training sequences, 202 validation sequences, and 150 testing sequences. The detection range is 75 meters, and the coverage area is 150 meters by 150 meters. We evaluate models using Average Precision (AP) and Average Precision weighted by Heading (APH) as described in [21, 42]. We include results for both LEVEL_1 (L1) and LEVEL_2 (L2) difficulty items, offering a thorough assessment and comparison of the models' performance.\n3.2 Evolution of WOD performance\nThe detailed performance of Q-ICVT, both single and ensemble variants, on the WOD test and validation sets is presented in Table 1 and Table 2. According to Table 1, Q-ICVT excels, surpassing other leading methods in both L1 and L2 difficulty levels. Compared to LoGoNet [21], Q-ICVT shows significant gains, especially in L2 difficulty with a 1.24 increase in APH value, achieved without ensemble techniques. Specifically, the non-ensemble version of Q-ICVT outperforms LoGoNet [21] by 1.22 AP/L1, 1.14 APH/L1, 1.15 AP/L2, and 0.80 APH/L2 for vehicles; 1.12 AP/L1, 1.38 APH/L1, 1.34 AP/L2, and 1.56 APH/L2 for pedestrians; and 1.80 AP/L1, 1.08 APH/L1, 0.98 AP/L2, and 1.35 APH/L2 for cyclists, leading to a total improvement of 1.24 mAPH/L2.The ensemble version of Q-ICVT also surpasses LoGoNet-Ens [21] with differences of 0.88 AP/L1, 1.11 APH/L1, 1.29 AP/L2, and 0.97 APH/L2 for vehicles; 0.03 AP/L1, 0.47 APH/L1, 1.65 AP/L2, and 2.54 APH/L2 for pedestrians; and 1.61 AP/L1, 1.85 APH/L1, 1.48 AP/L2, and 1.06 APH/L2 for cyclists, resulting in an overall improvement of 1.52 mAPH/L2.\nTable 2 presents an extensive comparison of model performance for 3D object detection on the WOD validation set. Remarkably, Q-ICVT shows substantial advancements across various difficulty levels. For L1 difficulty, it surpasses the validation results of Lo-GoNet [21] on WOD by 2.91 AP/L1, 2.24 APH/L1, 5.48 AP/L2, and 3.99 APH/L2 for vehicles; 2.70 AP/L1, 2.80 APH/L1, 3.71 AP/L2, and 2.36 APH/L2 for pedestrians; and 2.96 AP/L1, 2.77 APH/L1, 2.27 AP/L2, and 1.66 APH/L2 for cyclists, resulting in an overall improvement of 2.67 mAPH/L2. These improvements highlight Q-ICVT proficiency in accurately detecting all classes, emphasizing the effectiveness of multi-modal feature alignment in enhancing 3D object detection."}, {"title": "ABLATION STUDIES ON WOD", "content": "Influence of each component. Table 3 highlights the effects of individual components on L2 difficulty in the QICVT WOD test set. When only the Global Adiabatic Transformer (GAT) is used without the Sparse Expert of Local Fusion (SELF), performance decreases to 79.56 for vehicles, 80.12 for pedestrians, and 78.97 for cyclists. This drop is due to the limitations of the SELF component, which fails to achieve optimal fusion despite integrating voxel RoI features into the GAT module. The exclusion of GAT leads to an even greater decline in performance: 78.27 for vehicles, 79.51 for pedestrians, and 77.34 for cyclists. Therefore, even though local fusion centroids are closer to the image surface, voxel point centroids are not able to provide dense image feature information, diminishing the effectiveness of global cross-modal fusion. Combining both GAT and SELF components results in significant performance improvements, with scores of 82.69 for vehicles, 83.82 for pedestrians, and 81.12 for cyclists, underscoring the importance of both components for optimal performance."}, {"title": "CONCLUSION", "content": "We introduced QICVT, a 3D multi-modal object detection method based on transformers, consisting of two key components: GAT and SELF. These components were designed to capture both local and global dependencies, thereby enhancing the efficacy of 3D detection at both short and long distances. To determine the efficacy of QICVT, we conducted comprehensive experiments on the WOD benchmark datasets. QICVT demonstrated its efficacy in multi-modal object detection by achieving competitive performance when compared to state-of-the-art methods. In addition, we conducted comprehensive ablation experiments to compare the effect of each proposed component on QICVT's performance."}]}