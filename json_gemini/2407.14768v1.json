{"title": "Teach Harder, Learn Poorer: Rethinking Hard Sample Distillation for GNN-to-MLP Knowledge Distillation", "authors": ["Lirong Wu", "Yunfan Liu", "Haitao Lin", "Yufei Huang", "Stan Z. Li"], "abstract": "To bridge the gaps between powerful Graph Neural Networks (GNNs) and lightweight Multi-Layer Perceptron (MLPs), GNN-to-MLP Knowledge Distillation (KD) proposes to distill knowledge from a well-trained teacher GNN into a student MLP. In this paper, we revisit the knowledge samples (nodes) in teacher GNNs from the perspective of hardness, and identify that hard sample distillation may be a major performance bottleneck of existing graph KD algorithms. The GNN-to-MLP KD involves two different types of hardness, one student-free knowledge hardness describing the inherent complexity of GNN knowledge, and the other student-dependent distillation hardness describing the difficulty of teacher-to-student distillation. However, most of the existing work focuses on only one of these aspects or regards them as one thing. This paper proposes a simple yet effective Hardness-aware GNN-to-MLP Distillation (HGMD) framework, which decouples the two hardnesses and estimates them using a non-parametric approach. Finally, two hardness-aware distillation schemes (i.e., HGMD-weight and HGMD-mixup) are further proposed to distill hardness-aware knowledge from teacher GNNs into the corresponding nodes of student MLPs. As non-parametric distillation, HGMD does not involve any additional learnable parameters beyond the student MLPs, but it still outperforms most of the state-of-the-art competitors. HGMD-mixup improves over the vanilla MLPs by 12.95% and outperforms its teacher GNNs by 2.48% averaged over seven real-world datasets. Codes will be made public at https://github.com/LirongWu/HGMD.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, the emerging Graph Neural Networks (GNNs) [30, 31, 33, 42] have demonstrated their powerful capability in handling various graph-structured data. Benefiting from the powerful topology awareness enabled by message passing, GNNs have achieved great academic success. However, the neighborhood-fetching latency arising from data dependency in GNNs makes it still less popular for practical deployment, especially in computational-constraint applications. In contrast, Multi-Layer Perceptrons (MLPs) are free from data dependencies among neighboring nodes and infer much faster than GNNs, but at the cost of suboptimal performance. To bridge these two worlds, GLNN [39] proposes GNN-to-MLP Knowledge Distillation (KD), which extracts informative knowledge from a teacher GNN and then injects the konwledge into a student MLP.\nA long-standing intuitive idea about knowledge distillation is \"better teacher, better student\". In other words, distillation from a better teacher is expected to yield a better student, since a bet-ter teacher can usually capture more informative knowledge from which the student can benefit. However, some recent work has challenged this intuition, arguing that it does not hold true in all cases, i.e., distillation from a larger teacher, typically with more parameters and high accuracy, may be inferior to distillation from a smaller, less accurate teacher [14, 19, 21, 43]. To illustrate this, we show the rankings of three teacher GNNs, including Graph Convo-lutional Network (GCN) [11], Graph Attention Network (GAT) [24], and GraphSAGE [7], on seven datasets, as well as their correspond-ing distilled MLPs in Fig. 1(a), from which we observe that GCN is the best teacher on the Arxiv dataset, but its distilled student MLP performs the poorest. There have been many previous works [10, 15, 44] delving into this issue, but most of them attribute this counter-intuitive observation to the capacity mismatch between the teacher and student models. In other words, a student with fewer parameters may fail to \"understand\" the high-order semantic knowledge captured by a teacher with numerous parameters.\nHowever, we found that the above popular explanation from a model capacity perspective may hold true for KD in computer vision, but fails in the graph domain. For a teacher GCN and a stu-dent MLP with the same amount of parameters (i.e., the same layer depth and width), we plot the accuracy fluctuations of the teacher and distilled student with respect to the distillation temperature \\tau in Fig. 1(b). It can be seen that while the temperature t does not"}, {"title": "2 RELATED WORK", "content": "2.1 GNN-to-GNN Knowledge Distillation\nRecent years have witnessed the great success of GNNs in handling graph-structured data [32, 33]. However, most existing GNNs share the de facto design that relies on message passing to aggregate features from neighborhoods, which may be one major source of latency in GNN inference. To address this problem, several previous works on graph distillation try to distill knowledge from large teacher GNNs to smaller student GNNs, termed as GNN-to-GNN knowledge distillation (KD) [12, 16, 26, 28, 38], including RDD [40], TinyGNN [34], LSP [37], GraphAKD [8], GNN-SD [4], and FreeKD [5], etc. However, both teachers and students in the above works are GNNs, making these designs still suffer from the neighborhood-fetching latency arising from the data dependency in GNNs.\n2.2 GNN-to-MLP Knowledge Distillation\nTo bridge the gaps between powerful GNNs and lightweight MLPs, the other branch of graph KD is to directly distill from teacher GNNs to lightweight student MLPs, termed GNN-to-MLP KD. For example, GLNN [39] directly distills knowledge from teacher GNNs to vanilla MLPs by imposing KL-divergence between their logits. Instead, CPF [36] improves the performance of student MLPs by incorporating label propagation in MLPs, which may further burden the inference latency. Besides, FF-G2M [27] propose to factorize GNN knowledge into low- and high-frequency components in the spectral domain and propose a novel framework to distill both low-and high-frequency knowledge from teacher GNNs into student MLPs. Moreover, RKD [29] takes into account the reliability of GNN knowledge and adopts a parameterized distribution fitting to filter out unreliable GNN knowledge. For more approaches on GNN-to-MLP KD, we refer the interested reader to a recent survey [22]. Despite the great progress, most of the existing methods have focused on how to make better use of those simple samples, while little effort is made on those hard samples. However, we found in this paper that hard sample distillation may be a main bottleneck that limits the performance of existing GNN-to-MLP KD algorithms."}, {"title": "3 METHODOLOGY", "content": "3.1 Preliminary\n3.1.1 Notations. Given a graph \\(G = (V, \\mathcal{E})\\), where the node set and edge set are \\(V = \\{v_1, v_2, \\cdots, v_N\\}\\) and \\(\\mathcal{E} \\subseteq VXV\\), respectively."}, {"title": "3.1.2 Knowledge Hardness.", "content": "Inspired by the experiment in Fig. 1(c), where GNN knowledge samples with higher entropy are harder to be correctly distilled into the student MLPs, we use the information entropy \\(H(z_i)\\) of node \\(v_i\\) as a measure of its knowledge hardness,\n\\[H(z_i) = - \\sum_j \\sigma(z_{i,j}/\\tau)\\log(\\sigma (z_{i,j}/\\tau)).\\]\nWe default to using Eq. (2) for measuring the knowledge hardness in this paper and delay the definition of distillation hardness until Sec. 3.3. For more experimental results of using other more complex knowledge hardness metrics, please refer to Appendix D."}, {"title": "3.2 Bottleneck: Hard Sample Distillation", "content": "Recent years have witnessed the great success of knowledge distil-lation and a surge of related distillation techniques. As the research goes deeper, the rationality of \"better teacher, better student\" has been increasingly challenged. A lot of earlier works [10, 20] have found that as the performance of the teacher model improves, the accuracy of the student model may unexpectedly get worse. Most of the existing works attribute such counter-intuitive observation to the capacity mismatch between the teacher and student models. In other words, a smaller student may have difficulty \"understanding\" the high-order semantic knowledge captured by a large teacher. Although this problem has been well studied in computer vision, little work has been devoted to whether it exists in graph knowledge dis-tillation, what it arises from, and how to deal with it. In this paper, we get the same observation during GNN-to-MLP distillation that better teachers do not necessarily lead to better students in Fig. 1(a), but we find that this has little to do with the popular idea of capacity mismatch. This is because, unlike common visual backbones with very deep layers in computer vision, GNNs tend to suffer from the undesired over-smoothing problem [3, 35] when stacking deeply. Therefore, most existing GNNs are shallow networks, making the effects of capacity mismatch negligible during GNN-to-MLP KD.\nTo explore the criteria for better GNN knowledge samples (nodes), we conduct an exploratory experiment to evaluate the roles played by GNN knowledge samples of different hardnesses during knowl-edge distillation. For example, we report in Fig. 2 the distillation"}, {"title": "3.3 Hardness-aware GNN-to-MLP Distillation", "content": "One previous work [41] defined knowledge hardness as the cross entropy on labeled data and proposed to weigh the distillation losses among samples in a hardness-based manner. To extend it to the transductive setting for graphs in this paper, we adopt the information entropy defined in Eq. (2) instead of the cross entropy as the knowledge hardness, and derive a variant of it as follows,\n\\[\\mathcal{L}_{KD} = \\sum_{i\\in V} (1-e^{-H(h_i)/H(z_i)}) \\cdot D_{KL}(Z_i, h_i),\\]\nwhere \\(Z_i = \\sigma (z_i/\\tau)\\) and \\(h_i = \\sigma (h_i/\\tau)\\). As far as GNN knowledge hardness is concerned, Eq. (3) reduces the weights of those hard samples with large knowledge hardness, i.e., higher \\(H(z_i)\\), while leaving those simple samples to dominate the optimization. However, Sec. 3.2 shows that not only should we not ignore those hard samples, but we should pay more attention to them by providing more supervision. To this end, we propose a novel GNN-to-MLP KD framework, namely HGMD, which extracts a hardness-aware sub-graph (the harder, the larger) for each sample separately and then distills the subgraph-level knowledge into the corresponding nodes of student MLPs through two distillation schemes. A high-level overview of the HGMD framework is shown in Fig. 3."}, {"title": "3.3.1 Hardness-aware Subgraph Extraction.", "content": "We estimate the distillation hardness based on the knowledge hardness of both the teacher and the student, and then model the probability that the neighbors of a target node are included in the corresponding sub-graph based on the distillation hardness. To enable hardness-aware subgraph extraction, four heuristic factors that influence the dis-tillation hardness and subgraph size should be considered: (1) A harder sample with higher knowledge hardness \\(H(z_i)\\) in teacher GNNs should be assigned a larger subgraph for more supervision. (2) A sample with high uncertainty \\(H(h_i)\\) in student MLPs requires a larger subgraph for more supervision. (3) A node \\(v_j \\in N_i\\) with lower knowledge hardness \\(H(z_j)\\) has a higher probability of being included in the subgraph. (4) Nodes in the subgraph are expected to share similar label distributions with the target node \\(v_i\\). Inspired by these heuristic factors, we model the probability \\(p_{j\\rightarrow i}\\) that a neighboring node \\(v_j \\in N_i\\) of the target node \\(v_i\\) is included in the subgraph based on the distillation hardness \\(r_{j\\rightarrow i}\\), defined as follow\n\\[p_{j \\rightarrow i} = 1 - r_{ji}, \\text{where}\\]\n\\[r_{ji} = exp\\left( - \\eta \\cdot \\frac{\\mathcal{D}(z_i, z_j) \\cdot \\sqrt{H(h_i) \\cdot H(z_i)}}{H(z_j)} \\right),\\)\\]\nwhere \\(\\mathcal{D}(z_i, z_j)\\) denotes the cosine similarity between \\(z_i\\) and \\(z_j\\), and we specify that \\(p_{i\\rightarrow i} = 1\\). In addition, \\(\\eta\\) is a hyperparameter used to control the overall hardness sensitivity. In this paper, we adopt an exponentially decaying strategy to set the hyperparameter \\(\\eta\\). Extensive experiments are provided in Sec. 4.3 to demonstrate the effectiveness of such a non-parametric hardness estimation."}, {"title": "3.3.2 HGMD-weight.", "content": "Based on the sampling probabilities mod-eled in Eq. (4), we can easily sample a hardness-aware subgraph \\(g_i\\) with node set \\(V_i = \\{v_j \\sim Bernoulli(p_{j\\rightarrow i}) |j \\in (N_i \\cup i)\\} \\) for each target node \\(v_i\\) by Bernoulli sampling. Next, a key issue being left is how to distill the subgraph-level knowledge from teacher GNNs into the corresponding nodes of student MLPs. A straightfor-ward idea is to follow [27] to perform many-to-one (multi-teacher) knowledge distillation by optimizing the objective, as follows\n\\[\\mathcal{L}_{KD}^{weight} = \\frac{1}{|V|} \\sum_{i \\in V} \\frac{1}{|V_i|} \\sum_{j \\in V_i} p_{j\\rightarrow i} \\cdot D_{KL}(Z_i, h_i).\\]\nCompared to the loss weighting of Eq. (3), the strengths of the HGMD-weight in Eq. (5) are four-fold: (1) it extends knowledge"}, {"title": "3.3.3 HGMD-mixup.", "content": "Recently, mixup [1], as an important data augmentation technique, has achieved great success in various fields. Combining mixup with our HGMD framework enables the generation of more GNN knowledge variants as additional super-vision for those hard samples, which may help to improve the generalizability of the distilled student model. Inspired by this, we propose another hardness-aware mixup scheme to distill the subgraph-level knowledge from GNNs into MLPs. Instead of mix-ing the samples randomly, we mix them by emphasizing the sample with a high probability \\(p_{j\\rightarrow i}\\). Formally, for each target sample \\(v_i\\), a synthetic sample \\(u_{i,j}\\) (\\(v_j \\in V_i)\\) will be generated by\n\\[u_{i,j} = \\lambda \\cdot p_{j\\rightarrow i} \\cdot z_j + (1 - \\lambda \\cdot p_{j\\rightarrow i}) \\cdot z_i, \\lambda \\sim Beta(\\alpha, \\alpha),\\]\nwhere \\(Beta(\\alpha, \\alpha)\\) is a beta distribution parameterized by \\(\\alpha\\). For a node \\(v_j \\in V\\) in the subgraph with lower hardness \\(H(z_j)\\) and higher similarity \\(\\mathcal{D}(H(z_i), H(z_j))\\), the synthetic sample \\(u_{i,j}\\) will be closer to \\(z_j\\). Finally, we can distill the knowledge of synthetic samples \\(\\{u_{i,j}\\}_{v_j \\in V_i}\\) in the subgraph \\(g_i\\) into the corresponding node \\(v_i\\) of student MLPs by optimizing the objective, as follows\n\\[\\mathcal{L}_{KD}^{mixup} = \\frac{1}{|V|} \\sum_{i \\in V} \\frac{1}{|V_i|} \\sum_{j \\in V_i}  D_{KL}( \\sigma (u_{i,j}/\\tau), h_i).\\]\nCompared to the weighting-based scheme (HGMD-weight) of Eq. (5), the mixup-based scheme (HGMD-mixup) generates more variants of GNN knowledge through miuxp augmentation, which is more in line with our original intention of providing more additional supervision for knowledge distillation on hard samples."}, {"title": "3.4 Training Strategy", "content": "To achieve GNN-to-MLP knowledge distillation, we first pre-train the teacher GNNs with the classification loss \\(\\mathcal{L}_{label}\\), as follows\n\\[\\mathcal{L}_{label} = \\frac{1}{|V_L|} \\sum_{i \\in V_L} CE(y_i, \\sigma(z_i)),\\]\nwhere \\(CE()\\) denotes the cross-entropy loss. We further distill knowl-edge from teacher GNNs into student MLPs with the objective,\n\\[\\mathcal{L}_{total} = \\sum_{i \\in V_L}  \\frac{\\beta}{|V_L|} CE(y_i, \\sigma(h_i)) + (1 - \\beta) \\mathcal{L}_{KD},\\]\nwhere \\(\\beta\\) is the hyperparameter to trade-off the classification and dis-tillation losses. The pseudo-code of HGMD (taking HGMD-mixup as an example) has been summarized in Algorithm. 1."}, {"title": "3.5 Parameters and Computational Complexity", "content": "Compared to previous GNN-to-MLP KD methods, such as RKD [29], HGMD decouples and estimates knowledge and distillation hardness in a non-parametric fashion, which does not introduce any additional learnable parameters in the process of subgraph extraction and subgraph distillation. In terms of the computational complexity, the time complexity of HGMD mainly comes from two parts: (1) GNN training \\(O(|V|dF + |E|F)\\) and (2) Knowledge distillation \\(O(8|F)\\), where d and F are the dimensions of input and hidden spaces. The total time complexity \\(O(|V|dF+|E|F)\\) is linear w.r.t the number of nodes \\(|V|\\) and edges \\(|\\mathcal{E}|\\). This indicates that the time complexity of KD in HGMD is basically on par with GNN training and does not suffer from a high computational burden."}, {"title": "4 EXPERIMENTS", "content": "In this paper, we evaluate HGMD on eight real-world datasets, including Cora [17], Citeseer [6], Pubmed [13], Coauthor-CS, Coauthor-Physics, Amazon-Photo [18], ogbn-arxiv [9], and ogbn-products [9]. A statistical overview of these datasets is available in Appendix A. Besides, we defer the implementation details and hyperparameter settings for each dataset to Appendix B. In addition, we consider three common GNN architectures as GNN teachers, including GCN"}, {"title": "4.1 Comparative Results", "content": "To evaluate the effectiveness of the HGMD framework, we com-pare its two instantiations, HGMD-weight and HGMD-mixup, with GLNN of Eq. (1) and Loss-Weighting of Eq. (3), respectively. The experiments are conducted on seven datasets with three differ-ent GNN architectures as teacher GNNs, where imporv. denotes the performance improvements with respect to GLNN. From the results reported in Table. 1, we can make three observations: (1) Both HGMD-weight and HGMD-mixup perform much better than vanilla MLP, GLNN, and Loss-Weighting on all seven datasets, espe-cially on the large-scale ogbn-arxiv dataset. (2) Both HGMD-weight and HGMD-mixup are applicable to various types of teacher GNN architectures. For example, HGMD-mixup outperforms GLNN by 2.22% (GCN), 2.11% (SAGE), and 2.19% (GAT) averaged over seven datasets, respectively. (3) Overall, HGMD-mixup performs slightly better than HGMD-weight across various datasets, owing to more knowledge variants augmented by the hardness-aware mixup.\nFurthermore, we compare HGMD-weight and HGDM-mixup with several state-of-the-art graph distillation methods, including both GNN-to-GNN and GNN-to-MLP KD. The experimental re-sults reported in Table. 2 show that (1) Despite being completely non-parametric methods, HGMD-weight and HGMD-mixup both perform much better than existing GNN-to-MLP baselines on 5 out of 8 datasets. (2) HGMD-weight and HGMD-mixup outperform those GNN-to-GNN baselines on four relatively small datasets (i.e., Cora, Citeseer, Pubmed, and Photo). Besides, their performance"}, {"title": "4.2 Ablation Study", "content": "To evaluate how hardness-aware subgraph extraction (SubGraph) and two subgraph distillation strategies (weight and mixup) influ-ence performance, we compare vanilla GCNs and GLNN with the following five schemes: (A) Subgraph-only: extract hardness-aware subgraphs and then distill their knowledge into the student MLP with equal loss weights; (B) Weight-only: take the full neighbor-hoods as subgraphs and then distill by hardness-aware weighting as in Eq. (5); (C) Mixup-only: take the full neighborhoods as sub-graphs and then distill by hardness-aware mixup as in Eq. (7); (D) HGMD-weight; and (E) HGMD-mixup. We can observe from the experimental results reported in Table. 3 that (1) SubGraph plays a very important role in improving performance, which illustrates the benefits of performing knowledge distillation at the subgraph level compared to the node level, as it provides more supervision for those hard samples in a hardness-aware manner. (2) Both hardness-aware weighting and mixup help improve performance, especially the latter. (3) Combining the two different designs (subgraph ex-traction and subgraph distillation) together can further improve performance on top of each on all six graph datasets."}, {"title": "4.3 Deep Analysis on Hardness Awareness", "content": "4.3.1 Case Study of Hardness-aware Subgraphs. To intuitively show what \"hardness awareness\" means, we select three GNN knowledge samples with different hardness levels from four datasets,"}, {"title": "4.3.4 Asymmetric Property of Subgraph Extraction.", "content": "We sta-tistically calculate the ratios of two connected nodes among all edges that are and are not sampled into each other's subgraphs simultaneously, called symmetrized and asymmetrized sampling. The histogram in Fig. 5(d) shows that subgraph extraction is mostly asymmetric, especially for large-scale datasets. This is because sub-graph extraction is performed in a hardness-aware manner, where low-hardness neighboring nodes of a high-hardness target node have a higher sampling probability, but not vice versa. We believe that such asymmetric property of subgraph extraction is a key aspect of the effectiveness of HGMD framework, as it essentially transforms an undirected graph into a directed graph for processing."}, {"title": "5 CONCLUSION", "content": "In this paper, we explore thoroughly the knowledge hardness and distillation hardness for GNN-to-MLP knowledge distillation. We identify that hard sample distillation may be a major performance bottleneck of existing distillation algorithms. To address this prob-lem, we propose a novel Hardness-aware GNN-to-MLP Distillation (HGMD) framework, which distills knowledge from teacher GNNs at the subgraph level (rather than the node level) in a hardness-aware manner to provide more supervision for those hard samples. Extensive experiments have been provided to demonstrate the su-periority of HGMD across various datasets and GNN architectures. Limitations still exist, for example, designing better hardness met-rics or introducing additional learnable parameters for knowledge distillation may be promising directions for future work."}]}