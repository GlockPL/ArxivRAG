{"title": "Teach Harder, Learn Poorer: Rethinking Hard Sample Distillation for GNN-to-MLP Knowledge Distillation", "authors": ["Lirong Wu", "Yunfan Liu", "Haitao Lin", "Yufei Huang", "Stan Z. Li"], "abstract": "To bridge the gaps between powerful Graph Neural Networks (GNNs) and lightweight Multi-Layer Perceptron (MLPs), GNN-to-MLP Knowledge Distillation (KD) proposes to distill knowledge from a well-trained teacher GNN into a student MLP. In this paper, we revisit the knowledge samples (nodes) in teacher GNNs from the perspective of hardness, and identify that hard sample distillation may be a major performance bottleneck of existing graph KD algorithms. The GNN-to-MLP KD involves two different types of hardness, one student-free knowledge hardness describing the inherent complexity of GNN knowledge, and the other student-dependent distillation hardness describing the difficulty of teacher-to-student distillation. However, most of the existing work focuses on only one of these aspects or regards them as one thing. This paper proposes a simple yet effective Hardness-aware GNN-to-MLP Distillation (HGMD) framework, which decouples the two hardnesses and estimates them using a non-parametric approach. Finally, two hardness-aware distillation schemes (i.e., HGMD-weight and HGMD-mixup) are further proposed to distill hardness-aware knowledge from teacher GNNs into the corresponding nodes of student MLPs. As non-parametric distillation, HGMD does not involve any additional learnable parameters beyond the student MLPs, but it still outperforms most of the state-of-the-art competitors. HGMD-mixup improves over the vanilla MLPs by 12.95% and outperforms its teacher GNNs by 2.48% averaged over seven real-world datasets. Codes will be made public at https://github.com/LirongWu/HGMD.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, the emerging Graph Neural Networks (GNNs) [30, 31, 33, 42] have demonstrated their powerful capability in handling various graph-structured data. Benefiting from the powerful topology awareness enabled by message passing, GNNs have achieved great academic success. However, the neighborhood-fetching latency arising from data dependency in GNNs makes it still less popular for practical deployment, especially in computational-constraint applications. In contrast, Multi-Layer Perceptrons (MLPs) are free from data dependencies among neighboring nodes and infer much faster than GNNs, but at the cost of suboptimal performance. To bridge these two worlds, GLNN [39] proposes GNN-to-MLP Knowledge Distillation (KD), which extracts informative knowledge from a teacher GNN and then injects the konwledge into a student MLP.\nA long-standing intuitive idea about knowledge distillation is \"better teacher, better student\". In other words, distillation from a better teacher is expected to yield a better student, since a better teacher can usually capture more informative knowledge from which the student can benefit. However, some recent work has challenged this intuition, arguing that it does not hold true in all cases, i.e., distillation from a larger teacher, typically with more parameters and high accuracy, may be inferior to distillation from a smaller, less accurate teacher [14, 19, 21, 43]. To illustrate this, we show the rankings of three teacher GNNs, including Graph Convolutional Network (GCN) [11], Graph Attention Network (GAT) [24], and GraphSAGE [7], on seven datasets, as well as their corresponding distilled MLPs in Fig. 1(a), from which we observe that GCN is the best teacher on the Arxiv dataset, but its distilled student MLP performs the poorest. There have been many previous works [10, 15, 44] delving into this issue, but most of them attribute this counter-intuitive observation to the capacity mismatch between the teacher and student models. In other words, a student with fewer parameters may fail to \"understand\" the high-order semantic knowledge captured by a teacher with numerous parameters.\nHowever, we found that the above popular explanation from a model capacity perspective may hold true for KD in computer vision, but fails in the graph domain. For a teacher GCN and a student MLP with the same amount of parameters (i.e., the same layer depth and width), we plot the accuracy fluctuations of the teacher and distilled student with respect to the distillation temperature \u03c4 in Fig. 1(b). It can be seen that while the temperature t does not affect the teacher's accuracy, it influences the knowledge hardness of GNNs, which in turn leads to different student's accuracy.\nPresent Work. In this paper, we rethink what exactly are the criteria for \"better\" knowledge samples (nodes) in teacher GNNs from the perspective of hardness rather than correctness, which has been rarely studied in previous works. The motivational experiment in Fig. 1(c) indicates that most GNN knowledge of samples misclassified by student MLPs is distributed in the high-entropy zones, which suggests that GNN knowledge samples with higher uncertainty are usually harder to be correctly distilled. Furthermore, we explore the roles played by GNN knowledge samples of different hardness during distillation and identify that hard sample distillation may be a major performance bottleneck of existing KD algorithms. As a result, to provide more additional supervision for the distillation of those hard samples, we propose a simple yet effective Hardness-aware GNN-to-MLP Distillation (HGMD) framework. The proposed framework first models both knowledge and distillation hardness in a non-parametric fashion, then extracts a hardness-aware subgraph (the harder, the larger) for each node separately, and finally applies two distillation schemes (HGMD-weight and HGMD-mixup) to distill subgraph-level knowledge from teacher GNNs into corresponding nodes of the student MLPs.\nThe main contributions of this paper are as follows: (1) We are the first to identify that hard sample distillation is the main bottleneck that limits the performance of existing GNN-to-MLP KD algorithms, and more importantly, we have described in detail what it represents, what impact it has, and how to deal with it. (2) We decouple two different hardnesses, i.e., knowledge hardness and distillation hardness, and propose a non-parametric approach to estimate them. (3) We propose two distillation schemes based on the two decoupled hardnesses for hard sample distillation. Despite not involving any additional parameters, they are still comparable to or even better than most of the state-of-the-art competitors."}, {"title": "2 RELATED WORK", "content": "2.1 GNN-to-GNN Knowledge Distillation\nRecent years have witnessed the great success of GNNs in handling graph-structured data [32, 33]. However, most existing GNNs share the de facto design that relies on message passing to aggregate features from neighborhoods, which may be one major source of latency in GNN inference. To address this problem, several previous works on graph distillation try to distill knowledge from large teacher GNNs to smaller student GNNs, termed as GNN-to-GNN knowledge distillation (KD) [12, 16, 26, 28, 38], including RDD [40], TinyGNN [34], LSP [37], GraphAKD [8], GNN-SD [4], and FreeKD [5], etc. However, both teachers and students in the above works are GNNs, making these designs still suffer from the neighborhood-fetching latency arising from the data dependency in GNNs.\n2.2 GNN-to-MLP Knowledge Distillation\nTo bridge the gaps between powerful GNNs and lightweight MLPs, the other branch of graph KD is to directly distill from teacher GNNs to lightweight student MLPs, termed GNN-to-MLP KD. For example, GLNN [39] directly distills knowledge from teacher GNNs to vanilla MLPs by imposing KL-divergence between their logits. Instead, CPF [36] improves the performance of student MLPs by incorporating label propagation in MLPs, which may further burden the inference latency. Besides, FF-G2M [27] propose to factorize GNN knowledge into low- and high-frequency components in the spectral domain and propose a novel framework to distill both low- and high-frequency knowledge from teacher GNNs into student MLPs. Moreover, RKD [29] takes into account the reliability of GNN knowledge and adopts a parameterized distribution fitting to filter out unreliable GNN knowledge. For more approaches on GNN-to-MLP KD, we refer the interested reader to a recent survey [22]. Despite the great progress, most of the existing methods have focused on how to make better use of those simple samples, while little effort is made on those hard samples. However, we found in this paper that hard sample distillation may be a main bottleneck that limits the performance of existing GNN-to-MLP KD algorithms."}, {"title": "3 METHODOLOGY", "content": "3.1 Preliminary\n3.1.1 Notations. Given a graph G = (V, 8), where the node set and edge set are V = {v1, v2, \uff65\uff65, vN } and 8 \u2286 VXV, respectively. In addition, X \u2208 RN\u00d7d and A \u2208 [0, 1]N\u00d7N denotes the feature matrix and adjacency matrix, where each node vi \u2208 V is associated with a d-dimensional features vector xi \u2208 Rd and Ai,j = 1 iff (vi, vj) \u2208 E. Consider node classification in a transductive setting in which only a subset of node V\u2081 \u2208 V with corresponding labels YL are known, we denote the labeled set as DL = (VL, YL) and unlabeled set as Du = (VU, YU), where Vu = V\\VL. The objective of GNN-to-MLP knowledge distillation is to first train a teacher GNN Z = f (A, X) on the labeled data D\u2081, and then distill knowledge from the teacher GNN into a student MLP H = f(X) by imposing KL-divergence $D_{KL}(\u00b7,\u00b7)$ on the set V, as follows\n$L_{KD} = \\frac{1}{|V|} \\sum_{i \\in V} D_{KL}(\\sigma (z_i/t), \\sigma (h_i/t)),$ (1)\nwhere \u03c3(\u00b7) = softmax() is the activation function, and t is the distillation temperature. Beisdes, zi and h\u012f are the node embeddings of node vi in Z and H, respectively. Once distillation is done, the distilled MLP can be used to infer yi \u2208 Yu for unlabeled data.\n3.1.2 Knowledge Hardness. Inspired by the experiment in Fig. 1(c), where GNN knowledge samples with higher entropy are harder to be correctly distilled into the student MLPs, we use the information entropy H(zi) of node vi as a measure of its knowledge hardness,\n$H(z_i) = -\\sum_j \\sigma(z_{i,j}/t) log(\\sigma (z_{i,j}/t)).$ (2)\nWe default to using Eq. (2) for measuring the knowledge hardness in this paper and delay the definition of distillation hardness until Sec. 3.3. For more experimental results of using other more complex knowledge hardness metrics, please refer to Appendix D.\n3.2 Bottleneck: Hard Sample Distillation\nRecent years have witnessed the great success of knowledge distillation and a surge of related distillation techniques. As the research goes deeper, the rationality of \"better teacher, better student\" has been increasingly challenged. A lot of earlier works [10, 20] have found that as the performance of the teacher model improves, the accuracy of the student model may unexpectedly get worse. Most of the existing works attribute such counter-intuitive observation to the capacity mismatch between the teacher and student models. In other words, a smaller student may have difficulty \"understanding\" the high-order semantic knowledge captured by a large teacher. Although this problem has been well studied in computer vision, little work has been devoted to whether it exists in graph knowledge distillation, what it arises from, and how to deal with it. In this paper, we get the same observation during GNN-to-MLP distillation that better teachers do not necessarily lead to better students in Fig. 1(a), but we find that this has little to do with the popular idea of capacity mismatch. This is because, unlike common visual backbones with very deep layers in computer vision, GNNs tend to suffer from the undesired over-smoothing problem [3, 35] when stacking deeply. Therefore, most existing GNNs are shallow networks, making the effects of capacity mismatch negligible during GNN-to-MLP KD.\nTo explore the criteria for better GNN knowledge samples (nodes), we conduct an exploratory experiment to evaluate the roles played by GNN knowledge samples of different hardnesses during knowledge distillation. For example, we report in Fig. 2 the distillation accuracy of several representative methods for simple samples (bottom 50% hardness) and hard samples (top 50% hardness), as well as their overall accuracy. As can be seen from Fig. 2, those simple samples can be handled well by all methods, and the main difference in the performance of different distillation methods lies in their capability to handle those hard samples. In other words, hard sample distillation may be a major performance bottleneck of existing distillation algorithms. For example, FF-G2M improves the overall accuracy by 1.86% compared to GLNN, where hard samples contribute 3.27%, but simple samples contribute only 0.45%. Note that this phenomenon also exists in human education, where simple knowledge can be easily grasped by all students and therefore teachers are encouraged to spend more efforts in teaching hard knowledge. Based on the above observations, we believe that not only should we not ignore those hard samples, but we should provide them with more supervision in a hardness-based manner.\n3.3 Hardness-aware GNN-to-MLP Distillation\nOne previous work [41] defined knowledge hardness as the cross entropy on labeled data and proposed to weigh the distillation losses among samples in a hardness-based manner. To extend it to the transductive setting for graphs in this paper, we adopt the information entropy defined in Eq. (2) instead of the cross entropy as the knowledge hardness, and derive a variant of it as follows,\n$L_{KD} = \\sum_{i \\in V} (1-e^{-H(h_i)/H(z_i)}) \\cdot D_{KL}(Z_i, h_i),$ (3)\nwhere Zi = \u03c3 (zi/t) and h\u2081 = \u03c3 (hi/t). As far as GNN knowledge hardness is concerned, Eq. (3) reduces the weights of those hard samples with large knowledge hardness, i.e., higher H(zi), while leaving those simple samples to dominate the optimization. However, Sec. 3.2 shows that not only should we not ignore those hard samples, but we should pay more attention to them by providing more supervision. To this end, we propose a novel GNN-to-MLP KD framework, namely HGMD, which extracts a hardness-aware subgraph (the harder, the larger) for each sample separately and then distills the subgraph-level knowledge into the corresponding nodes of student MLPs through two distillation schemes. A high-level overview of the HGMD framework is shown in Fig. 3.\n3.3.1 Hardness-aware Subgraph Extraction. We estimate the distillation hardness based on the knowledge hardness of both the teacher and the student, and then model the probability that the neighbors of a target node are included in the corresponding subgraph based on the distillation hardness. To enable hardness-aware subgraph extraction, four heuristic factors that influence the distillation hardness and subgraph size should be considered: (1) A harder sample with higher knowledge hardness H(zi) in teacher GNNs should be assigned a larger subgraph for more supervision. (2) A sample with high uncertainty H(hi) in student MLPs requires a larger subgraph for more supervision. (3) A node vj \u2208 Ni with lower knowledge hardness H(zj) has a higher probability of being included in the subgraph. (4) Nodes in the subgraph are expected to share similar label distributions with the target node vi. Inspired by these heuristic factors, we model the probability pj\u2192i that a neighboring node vj \u2208 Ni of the target node vi is included in the subgraph based on the distillation hardness rj\u2192i, defined as follow\n$p_{j \\rightarrow i} = 1 - r_{j \\rightarrow i}, where$\n$r_{j \\rightarrow i} = exp( - \\eta \\cdot D(z_i, z_j) \\cdot \\frac{\\sqrt{H(h_i) \\cdot H(z_i)}}{H(z_j)}),$ (4)\nwhere D(zi, zj) denotes the cosine similarity between z\u012f and zj, and we specify that pi\u2192i = 1. In addition, \u03b7 is a hyperparameter used to control the overall hardness sensitivity. In this paper, we adopt an exponentially decaying strategy to set the hyperparameter \u03b7. Extensive experiments are provided in Sec. 4.3 to demonstrate the effectiveness of such a non-parametric hardness estimation.\n3.3.2 HGMD-weight. Based on the sampling probabilities modeled in Eq. (4), we can easily sample a hardness-aware subgraph gi with node set V = {vj ~ Bernoulli(pj\u2192i) |j \u2208 (N\u012f \u222ai)} for each target node vi by Bernoulli sampling. Next, a key issue being left is how to distill the subgraph-level knowledge from teacher GNNs into the corresponding nodes of student MLPs. A straightforward idea is to follow [27] to perform many-to-one (multi-teacher) knowledge distillation by optimizing the objective, as follows\n$L_{KD}^{weight} = \\frac{1}{|V|} \\sum_{i \\in V} \\frac{1}{|V_i|} \\sum_{j \\in V_i} p_{j \\rightarrow i} \\cdot D_{KL}(Z_i, h_i).$ (5)\nCompared to the loss weighting of Eq. (3), the strengths of the HGMD-weight in Eq. (5) are four-fold: (1) it extends knowledge distillation from node-to-node single-teacher KD to subgraph-to-node multi-teacher KD, which introduces additional supervision; (2) it provides more supervision (i.e., larger subgraphs) for hard samples in a hardness-aware manner, rather than neglecting them by reducing their loss weights; (3) it inherits the benefit of loss weighting by assigning a large weight pj\u2192i to a sample vj with low hardness H(zj) in the subgraph; (4) it takes into account not only the knowledge hardness of the target node but also the nodes in the subgraph and their similarities to the target, enjoying more contextual information. While the modification from Eq. (3) to Eq. (5) does not introduce any additional learnable parameters, it achieves a huge improvement as shown in the subsequent experiments.\n3.3.3 HGMD-mixup. Recently, mixup [1], as an important data augmentation technique, has achieved great success in various fields. Combining mixup with our HGMD framework enables the generation of more GNN knowledge variants as additional supervision for those hard samples, which may help to improve the generalizability of the distilled student model. Inspired by this, we propose another hardness-aware mixup scheme to distill the subgraph-level knowledge from GNNs into MLPs. Instead of mixing the samples randomly, we mix them by emphasizing the sample with a high probability pj\u2192i. Formally, for each target sample vi, a synthetic sample ui,j (vj \u2208 Vi) will be generated by\n$U_{i,j} = \\lambda \\cdot p_{j \\rightarrow i} \\cdot z_j + (1 - \\lambda \\cdot p_{j \\rightarrow i}) \\cdot z_i, \\lambda \\sim Beta(\\alpha, \\alpha),$ (6)\nwhere Beta(a, a) is a beta distribution parameterized by a. For a node vj e V in the subgraph with lower hardness H(zj) and higher similarity D(H(zi), H(zj)), the synthetic sample ui,j will be closer to zj. Finally, we can distill the knowledge of synthetic samples {ui,j}vjev in the subgraph gi into the corresponding node vi of student MLPs by optimizing the objective, as follows\n$L_{KD}^{mixup} = \\frac{1}{|V|} \\sum_{i \\in V} \\frac{1}{|V_i|} \\sum_{j \\in V_i} D_{KL}(\\sigma (u_{i,j}/\\tau), h_i).$ (7)\nCompared to the weighting-based scheme (HGMD-weight) of Eq. (5), the mixup-based scheme (HGMD-mixup) generates more variants of GNN knowledge through miuxp augmentation, which is more in line with our original intention of providing more additional supervision for knowledge distillation on hard samples."}, {"title": "3.4 Training Strategy", "content": "To achieve GNN-to-MLP knowledge distillation, we first pre-train the teacher GNNs with the classification loss Llabel, as follows\n$L_{label} = \\frac{1}{|V_L|} \\sum_{i \\in V_L} CE (y_i, \\sigma (z_i)),$ (8)\nwhere CE() denotes the cross-entropy loss. We further distill knowledge from teacher GNNs into student MLPs with the objective,\n$L_{total} = \\sum_{i \\in V_L} \\beta CE (y_i, \\sigma (h_i)) + (1 - \\beta) L_{KD},$ (9)\nwhere \u1e9e is the hyperparameter to trade-off the classification and distillation losses. The pseudo-code of HGMD (taking HGMD-mixup as an example) has been summarized in Algorithm. 1."}, {"title": "3.5 Parameters and Computational Complexity", "content": "Compared to previous GNN-to-MLP KD methods, such as RKD [29], HGMD decouples and estimates knowledge and distillation hardness in a non-parametric fashion, which does not introduce any additional learnable parameters in the process of subgraph extraction and subgraph distillation. In terms of the computational complexity, the time complexity of HGMD mainly comes from two parts: (1) GNN training O(|V|dF + |E|F) and (2) Knowledge distillation O(|8|F), where d and F are the dimensions of input and hidden spaces. The total time complexity O(|V|dF+|E|F) is linear w.r.t the number of nodes |V| and edges |8|. This indicates that the time complexity of KD in HGMD is basically on par with GNN training and does not suffer from a high computational burden."}, {"title": "4 EXPERIMENTS", "content": "In this paper, we evaluate HGMD on eight real-world datasets, including Cora [17], Citeseer [6], Pubmed [13], Coauthor-CS, Coauthor-Physics, Amazon-Photo [18], ogbn-arxiv [9], and ogbn-products [9]. A statistical overview of these datasets is available in Appendix A. Besides, we defer the implementation details and hyperparameter settings for each dataset to Appendix B. In addition, we consider three common GNN architectures as GNN teachers, including GCN [11], GraphSAGE [7], and GAT [24], and comprehensively evaluate two distillation schemes, HGMD-weight and HGMD-mixup, respectively. Furthermore, we also compare HGMD with two types of state-of-the-art graph distillation methods, including (1) GNN-to-GNN KD: [37], TinyGNN [34], GraphAKD [8], RDD [40], FreeKD [5], and GNN-SD [4]; and (2) GNN-to-MLP KD: CPF [36], RKD-MLP [2], GLNN [39], FF-G2M [27], NOSMOG [23], and KRD [29].\n4.1 Comparative Results\nTo evaluate the effectiveness of the HGMD framework, we compare its two instantiations, HGMD-weight and HGMD-mixup, with GLNN of Eq. (1) and Loss-Weighting of Eq. (3), respectively. The experiments are conducted on seven datasets with three different GNN architectures as teacher GNNs, where imporv. denotes the performance improvements with respect to GLNN. From the results reported in Table. 1, we can make three observations: (1) Both HGMD-weight and HGMD-mixup perform much better than vanilla MLP, GLNN, and Loss-Weighting on all seven datasets, especially on the large-scale ogbn-arxiv dataset. (2) Both HGMD-weight and HGMD-mixup are applicable to various types of teacher GNN architectures. For example, HGMD-mixup outperforms GLNN by 2.22% (GCN), 2.11% (SAGE), and 2.19% (GAT) averaged over seven datasets, respectively. (3) Overall, HGMD-mixup performs slightly better than HGMD-weight across various datasets, owing to more knowledge variants augmented by the hardness-aware mixup.\nFurthermore, we compare HGMD-weight and HGDM-mixup with several state-of-the-art graph distillation methods, including both GNN-to-GNN and GNN-to-MLP KD. The experimental results reported in Table. 2 show that (1) Despite being completely non-parametric methods, HGMD-weight and HGMD-mixup both perform much better than existing GNN-to-MLP baselines on 5 out of 8 datasets. (2) HGMD-weight and HGMD-mixup outperform those GNN-to-GNN baselines on four relatively small datasets (i.e., Cora, Citeseer, Pubmed, and Photo). Besides, their performance is comparable to those GNN-to-GNN baselines on four relatively large datasets (i.e., CS, Physics, and ogbn-arxiv, products). These observations indicate that distilled MLPs have the same expressive potential as teacher GNNs, and that \"parametric\" is not a must for knowledge distillation. In addition, we also evaluate HGMD in the inductive setting and the results are provided in Appendix C. A detailed comparison between HGMD and RKD using different knowledge hardness metrics can be found in Appendix D."}, {"title": "4.2 Ablation Study", "content": "To evaluate how hardness-aware subgraph extraction (SubGraph) and two subgraph distillation strategies (weight and mixup) influence performance, we compare vanilla GCNs and GLNN with the following five schemes: (A) Subgraph-only: extract hardness-aware subgraphs and then distill their knowledge into the student MLP with equal loss weights; (B) Weight-only: take the full neighborhoods as subgraphs and then distill by hardness-aware weighting as in Eq. (5); (C) Mixup-only: take the full neighborhoods as subgraphs and then distill by hardness-aware mixup as in Eq. (7); (D) HGMD-weight; and (E) HGMD-mixup. We can observe from the experimental results reported in Table. 3 that (1) SubGraph plays a very important role in improving performance, which illustrates the benefits of performing knowledge distillation at the subgraph level compared to the node level, as it provides more supervision for those hard samples in a hardness-aware manner. (2) Both hardness-aware weighting and mixup help improve performance, especially the latter. (3) Combining the two different designs (subgraph extraction and subgraph distillation) together can further improve performance on top of each on all six graph datasets."}, {"title": "4.3 Deep Analysis on Hardness Awareness", "content": "4.3.1 Case Study of Hardness-aware Subgraphs. To intuitively show what \"hardness awareness\" means, we select three GNN knowledge samples with different hardness levels from four datasets, respectively. Next, we mark the hardness of each knowledge sample as well as their neighboring nodes according to the color bar on their right side, where a darker blue indicates a higher knowledge hardness. In addition, we use the edge color to denote the probability of the corresponding neighboring node being sampled into the hardness-aware subgraph, according to another color bar displayed on the right. We can make two observations from the results of Fig. 4 that: (1) For a given target node, neighboring nodes with lower hardness (lighter blue) tend to have a higher probability of being sampled into the subgraph. (2) A target node with higher hardness (darker blue) has a higher probability of having its neighboring nodes sampled. In other words, the sampling probability of neighboring nodes is actually a trade-off between their own hardness and the hardness of the target node. For example, when the hardness of the target node is 0.36, a neighboring node with a hardness of 0.61 is still hard to be sampled, as shown in Fig. 4(a); however, when the hardness of the target node is 1.73, even a neighboring node with a hardness of 1.52 has a high sampling probability.\n4.3.2 3D Histogram on Hardness and Similarity. We show in Fig. 5(a) and Fig. 5(b) the 3D histograms of the sampling probability of neighboring nodes with respect to their hardness, their cosine similarity to the target node, and the hardness of the target node, from which we can observe that: (1) As the hardness of a target node increases, the sampling probability of its neighboring nodes also increases; (2) Neighboring nodes with lower hardness have a higher probability of being sampled into the corresponding subgraph; (3) As the cosine similarity between neighboring nodes and target node increases, their sampling probability also increases; However, when the hardness of the target node is high, an overly high similarity means that the hardness of neighboring nodes will also be high, which in turn reduces the sampling probability, which is actually a trade-off between high similarity and low hardness.\n4.3.3 Training Curves. We further report in Fig. 5(c) the average entropy of the nodes in student MLPs and the average size of sampled subgraphs during training on the Cora dataset. It can be seen from Fig. 5(c) that there exists an approximate resonance between the two curves. As the training progresses, the uncertainty of nodes in the student MLPs decreases and thus additional supervision required for hard sample distillation can be reduced accordingly.\n4.3.4 Asymmetric Property of Subgraph Extraction. We statistically calculate the ratios of two connected nodes among all edges that are and are not sampled into each other's subgraphs simultaneously, called symmetrized and asymmetrized sampling. The histogram in Fig. 5(d) shows that subgraph extraction is mostly asymmetric, especially for large-scale datasets. This is because subgraph extraction is performed in a hardness-aware manner, where low-hardness neighboring nodes of a high-hardness target node have a higher sampling probability, but not vice versa. We believe that such asymmetric property of subgraph extraction is a key aspect of the effectiveness of HGMD framework, as it essentially transforms an undirected graph into a directed graph for processing."}, {"title": "5 CONCLUSION", "content": "In this paper, we explore thoroughly the knowledge hardness and distillation hardness for GNN-to-MLP knowledge distillation. We identify that hard sample distillation may be a major performance bottleneck of existing distillation algorithms. To address this problem, we propose a novel Hardness-aware GNN-to-MLP Distillation (HGMD) framework, which distills knowledge from teacher GNNs at the subgraph level (rather than the node level) in a hardness-aware manner to provide more supervision for those hard samples. Extensive experiments have been provided to demonstrate the superiority of HGMD across various datasets and GNN architectures. Limitations still exist, for example, designing better hardness metrics or introducing additional learnable parameters for knowledge distillation may be promising directions for future work."}, {"title": "6 ACKNOWLEDGEMENT", "content": "This work was supported by National Science and Technology Major Project of China (No. 2022ZD0115101), National Natural Science Foundation of China Project (No. U21A20427), and Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University."}, {"title": "APPENDIX", "content": "A. Dataset Statistics\nA total of Eight real-world graph datasets are used in this paper to evaluate the proposed HGMD framework. An overview summary of the dataset characteristics is provided in Table. A1. For the three small-scale datasets, including Cora, Citeseer, and Pubmed, we follow the data splitting strategy by [11]. For the three large-scale datasets, including Coauthor-CS, Coauthor-Physics, and Amazon-Photo, we follow [36, 39] to randomly split the data into train/val/test sets, and each random seed corresponds to a different data splitting. For the two large-scale ogbn-arxiv and ogbn-products datasets, we use the public data splits provided by the authors [9].\nB. Implementation Details\nThe following hyperparameters are set the same for all datasets: Epoch E = 500, learning rate lr = 0.01 (0.005 for ogbn-axriv), weight decay decay =5e-4 (0.0 for ogbn-arxiv), and layer number L = 2 (3 for Cora and ogbn-arxiv). The other dataset-specific hyperparameters are determined by an AutoML toolkit NNI with the search spaces as: hidden dimension F = {256, 512, 2048}, KD temperature \u03c4 = {0.8, 0.9, 1.0, 1.1, 1.2}, loss weight \u1e9e = {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}, coefficient of beta distribution a = {0.3,0.4, 0.5}. Moreover, the hyperparameter \u03b7 in Eq. (4) is initially set to {1, 5, 10} and then decays exponentially with the decay step of 250. The experiments are implemented based on the DGL library [25] with Intel(R) Xeon(R) Gold 6240R @ 2.40GHz CPU and NVIDIA V100 GPU. For a fair comparison, the model with the highest validation accuracy will be selected for testing. Besides, each set of experiments is run five times with different random seeds, and the averages are reported.\nFor all baselines, we did not directly copy the results from their original papers but reproduced them by distilling from the same teacher GNNs as in this paper, under the same settings and data splits. As we know, the performance of the distilled student MLPs depends heavily on the quality of teacher GNNs. However, we have no way to get the checkpoints of the teacher models used in previous baselines, i.e., we cannot guarantee that the student MLPs"}]}