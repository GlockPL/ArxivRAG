{"title": "A Training-Free Framework for Precise Mobile Manipulation of Small Everyday Objects", "authors": ["Arjun Gupta", "Rishik Sathua", "Saurabh Gupta"], "abstract": "Many everyday mobile manipulation tasks require precise interaction with small objects, such as grasping a knob to open a cabinet or pressing a light switch. In this paper, we develop Servoing with Vision Models (SVM), a closed-loop training-free framework that enables a mobile manipulator to tackle such precise tasks involving the manipulation of small objects. SVM employs an RGB-D wrist camera and uses visual servoing for control. Our novelty lies in the use of state-of-the-art vision models to reliably compute 3D targets from the wrist image for diverse tasks and under occlusion due to the end-effector. To mitigate occlusion artifacts, we employ vision models to out-paint the end-effector thereby significantly en-hancing target localization. We demonstrate that aided by out-painting methods, open-vocabulary object detectors can serve as a drop-in module to identify semantic targets (e.g. knobs) and point tracking methods can reliably track interaction sites indicated by user clicks. This training-free method obtains an 85% zero-shot success rate on manipulating unseen objects in novel environments in the real world, outperforming an open-loop control method and an imitation learning baseline trained on 1000+ demonstrations by an absolute success rate of 50%.", "sections": [{"title": "I. INTRODUCTION", "content": "Mobile manipulators hold the promise of performing a wide range of useful tasks in our everyday environments. However, a major obstacle to realizing this vision lies in the lack of precise mobile manipulation capabilities of current systems. Many real world tasks require precise interaction with small objects, such as grasping a knob to pull open a cabinet or pressing a light switch, where even a small deviation can cause failure. A mobile manipulator's mobility makes these tasks even more challenging. For example, small errors during navigation, say on a thick carpeted surface, can easily exceed the tight tolerance required for precise tasks, and a non-holonomic base may limit precise repositioning. Furthermore, mobility means that mobile manipulators have to manipulate in varied locations under diverse lighting and unnatural viewpoints (e.g. looking top-down at a drawer from very close by), demanding significantly broader gen-eralization than stationary manipulators confined to a fixed environment. As a result, developing mobile manipulators capable of performing precise tasks while generalizing to diverse environments remains an open problem.\nMany precise mobile manipulation tasks involve stylized interactions with small objects: reaching a precise interaction site before executing a simple motion. The difficulty in these tasks lies in reaching the accurate pre-task pose around the small target object whereas the subsequent motion is easy to execute. For getting to the accurate pre-task pose, open loop execution using a sense-plan-act paradigm does"}, {"title": "II. RELATED WORK", "content": "Visual servoing with an eye-in-hand camera is an effective\ntechnique to close the loop to precisely pursue targets [1]\nwithout requiring large-scale task-specific training for broad\ngeneralization, unlike closed-loop imitation learning. How-\never, vanilla visual servoing makes strong assumptions, e.g.\nrequiring known 3D objects or target images, which are not\navailable in our in-the-wild setting. Our proposed approach,\nServoing with Vision Models or SVM, marries together\nvisual servoing with modern perception systems to mitigate\nthese limitations. This leads to an effective system which is\nable to operate in a closed loop manner, and at the same\ntime is versatile enough to operate in novel environments on\npreviously unseen objects.\nSVM leverages modern perception systems in two ways.\nFirst, we use them to specify targets for the visual servoing\nmodule. This alleviates the need for known 3D objects or\ntarget images. We experiment with two ways to specify\ntargets: a) semantic categories, and b) points of interaction.\nFor objects that have a well-known semantic category (e.g.\ndrawer knobs or cabinet handles), we use an open-world\nobject detector (e.g. Detic [2]) to continuously detect the\ntarget during visual servoing. However, not all mobile ma-\nnipulation interaction sites, e.g. the different buttons on a\nmicrowave, correspond to a semantic category. We tackle\nsuch cases by using point trackers (e.g. CoTracker [3]) to\ncontinuously track a user-indicated interaction site (e.g. a\nsingle user click in the image, specifying which button on\nthe microwave to push) over the course of visual servoing.\nThus, the use of strong perception systems takes care of the\ntarget specification problem in visual servoing.\nOne problem however still remains. The use of visual\nservoing with an eye-in-hand camera for manipulation tasks\nsuffers due to occlusion of the environment by the manip-\nulator. Such occlusion can be particularly detrimental if it\nleads to out-of-distribution input to the perception system\nthat now starts producing erroneous predictions (see second\nrow of Figure 2). We mitigate this issue using yet another\nadvance in computer vision: video in-painting models [4]\u2013\n[6]. We out-paint the robot end-effector to obtain a clean\nview of the scene (see last row of Figure 2). This improves\nthe detection performance of the vision system, leading to\nimproved overall success.\nWe test SVM across several real world tasks: grasping a\nknob to pull open drawers / cupboards, grasping a handle to\npull open a cabinet, pushing onto light buttons, and pushing\nbooks into place on bookshelves. We obtain a 85% success\nrate on these challenging tasks, zero-shot on novel objects in\npreviously unseen environments. As expected, SVM performs\nmuch better than open-loop control which only succeeds on"}, {"title": "A. Visual Servoing"}, {"title": "B. Eye-in-hand Imitation Learning", "content": "Imitation learning [21], [22] is a general tool for learning\nclosed-loop manipulation policies and has been applied to\neye-in-hand settings [7], [23]\u2013[26]. However, this generality\ncomes with the need for a large number of demonstrations\nfor generalization [27]. Recent one-shot imitation learning\nmethods [28], [29] leverage the structure of the task (getting\nto a bottleneck pose + motion replay) to learn from a single\ndemonstration but are then restricted to interacting with the\nobject they were trained on. We also leverage the same\nstructure in tasks, but by employing vision foundation models\ntrained on large datasets, our framework is able to operate\non novel objects in novel environments."}, {"title": "C. Detection, Point Tracking, and In-painting", "content": "Training on Internet-scale datasets [30]-[32] with large-\ncapacity models [33] has dramatically improved the gener-\nalization performance of vision systems. This coupled with\nalignment of visual representations with ones from language\n(e.g. CLIP [30]) has lead to effective open-vocabulary object\ndetectors, e.g. Detic [2], OVR-CNN [34]. Similar advances\nin diffusion-based generative models [35]-[38] and large-\nscale training have led to effective image generation models.\nThese models have been leveraged for image and video in-\npainting [4], [5]. In-painting models have also been used in\nrobotics to mitigate domain gap between human and robot\ndata [4], [39]. Last, point-based tracking in videos is seeing"}, {"title": "III. TASK", "content": "Many everyday household tasks involve precise manipula-\ntion followed by execution of a motion primitive. Examples\ninclude grasping a knob or a handle to pull open a drawers /\ncupboard, or pushing a button on a microwave. We consider\ntwo variants, where the interaction site can be identified\nvia a semantic label (e.g. knob / handle) or via a user-\nspecified point (e.g. a click on an image specifying the\nbutton to push); and assume that the motion primitive is\ngiven or easy to specify. Our goal is to enable a commodity\nmobile manipulator equipped with a RGB-D wrist camera to\naccomplish such tasks in previously unseen environments."}, {"title": "IV. METHOD", "content": "At a high-level, our method employs visual servoing on\neye-in-hand camera images (Section IV-C) to control the\nend-effector to reach the interaction site. Our innovation\nlies in the use of state-of-the-art vision models to reliably\ndetect / track the interaction site (Section IV-B) to provide\nthe visual feedback for visual servoing. As we will see,\nocclusion due to the end-effector in the wrist camera view\ncauses nuisance. We deal with this by painting out the end-\neffector (Section IV-A) before running the perception models\non images. Let's denote images from the wrist camera with\n$I_t$, current robot state by $x_t$. The output actions are computed\nas follows:\n$$a_t = \\eta(x_t, g(f(I_t, [I_1, ..., I_{t-1}]))),$$\nwhere $f(I,I)$ is a video inpainting function that paints out\nthe end-effector from image I using images in I as reference,"}, {"title": "A. Inpainting", "content": "Given RGB images from the wrist camera, the inpainting\nfunction $f$ uses past frames from the wrist camera to inpaint\nthe current frame $I_t$. We utilize a video inpainting method\n(as opposed to an image inpainting method) for better\nperformance: a video inpainting model has access to previous\nframes (where the object may not be occluded), which\ncan lead to improved inpainting. We adopt the ProPainter\nmodel [5] to realize $f$. It is a transformer-based video\ninpainting model trained on the YouTube-VOS [46] dataset.\nIf the object of interest is occluded by the end-effector\nin the first frame, even a video inpainting method may not\nbe able to accurately reconstruct the object. To combat this,\nwe design a \u201clook-around\" primitive that moves the end-\neffector around (vertically and laterally) to obtain contextual\ninformation about the scene. To limit the inference time in\neach iteration, we limit the inpainting model to only look at\nthe ten past images. The \"look-around\" provides an initial\nset of ten images.\nOut painting the end-effector also requires a mask of\nthe end-effector. We use a manually constructed mask that\ncoarsely covers the end-effector. We find this to work better\nfor out painting than a fine mask of the end-effector obtained\nusing the segmentation model SAM [32]."}, {"title": "B. Interaction Site Localization", "content": "Given an image with the end-effector painted out, our\nnext goal is to localize the object of interest to obtain 3D\nlocation for the target. We handle the two specifications for\nthe interaction site, via a semantic label or a user click,\nseparately as described below."}, {"title": "1) Detection:", "content": "For semantically specified targets (e.g.\nknobs / handles), we use Detic [2], an open-vocabulary\ndetector trained on large-scale datasets. We prompt Detic\nwith the object class 'handle' for handles and 'knob' for\nknobs. If Detic detects multiple handles in the image, we\nselect the handle closest to the center of the image. Detic\nalso outputs a mask for the object. We compute the center\nof the mask and use this as 2D position of the object of\ninterest."}, {"title": "2) Tracking:", "content": "For tasks specified via a user click, e.g. the\npoint on the book or the button in Figure 1, we use of\nCoTracker [3], a point tracking method for videos. Given\na point in the first frame, CoTracker is able to track it\nover subsequent frames seen during execution. CoTracker\nnotes that tracking performance is better when tracking many\npoints together. We therefore sample 40 points randomly\naround the user click and found that to drastically improve\ntracking performance.\nEither of these methods provides a 2D location in the\nimage. We lift this 2D target location to 3D using the depth\nimage. In the case that the 2D position of the target point\nis within the mask of the end-effector (i.e. occluded by the\nend-effector), we utilize the depth from the nearest previous\nframe for this 3D lifting."}, {"title": "C. Closed-loop Control", "content": "Given the interaction sites' 3D location, we employ visual\nservoing to realize $\\eta$ to compute velocity control commands.\nVisual servoing computes whole-body velocity control com-\nmands that minimize the distance between the end-effector\nand the 3D target point [1]."}, {"title": "V. EXPERIMENTS", "content": "Our experiments are designed to test a) the extent to which\nopen loop execution is an issue for precise mobile manip-\nulation tasks, b) how effective are blind proprioceptive cor-\nrection techniques, c) do object detectors and point trackers\nperform reliably enough in wrist camera images for reliable\ncontrol, d) is occlusion by the end-effector an issue and"}, {"title": "A. Tasks and Experimental Setup", "content": "We work with the Stretch RE2 robot. Stretch RE2 is a\ncommodity mobile manipulator with a 5DOF arm mounted\non top of a non-holomonic base. We upgrade the robot to use\nthe Dex Wrist 3, which has an eye-in-hand RGB-D camera\n(Intel D405). We consider 3 task families for a total of 6\ndifferent tasks: a) holding a knob to pull open a cabinet or\ndrawer, b) holding a handle to pull open a cabinet, and c)\npushing on objects (light buttons, books in a book shelf, and\nlight switches). Our focus is on generalization. Therefore,\nwe exclusively test on previously unseen instances, not used\nduring development in any way. Figure 5 shows the instances\nthat we test on.\nAll tasks involve some precise manipulation, followed by\nexecution of a motion primitive. For the pushing tasks,\nthe precise motion is to get the end-effector exactly at the\nindicated point and the motion primitive is to push in the\ndirection perpendicular to the surface and retract the end-\neffector upon contact. The robot is positioned such that the\ntarget position is within the field of view of the wrist camera.\nA user selects the point of pushing via a mouse click on the\nwrist camera image. The goal is to push at the indicated\nlocation. Success is determined by whether the push results\nin the desired outcome (light turns on / off or book gets\npushed in). The original rubber gripper bends upon contact,\nwe use a rigid known tool that sticks out a bit. We take the\ngeometry of the tool into account while servoing.\nFor the opening articulated object tasks, the precise\nmanipulation is grasping the knob / handle, while the motion\nprimitive is the whole-body motion that opens the cupboard.\nComputing and executing this full body motion is difficult.\nWe adopt the modular approach to opening articulated ob-\njects (MOSART) from Gupta et al. [47] and invoke it after\nthe gripper has been placed around the knob / handle. The"}, {"title": "B. Baselines", "content": "We compare against three other methods for the precise\nmanipulation part of these tasks.\n1) Open Loop (Eye-in-Hand): To assess the precision\nrequirements of the tasks and to set it in context with the\nmanipulation capabilities of the robot platform, this baseline\nuses open loop execution starting from estimates for the 3D\ntarget position from the first wrist camera image.\n2) MOSART [47]: The recent modular system for open-\ning cabinets and drawers [47] reports impressive perfor-\nmance with open-loop control (using the head camera from\n1.5m away), combined with proprioception-based feedback\nto compensate for errors in perception and control when\ninteracting with handles. We test if such correction is also\nsufficient for interacting with knobs. Note that such correc-\ntion is not possible for the smaller buttons and pliable books.\n3) SVM (no inpainting): To understand how much of an\nissue occlusion due to the end-effector is during manipula-\ntion, we ablate the use of inpainting.\n4) Robot Utility Models (RUM) [7]: For the opening\narticulated object tasks, we also compare to Robot Utility\nModels (RUM), a closed-loop imitation learning method\nrecently proposed by Etukuru et al. [7]. RUM is trained\non a substantial dataset comprising expert demonstrations,"}, {"title": "C. Results", "content": "Table I presents results from our experiments. Our\ntraining-free approach SVM successfully solves over 85% of\ntask instances that we test on. As noted, all these tests were\nconducted on unseen object instances in unseen environments"}, {"title": "1) Closing the loop is necessary for these precise tasks:", "content": "While the proprioception-based strategies proposed in\nMOSART [47] work out for handles, they are inadequate for\ntargets like knobs and just don't work for tasks like pushing\nbuttons. Using estimates from the wrist camera is better, but\nopen loop execution still fails for knobs and pushing buttons."}, {"title": "2) Vision models work reasonably well even on wrist camera images:", "content": "Inpainting works well on wrist camera\nimages (see Figure 2 and Figure 3). Closing the loop using\nfeedback from vision detectors and point trackers on wrist\ncamera images also work well, particularly when we use\nin-painted images. See some examples detections and point\ntracks in Figure 6 and Figure 7. Detic [2] was able to reliably\ndetect the knobs and handles and CoTracker [3] was able to\nsuccessfully track the point of interaction letting us solve\n24/28 task instances."}, {"title": "3) Erroneous detections without inpainting hamper per-formance on handles and our end-effector out-painting strat-egy effectively mitigates it:", "content": "As shown in Figure 8, presence\nof the end-effector caused the object detector to miss fire\nleading to failed execution. Our out painting approach mit-\nigates this issue leading to a higher success rate than the\napproach without out-painting. Interestingly, CoTracker [3] is\nquite robust to occlusion (possibly because it tracks multiple\npoints) and doesn't benefit from in-painting."}, {"title": "4) Closed-loop imitation learning struggles on novel objects:", "content": "As presented in Table II, SVM significantly out-"}, {"title": "VI. DISCUSSION", "content": "In this paper, we describe SVM, a training-free framework\nfor precise manipulation tasks that involve reaching a precise\ninteraction site followed by execution of a primitive motion.\nStrong vision models help us mitigate issues caused by\nocclusion by the end-effector thereby enabling the use of off-\nthe-shelf open-vocabulary object detectors and point trackers\nto estimate servoing targets during execution. Use of strong\noff-the-shelf models also provides broad generalization for\nperception while the use of servoing provides robust control.\nThis enables SVM to solve tasks in novel environments on\nnovel objects, obtaining a 85% zero-shot success rate across\n4 precise mobile manipulation tasks. Most surprisingly, even\nthough SVM is modular, it outperforms the strong end-to-end\nimitation learning system RUM [7] that was trained\non 1000+ demonstrations. This is particularly striking as\nimitation learning is the tool of choice for precise tasks"}, {"title": "VII. LIMITATIONS", "content": "Even though SVM performs quite well across many tasks\non novel objects in novel environments, it suffers from some\nshortcomings. Running these large vision models is compu-\ntationally expensive and we have to off load computation\nto a A40 GPU sitting in a server. Even with this GPU, we\nare only able to run the vision pipeline at a 0.1 Hz leading\nto slow executions. Building vision models specialized to\nwrist camera images may work better and faster. A second\nlimitation is the reliance on depth from the wrist camera\nwhich may be poor in some situations e.g. shiny or dark\nobjects. Use of learned disparity estimators [48] with stereo\nimages could mitigate this. As our focus is on the precise\nreaching of interaction sites, we work with a hand-crafted\ntask decomposition into the interaction site and primitive\nmotion. In the future, we could obtain such a decomposition\nusing LLMs, or from demonstrations, thereby expanding the\nset of tasks we can tackle."}]}