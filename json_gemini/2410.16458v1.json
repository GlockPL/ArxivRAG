{"title": "STAR: A Simple Training-free Approach for Recommendations using Large Language Models", "authors": ["Dong-Ho Lee", "Adam Kraft", "Long Jin", "Nikhil Mehta", "Taibai Xu", "Lichan Hong", "Ed H. Chi", "Xinyang Yi"], "abstract": "Recent progress in large language models (LLMs) offers promising new approaches for recommendation system (RecSys) tasks. While the current state-of-the-art methods rely on fine-tuning LLMs to achieve optimal results, this process is costly and introduces significant engineering complexities. Conversely, methods that bypass fine-tuning and use LLMs directly are less resource-intensive but often fail to fully capture both semantic and collaborative information, resulting in sub-optimal performance compared to their fine-tuned counterparts. In this paper, we propose a Simple Training-free Approach for Recommendation (STAR), a framework that utilizes LLMs and can be applied to various recommendation tasks without the need for fine-tuning. Our approach involves a retrieval stage that uses semantic embeddings from LLMs combined with collaborative user information to retrieve candidate items. We then apply an LLM for pairwise ranking to enhance next-item prediction. Experimental results on the Amazon Review dataset show competitive performance for next item prediction, even with our retrieval stage alone. Our full method achieves Hits@10 performance of +23.8% on Beauty, +37.5% on Toys and Games, and -1.8% on Sports and Outdoors relative to the best supervised models. This framework offers an effective alternative to traditional supervised models, highlighting the potential of LLMs in recommendation systems without extensive training or custom architectures.", "sections": [{"title": "1 Introduction", "content": "Personalized recommendation systems have become indispensable tools for enhancing user experiences and driving engagement across a wide range of online platforms. Recent advances in large language models (LLMs) present new opportunities for addressing recommendation tasks [1-8]. Current strategies primarily involve utilizing LLMs as either feature encoders [9-24] or as scoring and ranking functions [25-31]. When LLMs are employed as feature encoders, there is potential for transfer learning and cross-domain generalization by initializing embedding layers with LLM embeddings, although this approach requires extensive training. On the other hand, using LLMs for scoring and ranking demonstrates the ability to leverage their reasoning capabilities to address recommendation tasks. However, these models still lag behind the performance of fine-tuned models due to a lack of collaborative knowledge.\nThe primary motivation of this work is to develop a general framework that serves as a generalist across multiple recommendation domains. We demonstrate that recent advancements in LLMs align with this vision, effectively functioning as generalists without requiring any domain-specific fine-tuning. Based on our findings, we present a Simple Training-free Approach for Recommendation (STAR) framework using LLMs. The STAR framework involves two stages: Retrieval and Ranking. The Retrieval stage scores new"}, {"title": "2 Related Works", "content": "LLM as a Feature Encoder for RecSys. Recommendation systems typically leverage feature encoders to transform item and user profiles into suitable representations for model training. Traditionally, ID-based systems relied on one-hot encoding for structured features [34, 35]. However, recent advancements in LLMs have enabled the utilization of text encoders to capture rich semantic information from item metadata and user profiles [36-39] To further optimize these representations for specific applications, researchers have explored several approaches: (1) mapping continuous LLM embeddings into discrete tokens using vector quantization and training a subsequent generative model [12, 13, 21, 22]; (2) training sequential models by initializing the embedding layer with LLM embeddings [9, 14, 24]; and (3) training models to directly compute the relevance between item and user embeddings (i.e., embeddings of user selected items) [10, 11, 16-20, 23]. While optimizing representations can improve recommendation performance, this often comes at the cost of increased training expenses and reduced generalizability. In this work, we demonstrate that LLM embeddings can be directly used as effective item representations, yielding strong results in sequential recommendation tasks without requiring extensive optimization. This finding aligns with those of [15], but differs by usage of novel scoring rules that incorporates collaborative and temporal information.\nLLM as a Scoring and Ranking function for RecSys. Recent studies show that LLMs can recommend items by understanding user preferences or past interactions in natural language. This is achieved through generative selection prompting, where the model ranks"}, {"title": "3 STAR: Simple Training-Free Approach", "content": "This section initially outlines the problem formulation (Section 3.1). Subsequently, we detail the proposed retrieval (Section 3.2) and ranking pipelines (Section 3.3)."}, {"title": "3.1 Sequential Recommendation", "content": "The sequential recommendation task aims to predict the next item a user will interact with based on their interaction history. For a user $u \\in U$, where $U$ is the set of all users, the interaction history is represented as a sequence of items $S_u = \\{S_1, S_2, ..., S_n\\}$, with each $s_i \\in I$ belonging to the set of all items $I$. Each user history item $s_i$ is associated with a rating $r_i \\in \\{1, 2, 3, 4, 5\\}$ given by the user $u$. The goal is to predict the next item $s_{n+1} \\in I$ that the user is most likely to interact with."}, {"title": "3.2 Retrieval Pipeline", "content": "The retrieval pipeline aims to assign a score to an unseen item $x \\in I$ given the sequence $S_u$. To achieve this, we build two scoring components: one that focuses on the semantic relationship between items and another that focuses on the collaborative relationship.\nSemantic relationship. Understanding how similar a candidate item is to the items in a user's interaction history $s_i \\in S_u$ is key to accurately gauging how well candidate items align with user preferences. Here we leverage LLM embedding models, where we pass in custom text prompts representing items and collect embedding vectors of dimension $d_e$. We construct a prompt based on the item information and metadata, including the title, description, category, brand, sales ranking, and price. We omit metadata fields like Item ID and URL, as those fields contain strings that can contain spurious lexical similarity (e.g., IDs: \"000012\", \"000013' or URLs: \"https://abc.com/uxrl\", \"https://abc.com/uxrb\") and can reduce the uniformity of the embedding space and make it difficult to distinguish between semantically different items (See Appendix A.1"}, {"title": "Collaborative relationship.", "content": "Semantic similarity between a candidate item and items in a user's interaction history is a helpful cue for assessing the similarity of items based on the item information. However, this alone does not fully capture the engagement interactions of items by multiple users. To better understand the collaborative relationship, we consider how frequently different combinations of items are interacted with by users. These shared interaction patterns can provide strong indicators of how likely the candidate item is to resonate with a broader audience with similar preferences. For each item $i \\in I$, we derive an interaction array that represents user interactions, forming a set of sparse user-item interaction arrays $C\\in R^{n \\times m}$, where m is number of users in U. The collaborative relationship between two items $(i_a, i_b)$ is then computed by using the cosine similarity between their sparse arrays $C_{i_a}, C_{i_p} \\in C$, capturing the normalized co-occurrence of the items. To streamline the process, we pre-compute and store these"}, {"title": "Scoring rules.", "content": "The score for an unseen item $x \\in I$ is calculated by averaging both the semantic and collaborative relationships between items in $S_u = \\{S_1, S_2,..., S_n\\}$ as follows:\n$score(x) = \\frac{1}{n} \\sum_{j=1}^{n} r_j \\cdot t_j \\cdot [\\alpha \\cdot R_s^j + (1-\\alpha) \\cdot R_c^j]$ (1)\nwhere $R_s^j$ and $R_c^j$ represent the semantic and collaborative relationships between the unseen item $x$ and item $s_j\\in S_u$, respectively. In this equation, $r_j$ is the rating given by user $u$ to item $s_j$, and $t_i$ is an exponential decay function applied to the temporal order $t_j$ of $s_j$ in the sequence $S_u$. Here, $t_j$ is set to 1 for the most recent item in $S_u$ and increments by 1 up to n for the oldest item. The framework, illustrated in Figure 2, outputs the top k items in descending order based on their scores."}, {"title": "3.3 Ranking Pipeline", "content": "After retrieving the top k items, denoted as $I_k$, from the initial retrieval process, a LLM is employed to further rank these items to enhance the overall ranking quality. The items in $I_k$ are already ordered based on scores from the retrieval framework, which reflect semantic, collaborative, and temporal information. We intentionally incorporate this initial order into the ranking process to enhance both efficiency and effectiveness. This framework then leverages the capabilities of the LLM to better capture user preference, complex relationships and contextual relevance among the items."}, {"title": "3.3.1 Rank schema.", "content": "We present three main strategies for ranking: (1) Point-wise evaluates each item $x \\in I_k$ independently, based on the user sequence $S_u$, to determine how likely it is that user u will interact with item x. If two items receive the same score, their rank follows the initial order from $I_k$; (2) Pair-wise evaluates the preference between two items $x_i, x_j \\in I_k$ based on the user sequence $S_u$. We adopt a sliding window approach, starting from the items with the lowest retrieval score at the bottom of the list [50]. The LLM compares and swaps adjacent pairs, while iteratively stepping the comparison window one element at a time. (3) List-wise evaluates the preference among multiple items $x_i, ..., x_{i+w} \\in I_k$ based on the user sequence $S_u$. This method also uses a sliding window approach, with a window size w and a stride d to move the window across the list, refining the ranking as it passes [51]. In this setup, pair-wise is a special case of list-wise with w = 2 and d = 1."}, {"title": "3.3.2 Item information.", "content": "We represent the metadata (e.g., Item ID, title, category, etc.) for each item in the user sequence $s_j \\in S_u$ and each candidate item to be ranked $x \\in I_k$ as JSON format in the input prompt. Additionally, we incorporate two more types of information that can help the reasoning capabilities of the LLM: (1) Popularity is calculated as the number of users who have interacted with the item x, simply by counting the occurrences in the training data. This popularity value is then included in the prompt for both the items in the user sequence $s_j \\in S_u$ and the candidate item to be ranked $x \\in I_k$ as \"Number of users who bought this item: ###\"; (2) Co-occurrence is calculated as the number of users who have interacted with both item x and item $s_j \\in S_u$. The resulting value is then included for candidate items $x \\in I_k$ as \"Number of users who bought both this item and item $s_j$: ###\"."}, {"title": "4 Experimental Setup", "content": "Datasets. We evaluate the performance using public 2014 Amazon review datasets [32, 33]. Specifically, we select the Beauty, Toys and Games, and Sports and Outdoors categories, as these have been used in previous studies [40, 44] and provide data points for comparison (see Table 1). We follow the same data processing steps as in prior work, filtering out users and items with fewer than five interactions, maintaining consistent baseline settings.\nDataset Construction and Evaluation Metrics. We follow conventional supervised models, where the last item, $s_n$, is reserved for testing and the second to last item, $s_{n-1}$, is used for validation. The remaining items are used for training. For the final predictions of $s_n$, all training and validation items are used as input. Although our method does not train model parameters, we only use the training data to calculate the collaborative user interaction values used for $R_C$ in retrieval and for popularity and co-occurence in ranking. We report Normalized Discounted Cumulative Gain (NDCG) and Hit Ratio (HR) at ranks 5 and 10.\nCompared Methods. We compare our model with following supervised trained models: (1) KNN is a user-based collaborative filtering method that finds the top 10 most similar users to a given user and averages their ratings to score a specific item; (2) Caser uses convolution neural networks to model user interests [54]; (3) HGN uses hierarchical gating networks to capture both long and short-term user behaviors [55]; (4) GRU4Rec employs GRU to model user action sequences [56]; (5) FDSA uses a self-attentive model to learn feature transition patterns [57]; (6) SASRec uses a self-attention mechanism to capture item correlations within a user's action sequence [58]; (7) BERT4Rec applies a masked language modeling (MLM) objective for bi-directional sequential recommendation [9]; (8) S\u00b3-Rec extends beyond the MLM objective by pre-training with four self-supervised objectives to learn better item representations [59]. (9) P5 fine-tunes a pre-trained LM for use in multi-task recommendation systems by generating tokens based on randomly assigned item IDs [40]; (10) TIGER also fine-tunes LMs to predict item IDs directly, but these IDs are semantic, meaning they are learned based on the content of the items [21]; and (11) IDGenRec goes further by extending semantic IDs to textual IDs, enriching the IDs with more detailed information [44].\nImplementation Details. Unless otherwise specified, we use Gecko text-embedding-004 [39]\u00b9 to collect LLM embeddings for retrieval, and we use gemini-1.5-flash\u00b2 for LLM-based ranking. All API calls were completed as of September 1, 2024."}, {"title": "5 Experimental Results", "content": "In this section, we present a multifaceted performance analysis of our retrieval (Section 5.1) and ranking pipeline (Section 5.2)."}, {"title": "5.1 Retrieval Pipeline", "content": "The second group of Table 2 presents the performance of our retrieval framework. STAR-Retrieval alone achieves the best or second-best results compared to all baselines and fine-tuned methods. There is a significant improvement across all metrics for Toys and Games, ranging from +26.50% to +35.3%. In Beauty, all metrics besides NDCG@5 (-1.2%) are improved from a range of +6.1% to +20.0%. In Sports and Outdoors, the results are second best to ID-GenRec, trailing from a range of -19.6% to -5.57%. Furthermore, we conduct more studies to answer the following research questions:\nQ1. How do semantic and collaborative information affect predictions? To evaluate the impact of semantic information ($R_s$) and collaborative information ($R_C$) on retrieval performance, we conducted an analysis by adjusting the weighting factor $\\alpha$ between these two components. As shown in the left panel of Figure 4, the optimal performance is achieved from a range of 0.5 to 0.6. We choose 0.5 for other experiments as a simple, equal weighting. Furthermore, we see that when we only use one of the two components ($\\alpha$ = 0.0 or $\\alpha$ = 1.0) then the results are significantly worse than the combination. This validates our hypothesis that it is better to include both semantic and collaborative information and that having both types of information is additive to the overall system.\nQ2. How does the number of user history items l and recency factor A affect predictions? To assess the impact of the number of user history items l and the recency factor A on retrieval performance, we conducted an analysis by varying both parameters. As depicted in the right panel of Figure 4, we see that having more user history items can help up to a certain point (l = 3), but after that performance can be impacted. The middle panel of Figure 4 shows that setting an appropriate recency factor ($\\lambda$ = 0.7) to prioritize recent history items is more effective than not applying any recency factor"}, {"title": "5.2 Ranking Pipeline", "content": "The third group of Table 2 highlights the performance of the ranking framework, which improves upon the retrieval stage results. Pair-wise ranking improves all metrics over STAR-Retrieval performance by +1.7% to +7.9%. This further improves the results over other baselines for Beauty and Toys and Games, while closing the gap on IDGenRec in Sports and Outdoors. Point-wise and list-wise methods struggle to achieve similar improvements. We answer the following research questions:\nQ1. Is ranking an effective approach? How do window size and stride impact performance? Previous approaches use a selection prompt, instructing the LLM to choose the top k items in ranked order from a set of candidates [25, 27, 28]. In contrast, our method uses a ranking prompt, which explicitly instructs the LLM to rank all items within the available context window. We assess the effectiveness of the ranking approach in comparison to the selection approach and a point-wise prompt. Table 5 illustrates the performance differences when varying window size and stride. The results show that ranking with a small window size (such as pair-wise or list-wise with a window size of 4) consistently outperforms the selection and pair-wise methods. Furthermore, the pair-wise and selection prompts often fail to improve performance compared to the retrieval stage, ultimately harming overall effectiveness. These findings align with previous research in document retrieval, where list-wise ranking with large window sizes or reasoning prompts requires specific task knowledge, while pair-wise ranking can be effectively performed by much smaller language models [50].\nQ2. How does number of candidates k and number of history l affect predictions? As shown in Figure 6, varying the number of historical items included in the model, denoted as l, does not significantly impact prediction performance. On the other hand, increasing the number of candidate items, k, can be somewhat beneficial because it raises the likelihood of including the correct item among the"}, {"title": "6 Conclusion", "content": "In this paper, we introduced a Simple Training-free Approach for Recommendation (STAR) that uses the power of large language"}, {"title": "7 Limitations & Future Work", "content": "The STAR framework presents an effective alternative to traditional supervised models, showcasing the potential of LLMs in recommendation systems without the need for extensive training or custom architectures. However, several limitations remain, which also indicate directions for future improvement:\nImportance of item modality and enriched item meta-data. The STAR framework's ability to capture semantic relationships between items relies significantly on the presence of rich item text meta-data. Without such meta-data and with only user-item interaction data available, the framework's semantic relationship component will be less effective. To maximize the use of semantic relationships between items, future work should explore incorporating additional modalities, such as visual or audio data, to generate more comprehensive semantic representations of items, fully utilizing all the available information.\nImproving Retrieval Simplicity and Scalability. Although our work demonstrates the effectiveness of a general training-free framework, the current method requires different choices for parameters. In future work, we will explore ways to either reduce the number of parameters choices or select values more easily. In our current implementation, we compute the full set of item-item comparisons for both the semantic and collaborative information. This computation is infeasible if the item set is too large. In future work, we will run experiments to measure how effective approximate nearest neighbor methods are at reducing computation and maintaining retrieval quality.\nBeyond LLM ranking. The importance of our work highlights that high quality results can be achieved without additional fine-tuning. However, in the current method, our STAR ranking pipeline utilizes costly LLM calls that would result in high latency. This may be a suitable solution to use in offline scenarios, but would be prohibitive to serve large-scale and real-time user traffic. Future work needs to explore how we can improve efficiency, such as using a mix of pair-wise and list-wise ranking. Our work shows a promising first step to creating high quality, training-free, and general recommendation systems."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Item Encoding Prompt for Retrieval Example", "content": "Below is an example of an item prompt for encoding with an LLM embedding API."}, {"title": "A.2 Ranking Prompt Example", "content": "Below is an example of a single pass in a list-wise ranking pipeline with a window size of 4 and a stride of 2 (w = 4 and d = 2) assuming there are 3 history items (1 = 3)."}]}