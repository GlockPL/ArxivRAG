{"title": "AIGCodeSet: A New Annotated Dataset for AI Generated Code Detection", "authors": ["Basak Demirok", "Mucahid Kutlu"], "abstract": "With the rapid advancement of LLM models, they have become widely useful in various fields. While these Al systems can be used for code generation, significantly simplifying and accelerating the tasks of developers, their use for students to do assignments has raised ethical questions in the field of education. In this context, determining the author of a particular code becomes important. In this study, we introduce AIGCodeSet, a dataset for AI-generated code detection tasks, specifically for the Python programming language. We obtain the problem descriptions and human-written codes from the CodeNet dataset. Using the problem descriptions, we generate AI-written codes with CodeLlama 34B, Codestral 22B, and Gemini 1.5 Flash models in three approaches: i) generating code from the problem description alone, ii) generating code using the description along with human-written source code containing runtime errors, and iii) generating code using the problem description and human-written code that resulted in wrong answers. Lastly, we conducted a post-processing step to eliminate LLM output irrelevant to code snippets. Overall, AIGCodeSet consists of 2,828 AI-generated and 4,755 human-written code snippets. We share our code with the research community to support studies on this important topic and provide performance results for baseline AI-generated code detection methods.", "sections": [{"title": "Introduction", "content": "With the impressive advancements in generative artificial intelligence (AI) technologies, large language models (LLM) become one of the main resources for coding. AI can enable software developers to complete coding tasks up to twice as fast, according to a recent study \u00b9. Similarly, a survey conducted by Stack Overflow\u00b2 revealed that developers reported a 33% increase in productivity when using AI-powered tools. Furthermore, these tools can be beneficial for students as they offer a range of valuable resources, including sample solutions, various approaches to problem-solving, code reviews, exercise generation, code explanations, and illustrative examples (Becker et al., 2023).\nDespite the benefits of generative AI in programming, there are several concerns such as the risk of academic misconduct, plagiarism, issues with code reuse, and licensing. Furthermore, utilizing LLMs for coding poses significant risks as it may sometimes lack security, potentially introducing vulnerabilities (Ambati et al., 2024). Perry et al. (2023) report that AI coding tools can generate insecure code in laboratory settings, raising considerable concerns about their use in real-world scenarios. Additionally, Jesse et al. (2023) highlights that AI-generated code using LLMs may contain errors, bugs, or inefficiencies due to the model's lack of real-time testing and validation. Also, constantly relying on it can lead to the atrophy of coding skills (Prather et al., 2023) or cause real developers to lose their jobs (Barenkamp et al., 2020).\nDue to the potential negative impact of these tools, several researchers focused on how to detect AI-generated code (Hou et al.). The majority of these studies use LLMs developed by OpenAI and focus on using LLMs to generate code from scratch using the problem definition. However, LLMs can be used for other purposes in programming such as fixing an error in a given code snippet.\nIn this study, we introduce AIGCodeSet which is an annotated dataset for detecting AI-generated codes. We focus on Python due to its popularity. We first obtained 317 programming problems and corresponding submissions in Python from IBM's"}, {"title": "Related Work", "content": "Despite relatively recent advances in large language models (LLMs) for code generation, this emerging area has garnered significant attention from researchers focusing on various aspects such as developing methods to detect AI-generated code (Xu and Sheng, 2024; Oedingen et al., 2024; Bukhari et al., 2023; Hoq et al., 2024; Idialu et al., 2024), evaluating existing AI-generated code detectors (Pan et al., 2024; Wang et al., 2023), and detecting vulnerabilities in AI-generated codes (Cotroneo et al., 2025; Wang et al., 2024). The studies differ in terms of the languages investigated, number of problems, size of datasets, LLMs employed, and source of the data.\nThe comparison of research findings is complicated due to the limited dataset resources comparing the diversity in LLMs, programming languages, and coding problems. There are few studies introducing datasets specifically for AI-generated code. Tihanyi et al. (2023) created a dataset containing 112,000 C language code samples generated using GPT-3.5 Turbo. Similarly, Wang et al. (2024) developed the CodeSecEval dataset, utilizing several LLMs, including multiple GPT models, CodeLlama-7B, and Claude 3 Opus, for Python code. Both datasets primarily address the security of generated code rather than the detection of AI-generated code.\nResearch on AI-generated code has mainly focused on specific programming languages. Python has been the most studied (Pan et al., 2024; Cotroneo et al., 2025; Oedingen et al., 2024; Idialu et al., 2024; Wang et al., 2024), followed by C (Bukhari et al., 2023; Tihanyi et al., 2023), and Java (Hoq et al., 2024). Some studies have explored multiple languages; for instance, Xu and Sheng (2024) examined C, C++, C#, Java, JavaScript, and Python, while Wang et al. (2023) investigated a set including Ruby, JavaScript, Go, Python, Java, and PHP. In our study, we also focus on Python due to its popularity.\nIn creating a dataset for AI generated code detection, it is important to determine the problems to be covered and how to obtain the human-written codes. Existing studies use various resources for problem definitions and human-written codes. For instance, Pan et al. (2024) utilize codes and problems from Kaggle, Quescol, and LeetCode; Hoq et al. (2024) incorporate CodeWorkout; and Idialu et al. (2024) use CodeChef. Similar to our approach, Xu and Sheng (2024) utilize the CodeNet dataset, applying criteria such as selecting code with line lengths between 10 and 100, an alphanumeric character fraction greater than 0.25, and excluding all comments and duplicate files. In"}, {"title": "AIGCodeSet", "content": "In developing a dataset for detecting AI-generated code, the main requirement is to include both human-authored and AI-generated code samples. However, to ensure the dataset is of high quality and supports effective model training and reliable evaluation, several key considerations must be addressed. Specifically, it is essential to cover a wide range of coding problems and styles. Moreover, including both human-authored and AI-generated code for the same problem will allow for a more effective comparison of their differences. In addition, it is important to consider the level of involvement of AI tools and how they are used, as different individuals may use these tools in varying ways. Lastly, the dataset should cover various LLMs, as individuals may rely on different tools to solve their coding problems. Now, we explain our approach to constructing AIGCodeSet that aims to meet these objectives."}, {"title": "Acquiring Human Written Codes", "content": "Following prior work, we utilize IBM's CodeNet dataset (Puri et al., 2021) as a source for human-written code. This dataset contains 14 million code examples spanning approximately 4,000 coding problems across 55 programming languages. However, to align with our objectives and generate corresponding AI-generated versions at a reasonable cost, we selectively filter the dataset, retaining only the code samples most relevant to our study.\nIn our study, we focus specifically on the Python programming language due to its widespread popularity among programmers. The official webpage of the CodeNet dataset\u2074 also provides Python benchmark data that has 801 unique problems in Python. Upon examining the data, we observed that the problem descriptions in HTML files consistently began with the score information in the range of 100,200..,700, which we utilized as a criterion for selecting the data. To ensure a diverse selection of problems, we categorize the problems seven groups based on the score information. From the first five groups, we sample 60 problems per group. Since there are only 17 problems with scores of 600 and 700 in total, we include all of them, resulting in a total of 317 distinct problems.\nFor each problem, CodeNet provides multiple submissions, allowing us to capture a range of coding styles. Each submission is assigned a status, which can be: (i) accepted, (ii) compile-time error, (iii) runtime error, (iv) wrong answer, or (v) time limit exceeded. For each sampled problem, we select five submissions for each of the accepted, runtime error, and wrong answer statuses, resulting in a total of 15 (5x3) submissions per problem. This approach enables us to include a variety of coding styles and solutions, including incorrect ones. Note that there were no submissions with compile-time errors in our sample as Python is an interpreted language, not compiled. In addition, we exclude submissions with a time limit exceeded status, as we could not consistently find five such submissions for each problem. Moreover, since we cannot ascertain if these solutions are inherently incorrect or simply inefficient, we leave the inclusion of slow coding solutions for future work.\nOverall, we sampled a total of 4,755 (317 x5x 3) code snippets from the CodeNet dataset, covering various coding problems and a wide range of correct and incorrect solutions written by different individuals."}, {"title": "Creating AI-Generated Code Dataset", "content": "To build the AI-generated code dataset, we employ three large language models (LLMs): i) CodeLlama (Roziere et al., 2023) (34B parameters), ii) Codestral (team, 2024) (22B parameters), and iii) Gemini (Team et al., 2024) (1.5 Flash). Using the human-authored code samples, we generate AI code enabling us to examine the distinctions between human-authored and AI-generated code. Specifically, for each coding problem and LLM, we generate three variations: i) generating code from scratch for a given problem, ii) fixing human-written code that results in a runtime error, and iii) correcting human-written code with incorrect output. This approach allows us to cover a range of AI-driven code-generation scenarios.\nWe use the problem descriptions from CodeNet to define each coding task. As part of preprocessing, we replaced multiple instances of '\\n' in the original descriptions with single '\\n's to improve readability. When prompting the model to fix code, we randomly select one of the five code snippets with the relevant status. Table 1 provides the specific prompts used for each scenario.\nAcross all trials, we generated 2,852 (317x3x3) code snippets, covering three distinct LLMs, three usage scenarios, and 317 coding problems."}, {"title": "Post-processing", "content": "After generating the codes using the models, we performed a quality control check to ensure their validity. Several issues were identified in some of the generated code snippets, including: (i) failure to produce any output, (ii) inclusion of code written in C-family languages instead of Python, and (iii) presence of meaningless characters, sentences, numbers, or dots. These problematic snippets were excluded, reducing the total number of AI-generated code samples to 2,828.\nFurthermore, although the prompts specifically requested only code as output, some responses included additional explanations, either as standalone text or embedded within the code as comments. To ensure the dataset accurately reflects the intended challenge of detecting AI-generated code, we manually removed any explanations provided above or below the code. However, comments embedded within the code blocks were retained, provided they were appropriately marked as comment lines."}, {"title": "Brief Statistics", "content": "In order to provide more insight about our dataset, we provide sample code for the counting trailing zero problem. Specifically, the problem is as follows.\nFor an integer $n$ not less than 0, let us define $f(n)$"}, {"title": "Experimental Setup", "content": "We randomly sample 80% of the dataset and use it for training while the rest is used for testing. We report $F_1$, precision, recall, and accuracy scores for models we implemented.\nWe evaluate the performance of the following approaches.\n\u2022 Ada Embeddings. We represent the codes with Ada embeddings developed by OpenAI. We train Random Forest (RF), XGBoost, and Support Vector Machine (SVM) models separately using our training set, yielding three different models.\n\u2022 TF/IDF Vector. Following the approach of Oedingen et al. (2024), we represent the codes with TF/IDF vectors with a size of 5K, and train RF, XGBoost, and SVM models, separately.\n\u2022 Bayes Classifier. Oedingen et al. (2024) report that Bayes classifier is highly effective in detecting AI generated code. We use their implementation in our experiments."}, {"title": "Experimental Results", "content": "We assess the performance of the baseline models in our test test which includes all three LLMs and covers three different LLM usage scenarios. \nOur observation are as follows. Firstly, Bayes classifier outperforms others in terms of $F_1$ and recall scores. However, its precision score is lower than others except RF model with TF-IDF vectors. Secondly, using Ada embeddings yield higher F1 score compared to TF-IDF vectors. We achieve the highest accuracy and precision when we use SVM with Ada embeddings.\nIn order to explore the impact of LLM used for code generation, we split our test set based on LLMs and calculated the recall for each LLM. The results are shown in Table 4\nWe observe that the Bayes Classifier consistently achieves the highest recall across all cases. In addition, the Gemini model results in the lowest recall scores, indicating that it generates code that closely resembles human-written code.\nLastl, we explore how the models' performance is affected by the code generation scenario. In particular, we divide the test based on the generation"}, {"title": "Conclusion", "content": "In this work, we created AIGCodeSet, a dataset for the AI-generated code detection problem, focusing on Codestral, Codellama, and Gemini Flash 1.5 which are not well-studied in the literature. From the CodeNet dataset, we selected 317 problems and included five human submissions for each of the code status: Accepted, Runtime Error, and Wrong Answer, yielding 4,755 human-written code samples in total. Next, we utilized the mentioned LLMs in three approaches: i) we used them to generate code based solely on the problem description filei ii) we employed them to fix human-written code with runtime errors using the problem description file, iii) we prompted them to correct human-written code that resulted in wrong answers, again with the problem description file. After filtering out improper outputs, we obtained a total of 2,828 AI generated code samples. Eventually, AIGCodeSet consists of 7,583 (=4,755 + 2,828) data code samples. Furthermore, we conduct experiments on our dataset with baseline AI-generated code detection systems and share their results for different LLMs and LLM usage scenarios.\nIn the future, we plan to extend our dataset covering more programming languages, LLMs, and programming tasks to make our dataset more useful for effective training and reliable evaluation.\nWe also plan to cover more usage scenarios such as blended codes where LLMs are used to generate a portion of the code. Furthermore, we will conduct a user study across students and software developers on how they use LLMs to generate code to identify realistic LLM usage scenarios."}]}