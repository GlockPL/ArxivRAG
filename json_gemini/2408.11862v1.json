{"title": "Sentiment analysis of preservice teachers' reflections\nusing a large language model", "authors": ["Yunsoo Park", "Younkyung Hong"], "abstract": "In this study, the emotion and tone of preservice\nteachers' reflections were analyzed using sentiment analysis with\nLLMs: GPT-4, Gemini, and BERT. We compared the results to\nunderstand how each tool categorizes and describes individual\nreflections and multiple reflections as a whole. This study aims to\nexplore ways to bridge the gaps between qualitative, quantitative,\nand computational analyses of reflective practices in teacher\neducation. This study finds that to effectively integrate LLM\nanalysis into teacher education, developing an analysis method\nand result format that are both comprehensive and relevant for\npreservice teachers and teacher educators is crucial.", "sections": [{"title": "I. INTRODUCTION", "content": "In teacher education, field experiences that preservice\nteachers have in classrooms provide them with opportunities to\napply their learned content and pedagogical knowledge and to\ndevelop professional dispositions by teaching and working in\nreal-world settings [1], [2], [3]. Responding to the call for\nincreasing field experiences, teacher preparation programs\naround the world have extended the time that preservice teachers\nspend in classrooms [4]. However, as Zeichner [5] points out,\nthe increased quantity of field experience itself does not ensure\nthe quality of learning or provide adequate and responsive\nsupport that helps teachers improve. From Dewey's [6]\nperspective, reflection is a process of identifying a problem,\nconsidering strategies, and engaging in a recursive process. This\nfoundational perspective has influenced the use of reflective\napproaches, such as reflection essays or reflective discussions,\nwhich are often employed alongside field experiences. These\nreflective methods also serve as formative assessment tools to\nmonitor and evaluate the complex nature of field experiences\nand the learning processes of preservice teachers. Feucht et al.\n[7] highlight that reflection is not merely a theoretical or\nconceptual endeavor; rather, it includes actions aimed at\nimplementing approaches to solve identified problems. Scholars\nhave emphasized the importance of teacher educators' guidance\nand support in facilitating pedagogical knowledge and\nreflections in teacher education [1], [8], [9], [10]. For example,\nas Shandomo [11] illustrates, reflective practice should be\ndesigned for preservice teachers to critically reflect on their\nexperiences with an understanding of themselves, others, and\nbroader curricular and social contexts. Ulusoy [12] also argues\nthat teacher education should teach preservice teachers to\nanalyze the complex and complicated relationships involved in\nteaching practice for reflection. This is important in moving\nbeyond superficial retrospective discussions of self, mentor\nteachers, and student-related issues in an individualized manner.\nIn this study, the emotion and tone of preservice teachers'\nreflections are analyzed using sentiment analysis with Large\nLanguage Model(LLM)s: GPT-4, Gemini, and BERT. We\ncompare the results to understand how each tool categorizes and\ndescribes individual reflections and multiple reflections as a\nwhole. This study aims to explore ways to bridge the gaps\nbetween qualitative, quantitative, and computational analyses of\nreflective practices in teacher education. Through the use of\nsentiment analysis with LLMs, we explore possibilities for\nnuanced and systemic analysis of preservice teachers'\nexperiences as reflected in their writings. Given the ongoing\ndiscussion on the ethics of Artificial Intelligence (AI) [13], [14],\n[15] surrounding Al development and utilization, we also focus\non identifying biases and systemic oppressions that might be\ninherent in current Al models."}, {"title": "II. RELATED LITERATURE REVIEW", "content": "Preservice teachers' reflective writing has been analyzed\nusing various methodologies, not limited to certain ontological\nand epistemological perspectives, ranging from quantitative\nmethods to phenomenological approaches. Zhang et al. [16]\nconsider two broad methodological focuses in the analysis of\nreflective writing. One approach is manual analysis utilizing a\ntheoretical framework, and the other is computational analysis\nutilizing technologies. The authors believe that analysis based\non a theoretical framework provides a deeper understanding of\nreflective practice, while computational analysis tends to rely on\ndictionary definitions and following set rules.\nQualitative analysis of reflections has ranged from discourse\nanalysis [17], case study [18], qualitative interpretive study [19],\nto phenomenological inquiry [20]. For example, Pedro [19]\nutilized a qualitative interpretive study to understand the\nmeaning of reflective practice. This study illustrates the learning\npractice within contexts, interpreting the meaning of the\ndevelopment of professional identities through reflective\npractice. On the other hand, Yee et al.'s [18] case study, using\nthe six-stage framework, employed several quantitative methods,\nincluding sampling and quantifying the results of content\nanalysis.\nResearch that used quantitative and/or computational\nanalysis includes quantitative content analysis [21], text-mining\ntechniques [22], descriptive statistical analysis [23], and a pre-\ntrained language model (BERT) [24]. For instance, Chen et al.\n[22] utilized text-mining techniques to analyze broad themes\nand patterns in preservice teachers' reflections, and the study\nidentified the preservice teachers' interests and focus, as well as\nthe level of reflection based on the lengths of reflections.\nPoldner et al.'s [21] quantitative content analysis procedure\n(QCA) study focused on formatively evaluating student teachers'\nreflective process and compared the thoughts and engagements\nduring two semesters. Zhang et al.'s [16] mixed-method\napproach, using qualitative and computational analysis,\nprovides insights into a nuanced and multidimensional\nunderstanding of preservice teachers, thereby overcoming the\nshortcomings of existing studies on preservice teacher reflection\nthat rely solely on qualitative or quantitative inquiry."}, {"title": "III. METHODS", "content": "A. Data\nThe written reflections were collected from an elementary\nteacher education course in the Midwestern United States. These\nreflections were completed as part of preservice teachers'\ncoursework over a semester, along with their field experience in\na local elementary school.\nThe collected reflections underwent preprocessing by\nremoving titles, reflection prompts, headers, and footers and\nwere then converted into single paragraphs. Each reflection was\nappended to a row in the Pandas DataFrame. For data\npersistence, the Pandas DataFrame was stored in the form of a\nbinary pickle file, Python object serialization[25].\nB. Large language model (LLM)\nThe LLMs used in this study are GPT-4, Gemini, and BERT.\n1) Generative Pre-trained Transformer-4 (GPT-4): The\nGPT-4 is OpenAI's fourth large-scale, multimodal language\nmodel and was released on March 14, 2023. For more details,\nsee [26]. This research used the latest GPT-4 version, the gpt-\n4-turbo-preview model, for sentiment analysis.\n2) Gemini 1.5: On February 15, 2024, Google DeepMind\nannounced Gemini 1.5, which is a multimodal language model\nincluding a sparse mixture-of-experts architecture. For more\ndetails, see [27]. This research used the Gemini-1.0-pro model\nfor sentiment analysis.\n3) Bidirectional Encoder Representations from\nTransformers (BERT): The research paper for BERT was\npublished by a research team of Google Al Language in\nOctober 2018. For more details, see [28]. This research used the\nTweetNLP model, which is an improved model for sentiment\nanalysis based on BERT [29].\nC. Sentiment analysis process\nEach LLM was called through the provided API. Tone\nanalysis and emotion analysis were performed on the stored\npickle file. The prompt instructed by GPT-4 and Gemini to\nperform sentiment analysis is as follows.\ninstruction = \"\"\"\n*****\nYour task is to evaluate the level of tone(emotion) of the\nauthor in a given reflective journal text.\n/***\n*\n*\n*\n*\nTone is defined as \"tone is the author's attitude toward a\nsubject.\"\n(Emotion is defined as \"mood is how we are made to feel\nas readers or the emotion evoked by the author.\")\n*\n*\nYou should give the text a numeric grade between 0 and 2\nfor tone(emotion).\n*\n*\nTone:\n2. The text has a positive tone, such as optimism and\nadvertising.\n*\n*\n*\n1. The text has a neutral tone, such as factual and balanced.\n0. The text has a negative tone, such as pessimistic and\ncriticizing.\nEmotion:\n* 2. The text has positive emotions, such as joy, anticipation,\nand happiness.\n*\n1. The text has neutral emotions, such as surprise,\nconfusion, or nothing.\n*\n0. The text has negative emotions, such as frustration, fear,\nanger, disappointment, or outrage.\n*\n*\n*\n[Answer with a two-digit decimal number in the 0-2 range,\nfollowed by a semi-colon,\nand then a brief motivation and keywords with the reason\nwhy the author's tone is scored,\n*\nand then briefly summarize the text, focusing on the tone of\nthe text.\nFor instance: \"1.23; The text shows positive tone(emotion)\ntoward a subject matter.\n*\n*\n*\n*\nThese keywords reveal or are linked to the tone (keyword1,\nkeyword2, keyword3, keyword4).\nAnd brief summarization.\" Do not use quotation marks.]\n\"\"\"\nBERT conducted a reflection analysis by calling the function\nfor tone and motion analysis in the API.\nTo ensure the independence of sentiment analysis for each\nreflection, each analysis was performed at a time interval of 2\nseconds. In addition, each reflection was randomly selected by\nthe \"pandas.DataFrame.sample\" function and analyzed to\nprevent the analysis of the previous reflection from affecting the\nanalysis of the next reflection, as the analysis is performed\nsequentially. After the analysis for each reflection was\ncompleted, all individual reflections were integrated into one\ntext, and the analysis for the entire reflection was performed. In\nthe integration process, to minimize the influence of sequential\nanalysis, the analysis was performed by integration in the order\nselected in the individual analysis. Tone and emotion analysis\nusing each LLM was repeated five times, and the results were\ncompared."}, {"title": "IV. RESULTS", "content": "A. Sentiment analysis for overall reflection\nTable 1 presents the results for emotion and tone from the\nsentiment analysis performed on the overall reflections. The\nassigned scores are 0 for negative, 1 for neutral, and 2 for\npositive. 'Motivation' represents the result of the written\nanalysis of emotion and tone. For BERT, the output structure\ndiffers from that of GPT-4 and Gemini. The model provides the\nprobabilities for each category of emotion and tone.\nIn tone analysis, GPT-4 represented the data as a mixture of\nneutral and positive tones with a score of 1.5. Gemini analyzed\nit as generally positive with a score of 1.89, while BERT\nidentified it as neutral. GPT-4 interpreted the neutral tone as 'the\nauthors reflect on their experiences and observations in the\nschool field and analyze all the challenges and strengths\nobserved in educational practice, student behavior, and the\nschool's community.' The analysis revealed mixed positive\ntones, indicating 'the authors discuss the possibility of\nimproving the educational approach, understand the positive\naspects of the learning environment, and express their\nanticipation for future class practice.' Gemini highlighted\nnegative aspects of the learning environment and expressed\nanticipation for future practice, stating \u2018it refers to the system's\nshortcomings, such as the emphasis on standardized tests and the\nlack of time for meaningful math classes.' Meanwhile, the\nmodel placed more emphasis on a positive tone, describing 'the\ntext shows a positive tone about the students' potential, and\noverall, the authors' tone is one of anticipation and optimism.'\nBERT described the reflections as neutral with a high\nprobability, while both positive and negative tones had low\nprobabilities. Despite subtle differences, we observed that the\nprobability of a positive tone is slightly higher than that of a\nnegative tone.\nEmotion analysis revealed significant differences in scores\namong the three models. GPT-4 assigned a score of 0.75,\nanalyzing a mixture of neutral and negative emotions, whereas\nGemini gave a score close to positive at 1.5 points. Gemini\nemphasized the positive aspects while identifying the presence\nof some negative emotions. BERT identified optimism as the\nmost prominent emotion.\nGPT-4 found that the overall tone combined thoughtful\ncriticism with hopeful insight, reflecting a desire for\nimprovement and adaptation of teaching methods to better meet\nstudents' needs and leverage their strengths. However, the\nmodel emphasized neutral and negative emotions due to the\nconcerns and critical reflections expressed in the text. Gemini\nhighlighted positive emotions in overall reflections, such as\nexcitement and expectation of teaching and learning. The model\nalso mentioned negative emotions, specifically frustration and\nfear related to high-stakes testing and students' struggles with it.\nBERT captured feelings of optimism and anticipation,\nindicating them as having the highest probability.\nB. Sentiment analysis for individual reflection"}, {"title": "V. DISCUSSION", "content": "In comparison, GPT-4 and Gemini assigned similar scores\nfor tone but different scores for emotion. However, despite\nnumerical discrepancies, descriptive analysis results indicated\nthat both models produced similar outcomes in tone and\nemotion. In other words, while the two models may emphasize\ndifferent texts, their overall analyses are generally similar. We\nview that BERT is more suitable for expressing the overall tone\nand emotion of text in understandable numbers and\nprobabilities, but it is weaker in descriptive analysis due to\nlimited result description. However, BERT's emotion analysis\nresult, indicating 'the probability of optimism and anticipation\nwas the highest,' aligns with Gemini's tone analysis result,\nstating 'the authors' tone is one of anticipation and optimism.'\nThis suggests that the results from all three models are generally\nconsistent.\nA. Scores of tone and emotion\nIn the analysis conducted for each model, performed five\ntimes, the score expressed on a scale showed a somewhat\ndifferent form of expression compared to human common sense.\nIn the prompt provided for the analysis, a negative tone or\nemotion was to be rated on a scale of 0 for negative, 1 for neutral,\nand 2 for positive. Humans tend to understand tone and emotion\nas a spectrum, meaning they would likely assign a rating close\nto 2 for positive texts and close to 0 for negative ones. However,\nwe found that LLM's scoring approach differs from human\nperception and understanding of emotion and tone.\nThe results indicate that LLM analyzes the similarity or\nprobability of each positive, neutral, and negative by treating\nthem as distinct categories rather than as a spectrum. LLMs\ndetermine that each positive, neutral, and negative emotion is an\nindependent category and do not recognize a concept of\nneutrality between the negative and positive categories. This\nfinding suggests that BERT's output form, which provides\nprobabilities for each emotion, is a more suitable output form for\nsentiment analysis using LLM.\nB. Fine-tuned model for sentiment analysis for reflections of\npreservice teachers\nThis study employed a pre-trained model because the\nquantity of reflection data for the sentiment analysis was\ninsufficient to fine-tune LLMs. To successfully integrate LLM\nanalysis into teacher education, it is crucial to develop an\nanalysis approach and result format that are comprehensive and\nrelevant for preservice teachers and teacher educators. As\nindicated by Gemini's results, LLMs need to be fine-tuned for\nspecific contexts and purposes, aligning with the perspectives\nand directions pursued in teacher education. Considering that the\nresults from BERT, which specializes in sentiment analysis\nbased on data from Twitter, were relatively consistent, we can\nanticipate that Gemini's results could become more consistent\nand relevant as it is fine-tuned with a large dataset of reflections\nfrom preservice teachers."}, {"title": "VI. CONCLUSION", "content": "Given the key role of reflection in teacher education, it is\nimportant for teacher educators to provide timely and adequate\nfeedback on the reflective process. Our study has shown that\nLLMs have the potential to support preservice teachers in\ndeveloping their professional practice within the context of\nlarger social discourse and the reflective processes of others.\nHowever, we recognize the pitfall of LLMs in that they simplify\nand miss nuances when it comes to the complexity of the lived\nexperiences of preservice teachers. For example, if an analysis\nfocuses solely on literal meaning and numerical domination of\ncertain emotions and sentiments labeled as negative, concerning\nstandardized testing and the classroom/school environment, it\nwill miss the nuance of motivation, inspiration, and hope for\ntheir future practice. In contrast, the domination of positive\nemotions and sentiments does not necessarily mean that\npreservice teachers are developing pedagogical practices\nresponsive to their students and social contexts. When one is\nanalyzing the process of becoming an educator, it is crucial for\nan analytical model and tool to avoid reaching hasty conclusions\nwhile still being able to capture the nuances and relational\naspects of teaching practice and the larger social and educational\ncontexts."}]}