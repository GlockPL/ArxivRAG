{"title": "SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph Attention for Vision Transformers", "authors": ["Shravan Venkatraman", "Jaskaran Singh Walia", "Joe Dhanith P R"], "abstract": "Image classification is a computer vision task where a model analyzes an image to categorize it into a specific label. Vision Transformers (ViT) improve this task by leveraging self-attention to capture complex patterns and long-range relationships between image patches. However, a key challenge for ViTs is efficiently incorporating multi-scale feature representations, which is inherent in CNNs through their hierarchical structure. In this paper, we introduce the Scale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework that addresses this challenge by integrating multi-scale features. Using EfficientNet as a backbone, the model extracts multi-scale feature maps, which are divided into patches to preserve semantic information. These patches are organized into a graph based on spatial and feature similarities, with a Graph Attention Network (GAT) refining the node embeddings. Finally, a Transformer encoder captures long-range dependencies and complex interactions. The SAG-ViT is evaluated on benchmark datasets, demonstrating its effectiveness in enhancing image classification performance.", "sections": [{"title": "1. Introduction", "content": "The field of image classification has experienced significant advancements with the introduction of deep learning architectures. CNNs have long been the foundation for image classification tasks due to their proficiency in capturing local spatial hierarchies through convolutional operations [9]. However, their inherent limitations in modeling long-range dependencies restrict their ability to fully exploit global contextual information within images [12]. The introduction of Vision Transformers (ViT) [4, 18] has opened new avenues by leveraging self-attention mechanisms to model global relationships within images. ViTs treat images as sequences of patches (tokens) and have demonstrated competitive performance compared to traditional CNNs. Despite their success, ViTs often require large-scale datasets for effective training and may overlook fine-grained local details due to their fixed-size patch tokenization [17].\nRecent research has highlighted the importance of multi-scale feature representations in enhancing ViTs' performance across various vision tasks [1]. Multi-scale approaches enable models to capture objects and patterns of varying sizes, providing a more comprehensive understanding of the image content. While CNNs inherently capture multi-scale features through hierarchical layers, integrating this capability efficiently into Transformer-based models remains a challenge.\nTo handle this challenge, we propose a novel Transformer-based framework called Scale-Aware Graph Attention Vision Transformer (SAG-ViT). Our model begins by extracting rich, multi-scale feature maps from input images using a pre-trained EfficientNet backbone [16]. We then divide these feature maps into patches, preserving high-level semantic information and reducing information loss compared to raw image patching. We then construct graphs where each node represents a feature map patch, and edges are established based on spatial adjacency and feature similarity using a k-connectivity scheme. This graph captures both local and global relationships among image regions. A Graph Attention Network (GAT) [19, 24] processes the graph, dynamically focusing on the most relevant patches. The enriched node embeddings are then passed through a Transformer encoder, which captures long-range dependencies and complex interactions.\nOur contributions are summarized as follows:\n\u2022 We introduce a patching mechanism that operates on CNN-derived feature maps, retaining rich semantic information and efficiently capturing multi-scale features.\n\u2022 A k-connectivity and similarity-based edge weighting scheme is developed in the proposed Transformer architecture to construct graphs that model intricate spatial relationships between patches.\n\u2022 We employ a GAT Network to process the information-rich graph embeddings to effectively model both local and global dependencies within images.\n\u2022 We validate our method on multiple benchmark datasets across different domains, demonstrating higher performance compared to other transformer-based approaches.\nThe remainder of this paper is organized as follows: Section 2 reviews related work on graph transformers, attention mechanisms, multi-scale feature embedding, and their integration in image classification. Section 3 details our proposed method, including the architecture and graph construction process. Section 4 presents the experimental setup, datasets, and evaluation metrics. Section 5 discusses the results, and Section 6 concludes the paper."}, {"title": "2. Literature Survey", "content": "In this section, we review relevant literature on vision transformers, multi-scale feature representation, and graph neural networks for image classification."}, {"title": "2.1. Vision Transformers for Image Classification", "content": "Transformer-based models have gained significant attention in computer vision, initially popularized by the Vision Transformer (ViT), which treats images as sequences of patches and uses self-attention to capture global dependencies, achieving competitive results with CNNs for image classification [24]. However, ViT models often require large datasets and substantial computational resources, limiting their accessibility.\nTo improve data efficiency, DeiT leverages distillation and data augmentation, enabling ViTs to perform well on smaller datasets [10]. T2T-ViT [25] introduces a Tokens-to-Token transformation to better capture local structures, addressing ViT's limitation of naive tokenization. The Perceiver model uses an asymmetric attention mechanism to distill large inputs into a compact latent space, allowing it to scale effectively for high-dimensional data [8]. Similarly, PVT and CvT incorporate pyramid-like structures into transformers, merging CNN-like multi-scale processing with transformer advantages for richer feature extraction [23].\nThe Swin Transformer introduces a shifting window approach to self-attention, efficiently capturing both local and global contexts while maintaining manageable complexity, especially for dense tasks like segmentation and detection [11]. These models highlight a growing trend toward integrating multi-scale representations to improve vision transformers' ability to capture both fine-grained details and long-range dependencies."}, {"title": "2.2. Multi-Scale Feature Representation", "content": "Multi-scale feature representations are critical for recognizing objects and patterns at varying scales [1]. CNNs naturally capture multi-scale features through their hierarchical layers and receptive fields [9]. Techniques such as feature pyramid networks [10] and multi-branch architectures [2] have been proposed to enhance multi-scale learning in CNNs.\nIn the context of transformers, incorporating multi-scale features remains challenging due to the fixed-size patch tokenization. CrossViT [1] introduces a dual-branch transformer architecture that processes image patches of different sizes in separate branches, fusing them using cross-attention mechanisms. This approach effectively captures both fine-grained details and global context."}, {"title": "2.3. Graph Neural Networks for Image Classification", "content": "Graph Neural Networks have gained attention for their ability to model relational data. In image classification, representing images as graphs allows for capturing spatial relationships between different regions [24]. Nodes can represent super pixels or patches, and edges encode similarities or spatial connections. Constructing graphs directly from raw images can lead to information loss due to the reduction in spatial resolution [26]. By constructing graphs from CNN-derived feature maps, richer semantic information can be retained [5]. This approach enhances the modeling of complex spatial dependencies crucial for accurate classification.\nGraph Attention Networks extend the concept of attention mechanisms to graph-structured data [19]. GATs compute attention coefficients for neighboring nodes, allowing the network to focus on the most relevant connections. This dynamic weighting improves the learning of node representations by emphasizing important relationships. Incorporating GATs in image classification enables the modeling of both local and non-local dependencies [22]. When combined with multi-scale feature representations, GATs can effectively capture intricate patterns within images."}, {"title": "2.4. Hybrid Models", "content": "Recent studies suggest that combining transformer and convolutional layers into a hybrid architecture can harness the strengths of both approaches. BoTNet [15] modifies self-attention in the final three blocks of ResNet to integrate both architectures. The CMT [7] block incorporates depthwise convolutional layers for local feature extraction, alongside a lightweight transformer block. CvT [11] places pointwise and depthwise convolutions before the self-attention mechanism to enhance performance. LeViT [6] replaces the patch embedding block with a convolutional stem, enabling faster inference for image classification. MobileViT [13] combines Transformer blocks with the MobileNetV2 [14] block to create a lightweight vision transformer. Mobile-Former [3] bridges CNNs and transformers in a bidirectional manner to capitalize on both global and local features."}, {"title": "3. Method: SAG-ViT", "content": "In this section, we detail our proposed approach to enhance transformer performance for image classification through a multiscale feature embedding and high-fidelity graph attention-based patching. During graph construction in graph transformers, spatial hierarchies are often lost or insufficiently represented, especially as redundant or less relevant areas dilute the image's contextual representation. To overcome this limitation, we propose a novel framework that captures both local and global dependencies while preserving rich semantic information. Specifically, we begin by outlining our high-fidelity feature map patching strategy (\u00a73.1). We then detail the graph construction methodology based on k-connectivity and feature similarity (\u00a73.2). Finally, we explain the integration of Graph Attention Networks with Transformer encoders (\u00a73.3). Figure 1 illustrates the network architecture of our proposed Scale-Aware Vision Transformer with Graph Attention (SAG-ViT)."}, {"title": "3.1. High-Fidelity Feature Map Patching", "content": "We initiate the processing pipeline by extracting high-fidelity patches from feature maps generated by a lightweight convolutional backbone. By operating on feature maps rather than raw images, we retain higher-level semantic information. We process the input image $I \\in \\mathbb{R}^{H\\times W\\times C}$ through a deep CNN to exploit its compound multiscale feature scaling for receptive fields and efficient convolution paths, yielding a feature map $F \\in \\mathbb{R}^{H'\\times W'\\times D}$, where $H' = \\frac{H}{s}$, $W' = \\frac{W}{s}$, and $D$ denotes the depth of the feature channels with stride s.\nTo preserve detailed and multi-scale semantic information, we partition the feature map F into non-overlapping patches $P_{i,j} \\in \\mathbb{R}^{k\\times k\\times D}$, where k is the spatial dimension of each patch. Formally, the patch extraction is defined as:\n$P_{i,j} = F[i \\cdot k: (i + 1) \\cdot k, j \\cdot k : (j + 1) \\cdot k, :]$,\nfor all $i \\in \\{0,..., \\frac{H'}{k} - 1\\}$ and $j\\in \\{0,..., \\frac{W'}{k}-1\\}$.\nThis operation can be represented using an unfolding operator $U_k$:\n$U_k(F) = \\{P_{i,j} | P_{i,j} = F [i \\cdot k : (i + 1) \\cdot k, j \\cdot k : (j + 1) \\cdot k, :]\\}$.\nfor all $i = 0,..., \\frac{H'}{k}-1$ and $j = 0,..., \\frac{W'}{k}-1$.\nwhere $U_k : \\mathbb{R}^{H'\\times W'\\times D} \\rightarrow \\mathbb{R}^{\\frac{H'}{k} \\times \\frac{W'}{k} \\times k \\times k \\times D}$. Each patch $P_{i,j}$ is then vectorized into a feature vector $p_{i,j} \\in \\mathbb{R}^{k^2D}$ by flattening the spatial and channel dimensions:\n$p_{i,j} = vec(P_{i,j})$.\nThis results in a collection of patch vectors:\n$P = \\bigcup_{i=0}^{\\frac{H'}{k}-1} \\bigcup_{j=0}^{\\frac{W'}{k}-1} \\{p_{i,j} \\}$.\nBy extracting patches directly from the feature map F, we leverage the high-level abstractions learned by the CNN. This approach ensures that each patch $P_{i,j}$ encapsulates rich semantic information, capturing both local patterns and contextual relationships within the image. Moreover, extracting patches from the reduced spatial dimensions $H' \\times W'$ leads to fewer patches P, decreasing computational complexity while maintaining essential information.\nThe vectorized patches $p_{i,j}$ serve as nodes in the subsequent graph construction phase. The high-dimensional feature vectors facilitate the capture of intricate relationships between patches when constructing edges based on similarity measures. Additionally, the non-overlapping nature of patch extraction ensures that each patch maintains its spatial locality within the feature map, preserving the inherent spatial structure essential for accurate image classification.\nThis mathematical formulation ensures that the patch extraction process is both systematic and scalable, facilitating efficient downstream processing in the graph-based classification pipeline."}, {"title": "3.2. Graph Construction Using k-Connectivity and Similarity-Based Edges", "content": "Once the patches $P = \\{p_{i,j}\\}$ are extracted, we construct a graph $G = (V, E)$ to model the spatial and feature-based relationships among them. Here, $V = \\{v_{i,j}\\}$ represents the set of nodes corresponding to patches, and E denotes the set of edges connecting these nodes. Each node $v_{i,j} \\in V$ is associated with a feature vector $X_{i,j} = p_{i,j} \\in \\mathbb{R}^{Cp^2}$, where each patch of size (p,p) is vectorized into a $Cp^2$-dimensional feature vector. After extracting all patches, we organize them into a matrix\n$X_v = [x_1, x_2,...,x_{|V|}]^T \\in \\mathbb{R}^{|V| \\times Cp^2}$,\nwhere |V| is the number of patches (nodes) in the graph.\nNext, we define the edges $e_{u,v} \\in E$ based on k-connectivity and feature similarity. For each patch $p_i \\in V$, we consider its neighboring patches, which are spatially adjacent to it within the feature map. A patch pi is connected to its neighboring patches $p_j$, where $p_j \\in \\mathcal{N}(p_i)$ represents the set of neighbors of patch $p_i$. The neighborhood $\\mathcal{N}(p_i)$ is determined by the spatial adjacency of patches, considering a fixed local window size k around each patch. The adjacency matrix $A \\in \\mathbb{R}^{|V|\\times|V|}$ is defined as:"}, {"title": "3.3. Integration of Graph Attention Networks (GAT) with Transformer Encoders", "content": "After constructing the graph $G = (V, E)$, we employ a Graph Attention Network (GAT) to process the node features and capture fine-grained dependencies among patches. Integrating GAT with transformer encoders facilitates the modeling of both local and global interactions, enhancing the discriminative power of the feature representations. The attention mechanism in GAT dynamically assigns weights"}, {"title": "3.4. Ablation Study", "content": "To rigorously evaluate the contributions of each component in our proposed architecture, we conducted a comprehensive ablation study. This analysis aims to discern the individual impact of the EfficientNet backbone, the Graph Attention Network (GAT), and the Transformer encoder on the model's overall performance. By systematically removing or altering components, we can quantify their significance and validate the theoretical underpinnings of our design choices."}, {"title": "3.4.1 Experimental Setup", "content": "We designed three ablation experiments on the CIFAR-10 dataset to isolate the effects of each component:\n1. Backbone + GAT (No Transformer): In this configuration, we exclude the Transformer encoder, allowing us to assess the role of the Transformer in capturing global dependencies. The model processes the feature embeddings extracted by the EfficientNet backbone through the GAT, generating class predictions directly from the aggregated node representations.\n2. Backbone + Transformer (No GAT): Here, we omit the GAT to evaluate its contribution in modeling local dependencies and refining node features. The feature embeddings from the backbone are fed into the Transformer encoder, which attempts to learn both local and global relationships without the explicit attention mechanism provided by the GAT.\n3. GAT + Transformer (No Backbone): In this scenario, we remove the EfficientNet backbone to determine its impact on feature representation. Randomly initialized embeddings are used as input to the GAT and Transformer, highlighting the importance of high-quality feature extraction."}, {"title": "4. Results", "content": "In this section, we present a comprehensive evaluation of our proposed model across five diverse benchmark datasets: CIFAR-10, GTSRB, NCT-CRC-HE-100K, NWPU-RESISC45, and PlantVillage. These datasets encompass a wide range of domains, including natural images, traffic sign recognition, histopathological images, remote sensing data, and agricultural imagery. The diversity of these datasets allows us to thoroughly assess the effectiveness and generalization capability of our model across different types of image data."}, {"title": "4.1. Overall Performance", "content": "Our proposed model demonstrates superior performance compared to state-of-the-art architectures across all evaluated datasets. Table 1 summarizes the F1 scores achieved by our model and various baseline models utilizing different backbones.\nAnalyzing the results, our proposed model consistently outperforms the baseline models across all datasets. On CIFAR-10, our model achieves an F1 score of 0.9574, surpassing the next best model (ResNet-based within our architecture) by approximately 4.02%. This significant improvement underscores the effectiveness of integrating EfficientNet as the backbone in our model. EfficientNet's compound scaling strategy optimizes network depth, width, and resolution, providing richer feature embeddings that enhance the model's ability to capture intricate patterns in natural images when processed through our graph-based approach.\nOn the GTSRB dataset, which involves recognizing traffic signs under various challenging conditions, our model attains an F1 score of 0.9958. This is a notable improvement over the DenseNet201-based variant, which achieves 0.9862. The 0.96% increase, though seemingly modest due to the high baseline performance, demonstrates our model's superior ability to capture subtle variations in traffic signs, crucial for real-world traffic sign recognition tasks.\nFor the NCT-CRC-HE-100K dataset, consisting of histopathological images for colorectal cancer classification, our model achieves an F1 score of 0.9861, outperforming the ResNet-based variant's score of 0.9478 by approximately 3.83%. This substantial improvement indicates that the EfficientNet backbone, combined with our graph-based processing, effectively captures complex tissue structures, enhancing the model's discriminative power in medical image analysis.\nOn the NWPU-RESISC45 dataset, which includes remote sensing images from various land-use scenes, our model achieves an F1 score of 0.9549, outperforming the ResNet-based variant by 4.46%. This result demonstrates the model's ability to capture spatial relationships and patterns inherent in remote sensing data more effectively than other backbones within our architecture.\nLastly, on the PlantVillage dataset, our model records an F1 score of 0.9772, significantly higher than the ResNet-based variant's score of 0.8905, marking an improvement of approximately 8.66%. This considerable enhancement underscores the effectiveness of our model in agricultural imagery, particularly in detecting and classifying plant diseases where subtle visual cues are critical.\nWe evaluated our model's performance on the underwater trash dataset [20], benchmarking it against state-of-the-art algorithms (excluding backbone models) such as YOLOv8, RCNN, Fast-RCNN, and Mask-RCNN. Our model consistently performed better with a validation F1 of 0.96, exceeding the benchmark results of these models [21].\nComparing our model with standalone Vision Transformers (ViT-S and ViT-L), which do not incorporate our graph-based enhancements, we observe that while ViT models perform competitively on some datasets, they generally lag behind our proposed model. For instance, on CIFAR-10, ViT-L achieves an F1 score of 0.8637, which is 9.35% lower than our model's performance. This comparison highlights the advantage of our approach in integrating EfficientNet for feature extraction with graph attention mechanisms and Transformer encoding, providing a more comprehensive understanding of the data."}, {"title": "4.2. Hardware Efficiency", "content": "We also evaluated the hardware efficiency of our proposed model in terms of RAM and GPU VRAM usage.\nOur proposed model demonstrates competitive resource utilization, especially considering its superior performance. On CIFAR-10, our model uses 7.24% RAM, which is lower than several other variants using different backbones, such as the VGG16-based version that consumes 11.5% RAM. This indicates that incorporating EfficientNet in our architecture not only enhances performance but also improves hardware efficiency.\nRegarding GPU VRAM usage, our model maintains moderate consumption."}, {"title": "4.3. Ablation Study", "content": "To evaluate the contribution of each component in our proposed model, we conducted an ablation study on the CIFAR-10 dataset.\nWhen the model includes the EfficientNet backbone with the GAT but without the Transformer encoder, the F1 score drops to 0.7785. This significant decrease underscores the crucial role of the Transformer encoder in capturing global dependencies and enhancing classification accuracy. The self-attention mechanism in the Transformer allows the model to weigh the importance of all patches relative to each other, facilitating a holistic understanding of the image.\nConversely, using the EfficientNet backbone with the Transformer encoder but without the GAT results in an F1 score of 0.7593. This emphasizes the importance of the GAT in refining local feature representations before global processing. The GAT enhances node features by aggregating information from immediate neighbors, effectively capturing local structural information essential for accurate classification.\nWhen the model comprises the GAT and Transformer encoder without the EfficientNet backbone, the F1 score drops drastically to 0.5032. This significant decline highlights the importance of the EfficientNet backbone in providing rich and discriminative feature embeddings necessary for effective graph construction and subsequent processing."}, {"title": "4.4. Discussion", "content": "The results validate our hypothesis that integrating efficiently scaled feature embeddings with graph-based attention mechanisms and Transformer encoders significantly enhances model performance across diverse datasets. The EfficientNet backbone within our architecture provides superior feature representations, which, when used in our graph construction, enhance the model's ability to capture both local and global dependencies effectively.\nOur comparisons are designed to showcase the critical role that EfficientNet plays within our proposed model. The substantial improvements over models with other backbones highlight that the enhancements result from the synergistic integration of EfficientNet with our graph-based approach, rather than from the backbone alone. This integration allows for richer feature representations that, when processed through the GAT and Transformer encoder, lead to improved classification accuracy.\nThe ablation study confirms that the removal of any component leads to a significant drop in performance, establishing that the improvements are due to the cohesive integration of these elements within our architecture. By quantitatively demonstrating the impact of each component, we validate the architectural choices grounded in principles of deep learning, graph theory, and attention mechanisms.\nMoreover, the inclusion of diverse datasets such as GTSRB, NCT-CRC-HE-100K, NWPU-RESISC45, and PlantVillage demonstrates the robustness and generalization ability of our model across different domains. These datasets present various challenges, including fine-grained classification, medical image analysis, remote sensing, and agricultural disease detection. Our model's consistent superiority across these datasets emphasizes its versatility and effectiveness in handling complex and varied image data.\nIn conclusion, our proposed model, integrating EfficientNet as the backbone within a graph-based framework enhanced by attention mechanisms, significantly outperforms existing models. The improvements stem from the unique combination of high-quality feature extraction, graph-based representation of spatial relationships, attention mechanisms for local dependencies, and Transformer encoding for global context. This holistic approach ensures that our model effectively captures and utilizes the rich semantic information necessary for accurate image classification across diverse datasets."}, {"title": "5. Conclusion", "content": "This paper presents the Scale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework designed to address the challenge of multi-scale feature representation in Vision Transformers. By utilizing EfficientNet for feature extraction and organizing image patches into a graph, SAG-ViT effectively captures both local and global relationships in images. The incorporation of a Graph Attention Network (GAT) refines the node embeddings, while the Transformer encoder captures long-range dependencies and complex interactions. Experimental evaluations on benchmark datasets, including CIFAR-10, GTSRB, NCT-CRC-HE-100K, NWPU-RESISC45, and PlantVillage, demonstrate the model's effectiveness, showing significant improvements in image classification performance. Additionally, an ablation study provides insights into the importance of each component in the SAG-ViT framework, helping to understand their individual contributions to the overall performance. This work highlights the potential of integrating multi-scale features and graph-based attention mechanisms to enhance the capabilities of Transformer-based models in computer vision."}]}