{"title": "TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep Reinforcement Learning", "authors": ["Chengkai Xu", "Jiaqi Liu", "Shiyu Fang", "Yiming Cui", "Dong Chen", "Peng Hang", "Jian Sun"], "abstract": "Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs) each show promise in addressing decision-making challenges in autonomous driving, DRL often suffers from high sample complexity, while LLMs have difficulty ensuring real-time decision making. To address these limitations, we propose TeLL-Drive, a hybrid framework that integrates a Teacher LLM to guide an attention-based Student DRL policy. By incorporating risk metrics, historical scenario retrieval, and domain heuristics into context-rich prompts, the LLM produces high-level driving strategies through chain-of-thought reasoning. A self-attention mechanism then fuses these strategies with the DRL agent's exploration, accelerating policy convergence and boosting robustness across diverse driving conditions. The experimental results, evaluated across multiple traffic scenarios, show that TeLL-Drive outperforms existing baseline methods, including other LLM-based approaches, in terms of success rates, average returns, and real-time feasibility. Ablation studies underscore the importance of each model component, especially the synergy between the attention mechanism and LLM-driven guidance. Finally, we build a virtual-real fusion experimental platform to verify the real-time performance, robustness, and reliability of the algorithm running on real vehicles through vehicle-in-loop experiments. Full validation results are available on Our Website.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving technology has made significant advancements over the past decade, emerging as a transformative force poised to revolutionize the transportation sector [1], [2]. By promising enhanced safety, reduced traffic congestion, and increased mobility accessibility, autonomous vehicles (AVs) are set to redefine the landscape of modern transportation. Central to the operational efficacy of AVs is their ability to perform real-time, complex decision-making that rivals or surpasses human driving capabilities. Achieving such sophisticated decision-making necessitates the integration of advanced artificial intelligence methodologies capable of perceiving, interpreting, and responding to dynamic and often unpredictable driving environments [3].\nDeep Reinforcement Learning (DRL) has emerged as a key framework for autonomous decision-making [4], [5], with its ability to develop policies for complex tasks such as navigation through intersections [6], [7] and ramp merging [8], [9]. DRL's strength lies in its capacity to learn from experience and optimize driving policies based on trial and error. However, despite its potential, traditional DRL methods face several challenges, including high data demands, slow convergence rates, and limited generalization across diverse and dynamic driving environments [10]. These limitations hinder the scalability and efficiency of DRL in real-world autonomous driving tasks, where adaptability, safety, and real-time performance are paramount.\nIn parallel, Large Language Models (LLMs), exemplified by architectures such as GPT-40 [11], have demonstrated exceptional proficiency in natural language understanding and contextual reasoning. Leveraging vast repositories of knowledge and advanced contextual reasoning capabilities, LLMs can provide valuable insights for decision-making processes in autonomous driving systems [12]-[15]. However, the deployment of LLMs as standalone decision-making agents faces significant barriers [13]. Specifically, LLMs struggle to ensure real-time responsiveness and exhibit a degree of randomness in their decision outputs, which are critical limitations in time-sensitive and safety-critical applications inherent to autonomous driving systems.\nTo address the limitations, we propose TeLL-Drive, a novel framework that synergistically combines the strengths of both DRL and LLMs to enhance decision-making in autonomous vehicles. By leveraging the contextual understanding and reasoning capabilities of LLMs, TeLL-Drive enhances the sampling efficiency and quality of DRL, while mitigating the data inefficiency and slow convergence typically associated with DRL. Specifically, we introduce a risk-aware LLM agent, equipped with memory, reflective, and reasoning capabilities, that provides context-sensitive guidance to the DRL agent. This enables safer, more efficient decision-making in complex and dynamic traffic scenarios. Meanwhile, the DRL agent, built on the Actor-Critic architecture, ensures robust exploration and real-time responsiveness by employing hybrid strategies, addressing the inherent randomness in LLM-driven decisions.\nAs shown in Fig. 1, the LLM function as a teacher, providing expert-level guidance and contextual insights that inform and streamline the learning process of DRL agents. Subsequently, DRL serves as the \u201cstudent\u201d, acting as the final decision maker to ensure real-time responsiveness and mitigate the randomness associated with LLM-driven decisions. The main contributions of this article are listed as follows:\n1) We introduce TeLL-Drive, a decision-making framework for autonomous driving that combines a teacher LLM with a student DRL agent, which integrates the LLM's high-level reasoning and knowledge with DRL's adaptability and computational efficiency.\n2) A risk-aware LLM agent is developed, which is endowed with memory, reflection, and reasoning capabilities, to provide context-sensitive guidance in dynamic traffic environments and enhance driving safety and efficiency.\n3) Through real-vehicle experiments in multiple scenarios, TeLL-Drive outperforms standard DRL algorithms in exploration efficiency and achieves favorable overall results compared to alternative methods."}, {"title": "II. RELATED WORKS", "content": "DRL has emerged as a promising approach for autonomous driving, spanning tasks from basic lane-keeping to complex multi-agent interactions [16], [17]. DRL algorithms, both policy-gradient-based and value-based, have demonstrated substantial performance improvements in simulated driving environments [18], [19]. These methods, by learning from interactions with the environment, can develop highly effective policies for tasks such as intersection navigation [6], obstacle avoidance, and adaptive cruise control. However, two main challenges persist in applying DRL to autonomous driving.\nFirst, DRL's reliance on extensive environment interactions often leads to high data requirements, which can be both costly and time-consuming, particularly when training agents for complex driving tasks. This not only limits scalability but also makes real-world deployment more challenging [10]. Second, DRL models generally lack transparency and interpretability, which impedes their ability to make reliable decisions in rare or out-of-distribution scenarios [20]. This lack of transparency makes DRL less reliable for safety-critical applications, such as handling unexpected or unfamiliar traffic situations.\nTo address these issues, the integration of expert knowledge through Reinforcement Learning from Human Feedback (RLHF) has been proposed [21]. RLHF allows for faster convergence and improved robustness by leveraging human expertise to guide the learning process, reducing the number of required interactions with the environment. However, RLHF comes with its own set of challenges. First, it is resource-intensive due to the need for extensive human annotations [22]. Additionally, the human feedback may not cover the full range of possible driving scenarios, limiting the agent's ability to generalize effectively to unseen situations. These limitations point to the need for a more efficient and scalable method that integrates expert guidance while addressing DRL's inherent drawbacks."}, {"title": "B. LLMs in Decision-Making", "content": "LLMs have shown considerable promise in various high-level decision-making tasks, including autonomous driving. LLMs, such as GPT-4, have demonstrated their ability to handle complex reasoning, interpretation, and contextual awareness [23]. For example, LanguageMPC [24] leverages the common-sense reasoning capabilities of LLMs to guide Model Predictive Control (MPC) parameters for autonomous vehicles. Similarly, Fu et al. [25] and Wen et al. [26] have explored the application of LLM-based reasoning, interpretation, and memory capabilities to assist autonomous decision-making, particularly in complex and dynamic traffic environments. These models help in interpreting driving scenarios and proposing context-aware strategies based on learned knowledge.\nDespite these promising developments, the practical deployment of LLMs in autonomous driving faces several limitations. One of the key challenges is the high computational cost associated with running LLMs in real-time, making it difficult to meet the responsiveness required for safety-critical applications [13]. Additionally, LLMs typically generate outputs with a degree of randomness, which can result in unpredictable actions that are unsuitable for tasks demanding consistent and reliable decision-making. This unpredictability is particularly problematic in autonomous driving, where even minor deviations from expected behavior can have serious safety implications. Thus, while LLMs offer significant potential for decision-making in autonomous driving, their practical use as standalone decision-making agents is limited by their real-time performance and output consistency."}, {"title": "C. Hybrid DRL-LLM Approaches", "content": "With the rapid development of LLMs and DRL in various fields, researchers are increasingly exploring the synergistic potential of combining these two paradigms. While numerous studies have focused on using DRL methods to optimize and fine-tune LLMs to enhance their generative capabilities and task adaptability, the utilization of LLMs to assist DRL remains relatively underexplored, particularly in the context of autonomous driving decision-making.\nExisting research has begun to investigate how the reasoning and knowledge capabilities of LLMs can improve the exploration efficiency and learning effectiveness of RL agents [27]. For example, Zhang et al. [28] developed a semi-parametric RL framework based on LLMs by configuring long-term memory modules; Similarly, Trirat et al. [29] employed LLMS to achieve full-process automated machine learning, while Ma et al. [30] realized the automatic design of reward functions in RL without requiring specific enhancement tasks. Despite these advancements, the environmental understanding capabilities of LLMs are still not fully leveraged, and effective integration between LLMs and RL remains a challenge. Current approaches lack a comprehensive methodology for combining the strengths of both LLMs and RL, resulting in an underutilized potential to improve decision-making processes in autonomous driving systems."}, {"title": "III. PROBLEM FORMULATION", "content": "We formalize the autonomous driving decision-making task as a Partially Observable Markov Decision Process (POMDP), defined by the tuple (S, A, O,T,R, \u03b3), where S is the environmental states; A is the action space; O is the observation space; T is the state transition function; R is the reward function, and y is the discount factor. The agent's objective is to learn a policy \u3160 that maximizes the expected discounted return:\n$\\max J(\\tau) = arg \\max E_{(s_t,a_t) \\sim p_{\\tau}} [\\sum_{t=0}^{\\infty} \\gamma^t r (s_t, a_t)] $ (1)\nwhere \u03b3\u2208 [0,1] balances the emphasis on immediate and future rewards.\n1) Observation space: At each time step t, the agent receives an observation ot \u2208 O composed of two parts. The first is a matrix Mt \u2208 RFk\u00d7N capturing information about up to N nearby vehicles. Each column of Mt corresponds to one vehicle, described by a feature vector:\nFk = [Xk, Yk, Uxk, Uyk, cosh(\u03b8k), sinh(0k)] (2)\nwhere (Xk, Yk) and (Vxk, Vyk) denote the position and velocity of the k-th vehicle, and cosh(0k) and sinh(0k) encode its orientation. The second part of ot is the state of the ego vehicle. By concatenating these components, the agent obtains a compact yet informative representation of the driving environment.\n2) Action space: This work focuses on leveraging LLMs to provide high-level guidance for DRL, rather than controlling low-level vehicle dynamics. Consequently, the action space A consists of five high-level maneuvers:\nA = {slowdown, cruise, speedup, turnleft,turnright} (3)\nOnce a high-level maneuver is chosen, the corresponding steering and throttle commands are generated by a lower-level PID controller, enabling the vehicle to execute lateral and longitudinal movements."}, {"title": "IV. METHODOLOGY", "content": "TeLL-Drive leverages the prior knowledge of LLMs to guide the exploration and learning of DRL agents in diverse, complex traffic scenarios. By introducing policy integration, TeLL-Drive enhances sample efficiency and optimizes learning outcomes. As illustrated in Fig. 2, the framework comprises two main components: the LLM Teacher and the DRL Student. Based on multi-module collaboration, the Teacher Agent generates robust decision through its three key modules: Decision Engine, which provides real-time guidance; Memory Repository, which stores past experiences for context; and Reflective Evaluator, which refines the guidance based on previous performance. While the Student Agent refines the Teacher's actions through a multi-head attention-based policy-integration mechanism, integrating its own exploration experiences to effectively acquire knowledge from the LLM and enhance learning efficiency and quality."}, {"title": "B. LLM Teacher", "content": "The Decision Engine begins by estimating the Time to Conflict Point (TTCP) [23] for each potential collision, using a rotation-projection method that projects the relative motion vectors of the ego vehicle and other traffic participants onto a shared reference axis. Let dego(t) and dother(t) be the positions of the ego and another vehicle at time t, vego(t) and Vother(t) be the current speed. The TTCP Tis:\nT=\\underset{t\u22650}{\\arg \\min} \\frac{||P_{ego} (t) - P_{other} (t)||}{||V_{ego} (t) - V_{other} (t)||} (4)\nThis risk metric informs immediate maneuver priorities. Simultaneously, we retrieve context from a memory repository indexing historical driving scenarios as feature vectors {zi}. For the current state zt \u2208 Rd, we retrieve the most similar scenario zi via cosine similarity, thus leveraging outcomes of analogous past experiences to guide decision-making:\nsim(zt, zi) = \\frac{Z_tZ_i}{||Z_t|||| Z_i||} (5)\nBuilding on these real-time and historical insights, the engine constructs a comprehensive prompt Pt that integrates TTCP-derived risks, scenario-specific experiences, and traffic knowledge. This prompt includes road geometry, vehicle positions, and conflict zones, along with domain heuristics to provide the LLM with a rich contextual foundation for action proposals. To enhance reliability and minimize hallucinations, we then employ a chain-of-thought approach [31] in which the model iteratively evaluates collision severity, short- and long-term maneuver consequences, and broader traffic implications. This structured reasoning process reduces logical inconsistencies, resulting in safer and more interpretable autonomous driving policies."}, {"title": "2) Memory Repository", "content": "The Memory Repository stores and manages all pertinent knowledge required by the LLM Teacher. It operates as a dynamic database M that contains the prior scenarios and policies, which includes the historical states{s}, actions{a} and consequences{r}."}, {"title": "3) Reflective Evaluator", "content": "The Reflective Evaluator system-atically reviews driving episodes to improve decision-making by identifying risky events and integrating learned lessons into future policies.\nAfter each driving session, we first collect the experience tuples:\nD = {(st, at, St+1) | t = 1, 2, . . .,T} (7)\nwhere st and at denote the state and action at time t and st+1 denotes the subsequent state. To pinpoint high-risk segments, we define a risk function \u03a9 : S \u00d7 A \u2192 R+ that quantifies the potential for collisions or other undesirable outcomes.\n\u03a9(St, at) = \\frac{1}{max(TTTCP(Star), BI{infraction})} (8)\nwhere TTTCP(St, at) is the TTCP for action at in state st, I{} is an indicator function for specific infractions, and \u1e9e is a weighting constant.Any episode with maxt \u03a9(st, at) \u2265 \u03b4 is flagged for further reflection.\nFor the flagged segment {(si, ai)}mk, the LLM is prompted to analyze the sequence of risky actions and causes. Through CoT reasoning, it proposes a domin-specific adjustment:\n{\u2206Policy, \u2206Prompt, \u25b3Constraint} \u2190 fLLM(Qref) (9)\nThese updated constraints and policies are then integrated back into the memory repository M and the decision engine's prompt construction logic. By iterating this reflection process, the LLM-Teacher systematically reduces error recurrence and strengthens overall policy robustness."}, {"title": "C. DRL student", "content": "An actor-critic framework [32] is adopted, where both the state-value function V\u2122 and the action-value function Q\u2122 are recursively estimated. For a policy \u03c0(a | s), the state-value function and the corresponding action-value function at state St is:\nV\" (st) = E_{at~\\pi(St)} [Q (St, at)] (10)\nQ\u2122 (St, at) = r(st, at) + y E_{st+1~T(.\\st,at)} [V\" (St+1)] (11)\nThe goal of the algorithm is to determine the optimal policy \u03c0* that maximizes V\u2122(s) for all s \u2208 S. In our proposed algorithm, we iteratively learn the V function and Q function by minimizing the mean-squared Bellman error (MSBE) and optimize the policy \u3160by maximizing the Q value, where MSBE is defined as:\nL(i) = E_{(1\\E(statt,St+1)~B}[(Qi (stat) - (rt +Vg($t+1)))2] (12)\nwhere B is the experience replay buffer, and Vg represents a periodically updated target value function. The actor network is optimized by selecting actions at that maximize the critic's estimate Q (st, at), thereby promoting higher return.\nTo incorporate demonstration actions from the LLM Teacher's policy \u03c0\u00b9 into the actor-critic framework and guide the DRL agent's policy \u03c0\u03c2 during early exploration, we introduce a Kullback\u2013Leibler (KL) [33] divergence constraint. The agent's learning objective is formulated as a constrained optimization problem:\nmin E\\pi_{SSt~D}  [\u2212Q(St, \u0101t)]\ns.t. DKL(\\pi_{S} (St), \\pi^{L} (St)) \u2264 \u03c3 (13)\nwhere \u03c3 > 0 is a tolerance that bounds the KL divergence between the agent's policy \u03c0(st) and the teacher's policy \u03c0\u0395 (st). During early training, o is kept small to enforce proximity to the teacher's demonstrated actions, thereby accelerating convergence. As training proceeds, \u03c3 gradually expands, allowing the agent to rely more on its own exploration while still incorporating early guidance. This procedure balances leveraging teacher knowledge for rapid initial learning with the agent's intrinsic exploration for robust final performance."}, {"title": "2) Policy Distillation and Fusion", "content": "Although the Teacher Agent offers high-level guidance, it does not directly provide action probabilities or value estimates. To bridge this gap, we embed a Transformer-based self-attention mechanism shown in Fig. 3 within the Student's policy network. This component approximates the Teacher's implicit policy and fuses it with the Student's learned strategy in a flexible, data-driven manner. Let st \u2208 S be the state at time t. We introduce two embeddings:\nh = fs(st), h = fT(st) (14)\nwhere fs and fr are neural encoders for the Student and the Teacher, respectively. The vector hf is learned to approximate the implicit Teacher policy, \u0175, and its corresponding action-value function, QT. For each action a \u2208 A:\n\u00f1 (a | st) = softmax (Wph + bp) (15)\nQT (st, a) = W\u2081h\u0129 + bq (16)\nwhere {Wp, Wq, bp, bq} are learnable parameters.\nTo integrate these dual embeddings, a self-attention mechanism is employed:\nQ = Woh, K = Wkh\u00a5, V = Wv h, (17)\nwhere WQ, WK,Wv are learnable projections. The self-attention coefficient at can be described as:\nAt = softmax (\\frac{QK\u2122}{\\sqrt{d}}) (18)\nwith d denoting the dimensionality of K. The fused representation ht is:\nht = at V + h = at Wv h\u0129 + h\u0129 (19)\nIn multi-head settings, the process is replicated across several attention heads, and the outputs are concatenated. The Student uses the fused embedding ht to produce its final policy and action-value estimate Q:\n\u00f1(a | st) = softmax (Wht + b) (20)\nQ(st, a) = WQht + b\u0119 (21)\nThe self-attention parameters and teacher embeddings are optimized jointly. If demonstration data {(s, a\u012b)} is available, an auxiliary distillation loss enforces consistency with the Teacher's decisions:\nLdistill = - E(s,ar)\u2208DT [log \u0175T(at | s)] (22)\nThis term encourages hf to distill excellent policies from the demonstrated behavior of the LLM Teacher, while the Student continues to learn its own policy through exploration and reward feedback."}, {"title": "V. SIMULATION AND PERFORMANCE EVALUATION", "content": "We evaluate the comprehensive performance of our autonomous driving model using a gradient verification scenario constructed with Highway-Env [34] and OpenAI Gym. To capture a broad spectrum of driving complexities, we design three heterogeneous task systems with progressively decreasing difficulty, as illustrated in Fig. 4:\n1) Unsignalized intersection (Fig. 4(a)): The agent must execute an unprotected left turn at an unsignalized intersection, requiring conflict resolution and time-slot preemption to navigate crossing traffic safely.\n2) High-Speed Ramp Merging (Fig. 4(b)): The agent operates on an acceleration lane, performing speed matching and gap selection to merge seamlessly into highway traffic at elevated velocities.\n3) Four-Lane Adaptive Cruise (Fig. 4(c)): The agent focuses on fine-grained control of inter-vehicle distances and speeds across four lanes, highlighting precision in longitudinal control and continuous lane tracking.\nBy varying parameters, we simulate conservative, standard, and aggressive driver profiles, each featuring different desired speeds, accelerations, and tolerances for spacing. Vehicle speeds follow a normal distribution centered within a reasonable range, and we introduce a 15% abnormal speed disturbance to emulate real-world deviations."}, {"title": "B. Implementation Details", "content": "Both our model and the baseline methods utilize a policy network composed of a multilayer perceptron (MLP) with two hidden layers of size 128 \u00d7 128. We employ two self-attention heads, each also of dimension 128, to fuse the Student and Teacher representations.The clip range is dynamically adjusted using a linear schedule, starting with an initial value and decreasing according to the remaining training progress. Each model is trained for at least 105 time steps, with an evaluation performed every 500 time steps. We use GPT-4o-mini [11] as our LLM backbone, which shows reliable logical reasoning and real-time decision-making in driving tasks; it serves as the Teacher Agent for only the first 10% of training steps, after which constraints are gradually relaxed to encourage independent exploration. The specific parameter settings are shown in Table I. All experiments run on a computing platform equipped with Intel(R) Core(TM) i7-14700K CPU, an NVIDIA GeForce RTX 4080 SUPER GPU and 32 GB of RAM."}, {"title": "C. Performance Evaluation", "content": "To assess the effectiveness of our approach, we benchmark four algorithms: a value-based method (DQN [19]), a policy-gradient method (A2C [35]), a sequence-memory-based method (RecurrentPPO [36]), and the current LLM-based state-of-the-art (Dilu [26]). We record the average return during training in Fig. 5, where the solid lines denote mean performance and the shaded regions indicate 95% confidence intervals.\nIn unsignalized intersection shown in Fig. 5(a), Our model rapidly improves during the initial training phase and converges to the highest final return. A2C exhibits significant fluctuations, implying instability near convergence. While other baselines eventually stabilize, their end-stage returns remain notably below ours, highlighting a substantial performance gap. In high-speed ramp merging shown in Fig. 5(b), our method achieves high returns early on, stabilizing around 5\u00d7104 steps and consistently maintaining near-optimal performance thereafter. In contrast, DQN starts with negative returns and steadily climbs to a suboptimal plateau. A2C fares the worst, likely owing to its sensitivity in time-critical merging tasks. Although RecurrentPPO converges more promptly than A2C, its ultimate reward remains below our model's, underscoring the challenges of handling highly dynamic traffic with simple recurrent mechanisms. All methods experience rapid early gains, yet differ significantly in final returns and stability in four-lane adaptive cruise shown in Fig. 5(c). Our model maintains a leading position throughout and converges to a near-maximal reward. RecurrentPPO is intermittently competitive but prone to fluctuations. DQN and A2C both show moderate terminal performance, with A2C stabilizing late but still achieving a lower reward ceiling.\nWhat's more, Table II provides a numerical summary of success rate, evaluation return, average speed, ATTCP, and decision-making time for each approach. In the unsignalized intersection scenario, our method attains the highest success rate (88%) while balancing speed and safety margins. For high-speed ramp merging, it achieves 91% success and outperforms the baselines in average return. Notably, A2C, despite having the highest speed, completely fails (0% success), demonstrating that overly aggressive driving sacrifices safety and thus overall performance. In four-lane adaptive cruise, our method reaches a perfect success rate (100%) alongside near-optimal speed and return. Although Dilu shows better speed and safety margins than traditional DRL methods, its extended reasoning time limits online deployment. Overall, our approach surpasses both conventional DRL algorithms and the LLM-based Dilu, underlining its effectiveness and robustness across diverse scenarios."}, {"title": "2) Ablation Study", "content": "To evaluate the impact of each component, we conduct an ablation study comparing Vanilla PPO (V-PPO [18]), Attention-based PPO (A-PPO), and our LLM-Guided Attention PPO (LA-PPO). As shown in Fig. 6, LA-PPO demonstrates faster convergence and higher final rewards relative to V-PPO and A-PPO, indicating superior stability and robustness. In the more demanding scenarios, such as unsignalized intersection and high-speed ramp merging, LA-PPO quickly attains higher returns and preserves its advantage throughout training. Although all methods converge to similar rewards in the simpler four-lane adaptive cruise task, LA-PPO still displays a slight edge in convergence rate and peak performance. These observations confirm that leveraging LLM guidance in conjunction with an attention mechanism yields more effective teacher knowledge transfer and better-directed policy learning."}, {"title": "3) Teacher-Student Comparison", "content": "As shown in Fig. 7, we further examine success rates for the LLM Teacher and the DRL Student in each scenario. The Student outperforms the Teacher across all tasks, illustrating that while LLM guidance aids rapid early-stage learning, continual environment interaction empowers the Student to refine and ultimately surpass the Teacher's performance. This outcome highlights the strengths of a teacher-student paradigm in autonomous driving policy learning."}, {"title": "D. Case Analysis", "content": "To further explore the decision-making process and behavior characteristics of the proposed model in actual scenarios, we selected three representative cases, as shown in Fig. 8.\nIn the unsignalized intersection shown in Fig. 8(a), after the agent approaches the stop line of the intersection in step 4, it actively slows down to give way to other vehicles that arrive at the intersection first until step 6; when trying to accelerate again in step 12, it promptly observes that the vehicle in the adjacent lane is about to pass, quickly judges and slows down again, and accelerates to leave after it passes. This behavior fully reflects the model's understanding and compliance with traffic rules and social interactions, and can achieve safe and reasonable interactions with other traffic entities.\nIn the ramp merging scenario illustrated as Fig. 8(b), the model first accelerates quickly to complete the merging action; in step 6, it actively slows down to maintain a safe distance from the vehicle in front, and accelerates again after confirming that there is enough safe distance between it and the vehicle in front in step 14, and finally merges smoothly into the main road. This process shows that the agent has precise control over the acceleration and deceleration decisions in high-speed scenarios and a keen perception of risk factors.\nIn the example of four-lane adaptive cruise control shown in Fig. 8(c), the agent can continuously monitor and maintain a safe distance from the vehicle in front in dense traffic conditions or even in the presence of traffic disturbances, and adjust the speed in a timely manner to avoid rear-end collisions or excessive deceleration. This case shows that the model has good stability and active safety in long-term cruise tasks.\nFrom the above cases, it can be seen that our model can demonstrate good interaction capabilities and strategic decision-making levels in a variety of complex driving scenarios, which further supports the conclusions of the aforementioned quantitative experiments."}, {"title": "VI. VEHICLE-IN-LOOP EXPERIMENT", "content": "To further assess the robustness and real-time performance of TeLL-Drive, we conduct a vehicle-in-loop experiment that combines virtual and real-world testing. A fusion platform is developed to integrate virtual traffic simulations with real vehicle hardware, allowing for the evaluation of the intelligent driving function in dynamic and complex traffic environments. This experimental setup enables the testing of autonomous driving decision-making under various conditions, including scenarios with potential safety hazards, both in virtual and real-world settings.\nThe virtual-real fusion platform consists of two main components: the AV hardware and traffic flow simulation software. As shown in Fig. 9, the traffic flow simulation software generates a virtual traffic environment, providing background traffic data that interacts with the real-world data captured by the AV's sensors. These sensors collect real-time environmental information, which is then fused with the simulated traffic data through a data fusion process. This combined perception is transmitted to the planning control unit, which uses it to generate the vehicle's motion trajectory. The resulting vehicle trajectory is then fed back into the simulation software, allowing for interaction between the AV and the virtual traffic flow.\nIn this experiment, the intelligent agent trained under the TeLL-Drive framework serves as the decision-making algorithm for the autonomous vehicle. The simulation vehicle operates using TESSNG [37], a high-level microscopic simulation software, which enables detailed modeling of vehicle dynamics in complex traffic scenarios. The experiment was conducted at Tongji University's Autonomous Driving Smart Town, with the unprotected left turn at a complex intersection chosen as the test scenario. As the scenario with the lowest success rate in the simulation experiment, this scenario has inherent safety risks and requires precise decision-making in a dynamic environment. The integration of high-precision maps, precise timing positioning and full-element digitization enable complete synchronization between the real-world and virtual-world, ensuring that both environments are in sync during testing. This vehicle-in-loop setup provides a comprehensive platform for evaluating the performance of TeLL-Drive in real-time, dynamic driving scenarios."}, {"title": "B. Case Study Analysis", "content": "We conducted real-vehicle experiments on the virtual-real fusion platform to evaluate the performance of the TeLL-Drive framework. Two representative cases are shown in Fig. 10, each captured from various perspectives, including the virtual twin platform perspective, the drone bird's-eye view, the car-following view, the roadside view, and the in-car perspective. These multiple angles allow for a comprehensive analysis of the algorithm's performance across different scenarios. The specific experimental video can be accessed on our website\u00b9.\nIn Case 1, the autonomous vehicle equipped with TeLL-Drive begins from a standstill and accelerates toward the intersection. As it approaches the stop line, the vehicle slows down to create sufficient observation and decision space, enhancing its ability to assess the surrounding traffic. By the 7th second, the vehicle encounters an oncoming vehicle. Upon assessing the situation, the vehicle decides to slow further at the 12th second to yield and avoid a collision. After the oncoming vehicle passes, the autonomous vehicle resumes acceleration and approaches the exit road of the intersection by the 15th second. To maintain a safe distance from the vehicle in front, the system performs adaptive acceleration and deceleration, ensuring both safety and traffic efficiency. In this case, the autonomous vehicle is the last to leave the intersection, but the maneuver was executed safely and efficiently.\nIn Case 2, the autonomous vehicle follows similar actions up to the point before entering the intersection. However, at the 6th second, the vehicle observes fewer vehicles in the intersection and determines it can pass first, so it accelerates. By the 8th second, a vehicle on the left side approaches, prompting an interaction. After a brief period of strong interaction between the two vehicles, the simulation vehicle (SV) decides to slow down and stop, while our autonomous vehicle continues to pass first, successfully navigating the intersection.\nThese two cases demonstrate the robustness and reliability of the TeLL-Drive framework when deployed on real vehicles. The system effectively adapts to dynamic traffic scenarios, ensuring safe and efficient decision-making in complex environments. The ability of TeLL-Drive to handle both cooperative and conflict-driven interactions, while maintaining safety and traffic flow, underscores its potential for real-world autonomous driving applications."}, {"title": "VII. CONCLUSION", "content": "Our proposed TeLL-Drive framework integrates teacher-guided learning with attention-based policy optimization, enabling efficient knowledge transfer and robust decision-making. Experimental results demonstrate that TeLL-Drive outperforms conventional DRL methods and existing LLM-based approaches across multiple metrics, including success rate, average return, and real-time feasibility. Additionally, ablation studies highlight the significance of each model component, particularly the synergy between attention mechanisms and LLM teacher guidance. Finally, vehicle-in-the-loop experiments verify the robustness effectiveness of the model when deployed in practice. These findings confirm that our approach not only accelerates policy convergence but also enhances safety and adaptability across diverse traffic conditions. In the future, we will explore the application of the TeLL-Drive framework to more dynamic, multi-agent environments and verify its scalability and real-time adaptability through real-vehicle experiments in open road scenarios."}]}