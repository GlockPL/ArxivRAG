{"title": "Augmenting Prototype Network with TransMix for Few-shot Hyperspectral Image Classification", "authors": ["Chun Liu", "Longwei Yang", "Dongmei Dong", "Zheng Li", "Wei Yang", "Zhigang Han", "Jiayao Wang"], "abstract": "Few-shot hyperspectral image classification aims to identify the classes of each pixel in the images by only marking few of these pixels. And in order to obtain the spatial-spectral joint features of each pixel, the fixed-size patches centering around each pixel are often used for classification. However, observing the classification results of existing methods, we found that boundary patches corresponding to the pixels which are located at the boundary of the objects in the hyperspectral images, are hard to classify. These boundary patchs are mixed with multi-class spectral information. Inspired by this, we propose to augment the prototype network with TransMix for few-shot hyperspectrial image classification(APNT). While taking the prototype network as the backbone, it adopts the transformer as feature extractor to learn the pixel-to-pixel relation and pay different attentions to different pixels. At the same time, instead of directly using the patches which are cut from the hyperspectral images for training, it randomly mixs up two patches to imitate the boundary patches and uses the synthetic patches to train the model, with the aim to enlarge the number of hard training samples and enhance their diversity. And by following the data agumentation technique TransMix, the attention returned by the transformer is also used to mix up the labels of two patches to generate better labels for synthetic patches. Compared with existing methods, the proposed method has demonstrated sate of the art performance and better robustness for few-shot hyperspectral image classification in our experiments. All the codes are available at https://github.com/HENULWY/APNT.", "sections": [{"title": "I. INTRODUCTION", "content": "HYPERSPECTRAL images (HSIs) capture radiation information of ground objects over dozens or even hundreds of continuous spectral channels. It can obtain spectral features that reflect the unique characteristics of the targets. Compared to natural images with only RGB three channels, HSIs integrate spatial and spectral features of the ground objects, which can capture more subtle differences between them. Due to this advantage, HSIs have great application value in the fields such as environmental monitoring [1] and resource utilization and management [2], [3].\nHSI classification, which is one fundamental task for HSI applications, is to classify each pixel of HSIs and predict their classes. Because the spectral vectors behind each pixel of HSIs contain rich radiation information, the works for HSI classification in early stage focus mainly on the spectral features, and directly take these spectral vectors as the samples to be classified [4], [5]. But caused by the factors such as illumination and atmosphere, the phenomenon of same objects with different spectrum and different objects with same spectrum is widely present in HSIs. Therefore, later methods for HSIs classification pay much attention to integrating the spatial features with the spectral features of each pixel [6], [7], where the spatial features provide additional useful information about the shape, context, and layout around each pixel. To capture the spatial features, the fixed-size (e.g., 9\u00d79) patches centering around each pixel are often generated from HSIs and then taken as the samples to be classified.\nIn recent years, with the powerful feature extraction ability and a great success in a series of fields, deep learning has been widely used for HSI classification [8]\u2013[10]. Based on these typical deep learning models of convolutional neural networks (CNN), recurrent neural networks (RNN) and graph neural networks (GNN), many deep learning methods have been designed to obtain more distinguishable spatial and spectral features from HSIs. Some methods adopt the two branches architecture, which uses different neural networks to extract the spatial and spectral features respectively and then fuses them together [11]\u2013[13]. At the same time, many methods strive to use a single feature extractor such as 3DCNN to capture the spatial-spectrial joint features [14], [15].\nIn light of that deep learning methods often require a large number of labeled training samples and it is expensive and time-consuming to manually mark these samples, few-shot learning methods have also aroused lots of interest recently [16]-[21]. Few-shot learning, which is a branch of deep learning, aims to learn to identify some classes of samples by marking only few of them, e.g., three or five samples per class. This kind of ability has been viewed as one skill possessed by humans. With the aids of available abundant labeld samples, current few-shot learning methods mainly follow the way of transfer learning, that is, acquire prior knowledge from available abundant labeled samples and then transfer the knowledge to target tasks which contain only few labeled samples. To transfer knowledge effectively, few-shot learning methods often adopt the meta-learning technique which constructs lots of similar tasks by imitating the target tasks and uses the constructed tasks to train a model that can easily adapt to target tasks. Based on typical few-shot learning models such as prototype network [16], many few-shot methods have been designed for HSI classification. For example, DFSL [22] took the 3D residual network as feature extractor to extract better spatial-spectral joint features from HSIs. Different from DFSL, SSPN [23] used the local pattern coding technique to combine the spatial and spectral information, and then applied 1D convolutional neural network as extractor to obtain features. Instead of using the Euclidean distance, HSEMD-Net [24] used the Earth Mover distance to learn prototype representations for each hyperspectral class. Meanwhile, RL-Net [25] followed the relational network [26] which is an improved version of prototype network for few-shot HSI classification.\nWhen classifying HSIs under few-shot setting, early methods usually assume that the samples from which the prior knowledge are learned and the samples of the target tasks are coming from the same domain. This often means that these samples should be captured by the same sensors and in the same environment. To relax this assumption, a set of crosss-domain few-shot methods have been further proposed for HSI classification. Their purposes are to enhance model's cross-domain generalization ability. For example, DCFSL [27] augmented DFSL method with a domain discriminator to obtain domain-independent features; SSFT [28] changed the feature distribution by a feature-wise transformation module with the aim to obtain generalized features; CMFSL [29] used the task-adapted class-covariance metric to obtain better features under few-shot setting; Gia-CFSL [30] enhanced prototype network with domain alignment strategy to increase domain adaptation. By using supervised contrastive learning, RPCL [31] imposed triple constraints on prototypes of the support set to stabilize and refine the prototypes. Moreover, MRLF proposed to learn task-specific relations between samples, including the contrastive and affinitive relations, to futher improve the feature discriminability [32]. In addition, considering the availability of the large number of natural images, HFSL [33] took natural image datasets for pre-training to obtain prior classification knowledge, allowing the model to better distinguish hyperspectral samples.\nHowever, while taking the fixed-size patches around each pixel as samples to obtain spatial-spectral joint features, current few-shot research works for HSI classification, including these cross-domain methods, have paid less attention to the difference between the patches around the boundary pixels and the others. As shown in Fig. 1, each ground object in HSIs occupies a part of the images, which covers a set of pixels. The boundary patches built for the boundary pixels of the object will be different from these patches for the pixels located far from the boundary. These boundary patches will constitute the pixels belonging to the adjacent ground objects, which will be also mixed with multi-class spectral information. This means that the classification results of these boundary patches will be affected by these pixels from adjacent objects and the different pixels in the patches will have different importance. Generally, these boundary patches are the hard samples to be classified. Our primary experimental results shown in Fig. 1 have also indcated that current methods have shown lower performance for these boundary patches. What's worse, these boundary patches only constitute a minority in the training samples, so they have less contributions to improve the robustness of the classification model."}, {"title": "II. THE PROPOSED METHOD", "content": "In this section, we state the problem of few-shot HSI classification, and detail the proposed method."}, {"title": "A. Problem Statement", "content": "For the problem of few-shot HSI classification, there are two datasets given: source dataset and target dataset. In practical application, all the samples in the source dataset are labeled, which are used for training the classification model. While, there are only few labeled samples in the target dataset. The samples are the pixels of the HSIs. In order to fully preserve spatial information, the fixed-size patches centering around each pixel instead of the pixeles themselves are often treated as the samples. The few-shot learning task is to predict the classes of these unlabeled samples in the target dataset by using all these labeled samples in source dataset and target dataset. Nevertheless, another dataset in which all the samples are also labeled is often selected as the target dataset in the research environment, to facilitate the performance evaluation of the few-shot learning methods. Such dataset can be also called testing dataset.\nTo elaborate the problem formally, it is often assumed that there are $C_s$ classses in the source dataset $D_s$ and $C_t$ classses in the target dataset $D_t$, where $C_t$ is smaller than $C_s$. To imitate the setting of target dataset, lots of the tasks will be constructed by randomly selecting $C \\times (K+M)$ samples each time, and used to train the classification model in an episode manner. The $C$ refers to the number of classes selected, which is often set to the number of classes in the target dataset. $K$ is the number of few labeled samples per class, and $M$ is the number of query samples per class to be classified, where $K$ is smaller than $M$. The set consisiting of the $C \\times K$ labeled samples is often called support set $\\{(x_i, y_i)\\}_{C K}$, while these $C \\times M$ query samples will form the query set $\\{(x_i, y_i)\\}_{C \\times M}$. Such few-shot tasks are often called the C-way K-shot tasks, whose purpose are to predict the classes of these query samples by using support samples."}, {"title": "B. The Model Architecture and Learning Process", "content": "The architecture of the proposed APNT method and the flowchart of the learning process is shown in Fig. 2. APNT model mainly consists of three components: query sample mixing, transformer based feature extractor, and TransMix based few-shot learning loss. Given an input task consisting of the support and query set, the query sample mixing module will mix up each query sample with another randomly selected query sample in the way of CutMix [35], and output the synthetic query samples. Subsequently, the transformer based feature extractor extracts the embedding features of the support samples and the synthetic query samples, and also returns the attention maps reflecting the importance of each pixel of the original query samples. Following that, the mean of the embedding features of the support samples in the same classes will be caculated as the prototpyes of each class. In the meanwhile, the TransMix based few-shot learning loss module computes the cross entropy loss of the synthetic query samples according to their synthetic labels which are derived by using the attention maps returned by transformer.\nSimilar with the work of RPCL [31], We adopt the fine-tuning strategy to train the network. That is, we first pre-train the network by using the tasks constructed from $D_s$, and then fine-tune it by using the tasks from $D_t$. This also means there are some labeled samples selected from $D_t$ to participate in the training. Consistent with previous works, there are five samples per class selected, which are augmented to 200 samples per class in our experiments. The tasks from Dt will be constructed from these augmented smaples. Finally, the remaining samples in the target dataset $D_t$ are used for testing. And after extracting sample features with the trained transformer, the classes of these query samples are pedicted by using KNN algorithm during testing. It is worth noting that because the spectral dimensions of the samples from source and target datasets are different, there are also the mapping modules to unify the dimensionality."}, {"title": "C. Query Samples Mixing", "content": "When given an input task consisting of a support set and a query set, either the support or query samples will be first processed by the mapping modules to transform their channel dimensionality. After that, each sample in the query set is mixed up with another randomly selected sample to generate the synthetic query samples. We choose to only mix up the query samples without support samples for the purpose of ensuring the correctness of the class prototypes caculated from these support samples. For two query samples $x_i$ and $x_k$ with the labels of $y_i$ and $y_k$, the mixing up process can be formally stated as Eq. 1.\n$\\begin{aligned}\n\\tilde{x} &= M x_i + (1 - M) x_k  \\\\\n\\tilde{y} &= \\lambda_1 y_i + \\lambda_2 y_k\n\\end{aligned}$\nwhere $M \\in \\{0,1\\}^{W\\times H}$ denotes a binary mask indicating where to drop out the pixels from the sample with the spatial size of $W \\times H$. $1$ is a binary mask filled with ones, and $\\odot$ is the element-wise multiplication. $\\tilde{x}$ is the synthetic query sample, and $\\tilde{y}$ is the corresponding synthetic label, where $\\lambda_1$ and $\\lambda_2$ are the proportion of $y_i$ and $y_k$ in the synthetic label. The proposed method follows CutMix [35] to mix up the query samples. That is, a randomly sampled patch in $x_i$ is removed and filled with the patch cropped from the same region of $x_k$. CutMix uses the proportion of the sampled patch to the entire sample as the value of $\\lambda_1$ (i.e., $\\lambda_2 = 1-\\lambda_1$) to mix up the labels. Differently, we will caculate the values of $\\lambda_1$ and $\\lambda_2$ by using the attention maps returned from transformer, to take the different importance of the pixels into consideration. The details will be described in the following subsection of E."}, {"title": "D. Transformer based Feature Extraction", "content": "Once the synthetic query samples are generated, the original query samples, the synthetic query samples, and the support samples are input into the transformer module. The embedding features of the support samples and the synthetic query samples will be extracted. At the same time, the attention maps of the original query samples are also required. The whole process can be seen in Fig. 3.\nWhen inputting the samples, i.e., the patches with the three-dimensional size of $W \\times H \\times D$, into transformer, each sample will be first pull into a sequence $x \\in \\mathbb{R}^{WH\\times D}$ of which the composing items are the spectral vectors of the pixels. And then, the token vectors with $D$ dimension, which are used to learn global features of each sample, are randomly generated and appended into the sequences of each sample. After that, the features and the attention maps of each sample are obtained by directly inputting the updated sequences $x \\in \\mathbb{R}^{(WH+1)\\times D}$ into transformer module.\nA shown in Fig. 3, the transformer module of APNT is composed of a set of stacked transformer encoders. Each encoder consists of a self-attention layer and a feedforward layer. For the self-attention layer, there exist three learnable projection matrixs $W_q \\in \\mathbb{R}^{D\\times D'}$, $W_k \\in \\mathbb{R}^{D\\times D'}$ and $W_v \\in \\mathbb{R}^{D\\times D'}$. They will project the input sequences into different embedding spaces, as shown in Eq. 2.\n$\\begin{aligned}\nQ &= xW_q \\\\\nK &= xW_k \\\\\nV &= xW_v\n\\end{aligned}$\nwhere $Q$, $K$ and $V$ are the projected matrices of the sequence $x$. With these projected matrices, the attention maps of each input sequence $a_x \\in [\\mathbb{R}^{(WH+1)\\times (WH+1)}$, which capture the attention of each item in the sequence to the other items, can be caculated as Eq. 3. And the features of each item in the sequence are updated by Eq. 4.\n$a_x = \\text{Atttention}(Q, K) = \\text{softmax}(\\frac{QK^T}{\\sqrt{D'}})$\n$x' = \\text{Atttention}(Q, K)V$\nThere are multiple heads of self-attention compution in transformer. The features returned from each head will be concatenated together and restored to the original dimension through a linear operation. The updated features will be further processed through the feedforward layer without changing the dimensions. That is, a new sequence of $x'' \\in \\mathbb{R}^{(WH+1)\\times D}$ will be produced by a transformer encoder for each sample. At the same time, the attention maps $a_x$ from different heads will be averaged. Through two transformer encoders, the updated taken vectors in the sequence $x'$ generated by the last transformer encoder are selected as the feature vectors of the samples. And the attention vectors capturing attentions of the samples to their constituent pixels will be also selected from the attention maps $a_x$ returned by the last transformer encoder. When reshaping the obtained attention vectors into the shape of $W \\times H$, the required attention maps reflecting the importance of each pixel are obtained."}, {"title": "E. TransMix based Few-shot Learning Loss", "content": "After obtaining the attention maps $A \\in \\mathbb{R}^{W\\times H}$ of these query samples, the sum of the attention values belonging to the mixed region is directly used to derive the values of $\\lambda_1$ and $\\lambda_2$ for label mixing shown in Eq. 1. This is because that the sum of all values in a attention map $A$ is 1. This derivation process can be experssed as Eq. 5.\n$\\begin{aligned}\n\\lambda_1 &= M \\odot A_i  \\\\\n\\lambda_2 &= (1 - M) \\odot A_k\n\\end{aligned}$\nwhere $A_i$ and $A_k$ represent the attention maps of the query smaple of $x_i$ and $x_k$. $M$ represents the binary mask mentioned in Eq. 1. From Eq. 5, it can be seen that the values of $\\lambda_1$ and $\\lambda_2$ represent the probability that the synthetic sample $\\tilde{x}$ shown in Eq. 1 belongs to the classes of $y_i$ and $y_k$.\nIn addition, assuming the feature of each support smaple is $z_i$, the $c^{th}$ class prototype is computed as Eq.6.\n$p_c = \\frac{1}{\\mathbb{S}^c}\\sum_{z_i \\in S^c} z_i$\nwhere $S^c$ represents the set of features of $c^{th}$ class support samples.\nThen, through predicting the synthetic query sample $\\tilde{x}$ as the classes denoted by the labels of $y_i$ and $y_k$, the few-shot learning losses are caculated according to Eq. 7 and Eq. 8. The $\\tilde{x}_h$ in Eq. 7 and 8 represents the embedding feature of synthetic query sample $\\tilde{x}$. And $p(\\tilde{y}_h = y_i | \\tilde{x}_h)$ means the probability that the model accurately predicts the synthetic query sample $\\tilde{x}$ to the class of $y_i$. Meanwhile, $d(\\tilde{x}_h, p_{y_i})$ denotes the Euclidean distance between the feature of synthetic query sample $\\tilde{x}$ and the class prototype of $y_i$.\n$\\mathbb{L}^{l}_{fst} = \\frac{1}{C \\times M} \\sum_{h=1}^{C \\times M} logp(\\tilde{y}_h = y_i | \\tilde{x}_h)$\nwhere $p(\\tilde{y}_h = y_i | \\tilde{x}_h) = \\frac{exp(d(\\tilde{x}_h, p_{y_i}))}{\\sum_{j=1}^{C_s} exp(d(\\tilde{x}_h, p_j))}$\n$\\mathbb{L}^{l}_{fsl} = \\frac{1}{C \\times M} \\sum_{h=1}^{C \\times M} logp(\\tilde{y}_h = y_k | \\tilde{x}_h)$\nwhere $p(\\tilde{y}_h = y_k | \\tilde{x}_h) = \\frac{exp(d(\\tilde{x}_h, p_{y_k}))}{\\sum_{j=1}^{C_s} exp(d(\\tilde{x}_h, p_j))}$\nBecause the synthetic query samples only partially belong to the classes of the samples used for mixing, the total loss used for updating the model is caculated as Eq. 9, which uses the values of $\\lambda_1$ and $\\lambda_2$ to balance the losses of $\\mathbb{L}^{l}_{fst}$ and $\\mathbb{L}^{l}_{fsl}$.\n$\\mathbb{L} = \\lambda_1 \\mathbb{L}^{l}_{fst} + \\lambda_2 \\mathbb{L}^{l}_{fsl}$"}, {"title": "III. EXPERIMENTS", "content": "In order to validate the effectiveness of the proposed method, extensive experiments have been done. In this section, we describe the experimental setup and results."}, {"title": "A. Experimental Setup", "content": "Dataset: In order to fairly evaluate the performance of the proposed method, the datasets which had been widely used in related works were selected for training and testing in our work. We used the Indian Pine, University of Pavia, and Salinas datasets as the target datasets, and used the Chikusei dataset as the source dataset. We give a brief introduction about these datasets.\n(a) Chikusei: It was taken by Hyperspec-VNIR-C sensor in Chikusei, Japan. The dataset contains 128 bands with the wavelength ranging from 343nm to 1018nm, and 2517\u00d72335 pixels with a spatial resolution of 2.5m. As shown in Fig. 4, there are 19 classes of land cover, including urban and rural areas.\n(b) Indian Pines (IP): It was imaged by an airborne visible infrared imaging spectrometer in Indiana, USA. This dataset contains 200 bands with the wavelength ranging from 0.4-2.5(10-6)m. The size is 145 \u00d7 145 pixels, with a spatial resolution of about 20m. As shown in Fig. 5, there are 16 classes of land cover including crops and natural vegetation.\n(c) Salinas (SA): This dataset was also captured by the airborne visible infrared imaging spectrometer in the Salinas Valley, California, USA. It contains 204 wavebands with a size of 512 \u00d7 217 pixels, and a spatial resolution of about 3.7m. As shown in Fig. 6, there are 16 classes of land cover, which include vegetables, exposed soil, etc."}, {"title": "Implementation and Configuration", "content": "For the proposed method, the mapping modules are implemented with a Conv2D layer with 1 \u00d7 1 kernel. The feature extractor is implemented by two transformer encoders, each of which includes a self-attention layer and a fully connected feedforward layer. The specific parameters related to the implementation are shown in the Table I, where Head_Dim denotes the number of attention channels, and Feed_Dim represents the hidden layer dimension of the feedforward layer.\nAll linear and convolutional layers were normalized using Xavier, and the Adam optimizer was used to optimize the model, with an initial learning rate of 0.001. All comparison experiments used the same 10 random seeds to fairly compare the mean of AA, OA, and kappa. The patches with the size of 9\u00d79 pixels were cut from HSIs. Moreover, as mentioned early, there are five samples per class selected from the target dataset to fine-tune the model trained on the source dataset. And they are augmented to 200 samples per class through cropping and resizing, which is consistent with the work of [31]. There are 3000 training iterations adopted in our experiments. Among them, only the samples from source datasets are used in the first 1000 training iterations, and the remaining 2000 iterations are run on these augmented samples from target datasets."}, {"title": "B. Comparison with Related Methods", "content": "To verify the performance of the proposed method, we selected several typical classification methods for comparison, including DFSL [22], DCFSL [27], CMFSL [29], Gia-CFSL [30], RPCL [31], MRLF [32], and HFSL [33]. In the experiment, according to the specific training process of each model, DFSL only used these augmented samples from target dataset for training. DCFSL, CMFSL, Gia-CFSL, MRLF, RPCL and our proposed method used both the samples from source datasets and target datasets, while HFSL used the natural image dataset of Mini-ImageNet for pre-training and was fine-tuned on these augmented samples from target datasets. In addition, in order to prove that the proposed method still had good performance without pre-training on source datasets, the performance of APNT* method which is only trained on these augmented samples from target datasets is also evaluated.\nTable II to Table IV report the specific classification results of each class, as well as the AA, OA, and kappa values of each method on the three target datasets. From the comparison results, it can be seen that APNT* and APNT have superior classification results compared to other methods. Among them, APNT outperforms the other results by 1.5%, 1.7%, and 0.5% on the IP, PU, and SA datasets, respectively, reaching the optimal results on class 1, 4, 6, 7, 9, 10, and 11 of IP dataset, class 1, 3, and 7 of PU dataset, and class 2, 9, 11, and 15 of SA dataset. This result demonstrates the effectiveness of the proposed method. In the meanwhile, APNT* outperforms the other models by 1.1%, 0.9%, and 0.8% on the IP, PU, and SA datasets respectively, reaching the optimal results on class 1, 2, 7, 14, and 15 of IP dataset, class 4 and 6 of PU dataset, and class 3 and 16 of SA dataset. Because APNT* does not use the sampes from source dataset for pre-training, this result shows that the proposed method can eliminate the dependence on existing large-scale datasets and can achieve comparative performance by only using these few-shot labeled samples in target tasks. This will facilitate the use of the proposed method in a range of applications."}, {"title": "C. Evaluation on Boundary Patches", "content": "To validate the improvement of the predicting on boundary pixels, we processed the datasets and extracted all the boundary patches for testing. In the hyperspectral datasets, when all the pixels in the patches don't belong to the same class, we define such patches as boundary patches. Generally speaking, these boundary patches are the hard samples, and it is more difficult to predict their classes.\nWe calculated the overall accuracy of various methods on all the boundary patches, which are shown in Fig. 11. As can be seen from the results, consistent with the assumption, APNT has the highest overall prediction performance on the boundary patches over three datasets."}, {"title": "D. Ablation Experiments", "content": "While applying transformer to pay different attention to different pixels and extract better features, the proposed method also mixs up the query samples by following TransMix and use the synthetic samples to train the model. We divide our contribution into two parts: the transformer based feature extractor and the TransMix based sample mixing. To verify the contribution of each part, the ablation experiments were also conducted. By comparing the proposed method with the method with 3DCNN feature extractor which is used in the work of [31], we validate the contribution of transformer based feature extractor. At the same time, by comparing the proposed method with that of using CutMix [35] instead of TransMix [34] to mix up query samples, the contribution of TransMix based query sample mixing is also validated.\nTable V shows the results of the ablation experiments, where Trans denotes the transformer. From the first and second column, it can be seen that when adopting the same mixing technique of CutMix, better performance are achieved in most cases by adopting transformer as feature extractor. This indicates that compared with the 3DCNN feature extractor, transformer model can learn more point-to-point relationships in the patches. Furthermore, when using transformer as the feature extractor but adopting TransMix instead of CutMix for query sample mixing, there are also better performance which can be seen from the second and third column of Table V. This proves that compared with CutMix which mixs up the labels of two samples just according to the proportion of synthetic areas, TransMix can generate better mixing labels by using the attention on each pixel which reflect their relative importance."}, {"title": "E. Sensitive Analysis of Parameters", "content": "There are several parameters which affect the performance of the proposed method. We have done the experiments to study how sensitive the proposed method is to these parameters. In this section, we report the analysis results of two kinds of parameters, namely the number of transformer encoders and the size of the HSI patch.\n1) Effect of the Number of Transformer Encoders: To analyze the impact of the number of transformer encoders on classification performance, we set the number of transformer encoders in the [2,4,6,8] for testing. The changes of the performance of the proposed method on three datasets are shown in Fig. 12. It can be seen that as the number of transformer encoders increases, the performance in OA, AA, and kappa do not significantly improve, and even slightly decreases. This indicates that larger number of transformer encoders does't mean better performance. This may be because that limited by the number of training samples, the model has fully learned the classification knowledge in the samples when adopting fewer transformer encoders. It can be seen that when the number of transformer encoders in the model is 2, it achieves the best performance on PU and SA datasets, and achieves the second-best performance on IP dataset. On balance, in order to reduce the scale of model parameters, we finally set the number of transformer encoders to 2 in our experiments.\n2) Effect of the Patch Size: The proposed method uses transformer to learn the relationship among pixels in the patches and extract the spatial-spectral joint features from the patches. Then, increasing the patch size will introduce more spatial information. In order to illustrate the influence of patch size on classification performance, we tested the values in [3, 5, 7, 9, 11]. The changes of the performance of the proposed method on three datasets are shown in Fig. 13. It can be seen that with the change of patch size, the best effect is achieved when the patch size is 7 on IP dataset, while it is better to set the patch size to 11 on PU and SA dataset. When the size is 9, the proposed method achieves the second-best effect on all three datasets. In order to facilitate the comparison with other models, we set the patch size to 9 in our experiments."}, {"title": "F. Analysis of Parameter and Computational Time", "content": "To further illustrate the performance of the proposed method on time and space consumption, Table VI lists the size of model parameters (M) and the inference time including the training and testing time (s) of these comparsion methods. It can be seen that although our APNT method is not the lightest compared to other few-shot methods, it has a faster training speed and competitive testing time. This may be attributed to the parallel execution characteristics possessed by the Transformer model."}, {"title": "IV. CONCLUSION", "content": "In this paper, we augment the prototype network with Trans-Mix for few-shot HSI classification. Focusing on the practical problem of low predicting accuracy at boundary pixels, the proposed method uses the transformer as the feature extractor of prototype network, in order to pay different attention to different pixels in the patches adopted for obtaining spatial-spectral joint features. In the meanwhile, it randomly mixs up two patches to imitate the boundary patches which are mixed with multi-class spectral information, and uses these synthetic patches to train the model. It is expected to enlarge the number of hard training samples and enhance the diversity of training samples. And to mix up the labels of two patches, the attention captured by transformer are used to generate better labels for the synthetic patches. The proposed method requires less training time and achieves better results on these datasets widely used for HSI classification experiments. Particularly, in comparative experiments, we found that it can also achieve good results without using auxiliary datasets for pre-training. This shows that the proposed method can remove the dependence on the auxiliary datasets, and can be easily appied in practice."}]}