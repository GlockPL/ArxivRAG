{"title": "SAFR: Neuron Redistribution for Interpretability", "authors": ["Ruidi Chang", "Chunyuan Deng", "Hanjie Chen"], "abstract": "Superposition refers to encoding representations of multiple features within a single neuron, which is common in deep neural networks. This property allows neurons to combine and represent multiple features, enabling the model to capture intricate information and handle complex tasks. Despite promising performance, the model's interpretability has been diminished. This paper presents a novel approach to enhance model interpretability by regularizing feature superposition. We introduce SAFR, which simply applies regularizations to the loss function to promote monosemantic representations for important tokens while encouraging polysemanticity for correlated token pairs, where important tokens and correlated token pairs are identified via VMASK (Chen and Ji, 2020) and attention weights respectively. We evaluate SAFR with a transformer model on two classification tasks. Experiments demonstrate the effectiveness of SAFR in improving model interpretability without compromising prediction performance. Besides, SAFR provides explanations by visualizing the neuron allocation within the intermediate layers.", "sections": [{"title": "1 Introduction", "content": "Individual neurons in neural networks can represent multiple features from the input. This phenomenon, known as superposition, improves the model's ability to capture intricate relationships between features (Olah et al., 2020), while also complicating the understanding of the underlying processes behind the model's decision-making (Elhage et al., 2022). Facing these challenges, recent research like sparse autoencoders (SAEs) (Huben et al., 2024) artificially decomposes the activation space into a sparse vector space through auxiliary networks.\nWhile SAEs provide a method to interpret features through combinations of sparse activations, there is still a lack of sufficient research on controlling neuron distribution for interpretability.\nIn this paper, we ask the question: Can we enhance model interpretability by explicitly controlling the distribution of features across neurons? An intuitive approach is to encourage monosemantic neurons by regulating activations (Elhage et al., 2022; Bricken et al., 2023; Wang et al., 2024). However, focusing solely on monosemanticity may limit the model's ability to capture feature interactions, potentially hindering overall performance.\nTo address this challenge, we propose a novel method called Superposition-Aware Feature Regularization (SAFR) to enhance model interpretability by strategically redistributing neurons through a modified loss function, approached from two perspectives. As illustrated in Figure 1, our framework incorporates regularization techniques aimed at promoting monosemantic representations for important tokens, while simultaneously fostering polysemanticity among correlated token pairs. The identification of important tokens is achieved via a variational inference network, adapted from VMASK (Chen and Ji, 2020). Additionally, correlated token pairs are identified based on attention weights to foster polysemantic representation.\nWe evaluate the effectiveness of SAFR with a transformer model on the SST-2 (Socher et al., 2013) and IMDB (Maas et al., 2011) datasets. When the top 30% of words identified by SAFR are removed, we observe a significant drop in test accuracy\u201417.21% on SST-2 and 28.48% on IMDB. As we gradually remove additional words, accuracy continues to decline in a consistent manner. To further substantiate these findings, we visualize the neuron allocation within the FFN layers of a transformer block. Our experimental results demonstrate that SAFR can effectively redistribute features across neurons while preserving a reasonable"}, {"title": "2 Preliminaries", "content": "Given an input sequence of $T$ tokens $S = (s_1,..., s_T)$ and a neural network model $f(\u00b7)$ with $L$ layers, let $h_i^l$ denote the hidden representation obtained at layer $l \u2208 [1, . . . , L]$ and token position $i \u2208 [1, . . . , T]$. Following previous work (Scherlis et al., 2022; Elhage et al., 2022), which defines interference, polysemanticity, and capacity based on features, we extend these definitions to hidden representations in our analysis.\nInterference The interference $(I)$ measures the overlap or similarity between two representations at the same layer:\n$I_{i,j}^l = h_i^l \\cdot h_j^l$ (1)\nInterference quantifies how much the hidden representations interfere with each other. Higher interference indicates greater overlap, suggesting the tokens share representational dimensions.\nPolysemanticity The polysemanticity $(P)$ describes the extent to which a single hidden representation captures information from multiple tokens:\n$P^l_i = \\sum_{j \\neq i} (\\hat{h}_i^l \\cdot \\hat{h}_j^l)^2$ (2)\nHere, $\\hat{h}_i^l$ denotes the normalized representation of $h_i^l$ (divided by its own magnitude). A high polysemanticity value indicates that a single direction in the representation space is \u201cpolysemantic\u201d, meaning it simultaneously represents information from multiple tokens.\nCapacity The capacity $(C)$ quantifies how much of a hidden representation direction is dedicated to representing token $i$:\n$C^l_i = \\frac{(h_i^l \\cdot h_i^l)^2}{\\sum_j (h_i^l \\cdot h_j^l)^2}$ (3)\nwhere $0 < C_i^l < 1$ and $1 < \\sum_{i=1}^T C_i^l < T$ with $C_i^l = 1$ for all $i$ and $T$ denotes sequence length. A higher capacity value indicates that the representation at position $i$ is more focused on representing the $i$-th token in the input."}, {"title": "3 Methodology", "content": "SAFR enhances model interpretability by strategically redistributing neurons through a superposition regularization strategy. By promoting a well-structured neuron distribution that balances importance and interaction, it makes token representations more meaningful.\nThe baseline is defined using the original model with cross-entropy loss $L_{CE} = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{g=1}^G y_{ng} \\log p^n_g$, where $N$ is the number of samples in the dataset, and $G$ is the number of classes in text classification, $y_{ng}$ is an indicator that equals 1 if sample $n$ belongs to class $g$ (and 0 otherwise), $p^n_g$ is the predicted probability for sample $n$ being of class $g$. To improve model interpretability in a constrained representational space, we propose a two-part regularization strategy: one that promotes monosemantic representations for important tokens, and the other encourages polysemanticity for correlated token pairs. This approach enables the model to effectively allocate its representational resources, as shown in Figure 1.\nThe proposed loss function integrates these regularization terms accordingly:\n$L = L_{CE} + \\lambda_{Imp} \\cdot L_{Importance} + \\lambda_{Inter} \\cdot L_{Interaction}$\nwhere $\\lambda_{Imp}$ controls the importance loss term and $\\lambda_{Inter}$ controls the interaction loss term.\nImportance-Based Regularization We apply the VMASK (Chen and Ji, 2020) between the embedding layer and the positional encoder to select important tokens. A detailed introduction to VMASK is provided in Appendix A. To encourage monosemanticity for important tokens, we introduce a regularization term $L_{Importance} = \\sum_{i=1}^T P_i^V / E$, where $E$ represents the embedding dimension and $P_i^V$ denotes the polysemanticity for the hidden representation of the $i$-th token after the VMASK layer. This regularization penalizes important tokens with high polysemanticity.\nInteraction-Based Regularization We leverage the attention mechanism and employ the attention weights to identify correlated tokens. The $\\alpha$-th self-attention head is described as follows:\n$A_{\\alpha} = softmax(\\frac{Q_{\\alpha} K_{\\alpha}^T}{\\sqrt{d_k}})$,\nwhere $\\alpha \\in [1, ..., M]$, $Q_{\\alpha} = XW^Q_{\\alpha}$ and $K_{\\alpha} = XW^K_{\\alpha}$, with $Q_{\\alpha}, K_{\\alpha} \\in \\mathbb{R}^{T \\times d_k}$, $d_k$ denotes the dimension of the key and query vectors. $X = E_{pos}(S') \\in \\mathbb{R}^{T \\times E}$ denotes the input matrix to the attention layer, where $E_{pos}(S')$ is the positional encoding applied to the output $S'$ from the VMASK layer. The score $A_{\\alpha(i,j)}$ indicates how much attention token $i$ places on token $j$.\nTo encourage highly correlated token pairs to exhibit high polysemanticity, we introduce a loss term $L_{Interaction} = \\sum_{\\alpha} \\sum_{i,j}^T A_{\\alpha(i,j)} (1 - I_{i,j}^A)$, where $I_{i,j}^A$ is the Interference of the attention weights matrix for the $\\alpha$-th attention head. This loss term penalizes highly correlated tokens that exhibit low interference values.\nProposed Loss Function The loss term is now defined as:\n$L = L_{CE} + \\lambda_{Imp} \\cdot L_{Imp} + \\lambda_{Inter} \\cdot L_{Inter}$\n$= L_{CE} + \\frac{\\lambda_{Imp}}{NT} \\sum_{n=1}^N \\sum_{i=1}^T \\frac{P_i^V}{E}$\n$+ \\frac{\\lambda_{Inter}}{NMT^2} \\sum_{n=1}^N \\sum_{\\alpha=1}^M \\sum_{i,j} A_{\\alpha(i,j)} (1 - I_{i,j}^A)$"}, {"title": "4 Experimental Setup", "content": "The proposed method is evaluated on two classification tasks using a standard transformer model.\nDatasets We adopt two benchmark datasets: Stanford Sentiment Treebank binary version SST-2 (Socher et al., 2013) and movie reviews IMDB (Maas et al., 2011). Table 4 in Appendix B presents the dataset statistics.\nModel We use a typical transformer architecture with a single layer, following the standard setup (Vaswani, 2017). This includes the complete transformer framework with its attention mechanism and positional encoding. In the multi-layer perceptron (MLP) section, we use two fully connected layers: we first expand the dimensionality by a factor of four, apply a ReLU activation, then reduce it back by the same factor, aligning with the commonly used configuration in transformer models. The model uses random embeddings to avoid the influence of pre-trained embedding information. Table 5 in Appendix B presents the model statistics."}, {"title": "5 Results and Analysis", "content": "SAFR Improves Interpretability. After applying SAFR, the first fully connected layer redistributes neurons, making Capacity interpretable by jointly capturing interference and polysemanticity, thereby reflecting both interaction and importance, as validated by SRS evaluation. Table 1 shows that SAFR achieves SRS scores of 17.21 and 28.48 on the SST-2 and IMDB datasets, outperforming the baseline and demonstrating improved interpretability. Notably, even without regularization ($\\lambda_{Imp} = 0, \\lambda_{Inter} = 0$), the VMASK layer before the Transformer block enhances SRS scores,\nSensitivity to $k$ Selection. Figure 3 illustrates the effect of token removal based on capacity. The $k$% top-scored tokens are directly removed from the original text to ensure that the evaluation reflects the model's ability to perform with reduced input information. As tokens are gradually removed, accuracy declines consistently. The greater decline in our model compared to the baselines suggests better interpretability.\nAverage Capacity Across the Vocabulary. Table 3 presents an analysis of the average capacity per token status (important or not w.r.t VMASK) on the entire test set. Tokens are categorized based on VMASK scores, with the top 30% identified as important and the rest 70% as less important. These results demonstrate SAFR's effectiveness in prioritizing and allocating greater representational capacity to task-relevant tokens. This strategic allocation improves the model's clarity in decision-making."}, {"title": "6 Related Work", "content": "Superposition in neural networks has gained attention, with foundational work by (Arora et al., 2018; Goh, 2016). Olah et al. (2020) developed this idea into the \"superposition hypothesis\" and initiated studies on mechanistic interpretability concerning polysemantic neurons and circuits. Lecomte et al. (2024) showed that polysemanticity can emerge incidentally through regularization and neural noise. Elhage et al. (2022) illustrated superposition in simplified networks, while subsequent works explored its theoretical, empirical, and applications (Scherlis et al., 2022; Henighan et al., 2023; H\u00e4nni et al., 2024; Gurnee et al., 2023; Marshall and Kirchner, 2024; H\u00e4nni et al., 2024; Katta, 2024; Chen et al., 2023, 2024).\nInterpretability research includes works such as (Dreyer et al., 2024; Black et al., 2022; Wang et al., 2023). Meanwhile, challenges in knowledge (Hu et al., 2024) and identifying universal feature spaces across models (Lan et al., 2024) mark promising directions for future research."}, {"title": "7 Conclusion", "content": "In this work, we introduced SAFR, an approach to enhance model interpretability by strategically regularizing feature superposition. Experiments on SST-2 and IMDB show that SAFR improves interpretability, as measured by our SRS metric, without compromising prediction performance.\nOur method provides insights into the relationship between superposition and interpretability and offers a framework for visualizing neuron allocation. It contributes to mechanistic interpretability and suggests promising directions for extending the approach to larger models and wider applications."}, {"title": "8 Limitation", "content": "This study has several limitations. First, experiments were conducted using a single-layer transformer model; future work should examine the scalability of SAFR with more complex architectures. Second, while focused on classification task, the applicability of SAFR to other NLP tasks\u2014such as natural language inference, question answering, and text generation-remains unexplored. Third, there is a need for more comprehensive and standardized evaluation metrics to assess SAFR effectively. Finally, SAFR does not fully elucidate the causal mechanisms behind the model's decision-making process. Addressing these challenges offers valuable opportunities for future research."}, {"title": "9 Ethic Statements", "content": "Our research focuses on understanding and controlling the inner workings of transformer models, without collecting or using any human data; no personal or sensitive information is handled in this study. All datasets used in this work are public."}, {"title": "A VMASK Introduction", "content": "VMASK (Chen and Ji, 2020) is a variational word mask layer that is inserted into a neural text classifier and trained with the model. It learns to limit the flow of globally irrelevant or noisy word-level feature information to subsequent network layers, thus forcing the model to focus on the important features for prediction."}, {"title": "B Statistics", "content": "This section provides the statistical summaries of the datasets and the model."}, {"title": "C Neuron Allocation Accross Layers", "content": "This section presents the observations regarding neuron allocation across the various layers, as visualized in Figure 4 and 5."}, {"title": "D Regularization Hyperparameter Tuning", "content": "This section presents the results of hyperparameter tuning for $\\lambda_{Imp}$ and $\\lambda_{Inter}$, as summarized in Table 6."}]}