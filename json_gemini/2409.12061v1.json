{"title": "Generalized Robot Learning Framework", "authors": ["Jiahuan Yan", "Zhouyang Hong", "Yu Zhao", "Yu Tian", "Yunxin Liu", "Travis Davies", "Luhui Hu", "ZhiCheng AI", "Harvard University", "Tsinghua University"], "abstract": "Imitation based robot learning has recently gained significant attention in the robotics field due to its theoretical potential for transferability and generalizability. However, it remains notoriously costly, both in terms of hardware and data collection, and deploying it in real-world environments demands meticulous setup of robots and precise experimental conditions. In this paper, we present a low-cost robot learning framework that is both easily reproducible and transferable to various robots and environments. We demonstrate that deployable imitation learning can be successfully applied even to industrial-grade robots, not just expensive collaborative robotic arms. Furthermore, our results show that multi-task robot learning is achievable with simple network architectures and fewer demonstrations than previously thought necessary. As the current evaluating method is almost subjective when it comes to real-world manipulation tasks, we propose Voting Positive Rate (VPR)-a novel evaluation strategy that provides a more objective assessment of performance. We conduct an extensive comparison of success rates across various self- designed tasks to validate our approach. To foster collaboration and support the robot learning community, we have open- sourced all relevant datasets and model checkpoints, available at https://huggingface.co/ZhiChengAI", "sections": [{"title": "I. INTRODUCTION", "content": "Robotic research [1]-[3] has increasingly focused on the potential of using imitation learning [4] for robotic manip- ulation [5], driven in part by the growing integration of generative AI [6] into industry. However, the high cost of current robot learning pipelines has remained a significant barrier, limiting access to practical development and scaling up. Hence, we propose a low-cost real robot imitation learning framework that facilitates efficient data collection, training, and inference on a industrial-grade robotic arm with common household control devices, making it possible for a broader range of researchers and practitioners to engage in robotics innovation.\nTo rigorously evaluate the effectiveness of the proposed framework, we designed 10 distinct robotic tasks, each characterized by specific features tailored to real-world con- ditions. A comprehensive analysis of these tasks is provided, encompassing both the rationale behind their design and their empirical performance in deployment. Section III-D delves into the requirements and methodologies involved in the design of these tasks, while Section IV systematically ex- amines how task-specific characteristics impact performance outcomes in real-world testing scenarios.\nIn addition to the versatility of using a general-purpose robotic arm that can meet the demands of various in- dustrial scenarios, our framework also demonstrates model generalization. Specifically, we successfully enable a single checkpoint to perform multiple tasks by combining datasets and applying minor adjustments to the training strategy, which are discussed in Section IV-C and Section IV-E. To support the broader research community, we also release the entire dataset generated through our framework. This dataset, encompassing diverse tasks and environmental conditions, serves as an additional resource for future research in robot learning, promoting reproducibility and enabling further ad- vancements in the field.\nOur main contributions can be summarized as:\n\u2022 We introduce a novel low-cost imitation learning frame- work that is accessible even to individual researchers.\n\u2022 We collected over 4,000 episodes across 10 distinct real-world robotic tasks, which are publicly released alongside our findings on the correlation between task difficulty and performance.\n\u2022 We demonstrate model generalization by successfully enabling task adaptation through minimal dataset inte- gration and slight modifications to the training process."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Imitation Learning", "content": "Imitation Learning (IL) [7], [8] is a prominent approach in robotics and autonomous systems, enabling agents to acquire complex behaviors by mimicking expert demonstrations [9]\u2013 [11]. Among the various IL techniques, behavioral cloning (BC) has been widely used, framing the task as a super- vised learning problem where actions are directly mapped from perceptions [12], [13]. A well-known example is the ACT policy [14], which exemplifies explicit policy learning. However, explicit policies often struggle with multimodal behavior [15], as they tend to perform poorly in scenarios with diverse demonstrated actions."}, {"title": "B. End-to-End Robot Learning", "content": "A significant challenge in robotic learning is the avail- ability of diverse and abundant training data. Sim-to-real transfer has been proposed as a solution, where robot models are initially trained in simulation and then adapted to real- world tasks using domain adaptation techniques [19]\u2013[21]. Advances in simulation software, particularly in realistic graphics and physics [22]-[24], have improved the fidelity of simulated data, making it more comparable to real-world data. However, sim-to-real approaches often fail to capture unforeseen real-world events [25], and require specialized skills for setting up and tuning simulations.\nAlternatively, end-to-end robot learning offers more acces- sible, data-driven approach to robot training with imitation learning, as it eliminates many of the complexities of sim- to-real techniques. Projects such as Dobb-E [26], ALOHA [14], and UMI [27] have demonstrated successful learning from human demonstrations for end-to-end robot learning. However, these methods are either constrained by profes-"}, {"title": "III. FRAMEWORK SETUP", "content": ""}, {"title": "A. Hardware Preparation", "content": "Our hardware devices for data collection and model de- ployment facilities are listed as follows:\n\u2022 Robotic Arm: The robotic arm used in our experiments is an industrial-grade model, with a custom-built soft- ware development kit (SDK) and a cable connection for communication. While we refrain from disclosing the specific robot model, our framework is designed to be agnostic to the hardware platform. Thus, if our framework is deployable on this robot, it can be adapted for use on almost any robotic system.\n\u2022 Cameras: Two Intel RealSense D415 RGB-D cameras were utilized for frame acquisition. One camera was mounted on the end-effector of the robotic arm to provide a close-up perspective, while the second was positioned to offer a global view of the workspace. The camera type is not a strict requirement, any RGB camera can be used in place of the RealSense D415, depending on individual circumstances.\n\u2022 Controller: An Oculus Quest 2 right-hand controller was employed for data collection. Although the con- troller facilitated the robot's movement, the headset was also required to ensure a stable spatial coordinate system. The controller's \"B\" button was programmed to stop the robot's motion, enabling the operator to reposition the controller without interfering with the arm's operations."}, {"title": "B. Data Collection", "content": "Before arranging human manipulators to collect data for real-task, we took precautions to ensure the controller oper- ated within an obstacle-free area to avoid introducing sudden errors in the control data. To synchronize the operator's movements with the robotic arm's movements in real space, we adjusted the orientation of the headset. This adjustment aligned the coordinate systems, providing an intuitive inter- action experience where the controller's movements corre- sponded directly with the robotic arm's direction.\nWe employ a widely adopted strategy for robot data collection, where the trajectory of the robot is recorded alongside timestamps and corresponding video footage. The trajectory data includes the absolute position and orientation of the robot's end effector (x, y, z, ox, oy, oz), along with additional information indicating the gripper's state. In our setup, the gripper's state is represented using Pulse Width Modulation (PWM), which reflects the motor-driven force applied to the gripper.\nDuring data collection, two operators are involved: one arranges the objects based on specific scenarios and their own intuition, while the other uses a controller to remotely operate the robot arm to complete the task. These operators are excluded from the human evaluation process, as discussed in Section III-E. The collected data is tagged with the operators' names to distinguish between identical tasks performed by different collectors, a distinction that serves an important purpose, which will be elaborated on in Section IV-D.\nIn our setup, the number of episodes for each task is not fixed; it is primarily determined by the complexity of the task, including factors such as logical intricacy, the number of objects involved, and the required number of generalization scenarios. As a general guideline, we use 10 episodes per generalization scenario, typically requiring around 100 demonstrations for a specific task. Under optimal conditions, this process takes approximately 0.5 to 1 hour of manual effort. The detailed number is listed in Table I."}, {"title": "C. Robot Policy System", "content": "Following the design of Diffusion Policy [1], we decouple the policy for robot control system into two components: the perception module, which processes information from the physical world and generates embeddings, and the action prediction module, which takes these embeddings as input and outputs corresponding trajectories.\n1) Perception Module: Our perception data consists of visual inputs from two cameras: one mounted on the robotic arm's wrist for a first-person view, and the other providing a third-person perspective. Additionally, low-dimensional state information such as the position and orientation of the end effector and the gripper's PWM signal is included. This data represents the robot's perception of the physical world and is processed by a deep learning network to be transformed into embeddings. We experimented with several network architectures, including ResNet18, ResNet34, and ResNet34 configured as a Feature Pyramid Network (FPN) [28], [29]. Of these, the FPN ResNet34 yielded the best performance, significantly enhancing visual feature representation by lever- aging multiple resolutions.\n2) Action Prediction Module: This model is designed to map the encoded perception data into a real trajectory that can effectively control the robot arm and gripper to complete tasks. In our current setup, we use the Denoising Diffusion Probabilistic Model (DDPM) [18] as the backbone, iteratively denoising initially randomized trajectories to make them practical for robot manipulation. We explored two main network architectures within the DDPM framework: Convolutional Neural Networks (CNN) and Transformers.\nThese two modules function together as a policy control system within our framework, but the choice of network architecture for each module is flexible and not strictly constrained to any specific design."}, {"title": "D. Task Design", "content": "Well-defined tasks are crucial for evaluating model perfor- mance and ensuring consistent, quantifiable results. In this study, we present 10 real-world tasks, as shown in Figure 3, using low-cost, easily accessible objects to facilitate easy replication. Each task targets specific model capabilities, such as color recognition in \"BlockPick\" and size differen- tiation in \"PickSmall,\" allowing us to evaluate the model's ability to handle diverse features of scenarios in industrial demands.\nThe tasks are designed to balance simplicity in setup with complexity in execution, providing a broad range of challenges to test the model's generalization abilities. This approach ensures that the tasks are accessible to the wider research community while offering meaningful insights into the model's performance across different dimensions of perception and decision-making."}, {"title": "E. Voting Positive Rate", "content": "Evaluating model performance in real-world environments during training is challenging, as continuous real-time eval- uation is often impractical. A common approach is to test models in simulation environments, which can help approx- imate real-world conditions. However, simulations require meticulous design, including the modeling of physical dy- namics and defining success metrics. One key difficulty is de- termining when a model should be considered to have failed in simulation, a decision that is much easier for humans to make in real-world tests. We chose to forgo simulated evaluation not only because it is difficult to implement with low-cost hardware (designing a custom simulation environ- ment for a personal robot arm may not be feasible), but also because real-world testing is inevitable when transitioning into industry for practical applications. As such, simulated"}, {"title": "F. Checkpoint Selection", "content": "Checkpoint selection in imitation learning remains a chal- lenging problem, as optimizing the loss function does not necessarily correlate with achieving the highest success rate in real-world scenarios. As noted in [2], the loss curve during training often lacks a clear correlation with actual task performance, which is aligned with the phenomena we also observed in our experiments. Given the impractical- ity of evaluating model performance after every epoch in real-world tasks, our approach involves training the model overnight, saving checkpoints every 50 epochs, and subse- quently evaluating each checkpoint to select the one with the highest success rate."}, {"title": "G. Model Deployment", "content": "The deployment of the trained model is currently executed on our PC, with the generated actions transferred to the robot via a cable. Our alignment strategy involves having the model predict actions along with their corresponding desired timestamps. The robot then moves based on both the action and the timestamp. If a timestamp is deemed invalid (e.g., if it exceeds the allotted time), the action is discarded accordingly."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": ""}, {"title": "A. Task Analysis", "content": "Following the standardized checkpoint selection method discussed in Section III-F, we present our analysis of the relationship between task success rates and task-specific characteristics. To minimize the impact of model architecture and training strategy, we utilized a consistent architecture and selected the optimal checkpoint based solely on performance, regardless of training duration."}, {"title": "B. Model Architecture Ablation Study", "content": "As aforementioned III-C, we decoupled the whole con- trolling system into perception module and action prediction module, and each of them can be instanced by certain net- work architectures. Hence, we conducted an ablation study to evaluate the performance of various model architectures by modifying both of them.\nFor simpler tasks such as PickPlace and Basketball, the ResNet18+UNet architecture achieved relatively good VPR, with a notable 92% rate. This indicates that CNN-based noise prediction networks are effective when the task complexity is relatively low, likely due to their ability to capture local spatial patterns efficiently.\nHowever, transformer-based architectures perform gen- erally better compared to CNN-based. When task com- plexity increased, the outnum becomes larger. Transformers particularly excelled in the most complex tasks, such as PickSmall, where ResNet34+Transformer outperformed all other architectures with a 66.7% success rate. This confirms that transformers are well-suited for handling tasks requiring more sophisticated temporal and spatial reasoning, as their self-attention mechanism allows them to capture long-range dependencies more effectively than CNNs.\nAmong Transformer group, FPN+Transformer is consid- ered as the optimal choice, as it generally perform better. For instance, in the CupStack task, the FPN+Transformer architecture outperformed others, achieving an 80% suc- cess rate, compared to 70% for ResNet34+Transformer and 57.5% for ResNet18+Transformer. Similarly, in the PickBig task, FPN+Transformer achieved 76.6%, signifi- cantly outperforming the ResNet18+Transformer (50%) and ResNet34+Transformer (26.7%). When simply enlarge the perception model (e.g., from ResNet18 to ResNet34), the improvement of performance is not significant, ResNet34 even perfor worse on some simple tasks (e.g., 'PickPlace', 'BlockPick' and 'Basketball').\nThe ablation study underscores that while CNN-based architectures may perform well in simpler environments, transformer-based architectures offer a clear advantage in complex, dynamic tasks. These results suggest that, for tasks requiring intricate action sequences and environmental interactions, transformer-based models should be prioritized for their robustness and performance."}, {"title": "C. Dataset Scaling Matters More Than Training Scaling", "content": "We explicitly explored the effects of scaling both the dataset size and the model architecture by introducing addi- tional demonstrations and increasing the model's hidden di- mensions (i.e., adding more learnable parameters) alongside extended training epochs. Interestingly, our results indicate that simply increasing the number of model parameters and training time does not lead to improved performance. In fact, this approach often results in a significant drop in perfor- mance, rendering the model's success rate unacceptably low.\nConversely, increasing the number of demonstrations con- sistently improves model performance, as evidenced by the results from the 'PickSmall', 'PickBig', and 'PickBigV2' tasks. These three tasks were deliberately designed to be similar, with the primary difference being the number of demonstrations. The results are compelling: by increasing the number of demonstrations from 60 to 120, the VPR improves by 9.9%. However, this improvement tends to plateau; when increasing the demonstrations from 120 to 240, the VPR only improves by 1.7%. In addition to expanding the dataset, we also experimented with using a larger transformer model, but the results were far from expected. Although we aimed to replicate the scaling laws' success observed in other domains by simultaneously increasing both dataset size and model complexity, our findings indicate that this strategy requires further exploration to be successfully applied."}, {"title": "D. Data Quality", "content": "During our experiments, we identified a recurring data quality issue, which we present here. When two data col- lectors independently gathered data for the same tasks, we observed significant variations in model performance, even when the demonstration lengths and training epochs were kept constant. These findings were consistent across most of the tasks we designed, particularly for tasks with longer demonstrations. While it is possible that this performance fluctuation could be attributed to the checkpoint selection problem mentioned earlier, we are inclined to believe it is more closely related to the proficiency level of the data collectors [1], [2]. However, the definition of \"proficiency\" remains unclear to us. Therefore, we are simply reporting our perceptions here and leave this question open for further investigation by our peers."}, {"title": "E. Model Generalization", "content": "Multi-task Generalization: Similar to the early stages of deep learning, prior work in imitation learning has pre- dominantly focused on training models for single, specific tasks. In contrast, we have developed a multi-task learning approach by combining two datasets and training on a pre- existing checkpoint that was already fine-tuned on one of the tasks. The efficacy of this approach was demonstrated by fine-tuning the 'BlockPick' task using a model check- point previously trained on the 'Basketball' task for 650 epochs. Remarkably, the 'BlockPick' task was successfully completed after only 50 additional epochs of fine-tuning,"}, {"title": "V. FUTURE WORK", "content": "Imitation learning should be adaptable across various robotic platforms and industrial tasks, without relying on overly complex or impractical setups. In this study, we utilized a Diffusion Policy-inspired architecture with mod- ifications to the perception module and action prediction module. Future direction could explore the utilization of human language instruction [3], [33], [34] to strengthen the logic deduction capacity.\nThough our framework can work smoothly from data collection to model deployment, which provides a pipeline for efficient replicating based on data driven paradigm. One of the main obstacle is the required data size, and our demand is to decrease the data size needed while remaining the model performace. One of the convincing method is to leverage the capability of model pretrained on large, open- source robotic trajectory datasets, such as X-Embodiment [35], which allows policies to adapt to novel tasks and environments with minimal fine-tuning.\nIn summary, our future work will focus on progressively reducing the data dependency of our framework by lever- aging advanced transfer learning models and methods. We believe this approach will contribute significantly to the robot learning community."}, {"title": "VI. CONCLUSION", "content": "In this work, we present a cost-effective and generalized framework for deploying robotic systems in industry-relevant tasks, significantly reducing hardware expenses compared to conventional research setups. Our methodology minimizes the time required for data collection and model training. This efficiency makes the framework highly accessible to a wide range of users, from academic researchers to industry practitioners.\nWe also provided detailed guidelines for task design, suc- cess rate evaluation, and optimal checkpoint selection. Our experiments demonstrated the feasibility of training multi- task models on real-world tasks, and we observed that even minor adjustments to the model architecture can result in substantial performance improvements across different tasks. These findings contribute valuable insights into optimizing model architectures for diverse, complex environments.\nIn summary, we introduced a low-cost imitation learning framework supported by a dataset of 10 real-world tasks, designed to accelerate progress in embodied intelligence. By fostering research and open-source collaboration, we aim to enable the development of emergent capabilities in robotics, similar to those observed in large-scale language models, thus driving future advancements in autonomous systems."}]}