{"title": "Inference-Friendly Models With MixAttention", "authors": ["Shashank Rajput", "Ying Sheng", "Sean Owen", "Vitaliy Chiley"], "abstract": "The size of the key-value (KV) cache plays a critical role in determining both the maximum context length and the number of concurrent requests supported during inference in modern language models. The KV cache size grows proportionally with the number of attention heads and the tokens processed, leading to increased memory consumption and slower inference for long inputs. In this work, we explore the use of MixAttention, a model architecture modification closely related to a blog published by Character.AI [5]. MixAttention combines sliding window attention, where only a small subset of recent tokens is stored in the KV cache, with KV cache sharing across layers. Our experiments demonstrate that MixAttention significantly reduces memory usage and improves inference speed without sacrificing model performance in both short and long-context tasks. We also explore various configurations of this architecture, identifying those that maintain quality across evaluation metrics while optimizing resource efficiency.", "sections": [{"title": "Introduction", "content": "Transformer-based language models are getting increasing popular in consumer usage as well as industrial workloads. A general trend seen so far has been that bigger models are better at tasks than smaller models, but that comes at the cost of increased inference cost and slower speed [10, 23]. Further, the memory consumption and latency during inference for causal attention transformer models like Llama [29, 7], GPT [21], and Gemini [28] increases linearly with the input length. This causes problems for use cases such as Retrieval Augmented Generation (RAG) [16], where the input to the models can become very long [15].\nAn important component of the Transformer architecture whose memory footprint grows with model size and input length is its KV cache. When generating the next output token, the transformer model processes all the tokens in its context through the attention mechanism. For causal attention models, the internal representation of the previous tokens in the context is unaffected by the newer tokens, and hence it can be cached. This is stored in the KV cache, and its size increase with context length (since it caches information for each token seen so far) and with the size of the model (since there is a separate KV cache for each KV head in the model). Larger KV cache not only means more memory consumption by the model, but it also slows down inference because for long inputs, LLM inference can be dominated by the I/O cost of moving the KV cache from HBM to the GPU's shared memory. Thus, it has become imperative to reduce the size of the KV cache for faster and cost-effective inference with modern LLMs."}, {"title": "Related Work", "content": "Reducing the KV cache size has been an area of active research, with many different approaches. In this section we talk about some of them.\nTransformer models [30] differ from traditional Recurrent Neural Networks (RNNs) [25] and modern State Space Models [8, 22] in that Transformer models have an internal representation (the KV cache) that grows linearly with the length of the input. This allows RNNs and SSMs to be faster and more memory efficient during inference. However, it has been seen that while such models are competitive with Transformer models on certain tasks, Transformer models still outperform equally-sized pure RNN or pure SSM models on other tasks, especially some long context tasks [31]. Thus, hybrid architectures which interleave attention layers and SSM layers have been proposed, that show that such hybrid architectures exhibit good long context abilities."}, {"title": "MixAttention", "content": "Standard transformer models use global attention in each layer. To create inference-friendly model architectures, we use a combination of sliding window attention layers, standard attention, and KV cache reuse layers. Below is a brief discussion on each component:"}, {"title": "Experiments", "content": "We used LLM Foundry [20] to train MixAttention models. Similar to prior work on training long context models [5, 6], we followed a multi-stage training procedure to impart long context abilities to the models.\n1. We pretrained the models with a RoPE theta of 0.5M on 101B tokens, where each sequence sequence has been truncated to 4k token length.\n2. To increase the context length, we then trained the model on 9B tokens on a mix of natural language and code data, where the sequences have been truncated to 32k tokens. We increased the ROPE theta to 8M for this stage. When training at 32k context length (i.e., this step and the next step), we trained only the attention weights and froze the rest of the network. We found that this delivered better results than full network training.\n3. Finally, we trained the model on a 32K-length, synthetic, long-context QA dataset [5, 8].\nTo create the dataset, we took natural language documents and chunked them into 1k-token chunks. Each chunk was then fed to a pretrained instruction model and the model was prompted to generate a question-answer pair based on the chunk. Then, we concatenated chunks from different documents together to serve as the \"long context.\" At the end of this long context, the question-answer pairs for each of the chunks were added. The loss gradients were computed only on the answer parts of these sequences.\nThis phase of training was conducted on 500M tokens (this number includes the tokens from the context, questions, and answers). The RoPE theta was kept at 8M for this stage."}, {"title": "Evaluation", "content": "The models were evaluated on the Mosaic Evaluation Gauntlet v 0.3.0 [19] to measure model quality across various metrics including reading comprehension, commonsense reasoning, world knowledge, symbolic problem solving, and language understanding. To evaluate the models' long context abilities, we used RULER [12] at a context length of 32000 tokens. RULER is a composite benchmark consisting of 13 individual evals of the following types:\nNeedle-in-a-haystack (NIAH): These types of evals hide a single or multiple keys and values in a long text, and the model is evaluated on its ability to retrieve the correct value(s) from the long context for a given key(s).\nVariable Tracking (VT): This eval provides the model with a long context containing variable assignment statements, and the model is tasked to figure out which variables have a particular value by the end of all the variable assignments.\nCommon and Frequent Word Extraction (CWE and FWE): These tasks ask the model to extract the most common or frequent words from the text.\nQuestion Answering (QA): Given a long context, the model is asked a question from somewhere in the context and the model is evaluated on whether it can correctly answer that question.\nWe used SGLang [35] to deploy our models on 1 NVIDIA H100 GPU to run RULER and get inference speed and memory consumption metrics."}, {"title": "Results", "content": ""}, {"title": "Position and Count of Standard Attention KV Caches", "content": "To measure the effect of the position and count of the standard attention KV caches, we tried four variants (Figure 3). All the configurations are variants of the configuration proposed in Character.AI's blog [5].\nThis variant has a single standard attention KV cache, which is the KV cache of the first layer. All the other standard attention layers share this KV cache.\nThis variant is the same as MA, but the last layer is a sliding window attention layer. This was done to measure how much having standard attention in the last layer affects long-context abilities.\nThis variant is similar to MA, but the first standard attention layer is offset to a later layer to allow the model to process the local context for a few layers before the standard attention layer is used to look at longer contexts.\nThis variant computes two standard attention KV caches (at the first and thirteenth layer), which are then shared with another standard attention layer each.\nWe compared these models to a transformer model with Standard Attention and a transformer model with Sliding Window Attention in all layers.\nWhile the loss curves in Stages 1 and 2 of training were close for all the models, we found that in Stage 3 (training on long context QA dataset), there was a clear bifurcation in the loss curves (Figure 4, top). In particular, we see that configurations MA and MA-EndSlide show much worse loss than the others. These results are consistent with the long context RULER evals, where we found that MA and MA-EndSlide performed much worse than others (Figure 4, bottom). Their performance was similar to the performance of the network with only sliding window attention in all layers. We think the loss in Stage 3 correlates well with RULER evals because unlike Stages 1 and 2,"}, {"title": "KV cache sharing in sliding window layers", "content": "We found that increasing the sharing between sliding window layers (Figure 5) degraded the model's long context performance: MA-Offset-SlideShare was worse than MA-Offset and MA-Pairs-SlideShare was worse than MA-Pairs (Figure 6). This shows that the KV cache sharing pattern amongst the sliding window layers is also important for long context abilities. We have provided some more ablation experiments in the appendix."}, {"title": "Gauntlet Evals", "content": "Using the Mosaic Eval Gauntlet v0.3.0 [19], we measured the performance of MixAttention models on standard tasks like MMLU [9], HellaSwag [33], etc. to verify that they retain good shorter context abilities. All of the tasks in this eval suite have context lengths of less than a few thousand tokens.\nWe found that MixAttention models have similar eval metrics to the baseline model on commonsense reasoning, language understanding, and world knowledge. However, we see that they perform worse on reading comprehension. An interesting open question is if a different MixAttention configuration or training MixAttention models longer can recover the reading comprehension abilities."}, {"title": "Inference Speed and Memory Consumption", "content": "We benchmarked the inference speed and memory consumption of MixAttention models by deploying them on a single NVIDIA H100 GPU using SGLang and querying them with 300 prompts, with input length 31000 and output length 1000. In Figure 8, we see that the inference speed of MixAttention models is much faster than standard attention models. We also see in Figure 8 that with MixAttention, we can support a much larger inference batch size in terms of total number of tokens."}, {"title": "Conclusion", "content": "We find that MixAttention models are competitive with standard attention models on both long- and short-context abilities while being faster during inference and supporting larger batch sizes. We note that on some long context tasks like Variable Tracking and Common Word Extraction, neither MixAttention nor standard attention models perform well. We believe this was because our models weren't trained long enough or the models need a different kind of long context data to be trained for such tasks. More research needs to be done to measure the impact of MixAttention architectures on such metrics.\nWe encourage others to explore more MixAttention architectures to learn more about them. Below are a few observations to help with further research:\nAdding a standard attention layer in the initial layers by itself does not seem to help long context abilities (for example, see MA-NoShare-1 in the appendix), even if the KV cache from that layer is reused in layers deeper into the network (MA and MA-EndSlide). Hence we recommend placing the first standard attention layer deeper in the network (like MA-Offset) or having multiple standard attention layers, at least one of which is computed at a deeper layer (like MA-Pairs).\nSliding window layers also contribute to the model's long context abilities. Increasing the KV cache sharing amongst the sliding window layers worsened long context abilities (MA-Offset-SlideShare and MA-Pairs-SlideShare). For that reason, we think that the 2-3 sharing pattern in sliding window layers [5] seems to strike a good balance.\nSharing standard attention KV caches between consecutive layers gave mixed results, with slightly worse accuracy on long context QA tasks (see the appendix).\nIn our experiments, MA-Offset and MA-Pair showed great speedup and memory savings during inference, while also maintaining long and short context abilities. Hence, MA-Offset and MA-Pairs might be good configurations for further research.\nIn general, there is a large hyperparameter space to explore, and we look forward to seeing a variety of new strategies for reducing the cost of inference via combinations of sliding window attention and KV cache reuse."}, {"title": "A Additional Ablations", "content": ""}, {"title": "Sharing Standard Attention KV caches between consecutive layers", "content": "Since the transformer layers progressively update the latent representation of a token as it progresses through the layers, the Query, Key, and Value tensors might have significantly different representations for layers that are far apart. Hence, it might make more sense to share KV caches between consecutive layers. To test this, we compared four such configurations: MA-Successive-1, MA-Successive-2, MA-Successive-3, and MA-Successive-4 against MA-Pairs. These configurations vary the positions of the standard KV attention layers and the distance between the consecutive pairs of standard KV attention layers.\nWe determined that all the models have similar loss curves and similar performance on NIAH single 1, 2, and 3 tasks, which we consider to be the easiest long context tasks. However, we did not see a consistent pattern across the other NIAH tasks. For long context QA tasks, we found that MA-Pairs was slightly better than the others. These results indicate that sharing standard attention KV cache between layers that are further apart does not lead to any significant degradation in long context abilities as compared to sharing standard attention KV cache between consecutive layers."}, {"title": "Effect of sharing standard attention KV cache", "content": "To test the effect of sharing KV cache between standard attention layers, we tried out three configurations: MA-NoShare-1, MA-NoShare-2, and MA-NoShare-3 (Figure 11). We found that MA-NoShare-1 performed very badly on RULER, indicating its lack of long context abilities. However, MA-NoShare-2 and MA-NoShare-3 were comparable to MA-Offset on long context tasks. Hence, we think that further research is needed to ascertain the effects of sharing standard attention KV cache."}]}