{"title": "Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media", "authors": ["Zhen Sun", "Zongmin Zhang", "Xinyue Shen", "Ziyi Zhang", "Yule Liu", "Michael Backes", "Yang Zhang", "Xinlei He"], "abstract": "Social media platforms are experiencing a growing presence of AI-Generated Texts (AIGTs). However, the misuse of AIGTs could have profound implications for public opinion, such as spreading misinformation and manipulating narratives. Despite its importance, a systematic study to assess the prevalence of AIGTs on social media is still lacking. To address this gap, this paper aims to quantify, monitor, and analyze the AIGTs on online social media platforms. We first collect a dataset (SM-D) with around 2.4M posts from 3 major social media platforms: Medium, Quora, and Reddit. Then, we construct a diverse dataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines popular open-source datasets and our AIGT datasets generated from social media texts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors. With this setup, we identify the best-performing detector (OSM-Det). We then apply OSM-Det to SM-D to track AIGTs over time and observe different trends of AI Attribution Rate (AAR) across social media platforms from January 2022 to October 2024. Specifically, Medium and Quora exhibit marked increases in AAR, rising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast, Reddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the same period. Our further analysis indicates that AIGTs differ from human-written texts across several dimensions, including linguistic patterns, topic distributions, engagement levels, and the follower distribution of authors. We envision our analysis and findings on AIGTs in social media can shed light on future research in this domain.", "sections": [{"title": "1 Introduction", "content": "The rapid development of Large Language Models (LLMs) has markedly enhanced the quality of AIGTs, enabling the use of models like GPT-3.5 [30] in daily life to produce high-quality texts, such as in academic writing [11], question- answering [18], and translation [48]. These AIGTs are often indistinguishable from Human-Written Texts (HWTs), pre- senting AIGT detection as a crucial yet challenging task for effective classification. On social media platforms, the use of LLMs to answer questions can contribute to the spread of misinformation [52]. Furthermore, AIGTs may be delib- erately used for information manipulation or the dissemina- tion of fake news, potentially resulting in serious societal impacts [13]. To better understand the prevalence of AIGTS on social media platforms, we aim to quantify and monitor its presence, addressing the question: On social media, are we already interacting with AI-generated texts? Currently, numerous detectors have been developed to detect AIGTs. According to the MGTBench [14], these detectors are broadly divided into two categories: metric-based [9, 28] and model-based detectors [4, 16, 39], some of which have shown high accuracy and robustness. While these detectors have been applied in controlled settings, re- cent studies have explored their effectiveness in real-world scenarios. Hanley et al. [13] conduct AIGT detection on news website articles, with a primary focus on content generated by GPT-3.5 and others from Turing benchmark, which includes various pre-2022 models [46]. Furthermore, Liu et al. [24] carry out detection tests for ChatGPT-generated content on arXiv papers. However, they do not consider recent popu- lar models, such as Llama [44] and GPT-4 [31], which also possess powerful text generation capabilities and are widely adopted. We thereby consider a broader range of models in our efforts to detect AIGTs on social media. To quantify and monitor AIGTs on social media, we collect textual data across 3 popular platforms ranging from January 1, 2022, to October 31, 2024, as most LLMs are released after 2022. After data preprocessing, we obtain 1,170,821 posts from Medium, 245, 131 answers from Quora, and 982,440 comments from Reddit. We name it as SM-D, short for Social Media Dataset."}, {"title": "2 Related Work", "content": "The growth in model parameters and training data has recently empowered LLMs to demonstrate exceptional language pro- cessing capabilities and few-shot learning abilities [51]. Since then, LLMs have gradually gained popularity, like GPT-4 [31] and Llama [44], enabling users to generate high-quality texts effortlessly. Yet, LLMs have raised concerns about potential misuse, such as fake news generation [50], academic mis- conduct [47], and performance degradation of training LLMS using AI content [5], making the detection of AI-Generated texts (AIGTs, also known as machine-generated texts) in- creasingly important [8]. He et al. [14] introduce the first benchmark, MGTBench, for standardizing the evaluation of different LLMs and experimental setups within the AIGT de- tectors. They broadly categorize the detectors into two main types: metric-based and model-based detectors. Metric-based detectors use pre-defined metrics, such as log-likelihood val- ues and rankings, to capture the characteristics of texts and identify AIGTs [9, 28, 41]. In contrast, model-based detec- tors rely on trained models to distinguish between AIGTs and HWTs [4, 10, 12, 16, 20, 24, 39]. For more introduction, refer to Appendix \u0412. Based on these AIGT detectors, some researchers have applied them to text detection in real-world scenarios. Han- ley et al. [13] train a detector using data generated by the ChatGPT and Turing benchmark model and conduct detec- tion tests on multiple news websites. Their study reveals that, from January 1, 2022, to May 1, 2023, the proportion of synthetic articles increased on news sites. Liu et al. [24] also conduct detection tests on arXiv and find a significant rise in the proportion of papers using ChatGPT-generated content, reaching 26.1% by December 2023. In contrast to their de- tection targets, we focus on detecting AIGTs on social media platforms and covering a broader range of LLMs. Macko et al. [25] construct a multilingual dataset based on instant messaging and social interaction platforms such as Telegram, Discord, and WhatsApp, using it to compare the performance of existing detectors. In contrast, our research focuses on providing an in-depth temporal analysis of AIGTs on content-driven social platforms like Medium, Quora, and Reddit."}, {"title": "3 Data Collection", "content": "In this section, we elaborate on the data collection process, which primarily includes two datasets: the social media dataset (SM-D) and the detector training dataset (AIGTBench)."}, {"title": "3.1 SM-D (Social Media Dataset)", "content": "Unlike previous research, we focus on social media plat- forms, including Medium, Quora, and Reddit, emphasizing content creation, sharing, and discussion. The introduction of platforms is in Appendix C. These platforms stand out for hosting longer, more detailed posts where users emphasize the depth and quality of the information they share. As shown in Table 1, we collect data from these social media platforms from January 1, 2022 to October 31, 2024. We consider this part as our social media dataset for analysis. For each platform, the detection targets are determined based on their distinct characteristics. On Medium, a blog hosting platform, we extract both the titles and contents of articles, treating the entire article as the detection target. On Quora, a question-and-answer platform, we select the corre- sponding answers to questions as the detection target. Sim- ilarly, on Reddit, which is known for its user-driven discus- sions, we also choose the response content as the detection target. Furthermore, we apply data filtering with the rules described in Appendix E."}, {"title": "3.2 AIGTBench (Detector Training Dataset)", "content": "To train the AIGT detectors, we consider two parts of the data. First, we consider 6 publicly available AIGT datasets and 5 common SFT datasets to form the training dataset (see Tables Al and A2 for dataset statistics and Appendix D for more details). Second, to increase the detector's general- ization capabilities on social media, we additionally collect data from the 3 social media platforms ranging from January 1, 2018, to December 31, 2021. We classify this data as HWTs, given that most LLMs had not been published during this period. We also design different LLMs writing tasks to generate AIGTs that align with the characteristics of platforms (Table A3 describes the statistics details). For Medium, which is primarily used for sharing articles and blogs, the core tasks are centered on writing. We design two LLM writing tasks: (1) polish articles to create polished versions; (2) based on the article's title and summary, direct- ing the LLM to generate complete article content, thereby simulating a writing scenario. For Quora and Reddit, which mainly focus on question answering and user interaction, we design two tasks: (1) polish texts like Medium and (2) query LLM directly answer questions, simulating a user interaction scenario. Detailed prompts are provided in Appendix F. Overall, the datasets used for training our detector and the distribution of LLM series are shown in Figure 1. This dataset includes 12 different LLMs, with a detailed introduction pro- vided in Appendix A. Within these datasets, the two most prevalent model series are the GPT Series, which accounts for 42.99%, and the Llama series, which represents 39.05%. GPT Series is the most widely used proprietary model and has played a pivotal role in the evolution of generative AI. As of January 2023, approximately 13M users interact daily with GPT-3.5 [49]. The Llama series models also have significant influences, as the report indicates that downloads of Llama models on the Hugging Face platform have nearly reached around 350M [27]. Therefore, these two model series are the primary focus of our dataset. During the data genera- tion process, we notice that certain samples contain textual noise, like irrelevant or redundant information. To maintain data quality, we implement some data processing strategies (see Appendix E for details)."}, {"title": "4 Experimental Settings", "content": "In this Section 4, we present the experimental settings for this report. We will introduce the datasets and detectors, and we will introduce the corresponding training details."}, {"title": "4.1 Datasets", "content": "As mentioned in Section 3, we collect the social media dataset (SM-D) and the detector training dataset (AIGTBench). SM-D refers to the social media dataset that we conduct the quantifi- cation, with more details provided in Section 3.1. AIGTBench is the benchmark dataset for AIGT detectors, which includes samples generated by 12 different LLMs, as described in Sec- tion 3.2. We randomly divide AIGTBench into training, val- idation, and test sets in a 7:1:2 ratio. Specifically, the distribution of tokens across the texts in the training set is shown in Figure A1, and the validation and test sets maintain a consistent token distribution with the training set."}, {"title": "4.2 AIGT Detectors", "content": "Following the experimental setup of MGTBench [14], we evaluate 14 detectors. For metric-based detectors, we consider LogLikelihood, Rank, LogRank, Entropy, GLTR, LRR, DetectGPT, and NPR [9, 28, 39]. We choose the GPT-2 medium [34] as the base model, given its good detection performance at limited computational costs. During the detection process, we initially use the GPT-2 medium to extract multiple metrics, including log-likelihood and log-rank. Based on these extracted metrics, we train logis- tic regression models to enhance the accuracy of predictions. For the model-based detectors, we consider both pre-trained detectors and fine-tuned models with the AIGTBench, that is, OpenAI Detector [39], ChatGPT Detector [12], ConDA [4], GPTZero [10], CheckGPT [24], and LM-D [16]. Specifically, for the OpenAI Detector and ChatGPT Detector, we consider their pre-trained version and select the ROBERTa-base model as it demonstrates stable performance across multiple detec- tion tasks and typically provides better detection results. For ConDA and LM-D, we choose the Longformer-base-4096 model as the base model and fine-tune it with the AIGTBench. For GPTZero, we directly use its commercial API. For Check- GPT, we retrain the original training framework [24]."}, {"title": "4.3 Evaluation Metrics", "content": "To evaluate the performance of different detectors, we use accuracy and F1-score as the evaluation metrics, which are common standards in AIGT detection tasks. Besides, we in- troduce two new metrics AI Attribution Rate (AAR) and False Positive Rate (FPR) for social media text detection. The AAR indicates the proportion of texts that the model pre- dicts as AI-generated, while the FPR measures the proportion of HWTs that are mistakenly identified as AIGTS. To evaluate word usage, we calculate the term frequency and divide it by the total number of documents, obtaining the normalized term frequency (NTF), which represents the relative occurrence of the word in the document d, as follows: \n$\\displaystyle NTF(t, d) = \\frac{f_{t,d}}{N \\cdot \\Sigma_{t' \\in d} f_{t',d}}$\nwhere $f_{t,d}$ denotes the frequency of word t in document d. The $\\Sigma_{t' \\in d} f_{t'd}$ accounts for all words present in d. The N represents the total number of occurrences of the word across all documents."}, {"title": "5 Evaluation", "content": "In this section, we evaluated the experiments in detail and analysed the results."}, {"title": "5.1 Benchmarking Detectors", "content": "This section compares different AIGT detectors on the test set of the AIGTBench. Illustrated in Table 2, the metric-based detectors perform poorly. The F1-scores for Log-Likelihood, Rank, Log-Rank, and Entropy are 0.754, 0.730, 0.741, and 0.697, respectively. These low scores indicate that metric-based detectors face limitations in handling complex, multi-source datasets and struggle to capture subtle textual features effectively. Regarding model-based detectors, we observe that both OpenAI Detector and ChatGPT Detector perform worse than some metric-based detectors. Specifically, OpenAI Detector has an F1-score of only 0.484, with relatively low accuracy. This underperformance may be due to the detector being fine- tuned using GPT-2 output, which struggles to adapt to more complex data generated by modern LLMs, such as the Llama and Claude Series. Notably, LM-D and ConDA outperform the others in both accuracy and F1-score. ConDA achieves an accuracy of 0.972, while the LM-D performs even better, with an accuracy of 0.979 and an F1-score of 0.980, making it the most effective detector. Based on these benchmark results, we consider LM-D as the most effective detection method and name LM-D fine-tuned on AIGTBench as OSM-Det, which is subsequently used to quantify and monitor the AAR in social media dataset (SM-D)."}, {"title": "5.2 Evaluation on Social Media Platforms", "content": "As shown in Table 3, OSM-Det achieves False Positive Rates (FPR) of 1.82%, 1.36%, and 1.70% on Medium, Quora, and Reddit, respectively, while achieving a benchmark F1-score of 0.980 (see Table 2). These results highlight OSM-Det's low misclassification rate and high overall accuracy, making it a reliable choice for quantifying and monitoring AIGTs on social media."}, {"title": "5.3 Linguistic Analysis at Different Levels", "content": "We explore the interpretability of the OSM-Det model in the case study using two methods: Integrated Gradients [42], representing a model-dependent perspective, and Shapley Value [36], offering a model-independent perspective. Details of the two methods can be found in Appendix G.2. Word-Level Analysis. In the case study of Reddit (re- fer to Figures A4 and A6), words like \"and\" \"think\" and \"I\" have the highest Integrated Gradients and Shapley Value scores, which lead model to classify texts as human- written. Meanwhile, model-specific analysis shows the words \"think\u201d, \u201ccan\u201d, and \u201cOnline\" have the lowest scores, leading to AI-generated prediction. From these observations, we note that specifying clear word-level patterns between HWTs and AIGTs is challenging because certain words, like \"think\", contribute significantly to both classifications. This overlap suggests that word importance is highly context- dependent, complicating the task of isolating patterns that consistently distinguish the two text types. Similar challenges are also observed on Medium and Quora (refer to Figures A7, A9, A10 and A12). Given this difficulty, we then turn to a different approach: a statistical analysis of high-frequency adjectives, conjunctions, and adverbs (details provided in Appendix G.1). These high-frequency terms are then classified into human-preferred and AI-preferred vocabularies. We then track the trends of these lexical items on SM-D. As shown in Figures 3a and 3b, the NTF of AI-preferred vocabulary on the Medium and Quora is closely aligned with the development of LLMs. Following the release of LLMS such as GPT, Llama, and the Claude series, the NTF of human- preferred vocabulary has gradually declined. Meanwhile, AI-preferred vocabulary shows an increase. These results reflect an increasing usage of LLMs for content generation by Medium and Quora platform users. In contrast, the trends on Reddit show some differences (see Figure 3c). From 2022 to 2024, the NTF of human-preferred vocabulary always remains high, while the AI-preferred vocabulary consistently remains low. This indicates that Reddit users rely less on LLMs to produce content. From above, word frequency changes closely align with the AAR trends shown in Section 5.2. Sentence-Level Analysis. We also conduct a sentence-level analysis using Shapley values, as Integrated Gradients are only suitable for word-level. From the case studies of Medium, Quora, and Reddit (shown in Figures A5, A8 and A11), we observe that AIGTs are characterized by their objective and standardized structures, typically beginning with a noun or pronoun and following a verb-object pat- tern, like \"Online bullying...contributes...feelings...\" In contrast, HWTs often contain flexible sen- tence structures and informal expressions, as il- lustrated by \"That being said, why not both?\" and \"Why can't we restore...\u201d. In summary, the results suggest that sentence-level patterns provide more distinctive charac- teristics for distinguishing AIGTs and HWTs, as LLMs may usually follow a standardized pattern to generate texts."}, {"title": "5.4 Multidimensional Analysis of Posts", "content": "We analyze posts on social media from multi-dimensions to find the characteristics between posts predicted as AIGTs and those classified as HWTs, including topic, engagement, and author analysis. Topic Analysis. Classifying topics on platforms like Quora and Reddit is challenging due to their wide range. Therefore, we focus our analysis on 9 major topics listed on the Medium platform,\u00b9 examining them from a temporal perspective. The proportion of topics is shown in Figure A2. Figure 4 shows the trends of AAR across different topics. We observe a rapid increase in AAR for all topics following the release of GPT-3.5 in December 2022, indicating that the popularity of LLMs has impacted all topics on Medium. Besides, the AAR for \"Technology\" and \"Software Devel- opment\" remains consistently higher than other topics from December 2022 to October 2024, ranking respectively first and second. One possible reason is that people in the technol- ogy field are more likely to know about LLMs and frequently interact with them, leading to a higher AAR. Engagement Analysis. To understand how user engagement differs between articles predicted to be AIGTs or HWTs, we analyze the number of \"Likes\" (known as \"Claps\" on Medium) and \"Comments\" in Medium blogs. To ensure bal- anced comparisons, we randomly select 16,600 blogs with a 1:1 class ratio. Mann-Whitney U tests reveal statistically sig- nificant differences in the number of \"Likes\" and \"Comments\" between the two classes (p < 0.05). As shown in Figure 5a, the predicted-AIGTs receive fewer \"Likes\" on average than predicted-HWTs, with mean values of 69.15 and 127.59, respectively. And predicted-AIGTs exhibit a higher frequency of low \"Likes\" counts. Figure 5b shows that predicted-AIGTs receive fewer \"Comments\" on average compared to predicted-HWTs, with mean values of 4.16 and 7.38, respectively. To summarize, predicted-HWTs obtain more \u201cLikes\u201d and \"Comments\", which indicates that users in Medium are gen- erally more willing to engage with human-written content. However, the relatively small gap between the two suggests that AI-generated content appeals to users. Author Analysis. On Medium, we randomly select 1,000 au- thors from the predicted-AIGTs group who have published at least ten articles. We collect and detect all of their published articles to determine if they are AI-generated, aiming to ex- plore the potential relationship between an author's follower count and their usage of AI-generated content. As shown in Figure 6, we divide these authors into three groups based on their follower count. Among the groups, those with 1,000 or fewer followers exhibit a stronger con- centration in the high AAR range (\u2265 75.00%). This group also achieves the highest mean AAR at 54.02%. From the overall distribution, as the follower number increases, the AAR gradually shifts toward the lower range (< 25.00%). This trend may stem from more popular authors prioritizing content quality, while less-followed authors rely on LLMs to boost efficiency. Furthermore, Figure A3 illustrates the publication timeline of the first articles detected as AIGTs from these authors. It can be observed that there is a significant increase in such publications during the month GPT-3.5 is released, followed by a relatively stable trend in subsequent months."}, {"title": "6 Conclusion", "content": "In this paper, we collect a large-scale dataset, SM-D, encom- passing multiple platforms and diverse time periods, providing the first comprehensive quantification and analysis of AIGTs on online social media. We construct AIGTBench, an AIGT detection benchmark integrating diverse LLMs, to identify the most effective detector, OSM-Det. We then perform temporal tracking analyses, highlighting distinct trends in AAR that are shaped by platform-specific characteristics and the increasing adoption of LLMs. Finally, our analysis uncovers critical dif- ferences between AIGTs and HWTs across linguistic patterns, topical features, engagement levels, and the follower distribu- tion of authors. Our findings offer valuable perspectives into the evolving dynamics of AIGTs on social media."}, {"title": "7 Ethical Statement", "content": "We emphasize that the purpose of this research is not to ex- pose or criticize specific platforms or users for employing AIGTs nor to interfere with legitimate content-creation activi- ties. Instead, our goal is to provide valuable insights through scientific analysis to aid the research community and the public to better understand the current state and trends of generative AI usage on social media. All data used in our paper is publicly available, and we do not collect and monitor any private information."}, {"title": "8 Limitations", "content": "In this paper, we conduct long-term quantification of AIGTS on 3 commonly used social media platforms, but there are still some limitations:\n1. Limited coverage of LLMs: AIGTBench includes only 12 LLMs and does not cover all LLMs released across dif- ferent time periods. Although the current AIGT detectors can generalize to LLMs that are not involved in training to a certain extent [20], there may still be slight errors, which poses a potential impact on the accuracy of some results. We also note that AIGTBench exhibits a distributional bias in the number of LLM-generated texts, favoring the GPT series and Llama series models, which dominate its com- position at 42.9% and 39.05%, respectively. However, this bias is unlikely to significantly impact the analysis results, as these models are also the most widely used in real-world applications.\n2. Lack of analysis on multilingual platforms: Our re- search focuses on English-dominated social media plat- forms. Therefore, the applicability of our findings is re- stricted to these specific platforms and language contexts. Since data collection is a long-term process, we plan to gradually expand to multilingual environments and more platforms in future research to improve the universality of the conclusions.\n3. Insufficient dimensions of analysis across platforms: We conduct an in-depth analysis of the three dimensions of topic, engagement, and author on the Medium platform, but we are unable to conduct similar multi-dimensional research on Quora and Reddit. This is mainly due to the"}, {"title": "A Introduction of LLMs in Detector Training Dataset", "content": "In this paper, we have selected the most representative LLMS as our detection targets:\n\u2022 Llama-1 (Feb. 2023) [44], Llama-2 (Jul. 2023)[45], and Llama-3 (Apr. 2024) [7]: The Llama series (from Llama-1 to Llama-3) launched by Meta are powerful and extremely popular open source models. This series of models enables researchers to fine-tune diverse datasets, is highly scalable, and is suitable for various research and development envi- ronments. The latest version, Llama-3, is equipped with a larger parameter size and optimized training architecture, making it perform better in text generation, context under- standing, and complex task processing.\n\u2022 ChatGPT/GPT-3.5 Turbo (Nov. 2022) [30]: GPT-3.5, an optimized version of GPT-3 by OpenAI, was released in 2022. By incorporating a Reinforcement Learning from Human Feedback (RLHF) reward mechanism and human feedback data, GPT-3.5 achieves significant improvements in accuracy and coherence in text generation. This version includes the Text-DaVinci-003 and GPT-3.5 (or GPT-3.5 Turbo), which focuses on fluent and natural multi-turn con- versations and serves as the core model for systems like ChatGPT website.\n\u2022 GPT40-mini (Jul. 2024) [32]: Developed by OpenAI, GPT40-mini is a lightweight language model optimized from GPT-40 technology. This model is designed to deliver efficient language processing capabilities that are suitable for applications with lower resource requirements. It sup- ports both text and visual input, with future plans to expand into audio and video input and output. Since its release, the GPT40-mini has progressively replaced the GPT-3.5 Turbo as the core model on the ChatGPT website.\n\u2022 Claude (Mar. 2023) [2], : Claude is an advanced AI assis- tant developed by Anthropic. It is a closed-source model designed to communicate efficiently and intuitively with users through NLP technology. Claude can understand and generate human language to assist users in completing a variety of tasks, including answering questions, writing content, and programming assistance.\n\u2022 Alpaca 7B (Mar. 2023) [43]: Alpaca 7B is a lightweight instruction-following model released by Stanford Univer- sity, based on Meta's Llama-7B model and fine-tuned on the dataset of 52,000 instruction-following examples. This fine-tuning markedly enhances the model's performance in understanding and executing task instructions. In eval- uations of single-turn instruction-following tasks, Alpaca demonstrates performance comparable to OpenAI's Text- DaVinci-003, exhibiting high-quality responses to instruc- tions.\n\u2022 Vicuna 13B (Mar. 2023) [6]: Released by the LMSYS team, Vicuna 13B is based on Meta's Llama-13B model and trained on a large dataset of conversation data aggre- gated from high-quality models like GPT-3.5. The goal is to develop an open-source conversational model that ap- proaches the quality of GPT-3.5.\n\u2022 Moonshot-v1 (Oct. 2023) [29]: Developed by Moonshot AI, Moonshot-v1 is an advanced large language model for"}, {"title": "B Introduction of Detectors", "content": "In this work, we adopt metric-based detectors from the MGT- Bench framework to detect AIGTs, including:\n\u2022 Log-Likelihood [39]: We evaluate the likelihood of text generation by computing its log-likelihood score under a specific language model. The model constructs a reference distribution based on HWTs and AIGTs to calculate the log-likelihood score of the input text. A higher score suggests a greater likelihood of the text being LLM-generated.\n\u2022 Rank [9] and Log-Rank [28]: The Rank method identifies the source of generation by analyzing the ranking of each word in the text. The model calculates the absolute ranking of each word based on context and averages all word rank- ings to derive an overall score. Generally, a lower score indicates that the text is more likely to be LLM-generated. Log-Rank, a variant of Rank, employs a logarithmic func- tion when calculating each word's ranking, enhancing the detection of AIGTS.\n\u2022 Entropy [9]: The Entropy method calculates the average entropy value of each word in the text under context condi- tions. Studies show that AIGTs tend to have lower entropy values.\n\u2022 GLTR [9]: GLTR is a supportive tool for detecting AIGTs that use the ranking of words generated by a language model to sort the vocabulary of the text by predicted probability. Following Guo et al. [12], we employ the Test-2 feature to analyze the proportion of words in the top 10, 100, and 1000 ranks to assess the generative nature of the text.\n\u2022 DetectGPT [28], NPR, and LRR [41]: The DetectGPT method introduces minor perturbations into the original text and observes changes in the model's log probability to de- tect its source. AIGTs typically reside at the local optima of the model's log probability function, whereas HWTs show greater changes in log probability after perturbation. The NPR method, similar to DetectGPT, focuses on observing significant increases in log-rank following perturbations to differentiate between AIGTs and HWTs. By combining log-likelihood and log-rank information, the LRR method captures the adaptiveness of generated texts in probability distributions while reflecting the text's ordinal preference relative to HWTs. This dual metric markedly enhances the detection accuracy. We also consider model-based detectors, including:\n\u2022 OpenAI Detector [39]: This detector fine-tunes a ROBERTa [22] model using output data generated by the"}, {"title": "C Social Media Platforms", "content": "To select suitable social media platforms for testing AIGT detection, we particularly consider the platform's mainstream status, the diversity of content, and their unique characteris- tics. Ultimately, we choose Reddit, Medium, and Quora as representative platforms. \u2022 Reddit [35] is a social discussion platform where users au- tonomously create and manage \u201csubreddit\u201d sections featur- ing diverse and rich content themes. All content on the site is categorized into different \u201csubreddits\u201d according to user interests, covering a wide range of topics from technology to social issues. We choose Reddit not only for its active user base\u2014with around 330M monthly active users\u2014but also for its vast content diversity, including millions of sub- reddit topics, allowing it to cover a variety of discussion scenarios.\n\u2022 Medium [26] is an American online publishing platform developed by Evan Williams and launched in August 2012. It centers on high-quality original articles and blog content and exemplifies social journalism, known for its content's depth, length, and professionalism.\n\u2022 Quora [33] is a platform to gain and share knowledge. It enables users to ask questions and connect with people who provide unique insights or quality answers. Users can pose questions and receive answers from other users on top- ics ranging from daily life to highly specialized academic, technical, and professional queries. We have selected these 3 platforms because their main func-"}, {"title": "D Introduction of Open Source Datasets for Training Detectors", "content": "We consider 6 publicly available AIGT datasets and 5 com- mon supervised finetuning datasets as one part of AIGTBench. \u2022 The MGT-Academic dataset [23", "21": "produced using OpenAI's text-davinci-0035 model", "24": "based on the GPT-3.5 Turbo model", "tasks": "GPT-written", "40": "involves texts generated by Llama- 2", "write an Amazon review in the style of the author of the following review": "human review>\", where each prompt incorporates a real human-written Amazon review as a stylistic reference.\n\u2022 The HC3 dataset [12", "38": "samples human-generated content and content from seven popular open-source or API-driven LLMs, applied in real-world scenarios such as low-quality content generation, news fabrication, and student cheat- ing. Due to the markedly lesser capabilities of GPT-2 XL and GPT-J compared to GPT-3.5, these models were not included.\n\u2022 Given that high-quality Supervised Finetuning (SFT) datasets are frequently used for finetuning LLMs, and considering the lack of Claude and GPT-4 model-related content in"}]}