{"title": "Audit-LLM: Multi-Agent Collaboration for Log-based Insider Threat Detection", "authors": ["Chengyu Song", "Linru Ma", "Jianming Zheng", "Jinzhi Liao", "Hongyu Kuang", "Lin Yang"], "abstract": "Log-based insider threat detection (ITD) detects malicious user activities by auditing log entries. Recently, Large Language Models (LLMs) with strong common sense knowledge are emerging in the domain of ITD. Nevertheless, diverse activity types and overlong log files pose a significant challenge for LLMs to directly discern malicious ones within myriads of normal activities. Furthermore, the faithfulness hallucination issue from LLMs aggravates its application difficulty in ITD, as the generated conclusion may not align with user commands and activity context. In response to these challenges, we introduce Audit-LLM, a multi-agent log-based insider threat detection framework comprising three collaborative agents: (i) the Decomposer agent, breaking down the complex ITD task into manageable sub-tasks using Chain-of-Thought (COT) reasoning; (ii) the Tool Builder agent, creating reusable tools for sub-tasks to overcome context length limitations in LLMs; and (iii) the Executor agent, generating the final detection conclusion by invoking constructed tools. To enhance conclusion accuracy, we propose a pair-wise Evidence-based Multi-agent Debate (EMAD) mechanism, where two independent Executors iteratively refine their conclusions through reasoning exchange to reach a consensus. Comprehensive experiments conducted on three publicly available ITD datasets-CERT r4.2, CERT r5.2, and PicoDomain-demonstrate the superiority of our method over existing baselines and show that the proposed EMAD significantly improves the faithfulness of explanations generated by LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Insider threats are one of the most challenging attack pat-terns in practice as they are usually carried out by authorized users who have legitimate access to sensitive and confidential materials (Homoliak et al., 2019). To address the task, Insider Threat Detection (ITD) is coined to detect malicious activities by insiders, involving monitoring and analyzing logs. These logs contain critical records of various user behaviors essential for troubleshooting and security analysis.\nConventional ITD models (Du et al., 2017; Le et al., 2021; Li et al., 2023) utilize Deep Learning for capturing diverse user behavioral characteristics (Yuan and Wu, 2021). However, the inherent problems in this line of approaches, i.e., overfitting and opacity, hinder the further enhancement of their performance. The emergence of overfitting is caused by the scarcity of insider threats in comparison to benign activities, resulting in a bias towards benign behavior while neglecting critical malicious activities. Opacity limits practical log au-diting by delivering results in an opaque format, lacking the interpretability necessary for credible and actionable insights in security auditing.\nIn response, there is a booming trend of applying LLMs in the domain of ITD (Le and Zhang, 2023; Qi et al., 2023; Jin et al., 2024). Leveraging LLMs' extensive commonsense knowledge and capacity for intricate multi-step reasoning, existing methods require them to either justify each decision, thereby implicitly constructing logical chains of reasoning (Liu et al., 2024), manually define intermediate steps for log auditing (Qi et al., 2023), or use a few annotated log samples to provide context and guide predictions (Liu et al., 2024). These abilities empower them to conduct log auditing in a zero-shot manner without the need for training or fine-tuning, thereby fundamentally mitigating the risks of overfitting caused by highly imbalanced categories. Additionally, auditing results can be delivered in a pre-defined human-readable format, thereby avoiding issues of non-interpretable outcomes.\nDespite the promising applications of LLMs in ITD, current studies merely transfer them in a straightforward input-output way. This direct transformation fails to approach the specific features of ITD. Specifically, we identify challenges as follows: (1) Malicious behaviors exhibited by users are intrin-sically manifold. Insiders may leak confidential data through the network, mobile storage devices, or email (Glasser and Lindauer, 2013), requiring a multidimensional examination of user behaviors. (2) The input length constrained by LLMs might result in inadequate detection. Online APIs for LLMS commonly restrict the size of input windows (e.g., approx-imately 128K tokens for GPT-4), which proves inadequate for incorporating entire overlong logs, thereby disregarding crucial contextual details such as typical user behaviors. (3) The faithfulness hallucination brought by LLMs leads to the divergence of generated content from user instructions. For example, even if some sub-tasks' results are identified as malicious, there remains a possibility for LLMs to categorize the whole log set as benign.\nConsidering the multiple perspectives involved in identified challenges, distinct capabilities are required to collaboratively handle ITD, i.e., decomposing log auditing into sub-tasks, ac-cessing information beyond the input window, and possessing mechanisms to mitigate hallucinations. The requirement is in line with the main idea of multi-agent systems (Yu et al., 2024; Deng et al., 2024), where several specialized agents are played as specific roles to achieve a shared goal collectively. Thus, we draw upon the workflows of human log auditors to de-velop a multi-agent insider threat detection framework Audit-LLM. Enhanced by multi-agent collaboration, the Audit-LLM framework focuses on task decomposition, tool creation, and hallucination elimination, as depicted in Fig. 1.\nSpecifically, instead of achieving the final result in a single step, we instruct a Chain-of-Thought (Ji et al., 2024) (CoT)-oriented agent, referred to as the Decomposer, to tailor the complex ITD task into a series of sub-tasks. This agent facilitates a comprehensive evaluation of user behavior from multiple perspectives. Expanding on this analogy, we guide the next agent, known as the Tool Builder, to develop a suite of sub-task-specific tools to extract global characteristics for detection. These tools are engineered to derive insights from the log set, such as a user's historical login frequency and the verification of website legitimacy, thereby improving the final conclusion. Lastly, we develop a third agent named Executor, tasked with systematically accomplishing sub-tasks by invoking constructed tools to realize threat detection. Drawing inspiration from the human \u201cpeer review\" process, which improves the quality and reliability of work through mutual evaluations and feedback, we propose a pair-wise Evidence-Based Multi-Agent Debate (EMAD) mechanism to mitigate the faithfulness hallucination issue encountered in LLMs employed for ITD.\nIn this paper, we introduce Audit-LLM (a multi-agent log-based insider threat detection framework) that integrates the aforementioned ideas. Our contributions are three-fold:\n\u2022 To the best of our knowledge, we are the first to employ multi-agent collaboration for ITD and propose Audit-LLM, a multi-agent log-based insider threat detection framework.\n\u2022 We counter faithfulness hallucination issues by intro-ducing a pair-wise Evidence-based Multi-agent Debate mechanism. This enables agents to engage in an iterative refining process, thus bolstering the reliability of our ITD system.\n\u2022 We evaluate the proposed Audit-LLM alongside state-of-the-art baselines for the ITD task using three publicly ac-cessible datasets. Our findings demonstrate the superiority of Audit-LLM compared to the competitive baselines.\""}, {"title": "II. RELATED WORK", "content": "In this section, we first review related works of tradi-tional ITD methods and deep learning-based ITD methods in Sec. II-A. Then, in Sec. II-B, we provide a detailed discussion of various approaches for applying LLMs in the field of cybersecurity."}, {"title": "A. Log-based insider threat detection", "content": "Insider threat detection has attracted considerable research interest over the last decade as an important task in cyberse-curity. Over the years, extensive research has been conducted to develop effective approaches for detecting insider threats. Broadly, these approaches can be categorized into two main streams: traditional methods and deep learning methods.\nTraditional ITD methods can further be classified into two types: anomaly-based and misuse-based approaches. Anomaly-based detection is the prevalent approach. For in-stance, Brdiczka et al. (2012) propose a traitor assessment using Bayesian techniques that combined structural anomaly detection from information and social networks with psycho-logical profiling. Additionally, Cami\u00f1a et al. (2016) propose detection systems for masqueraders utilizing SVM and KNN as one-class techniques. In contrast, misuse-based methods incorporate softer forms of matching through similarity mea-surement. For instance, Agrafiotis et al. (2016) propose trip-wire grammar capable of capturing abstraction of policies that organizations adopted, as well as signatures of insider misbe-haviors. Moreover, Magklaras and Furnell (2012) design an insider threat prediction and specification language (ITPSL), which has markup features and utilizes logical operators.\nWith the development of deep learning methods, there is a departure from traditional approaches, as practitioners increasingly turn to neural networks to distinguish between benign and malicious behaviors. For instance, Yuan et al. (2019) employ hierarchical neural temporal point processes to capture activity types and time information within user sessions. Further, Liu et al. (2019) introduce a network security threat detection method based on heterogeneous graph embedding. It achieves user behavior detection by constructing a heterogeneous graph, conducting graph embedding learning, and employing detection algorithms. Moreover, Fang et al. (2022) present LMTracker for lateral movement path detection based on heterogeneous graphs and propose a representation method for lateral movement paths and devise an unsupervised detection algorithm utilizing reconstruction error.\nHowever, in real-world scenarios, malicious insider threat behaviors are extremely rare compared to benign behaviors. This rarity can lead deep learning models to exhibit bias, often favouring predictions of benign activity. Furthermore, deep learning typically outputs log classification results in an end-to-end manner, lacking interpretable intermediate results that are crucial for end-users such as auditors to trust. Instead, Audit-LLM leverages the extensive knowledge and zero-shot generation abilities of LLMs to accurately detect malicious behaviors and provide an interpretable analysis process."}, {"title": "B. LLM for cybersecurity", "content": "The constantly changing landscape of modern cybersecurity poses significant challenges, with adversaries adapting tactics to exploit vulnerabilities and avoid detection. However, Al advancements, especially Large Language Models (LLMs), offer promising avenues for strengthening cybersecurity, serv-ing not only as defensive measures but also as offensive tools. For instance, Xu et al. (2024) present a system named AutoAttacker, which leverages Large Language Models for automated network attacks, utilizing language models for plan-ning, summarization, navigation, and experience management. The paper proposes a system for automated penetration testing. Moreover, Fang et al. (2024) investigate the capability of LLMs to automatically exploit cybersecurity vulnerabilities. Employing the GPT-4 model in conjunction with CVE vulner-ability descriptions, they were able to successfully exploit 87% of real-world software vulnerabilities. Concurrently, LLMs can serve as potent instruments for the defensive side, aiding in the detection and identification of potential security threats. For in-stance, Jin et al. (2024) leverage large language models to en-hance strategic reasoning capabilities in cybersecurity, realiz-ing a comprehensive human-machine interactive data synthesis workflow for developing CVE to ATT&CK mapping datasets. It employs retrieval-aware training techniques to enhance the strategic reasoning capabilities of large language models in generating precise policies. Similarly, Fayyazi et al. (2024) comply a dataset of 639 descriptions by extracting tactics, techniques, and sub-techniques from the MITRE ATT&CK framework and evaluated different models' abilities to interpret process descriptions and map them to corresponding ATT&CK tactics.\nWhen focusing on log analysis, LLMs also demonstrate strong parsing and analytical capabilities. For instance, Le and Zhang (2023) assess the capability of ChatGPT in log parsing. They devised appropriate prompts to guide ChatGPT in under-standing log parsing tasks and extracting log events/templates from input log messages. Besides, Qi et al. (2023) introduce LogGPT, a log anomaly detection framework based on Chat-GPT. Leveraging ChatGPT's natural language understanding capabilities, it explores the potential of transferring knowledge from large-scale corpora to the task of log anomaly detection. Moreover, Karlsen et al. (2023) explore methodologies for log file analysis using Large Language Models (LLMs) and evaluates the performance of various LLM architectures in the context of application and system log security analysis.\nHowever, the abundance of overlong log files presents a significant hurdle for LLMs, potentially concealing anomalous behavior within truncated logs due to the constrained context length of LLMs. Moreover, methods that segment and analyze logs fail to provide LLMs with the historical context of logs. In contrast, we propose the utilization of automated tools to effectively extract user behavior characteristics from extensive log datasets and input them as contextual information into LLMs for assessment."}, {"title": "III. APPROACH", "content": "This section formalizes the task and presents the proposed model, including the framework and module details."}, {"title": "A. Framework", "content": "We first overview the Audit-LLM framework, as illustrated in Fig. 2. It is a multi-agent collaboration framework, consist-ing of three core agents: the Decomposer, the Tool Builder, and the Executor. Particularly, the Decomposer reformulates the log auditing task into a sequence of more manageable sub-tasks via the CoT reasoning (Sec. III-B). Then, the Tool Builder constructs a set of reusable tools tailored for each sub-task (Sec. III-C). Ultimately, two independent Executors dynamically invoke tools to accomplish sub-tasks, generating respective results, which are further refined by the pair-wise Evidence-based Multi-agent debate mechanism to culminate in a consensus on the final conclusion (Sec. III-D)."}, {"title": "B. Task definition and decomposition", "content": "1) ITD: Formally, consider an information system that accesses a time-ordered log set, \\(L = \\{a_{ij}\\} \\in R^{N \times |u_i|} | 1 \\leq i \\leq N, 1 \\leq j \\leq |u_i|\\), where each log belongs to an activity type from the set R, including actions such as logon events, website visits, file operations, and email contents. Here, the log set L is generated by N users, with each user \\(u_i (i = 1,..., N)\\) executing a sequence of activities, \\(S_{u_i} = \\{a_{i1}, ..., a_{i|u_i|}\\}\\), interwoven with the activities of others. For Insider Threat Detection (ITD), the objective is to train a model M to identify if the log set L contains malicious activities \\(L_M (L_M \\subseteq L)\\). Suppose all users are independent, ITD essentially can be simplified into analyzing the activity sequence for each user. Consequently, the detection outcomes \\(y_c\\) are defined as:\n\n\\(y_c =\n\\begin{cases}\n\\text{benign, } & \\text{if } L_M = \\emptyset \\\\\n\\text{malicious, } & \\text{if } L_M \\neq \\emptyset\n\\end{cases} , L_M \\leftarrow M(L) = M(\\bigcup_{i=1}^N S_{u_i}) = \\bigcup_{i=1}^N M(S_{u_i})\\) (1)\n\n2) CoT for ITD: The Chain-of-Thought (CoT) is a rea-soning mechanism where the Large Language Model (LLM) produces intermediate steps or justifications to reach the final conclusion, thereby improving interpretability and the model's proficiency in tackling intricate tasks. For an LLM M with a COT consisting of T reasoning stages, the iterative refinement of ITD for the log set L can be expressed as:\n\n\\[\n\\begin{cases}\nY_i = \\underset{v \\in V}{\\text{argmin }} M(v \\mid L, P, Y_{i-1}),\nY_0 = \\emptyset,\n\\end{cases}\n\\text{for } i = 1,...,T,\n\\]\n(2)\n\nHere, V represents the vocabulary of LLM M, with \\(Y_i\\) denoting the i-th reasoning step's outcome. And the final detection outcomes \\(y_c\\) is \\(Y_T\\).\nHowever, as previously noted, the log set L comprises diverse types of activities. Directly inputting log entries into the LLM may result in overlooking certain types of behav-iors. Thus, we develop an agent named Decomposer, tasked with decomposing the tasks of ITD into multiple sub-tasks \\(\\{z_{ta}^i\\}_{i=1}^{N_t}\\), enabling Audit-LLM to address ITD through a CoT paradigm as:\n\n\\(z_{ta} \\leftarrow M(Sample(R), P_{Deco}), \\text{ for } i = 1,..., N_t\\),\n(3)\n\nwhere \\(P_{Deco}\\) represents the prompt for constructing the De-composer, and Sample() denotes the process of sampling three data points from log files as examples for each activity category. An examination of a particular activity type may encompass more than one sub-task. For instance, the inspec-tion of website access necessitates concurrent checks for the URL, the content of the site, and the potential downloading of malicious payloads. Therefore, the number of sub-tasks is typically greater than or equal to the number of activity types, i.e., \\(N_t \\geq |R|\\).\nTo ensure thorough coverage of all potential malicious behavior patterns, we drew inspiration from the field of psy-chology's concept of \u201cmeta-cognitive dialogues\u201d\u2014a process of self-reflection and iterative improvement (Conway-Smith and West, 2024). Specifically, we present the Decomposer with slicing of log activities and guided its iterative exploration of sub-tasks by continually asking, \u201cWhat additional information do you need to detect threat behaviors?\u201d This strategy allows the Decomposer to progressively advance until barely surpass-ing the task's boundaries, i.e., the requirements of detecting the whole log set.\nIn practice, these sub-tasks guide the Tool Builder agent to construct sub-task-specific tools, which further help the Executor agent to think logically, generating corresponding intermediate results to facilitate the CoT process for ITD (see Eq.(2))."}, {"title": "C. Tool development and optimization", "content": "The objective of the Tool Builder agent is to construct a collection of sub-task-specific and reusable tools \\(\\{z_{to}^i\\}_{i=1}^{N_t}\\), implemented as Python functions, to facilitate the completion of sub-tasks \\(\\{z_{ta}^i\\}_{i=1}^{N_t}\\). The creation process can be formalized as:\n\n\\(z_{to}^i \\leftarrow M(z_{ta}^i, P_{Tool}), \\text{ for } i = 1, ..., N_t\\),\n(4)\n\nwhere \\(P_{Tool}\\) denotes the prompt for the Tool Builder. This process is further delineated into three stages:\n1) Intent recognition: In this stage, we applied the \u201cPro-gramming by Example\u201d (PbE) paradigm (Bauer, 1979), which streamlines the programming process and reduces complexity by guiding program writing through concrete demonstrations. Specifically, the demonstrations consist of two components, namely log examples and result examples. Log examples serve to acquaint the Tool Builder with accessible inputs and enable them to determine the necessary input parameters for the tools. Result examples aim to align the generated tool with the objectives of the sub-tasks.\n2) Unit test: In the next stage, we verify the functionality and accuracy of the generated tools through unit testing, ensuring that the agent can seamlessly invoke these tools and obtain accurate results. We begin by manually entering the necessary parameters for the tools to validate their function-ality. Subsequently, each tool undergoes three invocation tests by the LLM for every sub-task. Upon successful extraction of parameters from the logs and their placement in the intended positions by the LLM, the tool is incorporated into the toolkit for subsequent utilization. While any tool triggers errors during this process, we document the error details and utilize this information to guide the LLM in reconstructing the faulty tool.\n3) Tool decoration: Although unit testing has demonstrated the reliability of an agent's invocation of individual tools, the presence of multiple tools can still potentially lead to error in-vocation. Thus, the final stage involves enhancing the validated tools with decoration. The decoration process includes two main aspects: code documentation and output restructuring. Code documentation provides contextual information at the beginning of each tool, accurately describing its functionality to prevent missteps during agent calls. Additionally, we refine the output formats to ensure that the agent can better under-stand the meaning of the results obtained from using the tools in natural language terms.\n\nIn practice, the Tool Builder completes the process of tool development and optimization, and only needs to be performed once for each sub-task. The resulting tools \\(\\{z_{to}^i\\}_{i=1}^{N_t}\\) can be reused for all instances of ITD, thereby reducing the usage costs of Audit-LLM."}, {"title": "D. Task execution", "content": "The principal role of the Executor is to complete the sub-tasks generated by the Decomposer by invoking tools in a CoT manner. Hence, the intermediate results can be formulated as:\n\n\\(z_{ex}^i \\leftarrow M(z_{ta}, Invoke(z_{to}^i), P_{Exec}), \\text{ for } i = 1,..., N_t\\),\n(5)\n\nwhere the invoke function Invoke() returns the outputs invoked by tools. After that, the Executor synthesizes the tool-derived results to form the ultimate conclusion \\(y_c\\) i.e., \\(y_L = \\bigcap_{i=1}^{N_t} z_{ex}^i\\). We illustrate the collaborative process of all agents in Fig. 3. To manage output diversity, we instruct the Executor to categorize responses into predefined formats, e.g., sub-task results, anomalous behaviors, and final conclusions.\nAlthough generally effective, the Executor occasionally gen-erates final conclusions that may contradict the results of sub-tasks. For instance, consider the scenario where the Executor correctly invokes the website legitimacy verification tool and identifies some malicious activities, e.g., accessing suspicious sites or downloading malicious payloads. Nevertheless, the final conclusion synthesized by LLM may still categorize this log set as benign, resulting in what is known as faithfulness hallucination in LLM (Huang et al., 2023). This phenomenon can be attributed to the intrinsic event error from LLM (Kim et al., 2024), wherein the generated explanations misrepresent source events.\nTo address this issue, we propose a pair-wise Evidence-based Multi-Agent Debate (EMAD) mechanism, mimicking the human debate process to help LLM review each rea-soning step. Particularly, two Executors are designated as the proponent and opponent, with their respective reasoning processes forming the basis of their claims. During each debate round, these claims are presented to the opposing Executor for scrutiny, which helps each Executor to update their respective conclusion. Through multiple rounds of debate, a comprehen-sive and accurate conclusion consensus can be achieved."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "Within this section, we present a comprehensive overview of the experimental configuration, encompassing details about the experimental environment, datasets, and baselines. First, we present detailed information about the experimental con-figuration and discuss the research questions in Sec. IV-A. Second, we provide an in-depth description of the datasets used in our study in Sec. IV-B. Finally, we summarize the baseline models in Sec. IV-C"}, {"title": "A. Research questions and experimental configuration", "content": "1) Research questions: We list several research questions to guide the experiments and verify the effectiveness of our proposal.\nRQ1: Can the proposed Audit-LLM achieve better perfor-mance than state-of-the-art baselines for log-based ITD?\nRQ2: Which component contributes more to improving model performance?\nRQ3: When employing diverse LLMs as the base models, how does the performance of Audit-LLM vary?\nRQ4: What do the responses generated by Audit-LLM look like? How interpretable are they?\nRQ5: How does Audit-LLM perform in real-world system environments?\nRQ6: What are the time and economic implications of using online LLM APIs like ChatGPT and ZhipuAI?\n2) Experimental configuration: The experiments are con-ducted with an Intel Xeon(R) Gold 5218R CPU, 256 GB of RAM, and four Nvidia RTX A6000 (48 GB) GPUs. The agent in Audit-LLM is developed based on LangChain (LangChain, a), and we conduct our method in the Python 3.10.14 environ-ment. We build our Audit-LLM framework based on multiple large language models, including a snapshot of gpt-3.5-turbo-0125 released by Openai (OpenAI), To facilitate the reproduc-tion of the results in this paper, we share the code and data used to obtain these results on https://anonymous.address/. Additionally, we have shared the prompt we developed in the LangChain prompt hub under the identifier \u201canonymous-id\u201d."}, {"title": "B. Datasets", "content": "To conduct robust and convincing experiments, we utilize three publicly accessible insider threat datasets, namely, CERT r4.2, CERT 5.2 (University, 2020), and PicoDomain (Laprade et al., 2020). The data statistics are summarized in Table I.\nThe CERT datasets, provided by Carnegie Mellon Univer-sity (Glasser and Lindauer, 2013) in this work, are widely recognized in log-based insider threat detection (Liu et al., 2019; Xiao et al., 2024; Sun and Yang, 2022; Cai et al., 2024; Gon\u00e7alves and Zanchettin, 2024). On one hand, CERT r4.2 includes activity logs from 1,000 users and 1,003 computers, while CERT r5.2 simulates an organization with 2,000 employ-ees over 18 months. Both datasets encompass diverse multi-source activity logs, including user login/logoff events, emails, file access, website visits, device usage, and organizational structure data. Each malicious insider in the CERT dataset is categorized into one of four prevalent insider threat scenarios: data exfiltration, intellectual property theft, and IT sabotage.\nOn the other hand, PicoDomain consists of detailed Zeek logs spanning 3 days, collected from a simulated small-scale network where APT attacks occurred during the last two days. The data sources within PicoDomain can be broadly classified into 5 groups: file logs (files, smb_files), system logs (dhcp, hosts, services), authentication logs (kerberos, ntlm), and anomaly detection logs (weird)."}, {"title": "C. Baseline methods", "content": "1) Baseline methods: For all LLM-based models discussed, we employ the same base LLM, i.e., the snapshot of gpt-3.5-turbo-0125, to fairly compare their performance. Due to severe class imbalance in the CERT dataset, which impedes DL-based methods from effectively capturing features of mi-nority classes, we implemented an under-sampling approach following Xiao et al. (2024), restricting the number of benign class samples to below 20,000. Here, we list a series of state-of-the-art baselines for comparisons with our proposal in this paper:\nLogGPT (Qi et al., 2023): LogGPT utilizes LLMs for log auditing, extracting structured data from raw logs audited by ChatGPT.\nLogPrompt (Liu et al., 2024): LogPrompt enhances zero-shot log auditing using LLMs and advanced prompting techniques, employing their top-performing CoT prompt.\nLAN (Cai et al., 2024): LAN employs graph structure learn-ing to adaptively construct user activity graphs, addressing data imbalance with a hybrid predictive loss.\nDeepLog (Du et al., 2017): DeepLog treats log entries as sequential natural language, utilizing Long Short-Term Memory to detect anomalies.\nLMTracker (Fang et al., 2022): LMTracker uses event logs to construct heterogeneous graphs and apply unsupervised algorithms to detect malicious behavior.\nCATE (Xiao et al., 2024): CATE uses convolutional atten-tion and a transformer encoder for log statistical and sequential analysis.\nIn our comprehensive model performance evaluation, we cov-ered baselines using LLMs, LSTM for sequential data, GNNS for graph structures, and pre-trained Transformers for insider threat detection."}, {"title": "V. RESULTS AND ANALYSIS", "content": "To answer RQ1, we evaluate the performance of our pro-posed Audit-LLM and six competitive baselines for the insider threat detection task on three public datasets. We present the results of the involved models in Table II. Among these metrics, the higher the precision, detection rate, and accuracy, the better overall performance. Contrarily, the lower the False Positive Rate (FPR), the fewer false positives that cause false alarms.\nGenerally, comparing the model performance on CERT r4.2 against that on CERT r5.2, we can observe that the model mostly performs relatively better on the former than on the latter dataset. It could be explained by the fact that the r4.2 version of CERT is a \"dense needle\" dataset that contains more insiders and malicious activities than the r5.2 version. The more severe category imbalance problem results in difficulties for the model to classify the activities correctly. In particular, our proposed Audit-LLM performs the best among the models, with a noticeable performance improvement over the other six baselines. For instance, on the CERT r4.2 dataset, Audit-LLM presents an improvement of 21.8%, 10.5%, 3.3%, 2.7%, 12%, and 3.5% in terms of accuracy against the DeepLog, LMTracker, CATE, LAN, LogPrompt, and LogGPT models, respectively. These overwhelming results indicate that our Audit-LLM leads to consistent gains across different datasets.\nFor all LLM-based methods, namely LogPrompt, LogGPT, and Audit-LLM, their performance on the CERT dataset is significantly better than on the PicoDomain dataset. A similar trend is observed with Audit-LLM, where the accuracy de-creases by up to 3% compared to its performance on the CERT dataset. The reason is that, compared to CERT which includes more user behavior logs such as email communications and accessed website content, PicoDomain comprises more traffic and system activity logs. LLM inherently possesses strong natural language understanding capabilities, thus enabling it to effectively leverage email content or website content summaries for insider threat detection on the CERT dataset. Note that CATE (Xiao et al., 2024) integrates organizational structure information and psychological data (user profile data) for each user into a unified table, which is an integral part of the model and therefore not reproducible in terms of performance within PicoDomain.\nWhen zooming in on the False Positive Rate (FPR), it can be observed that the FPRs of Audit-LLM are 3.7%, 3.9%, and 6.7% on CERT r4.2, r5.2, and Picodomain, respectively, lower than all the baselines by at least 1.47%, 0.8%, and 2.8% on three datasets. These results indicate that Audit-LLM can well reduce the number of false positives, which has great value when the investigation budget is finite. In addition, LMTracker demonstrates the best performance in the baseline on PicoDomain, achieving 90.2% precision, 91.8% detection rate, 9.5% false positive rate, and 89.7% accuracy. This could be attributed to LMTracker's use of heterogeneous graphs to model relationships between computers and users, specifically focusing on designing models for lateral movement."}, {"title": "A. Overall performance", "content": "To answer RQ1, we evaluate the performance of our pro-posed Audit-LLM and six competitive baselines for the insider threat detection task on three public datasets. We present the results of the involved models in Table II. Among these metrics, the higher the precision, detection rate, and accuracy, the better overall performance. Contrarily, the lower the False Positive Rate (FPR), the fewer false positives that cause false alarms.\nGenerally, comparing the model performance on CERT r4.2 against that on CERT r5.2, we can observe that the model mostly performs relatively better on the former than on the latter dataset. It could be explained by the fact that the r4.2 version of CERT is a \"dense needle\" dataset that contains more insiders and malicious activities than the r5.2 version. The more severe category imbalance problem results in difficulties for the model to classify the activities correctly. In particular, our proposed Audit-LLM performs the best among the models, with a noticeable performance improvement over the other six baselines. For instance, on the CERT r4.2 dataset, Audit-LLM presents an improvement of 21.8%, 10.5%, 3.3%, 2.7%, 12%, and 3.5% in terms of accuracy against the DeepLog, LMTracker, CATE, LAN, LogPrompt, and LogGPT models, respectively. These overwhelming results indicate that our Audit-LLM leads to consistent gains across different datasets.\nFor all LLM-based methods, namely LogPrompt, LogGPT, and Audit-LLM, their performance on the CERT dataset is significantly better than on the PicoDomain dataset. A similar trend is observed with Audit-LLM, where the accuracy de-creases by up to 3% compared to its performance on the CERT dataset. The reason is that, compared to CERT which includes more user behavior logs such as email communications and accessed website content, PicoDomain comprises more traffic and system activity logs. LLM inherently possesses strong natural language understanding capabilities, thus enabling it to effectively leverage email content or website content summaries for insider threat detection on the CERT dataset. Note that CATE (Xiao et al., 2024) integrates organizational structure information and psychological data (user profile data) for each user into a unified table, which is an integral part of the model and therefore not reproducible in terms of performance within PicoDomain.\nWhen zooming in on the False Positive Rate (FPR), it can be observed that the FPRs of Audit-LLM are 3.7%, 3.9%, and 6.7% on CERT r4.2, r5.2, and Picodomain, respectively, lower than all the baselines by at least 1.47%, 0.8%, and 2.8% on three datasets. These results indicate that Audit-LLM can well reduce the number of false positives, which has great value when the investigation budget is finite. In addition, LMTracker demonstrates the best performance in the baseline on PicoDomain, achieving 90.2% precision, 91.8% detection rate, 9.5% false positive rate, and 89.7% accuracy. This could be attributed to LMTracker's use of heterogeneous graphs to model relationships between computers and users, specifically focusing on designing models for lateral movement."}, {"title": "B. Ablation study in Audit-LLM", "content": "For RQ2", "comparison": 1, "Vanilla": "hat removes all agent involvement, providing only the most basic task prompts to the LLM, (2) \"Audit-LLM w/o CoT\" that allows the LLM to make one-step decisions without employing reasoning through CoT, (3) \"Audit-LLM w/o Decomp\" that removes the Decomposer, thereby not delineating the subtasks that need to be accom-plished, and relies solely on tools to assist the Executor in making decisions, (4) \"Audit-LLM w/o tools\" that removes the use of tools and allows the Executor to make decisions based on the log entries within the current input, (5) \u201cAudit-LLM w/o EMAD\" that removes the evidence-based multi-agent debate process, adopting the initial decision made by the Executor. The results are presented in Table III.\nFrom Table III, we can observe that removing any compo-nent in Audit-LLM leads"}]}