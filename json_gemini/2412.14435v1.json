{"title": "Cherry-Picking in Time Series Forecasting: How to Select Datasets to Make Your Model Shine", "authors": ["Luis Roque", "Carlos Soares", "Vitor Cerqueira", "Luis Torgo"], "abstract": "The importance of time series forecasting drives continuous research and the development of new approaches to tackle this problem. Typically, these methods are introduced through empirical studies that frequently claim superior accuracy for the proposed approaches. Nevertheless, concerns are rising about the reliability and generalizability of these results due to limitations in experimental setups. This paper addresses a critical limitation: the number and representativeness of the datasets used. We investigate the impact of dataset selection bias, particularly the practice of cherry-picking datasets, on the performance evaluation of forecasting methods. Through empirical analysis with a diverse set of benchmark datasets, our findings reveal that cherry-picking datasets can signif-\nicantly distort the perceived performance of methods, of-ten exaggerating their effectiveness. Furthermore, our results demonstrate that by selectively choosing just four datasets what most studies report - 46% of methods could be deemed best in class, and 77% could rank within the top three. Additionally, recent deep learning-based approaches show high sensitivity to dataset selection, whereas classical methods exhibit greater robustness. Finally, our results indi-cate that, when empirically validating forecasting algorithms on a subset of the benchmarks, increasing the number of datasets tested from 3 to 6 reduces the risk of incorrectly identifying an algorithm as the best one by approximately 40%. Our study highlights the critical need for comprehen-sive evaluation frameworks that more accurately reflect real-world scenarios. Adopting such frameworks will ensure the development of robust and reliable forecasting methods.", "sections": [{"title": "Introduction", "content": "Time series forecasting is critical in various application do-mains, including finance, meteorology, and industry. Over the past decades, there has been significant interest in de-veloping accurate forecasting models, leading to a variety of methods, from traditional statistical techniques to advanced deep learning models.\nThe selection of datasets for evaluating forecasting mod-els can significantly impact the experimental results. For var-ious reasons, such as reducing computational complexity, re-searchers often select:\n1. A limited number of datasets,\n2. Datasets that may not be representative of real-world data,\n3. A subset of time series when working with large datasets, and\n4. A small set of baseline and state-of-the-art (SOTA) mod-els for comparison, often with inconsistent and unfair tuning efforts.\nRegarding point 3), recent work addresses this problem and identifies several flaws in the most commonly used datasets in the area of time series anomaly detection. It sug-gests that comparisons in many papers introducing new ap-proaches might not generalize to the real world (Wu and Keogh 2023). An example of point 4) is the comparison between simple one-layer linear models and sophisticated Transformer-based time series forecasting models (Zeng et al. 2022). To the best of our knowledge, no work has yet been published that attempts to understand the implications of point 1) and 2). In this paper, we focus on understand-ing the consequences of point 1) and how such selection can introduce bias, impacting the quality and generalizability of the results.\nIn the context of dataset selection, we use the term cherry-picking for the deliberate or random process of selecting a limited number of datasets that may not be representative of the broader data landscape. This practice involves select-ing specific datasets that might showcase the strengths of a model while ignoring others that could reveal its weak-nesses. Cherry-picking can lead to biased results and overly optimistic model performance estimates. Thus, it can also significantly impact the quality and generalizability of new forecasting models, making them less reliable in real-world applications.\nOur results show that cherry-picking specific datasets can significantly distort perceived model performance, even as the number of datasets used for reporting results increases. Our analysis shows that with a commonly used selection of 4 selected datasets, 46% of models could be reported as the best, and 77% could be presented within the top 3 positions, highlighting the potential for biased reporting.\nThe rest of this paper is organized as follows: Section 2 provides background information, including definitions of the forecasting problem and the modeling approaches used. Section 3 describes the materials and methods employed in"}, {"title": "Background", "content": "This section provides an overview of topics related to our work. We begin by defining the problem of time series fore-casting from both classical and machine learning perspec-tives. Next, we discuss the limitations of current evaluation frameworks and highlight recent works that address com-mon problems and inconsistencies. The following two sec-tions review prior work on classical methods and deep learn-ing approaches. Finally, we discuss the evaluation metrics and dataset selection used in forecasting problems."}, {"title": "Time Series Forecasting", "content": "A univariate time series can be represented as a sequence of values $Y = \\{Y_1, Y_2,..., Y_t\\}$, where $y_i \\in \\mathbb{R}$ denotes the value at the i-th timestep, and t represents the length of the series. The objective in univariate time series forecasting is to predict future values $Y_{t+1},..., Y_{t+h}$, where h is the fore-casting horizon.\nIn the context of machine learning, forecasting problems are treated as supervised learning tasks. The dataset is con-structed using time delay embedding (Bontempi, Ben Taieb, and Le Borgne 2013), a technique that reconstructs a time series into Euclidean space by applying sliding windows. This process results in a dataset $D = \\{(X_i, Y_i)\\}_{i=p+1}^t$, where yi denotes the i-th observation and $X_i \\in \\mathbb{R}^P$ is the corresponding set of p lags: $X_i = \\{Y_{i-1},Y_{i-2},\\cdots, Y_{i-p}\\}$. Time series databases often comprise multiple univariate time series.\nWe define a time series database as $\\mathcal{Y} = \\{Y_1, Y_2,..., Y_n\\}$, where n is the number of time series in the collection. Forecasting methods in these contexts are categorized into local and global approaches (Januschowski et al. 2020). Traditional forecasting techniques typically adopt a local approach, wherein an independent model is applied to each time series in the database. Conversely, global methods involve training a single model using all time series in the database, a strategy that has demonstrated superior forecasting performance (Godahewa et al. 2021). This performance improvement is attributed to the fact that related time series within a database-such as the demand series of different related retail products-can share useful patterns. Global models can capture these patterns across different series, whereas local models can only learn dependencies within individual series.\nThe training of global forecasting models involves com-bining the data from various time series during the data preparation stage. The training dataset D for a global model is a concatenation of individual datasets: $D = \\{D_1,..., D_n\\}$, where Dj represents the dataset correspond-ing to the time series Y\u2081. As previously described, the auto-"}, {"title": "Limitations to Current Evaluation Frameworks", "content": "Recent work has critically evaluated the effectiveness of var-ious experimental setups and how they provided inconsistent results compared to previous works.\nAn example is the widespread adoption of Transformer-based approaches in time series forecasting, which have consistently outperformed benchmarks. Nonetheless, a re-cent study raised doubts about the reliability of these re-sults (Zeng et al. 2022). It argues that the permutation-invariant self-attention mechanism in Transformers can re-sult in temporal information loss, making these models less effective for time series tasks. The study compares SOTA Transformer-based models with a simple one-layer linear model, which surprisingly outperforms the more complex counterparts across multiple datasets. This suggests that simpler approaches may often be more suitable.\nAnother critical perspective is offered regarding the limi-tations of anomaly detection tasks (Wu and Keogh 2023). In most cases, benchmarks often suffer from issues like triv-iality, unrealistic anomaly density, and mislabeled ground truth. These flaws can lead to misleading conclusions about the effectiveness of proposed models.\nAdditional works show that inflated accuracy gains of-ten result from unfair comparisons, such as inconsistent net-work architectures and embedding dimensions. Also, unre-liable metrics and test set feedback further aggravate the is-sue (Musgrave, Belongie, and Lim 2020). Similarly, many studies report significant improvements over weak base-lines without exceeding prior benchmarks (Armstrong et al. 2009). These findings emphasize the need for stricter ex-perimental rigor and transparent longitudinal comparisons. It is the only way to ensure the reliability of the reported progress.\nOne study introduces a framework designed to assess the robustness of hierarchical time series forecasting models un-der various conditions (Roque, Soares, and Torgo 2024). De-spite the deep learning adoption in the field and their capac-ity to handle complex patterns, the authors demonstrate that traditional statistical methods often show greater robustness. This happens even in cases when the data distribution under-goes significant changes."}, {"title": "Classical Methods", "content": "Several approaches have been developed to address time se-ries forecasting. Simple methods, such as Seasonal Naive (SNaive), predict future values based on the last ob-served value from the same season in previous cycles. Classical forecasting methods, including ARIMA, exponen-tial smoothing, and their variations, are favored for their simplicity, interpretability, and robustness (Hyndman and Athanasopoulos 2018; Gardner Jr 1985).\nARIMA models, which combine autoregression, differ-encing, and moving averages, are effective for linear time series with trends and seasonal components. Exponential smoothing methods, such as Holt-Winters, model seasonal-ity and trends through weighted averages."}, {"title": "Deep Learning Methods", "content": "Deep learning models have been showing steady progress in time series forecasting. The initial approach was based on Recurrent Neural Networks (RNNs) (Elman 1990), in-cluding Long-Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUS), which are designed to cap-ture long-term dependencies in sequential data. Neverthe-less, they can suffer from issues like vanishing gradients, which can impede their ability to model long sequences ef-fectively.\nThen, Convolutional models were adapted to time series, for example, the Temporal Convolutional Networks (TCNS) (Lea et al. 2016). They address some of these issues by en-abling parallel processing of sequences and capturing long-range dependencies more efficiently.\nRecently, Transformer models, initially developed for nat-ural language processing, have been increasingly applied to time series forecasting and have shown better performance than RNNS (Zhou et al. 2021). Transformers use a self-attention mechanism that allows each part of the input se-quence to attend to every other part directly. By avoiding the recurrent structure of RNNS, Transformers can handle long sequences and complex dependencies more effectively. Nevertheless, the self-attention mechanism has limitations due to its quadratic computation and memory consumption on long sequences. The Informer model was introduced to overcome these computational constraints. From the pa-per, we see an improvement in accuracy between 1.5 to 2 times the results obtained by an LSTM approach (Zhou et al. 2021).\nDespite the seemingly impressive results from Trans-former models, recent studies have shown that simple linear models can outperform Transformers on forecasting bench-marks (Zeng et al. 2022). This highlights the potential bias introduced by experimental setups and has renewed interest in simpler and more efficient approaches, such as the NHITS and TiDE models (Challu et al. 2023; Das et al. 2024).\nThe NHITS and TiDE models both utilize Multi-layer Perceptrons (MLPs) to achieve efficient time-series fore-casting. NHITS incorporates hierarchical interpolation and multi-rate data sampling techniques, assembling predictions sequentially to emphasize components with different fre-quencies and scales. This method allows NHITS to effi-ciently decompose the input signal and synthesize the fore-cast, making it particularly effective for long-horizon fore-casting. Experiments show that NHITS outperforms state-of-the-art methods, improving accuracy by nearly 20% over recent Transformer models (e.g., Informer) and signifi-cantly reducing computation time by an order of magnitude. On the other hand, TiDE is an encoder-decoder model that leverages the simplicity and speed of linear models while handling covariates and non-linear dependencies. The TiDE"}, {"title": "Evaluation Metrics", "content": "Evaluating forecasting performance involves various met-rics, which can be scale-dependent, scale-independent, percentage-based, or relative. Common metrics include Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and symmetric mean absolute percentage error (SMAPE). Hewamalage et al. (Hewamalage, Ackermann, and Bergmeir 2023) provide a comprehensive survey of these metrics, offering recommendations for their use in dif-ferent scenarios.\nIn the M4 competition (Makridakis, Spiliotis, and As-simakopoulos 2018), SMAPE and MASE (Mean Absolute Scaled Error) were used for evaluation:\n$\\text{SMAPE} = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2} $  (1)\nwhere \u011di and yi are the forecast and actual values for the i-th instance, n is the number of observations, and m is the seasonal period."}, {"title": "Dataset Selection in Experimental Evaluations", "content": "The selection of datasets is a key factor in determining the generalizability and reproducibility of time series forecast-ing experiments. It directly influences the robustness of the conclusions drawn from experimental results, making it es-sential for researchers to carefully consider both the type and number of datasets used.\nAcross the models discussed in this section, the number of datasets used in experimental setups is relatively small, typi-cally ranging from three to six. For instance, DeepAR (Sali-nas, Flunkert, and Gasthaus 2019) uses three standard public datasets: Parts (Snyder, Ord, and Beaumont 2012), Electric-ity (Trindade 2015), and Traffic (Olivares et al. 2024).\nThe selection of datasets often reflects the specific goals of each model. For example, models like Informer (Zhou et al. 2021), NHITS (Challu et al. 2023), and TiDE (Das et al. 2024) focus on long-term time series forecasting. They are evaluated using datasets similar to those used by DeepAR, such as Electricity and Traffic, as well as others like Weather (Zeng et al. 2022). Additionally, these models utilize the more recently introduced ETT series, which was made available by the authors of Informer when releas-ing their paper. These newer datasets feature a small number of time series but a very large number of observations per series.\nIt is important to note that NHITS, which evolved from N-BEATS (Oreshkin et al. 2020), exclusively adopts a long-term forecasting evaluation setup. In contrast, N-BEATS was originally tested using a more classical forecasting setup with datasets like Tourism (Athanasopoulos et al. 2011), M3 (Makridakis and Hibon 2000), and M4 (Makridakis, Spilio-tis, and Assimakopoulos 2018). These classical datasets are characterized by a significantly larger number of time series, though each series has relatively few observations."}, {"title": "Framework for Evaluating Cherry-Picking", "content": "In this section, we present our framework for assessing cherry-picking in time series forecasting evaluations. Our methodology is designed to systematically evaluate how the selection of specific datasets can bias the reported perfor-mance of forecasting models, potentially leading to mislead-ing conclusions.\nCherry-picking refers to the practice of selectively pre-senting data that supports a desired conclusion while ig-noring data that may contradict it. In the context of time series forecasting, this could mean reporting model perfor-mance only on datasets where a particular model performs well while omitting cases where it does not. Consider a sce-nario where you have five different forecasting models and ten datasets, each with unique characteristics like season-ality and trend. If you selectively report the performance of these models on just the datasets where your preferred model performs best, you might claim it as the \"top-performing model.\" Nevertheless, this claim could be misleading if, on the full set of datasets, the model does not perform as well overall. Our framework helps identify whether such cherry-picking has occurred by analyzing the performance of each model across various subsets of the datasets and comparing it to their overall performance.\nOur framework involves three key steps: 1) dataset selec-tion and categorization, 2) model selection, 3) performance evaluation and ranking, and 4) empirical analysis.\nStep 1) in our framework is to compile a compre-hensive set of benchmark datasets, denoted as $D = \\{D_1, D_2,..., D_m\\}$, where each Di represents a unique dataset. These datasets should be chosen to cover a wide range of domains, frequencies, and characteristics, such as seasonality, trend, noise, and intermittency. This diversity ensures that the experimental setup can effectively capture different challenges encountered in time series forecasting.\nIn step 2), we select a diverse set of forecasting models, denoted as $M = \\{M_1, M_2, ..., M_n\\}$, where each M\u1d62 rep-resents a forecasting model. The models are chosen to rep-resent a broad spectrum of approaches, including both clas-sical methods (e.g., ARIMA, ETS) and advanced deep learn-ing models (e.g., Informer, NHITS, TiDE). This diver-sity ensures that the analysis captures the performance of both simple statistical models and complex neural networks.\nStep 3) involves the performance evaluation and ranking. We evaluate the performance of each model on different sub-sets of the available datasets. For each model $M_i \\in M$ and each subset $D_j \\subset D$ of size n, we define the ranking function $R(M_i, D_j)$. It assigns a rank to model Mi based on its SMAPE values across the dataset subset Dj where $|D_j| = n$. Here, n represents the specific size of the subsets Dj considered from the overall dataset D, with n ranging from 1 to N. The models are ranked from 1 to m (where"}, {"title": "Experimental Setup", "content": "This experimental setup illustrates how our framework can be applied to assess the robustness of time series forecasting models. We examine how the rankings of thirteen forecast-ing models ranging from classical methods like ARIMA and ETS to advanced deep learning models such as NHITS and Informer are influenced by different dataset selec-tions. We use a set of thirteen diverse benchmark datasets commonly reported in time series forecasting papers. This setup allows us to explore the impact of selective dataset reporting (cherry-picking) on model performance. Many of these models have been reported as best in class. Our goal is to determine whether the choice of datasets significantly in-fluences these rankings and whether these models would still be considered top performers across different dataset scenar-ios.\nWe focus on three key research questions:\n*   Q1: How does the selection of datasets impact the overall ranking of time series forecasting models?\n*   Q2: How does cherry-picking specific datasets influence the perceived performance of models?\n*   Q3: How many models could be reported as top perform-ers using a cherry-picked subset of datasets?\nWe use a diverse set of benchmark datasets covering various sampling frequencies, domains, and applications. They are summarized in Table1.\nThe experiments include thirteen forecasting approaches, encompassing both classical and advanced deep learning methods."}, {"title": "Results and Discussion", "content": "In this section, we present the results of our analysis on the impact of cherry-picking datasets in the evaluation of time series forecasting models.\nWe start by answering Q1. The selection of datasets has a significant impact on the overall ranking of time series fore-casting models. Our findings indicate that while some mod-els demonstrate robustness across a wide range of datasets, most are very sensitive to the specific datasets used in their evaluation."}, {"title": "Conclusions", "content": "The main conclusion of this work is that selectively choos-ing datasets can significantly distort the perceived perfor-"}]}