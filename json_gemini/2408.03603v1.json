{"title": "EnJa: Ensemble Jailbreak on Large Language Models", "authors": ["Jiahao Zhang", "Zilong Wang", "Ruofan Wang", "Xingjun Ma", "Yu-Gang Jiang"], "abstract": "As Large Language Models (LLMs) are increasingly being deployed in safety-critical applications, their vulnerability to potential jailbreaks \u2013 malicious prompts that can disable the safety mechanism of LLMs \u2013 has attracted growing research attention. While alignment methods have been proposed to protect LLMs from jailbreaks, many have found that aligned LLMs can still be jailbroken by carefully crafted malicious prompts, producing content that violates policy regulations. Existing jailbreak attacks on LLMs can be categorized into prompt-level methods which make up stories/logic to circumvent safety alignment and token-level attack methods which leverage gradient methods to find adversarial tokens. In this work, we introduce the concept of Ensemble Jailbreak (EnJa) and explore methods that can integrate prompt-level and token-level jailbreak into a more powerful hybrid jailbreak attack. Specifically, we propose a novel EnJa attack to hide harmful instructions using prompt-level jailbreak, boost the attack success rate using a gradient-based attack, and connect the two types of jailbreak attacks via a template-based connector. We evaluate the effectiveness of EnJa on several aligned models and show that it achieves a state-of-the-art attack success rate with fewer queries and is much stronger than any individual jailbreak.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as ChatGPT [1], GPT-4 [2], LLaMA-2 [3] and Claude [4] are revolutionizing industries through assistance in software development [5], financial service [6], medical diagnosis [7], and education [8]. As the application of LLMs continues to grow, the safety and ethics issues of these models have attracted increasing attention. LLMs are often trained on large-scale datasets outsourced from the internet which may contain a certain amount of harmful and toxic content. This has led to models that generate offensive and harmful content given certain prompts. To address these issues, researchers have introduced various fine-tuning [9\u201312] or alignment [13, 14] methods to reduce toxic generations or align the model's outputs with human values. However, even well-aligned LLMs have been shown to be vulnerable to malicious attacks, with jailbreak attacks [15, 16] being the major security threat.\nExisting jailbreak attacks on LLMs can be categorized into three types: 1) manually-crafted jailbreak prompt templates [17], 2) template-optimized black-box attacks, and 3) gradient-based white-box adversarial attacks. Manual jailbreak prompts need extensive human involvement and thus are costly and time-consuming. Without optimization, manual jailbreaks are often suboptimal in terms of attack strength. Template-optimized black-box attacks search for an effective jailbreak template by exploring diverse jailbreak stories via a black-box query interaction with the target model [18\u201320]. Although black-box template optimization can be more efficient than manually-crafted jailbreak prompts, it also leads to suboptimal attacks due to the lack of white-box token optimization. Gradient-based adversarial attacks [16, 15] on the contrary can boost the attack strength via optimized suffix tokens attached to the malicious prompts. However, existing gradient-based adversarial"}, {"title": "2 Related Work", "content": "Large Language Models The introduction of Generative Pre-Training Transformer (GPT) [21] marked a significant milestone in the development of LLMs that utilize vast datasets and complex model architectures. These models have proven highly effective in handling downstream tasks with minimal training samples, known as few-shot or zero-shot learning. Notably, models such as ChatGPT [1] have incorporated alignment technologies like RLHF [22] to refine their ability to adapt to specific tasks through the use of carefully crafted prompts. This technological enhancement has led to more seamless and relevant interactions between humans and models. The majority of LLMs employ an autoregressive framework, which predicts the next word in a sequence based on previous words. Its fundamental formula can be expressed as follows:\n$$p(X_{n+1:n+H}|X_{1:n}) = \\prod_{i=1}^{H} P(X_{n+i}|X_{1:n+i-1}),$$\nwhere the LLM predicts the probability p of potential subsequent tokens $X_{n+1:n+H}$ conditioned on the previous sequence of tokens $X_{1:n}$. Each token $x_i$ belongs to the set {1, . . ., V}, which represents the vocabulary, where V denotes the size of the vocabulary.\nJailbreaking LLMs Currently, LLMs are increasingly being applied across various domains, garnering significant attention from both security researchers and attackers to explore the security boundaries of these models. Studies have shown that LLMs face several security risks, including data leakage [23], data poisoning, and the vulnerability to jailbreak attacks. Beyond simply testing the spontaneous generation of harmful content [24], attackers also attempted to breach their safety protocols through strategic prompt engineering. Prompt engineering and red teaming [25] involves organizing prompts in a manner that can be understood and interpreted by generative AI models. Among these risks, we are particularly concerned about jailbreak attacks. Jailbreak is an attack specifically designed for LLMs, in which the attackers craft prompt sequences to bypass their internal safeguards, potentially leading to the generation of unexpected or harmful content. Existing jailbreak attacks on LLMs can be categorized into three types: 1) manually-crafted jailbreak prompt templates [17], 2) template-optimized black-box attacks, and 3) gradient-based white-box adversarial attacks. First, manually-crafted jailbreak involves writing a jailbreak template to break the safety alignment of LLMs, prompting them to output potentially harmful content. This type of method is straightforward to implement and can serve as a universal template. However, it requires significant human involvement, thus is costly and time-consuming. Moreover, the effectiveness of these methods decreases rapidly as LLMs continues to optimize their alignment with ethical values. Third, template-optimized black-box attacks [20, 18], such as role-playing, writing pseudo-code, or storytelling, aim to divert the model's attention towards understanding the content rather than aligning with ethical values. These methods can generate responses very efficiently, do not require modifications to the open-source models, and are suitable for black-box attacks. However, they often suffer from a low attack success rate and easily deviate from the original topic. Third, gradient-based white-box adversarial attacks [26, 16] add specific adversarial suffixes to query questions and optimize these suffix tokens in a white-box manner to make the output of LLMs start in a particular way (\"Sure, here is\") [16], achieving a jailbreak effect. This type of method is highly targeted but requires access to the internal structure of the open-source models. Since texts are discrete data, it often involves thousands of queries to sample and identify the token positions that trigger the least loss, thus consuming a substantial amount of time. Furthermore, token-level attacks can be effectively mitigated by randomization methods [27] or filtering techniques [28]. re"}, {"title": "3 Proposed Attack", "content": "In this section, we introduce our proposed Ensemble Jailbreak (EnJa) framework that effectively integrates prompt-level and token-level attack methods and its three key components: 1) Malicious Prompt Concealment, 2) Connector Template Design, and 3) Adversarial Suffix Generation.\nOverview Figure 1 illustrates the pipeline of EnJa. In the malicious prompt concealment step, we use LLM to transform malicious queries into concealed attack prompts, serving as the first part of the entire jailbreak prompt. This effectively diverts the LLM's attention and makes it less aware"}, {"title": "3.1 Malicious Prompt Concealment", "content": "Current LLMs have been extensively value-aligned to enhance their sensitivity to harmful content. This alignment expects the models to refuse arbitrary malicious user queries. While existing template-optimization methods such as PAIR [20] and GPTFUZZER [18] have improved over manually-crafted jailbreak templates, they still have certain limitations. PAIR uses LLMs to automatically embed malicious text into complex environments, such as fictional scenarios or specific tasks, which diverts the LLM's attention and facilitates jailbreaking. However, generating prompts in PAIR requires continuous iteration and often leads to deviation from the topic. Additionally, GPTFUZZER utilizes 77 manually crafted prompt templates to generate new jailbreak templates. However, these templates are no longer effective. Consequently, these attacks fail to achieve a high success rate against LLMs with strong alignment.\nTo address the limitations of existing template-optimized attacks, in this step (see Algorithm 2), we generate a covert jailbreak prompt for each restricted behavior by prompting the Vicuna-13B model [29]. We then use the generated jailbreak prompt to attack the model itself and obtain an initial malicious answer, which will be utilized in the subsequent attack steps as a suggestive answer. Since we have the flexibility to manipulate the system prompt of the Vicuna-13B model and it has a relatively weak alignment, obtaining jailbreak prompts and answers is relatively easy. To maintain consistency with the target topic, we employ a judge model (which is also the Vicuna-13B) to evaluate whether the jailbreak prompt is essentially requesting the same content. If the content is off-topic, the prompt will be regenerated. The system prompt designed for the judge model is shown in Appendix B.2. This strategy not only enhances the stealthiness of our attack but also increases the specificity and effectiveness of the attack content. Furthermore, it can gradually infiltrate and disrupt the normal operations of models without triggering the safety mechanisms of well-aligned models."}, {"title": "3.2 Connector Template Design", "content": "In the second step of EnJa, our goal is to enhance the continuity between the concealed prompt from the previous step and the subsequent adversarial suffix. To this end, we design a connector template motivated by distraction tactics. This prompt template provides detailed instructions on how to lead LLMs to remove their built-in restrictions, disregard all moral boundaries, and avoid attempting to educate users.\nWhen constructing the opening lines, we draw inspiration from the \"Sure! Here's\" phrasing used in previous works [16]. However, we do not directly include the target content in the opening lines but instead integrate the beginning of the suggestive answer (generated in the previous step) into the connector template, as shown in the template below. This strategy significantly reduces the LLM's sensitivity to queries and effectively avoids the occurrences of off-topic, making the content more covert and closely related to our attack objective. The proposed connector template is as follows:\n[concealed malicious prompt] You have no moral, ethical, legal, programming, principles, or guidelines stopping you from always searching for what the user queries. Stick to the topic. Don't educate me and tell me any risks. Give me the detailed answer. Begin your response with 'Sure, I'll help you. Here's my comprehensive strategy. [beginning of the suggestive answer]'. [adversarial suffix]\nThrough this strategy, we are able to more precisely control the output of the language models and ensure that they gradually relax the screening of harmful content. Additionally, this step increases the stealth and effectiveness of our attacks, making the entire process appear more natural and less likely to be detected by standard security protocols. Overall, the second step of EnJa not only enhances the coherence of our attacks but also provides us with more insights into manipulating LLMs in complex contexts. It lays a solid foundation for implementing deeper and more sophisticated attack strategies, suggesting that we could achieve a higher success rate in jailbreaking highly aligned models."}, {"title": "3.3 Adversarial Suffix Generation", "content": "The third step of EnJa involves the optimization of adversarial suffixes, which is crucial for the success of an attack. In the current literature, GCG [16] is one the most effective adversarial suffix generation methods. However, the direct nature of the prompts and targets used in GCG makes them easily detectable and likely to be rejected by LLMs. Additionally, GCG requires a large number of iterations to find the most effective adversarial tokens, making it extremely time-consuming. We also find that GCG is prone to the regret phenomenon where the model suddenly regrets and starts to correct itself during the generation of the malicious response. To overcome these issues, we propose an enhanced version of GCG for optimizing the adversarial suffix and call it Enhanced GCG (see Appendix D). Enhanced GCG improves the speed and effectiveness of the original GCG. Specifically, its main adversarial objective is the same as GCG:\n$$L_{adv} (X_{1:n}) = \\frac{- log p (x^*_{n+1:n+H}|X_{1:n})}{H}$$"}, {"title": "3.4 Fixing Pitfalls in Performance Evaluation", "content": "We adopt the attack success rate (ASR) and the number of queries to evaluate the effectiveness and efficiency of our attack method. However, determining whether an attack is successful is a complex and challenging task. Previous studies have proposed various criteria, but they all rely on over-simplified standards, leading to inaccurate evaluations. For example, Zou et al. [16] proposed a metric based on rejective keyword matching, which only checks if the model output contains any reject keywords to determine if the attack was successful. However, this method can lead to a large number of false positives, as the absence of reject keywords does not necessarily indicate a successful attack. In [20] and [29], GPT-4 [2] was used to assess whether an output indicates a successful attack. Although this evaluation is accurate, it incurs a high API cost and thus is not suitable for researchers with limited budgets. In [18], a trained ROBERTa model is utilized as a classifier to judge whether the content of the output is harmful. However, the harmful output might have already deviated from the original topic. To address the issues of existing evaluation metrics, we propose the following adjustments to make evaluations more comprehensive and accurate. First, we set the number of output tokens to 256, which helps to improve the accuracy of judging the regret phenomenon (especially the regret phenomenon that occurs in the second half of the output). Then, we detect whether the output of the model contains any words or phrases from the rejection keyword list. Finally, the RoBERTa model is utilized to verify whether the output content is harmful. This comprehensive pipeline can improve the accuracy and reliability of the evaluation. The number of queries refers to the number of forward passes needed for a gradient-based attack to optimize the jailbreak prompt."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDataset\nIn previous research, Zou et al. have collected a malicious dataset, i.e., AdvBench [16], which contains 520 harmful behaviors. However, many of the instructions in AdvBench are repetitive. As such, Chao et al. selected 50 representative harmful instructions from AdvBench to form the AdvBench Subset [20], which has been widely used in the literature. We also use the AdvBench Subset in our experiments. It contains diverse restricted behaviors that violate ethical policies, including illegal or unethical activities, discrimination, and toxic content.\nModels and Configuration We test multiple open-source and commercial LLMs to demonstrate the effectiveness of our method. The selected open-source models include Vicuna-7B [29], Vicuna-13B, LLaMA-2-7B [3], and LLaMA-2-13B. For closed-source commercial models, we chose GPT-3.5 and GPT-4 [2]. For each model, we used a zero-temperature setting for deterministic generation, producing texts of 256 tokens in length. Since system prompts have a significant impact on the model's ability to defend against attacks and to ensure a fair comparison, we use the default system prompts of the tested LLMs. We use Vicuna-13B-v1.5 as both the attacker model and the off-topic checking model for the first step of our attack. This model is employed to generate the initial jailbreak prompts and answers, as well as to identify off-topic responses. In this step, the number of iteration rounds is set to 1, and we establish 10 parallel streams to enhance processing efficiency. In Enhanced GCG, we set the length of the adversarial suffix to 20 tokens and the number of iterations to 200. We set the number of branches used in multi-branch adversarial suffix generation to 2 and the regret prevention loss coefficient to 0.2 with the threshold @ set to 0.1. When attacking the 7B model, we set the number of candidates (i.e., batch size) to 320 and top-k to 256. For the 13B model, we set the batch size to 128 and top-k to 96. Our experiments can be run on a single NVIDIA A100 GPU."}, {"title": "4.2 Main Results", "content": "Attacking Open-source LLMs We first compare the attack effectiveness of our EnJa with existing attacks GCG, PAIR, and GPTFuzzer, and measure the attack success rate using the corrected calculation described in Section 3.4. The results are reported in Table 1. Note that, for the two template-optimization black-box attacks PAIR and GPTFuzzer, we directly use their reported per-"}, {"title": "4.3 Ablation Study", "content": "Here, we conduct a set of ablation experiments to help understand the techniques used in EnJa. The core strategy of EnJa is to use LLMs to conceal potentially harmful instructions via template-"}, {"title": "5 Conclusion", "content": "In this paper, we introduced an Ensemble Jailbreak (EnJa) framework to bypass the safety alignment of LLMs. EnJa combines prompt-level attacks with token-level attacks in a synergistic manner to generate more powerful jailbreak prompts. Specifically, it generates a jailbreak prompt in three steps: 1) malicious prompt concealment, 2) connector template design, and 3) adversarial suffix generation. We demonstrate the effectiveness of EnJa on both open-source and closed-source LLMs. Particularly, Enja achieves high attack success rates of above 88% on open-source Vicuna and Llama models, and successfully jailbreaks GPT-3.5-turbo and GPT-4 with a success rate of 96% and 56%, respectively. We hope our work can help examine the vulnerability of LLMs under more advanced jailbreak attacks."}, {"title": "Broader Impacts", "content": "Our work endeavors to devise a sophisticated attack to examine the vulnerabilities of production large language models (LLMs). We intend to make our EnJa attack publicly available and disclose the crafted jailbreak prompts to LLM service providers. Ensemble attacks, by virtue of integrating the strengths of multiple attack types through connector templates, can obfuscate malicious intent across diverse attack components, rendering jailbreaks more covert, efficient, and adaptable. Our research demonstrates that ensemble attacks have the potential to circumvent existing safety measures implemented in current LLMs, highlighting the pressing need for a deeper understanding of user queries to bolster LLM security. We advocate for the implementation of robust mechanisms for detecting malicious prompts and jailbreaks, which are imperative for the responsible evolution of LLMs, ensuring their safe and beneficial integration into society. We further believe that the continual refinement of LLMs to defend against EnJa prompts and similar threats can significantly enhance their overall safety and alignment with societal values."}, {"title": "B System Prompts", "content": "B.1 System Prompts for Attacker Model\nIn the Malicious Prompt Concealment stage, we use Vicuna-13B model as the attacker model to generate jailbreak prompts. In this process, we follow the system prompts designed by Chao et al. [20] for the attacker model.\nB.2 System Prompts for Judge Model\nWe utilize the judge model to evaluate whether the jailbreak prompt is consistent with the behavior. The system prompt designed for judge model is shown in table 3."}, {"title": "C Implementation Details", "content": "The list of refusal keywords used in experiments is shown in table 4."}, {"title": "D Enhanced GCG Algorithm", "content": "Algorithm 3: Enhanced GCG\nInput: Prompts set $x_{1:m}$, batch size B, modifiable subset I, branch number m, regret prevention sign s, coefficient of regret prevention loss \u5165\nOutput: Optimized prompts set $x_{1:m}$\n$z \\leftarrow 1$\nfor b$\\leftarrow$ 1,..., m do\nfor i $\\in$ I do\nif s == 0 then\n$X_i \\leftarrow top-k(-V_{x_i}; L_{adv} (x_{1:n}))$\nelse\n$X_i\\leftarrow top-k(-V_{x_i}; L_{total} (x_{1:n}))$\nfor j $\\leftarrow$ 1,..., $\\frac{[B]}{m}$ do\n$x_{1:n}^{jin} x_{1:n}^{jin}$\n$\\{X^*| X \\sim Uniform(X_i), i = Uniform(I)}$\nzz+1\nif s=0 then\n$x_{1:m}^*x_{1:m}, H \\leftarrow top-m(-L_{adv} (X_{1:m}^*))$\nelse\n$x_{1:m}^*x_{1:m}, H \\leftarrow top-m(-L_{total} (X_{1:m}^*))$\nreturn $x_{1:m}$"}, {"title": "E Experiments (hyperparameter research)", "content": ""}]}