{"title": "AquaticCLIP: A Vision-Language Foundation Model for Underwater Scene Analysis", "authors": ["Basit Alawode", "Iyyakutti Iyappan Ganapathi", "Sajid Javed", "Naoufel Werghi", "Mohammed Bennamoun", "Arif Mahmood"], "abstract": "The preservation of aquatic biodiversity is critical in mitigating the effects of climate change. Aquatic scene understanding plays a pivotal role in aiding marine scientists in their decision-making processes. In this paper, we introduce AquaticCLIP, a novel contrastive language-image pre-training model tailored for aquatic scene understanding. AquaticCLIP presents a new unsupervised learning framework that aligns images and texts in aquatic environments, enabling tasks such as segmentation, classification, detection, and object counting. By leveraging our large-scale underwater image-text paired dataset without the need for ground-truth annotations, our model enriches existing vision-language models in the aquatic domain. For this purpose, we construct a 2 million underwater image-text paired dataset using heterogeneous resources including YouTube, Netflix, NatGeo, etc. To fine-tune AquaticCLIP, we propose a prompt-guided vision encoder that progressively aggregates patch features via learnable prompts, while a vision-guided mechanism enhances the language encoder by incorporating visual context. The model is optimized through a contrastive pre-training loss to align visual and textual modalities. AquaticCLIP achieves notable performance improvements in zero-shot settings across multiple underwater computer vision tasks, outperforming existing methods in both robustness and interpretability. Our model sets a new benchmark for vision-language applications in underwater environments. The code and dataset for AquaticCLIP are publicly available on GitHub at xxx.", "sections": [{"title": "I. INTRODUCTION", "content": "Global aquatic\u00b9 ecosystems are under severe threats from human activities such as overfishing and coastal development, along with climate change impacts [37], [49], [60], [144]. Effective conservation efforts depend on precise monitoring, which requires an accurate and automatic aquatic scene understanding system [39], [46]. However, the complexity of understanding aquatic environments demands significant expertise from ocean scientists and marine biologists, creating challenges for efficient monitoring [102], [140]. Recently, Vision-Language Models (VLMs) have gained increasing attention in the computer vision field [85], [90], [97], [103], [126]. These models are typically pre-trained using large-scale image-text paired data, readily available online\n\u00b9 Aquatic is a broad term encompassing all water-based environments, including both freshwater and saltwater habitats. It encompasses a vast range of ecosystems, from rivers and lakes to oceans and coral reefs."}, {"title": "II. RELATED WORK", "content": "Numerous deep learning-based approaches have been proposed for various aquatic scene understanding tasks [45], [74], [102], [121], [133] including underwater image enhancement [34], [57], [123], species classification [70], [125], underwater object recognition [59], [64], coral segmentation [139], Object counting [111], and underwater tracking [133].\n1. CNN-based Methods: For underwater image enhancement and restoration, many CNN-based approaches have been proposed including WaterGAN [69], Image-2-Image Translation [29], AquaGAN [34], CycleGAN [84], AGCycleGAN [123] and others [54]. For underwater object detection and classification, several methods and datasets have led to significant progress [59], [64], [130]. Khan et al. introduced FishNet, a benchmark dataset for fish recognition and functional trait prediction [64].\n2. Transformer-based Methods: Many ViT-based techniques for detection, segmentation, counting, tracking, and classification have been proposed for underwater scenes [15], [95], [111], [127], [133]. For instance, UShape transformer [95] and transformer-driven GAN [117] have been proposed for underwater image restoration tasks. Alawode et al. proposed a combined approach for underwater image enhancement and visual tracking [16], and Zhang et al. developed a large-scale benchmark for advancing underwater object tracking [133]. A dense object counter [111] and density-guided attention [127] methods are proposed for underwater object counting task. For fish detection and classification [80], [81], and coral reef classification [15], [105] tasks, ViT-based models have also recently been proposed. A token-based selective ViT [107] and cascaded attention [136] models are proposed for fine-grained marine species classification. Additionally, some self-supervised learning methods have also been utilized for underwater image analysis [55], [101].\n3. Vision-Language Models (VLMs): In the context of safeguarding aquatic biodiversity, there is an increasing need for VLMs to facilitate AI-based aquatic scene understanding systems. However, VLMs have only been sparsely applied to aquatic scene analysis [139], [140], [149]. For instance, Zheng et al. introduced CoralSCOP, a foundational model for the automatic segmentation of coral reefs [139], and MarineGPT, a multimodal large language model for marine object classification and question-answering [140]. Zheng et al. further evaluated GPT-4V for marine images, but found its performance unsatisfactory for domain-specific needs of marine biologists [138]. Recently, the MarineInst foundational model was proposed, pre-trained on 2M images for segment-ing and captioning marine imagery [149]. To the best of our knowledge, VLMs have not been thoroughly explored for aquatic scene understanding, except for MarineGPT and MarineInst. Our work is the first to introduce AquaticCLIP, with comprehensive analysis and comparisons to existing SOTA methods."}, {"title": "III. PROPOSED AQUATICCLIP MODEL", "content": "This work introduces the Aquatic Contrastive Language-Image Pre-training (AquaticCLIP) model, which leverages a collection of 2M aquatic image-text pairs curated form several heterogeneous resources. The ground truth descriptions are also enriched by harnessing the existing VLM MarineGPT. The primary objective is to pre-train AquaticCLIP using diverse aquatic data sourced from various platforms, enhancing its capability for zero-shot transfer across different aquatic imagery tasks. This is particularly beneficial for recognizing unfamiliar marine species and coral reef categories that were not encountered during training. Fig. 2 illustrates the key components of the proposed model, which include the construction of the 2M aquatic image-text paired dataset further enriched by unsupervised generated textual descriptions, caption cleaning, a lightweight prompt-guided vision encoder, a vision-guided text encoder, and the pre-training process. The contrastive learning approach aligns positive image-text pairs while separating negative ones. The details of these processes are discussed in the subsequent sections.\nA. Aquatic Dataset Construction and Curation\nOur dataset construction pipeline involves two main steps: gathering image-text paired data from multiple sources and cleaning and filtering the collected data. We assembled a dataset of 2 million aquatic image-text pairs from various resources, including YouTube videos, marine and ocean sciences textbooks and articles, the Corals of the World [119], Fishes of Australia [86], [104], [106], [118], Marine Twitter, Netflix, and National Geographic (NatGeo) [32], [36], [40], [87], [89], [113], [114].\nFor YouTube videos, we searched using keywords such as \u201cunderwater world\u201d, \u201cmarine documentary\u201d, \u201cdeep oceans\u201d, \u201cgreat barrier reef\u201d, \u201caquatic scenes\u201d, and \u201ccoral reefs\u201d etc. For Netflix videos, we explored hundreds of documentaries, including \"My Octopus Teacher\u201d, \u201cLast Breath", "Wonders of the Reef": "etc. Subtitles provided by both resources were used to generate aligned image-text pairs, which were manually checked and refined. Unique frames were extracted every 50 seconds from the videos, which often contained challenges like low visibility, motion blur, background clutter, and color distortions.\nAdditionally, We utilized 1200 diverse textbooks on marine biology and oceanography, along with research articles from ocean and marine journals and NatGeo magazines, to further enrich the dataset. Figures and captions were extracted using PDF-Figures 2.0 tool [30], and we manually refined the data to ensure the selected images had meaningful associated text. Images not related to aquatic environments were discarded.\nWe also included image-text pairs from the Corals of the World repository and Fishes of Australia, only selecting pairs with detailed descriptions. Furthermore, we used the Twitter platform to search for relevant content using hashtags like #MarineBiology, #Oceans, and #Fisheries, considering only channels with over 100 followers. After a thorough cleaning and filtering process, we retained 2 million high-quality image-text pairs representing a diverse range of aquatic scenes. More details are provided in the supplementary material.\nB. Unsupervised Generation of Image and Instance-Level Descriptions (Fig. 2 (b))\nIn order to enrich the ground-truth textual descriptions, we generated additional textual descriptions at both the image and instance levels using a VLM MarineGPT [140], which"}, {"title": "C. Semantic Filtering and Cleaning of Generated Textual Descriptions (Figs. 2 (c)-(f))", "content": "The generated textual descriptions Si may contain noise, such as broken sentences, incorrect descriptions, or irrelevant keywords. To address these issues, we developed a textual description cleaning module aimed at identifying the semantically closest and most relevant keywords.\nIn this process, each generated textual description Si is broken down into a set of k-keywords (\\{s_i\\}_{i=1}^k). For each keyword, we compute its cosine similarity with the image embedding as follows:\n$\\hat{S}_{i} = argmax < \\Phi_v(I_i) \\cdot \\Phi_t(s) > , $ (1)\n$ s \\in S_i$\nwhere \\( \\Phi_v \\) is a vision encoder followed by an MLP, and \\( \\Phi_t \\) is a text encoder from the CLIP model. We retain the top-p%"}, {"title": "D. Prompt-guided Vision Encoder (Fig. 3 (a))", "content": "To generate efficient visual embeddings for each aquatic image Ii, we aggregate the patch features of the input image using learned visual prompts. First, the input image I\u00bf is divided into np non-overlapping patches \\{w_j\\}_{j=1}^{n_p}, each of size mxm. These patches are fed into the pre-trained image encoder \u03a6\u03c5 to produce embeddings P\u2081 = \\{p_j\\} \\in R^{d_pXn_p}.\nTo effectively aggregate these patch embeddings into a final image-level embedding for similarity calculation, we designed a prompt-guided image encoder, as illustrated in Fig. 3 (a). We randomly initialize a set of learnable prompt features Q = \\{r_j\\}_{j=1}^{n_r} \\in R^{d_p \\times n_r}, where nr represents the number of learnable prompts. These prompts guide the progressive fusion of patch embeddings. Cross-attention is then computed using the visual embeddings as keys K\u2081 = Pi and values Vi = Pi,"}, {"title": "E. Vision-guided Text Encoder (Fig. 3 (b))", "content": "In the text encoder branch, the enriched textual descriptions Ci are fed into the CLIP text encoder to obtain textual representations Ti corresponding to the descriptions of the i-th image. These representations are then passed through a vision-guided attention layer for refinement. The patch features Pi and the learned prompts Ei are concatenated as Vi, which serves as the key Kt and value Vt, while the textual representations Ti are used as the query, as shown in (Fig. 3 (b)). The vision-guided attention mechanism is computed as follows:\nUi = Softmax ($\\frac{T_iK_i}{\\sqrt{d_p}}$)Vi,Ti = Ti + Ui,  (5)\nEi+Ti\nThis context-guided text encoder further refines the textual features by incorporating image context and learned visual prompts. This process enhances the alignment between images and texts, improving the performance of the AquaticCLIP model."}, {"title": "F. Cross-Modal Contrastive Loss for Vision-Language Alignment (Fig. 2 (i))", "content": "We pre-train our prompt-guided vision encoder and vision-guided text encoder using a cross-modal contrastive loss function. This loss is formulated as a temperature-scaled vision-language pre-training loss, similar to W-way classification, where W represents the batch size of image-text pairs involved in the training process [25], [97], [115]. Given a batch of W paired normalized image and text embeddings \\{f_i, T_i\\}, we minimize the contrastive loss in two directions: image-to-text (i\u2192t) and text-to-image (t \u2192 i) as:\n$L_{i2t} = - \\frac{1}{W} \\sum_{i=1}^{W} log(\\frac{exp(\\tau T_i f_i)}{\\sum_{j=1}^{W} exp(\\tau T_i f_j)}),$ (6)\n$L_{t2i} = - \\frac{1}{W} \\sum_{j=1}^{W} log(\\frac{exp(\\tau f_i T_i)}{\\sum_{j=1}^{W} exp(\\tau f_i T_j)}),$ (7)\nwhere \u03c4 is a learnable temperature parameter that controls the smoothness of the distribution [97]. The overall contrastive loss, Lcont, is the sum of both losses: Lcont = Li2t + Lt2i. This loss minimizes the distance between embeddings of positive image-text pairs and maximizes the distance between negative pairs (fi, Tj), where i \u2260 j, ensuring that images and texts with the same semantic content have similar representations in the feature space."}, {"title": "G. Zero-shot Transfer for Image Classification", "content": "Radford et al. introduced the concept of zero-shot classification using a prompt-based approach [97]. In our method, each class in the test dataset is converted into one or more text prompts using predefined templates, such as \u201cAn image of {Sea Urchins}.\u201d or \u201cAn image of {Oyster}.\" For each test image, we compute the l2 normalized embeddings using our prompt-guided vision encoder and vision-guided text encoder. We then calculate the cosine similarity between the test image and the set of testing prompts to find the best match, resulting in zero-shot classification. Additional details are provided in the supplementary material.\""}, {"title": "IV. EXPERIMENTAL EVALUATIONS", "content": "We conducted extensive experiments to evaluate the performance of the proposed AquaticCLIP model across various tasks, including zero-shot classification of marine species, fine-grained fish, and coral species classification. Additionally, we performed zero-shot cross-modal retrieval tasks for aquatic images (see supplementary material). For downstream tasks,"}, {"title": "G. Object Detection and Classification Results", "content": "In this experiment, we replaced ResNet-50 in MRegionCLIP with our pre-trained image encoder \u03a6 and applied the same fine-tuning settings as discussed in the supplementary. We named this model AquaticDet and compared it against SOTA methods. Table IV shows object detection and classification results across four different datasets, with AquaticDet achieving the best mAP50 scores across all compared datasets by a significant margin. This superior performance is attributed to pre-training on the 2M image-text paired dataset, which allowed AquaticDet to extract highly efficient and effective visual features. The inclusion of the prompt-guided vision encoder and vision-guided text encoder, along with comprehensive captions at both the image and instance levels, contributed to substantial performance improvements, even under challenging conditions."}, {"title": "H. Computational Complexity", "content": "We also evaluated the computational complexity of Aquatic-CLIP during the inference stage across various tasks, including zero-shot classification, supervised object detection and classification, supervised segmentation, and object counting. For zero-shot classification, AquaticCLIP took 0.80 seconds, AquaticSAM took 1.23 seconds, AquaticDet took 1.19 seconds, while the AquaticOC model took 1.31 seconds to count objects. We used the same hardware settings as discussed above."}, {"title": "V. CONCLUSION", "content": "Current aquatic and underwater VLMs rely on paired image-text data for pre-training. In this work, we introduced AquaticCLIP, pre-trained using real aquatic image-text pairs and additional generated textual descriptions at both image and instance levels. To achieve this, we built a 2M image-text paired dataset sourced from various online repositories. We proposed a novel vision-language alignment model where the vision encoder is guided by learned prompts, and the text encoder benefits from visual prompts. Both lightweight encoders are pre-trained using cross-modal contrastive supervision for enhanced vision-language alignment. AquaticCLIP was evaluated across a diverse set of marine vision tasks, including zero-shot fine-grained object classification, fine-tuned instance and semantic segmentation, and object detection, and counting. Our model consistently delivered superior results compared to existing SOTA methods designed specifically for marine environments, demonstrating its robustness and effectiveness across multiple aquatic vision tasks."}]}