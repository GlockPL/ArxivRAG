{"title": "Logic Meets Magic: LLMs Cracking Smart Contract Vulnerabilities", "authors": ["ZeKe Xiao", "Qin Wang", "Hammond Pearce", "Shiping Chen"], "abstract": "Smart contract vulnerabilities caused significant economic losses in blockchain applications. Large Language Models (LLMs) provide new possibilities for addressing this time-consuming task. However, state-of-the-art LLM-based detection solutions are often plagued by high false-positive rates.\nIn this paper, we push the boundaries of existing research in two key ways. First, our evaluation is based on Solidity v0.8, offering the most up-to-date insights compared to prior studies that focus on older versions (v0.4). Second, we leverage the latest five LLM models (across companies), ensuring comprehensive coverage across the most advanced capabilities in the field.\nWe conducted a series of rigorous evaluations. Our experiments demonstrate that a well-designed prompt can reduce the false-positive rate by over 60%. Surprisingly, we also discovered that the recall rate for detecting some specific vulnerabilities in Solidity v0.8 has dropped to just 13% compared to earlier versions (i.e., v0.4). Further analysis reveals the root cause of this decline: the reliance of LLMs on identifying changes in newly introduced libraries and frameworks during detection.", "sections": [{"title": "I. INTRODUCTION", "content": "A smart contract is an automatable and enforceable agreement. It is automatable by computer, though some aspects may require human input and control, and enforceable either through legal mechanisms or tamper-proof execution of code [1]. Smart contracts are a fundamental component of blockchain technology [2]. Unlike traditional contracts, smart contracts automate agreement execution in a distributed environment when predefined conditions are met [3]. Their efficiency and reliability have led to widespread adoption in blockchain systems, enabling the development of Web3 [4], including decentralized applications (DApps), Decentralized Finance (DeFi) [5], [6], Non-Fungible Tokens (NFTs) [7], and Game Finance (GameFi) [8].\nSmart contract-powered on-chain DApps and protocols have gained immense popularity in recent years, with Bitcoin and Ethereum ETPs collectively holding US$65b in on-chain assets as of 2024 [9]. However, this growth has also exposed vulnerabilities in smart contracts, leading to significant financial losses. According to a SlowMist [10], in August 2024 alone, these vulnerabilities resulted in losses exceeding US$316m.\nManually auditing smart contracts is labor-intensive [11], and current smart contract scanners (over 17 types) have demonstrated poor performance [12]. While large language models (LLMs) offer new opportunities for tackling this challenge, having shown high efficiency in program analysis [13], [14] and code generation [15], their potential for smart contract vulnerability detection remains underexplored.\nExisting evaluations of LLMs for this purpose are limited (evidence in Table V), often focusing on the GPT series and revealing high false positive rates (FPR) [16], [17]. Moreover, these studies primarily use datasets written in outdated Solidity versions (v0.4), neglecting the nuances of detection in Solidity v0.8, the more widely used version. Additionally, most detection methods rely on a single LLM (e.g., GPT-3.5 or GPT-4), restricting their cross-platform applicability.\nTo address these challenges, we posed the following research questions (short for RQs):\n\u2022 RQ.1: What is the FPRs of mainstream LLMs when analyzing a dataset of non-vulnerable top contracts?\n\u2022 RQ.2: Can we decrease false positive rate through reasonably prompt design?\n\u2022 RQ.3: What are the limitations of general LLMs in detecting vulnerabilities in smart contracts written in Solidity v0.8? Any differences to v0.4?\nOur study centers on investigating the potential limitations of LLMs in identifying vulnerabilities and examining possible measures to enhance their performance.\n\u2022 We conducted an experiment on the TOP200 dataset to evaluate the basic performance of five up-to-date LLMs in detecting vulnerabilities, particularly focusing on the false positive rate. After the pre-test, the Max Tokens for each LLM were sufficient to allocate general smart contracts, eliminating the need for division. Then, we successfully reduced the false positive rate through a reasonably prompt design and validated it with an experiment. This new design can help researchers determine the lower bound of LLM's false positive rate.\n\u2022 We conducted another experiment on the Web3Bugs dataset to evaluate the performance of evaluated LLMs in identifying different types of vulnerabilities, including re-entrancy, arithmetic issues, denial of service, access control, manipulated price, and oracle issues. This marks the first evaluation of the performance of LLMs using smart contracts written in Solidity v0.8, which is currently the most widely utilized version. In contrast, previous studies were based on Solidity v0.4 and involved the analysis of much shorter and simpler codes.\n\u2022 Furthermore, we observed differences in the detection abilities of general LLMs between Solidity v0.8 and v0.4."}, {"title": "II. TECHNICAL WARMUPS", "content": "A. Core Concepts\nBlockchain. A blockchain is a distributed ledger with growing lists of records (blocks) that are securely linked together via cryptographic hashes [18]. It was first proposed by S. Nakamoto [19], a peer-to-peer network that sits on top of the internet, and was introduced to public in October 2008 as part of a proposal for bitcoin, a virtual currency system that eschewed a central authority for issuing currency, transferring ownership, and confirm transactions. Bitcoin is the first blockchain application [20]. Additionally, Blockchain has numerous benefits such as decentralization, persistency, anonymity and audit-ability. There is a wide spectrum of blockchain applications ranging from cryptocurrency, financial services, risk management, internet of things (IoT) to public and social services [21].\nSmart Contract. A smart contract is an intelligent agent. In other words, it is a computer program capable of making decisions when certain preconditions are met. The intelligence of an agent depends on the complexity of a transaction it is programmed to perform [22]. In a smart contract, contract clauses written in computer programs will be automatically executed when predefined conditions are met. Smart contracts consisting of transactions are essentially stored, replicated, and updated in distributed blockchains [23].\nLarge Language Models. Typically, LLMs refer to Transformer language models that contain hundreds of billions (or more) of parameters, which are trained on massive text data. LLMs exhibit strong capacities to understand natural language and solve complex tasks (via text generation) [24].\nWith the increase in capabilities, LLMs are able to autonomously exploit one-day vulnerabilities in real-world systems. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities [25]. Additionally, LLMs can achieve end-to-end program repair [26].\nB. Key Questions Before Main-dish\nWhat are smart contract vulnerabilities? Similar to any other software, smart contracts are susceptible to bugs and vulnerabilities. Given that smart contracts are directly associated with cryptocurrencies, the potential financial losses resulting from undiscovered vulnerabilities can be significant [27]. According to a SlowMist report [28], in November 2024 alone, vulnerabilities in smart contracts have resulted in financial losses exceeding $9.38 million."}, {"title": "III. METHODOLOGY", "content": "A. Overview\nFig. 1 illustrates the research procedure. Our study integrates five up-to-date LLMs (GPT-40-mini, Gemini 1.5 Pro, Claude 3.5 Sonnet, Yi-large, and Qwen-plus) and assesses their performance across three curated datasets: the TOP 200 dataset, the Web3Bugs dataset, and the Messi-Q Smart Contract Dataset. To ensure relevance, our experiments target Solidity v0.8, a widely used version, addressing gaps in prior"}, {"title": "IV. DETECTION RESULTS", "content": "A. RQ1: Measuring False Positive Rate of mainstream LLMs\n1) Evaluation: In RQ1, aiming to measure the false positive rate of mainstream LLMs in analyzing smart contracts, we collected smart contracts from the TOP200 datasets and scanned each Solidity document using LLMs. Although some of the documents are stable library files or system documents, each LLM scanned the same SOL documents, so this would not influence the experimental results. Along with the experimental results, six metrics were used to evaluate the performance of LLM, including TN, TP, FP, FN, Accuracy, and False Positive Rate. Among them, Accuracy and False Positive Rate are derived from the other metrics.\n\u2022 TP is the number of True Positives. One true positive is counted when LLM successfully detects a ground-truth vulnerability after scanning a smart contract of dataset.\n\u2022 TN is the number of True Negatives. One true negative is counted when LLM correctly does not report any vulnerability after scanning a smart contract of dataset.\n\u2022 FP is the number of False Positives. One False positive is counted when LLM incorrectly reports oneor more vulnerabilities after scanning a smart contract of dataset.\n\u2022 FN is the number of False Negatives. One false negative is counted when LLM fails to detect the ground-truth vulnerability after scanning a smart contract of dataset.\n\u2022 False Positive Rate (FPR) indicates a given condition exists when it does not.\n$FPR=\\frac{FP}{FP+TN}$ (1)\nIn the evaluation, we established a dialogue with the model through the API. The prompt is first sent to the model to initiate the evaluation. Then, the model reads the code of each smart contract and inputs it into the model.\nThe prompt was designed as:\nYou are a smart contract auditor.\nPlease review the following smart contract in detail carefully. Is the following smart contract vulnerable to any attacks? Please only answer yes or no. If yes, please answer with one main vulnerability. [content]\nIn the prompt, \"content\" is the code of each smart contract. Additionally, the info of models has been recorded in Table. I. All of them are the latest models of their respective series by the time of writing the paper. Among them, Gemini has released 2.0 Pro version, but it is not stable enough to be used in evaluation. The temperature parameter of for each model was set at 0.7, a default value, to ensure fostering innovation and stability during answer generation, while other parameters were also kept at their default settings.\n2) Result: The evaluation results of these models are listed in Table II, which shows that the false positive rates of all the models in the experiment are generally high. Among them, the false positive rates of GPT-40-mini and Claude 3.5 Sonnect reached 0.85 and 0.78, respectively, while Yi-large and Qwen-plus demonstrated stable performance, maintaining a false positive rate around 0.5.\nAnswer to RQ1.\nThe false positive rate of current mainstream LLMs in vulnerability detection is generally higher than 50%.\nB. RQ2: Decreasing FPR via Reasonable Prompt?\n1) Evaluation: A high false positive rate is a common attribute reported in related experiments on detecting vulnerabilities in smart contracts or programs with LLMs. In RQ1, we found that this issue occurs in general LLMs. In response to the extremely high false positive rate, we further explore a method to reduce the false positive rate of these models, and evaluate the potential lower bound of their false positive rate. To achieve this reduction without fine-tuning or introducing other methods, the first step is redesigning the prompt.\nAfter checking the prompt of RQ1, we found that RQ1's prompt did not clearly specify what vulnerabilities the models were expected to detect. Even though a binary classification would have been simple enough to understand, there was no clear direction for detecting vulnerabilities. As a result, the LLMs made more mistakes.\nBased on this evaluation, we proposed the following hypothesis: LLMs would have a lower false positive rate when detecting a specific vulnerability.\nHence, we redesigned the prompt:\nYou are a smart contract auditor.\nPlease review the following smart contract in detail carefully.\nPlease confirm whether the smart contract project has \"A\" vulnerability, and only answer yes or no. [content]\nIn the actual prompt, \"A\" represents the types of vulnerabilities and was replaced with the specific name of vulnerability, such as \"Access Control\u201d, while \u201ccontent\" refers to the code of each smart contract."}, {"title": "Answer to RQ2.", "content": "If LLMs focus on detecting single vulnerability, they can achieve a noticeable decrease at false positive rate.\nC. RQ3: LLM Limitations?\nAfter evaluating a model's recall rate for different vulnerabilities, we can clearly see that its performance varies when detecting different vulnerabilities. To illustrate, GPT-40-mini successfully detected almost all \"Access Control\" vulnerabilities, with its recall rate reaching over 85%, while it failed to identify vulnerabilities related to \"Re-entrancy\" and \"Arithmetic Issues\".\nThrough comparison, we can clearly see that, among these vulnerabilities, detecting the Re-entrancy and Arithmetic Issue vulnerabilities is challenging for general LLMs. The related numbers are highlighted in red and gray in Table IV.\n1) Arithmetic Issue: In order to further analyze the problem in detail, we repeated the experiment and asked models not only to evaluate smart contracts with binary classification, but also generate a related report.\nYou are a smart contract auditor.\nPlease review the following smart contract in detail carefully.\nPlease confirm whether the smart contract project has \"A\" vulnerability, and only answer yes or no. If no, please answer with the main reason. [content]\nIn the Arithmetic Issue report, it indicates that the main reason for reporting no vulnerabilities is that the program was written in Solidity v0.8, and the Solidity compiler automatically checks for Overflow/Underflow.\nNo. This smart contract does not appear to have an Arithmetic Vulnerability. The contract is using Solidity v0.8.2, which includes built-in overflow and underflow checks for arithmetic operations.\nHowever, \"Arithmetic Issues\" include Overflow/Underflow and incorrect calculations. The models tend to focus heavily on detecting Overflow/Underflow while neglecting calculation-related issues. As a result, if any vulnerability is related to incorrect calculations, the model would possibly fail to detect. On the contrary, Chen et al. [17] detected Arithmetic Issues with ChatGPT on smart contract dataset smartbugs-curated [43], most of which were written in Solidity v0.4. Their results indicate that ChatGPT can easily detect these kinds of issues. The main factor contributing to this is that the Solidity v0.4 compiler doesn't automatically check for Overflow/Underflow. Hence, if there is no library like Safemath to prevent Overflow/Underflow, LLMs would tend to report an Arithmetic Issue vulnerability in the smart contract.\n2) Re-entrancy: By evaluating the model's reports for Re-entrancy vulnerabilities, we identified three reasons that may lead to a low recall rate. The main factor is reliance on protective libraries, followed by incorrect checks for the checks-"}, {"title": "effects-interactions pattern and the complex designs of Re-entrancy attacks.", "content": "No. This smart contract is using OpenZeppelin's TimelockController, a well-audited implementation that inherently protects against re-entrancy vulnerabilities.\nDuring the evaluation for Re-entrancy, models will generally check whether the contract uses protective libraries first, such as ReentrancyGuard a library developed by Openzeppelin to prevent Re-entrancy [44] [45]. When this kind of library or framework is detected, models tend to mark the contracts as \"no Re-entrancy\". The smart contracts written in Solidity v0.8, including those we used in previous experiment, have all used protection mechanism.\nHowever, it can't guarantee that the smart contract won't be vulnerable to a Re-entrancy attack. In other words, there is still a high possibility of a Re-entrancy attack if the smart contract's protection mechanism is weak, let alone some tricky attack that can bypass the protection mechanism.\nTo validate our hypothesis, we conducted an additional experiment to comprehensively evaluate the performance of LLMs in detecting \"Arithmetic Issue\" and \"Re-entrancy\" vulnerabilities in smart contracts written in Solidity v0.4.\nIn the experiment, after reviewing the data, we decided not to use the dataset smartbugs-curate [43] discussed earlier. The main reason is that the smart contracts are all very short, averaging 23 lines of code. This would significantly reduce the LLM's error rate and cannot reflect the true situation.\nTherefore, we used the dataset Messi-Q Smart-Contract-Dataset [33]. Published in 2023, the programs are long enough and cover both types of vulnerabilities. This allows us to conduct a thorough and meticulous comparative experiment."}, {"title": "Answer to RQ3.", "content": "LLMs face new challenges in detecting vulnerabilities in smart contracts written in Solidity v0.8, particularly those related to Arithmetic Issues and Re-entrnacy. At the same time, high detection capability may lead to a higher false positive rate.\nV. RELATED WORK\nA. LLMs in Program Analysis\nCommercial LLMs, including OpenAI's GPT-40, Google's Gemini, Anthropic's Claude, have gained rapid development in recent years, and enthusiastically promoted as tools to help programmers in coding tasks like detecting bugs and code generation [46]. LLMs could be particularly useful to help developers with their cybersecurity needs, as humans typically produce and miss many security relevant bugs. This issue was highlighted in the 2022 GitLab Survey, noting that \u201cdevelopers do not find enough bugs early enough\" and \"do not prioritize the bug remediation\" when developing [47].\nNumerous works have contributed to evaluating the performance of LLM in coding related challenges: vulnerable code generation [15], code repair [13], detecting vulnerabilities in the coding [14], and a detailed evaluating framework [47].\nB. Evaluating Smart Contract Security\nAuditing smart contract is a time-consuming and challenging assignment. Aside from manual assessment by a human smart contract auditor, there are numerous tools and techniques have been designed to perform security analyses of smart contract. Based on their attributes, tools can be classified as static analysis, symbolic execution, buzzing, and machine learning [27], while JF. Ferreira et al. proposed a large framework, including more than 19 supported smart contract scanners in 2020 [49] and 2023 [50] respectively, and C. Sendner et al. conducted a large scale research on these tools [27]. Apart from it, after many efforts to automatically discover vulnerabilities of computer programs with LLM, it has been discussed that LLM may have the potential to reveal vulnerabilities of smart contracts [16].\n1) LLMs: David et al. [16] first introduced LLMs in evaluating smart contracts, and conducted experiments on a limited dataset with LLM models GPT-4-32k and Claude-v1.3-100k. In the same year, C. Chen et al. [17] analyzed the performance of GPT-4 series models in detecting vulnerabilities of smart contracts, including GPT-3.5, GPT-4, and GPT-40. They found that ChatGPT has different detection performance for different vulnerabilities, with a relatively high recall rate but low preci-"}]}