{"title": "Anytime Cooperative Implicit Hitting Set Solving", "authors": ["Emma Rollon", "Javier Larrosa", "Aleksandra Petrova"], "abstract": "The Implicit Hitting Set (HS) approach has shown to be very effective for MaxSAT, Pseudo-boolean optimization and other boolean frameworks. Very recently, it has also shown its potential in the very similar Weighted CSP framework by means of the so-called cost-function merging. The original formulation of the HS approach focuses on obtaining increasingly better lower bounds (HS-lb). However, and as shown for Pseudo-Boolean Optimization, this approach can also be adapted to compute increasingly better upper bounds (HS-ub). In this paper we consider both HS approaches and show how they can be easily combined in a multithread architecture where cores discovered by either component are available by the other which, interestingly, generates synergy between them. We show that the resulting algorithm (HS-lub) is consistently superior to either HS-lb and HS-ub in isolation. Most importantly, HS-lub has an effective anytime behaviour with which the optimality gap is reduced during the execution. We tested our approach on the Weighted CSP framework and show on three different benchmarks that our very simple implementation sometimes outperforms the parallel hybrid best-first search implementation of the far more developed state-of-the-art Toulbar2.", "sections": [{"title": "Introduction", "content": "Discrete Optimization problems are ubiquitous in life and solving them efficiently has attracted the interest of researchers for decades. When they are NP-complete, their optimization requires exponential time and sometimes is out of current technology. Then, anytime algorithms become crucial because they provide better and better solutions, the longer they keep running. An especially useful type of anytime algorithms are those that provide improving lower and upper bound of the optimum. They are very valuable because the optimality gap is an indication of the solution quality.\nThere exist several mathematical frameworks to model and solve discrete optimization problems. In this paper, we are concerned with Cost Function Networks, which represent an additive objective function over many discrete variables. Cost Networks belong to a family of frameworks called Graphical Models"}, {"title": "Related Work", "content": "There are various algorithms that provide both lower and upper bounds. In the context of Max-SAT, so-called core-based algorithms solve instances by sequentially making calls to a SAT solver. From the sequence of calls to satisfiable formulas one can in general produce improving upper bounds. From the sequence of calls to unsatisfiable formulas one can extract cores which are aggregated by means of pseudo-boolean constraints producing improving lower bounds [2, 21]. In the general context of Graphical Models, we can distinguish between inference and search approaches. In the first case, we find the well-known mini-buckets-elimination (MBE) algorithm proposed in [11]. MBE has a parameter that trades time for accuracy. By iteratively increasing this parameter, the algorithm obtains a decreasing optimality gap. In the context of branch and bound systematic search some algorithms traverse the search space with hybrid strategies that combine best-first and depth-first. The best-first component provides a natural lower bound as the minimum cost among the heuristic value of all the open nodes. The depth-first component reaches near-optimal solutions which provide a natural upper bound. This idea has been long applied in Integer Programming solvers [22] and, more recently, in Weighted CSP [1].\nSeveral parallel schemes have been proposed. An active area of research is the Distributed Constraint Reasoning [29], which deals with complex synchronization strategies and a quite restricted environment. In the context of constraint solving (see [14] for a recent review), the concept of parallel tree search, where the search space is partitioned in some way into independent sub-problems and each one is then solved in parallel, has been applied in [19, 23, 1]. In the context of Max-SAT, several solvers are also based in the parallel tree search and also on parallelized portfolio solvers [18]."}, {"title": "Preliminaries", "content": ""}, {"title": "CSPs and WCSPS", "content": "A Constraint Satisfaction Problem (CSP) is a tuple (X, C) where X is a set of variables taking values in a finite domain, and C is a set of constraints. Each constraint depends on a subset of variables called scope. Constraints are boolean functions that forbid some of the possible assignments of the scope variables. A solution is an assignment to every variable that satisfies all the constraints. Solving CSPs is an NP-complete problem [15].\nA Weighted CSP (WCSP) is tuple (X, C, F) where (X, C) is a CSP and F is a set of cost functions. A cost function $f \\in F$ is a mapping that associates a cost to each possible assignment of the variables in its scope. The cost of a solution is the sum of costs given by the different cost functions. The WCSP problem, which is known to be NP-hard [20], consists in computing a solution of minimum cost."}, {"title": "Vectors and Dominance", "content": "Given two vectors u and v, the usual partial order among them, noted $\\tilde{u} \\le \\tilde{v}$, holds iff for each component i we have that $u_i < v_i$. If $\\tilde{u} \\le \\tilde{v}$ we say that $\\tilde{v}$ dominates $\\tilde{u}$. Given a set of vectors V, we say that $\\tilde{u}$ hits V if none of the vector in V dominates $\\tilde{u}$.\nThe minimum cost hitting vector (MHV) of V is a vector that hits V with minimum cost. It is not difficult to see that MHV reduces to the classic hitting set problem [13] which, in its optimization version, is known to be NP-hard."}, {"title": "Cores and Solutions", "content": "In the following, we consider an arbitrary WCSP (X, C, F) with m cost functions $F = \\{f_1, f_2,..., f_m\\}$. We name $w*$ the cost of the optimal solution.\nA cost vector $\\tilde{v} = (v_1, v_2, ..., v_m)$ is a vector where each component $v_i$ is associated to cost function $f_i$, and value $v_i$ must be a cost occurring in $f_i$. The cost of vector v is $cost(v) = \\sum v_i$\nVector v induces a CSP (X, C \u222a $\\tilde{F}_v$) where $\\tilde{F}_v$ denotes the set of constraints $f_i \\le v_i$, for $1 \\le i \\le m$ (namely, cost functions are replaced by constraints). If the CSP induced by $\\tilde{v}$ is satisfiable we say that $\\tilde{v}$ is a solution vector (or simply a solution). Otherwise, we say that $\\tilde{v}$ is a core. We denote the set of cores as Cores. A core is maximal if there is no other core in Cores that dominates it. An optimal solution is a solution vector of minimum cost. It is easy to see that the cost of an optimal solution is $w*$"}, {"title": "Two HS-based Schemes", "content": "The HS approach relies on the following,\nTheorem 1. Consider a solution $\\tilde{h}$ and a set of cores K. Then, MHV(K) $\\le w* < cost(\\tilde{h})$.\nIn words, the optimal cost w* is lower and upper bounded in terms of core and solution vectors.\nIn the following, we present two approaches that consider the previous Theorem from two different perspectives."}, {"title": "HS-lb", "content": "The HS approach to WCSP was first proposed in [12], as a generalization of its application to MaxSAT [10].\nAlgorithm 1 (HS-lb) is a simple version of this idea. K is a set of cores, and lb and ub are the lower and upper bound of the optimum, respectively. At each itermation, the algorithm computes a minimum cost hitting vector $\\tilde{h}$. By Theorem 1, we know that the cost of $\\tilde{h}$ is a lower bound of the problem, so the algorithm updates lb. Then, it solves the CSP induced by $\\tilde{h}$. If it is satisfiable (i.e., $\\tilde{h}$ is a solution), then we know that lb = ub so the algorithm can stop. Otherwise, vector $\\tilde{h}$ is a core, and the algorithm computes a new maximal core $\\tilde{k}$ such that $\\tilde{h} \\le \\tilde{k}$ and adds it to K."}, {"title": "HS-ub", "content": "Each iteratin of HS-lb may be very time consuming because it needs to compute minimum cost hitting vector $\\tilde{h}$ which is an NP-hard problem. One way to decrease the work-load of each iteration is to rely on non-optimal hitting vectors. As suggested in [26], we can replace optimal hitting vectors by hitting vectors of bounded cost.\nAlgorithm 2 (HS-ub) implements this idea. As before, K is a set of cores, and lb and ub are the lower and upper bound of the optimum. At each iteration, the algorithm computes a hitting vector $\\tilde{h}$ with cost less than ub. If such $\\tilde{h}$ does not exists (i.e., it is NUL), it means that, by Theorem 1, the current ub is the optimum so the algorithm can stop. If $\\tilde{h}$ exists, then the CSP induced by $\\tilde{h}$ is solved. If it is satisfiable (i.e., $\\tilde{h}$ is a solution) the upper bound is updated. Otherwise, vector $\\tilde{h}$ is a core, and the algorithm computes a new maximal core $\\tilde{k}$ which is added to K."}, {"title": "Computing Maximal Cores", "content": "It is important to note that both algorithms are correct even if $\\tilde{h}$ is directly added to K (i.e., skipping the MaximalCore(\u00b7) call). However, as suggested in [10] and supported by our own experiments, adding a larger core is fundamental for both HS-lb and HS-ub. The rationale is that adding maximal cores to K results in larger (i.e., with higher cost) hitting vectors. As a consequence, when K is hit optimally, the lower bound grows faster, while when K is hit non-optimally, the area under the ub is further reduced.\nThe natural implementation of the MaximalCore(\u00b7) function is to obtain $\\tilde{k}$ as a sequence of increments to its components while preserving the core property until no more increments can be done. After each increment the algorithm needs to check if the new induced CSP remains unsatisfiable (i.e., a call to SolveCSP(.)). Interestingly, during this process solution vectors are found,"}, {"title": "Discussion", "content": "The main advantage of HS-ub compared to HS-lb is that iterations are likely to be faster. There are several reasons for it. First, it is much more efficient to find a bounded hitting vector (which is a decision problem) than finding an optimal hitting vector (which is an optimization problem). Second, only in the last call of CostBoundedHS(\u00b7) the problem will be unsatisfiable (which is typically a much more costly task to solve). Finally, the returned cost-bounded hitting vectors are likely to have a higher cost (near ub) because they are easier to find. Therefore, obtaining a maximal core $\\tilde{k}$ will not need so many SolveCSP(\u00b7) calls.\nThe advantage is at the cost of potentially more iterations. On the one hand, in HS-ub not all iterations end up adding a new core because some iterations, when the induced CSP is satisfiable, only decrease the upper bound. On the other hand, the cost bounded hitting vectors may not contribute to increase the domination of the area of interest either."}, {"title": "Any-time HS scheme", "content": "We discussed in the previous section that HS-lb and HS-ub are complementary implementations aiming at the same goal: finding a set of cores that dominates every core with cost less than the optimum. HS-lb is likely to be slower but its cores are likely to be of better quality. HS-ub is likely to be faster but it is more likely to need more cores. Most importantly, none of them is guaranteed to outperform the other."}, {"title": "Experimental Results", "content": ""}, {"title": "Evaluation setting", "content": "We implemented all our algorithms in C++. The SolveCSP function encodes the induced CSP $\\tilde{F}_h$ as a CNF formula and uses the assumption-based SAT solver"}, {"title": "Conclusion and Future Work", "content": "We have presented the first parallel anytime algorithm for discrete optimization based on the implicit hitting set approach. Our multithread implementation is simpler than other multithread alternatives such as hbfs. Its work-load is well"}]}