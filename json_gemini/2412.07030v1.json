{"title": "FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering", "authors": ["Amirhossein Abaskohi", "Spandana Gella", "Giuseppe Carenini", "Issam H. Laradji"], "abstract": "Multimodal multihop question answering is a complex task that requires reasoning over multiple sources of information, such as images and text, to answer questions. While there has been significant progress in visual question answering, the multihop setting remains unexplored due to the lack of high-quality datasets. Current methods focus on single-hop question answering or a single modality, which makes them unsuitable for real-world scenarios such as analyzing multimodal educational materials, summarizing lengthy academic articles, or interpreting scientific studies that combine charts, images, and text. To address this gap, we propose a novel methodology, introducing the first framework for creating a high-quality dataset that enables training models for multimodal multihop question answering. Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure quality data. We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks, our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) on average. We believe our data synthesis method will serve as a strong foundation for training and evaluating multimodal multihop question answering models\u00b9.", "sections": [{"title": "1. Introduction", "content": "Few-shot learning for multimodal multihop question answering (MMQA) aims to address complex queries by reasoning across multiple modalities, including text and images. This approach is crucial in scenarios where questions require reasoning and integrating information across long texts, images, tables, and other modalities. Recent advancements in this"}, {"title": "2. Related Work", "content": "Within the Question Answering (QA) literature, synthesis of training data has been predominantly focused on unimodal (text-only) scenarios. We review various seminal works that have established the foundation for our work in few-shot data synthesis."}, {"title": "Unimodal Data Synthesis", "content": "Synthetic data has become a widely used resource for model training. He et al. [16] propose a general framework that utilizes synthetic text generated by language models (LMs) for knowledge distillation, self-training, and few-shot learning, demonstrating that combining labeled and pseudo-labeled synthetic data can lead to notable performance gains in NLP tasks. In other cases, rather than synthetic text alone, entire datasets are generated for specific tasks such as classification [37]. For instance, Li et al. [26] evaluate the effectiveness of LLMs, specifically GPT-3.5, in producing synthetic datasets for text classification, offering insights into the consistency and reliability of LLM-generated data. In another work, Chen et al. [8] present a framework that significantly improves the performance of smaller language models on multi-hop QA tasks, even with minimal human-annotated data."}, {"title": "Multimodal Data Synthesis", "content": "Research on multimodal data synthesis using LVLMs is still underexplored, most work focuses on creating entirely new data based on knowledge encoded in these models. For instance, Zhang et al. [43] design a multimodal benchmark utilizing large vision and language models and their code capabilities to synthesize massive abstract images and visual reasoning instructions across daily scenarios. In another study, Mehta et al. [32]"}, {"title": "3. Proposed Method: FM2DS", "content": "Our five-stage pipeline for FM2DS shown in Figure 2 is designed to synthesize high-quality multimodal question-answer pairs. The process begins with retrieving relevant documents through topic matching and Wikipedia hyperlinks. The subsequent stages involve few-shot sample selection, synthesizing and validating multihop questions, generating corresponding answers, and constructing relevant document queries, with each stage incorporating specific validation mechanisms. We elaborate on individual components in the following sections. Refer to Appendix E for examples of generated data by FM2DS."}, {"title": "3.1. Stage 1: Creating a Pool of Related Documents", "content": "We collect a pool of relevant documents using Wikipedia pages leveraging Wikipedia's vast repository of interconnected information. For this, we used the WikiWeb2M dataset [4], which contains nearly 2 million Wikipedia pages, providing a comprehensive source of information. These documents are linked through two complementary methods: existing hyperlinks and topics extracted using multimodal topic modeling with Multimodal-Contrast model [15]. The hyperlink structure provides explicit, human-curated connections between related concepts, while the multimodal"}, {"title": "3.2. Stage 2: Creating Few-Shot Samples", "content": "To generate questions, we leverage existing multimodal multihop question-answering dataset MultiModalQA [35] that require reasoning across various modalities, including text, images, and tables. For our purposes, we utilized a custom subset of the MultiModalQA train set. Using the links to Wiikpedia pages provided in the dataset, we create a pool for few-shot samples that include the full Wikipedia pages -complete with images and tables in HTML format along with the question. In our experiments, we randomly select up to three of these samples for question generation."}, {"title": "3.3. Stage 3: Question Generation and Validation", "content": "Question Generation We use GPT-4-turbo [33] to generate questions based on few-shot samples created from the MultiModalQA dataset. Due to the model's context length limitations, we restricted the input to two or three related documents. Our prompt, detailed in Appendix A, query the model to produce multimodal, multihop questions that required input from at least two modalities and drew on information from all provided documents. For instance, if two documents were given, the question needed both to be answerable; if three documents are provided, it can utilize any two or all three of them. We explicitly instructed the model that simply concatenating two independent questions with an \"and\" to reference multiple documents would not qualify as a valid multihop question. For instance How did Albert Einstein contribute to the Theory of Relativity and when Princeton established? is not a true multihop question,"}, {"title": "Question Validation", "content": "Our framework incorporates multiple validation layers to verify that synthesized questions satisfy the requisite criteria for both multihop reasoning complexity and multimodal integration. Although the model was instructed during question generation to avoid creating questions that simply concatenate independent questions answerable by a single document, we evaluated this further. For this, we employed LLama-3.1-8B [2] to break each question down into simpler components. We then assessed whether each component could be answered using only one document. If all parts of the question were solvable with a single document, we discarded such question that include unrelated facts. Here is an example of question with unrelated facts:"}, {"title": "Example (Unrelated Facts):", "content": "In what year did Mike Tyson become the youngest heavyweight champion, and who is the president of the United States?"}, {"title": "Example (Related Facts, Open-ended):", "content": "In what year did Mike Tyson become the youngest heavyweight champion, and who was the president of the United States at that time?"}, {"title": "Example(Concise Multihop Query):", "content": "Who was the president of the United States when Mike Tyson became the youngest heavyweight champion?"}, {"title": "3.4. Stage 4: Answer Generation and Validation", "content": "Answer Generation For answer generation, we used GPT-4o to produce concise answers based on multiple documents, which included both textual and visual information such as images. The model was explicitly instructed to answer the question in the shortest way possible, providing only the key information without offering any additional explanation."}, {"title": "Answer Validation", "content": "We validated the generated answers using information extraction techniques, specifically named entity recognition (NER) and relation extraction, as they have been previously used for validating answers [14, 34]. NER identified key named entities (e.g., persons, organizations, locations) and numerical data in both the answer and the documents, which were compared to ensure alignment with the source content. Relation extraction was used to verify whether the relationships between entities in the generated answer matched those in the documents. By ensuring the correctness of both named entities and their relations, we validated that the answers were accurate, concise, and consistent with the multimodal data provided 2. For including the images in the process, we used captions generated by GPT-40. Specifically, based on what the question requires from the image (e.g., the color of a building), the caption is generated to include that information. Additionally, to mitigate hallucinations, we prompted GPT-40 five times, and if the model generated the same correct response across all five prompts, we accepted it. To evaluate the effectiveness of our answer validation step, we designed a human evaluation where the result can be found in Section 6."}, {"title": "3.5. Stage 5: Query Generation and Validation", "content": "Query Generation We create queries based on the question-answer pairs and the related documents to facilitate a more effective retrieval process. These queries are meticulously designed to guide the search for relevant information, allowing us to pinpoint specific data points that are crucial for formulating accurate answers. This strategic approach helped us narrow down the information within the documents, enabling us to identify and extract key details such as entities, relationships and contextual information that align with the questions. This targeted querying facilitated an effective information extraction approach, that the generated answers are comprehensive and directed supported by the evidence found in the documents."}, {"title": "4. Proposed Benchmark: M\u00b2QA-Bench", "content": "We also propose a benchmark, M2QA-Bench, to assess the LVLMs performance on a more complicated MMQA task with full documents. M2QA-Bench consists of 500 Q&A pairs, each designed to challenge the model's ability to perform a complex reasoning task. The questions are not templated into a specific structure (as in some existing works [35]), instead, they are diverse and challenging. Additionally, answering the questions require access to full documents, where both information extraction and reasoning across different modalities (e.g., images and tables) are essential. Table 1 presents the key statistics of M2QA-Bench. Refer to Appendix E for benchmark samples created using FM2DS."}, {"title": "5. Experiments and Results", "content": "This section evaluates our synthesized dataset against human-annotated datasets. In all experiments, the model was provided one in-context example during data synthesis, unless"}, {"title": "5.1. Comparison with Human-Annotated Datasets", "content": "Unlike previous dataset creation methods like MultiModalQA [35] and WebQA [5], our approach is fully automated, with no human intervention in data generation. This section provides a comparative analysis of the data quality synthesized by our method against these prior human-annotated datasets. We trained three models\u2014LLaVA-1.6 [27\u201329], InternVL-2 [11, 12], and Idefics-2 [23, 25]\u2014with varying sizes of the WebQA and MultiModalQA datasets and evaluated them on their respective test sets. Table 2 shows that samples generated by FM2DS help us train models that can handle MMQA better, even though its training samples include long documents. In contrast, training samples from MultiModalQA and WebQA consist of short, focused information snippets. We also notice that WebQA might be an easier task than MultiModalQA, as models tend to perform better on WebQA with fewer samples, even though it has a larger training set. Across all models and experiments with different numbers of training samples (where models trained on synthetic data were exposed to an equal or fewer number of real samples), the average improvement observed in EM was 1.81 in MultiModalQA and 1.96 in WebQA. While improvements in EM generally lead to increases in F1 score, there are cases where F1 decreases even as EM improves. This discrepancy may result from hallucinations in incorrect predictions, which reduce string overlap and, in turn, lower the F1 score.\nMoreover, models achieve the same performance with less synthetic samples generated by FM2DS as those trained on the full training dataset, demonstrating faster convergence with fewer samples. We explore this in details in Section 5.2. Also note that larger models achieves better performance on the same number of synthetic samples, see the difference between Llava-1.6-13B and Llava-1.6-7B. GPT-4o achieves better results among large VLMs, likely because it was also used for data generation. For qualitative analysis, refer to Appendix F."}, {"title": "5.2. Learning Efficiency Comparison", "content": "To assess the comparative learning efficiency between synthesized and human-annotated datasets, we conducted systematic experiments using InternVL-2-8B across incremental training sample sizes ranging from 1k to 10k instances. For synthesized data, we used the same training data for both MultiModalQA and WebQA. For the human-collected datasets, we trained the model on each corresponding training set."}, {"title": "5.3. Cross-Dataset Evaluation", "content": "To evaluate the generalizability and robustness of our synthesized data, we conducted a cross-dataset evaluation using the InternVL-2-8B model. For this experiment, we used our M2QA-Bench with 500 samples and matched it with 500 randomly selected samples from the MultiModalQA test set, including full Wikipedia pages for both. With evaluation sets now having a similar structure, we trained the model separately on 5k samples from our synthesized dataset and 5k training samples from the MultiModalQA dataset, both with full Wikipedia pages as context. As shown in Table 3, the model trained on our synthesized data outperformed the model trained on MultiModalQA samples across both test sets. This result highlights the broader generalizability of our dataset. Additionally, our benchmark proves to be more complex and diverse than MultiModalQA which has predominantly template-based questions."}, {"title": "7. Ablation Studies", "content": ""}, {"title": "7.1. Effect of Key Steps on Model Performance", "content": "To analyze the impact of various data filtering and validation steps on model performance, we evaluated the model across different configurations. Table 4 demonstrate the importance of each data refinement step on model performance. Question validation improved relevance by filtering questions tailored to the complex, multimodal, multihop requirements of the test sets, reducing hallucinations and boosting F1 by keeping answers concise and accurate. Answer validation and query generation proved crucial as answer validation filtered out incorrect samples, and queries supported learning by distilling knowledge from larger models, leading to improved EM and F1 scores. Finally, query validation reinforced training consistency by ensuring that queries followed the correct structure, minimizing confusion and helping the model accurately interpret questions. Together, these steps yielded the highest scores, underlining the cumulative impact of careful dataset curation."}, {"title": "7.2. The Effect of the Number of In-Context Documents", "content": "Table 5 presents the results of evaluating the Intervl-2 8B model with varying numbers of in-context documents on the MultiModalQA and WebQA datasets. In the zero-shot setting, FM2DS exhibits limited understanding of multimodal multi-hop question answering, and occasionally circumvents the validation step by simply generating a question that is not multihop. For example, \"Looking at the image of the Eiffel Tower, what engineering innovation allowed it to surpass previous structures in height?\" prompts the model to use the image, but the answer is available in the page's text on tall structures. As we move from zero-shot to one-shot, there is a significant boost in EM and F1 scores, reflecting improved performance with minimal context. The improvement from one-shot to two-shot is marginal, suggesting diminishing returns. With three in-context samples, the gains become"}, {"title": "7.3. Comparing Methods For Data Synthesis", "content": "To evaluate the effectiveness of different methods for synthetic data generation, we compared three prominent language models: GPT-4o, Claude 3.5 Sonnet, and Llama-3.2-90B, as shown in Table 6. Using Intervl-2-8B with 5K fine-tuning samples as our baseline model, we tested the quality of generated data on two distinct datasets: MultiModalQA and WebQA. The results, measured using EM and F1 scores, demonstrate that GPT-4o consistently outperforms other models across both datasets. We also see that Llama-3.2-90B shows competitive performance as an open-source model with less number of parameters, particularly in WebQA tasks. Claude 3.5 Sonnet generally yields lower scores across both datasets."}, {"title": "8. Conclusion & Future Works", "content": "We have presented a novel methodology for synthesizing high-quality data for multimodal multihop question answering, called FM2DS. Our approach addresses the limitations of existing methods, which focus on single-hop question answering with a single modality, and provides a robust framework for generating complex questions and answers that require reasoning over multiple sources of information with minimal human effort. We have demonstrated the effectiveness of our approach in creating a high-quality dataset that enables training models for multimodal multihop question answering. Our experiments have shown that the synthesized"}, {"title": "A. Prompts", "content": "In our data generation pipeline, FM2DS, which incorporates LVLMs, we carefully designed prompts to guide the model through tasks involving cross-modal reasoning and data synthesis. Each prompt was carefully designed with specific elements to ensure precision, clarity, and completeness in achieving the task's objectives, while also minimizing the need for error correction during the evaluation process. In the following sections, we outline the rationale behind the structure and components of these prompts."}, {"title": "Question Generation Prompt", "content": "Generate a multi-hop question\nbased on the provided information.\nA multi-hop question requires\nthe model to utilize information\nfrom all available documents\nin combination to reach the\ncorrect answer. Specifically,\nthe question should be designed to\nbe unanswerable if any one of the\ndocuments is missing. Furthermore,\nfocus on creating questions that\ncompel the model to extract and\nsynthesize relevant information\nacross multiple modalities|such as\nimages and text. This means that\nanswering the question correctly\nwill demand integrating insights\nfrom each source and modality,\nmaking it impossible to arrive at\nan accurate answer using any single\ndocument or modality alone.\nHere are the documents:\n[Documents]\nHere are examples:\n[Example(s)]"}, {"title": "A.1. Question Generation", "content": "Prompt 1 show the prompt used for question generation. Using this prompt, we ask the model to create multi-hop"}, {"title": "Answer Generation Prompt", "content": "You are provided with multiple\ndocuments, including both textual\ncontent and images, along with\na question. Your task is to\ncarefully review each document,\nanalyze the images, and derive an\nanswer based on the information\ncontained across all sources. Aim\nto combine insights from both\ndocuments and across modalities\nto deliver the most accurate and\ncomprehensive response possible.\nQuestion:\n[Question]\nHere are the documents:\n[Documents]\nHere are examples:\n[Example(s)]"}, {"title": "A.2. Answer Generation", "content": "The prompt for answer generation directs the model to analyze multiple documents, encompassing both text and images, to address the given question. It emphasizes integrating and synthesizing information from all sources to deliver the most accurate and comprehensive response. The prompt ensures that the model considers all modalities and documents without relying solely on a single source or the model's pre-trained knowledge, focusing exclusively on the provided materials. Refer to Prompt 2 for the answer-generation prompt."}, {"title": "A.3. Query Generation", "content": "As illustrated in Prompt 3, in query generation, the model is tasked with explaining the step-by-step process used to extract relevant information from the documents and determine the answer based on the extracted snippets. This task emphasizes transparency by requiring the model to identify the relevant sections of each document and describe how information from multiple sources is combined to arrive at the correct answer, promoting accountability in the model's reasoning process."}, {"title": "Query Generation Prompt", "content": "You are provided with multiple\ndocuments, a question, and the\nanswer. Your task is to explain\nthe step-by-step process you would\nuse to extract and verify the\nanswer using information from the\ndocuments and various modalities.\nClearly identify each document\ntitle and relevant sections, and\ndescribe how you locate, interpret,\nand integrate information across\nboth documents to derive the\ncorrect answer.\nQuestion:\n[Question]\nAnswer:\n[Answer]\nHere are the documents:\n[Documents]"}, {"title": "B. Experimental Settings", "content": "In this work, we conducted experiments on a cluster of 8 NVIDIA H100 80GB GPUs. The distributed setup allowed us to efficiently scale our fine-tuning process across multiple devices. The fine-tuning process was carried out using low-rank adaptation (LoRA) [19], a technique for efficient adaptation of pretrained models with low-rank matrices, reducing the number of trainable parameters. The key hyperparameters used in the fine-tuning procedure include a learning rate of le-4, a batch size of 8 per device (totaling 64 across 8 devices), LoRA rank set to 8, LoRA alpha set to 32, a weight decay of 0.01, and the number of epochs was 5. Additionally, AdamW optimizer was used with \u03b2\u2081 = 0.9,\n\u03b22 = 0.98, and \u03f5 = 1e-8. The models were fine-tuned using mixed-precision training to take full advantage of the 80GB memory on each H100 GPU. For inference time, we set the temperature to 0.7, which strikes a balance between randomness and coherence in the model's responses, producing more varied outputs without sacrificing too much quality. This setup ensured efficient usage of computational resources while maintaining high model performance."}, {"title": "C. Investigating FM\u00b2DS on Other Models", "content": "In addition to the models discussed in Section 5, we explored other model families, including Idefics3 [24], mPLUG-DocOwl-1.5 [18], and Phi-3.5-Vision-Instruct [1], as well as larger versions within the explored families presented in Table 2. The results in Table 7 demonstrate the reliability of our data synthesis approach, which consistently enhances model performance across all models and sizes compared to an equivalent number of real samples.\nAs Table 7 shows, within the same model architecture, as the number of parameters increases and the model complexity grows (e.g., InternVL-2), the performance generally improves, including the pre-trained version. These models also exhibit more effective learning, especially when provided with synthesized data generated by FM2DS, which makes the learning process more efficient. Moreover, Idefics-3 shows notable improvement over its predecessor, Idefics-2, indicating that the newer version has a better visual reasoning. When comparing mPLUG-DocOwl-1.5 with models like InternVL-2, Idefics-2, and Idefics-3, it demonstrates relatively lower performance. This could be attributed to the training objective of mPLUG-DocOwl-1.5, which focuses on multi-grained text recognition and parsing, potentially resulting in weaker performance when visual reasoning is required. Nevertheless, this model still outperforms LLaVA-1.6-7B overall, which might be due to the simpler structure of the LLaVA-1.6 family. Finally, Phi-3.5-Vision-Instruct, despite having fewer parameters compared to other models, performs competitively with other models and surpasses LLaVA-1.6-7B in performance."}, {"title": "D. Human Evaluation Details", "content": "To facilitate a rigorous human evaluation of our answer validation component, we created a Google Form to recruit participants willing to contribute to our evaluation. We shared this form widely and will acknowledge the contributions of participating individuals in the acknowledgment section of the paper's camera-ready version.\nUpon registration, participants were provided access to a custom evaluation application, as shown in Figure 6. This application was designed to streamline the evaluation process and ensure consistency across participants. For each session, the application randomly selected one of the 100 questions from our human evaluation set of samples. For each question, participants could review the question text, the associated Wikipedia pages, and the generated answers from two methods\u2014one method utilizing the answer validation component and the other without it. To minimize user bias, the application randomly alternated the positioning of the methods' answers (labeling them as \u201cAnswer A\" and \"Answer B\") so that users could not develop a tendency to select one model over the other based on position alone. After examining the question and relevant Wikipedia content, users were asked to select one of four options to indicate their assessment of answer accuracy: (1) Answer A is correct, (2) Answer B is correct, (3) both answers are correct, or (4) neither answer is correct.\nIn addition to these selections, participants had the option to provide a brief rationale for their choices. Although they have not been investigated for this research, this optional feedbacks were encouraged, as it offers valuable insights for qualitative analysis and potential future improvements in answer validation accuracy. The combination of structured and open-ended responses enhances the robustness of our evaluation and offers a more comprehensive view of user judgments, which we may explore in future iterations of our data synthesis methodology."}, {"title": "E. M\u00b2QA-Bench Samples", "content": "FM2DS uses LVLMs to generate multimodal and multihop questions based on the given documents and evaluate their answers. These samples aim to emulate few-shot examples typically provided to guide the model's behavior in a structured and relevant manner.\nIn some cases, the questions focus on understanding facts from different modalities\u2014such as images, text, and tables\u2014within the grouped documents and finding the answer from one of them. For example, in the case of the question shown in Figure 7:"}, {"title": "F. Qualitative Analysis", "content": "In the qualitative analysis, we compared three critical factors influencing model responses: model architecture, fine-tuning (FT) dataset (real samples or synthesized samples), and model size. To examine the effects of model architecture and FT dataset, we used InternVL-2-8B, LLaVA-1.6-7B, and Idefics-2-8B, fine-tuning them on both real and synthetic data generated by FM2DS. For analyzing the impact of model size, all versions of InternVL-2 were trained on the synthetic data. All of the mentioned models were fine-tuned on 5k samples.\nThis analysis was conducted for 100 samples from each of the following benchmarks: (1) M\u00b2QA-Bench, (2) MultiModalQA, and (3) WebQA. The results are presented in Tables 8, 9, and 10. The responses generated by different models were analyzed across these datasets, focusing on the following metrics:\nModel accuracy using the exact match (EM) metric.\nHallucination rate, corresponding to instances where the model generated wrong answer based on its pre-trained knowledge instead of the provided document.\nModel accuracy with EM metric for samples including image modality (may include other modalities).\n4. Model accuracy with EM metric for samples including table modality (may include other modalities).\n5. Model accuracy with EM metric for samples including both image and table modalities.\nFor WebQA, which only incorporates text and image modalities, the last three metrics were not applicable. Additionally, the distribution of modalities across samples for MultiModalQA and M2QA-Bench was as follows:\nM2QA-Bench: 66 samples included image modality, 62 samples included table modality, and 28 samples included both image and table modalities.\nMultiModalQA: 61 samples included image modality, 54 samples included table modality, and 15 samples included both image and table modalities.\nOverall, in all benchmarks, model hallucination rates de-"}]}