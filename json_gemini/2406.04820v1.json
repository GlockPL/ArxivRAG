{"title": "NAVIGATING EFFICIENCY OF MOBILEVIT THROUGH GAUSSIAN PROCESS ON GLOBAL ARCHITECTURE FACTORS", "authors": ["Ke Meng", "Kai Chen"], "abstract": "Numerous techniques have been meticulously designed to achieve optimal architectures for convolutional neural networks (CNNs), yet a comparable focus on vision transformers (ViTs) has been somewhat lacking. Despite the remarkable success of ViTs in various vision tasks, their heavyweight nature presents challenges of computational costs. In this paper, we leverage the Gaussian process to systematically explore the nonlinear and uncertain relationship between performance and global architecture factors of MobileViT, such as resolution, width, and depth including the depth of inverted residual blocks and the depth of ViT blocks, and joint factors including resolution-depth and resolution-width. We present design principles twisting magic 4D cube of the global architecture factors that minimize model sizes and computational costs with higher model accuracy. We introduce a formula for downsizing architectures by iteratively deriving smaller MobileViT V2, all while adhering to a specified constraint of multiply-accumulate operations (MACs). Experiment results show that our formula significantly outperforms CNNs and mobile ViTs across diversified datasets.", "sections": [{"title": "1 Introduction", "content": "Light-weight convolutional neural networks (CNNs) [1, 2, 3, 4] have played a significant role in enabling various computer vision tasks on mobile devices. However, vision transformers (ViTs) are yet to find popular use on such devices. Unlike lightweight CNNs, which are relatively easy to optimize and seamlessly integrate with task-specific networks, ViTs[5, 6, 7, 8] are considerably heavy-weight in terms of computational cost and memory footprint. This makes their deployment and utilization of resource-constrained mobile devices a challenging endeavor.\nDue to the exceptional performance of transformers across a range of tasks [6, 9, 10, 11, 12], with a particular emphasis on the outstanding capabilities of vision transformers, an increasing number of applications and companies are keen to deploy transformers on edge devices. Popular transformers on edge devices include MobileViT[13] and MobileViT V2[14] proposed by Apple and Mobile-former[9] proposed by Microsoft etc. Expanding on MobileViT[13], our goal is to thoroughly explore how architecture factors including resolution, width, the depth of inverted residual blocks, the depth of ViT blocks, and joint factors including resolution-depth and resolution-width affect model's performance. These architecture factors determine the size, computational cost, and memory usage of MobileViT. Especially, to deploy the transformers on mobile devices, we continuously adjusted the architecture factors above to reduce memory cost and latency. When given an upper limit of multiply-accumulate operations (MACs), we propose a valuable formula to recommend the ideal resolution and depth for MobileViT. The formula involves Gaussian process (GP) nonlinear fitting on the architecture factors of cutting-edge models [15, 16]."}, {"title": "2 Transformer for vision task", "content": "The transformer model was originally proposed by Google in 2017[19], aiming to solve problems such as poor model performance and difficulty in training for long sequence inputs. Compared with traditional sequence models such as recurrent neural networks(RNNs) and CNNs, a transformer model has the following characteristics. It does not need to maintain the order of time steps and can parallel process the entire input sequence, thus enabling the training of deeper models in a shorter time. Besides, it uses a self-attention mechanism, which can better capture long-term dependencies in the sequence.\nVision transformer: Besides, ViT is a variant of the transformer model that has been adapted for use in computer vision tasks, such as image classification, object detection, and semantic segmentation. ViT was first introduced by Google researchers in 2020 [20] and has gained popularity in the computer vision community due to its strong performance on a range of vision benchmarks. The basic idea behind ViT is to apply the transformer model to image patches extracted from an input image, rather than to the entire image. The input image is divided into a grid of non-overlapping patches, which are then linearly projected to a lower-dimensional embedding space[5, 8]. The resulting embeddings are fed into the transformer model, which processes them using self-attention and multi-layer perceptron (MLP) modules to produce a final output. One of the key capacities of ViT is that it can process variable-sized inputs[5], unlike many CNN models that require fixed-size inputs. This allows ViT to handle images of different sizes without the need for resizing or cropping. Additionally, ViT has demonstrated strong performance on several image classification benchmarks, often outperforming state-of-the-art CNN models[6].\nMobileViT: There are several excellent ViTs on mobile devices have been proposed. One such example is the MobileViT[13] introduced by Apple. MobileViT utilized the inverted residual block and MobileViT block to reduce the number of parameters. It is more suitable for deployment on mobile devices. MobileViT is specifically designed for vision tasks on mobile devices. It replaces the traditional CNN with a self-attention module for image feature extraction and attention computation. This replacement allows the model to learn global image representations without being limited by fixed-size receptive fields. The first version of MobileViT (denoted by MobileViT V1) is designed for"}, {"title": "Inverted residual block", "content": "The design concept of inverted residuals aims to increase the model's representational capacity and maintain low computational complexity by modifying the traditional residual connection structure. The inverted residual block utilizes lightweight depthwise separable convolutions to adjust the input and output channels for feature map matching. This block is widely used for lightweight deep models on mobile and embedded devices."}, {"title": "Mobile ViT block", "content": "To maintain an overall lightness of ViT and balance the usage of convolutions and transformers, MobileViT block replaces the local processing typically found in convolutions with global processing using transformers. This unique approach enables the MobileViT block to exhibit properties similar to both CNNs and ViTs. Therefore, the MobileViT block can learn more effective representations while utilizing fewer parameters and employing simpler training techniques."}, {"title": "Separable self-attention", "content": "Inspired by MHA, self-attention in traditional transformer models is computed for all token pairs in the sequence. This leads to a computational complexity of $O(l^2)$, where l is the sequence length. Separable self-attention only computes the context score for a latent token L, which significantly reduces the computational cost to O(l)[14].\nThe input x is processed using three branches, i.e., input I, key K, and value V. Specifically, the input x is first linearly projected to a d-dimensional space using a key branch K. The key branch K has weights $W_Q$ and produces an output $x_k$. The context vector $c_r$ is then calculated as a weighted sum of the projected tokens $x_k$. We use context scores $c_s$ to"}, {"title": "3 Related Work", "content": "Advances for efficient ViTs: Advanced methods that improve the performance and reduce the complexity of trans-formers include mixed-precision training[21], efficient optimizers[22], and knowledge distillation[6]. Compression oftransformers typically involves reducing the size of the model while preserving its accuracy. This can be done usingvarious techniques used in CNNs."}, {"title": "Pruning", "content": "Removing unnecessary weights or connections of transformer [23, 24, 25, 26] is known as pruning. By identifying and eliminating the connections that contribute the least to the model's performance, pruning reduces the model's size and speeds up its operations without significantly sacrificing accuracy."}, {"title": "Quantization", "content": "Reducing the precision of weights and activation in a model refers to quantization. Quantization uses fewer bits to represent each weight or activation, such as using 8-bit integer values instead of 32-bit floating-point values. Quantization obtains a smaller model that performs inference faster, although there might be a slight decrease in accuracy."}, {"title": "Distillation", "content": "In the original motivation of distillation, a smaller model can be trained to imitate the behavior of a larger, more complex model. During training, the outputs of the larger model serve as targets for the smaller model. By doing so, the smaller model can learn to replicate the performance of the larger model while being significantly smaller and faster."}, {"title": "Activation sparsity", "content": "To induce highly sparse activation maps without accuracy loss, kurtz[36] introduces a new regularization technique, coupled with a new threshold-based sparsification method based on a parameterized activation function. These techniques can be used individually or in combination to achieve the desired level of compression while maintaining acceptable levels of accuracy."}, {"title": "Exponential moving average (EMA)", "content": "EMA is a method used to stabilize and smooth the training process of deep neural networks (DNNs) [4, 7]. It involves maintaining a running average of model parameters throughout training. The EMA assigns exponentially decaying weights to the previous parameter values, with a higher weight given to more recent values. By incorporating this moving average into the training process, the model can benefit from the collective knowledge gained from earlier stages of training. This helps to reduce the impact of noisy or fluctuating gradients and improve the stability and generalization of the model. Like [13, 14], our work also employs EMA."}, {"title": "Neural architecture search (NAS)", "content": "NAS automates the design of DNNs and contributes significantly to advancements in tasks such as image classification [37, 38], object detection[39], and semantic segmentation[40]. While traditional NAS methods rely on reinforcement learning (RL) [37] and evolutionary algorithm (EA)[41], they tend to be computationally intensive due to the huge search space. However, there are three main differences between NAS and our method. Firstly, the NAS is more suitable for the design of complex DNNs other than efficient MobileViT. Secondly, NAS primarily focuses on the adjustment of local structures in DNNs. The uncertainty of the adjustment is very large due to the various size and diversity of local neural network structures. Our method adjusts the global architecture of MobileViT in terms of resolution, width, and depths, which is more succinct and efficient. Moreover, NAS typically explores an undetermined architecture of DNNs for each specific task[42]. Our method employs the base architecture of MobileViT, which is determined. Therefore, compared with the substantial computational cost of NAS, we pursue an efficient method for optimizing the global architecture factors and achieving high performance of MobileViT with minimal adjustments required."}, {"title": "4 Rule and navigation of efficient MobileViT", "content": "In this section, we rethink the impact of architecture factors (r, di, dm, w) in different inputs' sizes, MobileViT networks, and compression rules, where r, di, dm, and w denote the resolution, depth of inverted residual block, the depth of MobileViT block, and width respectively. We introduce a novel concise learning formula twisting magic 4D cube of the global architecture factors to produce efficient MobileViT."}, {"title": "4.1 Key global architecture factors of Mobile ViT", "content": "Resolution: For ViTs, the resolution typically refers to the size of an input image. Through calculating, we can get the sequence length or the number of tokens of this image. The resolution affects the memory and computational requirements of the model, as well as the ability to capture key information of an image.\nFactor of depth: It refers to the number of attention layers in transformer models. Each layer consists of multiple self-attention heads and feed-forward neural networks. Increasing the depth of the model allows it to capture more complex patterns and dependencies in the visual data. However, a deeper model may require more computational resources. Training a deeper transformer without proper regularization techniques is challenging.\nFactor of width: Width in ViTs indicates the dimensionality of token embeddings or the number of channels in the hidden layers. It determines the capacity or representational power of the model. A wider model can capture more intricate features and express complex patterns in the visual data. However, increasing the width also increases the number of parameters and computational requirements. We adjust the width by controlling the width multiplier in MobileViT V2 [14].\nCombination of architecture factors: Finding an appropriate balance between different factors in MobileViT is crucial for achieving good performance and efficiency on vision tasks. The optimal combination of these architecture factors is crucial for achieving good performance and efficiency on vision tasks, which depends on the specific dataset, computational resources, and task complexity. It often requires extraordinary and numerous experiments and fine-tuning to determine the appropriate architecture for a given task."}, {"title": "4.2 Impact of single factor", "content": "It is natural to investigate whether deeper or wider networks have a significant impact on MobileViT performance and computation complexity. Single factors, for instance, depth may play a role [36] for the balance between model performance and complexity. Due to the powerful representation ability of ViT and the limitations of computing capacity in mobile devices, it is necessary to quantify the importance of resolution, depth, and width in ViTs, particularly in the case of MobileViTs. Recognizing the architecture of MobileViT, we propose to investigate the impact of architecture factors (r, di, dm, w) while keeping certain constraints fixed, such as MACs. MACs represent the total number of multiplication and accumulation operations performed in a deep learning model. We use MACs to quantify the computational complexity of a MobileViT model and compare the computational efficiency of different MobileViT models.\nWe use a modeling problem concerning the accuracy of MobileViT to demonstrate how the model performance can be used to set global architecture factors in MobileViT. A GP is trained on our experimental evidence by maximizing the marginal likelihood (or evidence). The marginal likelihood refers to the marginalization over the latent function. The resulting GP model provides an excellent fit to the data as well as insights into its properties by interpretation of the posterior distribution. For the impact of single factors, the data is one-dimensional, and therefore the posterior distribution is easy to visualize. A total of four architecture factors are considered, which in practice rules out the use of cross-validation and NAS for setting architecture factors. Specifically, we denote the MACs of the given baseline MobileViT V2 as $m_0$. The resolution of an input image is denoted as $r_0 \\times r_0$, the width as $w_0$, and the depth as $d_0$.\nResolution and width are more significant: We perform a series of experiments with different factors (r, di, dm, w) in MobileViT V2. To reach the MACs around $m_0$, we randomly select the depth of inverted residual blocks, the depth of MobileViT blocks, and the width by tuning resolution to obtain different models. The resulting model has the target MACs. Similarly, we do the same thing to tune width and depths. To verify the performance, these models are trained with 100 epochs on the ImaegNet-100 dataset. As shown in Fig. 4, the accuracy is impacted by the resolution and width more, compared with the depths. When $d_i \\geq 1.5$ and $d_m \\geq 1.1$, the accuracy of MobileViT has reached a relatively stable state, with very few changes. Largely adjusting the depths has a relatively small impact on model performance. We observe that the highest top 1 accuracy falls within the range of resolutions 1.0 to 2.0. For r < 1.0, a larger resolution leads to higher accuracy, while the accuracy slightly decreases when r exceeds 1.6. As shown in subplot (a) Fig. 4, the accuracy increased from about 62% in r = 0.5 to about 83% in r = 1.5. The width demonstrates"}, {"title": "4.3 Impact of joint factors", "content": "In the design of efficient MobileViT, we must consider not only the impact of single factors but also the impact of joint factors. Therefore, to better understand the rule of MobileViT design, we explore how joint factors nonlinearly impact the performance and efficiency of MobileViT.\nModeling the impact of joint factors with a 2D grid GP:Mathematically, combinations of factors refer to choosing different sizes for two factors of MobileViT. Various methods exist for enumerating 2-combinations, such as cross-validation. These methods are usually less efficient and unexplainable, which involves exhaustively enumerating all 2 combinations of two factors. If both the sets of two factors have n elements, the number of 2-combinations is $n^2$. However, a 2D grid GP allows us to capture the joint impact of two factors, making it suitable for scenarios where joint factors nonlinearly determine the performance of MobileViT. The resulting 2D grid GP model not only fits the data exceptionally well but also offers insights into its properties through the interpretation of smoothness, variation, maximum, and minimum values on the posterior 2D grid. For the fitting of 2D grid GP, we utilize all experimental observations as training samples, with every point on the grid serving as test data. Through our analysis, we find that the resolution and width, as a pair of joint factors, have a more significant impact."}, {"title": "Joint impact of resolution and width", "content": "As shown in Fig. 6, the joint factor of resolution and width both impact model performance significantly. This might be caused by the fact that high-resolution image contains more semantic information and a wider network extracts more semantic features. The training data lies on a regularly spaced grid. In this scenario, we model a 2D function with the training data situated on an evenly spaced grid within the range (0.8, 1.3) $\\times$ (0.7, 1.3), containing 500 grid points along each dimension. Hence, there exist 250, 000 combinations of joint factors that neither cross-validation nor NAS can efficiently explore. Fig. 6 illustrates the posterior mean and variance of the model performance as a function of the characteristic resolution and width.\nFrom the posterior mean, we can see the 2D grid GP successfully reconstructs the model performances with different joint factors. The predictive model performance aligns with the training sample. The latent function varies more smoothly. The model performance has a clear maximum around the factor values \\{r = 1.1,w = 1.0\\}. The model"}, {"title": "Joint impact of resolution and depth of MobileViT block", "content": "We also consider the joint impact of resolution and depth to investigate how model performance varies with their combinations. The posterior mean and variance of model performance are presented in Fig. 7, with our objective being to model the performance based on the joint factors resolution and depth of the MobileViT block. Additionally, note in Fig. 7 that the 2D grid GP model produces relatively confident predictions. In this case, the joint factors are distributed on a uniformly spaced grid within the range (0.7, 1.45) $\\times$ (0.7, 1.3), containing 500 grid points along each dimension. In Fig. 7, the highest top 1 accuracy is achieved at the location r = 1.05, w = 1.05 in the grid. For r $\\leq$ 1.0, the model performances are poor even given different depths. A larger depth slightly enhances performance, which confirms the impact of depth. These experiments demonstrate how resolution and width can promote non-trivial Mobilevit, and the GP's ability to examine single and joint factors from the data proves valuable in practice. Despite the absence of simple parametric assumptions, the GP's capacity to learn nonlinear underlying functions makes it an attractive model for navigating the efficiency of MobileViT."}, {"title": "4.4 Efficiency rule for Mobile ViT", "content": "Based on previous investigation, unlike factors of resolution and width, MobileViT generally doesn't require many layers. Even with shallower layers MobileViT shows good performance. Therefore, when designing the architecture of MobileViT, a relatively small number of layers in the MobileViT block is enough.\nModeling efficiency rule as a GP: Given a baseline MobileViT V2 model, we aim at finding a feasible way to shrink the over-parameterized model in four dimensions, (r, di, dm, w), with some constraints of MACs. To make our model exquisite, we introduce a reduction factor $c\\in \\mathbb{R}$ with 0 < c < 1. Then, we assume that the optimal coefficients r, di, dm, w follow an underlying function of the reduction factor c as\n$r = f_r(c), d_i = f_{d_i}(c), d_m = f_{d_m}(c), w = f_w(c),$\nwhere $f_r(\\cdot), f_{d_i}(\\cdot), f_{d_m}(\\cdot)$ and $f_w(\\cdot)$ are functions describing the four factors. By verifying the models with different factors sampled randomly, we can learn the nonlinear relationship between (r, di, dm, w) and the model performance.\nTo delve deeper into the characteristics of the top-performing models, we focus on models situated along the Accuracy-MACs Pareto front. We set the constraint of Accuracy-MACs as follows.\n$\\begin{cases}min~m,&0.2 < m < 1.1m_o, \\\\max~acc,&0.5 < acc < 1.\\end{cases}$"}, {"title": "5 Experiments", "content": "In this section, we employ our efficiency formula tailored for the magic 4D cube to downsize MobileViT. The efficacy of our approach is demonstrated through rigorous testing on prominent vision benchmarks. We consider diversified datasets and evaluation metrics for efficient models. Datasets used in this paper include ImageNet-100, TieredImageNet, Cifar-10, and Cifar-100. More details of the dataset can be found in the appendix. We consider top 1 and top 5 accuracies as indicators of shrunk models by our rule.\nImplementation details: The MobileViT V2 and compressed MobileViT V2 are implemented using PyTorch and trained on NVIDIA GEFORCE RTX 4080. The batch size used in training is 64. We employ the AdamW optimizer[47] for the training of all models. For MobileViT V2 based models, we follow the same settings and configurations as [14]."}, {"title": "5.1 Experiments on ImageNet-100", "content": "In this experiment, we employ random sampling to generate a variety of MobileViT models with distinct architecture factors. Specifically, the resolution, depth, or width is randomly chosen within the range of 0.8 < r < 1.7, 1.3 $\\leq$ di $\\leq$ 2.3, 0.8 $\\leq$ dm $\\leq$ 1.6, 0.4 < w $\\leq$ 1.2. All models including the sampled MobileViT and MobileNetV1 undergo training on the ImageNet-100 dataset for 100 epochs, using the same training hyperparameters [14]. Table 1 presents the results for all the models, revealing a general trend where the larger models obtain higher accuracy. The model of the baseline MobileViT V2 achieves 82.36% accuracy. Our shrunken MobileViT V2 obtains top 1 accuracy with 83.21% and top 1 accuracy with 95.73%. Notably, it substantially outperforms other models. These findings underscore the need for a more effective efficiency rule shrinking MobileViT."}, {"title": "5.2 Experiments on TieredImageNet", "content": "To further investigate the generalizability of our formula, we conducted additional experiments using the larger TieredImageNet dataset. As shown in Table 2, the best top 1 accuracy of MobileNet is 68.12%. For the baseline MobileViT V2 model situated along the Accuracy-MACs Pareto front, it achieves top 1 accuracy with 69.50%. However, due to the use of the proposed efficiency rule, our proposed method consistently outperforms both the baseline Mobile ViT V2 and MobileNet."}, {"title": "5.3 Experiments on Cifar-10 and Cifar-100", "content": "In Table 3 and Table 4, we present the results of experiments on Cifar-10 and Cifar-100 for all the models. These experiments reveal that our shrunken MobileViT V2 still outperforms other models. Our shrunken MobileViT V2 surpasses the baselines in both datasets. As shown in Table 3 and Table 4, the overall top 1 and top 5 accuracies indicate a performance gain of our MobileViT V2. Both top 1 and top 5 demonstrate, unsurprisingly, that MobileViT using the rule of efficiency is superior to the original MobileViT. Notably, compared to the competitive baseline, Our shrunken MobileViT V2 outperforms it by 0.8%. These findings emphasize the necessity for a more efficient rule to enhance the compactness of MobileViT."}, {"title": "5.4 Experiments on CUB_200_2011 dataset and Car Parts dataset", "content": "We have augmented our study with new experiments on expanded datasets including CUB_200_2011, and Cars Parts in Table 5 and Table 6. These experiments provide a multifaceted perspective on the value of our approach. In addition, our focus is on a method to optimize the architectures of existing models rather than proposing a new model architecture. We believe that our extended experiments are sufficient to capture the complexity and variability required to thoroughly assess the optimized model's performance across different scenarios."}, {"title": "5.5 Latency on iPhone12", "content": "We have measured the latency of MobileViT on a mobile device (iPhone 12). We have reported the latency of the model in Table 7. This table reveal that our optimized model has not only achieved a marked improvement in performance but also reduced inference time, further validating the effectiveness of our approach. While MobileViT exhibits a higher inference time compared to other ViT models, its performance is significantly enhanced."}, {"title": "5.6 Visualization of learning curves", "content": "To effectively showcase the impact of our approach, we illustrate the learning curves of our efficient MobileViT V2 in Fig. 9. The learning curves in Fig. 9 include both validation loss and validation top 1. As depicted in Fig. 9, our MobileViT V2 maintains a significant convergence rate throughout the validation process. With EMA, the learning curves become smoother, which leads to a fast convergence of the training. We randomly choose several models, each adjusted with our proposed four factors, to showcase the validation loss. In Fig. 9, the average loss along with the upper and lower bounds of the selected models. These bounds are determined as the average plus 1.96 times the standard deviation and the average minus 1.96 times the standard deviation, respectively."}, {"title": "6 Conclusion", "content": "In this paper, we systematically explore the efficiency-centric MobileViT architecture using GP to understand the relationship between performance and key architecture factors. The proposed downsizing formula proves highly effective. We consider twisting 4D architecture factors including resolution, depth of the inverted residual block, depth of MobileViT block, and width to gain a smaller and better model. In particular, our proposed formula efficiently minimizes the search space of architecture factors. focusing solely on classification experiments limits the"}]}