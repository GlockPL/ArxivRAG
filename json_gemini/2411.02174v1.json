{"title": "Behavioral Sequence Modeling with Ensemble Learning", "authors": ["Maxime Kawawa-Beaudan", "Srijan Sood", "Soham Palande", "Ganapathy Mani", "Tucker Balch", "Manuela Veloso"], "abstract": "We investigate the use of sequence analysis for behavior modeling, emphasizing that sequential context often outweighs the value of aggregate features in understanding human behavior. We discuss framing common problems in fields like healthcare, finance, and e-commerce as sequence modeling tasks, and address challenges related to constructing coherent sequences from fragmented data and disentangling complex behavior patterns. We present a framework for sequence modeling using Ensembles of Hidden Markov Models, which are lightweight, interpretable, and efficient. Our ensemble-based scoring method enables robust comparison across sequences of different lengths and enhances performance in scenarios with imbalanced or scarce data. The framework scales in real-world scenarios, is compatible with downstream feature-based modeling, and is applicable in both supervised and unsupervised learning settings. We demonstrate the effectiveness of our method with results on a longitudinal human behavior dataset.", "sections": [{"title": "1 Introduction", "content": "Modeling human behavior is a complex task with applications spanning multiple domains such as user research, healthcare, payments, trading, and e-commerce. Applications range from classifying human activity (28), distinguishing humans from bots (16), detecting credit card fraud (25) etc.\nBehavior is often captured as sequences of actions or events over time, and understanding patterns within these sequences is crucial for tasks like classification, anomaly detection, and user modeling."}, {"title": "2 Background & Related Work", "content": "A key challenge in utilizing the captured sequences is class imbalance, where underrepresented behavior profiles or a disproportionate number of anomalous examples hinder model generalization.\nMany solutions leverage complex deep learning models (33), focusing on event-level classification with extensive feature engineering (6). However, such event-level or feature-aggregated methods frequently fall short in capturing the sequential dynamics essential for understanding and modeling behavior. These approaches are also susceptible to overfitting, with performance rapidly degrading in class-imbalanced scenarios.\nSequential context is crucial for effective behavior modeling. For example, in credit card fraud detection or anti-money laundering, a user's transaction history provides deeper insights into behavioral intent than isolated transactions or aggregated features. Yet, many datasets and approaches remain confined to the event level, overlooking the broader sequential context. In this study, we advocate for a sequence modeling approach to behavior modeling, particularly in scenarios where behaviors are represented as action sequences derived from unstructured data. Aggregating this data into coherent sequences that reflect an agent's decision-making process is a non-trivial challenge. Our method is tested in a real-world setting characterized by extreme class imbalance, with millions of diverse user behavior examples contrasted against only a few hundred instances of the anomalous class.\nWe propose a novel and lightweight ensemble-based framework for behavior modeling, and show efficacy on downstream (imbalanced) sequence classification tasks. While our approach is model-agnostic, we leverage Hidden Markov Models (HMMs) for their simplicity, interpratibility, and efficacy at capturing temporal dependencies and latent patterns. Our real-world deployment scales to millions of sequences, while being compatible with downstream machine learning methods. We demonstrate its effectiveness on a publicly available human behavior dataset (31).\nOur contributions: (1) We introduce a behavior modeling framework based on sequences of events/actions, applicable to various domains; (2) We outline its application to supervised and unsupervised tasks; (3) We demonstrate its effectiveness on a longitudinal human behavior dataset."}, {"title": "HMMs", "content": "Hidden Markov Models (HMMs) are statistical models for sequential data, which have a long history of use in natural language processing, finance, and bioinformatics (24; 5; 32; 22). HMMs have been used extensively for behavior modeling, including sensor surveillance (21), human-computer interfaces (11), and web user interactions (15), with recent applications in social media bot detection (19). While neural network-based approaches like CNNs, LSTMs, and Transformers have shown success in settings like sentiment analysis (13) and network intrusion detection (27), they face challenges such as high computational cost, overfitting, and reduced interpretability."}, {"title": "Resolution", "content": "Event-level classification still dominates in areas like anti-money laundering and network security, where sequence-level labels are often missing (1; 8). This lends itself to aggregate feature based approaches, missing key historical context. While some work has tackled sequence modeling in network intrusion detection with favorable results (26), much remains to be done."}, {"title": "Data Imbalance and Anomaly Detection", "content": "Many real-world problems, including intrusion detection (12), credit card fraud (25), and money laundering (2), involve detecting rare events and suffer from class imbalance. One-class anomaly detection focuses on robustly modeling the nominal class and identifying deviations (9), while more targeted approaches model both normal and anomalous sequences to detect specific behavioral anomalies (10)."}, {"title": "3 Approach", "content": "Consider a sequence observation $O = {a_1, a_2, ..., a_r}$, where each $a_i$ is drawn from a discrete set of actions A. Such sequences can represent various behaviors, such as user interactions in an app, trading actions in financial markets, or other human decision-making processes. Our goal is to model these behaviors, either discovering behavior clusters, or classifying behaviors when labels are available (e.g., online bot detection, credit card fraud detection, physical activity recognition (20; 33; 18)."}, {"title": "3.1 Sequence Construction", "content": "One of the primary challenges lies not in modelling but in organizing coherent data streams from raw, fragmented data D, which often contains interwoven behaviors from multiple agents/users.\nFor instance, in trading, D may span billions of transactions across participants, assets, and exchanges, requiring grouping data streams by participant, and further by exchange or asset to capture specific behaviors. In network analysis, interactions between devices and servers can be grouped by source IP for individual user activity, or further by target IP to constitute specific behavior streams.\nAs illustrated in Fig. 1, we begin by disentangling D into separate data streams $D_1, ..., D_H$, each corresponding to one of H agents. Feature engineering refines these data streams through dimensionality reduction, tokenization, or discretization, enhancing model generalization, particularly in the presence of imbalanced or sparse datasets.Continuous features can also be normalized and estimated directly, through techniques like Gaussian HMMs (23).\nOnce data is organized into streams $D_h$, sequences of observations $O_h^{(1)}, ..., O_h^{(n_{[h]})}$ are then extracted from each stream, with domain knowledge or sessions guiding the sequence span (start and end points). For example, web user behavior may span minutes to hours, whereas medical trial observations could extend over days or weeks. Breaks in continuous data streams often demarcate sequences, with shorter pauses treated as wait events and longer breaks as sequence endpoints. The number of sequences can vary significantly across agents, reflecting differing activity levels (e.g. power users vs intermittent monthly users)."}, {"title": "3.2 Ensembles of Hidden Markov Models", "content": "Singleton HMMS First, considering binary sequence classification, we separate training data by class and train two indivudal HMMs: one positive class HMM $\\lambda^+$, and one negative class HMM $\\lambda^-$. Given an unseen sequence O, the predicted class c(0) is determined by comparing the likelihoods:"}, {"title": "Variable Sequence Length", "content": "HMMs excel at sequence analysis (5), but struggle when comparing sequences of varying length, as length influences likelihood computation exponentially. We address this through model-driven normalization, computing likelihoods for a given sequence across multiple models, and deriving a rank-based composite score (rather than comparing sequence likelhihoods)."}, {"title": "HMM Ensembles", "content": "HMMs, while lightweight and efficient, can struggle to capture the complexity of behaviors in training data when using a singular model (per class). Ensemble methods train multiple models on subsets of the data (7), enabling each learner to specialize on distinct patterns or behaviors, while collectively capturing the full data distribution. This results in a more robust approach, particularly in scenarios with data imbalance, where monolithic models skew towards modeling the majority class (or underfitting for class specific models) (17). We propose HMM-e, an ensemble framework that computes composite scores from individual learners (14). While HMMs are effective for our case, this framework is model-agnostic and can incorporate other model classes such as neural networks, SVMs, or decision trees."}, {"title": "3.2.1 Formalization and Algorithmic Framework", "content": "First, we train N models {$\\lambda_1^+,..., \\lambda_N^+$} on the positive class and M models {$\\lambda_1^-, ..., \\lambda_M^-$} on the negative class, taking care to ensure diversity among the models by training each on a randomly selected subset of samples from the training data. Each model sees s% of the training data in its relevant class. While we set N = M for all settings, these parameters (M, N, s) can be established using typical hyperparameter optimization approaches. For any given sequence in the training data, the probability of not being selected for any model's random subset is $(1 \u2013 s)^N$. The expected number of unsampled sequences is the same, so it is important to select s and N to keep this proportion of the training data small.\nFor an unseen observation sequence O, we compute its likelihood scores under all models: {p(O | $\\lambda_1^+$), ..., p(O | $\\lambda_N^+$)} and {p(O | $\\lambda_1^-$), ..., p(O | $\\lambda_M^-$)}. We then compute a composite score:\nThe score s(0) represents the pairwise comparisons where positive-class models assign a higher likelihood than negative-class models, taking values in [0, N \u00d7 M]. A low score indicates that the sequence is more likely under the negative-class models, and vice versa. As likelihoods across different sequence lengths are not directly compared, this composite score acts as an implicit normalization technique. N and M should be chosen such that the score range adequately distinguishes the classes.\nFor our HMM and HMM-e approaches, we use 3 states in each of our models, and converge on an ensemble size of 250 and a subset factor of 1%. We perform hyperparameter search for ensemble size, trying other values in [10, 50, 100, 500, 1000]. We settle on 250 for its good performance at a relatively low complexity."}, {"title": "Downstream Modeling using HMM-e Scores", "content": "Given a corpus of sequences and corresponding scores {0, s(0)}, we classify sequences using a threshold $S_{thresh}$: $c(O_i) = 1{s(O_i) \\geq S_{thresh}}$. Alternatively, base learner likelihoods can serve as features for downstream classifiers. For each sequence $O_i$, we define a feature vector"}, {"title": "Clustering HMM-e Scores in Unsupervised Settings", "content": "In label-free settings, behavior clustering can be achieved using unsupervised learning approaches. We train N models {$\\lambda_1, ..., \\lambda_N$} on random s% data subsets and generate feature vectors of base learner likelihoods $f_i$ (Section 3.2.1). Unsupervised clustering like K-Means can be applied to discover behavioral groups, dimensionality reduction (e.g., PCA) can be helpful when N is large."}, {"title": "4 Experiments", "content": "We evaluate our approach on the GLOBEM dataset (31), a longitudinal human behavior study featuring over 3,700 attributes from 497 participants across four years (2018-2021). The dataset includes survey, smartphone, and wearable data, with a focus on depression detection (the task we consider). Data includes mood assessments, step counts, location variability, and sleep metrics. We utilize the data standardization platform for reproducibility provided by the authors (30)."}, {"title": "5 Conclusion", "content": "We explore the connection between human behavior modeling and sequence analysis, providing a general framework for extracting coherent sequences from fragmented data. We present HMM-e, an ensemble learning approach that effectively models behavior sequences with minimal feature engineering. Our experiments demonstrate that HMM-e outperforms traditional machine learning baselines and delivers results comparable to complex deep-learning models, despite using fewer features. This highlights the efficiency and potential of our approach for scalable and interpretable sequence modeling in behavior-driven applications."}]}