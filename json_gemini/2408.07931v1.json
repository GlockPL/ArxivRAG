{"title": "Surgical SAM 2: Real-time Segment Anything in Surgical Video by Efficient Frame Pruning", "authors": ["Haofeng Liu", "Erli Zhang", "Junde Wu", "Mingxuan Hong", "Yueming Jin"], "abstract": "Surgical video segmentation is a critical task in computer- assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, the Segment Anything Model 2 (SAM2) framework has shown superior advancements in image and video segmentation. However, SAM2 struggles with efficiency due to the high computational demands of processing high-resolution images and complex and long-range temporal dynamics in surgical videos. To address these challenges, we introduce Surgical SAM 2 (SurgSAM-2), an advanced model to uti- lize SAM2 with an Efficient Frame Pruning (EFP) mechanism, to fa- cilitate real-time surgical video segmentation. The EFP mechanism dy- namically manages the memory bank by selectively retaining only the most informative frames, reducing memory usage and computational cost while maintaining high segmentation accuracy. Our extensive ex- periments demonstrate that SurgSAM-2 significantly improves both ef- ficiency and segmentation accuracy compared to the vanilla SAM2. Re- markably, SurgSAM-2 achieves a 3\u00d7 FPS compared with SAM2, while also delivering state-of-the-art performance after fine-tuning with lower- resolution data. These advancements establish SurgSAM-2 as a leading model for surgical video analysis, making real-time surgical video seg- mentation in resource-constrained environments a feasible reality.", "sections": [{"title": "1 Introduction", "content": "Surgical video scene segmentation is a critical task in computer-assisted surgery, where the precise identification and delineation of surgical instruments and tis- sues within video sequences are essential. This capability underpins various ap- plications, such as instrument tracking and pose estimation, intraoperative guid- ance, and postoperative analysis [3,4], ultimately enhancing surgical precision,"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Surgical Instrument Segmentation", "content": "The field of surgical instrument segmentation has evolved significantly with deep learning, particularly through fully convolutional networks (FCNs) and encoder- decoder architectures like U-Net, which laid the foundation for the domain [15]. However, these early methods often faced challenges in dynamic surgical envi- ronments, struggling with spatial inconsistencies and the complex interactions between instruments and surrounding anatomy [16,17]. To address these challenges, recent advancements have focused on transformer- based models and attention mechanisms, such as Swin Transformers and multi- scale attention U-Nets, which offer better robustness and adaptability in han- dling the complex visual features of surgical instruments [18,19]. Following the introduction of the Segment Anything Model (SAM)[5] and SAM2[13], there has been a shift towards models tailored specifically for the medical image segmen- tation [39] and surgical video segmentation, such as SurgicalSAM [14]. Despite these advances, challenges remain, particularly in achieving efficient processing within resource constraints typical of surgical settings. This gap in efficiency is the core motivation behind SurgSAM-2, which seeks to optimize performance for real-time surgical applications [20,21]."}, {"title": "2.2 Segment Anything Model 2", "content": "SAM2 builds on Vision Transformers (ViTs) with enhanced multi-scale feature extraction, making it a powerful tool in image and video segmentation [13]. Following targeted modifications, SAM2 also shows substantial effectiveness in the 2D and 3D medical image segmentation [38]. However, its use in surgi- cal video segmentation faces significant challenges due to the computational intensity of ViTs, which require substantial resources, limiting their practical- ity in real-time, resource-constrained environments [27]. SAM2's reliance on a first-come-first-serve memory mechanism exacerbates inefficiencies, as it retains potentially redundant frames, further slowing down processing. The need for optimized models that reduce computational overhead while maintaining strong segmentation performance is critical, paving the way for more efficient solutions like SurgSAM-2."}, {"title": "2.3 Memory Bank Restriction", "content": "Efficient memory management is crucial for real-time applications, especially in the context of surgical video segmentation where computational resources are limited. Strategies like XMem [22] and RMem [23] have explored ways to priori- tize and retain only the most relevant frames during video analysis. Building on these approaches, SurgSAM-2 introduces an efficient frame pruning mechanism, which uses a cosine similarity-based scoring system to retain only the most in- formative frames, reducing memory usage and increasing processing speed. This approach directly addresses inefficiencies in SAM2 memory management, making SurgSAM-2 better suited to the fast-paced and resource-constrained demands of real-time surgical video analysis."}, {"title": "3 Methods", "content": "SurgSAM-2 is an advanced model designed specifically for the complex and resource-constrained environments of surgical video segmentation. Building upon the SAM2 [13], SurgSAM-2 incorporates a dynamic memory bank management mechanism to optimize the retention and use of video frames during segmen- tation tasks. This innovation not only reduces the computational load but also enhances segmentation accuracy by selectively retaining the most relevant and useful information. The memory bank consists of the current frame and dy- namically selected preceding frames, which are critical for maintaining temporal context. By integrating these advancements, SurgSAM-2 addresses the unique challenges posed by real-time surgical video analysis, offering a robust solution that balances efficiency and performance."}, {"title": "3.1 SurgSAM-2 Architecture", "content": "SurgSAM-2 builds upon the SAM2 [13], leveraging its robust ViT architecture specifically adapted for surgical video segmentation. The backbone of SAM2 is"}, {"title": "3.2 Efficient Frame Pruning", "content": "A key innovation in SurgSAM-2 is the implementation of an efficient frame prun- ing mechanism, designed to intelligently manage which video frames are retained for further processing. This mechanism dynamically evaluates the relevance of each incoming frame before it is added to the memory bank, ensuring that only the most informative frames are preserved while discarding those that contribute minimally to the segmentation task.\nFor each frame $f_t$ that is passed into the memory bank, the cosine similarity $S(f_t, f_i)$ between $f_t$ and the past n frames ${f_{t-n},..., f_{t-2}, f_{t-1}}$ in the sliding window is computed as follows:\n$S(f_t, f_i) = \\frac{f_t \\cdot f_i}{||f_t|| ||f_i||}$                            (1)\nAfter computing the n cosine similarities, the mechanism identifies the m most similar frames from these n frames. These m frames are pruned, and the remaining (n \u2013 m) frames plus frame $f_t$ are then stored in the memory bank"}, {"title": "3.3 Implementation Details", "content": "All experiments were conducted on an RTX A6000 GPU with 48GB using the ViT-Small backbone. The precision for the experiments was set to bfloat16, which helps to reduce computational load while maintaining model performance. Therefore, we opted to fine-tune using a resolution of 512 \u00d7 512 and adopted the weights of the ViT-Small version from SAM2. Following the training strategy outlined in SAM2, we alternated between video and image training. To fully explore the potential of SAM2, we train the multi-mask output, iou prediction, and occlusion prediction. To preserve the generalization ability of SAM2, we fine- tuned only the mask decoder and memory module, keeping the prompt encoder and image encoder frozen."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Dataset", "content": "We extensively evaluate the proposed SurgSAM-2 model on two widely recog- nized and publicly available datasets: the 2017 MICCAI EndoVis Instrument Challenge (EndoVis17) [24] and the 2018 MICCAI EndoVis Scene Segmenta- tion Challenge (EndoVis18) [25]. EndoVis18 presents more complicated surgical scenes, therefore proving more challenging compared to EndoVis17. The En- doVis17 dataset comprises eight training videos each containing 225 frames, eight testing videos collected followed by training videos, and another two hold-out testing videos (sequence 9, 10), comprising 1200 frames in total. The EndoVis18 dataset consists of 15 videos with each consisting of 149 frames."}, {"title": "4.2 Evaluation Metrics", "content": "To comprehensively evaluate the performance of SurgSAM-2, we adopt numer- ous and widely-used evaluation metrics in video object segmentation (VOS) that assess both the accuracy and computational efficiency of segmentation. These metrics are chosen to provide a holistic view of the model's capabilities in the context of surgical video segmentation. For these metrics, we follow the evalua- tion protocol in the video object segmentation benchmark, specifically excluding the first and last frames from the assessment. We also utilize the official evalua- tion protocol in EndoVis Challenge [24,25] for method validation.\nIntersection-over-Union (Jor IoU): Intersection-over-Union (IoU), denoted as J, measures the overlap between the predicted segmentation and the ground truth. It is calculated as the ratio of the intersection between the predicted and true positive regions to their union. IoU is a standard metric in segmentation tasks, offering a robust measure of the accuracy of the model's predictions.\n$J = \\frac{Area of Overlap}{Area of Union}$\nBoundary F1 Score (F): The Boundary F1 Score, denoted as F, assesses the accuracy of the predicted boundaries in the segmentation mask. It calculates the F1 score specifically along the edges of the segmented regions, providing insight into how well the model captures the precise contours of the surgical instruments.\n$F=2x \\frac{Precision \u00d7 Recall}{Precision + Recall}$\nJ&F Score: The J&F score is a composite metric that averages the IoU and Boundary F1 scores, providing a balanced assessment of both region overlap and boundary accuracy. This metric is particularly useful for evaluating segmentation quality in tasks where both precise region delineation and boundary accuracy are crucial.\n$J&F = \\frac{J+F}{2}$\nDice Coefficient (Dice): The Dice Coefficient, another widely used metric in segmentation tasks, measures the similarity between the predicted segmentation and the ground truth. It is closely related to IoU but places more emphasis on the overlap, making it a complementary metric to IoU.\n$Dice = \\frac{2 x Prediction \\cap Ground Truth}{Prediction + Ground Truth}$\nChallenge IoU (CIoU): The Challenge IoU metric follows the evaluation pro- tocol outlined in the EndoVis18 Challenge [25]. CIoU calculates the Intersection over Union (IoU) for each frame individually, considering only the objects present in that specific frame."}, {"title": "4.3 Experimental Results", "content": "Evaluation on Model Efficiency We conducted a thorough evaluation of SurgSAM-2's performance in terms of FPS and memory usage, comparing it with the vanilla SAM2 across various configurations. The results, as illustrated in Table 1, 2 and 3, clearly demonstrate that by implementing a cosine similarity- based efficient frame pruning mechanism and reducing the memory bank size, both FPS and memory efficiency are significantly improved."}, {"title": "5 Conclusion and Future Work", "content": "In conclusion, our proposed model SurgSAM-2 represents a significant advance- ment in the domain of surgical video segmentation. By integrating an EFP mech- anism with the robust SAM2 framework, SurgSAM-2 successfully addresses the challenges of real-time surgical video processing, enhancing both efficiency and accuracy. The ability of SurgSAM-2 to selectively retain the most relevant frames based on cosine similarity has reduced memory usage while simultaneously im- proving the model's segmentation performance across various tasks."}]}