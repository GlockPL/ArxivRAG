{"title": "Surgical SAM 2: Real-time Segment Anything in Surgical Video by Efficient Frame Pruning", "authors": ["Haofeng Liu", "Erli Zhang", "Junde Wu", "Mingxuan Hong", "Yueming Jin"], "abstract": "Surgical video segmentation is a critical task in computer- assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, the Segment Anything Model 2 (SAM2) framework has shown superior advancements in image and video segmentation. However, SAM2 struggles with efficiency due to the high computational demands of processing high-resolution images and complex and long-range temporal dynamics in surgical videos. To address these challenges, we introduce Surgical SAM 2 (SurgSAM-2), an advanced model to utilize SAM2 with an Efficient Frame Pruning (EFP) mechanism, to facilitate real-time surgical video segmentation. The EFP mechanism dynamically manages the memory bank by selectively retaining only the most informative frames, reducing memory usage and computational cost while maintaining high segmentation accuracy. Our extensive experiments demonstrate that SurgSAM-2 significantly improves both efficiency and segmentation accuracy compared to the vanilla SAM2. Remarkably, SurgSAM-2 achieves a 3\u00d7 FPS compared with SAM2, while also delivering state-of-the-art performance after fine-tuning with lower-resolution data. These advancements establish SurgSAM-2 as a leading model for surgical video analysis, making real-time surgical video segmentation in resource-constrained environments a feasible reality.", "sections": [{"title": "1 Introduction", "content": "Surgical video scene segmentation is a critical task in computer-assisted surgery, where the precise identification and delineation of surgical instruments and tissues within video sequences are essential. This capability underpins various applications, such as instrument tracking and pose estimation, intraoperative guidance, and postoperative analysis [3,4], ultimately enhancing surgical precision,"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Surgical Instrument Segmentation", "content": "The field of surgical instrument segmentation has evolved significantly with deep learning, particularly through fully convolutional networks (FCNs) and encoder-decoder architectures like U-Net, which laid the foundation for the domain [15]. However, these early methods often faced challenges in dynamic surgical environments, struggling with spatial inconsistencies and the complex interactions between instruments and surrounding anatomy [16,17].\nTo address these challenges, recent advancements have focused on transformer-based models and attention mechanisms, such as Swin Transformers and multi-scale attention U-Nets, which offer better robustness and adaptability in handling the complex visual features of surgical instruments [18,19]. Following the introduction of the Segment Anything Model (SAM)[5] and SAM2[13], there has been a shift towards models tailored specifically for the medical image segmentation [39] and surgical video segmentation, such as SurgicalSAM [14]. Despite these advances, challenges remain, particularly in achieving efficient processing within resource constraints typical of surgical settings. This gap in efficiency is the core motivation behind SurgSAM-2, which seeks to optimize performance for real-time surgical applications [20,21]."}, {"title": "2.2 Segment Anything Model 2", "content": "SAM2 builds on Vision Transformers (ViTs) with enhanced multi-scale feature extraction, making it a powerful tool in image and video segmentation [13]. Following targeted modifications, SAM2 also shows substantial effectiveness in the 2D and 3D medical image segmentation [38]. However, its use in surgical video segmentation faces significant challenges due to the computational intensity of ViTs, which require substantial resources, limiting their practicality in real-time, resource-constrained environments [27]. SAM2's reliance on a first-come-first-serve memory mechanism exacerbates inefficiencies, as it retains potentially redundant frames, further slowing down processing. The need for optimized models that reduce computational overhead while maintaining strong segmentation performance is critical, paving the way for more efficient solutions like SurgSAM-2."}, {"title": "2.3 Memory Bank Restriction", "content": "Efficient memory management is crucial for real-time applications, especially in the context of surgical video segmentation where computational resources are limited. Strategies like XMem [22] and RMem [23] have explored ways to prioritize and retain only the most relevant frames during video analysis. Building on these approaches, SurgSAM-2 introduces an efficient frame pruning mechanism, which uses a cosine similarity-based scoring system to retain only the most informative frames, reducing memory usage and increasing processing speed. This approach directly addresses inefficiencies in SAM2 memory management, making SurgSAM-2 better suited to the fast-paced and resource-constrained demands of real-time surgical video analysis."}, {"title": "3 Methods", "content": "SurgSAM-2 is an advanced model designed specifically for the complex and resource-constrained environments of surgical video segmentation. Building upon the SAM2 [13], SurgSAM-2 incorporates a dynamic memory bank management mechanism to optimize the retention and use of video frames during segmentation tasks. This innovation not only reduces the computational load but also enhances segmentation accuracy by selectively retaining the most relevant and useful information. The memory bank consists of the current frame and dynamically selected preceding frames, which are critical for maintaining temporal context. By integrating these advancements, SurgSAM-2 addresses the unique challenges posed by real-time surgical video analysis, offering a robust solution that balances efficiency and performance."}, {"title": "3.1 SurgSAM-2 Architecture", "content": "SurgSAM-2 builds upon the SAM2 [13], leveraging its robust ViT architecture specifically adapted for surgical video segmentation. The backbone of SAM2 is retained in SurgSAM-2 but with significant optimizations to meet the unique challenges posed by surgical environments.\nAt its core, the image encoder of SurgSAM-2 processes incoming video frames into detailed embeddings, capturing both local and global features necessary for accurate segmentation. While this architecture is consistent with the vanilla SAM2, SurgSAM-2 enhances it by integrating a dynamic memory management system that selectively prunes less relevant frames, ensuring that only the most critical data is retained for further analysis.\nThese refinements allow SurgSAM-2 to maintain the high performance associated with SAM2 while significantly improving efficiency, making it better suited for real-time applications in resource-constrained surgical settings."}, {"title": "3.2 Efficient Frame Pruning", "content": "A key innovation in SurgSAM-2 is the implementation of an efficient frame pruning mechanism, designed to intelligently manage which video frames are retained for further processing. This mechanism dynamically evaluates the relevance of each incoming frame before it is added to the memory bank, ensuring that only the most informative frames are preserved while discarding those that contribute minimally to the segmentation task.\nFor each frame $f_t$ that is passed into the memory bank, the cosine similarity $S(f_t, f_i)$ between $f_t$ and the past $n$ frames ${f_{t-n},..., f_{t-2}, f_{t-1}}$ in the sliding window is computed as follows:\n$S(f_t, f_i) = \\frac{f_t f_i}{|| f_t |||| f_i ||}$ (1)\nAfter computing the $n$ cosine similarities, the mechanism identifies the $m$ most similar frames from these $n$ frames. These $m$ frames are pruned, and the remaining $(n \u2013 m)$ frames plus frame $f_t$ are then stored in the memory bank for memory cross-attention. Notably, the first frame $f_0$, which serves as a key reference, is always retained in the memory bank and is not counted towards the dynamic memory bank size. This ensures that the memory bank always includes a critical reference frame while still optimizing for efficiency by retaining only the most relevant subsequent frames.\nConsidering that the vanilla SAM2 model uses a memory bank size of six past frames and the first reference frame for spatiotemporal modeling, our configuration is set as $n = 5$ and $m = 2$. We first compute the cosine similarity between frame $f_t$ and the past $n$ frames. Subsequently, we prune the two most similar frames $m = 2$, leaving four frames in the dynamic memory bank, which aligns with our configuration for the EFP mechanism in SurgSAM-2.\nBy implementing this selective EFP strategy, SurgSAM-2 effectively reduces memory usage and computational load, allowing the model to process video frames more efficiently. In surgical videos, where scenes often exhibit high visual similarity and repeated semantic content across frames, this approach is particularly effective in eliminating redundancy while maintaining efficiency. Despite the reduction in stored frames, the model maintains high segmentation accuracy by focusing computational resources on the most critical data, making it well-suited for real-time surgical video analysis in resource-constrained environments."}, {"title": "3.3 Implementation Details", "content": "All experiments were conducted on an RTX A6000 GPU with 48GB using the ViT-Small backbone. The precision for the experiments was set to bfloat16, which helps to reduce computational load while maintaining model performance. Note that the vanilla SAM2 uses a resolution of 1024 x 1024 and ViT-Base+.\nHowever, video training at this resolution is highly time-consuming and difficult to stabilize with limited computational resources in the surgical scenario. Therefore, we opted to fine-tune using a resolution of 512 \u00d7 512 and adopted the weights of the ViT-Small version from SAM2. Following the training strategy outlined in SAM2, we alternated between video and image training. To fully explore the potential of SAM2, we train the multi-mask output, iou prediction, and occlusion prediction. To preserve the generalization ability of SAM2, we fine-tuned only the mask decoder and memory module, keeping the prompt encoder and image encoder frozen.\nFor video training, a batch size of 12 was used, with each batch containing 8 frames and up to 3 objects per frame. In image training, the batch size was set to 32, with a maximum of 3 objects per image. This alternation between video and image data during training ensures that the model effectively learns from both dynamic and static content, enhancing its generalization capabilities. We utilized a learning rate of 2 \u00d7 10-4 for the mask decoder and 2 \u00d7 10-5 for the memory encoder. As for the video data augmentation strategy, we followed the strategy of Cutie [29], which is designed to improve the model robustness by simulating various challenging scenarios during training. During inference, we re-scaled the output segmentation to its original resolution for fair evaluation. To ensure high-quality segmentation for the first frame crucial for effective"}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Dataset", "content": "We extensively evaluate the proposed SurgSAM-2 model on two widely recognized and publicly available datasets: the 2017 MICCAI EndoVis Instrument Challenge (EndoVis17) [24] and the 2018 MICCAI EndoVis Scene Segmentation Challenge (EndoVis18) [25]. EndoVis18 presents more complicated surgical scenes, therefore proving more challenging compared to EndoVis17. The EndoVis17 dataset comprises eight training videos each containing 225 frames, eight testing videos collected followed by training videos, and another two hold-out testing videos (sequence 9, 10), comprising 1200 frames in total. The EndoVis18 dataset consists of 15 videos with each consisting of 149 frames. For Endovis17, we use the hold-out test set for evaluation. For EndoVis18, we split the sequences of 2, 5, 9, and 15 for testing following the standard procedure in ISINet [26].\nThe data from the EndoVis17 and EndoVis18 datasets were pre-processed following the approach described by Shvets et al. [28]. Given that the EndoVis17 and EndoVis18 datasets from ISINet [26] only include instrument-type labels rather than instance-level labels, we re-annotate the data to ensure that our model is evaluated on a more detailed and instance-specific level, allowing for precise instrument segmentation and better generalization across different surgical scenarios."}, {"title": "4.2 Evaluation Metrics", "content": "To comprehensively evaluate the performance of SurgSAM-2, we adopt numerous and widely-used evaluation metrics in video object segmentation (VOS) that assess both the accuracy and computational efficiency of segmentation. These metrics are chosen to provide a holistic view of the model's capabilities in the context of surgical video segmentation. For these metrics, we follow the evaluation protocol in the video object segmentation benchmark, specifically excluding the first and last frames from the assessment. We also utilize the official evaluation protocol in EndoVis Challenge [24,25] for method validation.\nIntersection-over-Union (Jor IoU): Intersection-over-Union (IoU), denoted as J, measures the overlap between the predicted segmentation and the ground truth. It is calculated as the ratio of the intersection between the predicted and true positive regions to their union. IoU is a standard metric in segmentation tasks, offering a robust measure of the accuracy of the model's predictions.\n$J = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}$\nBoundary F1 Score (F): The Boundary F1 Score, denoted as F, assesses the accuracy of the predicted boundaries in the segmentation mask. It calculates the F1 score specifically along the edges of the segmented regions, providing insight into how well the model captures the precise contours of the surgical instruments.\n$F=2x \\frac{\\text{Precision } \\times \\text{ Recall}}{\\text{Precision + Recall}}$\nJ&F Score: The J&F score is a composite metric that averages the IoU and Boundary F1 scores, providing a balanced assessment of both region overlap and boundary accuracy. This metric is particularly useful for evaluating segmentation quality in tasks where both precise region delineation and boundary accuracy are crucial.\n$J&F = \\frac{J+F}{2}$\nDice Coefficient (Dice): The Dice Coefficient, another widely used metric in segmentation tasks, measures the similarity between the predicted segmentation and the ground truth. It is closely related to IoU but places more emphasis on the overlap, making it a complementary metric to IoU.\nDice = $\\frac{2 \\times \\text{Prediction } \\times \\text{ Ground Truth}}{\\text{Prediction + Ground Truth}}$\nChallenge IoU (CIoU): The Challenge IoU metric follows the evaluation protocol outlined in the EndoVis18 Challenge [25]. CIoU calculates the Intersection over Union (IoU) for each frame individually, considering only the objects present in that specific frame. The IoU scores are then averaged across all frames to produce the final CIoU score, providing a more precise assessment of segmentation performance in the context of dynamic and frame-specific object presence.\nFrames Per Second (FPS): To evaluate the real-time performance of SurgSAM-2, we measure the FPS during inference. This metric is crucial for applications in surgical environments where timely processing of video frames is essential for effective decision-making.\nMemory Usage: We also assess the memory efficiency of SurgSAM-2 by calculating the model's memory footprint during inference. Given the resource-constrained nature of many surgical settings, reducing memory usage is a key objective of our approach. By optimizing the memory bank size and employing selective frame retention, SurgSAM-2 achieves a balance between high segmentation accuracy and efficient memory usage.\nThese metrics together provide a comprehensive evaluation of SurgSAM-2, highlighting its strengths in segmentation accuracy, boundary precision, computational speed, and memory efficiency. By balancing these aspects, SurgSAM-2 is well-positioned to meet the demands of real-time surgical video analysis."}, {"title": "4.3 Experimental Results", "content": "Evaluation on Model Efficiency We conducted a thorough evaluation of SurgSAM-2's performance in terms of FPS and memory usage, comparing it with the vanilla SAM2 across various configurations. The results, as illustrated in Table 1, 2 and 3, clearly demonstrate that by implementing a cosine similarity-based efficient frame pruning mechanism and reducing the memory bank size, both FPS and memory efficiency are significantly improved."}, {"title": "Fine-Tuning for Optimized Segmentation and Efficiency", "content": "We further investigated the efficacy of fine-tuning in our SurgSAM-2 model. Typically, higher input resolutions enhance segmentation accuracy. Surprisingly, we found SurgSAM-2, which predicts segmentation based on half the original resolution,(512, in our setting), can already outperform the vanilla SAM2 at the full resolution. This result is important in the surgical scenario because a smaller resolution allows for real-world model training with reduced memory requirements. Most importantly, this enables a dramatic increase in prediction speed for real-time segmentation of surgical video. In the 1-point setting, which requires very little effort from the surgeon-just a single click on the first frame of the entire surgical procedure\u2014our SurgSAM-2 can increase precision from 85.1% Dice to 87.3%, and also efficiency from 29 to 86 FPS in EndoVis17, as in shown in Table 2.\nLooking at all three tables 1, 2 and 3 into details, we can see that our SurgSAM-2 with fine-tuning shows a marked and consistent improvement in segmentation accuracy across different settings. For instance, for Five points, our SurgSAM-2 with fine-tuning led to an increase in the J&F metric from 83.6% to 88.0% and the Dice score from 86.9% to 91.4% in the EndoVis17 dataset. Similarly, our method resulted in an increase in the J&F metric from 76.3% to 80.8% and the Dice score from 80% to 84.9% in the EndoVis18 dataset. This significant enhancement underscores the effectiveness of our fine-tuning strategy in conditions that closely mimic clinical practice, further validating SurgSAM-2's capability to deliver precise and consistent segmentation in practical medical applications."}, {"title": "Comparative Model Evaluation", "content": "We compared SurgSAM-2 against the state-of-the-art methods specifically designed to surgical instrument segmentation and some advanced SAM-based methods, utilizing the Challenge IoU metric from EndoVis18 to assess performance. Results of other methods are quoted from their papers [14]. Note that a completely fair comparison cannot be achieved, as most of these existing methods do not require prompts in inference. Additionally, most methods for instrument segmentation are designed for type segmentation, which does not need to distinguish different instances of the same type, though this is one of the most challenging problems in surgical instrument segmentation. Our method aims at a more practical setting to segment instruments on an instance level. We also compared the vanilla SAM2 in this setting and listed all the results for the EndoVis18 dataset in Table 4.\nSimilar to the performance in other evaluation matrices, we can see SurgSAM-2 can consistently deliver superior results than the vanilla SAM2 in Challenge IoU, whether it is provided with detailed prompts (Full Mask) or more sparse prompts (1 Point and 5 Points). We also find that with the challenging instance-level setting, our method with mask prompt can achieve competitive IoU results, compared with these task-specific methods. More importantly, SurgSAM-2 manages to achieve significant improvements in FPS and memory efficiency, superior to all the other methods.\nIn real-world surgical environments, both performance and computational demands need to be carefully considered. Given promising segmentation precision, coupled with high efficiency, our SurgSAM-2 shows its great potential to facilitate the applicability of AI models in surgical deployment. Its consistent high performance across different prompting levels makes it a practical choice for integration into medical imaging workflows, providing surgeons with reliable and real-time segmentation results under various conditions."}, {"title": "5 Conclusion and Future Work", "content": "In conclusion, our proposed model SurgSAM-2 represents a significant advancement in the domain of surgical video segmentation. By integrating an EFP mechanism with the robust SAM2 framework, SurgSAM-2 successfully addresses the challenges of real-time surgical video processing, enhancing both efficiency and accuracy. The ability of SurgSAM-2 to selectively retain the most relevant frames based on cosine similarity has reduced memory usage while simultaneously improving the model's segmentation performance across various tasks.\nOur comprehensive evaluations on the EndoVis17 and EndoVis18 datasets demonstrate that SurgSAM-2 consistently outperforms the vanilla SAM2 model, offering superior processing speed and reduced computational demands without compromising on accuracy. These results suggest that efficient memory management is crucial for advancing video segmentation in resource-constrained environments, particularly in the high-stakes context of surgical interventions.\nLooking ahead, future research will focus on refining EFP strategies and experimenting with different memory bank sizes to identify the optimal configuration that maximizes both efficiency and segmentation accuracy. Additionally, we plan to expand the evaluation of SurgSAM-2 across more diverse and complex datasets, further validating its robustness and applicability in various surgical contexts. By continuing to explore and integrate more sophisticated memory management techniques, we aim to push the boundaries of what is possible in real-time video analysis, not only within the medical field but also in broader applications that require rapid and accurate video segmentation."}]}