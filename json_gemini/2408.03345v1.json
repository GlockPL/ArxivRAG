{"title": "ARTIFICAL INTELLIGENCE AND INHERENT MATHEMATICAL DIFFICULTY", "authors": ["WALTER DEAN", "ALBERTO NAIBO"], "abstract": "This paper explores the relationship of artificial intelligence to the task of resolving open questions in mathematics. We first present an updated version of a traditional argument that limitative results from computability and complexity theory show that proof discovery is an inherently difficult problem. We then illustrate how several recent applications of artificial intelligence- inspired methods respectively involving automated theorem proving, SAT- solvers, and large language models do indeed raise novel questions about the nature of mathematical proof. We also argue that the results obtained by such techniques do not tell against our basic argument. This is so because they are embodiments of brute force search and are thus capable of deciding only statements of low logical complexity.", "sections": [{"title": "1. INTRODUCTION", "content": "We write at a time when many endeavors are being called upon to reflect on the challenges brought about by recent developments in artificial intelligence. It"}, {"title": "2. THE PROOF DISCOVERY PROBLEM", "content": "Here is an initial formulation of our main thesis:\n(No) Advances in computing theory or technology \u2013 inclusive of those currently understood to exemplify artificial intelligence \u2013 will not radically alter the following aspect of our current understanding of mathematics and its practice: Settling the status of open questions is an inherently difficult problem.\nIn this section we will clarify this claim relative to what we refer to as the proof discovery problem and explain why we take this to be a central aspect of mathematical practice. This requires also clarifying what we mean by \"open questions\" and \"inherent difficulty\".\nBy an open question we mean a mathematical statement for which we currently lack a proof or a refutation which we take to establish that the statement in question is true or false. This notion is readily exemplified by what are often called \"sub- stantial\" or \"non-trivial\" open questions of the sort exemplified by those appearing on lists such as the Millennium Problems (Carlson et al., 2006) or those of Hilbert (1900), Landau (1912), Smale (1998), or Nash & Rassias (2016). Such collections include statements with elementary number-theoretic formulations (the existence"}, {"title": "", "content": "PROV = {\u03c6\u2208 Lr : FFs y}"}, {"title": "3. THE BASIC ARGUMENT", "content": "The story of artificial intelligence is often presented as beginning in the late 1950s with the work of figures such as Newell & Simon, Wang, Davis, and Putnam which we will discuss in the next section. Already at this time, the prospects for applying computing technology to proof discovery in mathematics were widely discussed. It was in the context of these developments that the optimistic predictions illustrated by the second and third quotes in the epigraph began to be announced.\nProgress was slow throughout the 1970s, during which the phenomenon of NP- completeness was discovered. This is now regarded as a characterization of an intractable (or inherently difficult) problem \u2013 a concept which had come into greater focus during the 1960s along with the availability of digital computers and the desire to apply them to problems of practical import. This class includes many classic examples about planning and search which originated in artificial intelligence. It was within this context that the phrase \"combinatorial explosion\" was coined by Lighthill (1973) in the course of presenting a form of the argument \u2013 which was then formulated more incisively by Rabin (1974) we are about to consider.\nThese observations contributed to the first of the so-called \"AI winters\". We are evidently living in the midst of a subsequent summer. Nonetheless, our contention is that the following updated form of the Lighthill-Rabin argument remains relevant for the prospects of artificial intelligence in mathematics:\n(P1) i) Proof discovery is a central goal of mathematical practice.\nii) The difficulty of proof discovery is accurately measured by the classi- fication of the decision problem ProVs with respect to the hierarchies of computability theory and computational complexity theory.\n(P2) For a wide range of relevant choices of \u0393 and Es, the problem PROVE is of high computational complexity in the sense of (P1.ii).\n(P3) If a problem is of high computational complexity in the sense of (P2), then it is inherently difficult (or intractible) to decide arbitrary instances of it using computing hardware which we can construct and apply in practice.\nBy combining P1, P2, and P3 we reach a restatement of our original thesis:"}, {"title": "3.1. Considerations from computability theory", "content": "An initial observation un- derlying (P2) is that for the most evident choices of \u0393and \u251cs, the decision problem PROV is formally undecidable. This is so because of familiar considerations sur- rounding G\u00f6del's First Incompleteness Theorem and Church and Turing's negative solution to the original Enstscheindungsproblem for first-order logic. But it will still be useful to collect together several related extensions as follows:\nTheorem 1. Suppose that a) Es is a computably enumerable derivability relation which extends that of classical first-order logic [FOL] and b) \u0393 is a consistent, computably axiomatizable theory that interprets Q (i.e. Robinson arithmetic) and which is additionally \u2211\u2081-sound relative to such an interpretation. Then:\nS\ni) \u0393 is incomplete relative to the definition of derivability Es. In particular, there will exist Lr-sentences y such that FH \u00a2 and FH \u00ab6. Furthermore, it is possible to find such y which are provably equivalent I-statements in the language La of first-order arithmetic.\nii) PROV is not \u25b3\u2081-definable and thus formally undecidable as a decision problem.\niii) PROVE is E-complete relative to many-one reductions.\nFor the sort of consensus choices for I and Es for formalizing core mathematics envisioned in \u00a72, conditions a) - b) will typically satisfied in a paradigmatic man- ner."}, {"title": "", "content": "PROV = {[4] \u2208N :\u2203xProof(x,\u310f\u03c6\u00af)}"}, {"title": "3.2. Considerations from complexity theory", "content": "The foregoing points are famil- iar and were appreciated early on. But the potential applications of artificial intelligence to proof discovery require us to proceed more carefully in arguing for (P3). This is so in light of at least two concerns. First, at least one of the applica- tions we will consider in \u00a74 pertains to choices of Iands which do not directly fall within the scope of Theorem 1. Second, one might also think advances in com- puting technology which will themselves be brought about by artificial intelligence will make the sub-argument (P3.1-4) obsolete."}, {"title": "3.3. On reasonable models and artificial intelligence", "content": "Before concluding our defense of (P3) \u2013 and thus of the basic argument itself one other caveat should be noted. When we transition from understanding \"difficulty\" from the perspective of computability theory to that of complexity theory, more care also needs to be taken in regard to the models of computation to which the (putatively) limitative theorems apply. For to read off concrete consequences about what we can and cannot prove in practice from Theorems 2 and 3 we need to restrict attention to the narrower class of reasonable models of computation in order to employ a complexity-theoretic analogue of (P3.3).\nThe notion of such a model plays a heuristic role in complexity theory similar to that played by an effective procedure in the early history of computability theory. Just as computability theorists have come to accept the equation of the class of problems decidable by effective procedures with those computable by a Turing machine (i.e. Church's Thesis), complexity theorists have come to accept a similar equation of the class of feasibly (or practically) decidable problems with those in the class P decidable in polynomial time by a deterministic Turing machine with a suitably efficient representation of inputs and outputs (i.e. the Cobham-Edmond's Thesis). This leads to the following characterization of a reasonable model in the form of what van Emde Boas (1990) originally called the Invariance Thesis: a model M is reasonable just in case for every machine M\u2208 M there exists T\u2208 I which simulates M with polynomial time overhead and constant space overhead.\nThe class of reasonable models is again robust and includes the RAM-like models on which the architecture of contemporary digital computers are based. There are, however, generalized finitary models (in the sense of P3.2) which are not reasonable. A paradigm example of is the so-called Parallel RAM model P which allows a single processor to spawn a fixed finite number of successor processors sharing a common memory at each step. This model has been of theoretical interest in complexity theory as it facilities the study of problems which can be efficiently decided by parallelism of bounded depth."}, {"title": "4. CASE STUDIES", "content": "To reiterate, the crux of the basic argument is that the proof discovery problem in mathematics is (provably) computationally complex and thus inherently difficult. But again, this argument concerns proof discovery understood as a decision problem with infinitely many instances. On this understanding, it seems there is little room to challenge the argument on its own terms. But we would be drawn to question its relevance if the specific cases we cared about in practice ended up being significantly easier than the worst case reported by results like Theorems 1, 2, and 3. It would, for instance, be highly germane if automated techniques already had resolved a longstanding open question or appeared to be on the cusp of doing so.\nAt the time of writing, we are unaware of any instances in which the application of computing technology can be reasonably credited with resolving a high profile open question of the sort described in \u00a72. But there been several instances of lower profile results obtained in part or whole by the use of artificial intelligence-related methods. The best known examples to date have been obtained via traditional automated theorem proving techniques. But there has also been a recent success employing SAT-solvers and large language models in a manner reminiscent of ap- plications which are currently fueling public debates."}, {"title": "4.1. Automated theorem proving and the Robbins Problem", "content": "Artificial in- telligence and automated theorem proving have been connected since their begin- nings. For instance at the 1956 conference where the expression \"artificial intelli- gence\" was coined, the only functioning system presented was the Logic Theorist of Newell, Shaw, and Simon (1957). This was a program for proving statements of propositional logic in a manner intended to mimic how humans approached logic problems. But as this method was incomplete, Newell, Shaw and Simon them- selves described it as heuristic and contrasted it with algorithmic methods which are complete but need to return an output in a feasible number of steps.\nThis situation was further addressed by Wang (1960) who developed a sound and complete method for propositional derivability but abandoned the goal of model- ing human reasoning. Wang's approach was based on an operationalization of Gentzen's sequent in which the rules are both cut free and invertible. This makes it possible to systematically perform proof search by working backwards from the main connective of the statement to be proved. Wang's method set the stage for the formulation of the Davis-Putnam algorithm which serves as the basis of con- temporary SAT-solvers of the sort discussed in \u00a74.2. But this method is itself based on the yet more fundamental rule of resolution which also remains at the core of most automated theorem proving systems for first-order logic.\nRecall first that a propositional formula o can be efficiently transformed into a logically equivalent formula 1 in conjunctive normal form i.e. so that 41 is a conjunction of clauses each comprised of a disjunction of literals which are themselves either atoms por negated atoms p. If 1 contains distinct clauses of the respective forms Vp and pVX, then any valuation which makes 41 true must also make & V X true. This allows us to obtain a simpler formula 42 which is still equivalent to in terms of satisfiability by removing all occurrences of p from 41. This idea is captured by the resolution rule:"}, {"title": "", "content": "\u03c6\u03bd \u03c8"}, {"title": "", "content": "\u00ac(\u00abx \u2228 \u00acy)"}, {"title": "4.2. SAT solvers and finite combinatorics", "content": "Recall that SAT denotes the satis- fiability problem for propositional logic - i.e. that of checking if a formula is true in some row of its truth table. A SAT-solver is a decision algorithm satisfying:\ni) Totality: \u03b1(\u03c6) returns output 1 - i.e. \"satisfiable\" or 0 - i.e. \"unsatisfiable\" - for all propositional formulas 4.\nii) Soundness: If \u03b1(\u03c6) = 1, then y\u2208 SAT.\niii) Completeness: If \u03b1(\u03c6) = 0, then \u00a2 \u00a3 SAT \u2013 i.e. \u03c6\u2208 SAT.\nRecall also that if is unsatisfiable i.e. false in all rows of its truth table then is a tautology and thus provable from no premises in the propositional calculus (by the completeness theorem). As such, an algorithm a satisfying (4i-iii) also serves as a decision method for PROVOL.\nAs we have discussed, SAT is NP-complete. Presuming that P \u2260 NP, there thus cannot exist an algorithm satisfying (4i-iii) implementable by a reasonable model of computation which always returns an output in time polynomial in the number of propositional variables in 4. And in fact no procedure with better than exponential worst case running time in the general case is known to exist. It is thus striking that a number of algorithms which perform well on large classes of SAT instances are currently being investigated.\nMost contemporary SAT solvers are refinements of the so-called Davis-Putnam algorithm. This is itself a recursive implementation of the method of resolution described in \u00a74.1 together with a strategy for selecting a literal on which to apply a rule known as unit propagation. However this is not a fully explicit algorithm. Specific SAT solvers thus rely on a combination of heuristics for choosing literals according to the properties of the input formula or backtracking when conflicts are found. Appropriate choices of this sort lead to algorithms which reduce the overall number of assignments which must be considered in large classes of cases.\nThe applicability of SAT solvers to proof discovery arises due to the possibility of using propositional logic to express mathematical statements either directly or as a parameterized family which is successively checked by an enumerative search. The process can be illustrated by examining one of the successful applications of such algorithms to answer an open question - i.e. the recent proof of a statement"}, {"title": "", "content": "Ta,b,c = (xa V x V xc)^(-xa \u2228 \u00acx -xc)"}, {"title": "", "content": "\u1e9em = (a,b,c)\u2208Tm Ta,b,c"}, {"title": "4.3. Large language models and the cap set problem", "content": "It is a commonplace that a distinction should be drawn between automated theorem proving and ma- chine learning techniques. But such a classification falls short of a precise character- ization of the difference between what we will call logic-based and statistics-based methods. This in turn complicates systematically addressing the topical question: How might the latter class novelly contribute to proof discovery beyond the former?\nIn approaching this we may also distinguish between what might be called sys- tematic and non-systematic applications of statistic-based methods to proof discov- ery. The former seek to find applications of machine learning within the framework of automated theorem proving surveyed in \u00a74.1. This is exemplified by attempts to refine the so-called method of hammers - i.e. general purpose techniques to fill in gaps in proofs by consulting libraries of previously formalized theorems in a manner reminiscent of expert systems (see, e.g., Blanchette et al., 2016).\nSuch libraries are now sufficiently large to support an ongoing project to improve upon the performance of hammers by applying machine learning to the so-called premise-selection problem \u2013 i.e. that of proposing candidate lemmas to prove a give target theorem based on the statistical properties of the linguistic structure of the theorem they contain within a corpus of previously formalized proofs. Techniques of this sort have become increasingly successful in improving the performance of automated theorem provers in rediscovering fully automated proofs of previously verified theorems. But at the time of writing (and to the best of our knowledge) this method has not been used to resolve any standing open questions.\nThis stands in contrast to the use of machine learning which has recently figured in what is the first successful application of a paradigmatic statistics-based method to proof discovery (again to the best of our knowledge). This comes in the form of using a large language model to aid in the discovery of a structure known as a cap set which improved upon known lower bound results. We will again summarize the mathematical result before describing the methods which were recently used by Romera-Paredes et al. (2024) to obtain it in regard to (C1) and (C2)."}, {"title": "5. CONCLUSION", "content": "The argument of \u00a73 concludes that proof discovery in mathematics is inherently difficult i.e. resistant to automation using techniques which can be carried out using computing machinery which can be constructed and and applied in practice.\nOn the other hand, the case studies reported in \u00a74 have been repeatedly reported as breakthroughs for the application of artificial intelligence in mathematics. But as we can now see, these are also of exactly the form which one would have predicted before the fact were amenable to automated resolution. In particular, the state- ments obtained can all be formulated as statements of low logical complexity. Thus not only do we known in advance that they are amenable to brute force search, but this is in fact how the demonstrations in question proceeded.\nBrute force, is of course, also a method for which we have already ample evidence that computers outperform human mathematicians. This is aptly illustrated by traditional \"computer proofs\" which do not involve techniques directly associated with artificial intelligence. As such, our case studies also suggest that while the role of techniques like SAT-solvers and machine learning may come to play a larger role in mathematics, their influence will remain evolutionary rather than revolutionary.\nWe will now adduce two additional observations refining our prior claim that the examples of \u00a74 are indeed special cases of proof discovery:\n(C1') Brute force search is not characteristic of how longstanding open questions in mathematics have historically been resolved.\n(C2') The logical complexity of many open questions is greater than 21.\nAn initial observation in regard to (C2') is that the natural formalization of the longstanding open questions consider in \u00a72 which are related to number theory is at least I and often I2. This is paradigmatically true of statements like the Goldbach and Twin Prime conjectures which can be directly expressed in language of first- order arithmetic. But it also known to be true of statements like the Riemann Hypothesis or the P\u2260 NP which can also be shown to be equivalent to statements of La as a consequence of additional lemmas and standard coding techniques.\nThe fact that these questions have remained open sometimes for centuries suggest that we should at least not dismiss as naive the hypothesis that the logical complexity of mathematical statements provides a prima facie indication of their discovermental complexity. Of course this invites (at least) the following questions:\nIs logical complexity - e.g. as measured in the familiar manner of quantifier al- ternations an intrinsic feature of mathematical propositions? If so, how do we determine the appropriate signature in which to formalize a proposition so that its complexity may be read off? Are such assignments absolute or relative to a base"}]}