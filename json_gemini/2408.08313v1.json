{"title": "Can Large Language Models Understand Symbolic Graphics Programs?", "authors": ["Zeju Qiu", "Weiyang Liu", "Haiwen Feng", "Zhen Liu", "Tim Z. Xiao", "Katherine M. Collins", "Joshua B. Tenenbaum", "Adrian Weller", "Michael J. Black", "Bernhard Sch\u00f6lkopf"], "abstract": "Assessing the capabilities of large language models (LLMs) is important to be able to characterize what they know (and don't) and how they can be appropriately used. Yet, capability assessments are often challenging, in part, because it is hard to find tasks to which they have not been exposed during training. We take one step to address this challenge by turning to a new task: focusing in on symbolic graphics programs, which are a popular representation for graphics content that procedurally generates visual data. Large language models (LLMs) have shown exciting promise towards program synthesis, but do they \u201cunderstand\" symbolic graphics programs? Unlike conventional programs, symbolic graphics programs can be translated to graphics content (e.g., 2D images, 3D geometry). Here, we characterize an LLM's \"understanding\" of symbolic programs in terms of their ability to answer questions related to the graphics (spatial) content. This task is challenging as the questions are difficult to answer from the symbolic programs alone \u2013 yet, they would be easy to answer from the corresponding graphics content as we verify through a human experiment. To understand symbolic programs, LLMs may need to possess the ability to \"imagine\u201d how the corresponding graphics content would look without directly accessing the rendered visual content. We use this task to evaluate LLMs by creating a large benchmark for the semantic understanding of symbolic graphics programs. This benchmark is built via a novel usage of program-graphics correspondence, hence requiring minimal human efforts. We evaluate both commercial and open-source LLMs on our benchmark to elucidate a preliminary assessment of their ability to reason about visual scenes from programs. We find that this task well distinguishes existing LLMs and models that are considered good at reasoning perform better. Lastly, we introduce a way to improve this ability \u2013 Symbolic Instruction Tuning (SIT). Specifically, we query powerful vision-language models (e.g., GPT-4O) with questions and images generated by symbolic programs. These program-question pairs are collected as our instruction dataset which is then used to finetune an LLM. With a small amount of data, we find that SIT can improve the understanding of LLMs regarding symbolic graphics programs. Assessing how well models \u201cunderstand\u201d symbolic graphics programs offers new possibilities for LLMs to perform visual reasoning. Finally, we showcase such possibilities in generic instruction tuning.", "sections": [{"title": "1 Introduction", "content": "What are large language models (LLMs) capable of? Recent studies [4, 56] have shown that LLMs are able to generate generic computer programs, indicating a certain level of understanding about the symbolic structure of programs. Motivated by such progress, we focus on another important family of computer programs, called symbolic graphics programs, where a graphics content (e.g., image, 3D asset) can be generated by running a program. We are interested in the following question: Can large language models \u201cunderstand\u201d symbolic graphics programs?"}, {"title": "3 Semantic Understanding of Symbolic Graphics Programs", "content": "We introduce the task of semantic symbolic graphics program understanding. Our goal is to assess to what extent a LLM is able to \"understand\" a symbolic graphics program, which may begin to belie some latent capability to \"visually imagine\". Specifically, we leverage the correspondence between deterministic symbolic graphics programs and rendered images, and then we characterize the understanding of symbolic graphics programs as the semantic understanding of the corresponding rendered image. To do so, we use the performance on question-answering to evaluate the semantic understanding of images. The same set of questions, along with the corresponding symbolic graphics programs, are then used to evaluate the symbolic program understanding of LLMs (the rendered image will not be used here). Figure 2 gives an illustration of symbolic graphics program understanding. The intuition behind this evaluation is that, if an LLM has a good sense of the symbolic graphics and implicit de-rendering, then the LLM should have a rough understanding about its rendered image such that it is able to answer arbitrary semantic questions regarding this rendered image.\nSymbolic graphics program understanding can also be viewed as a form of visual question answering in the sense that visual input is represented by a symbolic program representation. Compared to existing vision-language models [53, 54, 119] that encodes images with a text-aligned neural encoder [74], our paper considers the case where the visual input are encoded by a symbolic program that can exactly recover the graphics content. From this perspective, our task aims to study and uncover the potential of using symbolic programs as a representation to perform visual reasoning."}, {"title": "4 SGP-Bench: A Benchmark for Symbolic Graphics Program Understanding", "content": ""}, {"title": "4.1 Dataset Creation Pipeline", "content": "To construct our benchmark, we need questions about a symbolic program based on its rendered image. To build a large benchmark, it is essential to consider how we can effectively scale up the question collection with minimal human efforts. To this end, we use a powerful vision-language model (e.g., GPT-4O) to generate semantic questions based on the rendered images, and then we inspect them manually to make sure that these questions are reasonable and the answer to them is correct. We also run a human study over a randomized set of 500 of the automatically generated questions along with the corresponding images, and find high agreement (see Appendix B). The overall procedure for our dataset creation is given in Figure 3. In this pipeline, the rendering of symbolic programs and the GPT-4O querying are both scalable and can be done with minimal human involvement. Human annotators then inspect the generated question-answer pairs based on the rendered image, which requires much less efforts than manually writing questions and answers. We emphasize that this program-question-answer triplet creation method is general, as it works for most of the symbolic graphics programs. SVG and 2D CAD programs can directly produce 2D images, so it is straightforward to use this pipeline. For 3D CAD programs, they produce 3D models and we first render them into 2D images with a few fixed camera positions. These rendered images from different views are then combined together to query GPT-4O, and the following procedures are identical to the SVG case."}, {"title": "4.2 Benchmarking Semantic Understanding", "content": "SVG dataset statistics. We collect in total 1,085 SVG programs covering 19 categories, and each program has 4 semantic multiple-choice questions (with 4 options). We make sure that answers are evenly distributed across 4 options. Detailed dataset statistics are in Figure 4(a). Our SVG benchmark consists of 5 different types of questions, including \u201cSemantic\": 1,085 questions, \u201cColor\": 864 questions, \"Shape\": 1,217 questions, \u201cCount\u201d: 819 questions, and \u201cReasoning\": 355 questions. \"Semantic\" tests the global semantic meaning of the object represented by SVG codes, while the other four question types focus on detailed, local understanding of the object. \u201cColor\u201d is color-related questions about specific object parts, which evaluates the localization of the corresponding semantic part. \"Count\" is about counting the occurrences of certain patterns or semantic parts. \"Shape\" is about the shape of certain parts of the object, either finding the exact definition of the geometric shape or finding geometric shapes that resemble the object part.\nCAD dataset statistics. We collect 2, 400 CAD programs from three different datasets [84, 97, 101]. The CAD dataset consists of 1000 programs from DeepCAD [101], which forms the 3D subset; 700 programs from the Fusion360 Reconstruction Dataset [97], which constitutes the 3Drecon. subset; and 700 programs from SketchGraphs [84], which makes up the 2D subset (as shown in Table 1). Depending on the data, we either render one image or two images from two opposite isometric views (i.e., front and back). Then we feed the images to GPT-4O and generate one semantic multiple-choice question (with 4 options) and its answer. This gives us 2, 400 questions in total. We make sure that ground truth answers are evenly distributed across 4 options. Detailed dataset statistics are given in Figure 4(b). Some examples from our CAD dataset are provided in Figure 5.\nExperimental results and discussion. We find that graphics program \"understanding\u201d, as we operationalize it here, is challenging. The average accuracy of all models (proprietary and open-source) is below 65% (ranging from 31% to 63%) on SVG and below 80% (ranging from 29% to 78%) on CAD. Among this, SVG makes it even more difficult for the models to understand as these 2D graphics contain richer semantics. However, the models have substantial performance gains in accordance with the scaling law [111], and this increase, with larger model size, is observed across"}, {"title": "4.3 Benchmarking Semantic Consistency", "content": "Large language models (LLMs) are exposed to vast amounts of online data, including open-access SVG data. To investigate whether their semantic understanding ability is due to potential data leakage, we propose a semantic consistency test by introducing global translations or rotations to SVG graphics, ensuring SE(2) invariance. Such spatial interventions fundamentally alter the code representation, as SVG graphics consist of lines and Bezier curves with anchor points, and SE(2) operations change all numerical values in the code. However, the SVG's semantics\u2014such as shape or color\u2014remain unaffected by this perturbation. This allows us to examine how LLMs behave when the same vector graphics are presented with drastic code-numerical changes (see Appendix A.1 for more details). If the model maintains the performance consistency under these perturbations, it may suggest that the semantic understanding is based on a fundamental level of comprehension or visual imagery rather than a trivial memorization of the code.\nDataset specifics. We use our SVG dataset to evaluate the semantic consistency with respect to translation and rotation. For each SVG sample, we randomly choose 5 different translations (T) and rotations plus translations (SE(2), harder case), resulting in a visually small amount of spatial shifts of the rendered object, meaning nearly no changes in semantics, but complete change in SVG code"}, {"title": "4.4 Prediction Entropy of LLMs and Humans", "content": "To evaluate the consensus of different LLMs, we compare the average prediction entropy on 500 symbolic programs using GPT-4O, LLama3-8B, LLama3-70B, Mistral-7B, Yi-1.5-34B, Gemma-1.1-7B and Qwen-1.5-72B. We also conduct a human experiment on the rendered images of these programs and collect the answers (each question has at least 5 participants, see Appendix B). Figure 7 show that humans have strong consensus when answering questions based on images. In comparison, LLMs show low consensus when answering questions based on symbolic programs. This implies that LLMs might have different inner working mechanisms to understand symbolic programs. We are excited by future work to better investigate these differences."}, {"title": "5 Symbolic Instruction Tuning: Boosting Symbolic Program Understanding", "content": "Generating symbolic instruction data. Inspired by how visual instruction tuning [54] enables large vision-language models to understand images with visual-question-answering (VQA) data, we design a new method to perform symbolic instruction tuning for LLMs to take steps to better bridge the gap between semantic understanding and symbolic reasoning over the graphics programs. While to our knowledge there does not exist any semantic instruction-following datasets directly over symbolic programs, these symbolic graphics programs can be rendered into 2D images. With these images"}, {"title": "6 A Challenge for SVG Program Understanding", "content": "Is it really easy to answer semantic reasoning questions over symbolic graphics programs? We provide an intriguing experiment to demonstrate that SVG programs can be quite difficult for LLMs to understand such that even if the corresponding rendered images are fairly easy for humans to recognize, all these powerful LLMs still fail dramatically, only reaching a chance-level accuracy.\nSpecifically, we construct symbolic graphics programs that can produce MNIST-like images, as shown in Figure 9. The symbolic programs that can render MNIST-like images are quite simple, typically only containing 1 or 2 path operations. We introduce the SGP-MNIST challenge where we have 100 symbolic graphics programs per digit (in total 10 digits: 0-9). Therefore, we have 1,000 symbolic graphics programs in total. For each problem, we ask one multiple-choice question (with 10 options of digit 0-9): which digit does the SVG program represent? The results are given in Table 3. Despite the fact that these rendered images are quite easy for human to recognize, the corresponding symbolic programs turn out to be difficult for LLMs. Even the powerful GPT-4O can only achieve an accuracy slightly higher than the chance-level. This experiment demonstrates that the kind of symbolic graphics program understanding we explore here can be quite challenging in general, even if some LLMs might seem to work well in some cases. The results suggest that how LLMs understand SVG programs is very different from how humans understand images; better understanding similarities and differences in human and machine reasoning is important if we are to build systems that can appropriately work with us [14]."}, {"title": "7 Improving Generic Instruction Tuning with Symbolic Graphics Programs", "content": "We test the instruction-tuned models on a variety of popular LLM benchmarks, including benchmarks focusing on language understanding (XNLI [15], IFEval [117], HellaSwag [115], C-Eval [37], CoQA [78], MMLU [33],\nSQUAD2.0 [75]), generic reasoning (BigBenchHard [89], PIQA [6], AGIEval [116]) and mathematical problem-sovling (Arithmetic [8], MathQA [3], GSM8k [12], ASDiv [69]). Specifically, we use the Llama-3.1-8B base model (without any instruction tuning), and then the baseline is finetuned with the Open-Instruct dataset that contains 143K question-answer pairs (details are given in Appendix E.1). We evaluate the effectiveness of using SIT data to improve generic instruction tuning performance by mixing additional SIT data into the Open-Instruct data. Moreover, our SIT data can also be used in a reverse fashion (rev-SIT), i.e., rephrasing the answer as the new question and the question as the new answer. See the comparison between original and reverse SIT data in Figure 10. We test three ways to using SIT data: (1) mixing original SIT data into the Open-Instruct data; (2) mixing the reverse SIT data into the Open-Instruct data; (3) mixing both original and reverse SIT data into the Open-Instruct data. The results are given in Table 4. We can observe that mixing SIT data can generally improve the instruction following ability, suggesting that understanding symbolic graphics programs requires a combination of multiple reasoning abilities and training on SIT data can improve these abilities. More interestingly, we find that the reverse usage of SIT data (i.e., symbolic graphics program generation) can improve a set of reasoning abilities that are complementary to symbolic graphics program understanding. The mixture of both original and reverse SIT data can greatly improve the instruction following ability of LLMs and achieve better performance than the Open-Instruct baseline, the Open-Instruct + SIT baseline and the Open-Instruct + rev-SIT baseline."}, {"title": "8 Concluding Remarks", "content": "We propose a new task over which to evaluate LLMs to contribute towards a multi-faceted assessment of their capabilities: the task of \u201cunderstand\u201d images \u2013 without image; that is, directly from graphics programs. For such a task, we create SGP-Bench, a benchmark that shows distinguishable results between LLMs and introduce the method of Symbolic Instruction Finetuning (SIT) that improves LLMs' capability of understanding graphics programs. Our work helps deepen our collective understanding of what LLMs can and cannot do, and hopefully inspires a broader diversity of evaluation tasks. However, our work only represents the beginning of symbolic graphics program understanding for LLMs. The inner working mechanisms of such semantic understanding of LLMs and how to improve this ability remain to be explored."}, {"title": "A Benchmark Details", "content": "We adopt implementations from other projects to build our SGP-Bench. We follow the implemntation of MathVista\u00b9 for querying GPT or open-sourced Llama3.1-8B and perform LLM-based answer extraction, vLLM\u00b2 for efficient model inference and simple-evals\u00b3 for a unified benchmarking framework.\nOur data license follows the license of the original license of our data source."}, {"title": "A.1 Data preparation", "content": "SGP-Bench (SVG). Our SVG data are sampled from kaggle SVG Icons dataset\u2074 and we build our SGP-Bench (SVG) using text prompts from F.1. The original data from kaggle SVG Icons is crawled from SVGrepo\u2075. The website is merely an aggregator, so it follows that any content on it must at least be licensed permissively enough for SVGrepo to distribute it, and therefore it is acceptable to distribute as part of a collection in our benchmark. Refer to the SVGrepo license for further details.\nSVG Invariance: We use beautifulsoup and SvgLib to process the SVG XML code to perform translation and rotation perturbations for the invariance investigation, a visual sample can be found in Fig. 14. Specifically, as we assume that all XML elements in each SVG figure do not possess any \"transform\" attribute as it complicates the augmentation process. For elements that can be fully specified by coordinates (e.g., <rect>, <polygon>), we perform augmentation by perturbing these coordinates. For <path> elements in which path information is fully specified in \"d\" attributes, we first turn all relative operations (e.g., \"1 2 3\", meaning that draw a line from the current position (x, y) to the position (x + 2, y + 3)) into absolute ones, and later perturb the coordinates but not other path attributes. As mentioned in the main paper, small spatial perturbations can drastically change the numerics of the SVG XML code (See Section C for more details).\nSGP-Bench (CAD). Our CAD (3D) sequences data are sampled from DeepCAD [101] datasets, which contains around 180k manually constructed CAD sequences that are originally from the ABC dataset [45]. We manually sample 1000 sequences and use pythonocc (OpenCASCADE) to verify and normalize these CAD sequences, then render the front and back view of the 3D CAD models. Since all the CAD models are from the OnShape platform, the copyright of the CAD models is owned by their creators. For licensing details, see Onshape Terms of Use 1.g.ii. Our CAD (3Drecon.) sequences are sampled from Fusion360 Reconstruction Dataset [97] datasets, which contains 8,625 sequences, with more complex curve operations for constructing sketches. Our CAD (2D) sequences are sampled from SketchGraphs [84] dataset, which consists of 15 million sketches extracted from real-world CAD models. The major difference is it consists of 2D CAD sketches without Extrusion operations.\nSGP-MNIST. The MNIST SVG data is sampled from the kaggle MNIST-SVG dataset\u2076. We randomly sample 1000 samples from MNIST-SVG (100 samples per digit category) to build our SPG-MNIST benchmark. The data comes with CC BY-SA 3.0 license."}, {"title": "A.2 Evaluation protocol", "content": "Inference. To conduct massive-scale evaluation (8000+ question samples with 16 models), we leverage vLLM\u2077 to perform high-throughput and memory-efficient inference for all the open-source LLMs. And we use OpenAI API for evaluating different variants of the GPT models. We deploy the vLLM inference engine as a server that also uses the same format as OpenAI API to achieve a unified testing framework for both GPT and all open-source models. The vLLM inference engine is deployed on a node with 8 NVIDIA H100 80G GPUs."}, {"title": "A.3 Evaluated model specs", "content": "Here we list the model details of all LLMs we evaluated in the SGP-Bench, their performance are demonstrated in the table 2. Generally, we evaluated 3 types of LLMs, the representative open-sourced LLMs from tech giants and start-ups, the code-specific LLMs that were built for code generation and understanding and the strongest proprietary models from the GPT and Claude family."}, {"title": "A.3.1 Open-sourced LLMs", "content": "Gemma-1.1-2B/7B Gemma is a suite of lightweight, advanced open models created by Google DeepMind and other teams across Google, it's the best performing model at it class when it's released on Feb 21, 2024. Primarily designed for text generation, Gemma models come in multiple sizes, i.e. 2B / 7B, to fit various computing resources and deployment needs. The models are trained on 3T (2B) / 6T (7B) tokens of primarily-English data from web, mathematics, and code. It is based on a transformer decoder with context length of 8192 tokens. It leverages Multi-Query Attention, RoPE Embeddings, GeGLU Activations and RMSNorm. The Gemma models are using architectures, data and training recipes inspired by the Gemini model family. The models are available in two versions: a pretrained version and an Instruction-tuned version, the latter refined through human language interactions to perform well in conversational roles, similar to a chat bot. We only test and perform the symbolic instruction tuning on the Instruction tuned version.\nMistral-0.3-7B The Mistral-0.1-7B from Mistral AI is released September 27, 2023 and marked as the best 7B model at the time. The 0.1 version model features a 8k context window, with Grouped-query attention (GQA) for faster inference and SWA for handling longer sequences more effectively at a reduced computational cost. The model is further updated to 0.3 version in May 21, 2024, upgrading its context length to 32k, its vocabulery size and RoPE theta, but the SWA is removed in this version.\nMistral-NeMo and Mistral-Large2 Mistral NeMo is a 12B large language model built by Mistral AI with a context window size of up to 128k tokens. Mistral NeMo is trained with quantization awareness, allowing FP8 inference without any loss in performance. Mistral NeMo uses a new tokenizer, Tekken, based on Tiktoken, which enables more efficient compression of natural language text and source code, compared with previous Mistral model series.\nMistral Large 2 is the new generation of Mistral AI's flagship model, with a model size of 123 billion parameters. Especially, Mistral Large 2 is trained on a very large proportion of code data, resulting in state-of-the-art performance, on par with proprietary models like GPT-4O or Clause Opus."}, {"title": "A.3.2 Code-specific LLMs", "content": "CodeQwen1.5-7B CodeQwen1.5-7B is based on Qwen1.5-7B. It is further trained on 3T tokens of code data, and it also includes group query attention (GQA) for efficient inference.\nDeepSeek-Coder-V2-16B-Instruct DeepSeek-Coder-V2-16B-Instruct is an Mixture-of-Experts (MoE) code language model, that demonstrates comparable performance to GPT-4 Turbo in code-related tasks. Specifically, DeepSeek-Coder-V2 is continued to pre-train on intermediate checkpoint of DeepSeek-V2 with 6 trillion additional tokens, substantially enhance its reasoning capabilities in code and mathematical related tasks. DeepSeek-Coder-V2 supports 338 programming languages and has a context length of 128K.\nCodestral-22b-v0.1 Codestral-22B-v0.1 is the code-specific variant of Mistral-0.1-22B, it's trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash."}, {"title": "A.3.3 GPT family", "content": "GPT-3.5t (GPT-3.5 Turbo) is a text only language model released by OpenAI on November 2022. The specific version of the model we are using is gpt-3.5-turbo-0125. It has a knowledge cutoff of September 2021 and a context window with length of 16K tokens.\nGPT-4t (GPT-4 Turbo) is vision language model launched by OpenAI on March 2023. The specific version of the model we are using is gpt-4-turbo-2024-04-09. It has an updated knowledge cutoff of April 2023 and a context window with length of 128K tokens. It is more powerful than GPT-3.5.\nGPT-4O (GPT-4 Omni) is a multimodal model released by OpenAI on May 2024, which support data types such as audio, vision, and text. The specific version of the model we are using is gpt-4o-2024-05-13. It has similar performance as GPT-4t on English text and code, but with significant improvement on non-English text, i.e., over 50 languages. At the same time, it is able to reason with vision input. GPT-4O has knowledge up to October 2023 and supports context window with length of 128K tokens.\nGPT-4O mini is a multimodal model released by OpenAI in July 2024, which is a more cost-efficient and smaller modal than GPT-4. It has a context window size of 128K tokens and the has knowledge up to October 2023."}, {"title": "A.3.4 Claude family", "content": "Claude is a multimodal, multilingual, proprietary model series developed by Anthropic. The Claude series includes different models: Haiku, the fastest and most lightweight model; Sonnet, the best balanced model between performance and speed; and Opus, the highest-performing model. We did not evaluate Claude 3 Opus because, in June 2024, Anthropic released Claude 3.5 Sonnet, the newest best-performing model.\nSpecifically, we are using claude-3-5-sonnet-20240620 for Claude 3.5 Sonnet, claude-3-sonnet-20240229 for Claude 3 Sonnet, and claude-3-haiku-20240307 for Claude 3 Haiku for benchmark evaluation."}, {"title": "B Human Study Details", "content": "We ran a human study to verify the labels produced by GPT-4O for the benchmark over a subset of 500 stimuli. We recruited 55 participants from the crowdsourcing platform, Prolific [71]. Stimuli were batched into 10 sets of 50 stimuli each. Each participant was randomly assigned a batch of 50 stimuli; stimuli were presented in a random shuffled. On each trial, participants saw the question, original image, and set of multiple choice options. Participants selected an option by clicking a button. We include an example screenshot of a trial in Figure 11. Participants were paid at a base rate of $12.50/hr. They were informed that they could receive a bonus up to $15/hr based on the amount of correct answers they achieved. All participants received the full bonus. Our study was approved by our institutional ethics review board, and all participants provided informed consent. We include the set of instructions and sample screenshots in Figures 12 and 13, respectively. We found high inter-annotator agreement (participants in the same batch had between 0.7 \u2013 0.85 Fleiss Kappa's alpha agreement, where higher implies higher agreement). We find that humans' mode response matched GPT-4o on 90% of the examples (450 of the 500 stimuli)."}, {"title": "CSVG - Invariance Illustration", "content": "Our SVG - Invariance test is essential for testing whether a model has a fundamental understanding of the code, or it is able to pass the benchmark tests due to memorizing the SVG code samples, since we built our SVG-Bench using public available SVG datasets. In Figure 14 we see two SVG codes, illustrating two samples, that are semantically identical. The rotated sample is generated by ourself by applying a SE(2) transformation on the original sample (from SVG Icons). We can see that semantically these two samples are identical, the code changed drastically."}]}