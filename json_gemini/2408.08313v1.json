{"title": "Can Large Language Models Understand Symbolic Graphics Programs?", "authors": ["Zeju Qiu", "Weiyang Liu", "Haiwen Feng", "Zhen Liu", "Tim Z. Xiao", "Katherine M. Collins", "Joshua B. Tenenbaum", "Adrian Weller", "Michael J. Black", "Bernhard Sch\u00f6lkopf"], "abstract": "Assessing the capabilities of large language models (LLMs) is important to be able\nto characterize what they know (and don't) and how they can be appropriately used.\nYet, capability assessments are often challenging, in part, because it is hard to find\ntasks to which they have not been exposed during training. We take one step to\naddress this challenge by turning to a new task: focusing in on symbolic graphics\nprograms, which are a popular representation for graphics content that procedurally\ngenerates visual data. Large language models (LLMs) have shown exciting promise\ntowards program synthesis, but do they \u201cunderstand\" symbolic graphics programs?\nUnlike conventional programs, symbolic graphics programs can be translated\nto graphics content (e.g., 2D images, 3D geometry). Here, we characterize an\nLLM's \"understanding\" of symbolic programs in terms of their ability to answer\nquestions related to the graphics (spatial) content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone \u2013 yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need\nto possess the ability to \"imagine\u201d how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task to\nevaluate LLMs by creating a large benchmark for the semantic understanding of\nsymbolic graphics programs. This benchmark is built via a novel usage of program-\ngraphics correspondence, hence requiring minimal human efforts. We evaluate both\ncommercial and open-source LLMs on our benchmark to elucidate a preliminary\nassessment of their ability to reason about visual scenes from programs. We find\nthat this task well distinguishes existing LLMs and models that are considered good\nat reasoning perform better. Lastly, we introduce a way to improve this ability\u2014\nSymbolic Instruction Tuning (SIT). Specifically, we query powerful vision-language\nmodels (e.g., GPT-4O) with questions and images generated by symbolic programs.\nThese program-question pairs are collected as our instruction dataset which is\nthen used to finetune an LLM. With a small amount of data, we find that SIT\ncan improve the understanding of LLMs regarding symbolic graphics programs.\nAssessing how well models \u201cunderstand\u201d symbolic graphics programs offers new\npossibilities for LLMs to perform visual reasoning. Finally, we showcase such\npossibilities in generic instruction tuning.", "sections": [{"title": "1 Introduction", "content": "What are large language models (LLMs) capable of? Recent studies [4, 56] have shown that LLMs\nare able to generate generic computer programs, indicating a certain level of understanding about the\nsymbolic structure of programs. Motivated by such progress, we focus on another important family\nof computer programs, called symbolic graphics programs, where a graphics content (e.g., image, 3D\nasset) can be generated by running a program. We are interested in the following question: Can large\nlanguage models \u201cunderstand\u201d symbolic graphics programs?"}, {"title": "2 Related Work", "content": "Symbolic graphics programs. Generating visual data by procedural modeling with symbolic\nprograms has been essential to computer graphics since its inception, particularly for 2D shapes and\n3D geometry. They offer a concise, structured, and interpretable representation, attracting significant\nattention in the field - refer to [81] for an overview. Common program types include constructive-\nsolid geometry (CSG) [19, 43, 79, 86, 110], computer-aided design (CAD) [28, 48, 49, 85, 104],\nvector graphics (e.g., SVG) [76, 77], L-systems [31], and customized domains [16, 21, 22, 35, 67, 92].\nAmong these, SVGs, constructed from primitive shapes like vector paths, curves, or polygons\ndefined by mathematical equations, are widely used in artistic applications such as icon creation\nand typography. Central to SVGs is the vector path, providing detailed control over graphics\nand including primitives like rectangles, ellipses, and text. Similarly, procedural 3D geometric\nmodeling, particularly in CAD applications, involves parameterized operations that can be adjusted\nand re-executed to produce updated geometry. Datasets like the ABC dataset [45] and Fusion 360\nGallery [98] aid CAD research and machine learning, offering hierarchical decomposition, joints,\ncontact surfaces, construction sequences, and shape segmentation based on modeling operations. Our\npaper focuses on graphics programs of SVG and CAD by introducing a new semantic understanding\ntask that requires a challenging reasoning with the programs.\nGraphics program understanding and generation. As graphics programs often provide compact,\nscalable and potentially more semantic descriptions compared to raw pixels and voxels, it has been\nwidely explored to discover graphics programs for 2D images like 2D hand drawings and synthetic\npatterns [20, 27, 80, 85, 86, 94, 95], for 3D objects represented in voxels and meshes [7, 20, 27, 41,\n86, 92, 98] and for 3D scenes represented by multi-view images [18, 28, 29, 44, 47, 51, 60, 66, 67,\n100, 108]. [100] infers custom-designed markup code from images that can be easily translated to\nrenderer-friendly inputs. In follow-up work, [108] explore how graphics programs can be used for\nvisual question answering (VQA). Recently, [46] has advanced this direction by examining large\nlanguage models (LLMs) for synthesizing graphics programs to reconstruct visual input. In contrast,\nwe benchmark LLMs to perform semantic-level question answering, similar to VQA, but use graphics\nprograms as input without relying on any visual modality."}, {"title": "3 Semantic Understanding of Symbolic Graphics Programs", "content": "We introduce the task of seman-\ntic symbolic graphics program un-\nderstanding. Our goal is to as-\nsess to what extent a LLM is able\nto \"understand\" a symbolic graph-\nics program, which may begin\nto belie some latent capability to\n\"visually imagine\". Specifically,\nwe leverage the correspondence\nbetween deterministic symbolic\ngraphics programs and rendered\nimages, and then we character-\nize the understanding of symbolic\ngraphics programs as the semantic\nunderstanding of the correspond-\ning rendered image. To do so, we use the performance on question-answering to evaluate the semantic\nunderstanding of images. The same set of questions, along with the corresponding symbolic graphics\nprograms, are then used to evaluate the symbolic program understanding of LLMs (the rendered im-\nage will not be used here). Figure 2 gives an illustration of symbolic graphics program understanding.\nThe intuition behind this evaluation is that, if an LLM has a good sense of the symbolic graphics and\nimplicit de-rendering, then the LLM should have a rough understanding about its rendered image\nsuch that it is able to answer arbitrary semantic questions regarding this rendered image.\nSymbolic graphics program understanding can also be viewed as a form of visual question answering\nin the sense that visual input is represented by a symbolic program representation. Compared\nto existing vision-language models [53, 54, 119] that encodes images with a text-aligned neural\nencoder [74], our paper considers the case where the visual input are encoded by a symbolic program\nthat can exactly recover the graphics content. From this perspective, our task aims to study and\nuncover the potential of using symbolic programs as a representation to perform visual reasoning."}, {"title": "4 SGP-Bench: A Benchmark for Symbolic Graphics Program Understanding", "content": "To construct our benchmark, we need questions about a symbolic\nprogram based on its rendered image. To build a large benchmark,\nit is essential to consider how we can effectively scale up the\nquestion collection with minimal human efforts. To this end, we\nuse a powerful vision-language model (e.g., GPT-4O) to generate\nsemantic questions based on the rendered images, and then we\ninspect them manually to make sure that these questions are\nreasonable and the answer to them is correct. We also run a"}, {"title": "4.1 Dataset Creation Pipeline", "content": "Motivated by the significance of symbolic graphics program understanding, we build a benchmark,\ncalled SGP-Bench, for two important variants of symbolic graphics programs: scalable vector\ngraphics (SVG) as a generic language for representing 2D vector graphics, and customized computer-\naided design (CAD) as a domain-specific language (DSL) for representing 2D/3D objects. Our\nbenchmark consists of two types of evaluations. (1) Semantic understanding: We construct a number\nof semantic questions (i.e., multiple-choice questions with 4 options) from a set of images (from\nmultiple different categories). These questions are fed to LLMs along with the symbolic program\nto evaluate the semantic understanding. (2) Semantic consistency: To evaluate the robustness of\nLLM's semantic understanding, we perform random translation and rotation to the original symbolic\nprograms and then test the same semantic questions based on the perturbed programs. We evaluate\nthe consistency of the answers from LLMs using these perturbed symbolic programs with identical"}, {"title": "4.2 Benchmarking Semantic Understanding", "content": "SVG dataset statistics. We collect in total 1,085 SVG programs covering 19 categories, and each\nprogram has 4 semantic multiple-choice questions (with 4 options). We make sure that answers are\nevenly distributed across 4 options. Detailed dataset statistics are in Figure 4(a). Our SVG benchmark\nconsists of 5 different types of questions, including \u201cSemantic\": 1,085 questions, \u201cColor\": 864\nquestions, \"Shape\": 1,217 questions, \u201cCount\u201d: 819 questions, and \u201cReasoning\": 355 questions.\n\"Semantic\" tests the global semantic meaning of the object represented by SVG codes, while the other\nfour question types focus on detailed, local understanding of the object. \u201cColor\u201d is color-related\nquestions about specific object parts, which evaluates the localization of the corresponding semantic\npart. \"Count\" is about counting the occurrences of certain patterns or semantic parts. \"Shape\" is\nabout the shape of certain parts of the object, either finding the exact definition of the geometric shape\nor finding geometric shapes that resemble the object part. Figure 5 gives some SVG examples.\nCAD dataset statistics. We collect 2, 400 CAD programs from three different datasets [84, 97, 101].\nThe CAD dataset consists of 1000 programs from DeepCAD [101], which forms the 3D subset; 700\nprograms from the Fusion360 Reconstruction Dataset [97], which constitutes the 3Drecon. subset;\nand 700 programs from SketchGraphs [84], which makes up the 2D subset (as shown in Table 1).\nDepending on the data, we either render one image or two images from two opposite isometric views\n(i.e., front and back). Then we feed the images to GPT-4O and generate one semantic multiple-choice\nquestion (with 4 options) and its answer. This gives us 2, 400 questions in total. We make sure that\nground truth answers are evenly distributed across 4 options. Detailed dataset statistics are given in\nFigure 4(b). Some examples from our CAD dataset are provided in Figure 5."}, {"title": "4.3 Benchmarking Semantic Consistency", "content": "Large language models (LLMs) are exposed to vast\namounts of online data, including open-access SVG data.\nTo investigate whether their semantic understanding abil-\nity is due to potential data leakage, we propose a semantic\nconsistency test by introducing global translations or ro-\ntations to SVG graphics, ensuring SE(2) invariance. Such\nspatial interventions fundamentally alter the code repre-\nsentation, as SVG graphics consist of lines and Bezier\ncurves with anchor points, and SE(2) operations change\nall numerical values in the code. However, the SVG's se-\nmantics\u2014such as shape or color-remain unaffected by\nthis perturbation. This allows us to examine how LLMs\nbehave when the same vector graphics are presented with drastic code-numerical changes (see\nAppendix A.1 for more details). If the model maintains the performance consistency under these\nperturbations, it may suggest that the semantic understanding is based on a fundamental level of\ncomprehension or visual imagery rather than a trivial memorization of the code."}, {"title": "4.4 Prediction Entropy of LLMs and Humans", "content": "To evaluate the consensus of different LLMs, we compare the\naverage prediction entropy on 500 symbolic programs using\nGPT-40, LLama3-8B, LLama3-70B, Mistral-7B, Yi-1.5-34B,\nGemma-1.1-7B and Qwen-1.5-72B. We also conduct a human\nexperiment on the rendered images of these programs and col-\nlect the answers (each question has at least 5 participants, see\nAppendix B). Figure 7 show that humans have strong consensus when answering questions based on\nimages. In comparison, LLMs show low consensus when answering questions based on symbolic\nprograms. This implies that LLMs might have different inner working mechanisms to understand\nsymbolic programs. We are excited by future work to better investigate these differences."}, {"title": "5 Symbolic Instruction Tuning: Boosting Symbolic Program Understanding", "content": "Generating symbolic instruction data. Inspired by how visual instruction tuning [54] enables large\nvision-language models to understand images with visual-question-answering (VQA) data, we design\na new method to perform symbolic instruction tuning for LLMs to take steps to better bridge the gap\nbetween the semantic understanding and symbolic reasoning over the graphics programs. While to\nour knowledge there does not exist any semantic instruction-following datasets directly over symbolic\nprograms, these symbolic graphics programs can be rendered into 2D images. With these images"}, {"title": "6 A Challenge for SVG Program Understanding", "content": "Is it really easy to answer semantic reasoning questions over symbolic graphics programs? We\nprovide an intriguing experiment to demonstrate that SVG programs can be quite difficult for LLMs\nto understand such that even if the corresponding rendered images are fairly easy for humans to\nrecognize, all these powerful LLMs still fail dramatically, only reaching a chance-level accuracy."}, {"title": "7 Improving Generic Instruction Tuning with Symbolic Graphics Programs", "content": "We test the instruction-tuned\nmodels on a variety of pop-\nular LLM benchmarks, in-\ncluding benchmarks focusing\non language understanding\n(XNLI [15], IFEval [117], Hel-\nlaSwag [115], C-Eval [37],\nCoQA [78], MMLU [33],\nSQUAD2.0 [75]), generic reasoning (BigBenchHard [89], PIQA [6], AGIEval [116]) and math-\nematical problem-sovling (Arithmetic [8], MathQA [3], GSM8k [12], ASDiv [69]). Specifically,\nwe use the Llama-3.1-8B base model (without any instruction tuning), and then the baseline is\nfinetuned with the Open-Instruct dataset that contains 143K question-answer pairs (details are given\nin Appendix E.1). We evaluate the effectiveness of using SIT data to improve generic instruction\ntuning performance by mixing additional SIT data into the Open-Instruct data. Moreover, our SIT\ndata can also be used in a reverse fashion (rev-SIT), i.e., rephrasing the answer as the new question\nand the question as the new answer. See the comparison between original and reverse SIT data in\nFigure 10. We test three ways to using SIT data: (1) mixing original SIT data into the Open-Instruct\ndata; (2) mixing the reverse SIT data into the Open-Instruct data; (3) mixing both original and reverse\nSIT data into the Open-Instruct data. The results are given in Table 4. We can observe that mixing SIT\ndata can generally improve the instruction following ability, suggesting that understanding symbolic\ngraphics programs requires a combination of multiple reasoning abilities and training on SIT data can\nimprove these abilities. More interestingly, we find that the reverse usage of SIT data (i.e., symbolic\ngraphics program generation) can improve a set of reasoning abilities that are complementary to\nsymbolic graphics program understanding. The mixture of both original and reverse SIT data can\ngreatly improve the instruction following ability of LLMs and achieve better performance than the\nOpen-Instruct baseline, the Open-Instruct + SIT baseline and the Open-Instruct + rev-SIT baseline."}, {"title": "8 Concluding Remarks", "content": "We propose a new task over which to evaluate LLMs to contribute towards a multi-faceted assessment\nof their capabilities: the task of \u201cunderstand\u201d images \u2013 without image; that is, directly from graphics\nprograms. For such a task, we create SGP-Bench, a benchmark that shows distinguishable results\nbetween LLMs and introduce the method of Symbolic Instruction Finetuning (SIT) that improves\nLLMs' capability of understanding graphics programs. Our work helps deepen our collective\nunderstanding of what LLMs can and cannot do, and hopefully inspires a broader diversity of\nevaluation tasks. However, our work only represents the beginning of symbolic graphics program\nunderstanding for LLMs. The inner working mechanisms of such semantic understanding of LLMs\nand how to improve this ability remain to be explored."}, {"title": "A Benchmark Details", "content": "We adopt implementations from other projects to build our SGP-Bench. We follow the implemntation\nof MathVista\u00b9 for querying GPT or open-sourced Llama3.1-8B and perform LLM-based answer\nextraction, vLLM\u00b2 for efficient model inference and simple-evals\u00b3 for a unified benchmarking\nframework.\nOur data license follows the license of the original license of our data source."}, {"title": "A.1 Data preparation", "content": "SGP-Bench (SVG). Our SVG data are sampled from kaggle SVG Icons dataset\u2074 and we build our\nSGP-Bench (SVG) using text prompts from F.1. The original data from kaggle SVG Icons is crawled\nfrom SVGrepo\u2075. The website is merely an aggregator, so it follows that any content on it must at\nleast be licensed permissively enough for SVGrepo to distribute it, and therefore it is acceptable to\ndistribute as part of a collection in our benchmark. Refer to the SVGrepo license for further details.\nSVG Invariance: We use beautifulsoup and SvgLib to process the SVG XML code to perform\ntranslation and rotation perturbations for the invariance investigation, a visual sample can be found\nin Fig. 14. Specifically, as we assume that all XML elements in each SVG figure do not possess\nany \"transform\" attribute as it complicates the augmentation process. For elements that can be fully\nspecified by coordinates (e.g., <rect>, <polygon>), we perform augmentation by perturbing these\ncoordinates. For <path> elements in which path information is fully specified in \"d\" attributes, we\nfirst turn all relative operations (e.g., \"1 2 3\", meaning that draw a line from the current position (x, y)\nto the position (x + 2, y + 3)) into absolute ones, and later perturb the coordinates but not other path\nattributes. As mentioned in the main paper, small spatial perturbations can drastically change the\nnumerics of the SVG XML code (See Section C for more details).\nSGP-Bench (CAD). Our CAD (3D) sequences data are sampled from DeepCAD [101] datasets,\nwhich contains around 180k manually constructed CAD sequences that are originally from the ABC\ndataset [45]. We manually sample 1000 sequences and use pythonocc (OpenCASCADE) to verify\nand normalize these CAD sequences, then render the front and back view of the 3D CAD models.\nSince all the CAD models are from the OnShape platform, the copyright of the CAD models is\nowned by their creators. For licensing details, see Onshape Terms of Use 1.g.ii. Our CAD (3Drecon.)\nsequences are sampled from Fusion360 Reconstruction Dataset [97] datasets, which contains 8,625\nsequences, with more complex curve operations for constructing sketches. Our CAD (2D) sequences\nare sampled from SketchGraphs [84] dataset, which consists of 15 million sketches extracted from\nreal-world CAD models. The major difference is it consists of 2D CAD sketches without Extrusion\noperations.\nSGP-MNIST. The MNIST SVG data is sampled from the kaggle MNIST-SVG dataset\u2076. We\nrandomly sample 1000 samples from MNIST-SVG (100 samples per digit category) to build our\nSPG-MNIST benchmark. The data comes with CC BY-SA 3.0 license."}, {"title": "A.2 Evaluation protocol", "content": "Inference. To conduct massive-scale evaluation (8000+ question samples with 16 models), we\nleverage vLLM\u2077 to perform high-throughput and memory-efficient inference for all the open-source\nLLMs. And we use OpenAI API for evaluating different variants of the GPT models. We deploy\nthe vLLM inference engine as a server that also uses the same format as OpenAI API to achieve a\nunified testing framework for both GPT and all open-source models. The vLLM inference engine is\ndeployed on a node with 8 NVIDIA H100 80G GPUs."}, {"title": "A.3 Evaluated model specs", "content": "Here we list the model details of all LLMs we evaluated in the SGP-Bench, their performance are\ndemonstrated in the table 2. Generally, we evaluated 3 types of LLMs, the representative open-sourced\nLLMs from tech giants and start-ups, the code-specific LLMs that were built for code generation and\nunderstanding and the strongest proprietary models from the GPT and Claude family."}, {"title": "A.3.1 Open-sourced LLMs", "content": "Gemma-1.1-2B/7B Gemma is a suite of lightweight, advanced open models created by Google\nDeepMind and other teams across Google, it's the best performing model at it class when it's released\non Feb 21, 2024. Primarily designed for text generation, Gemma models come in multiple sizes, i.e.\n2B / 7B, to fit various computing resources and deployment needs. The models are trained on 3T\n(2B) / 6T (7B) tokens of primarily-English data from web, mathematics, and code. It is based on a\ntransformer decoder with context length of 8192 tokens. It leverages Multi-Query Attention, RoPE\nEmbeddings, GeGLU Activations and RMSNorm. The Gemma models are using architectures. data\nand training recipes inspired by the Gemini model family. The models are available in two versions:\na pretrained version and an Instruction-tuned version, the latter refined through human language\ninteractions to perform well in conversational roles, similar to a chat bot. We only test and perform\nthe symbolic instruction tuning on the Instruction tuned version.\nMistral-0.3-7B The Mistral-0.1-7B from Mistral AI is released September 27, 2023 and marked as the\nbest 7B model at the time. The 0.1 version model features a 8k context window, with Grouped-query\nattention (GQA) for faster inference and SWA for handling longer sequences more effectively at a\nreduced computational cost. The model is further updated to 0.3 version in May 21, 2024, upgrading\nits context length to 32k, its vocabulery size and RoPE theta, but the SWA is removed in this version.\nMistral-NeMo and Mistral-Large2 Mistral NeMo is a 12B large language model built by Mistral\nAl with a context window size of up to 128k tokens. Mistral NeMo is trained with quantization\nawareness, allowing FP8 inference without any loss in performance. Mistral NeMo uses a new\ntokenizer, Tekken, based on Tiktoken, which enables more efficient compression of natural language\ntext and source code, compared with previous Mistral model series.\nMistral Large 2 is the new generation of Mistral AI's flagship model, with a model size of 123 billion\nparameters. Especially, Mistral Large 2 is trained on a very large proportion of code data, resulting in\nstate-of-the-art performance, on par with proprietary models like GPT-4O or Clause Opus.\nYi-1.5-9B/34B The Yi model family, developed by LLM-focused startup 01.AI, includes 6B and 34B\npretrained language models. Their performance is attributed to high-quality data from meticulous\ndata-engineering efforts. For pretraining, 3.1 trillion tokens of English and Chinese corpora were\nconstructed using a cascaded data deduplication and quality filtering pipeline. Finetuning involved a"}, {"title": "4.3.2 Code-specific LLMs", "content": "CodeQwen1.5-7B CodeQwen1.5-7B is based on Qwen1.5-7B. It is further trained on 3T tokens of\ncode data, and it also includes group query attention (GQA) for efficient inference.\nDeepSeek-Coder-V2-16B-Instruct DeepSeek-Coder-V2-16B-Instruct is an Mixture-of-Experts\n(MoE) code language model, that demonstrates comparable performance to GPT-4 Turbo in code-\nrelated tasks. Specifically, DeepSeek-Coder-V2 is continued to pre-train on intermediate checkpoint\nof DeepSeek-V2 with 6 trillion additional tokens, substantially enhance its reasoning capabilities in\ncode and mathematical related tasks. DeepSeek-Coder-V2 supports 338 programming languages and\nhas a context length of 128K.\nCodestral-22b-v0.1 Codestral-22B-v0.1 is the code-specific variant of Mistral-0.1-22B, it's trained\non a diverse dataset of 80+ programming languages, including the most popular ones, such as Python,\nJava, C, C++, JavaScript, and Bash."}, {"title": "A.3.3 GPT family", "content": "GPT-3.5t (GPT-3.5 Turbo) is a text only language model released by OpenAI on November 2022.\nThe specific version of the model we are using is gpt-3.5-turbo-0125. It has a knowledge cutoff of\nSeptember 2021 and a context window with length of 16K tokens.\nGPT-4t (GPT-4 Turbo) is vision language model launched by OpenAI on March 2023. The specific\nversion of the model we are using is gpt-4-turbo-2024-04-09. It has an updated knowledge cutoff of\nApril 2023 and a context window with length of 128K tokens. It is more powerful than GPT-3.5.\nGPT-4O (GPT-4 Omni) is a multimodal model released by OpenAI on May 2024, which support\ndata types such as audio, vision, and text. The specific version of the model we are using is gpt-4o-\n2024-05-13. It has similar performance as GPT-4t on English text and code, but with significant\nimprovement on non-English text, i.e., over 50 languages. At the same time, it is able to reason with\nvision input. GPT-4O has knowledge up to October 2023 and supports context window with length of\n128K tokens.\nGPT-4O mini is a multimodal model released by OpenAI in July 2024, which is a more cost-efficient\nand smaller modal than GPT-4. It has a context window size of 128K tokens and the has knowledge\nup to October 2023."}, {"title": "A.3.4 Claude family", "content": "Claude is a multimodal, multilingual, proprietary model series developed by Anthropic. The Claude\nseries includes different models: Haiku, the fastest and most lightweight model; Sonnet, the best\nbalanced model between performance and speed; and Opus, the highest-performing model. We did\nnot evaluate Claude 3 Opus because, in June 2024, Anthropic released Claude 3.5 Sonnet, the\nnewest best-performing model.\nSpecifically, we are using claude-3-5-sonnet-20240620 for Claude 3.5 Sonnet, claude-3-sonnet-\n20240229 for Claude 3 Sonnet, and claude-3-haiku-20240307 for Claude 3 Haiku for benchmark\nevaluation."}, {"title": "B Human Study Details", "content": "We ran a human study to verify the labels produced by GPT-4O for the benchmark over a subset of\n500 stimuli. We recruited 55 participants from the crowdsourcing platform, Prolific [71]. Stimuli\nwere batched into 10 sets of 50 stimuli each. Each participant was randomly assigned a batch of 50\nstimuli; stimuli were presented in a random shuffled. On each trial, participants saw the question,\noriginal image, and set of multiple choice options. Participants selected an option by clicking a\nbutton. We include an example screenshot of a trial in Figure 11. Participants were paid at a base\nrate of $12.50/hr. They were informed that they could receive a bonus up to $15/hr based on the\namount of correct answers they achieved. All participants received the full bonus. Our study was\napproved by our institutional ethics review board, and all participants provided informed consent. We\ninclude the set of instructions and sample screenshots in Figures 12 and 13, respectively. We found\nhigh inter-annotator agreement (participants in the same batch had between 0.7 \u2013 0.85 Fleiss Kappa's\nalpha agreement, where higher implies higher agreement). We find that humans' mode response\nmatched GPT-4o on 90% of the examples (450 of the 500 stimuli)."}, {"title": "CSVG - Invariance Illustration", "content": "Our SVG - Invariance test is essential for testing whether a model has a fundamental understanding\nof the code, or it is able to pass the benchmark tests due to memorizing the SVG code samples,\nsince we built our SVG-Bench using public available SVG datasets. In Figure 14 we see two SVG\ncodes, illustrating two samples, that are semantically identical. The rotated sample is generated by\nourself by applying a SE(2) transformation on the original sample (from SVG Icons). We can see that\nsemantically these two samples are identical, the code changed drastically."}, {"title": "D More Examples in SGP-Bench", "content": null}, {"title": "D.1 SVG Data", "content": null}, {"title": "D.2 CAD Data", "content": null}, {"title": "D.3 Symbolic Instruction-following Data (SVG)", "content": null}, {"title": "E Details and More Results of Symbolic Instruction Tuning", "content": null}, {"title": "E.1 Implementation Details", "content": "We use the unsloth framwork to finetune the base models Llama3-8b-instruct and Gemma-1.1-7b-it.\nFor both models, we use the exact same training setting: we finetune the base models with LoRA [34]\non 1 NVIDIA H100 80GB gpu with learning rate 2e-4, batch size of 2 and for 1 epoch.\nWe use the PEFT\u00b9\u2070 framework to test different fine-tuning methods when performing SIT. We choose\ntwo common fine-tuning methods LoRA [34] and orthogonal finetuning [58, 73] to fine-tune the base\nmodel Llama3.1-8b-Instruct. For both fine-tuning methods, we train on 8 NVIDIA H100 80GB gpus\nwith learning rate 1e-4, per device batch size of 1 and for 1 epoch.\nAs introduced in Section 7, we also use PEFT to test if SIT can improve generic instruction tuning,\nby mixing our curated SIT data into the publicly available instruction tuning dataset open-instruct\u00b9\u00b9.\nWe use LoRA to fine-tune the base model Llama3.1-8b on 8 NVIDIA H100 80GB gpus with\nlearning rate le-4, per device batch size of 1 and for 1 epoch. We test mixing with different SIT\ndata splits, including 10K, 25K, 40K, 55K, and 72K. For example, Open-Instruct-SIT-10K, Open-\nInstruct-rev-SIT-10K and Open-Instruct-mixed-SIT-10K are constructed by mixing Open-Instruct\nwith SIT-10K, rev-SIT-10K and mixed-SIT-10K. More specifically, rev-SIT-10K is constructed from\nSIT-10K according to Figure 10, while mixed-SIT-10K uniformly samples exactly 5K of the SIT\ninstruction-following pairs and convert them to rev-SIT, while the rest 5K is kept unchanged. The\nbest result is reported in the Table 4. We employ the widely-used lm-evaluation-harness\u00b9\u00b2 to obtain\nthe results on a variety of LLM benchmarks."}, {"title": "E.2 More Experiments in Symbolic Instruction Tuning", "content": "We additionally provide an ablation study of using different-size SIT data to finetune the base\nLLMs and measure their performance after SIT on the SGP-Bench. We uniformly sample 72"}]}