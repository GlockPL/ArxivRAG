{"title": "A Full DAG Score-Based Algorithm for Learning Causal Bayesian Networks with Latent Confounders", "authors": ["Christophe Gonzales", "Amir-Hosein Valizadeh"], "abstract": "Causal Bayesian networks (CBN) are popular graphical probabilistic models that encode causal relations among variables. Learning their graphical structure from observational data has received a lot of attention in the literature. When there exists no latent (unobserved) confounder, i.e., no unobserved direct common cause of some observed variables, learning algorithms can be divided essentially into two classes: constraint-based and score-based approaches. The latter are often thought to be more robust than the former and to produce better results. However, to the best of our knowledge, when variables are discrete, no score-based algorithm is capable of dealing with latent confounders. This paper introduces the first fully score-based structure learning algorithm searching the space of DAGs (directed acyclic graphs) that is capable of identifying the presence of some latent confounders. It is justified mathematically and experiments highlight its effectiveness.", "sections": [{"title": "1 Introduction", "content": "Causal networks, a.k.a. causal Bayesian networks (CBN) [15], are graphical probabilistic models that encode cause-and-effect relationships. Like Bayesian networks (BN), they are constituted by i) a directed acyclic graph (DAG) whose nodes represent random variables and whose edges encode their relationships; and ii) a set of conditional probability distributions of the nodes/random variables given their parents in the graph. However, unlike BNs, the semantics of the edges is not merely correlation but rather a causal relationship, that is, an arc from A to B states that A is a direct cause of B. CBNs are important for Artificial Intelligence because they enable to perform the same kind of reasoning as humans do, in particular counterfactual reasoning (if I had done this, what would have happened?).\nAlthough it is well-known that learning the structure of CBNS from only (observational) data is theoretically not always possible [15], many algorithms have been proposed in the literature for this purpose. When there exists no unmeasured confounder, i.e., no unobserved direct common cause of some measured variables, they can be essentially divided into two classes: constraint-based and score-based approaches. The former [20, 16, 4, 30] rely on statistical conditional independence tests to uncover the independence properties underlying the probability distribution that generated the data, thereby learning the graphical structure of the causal model. These methods are often not able to uncover the whole DAG of the CBN, so they provide weaker information in the form of a Completed Partially Directed Acyclic Graph (CPDAG). In such a graph, only the directed edges represent \"true\" causal relations, the undirected ones representing correlations, their causal direction remaining unknown. On the other hand, score-based approaches [3, 6, 26] identify the DAG of the CBN as the one maximizing some fitness criterion on the data. They rely on either approximate or exact optimization techniques to uncover the searched DAG. However the orientations of its arcs may not always have a causal meaning. So this DAG is mapped into the CPDAG of its Markov equivalence class, which is the best that can be extracted in terms of causality from the data. Score-based approaches are usually considered more robust than constraint-based approaches, notably because, in the latter, errors in statistical tests can chain and decrease significantly the quality of the resulting CPDAGs.\nHowever, in most practical situations, some variables play an important role in the causal mechanism and, yet, for different reasons, they are not or cannot be observed in the data. For instance, their measuring may be too expensive or it would require unethical processes. Constraint-based methods have been successfully extended to cope with such latent (unobserved) variables [5, 21, 31]. For score-based algorithms, it is somewhat different: it is commonly admitted that they are unable to cope with latent variables because they rely on searching for DAGs and DAGs are inadequate in the presence of latent variables. An extension of DAGs called Maximal Ancestral Graphs (MAG) [18] has been introduced precisely to fix this issue and score-based approaches have been adapted to learn MAGs [17, 13, 23]. Unfortunately, currently, they can only cope with scoring MAGs over continuous variables. Yet, this is restrictive because there exist situations in which variables are discrete by nature and cannot be meaningfully extended as continuous ones, e.g., non-ordinal variables such as colors, locations, types of devices, etc.\nIn this paper, we address problems in which all the random variables are discrete. In [22],it was shown that causal models with arbitrary latent variables can always be converted into semi-Markovian causal models (SMCM), i.e., models in which latent variables have no parent and only two children, while preserving the same independence relations between the observed variables. So, to deal with latent confounders, we focus on learning SMCMs. More precisely, we show that, without any prior knowledge about the latent confounders or their number, DAGs learnt from observational data by latent confounders-unaware score-based approaches encode sufficient information to recover many latent confounders and their locations. Exploiting this property, we provide and justify a structure learning algorithm that i) only relies on scores; ii) uses only DAGs; and iii) is capable of identifying some latent confounders and their locations.\nThe rest of the paper is organized as follows. Section 2 presents formally causal models and some algorithms for learning BN and/or CBN structures from observational data that can cope with latent"}, {"title": "2 Causal Models and Structure Learning", "content": "In the paper, bold letters represent sets. X denotes a set discrete random variables. For a directed graph G, ChG(X) and PaG(X) denote the set of children and parents of node X respectively, i.e., the set of nodes Y such that there exists an arc from X to Y and from Y to X respectively. A causal model over X is defined as follows [15]:\nDefinition 1. A causal model is a pair (G, \u0398) where G = (X, E) is a DAG\u00b9 and E is a set of arcs. To each $X_i \\in X$ is assigned a random disturbance \u03bei. \u0398 = {$f_i(Pa_G(X_i), \u03be_i)$} $X_i \\in X \u222a$ {$P(\u03be_i)$} $X_i \\in X$, where fi's are functions assigned to $X_i$'s and P are probability distributions over disturbances \u03bei.\nIt is easy to see that a causal model can be represented equivalently by a Bayesian network \u2013 BN [14]:\nDefinition 2. A BN is a pair (G, \u2299) where G = (X,E) is a directed acyclic graph (DAG), X represents a set of random variables, E is a set of arcs, and \u2299 = {$P(X|Pa_G(X)) $} $X \\in X$ is the set of the conditional probability distributions (CPD) of the nodes / random variables X in G given their parents PaG(X) in G. The BN encodes the joint probability over X as P(X) = $\u220f_{X \\in X} P(X|Pa_G(X))$.\nCausal models impose that the arcs are oriented in the direction of causality, that is, an arc X \u2192 Y means that X is a direct cause of Y. In this case, the BN is called a CBN. In general, BNs do not impose this restriction since they only model probabilistic dependences. Hence a BN containing only Arc X \u2192 Y is equivalent to one containing Arc Y \u2192 X. More precisely, the independence model of a BN is specified by the d-separation criterion [14]:\nDefinition 3 (Trails and d-separation). Let G be a DAG. A trail C between nodes X and Y is a sequence of nodes ($X_1 = X, ..., X_k = Y$) such that, for every i \u2208 {$1, ..., k \u2212 1$}, G contains either Arc $X_i \\leftarrow X_{i+1}$ or Arc $X_i \\rightarrow X_{i+1}$.\nLet Z be a set of nodes disjoint from {$X, Y$}. X and Y are said to be d-separated by Z, which is denoted by $(X \u22a5_G Y | Z)$, if, for every trail C between X and Y, there exists a node $X_i \\in C, i \\notin$ {$1, k$}, such that one of the following two conditions holds:\n1. $(X_{i\u22121}, X_i, X_{i+1})$ is a collider, i.e., G contains Arcs $X_{i\u22121} \\rightarrow X_i$ and $X_i \\leftarrow X_{i+1}$. In addition, neither $X_i$ nor its descendants in G belong to Z. The descendants of a node are defined recursively as the union of its children and the descendants of these children.\n2. $(X_{i\u22121}, X_i, X_{i+1})$ is not a collider and $X_i$ belongs to Z.\nSuch trails are called blocked, else they are active. Let U, V, Z be disjoint sets of nodes. Then U and V are d-separated by Z if and only if X and Y are d-separated by Z for all $X \\in U$ and all $Y \\in V$.\nLet U, V, Z \u2286 X be disjoint sets and let P be a probability distribution over X. We denote by $U \u22a5_P V | Z$ the probabilistic conditional independence of U and V given Z. The independence model of BNs is the following:\n$(U \u22a5_G V | Z) \u21d2 U \u22a5_P V | Z.\n(1)\nTwo BNs with the same set of d-separation properties therefore represent the same independence model. Any graphical model satisfying Eq. (1) is called an I-map (independence map). The d-separation criterion implies that two BNs represent the same independence model if and only if they have the same skeleton and the same set of v-structures [29]: the skeleton of a directed graph G is the undirected graph obtained by removing all the orientations from the arcs of G; v-structures are colliders $(X_{i-1}, X_i, X_{i+1})$ such that G does not contain any arc between $X_{i-1}$ and $X_{i+1}$. The directions of the arcs in v-structures are therefore identical for all BNs that represent the same sets of independences. When Eq. (1) is substituted by:\n$(U \u22a5_G V | Z) \u21d4 U \u22a5_P V | Z$,\n(2)\nthen the graphical model is called a P-map (perfect map).\nTo learn the structure of a BN from observational data, it is usually assumed that the distribution P that generated the data has a P-map (although there exist some papers that relax this assumption [16, 10, 11]). Then, it is sufficient to learn the independence model of the data (the set of conditional independences): each independence $X \u22a5_P Y | Z$ necessarily implies the lack of an arc between X and Y in the BN. The (undirected) edges of the skeleton therefore correspond to all the pairs of nodes (X, Y) for which no conditional independence was found. The set of v-structures (X, Z, Y) is the set of triples (X, Z, Y) such that i) edges X \u2013 Z and Z - Y belong to the skeleton and ii) there exist sets Z \u2286 X\\{Z} such that $X \u22a5_P Y | Z$. The edges of the skeleton corresponding to v-structures can be oriented accordingly. Now, to avoid creating additional spurious v-structures or directed cycles (which are forbidden in DAGs), some edges need necessarily be oriented in a given direction. These are identified using Meek's rules [12] and can be computed in polynomial time [2]. The graph resulting from all these orientations is called a CPDAG (Completed Partially Directed Acyclic Graph). To complete the learning of the BN's structure, there just remains to orient the remaining undirected edges. To do so, it is sufficient to sequentially apply the following two operations until there remains no undirected edge: i) orient in any direction one (arbitrary) edge; and ii) apply Meek's rules. This is precisely what constraint-based algorithms like PC [20], PC-stable [4], CPC [16], IC [29] or FCI [21] do, relying on statistical independence tests. MIIC [30] essentially performs the same operations but exploiting multivariate information instead.\nThere exist many other algorithms, based notably on learning Markov blankets [25] or on scoring DAGs [6, 8, 3, 26]. The key idea of score-based approaches is to assign to each DAG a score representing its fitness on the data. Under some assumptions, the one maximizing this criterion is the one that generated the data. In the rest of the paper, we exploit the BIC score [19]:\n$S(X|Z) = \\sum_{X \\in \\Omega_X} \\sum_{Z \\in \\Omega_Z} N_{xz} log (\\frac{N_{xz}}{N_z}) - \\frac{1}{2} log(|D|)dim(X|Z)$,\nwhere S(X|Z) denotes the score of node X given parent set Z; \u03a9x and \u03a9z represent the domains of X and Z repectively; Nxz is the number of records in Database D such that X = x and Z = z; $N_z = \\sum_{x \\in \\Omega_x} N_{xz}$; and, finally, dim(X|Z) is the number of free parameters in the conditional probability distribution P(X|Z), i.e., dim(X|Z) = ($|\\Omega_x|$ \u2013 1) \u00d7 |\u03a9z|. The term $log(|D|)dim(X|Z)$ is called the penalty of the score.\nIn the literature, v-structures are considered to represent causal relations, that is, the directions of their arcs have a causal meaning. This implies that, when there exist no latent confounder, CPDAGS are precisely the best that can be extracted in terms of causality from the"}, {"title": "3 A New Full Score-based Causal Learning Algorithm", "content": "In the rest of the paper, X is divided into $X_o$ and $X_H$, which represent sets of observed and hidden (latent) variables respectively. We assume that there exists an underlying probability distribution P* over X = $X_o \u222a X_H$ that generated a dataset D*. But, as the variables in $X_H$ are unobserved (they are the latent confounders), only the projection D of D* over $X_o$, i.e., the dataset resulting from the removal from D* of all the values of the variables of $X_H$, is available for learning. In addition, we assume that P* is decomposable according to some DAG G*. The goal is to recover G*.\nThe key idea of our algorithm is summarized on Figure 1: the left side displays part of Graph G*, which contains a latent confounder $L \\in X_H$ and represents the structure of the causal network that generated the data; Graph G on the right should be the one learnt by a score-based algorithm. Indeed, in G*, there exists no set Z \u2286 $X_o$ that d-separates A and B (because Trail (A, L, B) is active). Hence, provided G* is a P-map, A and B should be dependent and a structure with an arc between A and B should have a higher score than one without. Assume that this arc is A \u2192 B. For the same reason, a structure with an arc between A and C (resp. B and D) should have a higher score that one without. Now, given any Z \u2289 {$A$}, node B is not d-separated from C in G* because Trail (C, A, L, B) is active. But it would be on Fig. 1.b if there were no arc between B and C. This is the reason why score-based algorithms tend to produce structures with such an arc and analyzing such triangles (A, B, C) in the learnt graph should provide some insight on the location of the latent confounders. This intuition is confirmed by the next proposition.\nProposition 1. Assume that there exists a perfect map G* = (X, E) for Distribution P* and that $X_H$ is the set of latent confounders, i.e., all the nodes of $X_H$ have no parent and exactly two children in G*, which both belong to $X_o$.\nLet $L \\in X_H$ be any variable such that both of its children A, B in G* have at least one parent in $X_o$. Then, if G is a DAG maximizing the BIC score over D, as |D| \u2192 \u221e, G contains an arc between A and B. Without loss of generality, assume this is Arc A \u2192 B. Then, for every $C \\in Pa_{G^*}(A) \u2229 X_o$, (A, B, C) is a clique in G.\nProof: All the proofs are provided in the supplementary material, Appendix A, at the end of the paper.\nNote that, in Proposition 1, both A and B have other parents than L. This is important because, as shown in Proposition 2 below, whenever node L is the only parent of A (resp. B), no learning algorithm can distinguish between a graph G* with a latent confounder L whose children are A and B (Fig. 2.a & 2.b), and a graph G* without latent confounder L but with an arc A\u2192B (resp. B \u2192 A) (Fig. 2.c).\nSimilarly, as shown in Proposition 3, it is impossible to distinguish between a graph G* containing both latent confounder L and Arc A\u2192B (Fig. 2.d), and a graph G* without latent confounder L but with both arcs A \u2192 B and C \u2192 B (Fig. 2.e).\nProposition 2. Let B* = (G*, \u0398) be a Bayesian network, with G* = (X, E). Let $L \\in X'_H$ be such that $Ch_{G^*}(L) = {A, B} \u2286 X_o$ and $Pa_{G^*}(L) = \u00d8$. In addition, assume that $Pa_{G^*}(A) = {L}$. Finally, let G be the graph resulting from the removal of L (and its outgoing arcs) from G* and the addition of arc A \u2192 B (if G* does not already contain it). Then, for any triple of disjoint subsets of variables U, V, W of $X_o$, we have that:\n$(U \u22a5_G V | W) \u21d4 (U \u22a5_{G^*} V | W)$.\n(3)\nProposition 3. Let B* = (G*, \u0398) be a Bayesian network such that G* = (X, E) and Arc A \u2192 B belongs to E. Let $L \\in X'_H$ be such that $Ch_{G^*}(L) = {A,B} \u2286 X_0$. Finally, let G be the graph resulting from the removal of L (and its outgoing arcs) from G* and the addition the set of arcs {$X \\rightarrow B : X \\in Pa_{G^*}(A)\\{L\\}$ } (if G* does not already contain them). Then, for any triple of disjoint subsets of variables U, V, W of $X_o$, we have that:\n$(U \u22a5_G V | W) \u21d4 (U \u22a5_{G^*} V | W)$.\n(4)\nIn the two situations mentioned above, algorithms like FCI deal with indistinguishability by not choosing a single structure to return but rather by labelling arcs with o to highlight the uncertainty about the locations of the arrow heads and by asking the user to personally select which structure seems the most appropriate. In a sense, this corresponds to completing the structure learning with some user's expert knowledge. Other algorithms like MIIC or our algorithm prefer to choose which structure seems the best, hence relieving the user of such a burden. The rule followed by our algorithm is to discard latent variables when they are not absolutely necessary (i.e., in Fig.2, it selects only Graphs 2.c & 2.e). This criterion can be viewed as a"}, {"title": "Algorithm", "content": "Rule 1. Only latent triangles contain exactly one pair of nodes that are independent given some set Z \u2286 $X_o$ but are dependent given Z union the third node of the triangle.\nFor the case of Fig. 1.a, any of the latent triangles of Fig. 3 can be learnt by score-based approaches because they encode exactly the same d-separation properties. In Fig. 3, the independent pair of nodes mentioned in Rule 1 is (B, C) for the three types. Triangles of types 2 and 3 share the fact that G*'s arc A \u2192 C has been reversed. If C had parents in G*, then these have become its children in G to avoid creating new v-structures. So, when triangles of types 2 or 3 are learnt, C should not have many parents other than A, which may not be the case for B. So, to minimize the BIC score's penalty induced by the arc between B and C, the latter will most probably be oriented as B \u2192 C. This intuition was confirmed in experiments we conducted: Type 2 triangles only allowed us to determine less than 0.5% of the latent confounders\u00b3. So, to speed-up our algorithm without decreasing significantly its effectiveness, we chose to exploit only Types 1 and 3 latent triangles. Note that this makes our algorithm only approximate but, as shown in the experiments, it still remains very effective. Rule 2 summarizes the features of the above triangles:\nRule 2. Triangles of Type 1 and Type 3 are such that C\u2192A\u2192B and A B\u2192C respectively.\nFor both types, there exists some set Z \u2286 $X_o$ \\{A, B, C} such that $B \u22a5_P C | Z$ and $B \u29f8\u22a5_P C | Z \u222a {A}$; and there exists no Z \u2286 $X_o$ \\{A, B, C} such that $A \u22a5_P B | Z$ or $A \u22a5_P C | Z$.\nIf score-based algorithms never made any mistake in their learning, we could simply use Rule 1 to identify latent triangles and the location of the latent confounders. However, in practice, similarly to constraint-based methods, they do make mistakes, which may induce our algorithm to incorrectly identify spurious latent triangles. Fortunately, it is possible to increase the robustness of the latent triangles identification by not only considering the dependences between nodes A, B and C but also by taking into account those involving node D, the parent of B in G* (See Fig. 1.a).\nThe dotted arcs in Fig. 4 are those involving D and nodes A, B, C that should be learnt by the score-based algorithm. In Type 1 triangles, there should therefore exist an arc D \u2192 B, which can be translated as \"node B must have at least 3 parents in G\". Triangles of Type 1 are therefore considered as latent only if they satisfy both this property and Rule 2. For Type 3 triangles, the situation is more complex because, in addition to Arc D \u2192 B, there should also exist an arc between D and C to account for $(D \u22a5_{G^*} C | {A, B})$. Note that Triangle (D, B, C) cannot be misinterpreted as latent because both pairs (B, C) and (C, D) can be made independent given some Z\u2286$X_o$, hence ruling out Rule 1. Indeed, according to Fig. 1.a, $(C \u22a5_{G^*} D | Z)$ for any Z\u2286$X_o$\\{C, D} equal to $\u2205$, {A} or {B}. Unfortunately, for the same reason that Types 1, 2 and 3 triangles encode the same d-separation properties, the orientations of the arcs of Triangle (D, B, C) can be reversed (provided they do not induce directed cycles). As a consequence, in the learnt graph G, node D may be a child of B rather than its parent. At first sight, it is difficult to discriminate between the true children of B in G* and its true parents in which the arcs of Triangle (D, B, C) have been reversed. However, note that if D were a true child of B in G*, $(C \u22a5_{G^*} D | Z)$ would hold for Z only equal to \u2205 or {B} (see Fig. 1.a). As a consequence, only the true parents D of B in G are such that $(C \u22a5_{G^*} D | Z)$ given some set Z\u2286$X_o$\\{C, D} containing node A. So Type 3 triangles are considered as latent only if they satisfy both this property and Rule 2. Overall, this results in Algorithm 1.\nRemark that, as mentioned previously, on Line 12, Algorithm 1 is allowed to remove Arc A\u2192B because, when this arc truly exists in G* (see Fig. 2.d), based on Dataset D, it is impossible for any learning algorithm to distinguish between G* and Fig. 2.e. In such a case, our algorithm deliberately chooses to return the structure of Fig. 2.e in order to enforce some Occam razor and to avoid requiring some expert knowledge to select the right structure."}, {"title": "Algorithm 1: Learning with confounders.", "content": "Input: Dataset D\nOutput: The CPDAG of the learnt CBN\n1 G \u2190 DAG learnt from D by a score-based algorithm\n2 T \u2190 the triangles of G satisfying Rule 2\n// Get the latent triangles\n3 T1 \u2190 \u2205 // latent triangles of type 1\n4 T3 \u2190 \u2205 // latent triangles of type 3\n5 foreach triangle T = (A, B, C) in T do\n6   if T is of Type 1 and |Pag(B)| \u2265 3 then\n7      T1 \u2190 T1 \u222a T\n8   else if T is of Type 3 and there exists\n9      D\u2208 (Pag(B)\\{A}) \u222a (Chg(B)\\{C}) such that there exists Z\u2286Xo\\{C, D} such that A \u2208 Z and $D \u22a5_P C | Z$ then\n10       T3 \u2190 T3 \u222a T\n// Recreate the latent variables\n11 foreach triangle T = (A, B, C) in T1 \u222a T3 do\n12  Add a new node L (confounder) to G\n13  Remove from G Arc A \u2192 B\n14  Remove from G the arc between B and C\n15  Add arcs L \u2192 A and L \u2192 B to G\n16 M \u2190 the CPDAG of G\n17 return CPDAG M\nRule 2 and Algorithm 1 require the determination of some set Z\u2286Xo such that some pairs of nodes are conditionally independent given Z. Below, we suggest two algorithms for this purpose. The first algorithm to determine Set Z consists of exploiting the fact that, in I-maps, d-separation implies conditional independences. Hence, applying a d-separation analysis on G using, e.g., van der Zander and Li\u015bkiewicz [28]'s algorithm, it is possible to get some d-separating set Z and, consequently, a set inducing a conditional independence. The following proposition justifies that this approach can be used\u2074:\nProposition 4. Let D* be a dataset generated by some distribution P* over X for which there exists a perfect map G* = (X, E), and let D be the projection of D* over $X_o$. Then, as |D|\u2192 \u221e, every DAG G maximizing the BIC score over D is a minimal I-map.\nHowever, when the database is not \"too\" large and Graph G is not highly trustworthy, another option is to exploit Algorithm 2 which adds iteratively to Z the variable X that allows to reduce the most the dependence between the pair of nodes U, V until an independence between U and V is inferred or no independence can be proven. In essence, this is the approach followed by MIIC [30], except that MIIC estimates dependences through an information theoretic criterion whereas we exploit the BIC score:\nDefinition 4. For every pair of variables U, V and every set of variables Z, let $f_{BIC}(U, V | Z)$ be defined as:\n$f_{BIC}(U, V | Z) = 2 \u00d7 (S(U \u222a V | Z) \u2212 S(U | Z) \u2212 S(V | Z) + \\frac{1}{2} log(|D|)\u03b4)$,\n(5)\nwhere \u03b4 = ($|\\Omega_U |$ \u2212 1) \u00d7 ($|\\Omega_V |$ \u2212 1) \u00d7 |$\u03a9_z$| and \u03a9u (resp. \u03a9v, \u03a9z) is the domain of variable U (resp. V, Z), and S(..) is the BIC score.\nThe following proposition justifies Algorithm 2:"}, {"title": "Algorithm 2: Finding a set Z s.t. U \u22a5 V | Z", "content": "Input: nodes U, V, Dataset D, max size h of Z, risk level \u03b1,\nsets C and F of compulsory and forbidden variables, i.e., Z \u2287 C and Z\u2229F = \u2205\nOutput: A set Z s.t. U \u22a5 V | Z if such set is found, else False\n1 Z \u2190 C; F \u2190 F \u222a {U, V}\n2 \u03b4 \u2190 (|$\u03a9_U$| \u2212 1) \u00d7 (|$\u03a9_V$| \u2212 1) $\u220f_{X \\in Z}$|$\u03a9_x$|\n3 if |Z| > h then return False;\n4 if $f_{BIC}(U, V | Z) < X^2_\u03b4(\u03b1)$ then return Z;\n5 while |Z| < h do\n6   X \u2190 argmin{$f_{BIC}(U, V | Z \u222a {Y}) : Y\u2208 Xo\\(Z \u222a F)$}\n7   Z \u2190 Z \u222a {X}\n8   \u03b4 \u2190 \u03b4 \u00d7 |\u03a9x|\n9   if $f_{BIC}(U, V | Z) < X^2_\u03b4(\u03b1)$ then return Z;\n10 return False\nProposition 5. If U and V are independent given a set Z, the formula of Eq. (5) follows a $x^2$ distribution of \u03b4 degrees of freedom. So, given a risk level \u03b1, U and V are judged independent if the value of Eq. (5) is lower than the critical value $X^2_\u03b4(\u03b1)$ of the $x^2$ distribution.\nTo conclude this section, we provide below the time complexity of Algorithm 1, assuming (as we did in our experiments) that the algorithm used for determining the conditioning sets Z required in Rule 2 and Algorithm 1 is Algorithm 2:\nProposition 6. Assume that the existence of d-separating sets Z is checked with Algorithm 2 with h the maximal size allowed for Z and BIC defined as Eq. (5). Let n and m denote the number of nodes and arcs of G as defined on Line 1 respectively. Let k be the maximum number of parents and children of the nodes in G. Then the time complexity of Algorithm 1 over dataset D is O($n^2k^3h|D|$ + $m log n$)."}, {"title": "4 Experiments", "content": "In this section, some experiments on classical benchmark CBNS (child ($|X_o|$ = 20), water ($|X_o|$ = 32), insurance ($|X_o|$ = 27), alarm ($|X_o|$ = 37), barley ($|X_o|$ = 48)) from the BNLearn Bayes net repository are performed to highlight the effectiveness of our algorithm to find latent confounders and to recover CBN structures.\nFor each experiment, a CBN is selected, and some new (latent) variables $L_i$ are added to it. To make them confounders of a semi-Markovian causal model, for each $L_i$, two nodes of $X_o$ are randomly chosen to become $L_i$'s children. To fit the propositions of the paper, Line 1 of Algorithm 1 is performed using the CPBayes exact score-based learning algorithm [26]. For this purpose, Datasets D need to be converted into so-called instances that are passed as input to CPBayes. These contain all the possible nodes' sets that CPBayes will consider as potential parent sets. So, to control the combinatorial explosion it has to face, CPBayes requires limiting the number of possible parents of the nodes. In the experiments, we set this limit to 4 because i) no node of $X_o$ had more than 4 parents in the original CBN (the one without confounders); and ii) this enabled to control the amount of computations performed by CPBayes (see [26] for more details). As a consequence, since the score-based algorithm may add 2 additional parents (A and C) to some $L_i$'s children (B), as shown in Fig. 1.b, all the $L_i$'s children are selected randomly but with the constraint that they have 1 or 2 parents in $X_o$. This upper bound"}, {"title": "5 Conclusion and Perspectives", "content": "In this paper, we have introduced the first score-based CBN structure learning algorithm for discrete variables that searches only the space of DAGs, exploits only observational data and, yet, is capable of identifying some latent confounders. It has been justified mathematically, notably through Proposition 1. In addition, theoretically, it is asymptotically guaranteed to produce an I-map. Experiments highlighted it effectiveness, especially for large datasets. Notably, both in terms of the CPDAGs and the latent confounders found, the results of this algorithm may be judged as very competitive compared to those of its constraint-based competitors like MIIC or FCI.\nFor future works, to be more scalable, we plan to substitute the use of the CPBayes on Line 1 of Algorithm 1 by a faster approximate algorithm like greedy hill climbing (GHC). Usually, GHC-like methods make more mistakes in the directions of the causal arcs learned than CPBayes. So, to compensate for this issue, the rules used in Lines 6 and 8 may certainly have to be improved. Notably, in Algorithm 1, we did not take into account Type 2 triangles (see Figure 3.b) because, empirically, they were very seldom encountered in the experiments. However, with GHC-like algorithms, this may not be the case anymore and they should be taken into account.\nPerhaps a more immediate improvement could be made by observing that, whenever Type 3 triangles are found, their A \u2192 C connection is in the wrong direction (see Figure 3.b). Therefore, to produce better CPDAGs, Algorithm 1 should reverse this arc and relearn the neighborhood of node C. In addition, as shown in Propositions 2 and 3, some structures are indistinguishable from the observational data point of view and, among all the structures of Fig. 2, Algorithm 1 makes the decision to select only those of Fig. 2.c and 2.e. So, in the same spirit as PAGs, the output of Algorithm 1 could be improved to express the uncertainty about which of these structures should be selected. Note that, in this case, PAG's labels are not sufficient since the uncertainty not only concerns the location of arrow heads but also the very existence of some arcs (like C \u2192 B of Fig. 2.e) for which no independence test can detect that they can be dispensed with."}]}