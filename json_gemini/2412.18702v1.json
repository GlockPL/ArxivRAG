{"title": "CYPHERBENCH: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era", "authors": ["Yanlin Feng", "Simone Papicchio", "Sajjadur Rahman"], "abstract": "Retrieval from graph data is crucial for augmenting large language models (LLM) with both open-domain knowledge and private enterprise data, and it is also a key component in the recent GraphRAG system [1]. Despite decades of research on knowledge graphs and knowledge base question answering, leading LLM frameworks (e.g., Langchain and LlamaIndex) have only minimal support for retrieval from modern encyclopedic knowledge graphs like Wikidata. In this paper, we analyze the root cause and suggest that modern RDF knowledge graphs (e.g., Wikidata, Freebase) are less efficient for LLMs due to overly large schemas that far exceed the typical LLM context window, use of resource identifiers, overlapping relation types and lack of normalization. As a solution, we propose property graph views on top of the underlying RDF graph that can be efficiently queried by LLMs using Cypher. We instantiated this idea on Wikidata and introduced CypherBench, the first benchmark with 11 large-scale, multi-domain property graphs with 7.8 million entities and over 10,000 questions. To achieve this, we tackled several key challenges, including developing an RDF-to-property graph conversion engine, creating a systematic pipeline for text-to-Cypher task generation, and designing new evaluation metrics.", "sections": [{"title": "1 Introduction", "content": "Graphs, as a natural modality for modeling entity-relation data, have been widely used for storing both large-scale encyclopedic knowledge and domain-specific enterprise data. Compared to raw textual documents, graphs enable efficient processing of complex multi-hop aggregation queries (e.g., What is the average height of point guards who have played for the Toronto Raptors?), where the answer might depend on information spread across thousands of documents or even the entire corpus. Graphs also provide a more compact representation of knowledge. For example, Wikidata [2] contains on average 4.6 times the entities covered by Wikipedia across the domains we experimented with. These advantages has motivated decades of research in knowledge graphs and knowledge base question answering (KBQA) [3, 4, 5, 6, 7], as well as the recent proposal of GraphRAG [1, 8].\nHowever, retrieval2 from modern encyclopedic knowledge graphs [2, 10, 11, 12], which are predominantly based on RDF, has proven challenging even with the use of LLMs, unlike the success achieved"}, {"title": "2 Knowledge Graph Modeling in the LLM Era", "content": "2.1 Preliminaries: Knowledge Graphs, RDF and Property Graphs\nThe Resource Description Framework (RDF) and property graph are two approaches to modeling and querying knowledge graphs. We begin with an abstract definition of a knowledge graph and then discuss how it is implemented in RDF and property graphs.\nIn its most basic form, a knowledge graph is a list of relations (also called triples) in the format (subject entity, relation type, object entity), such as (\"LeBron James\", playsFor, \"LA Lakers\"). 3 Additionally, entities are often assigned entity types (e.g., Person). Entities and relations can also be associated with literal values as properties (e.g., receivesAward.year).\nThe most popular public knowledge graphs to date (e.g., Wikidata, Freebase, and DBpedia) are predominantly based on the RDF and queried using SPARQL. In RDF, entities are stored and accessed using Internationalized Resource Identifiers (IRIs). Entity properties, including entity names, are stored as relations, with the subject being the entity IRI and the object being a literal value. To store relation properties, RDF uses a process called reification, which creates a copy of the relation as a special entity and links it to the relation property using an additional relation.\nProperty graph databases have gained significant popularity in industry in recent years, with Neo4j being the most popular graph database management system today. Unlike RDF, the property graph model treats entities and relations as objects, each of which can be assigned types and have associated properties. In property graphs, entities are often accessed directly using their names.\n2.2 Why is retrieval over modern KG hard?\nRetrieval over modern encyclopedic knowledge graphs, which are predominantly RDF graphs (shown in Figure 1), poses significant challenges due to several factors.\nOverly large schemas. Modern encyclopedic knowledge graphs aim to cover entities and relations across all domains within a single graph, resulting in an extremely large schema that far exceeds the context window sizes of typical LLMs. For instance, Wikidata currently includes over 4 million entity types and 12,000 relation types. Furthermore, RDF graphs allow entities of arbitrary type to serve as subjects or objects for the same relation types, which further increases the number of unique relation schemas.\nUse of resource identifiers. SPARQL queries require identifiers for entities, entity types, relation types which must be obtained via external linkers [21, 22]. This also makes SPARQL queries less readable. For instance, consider the SPARQL and Cypher queries for the question \"Q4. What are the names of taxa that feed on Synsphyronus lathrius?\""}, {"title": "Overlapping relation types", "content": "Wikidata contains semantically overlapping relation types that are created for domain-specific usage. For instance, there are at least six relation types to indicate the starting time of an entity: start time (P580), inception (P571), date of official opening (P1619), date of first performance (P1191), publication date (P577), service entry (P729). This leads to considerable confusion when selecting the correct relation type to use during retrieval.\nLack of normalization. RDF does not enforce type constraints and standardized units on values. As a result, literal values in Wikidata often appear with different units (e.g., centimeters and feet for"}, {"title": "2.3 Hasn't KBQA already solved KG retrieval?", "content": "KBQA requires graph retrieval to answer questions. However, most existing studies focused on simplified settings to evaluate algorithmic improvements. For example, a common simplification made by recent work is assuming that the entity and relation identifiers are already provided [16, 17, 18, 23, 24, 25], which reduces the task to retrieval over a small local subgraph. Moreover, many studies use custom-designed intermediate logical forms that lack support for certain graph querying functionalities (e.g., relation properties querying, grouping, variable-length path matching) [3, 26, 6, 27, 7, 15, 28]. As a result, the majority of existing KBQA approaches (see section 7 for a complete categorization) struggle with some or all of the following types of queries: 1) queries involving relation properties, such as time-sensitive queries; 2) global queries that do not contain any named entities; and 3) queries requiring complex aggregations over a large number of entities."}, {"title": "2.4 Our Proposal: Property Graphs and Cypher as a Unified Interface", "content": "To address the aforementioned challenges, we propose transforming the RDF graph into multiple domain-specific property graphs and using Cypher to query them (see Figure 2 for an illustration). We chose Cypher, the query language for Neo4j, because of its widespread adoption in open-source projects (including LLM frameworks and GraphRAG). These property graphs function as (materialized) views on top of the original RDF graph that can be queried efficiently by LLM. Each property graph view contains all data that conforms to its respective schema and can be updated when the underlying RDF data changes. These views operate independently, may overlap, and may be created on-demand, offering substantial flexibility.\nThis design enables scaling to a large number of domains without introducing an overly complex schema and eliminates ambiguity from overlapping relation types by contextualizing them within specific domains. Our RDF-to-property-graph transformation layer manages datatype conversion and unit standardization, producing schema-enforced, strictly typed data to ensure the result correctness for aggregation queries. This transformation is also efficient and takes only a few seconds for small graphs (fewer than 10,000 entities), as it is is achieved by executing SPARQL queries and aggregating the results, rather than processing the entire RDF dump.\nIn the following sections, we instantiated this idea on Wikidata, the largest knowledge graph today. We demonstrate that a direct prompting baseline using gpt-40, without the use of external linkers or retrievers, achieves reasonable performance."}, {"title": "3 Transforming RDF to Property Graphs", "content": "In this section, we introduce our approach for transforming RDF, specifically Wikidata, into property graphs as the initial step in building CypherBench (Figure 2). We selected Wikidata because it is the largest and most actively maintained knowledge graph, comprising 114 million entities and having received 270 million edits from over 42,000 active editors in the past 12 months6."}, {"title": "3.1 Domain-specific Schema Curation", "content": "We begin by selecting a domain and manually curating the property graph schema, along with the mapping from the Wikidata schema to this schema. This process typically involves identifying the entity and relation types and their properties relevant to the domain, followed by exploring Wikidata to find the corresponding Wikidata identifiers (QIDs for entity types and PIDs for relation types and properties). Sample entity and relation schemas are shown below:"}, {"title": "3.2 Automatic RDF-to-property-graph Transformation", "content": "Next, our RDF-to-property-graph engine issues SPARQL queries to Wikidata, using the identifiers from the curated schema to fetch all entities and relations that conform to the schema. For example, the following SPARQL query (simplified for illustration) fetches all Wikidata award received (P166) relations where the subject is an instance of Film (Q11424) and the object is an instance of Award (Q618779). These relations are then converted into receivesAward relations in the target property graph.\nThe actual conversion engine further incorporates the following functionalities:"}, {"title": "Datatype conversion", "content": "The engine enforces type constraints on property values by converting them into one of the following types and discarding those that cannot be converted: str, int, float, date, list[str]."}, {"title": "Date conversion", "content": "Wikidata stores precision for time values ranging from seconds up to centuries or more (e.g., a historical event might be recorded with century-level precision like 18th century). For date properties, we retrieve the precision using the predicate wikibase:timePrecision and keep only those with a precision at least as fine as a date, which are ultimately mapped to Neo4j's Date values."}, {"title": "Unit standardization", "content": "The engine enforces standardized units (e.g., centimeters) on property values that represent quantities by converting all values that can be converted. This eliminates the need for unit conversion during retrieval and ensures accuracy of aggregation queries."}, {"title": "Rank filtering", "content": "Wikidata uses rank (wikibase: rank) to indicate the reliability or recency of a relation, which can be one of three values: preferred, normal, or deprecated. For time-sensitive relations (e.g., the president of the United States), the currently valid entries are typically marked as preferred, while previously valid entries are marked as normal with additional properties indicating the relevant time period. For time-sensitive relations with time qualifiers in the target schema, we fetch all Wikidata relations with non-deprecated ranks, along with their starting and ending times. For other types of relations, we fetch only the highest-ranked available relation."}, {"title": "Selective entity fetching", "content": "The engine supports fetching only entities linked to certain relations to avoid out-of-memory issues for very broad entity types (e.g., people or organizations). For instance, in the movie graph, instead of fetching all instances of Person from Wikidata, the engine limits the fetch to only those connected to a Movie through relations like directedBy or hasCastMember.\nThe SPARQL queries issued by the engine are executed against a local Wikidata endpoint loaded with the April 2024 Wikidata dump, allowing us to bypass the time limit of the public endpoint, and the results are aggregated into the final property graph. The transformation time ranges from seconds to hours, depending on the graph size. The property graph is stored in a DBMS-independent JSON format and ultimately deployed using a custom Neo4j Docker image that initializes the data from the JSON file upon startup."}, {"title": "4 Constructing Questions", "content": "4.1 Graph Retrieval via Text-to-Cypher\nWith 11 large-scale property graphs as a testbed, the next step is to construct questions that require graph retrieval to be answered. The most straightforward approach for graph retrieval is to translate the question into a graph database query (e.g., Cypher) that fetches the relevant information or the answer. Alternative approaches used in previous KBQA studies (see Table 5 for an overview of existing graph retrieval methods), such as top-k embedding-based retrieval, typically cannot handle complex aggregation queries where the answer may depend on thousands of entities. While we adopt text-to-Cypher as the primary graph retrieval approach and develop a benchmark consisting of (question, Cypher) pairs, we also record execution results of the Cypher queries as answers so that it can serve as a generic KBQA benchmark for evaluating non-Cypher-based approaches.\nA text-to-Cypher task can be formulated as follows: given the graph schema7 and a natural language question as input, the goal is to output an executable Cypher query that returns the desired answer. We require the Cypher query to produce the final answer on its own, thus eliminating the need for an additional answer generation step with an LLM as in a standard RAG pipeline.\nOur Text-to-Cypher task generation pipeline involves two main steps: 1) generating initial (question, Cypher) pairs with diverse graph patterns using templates, and 2) rewriting the questions to sound more natural using a LLM."}, {"title": "4.2 Preliminaries: Cypher Query Structure", "content": "A Cypher query typically begins with a MATCH clause, which identifies the subgraphs that match the specified graph pattern. Following this, the remaining Cypher clauses perform various transformations\u2014such as filtering, ranking, or aggregation\u2014to generate the desired result. For simplicity, we refer to all clauses that follow the MATCH clause as the RETURN clause, which may include WHERE, WITH, ORDER BY and RETURN clauses."}, {"title": "4.3 Graph Pattern Design", "content": "At the core of graph retrieval is the task of locating the subgraph relevant to the query, which is a fundamental feature of all mainstream graph database query languages (e.g., MATCH clauses in Cypher, WHERE clauses in SPARQL, etc.). To ensure a balanced distribution across various graph matching patterns, we adopt a template-based generation approach rather than crowd-sourcing (as shown in Table 7, even crowd-sourced multi-hop QA benchmarks like HotpotQA tend to be biased toward only a few types of graph matching patterns).\nGraph patterns can be categorized based on the isomorphism structure of an undirected graph (see Table 1 for sample patterns and Table 11 for the complete notations). As shown in Table 11, we define seven basic graph patterns, covering all possible isomorphism structures with a single answer node and up to two edges. Additionally, we design five special graph patterns that cover comparison, grouping, optional matching, time-sensitive queries, and union.\nWe compare the graph patterns covered by representative benchmark in Table 7. A notable observation is that most existing KBQA benchmarks overlook global queries that GraphRAG targets [1], which we define as queries without any specific named entities. These can range from simple listing queries like \"Q13. List the names of all teams\" to more complex ones like \"Q7. What are the unique countries of citizenship of individuals who both wrote and acted in the same movie?\u201d ().\nThe answers to these global queries typically depend on a large number of documents and cannot be easily handled by standard RAG approaches."}, {"title": "4.4 Text-to-Cypher Task Generation", "content": "4.4.1 MATCH Clause Instantiation\nFor each graph pattern, we create multiple Cypher MATCH clause templates by enumerating all possible edge directions, with each MATCH template paired with a human-written question template. For example, one of the (question, Cypher) template for pattern \u2610is (\"MATCH (n)-[r0]->(m0<name>)\", \u201c${n_LABEL} that ${r0_LABEL} ${m0_name}\u201d). Next, the template is instantiated by sampling entity types for node variables (n, m0), relation types for edge variables (r0), and entity names for named"}, {"title": "4.4.2 RETURN Clause Instantiation", "content": "Each instantiated MATCH clause in the special categories is paired with its dedicated RETURN clause template, while those in the basic categories are paired with one of the six templates (showns in Table 12) that covers basic property fetching (NAME, PROPERTY), ranking (SORT), filtering (WHERE) and aggregation (AGGREGATE, ARGMAX). The RETURN clause template is instantiated by sampling properties, ranking orders, comparison operators (e.g., \u2264, \u2260) and aggregate functions (e.g., min, avg, count). We take the datatype into account when sampling operators and aggregate functions to ensure the question is realistic. For instance, we do not allow < on string properties or avg on dates. Similar to MATCH clauses, each RETURN clause also has a textual template that is instantiated and combined with the one for the MATCH clause to form the complete question.\nOne subtle design choice we made is to ensure the Cypher always returns literal values, such as entity names, instead of node objects. This allows the benchmark to be used to evaluate non-Cypher graph retrieval methods in the future."}, {"title": "4.4.3 Detecting Semantically Unrealistic Questions", "content": "Blind sampling often results in semantically unrealistic or uninteresting questions, such as \"Who is married to someone married to Rhaenyra Targaryen?\" This issue has been noted in previous KBQA studies [29], where it was addressed using heuristics that might incorrectly exclude realistic questions. In this work, we take a more systematic approach by modeling the cardinality and participation characteristics of relationships:"}, {"title": "Cardinality", "content": "A directional relationships can have one of four cardinalities: one-to-one, one-to-many, many-to-one, or many-to-many. For example, hasSpouse represents a one-to-one relationship. Cardinality is used to detect unrealistic groupings-groupings that would always result in a single member per group (e.g., \"For each character, return the number of fathers.\"), as well as unnecessary consecutive inverse relations, as in the hasSpouse example."}, {"title": "Participation", "content": "The participation of the subject or object in a relationship describes whether its entity instances are always associated with that relationship. Participation can either be total (e.g., Movie in directedBy, assuming every movie has a director) or partial (e.g., Movie in"}, {"title": "Entailment", "content": "A relationship can imply another (e.g., hasFather implies hasParent). This information is also used to detect redundant conditions."}, {"title": "4.5 Question Rewriting and Verification", "content": "We employ LLMs to rewrite the template-generated questions into more natural-sounding questions and to diversify their phrasing. However, we intentionally preserve entity names and string values to avoid introducing the additional complexity of entity linking. This design choice allows us to evaluate LLMs directly through prompting without introducing external linkers or retrievers, leaving the task of entity linking for future work.\nWe observe that LLMs sometimes alter the meaning of the question during rewriting. For example, a common mistake is reversing the direction of relations (e.g., confusing \"companies that are subsidiaries\" with \u201ccompanies that have subsidiaries\u201d). To address this, we implement three rounds of verification and revision using LLMs. Finally, the authors inspect all instances in the test set to ensure they are correct."}, {"title": "5 Evaluation Metrics", "content": "5.1 Execution Accuracy (EX)\nExecution accuracy, the standard metric in text-to-SQL evaluation, measures whether the results returned by the predicted query match those returned by the ground truth query. Cypher returns results in a tabular format, thus allowing us to borrow the execution accuracy implementation from the text-to-SQL literature. In this work, we adapt the execution accuracy implementation from the Spider [30] leaderboard, which considers two tables as identical if one can be transformed into the other through row and column permutations."}, {"title": "", "content": "EX(q, q) = 1v_v(V, V) (1)\nV=V\nwhere V and V are the execution results of the ground-truth and predicted Cypher. The final dataset-level metric is obtained by averaging across all instances. Note that execution accuracy can also be applied to non-Cypher-based graph retrieval approaches in future research, as long as the approach returns results in the tabular format."}, {"title": "5.2 Provenance Subgraph Jaccard Similarity (PSJS)", "content": "As discussed in subsection 4.3, the core task of graph retrieval is to locate the relevant subgraph using the MATCH clause. While a LLM might generate the correct MATCH clause, it can make subtle mistakes such as returning node objects instead of entity names, or including an extra column that was not requested by the question. In other cases, the MATCH clause might be partially correct, either missing or including a few extra entities. All these scenarios would result in zero execution accuracy.\nWe propose Provenance Subgraph Jaccard Similarity (PSJS) as an isolated measure of the subgraph matching performance. We define the provenance subgraph as the subgraph matched by the MATCH clause, which can be obtained by pairing the MATCH clause with RETURN *. For example, the provenance subgraph for \"Q18. What is the average longest lifespan of taxa that feed on Leporidae?\" would include the entity Leporidae and all taxa that feed on Leporidae. PSJS is then calculated as the Jaccard similarity a standard metric for comparing two sets between the provenance subgraph"}, {"title": "", "content": "PSJS(q, q) = GOG (2)\nGUG\nwhere G and G are the provenance subgraphs of the ground-truth and predicted Cypher.\nAs another example, a predicted Cypher query that satisfies only one condition in a UNION query would receive an execution accuracy of 0 and a PSJS score equal to the fraction of correctly retrieved nodes."}, {"title": "6 Experiments", "content": "6.1 Evaluation Details\nWe deployed the graphs using a custom Neo4j Docker image 11 on a local server with 1TB memory. Since the Neo4j community edition does not support multiple databases, we ran a separate Docker container for each graph.\nTo evaluate the zero-shot text-to-Cypher performance of state-of-the-art LLMs, we run a variety of popular LLMs of different sizes on the CypherBench test set12. For each task instance, the model was prompted with the question, the graph schema, and a brief instruction. The complete prompt is shown in Appendix A.3. The open-source models and yi-large were run using the Fireworks AI API, gemini1.5 and claude3.5-sonnet were run on Google Cloud Vertex AI, while gpt- models were run using OpenAI's API. The cost per run is $5.5 for gpt-40 and $0.3 for gpt-40-mini.\nFinally, the predicted Cypher queries were executed on Neo4j using 8-thread parallelization with a 120-second timeout (4x the maximum execution time of the ground-truth Cypher) to compute the metrics."}, {"title": "6.2 Main Results", "content": "As shown in Table 3, the best-performing model claude3.5-sonnet achieves an execution accuracy of 61.58% and a PSJS of 80.85%, with gpt-40 performing slightly worse. The highest-performing open-source model reaches only 41.87% execution accuracy, while smaller models in the <10B parameter range achieve less than 20% execution accuracy. These results highlight the difficulty of CypherBench.\nFurthermore, the low PSJS scores across most models indicate that the challenges are not merely due to basic formatting errors (e.g., including an extra column or duplicate entries) but stem from fundamental graph matching issues. In addition, smaller models within the same family perform significantly worse (as seen in the gpt-, llama-, and gemini- series), highlighting the benchmark's effectiveness in differentiating LLM capabilities."}, {"title": "6.3 Performance Across Graph Matching Patterns", "content": "Next, we analyze the performance breakdown across various dimensions. For this analysis, we focus on gpt-40, claude3.5-sonnet, qwen2.5-72b, and 11ama3.1-70b, which represent the top 2 performing proprietary LLMs and the top 2 open-source LLMs.\nIn Figure 5, the four charts on the left show the execution accuracy and PSJS of these models across various graph matching patterns. Among the basic categories, all models exhibit similar trends-achieving near-perfect accuracy on pattern while performing worst on pattern The PSJS chart, which evaluates graph matching alone, shows a consistent gradual decline in performance as the graph patterns include more relations.\nComparing the EX and PSJS charts provides insight into whether errors are caused by graph matching. For example, all models achieve near-perfect PSJS scores but low EX on pattern. Upon manual inspection, we identified that most errors for this pattern result from incorrect deduplication-merging distinct entities that have the same name.\nWithin the special categories, models display varying weaknesses across different patterns. For instance, gpt-40 struggles with time-sensitive questions, whereas claude3.5-sonnet performs poorly on comparison questions."}, {"title": "6.4 Performance Across RETURN Templates", "content": "The top right chart in Figure 5 displays execution accuracy across the RETURN templates. Here, the models also demonstrate different weaknesses depending on the template. Interestingly, claude3.5-sonnet achieves near-zero accuracy on SORT questions. Upon closer inspection, we observed that it frequently includes the variable used to rank the entities as an extra column, even though the questions only request the entity names, thus resulting in zero execution accuracy (however, PSJS is 1.0 in most of these cases since it is designed to be independent of the RETURN clause)."}, {"title": "6.5 Performance Across Domains", "content": "The bottom right chart in Figure 5 shows the execution accuracy across differnt domains. All four models exhibit similar trends, with flight_accident and nba being the easiest, while showing comparable performance across the remaining domains."}, {"title": "6.6 Error Analysis", "content": "We further conduct an error analysis to investigate the types of errors made by LLMs. Specifically, we focus on gpt-40 and llama3.1-8b to examine whether smaller models behave differently from larger ones. For each model, we randomly sample 50 task instances where they make incorrect predictions and annotate the errors observed in each instance. The error category taxonomy is developed during the annotation process (Appendix A.5 shows the definitions of each error category with examples). Two sample predictions are shown in Table 4 and the distribution of error categories is shown in Figure 6. Both models exhibit diverse errors spanning 10 distinct categories. Some error categories that frequently occur in both models include Reversed Direction (where the model reverses the direction of a relation), Entity Linking (where the entity name does not correspond to the intended entity in the database), and Pattern Not Aligned with Question (where the MATCH pattern conforms to the schema but does not align with the question's intent). Compared to gpt-40, llama3.1-8b makes a significantly higher proportion of schema violation errors, indicating its inferior schema following capabilities."}, {"title": "7 Related Work", "content": "7.1 KBQA and Graph Retrieval Methods\nOur work is related to knowledge base question answering (KBQA) as CypherBench serves as a benchmark for evaluating KBQA and graph retrieval methods.\nWe categorize existing KBQA and graph retreival methods into two types (see Table 5): approximate retrieval methods, which identify top relevant elements based on some notion of relevance to the question, and precise retrieval methods, which retrieve exactly what the question specifies by executing a formal language query.\nThe most common approximate retrieval method involves retrieving the k-hop neighborhood of the entities mentioned in the question [32, 33, 34, 35, 36, 37, 38, 39, 40, 23, 25]. Another line of work verbalizes entities or relations into text and uses embedding-based methods to retrieve the top-k most relevant elements [41, 13, 42, 24]. The fundamental limitation of these methods is their inability to"}, {"title": "7.2 Text-to-Query and KBQA Benchmarks", "content": "CypherBench takes the form of a text-to-query benchmark, consisting of databases along with (question, database query) pairs. KBQA benchmarks represent a specific type of text-to-query benchmarks, where the databases are knowledge graphs, and the queries are graph database queries. In Table 6, we compare CypherBench with current representative text-to-query benchmarks.\nLooking at the Schema Size and Data Size columns provides insights into the complexity of the databases in existing text-to-query benchmarks, both in terms of their schemas and stored data. Most existing KBQA benchmarks [3, 4, 5, 6, 7] are predominantly based on text-to-SPARQL over RDF knowledge graphs. However, as discussed in section 2, the massive schema of RDF knowledge graphs poses significant challenges when using these benchmarks to evaluate LLMs in zero-shot settings. In contrast, the graphs in CypherBench have a schema size comparable to those in text-to-SQL benchmarks [30, 48], while still encompassing up to 7 million entities.\nIn recent years, several benchmarks focusing on text-to-Cypher or text-to-nGQL13 have been proposed [15, 51, 49, 50, 54]. MetaQA-Cypher [15] and SpCQL [51] are the earliest efforts to develop text-to-Cypher benchmarks. MetaQA-Cypher is adapted from MetaQA [55], a KBQA dataset built on a movie knowledge graph, with Cypher queries annotated using rule-based methods. SpCQL is based on OwnThink\u00b94, a Chinese encyclopedic knowledge graph. The questions in SpCQL were collected from online forums and annotated with Cypher queries by database professionals. [49]"}, {"title": "7.3 GraphRAG", "content": "Recently, Microsoft introduced GraphRAG [1] to address corpus-level summarization queries (e.g., \"What are the main themes in the dataset?\"), which are similar to the global queries explored in this work and cannot be handled by standard top-k embedding-based retrieval methods. At a high level, GraphRAG leverages a centralized knowledge graph to index textual documents, enabling it to handle queries that rely on a large volume of documents. The GraphRAG system has two main stages: knowledge graph construction during indexing time and graph retrieval during query time [8]. It is worth noting that the original GraphRAG system in [1] uses a graph formalism slightly different from a typical knowledge graph, where the nodes are entity communities at various abstraction levels, with retrieval performed by fetching all communities at a specific level.\nSubsequently, LlamaIndex, the leading open-source LLM framework for RAG workflows, introduced the Property Graph Index [31] for general-purpose question answering. It constructs a Neo4j property graph from textual documents using LLMs during indexing time, and conducts graph retrieval via text-to-Cypher during query time. Our work provides the first comprehensive text-to-Cypher benchmark for evaluating graph retrieval, a critical component in GraphRAG."}, {"title": "7.4 Mapping RDF to Property Graphs", "content": "Several studies from the semantic web community have explored methods for transforming RDF graphs into property graphs [57, 58, 59, 60, 61]17. However, many of these methods require processing the entire RDF dump, which can be computationally expensive for full-size modern RDF graphs like Wikidata. For example, [57] proposed a two-step process for RDF*, an RDF extension: first, RDF triples are mapped directly to edges in the property graph, and then edges that represent entity properties are transformed into node properties. An exception is [61] 18, which adopts an approach similar to ours by transforming RDF into property graphs through executing SPARQL queries over RDF. However, their method lacks key functionalities described in section 3 which are essential for ensuring the output quality."}, {"title": "7.5 Knowledge Graph Subsetting", "content": "There are also a few tools developed to extract domain-specific subgraphs of Wikidata or general RDF knowledge graphs to tackle the scalability challenges of modern knowledge graphs19. For example, KGTK [63], WDumper20, and WDSub21 are a few such tools [64]. However, these tools also operate by processing the entire RDF dump and produce RDF as output."}, {"title": "8 Conclusion", "content": "Since its inception, Wikidata has received over 2 billion edits by users worldwide and continues to be actively maintained by over 42,000 editors in the past year, making it one of the most comprehensive knowledge sources available today. This study offers a viable pathway for integrating full-scale modern knowledge graphs like Wikidata with LLMs. The techniques we proposed, along with the numerous design choices made throughout this study, are all centered around accomplishing this goal. We believe our work offers new research opportunities in the areas of knowledge graphs and large-scale graph retrieval."}, {"title": "A Additional Technical Details", "content": "A.1 Graph Matching Patterns and RETURN Templates\nThe complete list of graph matching patterns is shown in Table 11 and the complete list of RETURN templates is shown in Table 12.\nA.2 Question Rewriting Prompt\nThe prompt used to rewrite template-generated questions into more natural-sounding ones is presented in Table 9.\nA.3 Text-to-Cypher Prompt\nThe text-to-Cypher prompt for evaluating LLMs is shown in Table 10.\nA.4 Schema Fetching in Neo4j\nIn a practical text-to-Cypher scenario where only the Neo4j database endpoint (host and port) is provided, the graph schema must be retrieved from the database by executing certain Cypher queries. Neo4j provides built-in procedures such as db.schema.visualization and apoc.metadata for this purpose. However, we observed that both methods yield inaccurate results when applied to large graphs: db.schema.visualization may return non-existent relationships, while apoc.meta.data can miss certain relationships. To address this issue, we use the following queries to retrieve the schemas:"}, {"title": "Cypher for fetching entity property schemas", "content": "MATCH (n)\nUNWIND labels (n) AS label\nWITH label, keys (n) AS propertyKeys, n\nUNWIND propertyKeys AS property\nWITH DISTINCT label, property,\napoc.meta.cypher.type (n[property]) as type\nWITH label AS label, apoc.coll.sortMaps (collect({\nproperty: property, type: type}),\n'property') AS properties\nRETURN label, properties ORDER BY label"}, {}]}