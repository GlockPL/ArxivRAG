{"title": "Exploiting Hybrid Policy in Reinforcement Learning for Interpretable Temporal Logic Manipulation", "authors": ["Hao Zhang", "Hao Wang", "Xiucai Huang", "Wenrui Chen", "Zhen Kan"], "abstract": "Reinforcement Learning (RL) based methods have been increasingly explored for robot learning. However, RL based methods often suffer from low sampling efficiency in the exploration phase, especially for long-horizon manipulation tasks, and generally neglect the semantic information from the task level, resulted in a delayed convergence or even tasks failure. To tackle these challenges, we propose a Temporal-Logic-guided Hybrid policy framework (HyTL) which leverages three-level decision layers to improve the agent's performance. Specifically, the task specifications are encoded via linear temporal logic (LTL) to improve performance and offer interpretability. And a waypoints planning module is designed with the feedback from the LTL-encoded task level as a high-level policy to improve the exploration efficiency. The middle-level policy selects which behavior primitives to execute, and the low-level policy specifies the corresponding parameters to interact with the environment. We evaluate HyTL on four challenging manipulation tasks, which demonstrate its effectiveness and interpretability. Our project is available at: https://sites.google.com/view/hytl-0257/.", "sections": [{"title": "I. INTRODUCTION", "content": "A primary objective in robotic learning is to enable the robot to automatically plan key points to accomplish a challenging task like humans acting with instructions. To achieve such human-level intelligence, it is essential to understand the semantics of instructions and predict the important states from the task feedback is crucial. Among numerous learning algorithms, reinforcement learning (RL) [1] exhibited strong potential in various applications [2]-\n[4]. While RL-based methods have empowered the agent to complete tasks from simple to complex ones, a significant yet challenging topic is how effectively the robots can plan key states as sub-goals from the task level to ease the exploration burden and even improve the agent's performance for long-horizon tasks. In detail, there are three key challenges: 1) In contrast to traditional manipulation methods that learn from demonstration, how can the robot plan the key states on its own to reduce the burden on exploration? 2) When manipulating a long-horizon task, how to design an effective decision-making process to ease the exploration burden and incorporate the task semantics to facilitate the learning efficiency? 3) When considering long-horizon tasks, how to interpret the robot motion planning in task level?\nHierarchical reinforcement learning (HRL) that combines high-level policy facilitating the accomplishment of long-horizon tasks and low-level control policies has shown superior performance over conventional RL in a number of domains such as game scoring [5] and [6] as well as motion planning [7] and [8]. When performing HRL in the field of robot manipulation, a hierarchical policy was developed in [9] that chooses the action primitives and executes the corresponding parameters to accomplish challenging manipulation tasks. A trajectory-centric smoothness term is incorporated in [10] to enhance the generalization in manipulation by using dynamic time warping to align different trajectories to measure the distance. Empirical studies have found that the performance advantage of HRL is mainly attributed to the use of sub-goal for augmenting exploration. However, many existing hierarchical architectures are designed to be implemented directly at environment-level manipulation, where the feedback of interaction determines the quality of decision, lacking the connection with task semantics to guide the robotic manipulations.\nOwing to the rich expressivity and capability, linear temporal logic (LTL) can describe a wide range of complex tasks combined by logically organized sub-tasks [11]-[13]. By transforming the LTL formula into an automaton, learning-based algorithms can be leveraged to solve manipulation tasks in robotic systems. For example, a non-deterministic B\u00fcchi automaton is exploited in a high resolution grasp network (HRG-Net) [14] to facilitate reactive human-robot collaboration in a locally observable transition system. Truncated LTL is transformed to a finite-state predicate automaton (FSPA) to facilitate the reward design and improve the performance of manipulation in [15]. Similar to the automaton, the representation module is also proposed by representing the LTL specification to guide the agent for complex tasks [16]. To augment the representation ability, the work of [17] uses Transformer [18] to express the semantics of LTL tasks for the robot's manipulation. However, it cannot provide the agent with specific guidelines about how to implement the LTL instructions from the task level to the concrete environment. The work of [19] develops a hierarchical setting to guide the robot to move in complex environments by waypoints. However, its architecture makes it difficult to address long-horizon manipulations and lacks the ability to provide interpretable guidance.\nTo bridge the gap, we consider planning the key points to ease the exploration burden of the policy from the task level, and incorporate the task semantics to provide meaningful interpretability for manipulations. The key contributions of this work are outlined below:\n1) We develop a Temporal-Logic-guided Hybrid policy framework (HyTL), which not only leverages a hybrid decision-making process to facilitate the learning, but also incorporates task semantics to improve performance.\n2) We design a novel waypoints planning module to ease the exploration burden, which exploits the task feedback to guide the agent interacting from the task level, empirically improving the agent's sampling efficiency.\n3) By leveraging gradients and disentangling features of the task representation module, the interpretability of motion planning guided by LTL specifications is further improved.\n4) We evaluate HyTL's performance on four challenging manipulation tasks with five baselines, which exhibit higher learning efficiency, better performance and interpretability compared to other methods especially to [17]."}, {"title": "II. PRELIMINARIES", "content": ""}, {"title": "A. Co-Safe LTL and LTL Progression", "content": "Co-safe LTL (sc-LTL) is a subclass of LTL satisfied by finite-horizon state trajectories [20]. An sc-LTL formula is built on a set of atomic propositions \u03a0 that can be true or false, standard Boolean operators such as \u2227 (conjunction), \u2228 (disjunction), \u00ac (negation), as well as temporal operators like \u25c7 (eventually). The semantics of an sc-LTL formula are interpreted over a word \u03c3 = \u03c30\u03c31...\u03c3n, which is a finite sequence with \u03c3i \u2208 2\u03a0, where i = 0, . . ., n.\nLTL formulas can also be progressed along a sequence of truth assignments [21]\u2013[23]. Specifically, give an LTL formula \u03c6 and a word \u03c3 = \u03c30\u03c31..., the LTL progression prog(\u03c3i, \u03c6) at step i, \u2200i = 0, 1,..., is defined as\nprog(\u03c3i, p) = True if p \u2208 \u03c3i, where p \u2208 \u03a0 and\nprog(\u03c3i, p) = False otherwise."}, {"title": "B. Reinforcement Learning and Labeled PAMDP", "content": "A parameterized action Markov decision process (PAMDP) [5] provides a primitive-augmented RL framework to address long-horizon tasks. The whole dynamics between the agent and environment over the sc-LTL tasks can be modeled as a labeled PAMDP Me = (S, T, H, pe, \u03a0, L, R, \u03b3, \u03bc), where S is the state space, T indicates the horizon, H = {h : (k, xk) | xk \u2208 Xk for all k \u2208 K} is the discrete-continuous hybrid action space where K = {1, ..., K} is the set of discrete behavior primitives and Xk is the corresponding continuous parameter set for each k \u2208 K, pe(s'|s, h) is the transition probability from s \u2208 S to s' \u2208 S under action h = (k, xk), \u03a0 is a set of atomic propositions, L : S \u2192 2\u03a0 is the labeling function, R : S \u2192 R is the reward function, \u03b3 \u2208 (0, 1] is the discount factor, and \u03bc is the initial state distribution. A hybrid policy \u03c0\u03b8 is exploited to interact with environment under the task \u03c6, which outputs a hybrid action pair h and receives the corresponding reward by rt = R(st)."}, {"title": "III. PROBLEM FORMULATION", "content": "In order to further elaborate the motivation of the proposed interpretable temporal-logic-guided hybrid decision-making framework, we will use the following example throughout the work to illustrate the main idea of our method.\nExample 1. Consider a long-horizon manipulation skill of Peg Insertion from [9] as illustrated in Fig. 1, in which the robot needs to grasp the peg Opeg and inserts it into the hole Ghole guided by the planned waypoints. The set of propositions \u03a0 is {peg_grasped, hole_reached, peg_inserted}. Using above propositions, an sc-LTL task is \u03c6peg = \u25c7(peg_grasped \u25c7(hole_reached \u25c7 peg_inserted)), which requires the robot to sequentially grasp the peg, reach the hole and insert it into the hole.\nIn this work, we are interested in designing a temporal-logic guided hybrid policy architecture, which not only plans key waypoints based on the task semantics, but also guides the agent through the waypoints to choose appropriate behavioral primitives and the corresponding parameters to facilitate robot learning. By predicting the key states via the planning module, we hope to take advantage of its foresight to generate hypothetical goals to guide the agent, and ease the exploration burden of the robot's motion planning. Compared to hierarchical architectures [9] that are committed to manipulation, the waypoints generated by the planning module are gradually updated according to LTL instructions to guide the agent towards sub-goals as quickly as possible, resulting in a three-way improvement, in which the LTL instructions and environmental rewards boost the planning module for better waypoints, the waypoints guide the robot's manipulation, and the output actions improve Transformer for better tasks semantics.\nBy exploiting the LTL progression, an augmented PAMDP with an LTL instruction \u03c6, namely the task-driven labeled PAMDP (TL-PAMDP), is developed as M\u03c6 = (\u0160, \u0128, H, pe, \u03a0, L, \u0158, \u03b3, \u03bc) like [17] (Detailed definitions can be found on our website), where the reward function is\n\\begin{equation}\nR(s, \\varphi) = \\begin{cases}\nr_{\\text{env}} + r_{\\varphi}, & \\text{if prog}(L(s), \\varphi) = \\text{True} \\\\\nr_{\\text{env}}, & \\text{if prog}(L(s), \\varphi) = \\text{False}. \\\\\nr_{\\text{env}}, & \\text{otherwise}\n\\end{cases}\n\\end{equation}\n\\text{(1)}\nThe problem of this work can be stated as follows.\nProblem 1. Given a TL-PAMDP M\u03c6 = (\u0160, \u0128, H, pe, \u03a0, L, R, \u03b3, \u03bc) corresponding to task \u03c6, this work is aimed at finding an optimal policy \u03c0\u03c6 over the LTL instruction \u03c6, so that the return E[\u2211k=0\u03b3krt+k | s0 = S] under the policy \u03c0\u03c6 \u22c6 can be maximized."}, {"title": "IV. ALGORITHM DESIGN", "content": "To address Problem 1, we present a novel approach, namely Temporal-Logic-guided Hybrid policy framework (HyTL), that offers a hybrid policy to plan waypoints, choose primitives, and determine parameters based on LTL representation encoded by Transformer. Section IV-A presents how the plan module in HyTL generates hypothetical waypoints to improve sampling efficiency. Section IV-B explains in detail how hybrid decision-making architecture facilitates the agent's training in long-horizon manipulations. Section IV-C shows how AttCAT interprets the LTL instruction for the robot's manipulation. The overall method of HyTL is illustrated in Fig. 2, which first extracts the LTL representation e by the task representation module, then samples a series of waypoints w based on the initial state s0 by the waypoints planning module, and outputs the action pair h = (k, xk) by selecting the appropriate action primitive k following the primitives choosing module and determines the corresponding parameters xk following the parameterization determining module."}, {"title": "A. Plan Waypoints from Task Feedback", "content": "One of the main challenges in solving Problem 1 is sampling efficiency in the exploration phase when utilizing RL for long-horizon manipulations. To address this problem, [17] considers augmenting the hierarchical policy with task semantics to enable agent making decisions corresponding to the sub-goal and accelerate learning. However, it is difficult for this approach to map abstract task semantics into a concrete manipulation. Therefore, we design a planning module that incorporates task feedback to generate waypoints to guide the agent by combining task rewards.\nWaypoints Planning Module. As shown in Fig. 1, we exploit the Gated Recurrent Unit (GRU) as a predictor in the form of a residual connection to incrementally construct waypoints to guide agents. Specifically, given an initial state s0 as the initial waypoints w0 and hidden state h0, the sequential waypoints w = w0w1... can be generated by the deviation between subgoals as wi+1 = wi + \u03b4wi where wi = GRU(wi, hi) and wi \u2208 R3. Since it's hard for GRU to update directly through rewards, we model the plan module as a stochastic policy with Gaussian distribution inspired by [19], i.e., \u03c0\u03c9s = N(w, \u03c3) with weights \u03c3 (where w is the mean of waypoint wi and \u03c3 is predicted by a linear transform from the hidden state hi. Thus a sequential waypoints w = w0w1... can be sampled from \u03c0\u03c9s(s0) and if the agent reaches wi, wi+1 will be exploited to guide. To bridge the environment and the feedback of the task (i.e., satisfy the LTL task \u03c6), we design the following loss function\n\\begin{equation}\nL_{\\pi_{\\omega}}(s) = \\mathbb{E}\\_{\\substack{\\omega \\sim \\pi_{\\omega s}\\\\s=s\\_0s\\_1...}}[-\\log \\pi_{\\omega s} \\cdot R_{\\varphi}(s, \\varphi)]\n\\end{equation}\nto update the planning module, where R\u03c6(s, \u03c6) is the cumulative reward from the state sequence s = s0s1... corresponding to w. Thus when the planning layer is updated, it not only uses the environment-level reward renv, but also exploits the task-level reward r\u03c6 to continuously optimize the waypoints.\nAs shown in Fig. 2(c), by using the waypoint planning module as a high-level policy layer, not only can the task-level semantics be taken over to guide the agent at the environment, but also the residual structure can be exploited to provide the agent with more flexible guidance by setting up more waypoints in hard-to-explore areas."}, {"title": "B. Hybrid Policy based on LTL Instructions", "content": "Another challenge in solving Problem 1 is to design an effective decision-making architecture to improve the agent's performance in long-horizon manipulations. To address this challenge, the work of [9] pre-built five behavioral primitives (reach, grasp, push, release, and atomic), and designed a hierarchical architecture to reduce the agent's exploration burden by first selecting appropriate primitives through a high-level policy and then determining the parameters of primitives via low-level policies. However, [9] focuses on the design of the decision structure, neglecting how task-level information can help the agent accomplish the task. To address this problem, based on LTL representation, this work proposes a hybrid decision-making framework that not only incorporates task semantics to improve the learning efficiency, but also plans waypoints from the task level to guide the agent to accomplish long-horizon tasks. Except for Waypoints Planning Module mentioned in Section IV-A, the HyTL framework shown in Fig.2 comprises following key modules.\nTask Representation Module. To incorporate the task semantics, we use Transfomer [18] to encode LTL representation. Given an input X\u03c6 = (x0, x1,...) generated by the LTL task \u03c6 where xt, t=0,1,..., represents the operator or proposition, X is preprocessed using the word embedding E as XE = [x0E; x1E; ...; xnE] \u2208 RB\u00d7M\u00d7D where B is the batch size, M = N + 1 is the length of input X and D is the model dimension of Transformer. XE is then combined with the frequency-based positional embedding Epos \u2208 RB\u00d7M\u00d7D to utilize the sequence order. Then the LTL representations encoded by Transformer can be computed by following steps:\n\\begin{equation}\n\\begin{aligned}\nX_{0} &= X_{E} + E_{\\text{pos}}, \\\\\nX_{l} &= \\text{MSA}(\\text{LN}(X_{l-1})) + X_{l-1}, l = 1, ..., L \\\\\nX_{l} &= \\text{MLP}(\\text{LN}(X_{l})) + X_{l}, l = 1, ..., L \\\\\nY &= \\text{LN}(X_{L})\n\\end{aligned}\n\\end{equation}\nwhere MSA denotes the multi-head self-attention, LN means the layer norm, MLP represents the position-wise fully connected feed-forward sub-layers, and Y is the output of the final layer from the Transformer encoder. The outline of Transformer Encoder for HyTL is shown in Fig. 2(a). Thus incorporating the LTL representation into HyTL, which not only helps to improve the agent's performance, but also lays the foundation for the interpretability of motion planning shown in Section IV-C.\nPrimitives Choosing Module. Based on the predicted waypoint wi as guidance, the primitive policy outputs a behavioral primitive appropriate for the current state and the progressed LTL instruction. Specifically, the behavior primitive k can be chosen by primitive policy \u03c0k\u03c8 (k|s, \u03c6e, w) with weights \u03c6e conditioning on the waypoint w. As a middle-level policy layer, the primitives choosing module not only considers the information from the task level and environment to offer the suitable primitive, but also guides the low-level parameter module on what to do for long-horizon manipulations.\nParameterization Determining Module. Conditioning on the current state, task representations, predicted waypoints, and selected primitives, the parameterized policy outputs an appropriate set of parameters to ensure effective interactions between the behavioral primitives and the environment. Specifically, the primitive parameter x can be determined by parameter policy \u03c0p\u03c6 (x|s, \u03c6e, w, k) with weights conditioning on the waypoint w and primitive k. As the last layer interacts with the environment, the parameterization determining module, based on the above conditions, instructs the agent on how to interact with the environment to accomplish the tasks.\nIn this work, we opt for SAC [24] as the RL backbone to update in the framework. Let Q\u03c8\u03b8(s, h, w) and Q\u03c8'\u03b8(s, h, w) be the Q-value function of task \u03c6 and \u03c6', and let \u03c0\u03c9s(s), \u03c0k\u03c8 (k|s, \u03c6e, w) and \u03c0p\u03c6(x|s, \u03c6e, w, k) be the hybrid policy networks. The loss for critic, plan policy, primitive policy, and parameter policy in adapted SAC are then designed respectively as\n\\begin{equation}\n\\begin{aligned}\nJ_{Q} &= \\mathbb{E}[(Q_{\\psi\\theta} - (\\hat{R}\\_{\\varphi} + \\gamma (Q_{\\psi'\\theta} - \\alpha \\log \\pi\\_{k\\psi} - \\alpha \\log \\pi\\_{\\rho \\varphi}))))^{2}], \\\\\nJ_{\\pi_{\\omega}}(s) &= \\mathbb{E}\\_{\\omega \\sim \\pi_{\\omega s}}[-\\log \\pi_{\\omega s} \\cdot \\hat{R}\\_{\\varphi}(s, \\varphi)], \\\\\nJ_{\\pi\\_k}(\\psi) &= \\mathbb{E}\\_{\\omega \\sim \\pi_{\\omega s}} \\mathbb{E}\\_{k \\sim \\pi\\_{k\\psi}}[\\alpha \\log \\pi\\_{k} - Q_{\\psi\\theta}], \\\\\nJ_{\\pi\\_{\\rho}}(\\xi) &= \\mathbb{E}\\_{\\omega \\sim \\pi_{\\omega s}} \\mathbb{E}\\_{k \\sim \\pi\\_{k\\psi}} \\mathbb{E}\\_{x \\sim \\pi\\_{\\rho \\varphi}}[\\alpha \\log \\pi_{\\rho \\varphi} - Q_{\\psi\\theta}].\n\\end{aligned}\n\\end{equation}\nNote that \u03c8\u03b8 and \u03c8'\u03b8 encoded by Transformer are indirectly updated by back-propagation of the above equations.\nThe pseudo-code is outlined in Alg. 1. In the exploration phase, Transformer first extracts the LTL representation \u03c6e and concatenates the valve with the state s0 (line 4). Then before the agent interacts with the environment, the waypoint planning module samples a series of waypoints w based on the initial state s0 and incorporates wi as part of the observation to guide the agent's exploration (line 6). Based on the above observation, the agent selects the appropriate action primitive following \u03c0k and determines the corresponding parameters by \u03c0p\u03c6 (line 11). During the interaction, the operator prog tracks the original instructions \u03c6 and checks whether the LTL task \u03c6 is complete (lines 7-10). In the training phase, HyTL updates all neural network weights following (3) (lines 14-16)."}, {"title": "C. Interpret Manipulation via AttCAT", "content": "Another advantage of representing LTL instructions with Transformer is that it provides interpretability for robot motion planning. When LTL specifications are encoded via Transformer [25], their interpretability can be represented by the value of heads' weights on different propositions. However, [25] only sums the weights of all heads, ignoring the effect of the gradients flowing in the Transformer architecture. Inspired by [26], this work further explores the impact of LTL representation on robot motion planning by interpreting Transformer via Attentive Class Activation Tokens (AttCAT), which not only utilizes the inputs\u2019 gradients combined with attention weights to generate impact scores, but also disentangles features flowing between intermediate layers of Transformer.\nBased on (2), we can write columns of Xl separately as xl, i = 0, 1, ..., M. To analyze the effect of i-th token on the output y across different proposition class c in the embedded LTL input X0, the relationship between yc and xi can be modeled by a linear relationship\n\\begin{equation}\ny_{c} = \\sum\\_{i=1}^{N+1} w\\_{c} x\\_{i}, \\text{where}\nw_{c} = \\frac{\\partial y_{c}}{\\partial x_{i}}\n\\end{equation}\nis the linear coefficient vector of x capturing the impact of i-th token on the target class c.\nBy considering the interaction among tokens, the impact score of the i-th token towards class c, denoted by AttCAT, can be formulated through the weighted combination:\n\\begin{equation}\n\\text{AttCAT}\\_{i} = \\sum\\_{l=1}^{L} E\\_{\\mathcal{H}}(\\alpha^{l} \\odot \\frac{\\partial y\\_{c}}{\\partial x\\_{i}}),\n\\end{equation}\nwhere \u2299 denotes the Hadamard product, \u03b1l represents the attention weights of the i-th token at l-th layer, and E\\mathcal{H}(\u00b7) stands for the mean over multiple heads.\nWhen provided with a pre-trained Transformer TF(\u00b7), an input LTL instruction \u03c6, and the interpretability method AttCAT(\u00b7), the magnitude of impact can be calculated by |AttCAT(TF(\u03c6))|, reflecting each token's contribution. By the visualization input tokens scores, the most impactful token on the output can be identified."}, {"title": "V. CASE STUDIES", "content": "In this section, the performance of the HyTL framework is evaluated in comparison to state-of-the-art algorithms through scenarios. We specifically represent the following aspects. 1) Performance: how effectively does HyTL surpass the state-of-the-art algorithm in long-horizon manipulations? 2) Architecture: How good is the waypoints planning module for algorithmic enhancement? 3) Interpretable: How well can the agent comprehend the LTL instruction?"}, {"title": "A. Baselines and Tasks Setting", "content": "Baselines. To demonstrate the efficacy of the HyTL framework, we empirically compare its performance against five baselines. 1) Our RL backbone SAC from [24] is the first baseline, which only executes atomic primitive. 2) The second baseline is Maple from [9] which augments the traditional RL methods with action primitives and corresponding parameters to improve the exploration efficiency. 3) The third baseline is Mapleway, which is based on Maple and exploits the planning module to ease the exploration burden. 4) The fourth baseline is MapleLTL2Action,which further exploits the encoder from [27] to represent semantics of LTL for improving sampling efficiency. 5) The fifth baseline is TARPSTF-LTL, which is a manipulation skill learning algorithm from [17] augmenting RL with temporal logic and hybrid action space.\nTasks Setting. To evaluate the algorithm performances, four challenging manipulations in [9] are employed, including Stack, Nut Assembly, Cleanup and Peg Insertion. The corresponding task descriptions and LTL instructions are stated in Table. II of [17] and Example. 1."}, {"title": "B. Experimental Results and Analysis", "content": "(1) Main Results. Fig. 3 illustrates the results performed by different approaches over 6 seeds. As shown in Fig. 3, it is observed that 1) algorithms guided by waypoints (Mapleway and HyTL) are more efficiently sampled and converge faster than those without waypoints (Maple, MapleLTL2Action, and TARPSTF-LTL); 2) HyTL exhibits better performance relative to TARPSTF-LTL and MapleLTL2Action which only incorporate the task semantics; 3) On the most challenging task Peg Insertion, HyTL demonstrates the shortest converge episode, with over 30% reduction compared to the best previous work TARPSTF-LTL [17].\nFrom the above observation, we conjecture that the unstable representation of the task module at the beginning of training affects the performance of the agents, which rely heavily on the task representation to improve the sampling efficiency. In contrast, the performance of HyTL, which has a hybrid decision architecture design, is not only affected by the task module's representation, but also relies on the waypoints generated by the planning module for guidance. Once either the planning module or the task module has been effectively updated, the gradient decreases rapidly in the direction that contributes to task completion. It is due to this complementary design in the hybrid decision architecture that HyTL can demonstrate higher learning efficiency.\n2) Primitive Compositionality Quantification. Since primitives are utilized in HyTL like Maple, we use the compositionality score from [9] to evaluate the degree of action primitives over different methods. The action sketches of HyTL with 6 seeds are visualized in Fig. 4, which shows the different action primitives that HyTL selects and combines in accomplishing above four manipulation tasks. The compositionality scores are shown in Table I, in which higher scores reflect better compositionality and more stable performance. As shown in Table I, Mapleway and HyTL can select and combine more appropriate behavior primitives by guiding within waypoints, resulting in higher compositionality scores than other methods.\n(3) Interpretability via AttCAT. To further illustrate the agent's understanding of the LTL task, Fig. 5 illustrates the heatmap of the task cleanup. As shown in the top table of Fig. 5, the cumulative scores for all tokens except for the eventually(-0.85) are almost zero, indicating that the agent lacks a clear understanding of the LTL instruction at the beginning of the training. Upon the convergence of Transforme, higher impact scores from all layers focus on the token jello_pushed(+0.50) as shown in the bottom row of Fig. 5, which suggests that the agent is more likely to move directly to the position corresponding to the jello_pushed proposition."}, {"title": "VI. CONCLUSIONS", "content": "In this work, we present an interpretable temporal-logic-guided hybrid decision-making framework to improve the agent's performance on four challenging manipulation tasks. In particular, a novel waypoints planning module is designed to ease the exploration burden, which exploits the task feedback to guide the agent interacting from the task level. And the hybrid decision-making process with three-level decision layers is proposed to facilitate learning of agent's manipulations. In addition, the interpretability of motion planning guided by LTL specifications is further improved by leveraging gradients and disentangling features of the task representation module. Experimental results and analysis demonstrate that HyTL improves the agent's sampling efficiency and offers reasonable interpretability. Future work will consider extending the method of HyTL to more challenging tasks, such as dexterous manipulations."}]}