{"title": "Composing Ensembles of Instrument-Model Pairs for Optimizing Profitability in Algorithmic Trading", "authors": ["Sahand Hassanizorgabad"], "abstract": "Financial markets are nonlinear with complexity, where different types of assets are traded between buyers and\nsellers, each having a view to maximize their Return on Investment (ROI). Forecasting market trends is a\nchallenging task since various factors like stock-specific news, company profiles, public sentiments, and global\neconomic conditions influence them. This paper describes a daily price directional predictive system of financial\ninstruments, addressing the difficulty of predicting short-term price movements. This paper will introduce the\ndevelopment of a novel trading system methodology by proposing a two-layer Composing Ensembles architecture,\noptimized through grid search, to predict whether the price will rise or fall the next day. This strategy was back-\ntested on a wide range of financial instruments and time frames, demonstrating an improvement of 20% over the\nbenchmark, representing a standard investment strategy.", "sections": [{"title": "1 INTRODUCTION", "content": ""}, {"title": "1.1 Background", "content": "A financial market is a nonlinear complex system in which different kinds of traders and investors trade various\nassets to maximize returns on investment [1]. Forecasting financial markets is continuouss and always challenging\nsince the markets keep on dynamically and volatily changing due to many factors that include news about the\ncompany, macroeconomic events, public mood, and international economic conditions. These are interdependent\nvariables, creating an environment in which forecasted trends are very difficult to achieve [2].\nThe years have thrown up, so to say, several forecasting methods from traditional statistical models to more\nmodern techniques involving machine learning. However, the attainment of consistent, reliable prediction has\nremained elusive given the inherent stochasticity of financial data. More relevant to traders, investors, and financial\ninstitutions than the accurate prediction of the increase or decrease in the price of an asset on a day-to-day basis,\neven minor improvements in the direction of the prediction may yield substantial financial benefits.\nThis development represents one of the more important growths in finance: algorithmic trading [3]. The\nautomation of the decision to trade comes via the power of computer algorithms according to predefined parameters.\nOperating near and beyond human capabilities in speed and frequency, they leverage historical and real-time data\nto predict near-term price movements.\nAlgorithmic trading has become an integral part of financial markets today, leveraging the speed and precision\nof automated systems to make trades based on a predetermined set of criteria. This form of trading revolutionized\nthe way trading occurs, now allowing complicated strategies in real time that would have been impossible to be\nmanually undertaken by humans. Algorithmic trading systems use an immense amount of historical and real-time\ndata to forecast near-term price movements, hence enabling the trader to gain from market inefficiencies at speeds\nnot even conceivable for human traders. The need for fast and exact trading decisions has made algorithmic trading\na very crucial element of today's financial markets."}, {"title": "1.2 Research Problem", "content": "Despite all the progress in the development of algorithmic trading, the stumbling block remains the challenge of\nachieving high accuracy and profitability on a continuous basis. While most models in existence suffer from being\nfocused on one instrument or some grouping of instruments, which makes them inflexible and adaptable only under"}, {"title": "1.3 Contribution", "content": "This paper is aimed at the weaknesses in several of the available algorithmic trading models, hence proposing a\nflexible, multi-model, multi-instrument trading system to improve accuracy and increase ROI. Unlike traditional\nmodels that focus on one single instrument or very narrow range of assets, our approach allows users to select a list\nof instruments and to dynamically adjust models and instruments while trading. This flexibility provides more\noptions for trading and helps the system avoid downtrends in specific instruments or markets. The model also allows\nfor instrument selection depending on the user's preference. This will be useful in cases where the use of certain\ninstruments is restricted by regulations-such as for some companies or in some countries. This extends the system\nto more users and further extends its usability across various regulatory frameworks.\nThis system, by incorporating a number of models and instruments, is more capable of adapting to different\nmarket conditions and behaviors of instruments. This two-layer algorithm trains multiple models of various\ninstruments in the first layer and, in turn, uses a VotingClassifier to determine the best instrument-model pairs in\nthe second layer according to a range of performance metrics including accuracy, precision, and backtest result.\nThe essence of this multi-instrument, multi-model approach is to increase flexibility for the system to adapt into\ndifferent market phases and therefore to enhance predictive accuracy and financial returns.\nThis paper's contribution in one major way is that it will allow the users themselves to decide on instruments,\nembedding user reliance in it. This will not only enhance user flexibility but also widen the horizons of the system\nitself-that is, more options to trade and adapt to various market conditions mean overall improvement in the\nperformance of the system. Another important aspect is the prevention of overfitting in this system, which includes\na set of performance metrics beyond just accuracy: precision, recall, F1-score, AUC, and results of backtests[4]\nwhich are also used in other researches [5]. These give further view on model performance in a more profound way,\nallowing to ensure high accuracy does not involve poor financial returns.\nIt is also designed to accommodate future development. The model continuously allows for the addition of new\ndatasets and models, which also improves overall performance over time. With more instruments and further models\nadded in, such a system can continue to adapt and improve toward trading. Thus, it is a solid, scalable platform\nsuitable for algorithmic trading.\nNamely, this is a novel, flexible, and highly customizable trading system that will incorporate multiple models\nand instruments to achieve better accuracy and ROI. It is capable of handling different market conditions, as it\nallows the user to select instruments against suitability for their purposes or even regulatory requirements. Finally,\nthe system architecture is open and developable on a continuous basis dynamic solution for ever-evolving market\nenvironments.\nThe paper is organized as follows: in the next sections, some related works will be reviewed, pointing out their\nadvantages and limitations; then a detailed description of the algorithm proposed will be given, with its multi-pair\nand multi-layer architecture. Finally, the algorithm's results are going to be presented, and conclusions based on the\nfindings and possible future directions are summarized."}, {"title": "2 RELATED WORKS", "content": "The section provides a review of the literature for several machine learning-based algorithmic trading\nmethodologies proposed and implemented. It discusses the most relevant approaches, highlighting their similarities\nand comparing methodologies and outcomes for the selected papers."}, {"title": "2.1 Algorithmic trading with directional changes", "content": "Recent works have considered multi-strategy approaches, which show fair potential to improve trading\nperformance in various financial markets. Paper [6] introduces a novel DC-based trading strategy based on a genetic\nalgorithm for finding the best combination among multiple DC-based strategy recommendations. Therefore, it\ncompares a multi-threshold DC strategy with five single-threshold DC strategies, three technical analysis indicators,\nand a buy-and-hold strategy. With this strategy, MTDC returns 1.15% per month, using 200 monthly datasets from\n20 Forex markets, more than a two-fold outperformance of the best single threshold return of 0.53%. Similarly, the\npresent paper also follows a multi-strategy approach. It makes use of a voting classifier to choose the most profitable\npairs of instruments and models. Though paper [6] has used 10-minute interval data, this paper applies daily interval\ndata. Though data has been analyzed at different intervals in this study, it reaches a return of 2.5% over a three-\nmonth period.\nMoreover, Paper [6] was executed on a non-dedicated Red Hat Enterprise Linux system with 24 cores and 24\nGB of memory, where the training process took 330 minutes to complete. In contrast, the execution in this paper\nutilized Apple Silicon ARM64 with M2 architecture, finishing training in 80 minutes. While the two studies differ\nin terms of the number of datasets and intervals used, the return on investment (ROI) perspective demonstrates a\nsignificant improvement in computational efficiency in this work. Despite the hardware and dataset differences,\nthis paper offers a more efficient execution time while achieving comparable or better returns, highlighting the\nadvancements in computational performance. While paper [6] suggests future research to dynamically choose the\nnumber of thresholds for each dataset, this paper has indeed implemented a flexible strategy selection mechanism\nusing the voting classifier, which allows dynamic selection of both instruments and models. Thus, it is far more\nadaptable to many changing market conditions. This paper, just like the paper [6], underlines the advantages of\nmulti-strategy optimization: in both cases, such models outperformed any single-strategy model both by\nprofitability and risk management."}, {"title": "2.2 Candlestick patterns and sequence similarity", "content": "This may provide a basis of comparison, as different algorithmic trading studies use a diversity of metrics when\nevaluating against others. The authors in paper [7] propose the multivariate financial time series stock forecasting\nmodel that integrates sequential pattern mining and sequence similarity. This model improves the accuracy of the\nstock trend forecast by using K-line pattern mining on multivariate time-series data. Experimental investigations\nwere conducted on financial time series data of the constituents of the CSI 300 and CSI 500. The proposed hybrid\nmodel achieved the average accuracy of 56.05% and 55.56% for two different datasets, while SVM and LSTM\nmodels achieved 51.83% and 51.32%, and 50.71% and 50.68%, respectively. Specifically, this project has some\nsimilarities with paper [7] in the type of data applied, evaluation metrics, and objectives. However, all critical\npreprocessing stages and machine learning models are very different.\nFrom the system structure point of view, paper [7] developed a model for predicting the direction of the next\ncandle, which is the same goal as this project. However, relying solely on accuracy may not work as effectively in\nalgorithmic trading. Candles differ in return: some candles, like doji candles, have negligible returns, whereas a\nsingle candle often results in a return of more than 10% price change. Thus, for two instruments, the results in paper\n[7] were more accurate. This paper uses a composition of machine learning layers with an ensemble model for\ninstrument-model pairs detection and improves accuracy on the prediction itself and most importantly by enhancing\nreturns, crucial for trading strategies."}, {"title": "2.3 Other related works", "content": "While such methods are indeed adequate in terms of achieving a high degree of accuracy, they do not meet the\nrequirements when it comes to ROI. All works in [7], [8], [9], [10] including [11] present algorithms of single\ninstrument prediction, where one model is tuned for one instrument. Contrasting that, the paper provides a better\nsolution that allows multiple instruments to be handled; thus, treating them as a portfolio. In addition, the suggested\nmodel is flexible, because it is easy to include new models for example [7], [8], [9], [10] and [11] strategies in the\nsystem."}, {"title": "3 Approach", "content": "This section introduces the approaches used for the inputs, outputs, and core functions of this project. Every\nalgorithmic trading system contains the same building blocks: data fetching, preprocessing, core model\ndevelopment, backtesting, and results presentation[12]. In this project, the two-layer multi-model machine learning\narchitecture is emphasized to ensure generality. The flexibility of the trading system allows for broad applicability\nacross financial instruments and enhances traditional investment methodologies. This also provides a base for other\nprojects, functioning like an internal search engine to find the most suitable instruments and ideal models."}, {"title": "3.1 Data representation and description", "content": "The time series data for this project is sourced from the yfinance library and OANDA broker APIs, encompassing\nprice data for supported symbols. Each dataset, as shown in Table 1, comprises six quantitative features: Open,\nHigh, Low, Close, Adj Close, and Volume [13]."}, {"title": "3.2 Feature Extraction and Pre-processing", "content": "In the preprocessing step, new features are generated for each instrument, including return, label, and indicators.\nTable 2 demonstrates all added features and their corresponding calculation formulas.\nAn indicator refers to a statistical calculation or measurement used to assess and analyze various aspects of\nfinancial markets, assets, or economic conditions. Indicators are often derived from financial data and are employed\nby traders, analysts, and investors to gain insights into market trends, potential price movements, and overall\neconomic health. These indicators can cover a wide range of metrics, including price levels, trading volumes,\nvolatility, and other relevant factors. In this project, the following indicators are utilized:\nMoving averages: Moving averages are commonly used to smooth out price data and identify trends over a\nspecific period. The Simple Moving Average (SMA) is calculated by summing up a set of prices over a specified\nperiod and dividing by the number of data points [14]."}, {"title": "3.3 Partitioning steps", "content": "Following the data collection and preprocessing steps, the dataset will be divided into model development and\nevaluation. Since this is a short-term prediction algorithm, the data will be split into a training set and test set using\na 95-5% split, with 95% going to training. The remaining 5% would be kept exclusively for the final evaluation of\nthe model, while the training set would further be divided into learning and validation sets in a 90-10 ratio."}, {"title": "3.4 Composing Ensembles of Instruments-Models", "content": "The system structure consists of a learning phase, validation evaluation, and the detection of successful models\namong all trained models for all instruments. Let's formalize this as a function composition, $Prediction =$\n$(V \\circ E \\circ M)(x)$ where $M$ is the first learning layer, $E$ is the evaluation of $M$, and $V$ is the second layer, which\nselects the best instrument-model pairs based on the evaluation metrics of the models. Here, $x$ represents the input:\na list of instruments along with their associated time series data. The final prediction will be the selected instrument\nand its pre-trained model. The input to the VotingClassifier will be a 3-dimensional matrix, this tensor will have\ndimensions $i * j * k$, where $i$ represents the number of instruments, $j$ represents the number of models, and $k$\ncorresponds to the evaluation metrics.\n\n$Prediction = VotingClassifier\\begin{bmatrix}E_{1} \\\\ E_{2} \\\\ ... \\\\ E_{i}\\end{bmatrix} * [M_{1} M_{2} ... M_{j}] * \\begin{bmatrix}X_{1} \\\\ X_{2} \\\\ ... \\\\ X_{i}\\end{bmatrix}$"}, {"title": "3.4.1 Training Classification Models with Instruments", "content": "In the learning phase, each of the classification models listed in Table 3 will be trained with a list of datasets\nbelonging to different instruments. The result will be a number of instrument-model pairs equal to the product of\nthe number of models and the number of instruments. The training models are hyperparameter-tuned using the\nGridSearch method to enhance their overall accuracy."}, {"title": "3.4.2 Evaluating Pairs", "content": "Following the learning phase, the models undergo validation with dedicated datasets, where their performance\nis assessed using various metrics such as accuracy, backtest, nnp, NormalizedAcc, precision, recall, f1, and auc.\nThese metrics play a crucial role in evaluating the effectiveness of the models and guiding the training of new\nmachine-learning model to identify a successful classification model and its instrument to be utilized.\nAll metrics are sourced from sklearn.metrics, except for backtest, nnp, and NormalizedAcc. The backtest metric\nrepresents the output of a function that displays the return of the validation dataset using the considered model. Nnp\nsignifies the normal return profit without any machine learning assistance. NormalizedAcc represents normalized\naccuracy calculated using the following formula:\n\n$NormalizeAcc = counts[1]/(counts[0]+counts[1])$\n\nCertainly, considering the potential issue with models setting all outputs to 0 or 1 to achieve higher accuracy in\nthe presence of imbalanced labels, the formula for NormalizedAcc can be adapted to address this challenge.\nNormalizedAcc provides a more balanced evaluation, considering the baseline accuracy that could be achieved by\nrandom guessing.\nAs a result of the initial phase of training, a data frame is generated that illustrates the effectiveness of models\nfor each symbol. In this study, the results are shown for 13 symbols and 11 models, producing a total of 141 rows.\n\n*   $M_{i}$ represent the i-th model where i = 1,2,...,11.\n*   $D_{j}$ represent the j-th dataset where j = 1,2,...,13.\n*   $E_{ij}$ represent the evaluation metrics for model $M_{j}$ on dataset $D_{i}$.\n*   k represents the number of evaluation metrics (e.g., accuracy, precision, recall, F1-score).\n\nWe can define the evaluation for each model-dataset pair as a matrix E, where each $E_{ij}$is a vetor of size k\n(evaluation metrics for the j-th model on the i-th dataset).\n\n$E = \\begin{bmatrix}E_{11} E_{12} ... E_{1j} \\\\ E_{21} E_{22} ...\\\\ : : ::\\\\ E_{i1} E_{iz} ... E_{ij}\\end{bmatrix}$\n\nWhere each $E_{ij} = [e_{ij}^{1}, e_{ij}^{2},..., e_{ij}^{k}]$, representing the set of evaluation metrics for model $M_{j}$ on dataset $D_{i}$."}, {"title": "3.4.3 Detecting Profitable Pairs with Voting Classifier", "content": "Since trading strategies must be able to generate sufficient returns to cover their associated costs with some\namount of certainty if they are to warrant an allocation[1], this layer performs the identification of financial\ninstruments with high profit potential and their respective models based on the output obtained from a\nVotingClassifier model trained on the historical performance data provided from the previous layer, which has\npersisted in a MongoDB database. In each further training phase, the second layer is retrained on more and more\ndata, each time with higher accuracy. Therefore, the algorithm improves with each increment of data provided in\nthe continuous learning and optimization process.\nWhat is the real contribution of the second layer? So far we have models, some of them are profitable, others\nnot. The accuracy of the second layer is crucial to determining the general performance. Suppose that the accuracy\nof our profitable models is 0.51 and the less profitable models are 0.49. Assuming that the accuracy of the second\nlayer is 0.80, then the general accuracy can be expressed as:\n\n$Mean Accuarcy = \\sum_{i=0}^{M} VP(V) = +1(80%) \u2013 1(20%) = 60%$\n\nTherefore, the second layer with its 0.80 accuracy generally improves the accuracy of the system to 0.60.\nIn the implementation part, which will be discussed further, the output, which consists of potential successful\nmodels, will be implemented on the OANDA API. These models will predict the direction of their linked\ninstruments for the following day."}, {"title": "3.5 Implementation Details", "content": "This section encompasses the implementation of the project with the OANDA API, consolidated into a single\ncell. The script is designed to be implemented in a cloud environment, running continuously 24/7. It incorporates a"}, {"title": "4 Evaluations and Results", "content": "To obtain the project's results, first, the Table 5 instruments are employed:"}, {"title": "5 CONCLUSION", "content": "Results obtained in the experiment testify to high efficiency of the approach being proposed. Currently, the\nproject has already been tested on real-world data provided by OANDA broker, and computations are performed\non the Apple Silicon ARM64 with M2 architecture. This underlines the possibility of obtaining promising results\neven with managing computational resources. Besides, this project has great potential for possible future\nimprovements. Since the system works like a search engine, constantly looking for an optimal opportunity, it relies\nheavily on having a flow of data at all times. Further development of the project could be done by increasing the\nnumber of financial instruments included, expanding the list of models like CNNs, RNNs and NLP models[17],\n[18], or implementing other preprocessing methods into the algorithm, such as advanced oscillators. Improvements\nmight also be made by incorporating more varied and enriched data sources, including financial news, oscillators,\nreliable model outputs, or correlations between these factors. Though the data and resources are at a premium, the\nmodels could still find promising opportunities to lay a foundation for further work."}]}