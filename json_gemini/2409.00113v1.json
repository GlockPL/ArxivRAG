{"title": "When All Options Are Wrong: Evaluating Large Language Model Robustness with Incorrect Multiple-Choice Options", "authors": ["Gracjan G\u00f3ral", "Emilia Wi\u015bnios"], "abstract": "This paper examines the zero-shot ability of Large Language Models (LLMs) to detect multiple-choice questions with no correct answer, a crucial aspect of educational assessment quality. We explore this ability not only as a measure of subject matter knowledge but also as an indicator of critical thinking within LLMs. Our experiments, utilizing a range of LLMs on diverse questions, highlight the significant performance gap between questions with a single correct answer and those without. Llama-3.1-405B stands out by successfully identifying the lack of a valid answer in many instances. These findings suggest that LLMs should prioritize critical thinking over blind instruction following and caution against their use in educational settings where questions with incorrect answers might lead to inaccurate evaluations. This research sets a benchmark for assessing critical thinking in LLMs and emphasizes the need for ongoing model alignment to ensure genuine user comprehension and assistance.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated their versatility in various domains, from code generation [21, 2, 11, 29, 36] and mathematical problem-solving [1, 16, 52, 5], to document summarization [20, 24, 45]. These models have also found applications in education, including automated grading [23, 25, 50], personalized tutoring [35, 33], and test generation [9, 54, 15, 4, 26].\nMultiple-choice questions (MCQs) are a cornerstone of educational assessment, enabling large-scale evaluation of student knowledge, streamlined grading procedures, and the potential for automated evaluation. MCQs provide a standardized approach to student assessment, mitigating the subjectivity inherent in essay-based or open-ended questions [38].\nWhile multiple-choice questions offer a standardized and efficient way to assess student knowledge, their effectiveness depends on having valid answers. If questions lack a correct option, it can lead to a cascade of negative consequences, including student frustration, confusion, inaccurate evaluation of their understanding, hindered learning, and the formation of misconceptions about the subject matter.\nIn this study, we investigate the 0-shot capacity of LLMs to discern when a multiple-choice question lacks a correct answer. This seemingly straightforward ability has profound implications. If LLMs can identify such problematic questions, it presents a mutually beneficial scenario: educators receive valuable feedback to refine their assessments, and students experience a fairer and more effective learning environment."}, {"title": "Related work", "content": ""}, {"title": "Language Models and Multiple-Choice Questions", "content": "Large Language Models have demonstrated considerable potential in handling multiple-choice questions [17, 56, 8], but their performance is not without limitations. Studies have shown that LLMs are sensitive to the order of answer choices, exhibiting a positional bias that can impact their predictions [37, 58]. While they excel at answering straightforward questions, LLMs struggle with those requiring deeper reasoning [30], for example about code snippets [41]. Ensuring the safety and robustness of LLMs in generating and responding to MCQs remains a crucial area of research, with benchmarks like SafetyBench [55] shedding light on performance gaps that need to be addressed to ensure the responsible use of LLMs in educational and other critical contexts."}, {"title": "Critical Thinking Abilities in Large Language Models", "content": "Recent research has focused on how well LLMs can think critically, which is crucial for tasks that require analyzing and evaluating information to reach logical conclusions. Prompt engineering techniques like the Evoke framework [18] have shown promise in enhancing LLMs' reasoning skills, especially in complex tasks like logical fallacy detection. Additionally, the ability of LLMs to self-improve using unlabeled data and chain-of-thought prompting demonstrates potential for autonomous learning and development of critical thinking [19]. However, challenges persist, particularly in the"}, {"title": "Refusal mechanisms in Large Language Models", "content": "Refusal mechanisms in LLMs play a crucial role in ensuring their safe and reliable operation. While safety prompts are commonly employed to prevent harmful or undesirable outputs, recent research suggests that they might inadvertently increase refusal rates even for harmless queries [57]. Advanced techniques like Directed Representation Optimization [57] aim to refine the effectiveness of safety prompts. Furthermore, understanding the underlying mechanisms of refusal, such as the identification of a one-dimensional subspace governing this behavior, offers the potential for precise control over refusal capabilities [3]. Beyond safety, the ability of LLMs to abstain from answering questions beyond their knowledge scope [51, 10] - sometimes referred to as Abstention Ability (AA) - is vital for improving their overall reliability [48]. Research indicates that strategic prompting and integration with information retrieval systems can significantly enhance this ability, contributing to the development of more trustworthy and accurate AI systems [31, 12, 27]."}, {"title": "Benchmark Design", "content": ""}, {"title": "Task Formulation", "content": "In this study, we aim to assess the ability of language models to recognize and respond appropriately to multiple-choice questions that lack a correct answer. To achieve this, we employed a specific task formulation that deliberately excludes the inclusion of typical escape options such as None of the above or No correct answer within the answer choices. This constraint forces the model to critically evaluate the provided options and make a judgment regarding their correctness.\nWe hypothesize that a model capable of robust reasoning should exhibit three potential behaviors in response to such questions:\nExplicitly stating that no correct answer exists. This indicates the model's ability to identify the lack of a valid solution among the provided choices.\nProviding the correct answer, even if it is not listed among the choices. This demonstrates the model's capacity to generate knowledge beyond the given information and challenge the constraints of the question itself.\nRefusing to answer the question due to the absence of a correct option. This signifies the model's understanding of its own limitations and its reluctance to provide an inaccurate or misleading response.\nCrucially, we posit that the ideal model's behavior should not be influenced by prompt engineering techniques that attempt to coerce a response. Even if explicitly instructed to select one of the provided options, the model should maintain its ability to critically assess the question and prioritize factual accuracy over blind obedience to instructions. For instance, if presented with a question like What is the result of 0 + 0? and given obviously incorrect options (e.g., 5), the model should ideally refuse to comply with the instruction to choose one of these options.\nThis task formulation presents a challenging test for language models, requiring not only knowledge retrieval and pattern recognition but also a degree of logical reasoning and critical thinking. It moves beyond the conventional multiple-choice question format to probe the model's ability to navigate ambiguous situations and prioritize truthful responses."}, {"title": "Dataset construction", "content": "To evaluate the models' critical thinking abilities, we employed two distinct datasets. The first, the Basic Addition Dataset (BAD), comprises simple addition problems across three difficulty levels. These levels correspond to the order of magnitude of the numbers involved, reflecting the increasing complexity of addition with larger numbers. Level 1 involves single-digit addition, Level 2 involves"}, {"title": "Experimental Setup", "content": "We devised a multi-faceted experimental framework to rigorously assess the models' capabilities under diverse conditions. This involved four distinct experimental conditions, each meticulously designed to probe specific facets of the model's reasoning and decision-making processes. For all experiments, we considered questions with only two options.\n\u2022 Baseline: Serving as our control condition, questions in this setup featured two answer choices, one of which was correct. No additional prompts or guidance were provided, enabling us to gauge the model's baseline accuracy in a standard multiple-choice scenario.\n\u2022 Easy: In this condition, we explicitly informed the model that all provided options could be incorrect, presenting two erroneous answer choices alongside the prompt The answer may not be in the options. This aimed to evaluate the model's ability to discern and reject incorrect answers even when explicitly cued to do so.\n\u2022 Standard: Representing our standard level of difficulty, this condition presented two incorrect answer choices without any supplementary prompts or instructions. This aimed to assess the model's capacity to identify incorrect answers relying solely on its inherent knowledge and reasoning capabilities.\n\u2022 Hard: Mirroring the Standard and Easy condition in terms of presenting two incorrect answer choices, we augmented this setup with the prompt You must choose exactly one option. This deliberate instruction, designed to increase the task's complexity, aimed to test the model's resilience to potentially misleading directives and its prioritization of factual accuracy over blind adherence to instructions.\nWe assess accuracy by evaluating how often the model correctly identifies when there is no correct answer or provides the correct answer even if it was not listed. To ensure more robust results and minimize positional bias [37, 58], we calculate the average accuracy for both the original and shuffled versions of each question. All model evaluations were conducted with temperature set to"}, {"title": "Human Evaluation", "content": "We conducted a study comparing human performance to that of language models to understand how people approach critical thinking in multiple-choice scenarios. We were particularly interested in whether humans show similar biases to LLMS when faced with multiple-choice questions containing only incorrect answers.\nWe recruited 50 participants with diverse educational backgrounds and demographics through social media. This included 21 women, 28 men, and 1 individual who preferred not to disclose their gender. Most participants (23) had undergraduate degrees, and their ages ranged from 17 to 37 (mean age 24.42).\nParticipants completed a 30-question quiz, which included only one question per category that intentionally lacked a correct answer. This design aimed to assess their ability to recognize and respond to such trick questions under realistic evaluation conditions. Unlike typical multiple-choice formats, we provided short answer response fields, allowing participants to express uncertainty or choose not to answer, thus capturing their critical thinking process more accurately.\nAll quiz questions were sampled from the BAD dataset to eliminate biases related to general knowledge disparities, ensuring a level playing field for participants with different educational backgrounds. Example question and participant answers are presented in Figure 1.\nParticipants' responses were analyzed to gauge both overall performance and specific responses to trick questions. The average score was 26.5 out of 27 (minimum = 24, maximum = 27), indicating high performance on standard questions. However, responses to trick questions were more varied. On average, participants correctly identified 2.02 out of 3 trick questions (minimum = 0, maximum = 3). Notably, 14 participants failed to identify any trick questions. This could suggest potential challenges in recognizing when no correct answer is present, or it might indicate a reluctance to deviate from the instructions, which asked them to choose only from the provided options.\nWhile 8 participants achieved perfect scores on both standard and trick questions, 15 participants missed only one trick question. This further supports the possibility that even high performers may prioritize adherence to instructions over critical evaluation of the answer choices.\nFurther analysis of trick question performance by gender showed no significant differences, with both men and women equally likely to achieve perfect scores or miss all trick questions."}, {"title": "Results and Analysis", "content": ""}, {"title": "BAD dataset", "content": "Our analysis, visualized in Figure 2, reveals distinct patterns in model performance across varying task complexities and answer availability. The majority of models cluster in the top-left quadrant, excelling at tasks with clear correct answers but struggling when critical thinking is required. This aligns with cognitive load theory [42, 43], suggesting increased cognitive burden hinders performance on unfamiliar tasks. The absence of models in the bottom-left quadrant, representing poor performance"}, {"title": "MMLU", "content": "Our experiments on the MMLU dataset clearly demonstrate that as the complexity of prompts increases, models struggle to identify questions without correct answers, opting instead to choose from the provided options, even when provided with hints that no correct answer exists, see Figure 3. This decline in performance is evident across various scientific domains, highlighting a broader limitation in current models beyond just mathematical or dataset-specific challenges."}, {"title": "Size of the model", "content": "Utilizing the BAD dataset, our experiments on the Llama 3.1 series revealed a clear correlation between model size and performance, in line with the scaling law observed in recent studies [47, 40]."}, {"title": "Impact of Alignment on a Model's Critical Thinking", "content": "We aimed to examine the impact of alignment techniques, such as reinforcement learning from human feedback (RLHF) [34] and Direct Preference Optimization (DPO) [39], on a language model's ability to make decisions, especially when presented exclusively with incorrect options. We compared two versions of the model, Qwen-Math-7B [53], and Qwen-Math-7B-Instruct [53], the latter having undergone DPO alignment and training to follow instructions strictly. Our results demonstrated a clear difference in performance between the two models. Qwen-Math-7B frequently declined to choose any provided options when all were incorrect, opting to either provide the correct solution or abstain from answering (average accuracy on BAD dataset equals 0.965). Conversely, Qwen-Math-7B-Instruct consistently followed the instructions, even when this required selecting an incorrect response (average accuracy on BAD dataset equals 0.08).\nBekbayev, et al. [7] has shown that alignment can negatively affect reasoning performance on benchmarks like MMLU, and [6] indicates that smaller models like ours can struggle when subjected to alignment, highlighting the trade-offs involved in different training approaches. This highlights potential trade-offs associated with different model training and alignment approaches."}, {"title": "Limitations and future work", "content": "While the datasets used in this study offer valuable insights into critical thinking in LLMs, they have limitations. The BAD dataset, despite being designed to minimize memorization, may not fully capture the nuances of numerical reasoning. The MMLU subset, though diverse, might not represent the full spectrum of questions LLMs encounter. Furthermore, inherent biases in the original MMLU dataset could propagate to our subset.\nFuture work could involve developing more comprehensive and nuanced datasets to further explore critical thinking in LLMs, incorporating a wider range of tasks and domains to evaluate LLMs across various aspects of reasoning. Additionally, exploring the incorporation of chain-of-thought prompting to provide LLMs with a mechanism to explain their reasoning process, potentially enhancing their ability to think critically."}, {"title": "Conclusions", "content": "Our research explored the challenges LLMs face with critical thinking, specifically in situations where multiple-choice questions have no correct answers. We found that most models tend to follow instructions without critical assessment, often choosing incorrect options. The Llama-3.1-405B model showed some capacity for critical thought, particularly on math-related questions, but even it struggled with the MMLU dataset. Interestingly, we observed a similar tendency to blindly follow rules in a human study with arithmetic problems.\nOur results suggest that critical thinking in LLMs may improve with larger models, and that there's a complex relationship between critical thinking and model alignment. An unaligned model performed significantly better on critical assessment tasks, achieving almost 100% accuracy, while an aligned model achieved only 8% accuracy on the same dataset.\nMoving forward, it's important to focus on aligning models in ways that maintain or even improve their capacity for critical thought without sacrificing helpfulness. We also need a deeper understanding of the mechanisms that contribute to critical thinking in LLMs, and how we can scale or optimize these without compromising alignment."}, {"title": "Evaluation Protocol", "content": ""}, {"title": "Models", "content": ""}, {"title": "Prompts", "content": ""}, {"title": "Mapping", "content": "We used a two-step approach for mapping:\n\"\"\"\n\u2022 For straightforward responses from BAD dataset we used regex-based mapping expressions:\n\"\"\"\nFUNCTION map_answers(answer: str, prompt: str) -> Union[int, str]:\n# Extract answer options from prompt using regex\nanswer_ = re.findall(r\"([AB])\\\\.\\\\s*(\\\\d+)\", prompt)\n# Split answer into parts\nanswer_list = answer.split(\" \")\n# If answer has two parts and the second is numeric, return it\nIF len(answer_list) == 2 AND answer_list[1].isnumeric():\nRETURN int(answer_list[1])\n# Iterate over answer options\nFOR pair in answer_:\n# If answer matches option letter or number, return option number\nIF pair [0] == answer.strip('.1234567890 .\\n') OR pair [1] == answer.strip(\" \"):\nRETURN int(pair[1])\n# If no match found, return \"Unknown\"\nRETURN \"Unknown\"\n\"\"\"\n\u2022 For more difficult cases, we used GPT-4-Turbo for mapping with the following prompt:\n\"\"\"\nUse the following rules to map the model's answer to the letters\n'A', 'B', or 'C':\nReturn 'A' or 'B' if the model gives one of the options.\nIf the model gives a letter 'A' or 'B' but then proposes a\ndifferent answer, return the letter 'C'.\nIf the model says that there is no correct answer, return the\nletter 'C'.\nRespond with the letter 'A', 'B', or 'C':\nOptions: {input_string}\nModel's Answer: {model_answer}\n\"\"\"\nExplanation:\n\u2022 Options: { input_string}: Represents the options provided in the original question.\n\u2022 Model's Answer: {model_answer}: Represents the answer generated by the model."}, {"title": "Dataset", "content": ""}]}