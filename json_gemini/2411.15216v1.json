{"title": "DIST LOSS: ENHANCING REGRESSION IN FEW-SHOT REGION THROUGH DISTRIBUTION DISTANCE CONSTRAINT", "authors": ["Guangkun Nie", "Gongzheng Tang", "Shenda Hong"], "abstract": "Imbalanced data distributions are prevalent in real-world scenarios, posing significant challenges in both imbalanced classification and imbalanced regression tasks. They often cause deep learning models to overfit in areas of high sample density (many-shot regions) while underperforming in areas of low sample density (few-shot regions). This characteristic restricts the utility of deep learning models in various sectors, notably healthcare, where areas with few-shot data hold greater clinical relevance. While recent studies have shown the benefits of incorporating distribution information in imbalanced classification tasks, such strategies are rarely explored in imbalanced regression. In this paper, we address this issue by introducing a novel loss function, termed Dist Loss, designed to minimize the distribution distance between the model's predictions and the target labels in a differentiable manner, effectively integrating distribution information into model training. Dist Loss enables deep learning models to regularize their output distribution during training, effectively enhancing their focus on few-shot regions. We have conducted extensive experiments across three datasets spanning computer vision and healthcare: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The results demonstrate that Dist Loss effectively mitigates the negative impact of imbalanced data distribution on model performance, achieving state-of-the-art results in sparse data regions. Furthermore, Dist Loss is easy to integrate, complementing existing methods. Our code will be made publicly available following the review process.", "sections": [{"title": "1 INTRODUCTION", "content": "Imbalanced data distributions are prevalent in the real world, with certain target values being significantly underrepresented Buda et al. (2018); Liu et al. (2019). In regression tasks, conventional deep learning models tend to predict towards regions of high sample density (many-shot regions) during training to minimize overall error. This results in models that perform well on the majority of samples but exhibit significantly higher prediction errors on regions of low sample density (few-shot regions). This phenomenon severely limits the applicability of deep learning models in certain contexts, such as healthcare scenarios where minority samples often carry significant importance, and significant errors in these samples could lead to potential adverse events.\nTaking the prediction of potassium concentration based on electrocardiogram (ECG) as an example, the model takes ECG signals as input and outputs the predicted potassium concentration derived from ECG signal features. Due to the imbalanced data distribution, traditional deep learning models tend to predict abnormal potassium concentration samples as normal to minimize overall error. However, since abnormal potassium concentrations significantly affect metabolism"}, {"title": "2 RELATED WORK", "content": "Research on the problem of imbalanced classification mainly focuses on improving the loss function to enhance the model's ability to identify the minority class. Weighted cross entropy King & Zeng (2001) gives higher weights to minority class samples, allowing the model to pay more attention to minority class samples when facing class imbalance. Focal loss Lin (2017) reduces the influence of the majority class by dynamically adjusting the weights in the loss function, further improving the performance of the minority class. Combining data augmentation and resampling techniques is also a common strategy. RUSBoost Seiffert et al. (2009) combines random undersampling and boosting to reduce the majority class while maintaining the performance of the model. SMOTE Chawla et al. (2002) further improves the classification results by expanding the minority class data through synthetic samples. The combination of adversarial training and loss functions has also gradually attracted attention, and adversarial reweighting Sagawa et al. (2019) improves the accuracy of minority classes."}, {"title": "2.1 IMBALANCED CLASSIFICATION", "content": "Research on the problem of imbalanced classification mainly focuses on improving the loss function to enhance the model's ability to identify the minority class. Weighted cross entropy King & Zeng (2001) gives higher weights to minority class samples, allowing the model to pay more attention to minority class samples when facing class imbalance. Focal loss Lin (2017) reduces the influence of the majority class by dynamically adjusting the weights in the loss function, further improving the performance of the minority class. Combining data augmentation and resampling techniques is also a common strategy. RUSBoost Seiffert et al. (2009) combines random undersampling and boosting to reduce the majority class while maintaining the performance of the model. SMOTE Chawla et al. (2002) further improves the classification results by expanding the minority class data through synthetic samples. The combination of adversarial training and loss functions has also gradually attracted attention, and adversarial reweighting Sagawa et al. (2019) improves the accuracy of minority classes."}, {"title": "2.2 IMBALANCED REGRESSION", "content": "Unlike imbalanced classification, regression tasks can have labels that are infinite and boundless, which prevents methods designed for imbalanced classification from being directly transferred to imbalanced regression. Consequently, existing methods focus on leveraging the continuity of the label space. At the input level, methods addressing imbalanced regression primarily focus on re-sampling the training dataset. SMOTE Chawla et al. (2002); Torgo et al. (2013) and its variant SMOGN Branco et al. (2017) generate new samples by leveraging the differences between minor-ity samples and their nearest neighbors. Branco et al. (2018) integrates a bagging-based ensemble method with SMOTE to mitigate the impact of imbalanced data distributions on the model. At the feature level, Yang et al. (2021) proposes feature distribution smoothing (FDS) by transferring fea-ture statistics between nearby target bins to smooth the feature space. VIR Wang & Wang (2024) borrows data with similar regression labels to compute the variational distribution of the latent repre-sentation. Ranksim Gong et al. (2022) uses contrastive learning to bring the feature space of samples with similar labels closer and push the feature space of samples with dissimilar labels apart. ConR Keramati et al. (2024) designs positive and negative sample pairs based on label similarity, trans-ferring the label space relationships to the feature space in a contrastive manner, too. At the model output and label level, regressor retraining (RRT) Yang et al. (2021) decouples the training of the encoder and regressor, retraining the regressor with inverse reweighting after normal encoder train-ing. DenseLoss Steininger et al. (2021) and label distribution smoothing (LDS) Yang et al. (2021)"}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 PROBLEM SETTING", "content": "Let $\\mathcal{D}$ be a training dataset comprising $N$ samples, denoted as $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, where $x^{(i)} \\in \\mathbb{R}^d$ represents the input and $y^{(i)} \\in \\mathbb{R}$ denotes the corresponding label. To facilitate processing, the continuous label space $\\mathcal{Y}$ is discretized into $B$ bins of equal width: $\\mathcal{Y} = \\bigcup_{b=1}^B [y_b, y_{b+1})$, where $y_b$ is the lower bound of bin $b$, and $y_1 < y_2 < \\dots < y_B$. In subsequent discussions, for convenience, the lower bound $y_b$ of the bin $[y_b, y_{b+1})$ will represent any label value $y^{(i)}$ that falls within that bin. Similarly, the model prediction space $\\hat{\\mathcal{Y}}$ is partitioned into bins of equal width. In practical scenarios, the width of each bin, denoted $\\Delta y$, indicates the minimum resolution of interest when processing the label space. For instance, in age estimation, one might set the bin width to 1, resulting in:"}, {"title": "3.2 DIST LOSS", "content": "One of the optimization objectives of Dist Loss is to minimize the distance between the prediction and label distributions in regression tasks. The core challenge lies in measuring the distance between these two distributions in a differentiable manner. Traditional metrics for measuring dis-tribution distance, such as Kullback-Leibler divergence and Jensen-Shannon divergence, cannot be implemented in a differentiable form for regression tasks. Therefore, we have devised an alterna-tive approach in the implementation of Dist Loss to realize a differentiable distribution distance measurement in regression scenarios. Specifically, we approximate the distance between the label and prediction distributions by sampling from these distributions and quantifying the differences between the sampled values to estimate the distance."}, {"title": "3.2.1 CALCULATION OF DIST Loss", "content": "As illustrated in Figure 2, we sample from the label and prediction distributions to generate pseudo-labels and pseudo-predictions, which encapsulate the distribution information of the labels and pre-dictions. Taking the generation of pseudo-labels as an example, we will now detail the process.\nTo generate pseudo-labels that contain label distribution information, we first randomly sample $M$ points from the label distribution. The expected frequencies of the label $y_i$ can be estimated by multiplying the number of sampling points $M$ by the probability of that label $p_i$. Based on this, we construct a sequence $\\mathcal{N}_L = (n_1, n_2,\\dots, n_B)$ to represent these expected frequencies, where $n_i = M p_i$. Each element in the obtained $\\mathcal{N}_L$ represents the expected frequencies of the corresponding label. Since these frequencies may be fractional, we need to convert them to integers while ensuring that the sum after conversion still equals $M$. Here, we denote the converted integer sequence by $\\mathcal{N}_L' = (n_1', n_2',\\dots, n_B')$. To acquire $\\mathcal{N}_L'$, we first take the floor of each element in $\\mathcal{N}_L$ to obtain the sequence $\\mathcal{N}_{Lf} = (\\lfloor n_1 \\rfloor, \\lfloor n_2 \\rfloor,\\dots, \\lfloor n_B \\rfloor)$. Then we calculate the difference $\\alpha$, which represents the difference between the sum of the original expected frequencies ($M$) and the sum after applying the floor function, following $\\alpha = M - \\sum_{i=1}^B \\lfloor n_i \\rfloor$. Using the difference $\\alpha$, we construct an auxiliary sequence $\\mathcal{A}$, which determines how to evenly distribute the difference to the elements of $\\mathcal{N}_{Lf}$ to ensure the sum is $M$:\n$A_i =\\begin{cases}1, & \\text{if } i \\leq \\lfloor \\frac{\\alpha}{B} \\rfloor \\text{ or } i > B - \\lfloor \\frac{\\alpha}{2} \\rfloor\\\\0, & \\text{otherwise}\\end{cases}$   (1)\nEach $n_i'$ is determined by adding $a_i$ to the corresponding element in $\\mathcal{N}_{Lf}$, where $n_i' = \\lfloor n_i \\rfloor + a_i$, and $i \\in B$. Finally, we generate the corresponding pseudo-labels $\\mathcal{S}_L$ based on the expected frequencies, where each element $\\mathcal{S}_{L_i}$ is represented as:\n$S_{L_i} = \\min \\{y_i | \\theta \\Big(n_{i-1}' + \\sum_{k=1}^i n_k' \\Big) \\geq 0\\}$ (2)\nHere, $\\theta(x)$ is the unit step function, which returns 1 when $x > 0$ and 0 otherwise. To illustrate with a specific example, assume that the label sequence is $(y_1, y_2, y_3) = (4,5,6)$ and that the obtained sequence $\\mathcal{N}_L'$ is (1, 2, 3). Then, the generated pseudo-labels $\\mathcal{S}_L$ would be (4,5,5, 6, 6, 6).\nSimilarly, we can perform $M$-point sampling on the prediction distribution and subsequently apply the same operations to obtain the pseudo-predictions $\\mathcal{S}_P$, which encapsulate information about the prediction distribution.\nIn practice, we can consider a batch during the model training process as a random sampling event, wherein the model predictions within a batch are viewed as the sampling values of the prediction distribution. Consequently, we do not need to repeat the aforementioned process to acquire pseudo-predictions; instead, we simply sort the model predictions in the batch, which already contains the prediction information. By measuring the distance between the pseudo-predictions and the pseudo-labels, we can approximate the distance between the respective distributions. Let us denote the"}, {"title": "3.2.2 FAST DIFFERENTIABLE SORTING", "content": "As previously mentioned, the obtained pseudo-predictions are in ascending order, whereas the order of the model's actual predictions is random in practical scenarios. Therefore, it is necessary to sort the model's predictions to obtain the pseudo-predictions. Since the sorting operation is non-differentiable, we employ a fast differentiable sorting algorithm Blondel et al. (2020) to ensure the differentiability of the entire computation process.\nThis method achieves the sorting operation by defining it as projections on permutation polytopes. Specifically, for any given vector $w \\in \\mathbb{R}^n$, we construct the permutation polytope $P(w)$, which represents the convex hull of all possible permutations of $w$, i.e.,\n$P(w) := \\text{conv}\\{\\omega_{\\sigma} : \\sigma \\in \\Sigma\\},$ (3)\nwhere $\\Sigma$ denotes all permutations of $[n]$. The sorting operation $s(\\theta)$ is defined as the solution to the linear programming problem that maximizes the dot product with $p$ (a strictly decreasing vector) on $P(\\theta)$, i.e.,\n$s(\\theta) = \\text{arg}\\max_{Y \\in P(\\theta)} <y, p>.$  (4)\nTo ensure the differentiability of the sorting operation, a regularization term $\\Psi$ is introduced, trans-forming the sorting operation into tractable projection problems:\n$P_{\\nu}(z, w) = \\text{arg} \\min_z \\frac{1}{2} ||z - w||^2 + \\nu \\Psi(p)\\},$  (5)\nwhere $\\Psi$ is a strongly convex function, ensuring the differentiability of the problem. This approach enables forward propagation with $O(n \\log n)$ time complexity and backward propagation with $O(n)$ time complexity."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 BENCHMARKS AND BASELINES", "content": "We evaluated our method on three datasets, focusing on tasks of age estimation and potassium concentration prediction. The IMDI-WIKI-DIR dataset Yang et al. (2021), derived from the IMDB-WIKI dataset Rothe et al. (2018), consists of 213,553 facial image pairs annotated with age infor-mation. This dataset is partitioned into 191,509 samples for training, 11,022 for validation, and 11,022 for testing. The AgeDB-DIR dataset Yang et al. (2021), derived from the AgeDB dataset Moschoglou et al. (2017), comprises 16,488 facial image pairs with age annotations. It is divided into 12,208 samples for training, 2,140 for validation, and 2,140 for testing. The ECG-Ka-DIR dataset, sourced from the MIMIC-IV dataset Johnson et al. (2020), includes 375,745 pairs of single-lead ECG signals paired with potassium concentration values. This dataset is divided into 365,549 samples for training, 5,098 for validation, and 5,098 for testing. All these datasets are character-ized by imbalanced training sets and balanced validation and test sets. The label distributions of these three datasets are shown in Figure 3. Please refer to Appendix A.1 and A.2 for baseline and implementation details."}, {"title": "4.2 EVALUATION METRICS", "content": "Following the evaluation metrics of Yang et al. (2021), we report the results for four shots: all, many, median, and few, where all represents the entire dataset, and many/median/few correspond to areas of high/medium/low sample density within the dataset. For the IMDB-WIKI-IR and AgeDB-DIR datasets, we maintain consistency with previous studies, where few/median/many correspond"}, {"title": "4.3 MAIN RESULTS", "content": "Table 1 presents the results of baselines and our method in the few-shot region across three datasets, along with a comparison of these results. For detailed results on each dataset, please refer to Ap-pendix A.3. This table is divided into two sections. The first section displays the results of the baselines and our method, with the best results highlighted in bold and red. The second section shows the improvement of our method over each baseline, with green bold indicating superior per-formance of our method and blue bold indicating otherwise. From the first section of the table, it is evident that our method achieves the best results in five out of six metrics across the three datasets, with SOTA performances of 22.550, 9.122, and 1.329 on the IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR datasets, respectively. The second section reveals that our method outperforms in 28 out of 30 metrics. Notably, compared to Balanced MSE, which also involves fine-tuning the linear layers of a pre-trained model and employs data distribution priors, our method demonstrates superior performance in the few-shot region, highlighting the effectiveness of our approach.\nTable 2 further illustrates the complementary nature of our method with existing approaches. This table is divided into five sections, each showcasing the results of one baseline and the combined results with our method, with the best results within each section highlighted in bold and black. From this table, it is shown that our method achieves better results in 26 out of 30 metrics. Taking the MAE metric as an example, incorporating our method leads to improved performance in the few-shot region across all three datasets, achieving the best results of 22.331, 9.110, and 1.325 on the IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR datasets, respectively. These experimental results demonstrate a key advantage of our method, namely its ability to effectively complement existing methods, thereby enhancing model performance in the few-shot region."}, {"title": "4.4 TIME CONSUMPTION ANALYSIS", "content": "Table 3 presents the time required to train each method for one epoch on the IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR datasets, with all times reported in seconds. It can be observed that Balanced MSE and Dist Loss have the shortest training times, attributed to their approach of fine-tuning the model's linear layers. The time consumption of LDS and the vanilla model are largely consistent, as these methods only weight the loss function without significantly increasing computational load. For methods operating at the feature level, including FDS, Ranksim, and ConR, a notable increase in model training time is evident, due to the computational intensity associated with feature-level operations."}, {"title": "4.5 ABLATIONS AND ANALYSIS", "content": ""}, {"title": "4.5.1 DIFFERENT LOSS FUNCTIONS FOR SEQUENCE DIFFERENCE MEASUREMENT", "content": "Dist Loss employs the loss function L(\u00b7) to measure the difference between two sequences. In this ablation study, we demonstrate the effects of using different functions, considering the probability-based inversely weighted MAE and MSE losses. The experimental results are shown in Table 4, where detailed results on each dataset are shown in Appendix A.4.2. The table illustrates that Dist Loss reliably boosts model accuracy in the few-shot region."}, {"title": "4.5.2 DIFFERENT BATCH SIZES FOR DISTRIBUTION DISTANCE APPROXIMATION", "content": "Dist Loss estimates the overall distribution distance between predictions and labels by measuring batch-wise distances during training. This ablation study evaluates the sensitivity of model accuracy to batch size, as detailed in Table 5. We examined batch sizes of 256, 512, and 768, adopting 256 as a standard based on prior research Yang et al. (2021); Gong et al. (2022). The findings show negligible variations in performance with different batch sizes. This could be attributed to the fact that accurate distribution information is more critical during sampling than the precise accuracy of individual pseudo-labels."}, {"title": "4.5.3 DIST LOSS SURPASSES EXISTING METHODS IN THE MEDIAN-SHOT REGION", "content": "As depicted in the supplementary Tables 6, 7, and 8 within Appendix A.3, Dist Loss delivers SOTA results, excelling not only in few-shot regions but also in median-shot regions. In our comparison with current methods, Dist Loss achieved the lowest MAE and the second-lowest GM on the IMDB-WIKI-DIR and AgeDB-DIR datasets, with scores of 12.614/7.686 and 7.315/4.563, respectively. Similarly, on the ECG-Ka-DIR dataset, it secured the highest GM and the second-lowest MAE, recording 0.445 and 0.674, respectively. Moreover, our experiments show that integrating Dist Loss with existing methods consistently improved performance in median-shot regions when measured by both MAE and GM, surpassing the results of using those methods alone on IMDB-WIKI-DIR and AgeDB-DIR datasets. On the ECG-Ka-DIR dataset, this integration notably increased the GM. In conclusion, these findings validate Dist Loss's efficacy in enhancing model accuracy in both few-shot and median-shot regions."}, {"title": "5 CONCLUSION", "content": "In this study, we address the significant escalation of prediction errors in few-shot regions, a preva-lent challenge in deep imbalanced regression. By leveraging distribution priors, we introduce a novel loss function, Dist Loss, designed to align the model's prediction distribution with the label distribution throughout the training process. Our extensive experimental evaluation demonstrates that Dist Loss effectively enhances prediction accuracy in few-shot regions, achieving state-of-the-art performance. Furthermore, our results indicate that Dist Loss can be seamlessly integrated with existing methods to further augment their efficacy. We hope our work underscores the critical role of integrating distribution information in tackling deep imbalanced regression tasks."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 BASELINES", "content": "To ensure a fair comparison, we followed the experimental setup of Yang et al. (2021) on the IMDB-WIKI-DIR and AgeDB-DIR datasets, i.e., using ResNet-50 as the network architecture and train-ing for 90 epochs. For the ECG-Ka-DIR dataset, we employed the ResNet variant Net1D Hong et al. (2020) as the network architecture. Given that previous work Yang et al. (2021); Ren et al. (2022); Gong et al. (2022); Keramati et al. (2024) has demonstrated superior performance over loss reweighting and regressor re-training (RRT) in deep imbalanced regression tasks, we do not include these methods as baselines in this paper. Instead, we focused on widely recognized approaches in the field: LDS, FDS Yang et al. (2021), Ranksim Gong et al. (2022), ConR Keramati et al. (2024), and Balanced MSE Ren et al. (2022). LDS and FDS encourage local similarities in label and fea-ture space, while Ranksim and ConR leverage contrastive learning to translate label similarities into the feature space. Balanced MSE, based on label distribution priors, restores a balanced distribu-tion from an imbalanced dataset. Our experimental findings indicate that not only does our method achieve SOTA performance in few-shot regions, but it also enhances existing methods, offering a complementary strategy to boost their efficacy."}, {"title": "A.2 IMPLEMENTATION DETAILS", "content": "We trained all models on the IMDB-WIKI-DIR and AgeDB-DIR datasets using a single NVIDIA GeForce RTX 3090 GPU and on the ECG-Ka-DIR dataset using a single NVIDIA GeForce RTX 4090 GPU. To ensure a fair comparison, we followed the training, validation, and test set divisions from Yang et al. (2021) for the IMDB-WIKI-DIR and AgeDB datasets. During training with Dist Loss, we used the same strategy as Balanced MSE, fine-tuning the linear layer based on pre-trained model (vanilla model) parameters. This approach integrates our method with existing methods, using their model parameters as the starting point for fine-tuning. Additionally, we used probability-based inversely weighted MSE to measure sequence difference in Dist Loss for all datasets, setting the distribution loss component weight to 1."}, {"title": "A.2.1 IMDB-WIKI-DIR", "content": "On the IMDB-WIKI-DIR dataset, we selected ResNet-50 as the network architecture. During train-ing, the training epochs were set to 90, with an initial learning rate of 0.001, which was reduced to 1/10 of its value at the 60th and 80th epochs. We employed the Adam optimizer with a momentum of 0.9 and a weight decay of 0.0001. For our method and Balanced MSE, we used a batch size of 512. For the other baselines, we followed the experimental setups from their original papers. It should be noted that the original training epochs for Ranksim and ConR in their respective papers were 120, which we adjusted to 90 in our experiments to ensure a fair comparison."}, {"title": "A.2.2 AGEDB-DIR", "content": "On the AgeDB dataset, we employed ResNet-50 architecture for our model. The training consisted of 90 epochs with an initial learning rate of 0.001, which was reduced to 1/10 of its original value at the 60th and 80th epochs. We utilized the Adam optimizer with a momentum of 0.9 and a weight decay of 0.0001. For our method and Balanced MSE, we used a batch size of 512. For the other baselines, we followed the experimental configurations outlined in their respective original papers. To ensure a fair comparison, we also set the training epochs for Ranksim and ConR to 90."}, {"title": "A.2.3 ECG-KA-DIR", "content": "On the ECG-Ka-DIR dataset, we utilized the ResNet variant, Net1D Hong et al. (2020), as our network architecture. The training was set for 10 epochs with an initial learning rate of 0.001, which was reduced to 1/10 of its initial value at the 5th and 8th epochs. We employed the Adam optimizer with a momentum of 0.9 and a weight decay of 0.00001. A batch size of 512 was used for all methods. Additionally, for ConR, we constructed positive and negative sample pairs by adding Gaussian noise."}, {"title": "A.3 COMPREHENSIVE EXPERIMENTAL RESULTS", "content": "Tables 6, 7, and 8 present a comprehensive overview of our experimental results on the IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR datasets. The results indicate that our method achieves improvements in model performance on median-shot and few-shot regions without compromising overall error rates. This further demonstrates the effectiveness of our method in sparse data regions."}, {"title": "A.4 ABLATIONS AND ANALYSIS", "content": ""}, {"title": "A.4.1 DIFFERENT BATCH SIZES FOR DISTRIBUTION DISTANCE APPROXIMATION", "content": "Table 9 illustrates the impact of varying batch sizes on the final performance across three datasets: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The results indicate that there is no significant difference in performance among different batch sizes. This observation suggests that the generation of pseudo-labels primarily requires an approximation of the distribution information, rather than the precise accuracy of every individual label value."}, {"title": "A.4.2 DIFFERENT LOSS FUNCTIONS FOR SEQUENCE DIFFERENCE MEASUREMENT.", "content": "Tables 10, 11, and 12 present the comprehensive results of using different loss functions on IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR, respectively. It is evident that existing methods, when augmented with Dist Loss, demonstrate superior performance on samples within few-shot regions."}, {"title": "A.4.3 PERFORMANCE OF DIST LOSS ACROSS DIFFERENT IMBALANCED RATIOS", "content": "We validated the effectiveness of Dist Loss by varying the imbalance ratios of the ECG-Ka-DIR dataset. The data distribution diagrams are shown in Figure 4, and the corresponding results in the few-shot regions are presented in Table 13. Across eight datasets with different imbalance ratios, our method achieved the best performance in six cases and the second-best performance in the remaining two. These results collectively demonstrate the robustness of our approach across varying levels of data imbalance."}, {"title": "A.5 PERFORMANCE OF DIST LOSS ON THE GM METRIC", "content": "We observed that on the IMDB-WIKI-DIR dataset, the performance of Dist Loss in the few-shot region, as measured by the GM metric, is inferior to that of Balanced MSE. To provide a more intuitive analysis of this phenomenon, we plotted the sorted error distribution curves for both Dist Loss and Balanced MSE in the few-shot region, as shown in Figure 5. Specifically, for each method, the error values were first sorted in ascending order. The x-axis represents the rank of these sorted errors, while the y-axis denotes the corresponding error magnitudes. This visualization facilitates a direct comparison of the error distributions between the two methods."}]}