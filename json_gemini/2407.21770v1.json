{"title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts", "authors": ["Xi Victoria Lin", "Akshat Shrivastava", "Liang Luo", "Srinivasan lyer", "Mike Lewis", "Gargi Ghosh", "Luke Zettlemoyer", "Armen Aghajanyan"], "abstract": "We introduce MoMa, a novel modality-aware mixture-of-experts (MoE) architecture designed for pre-training mixed-modal, early-fusion language models. MoMa processes images and text in arbitrary sequences by dividing expert modules into modality-specific groups. These groups exclusively process designated tokens while employing learned routing within each group to maintain semantically informed adaptivity. Our empirical results reveal substantial pre-training efficiency gains through this modality-specific parameter allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model, featuring 4 text experts and 4 image experts, achieves impressive FLOPs savings: 3.7\u00d7 overall, with 2.6\u00d7 for text and 5.2\u00d7 for image processing compared to a compute-equivalent dense baseline, measured by pre-training loss. This outperforms the standard expert-choice MoE with 8 mixed-modal experts, which achieves 3\u00d7 overall FLOPs savings (3\u00d7 for text, 2.8\u00d7 for image). Combining MoMa with mixture-of-depths (MoD) further improves pretraining FLOPs savings to 4.2\u00d7 overall (text: 3.4x, image: 5.3\u00d7), although this combination hurts performance in causal inference due to increased sensitivity to router accuracy. These results demonstrate MoMa's potential to significantly advance the efficiency of mixed-modal, early-fusion language model pre-training, paving the way for more resource-efficient and capable multimodal AI systems.", "sections": [{"title": "1 Introduction", "content": "Auto-regressive mixed-modal foundation models have shown significant promise in applications requiring the processing of mixed-modal inputs and the generation of mixed-modal outputs (Gemini et al., 2023, 2024; OpenAI et al., 2024; Lu et al., 2023; Chameleon Team, 2024). These models have demonstrated remarkable capabilities in tasks ranging from visual question answering to multimodal content generation, pushing the boundaries of AI's ability to understand and interact with our inherently multimodal world.\nWhile a popular architecture design for mixed-modal foundation models involves fusing modality-specific encoders or decoders (Gemini et al., 2023, 2024; Lu et al., 2023; OpenAI et al., 2024), this approach can limit the model's ability to integrate information across modalities and generate content with interleaved modalities. To address this limitation, Chameleon (Chameleon Team, 2024) recently introduced a single transformer architecture with a next-token prediction objective to model mixed-modal sequences consisting of discrete image and text tokens, allowing for seamless reasoning and generation across modalities. Chameleon, pretrained on approximately 10 trillion mixed-modal tokens, has demonstrated broad vision and language capabilities across various downstream tasks. Notably, it surpasses commercial baselines such as Gemini 1.0 Pro\u00b9 and GPT-4V2 in generating mixed-modal long-form responses (Chameleon Team, 2024). However, scaling such mixed-modal early-fusion foundation models to greater capacities presents significant computational challenges.\nTo address these challenges, we investigate the application of routed sparse architectures (Lepikhin et al., 2020; Fedus et al., 2022; Clark et al., 2022; Jiang et al., 2024; Raposo et al., 2024). These architectures have"}, {"title": "2 Model", "content": "previously shown effectiveness in scaling language and vision-specific foundation models, as well as multimodal contrastive learning (Mustafa et al., 2022). However, their application to mixed-modal early-fusion models presents unique opportunities and challenges.\nThe insight driving our approach is the inherent heterogeneity of modalities: text and image tokens possess distinct information densities and redundancy patterns (Liang et al., 2023). While we integrate these tokens into a unified early-fusion architecture, we propose further optimizing this framework by incorporating modality-specific modules. This concept, which we term modality-aware sparsity (MaS), enables models to better capture features specific to each modality while maintaining strong cross-modality integration through partial parameter sharing and attention mechanisms. Previous work such as VLMo (Bao et al., 2022), BEIT-3 (Wang et al., 2022a) and VL-MoE (Shen et al., 2023) have applied mixture-of-modality-experts (MoME) to train vision-language encoders and masked language models. We apply this type of sparse component to the mixed-modal, early-fusion language modeling setting.\nSpecifically, we adopt Chameleon as the base transformer architecture and introduce sparsity along two dimensions:\n1. Width: We apply mixture-of-experts (MoE) (Lepikhin et al., 2020) where tokens are routed across a set of feed-forward blocks (experts) at each layer. Crucially, we divide these experts into modality-specific groups, with each group processing only tokens of its designated modality while learned routing is done within each group for semantics-based adaptivity. We dub this approach mixture of modality-aware experts (MoMa).\n2. Depth: We apply mixture-of-depths (MoD) (Raposo et al., 2024) where tokens can selectively skip the attention and feed-forward computation at certain layers.\nFor both dimensions, we adopt expert-choice routing (Zhou et al., 2022) to ensure load balancing and maintain a static computation graph (Paszke et al., 2019), both important for high training throughput.\nWe conduct extensive FLOPs-controlled experiments comparing our proposed architecture with the dense baseline and multiple sparse variations. With a 1-trillion-token training budget, Chameleon-MoMa 1.4B employing 4 text experts and 4 image experts achieves a substantial 3.7\u00d7 overall FLOPs savings (text: 2.6\u00d7, image: 5.2\u00d7) compared to the 1.4B compute-matched dense baseline as measured by pre-training loss, while maintaining a relatively modest -17% throughput reduction. In contrast, the standard expert-choice MoE with 8 mixed-modal experts achieves 3\u00d7 FLOPs savings (text: 3x, image: 2.8\u00d7) under the same setting, with -9% throughput reduction. Further combination with MoD (Chameleon-MoMaD) boosts the FLOPs savings to 4.2x overall (text: 3.4\u00d7, image: 5.3\u00d7) as measured by pre-training loss. However, the auto-regressive inference performance of the MoD model is compromised due to its sensitivity to routing accuracy, resulting in subpar performance compared to Chameleon-MoMa (\u00a74.6).\nWe also demonstrate that it is possible to enhance the performance of Chameleon-MoMa using a modality-untied upcycling technique (Komatsuzaki et al., 2023). In particular, we can initialize the model with a seed sparse architecture consisting of 1 expert per modality, and effectively improve router learning for each modality. By training the seed model for only 10k steps, the resulting model achieves additional FLOP reduction while maintaining or improving performance.\nOur main contributions can be summarized as follows:\n1. We introduce mixture of modality-aware experts (MoMa) for mixed-modal early-fusion models, allowing for more efficient parameter allocation and utilization.\n2. We investigated sparse scaling along both the width and depth dimensions of transformers, demonstrating substantial speedup of pre-training loss convergence can be realized in each direction. However, effectively combining these two approaches for better performance in an autor-regressive inference setup remains an open research challenge.\n3. We present a modality-untied upcycling technique that further enhances the efficiency of our sparse architecture.\n4. We provide extensive empirical results and analysis, offering insights into our approach's scaling behavior and efficiency gains."}, {"title": "2.1 Early Fusion", "content": "Our model builds upon the early fusion architecture introduced by Chameleon (Chameleon Team, 2024), which represents images and text as a series of discrete tokens within a unified transformer. The core of Chameleon is a transformer-based model that applies self-attention mechanisms over the combined sequence of image and text tokens. This allows the model to capture complex relationships between and within modalities. The model is trained using a next-token prediction objective, learning to generate both text and image tokens autoregressively.\nIn Chameleon, images are tokenized using a learned image tokenizer that encodes a 512 \u00d7 512 image into 1024 discrete tokens from a codebook of size 8192. Text is tokenized using a BPE tokenizer with a vocabulary size of 65,536, which includes the 8192 image codebook tokens. This unified tokenization scheme enables the model to process arbitrary sequences of interleaved image and text tokens.\nBy adopting this early fusion approach, our model inherits several key advantages:\n1. Unified representation: The model learns a shared representation space for images and text, facilitating cross-modal reasoning and generation.\n2. Flexibility: The architecture can handle arbitrary sequences of images and text, enabling diverse multimodal tasks such as image captioning, visual dialogue, and mixed-modal document generation.\n3. Scalability: The token-based approach allows for uniform processing of both text and image data, enabling efficient scaling to larger model sizes and diverse datasets. This is evidenced by Chameleon's successful training on approximately 10 trillion mixed-modal tokens.\n4. End-to-end learning: The entire auto-regressive model, is trained end-to-end, allowing for joint optimization of the representation and task-specific performance.\nBuilding on this foundation (Figure 1a), our work introduces modality-aware sparsity techniques to further enhance the efficiency and performance of early fusion models, as detailed in the following sections. These"}, {"title": "2.2 Width Scaling: Mixture of Modality-Aware Experts", "content": "We propose a width scaling approach that incorporates modality-aware block sparsity in the feed-forward module, extending the standard mixture-of-experts (MoE) architecture (Lepikhin et al., 2020; Fedus et al., 2022; Wang et al., 2022b). The insight driving this approach is that tokens of different modalities have distinct characteristics and information densities. By creating separate expert groups for each modality, we allow the model to develop specialized processing pathways while maintaining the ability to integrate information across modalities.\nWe describe the key components in the mixture of modality-aware experts (MoMa) formulation below (Figure 1b).\nModality-Specific Expert Groups. We divide the experts in each MoE layer into distinct groups, each specialized in processing tokens from a specific modality E = {E{ ..., Em, E{ ...,E}: one group for processing text tokens and another for image tokens\u00b3. This separation allows each group to specialize in features relevant to its respective modality.\nBy implementing modality-aware block sparsity, we aim to achieve several benefits:\n\u2022 Improved Efficiency: by routing tokens to modality-specific experts, we reduce the computational overhead of processing tokens with experts not specialized for their modality.\n\u2022 Enhanced Specialization: modality-specific expert groups can develop more refined features relevant to their respective modalities.\n\u2022 Maintained Cross-Modal Integration: despite the separation into modality-specific groups, the model still can integrate information across modalities through the shared self-attention mechanisms in non-MoE layers.\nHierarchical Routing. We adopt a token-based routing mechanism (Lepikhin et al., 2020; Fedus et al., 2022; Jiang et al., 2024). For an input token x, our routing mechanism operates in two stages.\n1. Modality-aware routing: tokens are first routed to their corresponding modality-specific expert group based on their modality (text T or image I).\n2. Intra-modality routing: within each modality-specific expert group EM, tokens are then routed to specific experts using a learned routing function. Specifically, we use a projection matrix WM \u2208 [Rdx|EM|, d = transformer hidden dimension, to compute the token-to-expert affinity scores, following other established MoE formulations (Lepikhin et al., 2020; Fedus et al., 2022; Jiang et al., 2024).\nExpert choice. Within each modality group, we implemented expert-choice (EC) routing (Zhou et al., 2022), where each expert has a fixed bucket size and processes the top-ke tokens in a batch. We set ke = bM . Ce, where ce = E is a capacity factor and M is the total number of tokens of modality M in a batch. As a result, each token can be routed to a variable number of experts.\nEC routing ensures balanced expert utilization during training and eliminates needing a separate load-balancing loss term. Maintaining a single loss term promotes more stable optimization and a more straightforward training process. However, EC routing compromises causality in auto-regressive language modeling, as each expert selects the top tokens to process in a batch by comparing token scores across all tokens. We use a combination of two techniques to address this issue and enable expert-choice training for auto-regressive LMs.\n1. We use Sigmoid as the non-linearity in the router scoring function, enabling independent calculation of token-to-expert affinity scores for each token."}, {"title": "2.3 Mixture-of-Depths", "content": "In summary, the MoMa module for an input token x can be formally defined as:\ny =  \u03a3m i=19 T M(x)i \u00b7 FFNw iGLUj(x) if x \u2208 T\u03a3I i=19 M(x)i \u00b7 FFNswiGLUj(x) if x \u2208 I\n(1)\n9M(x)i =  Sigmoid(WM); if x is selected by Ej0 otherwise\nWe further apply residual connection and the Swin Transformer normalization (Liu et al., 2022a) post MoMa computation. Our experiments, detailed in later sections, demonstrate that MoMa significantly improves efficiency and performance compared to dense baselines and standard MoE architectures.\nWe further investigate introducing sparsity in the depth dimension. Prior work explores sparisty in depth through either stochastic layer drop (Elhoushi et al., 2024) or through learnable routers (Raposo et al., 2024). We focus on learnable routers, and incorporate the recently proposed mixture-of-depths (MoD) technique (Raposo et al., 2024). Specifically, in each MoD layer, we integrate MoD prior to any mixture-of-experts (MoE) routing, ensuring it is applied to the full batch before modality split (Figure 2).\nFollowing Raposo et al. (2024), for each MoD layer, we use a projection matrix \u0174g \u2208 Rd\u00d71 to compute the token-to-layer affinity score, followed by a Sigmoid non-linearity\n\u011d(x) = Sigmoid(x. Wg).\n(2)"}, {"title": "2.4 Inference", "content": "We cannot directly apply the expert-choice routing for MoE and layer-choice routing for MoD during inference time, as the top-k token selection within a batch breaks causality. Inspired by Raposo et al. (2024), to ensure causality during inference, we introduce auxiliary routers, to predict the likelihood of a token being selected by an expert or a layer solely based on its hidden representation.\nFormally, for each MoE layer, we introduce an auxiliary router\ngaux(x) = Sigmoid(SiLU(x \u00b7 WM) \u00b7 WM),\n(3)\nwhere WM\u2208 Rdx(d/2) and WM \u2208 R(d/2)\u00d7|EM|.\nFor each MoD layer, we introduce an auxiliary router\ngaux(x) = Sigmoid(SiLU(x \u00b7 \u0174a1) \u00b7 Wa2),\n(4)\nwhere Wa1 \u2208 Rdx(d/2) and \u0174a2 \u2208 R(d/2)\u00d71.\nWe employ a two-stage training approach, where the main model and auxiliary routers are trained separately. First, we train the main model to convergence. Then, we train the auxiliary routers using binary cross-entropy loss, supervised by the ground-truth top-k routing assignments computed over an entire batch. At inference time, the main routers are only used to generate the weight values, and tokens are selectively routed to an expert or layer based on thresholding on the auxiliary router score (> 0.5)."}, {"title": "2.5 Upcycling", "content": "Training MoE architectures with learnable routers from scratch presents unique challenges in optimizing both the representation space and routing mechanism (Xue et al., 2024). The insight we identified is that MoE routers are responsible for partitioning the representation space for each expert. However, this representation space is sub-optimal in the early stages of model training, leading to a sub-optimally trained routing function.\nTo address this limitation of router training, we propose an upcycling approach, inspired by Komatsuzaki et al. (2023). Specifically, we begin by training an architecture consisting of 1 FFN expert per modality. After a predetermined number of steps, we upcycle this model by converting each modality-specific FFN into an expert-choice MoE module, initializing each expert with the expert trained from the first stage. We reset the learning rate scheduler while preserving the data loader state from the previous stage, thereby ensuring the second stage training is exposed to refreshed data.\nTo promote expert specialization, we augment the MoE routing function with Gumbel noise (Liu et al., 2022b; Geng et al., 2020), allowing our router to differentiably sample experts. This is expressed in Equation 5:\nGumbel-Sigmoid(x) = Sigmoid(x + G' \u2013 G"}, {"title": "3 Efficiency Optimization", "content": "To facilitate the distributed training of mixture of modality-aware experts (MoMa), we employ Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023). However, this approach presents unique efficiency challenges compared to vanilla MoEs. This section discusses these challenges and our strategies to address them."}, {"title": "3.1 Load Balancing", "content": "Without constraints, load imbalance can occur in our system because the ratio of text to image tokens can vary significantly across different GPUs and iterations. Imbalances can create a cascading straggler effect, delaying weight prefetching for subsequent layers and gradient releases from previous layers. This effectively bounds the training latency by the maximum time required to process text and image experts across all GPUs within a batch.\nTo mitigate these issues, we developed a balanced data mix that aligns the text-to-image data ratio with the expert ratio on each GPU. This approach ensures load balancing in expectation. While alternative rebalancing algorithms, such as token redistribution at each FFN layer, are possible, they may introduce additional communication overhead."}, {"title": "3.2 Efficient Expert Execution", "content": "We explored several strategies to efficiently execute experts for different modalities. The first approach restricts to homogeneous experts across modalities and prohibits routing of text tokens to image experts and vice versa. This method allows processing of all tokens and modalities simultaneously, provided all experts share the same token count. Alternatively, we could enhance execution efficiency by employing block sparsity (Gale et al., 2023), which offers similar benefits to the first approach without requiring perfect expert token balance. Additionally, we considered running experts from different modalities sequentially when the number of modalities is limited. This approach allows better overlap of computation from previous modality experts with weight prefetching for subsequent modality experts, alleviating memory pressure. It also removes assumptions about expert load balance.\nGiven that our experiments involve a sufficiently large token count per GPU, hardware utilization is not a major concern even with multiple batched matrix multiplications. Thus, we find the sequential approach to be a clean and flexible choice for our experimental environment at the current scale."}, {"title": "3.3 Additional Optimizations", "content": "We implemented several optimizations to further enhance throughput. These include generic optimizations such as gradient communication quantization and automatic GPU kernel fusion, as well as graph optimizations via torch.compile (Ansel et al., 2024). Additionally, we developed MoMa-specific optimizations, including the reuse of modality token indices across different layers to minimize device synchronization between CPU and GPU. We also consolidated per-layer stats communication and moved these operations off the critical path of training."}, {"title": "4 Experiments", "content": "We use the same pre-training dataset and preprocessing as Chameleon Team (2024). To assess scaling performance, we train all models with over 1 trillion tokens. Unless specified otherwise, we employ a sequence length of 4096 tokens and a model parallel size of 1. Our training regimen includes a peak learning rate of 1e-4, a 4000-step warm-up period, and linear annealing of the learning rate to 1% of its peak value. For all MoE architectures, we implement MoE in every layer, setting each expert's training capacity ke to b\u043c Em to maintain FLOPs per token comparable to the base dense model. In MoD architectures, we implement MoD in alternating layers, beginning with layer 0, using a layer capacity factor ca of 25%. To achieve FLOPs parity with the base dense model, we increase the total layer count while maintaining a constant hidden dimension."}, {"title": "4.2 Scaling of Performance with Compute", "content": "We present the scaling performance of various models across multiple compute levels, with FLOPs matched to three dense model sizes: 90M, 435M, and 1.4B parameters. We report two key metrics: (1) training loss and (2) pre-training speed-up factor \u03b7, which indicates that a sparse model can match the pre-training loss of the iso-FLOPs dense baseline using only 1/n of total FLOPs."}, {"title": "4.3 Scaling Number of Experts", "content": "We conducted further ablation experiments to investigate the impact of scaling the number of experts. We explored two scenarios: allocating an equal number of experts per modality (balanced expert allocation) and allocating different numbers of experts per modality. We included the dense model and expert-choice MoE with 8 mixed-modal experts (moe_8x) as baselines."}, {"title": "4.4 Upcycling", "content": "We investigate further the influence of upcycling as described in \u00a72.5. Specifically, we examine the 2.3B MoD model, comparing the training dynamics of mod_moe_4t4i when trained from scratch versus when initialized from an mod_moe_1tli checkpoint. To ensure a fair comparison, we adjust the data loader and training steps to account for the number of training steps already completed by mod_moe_1tli, thereby maintaining equivalent training FLOPs. We ablate initializing the model from 10k and 20k steps."}, {"title": "4.5 Throughput Analysis", "content": "Sparse models often cannot immediately lead to performance gains due to their added dynamism and associated data balancing issues (\u00a73). To quantify the impact of our proposals on training efficiency, we conducted a controlled experiment comparing the training throughput of various architectures, including a combination of MoMa, mixture-of-experts (MoE), and mixture-of-depths (MoD). We compare architectures FLOPs-matched to the 435M dense baseline. We performed our experiments on 256 A100 GPUs with a sequence length of 4096 and a batch size of 6 per GPU and summarized the model throughput in Table 2.\nComparing expert-choice MoE (moe_8x) and dense models, introducing sparsity does incur overheads (-9%). This loss in throughput likely comes from the need to compute routing decisions and synchronize gradients for all experts despite being FLOPs-equivalent. On the other hand, as discussed in \u00a73, running experts sequentially by modality (moe_1tli) does not suffer from large execution overheads, and we can attribute most of the loss (-6%) to computing token indices for each modality, which can be amortized by precomputing the indices and share with each transformer layer.\nWhen combining modality conditioned feed-forward with learned routing (moe_4t4i), we observe a smooth degradation in throughput as the number of experts increases, incurring additional 11% overheads with 8 experts, which is on par with the throughput loss of 9% when transitioning from the dense model to MoE with 8 experts.\nWhile combining MoD and MoE achieves the best training loss in our experiments, introducing MoD triggers"}, {"title": "4.6 Inference-time Performance", "content": "We evaluate our models (1.4B dense, 1.4B moe_1tli, 1.4B moe_8x, 1.4B moe_4t4i, 2.3B mod_moe_- 4t4i) on held-out language modeling data and downstream tasks. We report perplexity on subsets of OBELICS (Lauren\u00e7on et al., 2023) and Shutterstock, which are held-out from our pre-training dataset. In addition, we report 0-shot performance on a set of commonsense reasoning of tasks commonly used for benchmarking pre-trained language models. We also selected several vision-language task datasets and report the perplexity of the ground truth output in these datasets for cross model comparison, where we format the examples in a zero-shot manner.\n\u2022 Commonsense Reasoning: We evaluated on a set of text-only tasks commonly used for benchmarking the commonsense reasoning capabilities of language models: PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and BoolQ (Clark et al., 2019). We score the prompt with each candidate's answer and compute accuracy using the candidate with the highest score.\n\u2022 Image Captioning: We take the Karpathy test split of MS-COCO (Lin et al., 2014) and the Karpathy test split of Flickr30k (Plummer et al., 2015), and report text-to-image and image-to-text conditional perplexity using these two datasets.\n\u2022 Visual Question Answering: We report the perplexity of ground truth answers on the test-dev split of VQA-v2 (Goyal et al., 2017).\nFor all sparse models, we perform the second-stage training of auxiliary routers and use the auxiliary router for causal inference. The details of auxiliary router training can be found in Appendix A.2.\nInterleaved data modeling. The relative performance of the dense model and various MoE configurations is consistent with the trends observed in pre-training loss (\u00a74.2). As shown in Table 3, by using 1 more image expert, the 1.4B MoMa 1tli model significantly outperforms the dense baseline on most metrics, except for image-to-text conditional perplexity on COCO and Flickr. Similar to the trend shown in \u00a74.2, adding the image expert leads to substantial performance gains on the image modality. Further scaling up the number"}, {"title": "5 Related Work", "content": "Early-fusion vision-language models. Early fusion techniques have gained traction in multi-modal learning due to their ability to capture cross-modal interactions from the onset of processing. Several notable works have explored this paradigm. PerceiverIO (Jaegle et al., 2021) introduced a fully attentional read-process-write architecture that operates over a modality-agnostic latent space for processing diverse inputs, including text and images. N\u00dcWA (Wu et al., 2021) presented a 3D-attention transformer capable of both understanding and generating text, image, and video in various combinations. CM3 (Aghajanyan et al., 2022) adopted a causally masked transformer to learn from mixed-modal documents on the Internet, showcasing the scalability of this paradigm to large-scale pretraining. We adopt Chameleon as the base transformer architecture and demonstrate that modality-aware sparsity can effectively improve its scaling performance further."}, {"title": "6 Limitations", "content": "Our current MoMa implementation relies on matching the token mix ratio in the dataset with the expert mix ratio in the model to maintain load balance across GPUs. Even so, minor imbalance may still occur because there is no hard limit for a batch to deviate from that ratio at per-GPI per-iteration level. We leave further improvements in this area as future work.\nExpert-choice routing alleviates the expert load balancing issue during training but presents additional challenges for auto-regressive Language Models (LMs) during inference (Zhou et al., 2022). Although auxiliary routers comprise only a small fraction of the network's parameters, their role is crucial. In our study, we trained the auxiliary router after completing the whole network training and limited this process to a few thousand steps, while previous work have demonstrated the possibility to jointly train such modules with the full network (Raposo et al., 2024). Future research should explore the architecture and training techniques for auxiliary routers to prevent them from becoming a performance bottleneck and ensure generalizability across diverse data distributions. Especially, further investigation is necessary for training mixture-of-depths architectures, including both the auxiliary routers and the original model, to ensure effective performance in causal inference scenarios.\nIn our work, we experimented only the vanilla formulation of MoD and its staged integration with MoE. We leave the investigation of other MoD variations, including modality-aware MoD to future work. In addition, batched sequence generation with mixture-of-depths (MoD) is non-trivial as unlike standard sequence generation there are dynamics shapes and dynamic updates to the KV cache for each layer as certain sequences and layers may skip different tokens. There remains further room to optimize the inference implementations for MoD models."}, {"title": "7 Conclusion", "content": "In this work, we have introduced a series of modality-aware sparse architectures for early fusion, mixed-modality foundation models. Our approach leverages domain specificity while preserving cross-modality knowledge sharing and feature interaction. We have developed highly efficient and effective model architectures by incorporating sparsity in both the width dimension (through modality-aware mixture-of-experts) and the depth dimension (via mixture-of-depths).\nOur best architecture, Chameleon-MoMa, demonstrate significant improvements over state-of-the-art baselines. In complexity-controlled experiments, we reduce total FLOPs requirements by up to 3.7\u00d7. Importantly, our experimental findings reveal that our modality-aware sparse architectures maintain an empirical scaling law. This characteristic suggests that our approach provides immediate performance benefits and a scalable framework for future developments in mixed-modal foundation model training.\nThe promising potential of Chameleon-MoMa opens up several promising directions for future research. These include exploring more sophisticated routing mechanisms, investigating the impact of different sparsity patterns across modalities, and extending our approach to a broader range of modalities and tasks."}]}