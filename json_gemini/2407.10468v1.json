{"title": "LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis", "authors": ["Zhenxiong Tan", "Xinyin Ma", "Gongfan Fang", "Xinchao Wang"], "abstract": "Latent diffusion models have shown promising results in audio generation, making notable advancements over traditional methods. However, their performance, while impressive with short audio clips, faces challenges when extended to longer audio sequences. These challenges are due to model's self-attention mechanism and training predominantly on 10-second clips, which complicates the extension to longer audio without adaptation. In response to these issues, we introduce a novel approach, LiteFocus that enhances the inference of existing audio latent diffusion models in long audio synthesis. Observed the attention pattern in self-attention, we employ a dual sparse form for attention calculation, designated as same-frequency focus and cross-frequency compensation, which curtails the attention computation under same-frequency constraints, while enhancing audio quality through cross-frequency refillment. LiteFocus demonstrates substantial reduction on inference time with diffusion-based TTA model by 1.99\u00d7 in synthesizing 80-second audio clips while also obtaining improved audio quality.", "sections": [{"title": "1. Introduction", "content": "Text-to-audio (TTA) has become an increasingly important area of research, with practical applications that span speech synthesis [1, 2], music production [3, 4] and assistive technologies [5]. The recent progress in TTA has been significantly propelled by advancements in deep learning and the scaling up of models [6, 7, 8, 9, 3]. Among them, the application of latent diffusion models [10], originally developed for image and video generation, shows a significant leap forward to audio synthesis [6, 7, 8]. This has led to notable improvements in audio fidelity, showcasing the latent diffusion model's capacity to elevate the quality and feasibility of audio content creation.\nDespite the significant successes of diffusion-based model in audio synthesis, the model encounters efficiency challenges. Previous work on accelerating diffusion-based TTA models focuses on reducing the timesteps by progressive distillation [11, 12] or consistency distillation [13, 14]. Another line of work is to accelerate the sampling of diffusion by SDE [15] or ODE [16, 17] sampling methods. Those methods all successfully reduce the sampling steps and thus increase the efficiency. However, beyond reducing the number of inference steps for diffusion models, it is necessary to place emphasis on special time-consuming structures within the diffusion model. A primary issue is the drastic increase in inference time as the length of the generated audio extends [18], as the self-attention mechanism [19] within the unet model [20] has O(N2) complexity. For instance, generating an 80-second audio on the model AudioLDM2 takes approximately 10 minutes (details in Section 4.1). In Figure 1, we show that the inference time exhibits a quadratic increase with the length of the audio. Furthermore, as the length of the audio increases, self-attention gradually dominates the overall inference time. This increasing proportion underscores the importance of enhancing the efficiency of self-attention calculation in audio diffusion models under the scenario of long-form audio synthesis.\nInspired by this, we focus on the acceleration of attention mechanism in diffusion-based TTA model. Our approach does not require retraining the model, making it cost-effective and efficient. Our contributions can be summarized as follows:\n\u2022 We uncover a distinctive attention pattern specific to the mel-spectrogram within the attention blocks. The high interactions within the same frequency band reveals the redundancy of attention computation.\n\u2022 Drawing on these findings, we propose LiteFocus, a method that sparisifies the computation of attention. We factorize the computation of attention into two parts: sample-frequency focus and cross-frequency compensation.\n\u2022 Our experimental evaluations suggest that LiteFocus not only potentially reduces inference time of audio diffusion models but also may enhance the quality for longer audio segments in comparison to baseline methods."}, {"title": "2. Related Works", "content": "Audio Diffusion Models. Following the success in computer vision [10, 21, 22], diffusion models have been adeptly adapted for tasks such as text-to-speech synthesis and speech enhancement, demonstrating remarkable capabilities [23, 24, 6, 7, 8]. Earlier models like DiffWave [24] and Diff-TTS [23] showcased diffusion models' potential in audio synthesis. Recently, advancements with Diff-sound [24] and AudioLDM [7, 8] leveraged larger datasets and models to achieve superior audio quality, illustrating diffusion models' growing capability in high-quality audio generation from text.\nAccelerating Audio Synthesis Model. To speed up audio synthesis, methods such as InferGrad [25] use joint training to make inference faster, ProDiff [11] applies knowledge distillation to cut down on the number of steps needed, and techniques like Diffsound [6] and NoreSpeech [26] use VQ-VAE [27] for quicker token generation. Additionally, some general methods for speeding up diffusion models [28, 29, 30, 31] can also be applied in audio synthesis. For instance, Token Merging [28] optimizes the transformer module, offering a direct approach to enhancing the efficiency of diffusion models."}, {"title": "3. Methods", "content": "3.1. Preliminary\nThe audio latent diffusion model [6, 7, 8] can be succinctly divided into three main components: the text encoder [32], the denoise U-Net model [20], and a mel-spectrogram-based variational auto-encoder (VAE) [33]. The pipeline for converting text to audio operates as follows: Initially, an input text is processed by the text encoder, which encodes the text into a corresponding embedding. Subsequently, this specific text embedding serves to guide the iterative denoising of a sampled noise within the latent space, executed through a U-Net architecture. Upon completion of the denoising process, the latent representation is decoded by the VAE decoder into a mel-spectrogram, which can then be converted into audible sound.\nWithin the inference process of the audio latent diffusion model, self-attention modules play a pivotal role as one of the key units. The self-attention module takes an input \\(X \\in \\mathbb{R}^{(N_t\\times N_f)\\times C}\\) and transforms it into corresponding keys, queries, and values. \\(C\\) stands for the number of channels, and \\(N_t\\) and \\(N_f\\) delineate the dimensions within the latent space, corresponding respectively to the time and frequency bands of the mel spectrogram. The fundamental formula for computing self-attention can be expressed as follows:\n\\(Y = \\text{attention}(X) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\)\nwhere \\(K = W_kX, Q = W_qX, V = W_vX\\)\nwhere \\(d_k\\) represents the dimensionality of the keys, and \\(W_q\\), \\(W_k\\) and \\(W_b\\) are the projection matrices. This self-attention mechanism allows the model to weigh the importance of different parts of the input in relation to each other, enhancing its ability to generate coherent and contextually relevant audio outputs. However, in the context of audio latent diffusion, the computational load of the \\(QK^T\\) exhibits quadratic growth w.r.t the length of the audio.\n3.2. Attention Pattern for Audio Latent Diffusion\nWe show the attention maps in Figure 2. The results highlight a unique attention pattern, focusing on interactions within the same frequency band. We observe that the token's attention is specifically directed towards other tokens sharing the same frequency coordinate. From Figure 2(a), we observe that tokens of the same frequency have higher attention values relative to each other. In Figure 2(b), this is shown as the attention pattern exhibiting equidistant repeating pattern, and the interval of this repetition corresponds to the number of frequency dimensions in the mel spectrogram within the latent space. Additionally, we find that this pattern is more pronounced in the down-sampling block compared to the up-sampling block.\n3.3. LiteFocus\nMotivated by the aforementioned approach, we posit that dedicating substantial computational resources to the repetitive patterns represents a promising avenue for optimization. Particularly for longer audio sequences, enhancing the efficiency of attention computations and eliminating superfluous operations stand to offer considerable improvements in inference speed, all while maintaining the quality of the generated audio.\nTo facilitate the streaming of audio diffusion model without extra post-training, we propose LiteFocus that sparsifies the attention mechanism. Our approach focuses on the attention process by sparsifying each query's attention operation on a subset of all tokens, rather than computing attention across all keys and values for every token. We factorize the sparsification of attention into two directions: Same-frequency Focusing and Cross-frequency Compensation.\nTo elaborate, within the attention mechanism, a token located in the mel-spectrogram latent space at coordinates (a, b) is denoted as \\(X_i\\), where i is the index after flattening the dimensions, computed as \\(i = a N_t + b\\). Here, \\(N_t\\) represents the total number of time steps in the latent space, and (a, b) specifies the token's position with a and b indicating its time and frequency coordinates, respectively. For this token \\(X_i\\), there is a specifically associated focus-set \\(F_i\\). This focus-set \\(F_i\\) represents the collection of indices from keys K and values V with which query \\(Q_i\\) is intended to interact. The output \\(y_i\\) obtained from this query is formulated as follows:\n\\(y_i = \\text{softmax}\\left(\\frac{Q_iK_{F_i}V_{F_i}}{\\sqrt{d_k}}\\right)\\)\nwhere \\(K_{F_i}\\) and \\(V_{F_i}\\) denote the subsets of keys and values selected for interaction with \\(Q_i\\), defined as:\n\\(K_{F_i} = \\{K_j|j \\in F_i\\}\\)\n\\(V_{F_i} = \\{V_j|j\\in F_i\\}\\)\nThe focus-set \\(F_i\\) for each query \\(q_i\\) is formed by the union of two distinct sets:\n\\(F = S \\cup C\\).\n\u2022 Focus on Same-frequency Tokens.\nThe set of same-frequency tokens \\(S_i\\) is composed of indices corresponding to tokens that are in the same frequency band as \\(Q_i\\). This can be formulated as:\n\\(S_i = \\{j | j \\mod N_t = i \\mod N_t\\}\\)\n\u2022 Compensation on Cross-frequency Tokens.\nThe set of Cross-frequency Compensation C is obtained by randomly selecting indices from the full index set I of keys and values according to the percentage r:\n\\(C = \\text{RandomSample} (I, [r\\cdot |I|]),\\)\nwhere r represents the percentage of the total index set I to be included in the subset C, and |I| denotes the cardinality (or total number) of I. This selection process provides a global context through a diverse sample from I, proportional to r.\nFigure 3 illustrates the process of selecting the focus-set by LiteFocusCombining both the Same-frequency Focusing Si and the Cross-frequency Compensation C allows the attention mechanism to achieve both a broad contextual understanding and a detailed insight into frequency-specific relationships within the audio. By ensuring that each query interacts with only a limited number of tokens, our method effectively reduces the total computational load."}, {"title": "4. Experiments", "content": "4.1. Setup\nBase Model and Inference Setting: In our experiments, we focused on the AudioLDM2 model, one of the standout models in the audio latent diffusion category, specifically using the audioldm2-full checkpoint \u00b9. The diffusion steps were set to the default 200 steps.\nEvaluation: Our experiments utilized the AudioCap Evaluation Set [34], generating 4845 unique audio clips from 4845 captions to serve as text prompts. The quality and efficiency of these audio generations were evaluated using several metrics, including Frechet Audio Distance (FAD) [35], Kullback-Leibler (KL) divergence, Contrastive Language-Audio Pretraining (CLAP) score [36] and the inference time required for each audio clip. The code used for these evaluation experiments is based on the repository Audio Generation Evaluation\u00b2.\nInfrastructure To ensure consistency and reliability in our measurements, especially for those related to computational costs during inference, all experiments were conducted on a single Nvidia A6000 GPU.\nLiteFocus Setting: For LiteFocus we applied it to all Transformer Modules within the second block of the down blocks and the second block of the up blocks of the audio latent diffusion model. Furthermore, the proportion r for Cross-frequency Compensation is set to 0.1.\nBaseline: We also applied the Token Merging [28] approach to audio latent diffusion model as our baseline. Similar to our method, Token Merging aims to reduce the redundant computations within the attention operation. Its primary strategy involves merging similar tokens into a single token before executing the attention operation, thereby reducing the computational load of the attention mechanism. Token Merging was applied to the same transformer modules as LiteFocus and utilized all default parameters.\n4.2. Performance Comparison\nTable 1 presents a comparative analysis of the experimental results for the original inference, acceleration with LiteFocus, and acceleration with Token Merging. It's observed that LiteFocus's acceleration factor increases with the length of the generated audio. Notably, for 80-second clips, LiteFocus achieves a 1.99\u00d7 faster processing while also improving the audio's quality metrics over those generated by the standard inference. Additionally, as the length of the generated audio increases, its three performance metrics-FAD, KL, and CLAP-all deteriorate. This decline in performance may be attributed to the original model is primarily trained on 10-second audio clips.\nWhen comparing Token Merging's efficiency to LiteFocus, as shown in Table 1, we observed that for 80-second audio generation, Token Merging only achieved a 1.56 times faster inference speed, less impressive than LiteFocus's 1.99 times speedup. Additionally, unlike LiteFocus, which mitigated the decline in audio quality metrics as audio length increased, audio generated through Token Merging exhibited poorer quality metrics compared to the original inference result.\n4.3. Analysis\nIn LiteFocus, the focus-set \\(F_i\\) associated with each query \\(Q_i\\) is union of distinct sets: one is same-frequency focusing \\(S_i\\), which targets attention within identical frequency bands, and the other part is cross-frequency compensation C addressing potential gaps missed by \\(S_i\\).\nBuilding upon the original LiteFocus framework, we firstly conducted experiments without the cross-frequency compensation C, and the results are presented in Table 3. We observed that when retaining only same-frequency focusing, the audio generated for durations of 10 seconds and 20 seconds exhibited a decline in quality metrics compared to the original LiteFocus. However, this degradation becomes less pronounced with longer audio. Interestingly, for 80-second audio clips, the quality metrics with only same-frequency focusing slightly surpassed those of the original LiteFocus.\nWe also conducted LiteFocus experiments exclusively employing cross-frequency compensation, withoud same-frequency focusing, and tested various r percentages across the generation of audio clips of different durations. Table 2 presents these adjustments' impact on the quality metrics of generated audio clips, illustrating the relationship between reduced cross-frequency compensation ratios r and audio quality. We can observe that in shorter audio clips (10 seconds and 20 seconds), a lower cross-frequency compensation percentages r leads to a quicker degradation in performance metrics. However, this trend slows down in the case of longer audio clips, such as those in 40 and 80 seconds. This indicates the presence of considerable redundancy in the generation of long audio, suggesting that longer sequences can tolerate a higher degree of sparsity without significant loss in quality.\nThese findings reveal that same-frequency focusing alone can enhance long audio quality, while optimal cross-frequency compensation improves short audio synthesis. This suggests that attention sparsity based on audio length is key for maintaining high-quality audio generation."}, {"title": "5. Conclusion and Future Work", "content": "In conclusion, our work presents LiteFocus, an approach designed to improve the generation of longer audio sequences by latent diffusion models. By employing a dual sparse attention mechanism, focusing on same-frequency and cross-frequency compensation, we address some of the computational challenges and performance limitations associated with these models. This method has shown to reduce inference time and modestly enhance audio quality for longer clips without necessitating model retraining. Future work can concentrate on integrating the LiteFocus mechanism directly into the training process to develop inherently efficient models."}]}