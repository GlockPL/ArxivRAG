{"title": "LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis", "authors": ["Zhenxiong Tan", "Xinyin Ma", "Gongfan Fang", "Xinchao Wang"], "abstract": "Latent diffusion models have shown promising results in audio generation, making notable advancements over traditional methods. However, their performance, while impressive with short audio clips, faces challenges when extended to longer audio sequences. These challenges are due to model's self- attention mechanism and training predominantly on 10-second clips, which complicates the extension to longer audio without adaptation. In response to these issues, we introduce a novel approach, LiteFocus that enhances the inference of existing au- dio latent diffusion models in long audio synthesis. Observed the attention pattern in self-attention, we employ a dual sparse form for attention calculation, designated as same-frequency fo- cus and cross-frequency compensation, which curtails the at- tention computation under same-frequency constraints, while enhancing audio quality through cross-frequency refillment. LiteFocus demonstrates substantial reduction on inference time with diffusion-based TTA model by 1.99\u00d7 in synthesizing 80- second audio clips while also obtaining improved audio quality.", "sections": [{"title": "1. Introduction", "content": "Text-to-audio (TTA) has become an increasingly important area of research, with practical applications that span speech synthe- sis [1, 2], music production [3, 4] and assistive technologies [5]. The recent progress in TTA has been significantly propelled by advancements in deep learning and the scaling up of mod- els [6, 7, 8, 9, 3]. Among them, the application of latent dif- fusion models [10], originally developed for image and video generation, shows a significant leap forward to audio synthe- sis [6, 7, 8]. This has led to notable improvements in audio fidelity, showcasing the latent diffusion model's capacity to el- evate the quality and feasibility of audio content creation.\nDespite the significant successes of diffusion-based model in audio synthesis, the model encounters efficiency challenges. Previous work on accelerating diffusion-based TTA models fo- cuses on reducing the timesteps by progressive distillation [11, 12] or consistency distillation [13, 14]. Another line of work is to accelerate the sampling of diffusion by SDE [15] or ODE [16, 17] sampling methods. Those methods all success- fully reduce the sampling steps and thus increase the efficiency. However, beyond reducing the number of inference steps for diffusion models, it is necessary to place emphasis on spe- cial time-consuming structures within the diffusion model. A primary issue is the drastic increase in inference time as the length of the generated audio extends [18], as the self-attention mechanism [19] within the unet model [20] has O(N2) com- plexity. For instance, generating an 80-second audio on the model AudioLDM2 takes approximately 10 minutes (details in Section 4.1). In Figure 1, we show that the inference time ex- hibits a quadratic increase with the length of the audio. Further- more, as the length of the audio increases, self-attention gradu- ally dominates the overall inference time. This increasing pro- portion underscores the importance of enhancing the efficiency of self-attention calculation in audio diffusion models under the scenario of long-form audio synthesis.\nInspired by this, we focus on the acceleration of attention mechanism in diffusion-based TTA model. Our approach does not require retraining the model, making it cost-effective and efficient. Our contributions can be summarized as follows:\n\u2022 We uncover a distinctive attention pattern specific to the mel- spectrogram within the attention blocks. The high interac- tions within the same frequency band reveals the redundancy of attention computation.\n\u2022 Drawing on these findings, we propose LiteFocus, a method that sparisifies the computation of attention. We factorize the computation of attention into two parts: sample-frequency focus and cross-frequency compensation.\n\u2022 Our experimental evaluations suggest that LiteFocus not only potentially reduces inference time of audio diffusion models but also may enhance the quality for longer audio segments in comparison to baseline methods."}, {"title": "2. Related Works", "content": "Audio Diffusion Models. Following the success in computer vision [10, 21, 22], diffusion models have been adeptly adapted for tasks such as text-to-speech synthesis and speech enhance- ment, demonstrating remarkable capabilities [23, 24, 6, 7, 8]. Earlier models like DiffWave [24] and Diff-TTS [23] showcased diffusion models' potential in audio synthesis. Recently, ad- vancements with Diff-sound [24] and AudioLDM [7, 8] lever- aged larger datasets and models to achieve superior audio qual- ity, illustrating diffusion models' growing capability in high- quality audio generation from text.\nAccelerating Audio Synthesis Model. To speed up audio syn- thesis, methods such as InferGrad [25] use joint training to make inference faster, ProDiff [11] applies knowledge distilla- tion to cut down on the number of steps needed, and techniques like Diffsound [6] and NoreSpeech [26] use VQ-VAE [27] for quicker token generation. Additionally, some general methods for speeding up diffusion models [28, 29, 30, 31] can also be applied in audio synthesis. For instance, Token Merging [28] optimizes the transformer module, offering a direct approach to enhancing the efficiency of diffusion models."}, {"title": "3. Methods", "content": "3.1. Preliminary\nThe audio latent diffusion model [6, 7, 8] can be succinctly di- vided into three main components: the text encoder [32], the denoise U-Net model [20], and a mel-spectrogram-based vari- ational auto-encoder (VAE) [33]. The pipeline for converting text to audio operates as follows: Initially, an input text is pro- cessed by the text encoder, which encodes the text into a cor- responding embedding. Subsequently, this specific text embed- ding serves to guide the iterative denoising of a sampled noise within the latent space, executed through a U-Net architecture. Upon completion of the denoising process, the latent represen- tation is decoded by the VAE decoder into a mel-spectrogram, which can then be converted into audible sound.\nWithin the inference process of the audio latent diffu- sion model, self-attention modules play a pivotal role as one of the key units. The self-attention module takes an input X \u2208 R(Nt\u00d7Nf)\u00d7C and transforms it into corresponding keys, queries, and values. C stands for the number of channels, and Nt and Nf delineate the dimensions within the latent space, corresponding respectively to the time and frequency bands of the mel spectrogram. The fundamental formula for computing self-attention can be expressed as follows:\nY = attention(X) = softmax ( \\frac{QKT}{\\sqrt{dk}} )V,\nwhere K = WkX, Q = WqX, V = WvX\nwhere dk represents the dimensionality of the keys, and Wq, Wk and Wb are the projection matrices. This self-attention mechanism allows the model to weigh the importance of dif- ferent parts of the input in relation to each other, enhancing its ability to generate coherent and contextually relevant audio out- puts. However, in the context of audio latent diffusion, the com- putational load of the QKT exhibits quadratic growth w.r.t the length of the audio.\n3.2. Attention Pattern for Audio Latent Diffusion\nWe show the attention maps in Figure 2. The results highlight a unique attention pattern, focusing on interactions within the same frequency band. We observe that the token's attention is specifically directed towards other tokens sharing the same frequency coordinate. From Figure 2(a), we observe that tokens of the same frequency have higher attention values relative to each other. In Figure 2(b), this is shown as the attention pattern exhibiting equidistant repeating pattern, and the interval of this repetition corresponds to the number of frequency dimensions in the mel spectrogram within the latent space. Additionally, we find that this pattern is more pronounced in the down-sampling block compared to the up-sampling block.\n3.3. LiteFocus\nMotivated by the aforementioned approach, we posit that dedi- cating substantial computational resources to the repetitive pat- terns represents a promising avenue for optimization. Partic- ularly for longer audio sequences, enhancing the efficiency of attention computations and eliminating superfluous operations stand to offer considerable improvements in inference speed, all while maintaining the quality of the generated audio.\nTo facilitate the streaming of audio diffusion model without extra post-training, we propose LiteFocus that sparsifies the at- tention mechanism. Our approach focuses on the attention pro- cess by sparsifying each query's attention operation on a subset of all tokens, rather than computing attention across all keys and values for every token. We factorize the sparsification of atten- tion into two directions: Same-frequency Focusing and Cross- frequency Compensation.\nTo elaborate, within the attention mechanism, a token lo- cated in the mel-spectrogram latent space at coordinates (a, b) is denoted as Xi, where i is the index after flattening the di- mensions, computed as i = a Nt + b. Here, Nt represents the total number of time steps in the latent space, and (a, b) spec- ifies the token's position with a and b indicating its time and frequency coordinates, respectively. For this token Xi, there is a specifically associated focus-set Fi. This focus-set Fi repre- sents the collection of indices from keys K and values V with which query Qi is intended to interact. The output yi obtained from this query is formulated as follows:\nyi = softmax ( \\frac{QiKFVF}{\\sqrt{dk}} )V,\nwhere KF; and VF. denote the subsets of keys and values se- lected for interaction with Qi, defined as:\nKF\u2081 = {Kj|j \u2208 F;}\nVF\u2081 = {Vj|j\u2208 F;}\nThe focus-set Fi for each query qi is formed by the union of two distinct sets:\nF = SUC.\n\u2022 Focus on Same-frequency Tokens.\nThe set of same-frequency tokens Si is composed of indices corresponding to tokens that are in the same frequency band as Qi. This can be formulated as:\nSi = {j | j mod Nt = i mod Nt}\n\u2022 Compensation on Cross-frequency Tokens.\nThe set of Cross-frequency Compensation C is obtained by randomly selecting indices from the full index set I of keys and values according to the percentage r:\nC = RandomSample (I, [r\u00b7 |I|]),\nwhere r represents the percentage of the total index set I to be included in the subset C, and I denotes the cardinality (or total number) of I. This selection process provides a global context through a diverse sample from I, proportional to r.\nFigure 3 illustrates the process of selecting the focus-set by Lite- FocusCombining both the Same-frequency Focusing Si and the Cross-frequency Compensation C allows the attention mecha- nism to achieve both a broad contextual understanding and a detailed insight into frequency-specific relationships within the audio. By ensuring that each query interacts with only a lim- ited number of tokens, our method effectively reduces the total computational load."}, {"title": "4. Experiments", "content": "4.1. Setup\nBase Model and Inference Setting: In our experiments, we focused on the AudioLDM2 model, one of the standout mod- els in the audio latent diffusion category, specifically using the audioldm2-full checkpoint \u00b9. The diffusion steps were set to the default 200 steps.\nEvaluation: Our experiments utilized the AudioCap Eval- uation Set [34], generating 4845 unique audio clips from 4845 captions to serve as text prompts. The quality and efficiency of these audio generations were evaluated using several met- rics, including Frechet Audio Distance (FAD) [35], Kullback- Leibler (KL) divergence, Contrastive Language-Audio Pretrain- ing (CLAP) score [36] and the inference time required for each audio clip. The code used for these evaluation experiments is based on the repository Audio Generation Evaluation\u00b2.\nInfrastructure To ensure consistency and reliability in our measurements, especially for those related to computational costs during inference, all experiments were conducted on a sin- gle Nvidia A6000 GPU.\nLiteFocus Setting: For LiteFocus we applied it to all Transformer Modules within the second block of the down blocks and the second block of the up blocks of the audio la- tent diffusion model. Furthermore, the proportion r for Cross- frequency Compensation is set to 0.1.\nBaseline: We also applied the Token Merging [28] ap- proach to audio latent diffusion model as our baseline. Simi- lar to our method, Token Merging aims to reduce the redundant computations within the attention operation. Its primary strat- egy involves merging similar tokens into a single token before executing the attention operation, thereby reducing the compu- tational load of the attention mechanism. Token Merging was applied to the same transformer modules as LiteFocus and uti- lized all default parameters.\n4.2. Performance Comparison\nTable 1 presents a comparative analysis of the experimental re- sults for the original inference, acceleration with LiteFocus, and acceleration with Token Merging. It's observed that LiteFocus's acceleration factor increases with the length of the generated au- dio. Notably, for 80-second clips, LiteFocus achieves a 1.99\u00d7 faster processing while also improving the audio's quality met-"}, {"title": "4.3. Analysis", "content": "In LiteFocus, the focus-set Fi associated with each query Qi is union of distinct sets: one is same-frequency focusing Si, which targets attention within identical frequency bands, and the other part is cross-frequency compensation C addressing potential gaps missed by Si.\nBuilding upon the original LiteFocus framework, we firstly conducted experiments without the cross-frequency compensa- tion C, and the results are presented in Table 3. We observed that when retaining only same-frequency focusing, the audio generated for durations of 10 seconds and 20 seconds exhib- ited a decline in quality metrics compared to the original Lite- Focus. However, this degradation becomes less pronounced with longer audio. Interestingly, for 80-second audio clips, the quality metrics with only same-frequency focusing slightly sur- passed those of the original LiteFocus.\nWe also conducted LiteFocus experiments exclusively employing cross-frequency compensation, withoud same- frequency focusing, and tested various r percentages across the generation of audio clips of different durations. Table 2 presents these adjustments' impact on the quality metrics of generated audio clips, illustrating the relationship between reduced cross- frequency compensation ratios r and audio quality. We can ob- serve that in shorter audio clips (10 seconds and 20 seconds), a lower cross-frequency compensation percentages r leads to a quicker degradation in performance metrics. However, this trend slows down in the case of longer audio clips, such as those in 40 and 80 seconds. This indicates the presence of consider- able redundancy in the generation of long audio, suggesting that longer sequences can tolerate a higher degree of sparsity with- out significant loss in quality.\nThese findings reveal that same-frequency focusing alone can enhance long audio quality, while optimal cross-frequency compensation improves short audio synthesis. This suggests that attention sparsity based on audio length is key for main- taining high-quality audio generation."}, {"title": "5. Conclusion and Future Work", "content": "In conclusion, our work presents LiteFocus, an approach de- signed to improve the generation of longer audio sequences by latent diffusion models. By employing a dual sparse attention mechanism, focusing on same-frequency and cross-frequency compensation, we address some of the computational chal- lenges and performance limitations associated with these mod- els. This method has shown to reduce inference time and mod- estly enhance audio quality for longer clips without necessitat- ing model retraining. Future work can concentrate on integrat- ing the LiteFocus mechanism directly into the training process to develop inherently efficient models."}]}