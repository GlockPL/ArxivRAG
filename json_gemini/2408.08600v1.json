{"title": "MM-UNet: A Mixed MLP Architecture for Improved Ophthalmic Image Segmentation", "authors": ["Zunjie Xiao", "Xiaoqing Zhang", "Risa Higashita", "Jiang Liu"], "abstract": "Ophthalmic image segmentation serves as a critical foundation for ocular disease diagnosis. Although fully convolutional neural networks (CNNs) are commonly employed for segmentation, they are constrained by inductive biases and face challenges in establishing long-range dependencies. Transformer-based models address these limitations but introduce substantial computational overhead. Recently, a simple yet efficient Multilayer Perceptron (MLP) architecture was proposed for image classification, achieving competitive performance relative to advanced transformers. However, its effectiveness for ophthalmic image segmentation remains unexplored. In this paper, we introduce MM-UNet, an efficient Mixed MLP model tailored for ophthalmic image segmentation. Within MM-UNet, we propose a multi-scale MLP (MMLP) module that facilitates the interaction of features at various depths through a grouping strategy, enabling simultaneous capture of global and local information. We conducted extensive experiments on both a private anterior segment optical coherence tomography (AS-OCT) image dataset and a public fundus image dataset. The results demonstrated the superiority of our MM-UNet model in comparison to state-of-the-art deep segmentation networks.", "sections": [{"title": "1 Introduction", "content": "In clinical diagnosis, the segmentation of ophthalmic images is a critical step [20,21]. Many automatic measurements of ophthalmic clinical parameters rely on accurate segmentation results [17]. Early research primarily utilized traditional image processing algorithms for segmentation, such as Canny edge detection [4,11], often accompanied by complex image preprocessing. With the success of deep learning across various fields, Convolutional Neural Networks (CNNs) have demonstrated substantial progress in many ophthalmic segmentation tasks [2,14,13], ranging from lens segmentation [2] in the ophthalmic anterior segment to disc segmentation [13] in the posterior segment. However, CNN-based models generally exhibit limitations in establishing long-range dependencies, as convolution operations primarily capture local information. Consequently, CNN-based approaches often perform inadequately on target structures with diverse shapes and textures.\nThe introduction of transformer architectures [16] provides a practical solution to overcome these limitations, with examples including ViTs [5], TransUNet [3], and UTNet [7]. These transformer-based methods have also achieved significant performance in ophthalmic image segmentation [22]. In recent years, it has been commonly believed that the self-attention mechanism is necessary for establishing long-range dependencies in deep networks. However, the recently proposed pure multi-layer perceptron (MLP) models, such as the MLP-Mixer [15], demonstrate that a simple stack of MLPs also holds great potential. The MLP-Mixer consists of a repeated unit called the mixer layer, which has two main components: the channel-mixing MLP and the token-mixing MLP. The channel-mixing MLP is similar to a depthwise convolution, designed for interaction between channels, while the token-mixing MLP facilitates interaction between spatial patches. Unlike transformer models, the MLP-Mixer does not require a complex self-attention mechanism and instead alternates between stacking the channel-mixing MLP and the token-mixing MLP. Despite their simpler structure, MLP-based models have achieved accuracy comparable to that of transformer models.\nInspired by recent advancements, several MLP-based models [19,18] have rapidly emerged, showcasing tremendous potential. This motivates us to design an MLP-based model for ophthalmic image segmentation. However, we face two significant challenges in developing MLP-based ophthalmic segmentation models. Firstly, unlike transformer architectures that incorporate a position embedding operator, MLP-based models can easily lose location information after several fully-connected layers. While this may not be problematic for classification tasks, maintaining location information is crucial for segmentation tasks. Secondly, similar to transformer architectures, most MLP-based models require pre-training on large datasets to perform well, due to the absence of image-specific inductive biases. This presents a challenge, as ophthalmic datasets are typically small [10].\nTo address these challenges, we have designed a Multi-Scale MLP (MMLP) module and proposed a mixed MLP architecture termed MM-UNet. The hybrid design, integrating convolutional and MLP components, aims to leverage the strengths of both approaches. Initially, we utilize UNet [12] to extract multi-level local features. Subsequently, the MMLP module re-establishes long-range dependencies while partially retaining local information. Compared to the MLP-mixer, our MMLP module omits the channel-mixing MLP, as the convolutional components in our model already establish inter-channel relationships. Furthermore, the MMLP module groups channels into different scales for local token-mixing rather than employing global token-mixing, thereby constraining the token-mixing range to several defined sizes. Additionally, the MMLP module"}, {"title": "2 Method", "content": "reduces computational consumption compared to the MLP-mixer. We evaluated MM-UNet using two datasets: one from the anterior segment and the other from the posterior segment. Experimental results demonstrate that our mixed MLP architecture exhibits significant potential in ophthalmic image segmentation."}, {"title": "2.1 Revisiting MLP-mixer Mechanism", "content": "MLP-mixer is a pioneering architecture that shows the potential of the pure-MLP models. It consists of three parts: per-patch embedding layer, Mixer layers, and classification layer. In this section, we briefly review this inspiring pure-MLP method.\nPer-patch embedding layer. For an given image input $I \\in \\mathbb{R}^{3 \\times W \\times H}$, we firstly crop $I$ into $n$ non-overlap patches $p_i \\in \\mathbb{R}^{P \\times P \\times 3}$, where $n = WH/P^2$. After that, each $p_i$ is unfolded into a vector in the space of $\\mathbb{R}^{3P^2}$. At this time, a shared fully connected layer named the embedding layer projects each patch $p_i$ into a hidden dimension $C$. Now we get an output $X \\in \\mathbb{R}^{n \\times C}$ that represents $n$ patches with dimension $C$.\nMixer layers. The mixer layer is built upon two types of MLP layers: the channel mixing MLP and the token mixing MLP. The former realizes the information exchange between channels, and the latter realizes the establishment of tokens. For the input $X$ calculated by the embedding layer, the mixer layers can be written as follows:\n$U = U + W_2\\sigma[W_1LayerNorm(X)],$ (1)\n$Y = U + \\sigma(LayerNorm(U)W_3)W_4.$ (2)\nHere, the Eq. 1 represents channel mixing, $W_1$ is the weights of a fully-connected layer increase the feature dimension by a ratio $r$. Moreover, $W_2$ denotes a subsequent fully-connected layer that reduces the feature dimension back to the original size. Moreover, $LayerNorm(\\cdot)$ represent the layer normalization [1] and $\\sigma(\\cdot)$ denotes the nonlinear activate function GELU[9]. Equation 2 represents token mixing, which is similar to channel mixing, except changing the target dimension from channel to block.\nClasification layer. After a repeat of N mixer layers, we suppose the final output of mixer layers is $Y \\in \\mathbb{R}^{n \\times d}$. The classification layer can be formulated as follow:\n$Classifier(Y) = fc(GAP(Y)),$ (3)\nwhere $GAP()$ represents global average pooling, and $fc(.)$ denote the fully-connected layer.\nIt is evident that the overall design of the MLP-mixer is remarkably simple, and its robust capability to model long-range dependencies primarily stems from the token-mixing operation."}, {"title": "2.2 Multi-scale MLP Block", "content": "Upon revisiting the concept, we recognize the potential of token-mixing MLP. However, the token-mixing step also completely intermixes spatial information, resulting in the loss of location information. While location information may not be crucial for classification tasks, it is highly significant for segmentation. To preserve location information during the token-mixing process, we have made improvements to the original token-mixing operator and proposed a new MLP structure, the Multi-Scale MLP (MMLP), as illustrated in Fig. 1.\nFor an input $X \\in \\mathbb{R}^{C \\times W \\times H}$, we first divide $X$ into $k$ groups by channel, denoted as\n$g_1 = X[0: i_1, :, :],$\n$g_j = X[i_{j-1} : i_j, :, :],$\n$g_k = X[i_{k-1}: C + 1, :, :],$ (4)\n$0 < i_1 < i_2 < \\dots < i_{k-1} < C + 1,$\nwhere $C$ is the number of channels, and $g_i, 1 \\le i \\le k$ denotes the $i$-th group.\nFor the $i$ th group $g_i$, the mixing method is defined follow:\n$g_{si} = crop(g_i, s_i)$\n$Y_i = LTM(g_{si}, n_i)$ (5)\nHere, $crop()$ refers to cropping a feature map into several $s_i \\times s_i$ patches, similar to the process in Mixer-MLP. Unlike the global correlations established"}, {"title": "2.3 Network Architecture", "content": "We proposed a mixed MLP architecture that combines the MLP and UNet architectures, termed MM-UNet. Figure 2 illustrates the architecture of MM-UNet."}, {"title": "3 Experiment", "content": "We selected two modalities of commonly used ophthalmic images, the Anterior Segment Optical Coherence Tomography (AS-OCT) and fundus, to verify the effectiveness of MM-UNet.\nAS-OCT Dataset. The AS-OCT is our private dataset collected through the CASIA2 (Tomey Corporation, Japan) Ophthalmic Imaging device, with the label of lens substructure, which is shown in Fig. 3. Our task is to divide the lens area into nucleus, cortex, and capsule, which is crucial for the surgical judgment of cataracts. There are 1844 images here from 284 subjects, including 154 cataract lenses and 130 healthy lenses. We label four images with equal intervals for each eye. So we labeled 461 eyes, which contain 230 right eyes and 231 left eyes, and some images are missing due to the occlusion of the eyelids during collection.\nFundus Dataset. We use the REFUGE2 fundus dataset in MICCAI 2019[6], which is a multi-domain dataset collected from different devices. Although the entire REFUGE2 dataset contains 2000 images with multi-task labels for segmentation, classification, and localization, we only used the available 1200 fundus images for the optic cup and optic disc segmentation task."}, {"title": "3.2 Experiment Setup", "content": "For data division, we divide the AS-OCT dataset by subject into three disjoint subsets: training dataset, validation dataset, and testing dataset in a ratio of 6:2:2 on a random selection basis. Furthermore, in the REFUGE2 dataset, we preserve the original division strategy: 400 images for training, 400 images for validation, and 400 images for testing. We train all models from scratch for 150 epochs. We use the stairs learning rate scheduler with a base learning rate of 0.015, which decreased by 10 every 10 epochs after 100 epochs. We use the SGD optimizer with a batch size of 16 on one 12G Titan V GPU with a momentum of 0.9 and a weight decay of $10^{-4}$, respectively. All images are resized to 256 \u00d7 256 before entering the models. We use the cross-entropy loss to train all models.\nWe use the mIoU value to evaluate the segmentation performance of the model. The calculation formulas are as follows:\n$mIoU = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{A_i \\cap B_i}{A_i \\cup B_i},$ (7)\nwhere $A_i$ represents the model prediction area for category i, $B_i$ represents the ground truth area for category i, and $N$ is the total number of categories. Furthermore, we use #P to denote the number of parameters and Acc to represent accuracy."}, {"title": "3.3 Segmentation Results", "content": "Table 2 shows the compared results of our proposed MM-UNet with other CNN-based and transformer-based models. We can see that our proposed MM-UNet achieves the best performance both on the AS-OCT dataset and on the REFUGE2 dataset. In the AS-OCT dataset, our method achieves 98.2% accuracy and 92.64% mIoU, respectively, which outperforms 1.2% mIoU than UNet with almost no parameter increase. In the REFUGE2 dataset, MM-UNet also"}, {"title": "3.4 Ablation Study", "content": "To verify the effectiveness of our proposed MMLP block, we tried to replace the LTM operator with the global token-mixing MLP. Moreover, the compared result is shown in Tab. 3. To verify the effectiveness of our proposed MMLP block, we replaced the LTM operator with the global token-mixing MLP. The comparative results are presented in Table 3. Our LTM demonstrates improvements in both mIoU and Acc compared to the token-mixing methods in both AS-OCT and REFUGE2."}, {"title": "4 Conclusion", "content": "The growing popularity of MLP architecture underscores its significant potential in various MLP-based applications. In this study, we introduce a novel mixed MLP architecture specifically designed for ophthalmic image segmentation, representing a pioneering effort in the field of ophthalmology. Furthermore,"}]}