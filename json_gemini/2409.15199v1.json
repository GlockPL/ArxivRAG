{"title": "Learning from Contrastive Prompts: Automated Optimization and Adaptation", "authors": ["Mingqi Li", "Karan Aggarwal", "Yong Xie", "Aitzaz Ahmad", "Stephan Lau"], "abstract": "As LLMs evolve, significant effort is spent on manually crafting prompts. While existing prompt optimization methods automate this process, they rely solely on learning from incorrect samples, leading to a sub-optimal performance. Additionally, an unexplored challenge in the literature is prompts effective for prior models may not perform well on newer versions or different languages. We propose the Learning from Contrastive Prompts (LCP) framework to address these gaps, enhancing both prompt optimization and adaptation. LCP employs contrastive learning to generate effective prompts by analyzing patterns in good and bad prompt examples. Our evaluation on the Big-Bench Hard dataset shows that LCP has a win rate of over 76% over existing methods in prompt optimization and demonstrates strong adaptability across different model versions, families, and languages. LCP offers a systematic approach to prompt engineering, reducing manual effort in deploying LLMs across varied contexts.", "sections": [{"title": "Introduction", "content": "The current approach to utilize Large Language Models (LLMs) begins with users providing their queries. These queries are then augmented with additional instructions, called prompts, by the system to enhance response quality. Prompts often include contextual information or instructions that help the model better understand and respond to a query. Prompts may also include guidelines to restrict the LLM from generating harmful or inappropriate content, ensuring safer and more reliable interactions. The process of writing these prompts typically involves trial-and-error. This intermediate step, known as prompt engineering, is crucial for optimizing the performance of LLMs.\nRecent advancements in prompt engineering have introduced various techniques to enhance the effectiveness of prompts. One notable example is zero-shot chain-of-thought prompting (Kojima et al., 2022), where simply adding the phrase \"Let's think step-by-step\" can stimulate the LLMs' reasoning capabilities, encouraging it to think aloud and process the query in a logical sequence. However, this seemingly magical and straightforward phrase is hard to come up with, as current LLMs are very sensitive to phrasing prompts. Semantically similar prompts can lead to significant performance variations (Kojima et al., 2022; Zhou et al., 2023; Salinas & Morstatter, 2024), with minor modifications resulting in a performance drop. This variation requires numerous experiments to find the optimal prompt, resulting in a labor-intensive and time-consuming prompt engineering process.\nPrior works in literature (Yang et al., 2024; Guo et al., 2024; Wang et al., 2024; Zhou et al., 2023; Sun et al., 2023) have addressed these limitations by automatically optimizing prompts. For instance, AutoHint (Sun et al., 2023) proposed learning from wrong samples by using LLMs to generate hints for selected incorrect samples, which are used to refine prompts. However, learning from only incorrect samples can make the prompts too specific to the wrong samples, losing an understanding of what worked. Another approach by OPRO (Yang et al., 2024) involves using LLMs as optimizers, where the model generates new prompts iteratively based on a ranking list of previous prompts and their corresponding scores. However, OPRO lacks the incorporation of feedback from incorrect samples, potentially limiting its ability to achieve optimal performance.\nWhile prompt optimization has prior art, an unexplored and significant challenge in prompt engineering is prompt adaptation. As LLMs are continually updated and more capable LLMs are introduced, existing prompts often need to be rewritten and tailored to align with the new model version or an entirely new model. This constant adaptation is necessary to maintain the effectiveness of the prompts to ensure they produce high-quality results. Additionally, prompt adaptation across various languages is crucial for ensuring performance in multilingual contexts. However, this area remains unexplored in the literature.\nTo address these gaps, we propose Learning from Contrastive Prompts (LCP), an automatic prompt optimization and adaptation framework. In particular, our framework consists of two stages: prompt candidate generation and new prompt generation. We inject diverse prompts into prompt optimization by generating multiple prompt candidates to explore the prompt space. To overcome the shortcomings of existing methods, we take an inspiration from the principle of contrastive learning (Chen et al., 2020) by allowing the LLM to contrast between good and bad prompts from the generated prompt candidates while learning to improve on error cases. This helps the LLM reason on the prompts that work versus those that do not, using exploration, to incorporate good prompts without being too specific to the error cases.\nWe demonstrate the effectiveness of our approach for both the scenarios of prompt optimization and prompt adaptation. We evaluate our framework on the Big-Bench Hard dataset (Suzgun et al., 2022), which comprises diverse tasks considered challenging even for human evaluators. Our framework achieves a win rate of over 76% versus OPRO (Yang et al., 2024) and AutoHint (Sun et al., 2023) on prompt optimization. It especially excels at algorithmic and multi-step arithmetic reasoning tasks.\nOur prompt adaptation framework leverages feedback from the target model to enhance performance on the prompts from the source model. It achieves comparable or better results than prompt optimization from scratch on the target model when the target model is a weaker model. Our results show that prompt adaptation is a delicate balance between the target model's abilities and the source model's abilities. It can slightly degrade performance on tasks where the source model excels, while improving performance on tasks where the target model is stronger. This observation holds true across model versions and families, with our framework creating a balance between the strengths of the source and target models. Results on the XCOPA dataset further demonstrate our framework's capability to adapt prompts across languages with a better performance on 7 out of 11 languages versus prompt refinement baselines, especially for low resource languages like Swahilli and Southern Quechua.\nIn summary, we present a novel framework using contrastive learning for prompt optimization and an unexplored problem of prompt adaptation. Our results show promising results on both the prompt optimization and prompt adaptation across model versions, families, and languages."}, {"title": "Methodology", "content": "Our proposed framework is illustrated in Figure 1. Both prompt optimization and prompt adaptation utilize the same framework with minor modifications. The framework is designed to enhance the effectiveness of prompts across various scenarios, including adapting to different model versions, model families, and languages. In this section, we will explain the processes of prompt optimization and prompt adaptation separately, detailing the specific stages and mechanisms involved in each."}, {"title": "Motivation", "content": "In line with recent advancements, our work harnesses the reasoning capabilities of LLMs to automatically optimize prompts. However, a significant challenge persists in effectively instructing LLMs to maximize their potential for generating high-quality prompts.\nOur approach emulates how humans learn: by understanding failures and their reasons, as well as contrasting good and bad examples to grasp what works and what does not. We provide LLMs with error case feedback and a spectrum of prompt quality. We expose them to failures, their reasons, and ask them to contrast between good and bad prompts. This enables learning from diverse perspectives for an improved prompt generation. This unexplored avenue holds significant potential for gaining a comprehensive understanding and unlocking LLMs' full capabilities in generating high-quality prompts.\nThe concept of contrasting high-quality prompts with low-quality ones draws parallels to the principles of contrastive learning, which has gained significant traction across various domains (Chen et al., 2020). Contrastive learning aims to learn meaningful representations by maximizing the similarity between positive pairs (analogous to high-quality prompts) while minimizing the similarity between negative pairs (analogous to low-quality prompts). By leveraging contrastive learning techniques in our context, we can potentially guide LLMs to capture the essential characteristics of effective prompts. In particular, given a list of prompts with their corresponding quality scores, we compare a batch of high-quality prompts to a batch of low-quality prompts, drawing conclusions about the patterns that characterize effective prompts."}, {"title": "Prompt Optimization", "content": "Prompt optimization improves the performance of prompts starting with a simple initial prompt and iteratively refining it to enhance the performance of the LLM on the tasks at hand. We present our two main components next: Prompt Candidate Generation to generate candidate prompts and New Prompt Generation to generate a final prompt using the insights from the candidate prompts."}, {"title": "Prompt Candidate Generation", "content": "We start with a small training set and an initial prompt, which is appended to each query. Given an input and its expected output, LLMs are capable of understanding how to achieve the expected output. We evaluate the LLM generated outputs on the small training set to identify failure cases. Taking a motivation from AutoHint (Sun et al., 2023), we use LLMs to learn from failures by\nunderstanding the failures, and generating reasons behind the failures. The prompt template in this step is shown in Appendix A.5.\nConsistency and diversity injection. A problem with generating the reason for each wrong sample and crafting a prompt candidate based on it, is that the candidate prompt can be biased towards that sample, making it too specific. To inject some consistency, we select multiple incorrectly predicted samples and summarize the common failure reasons (See Appendix A.5). AutoHint also summarized feedback on incorrect samples, but they directly used these summaries as new prompts, which led to their performance being significantly influenced by the selected samples. This presents a dilemma; if we select samples which are quite similar, this could lead to model over-fitting on these samples, trapping the generated prompt in a local minimum. To address this, we add diversity to the generated summary by setting a more creative temperature and repeating this step multiple times to generate N prompts, referred to as prompt candidates. This approach helps the model to explore the prompt space. We use N = 10 based on our experiments.\nHistory integration. The generated prompts from previous iterations can also influence the optimization process, leading to a better performance. Therefore, we integrate these prompts into the prompt candidates, ensuring that the accumulated knowledge from past iterations contributes to the ongoing optimization process."}, {"title": "New Prompt Generation", "content": "Now that we have N prompt candidates, our goal is to generate a new prompt using them. First, we assign a score to each candidate based on its inference performance on the training set. We then rank all the candidates according to their scores. Inspired by contrastive learning, we instruct the LLM to identify the underlying patterns that distinguish good prompts from bad prompts. Specifically, we define the top-K prompts as the good prompts and the bottom-K prompts as the bad prompts, and we use the meta-prompt shown in Appendix A.5 to instruct the LLM to generate a new prompt that follows the underlying pattern of good prompts while improving the performance.\nThis approach simplifies the learning process for LLMs compared to OPRO, which directly learns from a ranked list and also did not provide any error case feedback. Viewing batches of good or bad prompts can also be considered as injecting the consistency. Additionally, since we integrate prompts generated in previous iterations, the differentiation between good and bad prompts becomes more pronounced over time."}, {"title": "Prompt Adaptation", "content": "Prompt adaptation addresses the need to switch between different model versions, model families, or languages, ensuring that optimized prompts remain effective across diverse conditions. While our main framework remains unchanged, we will detail the specific strategies used for adaptation.\nCross-model version. In real-world scenarios, underlying models are being continuously enhanced with new version roll-outs every few months. For example, the GPT family has evolved to include ChatGPT-40, and Meta recently released LLAMA-3. Users can address their issues by switching to\na more advanced model when the previous version lacks the required capabilities. Conversely, they can revert to the less capable model version considering costs.\nOur goal is to refine an optimized prompt for one model version to adapt it effectively to another model version. Integrating feedback from the original model version can provide valuable insights, enhancing the adaptation process for the new model version. To facilitate this, we improve our data sampling strategy (see Figure 2). Instead of randomly selecting incorrectly predicted samples, we focus on samples that are correctly predicted by the original model version but incorrectly predicted by the current model version. This ensures the LLM pays more attention to these critical samples, leading to a more effective adaptation.\nCross-model. Next, we want to take it a step further. Users may want to switch to models from other families that are more accessible or have proven to be more effective for their specific tasks. This setting is more challenging because the underlying models are fine-tuned with different data distributions, tasks, and instructions, resulting in significant variations. In this scenario, we maintain the same data sampling strategy used in the cross-model version, while incorporating error tolerance through wrong format rejection to accommodate less effective models like LLAMA. In particular, if the generated prompt does not adhere to the defined output format, we regenerate it until we reach the maximum allowable number.\nCross-lingual. Adapting prompts across different languages presents unique difficulties due to variations in linguistic structures, vocabularies, and resources. To simplify the process and provide a universal approach, we extend the same strategy to handle prompt adaptation across languages, demonstrating its broader applicability. In particular, we have the LLM translate samples from the target languages into English and then conduct the inference step. We select data samples that are correctly predicted when translated into English but incorrectly predicted in their original languages."}, {"title": "Experiments", "content": "Benchmarks. Our evaluation benchmark is a subset of the Big-Bench Hard dataset (Suzgun et al., 2022), consisting of 17 challenging multi-choice tasks. The tasks are diverse, spanning across various categories like natural language understanding, the use of world knowledge (both general and factual), multilingual knowledge and reasoning, and algorithmic and multi-step arithmetic reasoning, making it a comprehensive test for our framework. We report results for each task category based on the keyword taxonomy provided by Big-Bench Hard dataset\u00b2. For the cross-lingual setting, we use the XCOPA dataset (Ponti et al., 2020), which demonstrates common sense reasoning ability and requires world knowledge understanding.\nModels. For our experiments, we used several state-of-the-art LLMs to evaluate the effectiveness of our framework. These models include: Claude-3-sonnet (Anthropic, 2024) and Claude-3-haiku, LLAMA-3-70b (Dubey et al., 2024). Claude-3-haiku is a smaller and faster model while Claude-3-sonnet is a more powerful model on the leaderboards\u00b3.\nBaselines. We compare our framework with two existing methods: AutoHint (Sun et al., 2023) and OPRO (Yang et al., 2024). AutoHint optimizes prompts based on wrong samples in two iterations, using hint generation and summarization. OPRO optimizes prompts by maintaining a ranking list of historical prompts and relying solely on that. Since these works used LLMs such as the GPT family and the PaLM family, which we don't have access to, we reimplemented their techniques on our target LLMs for a fair comparison.\nPrompt selection strategy. Our framework and OPRO both involve optimizing prompts iteratively, which can lead to performance fluctuations even upon convergence. Additionally, each task from the Big-Bench Hard dataset consists of only 250 samples, making it infeasible to create a validation set. This limitation is consistent with real-world scenarios where data availability is often restricted. We simply use the prompt generated in the last iteration and also present the performance of the prompt with the best training set accuracy during the process."}, {"title": "Results", "content": "Our prompt optimization begins with the initial prompt \"Let's solve the problem.\" in the same fashion as OPRO and AutoHint. All the experiments in this section are conducted using Claude-3-sonnet to ensure better performance. We report the results on last iteration from our method and baselines as well as from the prompt with best performance on the training set. Either choice is in line with previous works (Yang et al. (2024); Sun et al. (2023)) as strategies like a separate validation set, does not provide any benefit owing to being highly correlated with training performance. Also, we did not observe over-fitting with LCP. For further discussion, please refer to Appendix A.1. While we report results from both, given a relatively high variation and a slightly lower performance using the last prompt (47% win rate versus 53% for best training set prompt), we recommend using best prompt on the training set as the selected prompt."}, {"title": "Prompt Adaptation", "content": "Next, we present the results of our experiments on adapting prompts across different model versions, model families, and languages from a source model/language to target model/language.\nCross-model version and family. presents a summary of adapting prompts generated from Claude-3-haiku to the more advanced Claude-3-sonnet, and vice-versa. We also present cross-model family results with Claude-3-sonnet to LLAMA prompt adaptation. Detailed results can be found in Table 9, Table 10, and Table 11 in appendix. We compare LCP adaptation with directly performing the prompt optimization process on the target model from scratch (LCP Optimization on Target) and by using an optimized prompt (last iteration or best prompt on training set) from source model directly without any change.\nThe results show that our adaptation framework effectively leverages feedback from the prior model version (even it is less effective) to enhance performance on the new model version it is typically better (best training prompt) or at par (last prompt) with prompt optimization from scratch on the target model. From the results in task types, one clear observation is how adaptation is a fine balance between the target model generated prompts (LCP Optimization on Target) and source model generated prompts (Source Optimized). For example, in Haiku \u2192 Sonnet setting, LCP Adaptation works better than source generated prompts but worse than target generated prompts for Algorithmic and multi-step Arithmetic reasoning tasks. Situation is completely reversed for Natural Language un-\nderstanding tasks. This shows that prompt adaptation can slightly degrade performance compared to source on the tasks where the source model is relatively stronger while increasing the performance compared to source where the target model is relatively stronger. This observation is repeated even in the cross model setting. Hence, our LCP adaptation framework creates a balance between strengths of source and target models.\nThis observation can be attributed to our framework's ability to effectively leverage the strengths of the source model and transfer this knowledge to the prompts for target model via feedback. Our approach refines and tailors prompts to align with the nuances of the target model, complementing the target model. This is especially beneficial for scenarios where the tasks need target and source model's complementary capabilities making our approach a valuable tool that enables them to improve response quality even with weaker but specialized models, thereby expanding its applicability to a wider range of scenarios.\nCross-lingual. We report the results of our cross-lingual experiments in Table 3. We categorized the methods into two groups: prompt refinement and query translation. While our approach focuses on prompt refinement, we also present the results of query translation to provide additional insights. We compare our method with directly inputting the test query (Blank Prompt), adding an optimized English prompt generated by our prompt optimization method using the COPA dataset (Optimized Prompt), and translating the optimized prompt to the target language. The results indicate that our prompt adaptation approach outperforms the prompt baselines for 7 out of 11 languages.\nFor query translation, we translate the input non-English language test query into English and either use a blank prompt or use the English prompt optimized by our method on the translated training data. Our results show that query translation works better than prompt refinement methods on 7 out of 11 tasks. This is in line with work from Lin et al. (2022), where query translated worked better than human expert prompts in the query language. This is a function of English heavy training of current LLMs. It is important to note that query translation methods come with an additional computational cost, as each query must be translated into English before processing. However, as LLMs continue to improve their performance on non-English languages, we anticipate a narrowing of the gap between prompt refinement and query translation methods. Important to note that on two low resource languages: Swahili (sw) and Southern Quechua (qu), LCP even beats query translation methods. Our study not only presents a comprehensive analysis of cross-lingual performance but also introduces a novel prompt adaptation technique that bridges the gap between the prompt refinement and query translation methods."}, {"title": "Ablation Study", "content": "Diversity Injection with multiple prompt candidates We generate multiple prompt candidates (N = 10) to explore the prompt space which is used for the our contrastive learning framework to identify the patterns between good and bad prompts from these prompt candidates evaluated on the training set. Multiple prompt candidates help us explore the diversity of the accuracy-prompt space, unlike previous methods dependent on single prompts. To explore the effectiveness of this mechanism, we use N = 2, 4, and 6, with top-$\\lceil \\frac{N}{2} \\rceil$ and bottom-$\\lfloor \\frac{N}{2} \\rfloor$ prompts used for contrastive learning. Win rates are shown in Figure 3 and detailed results in Table 8. We clearly see that the win\nrates increase as we increase N. This clearly demonstrates the effectiveness of injecting diversity while generating the prompt using contrastive learning with multiple prompt candidates. Increasing N further, we saw limited benefit and a much higher computation cost, so we use N = 10.\nContrastive Learning One of the key contribution of our work is using contrastive learning to learn from both good and bad prompts, instead of just focusing on top prompts (OPRO) or wrong samples (AutoHint). We show the effectiveness of contrastive learning in our framework by comparing it with a setting providing the LLM with just the N ranked prompt candidates similar to OPRO. Win rates are shown in Figure 3 and detailed results in Table 7. Contrastive learning has a win rate of 76%, especially benefiting the algorithmic and arithmetic tasks, which need more involved instruction writing. These results combined with diversity injection ablation clearly show the benefit of exploring and analyzing the prompt manifold to incorporate it in the prompt optimization.\nCross Optimizers. Based on the results from prompt adaptation, a natural question arises: can we use stronger models to optimize prompts for a weaker model? We aimed to investigate whether employing a more advanced model as the optimizer could further enhance performance. During the prompt optimization of Claude-3-Haiku, we utilize Claude-3-Sonnet to generate new candidate prompts, while still using Claude-3-Haiku for evaluation. We run this on selected four tasks due to cost constraints as shown in Table 4. We observe this approach significantly improves performance due to the capabilities of Claude-3-Sonnet. Claude-3-Sonnet as optimizer more effectively improved best/last prompts by 3.5%/7% on the four selected tasks. These results demonstrate the promising direction of leveraging more advanced models to optimize prompts for weaker models.\nNumber of Training Examples. To provide insights into the number of examples required for our method to maintain effectiveness, we report the performance when using 5, 10, 25, and 50 examples for training, in Figure 4 for three tasks. We notice a trend when we analyzed the training plots. For tasks like reasoning about colored objects whose training accuracy was relatively flat during the iterations, number of examples had little effect, while for tasks like geometric shapes with training curves showing considerable improvement across training iterations, we see a consistent improvement in the performance as number of examples increased. Further, we observe\nthat LCP is relatively more sample efficient, giving a relatively higher performance at lower number of samples versus AutoHint or OPRO. This can be attributed to our multiple candidate generation for contrastive learning that helps model explore diverse prompts to derive insights."}, {"title": "Related Work", "content": "Soft prompt optimization. Recent studies have explored soft prompt-tuning, which involves prepending continuous vectors that are out of vocabulary and serve as prompts for specific tasks (Li & Liang, 2021; Lester et al., 2021; Liu et al., 2022; Qin & Eisner, 2021; Ben-David et al., 2022). These approaches are model-agnostic, but they do not focus on adapting prompts to other models. Some work does leverage soft prompt-tuning for cross-lingual adaptation (Li et al., 2023b; Huang et al., 2022; Zhao et al., 2023; Park et al., 2023). However, these methods often require access to model logits, which can be a constraint in practical applications with the recent proprietary models.\nHard prompt optimization. Hard prompt optimization involves crafting discrete, human-interpretable prompts. Prior works have focused on prompt engineering, iterative refinement, and search-based techniques to improve performance (Guo et al., 2024; Zhou et al., 2023; Pryzant et al., 2023; Wang et al., 2024; Sun et al., 2023; Yang et al., 2024; Wan et al., 2023; Li et al., 2023a; Yuksekgonul et al., 2024). Specifically, APE (Zhou et al., 2023) selects the top instructions with the highest accuracies and then prompts the LLM to generate semantically similar variants for each selected instruction. AutoHint (Sun et al., 2023) generates hints from incorrect samples, summarizes these hints, and uses the summary as the new prompt. OPRO (Yang et al., 2024) generates new prompts by leveraging historical prompts and their corresponding scores, instructing the LLM to create improved versions.\nWhile these works focused on prompt optimization they did not explore prompt adaptability to various model versions, model families, and languages. Our proposed approach bridges this gap by providing a novel comprehensive framework for prompt optimization and adaptation, ensuring effectiveness across different models and linguistic contexts using contrastive learning."}, {"title": "Conclusion", "content": "In this paper, we proposed Learning from Contrastive Prompts (LCP), a comprehensive framework for prompt optimization and adaptation. Our approach addresses the limitations of existing optimization methods and addresses an unexplored but common problem of adapting prompts across different model versions, model families, and languages. It involves a systematic process of prompt candidate generation and new prompt generation through contrastive learning, and feedback for prompt adaptation setting to ensure that the prompts remain effective and relevant in diverse scenarios. We conducted extensive experiments on the Big-Bench Hard dataset, demonstrating that our framework significantly outperforms existing methods.\nOur results showed that our approach maintains high performance when adapting prompts across different model versions complementing the strengths of the source and target models. Additionally, our framework proved robust in cross-lingual scenarios, effectively handling the challenges posed by different linguistic contexts. Our results also show that using a stronger model for prompt optimization and adaptation could significantly boost performance on weaker LLMs instead of prompt adaptation from scratch using our framework.\nOne of the key areas of investigation from our work is exploration and exploiting prompt manifold in a more systematic way. The current prompt optimization methods including ours are unstable over iterations, and its not clear how to navigate the prompt manifold (see Appendix A.2 for more discussion). Some avenues could include a richer feedback mechanism across iterations, as we only rely on feedback from the prompt generated in the preceding iteration or giving a higher weightage to better hints. Further, letting the LLMs explain the feedback and incorporating that reasoning could also be potentially helpful. Prompt adaptation, which can be thought of through the lens of classical domain adaptation can be helped by a more sophisticated feedback design to get best of both the target model and source model, as we see it currently tries to strike the balance between the two. Our cross-optimizer results also show a promising direction and needs further exploration, especially in domains like law or medical where weaker but domain specialized model could guide or be guided in a collaborative fashion by more powerful general LLMs to generate powerful prompts."}, {"title": "Appendix", "content": "A.1 Discussion about Validation Set\nWe did not use a validation set for our experiments following OPRO (Yang et al., 2024) and AutoHint (Sun et al., 2023), and based on our experiments. We show the training and validation accuracy curves when we do setup a validation set aside in Figure 5 on two tasks. We use a split of 33.33% training, 33.33% validation, and 33.33% testing sets to show these results.\nWe observe that there is no inherent bias-variance trade-off between the training/validation accuracies; typically validation accuracy follows training accuracy. We observe a moderate Spearman's correlation between 0.45-0.55 (p-values<0.001), showing that they are quite correlated. Further, we see that our method does not really overfit; training accuracy is lower or similar to validation accuracy unlike overfitting exhibited by OPRO as noted by Yang et al. (2024). Unlike traditional fine-tuning machine learning regime where the training data gets embedded into the model weights, it is quite clear on how to define overfitting in prompt optimization except prompts becoming too specific to the training samples. Since prompt accuracies change significantly iteration-over-iteration, further exploration is needed in this space to devise a way of final prompt selection. To keep it consistent with prior works (Yang et al., 2024; Sun et al., 2023) and to keep things simple in absence of an evidence of over-fitting, we chose to use last iteration prompt and prompt with best accuracy on training set.\nA.2 Prompt Similarity Visualization\nWe visualize the performance over prompt embeddings in Figure 6. Using sentence transformer (Reimers & Gurevych, 2019), we embed the prompts generated over 50 iterations on the snarks task. This three-dimensional histogram plots the distribution of prompts in a two-dimensional embedding space selected using the first two principal components representing a reduced-dimensional representation of the prompt space. The z-axis, represents the performance metric of each prompt. The varying heights of the uniform blue-gray bars illustrate the performance landscape across the embedding space.\nTwo regions appear prominently on this: low performing prompts facing us and high performing prompts facing away from us. There is a notion of a boundary line dividing the two regions. Our analysis of this visualization reveals that semantically similar prompts, represented by nearby points in the embedding space, tend to yield comparable performance results. This is evidenced by clusters of bars with similar heights. However, even slight changes in the prompt, especially for the prompts closer to the boundary, can lead to significant variations in performance, highlighting the sensitivity of optimization methods to prompt formulation.\nIt demonstrates that while semantic similarity often correlates with performance similarity, the relationship is not always straightforward. The complex landscape depicted here emphasizes the challenges and opportunities in prompt optimization with a hard to map prompt-accuracy manifold. Our diversity injection and contrastive learning framework helped explore and guide the prompt opti-\nmization through this space. More work needs to be done to understand how to create methods to navigate this manifold.\nA.3 Number of Contrastive Prompts\nA.4 Number of selected wrong data samples\nAutoHint (Sun et al., 2023) observed that using no more than 3 samples per iteration achieves the best performance, as more samples could confuse the LLM when generating the summary. We also investigate how the performance varies with different numbers of selected wrong samples in Table 6. We do not observe a clear benefit of increasing the number from three. Hence, we use three wrong samples during our experiments in accordance with AutoHint.\nA.5 Meta prompt"}, {"title": "Reason Generation Prompt", "content": "Given input: [INPUT]\nAnd its expected output: [OUTPUT]\nExplain the reason why the input corresponds to the given expected output. The reason should be placed within tag .", "Reason Generation Prompt": "Given input: [INPUT]\nAnd its expected output: [OUTPUT]\nExplain the reason why the input corresponds to the given expected output. The reason should be placed within tag ."}, {"title": "Summarization Prompt", "content": "Given input and expected output pairs, along with the reason for generated outputs, provide a summarized common reason applicable to all cases within tags  and .\nThe summary should explain the underlying principles, logic or methodology governing the relationship between the inputs and corresponding outputs. Avoid mentioning any specific details, numbers, or entities from the individual examples, and aim for a generalized explanation.", "Summarization Prompt": "Given input and expected output pairs, along with the reason for generated outputs, provide a summarized common reason applicable to all cases within tags  and .\nThe summary should explain the underlying principles, logic or methodology governing the relationship between the inputs and corresponding outputs. Avoid mentioning any specific details, numbers, or entities from the individual examples, and aim for a generalized explanation."}, {"title": "High-level Contrastive Prompt", "content": "Given m examples of good prompts and their corresponding scores and m examples of bad prompts and their corresponding scores, explore the unerlying pattern of good prompts, generate a new prompt based on this pattern. Put the new prompt within tag  and .", "High-level Contrastive Prompt": "Given m examples of good prompts and their corresponding scores and m examples of bad prompts and their corresponding scores, explore the unerlying pattern of good prompts, generate a new prompt based on this pattern. Put the new prompt within tag  and ."}, {"title": "Low-level Contrastive Prompts", "content": "Given m prompt pairs and their corresponding scores, explain why one prompt is better than others.\nSummarize these explanation and generate a new prompt accordingly. Put the new prompt within tag  and .", "Low-level Contrastive Prompts": "Given m prompt pairs and their corresponding scores, explain why one prompt is better than others.\nSummarize these explanation and generate a new prompt accordingly. Put the new prompt within tag  and ."}]}