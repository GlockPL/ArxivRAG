{"title": "Sequential Large Language Model-Based Hyper-Parameter Optimization", "authors": ["Kanan Mahammadli"], "abstract": "This study introduces SLLMBO, an innovative framework that leverages Large Language Models (LLMs) for hyperparameter optimization (HPO), incorporating dynamic search space adaptability, enhanced parameter landscape exploitation, and a hybrid, novel LLM-Tree-structured Parzen Estimator (LLM-TPE) sampler. By addressing limitations in recent fully LLM-based methods and traditional Bayesian Optimization (BO), SLLMBO achieves more robust optimization. This comprehensive benchmarking evaluates multiple LLMs\u2014including GPT-3.5-turbo, GPT-40, Claude-Sonnet-3.5, and Gemini-1.5-flash-extending prior work beyond GPT-3.5 and GPT-4 and establishing SLLMBO as the first framework to benchmark a diverse set of LLMs for HPO. By integrating LLMs' established strengths in parameter initialization with the exploitation abilities demonstrated in this study, alongside TPE's exploration capabilities, the LLM-TPE sampler achieves a balanced exploration-exploitation trade-off, reduces API costs, and mitigates premature early stoppings for more effective parameter searches. Across 14 tabular tasks in classification and regression, the LLM-TPE sampler outperformed fully LLM-based methods and achieved superior results over BO methods in 9 tasks. Testing early stopping in budget-constrained scenarios further demonstrated competitive performance, indicating that LLM-based methods generally benefit from extended iterations for optimal results. This work lays the foundation for future research exploring open-source LLMs, reproducibility of LLM results in HPO, and benchmarking SLLMBO on complex datasets, such as image classification, segmentation, and machine translation.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine learning models generally have two sets of parameters: model and hyperparameters [1]. Model parameters are learned during the training, and no human interaction is required. However, hyperparameters need to be defined before training. Therefore, hyperparameter optimization is crucial in machine learning in finding optimal solutions based on some evaluation function [1, 2]."}, {"title": "A. Hyperparameter Optimization Importance", "content": "The early stages of HPO required human experts with domain knowledge for the manual tuning process [3, 4], which is improved by sequential model-based global optimization (SMBO) methods [3]. It has been shown that Bayesian Optimization (BO) is superior to other global optimization"}, {"title": "B. Manual and Bayesian Optimization", "content": "methods and is a state-of-the-art method for hyperparameter tuning [1, 2, 5]. However, it doesn't fully automate the HPO process [2] and has several limitations."}, {"title": "C. Bayesian Optimization Disadvantages:", "content": "To overcome the limitations of Bayesian optimization, various ideas have been proposed, such as transfer-learned-based search spaces to reduce human effort [9, 10]; designing algorithmic adaptive search spaces [11, 12] to solve fixed search space problems; multi-fidelity optimization [5, 13] or early-stopping [1] to speed up sequential learning; warm-starting to initialize parameters based on successful parameter values learned on similar datasets [6, 7, 8, 14]. However, each method suits a specific problem and does not lead to a unified framework."}, {"title": "D. Large Language Models to Optimize Hyperparameters", "content": "Recent research has begun to explore using Large Language Models (LLMs) to replace traditional random initialization or warm-starting and Bayesian Optimization methods in automated hyperparameter tuning [15, 16, 17]. Preliminary studies indicate that LLMs can define the parameter search space through zero-shot learning without human expertise. Moreover, they may provide more effective initializations compared to random approaches. With few-shot learning, LLMs can perform optimization processes conditioned on the chat history of previous iterations, leveraging their internet-scale pretraining."}, {"title": "E. Limitations of Current LLM-based Approaches and Lack of Research", "content": "These studies have been conducted with only OpenAI's models, can only run for 10 or 30 iterations based on the LLM prompting strategy due to the maximum input token limit and does not use adaptive search space methods. Hence, there are several important research questions regarding the usefulness of LLMs in reaching fully-automated HPO have yet to be answered:"}, {"title": "F. Sequential Large Language Model-Based Optimization (SLLMBO).", "content": "To address the identified research gaps, we propose SLLMBO, a novel method that leverages the strengths of LLMs and combines them with Tree-structured Parzen Estimator (TPE) sampling. SLLMBO features an adaptive search space that evolves based on prior iterations and employs a hybrid LLM-TPE Sampler. It warm-starts with LLM and alternates between LLM-based sampling and TPE-based exploration to balance exploration and exploitation. Additionally, early stopping is integrated to terminate the optimization when no improvements are observed, thus reducing unnecessary computational costs."}, {"title": "G. Contributions", "content": "The contributions of this study are summarized as follows:"}, {"title": "II. LITERATURE REVIEW", "content": "Hyperparameter optimization (HPO) is critical to machine learning, as hyperparameters control the training process and significantly affect model performance. Unlike model parameters learned from the data, hyperparameters are predefined and must be set before training begins. Manual tuning is inefficient, error-prone, and often leads to non-reproducible results. This has led to automated HPO techniques, which aim to find optimal hyperparameters with minimal human intervention [1, 3, 4]. The increasing complexity of machine learning models has further motivated research into more sophisticated HPO methods [1, 3]."}, {"title": "A. Classical HPO Methods.", "content": "The earliest methods for HPO, grid search, and random search, are widely used but have notable limitations. Grid search systematically explores all hyperparameter combinations in a predefined range. However, it becomes impractical due to the curse of dimensionality, where the number of evaluations grows exponentially with the number of hyperparameters [1, 2, 4, 5, 18]. Random search [4] improves efficiency by sampling hyperparameters randomly from the search space. It has been shown to outperform grid search in high-dimensional spaces [3, 4, 5]. However, random search is unreliable for complex models, as it lacks structure and fails to leverage prior knowledge gained from previous evaluations [2, 4, 5, 18]."}, {"title": "B. Bayesian Optimization", "content": "Bayesian Optimization (BO) is a more advanced method that addresses the limitations of classical approaches by treating HPO as a black-box optimization problem [19]. BO aims to minimize an unknown objective function $f(x)$, where $f(x)$ represents the hyperparameter configuration. It builds a surrogate model of the objective function with probabilistic modeling based on previously evaluated hyperparameter configurations and uses an acquisition function to decide which configuration to evaluate next [19]. The Bayesian theorem forms the core of BO, and the model is updated using new data as follows:\n$P(\\theta|D) \\propto P(D|\\theta)P(\\theta)$\nThis equation expresses that the posterior distribution $P(\\theta|D)$ (updated belief about the hyperparameters given the data) is proportional to the product of the likelihood $P(D|\\theta)$ (the probability of observing the data given the hyperparameters) and the prior $P(\\theta)$ (initial belief about the hyperparameters). BO selects the next hyperparameter configuration by balancing exploration (sampling in less-known regions of the search space) and exploitation (sampling in regions known to perform well) using an acquisition function like Expected Improvement (EI), which chooses the next point based on the expected gain [5, 19]. BO's most common surrogate model is the Gaussian Process (GP), which models the objective function as a distribution over functions. A GP provides a mean and variance for each point in the search space, offering uncertainty estimates exploited by the acquisition function to guide the search [5].\nDespite its popularity, GPs have several limitations. Their computational complexity of fitting variances is $O(n^3)$, making them inefficient for large datasets. GPs also struggle with discrete and conditional hyperparameters, which limits their applicability in real-world problems [1, 5]. To overcome the limitations of GPs, alternative surrogate models like the Tree-Structured Parzen Estimator (TPE) [3] and Sequential Model-based Algorithm Configuration (SMAC) [20] have been developed. TPE models the objective function by dividing the search space into two distributions with a predefined threshold $y^*$: a subspace containing observations whose objective value is smaller than the threshold and a second subspace with a higher than or equal to the threshold [3]:\n$P(x|y) = \\begin{cases} l(x) & \\text{if } y < y^* \\\\ g(x) & \\text{if } y \\geq y^* \\end{cases}$\nOptimizing the expected improvement of the acquisition function for the TPE is the same as optimizing the following ratio: $g(x)/l(x)$. SMAC extends BO to more complex hyperparameter spaces using Random Forests instead of GPs as the surrogate model [20]. SMAC performs well in discrete and mixed search spaces and scales better than GPs. It also supports conditional hyperparameters, where specific hyperparameters only take effect if others are set to particular values [5]. Both"}, {"title": "III. METHODOLOGY", "content": "The SLLMBO framework in Figure 1 is a fully automated hyperparameter optimization (HPO) method that leverages the capabilities of Large Language Models (LLMs) to guide the entire optimization process. SLLMBO interacts with the user-provided task description, followed by the automated generation of search spaces and parameter suggestions using zero-shot and few-shot learning. At the beginning of the process, the LLM initializes the search space and suggests initial hyperparameters. In subsequent iterations, the LLM dynamically updates the search space or keeps previously suggested ranges and suggests new parameters, informed by the history of prior iterations and the performance of the previously chosen values.\nSLLMBO operates in iterative cycles between the following components: the Initializer, which is responsible for the first set of parameter ranges and values; the Optimizer, which refines parameter suggestions based on previous performance; the Evaluator, which calculates performance metrics through cross-validation; and the History Manager, which stores and summarizes previous interactions to prevent token limit overflows. Additionally, SLLMBO introduces a novel hybrid LLM-TPE Sampler, which dynamically balances exploration and exploitation by alternating between LLM-based and TPE-based sampling. Optional features such as reasoning-enabled optimization and early stopping further enhance the optimization process, making SLLMBO a robust and adaptive solution for complex HPO tasks."}, {"title": "A. Initializer: Zero-Shot Parameter Initialization", "content": "The Initializer is the starting point of the SLLMBO algorithm, where the LLM employs zero-shot learning to define the initial hyperparameter search space and suggest starting values. Based on the user-provided task description, machine learning model, evaluation metric, and optimization direction, the Initializer"}, {"title": "B. Optimizer: Few-Shot Learning for Dynamic Hyperparameter Updates", "content": "Once the initialization is complete, the Optimizer component handles iterative updates of the search space and hyperparameter suggestions using few-shot learning. During each optimization cycle, the LLM is provided feedback from the previous iterations, such as chat history and current best score, best parameter values, and the most recent search space in the optimization prompt. The LLM must decide whether to update the search space or continue with the current one, balancing exploration and exploitation.\nIn fully LLM-powered runs, the LLM suggests new parameters at every iteration. In contrast, in the LLM-TPE hybrid version, the LLM-TPE Sampler alternates between LLM-based suggestions and TPE-based sampling. This allows for a more diverse parameter space exploration, leveraging the LLM's generalization capabilities and TPE's statistical strengths."}, {"title": "C. Evaluator: Cross-Validation for Metric Calculation", "content": "The Evaluator is responsible for assessing the performance of each set of hyperparameters through cross-validation. In this study, 5-fold cross-validation was used, and the evaluation metric (e.g., F1-score or MAE) is passed back to the optimizer. This performance score guides the optimization process, with the LLM adjusting its future parameter suggestions based on how well the current set performs."}, {"title": "D. History Manager: Memory Handling and Summarization", "content": "The History Manager ensures that the optimization process maintains continuity across iterations by storing all parameter suggestions, evaluation scores, and decision points. The History module stores this information fully, while the Summarizer condenses the history when input token limits are reached. In the experiments, two summarization methods were used:"}, {"title": "E. LLM-TPE Sampler: Hybrid Sampling Strategy", "content": "The LLM-TPE Sampler is a hybrid approach alternating between LLM-guided parameter suggestions and TPE-based sampling to balance exploration and exploitation. This sampler uses the following function:\n$S(X) = \\begin{cases} LLM(X_{chat\\_history}) & \\text{if } p < 0.5 \\\\ TPE(X_{(prev\\_iters,search\\_space)}) & \\text{otherwise} \\end{cases}$\nHere, $p\\sim U(0,1)$. This means that half the iterations are guided by the LLM, using previous chat history and performance results, while the other half rely on TPE to explore regions of the search space that the LLM may not fully exploit. This dynamic approach ensures that SLLMBO avoids getting stuck in local optima and can effectively explore uncertain areas of the search space. The LLM-TPE sampler is designed as an alternative sampler for the Optuna library."}, {"title": "F. Reasoning-Enabled Optimization", "content": "To enhance transparency and interpretability, SLLMBO offers an optional Reasoning Engine. This feature requires the LLM to explain each decision made during the optimization process-whether to update or retain the current search space and why specific parameters were chosen. These explanations are stored alongside the optimization history, allowing for post-hoc analysis of the optimization decisions."}, {"title": "G. Early Stopping Mechanism", "content": "SLLMBO also incorporates an early stopping mechanism to prevent unnecessary iterations and reduce computational costs. The optimization process halts if the evaluation metric does not improve for a predefined number of iterations (five in our experiments). This mechanism is applied to the fully LLM-powered runs and the hybrid LLM-TPE method, ensuring efficiency and preventing overfitting in cases where further iterations are unlikely to yield better results."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "The experiments were carried out sequentially to explore different strategies for hyperparameter optimization using LLM-powered methods, LLM-TPE hybrid samplers, and traditional Optuna/Hyperopt approaches. Each successive experiment was designed based on the findings of the previous one, which led to further refinements and informed decisions on the next phase of experimentation. This section details each process step's key findings, reasoning, and outcomes, followed by a discussion of the results and answers to the research questions posed. To compare SLLMBO results with different strategies with each other and with Optuna and Hyperopt libraries using TPE as a surrogate model, each method is experimented on seven datasets, with both LightGBM and XGBoost models, making a total of 14 tasks. For each dataset, approximately 20% of the data is kept testing the performance of the best parameters found by some HPO method on the rest of the data, helping to explore overfitting. All the experimentation results are given in Appendix C, and optimization history plots are in Appendix D."}, {"title": "A. Initial Exploration: SLLMBO with Fully LLM-powered Methods and Intelligent Summary", "content": "The initial experiments utilized the GPT-3.5-turbo model for initialization, optimization, and iterative parameter summarization of the optimization history. The goal was to assess the effectiveness of LLM in HPO compared to established methods such as Optuna and Hyperopt.\nRegarding parameter initialization, the LLM-powered approach consistently outperformed traditional random initialization in 9 out of 14 tasks. Notably, in 6 out of 8 regression tasks, the initial parameters suggested by the LLM yielded scores closer to the best score than those suggested by Optuna and Hyperopt. This demonstrates the LLM's superior capability for parameter initialization, especially in regression tasks.\nWhen evaluating the cross-validation (CV) and test scores of the best parameters found through optimization, the LLM-based HPO method also performed competitively. Across multiple tasks, it provided better or comparable results to both Optuna and Hyperopt. Specifically, the LLM surpassed Hyperopt in 5 tasks while achieving competitive results with Optuna in several tasks and two tasks better than Optuna and Hyperopt. In the energy dataset with XGBoost, the LLM-based approach produced a 17.5% lower MAE on the test set than Optuna despite Optuna finding a slightly better cross-validation score. In contrast, in the energy dataset with LightGBM, the LLM-based method resulted in a 25% higher test error (MAE) than Optuna. All optimization methods exhibited overfitting in the bike_sharing and energy datasets, spanning four tasks. However, the LLM-based approach for the energy dataset with XGBoost showed noticeably softer overfitting than the other"}, {"title": "B. SLLMBO with Fully LLM-Powered Methods and LangChain", "content": "With the transition to the LangChain framework, the focus shifted to analyzing whether the improved technical setup could positively impact the overall optimization process beyond addressing memory and API-related issues. The experiments in this phase sought to investigate the performance of various LLM models-GPT-3.5-turbo and GPT-40, Gemini-1.5-flash, and Claude-Sonnet-3.5-within LangChain's more stable environment and to analyze whether these models could further improve the optimization outcomes.\nRegarding parameter initialization, the results remained consistent with earlier findings, where LLM models, especially GPT-3.5-turbo and Sonnet-3.5, outperformed traditional random initialization strategies from Optuna and Hyperopt. All LLM models demonstrated superior initialization compared to the traditional methods in several tasks, further reinforcing the LLM's ability to suggest effective starting points for optimization, particularly in regression tasks.\nWhen evaluating the overall HPO performance, LangChain-based LLMs provided significant improvements. While the earlier LLM-based method only surpassed Optuna and Hyperopt in two tasks, the LangChain-based strategies outperformed them in eight tasks, with GPT-3.5-turbo leading in half of these instances. This method demonstrated consistently strong results across different datasets, further solidifying its position as a reliable model for HPO. GPT-40 followed closely, though its average performance was slightly below GPT-3.5-turbo. Notably, Sonnet-3.5 delivered competitive results in classification tasks but proved the most expensive API usage, making it less efficient in computational cost. Gemini-1.5-flash, while the fastest and cheapest model, ranked last in performance, with premature stopping in most cases after early promising results.\nAnalyzing the optimization histories reveals that LangChain-based LLMs benefitted from longer runs, improving the balance between exploration and exploitation, particularly in early iterations. GPT-3.5-turbo, in contrast to its earlier runs with LLM summarization, now consistently reached 50 iterations in nearly half of the tasks. Although early-stopping issues due to no improvement persisted across all models, this occurred less frequently in GPT-3.5-turbo and GPT-40 compared to the other models. Gemini-1.5-flash frequently exhibited a pattern of initial solid performance, only to plateau after a few iterations, resulting in premature stopping and consistently lower rankings. Sonnet-3.5, while competitive in some tasks, faced similar issues with overexploitation, where initial gains were not sustained throughout the optimization process."}, {"title": "C. SLLMBO with LLM-TPE Sampler", "content": "Following the improvements made with LangChain-based LLM methods, this phase addresses the remaining overexploitation issues by integrating the LLM's initialization and reasoning capabilities with the Tree-structured Parzen Estimator (TPE) for better sampling. The LLM-TPE hybrid sampler aims to reduce premature early stoppings and explore parameter spaces more thoroughly.\nTo achieve this, GPT-40 and Gemini-1.5-flash were selected. Gemini was the fastest and cheapest model, but it had premature stoppings, so it is included to assess whether the hybrid strategy could improve its performance. GPT-3.5-turbo was excluded due to its consistent success in previous phases, and Sonnet was omitted due to high costs.\nTo further assess the effectiveness of LLM-based initialization in hyperparameter optimization (HPO), the GPT-40-based LLM-TPE sampler was tested with random and LLM initialization strategies. The results reaffirmed the strength of LLM-based initialization, as in 13 out of the 14 tasks, GPT-40's LLM initialization outperformed random initialization in providing a better starting point. However, a more detailed analysis of the final optimized parameters after 50 iterations revealed a notable distinction between regression and classification tasks. In regression tasks, best parameters starting with random initialization consistently resulted in better test scores, whereas in nearly all classification tasks, starting with GPT-40 initialization led to superior results.\nRegarding overall HPO performance, the GPT-40 LLM-TPE sampler consistently outperformed the previous GPT-40 fully LLM sampler approach in 9 of the 14 tasks, delivering competitive scores in the remaining tasks. Gemini-1.5-flash, which previously exhibited poor performance and frequent early stoppings, showed significant improvement with the LLM-TPE sampler. The Gemini-based LLM-TPE method outperformed its fully LLM-powered counterpart in 12 tasks, rising from last place in many previous evaluations to a more competitive ranking. Compared to Optuna, Hyperopt, and the fully LLM sampler with GPT-40 and Gemini-1.5-flash, the LLM-TPE sampler demonstrated superior performance, with GPT-40 consistently ranking in the top one or two positions, followed closely by Gemini-1.5-flash.\nThe success of the LLM-TPE sampler can be attributed to its ability to maintain a balanced exploration-exploitation trade-"}, {"title": "V. CONCLUSION", "content": "This research explored the potential of large language models (LLMs) for hyperparameter optimization by comparing various models-GPT-3.5-turbo, GPT-40, Claude-Sonnet-3.5, and Gemini-1.5-flash with traditional Bayesian Optimization methods such as Optuna and Hyperopt. The study comprehensively analyzed LLMs' performance in parameter initialization and optimization, adaptive search spaces, and the management of exploration-exploitation trade-offs. Two key contributions emerged: a novel hybrid LLM-TPE sampler for improved exploration-exploitation balance and an adaptive search space mechanism driven by LLMs to automatically evolve the parameter space. Early stopping criteria were also integrated to reduce computational costs.\nInitial experiments with fully LLM-powered methods demonstrated that LLMs consistently outperformed random initialization, particularly in regression tasks, and offered competitive performance in hyperparameter optimization"}]}