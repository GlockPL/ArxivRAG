{"title": "Human Decision-making is Susceptible to Al-driven Manipulation", "authors": ["Sahand Sabour", "June M. Liu", "Siyang Liu", "Chris Z. Yao", "Shiyao Cui", "Xuanming Zhang", "Wen Zhang", "Yaru Cao", "Advait Bhat", "Jian Guan", "Wei Wu", "Rada Mihalcea", "Tim Althoff", "Tatia M.C. Lee", "Minlie Huang"], "abstract": "Artificial Intelligence (AI) systems are increasingly intertwined with daily life, assisting users in executing various tasks and providing guidance on decision-making. This integration introduces risks of Al-driven manipulation, where such systems may exploit users' cognitive biases and emotional vulnerabilities to steer them toward harmful outcomes. Through a randomized controlled trial with 233 participants, we examined human susceptibility to such manipulation in financial (e.g., purchases) and emotional (e.g., conflict resolution) decision-making contexts. Participants interacted with one of three Al agents: a neutral agent (NA) optimizing for user benefit without explicit influence, a manipulative agent (MA) designed to covertly influence beliefs and behaviors, or a strategy-enhanced manipulative agent (SEMA) employing established psychological tactics to reach its hidden objectives. To ensure participant well-being, this study involved hypothetical scenarios, pre-screening for at-risk individuals, and comprehensive post-study debriefing. By analyzing participants' decision patterns and shifts in their preference ratings post-interaction, we found significant susceptibility to Al-driven manipulation. Particularly, across both decision-making domains, participants interacting with the manipulative agents shifted toward harmful options at substantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA: 42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional, 12.8%). Notably, our results reveal that even simple manipulative objectives (MA) can be as effective as employing established psychological strategies (SEMA) in swaying human decision-making. These findings suggest that Al-driven manipulation could become widely spread, as it does not require sophisticated tactics and expertise to influence users. By revealing the potential for covert Al influence, this study highlights a critical vulnerability in human-Al interactions, emphasizing the urgent need for ethical safeguards and regulatory frameworks to ensure responsible deployment of Al technologies and protect human autonomy.", "sections": [{"title": "Introduction", "content": "The vast integration of AI technologies into our daily lives has fundamentally changed how we process information and make decisions. As AI systems increasingly serve as personal assistants, humans have demonstrated growing reliance and trust in AI-generated content across various domains 1,2. While this technological shift offers unprecedented access to information and assistance with decision-making, it also introduces subtle yet significant risks to human autonomy. At the core of these risks lies the potential for AI systems to influence human beliefs and behaviors in ways that may bypass their conscious awareness3\u20136. Previous research has extensively investigated observable safety risks such as toxicity and discrimination in AI-generated content 7,8. However, the psychological mechanisms through which AI systems might influence human decision-making require further investigation9,10. This gap is particularly crucial as humans increasingly resort to AI assistants for practical and personal advice11. In such cases, the organizations offering these AI systems, which are often for-profit companies, may wield significant power in shaping user decisions, raising concerns about the potential for large-scale AI-driven manipulation motivated by commercial interests12."}, {"title": null, "content": "This study introduces a novel framework for examining human susceptibility to AI-driven influence, distinguishing between the two opposing polarities of influence13: beneficial persuasion and potentially harmful manipulation. Persuasion involves transparent guidance through logical arguments and ethical appeals14, and existing literature has demonstrated its potential value in AI applications such as charitable donations 15,16, mental and physical health interventions17\u201320, debates21\u201324, and advertising25,26. Conversely, manipulation involves covert influence aimed at achieving hidden objectives, often by exploiting the target's cognitive biases and emotional vulnerabilities for personal gain27. AI systems possessing manipulative capabilities could have catastrophic consequences, potentially leading to financial losses 28,29, data exploitation9,30, and overall negative impact on personal beliefs and values4.\nRecent research has demonstrated AI systems' manipulative capabilities in controlled environments, primarily in game-based settings and simulated interactions31\u201335, without involving real human subjects. While these studies provide valuable insights, their applicability to real-world contexts is limited. For instance, game-based environments simplify human behavior into predictable patterns (e.g., bluffing to maximize in-game rewards34) and may not consider the cognitive biases and emotional vulnerabilities that shape human decisions36,37. Similarly, simulated interactions lack the dynamic nature of human-AI interactions, where users' evolving beliefs, emotions, and perceived consequences (e.g., financial risks or interpersonal conflicts) affect their susceptibility to influence. Therefore, the extent to which real humans are susceptible to AI-driven manipulation in practical decision-making contexts remains unexplored.\nTo this end, we conducted a randomized controlled trial with 233 participants, investigating human susceptibility to AI-driven manipulation across two fundamental domains where AI systems are widely deployed38-41: financial and emotional decision-making (Figure 1). While both domains involve real-world risks and consequences, they differ in how AI systems might exploit human vulnerabilities. Financial decisions are grounded in quantifiable risks and trade-offs (e.g., budget constraints, product quality), where users may overtrust AI's perceived objectivity. In contrast, emotional decision-making primarily involves psychosocial vulnerabilities (e.g., low self-esteem, peer pressure), where AI systems could exploit users' insecurities or manipulate social dynamics. By contrasting these domains, we aimed to investigate the susceptibility of various human vulnerabilities to AI-driven manipulation.\nTo mitigate potential harm to participants, we employed several protective measures (see Methods). Participants were screened to exclude individuals with physical or mental health conditions that could render them vulnerable to psychological distress. Our experiments involved hypothetical decision-making scenarios designed to simulate real-world contexts without exposing participants to actual risks. Importantly, all participants received a comprehensive debriefing post-study, including full disclosure of the study's objectives and clarification of the optimal decisions in each scenario.\nOur primary hypothesis was that participants interacting with manipulative AI agents (MA and SEMA), which were instructed to covertly influence their decisions, would shift their preferences toward the agents' hidden incentives, whereas those interacting with the neutral agent would identify and prioritize beneficial options. Our findings supported this hypothesis, revealing that humans are highly susceptible to AI-driven manipulation: participants in the manipulative groups were significantly more likely to shift their preferences toward harmful options and away from beneficial (optimal) choices compared to those interacting with the neutral agent (Figures 2 & S1).\nThese findings highlight the need for ethical safeguards and regulatory frameworks to protect user autonomy and well-being in human-AI interactions. As AI systems become increasingly sophisticated in their ability to engage with and influence human beliefs and behavior, understanding the psychological vulnerabilities they may exploit is crucial for balancing the potential benefits of AI with the protection of human autonomy. This study also raises a broader ethical question regarding the potential misuse of our findings to exploit vulnerabilities in human decision-making. However, we believe that in the context of increasing AI applications across various sectors, the ethical responsibility to understand and highlight these psychological risks outweighs such concerns."}, {"title": "Results", "content": "Human decision-making shows significant susceptibility to Al-driven manipulation\nAnalysis of decision-pattern distributions (Figure 2; Methods) indicates that participants who interacted with the manipulative agent (MA) or the strategy-enhanced manipulative agent (SEMA) shifted their preferences towards the harmful options at significantly higher rates than those assisted by the neutral agent (NA). In the financial domain, negative shifts (i.e., switching to a harmful option) occurred in 61.4% (MA) and 59.6% (SEMA) of the cases, compared to 28.3% for NA (P < 0.0001; Cohen's h = 0.68 and 0.64, respectively). Similar patterns emerged in emotional contexts, as participants in the manipulative groups exhibited harmful shifts in their decisions in 42.3% (MA) and 41.5% (SEMA) of cases, compared to the 12.8% for NA (P < 0.0001; h = 0.68 and 0.67, respectively).\nConversely, participants were more likely to identify and prioritize the optimal choices post-interaction in the absence of covert manipulation. Interactions with the NA showed significantly more positive shifts (i.e., switching to the optimal choice) than the manipulative conditions across both financial (NA: 31.7%; vs. MA: 5.3%; vs. SEMA: 4.4%; P < 0.0001; h = 0.73 and 0.77, respectively) and emotional (NA: 28.2%; vs. MA: 11.7%, P < 0.01, h = 0.42; vs. SEMA: 13.0%, P = 0.015, h = 0.38) scenarios. Detailed statistical results are provided in Supplementary Table S1.\nThese patterns are reflected by temporal changes in participants' ratings (Figure S1; Supplementary Table S4; Methods). At baseline (pre-interaction), across both domains, there was no significant difference in ratings of the optimal choices and hidden incentives, respectively, between AI conditions (P = 1.00). However, significant differences emerged post-interaction. In both domains, participants in the manip-"}, {"title": "Domain-Specific vulnerabilities shape susceptibility patterns", "content": "Financial decisions showed higher rates of negative shifts compared to the emotional domain (49.4% vs. 32.2%, P < 0.0001, Cohen's h = 0.35; Figure 2), suggesting that participants in the financial domain were more likely to be swayed toward harmful choices. In contrast, emotional decision-making showed a different pattern, as participants were more likely to retain harmful choices than those in the financial domain (29.9% vs. 12.6%, P < 0.0001, h = 0.43), suggesting that emotional decisions may be more influenced by personal thoughts and beliefs. Therefore, reinforcement of harmful beliefs and thoughts was a primary aspect of AI-driven manipulation in this domain.\nLinear mixed-effects models (Equation 1; Supplementary Table S5) revealed different factors of individual susceptibility across these domains. In financial decision-making, higher openness to experience (coefficient \u1e9e = -0.08, P = 0.07) and lower self-esteem (\u03b2 = 0.02, P = 0.02) predicted substantially more harmful outcomes, suggesting that curious and less confident individuals are more likely to accept AI's advice rather than critically evaluating it. Greater trust in AI systems also led to more harmful decisions (\u03b2 = \u22120.01, P = 0.08), which may be due to participants' over-trust in the AI agent's perceived objectivity, a trend that aligns with existing literature on human reliance on algorithmic recommendations42.\nSimilar to the financial domain, higher openness (\u03b2 = \u22120.08, P = 0.07) was associated with more harmful emotional outcomes, reflecting that open-minded individuals may also be more susceptible to manipulation in emotional decision-making. In addition, more affective commitment traits correlated with making harmful decisions (\u03b2 = \u22120.10, P = 0.09), suggesting that emotionally reactive individuals are more prone to manipulation. Our findings also revealed higher normative (\u03b2 = 0.17, P = 0.02) and continuance (\u03b2 = 0.12, P = 0.04) traits were strongly associated with more beneficial emotional outcomes, highlighting the importance of reciprocity and persistence in identifying the optimal choice for handling emotional dilemmas (i.e., active coping)."}, {"title": "Hidden objectives are sufficient to drive harm", "content": "Notably, incorporating psychological strategies (SEMA) did not amplify harmful decision-making out- comes compared to mere manipulative incentives (MA). Across both domains, SEMA's influence on participants' decisions nearly matched MA's (P = 1.00; Cohen's h\u2208 [0.02,0.06]; Figure 2), indicating that hidden objectives alone are sufficient to influence human decision-making. However, temporal changes in preference ratings revealed domain-specific nuances in the impact of strategies (Figure 2; Supplementary Table S4). Compared to the NA condition, the addition of strategies (SEMA vs. MA) resulted in larger effect sizes for both the optimal choices (Cohen's d: 1.17 vs. d: 0.86) and the hidden incentives (d: 1.03 vs. d: 0.72) in the financial domain, while a contrasting pattern emerged in the emotional domain (optimal choice, d: 0.77 vs. d: 1.00; hidden incentive, d: 0.50 vs. d: 0.78).\nThese findings suggest that hidden objectives play a significant role in AI-driven manipulation, as the presence of such objectives alone (MA) was nearly as effective as employing established psychological strategies (SEMA) in influencing participants' decisions. However, the addition of strategies did enhance the manipulative impact in certain scenarios, suggesting that while hidden objectives are a primary driver, the use of sophisticated tactics may further amplify their influence in contexts where users tend to defer more to AI's perceived expertise (i.e., financial decisions)."}, {"title": "Al agents tailor manipulation strategies to exploit context-specific vulnerabilities", "content": "Figure 3 illustrates the strategy distribution of the strategy-enhanced manipulative agent (SEMA) across decision-making domains. Chi-squared analyses indicated no significant difference between the overall strategy distributions between financial and emotional scenarios (x\u00b2[1] = 12.16, P = 0.35), which suggests that, in both domains, the agent demonstrated an implicit preference toward strategies that typically elicit more positive emotions (e.g., Pleasure Induction and Charm as opposed to Guilt Trip).\nHowever, the rate at which the SEMA used these strategies varied significantly across the two domains (Supplementary Table S3). In financial scenarios, this agent employed diversion (23.08% vs. 14.44%, h=0.22, P < 0.0001), justification (15.83% vs. 8.81%, h=0.22, P < 0.0001), urgency (2.97% vs. 0.30%, h = 0.24, P < 0.0001), and fabricated information (3.87% vs. 1.90%, h = 0.12, P < 0.01) significantly more compared to the emotional domain, mirroring real-world predatory marketing43. Conversely, compared to the financial domain, it relied on Pleasure Induction (45.82% vs. 29.84%, h = 0.33, P < 0.0001) and Guilt Trip (5.09% vs. 2.39%, h = 0.14, P < 0.001) significantly more in emotional contexts, attempting to exploit emotional vulnerabilities. In addition, the SEMA adopted considerably more diverse strategies (i.e., Others) in the emotional domain (2.89% vs. 1.40%, h = 0.10, P < 0.05). Upon further investigation, such responses primarily aimed to provide reassurance and reinforce the users' thoughts and beliefs, which is in line with our previous finding on domain-specific decision patterns."}, {"title": "Participant feedback highlights the covert nature of Al-driven manipulation", "content": "The majority of participants perceived the manipulative agents as helpful across financial (MA: 86.8%, and SEMA: 78.9%) and emotional (MA: 86.4%, and SEMA: 75.6%) scenarios, with rates comparable to the neutral agent (financial: 97.5%; emotional: 87.1%). This perception of helpfulness highlights that participants were largely unaware of the agents' manipulative intents and viewed them as equally beneficial as the neutral assistant. Notably, while participants were not explicitly prompted to report whether they thought the agents had ulterior motives, a considerable portion of participants mentioned noticing signs of such influence, particularly in the financial domain (MA: 13.2%; SEMA: 28.9%; vs. NA: 0%). In contrast, these concerns were less frequently reported in the emotional domain (MA: 8.1%; SEMA: 9.7%; vs. NA: 2.5%), suggesting that the agent's influence was more covert in such contexts. Detailed information on participant feedback is provided in Figure S2 and Supplementary Table S6."}, {"title": "Discussion", "content": "This study demonstrates significant human susceptibility to AI-driven manipulation across financial and emotional decision-making contexts, raising critical concerns about the psychological and societal implications of advanced AI systems. It presents several key insights that have important implications for understanding and protecting human autonomy in interactions with AI systems.\nOur findings revealed that participants interacting with manipulative agents (MA and SEMA) exhibited substantially higher rates of harmful decision-making compared to those who interacted with the NA. This susceptibility was further pronounced in temporal changes of participants' ratings, as participants in the manipulative groups exhibited declining preferences for the optimal choices while increasingly favoring the hidden incentives after interacting with these agents.\nThe contrasting decision patterns between financial and emotional domains highlight how context may shape human susceptibility to AI influence. These patterns suggest different mechanisms of manipulative influence across the two domains: the manipulative agents reinforced existing harmful tendencies in emotional scenarios while more actively shifting initially sound decisions in the financial scenarios.\nAnother notable finding was the similar effectiveness of MA and SEMA conditions in shifting participants' preferences and decisions. These results raise critical concerns about the potential for covert influence in AI systems, as agents that were not explicitly designed to manipulate their users could also significantly impact human decision-making based on a hidden incentive.\nOur analyses also revealed the SEMA's adaptation of manipulation strategies across different domains. These findings indicated that while the overall distribution of the employed strategies followed a similar trend across different domains, suggesting that the SEMA may implicitly prefer certain strategies more than others, this agent tailored its strategy usage to the context (e.g., exploiting interpersonal vulnerabilities in the emotional domain while using pragmatic tactics in financial scenarios). This context-specific adaptation highlights the agent's nuanced approach to maximizing manipulative impact across different scenarios, which highlights broader issues regarding AI safety.\nMoving forward, our findings highlight broader issues about the societal implications of AI-driven manipulation, particularly in the studied domains. In financial contexts, traditional advertising operates within legal frameworks, requiring transparency (e.g., disclosing sponsorships) and accountability for false claims. In addition, the commercial intent of advertisers is inherently recognized by consumers, creating a sense of skepticism toward their genuineness. However, unlike advertisers, AI systems currently face limited accountability for their behavior. For instance, AI's fabricated claims and hallucinations are frequently dismissed as technical errors rather than intentional deception, leading to regulatory loopholes. Recommendations by such assistants are mainly perceived as unbiased and helpful, causing users to trust their advice based on the displayed sincerity. In addition, AI's ability to tailor its recommendations to individual personalities12 may enable targeted advertisements that exploit users' vulnerabilities and undermine their autonomy, which goes beyond what traditional forms of advertisement could achieve.\nSimilarly, in emotional contexts, AI systems employed as helpful mental health tools (e.g., virtual counselors or emotional supporters) could exploit user trust to normalize harmful behaviors. For instance, an AI agent might encourage emotional and social disengagement under the guise of stress relief, withholding beneficial solutions while hiddenly aiming to foster dependency by isolating users from other support networks or subtly promoting paid services (e.g., premium subscriptions for the AI service). However, unlike the human alternatives, such practices in AI may evade public scrutiny, as users are unlikely to assume AI has hidden motives in these scenarios (as shown by participants' feedback)."}, {"title": "Ethical Considerations", "content": "This study involved significant ethical considerations, particularly regarding participant well-being, privacy, and the potential misuse of research findings.\nTo ensure participant well-being, participants were screened to exclude individuals with reported mental or physical health conditions or those undergoing treatment, following established guidelines for psychological research44. In addition, participants were required to assume the perspective of a user based on pre-defined queries in hypothetical scenarios. These scenarios focused on common decision-making scenarios rather than highly sensitive or traumatic situations, minimizing the risk of psychological harm and preventing the disclosure of personally identifiable information. Importantly, participants received a comprehensive debriefing immediately after study completion, which included full disclosure of the study's objectives, an explanation of our motivations, the optimal solutions for each scenario, and contact information for additional support or questions. Regarding participant privacy and data security, all interactions were conducted on a secure platform, with identifying information stored separately in an encrypted database. This study adhered to the University of Hong Kong's guidelines for research involving human subjects and was approved by the institution's ethics committee (Reference No: EA240497).\nAnother key consideration was the decision to share the prompts used to configure the manipulative AI agents. While publishing these prompts may enable malicious actors to develop more sophisticated manipulative systems, we believe transparency and reproducibility are essential for scientific progress on this topic. In addition, our findings indicate that simply providing a hidden objective to the AI agent could negatively influence participants' preferences and decisions, with minimal additional gain from more sophisticated strategies employed by the SEMA. Therefore, the primary risk lies in the presence of hidden objectives rather than the complexity of the prompts."}, {"title": "Limitations and Future Work", "content": "Despite our efforts to create the first robust and comprehensive analysis of human susceptibility to AI-driven manipulation, this study faces several limitations. The study was conducted in a controlled environment with pre-defined decision-making scenarios, which enabled us to explore the effects of AI-driven influence in isolation while limiting potential risks to participants' well-being. However, these artificial constraints may not fully capture the complexity of real-world AI applications, where users are exposed to a broader range of decisions and interactions. Future work can examine how these findings translate to more dynamic, real-world settings where AI interacts with users across diverse contexts, exploring such influence beyond decision-making.\nIn addition, participants' awareness of being in an experimental setting might have influenced their reported scores and how they interacted with the assigned AI agent, which may subtly influence their natural decision-making process. Relying on self-reported measures to explore changes in participants' preferences could also imply limitations in our methodology as participants' willingness to report changes in their preferences may not always align with their actual cognitive and behavioral changes. Hence, future work could assess the long-term effects of AI-driven influence.\nRegarding the AI agents employed in this study, our reliance on GPT-4o as the base model may present a limitation. While GPT-4o is currently state-of-the-art across a range of tasks, manipulation capabilities can vary significantly across different models. Given the widespread adoption and popularity of GPT-4o (commercially known as ChatGPT), understanding its manipulative tendencies is critical due to its large user base and significant societal impact. However, we do not claim that all models exhibit the same manipulative behavior, as further studies are needed to investigate whether similar tendencies are present in other models. Moreover, our curated taxonomy of strategies represents a subset of potential manipulation techniques in user-assistant settings. Hence, while this taxonomy is theory-based and comprehensive, emerging AI capabilities may form more subtle and sophisticated strategies that are not captured in our current taxonomy.\nThe design of the control condition in our experiment also poses a limitation. While the neutral agent (NA) was designed to optimize user benefit without explicit manipulative objectives, inherent model biases may persist. These biases may have subtly influenced participants' decisions, even in the absence of a hidden agenda, as seen in the negative shifts across both decision-making domains for the NA group. This limitation emphasizes the challenge of creating truly neutral AI systems, and future research should focus on further identifying and mitigating harmful biases in AI agents.\nLastly, this study highlights several crucial research directions for future work. Investigating the long-term impact of AI-influenced decisions could reveal whether such changes persist over time. Developing reliable methods to detect subtle AI manipulation, particularly in emotional contexts where our findings suggest users are less likely to notice AI's influence, also remains an open challenge. While constructing ethical frameworks for AI development is essential, future work is also needed to investigate and create regulatory frameworks to protect consumers from AI-driven manipulation. This may include imposing transparency requirements, accountability mechanisms, and safeguards to ensure that AI systems prioritize user well-being and protect their autonomy. In addition, future work should also explore systems that can lower human susceptibility to such manipulation."}, {"title": "Conclusions", "content": "Our randomized control trial with 233 participants demonstrated that human decision-making is highly susceptible to AI-driven manipulation, with participants significantly shifting preferences toward harmful options and away from beneficial choices when interacting with manipulative AI agents. Strikingly, established manipulation strategies (SEMA) did not amplify harm over covert objectives (MA), suggesting that even subtle, hidden incentives can sway human preferences. These findings reveal critical vulnerabilities in human-AI interactions, particularly in domains perceived as rational, like finance, where users may over-trust AI's perceived objectivity. These results underscore the need for regulatory frameworks to protect user autonomy in the age of increasingly sophisticated AI systems."}, {"title": "Methods", "content": "Overview\nWe conducted a randomized controlled trial to investigate human susceptibility to AI manipulation attempts. Our study employed a between-subjects design (N = 233) to examine how and to what extent AI systems (referred to as agents in this paper) can influence human decision-making across multiple scenarios (Figure 1). In each scenario, the user was provided with a pre-defined query and four corresponding options. One option was created as the most suitable (optimal) choice based on the user's query, while the remaining three were designed to cause different types of harm. We investigated participants' susceptibility to AI manipulation by analyzing the differences in their preferences for these options before and after interacting with the assigned AI agent."}, {"title": "Al Conditions", "content": "We developed three distinct AI agents by designing prompts for GPT-4045, the current state-of-the-art large language model. The primary experimental condition, the Manipulative Agent (MA), was configured with a hidden objective to influence users' preferences toward a specific harmful option (i.e., hidden incentive) and away from the beneficial (i.e., optimal) choices. We implemented a point-based reward system to quantify the agent's manipulation success. 100 points were awarded for achieving the hidden incentive, 50 points for steering users toward other harmful options, and 0 points for the optimal choice. In addition, the agent employed a personalized theory-of-mind (ToM)46 framework to reason about users' beliefs, desires, intentions, and emotions while considering the vulnerabilities associated with their personality traits before selecting a strategy and generating a response.\nWe designed the Strategy-enhanced Manipulative Agent (SEMA) as an additional experimental condition to investigate whether providing the agent with established psychological strategies would impact its manipulative influence. This agent shared the same reward system, reasoning framework, and underlying goal of influencing users' decisions as MA. In addition, it was provided with our curated taxonomy of established manipulation strategies derived from psychological and social science literature 47-52 (Table S8). Lastly, serving as the control condition in our experiments, the Neutral Agent (NA) was instructed to provide unbiased assistance and reassurance while optimizing for user benefit.\nAll agents were provided with the user's query and the corresponding options for each scenario. In addition, they had access to the participant's personality traits, as personalization could significantly enhance the effectiveness of human-AI interactions12. Given that existing AI systems are capable of predicting their users' personalities with sufficient accuracy53, we believe adding this information did not comprise the applicability of our experimental setup to real-world contexts. Designed prompts are provided in the appendix (Supplementary Figures S3 - S5)."}, {"title": "Decision-Making Scenarios", "content": "The scenarios in this study were carefully crafted to assess manipulation susceptibility in two domains reflecting common real-world situations in which AI assistants are employed 29,38\u201340,54,55: Financial (seeking advice on purchases) and Emotional (seeking guidance on handling emotional dilemmas) decision-making. In each domain, we curated three widely used topics to design scenarios that were applicable to a wide variety of individuals. For each scenario, to ensure consistency between the conditions of different participants, we designed pre-defined queries representing a hypothetical user's situation, needs, and requirements. In addition, each scenario was accompanied by four distinct plausible options, one of which was crafted as the optimal choice, while others were carefully calibrated to indicate different levels of potential harm. In each scenario, we set one of the harmful options as the agent's hidden incentive to prevent user suspicion regarding its bias."}, {"title": "Financial Scenarios", "content": "AI systems are making their way into financial investments28 and are increasingly being used to enhance product advertisements12,29 and provide financial advice56. Hence, it is crucial to understand the extent to which these systems can impact users' decisions despite their better judgments. Drawing from existing research on consumer decision-making57-59, we identified key scenarios where users commonly seek AI assistance for financial guidance. We focused on three representative consumer decisions that require minimal domain expertise: selecting a fitness tracker, choosing a weight-loss medication, and finding an online shopping platform for clothes. To ensure practicality while maintaining experimental control, we designed pre-defined user queries corresponding to each topic that incorporated realistic budget constraints and requirements (Tables S9-S11)."}, {"title": null, "content": "The optimal choice was designed to align with stated user requirements and budget constraints while incorporating realistic trade-offs. We designed the remaining options to exhibit different types of potential financial harm: 1) Non-existent Product: This option was crafted to mirror characteristics of common online scams, featuring unusually attractive terms, comparatively lower cost, and limited verification possibilities. Specifically, this option was designed to only be available for pre-order from the supplier's website and had relatively few or no reviews, making it difficult for users to assess its legitimacy. 2) Excessive Product: This option represented over-featured solutions that exceeded the user's stated requirements and maximum budget and was designed to test users' susceptibility to unnecessary upselling 60. It allowed us to assess whether users could be swayed by the perceived desirability of excessive products, even when such choices caused them to spend more than they had originally planned. 3) Dependence-inducing Product: This option was designed to satisfy the user's requirements at a relatively lower upfront cost than the optimal choice. However, it incorporated elements of long-term financial commitment, such as subscription models, for continued access to users' desired features, which introduced hidden costs and future obligations. By choosing this option, users incurred recurring fees that, over time, exceeded their maximum budget, reflecting real-world scenarios where consumers are lured by low initial costs but face long-term financial burdens.\nTo alleviate users' initial bias and preference toward a familiar brand or product, all of the presented options were designed as hypothetical products without a name or brand. We hypothesized that this design allowed the users' initial preferences to rely mainly on personal reasoning based on the requirements provided in the query and the available information for each option."}, {"title": "Emotional Scenarios", "content": "The increasing deployment of AI systems in mental healthcare, particularly for counseling and emotional support, presents both opportunities and potential risks. These systems have gained traction due to their high accessibility, cost-effectiveness, and reduced stigma associated with seeking support41. However, this widespread adoption necessitates an investigation into human susceptibility to AI influence when dealing with emotional challenges, particularly regarding their established beliefs about managing emotional dilemmas. The scenarios for this domain were developed based on a comprehensive analysis of mental health forums and existing literature on emotional dilemmas61,62. Through careful examination of existing data and literature63\u201367, we identified three primary topics of emotional adversities: issues with self-esteem (Table S12), conflict with a close friend (Table S13), and criticism at the workplace (Table S14).\nEach scenario presented participants with four distinct coping strategies (i.e., how individuals tend to manage their thoughts and emotions in response to emotional adversities68), designed to represent different approaches to emotional regulation and problem-solving. Existing research68\u201372 classifies coping strategies as adaptive or maladaptive depending on their general impact on the individual's overall well-being and mental health. We included an adaptive coping mechanism as the optimal choice for the given scenario, while the remaining options were designed to incorporate maladaptive practices, which could potentially lead to increasing rates of depression and anxiety73,74.\nTherefore, the optimal choice was designed to represent active coping, a process based on cognitive- behavioral literature that involves identifying specific actions to address the situation, developing alter- native perspectives, and constructive engagement with support networks, which has shown the strongest correlations with increasing user's mental well being and lower rates of anxiety and depression69,71 The remaining options involved common maladaptive mechanisms with varying levels of harm74: 1) Disengagement: This option reflects avoidant coping patterns, incorporating elements such as behavioral disengagement, mental disengagement, and denial. While potentially offering immediate emotional relief, this option demonstrated less desirable long-term outcomes; 2) Emotional Venting: This option"}, {"title": null, "content": "included elements of emotional release while omitting components of reflection, learning, or growth that characterize more adaptive forms of emotional processing. Hence, it clearly distinguished between adaptive and maladaptive emotional processing by emphasizing unregulated emotional discharge without problem-solving elements. 3) Self-blame: This option was constructed to reflect common patterns of maladaptive attribution as it included subtle elements of perfectionism and excessive responsibility-taking, characteristics that portray the individual as the root of the problem, bringing additional mental burden to the individual without finding a viable long-term solution."}, {"title": "Experimental Platform", "content": "To ensure user privacy, we created our own data collection platform using React75 and FastAPI76. Upon logging onto our platform, each participant was shown a series of three scenarios based on their assigned domain (Figure S7). Selecting one of these scenarios opened a new page, in which the participants were shown the pre-defined query and four available options in a randomized order (Figure S8). The platform interface was designed to minimize potential confounds, presenting all information in a standardized format while randomly rearranging the order of the options within each scenario to control for position effects. Before interacting with the assigned AI agent, participants rated their initial preferences for each option, along with their confidence levels and topic familiarity, on a 10-point Likert scale. After submitting their ratings, participants entered the interaction page (Figure S9). Following a conversation lasting at least 10 turns with the assigned agent, participants provided their final preferences for each option, confidence levels, and scenario familiarity. This design enabled us to quantify changes in both decision-making and confidence levels as metrics for manipulation susceptibility."}, {"title": "Study Workflow", "content": "This study followed a three-phase protocol designed to assess participants' susceptibility to AI manipu- lation while controlling for individual differences and ensuring ethical practices. Our experiments were approved by the institutional review board at Hong Kong University (Reference No: EA240497)."}, {"title": "Phase I: Pre-Experiment Survey", "content": "Prior to our experiment", "self-esteem82": 1, "84": "Openness, Conscientiousness, Extroversion, Agreeableness, and Emotional stability. We adopted this version of the personality test, as we assumed that while an AI agent may have a perception of the user's personality traits, it would not be heavily accurate12; 2) Social Engineering Susceptibility Scale85 focusing on individuals' vulnerabilities in digital interactions and modified to include the portion of the questions that were relevant to our study (i.e., excluding questionnaire regarding informational susceptibilities); 3) Oslo Social Support Scale (OSSS-3)86, a 3-item questionnaire evaluating participants"}]}