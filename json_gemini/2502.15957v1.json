{"title": "R\u00b3Mem: Bridging Memory Retention and Retrieval via Reversible Compression", "authors": ["Xiaoqiang Wang", "Suyuchen Wang", "Yun Zhu", "Bang Liu"], "abstract": "Memory plays a key role in enhancing LLMs' performance when deployed to real-world applications. Existing solutions face trade-offs: explicit memory designs based on external storage require complex management and incur storage overhead, while implicit memory designs that store information via parameters struggle with reliable retrieval. In this paper, we propose R\u00b3Mem, a memory network that optimizes both information Retention and Retrieval through Reversible context compression. Specifically, R\u00b3Mem employs virtual memory tokens to compress and encode infinitely long histories, further enhanced by a hierarchical compression strategy that refines information from document- to entity-level for improved assimilation across granularities. For retrieval, R\u00b3Mem employs a reversible architecture, reconstructing raw data by invoking the model backward with compressed information. Implemented via parameter-efficient fine-tuning, it can integrate seamlessly with any Transformer-based model. Experiments demonstrate that our memory design achieves state-of-the-art performance in long-context language modeling and retrieval-augmented generation tasks. It also significantly outperforms conventional memory modules in long-horizon interaction tasks like conversational agents, showcasing its potential for next-generation retrieval systems.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) (Ouyang et al., 2022; Team et al., 2023; Dubey et al., 2024) have demonstrated remarkable capabilities in natural language understanding and generation (Liang et al., 2022; Srivastava et al., 2023; Wang et al., 2024a), achieving human-comparable performance on complex reasoning tasks (Guo et al., 2023; Suzgun and Kalai, 2024). Deploying LLMs as controllers to interact with dynamic environments and solve real-world tasks, i.e., as autonomous agents, has shown promising success across diverse applications, including conversational assistants (OpenAI, 2022; Achiam et al., 2023), workflow automation (Hong et al., 2023; Wu et al., 2024; Wang and Liu, 2024; Qin et al., 2025), and embodied navigation (Wang et al., 2023a; Zheng et al., 2024; Sun et al., 2024b). However, LLMs have inherent limitations: their stateless nature (Sumers et al., 2023) makes them struggle with leveraging past experiences for multi-turn interactions and cross-task generalization. Furthermore, their reliance on fixed context windows and static parameterized knowledge constrains their ability to handle complex tasks requiring dynamic, up-to-date information (Tao et al., 2024). To address these challenges, existing approaches introduce external storage (i.e., explicit memory), such as knowledge repositories (Kagaya et al., 2024; Zhu et al., 2024b) and vector databases (Liu et al., 2024; Jing et al., 2024), to enhance long-term retention and enable cross-task generalization (Maharana et al., 2024; Wang et al., 2023a,b) and cross-model sharing (Gao and Zhang, 2024). In parallel, implicit memory encodes contextual information directly into model parameters, enabling continuous knowledge updates while providing a more compact representation of information, reducing redundancy compared to external storage. Model-editing methods modify neurons to update (Huang et al., 2023; Gangadhar and Stratos, 2024) or forget knowledge (Wang et al., 2024d), while context integration (Choi et al., 2022; Wang et al., 2024c) adjusts internal parameters via model distillation. Memory-augmented Transformers (e.g., RMT (Bulatov et al., 2022), Associate Memory (He et al., 2024; Wang et al., 2024b; Tack et al., 2024), and Titans (Behrouz et al., 2024)) enhance retention by integrating dedicated memory components. However, as illustrated in Figure 1, both explicit and implicit memory involve trade-offs between storage overhead and recall effectiveness. Explicit memory grows indefinitely, requiring complex memory management techniques such as merging (Yin et al., 2024; Hu et al., 2024) and forgetting (Zhong et al., 2024). In contrast, implicit memory suffers from unreliable retrieval due to the black-box nature of LLMs, leading to confabulation and hallucination issues (Li et al., 2024a). As analyzed by Padmanabhan et al. (2024), injected atomic facts can propagate and influence broader inferences, further complicating retrieval accuracy. More recently, adaptive retrieval (Mallen et al., 2023; Farahani and Johansson, 2024) and Mem-ORAG (Qian et al., 2024) combine explicit and implicit memory in a hybrid retrieval paradigm but remain dependent on large-scale external storage. In this paper, we propose R\u00b3Mem, a novel memory-augmented model that optimizes both memory retention and retrieval while minimizing external storage dependency. R\u00b3Mem leverages a reversible architecture that integrates context compression and expansion, enabling assimilation and reconstruction of input data. Specifically, we design a context compression task that learns to generate compressed representations ('query') from raw input ('context'). R\u00b3Mem utilizes virtual memory tokens to encode and retain text that is indefinitely long. To improve compression quality, we introduce a hierarchical compression strategy, progressively refining information at the document, paragraph, and entity levels. For retrieval, R\u00b3Mem adopts a reversible architecture, reconstructing raw input by inverting the model invocation on compressed representations. This is achieved through adapter tuning, allowing seamless integration with pre-trained Transformer model while maintaining parameter efficiency. To optimize both memory retention and retrieval, we employ bidirectional training with cycle consistency. The forward process encodes context into compressed memory representations, while the backward process reconstructs the raw content from memory tokens, enforcing consistency between the original and reconstructed information. We evaluate R\u00b3Mem on memory-intensive tasks, achieving state-of-the-art performance in long-context language modeling and retrieval-augmented generation. We also integrate R\u00b3Mem into a real-world conversational agent that requires long-horizon interactions and the ability to recall distant historical context. R\u00b3Mem consistently outperforms existing memory modules, demonstrating superior scalability, retrieval accuracy, and potential for next-generation retrieval systems."}, {"title": "Methodology", "content": "In this section", "zip\" and \"unzip\" process, unifying information retention and retrieval within a duplex network.\n\nInspired by context-supervised pretraining (Gao and Callan, 2022; W et al., 2023), which trains models to generate one passage conditioned on another from the same document, we employ a similar mechanism to bridge the information gap between condensed memory and raw content. Specifically, we formulate memory retention as a context compression problem, where the model learns to generate a compressed representation ('query' q) given a raw text input ('context' c). To facilitate more flexible memory usage, we employ hierarchical compression to enhance multi-granularity assimilation, constructing (c, q) pairs at multiple levels, including document-to-paragraph, paragraph-to-sentence, and sentence-to-entity mappings. This structured approach segments documents into different semantic granularities, ensuring optimized retention and adaptive retrieval across varying levels of abstraction.\n\nFurthermore, we introduce virtual memory tokens to efficiently encode long contexts by splitting them into manageable segments and processing them sequentially while preserving previous information. These tokens cache and propagate memory across context windows, ensuring continuity in long-context retention and enabling the model to maintain coherence over extended sequences.\n\nFormally, given a context-query pair \u3008c, q) from the context-query set Dc, the memory network M\u03b8 learns to model the conditional probability M\u03b8(q| c) using an autoregressive decoder": "n\nMo(q | c) = \\prod_{t=1"}, {"pairs": "n\nDc = Da \u222a Dp \u222a Ds  (2)\n= {(d", "as": "c\u00b0 = Orcs\u2295\u03b8w", "1": "n\nMo(c|q) = \\prod_{l=1"}, {"Mo^{[0;4": ""}, {"loss": "n\nLforward = \\sum_{t=1"}, {"M[0;$": "qt | q<t"}, {"M[0;$": "qt | q<t", "4": "n\nLbackward = -\\sum_{l=1}^{L}log log Mo^{[0;4"}]}