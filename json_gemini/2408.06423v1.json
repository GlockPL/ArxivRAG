{"title": "Evaluating Language Models on Entity Disambiguation in Tables", "authors": ["Federico Belotti", "Fabio Dadda", "Marco Creamschi", "Roberto Avogadro", "Riccardo Pozzi", "Matteo Palmonari"], "abstract": "Tables are crucial containers of information, but understanding their meaning may be challenging. Indeed, recently, there has been a focus on Semantic Table Interpretation (STI), i.e.,, the task that involves the semantic annotation of tabular data to disambiguate their meaning. Over the years, there has been a surge in interest in data-driven approaches based on deep learning that have increasingly been combined with heuristic-based approaches. In the last period, the advent of Large Language Models (LLMs) has led to a new category of approaches for table annotation. The interest in this research field, characterised by multiple challenges, has led to a proliferation of approaches employing different techniques. However, these approaches have not been consistently evaluated on a common ground, making evaluation and comparison difficult. This work proposes an extensive evaluation of four state-of-the-art (SOTA) approaches - Alligator (formerly s-elBat), Dagobah, TURL, and TableLlama; the first two belong to the family of heuristic-based algorithms, while the others are respectively encoder-only and decoder-only LLMs. The primary objective is to measure the ability of these approaches to solve the entity disambiguation task, with the ultimate aim of charting new research paths in the field.", "sections": [{"title": "1 Introduction", "content": "Tables are commonly used to create, organise, and share information in various knowledge-intensive processes and applications in business and science. Disambiguating values occurring in the table cells using a background Knowledge Graph (KG) is useful in different applications. First, it is part of the broader objective of letting machines understand the table content, which has been addressed by different research communities with slightly different formulations like Semantic Table Interpretation (STI) [49] - the one considered in this paper, semantic labelling [62], and table annotation [68]. The main idea behind these efforts is to match the table against a background knowledge graph by annotating cells (mentions) with entities (Cell-Entity Annotation - CEA), columns with class labels (Column-Type Annotation - CTA), and pairs of columns with properties (Column-Property Annotation - CPA) [37]. Second, annotations produced by table-to-graph matching algorithms can be used to transform the tables into Knowledge Graphs (KGs) or populate existing ones. Third, links from cells to entities of KGs support data enrichment processes by serving as bridges to augment the table content with additional information [20]. This conceptualisation covers most of the proposed definitions by generalizing some aspects (e.g., consideration of NILs and selection of column pairs to annotate).\nIn particular, we focus on Entity Linking (EL) in tables (CEA, in the STI terminology), which is relevant not only to support table understanding but also to support data transformation, integration and enrichment processes, which are particularly interesting from a data management point of view. The Cell-Entity Annotation (CEA) tasks can be broken down into two sub-tasks: candidate Entity Retrieval (ER), where a set of candidates for each mention is collected and, often, associated with an initial score and rank, and Entity Disambiguation (ED), where the best candidate is selected (and, in some case, a decision whether to link or not is also considered [6]).\nWhen considering approaches to STI and, especially, CEA, it should be considered that the content and the structure of tables may differ significantly, also depending on application-specific features: column headers may have interpretable labels or be omitted; the number of rows can vary from a dozen (e.g., as typical in tables published on the web or in scientific papers) to hundreds thousand or even millions (e.g., in business data); the cells may include reference to well-known entities (e.g., geographical entities) as well as to specific ones (e.g., biological taxa); tables may come with a rich textual context (e.g., caption or other descriptions in web or scientific documents), or no context at all (e.g., in business data) [49].\nA first generation of approaches to CEA has exploited different matching heuristics, traditional machine learning approaches based on engineered features (in the following \"feature-based ML\"), or a combination of both [49]. The International Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab), at its sixth edition in 2024, is a community-driven effort to compare different approaches systematically with a common experimental setting. The SemTab challenge has pushed different researchers to increase the performance of their approaches and publish datasets for evaluating STI approaches. For example, some effort has been"}, {"title": "2 Related Work", "content": "CEA is usually divided into two sub-tasks: retrieval of candidate entities, i.e., ER [64], and entity disambiguation, i.e., ED [63], where one or no candidate is selected as a link, usually after scoring and ranking the candidates. Approaches to ED for tables have been mostly proposed as part of broader STI systems [49]. For this paper, we group these approaches into two main categories: i) those based on heuristics, (feature-based) ML and probabilistic approaches, and ii) those based on LLM.\nHeuristic, ML and Probabilistic-based Approaches. We refer to [49] for a thorough review of STI approaches and ED approaches proposed therein up to 2022. The ED task in the STI can be performed by applying multiple techniques while focusing on different inherent information: i) similarity, ii) contextual information, iii) ML techniques, and iv) probabilistic models. Often, the disambiguation step involves selecting the winning candidate based on heuristics, like the string similarity between the entity label and mention [3, 7, 14, 17-20, 33, 34, 44, 48, 59, 66, 67, 71, 73, 82].\nContextual information during the CEA task considers the surrounding context of a table cell, such as neighbouring cells, column headers, or header row. Contextual information provides additional clues or hints about the meaning and intent of the mention. By analysing the context, a system can better understand the semantics of the cell and make more accurate annotations [2, 7, 9, 10, 12-14, 17, 19, 25, 32-34, 50, 51, 56-58, 65, 70, 71].\nOther methods that can be employed are ML techniques. These techniques typically involve training a ML model on a labelled"}, {"title": "3 Considered Approaches", "content": "We select four approaches representative of different categories of algorithms that solve semantic tasks on tables, and, especially CEA: TableLlama is the first autoregressive LLM that is specifically instruction-tuned on tabular data and reports SOTA results [80]; TURL, cited as previous SOTA in [80], is a BERT-based encoder-only model performing the three STI tasks, including CEA [23]; Alligator is a recent feature-based ML algorithm focusing on CEA, is publicly available, and has been evaluated in settings similar to the moderately-out-of-domain settings discussed in this paper [6]; Dagobah, a heuristic-based algorithm, has been the winner of various rounds of the SemTab challenge in 2020 [34], 2021 [33] and 2022 [32] and is publicly available.\nWe remark that all the selected approaches perform all three STI tasks (which is rare and not obvious, especially for the LLM-based approaches), i.e., CTA, CPA and CEA, and we focus only on the CEA one.\nTURL [23] introduces the standard BERT pre-training finetuning paradigm for relational Web tables and is composed of three main modules: i) an embedding layer to convert different components of a table into embeddings, ii) a pre-trained TinyBERT Transformer [36] with structure-awareness to capture both the textual information and relational knowledge, and iii) a final projection layer for pre-training/fine-tuning objectives. The embedding layer is responsible for embedding the entire table, distinguishing between word embeddings $x_w$, derived from table metadata and cell text (mentions), and entity embeddings $x_e$, representing the unique entities to be linked. The sequence of tokens in $x_w$ and the entity representation $x_e$ are sent to a structure-aware TinyBERT Transformer to obtain a contextualised representation. To inject the table structural information into the contextualised representations, a so-called visibility-matrix M is created, such that entities and text content in the same row or column are visible to each other, except table metadata that are visible to all table components. During pre-training, both the standard Masked Language Model (MLM) and the newly introduced Masked Entity Retrieval (MER) objectives are employed, with the projection layer that is learned to retrieve both the masked tokens and entities\u00b2. During fine-tuning, the model is specialised to address the specific downstream task. Specifically, for CEA, when provided with the sub-table containing all mentions to be linked to an external KG, TURL is fine-tuned to produce a probability distribution over the combined set of candidates for each mention to be linked. This means that TURL lacks the context provided by all the not-to-be-linked cells, without the possibility of abstaining from answering or responding with a NIL. Also, in the original paper, we observe that the best candidate is chosen by a function that compares the best score computed by TURL and the score assigned by the model to the first candidate retrieved by"}, {"title": "4 Study Set-up", "content": "As specified by the SemTab challenge [30], the metrics adopted to evaluate an approach are: Precision (P) = $\\frac{\\text{# correct annotations}}{\\text{# submitted annotations}}$, Recall (R) = $\\frac{\\text{# correct annotations}}{\\text{#ground-truth annotations}}$ and F1 = $\\frac{2PR}{P+R}$. Since we are mainly interested in measuring the ability of the models to disambiguate the correct entity among a limited set of relevant candidates, we ensure the correct entity is always present in the candidates set, and otherwise, we inject it, as in previous work [23, 80]. Under this setting, Precision, Recall and F1 are the same and equal to the Accuracy metric. Therefore, we report the accuracy as $\\frac{\\text{# correct annotations}}{\\text{mentions to annotate}}$.\nTo maintain a good balance between speed, coverage, and diversity, we retrieve and inject at most 50 candidates for every mention, eventually adding the correct one.\nOur evaluation has the main objective of evaluating the selected approaches on datasets not considered in the original experiments, considering especially the evaluation of TURL and TableLlama on STI-derived datasets and of Alligator and Dagobah (with some limitations) on the datasets used to train and evaluate the first two approaches. Therefore, we first discuss the datasets used in our study (Section 4.1), and the evaluation protocol with its associated objectives (Section 4.1). Then, we provide details about ER (Section 4.3) and the training of the models (Section 4.4)."}, {"title": "4.1 Datasets", "content": "The datasets considered in this work come from different sources and contain information about domains. In particular, we have selected the following:\n\u2022 SemTab2021 - R3 (BioDiv) [4]: the BioDiv dataset is a domain- specific benchmark comprising 50 tables from biodiversity research extracted from original tabular data sources; annotations have been manually curated.\n\u2022 SemTab2022 - R2 (2T) [21]: the dataset consists of a set of high-quality manually-curated tables with non-obviously linkable cells, i.e., where mentions are ambiguous names, typos, and misspelt entity names.\n\u2022 SemTab2022 - R1 & R2 (HardTables) [1]: datasets with tables generated using SPARQL queries [37]. The datasets used from HardTables 2022 are round 1 (R1) and round 2 (R2). The target KG for this dataset was Wikidata, and as with previous years, the tasks were CEA, CTA, and CPA;\n\u2022 SemTab2023 - R1 (WikidataTables) [30]: datasets with tables generated using SPARQL queries for creating realistic-looking tables. The dataset includes Test and Validation tables, yet we exclusively employ the Validation tables due to Gold Standard (GS) being provided. The target KG for"}, {"title": "4.2 Distribution-aware Experimental Objectives", "content": "The datasets used for training or evaluating ED are generated from different sources, contain tables of different sizes, and hold information about disparate domains. We assume each dataset is associated with a data distribution generating it [4, 21, 23, 30, 37, 38].\nWe introduce the concepts of \u201cin-domain\u201d, \u201cout-of-domain\" and \"moderately-out-of-domain\" settings related to the evaluation of a particular approach on a test set, specialising a distinction between \"in-domain\" and \"out-of-domain\" used in [80]. These denominations depend on the source the data has been generated from and the domain(s) it holds the information about. In particular, we define:\n\u2022 \"in-domain\" (IN): a test set Y for an approach A is considered \"in-domain\" for A if A has been trained on a dataset X generated from the same data source and covering similar domain(s) as Y;\n\u2022 \"out-of-domain\" (OOD): a test set Y for an approach A is considered \"out-of-domain\" if A has been trained on a set of data X generated from a different data source and covering different domain(s) as Y;\n\u2022 \"moderately-out-of-domain\" (MOOD): a test set Y for an approach A is considered \u201cmoderately-out-of-domain\" for A if A has been trained on a set of data X generated from the same data source but covering different domain(s) as Y or vice versa, i.e., if A has been trained on a set of data X generated from a different source but covering similar domain(s) as X. This covers a setting such as the evaluation of TableLlama, pre-trained on the TURL dataset [23], on the STI-derived test set 2T: the two datasets contain different tables but cover cross-domain information linked to Wikidata in a quite similar way.\nGiven the definitions above, the evaluation procedure has been divided into two steps\n(1) We assess the performance of the considered approaches on the test data defined in Section 4.1, without further fine-tuning. The heterogeneity of the test data implies all the algorithms are tested against \u201cin-domain\u201d, \u201cmoderately--out-of-domain\" and \"out-of-domain\" data, as visible in Table 3\n(2) We fine-tune TURL, TableLlama and Alligator on the train splits from their MOOD data, i.e., on SemTab2022 R1 (HardTables), Sem Tab2022 R2 (HardTables), Sem Tab2023 R1 (WikidataTables) and 2T-red for TURL and TableLlama, and on TURL120k for Alligator; finally we test the fine-tuned models on our test data"}, {"title": "4.3 Candidate entity retrieval with LamAPI", "content": "The candidates for the mentions contained in the TURL dataset [23], along with its sub-sampled version TURL-2K [80], were retrieved through the Wikidata-Lookup-Service, which is known to have a low coverage w.r.t. other ERs [8]. For this reason, we researched to identify a state-of-the-art approach/tool specific to the ER. The final choice fell on LamAPI, an ER system developed to query and filter entities in a KG by applying complex string-matching algorithms. As suggested in the paper [8], we have integrated DBpedia (v. 2016-10 and v. 2022.03.01) and Wikidata (v. 20220708), which are the most popular KGs also adopted in the SemTab challenge. In LamAPI, an ElasticSearch index has been constructed, leveraging an engine designed to search and analyse extensive data volumes in nearly real-time swiftly. These customised local copies of the KGs are then used to create endpoints to provide ER services. The advantage is that these services can work on partitions of the original KGs to improve performance by saving time and using fewer resources. This simulates an application setting of large-scale entity disambiguation (large tables), where a local copy can speed up operations substantially. The LamAPI Lookup service was used to extract the candidates, as carried out by other services [6]. Given a string input, the service retrieves a set of candidate entities from the reference KG.\nThe choice to use LamAPI as an ER system is based on its availability and performance compared to other available systems (e.g., Wikidata Lookup) [8]. To further validate the choice of LamAPI we have computed the number of mentions with K candidates for the TURL-2K-red dataset: the original TURL - 2K dataset has almost 600 mentions with 1 candidate (the correct one, with 969 mentions (\u2248 48%) with at most 5 candidates included the correct one). For all those mentions the Wikidata-Lookup service fails to retrieve something meaningful. On the contrary LamAPI retrieves for 1650 mentions (\u2248 91%) in our sub-sampled dataset at least 45 candidates. In particular, for the TURL-2K-red dataset, the coverage (i.e., how many times the correct candidate is retrieved by the ER system over the total number of mentions to cover) of LamAPI and Wikidata-Lookup is 88.17% and 71.75% respectively. For all these reasons we decided to replace the candidates extracted from LamAPI to build a new version of TURL-2K-red dataset which we called TURL-2K-red-LamAPI."}, {"title": "4.4 Training and implementation details", "content": "Pre-training and model usage. For TURL we first replicated the pre-training with the same hyperparameters as specified by the authors in [23] but in a distributed setting on 4 80GB-A100 GPUs, following the findings in [29], then we fine-tuned it with the default hyper-parameters, matching the CEA results in the original paper. We use the open-source version of TableLlama made available on HuggingFace. Alligator is pre-trained on different SemTab datasets before 2022 as in the original paper. We refer to Table 3 for details about the datasets used for pre-train. Dagobah is run with the default hyperparameters as specified in the corresponding GitHub repository.\nFine-tuning. We remind that we consider two evaluation settings (see Section 4.2): 1) the approaches based on their pre-trained state without having directly seen any table of the test datasets; 2) fine-tuning TURL, TableLlama and Alligator on MOOD data (see Table 1). For the fine-tuning of TURL the default hyperparameters have been"}, {"title": "5 Results and discussion", "content": "We first focus on the main results, then we discuss evidence from ablation studies."}, {"title": "5.1 Main results", "content": "Table 4 reports the performances achieved by the four different approaches on the test data both for the pre-trained and fine-tuned models. We observe that Alligator excels on HT-R1, HT-R2 and WikidataTablesR1 SemTab datasets, it performs poorly on both BioDiv-red and TURL-2K-red-LamAPI while performing discretely on 2T-red dataset w.r.t. TableLlama. The opposite is observed for TableLlama and TURL, with TableLlama achieving generally higher accuracy than TURL. This trend can be explained by the fact that HT-R1, HT-R2 and WikidataTables-R1 are considered IN data for Alligator, while both BioDiv and TURL-2K-red-LamAPI, being OOD and MOOD data respectively, contain tables coming from different specific domains, with misspelled, repeated and abbreviated mentions, whose heterogeneity is difficult to capture with handcrafted features. Surprisingly, both TURL and TableLlama excel on BioDiv, even though it's considered OOD for both approaches: we hypothesise that the enormous and general pre-training knowledge retained by these models explains this excellence. The poor performance achieved by TURL on all the SemTab data, considered MOOD for it, can be explained by the fact that TURL is already fine-tuned on web tables, which are generally coming from a different data distribution than the ones of the SemTab challenge. Interestingly, we observed that both TableLlama and TURL underperforms themselves on the TURL-2K-LamAPI dataset: we argue that the drop in performances, especially for TURL, is due to the increased number of candidates we have retrieved with LamAPI (see the ablation study in section 5.2)."}, {"title": "5.2 Ablation studies", "content": "To explain the drop in performances observed for TURL and TableLlama on the TURL-2K-red-LamAPI dataset, we have measured the accuracy w.r.t. the number of candidates per mention, with the intuition that the higher the number of candidates the lower the performances. Figure 4 reports the accuracy of TURL and TableLlama given a different number of candidates per mention, considering also the correct one. Our intuitions are empirically confirmed by observing a drop in performance for both approaches, with a more severe one for TURL. This could happen because TURL aggregates the candidates of every mention in a table and passes them through a Transformer, increasing the context length to at most O(NK), where N is the number of mentions in a table and K is the maximum number of candidates retrieved per mention.\nWe ran an additional ablation study to measure the impact of the table's metadata (e.g., the title of the Wikipedia page the table is found in, the section title or the table caption to name a few) on the final accuracy achieved by both TURL and TableLlama, testing both on the TURL-2K-red-LamAPI dataset, the only dataset with table metadata, with i) No-Meta, i.e., a setting where the page title, section title and the table caption are removed; ii) No-Header, i.e., the table header is changed to [colo, col1, ..., colN] and iii) No-Meta-No-Header, i.e., the combination of both i) and ii). From Figure 5, we observe that TURL is heavily dependent on the table metadata, with a severe drop in performance when both metadata and header are removed. On the other hand, TableLlama is almost unaffected by removing metadata from the prompt, indicating a higher generalisation capability and a greater focus on the context provided by the table itself rather than by the metadata)."}, {"title": "5.3 Discussion", "content": "Generative GTUM approaches with a high number of parameters seem to have interesting properties in terms of accuracy, generalisation, and robustness to the number of candidates that are processed and the table metadata, especially on MOOD and OOD data; however, specific STI approaches trained on in-domain data, and optionally fine-tuned on MOOD data, using a tiny fraction of these parameters can still outperform generalistic approaches with speed higher by an order of magnitude; these results may suggest that generative GTUM approaches are at the moment the best choice for processing small tables, while more specific entity disambiguation approaches may still be a better fit for applications on large business data. Generative approaches like TableLlama seem more promising than encoder-based ones in terms of performance, with a negligible risk of hallucinations; however, the comparison considered models of uncomparable size (7B vs 300M for TableLlama and TURL resp.), with TURL being the fastest approach among those that were tested. Regarding scalability, long context allows the encapsulation of large tables with a high number of candidates, but some tables"}, {"title": "6 Conclusions and Future Works", "content": "STI, the process of annotating tabular data with information from background KGs, is proposed to support the understanding, interpretation and labeling of tables. Among the STI's tasks, CEA, i.e., matching cell values to entities in the KG, is particularly relevant to support additional downstream transformation, integration, and enrichment processes, and is particularly subject to scalability constraints, e.g., when applied to tables with a large number of rows. LLMs pretrained with a vast amount of data have been applied to STI and CEA, complementing previous approaches based on heuristic and featured-based ML approaches. However, these different families of approaches have not been exhaustively examined on a common ground. In this work, we tackled this gap by selecting four representative approaches and comparatively evaluating them in terms of accuracy, generalisability, time, and memory requirements to better study their strengths and limitations, as well as their potential applications to different scenarios. We defined different evaluation settings, i.e., \u201cin-domain\", \u201cout-of-domain\" and \"moderately out-of-domain\" for a better analysis of generalisability.\nOur experiments suggest that an approach like TableLlama, based on a large generative LLM excels in accuracy and generalisation, as demonstrated by the results on MOOD and OOD data (see Table 4), at the price of an excessive execution and training time. TURL, an encoder-only model based on TinyBERT, is the most efficient, but lacks on generalisation capabilities: however, fine-tuning using data from similar distributions can lead to improvements with the new data at the price of a drop with pre-train data. Evaluating the impact of a larger encoder-based LLM on an approach like TURL could be an interesting research direction. However, our experiments suggest that specific STI models like Alligator, despite their limited number of parameters can still outperform generalistic models in IN-domain settings with a huge gain in efficiency.\nFuture works include the possibility to train a smaller and cheaper LLM-based model, e.g., Phi [5], while enabling also the handling of NIL entities and a score associated with cell annotations. Another possible direction is to let both TURL and TableLlama be cell-based instead of table-based to reduce the occupied memory by both approaches and to enable standard augmentation techniques for the former."}]}