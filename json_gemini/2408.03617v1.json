{"title": "Is Child-Directed Speech Effective Training Data for Language Models?", "authors": ["Steven Y. Feng", "Noah D. Goodman", "Michael C. Frank"], "abstract": "While high-performing language models are typically trained on hundreds of billions of words, human children become fluent language users with a much smaller amount of data. What are the features of the data they receive, and how do these features support language modeling objectives? To investigate this question, we train GPT-2 models on 29M words of English-language child-directed speech and a new matched, synthetic dataset (TinyDialogues), comparing to a heterogeneous blend of datasets from the BabyLM challenge. We evaluate both the syntactic and semantic knowledge of these models using developmentally-inspired evaluations. Through pretraining experiments, we test whether the global developmental ordering or the local discourse ordering of children's training data support high performance relative to other datasets. The local properties of the data affect model results, but somewhat surprisingly, global properties do not. Further, child language input is not uniquely valuable for training language models. These findings support the hypothesis that, rather than proceeding from better data, children's learning is instead substantially more efficient than current language modeling techniques.", "sections": [{"title": "Introduction", "content": "Transformer-based language models (LM) show very strong performance on a wide variety of downstream tasks, but typically only after pretraining on hundreds of billions to trillions of words (Brown et al., 2020). In contrast, human learners use language fluently after far less training data \u2013 in the 10s to 100s of millions of words. This \"data gap\" (Frank, 2023a) of several orders of magnitude poses a substantial challenge for machine learning.\nIs the source of human children's efficient learning a function of their data or their learning algorithms? While children receive rich multi-modal input from their exploration of the world, here we focus primarily on their language input, which has been a major focus of study in developmental psychology (MacWhinney, 2014). One hypothesis is that the language data that children receive is a uniquely rich learning signal \u2013 conversational interaction with their caregivers \u2013 that is curricularized optimally to support learning (Eaves Jr et al., 2016; You et al., 2021; Newport, 1990). Indeed, interventions to increase the quality of caregiver language do produce improvements in children's language learning (Ferjan Ram\u00edrez et al., 2020), and interventions to simplify model training data also result in stronger performance (Muckatira et al., 2024; Eldan and Li, 2023).\nLanguage model pretraining experiments provide a targeted method for investigating dataset quality (Kallini et al., 2024): we can manipulate the training data available to models to create \"controlled rearing\" experiments. We take advantage of this method to investigate the properties of child-directed speech for learning the syntactic and semantic structure of language. We use GPT-2 as our simulated learner, and pretrain on natural and synthetic child-language data. For each of these, we conduct two experiments. First, we investigate whether the natural curricularization of children's input \u2013 from simpler utterances to more complex conversations \u2013 affects language model learning. Second, we test whether the local discourse coherence structure of dialogue results in better learning. Finally, we compare to learning on a more heterogeneous blend of data from various sources.\nWe find that the curricularization of child language does not provide a uniquely valuable signal for language models, supporting the hypothesis that other aspects of children's learning (not simply the data) \u2013 perhaps interactions with their training data \u2013 are responsible for their efficiency relative to language models. On the other hand, the source, composition, and local properties of the training data have measurable effects on model performance."}, {"title": "Related Work", "content": "The efficiency of children's learning has been an important focal point for recent NLP efforts (Huebner et al., 2021; Zhang et al., 2021). For example, last year's BabyLM challenge held the training data for models constant, while encouraging entrants to investigate alternative learning architectures (Warstadt et al., 2023). Smaller models of this type must be evaluated using targeted benchmarks more appropriate to their performance levels, including evaluations of semantic (Zhuang et al., 2023) and grammatical abilities (Huebner et al., 2021; Warstadt et al., 2020). These evaluations have even been used to benchmark performance based on data from a single child (Qin et al., 2024).\nThe method of \u201ccontrolled rearing\" (manipulating data while holding the model constant) for language models (Frank, 2023b) has a long history in cognitive science, e.g. Christiansen and Chater (1999), but has recently become prominent for testing learnability claims (Warstadt and Bowman, 2024; Kallini et al., 2024; Misra and Mahowald, 2024). Often, models trained on naturally-occurring corpora are contrasted with counterfactual corpora constructed via targeted experimental manipulations \u2013 for example, shuffling sentence ordering (Kallini et al., 2024) or removing particular constructions (Misra and Mahowald, 2024).\nCurricularization of training data is widely investigated in machine learning (Bengio et al., 2009), with the guiding idea being that an appropriate ordering of training examples can lead to a smoother path to the desired objective. Children's development is argued to create a curriculum to facilitate their learning (Smith et al., 2018; Cusack et al., 2024). In one study from the visual domain, Sheybani et al. (2024) trained self-supervised models on data from infants and found that a developmental ordering leads to stronger eventual performance compared with a reversed ordering. Our study tests this hypothesis in the language domain."}, {"title": "Methods", "content": "CHILDES The Child Language Data Exchange System (CHILDES) is a repository of human-transcribed corpora of children and caregivers' talk (MacWhinney, 2014), with children ranging from birth to age 13. We take the English subset, which consists of approximately 29M total words (including speaker labels and other metadata) across \u224811k conversations. CHILDES is heavily skewed towards younger ages; ~90% of the data is for children ages 2-5 (see Figure 1 in Appendix C).\nTinyDialogues Inspired by TinyStories (Eldan and Li, 2023), we collect a synthetic dataset consisting of approximately 29M words called TinyDialogues (TD). Using GPT-4, we prompted the generation of realistic conversations involving children of ages 2, 5, 10, and 15 years as the central participant, along with a list of other potential participants (e.g. mom, teacher, babysitter). To diversify, we seeded each conversation based on a list of words known by children at the relevant age and varied the conversation type and length (see Appendix A).\nBabyLM We further compare to the dataset distributed by the BabyLM challenge (Warstadt et al., 2023), a 100M word dataset that is a mixture of several sources including transcribed speech, child-directed speech (e.g. CHILDES), children's storybooks, and simple Wikipedia. It is designed to approximate the language data that a 10-year-old child could receive. We sub-sampled \u224829M words from BabyLM to match the size of this dataset to our CHILDES and TinyDialogues data.\nPreprocessing Training data for CHILDES and TD was set up so that each line corresponded to a single conversation. Training data for BabyLM was set up using the pre-existing format in the BabyLM challenge, which varied depending on the sub-dataset. Each dataset was then split into 85/15 train/val splits, of approximately 24.5M training words and 4.5M validation words. During training, GPT-2 breaks up the data into chunks of 1024 consecutive tokens which are fed into the model."}, {"title": "Evaluation", "content": "Zorro (Huebner et al., 2021) is designed for child-directed language and aims to quantify the syntactic and grammatical knowledge of language models. It does so by assessing their capability to distinguish between minimal pairs of sentences that exhibit various grammatical contrasts. We report final averages (of accuracy, higher is better) across individual Zorro tasks in Section 4.\nWord Similarity To assess the semantic knowledge of our models, we employ a word similarity (WS) metric (Zhuang et al., 2023), which measures the ability of models to capture semantic similarities between pairs of words. Many assessments of children's language learning are multi-"}, {"title": "Experiments", "content": "Global Ordering To test whether the natural ordering of speech to children presents an effective curriculum for model learning, we ordered our CHILDES and TD conversations (training examples) in three ways: 1) age order (from younger to older), 2) reverse order (from older to younger), and 3) random order (equivalent to default LM training practices of randomly shuffling the training data). CHILDES includes fine-grained age information of the target (main) child involved in each conversation, down to fractions of months (essentially days), and we ordered conversations based on this information. TD was ordered based on the conversation seed ages of 2, 5, 10, and 15 years old. For the random order experiments, we randomly shuffled the conversations and kept this shuffled order for all experiments for consistency purposes.\nLocal Ordering To investigate the effects of local conversation and discourse coherence on learning, we ordered utterances within each CHILDES and TD conversation in two ways: 1) normal (original) order, and 2) random order. The latter breaks the local discourse coherence, by randomly shuffling utterances within each conversation.\nModel Training\nWe use GPT-2 (Radford et al., 2019) with 124M parameters (small version), following prior \u201ccontrolled rearing\" work (Kallini et al., 2024; Misra and Mahowald, 2024; Qin et al., 2024). We trained a separate tokenizer on each of our datasets, and pretrained GPT-2 from scratch using a learning rate (LR) of le \u2013 04, linear LR scheduler with no warmup steps, varying batch sizes (e.g. 4, 8, 16) per GPU, training seed of 42, and Adam optimizer with \u03b2 = (0.9, 0.999) and \u0454 = 1e \u2013 08.\nFor our global ordering experiments, we split each dataset into b approximately equal sections (buckets), and trained on each repeatedly (n times) before moving to the next bucket or section of the dataset. This technique was intended as a compromise between standard techniques for model training - which require iterated training on a dataset and human learning \u2013 which operates via a single pass through ordered training data. In Section 4, we report results averaging across n = 3, 5, 10, 20.\nFor TD, we used the data corresponding to the four seed ages as the four buckets. For CHILDES, we experimented with different numbers of buckets (b) and settled on b = 5 for most experiments. To compare to BabyLM (which cannot be bucketed), we also trained iteratively on each dataset for 20 epochs, selecting the epoch that performed best on the respective validation split (lowest val loss)."}, {"title": "Results and Analysis", "content": "Major results of our experiments can be found in Tables 1, 2, 3, and 4. More detailed results of the curricularization experiments (i.e. broken down by different values of b and n) can be found in Appendix G.\nAs seen in Tables 1 and 2, training on BabyLM yields noticeably better results than CHILDES and TD on Zorro (syntax), and substantially better results than CHILDES on WS (semantics). Overall, these results are surprising: a mixture of different data sources may be more effective for training smaller models on a limited amount of data, and even synthetic conversation data appears to be more effective than natural conversation data for training small-scale models on limited data.\nSome potential explanations include fundamental differences in the data itself. CHILDES is heavily skewed towards younger ages (see Figure 1 in Appendix C), whereas TD is a more uniform distribution across ages with more sophisticated conversations intended to simulate speech to older children. As such, it contains a higher fraction of more grammatical utterances and text. While collecting TD, we ensured that TD was also diverse in the type of conversation, participants, and content. This feature likely leads to a more comprehensive coverage of the distribution of potential conversations. Both these factors may lead to more effective learning of syntax and semantics. Lastly, high-quality synthetic data - in contrast to naturalistic data, which contains disfluencies and occasional garbled tokens due to transcription issues \u2013 may simply be better suited for training LMs, especially when data is limited (and quality is likely even more important).\nAs seen in Table 3, the effects of global ordering seem to have negligible effects on model performance. Zorro and WS values remain relatively stable despite changes to global order. This result is also surprising, as one would expect that, similar to humans, curricularization based on difficulty would affect model learning and performance, i.e. it would be easier to learn from simpler utterances and conversations before moving to more difficult ones. Aligning with this, model convergence behavior, especially training loss, differed on a local level (e.g. per epoch or bucket) depending on the ordering method, but general high-level behavior (especially of the validation loss) was relatively stable (see Appendix I). Language models, unlike humans, may be less affected by curricularization, especially when the amount of data is limited.\nFrom Table 4, we see that local ordering affects model performance. Breaking local discourse coherence negatively impacts both Zorro and WS evaluations, even though Zorro consists of single sentence evaluations. Effects on WS are more evident for CHILDES training than TD, where WS remains the same. A potential explanation is that CHILDES contains noticeably shorter utterances, on average, than TD (\u2248 4 words vs. 13). Hence, re-ordering CHILDES utterances likely has a greater effect on the model's ability to learn semantics from similarity across a larger set of short utterances."}, {"title": "Conclusion & Future Work", "content": "Why do children require so much less training data than language models to become fluent language users? We ran experiments using GPT-2 on CHILDES, BabyLM, and a new synthetic conversation dataset that we collected called TinyDialogues. A heterogeneous mixture of different data sources performed better than homogeneous conversation data - further, high-quality synthetic conversation data yielded better performance than natural conversation data. Additionally, we found that global developmental ordering and curricularization did not have noticeable effects on performance, whereas local discourse coherence structure did. In sum, it seems that the curricularization of child language does not provide a uniquely valuable signal for language models. However, the source, composition, and local properties of the training data affect model learning. We hope that future work builds on our work here to expand upon the available evaluation benchmarks and data mixtures for comparison between models and children, and extends this comparison to multi-modal datasets and models."}, {"title": "Limitations", "content": "Some limitations of our work include our current suite of evaluation benchmarks and models. We can expand our benchmarks to include more theory of mind and developmental psychology-inspired benchmarks, and ones for longer coherency evaluation. We can also experiment with larger language models such as LLama-3. Furthermore, we limited our investigations to conversation data and the BabyLM mixture. We could explore more types and sources of data, and different varieties and proportions of data mixtures. Additionally, the CHILDES dataset is heavily skewed towards younger ages. Unfortunately, to the best of our knowledge, a more balanced and uniform dataset of high-quality textual transcriptions of child-directed conversations is not currently available, but we could consider collecting one in the future. Overall, these are directions to potentially improve and expand upon our work in the future. We feel that, despite these limitations, our current work is an insightful and focused contribution."}, {"title": "Ethical Considerations", "content": "The majority of our datasets and evaluation benchmarks are already existing, publicly available datasets and benchmarks, intended for public use. We collected TinyDialogues using GPT-4, following all intended use purposes and OpenAI's policies. Further, the dataset is entirely synthetic, and does not include personal or private information. As a safe and controlled language model, there is an incredibly low risk of offensive content, especially as it involves conversations with younger children. We also manually examined a large subset of the data and ensured there were no ethical issues. This includes profanities, racism, bias, offensive words, and other malicious language.\nWe acknowledge the potential weaknesses of our trained models, which are small in scale and limited in performance. We will never use or encourage their use for real-world purposes. Our initial experiments are conducted purely for investigation purposes to test our hypotheses. We feel that our work is an important contribution to the ML, NLP, cognitive science, and psychology communities, and we encourage researchers to expand upon it.\nOur models, TinyDialogue dataset, and accompanying publication are intended only for research purposes and to assess the effectiveness of child-directed speech for training language models. We do not foresee any explicit way that malicious actors could specifically misuse our trained models or models that could be trained on our dataset."}, {"title": "TinyDialogues: Dataset Collection Details & Examples", "content": "Here we discuss some further dataset collection details for TinyDialogues (TD), with examples of TD conversations in Table 5.\nThe specific GPT-4 model we use for collecting our entire dataset is gpt-4-1106-preview, which is GPT-4 Turbo with training data up to Apr 2023. To increase the diversity of the generated conversations, when prompting GPT-4, we also specify the particular type of conversation (Table 6), the approximate length or number of turns (5 or 10), other potential participants in the conversation (Table 7), and certain words (one noun, one verb, and one adjective) sampled from Wordbank CDI (Frank et al., 2021) (ages 2 & 5) and AoA (Kuperman"}, {"title": "Data Format & Preprocessing", "content": "CHILDES: We noticed some duplicate utterances in CHILDES conversations and removed these to the best of our ability. We preprocessed the CHILDES data to match the format of the TD examples in Table 5. See below for an example of part of a single training example for CHILDES. Double asterisks surround speaker labels, double newline tokens separate utterances, and an end-of-text token marks the end of the conversation. Hence, this format was consistent across all conversations in both CHILDES and TD datasets.\nTinyDialogues: One inconsistency in the collected TD data was that the speaker label for the target child varied across conversations and ages. For 2-year-olds, GPT-4 labeled the child toddler, and 15-year-olds were labeled teenager. This is likely due to our prompt as discussed in Appendix A. Further, within the same age, sometimes the label also differed (e.g. Child, 5-year-old child, 5-year-old). To align with CHILDES, which only used the speaker label CHI for every target child, and make Zorro evaluation consistent across the board (see Appendix E), we converted several plausible target child speaker labels in our TD dataset (based on manual examination) to Child. We also tried our best to correct any other issues in the GPT-4 outputs, including occasional inconsistencies with newlines and double newline tokens.\nBabyLM: We concatenated the data across the BabyLM sub-datasets, then sampled approximately 29M words to match the amount of data in CHILDES and TD, while keeping the original distribution among its sub-datasets intact. We sampled in order (i.e. starting from the beginning of each sub-dataset), as several BabyLM examples (e.g. for Wikipedia) span multiple lines, and random sampling would have broken up individual examples and eliminated coherence. We do no further preprocessing to the BabyLM data and keep the format of the original sub-datasets. In particular, we do not add an  token to the end of each example (like we do with CHILDES and TD) as it is unclear where each example ends.\nThe Python NLTK package's word_tokenize function was used as part of our statistics calculations (discussed in Appendix C). The parameters for this function are: language is 'english' (default) and preserve_line is 'False' (default) so line breaks are ignored. Specifically, it was used for calculating the number of unique words in Appendix C. For consistency purposes, data processing and sampling, and other word-related statistics (e.g. total word count, avg. words per utterance) were done by simply splitting the text by space."}, {"title": "Further Training & Compute Details", "content": "We searched through different values of the learning rate (LR) for GPT-2 training. Specifically, LR = {1e-06, 5e \u2013 06, 1e \u2013 05, 5\u0435 \u2013 05, 1e \u2013 04, 5e-04, 1e \u2013 03}. Through initial experiments, we found that LR = 1e-04 seemed to result in the best convergence behavior across the board, and used that for all our training experiments.\nOur experiments were run on varying GPUs. This included a single RTX 3090TI (24GB VRAM), up to eight A40s (48GB VRAM each), up to eight A100s (80GB VRAM each), and up to four H100s (80GB VRAM each). Training time varied by the type and number of GPUs and the particular experiment (e.g. number of repeated buckets)."}, {"title": "Zorro Evaluation Details", "content": "Zorro was inspired by BLiMP (Warstadt et al., 2020) and is a modification for child-directed language (e.g. lower vocabulary). However, it was designed specifically for masked language models such as ROBERTa. To adapt it to GPT-2, we reformatted the Zorro data to match the BLiMP format and used the BLiMP evaluation in the BabyLM evaluation suite since the difference is mainly just the data. Further, we use the full Zorro test suite and do not filter examples by vocabulary. Hence, our results are not comparable to Qin et al. (2024) who filter Zorro examples by training vocabulary.\nTo better match the training data format and assess the effects of speaker labels, we came up with three variations of Zorro: 1) the original Zorro sentences (used to assess BabyLM), 2) the sentences with a common CHILDES speaker label prepended (used to assess CHILDES), and 3) the sentences with a common TD speaker label prepended (used to assess TD). To further match the training data as shown in Appendix B, the speaker labels were surrounded by double asterisks, and sentences included double newline tokens (before and after). We used the mother speaker label (MOT) for CHILDES, and the child speaker label (Child) for TD (see Appendix B), as these were some of the most frequent speaker labels for each dataset respectively (see Table 9). Further, preliminary experiments showed that these particular labels worked best for assessing each model."}, {"title": "Further Experimental Motivation", "content": "If a dataset can be described as a concatenation of equal-sized buckets A, B, and C, the repeated bucket approach can be described as An \u0412\u043f \u0421\u043f. The inspiration for this approach is due to several reasons, other than being a compromise between standard iterated training and human learning (as discussed in Section 3.4). Firstly, the iterative approach (training across the entire dataset several times) can potentially wash away global ordering effects (especially when the epoch count is high) as"}, {"title": "Further Experimental Results", "content": "In addition to the experiments discussed in Section 4, we tried different values of n (number of times to repeat each bucket) for CHILDES and TD repeated buckets experiments. In particular, n = 3, 5, 10, 20. The chosen models for the repeated bucket experiments are the final models at the end of training.\nFor CHILDES, we also tried different values of b (number of buckets, or approximately equal sections to divide the dataset into) using the global age order. In particular, b = 3,5,10. We report average results for different values of n and b in Tables 10 and 11, respectively. We also compare the typical iterative training approach (20 epochs) to repeated buckets using n = 20 (analogous to 20 epochs). Results are in Table 12."}, {"title": "Importance of Speaker Labels", "content": "As an additional experiment, we also assess the importance of having speaker labels for each utterance."}, {"title": "Convergence Behavior of Models", "content": "We plot the convergence graphs (train and validation losses vs. epoch) for several sets of our experiments in Figures 2 to 8. For the repeated buckets experiments, we treat the entire training run as a single epoch. We can notice interesting patterns/trends in the convergence behavior of models depending on several factors including the global ordering and curricularization method.\nFrom Figure 2, we see that BabyLM converges to higher losses than CHILDES and TD, although it seems to perform better at test-time for syntax and semantics (as discussed in Section 4). Losses during training could simply be higher as the dataset is more complicated and varied since it is a mixture.\nFrom Figure 3, we can see that when we train using the typical iterative epochs approach, the training loss has a cyclical pattern using global age order and reverse order, while it converges smoothly for random order. From Figures 4 and 5, we see that when using the repeated buckets approach for both CHILDES and TD, global age order leads to a slowly cyclical increase in the training loss, while it generally decreases for reverse and random order. Throughout these experiments, while the training loss differs and individual buckets exhibit differing patterns, the high-level behavior of the validation loss and hence overall learning is similar. This aligns with the results we saw in Section 4.\nFrom Figures 6 and 7, we see that varying b and n result in minor changes in behavior for the training loss. Specifically, by increasing n, the training loss has a more clearly defined cyclical pattern, and the losses converge to lower values. This is expected, since increasing n is analogous to training on more epochs. From Figure 8, we see that local interventions \u2013 randomly shuffling utterances and removing speaker labels have minor effects on convergence behavior."}, {"title": "Licenses and Intended Use", "content": "We used all existing datasets and models for their intended use. GPT-2 is licensed under the Modified MIT License. CHILDES is made available under TalkBank which is governed by the Creative Commons CC BY-NC-SA 3.0 copyright license (see https://talkbank.org/share/rules.html). We plan to release the TinyDialogues dataset under the standard MIT license. Information about the BabyLM challenge and its dataset (which is a collection of portions of several sub-datasets) is at https://babylm.github.io/index.html."}, {"title": "Code & Data", "content": "We plan to publicly release all our code and data. Some of the code was written with the assistance of ChatGPT (specifically, GPT-4 and GPT-40)."}]}