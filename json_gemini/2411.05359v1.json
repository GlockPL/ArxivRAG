{"title": "Agricultural Landscape Understanding At Country-Scale", "authors": ["Radhika Dua", "Nikita Saxena", "Aditi Agarwal", "Alex Wilson", "Gaurav Singh", "Hoang Tran", "Ishan Deshpande", "Amandeep Kaur", "Gaurav Aggarwal", "Chandan Nath", "Arnab Basu", "Vishal Batchu", "Sharath Holla", "Bindiya Kurle", "Olana Missura", "Rahul Aggarwal", "Shubhika Garg", "Nishi Shah", "Avneet Singh", "Dinesh Tewari", "Agata Dondzik", "Bharat Adsul", "Milind Sohoni", "Asim Rama Praveen", "Aaryan Dangi", "Lisan Kadivar", "E Abhishek", "Niranjan Sudhansu", "Kamlakar Hattekar", "Sameer Datar", "Musty Krishna Chaithanya", "Anumas Ranjith Reddy", "Aashish Kumar", "Betala Laxmi Tirumala", "Alok Talekar"], "abstract": "Agricultural landscapes are quite complex, especially in the Global South where fields are smaller, and agricultural practices are more varied. In this paper we report on our progress in digitizing the agricultural landscape (natural and man-made) in our study region of India. We use high resolution imagery and a UNet style segmentation model to generate the first of its kind national-scale multi-class panoptic segmentation output. Through this work we have been able to identify individual fields across 151.7M hectares, and delineating key features such as water resources and vegetation. We share how this output was validated by our team and externally by downstream users, including some sample use cases that can lead to targeted data driven decision making. We believe this dataset will contribute towards digitizing agriculture by generating the foundational baselayer.", "sections": [{"title": "1 Introduction", "content": "The global food system is facing unprecedented challenges. In 2023, 2.4 billion people experienced moderate to severe food insecurity [1], a crisis precipitated by anthropogenic climate change and evolving dietary preferences. Furthermore, the food system itself significantly contributes to the climate crisis, with food loss and waste accounting for 2.4 gigatonnes of carbon dioxide equivalent emissions per year (GT CO2e/yr) [2], and the production, mismanagement, and misapplication of agricultural inputs such as fertilizers and manure generating an additional 2.5 GT CO2e/yr [3]. To sustain a projected global population of 9.6 billion by 2050, the Food and Agriculture Organization (FAO) estimates that food production must increase by at least 60% [1]. However, this also presents an opportunity: transitioning to sustainable agricultural practices can transform the sector from a net source of greenhouse gas emissions to a vital carbon sink. Meeting this escalating demand while mitigating environmental impact necessitates a fundamental paradigm shift in agricultural land management. While significant resources are already allocated to enhance food security globally, their efficacy is constrained by a lack of targeted allocation [4]. This paradigm shift necessitates the digitization of the agricultural sector, enabling precise monitoring and facilitating data-driven decision-making.\nThe initial step in this digitization is achieving a comprehensive understanding of agricultural landscapes. This is particularly critical in the Global South, where smallholder farms(cultivating less than two hectares of land) are prevalent, and the intricate mosaic of diverse land use, interspersed with vital water resources and natural vegetation, presents unique challenges for accurate mapping and monitoring. These landscapes are characterized by their complexity and dynamism, encompassing features such as cultivated land (e.g., fields, orchards), water resources (e.g., lakes, rivers, canals, ponds, wells), and vegetation (e.g., trees, shrubs, forests). Accurate identification and classification of these features are not only crucial for precise monitoring and targeted resource allocation but also lay the foundation for subsequent layers of digitization. This foundational layer enables the development of sophisticated analytical tools capable of monitoring crop types, spatial distribution, planting dates, and anticipated harvest times."}, {"title": "1.1 Core Contributions", "content": "This study significantly advances the field of agricultural land-use understanding by leveraging high-resolution satellite imagery and machine learning. Our model outputs, accessible via instructions at http://agri.withgoogle.com, provide granular insights into agricultural landscapes at a national scale, with a particular focus on smallholder farms. The core contributions of this work are:\nNational-Scale Mapping with Emphasis on Smallholder Farms This study represents the first comprehensive mapping of agricultural land use at a national scale with sufficient granularity to support targeted interventions, particularly for small-holder farms. We have successfully identified hundreds of millions of smallholder fields and millions of minor irrigation structures, previously unmapped and undocumented. This focus is critical, as smallholder farmers, cultivating less than two hectares of land, comprise 86.2% of India's agricultural workforce and manage approximately half of the total cultivated area [6].\nComprehensive Multi-class Segmentation Moving beyond traditional field boundary delineation, our approach incorporates the identification of trees and water bodies within agricultural landscapes. This multi-class segmentation, novel in the context of satellite-based land-use mapping, provides a nuanced understanding of land-use patterns crucial for diverse applications, including:\nWater Infrastructure Assessment Precisely locating ponds and wells enables detailed assessments of water access for individual farms and regions, facilitating the development of sustainable water management strategies.\nAgroforestry Analysis Mapping trees within agricultural settings allows for the analysis of agroforestry practices and their potential for carbon sequestration, contributing valuable data to climate mitigation efforts.\nRigorous On-Ground Validation To ensure the reliability and real-world applicability of our model, we conducted meticulous on-ground validation in collaboration with external partners. This validation involved comparing model outputs with in-situ surveys and aligning them with non-geo-referenced data. This rigorous validation process, exceeding the reliance on fixed test sets common in similar studies, strengthens our findings and bolsters confidence in the utility of our approach for informing agricultural interventions and policy decisions."}, {"title": "1.2 Related Work", "content": "The analysis of land-use patterns is fundamental to informing resource planning and allocation decisions, particularly in the context of food and water security [7]. Historically, land-use monitoring has progressed through three distinct phases: (a) labor-intensive manual land records [8-10], (b) smartphone-based data collection using tools like Open Data Kit (ODK 1.0) [11] and Ground [12], and (c) remote sensing techniques employing Unmanned Aerial Vehicles (UAVs) and satellites.\nRemote Sensing While manual and smartphone-based surveys remain valuable for localized assessments, they are inherently labor-intensive and time-consuming. Remote sensing methods, utilizing UAVs and satellites, have emerged as a more efficient and cost-effective solution for large-scale land-use monitoring. These technologies capture imagery across extensive areas and a broad spectrum of wavelengths, providing rich data for analysis. While satellite imagery may have lower spatial resolution compared to UAV imagery, its expansive coverage and historical data availability render it particularly well-suited for monitoring agricultural landscapes and informing land management initiatives.\nDeep Learning for Remote Sensing Given the complexity and volume of remote sensing data, deep learning approaches have been widely adopted for automated land-use analysis. Convolutional Neural Networks (CNNs), with their capacity to learn hierarchical representations from image data, have become the dominant architecture for segmentation tasks in remote sensing. The U-Net architecture [13], with its symmetric encoder-decoder structure and skip connections, has proven particularly effective in capturing multi-scale context, essential for handling the diverse object sizes encountered in remote sensing images. Our approach also employs a U-Net architecture, detailed in Sec. 2. To address the challenge of limited labeled data in remote sensing, we leverage transfer learning in conjunction with supervised learning to achieve effective semantic segmentation.\nNumerous studies have focused on delineating field boundaries from remote sensing data. However, most approaches utilizing publicly available datasets, such as Sentinel-2, operate at a 10-meter resolution [14, 15] which is insufficient for resolving smallholder farms that require sub-meter accuracy. Consequently, this study utilizes high-resolution satellite imagery. While some studies have explored super-resolution techniques [16], alternative satellite sources [17], or data fusion approaches [18] to achieve near sub-meter accuracy, their focus has often been limited to larger fields [19] or specific crop types [20]]. Other studies have addressed field delineation across diverse sizes and categories [21-24], but often for a single land-use class. In contrast, this work presents a novel approach by performing multi-class segmentation at high resolution, enabling the identification of not only field boundaries but also trees and water bodies within smallholder farming systems.\nLabeled Datasets and Temporal Context The development of labeled datasets for agricultural applications has progressed rapidly, with initiatives such as AI4SmallFarms [25], PASTIS [26], and Pula/Tetra Tech's Bird's Eye [27] providing valuable resources. However, these datasets often lack temporal context, limiting their utility for assessing the accuracy of field segmentation in dynamic agricultural environments. Our dataset distinguishes itself by explicitly incorporating timestamps with field segmentation outputs. This unique approach facilitates time-sensitive analysis, which is crucial in regions like India, where agricultural landscapes undergo rapid transformations due to seasonal variations, crop rotations, land-use modifications, and ownership changes. By capturing the temporal dynamics of agricultural fields, our dataset enables a more comprehensive understanding of land-use patterns and empowers stakeholders to make informed decisions."}, {"title": "2 Agricultural Landscape Understanding", "content": "This study addresses the challenge of Agricultural Landscape Understanding (ALU) by employing multi-class panoptic segmentation on high-resolution satellite imagery. Panoptic segmentation provides a rich representation of agricultural landscapes by classifying each pixel into semantic categories (e.g., fields, water bodies, trees) while simultaneously identifying individual instances within those categories (e.g., distinct fields, individual ponds). This granular understanding is crucial for informing agricultural management practices, resource allocation, and policy decisions.\nFig. 2 illustrates the overall workflow for our ALU system. Due to the expansive geographic extent of the study area (in this case, an entire country) and orbital constraints of satellites, a single satellite image cannot encompass the entire region of interest and multiple images acquired at different times (t\u2081 to tn) are required to achieve complete coverage (Fig. 2a). This temporal variation in image acquisition introduces the challenge of reconciling potentially disparate representations of the landscape, as weather patterns and seasonal phenomena (e.g., changes in crop stage) can significantly alter the appearance of agricultural features over time. Our system addresses this by systematically processing each image in the collection through our machine learning model, which performs multi-class panoptic segmentation to identify and delineate various agricultural features (Fig. 2c-e). To reconcile temporal variations, we rely on sufficient overlap between images acquired at different times. The resulting segmentation masks are then subjected to a series of post-processing steps, including vectorization, de-duplication, and merging, to generate a comprehensive and spatially accurate representation of the agricultural landscape, effectively merging information from overlapping images while accounting for temporal inconsistencies (Fig. 2f-h).\nTo ensure scalability and computational efficiency, the processing workflow is sharded by S2 cells [28] (Fig. 2b). S2 cells are a hierarchical spatial indexing system that subdivides the Earth's surface into a grid of cells, providing a convenient and efficient way to organize and access geospatial data. This spatial partitioning strategy allows for parallelization of the image processing and analysis, enabling the efficient handling of vast amounts of data required for national-scale land-use mapping."}, {"title": "2.2 Dataset Creation", "content": "Creating a high-quality annotated dataset is crucial for developing and evaluating machine learning models for agricultural land-use understanding, especially given the lack of standardized benchmarks at high resolution and lack of existing consensus on dataset labeling guidelines, which presents a bottleneck within agricultural remote sensing research. We have built a comprehensive dataset comprising high-resolution satellite imagery and corresponding pixel-level annotations."}, {"title": "2.2.1 Labeling Process", "content": "The labeling process involved the manual annotation of 1119 image tiles, each measuring 1500 x 1500 pixels (corresponding to a ground sampling distance of 30 cm and"}, {"title": "2.2.2 Dataset Preparation", "content": "To facilitate model training and evaluation at different spatial resolutions, we created two versions of the dataset: one with the original 30 cm resolution imagery and another with 1 m resolution imagery obtained through downsampling. Next we performed rasterization, converting the vector annotations into a raster format where each pixel is assigned a label.\nTo further align the data with the requirements of our model architecture (as explained in the following section), we generated multiple layers of labeled data to cater"}, {"title": "2.3 Task Description", "content": "As previously mentioned, we pose the overall problem of ALU as that of multi-class panoptic segmentation, where we must identify multiple (potentially overlapping) instances of various classes, along with labeling each instance with the appropriate class. Given an input image, the model must assign class labels and instance labels to each pixel, where an instance is defined by the collection of pixels sharing an instance label. For our problem setting, each pixel can have multiple class labels and instance labels, with some constraints. This is illustrated in Fig. 5\nConcretely, for each input image of shape (384, 384) which is obtained after the cropping of the (500,500) patches, we must produce a per-pixel output of the set of labels for each pixel of the form (class labels, instance labels)."}, {"title": "2.3.1 Classes and Layers", "content": "The dataset comprises the following classes: fields, trees/woodland, dug wells, farm ponds, other water bodies, opaque cloud, transparent cloud, background, and ignore. 'Background' denotes any pixel not belonging to the ground, well, tree, or cloud layers, while 'ignore' indicates masked regions excluded from the analysis. We define four distinct layers of entities: ground, well, tree, and cloud (as noted in Table 2). The rationale for establishing these layers is threefold:\nHandling Overlap A single pixel can belong to multiple instances across different layers (e.g., a tree on a field). The labeling scheme and model architecture must accommodate this potential overlap.\nComputational Efficiency and Physical Constraints By separating features into layers, we enforce physical constraints (e.g., a tree can overlap with a field, but a pond cannot overlap a field) and reduce the computational dimensionality of the problem.\nRepresenting Z-axis The layers conceptually represent the z-axis (height) of landscape features. Features at the same z-axis level (e.g., ground features like fields and ponds) belong to the same layer. Entities within a layer cannot overlap, but entities across layers can. The model must identify overlapping instances while adhering to these layer-specific constraints."}, {"title": "2.4 Machine Learning model", "content": "We adopt an affinity mask-based segmentation approach, as applied in [29]. This approach involves predicting pixel-pair affinities to identify instances, followed by assigning class labels based on semantic segmentation."}, {"title": "2.4.1 Model Architecture", "content": "Our model architecture, illustrated in Fig. 6, is based on U-Net [30] with a ResNet50 encoder that has been pre-trained on ImageNet. The U-Net architecture enables multi-scale feature extraction, producing outputs at resolutions of 1x, x, x, and x. At each scale, the model generates a semantic segmentation map (Si) and four affinity mask layers (A\u00bf layer for each layer). As depicted in Figure 5, the input image is processed by the encoder, and the resulting feature maps are passed to both the semantic and affinity branches. The semantic branch produces the semantic segmentation maps at different scales. The affinity branch generates the affinity masks, which are 256-dimensional vectors per pixel, defining the probability of neighboring pixels belonging to the same instance within a layer. These affinity masks (A\u00bf layer) are then converted to instances using a graph-based method [29], and each instance is assigned a class label based on the corresponding semantic segmentation map. Note that 'ground*' represents the combined set of ground classes in the ground layer. This multi-scale and multi-task learning approach allows the model to capture both semantic and instance-level information at various levels of detail, contributing to a comprehensive understanding of the agricultural landscape."}, {"title": "2.4.2 Training Objective", "content": "We use the same loss functions from [29], with a few modifications. Specifically, there are two types of loss functions used:\nSemantic segmentation loss The semantic segmentation map at each scale is trained using CE-Focal loss (Cross-Entropy Focal Loss), which combines softmax cross-entropy loss with focal loss [31].\nAffinity mask Each affinity mask is trained using instance-aware pixel-pair affinity hierarchically (as shown in Fig. 6). The pixel-pair affinity method computes the probability that two pixels belong to the same instance at multiple scales in a hierarchical manner. Hierarchical manner implies that the affinity is computed at multiple scales, starting with neighboring pixels and progressively considering larger distances. The loss is computed across affinity masks at all resolutions. The mathematical formulation for the loss function is same as [29] with some additional weights to the edge pixel. We add an extra weighting to pixels that lie at the edge of an instance for better boundary delineation. The loss functions are applied to outputs produced at all scales, giving the model feedback at multiple resolutions."}, {"title": "2.4.3 Model Selection Criteria", "content": "We use the mean instance-level Intersection over Union (IoU) average over each class to pick the best checkpoint in the training process. We evaluate the model on additional metrics which are discussed in Sec. 3"}, {"title": "2.5 Post Processing", "content": "The raw output from the model undergoes several post-processing steps to refine the segmentation results and generate the final land-use map. These steps are crucial for ensuring the accuracy, consistency, and usability of the data for downstream applications.\nVectorization The initial model output consists of pixel-based raster segmentation masks. These masks are vectorized to create polygon representations of the identified features. Vectorization provides a more compact and efficient representation of the land-use features, facilitating subsequent processing and analysis.\nDe-duplication Due to the nature of satellite image acquisition, multiple images may cover the same geographic area, resulting in overlapping and potentially redundant predictions. A de-duplication process is employed to reconcile these overlaps and ensure that each feature is represented uniquely in the final output. This process considers various criteria, such as the coverage area and the recency of the images, to prioritize the most reliable predictions. For the purpose of reconciling and avoiding edge artifacts data across shards needs to be observed holistically, limiting opportunities of parallelizing operations.\nSmoothing and Regularization The vectorized polygons may exhibit irregularities or artifacts due to limitations in the model's predictions or noise in the input imagery. To address this, a smoothing and regularization step is performed to refine the boundaries of the polygons, resulting in a more visually appealing and accurate representation of the land-use features. This step involves removing sharp angles or \"daggers\" (as indicated by the dashed polygons in Fig. 7) and applying smoothing algorithms to create more natural and realistic shapes.\nFeature Identification To enable efficient indexing and referencing of individual features, an identifier is assigned to each feature based on the Plus Code [32] of its centroid. Plus Codes are a geocoding system that provides short, alphanumeric codes for locations, offering a human-readable and easily shareable way to identify geographic coordinates. By using the Plus Code of the centroid, we can assign a mostly unique identifier to each feature, facilitating efficient data management and retrieval."}, {"title": "Spatial Indexing and Data Partitioning", "content": "To facilitate efficient data storage, retrieval, and analysis, the processed data is partitioned using S2 cells at level 13. This partitioning strategy allows users to query and retrieve data for specific regions of interest, facilitating targeted analysis and application. A level 13 S2 cell covers an area of approximately 1 km x 1 km and typically requires about 1 MB of storage. This granularity offers a good balance between data volume and spatial coverage, making it suitable for sharing data as API responses while providing sufficient information for various analyses and applications. The final output is stored in GeoJSON format, with S2 cells as the fundamental spatial unit. GeoJSON is an open standard format for representing simple geographical features and their non-spatial attributes using JavaScript Object Notation (JSON). This format is widely used in web-based mapping applications and geographic information systems due to its human-readability and compatibility with various software tools. This sharding strategy allows users to access data for specific regions by querying with S2 cell IDs, geographic coordinates, or region names. This enables flexible and targeted access to the rich information contained within the ALU output, facilitating a wide range of analyses and applications.\nIdentification and Exclusion of Non-Agricultural Areas To further enhance the utility of the data for agricultural applications, we identify and flag S2 cells that are unlikely to contribute significantly to agricultural production. These include areas classified as urban, desert, and hilly terrain, which typically exhibit low agricultural activity. However, recognizing that many S2 cells may only partially overlap with these non-agricultural regions, we apply a threshold-based exclusion criterion. This criterion ensures that only cells with a substantial portion of their area occupied by non-agricultural land are flagged, thereby preserving cells with mixed land use that may still contain agriculturally relevant features."}, {"title": "3 Model Evaluation", "content": "In this section, we first discuss different failures/ errors that may arise in segmentation tasks, followed by quantitative and qualitative evaluations of our model."}, {"title": "3.1 Segmentation Errors and Failure Types", "content": "Segmentation models can exhibit a variety of undesirable behaviours that may affect downstream use cases. Some of the important types of failures are:\nUnder-segmentation This occurs when one predicted instance covers multiple ground truth instances. Fig. 8a. presents an example where a single field is predicted when, in reality, there are multiple fields\nOver-segmentation This occurs when one ground truth instance is covered only when multiple predicted instances are combined. Fig. 8b. presents an example where multiple fields are predicted when, in reality, there is a single field.\nFalse negatives This occurs when the model fails to detect an instance. Fig. 8c. presents an example where a farm pond is missing in the segmentation output.\nFalse positives This occurs when the model predicts an instance where, in reality, there is none. Fig. 8d. presents an example where the model predicts a field over a railway track."}, {"title": "3.2 Metrics and Quantitative Evaluation", "content": "Panoptic segmentation models, such as the model we train in this work, are typically evaluated with a metric known as 'panoptic segmentation quality' [5]. This metric quantifies the degree of overlap between the predicted instances and the ground truth instances, along with the accuracy of classifying the type of each instance. This provides a single metric to characterize the overall performance of the model and has been widely used in literature [33], [34]. However, a single metric cannot capture all aspects of the model's behavior. Recognizing that different downstream applications might be sensitive to different kinds of prediction errors, we seek to comprehensively understand the performance of a segmentation model by using multiple metrics, each targeting a distinct failure mode discussed earlier.\nThe various metrics we use and our model's performance are discussed below. We evaluate our model on our labeled data with four metrics:\n1. mIoU (mean Intersection-over-Union) mIoU provides an aggregated evaluation that encompasses all kinds of errors in segmentation. We use a modified version of mIoU as we find it more suitable for the data we are dealing with. Our mIoU is computed as follows. We first match all predicted instances to ground truth instances per class. A predicted instance is considered to match with a ground truth instance if they both belong to the same class AND the overlap covers at least 10% of the ground truth instance. All predicted instances that match the same ground truth instance are merged together, and predicted instances that do not match any ground truth instance are ignored. We then compute the IoU of the merged predictions w.r.t the matching ground truth instances, and take the average over all ground truth instances per class. We allow multiple predictions to match with a"}, {"title": "Over segmentation Metric", "content": "We introduce a metric to quantify the degree of over-segmentation by our model, i.e. how frequently does our model break up one ground truth instance across multiple predicted instances. It is defined as:\n$Over-Seg Metric = \\frac{\\sum_{i=1}^{N}PRED\\_inst\\_count\\_per\\_GT\\_inst_{i}}{\\sum_{i=1}^{N} 1\\{PRED\\_inst\\_count\\_per\\_GT\\_inst_{i} >0\\}}$\nwhere $PRED\\_inst\\_count\\_per\\_GT\\_inst_{i}>0$ is the total number of predicted instances for which at least one GT instance is present, $PRED\\_inst\\_count\\_per\\_GT\\_inst_{i}$ is the number of unique overlapping inferred instances per i-th ground truth instance, If $PRED\\_inst\\_count\\_per\\_GT\\_inst_{i}>1$, then the instance is over-segmented."}, {"title": "Under segmentation Metric", "content": "We introduce a metric to quantify the degree of under-segmentation by our model, i.e. how frequently does our model cover up multiple ground truth instances with a single predicted instance. It is defined as:\n$Under-Seg Metric = \\frac{\\sum_{i=1}^{N}GT\\_inst\\_count\\_per\\_PRED\\_inst_{i}}{\\sum_{i=1}^{N} 1\\{GT\\_inst\\_count\\_per\\_PRED\\_inst_{i}>0\\}}$\n0 is the total number of\nwhere $GT\\_inst\\_count\\_per\\_PRED\\_inst_{i}>$ predicted instances for which at least one GT instance is present,\n$GT\\_inst\\_count\\_per\\_PRED\\_inst_{i}$ i is the number of unique overlapping ground truth instances per i-th inferred instance. If $GT\\_inst\\_count\\_per\\_PRED\\_inst_{i}>1$, then the instance is under-segmented."}, {"title": "3.2.1 Quantitative Results", "content": "As shown in Table 3, our model achieves strong performance on the majority class (Fields exhibited mean IOU = 0.68). However, performance is significantly lower for Ponds & Wells (mean IOU = 0.05 and 0.03 respectively), likely due to their severe under-representation in the training data."}, {"title": "3.3 On Ground Validation", "content": "Our partners have performed rigorous on ground validation with this dataset.\nAcross 10 pilot villages in Maharashtra, our partners at IIT Bombay and Department of Land Record (DoLR), Government of Maharashtra, were able to demonstrate use of our dataset in digitizing land records by reconciling dated and non-georeferenced paper maps with our satellite observations in a mostly automated way. The algorithm develop by IIT Bombay based on our dataset and paper maps provided by DoLR generated outputs which were able to satisfy the legal requirements set by DoLR. More details can be found in Sec. C.1\nIn 19 villages in Telangana, in a partnership with the Department of Agriculture of the state Government of Telangana and a local startup, TeamUp, the field level dataset achieved an accuracy of 82%. More details can be found in Sec. C.2."}, {"title": "3.4 Aggregate Analysis & Geo-spatial Distribution", "content": "Fig. 9 displays a comparative analysis between [6] and model outputs, excluding non-agricultural regions. We observe that our model's performance varies with variance in terrain and geographical features. Our model demonstrates comparable performance in regions characterized by plain topography, such as Bihar, Gujarat, Jharkhand, and Punjab. However, we observe some divergence in hilly terrains and coastal regions, such as Kerala, Nagaland, Tamil Nadu, Himachal Pradesh, and Uttarakhand.\nDetailed tabular data containing state-wise counts and areas of landscape features are present in Sec. B."}, {"title": "4 Conclusion", "content": "This work demonstrates the feasibility of mapping agricultural landscapes at a national scale with high granularity, even in smallholder-dominated regions. Our approach, based on multi-class panoptic segmentation of high-resolution satellite imagery, allows for the identification and delineation of diverse agricultural features, including fields, trees, and water bodies. This detailed mapping provides valuable insights into land-use patterns, water resource availability, and agroforestry practices, which are crucial for informing sustainable agricultural interventions and policy decisions. We have also contributed to the research by developing comprehensive annotation guidelines for this domain.\nWe present high-resolution satellite imagery-derived segmentation outputs for various agricultural landscape features at a national scale, which have been extensively verified for downstream use cases by our partners (See Sec. C. This demonstrates the potential for general landscape understanding at a sub-global scale, with future work aimed at global expansion.\nHowever, our approach has limitations. We were able to incorporate locally relevant landscape features like farm ponds, which are likely less relevant globally. A good general global landscape segmentation model should be able to expand vocabulary to locally relevant features. The class imbalance in the dataset poses challenges for accurately identifying less frequent features like wells and ponds. Lack of locally relevant feature vocabulary and corresponding annotated labels limits our ability to scale and handle distribution shifts with ease, given the degree of supervision needed for the model. Our current approach is also limited by the availability of fresh high-resolution imagery. Landscape features (especially field boundaries) can (and do) change each agricultural season, and an ideal approach is capable of identifying appropriate field boundaries in-season.\nFuture work will focus on addressing these limitations and further improving the model's accuracy and generalization capabilities. By harnessing the power of remote sensing and machine learning, we can unlock valuable insights to support sustainable and resilient agricultural systems worldwide."}, {"title": "Appendix A Annotation Guidelines", "content": "To facilitate accurate and consistent labeling of agricultural features by annotators, we established a comprehensive annotation protocol (see Figure B1). Recognizing the variability in class definitions across different implementations, we aimed to create a standardized benchmark for the following classes we annotated. These guidelines were validated by agricultural experts and partners.\nFor ambiguous cases, such as differentiating barren farm ponds from fields, we provided detailed morphological criteria. For instance, a 'farm pond' was defined as exhibiting clear depth along its edges, even when dry. Similarly, 'wells' were distinguished from 'tree shadows' based on their regular, often circular shape. Annotators were instructed to encompass entire clusters when labeling features like 'trees,' rather than individual instances, to capture the spatial extent of these features accurately. Additionally, we permitted overlapping polygons to reflect the potential overlap of ground, tree, and well layers in the real world.\nThis annotation protocol serves several purposes beneficial to the community:\n\u2022 Standardization: It provides a common framework for defining and labeling agricultural features, promoting consistency across different research projects and datasets.\n\u2022 Reproducibility: The explicit guidelines enable other researchers to replicate our annotation process, enhancing the reproducibility of our findings.\n\u2022 Benchmark: The resulting dataset can serve as a benchmark for evaluating the performance of machine learning models in agricultural landscape classification tasks.\nTo ensure annotation quality, we ran pilots to train and calibrate annotators. We also requested annotators to proceed to the next image after spending one hour on an image."}, {"title": "Appendix B Aggregate Analysis & Geo-spatial Distribution", "content": "In Table B1 and Table B2, we provide quantitative results as counts of different features and their respective areas. Certain features like trees are not counted because we only identify separated clusters of trees and not individual trees. Table B3 and Table B4 represent the counts of different features and respective areas after excluding non agricultural regions such as mountains, desserts and urban land use."}, {"title": "Appendix C External Evaluations and Applications", "content": "Land records play a crucial role in maintaining property ownership and land use information, with implications for legal, economic, and societal activities. The transition from traditional paper-based records to digital solutions using Geographic Information System (GIS) technology is an important step and has led to great efficiencies, especially in efficient dispute management and land transfer. This appendix describes a pilot project done jointly by IIT Bombay and th Department of Land Records, Govt. of Maharashtra. It utilizes Google ALU outputs, hereby termed as \"Google farm plots\", as a key input in the overall modernization of land records in the state of Maharashtra within India."}, {"title": "C.1.2 Geo-referencing and Data Mismatch", "content": "Maharashtra, with an area of about 300,000 sq.km. has roughly 44,000 villages, each with an area between 5-20 sq. km. Each village consists of about 50-300 survey plots. The maps for these villages began as paper or cloth maps stored at the regional land records office. These were eventually digitized and stored as a drawing without geo-referencing (See Figure C2).\nAn important first step was geo-referencing, i.e., the process of importing these drawings into a GIS system, and subsequently, scaling and translating these maps to align with ground reality, to obtain, what we call as Survey Maps M\u00ba. This was achieved by the state agencies over the last decade. However, significant mismatches were often observed between the survey maps and agricultural field boundaries (as visible in Figure C3). This is due to many reasons limitations of historical tools, changes in"}, {"title": "Mathematical Formulation: Two Optimization Problems", "content": "Matchfit takes a collection of survey plots as a set of polygons $M = \\{Q1,...,Qk\\}$, and the Google farm plots for the region as a collection of polygons $F = \\{P1,...,Ps\\}$, as the inputs. It is designed as two sequential optimization problems on finding suitable geometric transformations to minimize two related error functions.\nWe now come to the first error function $e_{a}(M, F)$ which quantifies the mismatch between the family M and F. This is an aggregate of errors $e_{a}(Q, F)$ for individual survey plots $Q \u2208 M$ and F. Heuristically, it is clear that all farm area seen to protrude into a survey plot, or extrude outside, constitutes an error (i.e., a mismatch which is to be minimized). To formalize this notion, we define the excess area $e_{a}(Q, F)$ as"}, {"title": "C.1.4 Description of MatchFit", "content": "MatchFit proceeds in two steps, called JitterFit and SplineFit. JitterFit takes the original survey map M\u00ba, and F as input and uses the group T of all translations, rotations and scaling as the allowed transformations. For any $t \u2208 T$, let $M^{t}$ denote the survey plots obtained by applying t to M\u00ba. JitterFit minimizes $e_{a}(M^{t}, F)$ to obtained the the map $M^{1} = \\{Q_{1},...,Q_{k}\\}$. This map is thus an optimal geo-referencing of the input map M\u00ba. Note that scaling is also allowed since the original paper maps did not record aggregate distances.\nThe second optimization is more tricky and is motivated by the field observation provided to us by the agency. Most villages have regions of good and bad match and the identification of such regions would help in the eventual reconciliation. SplineFit is designed around this observation. It proceeds in three steps.\n1. JitterFit each polygon $Q^{1} \u2208 M^{1}$ independently to minimize the dtb-metric.\n2. Select Anchor Plots A\u2282M.\n3. Compute interpolating spline-fit transformation and apply to $M^{1}$ to obtain final map M\u00b2.\nIn the first step, individual polygons Q of M\u00b9, are unhooked and JitterFitted to minimize dtb((Q), F) to obtain Q. The transformations allowed is the same set T above.\nNext, of the set $\\{Q_{1},\u2026\u2026\u2026,Q_{k}\\}$, an index set I is chosen so that the set A = M"}]}