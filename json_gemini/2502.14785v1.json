{"title": "Real-Time Device Reach Forecasting Using HLL and MinHash Data Sketches", "authors": ["Chandrashekar Muniyappa", "Kendall Willets", "Sriraman Krishnamoorthy"], "abstract": "Predicting the right number of TVs (Device Reach) in real-time based on a user-specified targeting attributes is imperative for running multi-million dollar ADs business. The traditional approach of SQL queries to join billions of records across multiple targeting dimensions is extremely slow. As a workaround, many applications will have an offline process to crunch these numbers and present the results after many hours. In our case, the solution was an offline process taking 24 hours to onboard a customer resulting in a potential loss of business. To solve this problem, we have built a new real-time prediction system using MinHash and HyperLogLog (HLL) data sketches to compute the device reach at runtime when a user makes a request. However, existing MinHash implementations do not solve the complex problem of multilevel aggregation and intersection. This work will show how we have solved this problem, in addition, we have improved MinHash algorithm to run 4 times faster using Single Instruction Multiple Data (SIMD) vectorized operations for high speed and accuracy with constant space to process billions of records. Finally, by experiments, we prove that the results are as accurate as traditional offline prediction system with an acceptable error rate of 5%.", "sections": [{"title": "I. INTRODUCTION", "content": "In the Ad business, advertisers will configure the targeting attributes like device type and year, programs and channels watched, user country, age and language preferences to name a few, at both placement and creative levels as include (only include the population that satisfies the selected targeting dimensions) and exclude (consider the population that compliments the selected targeting dimensions) criteria [14] and would like to know how many audiences they can reach. In Table I, one can see the targeting dimensions that are currently supported in the system, each dimension consists of at least 20 attributes. These are unique record counts only the actual raw dataset size is at least 5 times larger than these counts.\nTraditional SQL joins on billions of records across multiple targeting dimensions will take a lot of time to compute the results. Therefore, in the current system predicting device reach is an offline process, where advertisers have to wait for 24 hours for the algorithms to complete and then make a decision, if they decide to make any changes then again, they have to wait for another 24 hours to see the effect of changes. This has adverse effects on business and revenue. To solve this crucial problem, we have built this new real-time forecasting system, which will provide real-time estimates to users as and when they modify the targeting attributes and help them in making the business decisions instantaneously.\nForecasting plays a pivotal role in the Ad booking system as advertisers and publishers will make the budget decisions and sign the contract based on the forecasting numbers. Hence, any kind of over or under predictions will result in a revenue"}, {"title": "II. BACKGROUND", "content": "Prediction systems can be developed in multiple ways using AI and statistical modelling techniques; however, they are complex, difficult to implement and maintain, and error-prone [1, 2, 3]. On the contrary, there is a family of stochastic algorithms that can be used for forecasting [4], they are gaining popularity [15] as they are fast, efficient, and have permissible error rates. In this work, we will show how HLL and MinHash algorithms can be used to forecast the device reach. This technique works on any dataset and the solution can be easily extended to build a forecasting system in any domain.\nHyperLogLog [5] \u2013 HLL is mainly used to estimate the cardinality of a stream with constant space. We can adjust the size of memory to keep the error-rate less than 2%. Furthermore, the algorithm can be modified for better performance and accuracy [6]. However, by design HLL can be used to determine only the cardinality of a population, to obtain the intersection one can use inclusion-exclusion principle [7] but it is a cumbersome process and has a high error rate [4, 8, 9]. To overcome these problems, MinHash is used to find the intersection.\nMinHash [10] is based on Locality Sensitive Hashing mainly used to determine how similar the given sets are based on the Jaccard similarity. Many latest DBs [15] provide these algorithms as a built-in feature, but the problem is they do not expose signatures that can be broadcasted over a network, so that, the signatures can be held in memory of an online application. There are open-source libraries [16] to solve this problem. However, none of the existing solutions support multilevel or nested set aggregation feature which was crucial for our use case. This work will show how we have solved this problem. The contributions of this work are as follows:\n\u2022 Improved the MinHash algorithm to run 4 times faster by applying SIMD vectorized operations [17].\n\u2022 We have enhanced the open-source library [16] to support multilevel set aggregation, so that, the MinHash algorithm can work on both original MinHash signatures and also on the intermediate MinHash signatures representing the Jaccard similarity between sets.\nIn our discussion, set and subsets represent Hypercube records [11]. Union and Intersection refers to the HLL cardinality and MinHash intersection (Jaccard Similarity) functions. The HLL and MinHash data sketches are custom MapReduce UDAFS (User Defined Aggregate Functions) installed on Vertica columnar DB [12]."}, {"title": "III. APPROACH", "content": "The work flow has the following two main steps:\n\u2022 Generating the sketches\n\u2022 Querying the sketches\nA. Generating the Sketches\nApply Extraction, Transformation, and Loading (ETL) techniques to transform the data as per the business requirements. This is the first step in the estimation process, as part of this step, targeting dimensions will be fetched from S3 at scheduled intervals and will be transformed based on the business rules using spark jobs [13]. At the end of this step, the data stored on S3 will be loaded into Vertica DB temporary tables, an example of a simplified version of the Device Profile dataset is shown in the Table II.\nNext step is to classify or aggregate the data into different subsets, using HLL and MinHash data sketches. We will partition a set (Targeting dimension) containing billions of records into many subsets, based on the business rules of a given set. The Pseudo Identifier - PSID unique device ID (64-bit hash, MAC address) of a TV for each record in a subset will be hashed using HLL and MinHash sketches to reduce entire subset into a single aggregated record. We will compute include and exclude sets signatures for each subset, where an include signature will give the population of the devices which belong to the subset and exclude signature will give the complement of a given include-set. The process will be repeated for each of the subsets and the final aggregated set (Hypercube) will have one record (base cuboid) for each subset with include and exclude HLL and MinHash hash values. The hypercubes generated like this will be stored in the Vertica analytics DB. Please find the example Table III, which demonstrates this process, for the input Table II from the previous step.\nTo compute the include-set signatures (HLL and MinHash columns) for a given targeting dimension, we will aggregate a targeting dimension into subsets based on the given set of attributes using SQL group by operator and aggregate the PSIDs in each subset by calling the HLL and MinHash"}, {"title": "B. Querying the Sketches", "content": "The advertiser will set up a campaign with the preferred targeting dimensions at placement and creative levels and submit a request to get an estimate, once he is satisfied with the setup, the placement will be approved to deliver. The Fig(2) in the appendix section shows an approved live placement delivering the Ads, Fig(3) shows placement level targetings and a creative assigned to it, Fig(4) shows the details of a creative, and finally, Fig(5) shows all the targeting dimensions that can be added at placement and creative levels.\nUpon receiving the request, the server will translate it into a dynamic SQL query to query the include and exclude HLL and MinHash sketches of the given targeting dimension hypercubes. A placement can have multiple targetings and creatives assigned with both include and exclude criteria. To compute the estimation, first similarity is measured between all the placement level targetings and then the similarity between the creative level targetings are measured, if there is more than one creative then the similarity measurements of all the creatives are unioned to form one single aggregated creative level similarity measurement. Finally, the similarity between the two intermediate signatures is measured. For example, if P1 is the Placement, with Creatives C1, C2, C3 ... CN and has the targeting attributes T1, T2, T3, ... TN, and say each of the Creatives have targeting attributes CT1, CT2, CT3, ...CTN assigned then we compute the estimation as follows.\nSelect hllest(hllagg(hll or exhll)) as cardinality * mhjaccard(mhagg(minhash or exminhash)) as jaccardRatio FROM\n((P1(T1 T20 ... \u2229 TN )))\n((C1(CT1CT2 \u2229 ... \u2229 CTN))\n((C2(CT1CT2 \u2229 ... \u2229 CTN))...\n((CN(CT10 CT2 \u2229 ... \u2229 CTN ))))\nwhere hllest, hllagg, mhagg, and mhjaccard are UDAFs developed using C programming language and installed on a Vertica DB. The above computation can be pictorially represented as shown in the Fig (1).\nExisting MinHash algorithm implementations does not support multilevel aggregation or intersection. They will only accept first level signatures and not the intermediate signature of the common elements across sets as shown in Fig (1). \u03a4\u03bf solve this problem, first, we need to capture the signature of common elements (Jaccard similarity signature) across sets this is done as shown in the code listing 1 in the appendix section. The function is registered as mhjaccard UDAF in Vertica DB, once the function completes we can save the Jaccard signatures in a DB as shown in Table III. The final Jaccard signatures are used to compute the Jaccard ratio, by counting the number of bits set in the Jaccard bitmask and dividing it by the number of bins (length of the bitmask)."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The Table V shows the device reach forecasted and the time taken for a single Placement with different combinations of include and exclude criteria at Placement and Creative level targetings.\nWith this new approach, the system is forecasting device reach in few seconds when compared to the existing system which takes 24 hours. As the predictions are real-time, business is now able to make the decisions instantaneously and sign the contracts without having to wait for 24 hours, resulting in, doubling the revenue and expanding the business to new countries.\nTo measure the accuracy of the forecasted values, we can compute the relative error rate as follows:\n(|Truevalue-Observedvalue| / Truevalue) * 100.\nWe ran the experiments on several data points in the production environment and computed the accuracy the error rates were less than 5%. A sample of this is shown in Table VI."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "As shown in this work, we have enhanced the MinHash algorithm implementation to run 4 times faster and to handle multilevel aggregations, and have successfully applied to forecast the device reach in real-time with error-rate less than 5%. Now, the business can make immediate decisions and onboard new advertisers for higher revenue growth. However, this approach works only with non-seasonal data, as part of the future work one can enhance the algorithm to work with seasonal data by adding seasonality factor to the objective function."}, {"title": "APPENDIX: MULTILEVEL SET AGGREGATION C CODE", "content": "void mh_jaccard (inputMinash) {\nif(bins is empty){\ninit(bins, inputMinash)\n} else{\nm128i const a=_mm_load_si128 (inputMinhash); _m128i const b=_m_load_si128(bins);\n_m128i const eq = _mm_cmeq_epi32(a,b);\n_m128i const common =_mm_store_si128(bins,eq);\nuint32_t *ucommon = (uint32_t *)&common;\nuint32_t int_values[4]={0,0,0,0};\nuint32_t *pa =((uint32_t *) inputMinash.data())+4*i;\nuint32_t *pb = ((uint32_t *) bins.data()) + 4*i;\nfor(int j = 0; j < 4; j++){\nif( ucommon[j] == -1){\nif(pb[j] == 0) {\n}\n}}\nubins[j] = 0;\nint_values[j]=pb[j];\n_mm_store_si128(_m + i, cum);\n_mm_store_si128(_mIntersect +\ni,\n_mm_set_epi32(int_values[3], int_values[2],int_values[1],\nint_values[0])); }"}]}