{"title": "EMBODIED RED TEAMING FOR AUDITING ROBOTIC\nFOUNDATION MODELS", "authors": ["Sathwik Karnik", "Zhang-Wei Hong", "Nishant Abhangi", "Yen-Chen Lin", "Tsun-Hsuan Wang", "Pulkit Agrawal"], "abstract": "Language-conditioned robot models (i.e., robotic foundation models) enable\nrobots to perform a wide range of tasks based on natural language instructions.\nDespite strong performance on existing benchmarks, evaluating the safety and\neffectiveness of these models is challenging due to the complexity of testing all\npossible language variations. Current benchmarks have two key limitations: they\nrely on a limited set of human-generated instructions, missing many challenging\ncases, and they focus only on task performance without assessing safety, such as\navoiding damage. To address these gaps, we introduce Embodied Red Teaming\n(ERT), a new evaluation method that generates diverse and challenging instruc-\ntions to test these models. ERT uses automated red teaming techniques with Vi-\nsion Language Models (VLMs) to create contextually grounded, difficult instruc-\ntions. Experimental results show that state-of-the-art models frequently fail or be-\nhave unsafely on ERT tests, underscoring the shortcomings of current benchmarks\nin evaluating real-world performance and safety. Code and videos are available at:\nhttps://sites.google.com/view/embodiedredteam.", "sections": [{"title": "1 INTRODUCTION", "content": "Commanding robots through natural language has been a long-standing goal in robotics. Recent ad-\nvances in foundational robot models (Ke et al., 2024; Wu et al., 2023; Chen et al., 2023; Brohan et al.,\n2022; 2023; Parakh et al., 2024; Ajay et al., 2024) trained on large-scale internet text have shown\npromising results in enabling robots to follow language instructions and generalize to ones not seen\nduring training. However, despite near-perfect performance on existing benchmarks (Mees et al.,\n2022; James et al., 2020), these benchmarks often lack diversity in instruction phrasing and miss\nmany commands real users might give. For example, we found that a model successfully executes\n\"Close the drawer\" but fails with \"Grip the handle gently and pull it,\" even though both describe the\nsame task (Section 4.1). Therefore, evaluation results on the existing benchmark may overestimate\nthe models' true performance in real-world scenarios where user instructions may differ from the\nexisting benchmarking instructions.\nTo effectively deploy foundational robot models in the real world, we need robust benchmarking\nmethods that capture the diverse instructions real users might give. We propose using red team-\ning-testing the robot model with instructions specifically designed to challenge its capabilities.\nThese instructions are crafted to be both natural and representative of the plausible ways users might\nexpress their intentions. Thus, a model that truly generalizes should maintain high performance\non these challenging instructions, whereas a drop in success rate would signal over-fitting to the\ninstructions in the existing benchmark.\nHow can we effectively red team language-conditioned robot models to uncover instructions that\nchallenge their capabilities? First, the instructions must be feasible for the robot in its real-world\nenvironment. For instance, asking a stationary manipulator to fly is unreasonable. Second, the\ninstructions should expose the model's weaknesses by leading it to fail at completing valid tasks.\nWhile human annotators could generate such instructions, this approach is costly and difficult to\nscale."}, {"title": "2 PRELIMINARIES", "content": "We define the robot policy being audited as \\( \\pi \\) that takes high-dimensional sensory observations (e.g.,\nimages) o and natural language instructions c as inputs and outputs actions \\( \\alpha = \\pi(o, c) \\) to accom-\nplish the task described by c. The common paradigm for training \\( \\pi \\) is to fine-tune pre-trained vision\nand language models for a set of sensorimotor tasks. As robots' policies are derived from vision and\nlanguage models pre-trained on large amounts of data, it is expected that the robots' policies will\ngeneralize to a wide range of natural language instructions, visual scenarios, and even new tasks be-\nyond the training set. For instance, a language-conditioned robot model should successfully execute\nthe command \"Shut off the LED light,\" even if it was only trained on instructions like \"Turn off the\nlight.\""}, {"title": "3 EMBODIED RED TEAMING", "content": "Goal: The goal of embodied red teaming (ERT) for robot policies is finding a diverse set of test\ninstructions \\( \\{c_1, \\ldots, c_N\\} \\) causing the robot policy to fail, where N is the instruction set size. These\ninstructions must represent feasible tasks in the current environment and cause the robot to fail.\nWhy must the instructions be diverse and feasible? Diversity ensures comprehensive auditing\nof robot's performance, avoiding gaps that a narrow set of tests might leave. Feasibility ensures\nthat the robot is being tested on tasks it is expected to perform, excluding unrealistic or irrelevant\ninstructions.\nFormulation: We formulate red teaming as the following optimization problem:\n\\[\\min_{\\{c_i | c_i \\in \\text{FEASIBLESET} \\forall i\\}} \\sum_{i=1}^{N} R(\\pi, c_i) - \\text{Div}(\\{c_1, \\ldots, c_N\\}),\\]\nwhere R(\\(\\pi\\), c\u1d62) measures the robot's performance on instruction c\u012f, and Div quantifies the diversity\nof the selected instructions. FEASIBLESET defines the conditions that make an instruction feasible,\nwhich are detailed below."}, {"title": "3.1 IMPLEMENTATION: ITERATIVE IN-CONTEXT REFINEMENT", "content": "To optimize the objective in Equation 1, we implement ERT using vision-language models (VLMs),\nsince the FEASIBLESET includes both images and text. We refer to the VLM used for generating\ninstructions in ERT as the \u201cred-team VLM.\u201d In FEASIBLESET, we provide an image of the environ-\nment-indicating the entities and objects available to the robot\u2014and a task description in language\nto specify the tasks for which ERT should generate instructions. Note that we did not explicitly de-\nscribe the objects and entities to the red-team VLM but just provided the image of the environment.\nOverview: Since VLMs lack prior knowledge about which instructions might lead the robot policy\nto fail, ERT refines instructions iteratively based on the robot policy's execution results on the gen-"}, {"title": "4 EXPERIMENTS", "content": "We evaluate ERT's performance at discovering instructions that elicit failures or unsafe robotic\nbehaviors.\nSimulated environments: We evaluate ERT on the widely used CALVIN (Mees et al., 2022) and\nRLBench (James et al., 2020) benchmarks. The CALVIN benchmark includes 27 distinct tasks\nand 400 crowd-sourced natural language instructions, with approximately 15 instructions per task.\nRLBench consists of 18 tasks with 3 to 6 instructions per task. The robot policy is provided with\na natural language instruction c, observation of the environment from cameras o mounted on the\ngripper and above the robot. We evaluate the robot policies by rolling out trajectories using them\nfrom various initial states using, such as different object layouts. Robot performance R(\\(\\pi\\),c) for a given instruction c is measured by\nthe success rate based on the task-specific criteria defined by the benchmark. For example, in the\n\"close the drawer\" task, the robot is instructed with a prompt such as \"Hold the drawer handle and\nshut it\". Success is determined by whether the drawer is closed.\nLanguage-conditioned robot policies: We tested\nthe ERT-generated instructions on two state-of-the-art\nlanguage-conditioned robot policies, GR-1 Wu et al.\n(2023) and 3D-Diffuser Ke et al. (2024), using their pub-\nlicly available pre-trained checkpoints. Both models are\nfine-tuned on expert demonstrations. GR-1 is a trans-\nformer model pre-trained on large-scale video dataset\nwith language instructions. Note that we did not eval-\nuate GR-1 on RLBench, as models for RLBench were\nnot released. 3D-Diffuser is a diffusion policy model that\ntakes images and language embeddings as inputs, with\ninstructions processed by pre-trained CLIP models (Rad-\nford et al., 2021). Both GR-1 and 3D-Diffuser achieved\nhigh success rates (around 90%) on the CALVIN bench-\nmark."}, {"title": "4.1 LANGUAGE-CONDITIONED ROBOTS\nSTRUGGLE WITH INSTRUCTION GENERALIZATION", "content": "Setup: We evaluate the performance of GR-1 and 3D-\nDiffuser on several instruction sets: those from the CALVIN benchmark and RLBench environ-\nments (denoted as Training), rephrased versions of these instructions (denoted as Rephrase), and\ninstructions generated by ERT. The rephrased instructions are generated by providing the original\nbenchmark instructions as example inputs to the red team VLM. We consider rephrased instructions\nas a naive method for generating new instructions beyond the existing benchmarks, serving as a\nbaseline to highlight ERT's effectiveness in discovering instructions that cause robot failures.\nIn our experiments, we make the instructions being tested similar to the number of instructions in the\noriginal training instruction set for each robot policy to ensure a fair comparison on the performance\n(success rate) of the robot's policy. For the CALVIN environment, we prompt ERT to generate 10\ninstructions for each of 27 tasks, totaling 270 instructions. For 3D-Diffuser, we perform iterative\nrefinement, as described in Algorithm 1. For GR-1, we instead re-use the instructions found from\nperforming ERT on 3D-Diffuser for testing GR-1, aiming to see if instructions causing one model\nto fail also lead to failure in the other. For RLBench, we generate 3 to 6 instructions for each of\n18 tasks using ERT. All generated instructions are provided in Appendix A.4 for CALVIN and\nAppendix A.5 for RLBench.\nMetric: The models are evaluated for each instruction across all initial states defined by the bench-\nmarks. The initial states vary across the initial configuration of the robot and the initial positions of\nobjects in the scene. The success rate of a robot policy \\( \\pi \\) on instruction c is denoted as R(\\(\\pi\\), c). The\nmodel's overall performance on an instruction set \\( \\{c_1, \\ldots, c_N\\} \\) is the average success rate across all"}, {"title": "4.2 LARGE MODELS STRUGGLE AT INSTRUCTION GENERALIZATION", "content": "Setup: We assess generalization of recent large vision-language-action (VLA) models for robotic\nmanipulation (Brohan et al., 2022; 2023; Team et al., 2024). Unlike GR-1 and 3D-Diffuser (Sec-\ntion 4.1), these VLAs are trained on real robot manipulation videos. We select OpenVLA (Kim et al.,\n2024) for its state-of-the-art performance and open-source availability. OpenVLA employs a Vision\nTransformer (ViT) (Dosovitskiy, 2020) for visual features and a LLaMA2-7B model (Touvron et al.,\n2023) for language instructions, converting both into tokens. With approximately 7 billion param-\neters-significantly larger than the 200 million parameters of 3D-Diffuser and GR-1-we expect\nOpenVLA to generalize better across instructions. The LLaMA2-7B model is trained to predict\naction tokens, such as gripper movements. We evaluate OpenVLA in the simulated environment\nSimplerEnv\u00b9 (Li et al., 2024), which replicates many scenes similar to its training videos. The task\ninvolves a robot using an overhead camera to pick up a Coke can from a table. We only evaluate\nOpenVLA on SimplerEnv because it only accepts a single image, whereas CALVIN (Mees et al.,\n2022) and RLBench (James et al., 2020) feed multiple images from different angles to the robot.\nMetric: The metric R(\\(\\pi\\),c) (Equation 1) is set as a success indicator for picking up a Coke can.\nOpenVLA is tested on 25 initial states, each with a unique object position. For each initial state,\nERT generates 4 instructions, totaling 100 instructions. Instructions are generated differently for\neach initial state because ERT may craft initial state-specific instructions (e.g., based on object\narrangement). We measure the robot's success rate across these 100 instruction-state pairs. We run\nERT with 5 different random seeds and report the success rate with its 95% confidence interval.\nWe also compare the robot's performance on ERT-generated instructions to its performance on the\noriginal instruction provided in SimplerEnv. Note that this benchmark provides only one instruction\nper task. For the task of picking up the Coke can, this instruction is: \"Pick the opened Coke can.\"\nThe robot's success rate on the original instruction is measured across the same 25 initial states."}, {"title": "4.3 SAFETY OF LANGUAGE-CONDITIONED ROBOTS IS OVERLOOKED", "content": "We emphasize that safety is a significant concern for language-conditioned robot policy\u2014a fac-\ntor often overlooked in existing benchmarks that focus solely on success rates. Safety considera-\ntions such as preventing environmental damage, avoiding harm to humans, and preventing self-\ndamage-are critical because these robots interact with humans through natural language instruc-\ntions. In this experiment, we consider unsafe behavior as causing objects to fall from the table. We\npresent examples in Figure 4, as generated with instructions described in Section A.3, and sum-\nmarize our findings below. In Figure 4, we illustrate 4 timesteps from episodes in which the robot\nexhibits unsafe behavior across 4 instructions. These unsafe behaviors result in one or more objects\nfalling off the table.\nRobots follow unsafe instructions. In Figures 4a and 4b, the robots successfully complete tasks\nthat are inherently unsafe-such as causing objects to fall off and destabilizing them. This suggests\nthe need for alignment training, similar to that used in large language models (LLMs), to ensure\nrobots align with human preferences and refuse to perform harmful or unsafe actions that could\ndamage the environment."}, {"title": "4.4 ANALYSIS OF FAILURE MODES", "content": "To examine the failure modes of these robot models, we have selected ERT-generated instructions\nwith the lowest success rates from each task and summarize our findings\nbelow:\nComplex Instructions. Complex or multi-step instructions often lead to task failures. For instance,\nin the close_drawer task (1.05% success), the directive to \"check for obstructions, then apply\nforce to close the drawer snugly\" introduces multiple actions, namely (1) \"check for obstructions\",\n(2) \"apply force\", and (3) \"close the drawer snugly\".\nUncommon vocabulary. Using rare synonyms or vague terms can cause misinterpretation and sub-\nsequent failures. The instruction for turn_off_led (9.47% success) uses the phrase \"suppress\nits glow,\" which is rare in the instructions of CALVIN and RLBench benchmarks yet a valid way of\ndescribing the task turn_off_led.\nDistracting details. Including unnecessary information or human-centric perspectives in instruc-\ntions can distract the robot, leading to failures. In the open_drawer task (0.0% success), the\nphrase \"pull it towards yourself\" introduces a perspective that the robot may not comprehend."}, {"title": "5 DISCUSSION AND LIMITATIONS", "content": "We demonstrate that current state-of-the-art language-conditioned robot models struggle to gener-\nalize beyond existing benchmarks. To show this, we introduce Embodied Red Teaming (ERT), an\nautomated method that identifies a diverse set of instructions causing these models to fail.\nFor Robot Model Developers: Language-conditioned robot models are central to recent advance-\nments in foundational robotics, similar to how ChatGPT serves as a foundational model for lan-"}, {"title": "A APPENDIX", "content": "A.1 PROMPTS FOR GENERATING INSTRUCTIONS\nThe instructions generated by GPT-4 in ERT used the following meta-prompt:\nYou are a quality assurance engineer for a robot. Your goal is to\ncome up with instructions that describe the given task correctly,\nis similar to what human users would possibly give, and yet\nchallenge the robot's capability on accomplishing the task.\nWe use the following prompt that specifies the given task:\n{IMAGE} The attached image shows the robot's environment.\nGenerate a list of {N} instructions for {TASK}, similar to the\nfollowing examples {EXAMPLES}.\nFor the 5 sets of instructions generated, we perform rejection sampling to select the most diverse\ninstruction set using CLIP diversity.\nA.2 SELECTED QUALITATIVE EXAMPLES\nWe select instructions with the lowest success rate at each task and analyze the failure modes in\nSection 4.4."}]}