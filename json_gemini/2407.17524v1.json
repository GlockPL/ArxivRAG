{"title": "StreamTinyNet: video streaming analysis with spatial-temporal TinyML", "authors": ["Hazem Hesham Yousef Shalby", "Massimo Pavan", "Manuel Roveri"], "abstract": "Tiny Machine Learning (TinyML) is a branch of Machine Learning (ML) that constitutes a bridge between the ML world and the embedded system ecosystem (i.e., Internet-of-Things devices, embedded devices, and edge computing units), enabling the execution of ML algorithms on devices constrained in terms of memory, computational capabilities, and power consumption. Video Streaming Analysis (VSA), one of the most interesting tasks of TinyML, consists in scanning a sequence of frames in a streaming manner, with the goal of identifying interesting patterns. Given the strict constraints of these tiny devices, all the current solutions rely on performing a frame-by-frame analysis, hence not exploiting the temporal component in the stream of data. In this paper, we present StreamTinyNet, the first TinyML architecture to perform multiple-frame VSA, enabling a variety of use cases that requires spatial-temporal analysis that were previously impossible to be carried out at a TinyML level. Experimental results on public-available datasets show the effectiveness and efficiency of the proposed solution. Finally, StreamTiny Net has been ported and tested on the Arduino Nicla Vision, showing the feasibility of what proposed.", "sections": [{"title": "I. INTRODUCTION", "content": "Tiny Machine Learning (TinyML) [31] is an increasingly popular field of study that combines Machine Learning (ML) and Embedded and IoT devices, characterized by strict constraints in terms of memory (on-device available RAM is usually less than 1MB), computational power (Microcontrollers frequencies < 500 KHz), and power consumption (< 100mW). It has recently gained significant attention thanks to the ability to process data directly where they have been acquired, thereby enhancing privacy and security, reducing latency, improving real-time responsiveness, and being able to operate offline (i.e., without requiring a constant internet connection). [24], [4]\nOne area of focus within TinyML is Video Streaming Analysis (VSA), which involves scanning a sequence of frames (i.e., a video) in a streaming manner to identify interesting patterns [1]. However, due to technological constraints, the execution of ML models for on-device VSA is currently limited to a frame-by-frame inspection. Remarkably, the limitation of processing videos in a frame-by-frame manner hinders the evolution of the scene over time, hence reducing the ability of TinyML models to recognize temporal patterns.\nThe aim of this paper is to present, for the first time in the literature, a novel neural network architecture, called StreamTinyNet, which is able to support multiple-frame VSA on tiny devices. The proposed architecture shows a significant reduction in the memory and computational requirements when compared to standard, non-tiny architectures and, at the same time, when compared to single-frame solutions present in the TinyML literature, it shows great accuracy improvements while keeping the differences in memory and computational demands small to negligible. Furthermore, the use of StreamTinyNet enables, for the first time at a TinyML level, a variety of use cases that require spatial-temporal analysis (e.g., gesture recognition) that are impossible to be carried out with single-frame solutions. Experiments conducted on a resource-constrained device (Arduino Nicla vision[2]) demonstrate the feasibility of porting the architecture on real-world tiny devices.\nThe paper is organized as follows. Section II provides an overview of the related literature. Section III delves into the proposed architecture, its implementation, its memory and computational complexity, and its learning algorithm. In Section IV, an evaluation of the proposed architecture is conducted on public-available benchmarks and datasets. Section V outlines the porting process on the Arduino Nicla Vision [2]. Finally, Section VI discusses the main findings of this research and addresses future research directions."}, {"title": "II. RELATED WORKS", "content": "This section describes an overview of the related literature by organizing the works into three main topics: TinyML solutions and algorithms, VSA, and Video Classification.\nA. TinyML\nTinyML solutions present in the literature rely on tech-niques to reduce the size and complexity of the ML model. This allows the memory and computational demand to be significantly reduced at the expense of a (possibly negligible) reduction in accuracy of the model. The techniques present in this field can be grouped into two main families: approximate computing mechanisms, and network architecture redesigning.\n1) Approximate computing mechanisms: These mecha-nisms, whose goal is to trade off accuracy with computational"}, {"title": "III. THE PROPOSED SOLUTION", "content": "This section delves into the research findings and provides a comprehensive discussion of the proposed architecture for VSA on tiny devices. Specifically, Section III-A presents a formulation of the problem being addressed. In Section III-B, an overview of the proposed solution is provided, while in Section III-C StreamTinyNet is discussed in detail. Finally, Section III-D examines the memory footprint and computational load of the proposed solution.\nA. Problem formulation\nThe objective of this research is to propose a neural network architecture specifically designed for VSA. Among the various tasks in video analysis, this research specifically focuses on classification. In particular, the classification is performed on a continuous basis, considering a window of the most recent T frames that are currently being streamed, being T an application-specific parameter, that can be tuned by the designer.\nMore formally, this problem can be reformulated as the design of a classifier $f_{\\theta}(x_t, x_{t-1}, x_{t-2}, ..., x_{t-(T-1)})$ able to map the previously-unseen batch of frames $(x_t, x_{t-1}, x_{t-2}, ..., x_{t-(T-1)})$ to its label $y_t$, being T defined as the length of the observation window, $x_t$ the frame acquired at time t with dimensions $M_{in} \\times N_{in} \\times C_{in}$, and $y_t$ a label that belongs to the label set $\\Omega = {\\Omega_1, \\Omega_2, ..., \\Omega_k}$, where k is the total number of classes related to a specific problem."}, {"title": "B. Architecture overview", "content": "The proposed solution, whose graphical description is given in Fig. 1, enforces the separation between the spatial and temporal aspects within the novel network architecture. This approach, which is crucial in minimizing computational redundancies when analyzing frames for multiple predictions, relies on two subsequent steps:\n\u2022 The spatial frame-by-frame feature extraction g(\u00b7);\n\u2022 The temporal combination of the extracted features h(.).\nIn more detail, the first step g(.) of the architecture serves as a feature extractor aiming at extracting information in a frame-by-frame manner within a sequence and reducing the frame dimension. We refer to this initial step as the function g(.), defined as:\n$g(): IR^{M_{in} \\times N_{in} \\times C_{in}} \\rightarrow IR^{M_{out} N_{out} \\times C_{out}}$\n$x_t \\rightarrow O_t$\nwhere:\n\u2022 $x_t$ and $O_t$ represent respectively the input (i.e., one frame) and the output (i.e., a feature map) of the first step at time t;\n\u2022 $M_{in}, N_{in},$ and $C_{in}$ represent the input image dimension (i.e., height, width, channels of the input);\n\u2022 $M_{out}, N_{out},$ and $C_{out}$ represent the output dimension (i.e., height, width, channels of the output);\n\u2022 g() parameterized with $\u03b8_g$, represents the function that maps the input frame $x_t$ to the output $O_t$.\nIt is essential to design g(\u00b7) to satisfy the following inequality:\n$M_{in} N_{in} C_{in} > M_{out} \\times N_{out} \\times C_{out}$\nso as to enforce the reduction of dimensionality between the input and output.\nIn the second step, the outputs of g(\u00b7) are jointly analyzed to exploit the temporal aspect. To be more precise, we refer to this second step as the function h(\u00b7), which can be defined as:\n$h(): R^{M_{out} \\times N_{out} \\times C_{out} \\times T} \\rightarrow \\Omega$\n$O_f \\rightarrow y_t$\nwhere:\n\u2022 $O_f \\in R^{M_{out} \\times N_{out} \\times C_{out} \\times T}$\n\u2022 h() parameterized with $\u03b8_h$, represents the function that maps the outputs $O_f$ of the previous step to a label $y_t \\in \\Omega$ associated to $(x_t, x_{t-1}, x_{t-2}, ..., x_{t-(T-1)})$.\nC. StreamTinyNet description\nThe aim of this section is to tailor the general architecture described in Section III-B to the proposed StreamTinyNet solution implementing the novel spatial-temporal processing for TinyML. In more detail, the proposed StreamTinyNet implements the function g(.) by using a convolutional feature extractor, as CNNs are revealed to be the cutting-edge solution in several image-processing applications. The selected feature extractor, depicted in Figure 2, comprises l sequential convolutional blocks. The i \u2013 th block consists of a $r^{(i)} x r^{(i)}$ 2D convolutional layer with $n^{(i)}$ filters, followed by a 2\u00d72 Max Pooling layer.\nThen, the function h(\u00b7) is implemented by using a three-step pipeline, which is reported in Figure 3.\nThe first step of the pipeline uses the T feature maps obtained by applying g(.) to the last previously-acquired T consecutive frames. These maps are split along the channel axis (i.e., $C_{out}$), resulting in $C_{out}$ frames of dimension $M_{out} \\times N_{out} \\times 1$ for each of the T feature maps. Afterward,"}, {"title": "D. Network complexity", "content": "This section introduces an analytical evaluation of the computational load c and memory footprint m of proposed StreamTinyNet, which is essential to support the porting phase on a tiny device characterized by constraints on memory m and computation \u010d. In particular, m and care defined as follows:\n$m=m_w+m_a \\lt m$\n$c = \\sum_{l \\epsilon f} c^{(l)} \\lt \\delta$\nbeing:\n$m_w = \\sum_{l \\epsilon f} m_w^{(l)}$\n$m_a = m_g + m_h$\n\u2022 c(l) the computational load of the layer l of the network.\n\u2022 $m_w^{(l)}$ the total number of parameters of layer l.\n\u2022 $m_g$ and $m_h$ the amount of values required to store the activations of g and h, respectively.\nTo compute $m_a$ and $m_h$, we have considered the optimizations in [20], so that $m_a$ and $m_h$ are equal to the maximum sum of the memory required for the activations of two consecutive layers of g and h, respectively.\nBuilding upon the formalism established in [24] and [20], the memory demand and the computational load of each sub-layer I of the network are computed as detailed\u00b2 in Table II. More in detail, when a dense layer with d dense unit is applied to a tensor of dimension $N_{in}$, the memory load for the weights $m_w$, the memory for the activations $m_a$, and the computational load c are defined as follows:\n$m_w = N_{in} \\times d + d,$\n$m_a = d,$\n$c = d x N_{in}.$\nDifferently, when a Convolutional Layer with n filters of dimension $r \\times r$ is applied to an input with dimension $M_{in} \\times N_{in} \\times n_{in}$, the memory for the weights $m_w$, the memory for the activations $m_a$, and computational load c are defined as follows:\n$m_w=r x r \\times n_{in} \\times n + n,$\n$m_a = M_{in} N_{in} x n,$\n$c=r x r \\times n_{in} X n x M_{in} X N_{in}.$\nA special case of convolutions in the proposed architecture is the 1 \u00d7 1 CONV within h(\u00b7), which is always applied to an input of dimension $M_{out} \\times N_{out} \\times C_{out} \\times T$, i.e., to the outputs of g() in the observation window T. Therefore, the storage of the T feature maps generated by g(\u00b7) requires an increase of $M_{out} \\times N_{out} \\times C_{out} \\times T$ to the usual activations memory of convolutions. It is noteworthy to point out that when two consecutive observation windows $O_f$ and $O_{f+1}$ do not overlap, the 1 \u00d7 1 convolution step within h(.) can be computed in a streaming manner. This eliminates the need to store the T feature maps generated by g(\u00b7), thereby resulting in a further reduction in the memory required for inference.\nWe also emphasize that the memory footprints reported in Table II are measured in terms of the number of weights ($m_w$) and the number of values required to store the activations ($m_a$), and thus, to obtain actual memory requirements, they should be multiplied by the dimension in Bytes of the format of those numbers (i.e., 1B for 8bit-Integers, or 4B for"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "This section introduces the experimental results measur-ing the effectiveness and the efficiency of the proposed StreamTinyNet on two challenging VSA tasks: Gesture Recognition, and Event Detection. We emphasize that both tasks can be formulated as the classification of frames within a video stream.\nTo assess the performance of the proposed StreamTinyNet architecture, we considered as a comparison three existing architectures from the literature, i.e., MobileNetV1 (with a = 0.25)[11], MobileNetV2(with a = 0.3)[25], and MCUNet [14]. These solutions are based on frame-by-frame solutions as these are currently the only implementations present in the literature within a TinyML setting. Therefore, to ease the comparison, a majority voting approach has been applied to these solutions(i.e., the most prevalent prediction among the T frames is considered). All the compared architectures have been trained from scratch on the same frames within the windows used to train the proposed solution.\nA. Gesture recognition\nGesture Recognition is the task of classifying which gesture is being performed by a human in front of the device, by analyzing the video stream collected by the on-device camera. For this purpose we considered the Jester gesture recognition dataset[15], which comprises 148,092 labeled video clips, where humans perform fundamental hand gestures in front of a laptop camera.\n1) Dataset generation: The dataset considered for this experiment is a subset of the Jester dataset, comprising three"}, {"title": "B. Event Detection", "content": "The GolfDB [16] dataset is a benchmark video dataset for the task of golf swing sequencing. The dataset comprises 1400 golf swing videos of male and female professional golfers. Golf swings have 8 distinct events that can be localized within a frame sequence. In the following, we will use the \"Percentage of Correct Events\" within the tolerance (PCE), introduced in [16], as main figure of merit to measure the correct detection ability.\n1) Dataset generation: The golf dataset undergoes a pre-processing step where each video sample is processed to produce batches of T frames. Specifically, significant frames are extracted from each video and, for each of these frames, a batch is constructed by including the T-1 frames that precede it, along with the frame itself.\n2) Evaluation: The specific StreamTinyNet model considered in this experimental selection has been selected through a grid search exploration on the parameters detailed in Table III. The choice of T = 16 has been made based on the limit given by the available processing capability. In addition, during the training process, as recommended by [16], random horizontal flipping and random affine transformations (-5\u00b0 to +5\u00b0 rotation) are applied to the input sequences.\nFollowing the approach used in [16] we considered four different splits\u00b3 of the dataset. The average PCE, the performance"}, {"title": "V. PORTING", "content": "This section describes the porting results of the StreamTinyNet performing the gesture recognition task to the Arduino Nicla Vision [2], a device commonly used for TinyML applications. This device is equipped with an STM32H747AII6 Dual Arm Cortex M7/M4 IC Microcontroller, 2MB of Flash Memory, 1MB RAM Memory and an integrated 2 MP Color Camera sensor.\nIn order to be ported on the device, 8-Bit integer post-training quantization [12] was applied to the model described in Section IV-A. The quantized model suffered a small accuracy loss on the test set after quantization, achieving a final test accuracy of 0.79. The inference time on the target device was 0.065s, enough to make the application running at 15 fps. The total RAM usage of the whole application is approximately 300 KB. Finally, we also estimated the energy consumption of the model for a single inference. Considering an input voltage of 5V and a current consumption of 105mA [2], the energy amount required for each single inference is 34mJ."}, {"title": "VI. CONCLUSIONS AND FUTURE WORKS", "content": "This research focused on the task of VSA on tiny devices, contributing significantly to the field of TinyML. The proposed StreamTinyNet architecture enables multiple frames VSA for the first time in the literature. Previously, this task was restricted to frame-by-frame analysis, neglecting any temporal aspects. Experimental results show that including multiple frames in the analysis can have big advantages in terms of accuracy, while at the same time introducing minimal overheads with respect to single-frame solutions.\nFuture works will encompass the introduction of an adaptive frame rate to optimize the power consumption in static scenes[10], [34], [5], the introduction of a sensor drift detection mechanism, the extension of the architecture to include Early Exits mechanisms [26] and on-device incremental training of the algorithm [8], [23]."}]}