{"title": "Beyond the Veil of Similarity: Quantifying Semantic Continuity in Explainable AI", "authors": ["Qi Huang", "Emanuele Mezzi", "Osman Mutlu", "Miltiadis Kofinas", "Vidya Prasad", "Shadnan Azwad Khan", "Elena Ranguelova", "Niki van Stein"], "abstract": "We introduce a novel metric for measuring semantic continuity in Explainable AI methods and machine learning models. We posit that for models to be truly interpretable and trustworthy, similar inputs should yield similar explanations, reflecting a consistent semantic understanding. By leveraging XAI techniques, we assess semantic continuity in the task of image recognition. We conduct experiments to observe how incremental changes in input affect the explanations provided by different XAI methods. Through this approach, we aim to evaluate the models' capability to generalize and abstract semantic concepts accurately and to evaluate different XAI methods in correctly capturing the model behaviour. This paper contributes to the broader discourse on AI interpretability by proposing a quantitative measure for semantic continuity for XAI methods, offering insights into the models' and explainers' internal reasoning processes, and promoting more reliable and transparent AI systems.", "sections": [{"title": "Introduction", "content": "Human intelligence can project objects and events to higher-order semantics. Starting from concrete objects it then generates abstractions that are invariant to the change of the environments in which those objects were initially identified. Over the years, given the growing power of representation shown by deep learning (DL) models, specifically deep neural networks (DNNs), researchers have started to wonder whether this capacity to abstract is only exclusive to humans or also embedded in the neural architecture. The field of Explainable AI (XAI) aims to find the answer to this question, highlighting the features contributing to"}, {"title": "Explainable AI", "content": "The field of XAI, motivated by the imperative to understand the inner workings of AI models, has undergone many advancements in recent years. This has resulted in various explanation methods being developed that may be broadly categorized into attribution-based, model-based, and example-based explanations [15]. Particularly, attribution-based methods provide insights into the importance of the features in a model by assigning importance values or ranks based on the relevance of these features to final predictions. There are global explanation methods such as Global Sensitivity Analysis [32], which attribute importance to features for a given model on a global level, and there exist various methods for single predictions (local methods), such as perturbation-based, gradient-based, surrogate-based, and propagation-based methods [27]. This discussion mainly focuses on perturbation-based and gradient-based methods for single predictions."}, {"title": "Related Work", "content": "Over the past years, several evaluation frameworks and metrics have emerged to assess the performance and compare different XAI methods with each other. One of these measures is continuity. Continuity is a critical aspect that ensures the stability and generalizability of XAI solutions and is often associated with robustness. In [2], an AutoXAI framework is proposed that automates the selection of XAI solutions where continuity is a property that plays a significant role. Understanding this property becomes even more crucial in scenarios such as Part-Prototype Image classifiers, where continuity directly influences user trust and the model's ability to generalize [17]."}, {"title": "Semantic Continuity", "content": "To define semantic continuity, we first look at the definition of continuity in the context of explainable AI from the work [18]. The definition of continuity in [18] is loosely defined as \u201csimilar inputs should have similar explanations\". We expand this idea with the notion of \"semantic continuity\" so that \u201csemantically similar inputs should have similar explanations.\" This definition brings us to the following hypothesis:\n\u2013 A slight change in the input will correspond to a slight change in the output, which is the result of the explainer.\n\u2013 Given the explanation of a reference prediction (the base case), the bigger the input change, the bigger the change between the explanation of the current output and the explanation of the initial (reference) output. This will yield an increasing monotonic correlation between changes in explanations and changes in the original input (images).\nDefinition 1. Let \\(x_0\\) denote the reference input data, and \\(f\\) be a function that applies a semantic variation \\(\u03b8\\) with a domain \\(\u0398\\) on the input data, resulting in \\(x_i = f(x_0; \u03b8_i)\\). Let \\(M\\) be a deep learning model and \\(E(M)\\) the explainer of the model. We define semantic continuity as follows:\n\\(\\theta_j - \\theta_0 > \\theta_i - \\theta_0 \\Rightarrow D(E(M(x_j)), E(M(x_0))) > D(E(M(x_i)), E(M(x_i))), \\)(1)\n\\(\u03b8_i, \u03b8_j \u2208 \u0398\\), where \\(\u03b8_0\\) corresponds to an identity transformation, i.e. \\(x_0 = f(x_0; \u03b8_0)\\) and \\(E(M(x_0))\\) corresponds to its explanation. The function \\(D\\) corresponds to a distance function between the two explanations.\nTo test whether XAI methods are semantically continuous, given that the predictor is a perfect predictor that is also semantically continuous, and thus respects the mathematical definition on which we ground the concept of semantic continuity, we propose the following approach: First, given a trained predictor (for example a DL model) for a boolean classification task, we define one or more semantic variations we can apply to the data. For example in the case of image classification, we can apply image transformation techniques such as"}, {"title": "Proof-of-concept Experiment", "content": "To investigate the semantic continuity of XAI methods, the simplest case study consists of binary classification. We will first explore semantic continuity by considering the case in which the machine learning model must distinguish between triangles and circles, where the grayscale images contain only one uniform triangle or circle positioned in the centre of a uniform background. We selected this case study as proof of concept as the task is simple and can be easily manipulated. In addition, we can assume (and verify) that the model on this task is more or less a perfect predictor.\nConsidering that the case study is a binary classification task, we build a Convolutional Neural Network (CNN) of 2 hidden layers, which is sufficient for a model to learn the features that enable it to distinguish between triangles and circles (100% test accuracy).\nGeneration of Datasets with Semantic Variations Once the model has been trained, we generate datasets that enable us to measure the semantic continuity of XAI methods in different scenarios. As shown in Figure 1, we analyze three possible cases of semantic variation and continuity:\n\u2013 Rotation: the explainer is semantically continuous concerning the rotation of triangles. To check this property, we generate a dataset of 100 images of the same triangle on the same background. The dataset is a sequence of images with the triangle rotating clockwise by one degree.\n\u2013 Contrast: the explainer is semantically continuous concerning variations in the background contrast. To check this property, we built a dataset composed of 200 images. The first 100 are images of triangles, and the second 100 are circles. In this case, the progressive change consists of a constantly diminishing contrast of the shape with the background until the shape is no longer recognizable.\n\u2013 Transition: the most complex semantic transformation. The rotation and contrast are fixed. The dataset, composed of 100 images, is a sequence where the starting image depicts a circle and the ending image a triangle. The shape gradually changes from a circle to a triangle in the in-between images."}, {"title": "From Perfect Predictor to Imperfect Predictor", "content": "As the first proof-of-concept experiment relies on a perfect predictor, we refine our definition of semantic continuity to be able to verify semantic continuity of the XAI method not only when the model is a perfect predictor but also when the model output is not semantic continuous. In addition, the binary classification of geometric shapes is fairly straightforward for the classifier being evaluated, and we would like to show the applicability of our proposed approach in a more realistic real-world setting. It is noteworthy that in Definition 1, we aim to directly establish a connection between the semantic changes in input data and the changes in post hoc explanations generated by XAI methods. However, recall one of the fundamental requirements of XAI methods, correctness, which aims"}, {"title": "Semantic variation", "content": "Definition 2 (Semantic variation). Let \\(\u03b8\\) denote a semantic variation in the range \\([\u0398_A, \u0398_B]\\), \\(\u0398_A < \u0398_B\\). We let \\(f\\) be a deterministic function that applies such variations \\(\u03b8\\) on \\(x_0\\), resulting in \\(x_i = f(x_0; \u03b8_i)\\), where \\(\\theta_i\\) is a real number that quantifies the scale of such variations. Here, the function \\(f\\) and the variation indicator \\(i\\) firstly satisfies:\n\\(P(H(x_i) = \u0398_A | x = x_i) + P(H(x_i) = \u0398_B | x = x_i) = 1,  \\)(2)\nwhere \\(H(\u00b7)\\) is a fixed (hypothetically) perfect semantic percipient, and \\(P\\) is the probability symbol. Secondly, for any pairs of valid \\((\u03b8_i, \u03b8_j)\\), the following causal property shall be held:\n\\(\u03b8_j > \u03b8_i = P(H(x_j) = \u0398_B | x = x_j) > P(H(x_i) = \u0398_B | x = x_i). \\)(3)\nDefinition 2 establishes the concept for a data-dependent, unique, deterministic, and controlled semantic variation process between the two semantics (or domains or concepts). Based on this definition, we now provide formal definitions of semantic continuity for both predictors and XAI methods (explainers).\nLet \\(M(x)\\) be a machine learning model that determines the semantics for an input \\(x\\), and a post-hoc explainer \\(E(M; x)\\) of \\(M\\). Notably, we only consider explainers that implicitly or explicitly produce a real-valued heatmap over the entire \\(x\\) or a feature importance score for each element in \\(x\\)."}, {"title": "Predictor semantic continuity", "content": "Definition 3 (Predictor semantic continuity). Given a reference data point \\(x_0\\) of semantic \\(\u0472_A\\) and a variation function \\(f(\u00b7; \u03b8)\\) that can transform \\(x_0\\) from \\(\u0398_A\\) to \\(O_B\\) as defined in Definition 2. We say the model \\(M\\) is semantically continuous between \\(\u0398_A\\) and \\(\u0398_B\\) shortly, \\(\u0398_{A,B}\\), on \\(x_0\\) if for any valid pair of \\((\u03b8_i, \u03b8_j)\\):\n\\(\\theta_j > \\theta_i = P(M(x) = \u0398_B | x = x_i) > P(M(x) = \u0398_B | x = x_i), \\)(4)\nwhere \\(x_k = f(x_0; \u03b8_k)\\), a semantic variation following Definition 2, and \\(P(M(x) = \u0398_B | x = x_k)\\) denotes the probability (confidence) of \\(x_k\\) to be an instance of domain \\(\u0398_B\\), estimated by model \\(M\\)."}, {"title": "Explainer semantic continuity", "content": "Definition 4 (Explainer semantic continuity). Similarly, given with a predictive model \\(M\\), a semantic variation function \\(f(x; \u03b8)\\) as defined in Definition 2,"}, {"title": "Synthesis of the human facial dataset", "content": "With the aforementioned formal definitions of semantic continuity, we propose to design and create a binary classification (class A vs class B) dataset S that satisfies Definition 2 but maximally prohibits a strong baseline model from being \\(\u0398_{A,B}\\) on all reference points in S.\nIn this research, particularly, we consider classification on artificially generated human faces, where the two non-overlapping classes are with glasses and without glasses. The training data is generated using stable diffusion [26], and all unrealistic samples are manually removed."}, {"title": "Experimental Setup", "content": "To explore the (easier to analyse) proof-of-concept scenario related to the capacity of the explainers to capture the semantic continuity in images, we prepare three gray-scale image datasets, each related to a different semantic scenario:\n\u2013 Rotation: fixing the background contrast, this dataset contains 100 images of equilateral triangles with different degrees of rotation. Starting from a base triangle, we apply a progressive rotation to the shape, allowing it to complete a 120-degree rotation.\n\u2013 Contrast: fixing the rotation, this dataset contains 100 images of triangles and 100 images of circles with different background contrast. Starting from the base case of a triangle and a circle with maximum gray-level contrast, we progressively diminish the contrast with the background resulting in a shape that is indistinguishable from the environment.\n\u2013 Transition: fixing rotation and contrast, this dataset contains x images of shapes. Starting from a triangle, the images show a progressive transition until the final image is one of a circle.\nThe model used [14] to test the semantic continuity of XAI methods, tested with the above datasets, has been trained on the Simple geometric shapes dataset [19]."}, {"title": "Synthesis facial dataset", "content": "In section 4.3, the methodology for creating the dataset has been introduced. In total, the training data contains 1000 balanced samples and the test set contains 100 balanced samples. Among the test samples, 48 samples with the label no glasses are chosen to be the reference image for semantic variations, where each of them is gradually shifted towards the with glasses class twenty times uniformly. This results in 1,008 images for testing the explainers' semantic continuity.\nRegarding the classifier, we choose the well-known baseline model ResNet [4] with 18 convolution layers and a Sigmoid output layer. The model is fitted exclusively on the training data from scratch using binary cross entropy loss and Adam optimizer [10] for about 20 epochs. As a result, the ResNet-18 achieves 100% predictive accuracy on the 100 test samples.\nFor consistency across the setups between experiments, following setups for the Shape dataset, we evaluated instance-wise semantic continuity of RISE, LIME, GradCAM, and KernelSHAP explainers. We choose mean squared deviation and Wasserstein distance as the distance metric (regarding \\(D_i\\)) for Definition 4) in quantitative analysis."}, {"title": "Software", "content": "For the experiments, the uniform implementation of the XAI explainers in the Deep Insight and Neural Network Analysis (DIANNA) [24], [23] python library has been used."}, {"title": "Results", "content": "The results section is organized into two subsections: i) results and insights from the proof-of-concept shape dataset to show the outcome of comparisons of different explainers and correlation metrics and ii) results of semantic continuity of explainers on the complex facial image classification task with realistic images."}, {"title": "Proof-of-concept Results: Shape Dataset", "content": "To calculate the extent to which changes in the input lead to changes in the output, we calculate the correlation between the independent variable x, which represents the change in the input, and the dependent variable y, which represents how explanations vary.\nThe distance metrics considered are the Pearson correlation, Spearman's correlation and Kendall's Tau correlation. Considering that the ideal result consists of a monotonic function, that would be able to show the increasing change that affects the output once the input is modified, these correlation metrics have been selected on the basis of their capacity to capture monotonicity.\nFor each transformation (contrast, rotation and gradual change from circle to triangle), we perform three types of analysis:\n1. Saliency maps (heatmaps) of the predictor extracted by the XAI methods. These maps highlight the region of interest in the image for a machine learning model, as inferred by the given explainer.\n2. Relational plots that visualize the correlation among the observed properties:\nSemantic variations measures the degree of semantic variation between the varied images and the reference image.\nSaliency distances denotes the mean squared deviation distances between the saliency maps of varied images and that of the reference image.\n3. Statistical correlations between the changes in the input and the changes in distances between the heatmaps. We report Pearson correlation, Spearman's rank correlation, and Kendall rank correlation (Kendall's \u03c4) to quantify the degree of explainer continuity.\nThe analysis of the first transformation (Rotation) can be seen in Figure 5. We can observe that GradCAM focuses on the edges of the triangle and shows a perfect explanation for the triangle class. When looking at the Saliency Distances for GradCAM in Figure 5b, GradCAM shows an oscillating pattern that matches the fact that after 60 degrees of rotation, the original image is obtained. Also, RISE shows the expected pattern for the rotation case with the exception"}, {"title": "Synthesis facial dataset", "content": "In this section, we discuss the semantic continuity of the four evaluated XAI methods, case by case regarding different relationships between the semantic and the ResNet predictor. For each case, we present three types of analysis:\n1. Saliency maps (heatmaps) of the predictor extracted by the XAI methods. These maps highlight the region of interest in the image for a machine learning model, as inferred by the given explainer.\n2. Relational plots that visualize the correlation among the observed properties:\nSemantic variations measures the degree of semantic variation between the varied images and the reference image.\nModel confidence represents the probability of the given image to be of the class with glasses.\nSaliency distances denotes the first Wasserstein distances between the saliency maps of varied images and that of the reference image.\nConfidence changes quantifies the differences in model confidence between the varied to-be-measured images and the reference image.\n3. Statistical correlations between the changes in model confidence and the changes in distances between the heatmaps. We report Pearson correlation and Kendall rank correlation (Kendall's \u03c4) to quantify the degree of explainer continuity as introduced in 4. Apart from the first Wasserstein distance, we"}, {"title": "Conclusions and Outlook", "content": "In this paper, we presented a novel methodology for evaluating semantic continuity for Explainable AI (XAI) methods and subsequently the predictive models. Our focus on semantic continuity emphasizes the importance of consistent explanations for similar inputs. We characterize an explainer as semantically continuous if similar inputs, lead to similar model predictions, having similar explanations. We explored semantic continuity for image classification tasks, assessing how sequential input changes impact the DL model explanations. We explored popular explainers, including LIME, RISE, GradCAM and KernelSHAP.\nWe performed an in-depth instance-based analysis for a realistic and complex image binary classification task and different XAI methods. We found that regarding the relational plots and statistical correlations, GradCAM shows to be the most semantically continuous explainer, with KernelSHAP following as a good second. Visual inspection results of saliency maps are mostly in agreement with the proposed qualitative and quantitative semantic continuity measures.\nThe investigation of semantic continuity extends our understanding of the interpretability of DL models and the capacities of different XAI methods, introducing a crucial dimension to the evaluation of XAI methods.\nNumerous promising avenues exist for future research in the intersection of semantic continuity and XAI. First, the proposed metric can be further extended to support different types of deep learning tasks beyond image classification and also extend to other domains, such as text or speech. Additionally, exploring the impact of semantic continuity on user trust and acceptance of DL models and XAI methods can provide insights into the practical implications of our findings. To quantify the semantic continuity, different metrics could be explored such as Distance correlation [33]. In conclusion, we hope that our comprehension of semantic continuity and its nuanced implications not only enhances the evolution of Explainable AI methods but also supports the usage of deep learning across diverse domains."}]}