{"title": "Beyond the Veil of Similarity: Quantifying\nSemantic Continuity in Explainable AI", "authors": ["Qi Huang", "Emanuele Mezzi", "Osman Mutlu", "Miltiadis Kofinas", "Vidya Prasad", "Shadnan Azwad Khan", "Elena Ranguelova", "Niki van Stein"], "abstract": "We introduce a novel metric for measuring semantic conti-\nnuity in Explainable AI methods and machine learning models. We posit\nthat for models to be truly interpretable and trustworthy, similar inputs\nshould yield similar explanations, reflecting a consistent semantic under-\nstanding. By leveraging XAI techniques, we assess semantic continuity\nin the task of image recognition. We conduct experiments to observe\nhow incremental changes in input affect the explanations provided by\ndifferent XAI methods. Through this approach, we aim to evaluate the\nmodels' capability to generalize and abstract semantic concepts accu-\nrately and to evaluate different XAI methods in correctly capturing the\nmodel behaviour. This paper contributes to the broader discourse on AI\ninterpretability by proposing a quantitative measure for semantic conti-\nnuity for XAI methods, offering insights into the models' and explainers'\ninternal reasoning processes, and promoting more reliable and transpar-\nent AI systems.", "sections": [{"title": "1 Introduction", "content": "Human intelligence can project objects and events to higher-order semantics.\nStarting from concrete objects it then generates abstractions that are invariant\nto the change of the environments in which those objects were initially identified.\nOver the years, given the growing power of representation shown by deep learning\n(DL) models, specifically deep neural networks (DNNs), researchers have started\nto wonder whether this capacity to abstract is only exclusive to humans or also\nembedded in the neural architecture. The field of Explainable AI (XAI) aims\nto find the answer to this question, highlighting the features contributing to"}, {"title": "2 Explainable AI", "content": "The field of XAI, motivated by the imperative to understand the inner work-\nings of AI models, has undergone many advancements in recent years. This has\nresulted in various explanation methods being developed that may be broadly\ncategorized into attribution-based, model-based, and example-based explana-\ntions [15]. Particularly, attribution-based methods provide insights into the im-\nportance of the features in a model by assigning importance values or ranks\nbased on the relevance of these features to final predictions. There are global\nexplanation methods such as Global Sensitivity Analysis [32], which attribute\nimportance to features for a given model on a global level, and there exist vari-\nous methods for single predictions (local methods), such as perturbation-based,\ngradient-based, surrogate-based, and propagation-based methods [27]. This dis-\ncussion mainly focuses on perturbation-based and gradient-based methods for\nsingle predictions."}, {"title": "3 Related Work", "content": "Over the past years, several evaluation frameworks and metrics have emerged\nto assess the performance and compare different XAI methods with each other.\nOne of these measures is continuity. Continuity is a critical aspect that ensures\nthe stability and generalizability of XAI solutions and is often associated with\nrobustness. In [2], an AutoXAI framework is proposed that automates the se-\nlection of XAI solutions where continuity is a property that plays a significant\nrole. Understanding this property becomes even more crucial in scenarios such as\nPart-Prototype Image classifiers, where continuity directly influences user trust\nand the model's ability to generalize [17]."}, {"title": "4 Semantic Continuity", "content": "To define semantic continuity, we first look at the definition of continuity in the\ncontext of explainable AI from the work [18]. The definition of continuity in\n[18] is loosely defined as \u201csimilar inputs should have similar explanations"}, {"title": "4.1 Proof-of-concept Experiment", "content": "To investigate the semantic continuity of XAI methods, the simplest case study\nconsists of binary classification. We will first explore semantic continuity by con-\nsidering the case in which the machine learning model must distinguish between\ntriangles and circles, where the grayscale images contain only one uniform trian-\ngle or circle positioned in the centre of a uniform background. We selected this\ncase study as proof of concept as the task is simple and can be easily manip-\nulated. In addition, we can assume (and verify) that the model on this task is\nmore or less a perfect predictor.\nConsidering that the case study is a binary classification task, we build a\nConvolutional Neural Network (CNN) of 2 hidden layers, which is sufficient for\na model to learn the features that enable it to distinguish between triangles and\ncircles (100% test accuracy).\nGeneration of Datasets with Semantic Variations Once the model has\nbeen trained, we generate datasets that enable us to measure the semantic con-\ntinuity of XAI methods in different scenarios. As shown in Figure 1, we analyze\nthree possible cases of semantic variation and continuity:\nRotation: the explainer is semantically continuous concerning the rotation of\ntriangles. To check this property, we generate a dataset of 100 images of the\nsame triangle on the same background. The dataset is a sequence of images\nwith the triangle rotating clockwise by one degree.\nContrast: the explainer is semantically continuous concerning variations in\nthe background contrast. To check this property, we built a dataset com-\nposed of 200 images. The first 100 are images of triangles, and the second\n100 are circles. In this case, the progressive change consists of a constantly\ndiminishing contrast of the shape with the background until the shape is no\nlonger recognizable.\nTransition: the most complex semantic transformation. The rotation and\ncontrast are fixed. The dataset, composed of 100 images, is a sequence where\nthe starting image depicts a circle and the ending image a triangle. The\nshape gradually changes from a circle to a triangle in the in-between images."}, {"title": "4.2 From Perfect Predictor to Imperfect Predictor", "content": "As the first proof-of-concept experiment relies on a perfect predictor, we refine\nour definition of semantic continuity to be able to verify semantic continuity of\nthe XAI method not only when the model is a perfect predictor but also when the\nmodel output is not semantic continuous. In addition, the binary classification\nof geometric shapes is fairly straightforward for the classifier being evaluated,\nand we would like to show the applicability of our proposed approach in a more\nrealistic real-world setting. It is noteworthy that in Definition 1, we aim to\ndirectly establish a connection between the semantic changes in input data and\nthe changes in post hoc explanations generated by XAI methods. However, recall\none of the fundamental requirements of XAI methods, correctness, which aims"}, {"title": "Definition 2 (Semantic variation).", "content": "Let $\\theta$ denote a semantic variation in the\nrange [$\\Theta_A, \\Theta_B$], $\\Theta_A < \\Theta_B$. We let $f$ be a deterministic function that applies such\nvariations $\\theta$ on $x_0$, resulting in $x_i = f(x_0;\\theta_i)$, where $\\theta_i$ is a real number that\nquantifies the scale of such variations. Here, the function $f$ and the variation\nindicator $i$ firstly satisfies:\n$P(H(x_i) = \\Theta_A | x = x_i) + P(H(x_i) = \\Theta_B | x = x_i) = 1$, (2)\nwhere $H(\\cdot)$ is a fixed (hypothetically) perfect semantic percipient, and $P$ is the\nprobability symbol. Secondly, for any pairs of valid $(\\theta_i, \\theta_j)$, the following causal\nproperty shall be held:\n$\\theta_j > \\theta_i \\Rightarrow P(H(x_j) = \\Theta_B | x = x_j) > P(H(x_i) = \\Theta_B | x = x_i)$. (3)\nDefinition 2 establishes the concept for a data-dependent, unique, determin-\nistic, and controlled semantic variation process between the two semantics (or\ndomains or concepts). Based on this definition, we now provide formal definitions\nof semantic continuity for both predictors and XAI methods (explainers).\nLet $M(x)$ be a machine learning model that determines the semantics for\nan input $x$, and a post-hoc explainer $E(M;x)$ of $M$. Notably, we only consider\nexplainers that implicitly or explicitly produce a real-valued heatmap over the\nentire $x$ or a feature importance score for each element in $x$."}, {"title": "Definition 3 (Predictor semantic continuity).", "content": "Given a reference data point\n$x_0$ of semantic $\\Theta_A$ and a variation function $f(\\cdot;\\theta)$ that can transform $x_0$ from\n$\\Theta_A$ to $\\Theta_B$ as defined in Definition 2. We say the model $M$ is semantically\ncontinuous between $\\Theta_A$ and $\\Theta_B$ shortly, $\\Theta_{A,B}$, on $x_0$ if for any valid pair of\n$(\\theta_i, \\theta_j)$:\n$\\theta_j > \\theta_i \\Rightarrow P(M(x) = \\Theta_B | x = x_i) > P(M(x) = \\Theta_B | x = x_i)$, (4)\nwhere $x_k = f(x_0; \\theta_k)$, a semantic variation following Definition 2, and $P(M(x) =$\n$\\Theta_B | x = x_k)$ denotes the probability (confidence) of $x_k$ to be an instance of do-\nmain $\\Theta_B$, estimated by model M."}, {"title": "Definition 4 (Explainer semantic continuity).", "content": "Similarly, given with a pre-\ndictive model M, a semantic variation function $f(x;\\theta)$ as defined in Definition 2,"}, {"title": "4.3 Synthesis of the human facial dataset", "content": "With the aforementioned formal definitions of semantic continuity, we propose\nto design and create a binary classification (class A vs class B) dataset S that\nsatisfies Definition 2 but maximally prohibits a strong baseline model from being\n$\\Theta_{A,B}$ on all reference points in S.\nIn this research, particularly, we consider classification on artificially gener-\nated human faces, where the two non-overlapping classes are with glasses and\nwithout glasses. The training data is generated using stable diffusion [26], and\nall unrealistic samples are manually removed."}, {"title": "5 Experimental Setup", "content": ""}, {"title": "5.1 Shape Dataset", "content": "To explore the (easier to analyse) proof-of-concept scenario related to the c\u0430\u0440\u0430c-\nity of the explainers to capture the semantic continuity in images, we prepare\nthree gray-scale image datasets, each related to a different semantic scenario:\nRotation: fixing the background contrast, this dataset contains 100 images\nof equilateral triangles with different degrees of rotation. Starting from a\nbase triangle, we apply a progressive rotation to the shape, allowing it to\ncomplete a 120-degree rotation.\nContrast: fixing the rotation, this dataset contains 100 images of triangles\nand 100 images of circles with different background contrast. Starting from\nthe base case of a triangle and a circle with maximum gray-level contrast,\nwe progressively diminish the contrast with the background resulting in a\nshape that is indistinguishable from the environment.\nTransition: fixing rotation and contrast, this dataset contains x images of\nshapes. Starting from a triangle, the images show a progressive transition\nuntil the final image is one of a circle.\nThe model used [14] to test the semantic continuity of XAI methods, tested\nwith the above datasets, has been trained on the Simple geometric shapes dataset\n[19]."}, {"title": "5.2 Synthesis facial dataset", "content": "In section 4.3, the methodology for creating the dataset has been introduced.\nIn total, the training data contains 1000 balanced samples and the test set con-\ntains 100 balanced samples. Among the test samples, 48 samples with the label\nno glasses are chosen to be the reference image for semantic variations, where\neach of them is gradually shifted towards the with glasses class twenty times\nuniformly. This results in 1,008 images for testing the explainers' semantic con-\ntinuity.\nRegarding the classifier, we choose the well-known baseline model ResNet [4]\nwith 18 convolution layers and a Sigmoid output layer. The model is fitted\nexclusively on the training data from scratch using binary cross entropy loss and\nAdam optimizer [10] for about 20 epochs. As a result, the ResNet-18 achieves\n100% predictive accuracy on the 100 test samples.\nFor consistency across the setups between experiments, following setups for\nthe Shape dataset, we evaluated instance-wise semantic continuity of RISE,\nLIME, GradCAM, and KernelSHAP explainers. We choose mean squared de-\nviation and Wasserstein distance as the distance metric (regarding $D_i$) for Def-\ninition 4) in quantitative analysis."}, {"title": "5.3 Software", "content": "For the experiments, the uniform implementation of the XAI explainers in the\nDeep Insight and Neural Network Analysis (DIANNA) [24], [23] python library\nhas been used."}, {"title": "6 Results", "content": "The results section is organized into two subsections: i) results and insights\nfrom the proof-of-concept shape dataset to show the outcome of comparisons of\ndifferent explainers and correlation metrics and ii) results of semantic continuity\nof explainers on the complex facial image classification task with realistic images."}, {"title": "6.1 Proof-of-concept Results: Shape Dataset", "content": "To calculate the extent to which changes in the input lead to changes in the\noutput, we calculate the correlation between the independent variable $x$, which\nrepresents the change in the input, and the dependent variable $y$, which repre-\nsents how explanations vary.\nThe distance metrics considered are the Pearson correlation, Spearman's cor-\nrelation and Kendall's Tau correlation. Considering that the ideal result consists\nof a monotonic function, that would be able to show the increasing change that\naffects the output once the input is modified, these correlation metrics have been\nselected on the basis of their capacity to capture monotonicity.\nFor each transformation (contrast, rotation and gradual change from circle\nto triangle), we perform three types of analysis:\n1. Saliency maps (heatmaps) of the predictor extracted by the XAI methods.\nThese maps highlight the region of interest in the image for a machine learn-\ning model, as inferred by the given explainer.\n2. Relational plots that visualize the correlation among the observed properties:\n\u2013 Semantic variations measures the degree of semantic variation be-\ntween the varied images and the reference image.\n\u2013 Saliency distances denotes the mean squared deviation distances be-\ntween the saliency maps of varied images and that of the reference image.\n3. Statistical correlations between the changes in the input and the changes in\ndistances between the heatmaps. We report Pearson correlation, Spearman's\nrank correlation, and Kendall rank correlation (Kendall's $\\tau$) to quantify the\ndegree of explainer continuity.\nThe analysis of the first transformation (Rotation) can be seen in Figure 5.\nWe can observe that GradCAM focuses on the edges of the triangle and shows\na perfect explanation for the triangle class. When looking at the Saliency Dis-\ntances for GradCAM in Figure 5b, GradCAM shows an oscillating pattern that\nmatches the fact that after 60 degrees of rotation, the original image is obtained.\nAlso, RISE shows the expected pattern for the rotation case with the exception"}, {"title": "6.2 Synthesis facial dataset", "content": "In this section, we discuss the semantic continuity of the four evaluated XAI\nmethods, case by case regarding different relationships between the semantic\nand the ResNet predictor. For each case, we present three types of analysis:\n1. Saliency maps (heatmaps) of the predictor extracted by the XAI methods.\nThese maps highlight the region of interest in the image for a machine learn-\ning model, as inferred by the given explainer.\n2. Relational plots that visualize the correlation among the observed properties:\n\u2013 Semantic variations measures the degree of semantic variation be-\ntween the varied images and the reference image.\n\u2013 Model confidence represents the probability of the given image to be\nof the class with glasses.\n\u2013 Saliency distances denotes the first Wasserstein distances between the\nsaliency maps of varied images and that of the reference image.\n\u2013 Confidence changes quantifies the differences in model confidence be-\ntween the varied to-be-measured images and the reference image.\n3. Statistical correlations between the changes in model confidence and the\nchanges in distances between the heatmaps. We report Pearson correlation\nand Kendall rank correlation (Kendall's $\\tau$) to quantify the degree of explainer\ncontinuity as introduced in 4. Apart from the first Wasserstein distance, we"}, {"title": "7 Conclusions and Outlook", "content": "In this paper, we presented a novel methodology for evaluating semantic continu-\nity for Explainable AI (XAI) methods and subsequently the predictive models.\nOur focus on semantic continuity emphasizes the importance of consistent ex-\nplanations for similar inputs. We characterize an explainer as semantically con-\ntinuous if similar inputs lead to similar model predictions, having similar expla-\nnations. We explored semantic continuity for image classification tasks, assessing\nhow sequential input changes impact the DL model explanations. We explored\npopular explainers, including LIME, RISE, GradCAM and KernelSHAP.\nWe performed an in-depth instance-based analysis for a realistic and com-\nplex image binary classification task and different XAI methods. We found that\nregarding the relational plots and statistical correlations, GradCAM shows to\nbe the most semantically continuous explainer, with KernelSHAP following as a\ngood second. Visual inspection results of saliency maps are mostly in agreement\nwith the proposed qualitative and quantitative semantic continuity measures.\nThe investigation of semantic continuity extends our understanding of the\ninterpretability of DL models and the capacities of different XAI methods, in-\ntroducing a crucial dimension to the evaluation of XAI methods.\nNumerous promising avenues exist for future research in the intersection of\nsemantic continuity and XAI. First, the proposed metric can be further extended\nto support different types of deep learning tasks beyond image classification and\nalso extend to other domains, such as text or speech. Additionally, exploring the\nimpact of semantic continuity on user trust and acceptance of DL models and\nXAI methods can provide insights into the practical implications of our findings.\nTo quantify the semantic continuity, different metrics could be explored such\nas Distance correlation [33]. In conclusion, we hope that our comprehension of\nsemantic continuity and its nuanced implications not only enhances the evolution\nof Explainable AI methods but also supports the usage of deep learning across\ndiverse domains."}]}