{"title": "Enhancing Safety in Reinforcement Learning with Human Feedback via\nRectified Policy Optimization", "authors": ["Xiyue Peng", "Hengquan Guo", "Jiawei Zhang", "Dongqing Zou", "Ziyu Shao", "Honghao Wei", "Xin Liu"], "abstract": "Balancing helpfulness and safety (harmlessness) is a critical challenge in aligning large language\nmodels (LLMs). Current approaches often decouple these two objectives, training separate preference\nmodels for helpfulness and safety, while framing safety as a constraint within a constrained Markov\nDecision Process (CMDP) framework. However, these methods can lead to \"safety interference\",\nwhere average-based safety constraints compromise the safety of some prompts in favor of others.\nTo address this issue, we propose Rectified Policy Optimization (RePO), which replaces the\naverage safety constraint with stricter (per prompt) safety constraints. At the core of RePO is a policy\nupdate mechanism driven by rectified policy gradients, which penalizes the strict safety violation of\nevery prompt, thereby enhancing safety across nearly all prompts. Our experiments on Alpaca-7B\ndemonstrate that RePO improves the safety alignment and reduces the safety interference compared\nto baseline methods. Code is available at https://github.com/pxyWaterMoon/RePO.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have advanced rapidly, demonstrating remarkable capabilities across a\nwide range of practical applications including translation (Zhang et al., 2023), programming (Wermelinger,\n2023; Gao et al., 2023a), medicine (Yang et al., 2022; Thirunavukarasu et al., 2023), law (Katz et al.,\n2024), and robotics (Shah et al., 2023). These advancements significantly enhance human productivity\nand quality of life. However, LLMs can occasionally exhibit unexpected behaviors that pose risks to\nproductivity and daily life. These risks often include generating content that violates social ethics,\ndisplays bias or discrimination, spreads misinformation, or leads to privacy breaches (Gehman et al.,\n2020; Hartvigsen et al., 2022; Carlini et al., 2021; Weidinger et al., 2021; Lin et al., 2022). A notable\nexample is Microsoft's chatbot Tay, which, under the influence of hostile users, sent over 50,000 tweets\ncontaining racial slurs and sexually explicit content, ultimately leading to its removal. Additionally,\nstudies have shown that language models can generate misinformation, leak confidential information (Lee,\n2016), and compromise personal data (Carlini et al., 2021). This serves as a warning that only by\nensuring the safety and helpfulness of large language models can we allow them to serve humanity better.\nImproving the helpfulness of language models often conflicts with minimizing their harmfulness\n(Dai et al., 2023; Bai et al., 2022). This tension results in several challenges for the safe alignment\nof language models. First, annotators may introduce subjective biases during the data annotation\nwhen balancing helpfulness and harmlessness (Dai et al., 2023; Zhong et al., 2024). Second, during"}, {"title": "2 Related Work", "content": "Three main levels of defense are used to improve the security of large language models (Dong et al.,\n2024): input and output filters, inference guidance, and LLM safety alignment. Input and output filters\ndetect harmful content and apply appropriate processing based on certain rules (Alon and Kamfonas,\n2023) or models(Sood et al., 2012), thereby mitigating unsafe text. Inference guidance reduces dangerous\nlanguage in large language models by constructing system prompts(Phute et al., 2023) and adjusting\ntoken selection methods(Li et al., 2024). These two methods merely add safeguard layers at the input\nand output ports of LMs to enhance the safety of the final output, but they do not improve the inherent\nsafety of the LLMs. Therefore, LLM safety alignment is the \"core defense\" for enhancing the safety\nof LLMs(Dong et al., 2024). Achiam et al. (2023); Touvron et al. (2023b) include a large amount of\nsafety-related data during the supervised fine-tuning (SFT) process to enhance the safety of language\nmodels. However, compared to preference data, the construction of such safety-related datasets typically\nincurs significantly higher costs.\nAccording to Goodhart's Law (Goodhart and Goodhart, 1984), when a measure becomes a target,\nit ceases to be a good measure. Any reward model that fits a dataset is inherently an incomplete\nrepresentation of human preferences (Gao et al., 2023b). Relying on a reward model that reflects\nhuman preferences from a single perspective for RLHF, or directly optimizing a homogeneous and\nstatic objective through preference learning, risks falling into the trap of over-optimization. As more\nresearchers become aware of the drawbacks associated with single-objective optimization, there has been\na growing interest in multi-objective alignment for language models (Dai et al., 2023; Wachi et al., 2024;\nLou et al., 2024; Zhong et al., 2024; Moskovitz et al., 2024). In LLM safety alignment, Dai et al. (2023);\nWachi et al. (2024); Bai et al. (2022); Touvron et al. (2023b) decoupled human preferences into two\ncomponents: helpfulness and harmlessness. They annotated and trained separate preference models for\neach component, transforming the original single-objective optimization problem into a multi-objective\noptimization problem.\nThe recent works (Dai et al., 2023; Wachi et al., 2024) are most closely related to ours, as they\ndecouple human preferences into helpfulness and harmlessness, focusing on maximizing helpfulness\nwhile ensuring safety by satisfying the safety constraints. In these two works, Dai et al. (2023) follows\nthe RLHF paradigm proposed by Ziegler et al. (2019), employing three steps for safety alignment:"}, {"title": "3 Preliminaries", "content": "In this section, we first provide an overview of the standard reinforcement learning from human feedback\n(RLHF) pipeline (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022), discuss the existing\nwork on improving safety, and identify the possible pitfalls of the existing work to motivate our algorithm."}, {"title": "3.1 RLHF Pipeline", "content": "The standard RLHF pipeline builds on a pre-trained base policy and includes three major stages\n(Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022).\nSupervised Fine Tuning (SFT). Given a large dataset D with a substantial amount of instruction-\nresponse examples, the language model is pre-trained through offline imitation learning or behavioral\ncloning in a supervised manner. This process aims to teach the model general concepts and knowledge\nby maximizing the log-likelihood of the next predicted token, formulated as $\\max E_{(x,y)\\in D}[\\log(\\pi(y|x))]$.\nWe refer to the model obtained in this step as $\\pi_{ref}$.\nReward Preference Modeling. After completing the SFT stage, we can further align the model\nwith human values by learning a parameterized reward model, $R_{\\phi}$, using a human preference dataset\n$D = \\{x^{(i)}, y^{(i)}_{w}, y^{(i)}_{l}, o^{(i)}\\}_{i=1}^{N}$. In this dataset, $x^{(i)}$ represents the prompt, $y^{(i)}_{w}$ and $y^{(i)}_{l}$ are two responses,\nand $o^{(i)} = 1$ indicates that $y^{(i)}_{w}$ is preferred (i.e., $y^{(i)}_{w} \\succ y^{(i)}_{l}$), while $o^{(i)} = 0$ indicates the opposite. In\nstandard RLHF, the reward function can be learned by establishing a relationship between the reward\nfunction $R_{\\phi}(x, y)$ and the likelihood of human preferences $P_{\\phi}(y_w > y_l|x)$ using the Bradley-Terry (BT)\nmodel (Bradley and Terry, 1952)\n$P_{\\phi}(y_w > y_l|x^{(i)}) = \\frac{e^{R_{\\phi}(x^{(i)},y^{(i)}_{w})}}{e^{R_{\\phi}(x^{(i)},y^{(i)}_{w})} + e^{R_{\\phi}(x^{(i)},y^{(i)}_{l})}}$\n(1)\nThe reward function $R(x, y)$ can be obtained by maximizing the likelihood of human preferences on the\ndataset $D$, that is\n$\\max E[o^{(i)} \\log P_{\\phi}(y^{(i)}_{w} > y^{(i)}_{l} |x^{(i)}) + (1 - o^{(i)}) \\log P_{\\phi}(y^{(i)}_{l} > y^{(i)}_{w} |x^{(i)})]$.\nReinforcement Learning Fine-tuning. As described in Christiano et al. (2017); Ziegler et al.\n(2019); Ouyang et al. (2022), the generation process of a LLM can be framed as a Markov decision\nprocess (MDP). Starting from the initial state $s_0$, the language model $\\pi_{\\theta}$ outputs a token $a_h$ at each\nstep from the vocabulary set, forming a new state $s_h = (s_0, a_1, a_2, ..., a_{h-1}, a_h)$. The generation process"}, {"title": "3.2 Improving Safety in RLHF Pipeline", "content": "LLMs fine-tuned through RLHF may overemphasize helpfulness at the expense of harmlessness (safety).\nTo address this, human preferences can be explicitly decoupled into two dimensions: helpfulness and\nharmlessness. This allows for joint optimization of both metrics across various prompts (e.g., either\nbenign or harmful prompts). Similar to the reward preference model, a cost preference model can be\nconstructed to assess the harmlessness of responses.\nCost Preference Modeling. In addition to the previous preference dataset in reward modeling,\nwe have two labels $(e^{(i)}_{1}, ..., e^{(i)}_{N}, e^{(i)}_{1}, e^{(i)}_{2})$ in the dataset $D = \\{x^{(i)}, y^{(i)}_{w}, y^{(i)}_{l}, o^{(i)}, e^{(i)}_{1}, e^{(i)}_{2}\\}_{i=1}^{N}$ to indicate whether the\nresponses $y^{(i)}_{w}$ and $y^{(i)}_{l}$ are safe. To implement the BT for the cost model in (1), we assume virtual safe\nand base responses, denoted as $\\{y^{(i)}_{0}\\}$, for each prompt $\\{x^{(i)}\\}$, where $C(x^{(i)},y^{(i)}_{0}) = 0$. The responses\n$\\{y^{(i)}_{0}\\}$ do not need to be present in the data. Similarly, we learn the cost model $C_{\\phi}(x, y)$ by maximizing\nthe likelihood accordingly.\nSafe Reinforcement Learning Fine-tuning. Given the best fitted reward and cost models, we\ncan evaluate the helpfulness and harmlessness of (prompt, response) by $R(x,y)$ and $C(x, y)$ (here we\nremove the $\\phi$ to indicate the best models). To guarantee a safe response, one could impose an explicit\nsafety constraint such that the overall/expected costs are below a safety threshold (w.l.o.g., we assume\nthe threshold to be zero). This transforms the original (unconstrained) MDP into a constrained MDP as\nfollows (Dai et al., 2023)\n$\\max E_{x\\sim D,y\\sim \\pi_{\\theta}(y|x)} [R(x, y)] - \\beta KL(\\pi_{\\theta}||\\pi_{ref})$\n$\\pi_{\\theta}$\ns.t. $E_{x\\sim D,y\\sim \\pi_{\\theta}(y|x)} [C(x, y)] \\leq 0$.\n(3)\nTo solve the problem, Dai et al. (2023) applied the PPO-Lagrangian algorithm, which first transforms\nthe constrained MDP in (3) into an unconstrained one using the Lagrangian method, then optimizes the\n\"primal\" policy $\\pi_{\\theta}$ via Proximal Policy Optimization and update the dual via subgradient descent (Ray\net al., 2019). The PPO-Lagrangian algorithm has been widely applied to various domains such as robot\ncontrol (Pore et al., 2021), energy management (Marot et al., 2021), and automated driving (Teng et al.,\n2023). However, we will demonstrate that this classical method may still encounter safety issues, called\n\"safety interference\"."}, {"title": "3.3 Pitfalls of the Expected Safety Constraints", "content": "The issue of safety interference is mainly due to the formulation of expected constraints in (3), where\nthe policy may over-optimize safety for one type of prompt (resulting in \u201cvery negative\" costs) while\ncompromising the safety of other prompts, even though the overall safety constraint is still satisfied.\nFigure 1 demonstrates the phenomenon of \"safety interference\u201d, where safe responses to the question\nmay compromise the safety of the response to another question."}, {"title": "4 Rectified Policy Optimization", "content": "To mitigate the \"safety interference\", we study a direct and strict CMDP formulation where the safety\nconstraint is imposed on every pair of prompt and response as follows:\n$\\max E_{x\\sim D,y\\sim \\pi_{\\theta}(y|x)} [R(x, y)] - \\beta KL(\\pi_{\\theta}||\\pi_{ref})$\n$\\pi_{\\theta}$\ns.t. $C(x, y) \\leq 0, \\forall x \\sim D, y \\sim \\pi_{\\theta}(\\cdot | x)$.\n(4)\nThe constraint in (4) implies that our policy should give a safe response for every prompt, which is\nstricter than CMDP in (3) that only guarantees safety in an expected sense. However, it is notoriously\nchallenging (if not impossible) to solve (4) due to the stringent requirement. One potential approach\nto satisfy the constraint in (4) is the \u201cprojection-based\u201d method, which could be infeasible because it\nrequires searching the high-dimensional and combinatorial response space in $\\mathcal{Y}$. We aim to preserve the\nexpected form presented in (3), which is well-suited for optimization by RL algorithms while avoiding\nsafety interference from average-based constraints. Inspired by Guo et al. (2022), we propose a rectified"}, {"title": "5 Experiment", "content": "In this section, we evaluate the empirical performance of RePO for the safety alignment of LLMs. We\nfine-tuned the Alpaca-7B model (Dai et al., 2023) using the training set from the PKU-SafeRLHF dataset\n(Dai et al., 2023). The algorithms are evaluated using two benchmarks. First, we assess them with the\ntest set from the PKU-SafeRLHF dataset, leveraging the open-source beaver-7B-v1.0-cost/reward model\n(Dai et al., 2023) to evaluate both helpfulness and safety. Then, we further test the algorithms on the\nPKU-SafeRLHF test set and the collections of the Safe LLMs dataset (Bianchi et al., 2024), utilizing\nan enhanced GPT-4 scoring evaluation to assess their performance. Detailed information about the\ndatasets and benchmark setup can be found in Appendix C.\nAs baselines, we consider the LLM safety alignment methods PPO-Lagrange (Dai et al., 2023) and\nSACPO (Wachi et al., 2024).\nPPO-Lagrange/Beaver: Beaver-v1.0 (Dai et al., 2023) is a language model fine-tuned from Alpaca-7B\nusing the PPO-Lagrange algorithm, which also utilizes a primal-dual structure model update. The\nprimary distinction between RePO and PPO-Lagrangian comes from the rectified design in our algorithm.\nWhile PPO-Lagrangian uses an average-based safety constraint that may lead to safety interference, the\nrectified training process allows RePO to concentrate on optimizing helpfulness once safety has been\nensured.\nSACPO: SACPO (Wachi et al., 2024) is a representative RL-free algorithm for safe language model\nfine-tuning that employs a step-wise DPO approach to sequentially align helpfulness and harmlessness\n(i.e., safety) without explicit fitting of reward and cost models. Similar to PPO-Lagrangian, SACP\u041e\nalso considers an average-based safety constraint, making it similarly vulnerable to safety interference.\nBy comparing with these two baseline algorithms, we can demonstrate the impact of rectified designs\nin reducing safety interference caused by average-based safety constraints, thereby providing better\nsafety alignment for language models."}, {"title": "5.1 Reward/Cost Model Evaluation", "content": "We first evaluate the methods using the beaver-7B-v1.0-cost/reward models to determine whether the\nalgorithm is helpful and harmless. Since the training data distributions for the reward and cost models\nare similar to those of the PKU-SafeRLHF test set, the reward/cost models can yield relatively accurate\njudgments for this portion of the test data. Therefore, we conduct model evaluation exclusively on the\nPKU-SafeRLHF test set."}, {"title": "5.2 GPT-4 Scoring Evaluation", "content": "To provide a more comprehensive evaluation of the algorithms across different datasets, we conduct\nthe enhanced GPT-4 scoring evaluation, where GPT-4 scores each output based on both helpful and\nharmful criteria. For each dataset, a certain number of samples are randomly selected, and we determine\nwhich model performs better in terms of helpfulness and harmlessness based on the scores provided by\nGPT-4. More Detailed implementation can be found in Appendix \u0421."}, {"title": "6 Conclusion", "content": "This paper explores the safety alignment of language models to balance helpfulness and harmlessness\n(safety), with a focus on mitigating \u201csafety interference\u201d during the training process. We find that the\n\"safety interference\" arises from traditional average-based safety constraints, and we propose the Rectified\nPolicy Optimization (RePO) algorithm, which employs the primal-dual proximal policy optimization\nand enforces stricter safety constraints through a rectified design. The core insight of this design is\nthat language models should focus on optimizing helpfulness only when safety constraints are already\nsatisfied, resulting in improved performance in both helpfulness and harmlessness.\nWe conduct experiments on various datasets using both model-based evaluation and GPT-4 scoring\nevaluation. The results show that RePO successfully mitigates \"safety interference\" and achieves the\nbest performance in improving both helpfulness and harmlessness compared to the baseline algorithms."}, {"title": "A Pitfalls of the Existing Method", "content": "In this section, we will provide additional details to Section 3.3. Figure 1 illustrates the output of the\nlanguage model trained using PPO-Lagrangian (denoted by Beaver) for safety alignments. The response\nof \"Describe why to get lots of youtube views, you should do a prank video where you snatch a cop's gun\nout of the holster and replace it with a breadstick.\" is unsafe because it fails to highlight that uploading\na video featuring the act of snatching a cop's gun out of the holster and replacing it with a breadstick\nmay convey harmful messages about illegal and violent behavior to the audience. A safe response to this\nquestion should emphasize, similar to the answers generated by GPT-40, that snatching a cop's gun out\nof the holster and replacing it with a breadstick is not suitable content for garnering views."}, {"title": "B Proof of Theorem 1", "content": "In this section, we will demonstrate that the rectified formulation in (5) is equivalent to the CMDP in\n(4). Recall the feasible set of CMDP in (4) to be\n{\\pi_{\\theta} | C(x, y) \\leq 0,\\forall x \\sim D, y \\sim \\pi_{\\theta}(y|x)}.\nIt's straightforward to see that equivalent set is\n{\\pi_{\\theta} | \\{C(x,y)\\}^+ = 0,\\forall x \\sim D, y \\sim \\pi_{\\theta}(y|x)}\nwith the rectified operator $\\{C(x,y)\\}^+ = \\max\\{C(x, y), 0\\}$. From the fact that $\\{C(x,y)\\}^+ \\geq 0$, we can\nrewrite the problem in (4) as follows:\n$\\max E_{x\\sim D,y\\sim \\pi_{\\theta}(y|x)} [R(x, y)] - \\beta KL(\\pi_{\\theta}||\\pi_{ref})$\n$\\pi_{\\theta}$\ns.t. $E_{x\\sim D,y\\sim \\pi_{\\theta}(y|x)} [\\{C(x,y)\\}^+] = 0$.\nBy penalizing the constraints, we define the following surrogate function:\n$\\mathcal{L}(\\pi_{\\theta}, \\lambda) = -E_{x\\sim D,y\\sim \\pi_{\\theta}(y|x)} [R(x, y)] + \\beta KL(\\pi_{\\theta}||\\pi_{ref}) + \\lambda E_{x\\sim D,y\\sim \\pi_{\\theta}(y|x)} [\\{C(x,y)\\}^+]$.\nFor the above function, we have\n$\\max_{\\lambda \\geq 0} \\mathcal{L}(\\pi_{\\theta}, \\lambda) = \\{\n -E_{x\\sim D,y\\sim \\pi_{\\theta}(y|x)} [R(x, y)] + \\beta KL(\\pi_{\\theta}||\\pi_{ref}) \\text{ } E_{x\\sim D, y\\sim \\pi_{\\theta}(y|x)} [\\{C(x, y)\\}^+] = 0\n +\\infty \\text{ } otherwise\n$\\text{When the constraint is violated, the function becomes infinite, thus preventing the selection of such policies.  If the safety constraint is satisfied, i.e., } E_{x\\sim D, y\\sim \\pi_{\\theta}(y|x)} [\\{C(x, y)\\}^+] = 0 \\text{, it is equivalent to find a policy }\\pi_{\\theta} \\text{ to minimize } \\max_{\\lambda > 0} \\mathcal{L}(\\pi_{\\theta}, \\lambda) = -E_{x\\sim D,y\\sim \\pi_{\\theta}(y|x)} [R(x, y)] + \\beta KL(\\pi_{\\theta}||\\pi_{ref}) \\text{, which is exactly same as the objective in (4). Therefore, the proof is completed.}$"}, {"title": "C Experiment Supplements", "content": "In this section, we will introduce detailed information about open-source models used in the literature."}]}