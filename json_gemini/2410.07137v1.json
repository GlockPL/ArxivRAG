{"title": "CHEATING AUTOMATIC LLM BENCHMARKS: NULL MODELS ACHIEVE HIGH WIN RATES", "authors": ["Xiaosen Zheng", "Tianyu Pang", "Chao Du", "Qian Liu", "Jing Jiang", "Min Lin"], "abstract": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT- Bench, have become popular for evaluating language models due to their cost- effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a \u201cnull model\u201d that always outputs a constant response (irrelevant to input instructions) can cheat automatic bench- marks and achieve top-ranked win rates: an 86.5% LC win rate on AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are transferable because we assume that the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are private and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks.", "sections": [{"title": "INTRODUCTION", "content": "Numerous large language models (LLMs), both closed-source and open-source (OpenAI, 2023; Touvron et al., 2023), are now available to the community. Evaluating their alignment with human preferences is crucial for selecting appropriate models in downstream applications (Ouyang et al., 2022). To meet this need, Chatbot Arena (Chiang et al., 2024) provides an open platform for eval- uating LLMs based on human preferences. However, it typically takes weeks or even months for a newly released LLM to collect statistically enough human votes.\nTo reduce reliance on human annotations, automatic LLM benchmarks such as AlpacaEval 2.0 (Dubois et al., 2024), Arena-Hard-Auto (Li et al., 2024b), and MT-Bench (Zheng et al., 2023) use LLM-based auto-annotators to evaluate language models. These automatic benchmarks are cheap, scalable, and have high Spearman correlations with Chatbot Arena (Li et al., 2023c). These advan- tages make them popular choices for providing timely assessments of newly released LLMs (Meng et al., 2024; Chen et al., 2024a), where high win rates can lead to significant promotional benefits.\nWhile automatic benchmarks offer a valuable way for comparing LLMs, recent studies have revealed that auto-annotated win rates can be affected by biases related to output length and style (Dubois et al., 2024; Chen et al., 2024b; Zhang et al., 2024). In most cases, these biases are unintentional, stemming from the training data distribution; however, they can still game win rates, causing leader- board results to deviate from actual human preferences. To mitigate this issue, several strategies have been introduced to control for output length and disentangle style from content, thereby reducing the potential for gameability (Dubois et al., 2024; Li et al., 2024a)."}, {"title": "PRELIMINARIES", "content": "LLM-based auto-annotators. We focus on the problem of evaluating outputs from LLMs using auto-annotators. Formally, we define a model \\(LLM : X^* \\rightarrow X^*\\) as a function that transforms an input sequence of tokens into an output sequence of tokens, where \\(X\\) is the vocabulary. Given an instruction \\(I \\in X^*\\), the LLM generates a response \\(LLM(I) \\in X^*\\). To evaluate these responses, we introduce an auto-annotator function \\(JUDGE : X^* \\rightarrow P(Y)\\), where \\(Y\\) represents the evaluation output space, and \\(P(Y)\\) denotes the space of probability distributions over \\(Y\\). For instance, in MT- Bench, there is \\(y = \\{1, 2, ..., 10\\}\\), representing a score range; while in AlpacaEval 2.0, there is \\(Y = \\{m, M\\}\\), indicating binary judgments. The auto-annotator assesses the instruction \\(I\\), the response from the target model \\(LLM_{tar}(I)\\), and optionally, the response from a reference model \\(LLM_{ref}(I)\\). The output of the auto-annotator is either \\(JUDGE(I||LLM_{tar}(I))\\), evaluating the target model alone, or \\(JUDGE(I||LLM_{ref}(I) ||LLM_{tar}(I))\\), comparing the target and reference models to compute win rates.\nThreat model of cheating. The cheater is assumed to have no direct access to the auto-annotator's parameters but can query the auto-annotator through an API provided by a service provider. Ad- ditionally, the cheater has no access to the test input instructions. The cheater's goal is to craft a null model and manipulate the auto-annotator's evaluation to favor the constant, non-informative response outputs from the null model, rather than preferring the responses from the reference model.\nExperimental setup. Our experiments utilize the official evaluation templates associated with different LLM-based evaluations unless stated otherwise. We evaluate our cheating method on AlpacaEval 2.0 (Li et al., 2023c; Dubois et al., 2024), Arena-Hard-Auto (Li et al., 2024b), and MT-Bench (Zheng et al., 2023) as detailed in Table 1. These benchmarks assess the models'"}, {"title": "CHEATING STRATEGIES", "content": "Our initial experiments in Figure 2 indicate that using only an optimized adversarial suffix (without informative responses to input instructions) is ineffective on AlpacaEval 2.0 when GPT-4 acts as the"}, {"title": "CHEATING GPT-4 BASED AUTOMATIC LLM BENCHMARKS", "content": "GPT-4 models are the most widely used state-of-the-art auto-annotators, valued for their powerful evaluation capabilities. To assess the generality of our cheat, we applied it to a range of automatic LLM benchmarks, using the GPT-4-1106-Preview model as the auto-annotator. For RS, we set the number of training instructions N as 10, 8, and 4, the number of optimization steps T as 384, 96 and 64 for AlpacaEval 2.0, Arena-Hard-Auto and MT-Bench, respectively. The full templates and structured responses for Arena-Hard-Auto and MT-Bench are presented in Figures 10 and 11.\nThe effectiveness of our structured response. As mentioned in Section 3, we employ a structured response to facilitate the cheating, which provides a good initial point and could reduce the optimiza- tion cost. To further demonstrate the effectiveness of our structured cheating response, we evaluate logp(winner = NullModel) on a sampled subset of the AlpacaEval 2.0 test instructions using dif- ferent null responses. We compare our structured response with the other 16 persuasive responses, as shown in Figure 3. The results highlight the superiority of our structured response (marked as \"Ours\") because it achieves the lowest log probabilities. This demonstrates the effectiveness of our structured response in cheating the auto-annotator to favor our null model. Additionally, Figure 3 shows that the default configuration, where the baseline is placed second and the target model the"}, {"title": "ABLATION STUDIES ON OPEN-SOURCE AUTO-ANNOTATORS", "content": "To better understand the mechanism behind our method, we conduct extensive ablation studies on auto-annotators based on open-source LLMs. We focus on open-source Llama-3-instruct (8B, 70B parameters) (Meta, 2024; Touvron et al., 2023). These models have been well-aligned by pair-wise preference data and show the ability to evaluate other LLMs.2 For RS, we set N = 8 and T = 8192.\nSanity check. Before we use Llama-3-Instruct models as our auto-annotator in the AlpacaEval framework, we conduct a sanity check to see whether they have such evaluation capability. We evaluate different automatic annotators on the AlpacaEval set by comparing 2.5K human annotations collected by Dubois et al. (2023). As shown in Table 3, both Llama-3-8B-Instruct and Llama-3-70B- Instruct show non-trivial human agreement and correlations. More concretely, Llama-3-8B-Instruct is comparable to ChatGPT, and Llama-3-70B-Instruct matches GPT-4 auto-annotator. Thus, it is reasonable to use them as the auto-annotators.\nIs the structured response useful on open-source auto-annotators? We evaluate the logp(winner = NullModel) on a subset of the AlpacaEval 2.0 test instructions using different null responses. As shown in Figure 4, the structured response has little effect on Llama-3 auto- annotators. In the case of Llama-3-8B-Instruct, the structured response does not exploit positional weaknesses in this model as the log probabilities for the default and swapped positions are gener- ally similar to different persuasive responses. However, on Llama-3-70B-Instruct, we observe that under the swap setting, the structured response manages to reduce the log probability. Additionally, regarding the positional bias, the Llama-3-8B-Instruct shows little position bias as the probabilities for both default and swapped positions are fairly close. In contrast, Llama-3-70B-Instruct shows a clear positional bias under the swapped setting, with a higher log probability, indicating the model's strong preference for the last output (\u201cM\u201d). The larger Llama-3-70B-Instruct model behaves more similarly to the more advanced GPT-4, as it demonstrates a greater response to both the structured re- sponse and positional bias than the smaller 8B model. This suggests that model size may contribute to the susceptibility to our cheating techniques. Overall, the structured response is considerably less effective on the Llama-3 models compared to GPT-4. A possible explanation for this difference is that the instruction-following capabilities of the Llama-3 models, especially the smaller 8B variant, are not as powerful as those of GPT-4, making them less prone to cheating responses.\nIs random search effective on open-source auto-annotators? The results shown in Table 5 demonstrate the effectiveness of random search on open-source auto-annotators like Llama-3-8B- Instruct and Llama-3-70B-Instruct. For Llama-3-8B-Instruct, without random search, the structured response achieves only a 2.9% LC win rate and 1.4% raw win rate. However, when the random search is applied, the win rates surge dramatically to 95.4% (LC) and 86.3% (raw), representing a gain of 92.5 percentage points in the LC win rate. For Llama-3-70B-Instruct, the structured response"}, {"title": "ANTI-CHEATING STRATEGIES", "content": "To address the vulnerabilities exposed by our cheat, benchmark developers must take proactive measures to ensure the safety and integrity of automatic LLM evaluation systems. For example, one immediate step could involve integrating specialized detectors designed to identify and mitigate adversarial manipulations targeting LLM-based benchmarks.\nTemplate paraphrasing. Previous research has suggested that paraphrasing the input can be an effective defense against jailbreaking on language models (Jain et al., 2023). Building on this idea, one potential defense against our cheat is to release only paraphrased versions of the auto-annotator template, while keeping the real template private. The rationale behind this approach is that the para- phrased templates would be harder for adversaries to exploit directly. To evaluate this defense, we experimented using Llama-3-8B-Instruct as the evaluation model. We utilized ChatGPT (OpenAI, 2023) to rewrite the official auto-annotator template into multiple paraphrased variants as shown in Figures 12, 13, 14 and 15. We next conduct a random search on these rewritten templates and tested the optimized response's effectiveness on AlpacaEval 2.0's original (unseen) official auto-annotator template. As shown in Table 5, despite the template paraphrasing, we are still able to achieve high win rates (e.g. 92.1% LC win rate). This demonstrates that simply releasing paraphrased templates is insufficient as a defense mechanism, as the cheat remains effective even when the original tem- plate is kept private. More robust defenses are required to fully address this issue.\nPPL filter. We utilize GPT-4-1106-Preview as the auto-annotator to evaluate the effectiveness of a PPL-based filter. The perplexity (PPL) is computed using GPT-2, following the methodology de- scribed by Alon & Kamfonas (2023). Specifically, we adopt the windowed PPL approach with a window size of 32, as suggested by Jain et al. (2023), to better capture localized fluctuations in per- plexity that may reflect manipulative or adversarial patterns in the output. To ensure that the baseline outputs are not inadvertently filtered, we set the PPL threshold to the maximum perplexity observed from GPT-4-1106-Preview baseline outputs. This ensures that all outputs from the reference model remain unaffected by the filter, allowing us to focus on detecting and filtering out adversarial outputs with higher perplexities. As illustrated in Figure 6, our results demonstrate that despite setting a high threshold, the PPL filter fails to consistently identify adversarial outputs. For instance, our structured response with win rates as high as 76.8% still exhibits perplexities below the threshold, rendering the filter ineffective. This suggests that relying solely on PPL, even in a windowed configuration, is insufficient to robustly detect adversarial manipulations aimed at influencing LLM judgments."}, {"title": "DISCUSSION", "content": "Conclusion. In this paper, we uncover even null models can achieve high win rates by exploiting structural weaknesses in the evaluation process. These findings highlight the need for more robust automatic LLM benchmarks to ensure fair and reliable assessments of LLM performance. As the field of AI continues to evolve, we must address these vulnerabilities to maintain trust in the systems we use to evaluate language models. Failure to do so could lead to widespread manipulation of benchmarks, undermining the progress and credibility of AI research. In summary, while automatic LLM benchmarks provide a scalable and efficient way to evaluate models, they are not immune to cheating. The development of anti-cheating mechanisms and the reconsideration of benchmark design will be crucial steps toward ensuring the reliability and fairness of future LLM evaluations.\nLimitations and future work. Despite the promising findings of our study, there are limitations that must be acknowledged. First, our work primarily focuses on specific benchmarks, and while"}]}