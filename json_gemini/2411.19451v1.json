{"title": "Learning Visual Abstract Reasoning through Dual-Stream Networks", "authors": ["Kai Zhao", "Chang Xu", "Bailu Si"], "abstract": "Visual abstract reasoning tasks present challenges for deep neural networks, exposing limitations in their capabilities. In this work, we present a neural network model that addresses the challenges posed by Raven's Progressive Matrices (RPM). Inspired by the two-stream hypothesis of visual processing, we introduce the Dual-stream Reasoning Network (DRNet), which utilizes two parallel branches to capture image features. On top of the two streams, a reasoning module first learns to merge the high-level features of the same image. Then, it employs a rule extractor to handle combinations involving the eight context images and each candidate image, extracting discrete abstract rules and utilizing an multilayer perceptron (MLP) to make predictions. Empirical results demonstrate that the proposed DRNet achieves state-of-the-art average performance across multiple RPM benchmarks. Furthermore, DRNet demonstrates robust generalization capabilities, even extending to various out-of-distribution scenarios. The dual streams within DRNet serve distinct functions by addressing local or spatial information. They are then integrated into the reasoning module, leveraging abstract rules to facilitate the execution of visual reasoning tasks. These findings indicate that the dual-stream architecture could play a crucial role in visual abstract reasoning.", "sections": [{"title": "Introduction", "content": "One goal of artificial intelligence (AI) is to equip machines with universal reasoning capabilities. Presently, deep learning has emerged as the dominant paradigm in AI, enabling the modeling of data to execute intricate tasks such as image classification (He et al. 2016; Dosovitskiy et al. 2021), object recognition (Girshick et al. 2015; Ronneberger, Fischer, and Brox 2015), and natural language processing (Vaswani et al. 2017). In the field of cognitive science, analogical reasoning has consistently been regarded as the foundation of general intelligence that sets humans apart from animals and is considered the essence of cognition. It is often shaped by the interplay between higher cognitive abilities and the quality of incoming representations (Norman, 1975). However, current deep learning systems still struggle to excel in tasks that demand analogical and relational reasoning."}, {"title": "Related Work", "content": "Most previous work on RPMs has focused on single-stream network frameworks, such as ConvNets with inductive bias and ViTs focused on self-attention.\nEarly attempts at deep learning for RPM used a convolutional neural network (CNN) deep learning model by Hoshen and Werman (Hoshen and Werman 2017). Modern architectures such as WREN (Santoro et al. 2018), CoPINet (Zhang et al. 2019b), Rel-Base (Spratley, Ehinger, and Miller 2020), SRAN (Hu et al. 2021), MRNet (Benny, Pekar, and Wolf 2021), PredRnet (Yang et al. 2023), etc. use variants of convolutional neural networks (LeCun et al. 1998; He et al. 2016) for feature extraction. This suggests that the inductive bias can potentially generalize well in different configurations (Santoro et al. 2017; Jahrens and Martinetz 2020; Zhuo and Kankanhalli 2020; Zhang et al. 2022b; Mondal, Webb, and Cohen 2023; Ma\u0142ki\u0144ski and Ma\u0144dziuk 2022b). Among all previous studies, SCL (Wu et al. 2020) uses the compositional representation of object attributes and their relations for reasoning. Some symbolically inspired models to incorporate logical rule or object vectors into the unidirectional flow framework, e.g. PrAE (Zhang et al. 2021), ALANS learner (Zhang et al. 2022a) and NVSA (Hersche et al. 2023).\nAnother line of work focuses on attention mechanisms (Hahne et al. 2019; Rahaman et al. 2021; Mondal, Webb, and Cohen 2023; Sahu, Basioti, and Pavlovic 2022; Ma et al. 2022). A recent study shows that visual transformers retain more spatial information than CNNs (Raghu et al. 2021)."}, {"title": "Two Stream Networks", "content": "Inspired by the two stream hypothesis, two stream networks are widely explored in the video action recognition field (Simonyan and Zisserman 2014; Carreira and Zisserman 2017; Feichtenhofer, Pinz, and Zisserman 2016; Zolfaghari et al. 2017). I3D (Carreira and Zisserman 2017) builds a two stream 3D-CNN architecture and takes RGB video and optical flow as inputs. SlowFast (Feichtenhofer et al. 2019) is a model that encodes videos with different frame rates. DSNet (Mao et al. 2021) uses a dual-stream network to explore the representation capacity of local and global pattern features for image classification. Chen et al. employ a dual visual encoder containing two separate streams to model both the raw videos and the key-point sequences for sign language understanding (Chen et al. 2022). We introduce dualstream networks to abstract visual reasoning tasks. Although our work uses dual-stream networks like previous studies, we have a different implementation. First, dual-stream network was implemented as a dual encoder module to extract high level image features across different images. Second, we introduce a reasoning module in DRNet, which allows it to extract rules from different RPM problems.\nHow to model the interactions between different streams is non-trivial. I3D (Carreira and Zisserman 2017) uses a late fusion strategy by simply averaging the predictions of two streams. Another way is to fuse the intermediate features of each stream in the early stage by lateral connections (Feichtenhofer et al. 2019), concatenation (Zhou et al. 2021), or addition (Cui, Liu, and Zhang 2019). In this work, our approach directly models local and spatial features via a dual encoder, and then a learnable module is used to fuse the intermediate features of each stream."}, {"title": "Methods", "content": "The structure of DRNet is shown in Figure 2. It consists of two components: (1) a dual encoder module to transform each image into two high-level features, and (2) a reasoning module to score each context candidate group's features. The highest score is selected as the final predicted answer."}, {"title": "Dual Encoder Module", "content": "This module consists of two parallel streams, where a CNN is used to recognize objects to acquire local features, while ViT is used to play a role in attending to the spatial location of objects."}, {"title": "CNN branch", "content": "Our CNN stream has two ResBlocks, each containing a residual branch and a shortcut connection. Each residual branch has two convolutional layers with kernel sizes of 7. Each convolutional layer down-samples the input feature with a stride of 2, which expands the receptive fields of the neurons and allows for the extraction of higher-level information. The shortcut connection applies the Max Pool2d operation twice to match the output size of the residual branch with a stride of 2. In total, our first ResBlock can be formulated as:\n$x^l = ReLU(BN(Conv_{7\\times7}(x^{l-1}))), l \\in 1, 2$ (1)\n$x' = x^l + Maxpool_{2d}(x^0), l=2$ (2)\nwhere $x^l$ represent input features, $l$ represents the layer index of convolutional layers. $x^{cnn\\_out}$ can be obtained by treating $x^2$ in the same way as above through the second ResBlock. We set the filters as [64, 64, 64, 16] from the first to the last convolutional layer."}, {"title": "ViT branch", "content": "ViT branch processes each image parallelly. Our ViT has the same network framework as in (Vaswani et al. 2017), except that we employ 1D learnable positional encodings to add them to patch embeddings for retaining positional information. Our ViT has 8 attentional heads with depth of 12. We first split each image into 16 patches, with each patch size of 20\u00d720. One convolutional layer with a kernel of 20 and a stride 20, is applied to transform into a patch embedding with size of 400. After transformer encoder, we finally obtained an averaged feature vector. The ViT branch can be formulated as:\n$x^{vit\\_out} = ViT(x)$ (3)\nwhere $x^{vit-out}$ represents the output features of ViT branch. To clarify the process, we describe the data flow illustrated in Figure 2. For each RPM problem, we have 8 context images $I_i$, where $i \\in \\{1, 2, ..., 8\\}$, and 8 candidate images $I_i$, where $i \\in \\{1, 2, ..., 8\\}$, which are combined to create an input denoted as $I = [I_1, I_2, ..., I_8, I_1^c, I_2^c, ..., I_8^c]$, with $I \\in R^{(16 \\times 1 \\times 80 \\times 80)}$. This input $I$ is then simultaneously fed into both the CNN branch and the ViT branch with a batch approach. The result is image features: $x^{cnn\\_out} \\in R^{(B \\times 16, 1, 20, 20)}$ from the CNN branch and $x^{vit\\_out} \\in R^{(B \\times 16, 400)}$ from the ViT branch. To ensure compatibility, we reshape $x^{cnn\\_out}$ to $R^{(B \\times 16, 400)}$, aligning its shape with that of $x^{vit\\_out}$. Here, $B$ represents the batch size. Finally, the outputs $x^{vit\\_out}$ and $x^{cnn\\_out}$ are passed to the reasoning module."}, {"title": "Reasoning Module", "content": "This reasoning module consists of an integration module and a rule extractor to fuse high-level features and extract abstract rule representations of RPM problems.\nIntegration Module. Just as the two streams ultimately project to the hippocampus (Huang et al. 2021), DRNet designs an integration module to model the interactions between different streams. To promote order-invariance between the two vectors, a permutation-invariant operator is recommended. One can use the sum operator (SUM). This approach has been employed by (Niebur and Koch 1995; Benny, Pekar, and Wolf 2021).\n$SUM(\u00b7) := x^{cnn\\_out} + x^{vit\\_out}$ (4)\nTo reduce the variance of SUM, a mean (MEA) operator is defined as follows:\n$MEA(\u00b7) := (x^{cnn\\_out} + x^{vit\\_out})/2$ (5)\nAs a variation of the above operators, we propose an adaptive attention operator AUT to automatically combine two streams,\n$AUT(\u00b7) := W_1x^{cnn\\_out} + W_2x^{vit\\_out}$ (6)"}, {"title": null, "content": "Where $w_1$ and $w_2$ are learnable tensors. We provide three methods for determining changes in these two parameters: AUT - L1 normalization, AUT \u2013 L2 normalization, and the unrestricted way (AUT).\nIn the last approach, we concatenate these two streams and implement the learnable attention operator LIN using a linear layer:\n$LIN(\u00b7) := concat(x^{cnn-out}, x^{vit-out})A^T + b$ (7)\nWe compare different operators in Figure 3 and adopt the LIN operator in DRNet.\nAfter feature fusion, we split the fused features x into two groups: $e_i$ and $c_i$, where $i \\in 1, 2, ..., 8$. We then concatenate each $c_i$ with the 8 context features to form $r_i = [e_1, c_2, ..., e_8, c_i]$, with $r_i \\in R^{(B,i,9,400)}$. Next, we pass $r_i$ into the rule extractor to infer the relationships between the nine feature vectors, as depicted in the reasoning module of Figure 2."}, {"title": "Rule Extractor", "content": "The rule extractor consists of two ResBlocks. Each residual branch has two 1D convolutional layers with a kernel size of 7. Each convolutional layer learns to expand the receptive fields of the neurons to extract higher-level relations with a stride of 1. The shortcut connection applies a 1D convolutional layer to the two ResBlocks with a kernel size and stride of 1. In total, our rule extractor can be formulated as:\n$r^l = ReLU(BN(Conv_7(r^{l-1}))), l \\in 1, 2$ (8)\nwhere $l$ represents the layer index of convolutional layers. We set the filters to [64,128] for the first and second convolutional layers. After the skip connection, we apply MaxPool1d(r) to reshape $r_i \\in R^{(B,i,128,400)}$ into $r_i \\in R^{(B,i,128,100)}$. Then, we send $r_i$ to the second ResBlocks as follows:\n$r^l = ReLU(BN(Conv_7(r^{l-1}))), l \\in 3, 4$ (9)\nWe set the filters to [128,64] for the third and fourth layers. After the skip connection, we apply Adaptive Avg Poolld(r) to reshape $r_i \\in R^{(B,i,64,100)}$ into $r_i \\in R^{(B,i,64,16)}$. Finally, we flatten $r_i \\in R^{(B,i,64,16)}$ into $r_i \\in R^{(B,i,1024)}$ to obtain 8 embeddings. The embeddings corresponding to the correct labels are both abstract representations of the rules."}, {"title": "Classifier", "content": "Lastly, we use an MLP consisting of three linear layers to score these features, and the highest score determines the best answer:\n$Answer = arg \\underset{i \\in \\{1,...,8\\}}{max} [MLP(r_i)]$ (10)\nBetween every two linear layers, we have added an ELU function and a BatchNorm1d layer, with a dropout probability of 0.5. For each linear layer, the output dimensions are 512, 256, and 1 respectively."}, {"title": "Experiments", "content": "All datasets include training, validation, and test sets. We utilize a standard batch size of 256 and evaluate the reported accuracy on the test set using the best validation accuracy checkpoint. The same set of hyperparameters is applied across all benchmarks, employing Adam (Da 2014) optimizer with a learning rate of 3e-4, \u03b2 values of (0.9, 0.999), and a weight decay of le-6. No additional supervision signals, such as metadata, are utilized during training. Additionally, for RAVEN-style datasets, we present the median outcome from 5 distinct runs. Given the computational demands of training on large-scale PGM datasets, we provide a single result, aligning with the approach of prior works (Zhang et al. 2019b; Benny, Pekar, and Wolf 2021; Yang et al. 2023). In the experimental results presented below, when the validation loss no longer decreases within 20 epochs, we perform early stopping."}, {"title": "Main Results", "content": "We conducted experiments on PGM, RAVEN, I-RAVEN and RAVEN-FAIR, all of which have predefined training, validation and test data splits. During training, we used the training set for model training and the test set for evaluation, while the validation set was used to select the optimal checkpoint for evaluation. We used vertical/horizontal flip data augmentation with a probability of 0.3 for RPM training samples."}, {"title": "State-of-the-art Comparisons", "content": "We compare DRNet with several previous models, including WReN (Santoro et al. 2018), CoPINet (Zhang et al. 2019b), MRNet (Benny, Pekar, and Wolf 2021), SCL (Wu et al. 2020), MLRN (Jahrens and Martinetz 2020), Rel-Base (Spratley, Ehinger, and Miller 2020), ARII (Zhang et al. 2022b), STSN (Mondal, Webb, and Cohen 2023) and PredRNet (Yang et al. 2023). We have also compared our method with end-to-end symbolic methods such as NVSA (Hersche et al. 2023). Experiments were conducted on PGM Neutral and three RAVEN-style datasets.\nTable 1 shows the main results on four datasets. First, our DRNet achieves the best average performance on the four datasets compared to single-stream models, such as STSN and PredRNet. STSN introduces slot attention to extract image-wise features and then proposes a transformerbased module to explore relationships between contexts and choices for reasoning. PredRNet introduces prediction error into ConvBlocks to improve reasoning performance. PredRNet provides the best average performance (96.7%) among all compared methods. While DRNet outperforms PredRNet with an average performance of 97.78%. In addition, DRNet actually achieves better performance on the RAVEN (1.09%), I-RAVEN (+1.12%), and RAVEN-FAIR (+0.48%) datasets than PredRNet, respectively. Some recently proposed methods, such as STSN, MLRN, and ARII, only show good results on one or two datasets. For example, CoPINet and MLRN only perform well on RAVEN (91.4%) and PGM-N (98.0%) respectively. In contrast, DRNet shows superior results on all 4 datasets, which suggests the generalization performance on different datasets. Second, compared to certain competitive models like MRet, our approach extracts rules only from combinations of 8 image sets, without the need for column rule learning. If we remove the ViT branch in DRNet, our model is similar to Rel-Base. If we remove the CNN branch, our model completely degenerates into an attention-based model. However, neither branch performed well enough (see ablation experiment), indicating that the two-stream design of our model was critical and helped our model achieve an average result of 97.78%."}, {"title": "Out-of-Distribution Generalization in PGM", "content": "In addition to the Neutral dataset of PGM, the remaining seven datasets were employed to evaluate the model's capacity to handle out-of-distribution scenarios. We assessed DRNet's performance across all sub-datasets of PGM while maintaining consistent model settings. The outcomes of these evaluations are meticulously documented in Table 2. A careful examination of the data presented in Table 1 reveals that our proposed model exhibited only a marginal average enhancement of 1.09% for in-distribution datasets. However, when confronting challenges posed in out-of-distribution scenarios, our model showcased a notable enhancement, reaching up to 11.23%. This highlights the robust learning prowess inherent in the dual-stream architecture, enabling it to effectively handle OOD challenges."}, {"title": "Ablation Experiments", "content": "We conducted ablation experiments on both the I-RAVEN dataset, representing in-distribution, and the PGM HO AP dataset, representing OOD. Since the three RAVEN-style datasets have similar distributions, we only tested on one."}, {"title": "Different hyper-parameters", "content": "We evaluated the impact of different depths of ViTs and various sizes of convolutional kernels on model performance. We selected ViT depths of 4, 8, 12 (DRNet), and 16. Regarding convolution, we investigated common kernel sizes: 3, 5, and 7 (DRNet). The results are presented in Table 3. DRNet demonstrates a gradual improvement in performance with increasing ViT depth. However, when the depth is 16, the performance of DRNet on HO AP decreases from 93.7% to 89.7%. Additionally, DRNet's performance gradually improves with larger convolutional kernel sizes on both datasets. These results indicate that the current hyperparameters for DRNet are optimal."}, {"title": "Single stream v.s. Dual stream", "content": "Compared with singlestream models like MRNet and PredRNet, DRNet performs very well, especially in the OOD scenario. To help understand our proposed model, DRNet, the first thing we aimed to clarify is the role of each branch. The results are shown in Table 4."}, {"title": "Dual CNN Stream v.s. Dual ViT Stream", "content": "Subsequently, we embarked on the replacement of the network's branches to ascertain whether any dual-stream model comprising distinct network components possesses adept relational reasoning capabilities. Given that the tensor shapes of input and output for each stream in DRNet remain consistent, our focus is centered on assessing whether the parameter count of the new networks was comparable to that of DRNet. For simplicity of description, we define the CNN branch network in DRNet as CNN-base and the ViT branch as ViT-base.\nFirst, we replace the ViT-base in DRNet with ResNet-32 to form a dual-CNN network (DCNet-24M), where M represents learnable parameters, in millions. For the ResBlocks, we set filters to [64 \u00d7 3, 128 x 4, 256 \u00d7 6,512 \u00d7 3], where [3, 4, 6, 3] represents the repeat times of ResBlocks, and all convolutional layers in the ResBlocks have a kernel size of 7. The first convolutional layer in ResNet-32 has a kernel size of 3, and the filter is set to 64.\nSecond, we replace the CNN-base in DRNet with a shallow ViT-small of depth 1 to create a dual-ViT network (DVNet-s-26M). The network architecture of ViT-small is the same as ViT-base.\nThird, we use two ViT-b models to construct a larger dualViT network (DVNet-h-47M). Fourth, DVNet-h increases the number of parameters drastically. Given that we are applying attention to the sum of ViT-base and CNN-base, we instead utilized a patch-based CNN from (Brutzkus et al. 2022), replacing ViT-base to create a patch-based DRNet (DRNet-P-3.4M). We tested the four aforementioned network configurations on the I-RAVEN and PGM HO AP datasets, and the results are shown in Table 5."}, {"title": "Rule Representations", "content": "Although DRNet achieves a high recognition accuracy, we still lack an understanding of its reasoning process. All rule extraction takes place within the Rule Extractor. Therefore, we conducted a t-SNE (van der Maaten and Hinton 2008) analysis of the embeddings corresponding to the correct answers of the rule extractor. Due to the variety of rules covered by RAVEN problems, it is challenging to visualize vectors with multiple rule labels. Therefore, we selected the PGM Neutral dataset for rule visualization due to its smaller number of rules. Based on the actual rule types of each problem (AND, OR, XOR, Consistent union, Progression), we visualized 200k test samples, and the results are shown in Figure 4. The rule extractor module can identify and form abstract rule representations for downstream classification task.\nFurthermore, we utilized radial basis function kernel principal component analysis to reduce the 200k rule representations from 1024 to 768 dimensions, preserving spatial information and reducing redundancy. Subsequently, we computed pairwise cosine similarities for the 5 rule categories, resulting in 10 sets of scores. The mean values of these scores range from -0.01 to 0.007, indicating near orthogonal representations and this may be a reason for the superior performance of DRNet."}, {"title": "Visualization of Two Streams", "content": "Cadieu and Olshausen shows that learning both ventral and dorsal-like representations in a single ANN with two pathways is possible if one forces the two pathways to process separately the phase and amplitude of a complex decomposition of the stimuli (Cadieu and Olshausen 2012).\nTo begin to understand how the two streams in DRNet process images, we analyze their internal representations. Self-attention allows ViT to integrate information across the entire image even in the lowest layers. We investigate to what degree the network makes use of this capability. For the analysis of ViT, we adopt the approach used in (Vaswani et al. 2017). We find that ViT attends to image regions that are relevant for spatial information, as shown in Figure 5 (a), working like the where pathway.\nFor the visualization of CNN convolutional layers, we obtained the results through a single forward pass, as shown in Figure 5 (b). We found that during the learning process, CNN gradually acquires high-level image representations by combining local features, working like the what pathway. We also investigated the similarity between the emerging representations. We computed the cosine similarity for dual encoder representations on PGM-N and I-RAVE test sets (Figure 6). It can be seen that in both datasets, the learned dual encoder representations exhibit small cosine similarities."}, {"title": "Discussion", "content": "DRNet shows superior performance on multiple datasets, highlighting the potential of this dual-stream architecture. In our work, we use two backbone networks to mimic the two streams of visual processing in mammalian brains, and the high-level features obtained by these two streams form clearer discrete abstract rules through a rule extractor. With its remarkable generalization performance and accuracy across multiple benchmarks, DRNet provides an effective and powerful baseline in visual abstract reasoning.\nTo our knowledge, there is no existing research on dualstream architectures within these benchmarks. To encourage further work, the technical shortcomings of the model are highlighted and explained in detail. The first question is whether a larger convolution kernel affects the model's ability to extract features, and the second is that the visual transformer module we used selected a large patch size to match the size of the CNN branch, which may have affected the visual transformer's ability to generalize global and local information. Currently, our model has poor recognition results in RAVEN-style 3 \u00d7 3 grid configuration. This may be due to the size of the convolution kernel and the patch size of ViT, which needs further investigation in the future.\nThe two-stream hypothesis involves many brain regions, and rigorous modeling is very challenging. At the framework level, we can extend our model to include more regions, in particular the hippocampal formation, which has long been considered as the basis for memory formation, flexible decision-making and reasoning (Behrens et al. 2018; Whittington et al. 2020, 2022). In (Bakhtiari et al. 2021), the functional specialization of the visual cortex emerges from training parallel pathways with self-supervised predictive learning; the potential to incorporate such functionality into DRNet for similar tasks is still under exploration.\nMultimodal perception is necessary to achieve general artificial intelligence. Models combined with language models can exhibit zero-shot or few-shot learning capabilities in such non-verbal reasoning tasks (Huang et al. 2023), and our framework can flexibly integrate multimodal information in the future."}, {"title": "Conclusion", "content": "We applied DRNet to multiple datasets and observed that it achieves a high level of recognition accuracy and demonstrates an ability to generalize. Our experiments revealed that the dual-stream framework does not lead to the superposition of effects from the respective branches. In ablation experiments, we discovered that having more learnable parameters in a dual-stream network does not necessarily result in improved performance, and the rule extractor designed in DRNet can learn discrete abstract rule representations. We have illustrated the effectiveness of this work and its potential for abstract visual reasoning."}]}