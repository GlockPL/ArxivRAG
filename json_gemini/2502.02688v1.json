{"title": "EFFICIENT IMPLEMENTATION OF THE GLOBAL CARDINALITY CONSTRAINT WITH COSTS", "authors": ["Margaux SCHMIED", "Jean-Charles REGIN"], "abstract": "The success of Constraint Programming relies partly on the global constraints and implementation of\nthe associated filtering algorithms. Recently, new ideas emerged to improve these implementations\nin practice, especially regarding the all different constraint.\nIn this paper, we consider the cardinality constraint with costs. The cardinality constraint is a gener-\nalization of the all different constraint that specifies the number of times each value must be taken\nby a given set of variables in a solution. The version with costs introduces an assignment cost and\nbounds the total sum of assignment costs. The arc consistency filtering algorithm of this constraint\nis difficult to use in practice, as it systematically searches for many shortest paths. We propose a new\napproach that works with upper bounds on shortest paths based on landmarks. This approach can be\nseen as a preprocessing. It is fast and avoids, in practice, a large number of explicit computations of\nshortest paths.", "sections": [{"title": "1 Introduction", "content": "In Constraint Programming (CP), a problem is defined on variables and constraints. Each variable is provided with\na domain defining the set of its possible values. A constraint expresses a property that must be satisfied by a set of\nvariables. CP uses a specific resolution method for each constraint.\nThe success of CP relies on the use of high-performance filtering algorithms (also known as propagators). These\nalgorithms remove values from variable domains that are not consistent with the constraint, i.e. that do not belong\nto a solution of the constraint's underlying sub-problem. The most well-known propagator is that of the all different\n(alldiff) constraint, which specifies that a set of variables must all take different values. The efficiency in practice\nof that propagator strongly depends on its implementation. Thus, algorithms proposing practical improvements on\nR\u00e9gin's algorithm [1] are still appearing [2, 3].\nIn this article, we consider another constraint introduced by R\u00e9gin that is also popular [4, 5, 6, 7, 8]: the cardinality\nconstraint with costs [9]. We propose to try to speed up its filtering algorithm when there is nothing to deduce. This\nis often the case at the start of the search, particularly as the optimal value is far from known. In addition, at this\nstage, the gains can be significant since few values have been removed from the domains, and so the complexity of\nthe algorithms is greater. This approach can be particularly interesting with aggressive restarting methods and could\nsimplify the use of CP: there is less need to worry about the inference strength of constraints versus their cost. We can\nworry less about the type of filtering to be used and consider the arc consistency right away.\nThe global cardinality constraint (gcc) [10] is a generalization of the alldiff constraint. A gcc is specified in terms of a\nset of variables X = {x1, ..., Xp} which take their values in a subset of V = {v1, ..., va}. It constrains the number of\ntimes a value vi \u2208 V is assigned to variables in X to belong to an interval [li, ui].\nA gcc with costs (costgcc) is a generalization of a gcc in which a cost is associated with each value of each variable.\nThen, each solution of the underlying gcc is associated with a global cost equal to the sum of the costs associated with\nthe assigned values of the solution. In a costgcc constraint the global cost must be less than a given value, H."}, {"title": "2 Preliminaries", "content": "The following definitions, theorems and algorithms are based on the following papers and books: [9, 12, 13, 14, 15].\nConstraint Programming: A finite constraint network N is defined as a set of n \u2208 N variables X = {x1,..., Xn},\na set of current domains D = {D(x1),..., D(xn)} where D(xi) is the finite set of possible values for variable xi,\nand a set C of constraints between variables. We introduce the particular notation Do = {Do(x1), ..., Do(xn)} to\nrepresent the set of initial domains of N on which constraint definitions were stated. A constraint C on the ordered set\nof variables X (C) = (Xi\u2081, ..., Xi\u2084) is a subset T(C) of the Cartesian product Do(Xi\u2081) \u00d7\u30fb\u30fb\u30fb\u00d7 Do(xi) that specifies\nthe allowed combinations of values for the variables Xi\u2081,..., Xir. An element of Do(xi\u2081) \u00d7 \u00b7\u00b7\u00b7 \u00d7 Do(xi) is called\na tuple on X(C) and is denoted by \u0442. In a tuple t, the assignment of the ith variable is denoted by Ti. var(C,i)\nrepresents the ith variable of X (C). A value a for a variable x is often denoted by (x, a). Let C be a constraint. A\ntuple 7 on X (C) is valid if \u2200(x, a) \u2208\u03c4,\u03b1 \u2208 D(x). C is consistent iff there exists a tuple \u0442 of T(C) which is valid. A\nvalue a \u2208 D(x) is consistent with C iff x \u2209 X (C) or there exists a valid tuple 7 of T(C) with (x, \u03b1) \u0395\u03c4.\nThe costgcc constraint is formally defined as follows.\nDefinition 1 ([9]) A global cardinality constraint with costs is a constraint C associated with a cost function on\nX(C) cost, an integer H and in which each value a\u017c \u2208 D(X(C')) is associated with two positive integers li and u\u017c\nT(C) = { + such that \u0442 is a tuple on X (C)\nand $\\forall a_i \\in D(X(C')): l_i \\leq \\#(a_i, \\tau) \\leq u_i $\nand $\\sum_{(C)} cost(var(C, i), \\tau[i]) \\leq H $\nIt is denoted by costgcc(X, l, u, cost, H).\nTo understand how arc consistency on a costgcc is established, some concepts from graph theory and flow theory are\nrequired.\nGraph theory: A directed graph or digraph G = (X,U) consists of a node set X and an arc set U, where every\narc (x, y) is an ordered pair of distinct nodes. We will denote by X (G) the node set of G and by U(G) the arc set of\nG. The cost of an arc is a value associated with the arc. When costs are associated with arcs, one should talk about\nweighted directed graphs.\nA path from node 11 to node xk in G is a list of nodes [x1,...,xk] such that (xi, Xi+1) is an arc for i \u2208 [1, k \u2212 1]. The\npath is called simple if all its nodes are distinct. The cost of a path P, denoted by cost(P), is the sum of the costs of\nthe arcs contained in P. A shortest path from a node s to a node t is a path from s to t whose cost is minimum.\nFlow theory: Let G be a digraph where each arc (x, y) is associated with three information: lxy the lower bound\ncapacity, Uxy the upper bound capacity and Cry the cost of the arc.\nA flow in G is a function f satisfying the following two conditions:"}, {"title": "3 Filtering Algorithm", "content": "Our work builds on top of the original costgcc filtering (i.e., [9]). Before presenting how we speed up the algorithm\nfor costgcc, let us briefly review the original algorithm.\nThere is a relation between a costgcc and the search for min-cost flow in a particular graph.\nDefinition 3 ([9]) Given C = costgcc(X,1,u, cost, H). The value graph of C is the bipartite graph GV(C) =\n(X(C'), D(X(C')), U) where (x, a) \u2208 U if a \u2208 Dx. The value network of C is the directed graph N(C) with lxy the\nlower bound capacity, uxy the upper bound capacity and cxy the cost on arc from the node x to the node y. N(C) is\nobtained from the value graph GV (C) by:\n\u2022 Orienting each edge of GV (C') from values to variables. $\\forall x \\in X(C') : \\forall a \\in D(x) : l_{ax} = 0, u_{ax} = 1$ and\n$C_{ax} = cost(x, a)$.\n\u2022 Adding a node s and an arc from s to each value. $\\forall a \\in D(X(C')) : l_{sa} = l_a, u_{sa} = u_a$ and $c_{sa} = 0$.\n\u2022 Adding a node t and an arc from each variable to t. $\\forall x \\in X(C) : l_{xt} = 1, U_{xt} = 1$ and $C_{xt} = 0$.\n\u2022 Adding an arc (t, s) with $l_{ts}= u_{ts}=|X(C)|$ and $c_{ts}=0$.\n\u2022 For any arc (x, y), fxy represents the amount of some commodity that can \u201cflow\u201d through the arc. Such a flow\nis permitted only in the indicated direction of the arc, i.e., from x to y. For convenience, we assume fxy = 0 if\n(x, y) \u2209 U(G).\n\u2022 A conservation law is fulfilled at each node: $\\forall y \\in X(G) : \\sum_x f_{xy} = \\sum_z f_{yz}$.\nThe cost of a flow f is cost(f) = \u2211(x,y)\u2208U(G) fxyCxy.\nThe feasible flow problem consists in computing a flow in G that satisfies the capacity constraint. That is finding f\nsuch that $\\forall(x, y) \\in U(G) l_{xy} \\leq f_{xy} \\leq U_{xy}$. The minimum cost flow problem consists in finding a feasible flow f\nsuch that cost(f) is minimum.\nA min cost flow can be computed thanks to the augmenting shortest path algorithm. The main idea of the basic\nalgorithms of flow theory is to proceed by successive improvement of flows that are computed in a graph in which all\nthe lower bounds are zero and the current flow is the zero flow (i.e., the flow value is zero on all arcs).\nFirst, assume that there is no lower capacity. So, consider that all the lower bounds are equal to zero and suppose that\nyou want to increase the flow value for an arc (x, y). In this case, the zero flow is a feasible flow. Let P be a shortest\npath from y to x different from (y, x), and val = min({uxy}\u222a{Uab s.t. (a, b) \u2208 P}). Then we can define the function\nf on the arcs of G such that fab = val if (a, b) \u2208 Por (a, b) = (x, y), and fab = 0 otherwise. This function is a flow\nin G and fry > 0. Now, from this flow we can define a particular graph without any flow value and all lower bounds\nequal to zero, the residual graph.\nDefinition 2 The residual graph for a given flow f, denoted by R(f), is the digraph with the same node set as in G\nand with the arc set defined as follows:\n\u2200(x, y) \u2208 U(G):\n\u2022 fxy < Uxy(x,y) \u2208 U(R(f)) and has cost rCxy = Cxy and upper bound capacity rxy = Uxy \u2212 fxy.\n\u2022 fxy> lxy (y, x) \u2208 U(R(f)) and has cost rcyx = -Cxy and upper bound capacity ryx = fxy \u2013 lxy.\nAll the lower bound capacities are equal to 0.\nThen, we can select an arc and apply the previous algorithm on this arc in order to increase its flow value. By dealing\nonly with shortest path we can guarantee that the computed flow will have a minimum cost.\nNow consider the lower capacities. In this case, we can use the algorithm mentioned by R\u00e9gin:\nStart with the zero flow f\u00ba. This flow satisfies the upper bounds. Set f = f\u00b0, and apply the following process while\nthe flow is not feasible:\n1) pick an arc (x, y) such that fxy violates the lower bound capacity in G (i.e., fxy <lxy).\n2) Find P a shortest path from y to x in R(f) \u2013 {(y,x)}.\n3) Obtain f' from f by sending flow along P; set f = f' and goto 1)\nIf, at some point, there is no path for the current flow, then a feasible flow does not exist. Otherwise, the obtained flow\nis feasible and is a minimum cost flow."}, {"title": "4 Upper Bounds of Shortest Paths", "content": "Although Corollary 1 reduces the number of computations required to establish the arc consistency of the constraint,\nit systematically computes a large number of shortest paths. Precisely, the algorithm involves computing the shortest\npath between each assigned value and all other values which makes it difficult to use in practice. In addition, the\nconstraint is often arc consistent, rendering any computation useless. The aim of our approach is therefore to reduce\nthe number of operations computed unnecessarily.\nWe present a much more applied approach, based on the fact that Corollary 1 relies on the existence of a path of length\nless than a given value. It is not necessary to know the path precisely, or even to know its value. An upper bound is\nsufficient.\nWe can therefore immediately establish the following proposition:\nProposition 1 Let B+(x,a) \u2265 dr(f)(x, a) be any upper bound on the length of the shortest path from x to a. If\n$B^+(x, a) < H \u2212 cost(f) - r_{cax}$\nthen the value a of a variable x is consistent with C.\nA good way of obtaining an upper bound on a distance between two points is to use the triangle inequality. Here we\nare talking about the triangle inequality with respect to the shortest path distances in the graph, not an embedding in\nEuclidean space or some other metric, which need not be present.\nProperty 3 Let x, y, and p be three nodes of a graph. According to the triangle inequality computed on shortest paths,\nwe have:\n$d(x,p) + d(p, y) \\geq d(x, y)$\nHere, p is a particular node called landmark.\nUpper bounds obtained by the triangular inequality have been shown to be useful for guiding the computation of\nshortest paths. The ALT algorithm, yielding excellent results in practice for computing shortest paths in a very large\ngraph, is based on this technique [11].\nProperty 2 and Corollary 1 can be rewritten for landmarks:\nProposition 2 Given any variable x such that fbx = 1, a any value of x and p any landmark. If one of the two\ncondition is satisfied\n$d_{R(f)}(x,p) +d_{R(f)}(p,a) \\leq H \u2212 cost(f) - r_{cax}$\n$d_{R(f)}(b,p) +d_{R(f)}(p,a) \\leq H \u2212 cost(f) - r_{cax} - r_{cxb}$\nthen the value a of x is consistent with C.\nThe residual graph may have several strongly connected components. Each component must be treated separately.\nThus, at least one landmark per component must be selected.\nThanks to the use of upper bounds we can go even further. It is possible to compute the consistency of all values of\nvariables of a strongly connected component by checking a single condition.\nDefinition 4 Consider S a strongly connected component of R(f), p a landmark in S, x \u2208 S a variable, and a a\nvalue of x. We define:\n\u2022 $J_{max}^{R(f)}(\\cdot, p) = max_{x \\in S}(d_{R(f)}(x, p))$\n\u2022 $d_{max}^{R(f)}(p, \\cdot) = max_{x \\in S}(d_{R(f)}(p, x))$\n\u2022 $r_{cmax} = max_{x \\in S, a \\in D(x)}(r_{Cax})$\nThis leads to the following proposition:\nProposition 3 Given S a strongly connected component of R(f) and p a landmark in S. If\n$d^{nf)}_{R(f)}(\\cdot, p) + d^{nf)}_{R(f)}(p,\\cdot) \\leq H \u2212 cost(f) \u2013 r_{cmax}$\nthen all the values of all the variables involved in S are consistent with C."}, {"title": "5 Improved Filtering Algorithm", "content": "We can now describe Algorithm 1, which eliminates values that are inconsistent with the constraint. The algorithm\ntakes as parameters a min cost flow f, its residual graph R(f), a strongly connected component represented by its\nset of nodes S and P a set of landmarks of S. This algorithm must therefore be called for each strongly connected\ncomponent. The algorithm begins by checking whether Proposition 3 holds. If true, then the algorithm stops, since\nthis means that all the values of the variables in the connected component S are consistent. Otherwise, it is necessary\nto check each value potentially inconsistent individually. So, for each of those values Proposition 2 is checked. If it is\nsatisfied, then the value is consistent, otherwise an explicit shortest path is computed to determine whether the value\nis consistent or not.\nWhen testing Corollary 1, we could refine the algorithm by identifying the nodes for which we need to search for a\nshortest path from b to them, but this is not interesting in practice as the shortest path algorithm will quickly find that\nthey are at an acceptable distance from b.\nPractical improvements: One can compute landmarks only when they are needed. This consideration is effective\nin practice and a simple modified version of the basic algorithm is possible. This modification proceeds by iteration\nover the landmarks. Consider V the set of values for which a shortest path must be computed.\nThe following process is defined: The landmark p is considered. Proposition 3 is checked according to p. If it is\nsatisfied then V is emptied (all values are consistent) otherwise the values V that satisfies Proposition 2 according to\np are removed from V, because they are consistent.\nThis process is repeated while V is not empty and some landmarks remain. In other words, the landmarks are succes-\nsively considered while the status of some values is not determined.\nIf there are no more landmarks to compute, then, and only then, shortest paths are explicitly computed for the value\nin V. In practice, it is frequent to find that all values are consistent without using all the landmarks. This practical\nimprovement means that not all landmarks need to be systematically computed.\nNote that the landmark approach subsumes all the practical improvements proposed by R\u00e9gin."}, {"title": "6 Landmark Selection", "content": "There are different methods for selecting landmarks.\nRandom: A landmark is randomly selected. This method is fast to find landmarks, so we used it to compare to other\nmethods.\nOutline: The method is based on an approximation of the outlines of a graph.\nDefinition 5 The outlines of a graph G are defined by one or more pairs of nodes (x, y) with x, y \u2208 X that maximize\nthe minimum distance between x and y among all pairs of nodes in the graph.\nTo find the pair of nodes representing the outline, we use a well-known 2-approximation. First, we perform a shortest-\npath search starting from an arbitrary node x, then select the node y, which is the furthest node from x, as a landmark.\nAlgorithm 1: Arc Consistency Algorithm for a Strongly Connected Component\nARCCONSISTENCYWITHLANDMARKS(f, Rf, S, P);\nfor p E P do\nL\nd(p,\u00b7) \u2190 shortestPathr(f)(p, \u00b7) // shortest path in R(f) ;\nd(,p) \u2190 \u00b7 shortestPath\u0101(f)(p, \u00b7) // shortest path in R(f), the reverse graph of R(f) ;\n// Check of Proposition 3;\nrcmax \u2190 maxx\u2208S,a\u2208D(x)(rCax);\nfor p E P do\ndnf(,p)\nmaxx\u2208s(dr(f)(x,p));\ndnf) (p,)\nmaxx\u2208s(dr(f) (p, x));\nif d'f(, p) + dr(f)(p,\u00b7) < H \u2212 cost(f) \u2013 rcmax then\n// all values of all variables of S are consistent ;\nreturn;\nA \u2190 {a such that fsa > 0};\nfor value b \u2208 A do\nfor x such that fbx = 1 do\n(b) \u2190 {a such that a \u2208 D(x) and a \u2260 b};\ncomputePath \u2190 false\n// Check of Proposition 2;\nfor a \u2208 D(x) while not compute Path do\ndpmin \u2190 minp\u2208P(dR(f)(x,p) + dr(f)(p,a));\nif dpmin > H - cost(f) - rcax then\ncomputePath \u2190 true ;\n// Check for an explicit shortest path computation ;\nif compute Path then\ndr(f)(b, ) \u2190 shortestPath(b, \u00b7) ;\nfor a \u2208 D(x) do\nif dr(f)(b,a) > H \u2212 cost(f) - rcax - rCab then remove a from D(x);\nAs far as the shortest path algorithm is concerned, it is interesting to remove the negative costs from the residual graph\nin order to use Dijkstra's algorithm, as mentioned by R\u00e9gin. It only requires one shortest path computation [9].\nComplexity: Let SP be the complexity of computing a shortest path from one node to all others. R\u00e9gin's algorithm\nhas a complexity of \u03a9(\u03b4 \u00d7 SP) in the best case and O(\u03b4 \u00d7 SP) in the worst case, where d is the number of assigned\nvalues. With landmarks, the complexity in the best case is in \u03a9(FindP + |P| \u00d7 SP) where |P| is the number of\nlandmarks and FindP is the complexity of finding the landmarks. This complexity is obtained when Proposition 3\ndetects that every value is consistent. Note that, this detection can happen on the first landmark and so we can have\n|P| = 1. In the worst case, the complexity is the same as that of R\u00e9gin's algorithm, provided that |P| is in O(\u03b4) and\nFindP is in O(\u03b4 \u00d7 SP). As with the ALT method, we consider several landmarks in order to have a better chance of\nfinding landmarks that avoid explicit shortest path computations."}, {"title": "7 Experimentation", "content": "The experiments were carried out on a computer with an Intel Core i7-3930K CPU 3,20 GHz processor, 64 GB of\nmemory and running under Windows 10 Pro. All algorithms were implemented in Java (openjdk-17) in an internal CP\nsolver.\nThe results relate to the solving of four problems, the traveling salesman problem (TSP) [16], the StockingCost prob-\nlem [17], the flexible job shop scheduling problem (FJSSP) [18, 19] and a problem of assigning child to activities\n(CHILD) [20]. The TSP data are the instances (77) of the TSPLIB [21] having less than 1,500 cities. Some of them\ninvolve more than a million of edges. The StockingCost data are those used in a Houndji's paper [17], this is random\ndata distributed define as 100 instances with 500 periods. Precisely, the StockingCost instances have 500 variables and\n500 values. The FJSSP data come from two different sources, given by Pelleau [18] and Weise [19]. There are 370\ninstances with between 5 and 20 variables linked to a few values (between 5 and 10), and most instances have between\n50 and 300 arcs. The CHILD instance contains only real-life data from [20]. There are 623 children and 317 activities.\nEach child must be assigned to one activity. One activity can be associated with multiple children.\nFor each instance of each problem, we measure the information relating to the establishment of the arc consistency of\nthe costgcc constraint at the root of the search tree. The mean of the results for each data set are reported in the tables."}, {"title": "7.2 Results Analysis", "content": "Table 3 gives the time required by each method.\nTable 1 shows that our approach generally computes significantly fewer shortest paths than R\u00e9gin's algorithm for all\nlandmarks selection methods. For the TSP instances, we compute on average between 2 and 47 times fewer shortest\npaths than R\u00e9gin's algorithm. The difference is significant for all instances except for the StockingCost instances with\nRegular H. It should be noted that our approach is always better or equivalent and allows us to detect quickly whether\nthe constraint is arc consistent in certain cases.\nIn the best case, our approach does not compute any shortest paths other than those required to determine landmarks.\nOur approach can compute more shortest paths only when there is no inconsistent arc and the extra computation is due\nto the landmarks. The number of useless path computations is also reduced by our method (See Table 2).\nFor computation times, we find the same kind of results as before (See Table 3). The gain average factors evolve\nbetween 1 and 57."}, {"title": "8 Conclusion", "content": "This paper proposes an efficient implementation of the arc consistency algorithm of the cardinality constraint with\ncosts. This constraint is present in many industrial problems and the establishment of the arc consistency is often too\nslow to be used in practice, as it is based on finding the shortest paths from the assigned values. We introduce a new\nmethod that uses upper bounds on shortest paths based on triangular inequalities and landmarks. This approach avoids\nthe computation of many shortest paths and improves the computation time of the arc consistency filtering algorithm.\nThe larger the graph and the larger the margins, the greater the improvement will be. In addition, we have introduced\na sufficient condition, which is quick to compute, for a costgee constraint to be arc consistent."}]}