{"title": "Sortability of Time Series Data", "authors": ["Christopher Lohse", "Jonas Wahl"], "abstract": "Evaluating the performance of causal discovery algorithms that aim to find causal relationships between time-dependent processes remains a challenging topic. In this paper, we show that certain characteristics of datasets, such as varsortability Reisach et al. [2021] and R2-sortability Reisach et al. [2023], also occur in datasets for autocorrelated stationary time series. We illustrate this empirically using four types of data: simulated data based on SVAR models and Erd\u0151s-R\u00e9nyi graphs, the data used in the 2019 causality-for-climate challenge [Runge et al., 2019], real-world river stream datasets, and real-world data generated by the Causal Chamber of [Gamella et al., 2024]. \u03a4\u03bf do this, we adapt var- and R2-sortability to time series data. We also investigate the extent to which the performance of score-based causal discovery methods goes hand in hand with high sortability. Arguably, our most surprising finding is that the investigated real-world datasets exhibit high varsortability and low R2-sortability indicating that scales may carry a significant amount of causal information.", "sections": [{"title": "INTRODUCTION", "content": "Inferring causal relationships between variables in a multivariate time series setting is an ongoing topic of research in many fields, including economics/econometrics [Varian, 2016], climate science [Runge et al., 2019, Runge, 2020], medicine [Yazdani and Boerwinkle, 2015] and neuroscience [Bergmann and Hartwigsen, 2021]. The process of inferring causal structure from data is often referred to as causal discovery or causal structure learning. Most of the literature on causal discovery is devoted to the following two types of methods: Constraint-based causal structure learning algorithms, such as the PC-algorithm [Spirtes and Glymour, 1991] and PCMCI [Runge, 2020] for time series data, use tests of conditional independence to iteratively learn a causal graph. Score-based method such as GES [Chickering, 2002] fit a directed acyclic graph (DAG) to the data by optimising a score function, but need to search a large discrete space of DAGs which is an NP-hard problem.\nMore recently, [Zheng et al., 2018] have proposed to embed the discrete DAG-search space into a continuous one through a differentiable acyclicity constraint to make the problem amenable to continuous (gradient based) optimisation. The method introduced in [Zheng et al., 2018], NOTEARS, showed impressive performance on data simulated with linear additive noise models (LANMs). However, as shown by Reisach et al. [2021] the strong performance of NOTEARS and similar methods on LANM data vanished after the data was normalized. Reisach et al. [2021] noticed that, before normalization, additive noise model data is highly varsortable, meaning that, on average, the causal order of the system can be recovered well by sorting the variables by the amplitude of their estimated variances. Since high varsortability and good NOTEARS performance were highly correlated, they therefore conjectured that NOTEARS implicitly made use of variance sorting in its optimisation. The issue has recently been revisited by Ng et al. [2024], who pointed out that (a) NOTEARS does not necessarily perform well in the presence of high varsortability and that (b) normalisation of data generated by LANMs moves the data far away from the assumption of equal noise variances that underlies the NOTEARS methodology. Thus, according to Ng et al. [2024], NOTEARS should not have been expected to perform well on normalized LANM data in the first place as one of its fundamental assumptions is not satisfied.\nRather than weighing in on this discussion, in this workshop contribution we explore how much var- and R2-sortability (another sortability criterion introduced in [Reisach et al., 2023]) can be seen in data commonly used in method validation of time series causal discovery methods. We evaluate the degree of var-/R2-sortability in different datasets as well as the performance of different algorithms for time series causal discovery in the presence/absence of var-/R2-sortability and before/after normalisation. One of the main questions underlying the discussions on sortability is this: how much var-/R2-sortability do we expect to see in real-world data? The answer to this question is likely highly context-dependent, and the question can be notoriously difficult to settle even for a single dataset, since one needs to know the causal ground truth to compute sortability. We investigate sortability scores in two of the rare cases where a causal ground-truth is available, the river flow dataset used in Tran et al. [2024] and the Causal Chamber datasets recently published by Gamella et al. [2024]. In both cases, we find sortability values quite far from 0.5. In the river stream datasets, marginal variances tend to decrease along the causal order (varsortability close to zero) while in the Causal Chamber case, marginal variances increase along the causal order (varsortability close to one). For the R\u00b2-score we find the exact opposite: values close to one for some rivers in the river dataset, and values close to zero for some of the causal chamber datasets.\nThese observations illustrate that discarding scales in causal discovery as arbitrary may be premature, as scales may encode significant causal information. We hypothesise plausible physical explanations for the observed varsortability in the investigated datasets which also call into question the validity of an equal noise variance assumption in these cases.\nIn more detail, our main contributions are:\n1. We extend var- and R2-sortability and the simple benchmarking algorithm of Reisach et al. [2021, 2023] to the time series setting.\n2. We show empirically that simulated data typically used to evaluate causal discovery algorithms for time series data is varsortable, meaning the amplitude of the marginal variance increases the lower the variable is in the causal ordering of the ground truth summary graph. We also show that, in this case, varsortability is largely driven by contemporanous dependencies.\n3. We demonstrate that there is a positive correlation between the performance of score-based causal discovery algorithms for time series and the varsortability of data generated by structural autoregressive processes.\n4. We investigate sortability of the data used in the 2019 causal discovery challenge\u00b9 by Runge et al. [2019].\n5. We calculate var- and R\u00b2-sortability of the river flow dataset used in Tran et al. [2024], and of the recently published datasets generated by the Causal Chamber[Gamella et al., 2024]. We find that in these real-live datasets, the scale plays an important role for potential causal discovery algorithms."}, {"title": "PRELIMINARIES", "content": "2.1 CAUSAL DISCOVERY FOR TIME SERIES\nA stationary time series graph (ts-graph) is a directed graph G = (V \u00d7 Z,D), V = {1....,d}, whose edges ((i, t - k), (j, t)) are assumed invariant under translation of the time component. In addition, it is typically required that there is a finite maximal lag $T_{max} = maxi,j\u2208v{k|((i, t - k), (j,t)) \u2208 D} < \u221e$ and that the contemporaneous component of G is acyclic. Any stationary ts-graph induces a directed, potentially cyclic summary graph Gsum over V that contains a directed edge (i, j) if (i, t \u2212 k) \u2192 (j, t) \u2208 D for some k. The adjacency matrices of G, Gsum will be denoted by W, Wsum respectively.\nThe aim of most causal discovery methods for time series is to recover either G or Gsum from observational or interventional data. Often the data is assumed to be generated by a discrete multivariate process $(X_t)_{t\u2208Z}, X_t = (X_1,..., X_d)$ compatible with G. The process $(X_t)_{t\u2208Z}$ is typically modelled as a structural vector autoregressive (SVAR) process, in which case, compatibility with G means that $(X_t)_{t\u2208Z}$ follows the evolution rule\n$\nX_{j}^{t} = \\sum_{i \u2208 pag^{sum(j)}} \\sum_{k=0}^{T_{max}} a_{i}^{t-k}X_{i,t-k} + \\eta_{j}^{t}\n$(1)\nfor $j \u2208 {1, ..., d}$, where $\\eta_{j}^{t}$ are Gaussian white noise processes, pagsum (j) denotes the parents of node j in the summary graph, and $a_{i}^{t-k}$ is only allowed to be non-zero if (i,t-k) \u2192 (j,t) is an edge in G. SVAR-processes can be considered the time series analogue of additive linear noise models for which sortability was discussed in [Reisach et al., 2021, 2023]. When generating data from such a model, coefficients are typically randomly drawn, sometimes with a pre-specified proportion of contemporaneous links, and the process is being run until it has converged to a stationary distribution (or is discarded if the distribution is non-stationary).\n2.2 NOTEARS AND DERIVATIVES\nZheng et al. [2018] propose the score-based causal discovery method NOTEARS, which embeds the discrete search space of DAGs into a continuous one by using the differentiable function h(W) = tr eW\u00b0W - d. This function is 0 if and only if W is the adjacency matrix of an acyclic graph and hence measures the DAGness of W [Zheng et al., 2018]. By combining this function with a score evaluating how well the estimated weight matrix W fits the data, Zheng et al. [2018] formulate the constrained optimisation problem to find\n$\nmin_W \\frac{1}{2n} ||X - XW||^2_F\n$(2)\ns.t. W is acyclic, which is modelled by h. $|| ||$ is the Frobenius norm.\nThe DYNOTEARS algorithm Pamfil et al. [2020] modifies the NOTEARS algorithm to work with time-lagged and auto-correlated dependencies by redefining the optimisation problem to minw 1l(Wc, W1) s.t. We is acyclic, where $W_1 \u2208 R^{d\u00d7d\u00d7T_{max}}$ is the lagged adjacency matrix and We is the contemporaneous adjacency of the underlying time series process. As the underlying time series graph to estimate is acyclic if and only if We is acyclic, it suffices to enforce the acyclicity constraint only on We [Pamfil et al., 2020]. To ensure sparsity of W, Pamfil et al. [2020] also add an l\u2081 penalty term, leading to the constraint optimisation problem\n$\nmin_W \\frac{1}{2n} ||X - XW_c - X_1W_1||^2_F + \u03bb_1||W_c||_1 + \u03bb_2||W_1||_1\n$(3)\ns.t. We is acyclic. Here \u5165\u2081 and 2 are two regularisation parameters and W\u2081 is the lagged adjacency matrix.\nThe continuous optimisation problems of NOTEARS and DYNOTEARS can be solved efficiently by rewriting the problem using the augmented Lagrangian method and using a numerical solver such as L-BFGS [Zheng et al., 2018, Pamfil et al., 2020]. After applying the numerical optimisation algorithm in both of the algorithms, a threshold t is applied to remove weights close to zero [Zheng et al., 2018, Pamfil et al., 2020].\n2.3 SORTABILITY CRITERIA\nIn the following, we briefly reiterate the two sorting criteria varsortability and R2-sortability introduced by Reisach et al. [2021] and Reisach et al. [2023]. In essence, both of these approaches calculate a score s of sortability in a comparable manner; s is the measurement of the degree of agreement between the true causal order and the increasing order of a sortability criterion cri.\nFor any causal model containing the variables {X(1),...,x(d)} with a (non-degenerate) adjacency matrix W, the sortability score is the fraction of directed paths that start from a node with a strictly lower sortability criterion than the node they end in. Thus the sortability for one selected criterion cri can be calculated as\n$\ns := \\frac{\\sum_{i=1}^{d-1} \\sum_{j \\epsilon w_k} increasing(cri(X^i), cri(X^j))}{\\sum_{i=1}^{d-1} \\sum_{j \\epsilon w_k} 1} \u2208 [0, 1],\n$(4)\n$\nincreasing(a, b) = \\begin{cases}\n1 & a < b \\\\\n1/2 & a = b \\\\\n0 & a > b.\n\\end{cases}\n$(5)\nIn the case of varsortability Reisach et al. [2021], the criterion cri(X\u00b2) = Var(X\u00b2) is the marginal variance, whereas in the case of R\u00b2-sortability, it is the coefficient of determination cri(X\u00b2) = R2(X\u00b2) which acts as a proxy for the fraction of the variance of X that is explained by its causal parents, see [Reisach et al., 2023] for details. Varsortalility v is defined by using the marginal variance as cri. R\u00b2-sortability r is defined by using the obtained R2-coefficients as the sorting criterion cri [Reisach et al., 2021, 2023].\nReisach et al. [2021] showed that varsortability is usually high in data generated by LANMs (see the analytical and empirical proof in their paper). Based on their sortability criteria, Reisach et al. [2021] and Reisach et al. [2023] introduced the baseline methods varsortnregress and R\u00b2-sortnregress respectively. These algorithms sort the system variables based on their variances or R2-scores which are estimated by fitting a regression model; these simple benchmark methods are then shown to have similar performance to some state of the art causal discovery algorithms [Reisach et al., 2021, 2023] on LANM data."}, {"title": "MODIFIED SORTABILITY CRITERIA FOR SUMMARY GRAPHS", "content": "In a time series causal discovery setting, the summary graph Gsum describing the relationship between the different time-evolving processes may contain cycles. Since the marginal variances Var(X) do not depend on the time index t due to the assumed stationarity of the processes, to compute varsortability in this situation, in principle, one could still use Equation 4 as is.\nHowever, any pair of processes X\u00b2 and X that belong to the same strongly connected component of the summary graph (i.e. that are connected by a cycle) would always contribute a 1 to the denominator and a 2 to the numerator of Equation 4.\nTherefore the presence of cycles would dilute the sortability signal and would naturally push it closer to 1/2. In other words, R2-, and varsorting of cyclicly connected processes is meaningless, and we are only interested in whether nodes that are not cyclicly connected can be sorted. Our sortability criterion thus becomes\n$\ns:= \\frac{\\sum_{(i,j)\u2208AP(G^{sum})} increasing(cri(X_i), cri(X_j))}{\\sum_{k=1}^{(d-1)} \\sum_{(i,j)\u2208AP(G^{sum})} 1} \u2208 [0, 1],\n$(6)\nwhere AP(G') = {(i, j) \u2208 V' \u00d7 V' | i \u21d2 j, j \u21d2 i} is the set of admissible node pairs (the long double arrow indicates the existence of a directed path).\nFor the example in Figure 1, the sortability score s is $\\frac{1+1+1+0}{1+1+1+1}$ = 0.75. If we would calculate it including cyclicly connected pairs, it would result in\u2248 0.67.\nHigh varsortability and the performance of DYNOTEARS Based on heuristics and empirical findings, Reisach et al. [2021] argue that high variability causes gradient-based optimisation algorithms such as NOTEARS[Zheng et al., 2018] to favor graphs whose edges point in the direction of increasing marginal variances during their first optimisation step. Since DYNOTEARS is a time-series adaptation of NOTEARS, that essentially coincides with NOTEARS on the contemporaneous components of a ts-graph, [Reisach et al., 2021]'s arguments might be applicable to (the contemporaneous part of) DYNOTEARS as well.\nHowever, in the recent work Ng et al. [2024] provide examples in which continuous optimization-based methods can not perform well in the presence of high varsortability. Ng et al. [2024] also give an alternative explanation for why continuous structure learning performs significantly worse after standardisation of LANM data. Continuous structure learning assumes equal noise variances for all variables in the system, which is violated in the standardised model leading to poor performance."}, {"title": "SORTNREGRESS FOR TIME SERIES GRAPHS", "content": "In order to have simple algorithms that exploit high R\u00b2- and varsortability, we present our time series adapted sortnregress algorithm based on sortnregress from Reisach et al. [2021, 2023]. To estimate contemporaneous dependencies, we use the standard sortnregress algorithm, which consists of two steps:\n1. Sort nodes by increasing marginal variance or R2-score.\n2. Each node is regressed on its predecessor, determined by order, using a penalised regression technique. As described in Reisach et al. [2021], LASSO regression is used, using the Bayesian Information Criterion (BIC) for hyperparameter selection.\nThis gives us an estimated contemporaneous adjacency matrix Wc. A random sortnregress algorithm is also used, where we determine the order of the variables randomly using i.i.d. Bernoulli trials. To estimate lagged dependencies between variables, we use the same first step and change the second step: We now regress each node on each of its predecessors pi,t, where t \u2208 [1, Tmax] indicates the time lag. After this step we have an estimated lagged adjacency matrix W1."}, {"title": "NUMERICAL EXPERIMENTS & RESULTS", "content": "In the following section, we first give an overview of the evaluation metrics which are used for all the considered datasets. After that we outline the setup and the results for each dataset. We conduct our experiments in Python, using the TIGRAMITE library\u00b2 for simulating data with SVAR models. When assessing the performance of different algorithms across a range of sortability values, the hyperparameters of the DYNOTEARS algorithm are set to x1 = 2 = 0.05. The weight threshold is set to 0.1. As a constraint-based comparison algorithm, PCMCI+[Runge, 2020] is run with a = 0.01 and the ParCorr conditional independence test. We further use the varsortnregress, R\u00b2-sortnregress and random regress algorithms as described in Section 4.\nWe assess the F1-score using the formula:\n$\nF1 = \\frac{TP}{TP + 0.5(FP + FN)}\n$\nto gauge the performance of the selected algorithms concerning the comparison between the estimated binary time series adjacency matrices W and the ground truth W.\nHere, TP represents the number of true positives, FP represents the number of false positives, and FN represents the number of false negatives for edge detection.\nWe refer to this metric as the overall F1-score. Additionally, we calculate the F1-scores comparing the estimated contemporaneous adjacency matrix We with the true contemporaneous adjacency matrix Wc, and the F1-scores comparing the estimated lagged adjacency matrix Wt with W\u2081. These metrics are denoted as F1-contemp and F1-lagged, respectively. In cases where only information about the summary graph is available, we calculate the F1-score between Wsum and the estimated summary adjacency matrix Wsum.\n5.1 NEURIPS COMPETITION DATA\nSetup We also assess var- and R2-sortability on the 2019 Causality-for-Climate-competition[Runge et al., 2019] data. This dataset is relevant not only because it was used in the competition, but also because it follows the same structure as the CauseMe platform [Munoz-Mar\u00ed et al., 2020], which is widely used to evaluate causal discovery algorithms [Bussmann et al., 2021, Langbridge et al., 2023, Runge, 2020]. The dataset includes simulated and partially simulated datasets of varying complexity, including high-dimensional datasets and non-linear dependencies, with 100, 150, 600 or 1000 realisations for each dataset specification. We excluded the datasets with missing values.\nWe set the maximal time-lag in our methods Tmax following the description of the respective dataset (ranging from 3 to 5).\n5.2 DATA GENERATION WITH ERD\u0150S-R\u00c9NYI GRAPHS AND SVAR MODELS\nSetup In order to investigate var- and R2-sortability for datasets used to evaluate score-based causal discovery methods, we replicate one of the two data generation methods used by Pamfil et al. [2020].\nFollowing Pamfil et al. [2020], Zheng et al. [2018], when generating random time series graphs, we use Erd\u0151s-R\u00e9nyi Graphs (ER Graphs) [Newman, 2018] to draw the contemporaneous edges with i.i.d Bernoulli trials. Sampling only lower triangle entries of the contemporaneous adjacency matrix and then permuting the node order ensures that the contemporaneous adjacency matrix We is acyclic. In order to ensure that the expected mean degree is de the probability of each Bernoulli trial is set to dc/(d - 1), where d is the total number of variables. The edge coefficients are sampled uniformly at random from [-2, -0.5] U [0.5, 2.0] Pamfil et al. [2020]. Again following [Pamfil et al., 2020], the edge weights for lagged variables are sampled depending on t from [\u22120.50, -0.30] U [0.3\u03b1, 0.5\u03b1], where a = 1/8t\u22121. The weight decay 8 > 1 reduces the influence (weights) of variables further back in time. We randomly sample Erd\u0151s-R\u00e9nyi graphs with degree de = 4 for the contemporaneous dimension, and for each lag we set d\u2081 = 1; 8 is set to 1.1.\nWe examine sortability values for different numbers of nodes d\u2208 {10, 20, 50, 100}. For each number of nodes we randomly generate 500 different graphs and generate n = 500 samples per graph. We the calculate the overall varsortability, the varsortability of only the contemporanous dependencies and the varsortability of all all the lagged dependencies.\nWe also want to investigate whether varsortability is driven by contemporaneous or lagged dependencies. This is why we compute var- and R2-sortability over a grid of dc,d\u0131 \u2208 [0, 0.5, 1, 2, 3, 4, 6, 8]. We do this for d = 10 and d = 20 nodes.\nWe further investigate the influence of the two sortability criteria on the performance of score and constraint-based algorithms. In order to do so, we generate data for d = 10 variables and then randomly draw m = 30 samples per sortability interval, which we set to [0, 0.2], [0.2, 0.4], [0.4, 0.6], [0.6, 0.8], [0.8, 1]. We report the performance of the following algorithms for varying var- and R2-sortability: DYNOTEARS, DYNOTEARS standardised (run after standardising the data), PCMCI+, varsortnregress/R\u00b2-sortnregress and randomregress. This means that for DYNOTEARS, the data has a varsortability as defined by the respective bin before standardising (after standardisation the varsortability is always 0.5).\nResults We observe that the overall mean varsortability for the ER-SVAR data used by [Pamfil et al., 2020] ranges from 0.58 for ten nodes to 0.54 for 100 nodes. We do not see a trend in varsortability for different numbers of nodes. The varsortability of the contemporaneous dimension is around 0.7 for all numbers of nodes. The lagged varsortability is around 0.54 for all numbers of nodes except 10 nodes where it is 0.56. The R\u00b2 sortability is around 0.5 for all numbers of nodes. Varsortability of the contemporaneous component of G is always above 0.7 and higher than the overall varsortability. Consequently, lagged varsortability is always lower than contemporanous and overall varsortability.\nFigure 2 shows that the higher the varsortability, the higher the F1-score of DYNOTEARS. The varsortnregress benchmark model also improves with higher varsortability and seems to outperform the DYNOTEARS algorithm for varsortability values higher than 0.6. The constraint-based PCMI+ algorithm does not seem to be as affected by varsortability as the randomregress algorithm. The DYNOTEARS algorithms perform better on standardised data for low varsortability values before standardisation.\nFor high R2-sortability values, we observe a different behaviour: Both DYNOTEARS and PCMCI+ seem unaffected by varying values. Again the R2-sortnregress algorithm seems to outperform DYNOTEARS for R\u00b2-sortability values of 0.6 or higher (see Figure 3).\nWe also investigate whether high varsortability leads to a higher F1-score of DYNOTEARS due to better identification of contemporaneous edges or lagged dependencies. As we can see in Figure 4, the effect of increasing F1-scores with increasing varsortability is even higher for contemporaneous dependencies. The F1-score for lagged dependencies seems to be unaffected. Furthermore, we observed in our experiments that the contemporaneous F1-score has a Pearson correlation of more than 0.6 with the varsortability score. Furthermore, higher weighting thresholds of the DYNOTEARS algorithm seem to increase the influence of varsortability on the performance of DYNOTEARS as measured by the F1-score.\n5.3 EXTREMAL RIVER FLOW PROBLEM\nSetup Next, we investigate the two sortability criteria on four real-world datasets modelling extremal river flow previously used in [Tran et al., 2024]. The aim of the problem is to recover the direction and connections of a river network with only extreme flow measurements at certain stations, without knowing the location of these stations. This is a time-dependent process, as an extreme measurement recorded at one station at time t 1 may lead to an extreme measurement at another station at time t[Tran et al., 2024]. Having the ground truth river network allows us to report R\u00b2 and varsortability on this real-world dataset and investigate whether high values can also occur on real time series data. We also include the F1-scores of selected benchmark algorithms for a Tmax value of 3 as this gave the best results. Following [Tran et al., 2024], we treat the downstream river direction as the causal ground truth Gsum, which perhaps may be controversial due to the fact that downstream extreme events might have damming effects further upstream. Nevertheless, even if one is not comfortable calling the flow direction causal, the results below show that varsorting is able to cover the 'flow graph'.\nAs we only know the ground truth/flow graph on the summary level, i.e. we know Gsum, we calculate the F1-score between the ground truth for Wsum and the estimated adjacency matrix W sum for the following algorithms: varsortnregress, R\u00b2-sortnregress, randomregress, PCMCI+ and reversed varsortnregress. We use reversed varsortnregress as we observed very low values for the varsortability and wanted to investigate if an algorithm that order by decreasing variance could exploit this fact.\nResults As illustrated in Table 2, the varsortability for the entire river network is below 0.5, with the varsortability value for the Danube being close to 0. This indicates that as the river network follows a tree structure, the variance of the extremal river velocities for nodes decreases with increasing distance from the river source. In other words, the real-life dynamics of the river flow systems entail sortable marginal variances. It can be observed that for high R2-sortability, the R\u00b2-sortnregress algorithm performs as well as PCMCI+. Conversely, for low varsortability values, the reversed varsortability algorithm, while being superior to random, is unable to match the performance of PCMCI+. Notably, the varsortnregress algorithm outperforms the reverse varsortnregress algorithm on the upper Colorado dataset even though the varsortability is below 0.5. We also tried to run DYNOTEARS on this dataset. However, the algorithm did not converge after a couple of hours run time as it did not manage to satisfy the acyclity constraint, at least for our choices of hyperparameters.\n5.4 CAUSAL CHAMBER DATA\nSetup We now investigate the values of the two sortability criteria for data generated by the recently introduced Causal Chamber Gamella et al. [2024], which provides a toolbox consisting of real physical systems that can be used to evaluate causal discovery or other AI algorithms on real data."}, {"title": "DISCUSSION & CONCLUSION", "content": "In general, both varsortability and R2-sortability are present in both simulated and real datasets for benchmarking causal discovery algorithms. In line with Reisach et al. [2021], we observe that DYNOTEARS, which is a NOTEARS based algorithm, seems to perform better when varsortability is higher.\nFurthermore, we observe that the data used in the NEURIPS competitionRunge et al. [2019] are highly varsortable, especially the realistic datasets, by our defined time series varsortability metric. The low varsortability in the simulated var models could potentially be due to the fact that these datasets do not have contemporaneous dependencies, and as we showed earlier, contemporaneous dependencies seem to be the driver for high varsortability in simulated time series data. This is hardly surprising given that a team exploiting this effect won the competition[Weichwald et al., 2020]. As for R2-sortability, we see that high values can lead to better performance of simple benchmark algorithms. However, R\u00b2-sortability values observed on the NEURIPS and ER Datasets seem to be too low to be exploited by the sorting algorithm.\nThe low varsortability value observed for the river data, particularly for the Danube, may be attributed to the fact that the width and catchment area of a river increase from the source to the mouth, resulting in a reduction in the impact of extreme flows on river velocity closer to the river's mouth. Consequently, the marginal variance decreases. This serves to illustrate the importance of scales in real dataset. The data for the other rivers only covers parts of the river, which probably results in a less intense effect and higher values for varsortability.\nIn addition, we can see that the Causal Chamber data is highly varsortable overall while having low R\u00b2-values. This could be due to the fact that the variables controlled by the user are high in the causal order. We conjecture that deeper in the system and further from the user-controlled variables, dynamic turbulence and other sources of noise start having a larger and larger influence. Thus, unexplained noise is higher the lower we are in the causal order. This is in line with the low R\u00b2-sortability values as the causal parents explain less and less down the causal order. The increase in noise variance also drives the increase in total marginal variance and affects varsortability in this way.\nLimitations This paper is an empirical study and we do not determine analytically why high varsortability leads to better performance of score based causal discovery algorithms for time series data. While we believe that our explanations for varsortability in the considered real-world datasets are plausible, they should be treated cautiously as hypotheses only. The only thing that we can say with certainty that sortability is highly context-dependent and therefore discarding scales as arbitrary for causal discovery seems premature.\nConclusion In conclusion, our paper represents an empirical extension of the work of [Reisach et al., 2021, 2023]. We demonstrate that high var-sortability occurs in SVAR-simulated time series data, resulting in enhanced performance of score-based causal discovery algorithms assuming equal noise variance. Consequently, we advise caution when assuming equal noise variance for time series causal discovery algorithms. Furthermore, it may be advisable to examine simulated data for high varsortability before using them as benchmark data, as was done in the 2019 NEURIPS competition[Runge et al., 2019]. Finally, we observe high and low varsortability, as well as R2-sortability, in two different types of real-life datasets: the Causal Chamber data is generated in a controlled environment while the river flow dataset is measured in an uncontrolled environment. This indicates that var- and R2-sortability in auto-correlated time series data is not solely a phenomenon observed in simulated data. For this reason, we believe that marginal variances may contain relevant causal information and exploiting variance or inverse variance sorting may be justified in some situations, if one can combine it with physical reasons for one or the other (even though these physical reasons might already eliminate the need for causal discovery in the first place). Furthermore, the observed R2-sortability scores indicate that the assumption of equal noise variance is a tricky one as well. At the very least the relative fraction of unexplained variance may change throughout the graph and unequal noise variances are one possible reason for this."}]}