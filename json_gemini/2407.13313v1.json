{"title": "Sortability of Time Series Data", "authors": ["Christopher Lohse", "Jonas Wahl"], "abstract": "Evaluating the performance of causal discovery algorithms that aim to find causal relationships between time-dependent processes remains a challenging topic. In this paper, we show that certain characteristics of datasets, such as varsortability Reisach et al. [2021] and $R^{2}$-sortability Reisach et al. [2023], also occur in datasets for autocorrelated stationary time series. We illustrate this empirically using four types of data: simulated data based on SVAR models and Erd\u0151s-R\u00e9nyi graphs, the data used in the 2019 causality-for-climate challenge [Runge et al., 2019], real-world river stream datasets, and real-world data generated by the Causal Chamber of [Gamella et al., 2024]. To do this, we adapt var- and $R^{2}$-sortability to time series data. We also investigate the extent to which the performance of score-based causal discovery methods goes hand in hand with high sortability. Arguably, our most surprising finding is that the investigated real-world datasets exhibit high varsortability and low $R^{2}$-sortability indicating that scales may carry a significant amount of causal information.", "sections": [{"title": "1 INTRODUCTION", "content": "Inferring causal relationships between variables in a multivariate time series setting is an ongoing topic of research in many fields, including economics/econometrics [Varian, 2016], climate science [Runge et al., 2019, Runge, 2020], medicine [Yazdani and Boerwinkle, 2015] and neuroscience [Bergmann and Hartwigsen, 2021]. The process of inferring causal structure from data is often referred to as causal discovery or causal structure learning. Most of the literature on causal discovery is devoted to the following two types of methods: Constraint-based causal structure learning al-gorithms, such as the PC-algorithm [Spirtes and Glymour, 1991] and PCMCI [Runge, 2020] for time series data, use tests of conditional independence to iteratively learn a causal graph. Score-based method such as GES [Chickering, 2002] fit a directed acyclic graph (DAG) to the data by optimising a score function, but need to search a large discrete space of DAGs which is an NP-hard problem. More recently, [Zheng et al., 2018] have proposed to embed the discrete DAG-search space into a continuous one through a differentiable acyclicity constraint to make the problem amenable to continuous (gradient based) optimisation. The method introduced in [Zheng et al., 2018], NOTEARS, showed impressive performance on data simulated with linear additive noise models (LANMs). However, as shown by Reisach et al. [2021] the strong performance of NOTEARS and similar methods on LANM data vanished after the data was normalized. Reisach et al. [2021] noticed that, before normalization, additive noise model data is highly varsortable, meaning that, on average, the causal order of the system can be recovered well by sorting the variables by the amplitude of their estimated variances. Since high varsortability and good NOTEARS performance were highly correlated, they therefore conjectured that NOTEARS implicitly made use of variance sorting in its optimisation. The issue has recently been revisited by Ng et al. [2024], who pointed out that (a) NOTEARS does not necessarily perform well in the presence of high varsortability and that (b) normalisation of data generated by LANMs moves the data far away from the assumption of equal noise variances that underlies the NOTEARS methodology. Thus, according to Ng et al. [2024], NOTEARS should not have been expected to perform well on normalized LANM data in the first place as one of its fundamental assumptions is not satisfied."}, {"title": "2 PRELIMINARIES", "content": "### 2.1 CAUSAL DISCOVERY FOR TIME SERIES\nA stationary time series graph (ts-graph) is a directed graph $\\mathcal{G}=(V \times \\mathbb{Z}, \\mathcal{D}), V=\\{1, \\ldots, d\\}$, whose edges $((i, t-k),(j, t))$ are assumed invariant under translation of the time component. In addition, it is typically required that there is a finite maximal lag $\tau_{\text {max }}=\\max _{i, j \\in V}\\{k \\mid((i, t-$ $k),(j, t)) \\in \\mathcal{D}\\}<\\infty$ and that the contemporaneous component of $\\mathcal{G}$ is acyclic. Any stationary ts-graph induces a directed, potentially cyclic summary graph $\\mathcal{G}_{\text {sum }}$ over $V$ that contains a directed edge $(i, j)$ if $(i, t-k) \rightarrow(j, t) \\in \\mathcal{D}$ for some $k$. The adjacency matrices of $\\mathcal{G}, \\mathcal{G}_{\text {sum }}$ will be denoted by $\\mathbf{W}, \\mathbf{W}_{\text {sum }}$ respectively.\n\nThe aim of most causal discovery methods for time series is to recover either $\\mathcal{G}$ or $\\mathcal{G}_{\text {sum }}$ from observational or interventional data. Often the data is assumed to be generated by a discrete multivariate process $\\left(\\mathbf{X}_{t}\right)_{t \\in \\mathbb{Z}}, \\mathbf{X}_{t}=$ $\\left(X_{t}^{1}, \\ldots, X_{t}^{(d)}\right)$ compatible with $\\mathcal{G}$. The process $\\left(\\mathbf{X}_{t}\right)_{t \\in \\mathbb{Z}}$ is typically modelled as a structural vector autoregressive (SVAR) process, in which case, compatibility with $\\mathcal{G}$ means that $\\left(\\mathbf{X}_{t}\right)_{t \\in \\mathbb{Z}}$ follows the evolution rule\n\n$$X_{t}^{j}=\\sum_{i \\in \\mathrm{pa}_{\\mathcal{G}_{\text {sum }}}(j)} \\sum_{k=0}^{\tau_{\\max }} a_{i, t-k}^{j} X_{t-k}^{i}+\\eta_{t}^{j}$$\nfor $j \\in\\{1, \\ldots, d\\}$, where $\\eta^{j}$ are Gaussian white noise processes, $\\mathrm{pa}_{\\mathcal{G}_{\text {sum }}}(j)$ denotes the parents of node $j$ in the summary graph, and $a_{i, t-k}^{j}$ is only allowed to be non-zero if $(i, t-k) \rightarrow(j, t)$ is an edge in $\\mathcal{G}$. SVAR-processes can be considered the time series analogue of additive linear noise models for which sortability was discussed in [Reisach et al., 2021, 2023]. When generating data from such a model, coefficients are typically randomly drawn, sometimes with a pre-specified proportion of contemporaneous links, and the process is being run until it has converged to a stationary distribution (or is discarded if the distribution is non-stationary)."}, {"title": "2.2 NOTEARS AND DERIVATIVES", "content": "Zheng et al. [2018] propose the score-based causal discovery method NOTEARS, which embeds the discrete search space of DAGs into a continuous one by using the differentiable function $h(\\mathbf{W})=\\operatorname{tr} e^{\\mathbf{W} \times \\mathbf{W}}-d$. This function is 0 if and only if $\\mathbf{W}$ is the adjacency matrix of an acyclic graph and hence measures the DAGness of W [Zheng et al., 2018]. By combining this function with a score evaluating how well the estimated weight matrix $\\mathbf{W}$ fits the data, Zheng et al.\n[2018] formulate the constrained optimisation problem to find\n\n$$\\min _{\\mathbf{W}} \\frac{1}{n}\\|\\mathbf{X}-\\mathbf{X W}\\|_{2}^{2}$$\ns.t. $\\mathbf{W}$ is acyclic, which is modelled by $h .\\|\\cdot\\|_{2}^{2}$ is the Frobenius norm.\n\nThe DYNOTEARS algorithm Pamfil et al. [2020] modifies the NOTEARS algorithm to work with time-lagged and auto-correlated dependencies by redefining the optimisation problem to $\\min _{\\mathbf{W} 1} \\ell\\left(\\mathbf{W}_{c}, \\mathbf{W}_{\\mathbf{1}}\right)$ s.t. $\\mathbf{W}_{c}$ is acyclic, where $\\mathbf{W}_{1} \\in \\mathbb{R}^{d \times d \times \tau_{\\max }}$ is the lagged adjacency matrix and $\\mathbf{W}_{c}$ is the contemporaneous adjacency of the underlying time series process. As the underlying time series graph to estimate is acyclic if and only if $\\mathbf{W}_{c}$ is acyclic, it suffices to enforce the acyclicity constraint only on $\\mathbf{W}_{c}$ [Pamfil et al., 2020]. To ensure sparsity of $\\mathbf{W}$, Pamfil et al. [2020] also add an $\\ell_{1}$ penalty term, leading to the constraint optimisation problem\n\n$$\\min _{\\mathbf{W}} \\frac{1}{2 n}\\left\\|\\mathbf{X}-\\mathbf{X} \\mathbf{W}_{c}-\\mathbf{X}_{\\mathbf{1}} \\mathbf{W}_{l}\right\\|_{2}^{2}+\\lambda_{1}\\left\\|\\mathbf{W}_{c}\right\\|_{1}+\\lambda_{2}\\left\\|\\mathbf{W}_{l}\right\\|_{1}$$\ns.t. $\\mathbf{W}_{c}$ is acyclic. Here $\\lambda_{1}$ and $\\lambda_{2}$ are two regularisation parameters and $\\mathbf{W}_{l}$ is the lagged adjacency matrix.\n\nThe continuous optimisation problems of NOTEARS and DYNOTEARS can be solved efficiently by rewriting the problem using the augmented Lagrangian method and using a numerical solver such as L-BFGS [Zheng et al., 2018, Pamfil et al., 2020]. After applying the numerical optimisation algorithm in both of the algorithms, a threshold $t$ is applied to remove weights close to zero [Zheng et al., 2018, Pamfil et al., 2020]."}, {"title": "2.3 SORTABILITY CRITERIA", "content": "In the following, we briefly reiterate the two sorting criteria varsortability and $R^{2}$-sortability introduced by Reisach et al. [2021] and Reisach et al. [2023]. In essence, both of these approaches calculate a score $s$ of sortability in a comparable manner; $s$ is the measurement of the degree of agreement between the true causal order and the increasing order of a sortability criterion $c r i$.\n\nFor any causal model containing the variables $\\left\\{X^{(1)}, \\ldots, X^{(d)}\right\\}$ with a (non-degenerate) adjacency matrix $\\mathbf{W}$, the sortability score is the fraction of directed paths that start from a node with a strictly lower sortability criterion than the node they end in. Thus the sortability for one selected criterion $c r i$ can be calculated as\n\n$$s:=\\frac{\\sum_{k=1}^{d-1} \\sum_{i \rightarrow j \\in \\mathbf{W}^{k}} \text { increasing }\\left(c r i\\left(X^{i}\right), \\operatorname{cri}\\left(X^{j}\right)\right)}{\\sum_{k=1}^{d-1} \\sum_{i \rightarrow j \\in \\mathbf{W}^{k}} 1} \\in[0,1]$$, where\n\n$$\text { increasing }(a, b)= \begin{cases}1 & a<b \\ 1 / 2 & a=b \\ 0 & a>b\\end{cases}$$\n\nIn the case of varsortability Reisach et al. [2021], the criterion $\\operatorname{cri}\\left(X^{i}\right)=\\operatorname{Var}\\left(X^{i}\right)$ is the marginal variance, whereas in the case of $R^{2}$-sortability, it is the coefficient of determination $\\operatorname{cri}\\left(X^{i}\right)=$ $R^{2}\\left(X^{i}\right)$ which acts as a proxy for the fraction of the variance of $X_{i}$ that is explained by its causal parents, see [Reisach et al., 2023] for details. Varsortability $v$ is defined by using the marginal variance as $c r i . R^{2}$-sortability $r$ is defined by using the obtained $R^{2}$-coefficients as the sorting criterion cri [Reisach et al., 2021, 2023].\n\nReisach et al. [2021] showed that varsortability is usually high in data generated by LANMs (see the analytical and empirical proof in their paper). Based on their sortability criteria, Reisach et al. [2021] and Reisach et al. [2023] introduced the baseline methods varsortnregress and $R^{2}$-sortnregress respectively. These algorithms sort the system variables based on their variances or $R^{2}$-scores which are estimated by fitting a regression model; these simple benchmark methods are then shown to have similar performance to some state of the art causal discovery algorithms [Reisach et al., 2021, 2023] on LANM data."}, {"title": "3 MODIFIED SORTABILITY CRITERIA FOR SUMMARY GRAPHS", "content": "In a time series causal discovery setting, the summary graph $\\mathcal{G}_{\text {sum }}$ describing the relationship between the different time-evolving processes may contain cycles. Since the marginal variances $\\operatorname{Var}\\left(X_{t}^{i}\right)$ do not depend on the time index $t$ due to the assumed stationarity of the processes, to compute varsortability in this situation, in principle, one could still use Equation 4 as is.\n\nHowever, any pair of processes $X^{i}$ and $X^{j}$ that belong to the same strongly connected component of the summary graph (i.e. that are connected by a cycle) would always contribute a 1 to the denominator and a 2 to the numerator of Equation 4.\n\nTherefore the presence of cycles would dilute the sortability signal and would naturally push it closer to $1 / 2$. In other words, $R^{2}$-, and varsorting of cyclicly connected processes is meaningless, and we are only interested in whether nodes that are not cyclicly connected can be sorted. Our sortability criterion thus becomes\n\n$$s:=\\frac{\\sum_{(i, j) \\in \\mathcal{A P}\\left(\\mathcal{G}_{\text {sum }}\right)} \text { increasing }\\left(\\operatorname{cri}\\left(X_{i}\right), \\operatorname{cri}\\left(X_{j}\right)\right)}{\\sum_{k=1}^{d-1} \\sum_{(i, j) \\in \\mathcal{A P}\\left(\\mathcal{G}_{\text {sum }}\right)} 1} \\in[0,1]$$, where $\\mathcal{A P}\\left(\\mathcal{G}^{\\prime}\right)=\\left\\{(i, j) \\in V^{\\prime} \times V^{\\prime} \\mid i \\Longrightarrow j, j \\Rightarrow k i\right\\}$ is the set of admissible node pairs (the long double arrow indicates the existence of a directed path).\n\nFor the example in Figure 1, the sortability score $s$ is $\\frac{|+|+|+|}{1 \\cdot+1 \\cdot+1}=$ 0.75 . If we would calculate it including cyclicly connected pairs, it would result in $\\frac{4}{6} \\approx 0.67$.\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Example of the calculation of $s$ with cycles. The contribution of the pairs $(B, C)$ and $(C, B)$ is ignored since the two nodes belong to the black cycle. Cycle-free directed paths that connect admissible node pairs are depicted in colour."}, {"title": "4 SORTNREGRESS FOR TIME SERIES GRAPHS", "content": "In order to have simple algorithms that exploit high $R^{2}$ - and varsortability, we present our time series adapted sortnregress algorithm based on sortnregress from Reisach et al. [2021, 2023]. To estimate contemporaneous dependencies, we use the standard sortnregress algorithm, which consists of two steps:\n\n1. Sort nodes by increasing marginal variance or $R^{2}$-score.\n2. Each node is regressed on its predecessor, determined by order, using a penalised regression technique. As described in Reisach et al. [2021], LASSO regression is used, using the Bayesian Information Criterion (BIC) for hyperparameter selection.\n\nThis gives us an estimated contemporaneous adjacency matrix $\\hat{\\mathbf{W}}_{c}$. A random sortnregress algorithm is also used, where we determine the order of the variables randomly using i.i.d. Bernoulli trials. To estimate lagged dependencies between variables, we use the same first step and change the second step: We now regress each node on each of its predecessors $p_{i, t}$, where $t \\in\\left[1, \tau_{\\max }\right]$ indicates the time lag. After this step we have an estimated lagged adjacency matrix $\\hat{\\mathbf{W}}_{l}$."}, {"title": "5 NUMERICAL EXPERIMENTS & RESULTS", "content": "In the following section, we first give an overview of the evaluation metrics which are used for all the considered datasets. After that we outline the setup and the results for each dataset. We conduct our experiments in Python, using the TIGRAMITE library $^{2}$ for simulating data with SVAR models. When assessing the performance of different algorithms across a range of sortability values, the hyperparameters of the DYNOTEARS algorithm are set to $\\lambda_{1}=\\lambda_{2}=0.05$. The weight threshold is set to 0.1 . As a constraint-based comparison algorithm, PCMCI ${ }^{+}$[Runge, 2020] is run with $\\alpha=0.01$ and the ParCorr conditional independence test. We further use the varsortnregress, $R^{2}$-sortnregress and random regress algorithms as described in Section 4.\n\nWe assess the $F 1$-score using the formula:\n\n$$F 1=\\frac{T P}{T P+0.5(F P+F N)}$$\nto gauge the performance of the selected algorithms concerning the comparison between the estimated binary time series adjacency matrices $\\hat{\\mathbf{W}}$ and the ground truth $\\mathbf{W}$.\n\nHere, $T P$ represents the number of true positives, $F P$ represents the number of false positives, and $F N$ represents the number of false negatives for edge detection.\n\nWe refer to this metric as the overall $F 1$-score. Additionally, we calculate the $F 1$-scores comparing the estimated contemporaneous adjacency matrix $\\hat{\\mathbf{W}}_{c}$ with the true contemporaneous adjacency matrix $\\mathbf{W}_{c}$, and the $F 1$-scores comparing the estimated lagged adjacency matrix $\\hat{\\mathbf{W}}_{l}$ with $\\mathbf{W}_{l}$. These metrics are denoted as $F 1$-contemp and $F 1$-lagged, respectively. In cases where only information about the summary graph is available, we calculate the $F 1$-score between $\\mathbf{W}_{s u m}$ and the estimated summary adjacency matrix $\\hat{\\mathbf{W}}_{s u m}$."}, {"title": "5.1 NEURIPS COMPETITION DATA", "content": "Setup We also assess var- and $R^{2}$-sortability on the 2019 Causality-for-Climate-competition[Runge et al., 2019] data. This dataset is relevant not only because it was used in the competition, but also because it follows the same structure as the CauseMe platform [Munoz-Mari et al., 2020], which is widely used to evaluate causal discovery algorithms [Bussmann et al., 2021, Langbridge et al., 2023, Runge, 2020]. The dataset includes simulated and partially simulated datasets of varying complexity, including highdimensional datasets and non-linear dependencies, with 100, 150, 600 or 1000 realisations for each dataset specification. We excluded the datasets with missing values.\n\nWe set the maximal time-lag in our methods $\tau_{\text {max }}$ following the description of the respective dataset (ranging from 3 to 5 ).\n\n[^0]\n[^0]:    ${ }^{2}$ https://github.com/jakobrunge/tigramite\n\n| Dataset | varsortability | $R^{2}$-sortability |\n| :-- | :-- | :-- |\n| Testlinear-VAR_N-10_T-150 | $0.59 \\pm 0.17$ | $0.61 \\pm 0.19$ |\n| Testlinear-VAR_N-100_T-150 | $0.65 \\pm 0.11$ | $0.64 \\pm 0.12$ |\n| Testnonlinear-VAR_N-20_T-600 | $0.56 \\pm 0.13$ | $0.57 \\pm 0.14$ |\n| Finallinear-VAR_N-10_T-150 | $0.62 \\pm 0.18$ | $0.62 \\pm 0.18$ |\n| Finallinear-VAR_N-100_T-150 | $0.66 \\pm 0.11$ | $0.63 \\pm 0.11$ |\n| FinalCLIM_N-5_T-100 | $0.91 \\pm 0.13$ | $0.29 \\pm 0.25$ |\n| FinalCLIM_N-40_T-100 | $0.9 \\pm 0.09$ | $0.19 \\pm 0.11$ |\n| FinalCLIMnoise_N-5_T-100 | $0.91 \\pm 0.13$ | $0.3 \\pm 0.25$ |\n| FinalCLIMnoise_N-40_T-100 | $0.9 \\pm 0.09$ | $0.23 \\pm 0.13$ |\n| Finallogistic-largenoise_N-5_T-150_medium | $0.21 \\pm 0.32$ | $0.58 \\pm 0.35$ |\n| FinalWEATHnoise_N-5_T-1000 | $0.77 \\pm 0.21$ | $0.5 \\pm 0.3$ |\n| FinalWEATHnoise_N-10_T-1000 | $0.82 \\pm 0.17$ | $0.51 \\pm 0.23$ |\n| FinalWEATH_N-10_T-1000 | $0.84 \\pm 0.16$ | $0.52 \\pm 0.22$ |\n| FinalWEATH_N-5_T-1000 | $0.81 \\pm 0.2$ | $0.47 \\pm 0.31$ |\n\nTable 1: Mean Sortability criteria over different realisations on the NeurIps competition data. The standard deviation is given after $\\pm$.\n\nResults As illustrated in Table 1, we observe a varsortability above 0.5 for all data except the logistic model, which has a varsortability of 0 , meaning that each causal child has a lower marginal variance than its parent. In particular, the realistic climate and weather models have a high varsortability, with a mean of over 0.86 for all of them. For $R^{2}$-sortability, we observe a value around 0.5 for most of the realistic models, with the exception of the FinalCLIM models with values of 0.25 and 0.16 for the 5 and 40 variable datasets respectively. The linear and logistic models have scores between 0.5 and 0.6 ."}, {"title": "5.2 DATA GENERATION WITH ERD\u00d5S-R\u00c9NYI GRAPHS AND SVAR MODELS", "content": "Setup In order to investigate var- and $R^{2}$-sortability for datasets used to evaluate score-based causal discovery methods, we replicate one of the two data generation methods used by Pamfil et al. [2020].\n\nFollowing Pamfil et al. [2020], Zheng et al. [2018], when generating random time series graphs, we use Erd\u0151s-R\u00e9nyi Graphs (ER Graphs) [Newman, 2018] to draw the contemporaneous edges with i.i.d Bernoulli trials. Sampling only lower triangle entries of the contemporaneous adjacency matrix and then permuting the node order ensures that the contemporaneous adjacency matrix $\\mathbf{W}_{c}$ is acyclic. In order to ensure that the expected mean degree is $d_{c}$ the probability of each Bernoulli trial is set to $d_{c} /(d-1)$, where $d$ is the total number of variables. The edge coefficients are sampled uniformly at random from $[-2,-0.5] \\cup[0.5,2.0]$ Pamfil et al. [2020]. Again following [Pamfil et al., 2020], the edge weights for lagged variables are sampled depending on $t$ from $[-0.5 \\alpha,-0.3 \\alpha] \\cup[0.3 \\alpha, 0.5 \\alpha]$, where $\\alpha=1 / \\delta^{t-1}$. The weight decay $\\delta>1$ reduces the influence (weights) of variables further back in time. We randomly sample Erd\u0151s-R\u00e9nyi graphs with degree $d_{c}=4$ for the contemporaneous dimension, and for each lag we set $d_{l}=1 ; \\delta$ is set to 1.1 .\n\nWe examine sortability values for different numbers of nodes $d \\in\\{10,20,50,100\\}$. For each number of nodes we randomly generate 500 different graphs and generate $n=500$ samples per graph. We the calculate the overall varsortability, the varsortability of only the contemporanous dependencies and the varsortability of all all the lagged dependencies. We also want to investigate whether varsortability is driven by contemporaneous or lagged dependencies. This is why we compute var- and $R^{2}$-sortability over a grid of $d_{c}, d_{l} \\in$ $[0,0.5,1,2,3,4,6,8]$. We do this for $d=10$ and $d=20$ nodes.\n\nWe further investigate the influence of the two sortability criteria on the performance of score and constraint-based algorithms. In order to do so, we generate data for $d=10$ variables and then randomly draw $m=30$ samples per sortability interval, which we set to $[0,0.2],[0.2,0.4],[0.4,0.6],[0.6,0.8],[0.8,1]$. We report the performance of the following algorithms for varying var- and $R^{2}$ sortability: DYNOTEARS, DYNOTEARS standardized (run after standardising the data), $\\mathrm{PCMCI}^{+}$, varsortnregress $/ R^{2}$-sortnregress and randomregress. This means that for DYNOTEARS, the data has a varsortability as defined by the respective bin before standardising (after standardisation the varsortability is always 0.5 )."}, {"title": "5.3 EXTREMAL RIVER FLOW PROBLEM", "content": "Setup Next, we investigate the two sortability criteria on four real-world datasets modelling extremal river flow previously used in [Tran et al., 2024]. The aim of the problem is to recover the direction and connections of a river network with only extreme flow measurements at certain stations, without knowing the location of these stations. This is a time-dependent process, as an extreme measurement recorded at one station at time $t-1$ may lead to an extreme measurement at another station at time $t$ [Tran et al., 2024]. Having the ground truth river network allows us to report $R^{2}$ and varsortability on this real-world dataset and investigate whether high values can also occur on real time series data. We also include the $F 1$-scores of selected benchmark algorithms for a $\tau_{\\max }$ value of 3 as this gave the best results. Following [Tran et al., 2024], we treat the downstream river direction as the causal ground truth $\\mathcal{G}_{\text {sum }}$, which perhaps may be controversial due to the fact that downstream extreme events might have damning effects further upstream. Nevertheless, even if one is not comfortable calling the flow direction causal, the results below show that varsorting is able to cover the 'flow graph'.\n\nAs we only know the ground truth/flow graph on the summary level, i.e. we know $\\mathcal{G}_{\text {sum }}$, we calculate the $F 1$-score between the ground truth for $\\mathbf{W}_{\text {sum }}$ and the estimated adjacency matrix $\\hat{\\mathbf{W}}_{\text {sum }}$ for the following algorithms: varsortnregress, $R^{2}$-sortnregress, randomregress, $\\mathrm{PCMCI}^{+}$and reversed varsortnregress. We use reversed varsortnregress as we observed very low values for the varsortability and wanted to investigate if an algorithm that order by decreasing variance could exploit this fact.\n\n| river | var | $R^{2}$ | varsortnregc | $R^{2}$-sortnregc | randomregc | $\\mathrm{PCMCI}^{+}$ | varsortnregc rev. |\n| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| dambe | 0.07 | 0.82 | 0.08 | 0.18 | 0.11 | 0.96 | 0.20 |\n| lower-colorado | 0.34 | 0.61 | 0.08 | 0.08 | 0.10 | 0.36 | 0.08 |\n| middle-colorado | 0.29 | 0.94 | 0.09 | 0.26 | 0.22 | 0.26 | 0.23 |\n| upper-colorado | 0.43 | 0.95 | 0.26 | 0.30 | 0.22 | 0.41 | 0.11 |\n\nHowever, the algorithm did not converge after a couple of hours run time as it did not manage to satisfy the acyclity constraint, at least for our choices of hyperparameters."}, {"title": "5.4 CAUSAL CHAMBER DATA", "content": "Setup We now investigate the values of the two sortability criteria for data generated by the recently introduced Causal Chamber Gamella et al. [2024], which provides a toolbox consisting of real physical systems that can be used to evaluate causal discovery or other AI algorithms on real data.\n\nWe use each dataset contained in their PYTHON library ${ }^{3}$ and calculate the var- and $R^{2}$-sortability for each dataset and each experiment in the dataset. We then calculate the mean and standard deviation of the different datasets, where one value is one experiment performed on the dataset. We again run benchmarks on varsortnregress, $R^{2}$-sortnregress, randomregress, $\\mathrm{PCMCI}^{+}$and evaluate the $F 1$-score between $\\mathbf{W}_{\text {sum }}$ and $\\mathbf{W}_{\text {sum }}$.\n\n| Dataset | varsortability | $R^{2}$-sortability |\n| :--: | :--: | :--: |\n| lt_camera_test_v1 | $0.94 \\pm 0.01$ | $0.25 \\pm 0.24$ |\n| lt_camera_validate_v1 | $0.99 \\pm 0.02$ | $0.01 \\pm 0.03$ |\n| lt_camera_walks_v1 | $0.95 \\pm 0.0$ | $0.23 \\pm 0.07$ |\n| lt_color_regression_v1 | $0.94 \\pm 0.02$ | $0.15 \\pm 0.06$ |\n| lt_interventions_standard_v1 | $0.94 \\pm 0.03$ | $0.46 \\pm 0.04$ |\n| lt_malus_v1 | $0.98 \\pm 0.02$ | $0.02 \\pm 0.01$ |\n| lt_test_v1 | $0.96 \\pm 0.03$ | $0.01 \\pm 0.02$ |\n| lt_validate_v1 | $0.99 \\pm 0.02$ | $0.02 \\pm 0.02$ |\n| lt_walks_v1 | $0.9 \\pm 0.08$ | $0.24 \\pm 0.04$ |\n| wt_bernoulli_v1 | $0.97 \\pm 0.06$ | $0.06 \\pm 0.05$ |\n| wt_changepoints_v1 | $0.94 \\pm 0.01$ | $0.14 \\pm 0.02$ |\n| wt_intake_impulse_v1 | $1.0 \\pm 0.0$ | $0.32 \\pm 0.08$ |\n| wt_pc_validate_v1 | $0.78 \\pm 0.0$ | $0.14 \\pm 0.0$ |\n| wt_pressure_control_v1 | $0.92 \\pm 0.0$ | $0.36 \\pm 0.0$ |\n| wt_test_v1 | $0.94 \\pm 0.08$ | $0.14 \\pm 0.13$ |\n| wt_validate_v1 | $0.97 \\pm 0.05$ | $0.03 \\pm 0.04$ |\n| wt_walks_v1 | $0.92 \\pm 0.03$ | $0.28 \\pm 0.06$ |\n\nTable 3: Mean and standard deviation of var- and $R^{2}$ sortability obtained on each dataset with different experiments. The standard deviation is given after $\\pm$.\n\nResults We observe very high varsortability values for all datasets; almost all datasets have values over 0.9 with the wt_pc_validate_v1 dataset beeing the only exception at 0.78 as shown in Table 3. All $R^{2}$-sortability values are below 0.25 . Most of them are between 0.2 and 0.32 . The lt_camera_validate_v1, lt_malus_v1, lt_test_v1, wt_bernoulli_v1 and wt_validate_v1 have values very close to 0 . The values for each individual experiment contained in one dataset can be found in Appendix C."}, {"title": "6 DISCUSSION & CONCLUSION", "content": "In general, both varsortability and $R^{2}$-sortability are present in both simulated and real datasets for benchmarking causal discovery algorithms. In line with Reisach et al. [2021], we observe that DYNOTEARS, which is a NOTEARS based algorithm, seems to perform better when varsortability is higher.\n\n[^0]Furthermore, we observe that the data used in the NEURIPS competitionRunge et al. [2019] are highly varsortable, especially the realistic datasets, by our defined time series varsortability metric. The low varsortability in the simulated var models could potentially be due to the fact that these datasets do not have contemporaneous dependencies, and as we showed earlier, contemporaneous dependencies seem to be the driver for high varsortability in simulated time series data. This is hardly surprising given that a team exploiting this effect won the competition[Weichwald et al., 2020]. As for $R^{2}$-sortability, we see that high values can lead to better performance of simple benchmark algorithms. However, $R^{2}$-sortability values observed on the NEURIPS and ER Datasets seem to be too low to be exploited by the sorting algorithm.\n\nThe low varsortability value observed for the river data, particularly for the Danube, may be attributed to the fact that the width and catchment area of a river increase from the source to the mouth, resulting in a reduction in the impact of extreme flows on river velocity closer to the river's mouth. Consequently, the marginal variance decreases. This serves to illustrate the importance of scales in real dataset. The data for the other rivers only covers parts of the river, which probably results in a less intense effect and higher values for varsortability.\n\nIn addition, we can see that the Causal Chamber data is highly varsortable overall while having low $R^{2}$-values. This could be due to the fact that the variables controlled by the user are high in the causal order. We conjecture that deeper in the system and further from the user-controlled variables, dynamic turbulence and other sources of noise start having a larger and larger influence. Thus, unexplained noise is higher the lower we are in the causal order. This is in line with the low $R^{2}$-sortability values as the causal parents explain less and less down the causal order. The increase in noise variance also drives the increase in total marginal variance and affects varsortability in this way.\n\nLimitations This paper is an empirical study and we do not determine analytically why high varsortability leads to better performance of score based causal discovery algorithms for time series data. While we believe that our explanations for varsortability in the considered real-world datasets are plausible, they should be treated cautiously as hypotheses only. The only thing that we can say with certainty that sortability is highly context-dependent and therefore discarding scales as arbitrary for causal discovery seems premature.\n\nConclusion In conclusion, our paper represents an empirical extension of the work of [Reisach et al., 2021, 2023]. We demonstrate that high var-sortability occurs in SVAR-simulated time series data, resulting in enhanced performance of score-based causal discovery algorithms assuming equal noise variance. Consequently, we advise caution when assuming equal noise variance for time series causal discovery algorithms. Furthermore, it may be advisable to examine simulated data for high varsortability before using them as benchmark data, as was done in the 2019 NEURIPS competition[Runge et al., 2019]. Finally, we observe high and low varsortability, as well as $R^{2}$-sortability, in two different types of real-life datasets: the Causal Chamber data is generated in a controlled environment while the river flow dataset is measured in an uncontrolled environment. This indicates that var- and $R^{2}$-sortability in auto-correlated time series data is not solely a phenomenon observed in simulated data. For this reason, we believe that marginal variances may contain relevant causal information and exploiting variance or inverse variance sorting may be justified in some situations, if one can combine it with physical reasons for one or the other (even though these physical reasons might already eliminate the need for causal discovery in the first place). Furthermore, the observed $R^{2}$-sortability scores indicate that the assumption of equal noise variance is a tricky one as well. At the very least the relative fraction of unexplained variance may change throughout the graph and unequal noise variances are one possible reason for this."}]}