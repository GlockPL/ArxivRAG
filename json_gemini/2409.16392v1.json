{"title": "Rao-Blackwellized POMDP Planning", "authors": ["Jiho Lee", "Nisar Ahmed", "Kyle Hollins Wray", "Zachary Sunberg"], "abstract": "Partially Observable Markov Decision Processes (POMDPs) provide a structured framework for decision-making under uncertainty, but their application requires efficient belief updates. Sequential Importance Resampling Particle Filters (SIRPF), also known as Bootstrap Particle Filters, are commonly used as belief updaters in large approximate POMDP solvers, but they face challenges such as particle deprivation and high computational costs as the system's state dimension grows. To address these issues, this study introduces Rao-Blackwellized POMDP (RB-POMDP) approximate solvers and outlines generic methods to apply Rao-Blackwellization in both belief updates and online planning. We compare the performance of SIRPF and Rao-Blackwellized Particle Filters (RBPF) in a simulated localization problem where an agent navigates toward a target in a GPS-denied environment using POMCPOW and RB-POMCPOW planners. Our results not only confirm that RBPFs maintain accurate belief approximations over time with fewer particles, but, more surprisingly, RBPFs combined with quadrature-based integration improve planning quality significantly compared to SIRPF-based planning under the same computational limits.", "sections": [{"title": "I. INTRODUCTION", "content": "Partially Observable Markov Decision Processes (POMDPs) are a powerful mathematical framework for modeling decision-making under uncertainty where an agent operates in an environment with incomplete or noisy information [1]. POMDPs have been widely applied to various domains such as aircraft collision avoidance, automated driving, and search-and-rescue with drones [2] [17] [18] [19]. However, one of the key challenges in implementing POMDPs is the need for efficient belief updates that maintains a reliable probability distribution across the system's possible states.\nParticle filters are effective for modeling the agent's beliefs when the state space is large or continuous [4] [5] [16]. However, particle filters often suffer from particle deprivation where the diversity of particles diminishes over time. To address this issue, Sequential Importance Resampling Particle Filtering (SIRPF), also known as Bootstrap Particle Filtering, is commonly employed. SIRPF mitigates particle deprivation by resampling particles based on their weights, discarding low-weight and duplicating high-weight particles. Additionally, various strategies, such as adaptive particle injection and rejection, have been used to mitigate the deprivation issue [1]. While SIRPF is effective in many scenarios, it still remains sensitive to outliers and unlikely observations, leading to a low Effective Sample Size (ESS) where only a small fraction"}, {"title": "II. BACKGROUND", "content": "In Markov Decision Processes (MDPs), agents operate with complete knowledge of the current state of the environment. At any given time, the agent fully understands where it is or what the situation is despite uncertainties in future state transitions. In POMDPs, however, agents do not have perfect knowledge of the current state. They must make decisions un-der uncertainty about the current as well as future states; thus, POMDPs are more suitable for real-world robotics scenarios where perfect state information is rarely available [26].\nA POMDP is formally defined by the tuple $(S,A,T,O,R, Z, \\gamma)$ where S is the set of possible states, A is the set of actions, O is the set of possible observations, T is the state transition function, R is the reward function, Z is the observation function, and $\\gamma$ is the discount factor that balances between immediate and future rewards. Because the agent lacks full visibility of the state, it maintains a probabilistic belief over the current possible states of the environment. This belief is updated using the following Bayesian equation:\n$b'(s') \\propto P(o|s', a) \\sum_{s \\in S} P(s'|s, a)b(s)$,\nwhere b'(s') is the posterior belief of being in state s' after taking action a and receiving observation o, and b(s) is the prior belief of being in state s.\nSolving POMDPs involves finding optimal policies that specify the best action to take based on the current belief. The value function, which guides the selection of the best action, can be expressed as:\n$V^*(b) = \\max_{a \\in A} r(b,a) + \\gamma \\sum_{o \\in O} P(o|b, a)V^*(b'),$\nwhere $V^* (b)$ is the optimal value of the belief state b, r(b, a) is the expected immediate reward for taking action a, P(o|b, a) is the probability of receiving observation o after taking action a, and b' is the updated belief. Hence, by continuously updating its belief, the agent can make more informed decisions in uncertain environments."}, {"title": "B. Rao-Blackwellized Particle Filter (RBPF)", "content": "Rao-Blackwellized Particle Filtering (RBPF) is an advanced variant of particle filtering that leverages the Rao-Blackwell theorem to enhance computational efficiency and estimation accuracy. This theorem is based on sufficient statistics to reduce the variance of estimators [13] as represented by the following inequality:\n$\\text{var}[\\tau(s)] \\geq \\text{var}[\\tau(s|\\Theta)].$\nHere, $\\tau$ is any kind of estimator of s, and $\\Theta$ represents the sufficient statistics for s. Essentially, conditioning on $\\Theta$ retains all necessary information about s, which can only reduce the variance of the estimator $\\tau$. Applying this principle to Monte Carlo-based estimators, RBPF can use fewer particles by incorporating sufficient statistics to capture some state parameters analytically."}, {"title": "III. TECHNICAL APPROACH", "content": "Several adjustments are necessary to implement analytical filters within sampling-based approximate POMDP solvers, an approach we dub the RB-POMDP framework. In an RBPF, each particle is not just a discrete state sample but is also associated with a conditional analytical distribution. This requires modifications in both the belief update process and the planning algorithm. This section describes our novel approach to address both these issues."}, {"title": "A. Rao-Blackwell factorization of POMDPs", "content": "The main idea in RPBF is to factorize the state space into two components: tractable and non-tractable states. More formally, the joint posterior distribution of these tractable and non-tractable components can be factored into the following equation using the chain rule:\n$p(s_{k+1}, S_{k+1} | o_{1:k}) = P(s_{k+1} | S_{k+1}, o_{1:k})P(S_{k+1} | o_{1:k}) \\qquad(1)$\nThe RBPF uses this relationship to update the marginalized distribution of the tractable (or \"easy\") state $s_{k+1}$ analytically using methods like Kalman filters, while the non-tractable (or \"tough\") state $S_{k+1}$ is updated through particle filtering. Consequently, each particle in RBPF is associated with a conditional analytical distribution, which in many practical cases can be summarized by sufficient statistics.\nEquation 1 implies that if the state space can be correctly decomposed into non-tractable and tractable components, then we can leverage RBPF within POMDP models. This approach allows RBPFs to focus the particle filter sampling only on the \"tough\" component, reducing the overall number of particles."}, {"title": "B. RBPF Belief Updates", "content": "The most common choice for the importance distribution in the sequential importance sampling step is the transition function p(s'|s, a) which simplifies the importance weight to be equal to the observation likelihood [15]. For RBPFs, the weights are updated based on this observation likelihood that is calculated using the analytical method applied to the tractable components [23]. When using a Kalman filter, for example, the update involves the innovation covariance matrix. This approach differs from the standard SIRPF where the weights are updated solely based on the likelihood of the observation and does not incorporate an analytical step. The RBPF belief update is detailed in Algorithm 1."}, {"title": "C. RBPF in Sampling-Based Online Planners", "content": "Partially observable Monte Carlo planning (POMCP), a widely used online planner, and its variant POMCPOW rely on Monte Carlo sampling via the particle filter to estimate the value function [4] [5]. As shown in Fig. 1 and Alg. 2, these planners build a local policy from the agent's current belief state by running N tree search simulations to a depth D. The value at an observation-action history $\\tau$ steps into the future, h is estimated based on the rewards for the set of simulations consistent with that history, I(h), leading to the following value approximation:\n$V(h) \\approx \\frac{1}{|I(h)|} \\sum_{i \\in I(h)} \\sum_{d=\\tau}^{D} \\gamma^{d-T} \\frac{\\gamma^{d-T}E[r(s_{i,d}, s_{i,d}, s_{i,d,k}, a_{i,d})]}$\nMonte Carlo sampling, however, is known for its slow con-vergence rate of O(1/$\\sqrt{N}$). As a result, this approach can be computationally heavy and can require a large number of sam-ples to achieve accurate estimates. By leveraging RBPF, we can compute expectations over the tractable components using deterministic numerical integration methods (e.g. quadrature methods) such that the overall number of Monte Carlo samples (i.e. tree iterations) can be reduced, as detailed below."}, {"title": "D. Rao-Blackwellized POMCPOW (RB-POMCPOW)", "content": "The standard logic of POMCP and POMCPOW solvers can be adapted to effectively utilize the analytical distributions that each Rao-Blackwellized particle carries as depicted in Figure 1. To begin, consider the immediate reward for the states consistent with h approximated as follows:\n$R(h) \\approx \\frac{1}{|I(h)|} \\sum_{i \\in I(h)} \\sum_{s \\in selt} E [R(s_{i,d}, a_{i,d})].$\nHere, the expectation is taken over $s_{elt}^{i}$ set to account for the uncertainty in the analytical distributions. In other words, we need to employ appropriate integration methods that allow us to compute the value function in a manner that incorporates different possible realizations of the tractable state compo-nents. Next, we extend the approximation over a multi-step planning horizon of depth D where the agent accumulates discounted rewards from future actions $a_{i,d}$:\n$V(h) \\approx \\frac{1}{|I(h)|} \\sum_{i \\in I(h)} \\sum_{d=\\tau}^{D} \\gamma^{d-T} E[r(s_{i,d}, s_{i,d}, s_{i,d,k}, a_{i,d})].$\nWhile straightforward Monte Carlo sampling can also be used here to compute these expectations, deterministic quadrature techniques (e.g., tensor product quadrature) offer the advantage of faster convergence and higher accuracy, particularly for lower dimensional integrals. Each expectation is approximated using a set of M quadrature points $s_{i,d,k}^{elt}$ and corresponding weights $W_{i,d,k}$, enabling us to replace the expectation over the tractable components with a weighted sum:\n$V(h) \\approx \\frac{1}{|I(h)|} \\sum_{i \\in I(h)} \\sum_{d=\\tau}^{D} \\gamma^{d-T} \\sum_{k=1}^{M} W_{i,d,k} R(s_{i,d}, s_{i,d}, s_{i,d,k}, a_{i,d}).$\nWhen using quadrature techniques, the choice of method de-pends on the distribution used for the analytically marginalized states. For example, we can employ Gaussian-Hermite quadra-ture to interpolate and perform the necessary integration when using Gaussian beliefs for the linear states [8]. If different distributions were considered, other methods from the Askey family of orthogonal polynomials could be utilized [6] [7]. To mitigate the curse of dimensionality, we can implement a Smolyak sparse grid, which efficiently reduces the number of quadrature points required while maintaining accuracy [9] [10]. The Smolyak formula for constructing the sparse grid is:\n$A(q, d) = \\sum_{q-d+1<|i|<q} (-1)^{q-|i|} \\binom{d}{|i|-q+d-1}\\bigotimes_{j=1}^{d} Q_{i_j}$\nwhere $A(q, d)$ is the set of Smolyak quadrature points and their corresponding weights for a given sparse grid level q and dimensionality d, $i$ is a multi-index with $i_j$ representing the level of the univariate quadrature rules, and $\\bigotimes_{j=1}^{d} Q_{i_j}$ denotes the tensor product of the univariate quadrature rules $Q_{i_j}$ at levels $i_j$ for each dimension j.\nA higher level of the sparse grid results in greater accuracy but requires more computational time. By choosing different"}, {"title": "IV. EXPERIMENTS", "content": "This section focuses on a specific localization problem, but the approach presented here can be applied to a broader class of planning problems. We used the POMDPs.jl [25] in implementing the methods presented in this work."}, {"title": "A. Localization Problem", "content": "The goal of our planning module is to devise a navigational strategy for an agent in a 2-dimensional environment to reach the target location at the origin while avoiding the obstacle. Additionally, the agent operates in a GPS-denied environment, making it a localization problem where the agent predicts its own position based on noisy observations from static landmarks. This section details the elements of the POMDP model $P = (S, A,T,O,R, Z)$ that we utilize for our study.\nState Space S: The state space consists of continuous variables representing the agent's position and orientation. Specifically, the state vector is defined as $s = {\\xi, \\eta, \\theta}$, where $\\xi$ and $\\eta$ are the agent's coordinates and $\\theta$ is its heading.\nActions A: The action set available to the agent includes different movement modes, represented as tuples (st, wt) where st is the speed and w\u2081 is the turning rate. The agent can move forward, backward, or turn left and right by adjusting these parameters.\nObservations and Observation Model O, Z: The ob-servation space consists of noisy sensor measurements of known landmarks. The observation vector is defined as o= {($\\rho_j, \\phi_j$)}$_{j=1}^J$ where $\\rho_j$ and $\\phi_j$ are relative range and bearing to each landmark j, respectively, and J is the total number of landmarks the agent uses for localization. The observation model includes these noisy relative range $\\rho_j$ and bearing $\\phi_j$ measurements which are modeled as:\n$\\rho_j(k) = \\sqrt{(\\xi_j - \\xi(k))^2 + (\\eta_j - \\eta(k))^2} + v_r(k)$,\n$\\phi_j(k) = atan2(\\eta_j - \\eta(k), \\xi_j - \\xi(k)) - \\theta(k) + v_b(k)$,\nwhere $v_r(k) \\sim N(0, 0.25)$ and $v_b(k) \\sim N(0, 0.01)$ represent the additive white Gaussian noise (AWGN) components in the range and bearing measurements, respectively."}, {"title": "Transition Model T", "content": "The dynamics of the agent are described by the following differential equations:\n$\\dot{\\xi} = s_t \\cdot \\tilde{\\omega}_s \\cdot \\cos(\\theta)$,\n$\\dot{\\eta} = s_t \\cdot \\tilde{\\omega}_s \\cdot \\sin(\\theta)$,\n$\\dot{\\theta} = \\omega_t \\cdot \\tilde{\\omega}_\\theta.$\nHere, $\\tilde{\\omega}_s$ and $\\tilde{\\omega}_\\theta$ represent the noise components, distributed as $N_w([1; 1]^T, 0.1\\cdot I_{2x2})$, affecting the speed and turning rate, respectively. The variables st and $ \\omega_t$ are defined by the chosen action. Note that $\\dot{\\xi}$ and $\\dot{\\eta}$ are dependent on $\\theta$.\nReward Model R: The reward function is defined as quadratic to penalize deviations from the desired state (the origin) and the control input exerted by the agent, while also providing a terminal reward for successfully reaching the goal. The overall reward function is defined as:\n$R(s, a) = -(\\Psi s^T s + \\Phi_a a^T a) + R_{terminal}$,\nwhere $\\Psi_s$ and $\\Phi_a$ are weighting matrices.\nFor our problem, we selected POMCPOW over POMCP for online planning due to its ability to handle continuous obser-vation spaces through Observation Widening (OW) [5]. Also, the Unscented Kalman Filter (UKF) [12] is used to derive the Gaussian belief $N(\\mu, \\Sigma)$ for $\\xi$ and $\\eta$ when given a predicted $\\theta$. We conducted 100 simulations to analyze and compare the performance of SIRPF and RBPF in our localization problem by using POMCPOW and RB-POMCPOW as the respective planners. A representative simulation is depicted in Figure 2."}, {"title": "B. Filter Performance Testing", "content": "RBPF enables analytical consistency testing that provides quantitative metrics to assess the filter's accuracy and consis-tency for the tractable states. This is an advantage over SIRPF which relies solely on particles for all state components. To ensure the filter's accuracy, we conducted Normalized Estimation Error Squared (NEES) and Normalized Innovation Squared (NIS) tests [28] and fine-tuned the UKFs within the RBPF. Following this, we evaluated the efficiency of the particles in each filter using the Effective Sample Size (ESS) as illustrated in Figure 3. ESS measures the number of effective"}, {"title": "C. Computational Costs and Cumulative Rewards", "content": "We focus on two key aspects of the computational costs: the time required for belief updates and the time needed for the planner's action selection. From Table I, it is evident that RBPF is slower than SIRPF for belief updates when using the same number of particles. This difference is attributed to the increased computational complexity involved in using UKFS and analytically updating the Gaussian belief associated with each particle. Expectedly, the time taken for belief updates increases linearly with the number of particles. As discussed in Section IV-B, it was observed that RBPF consistently achieved a high ESS with 100 particles throughout the simulations. This informed our decision to use 100 particles for RBPF and 1000 particles for SIRPF. Consequently, this choice provided com-putational benefits during belief updates, with RBPF taking only 9 ms per step compared to 32 ms for SIRPF.\nNext, we analyzed the impact of value function accuracy on cumulative rewards. In the RB-POMCPOW algorithm, we"}, {"title": "V. DISCUSSION", "content": "The higher ESS observed in RBPF, even with fewer parti-cles, demonstrates its efficiency in state estimation within the POMDP model. In contrast, SIRPF required a substantially larger number of particles to estimate all three states and still exhibited low ESS. Additionally, we had to employ adaptive resampling and regularization by introducing a small amount of noise after the resampling step to prevent particle degeneracy. This highlights common issues in particle filtering and illustrates how RBPF can be more effective in addressing these limitations.\nAlthough RBPF incurs higher computational costs for belief updates due to the added complexity of analytical updates, these costs are mitigated by reducing the number of particles. Also, our results show that RB-POMCPOW, using a sparse grid level of 3 with 50 tree iterations, achieved comparable performance to POMCPOW using 1000 tree iterations and was approximately seven times faster. Moreover, RB-POMCPOW outperformed POMCPOW under the same computational time at each sparse grid level. This validates the computational advantages of RBPF in both planning and belief updates without sacrificing accuracy or cumulative rewards.\nFuture research could explore different Askey family dis-tributions for sparse grid quadrature or simpler analytical updates, such as the Extended Kalman Filter (EKF), to further reduce computational time and apply the RB-POMDP frame-work to more complex problems. Since the number of particles needed for SIRPF to avoid particle collapse can grow \"super-exponentially\" with the state dimension [3], this work presents a compelling alternative for use in those complex models.\nLastly, while this generic framework gives flexibility to practitioners, it also introduces additional parameters that need to be carefully considered and calibrated. This requires signif-icant time and effort to fine-tune the model. One promising way to mitigate this burden is using the Bayesian optimization strategies proposed in [11] that could be employed to automate the tuning of Kalman filters. By adopting such methods when using Kalman filters for analytical updates, we could develop a more efficient framework with reduced reliance on heuristic calibration."}, {"title": "VI. CONCLUSION", "content": "In this study, we introduced the RB-POMDP framework that leverages RBPFs to enhance belief estimation and decision-making processes. Our flexible approach enables practitioners to incorporate analytical methods tailored to the distribution of tractable states and provides a generic framework that can be used for various applications. Additionally, the presented RB-POMCPOW planning algorithm allows users to balance accuracy and computational efficiency by selecting appropriate integration methods. The effectiveness of this approach was demonstrated in a localization problem where we employed UKF with Gaussian Hermite quadrature and Smolyak sparse grid. We believe this work highlights the potential of the RB-POMDP framework and opens the door to more computation-ally viable solutions in complex decision-making scenarios."}]}