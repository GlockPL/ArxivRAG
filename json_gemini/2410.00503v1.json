{"title": "Drone Stereo Vision for Radiata Pine Branch Detection and Distance Measurement: Utilizing Deep Learning and YOLO Integration", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "abstract": "This research focuses on the development of a drone equipped with pruning tools and a stereo vision camera to accurately detect and measure the spatial positions of tree branches. YOLO is employed for branch segmentation, while two depth estimation approaches, monocular and stereo, are investigated. In comparison to SGBM, deep learning techniques produce more refined and accurate depth maps. In the absence of ground-truth data, a fine-tuning process using deep neural networks is applied to approximate optimal depth values. This methodology facilitates precise branch detection and distance measurement, addressing critical challenges in the automation of pruning operations. The results demonstrate notable advancements in both accuracy and efficiency, underscoring the potential of deep learning to drive innovation and enhance automation in the agricultural sector.", "sections": [{"title": "I. INTRODUCTION", "content": "Radiata pine (Pinus radiata) is a highly valuable species extensively cultivated in New Zealand due to its rapid growth and significant contribution to the forestry and timber industries [54]. Its wood is crucial for construction and manufacturing, making it a key economic asset for the country [55]. Regular pruning is essential to maintain wood quality and promote the development of straight trunks [56]; however, manual pruning is labor-intensive and poses substantial safety risks [57]. Consequently, there is a pressing need for automated solutions to address these challenges.\nIn response, we propose an autonomous drone equipped with a stereo camera and a pruning tool capable of detecting and trimming branches as thin as 10mm. By leveraging stereo vision for branch identification and distance measurement, our system aims to enhance the precision, efficiency, and cost-effectiveness of drone-assisted pruning [1].\nTo develop and evaluate this system, we already systematically collected indoor data using a ZED Mini camera\u00b9, capturing high-resolution images (1920\u00d71080) from various strategic locations within a laboratory setting. Tree branches were photographed under diverse lighting conditions to create a robust and representative dataset, mitigating the risk of overfitting to idealized training data. This process resulted in\n61 pairs of images (totaling 122) for training and an additional 10 pairs for testing.\nUtilizing this dataset, we evaluated several object detection models and found that the YOLO [11] [12] architecture outperformed Mask R-CNN [9] in accurately detecting tree branches. Additionally, the application of Semi-Global Matching (SGBM) [29] combined with Weighted Least Squares (WLS) [58] post-processing yielded depth maps of reasonable clarity. However, persistent pixel mismatches around tree branches continued to limit the precision of depth estimation.\nGiven the absence of ground-truth depth data and the limitations of depth maps produced by SGBM with WLS post-processing, the present research aims to fine-tune deep neural networks to generate depth maps that closely approximate ground-truth data. By enhancing depth estimation accuracy, these improved depth maps can be integrated into drone systems to enable precise evaluation of tree branch depth information. This advancement is expected to significantly improve drone navigation and pruning capabilities, thereby contributing to more efficient and safer forestry management operations."}, {"title": "II. RELATED WORK", "content": "Depth map generation is a fundamental component of computer vision, enabling the reconstruction of a scene's three-dimensional structure from visual data [17]. In our drone application, which utilizes a stereo camera system such as the ZED Mini, depth maps are computed from two distinct viewpoints. These maps are essential for the precise identification of tree branches and the accurate measurement of their distance from the drone, thereby facilitating effective pruning operations.\nTraditional depth estimation methods like Block Matching (BM) [27], [28] and Semi-Global Block Matching (SGBM) [29] have been widely employed. BM is a local search-based technique that calculates depth values by finding the best match within a fixed window, making it suitable for real-time applications. However, it is prone to errors in regions with sparse textures or occlusions. SGBM improves upon BM by introducing a semi-global cost aggregation strategy, enhancing the accuracy and robustness of depth estimation by optimizing pixel correspondences across the entire image-a method particularly effective in handling texture-rich scenes.\nWith the advancement of deep learning techniques, neural network-based methods have significantly improved the accuracy and efficiency of depth estimation. Stereo matching networks such as ACVNet [30] and GWCNet [31] introduce sophisticated learning mechanisms to process stereo images, generating more accurate depth maps by learning matching information from paired images through deep models. MobileStereoNet [32] optimizes the network architecture for mobile devices, achieving efficient depth prediction suitable for resource-constrained environments. PSMNet [33] enhances feature extraction and depth estimation through pyramid pooling and 3D convolutions, while RAFT-Stereo [34] employs recurrent neural networks to perform pixel-level correlation analysis, further improving the accuracy of depth estimation.\nInnovative approaches like Neural Radiance Fields (NeRF) [35] have also emerged, demonstrating the capability to reconstruct complex 3D scenes through the generation of dense depth maps using deep learning. Additionally, monocular depth estimation methods, including models like MiDaS [49] and Depth Anything [38], leverage single-camera input to estimate depth information. These models are suitable for applications where stereo or LiDAR setups are impractical, with MiDaS recognized for its versatility across various environments and Depth Anything extending this capability by enabling depth estimation in a wide array of scenes with improved generalization.\nIn this research, we focus on improving depth maps without relying on ground-truth data, diverging from traditional methods. By employing deep neural network architectures based on stereo and monocular inputs, we aim to generate accurate depth maps of tree branches. This approach enhances the effectiveness of drone-assisted pruning operations by providing precise depth information crucial for manipulation tasks."}, {"title": "III. METHODS", "content": "In this section, we focus on depth map generation for the small branches dataset, where YOLO is employed for both detection and segmentation. By integrating YOLO with additional deep learning techniques, we systematically process the data to achieve precise branch detection and accurate depth estimation using a stereo vision camera. The complete workflow is depicted in Fig. 2."}, {"title": "A. Branches Data Preparation and Application of the YOLO Instance Segmentation Method", "content": "Previous research [1] have collected and annotated indoor branch data, resulting in a relatively small dataset, and conducted comparative analyses of various Mask R-CNN backbones and different versions of YOLO, including YOLO v8 and v9. The findings demonstrate that YOLO achieves highly accurate branch segmentation, with YOLOv8n-seg exhibiting the smallest number of parameters (Params) and computational complexity (FLOPs) [53], making it the most efficient model for this dataset. Accordingly, YOLOv8n-seg was selected for this research. As illustrated in Fig. 3, the model accurately segments branch contours in the images."}, {"title": "B. Deep learning Methods for Generating Depth Map", "content": "Compared to traditional algorithms, deep learning methods for generating depth maps primarily focus on two areas: monocular depth estimation and stereo depth estimation. In the field of monocular depth estimation, two leading models are MiDaS [49] and Depth Anything [38]. MiDaS employs transformer-based backbones such as BEiT, Swin, and SwinV2, trained on large, diverse datasets using a scale-and-shift-invariant loss function, enabling robust cross-dataset zero-shot inference. In contrast, Depth Anything leverages vast amounts of unlabeled data while retaining rich semantic priors from pre-trained models. This approach allows Depth Anything to excel in zero-shot depth estimation and serve as a robust foundation for metric depth estimation. We directly applied MiDas and Depth Anything to our own dataset to observe its prediction results.\nIn the domain of stereo depth estimation, widely used methods include ACVNet [30], GwcNet [31], PSMNet [33], and RAFT-Stereo [34]. Additionally, NeRF-Supervised Deep Stereo [35] offers significant advantages by generating high-quality depth maps without requiring ground-truth depth data. In our research, we selected several commonly used stereo matching datasets, including KITTI [47] and Scene Flow [48]. Each model architecture was thoroughly fine-tuned on these datasets to ensure adaptability and robustness across different scenarios.\nWe then examined the impact of fine-tuning on disparity map predictions using PSMNet as a representative model. Specifically, we fine-tuned the model on three datasets-KITTI2012, KITTI2015, and Scene Flow and conducted detailed testing. The model was trained for 100 epochs on the KITTI2012 and KITTI2015 datasets, which primarily consist of stereo image pairs from real-world urban street scenes with high-quality ground-truth data. During the fine-tuning process, we observed that, despite differences in scenes and labeling across datasets, extended training on a single dataset significantly improved the model's prediction accuracy.\nFinally, we fine-tuned various models on our own dataset. Despite the absence of ground-truth data for precise quantitative evaluation, we assessed the accuracy of depth predictions around tree branches by visually inspecting the clarity of pixel depth information. The comparative experimental results are presented and discussed in Section IV-A.\""}, {"title": "C. Integration of Image Instance Segmentation and Depth Map Generation", "content": "In the preceding sections, we have extensively examined instance segmentation and depth estimation as independent tasks. However, the overarching goal is to equip the stereo camera mounted on the drone with the ability to perform both instance segmentation and depth estimation concurrently, enabling precise spatial localization of tree branches. Achieving this requires the integration of the segmentation and depth estimation models.\nPrevious integration approaches [1] involved connecting all segmented points into a plane, recording their coordinates, and mapping these points to their corresponding positions on the depth map, thereby generating a distribution. This method ultimately estimated the distance between the branch and the camera. In contrast, we propose a more efficient approach, which is mathematically formalized in this research.\nOur method aims to identify the coordinates of points surrounding the branch and apply the Triangulation Method algorithm to accurately determine their locations. By correlating these points with their respective positions on the depth map, which represent the distances from the points to the camera plane, we refine the results through median calculation, outlier removal, and averaging. This process yields a precise estimation of the distance between the branch and the camera plane.\nWe define the P be the set of the predict points, and the ith point is pi, Assume we have n points, so P = {P1, P2, ..., Pn}, 0 < i < n. and pi = (xi, Yi), Where xi is the horizontal coordinate value of point pi in the depth map, and yi is the Vertical coordinate value of point pi in its depth map.\nwe through the 3 points can get it's centroid point, and the centroid mostly should in the branch, let the centroid points set be the P', P' = {P'1,P'2, ...,P'k}, 0 < k \u2264 3, p'i = (x'g, y'g). Where xg is the horizontal coordinate value of centroid p'g in the depth map, and yg is the Vertical coordinate value of centroid p'g in its depth map. Let each centroid increase by m values, and the jth point added to centroid pi is pi,j. Let the expanded points set be P'', where P'' = {P1, P2, ..., Pn}, with P'' = {Pi,1, Pi,2,\u00b7\u00b7\u00b7, Pi,m}, and pi,j = (xi,j, Yi,j). We then define the total points set as P''' = P'' + P'. Finally, use MAD (Median Absolute Deviation) to remove values where the error is too large or too small.\n$MAD = median(|P''' \u2013 median(P''')|)$ (1)\nif any point in P''' is in the median(P''')\u00b1k\u00d7MAD, whose will be left and then compose P, and we calulate the the mean to make sure the final distance between the branche and caerma,\n$distance = mean(P)$ (2)\nSo, we get the finall distance between the camerea and branches. And the result is shown in Fig.9. Compose all of the red dots in the picture are P. Then, each point is mapped to its corresponding position on the depth map, yielding a set of corresponding depth values. After applying MAD (Median Absolute Deviation) and averaging, the final distance between the tree branch and the camera is obtained."}, {"title": "IV. RESULTS AND ANALYSIS", "content": "In this section, we present the results of monocular depth estimation for tree branches at distances of 1m, 1.5m, and 2m. Additionally, we provide a case study of PSMNet fine-tuning across different datasets, followed by the outcomes of fine-tuning various deep learning models. A comparative analysis between NeRF and SGBM at the specified distances of 1m, 1.5m, and 2m is also included. Furthermore, we present the results obtained using our Integration Centroid Point Calculation method."}, {"title": "A. Comparative research of Various Deep Learning Approaches for Depth Map Generation", "content": "we positioned the branches at distances of 1m, 1.5m, and 2m from the camera for testing using a monocular camera. The results are illustrated in Fig. 4. At a distance of approximately 2m, the image quality deteriorates noticeably, becoming significantly blurred."}, {"title": "B. Final Results Achieved by Combining YOLO with NeRF Using the Second Combination Method", "content": "We employed YOLOv8n-seg as the instance segmentation model. Utilizing the original combination method [1], a comparative analysis was conducted between two leading depth estimation models, SGBM and NeRF, as demonstrated in Fig. 7. The resulting data distribution is presented in Fig. 8. The analysis indicates that while SGBM primarily detects points within a more accurate depth range, it yields fewer detected points compared to NeRF. Consequently, future research will focus on enhancing depth map accuracy for tree branches through the application of NeRF, with the aim of approximating ground-truth depth data. Nonetheless, a key limitation of NeRF is its processing time, currently requiring approximately 6 seconds to generate a depth map. Addressing this computational efficiency will be a major priority in our subsequent investigations."}, {"title": "V. CONCLUSIONS", "content": "In this research, we have integrated advanced branch detection and segmentation techniques with high-precision depth maps generated by deep learning models to enhance the accuracy of distance measurements between tree branches and drones, thereby facilitating precise and efficient pruning operations. While depth map generation through deep learning methods such as NeRF has demonstrated exceptional accuracy, its slow processing speed remains a significant limitation.\nOur integration aims to mitigate this issue by leveraging the strengths of both advanced detection techniques and deep learning models. Recognizing the constraints imposed by our current small dataset, future work will involve utilizing drones to collect a larger and more diverse dataset with increased environmental complexity. This expanded dataset is expected to enhance the robustness of our models and provide a valuable foundation for continued research in this domain."}]}