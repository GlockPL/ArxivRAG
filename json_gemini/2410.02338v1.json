{"title": "HOW MUCH CAN RAG HELP THE REASONING OF LLM?", "authors": ["Jingyu Liu", "Jiaen Lin", "Yong Liu"], "abstract": "Retrieval-Augmented Generation (RAG) has gained significant popularity in modern Large Language Models (LLMs) due to its effectiveness in introducing new knowledge and reducing hallucinations. However, the deep understanding of RAG remains limited, how does RAG help the reasoning process and can RAG help improve the reasoning capability remains question. While external documents are typically considered as a method to incorporate domain-specific information, they also contain intermediate reasoning results related to the query, this suggests that documents could enhance the reasoning capability of LLMs, which has not been previously explored. In this paper, we investigate this issue in depth and find that while RAG can assist with reasoning, the help is limited. If we conceptualize the reasoning process as a tree with fixed depth, then RAG struggles to assist LLMs in performing deeper reasoning. Additionally, the information in the documents requires preprocessing to filter out noise. We demonstrate that this preprocessing is difficult to achieve simply fine-tuning of the LLM, it often necessitates numerous additional transformer layers to solve the problem. To simplify the problem, we propose DPrompt tuning, which effectively resolves the issue within just limited transformer layers, leading to improved performance.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) (Brown, 2020) have demonstrated remarkable capabilities across a variety of tasks, including text generation and question answering (Ouyang et al., 2022; Wei et al., 2022), code generation (Gu, 2023), and information retrieval (Dai et al., 2024). However, current LLMs often suffer from serious hallucinations (Huang et al., 2023) due to a lack of factual information. Moreover, the knowledge embedded within LLMs is encoded in their parameters (Yang et al., 2024), meaning that incorporating new knowledge requires further fine-tuning, which is both time-consuming and resource-intensive. Consequently, augmenting LLMs with external retrievers has led to significant performance improvements (Lewis et al., 2020; Zhao et al., 2024; Izacard et al., 2023).\nDespite the widespread adoption of RAG in modern LLMs, a comprehensive understanding of how RAG aids inference remains an open question. Most researchers currently view RAG primarily as a method to provide domain-specific knowledge (Zhang et al., 2024), often seeking to adapt LLMs to particular domains through RAG (Soudani et al., 2024). However, the impact of RAG on enhancing reasoning capacity has yet to be thoroughly investigated.\nFirst, we aim to understand how RAG operates and whether it can improve the reasoning capabilities of LLMs. We can consider the LLM as computing p(y|q), where q represents the query and y is the corresponding answer. In this context, retrieval-augmented generation can be expressed as p(y|q, d1, d2,..., dk), where di is the i-th document retrieved based on the query q. Additionally, the well-known prompting technique, Chain of Thought (CoT) (Wei et al., 2022), which significantly enhances the reasoning ability of LLMs, can be represented as p(y|q, x1, x2,...,xk), where"}, {"title": "2 How DOES RAG HELP REASONING", "content": "In this section, we explore the extent to which RAG can enhance the reasoning ability of LLMs, aim-ing to deepen our understanding of its role in reasoning. As demonstrated by Merrill & Sabharwal (2023), transformer-based models are constrained to a fixed depth of reasoning due to the limited number of layers. Chain of Thought (CoT) facilitates step-by-step reasoning, effectively stacking multiple LLMs to address a single problem. Thus, the question is: How can RAG assist during the reasoning process, and how much additional reasoning capacity can RAG provide to LLMs?"}, {"title": "2.1 THE FISSION OF RAG", "content": "If we consider the reasoning steps of an LLM as a tree, the maximum reasoning depth is fixed at L due to the constraints of the depth of the model. Additionally, the maximum width of this tree is also limited, as the number of attention heads per layer is finite. For a reasoning tree T with L layers, let the number of nodes in layer i be denoted as ni, and refer to the j-th node in the i-th layer as ui,j. The retrieved document d contains relevant information that can be leveraged to replace certain reasoning nodes with extracted document content. For instance, consider the query Who is the actor playing Jason on General Hospital?' (Yoran et al., 2023). In this scenario, there may be a node ui,j representing the information about What is General Hospital?'. If we provide a document containing detailed information about General Hospital, the computation for ui,j can effectively be replaced by extracting relevant information from that document.\nDefinition 2.1 (Reasoning Tree). For a reasoning tree TL of L layers, and there are ni nodes in each layer l, nL\u22121 = 1, all nodes in layer l are connected to at least to one node in layer l +1. with probability 1 q\u0131, node ul,i are connected to ul+1,j, and the isolated nodes are randomly connected to a node in case of useless nodes.\nIn the reasoning tree, each node represents a calculation, with the output being some intermediate reasoning results. The connection between nodes ui,j and Ui-1,k indicates that the calculation of Ui,j relies on the output of Ui\u22121,k. The parameter q\u012b reflects the sparsity of these connections. Consequently, for a node ui,j, a document may contain relevant information, allowing the calculation for ui,j to be effectively replaced by information extracted from the document.\nDefinition 2.2 (Retrieval). For a reasoning tree TL, with probability pi, the nodes in layer l can be replaced by retrieved documents.\nIt's important to recognize that processing documents also necessitates a certain depth of reasoning. However, extracting information from a document is fundamentally an information extraction pro-cess, whereas directly addressing a query is a purely inferential process. Therefore, we can assume that the cost of information extraction is lower than that of inference. This enables LLMs to leverage external information, thereby simplifying complex reasoning tasks.\nAssumption 2.3. If the document d contains information about node u at the l-th layer, then only \u03bbl layers are needed to extract the information from d, \u03bb < 1.\nThis assumption suggests that while processing documents requires some reasoning depth, it is generally shallower than the inference needed for directly addressing a query. Therefore, utilizing information from documents should be a more efficient solution. In this way, RAG can eliminate"}, {"title": "3 NOISE IN THE DOCUMENT", "content": "In real-world applications of RAG, Due to the limited capacity of the retriever, distractions fre-quently arise in the retrieved documents (Shi et al., 2023a; Wu et al., 2024). Moreover, even in relevant documents, the presence of noise information is common (Jiang et al., 2023b;a; Xu et al., 2024). So we cannot directly use the information from retrieved documents; further processing is"}, {"title": "3.1 FINETUNING HELPS BUT NOT PERFECTLY", "content": "Clearly, noise in the documents negatively impacts the performance of LLMs. Some researchers focus on fine-tuning the model to better filter out irrelevant documents, thereby enhancing perfor-mance (Yoran et al., 2023; Zhang et al., 2024; Jiang et al., 2023b). However, filtering out noise may require additional reasoning layers, which could hinder the reasoning ability of RALM. There-fore, we aim to explore whether we can incorporate the filtering process into the reasoning process, specifically addressing the question: can we filter out irrelevant information while conducting the original inference?\nLet r represents the relevance of tokens, ri = 0 means that token \u00e6i is a noise token, otherwise the token is relevant. Let attn(xi, xj) = (Wqxi)TWkxj represent the original attention layer of the LLM. And we assume that the desired self-attention function is:\nattn(xi, xj) =  {(Wqxi)TWkxj if rjelse,\n0,\n(2)\nThe desired attention pattern should effectively exclude noise while preserving the original attention of relevant tokens. We assume that, in the presence of relevant tokens, the original model represents"}, {"title": "4 EXTRA LYAERS FOR FILTERING", "content": "Clearly, noise in the documents adversely affects the performance of large language models (LLMs), and filtering out this information without compromising the performance ability of the retrieval-augmented language model (RALM) is challenging. Additional layers are necessary for effectively filtering out irrelevant information. If we cannot resolve the filtering issue within a limited number of layers, the overall reasoning capability of the RALM may fall below that of standard reason-ing models, resulting in retrieved documents failing to enhance the reasoning abilities of the LLM. Therefore, the crucial question is: how many layers are required to effectively address the filter-ing problem?"}, {"title": "4.1 THE TRIPLE-WISE PROBLEM", "content": "For an input sequence X = [x,x1,...,x\u22121], the vector r indicates the relevance of each token. Specifically, for each token xi, a relevance score of ri = 0 signifies that the token is irrelevant to the query. It's crucial to note that calculating ri does not solely depend on the token embedding xi and the query; rather, it may require the involvement of three or more tokens. For instance, in the query \"Alice is exhausted, but Bob is still very excited, showing no signs of fatigue. How does Bob feel?\u201d, the word \u201cexhausted\u201d acts as noise and should be excluded during inference. However, determining relevance necessitates considering \u201cBob\u201d in the query alongside \"Alice\", the subject of \"exhausted\". Therefore, identifying the relevance of a token demands information from multiple tokens, yet self-attention computes relationships only between pairs, making it challenging to address this issue within a single transformer layer.\nConsider a simple scenario in which three tokens are sufficient to determine the relevance of a given token. The task is to ascertain whether there exists information indicating that token xi is noise relative to the query. Let g(xi, xa, xb) be the function designed to identify whether tokens \u00e6a and xb suggest that token \u00e6\u00bf is a relevant token, if g(xi, xa, xb) = 0, then xi is a noise token, otherwise it is relevant. Thus, the problem can be framed as follows:\nri = {0 if \u2203a, b s.t. g(xi, xa, xb) = 0,\n1 else.\nAnd we need a transformer based model M to calculate r = M(X), and we can show that one layer of transformer can hardly complete the task.\nTheorem 4.1. For input documents of length n, and a constant c if mpH < c. nH(w)/log log n, then there is no one layer transformer M with embedding size m, precision p and H heads to judge the relevance of tokens.\nConsequently, a single layer of multi-head attention struggles to assess the relevance of a token. As noted in Conjecture 19 of Sanford et al. (2024), even multiple layers of multi-head attention may not effectively resolve this issue. The challenge arises from the triple-wise nature of the problem contrasted with the pair-wise nature of attention; the model can only evaluate a token's relevance when its embedding contains substantial information. For example, to assess the token \u201cfatigue\u201d in the sentence \"Alice is exhausted, but Bob is still very excited, showing no signs of fatigue. How does Bob feel?", "Bob": "as well as con-textual details from phrases like", "showing\". Therefore, a significant amount of information must be incorporated into the embedding before any judgement can be made. However, a single layer of self-attention can only consider the input \u2211jai,jxj, which contains information up to mp, where m represents the embedding dimension and p indicates precision. The term mp signifies the maximal entropy of the embedding, which means the maximal information the embed-ding can carry. This suggests that multiple attention layers may be necessary to encompass all relevant information effectively.\nFurthermore, this method renders Assumption 2.3 unrealistic, as assessing relevance requires a deep understanding of the document, thereby increasing the number of layers necessary for effective in-formation extraction. If we assume that t layers are needed to evaluate relevance, then extracting information about node ui,j would require \u03bbl + t layers. This could potentially exceed l, and extracting information from documents may necessitate a greater reasoning depth than standard rea-soning. Consequently, nodes located below layer cannot access document information because their reasoning would be finished faster than information extraction of documents. Then the reason-ing depth cannot be easily reduced, as it primarily occurs in the lower layers. Therefore, leveraging document information may not enhance reasoning ability; in fact, it could potentially degrade perfor-mance if the model becomes overly reliant on document information rather than employing vanilla reasoning.\"\n    },\n    {\n      \"title\": \"4.2 SIMPLICIFY THE PROBLEM\",\n      \"content\": \"Therefore, reducing the number of layers required for filtering is essential to enhancing the reasoning ability of retrieval-augmented language models (RALMs). In the retrieval-augmented generation\"\n    },\n    {\n      \"title\": \"5 DPROMPT TUNING\",\n      \"content\": \"As stated in the previous section, effectively addressing this problem requires the use of virtual tokens at the beginning of the prompt to represent the document's information. To accomplish this, we propose training an additional model to extract information from the document and utilize the embeddings to encapsulate this information. We then append these virtual tokens to the front of the prompt before conducting inference.\nIt is important to note that the model used for information extraction does not impose any additional inference costs on the LLM, because the calculations are solely based on the document, This allows us to precalculate the embeddings and directly retrieve the information while retrieving documents, ensuring that there is no extra cost during inference.\nSpecifically, we fine-tune a BERT-base-uncased model with an additional multi-layer perceptron (MLP) to project the embeddings to the appropriate dimensions. Given an input prompt, we identify the relevant documents, encode them as virtual tokens, and then inject these tokens at the front of the prompt.\nAdditionally, we perform inference with the query both before and after the documents to optimize performance. And we put the virtual tokens represents the documents before the query for all tokens to effectively take advantage of the virtual token. We also employ LoRA fine-tuning methods to enhance the model's ability to leverage the information in the virtual document tokens.\"\n    },\n    {\n      \"title\": \"6 CONCLUSION\",\n      \"content\": \"In this paper, we highlight that while RAG can enhance the reasoning capabilities of LLMs, it often only expands a limited number of reasoning depth unless we directly obtain information leading to the answer. However, noise within the documents can negatively affect performance, and processing these documents demands a certain depth of reasoning, we further demonstrate that filtering irrele-vant information cannot be achieved within a few transformer layers due to its intrinsic triple-wise nature. To address this, we propose a method called DPrompt tuning which allows for effective resolution within few layers of the transformer.\"\n    },\n    {\n      \"title\": \"A EXPRESSIVE POWER OF RAG\",\n      \"content\": null\n    },\n    {\n      \"title\": \"A.1 PROOF OF THEOREM 2.4\",\n      \"content\": \"q1,\nAssume there is a tree of L layers and each layer has ni nodes, the top of the tree got only one node.\nThere are connections between the layer and its upper layer, the probability of connection is 1\nif node uzi is not connected to any node, than we randomly sample a node and set the connection,\nthe probability of no connection is obviously qr\u0131+1\nThe connection can be regarded as reasoning paths, and RAG can ignore the reasoning path, directly\ngive the answer corresponding to node uji, uli stands for the ith node of the lth layer. Assume that\nnode ui,i is retrived by RAG, then all the downstream nodes connect to and only to node ui,i can\nalso be erased.\nAssume in layer l + 1, r\u0131+1 nodes are erased, then for layer l, let n = n1+1, the probability of nodes\nbeing erased is,\npe = (1 \u2212 ql+1)ql+1n\u2212rl+1n + ql+1nrl+1n,\n(4)\nmeans it connects to at least one of the ri+1 nodes, and qr\u2212r\u0131+1 means that it is not\nconnected to other nodes. So the first item (1\u2212ql+1)ql+1n\u2212rl+1n means that when the node is connected\nto at least one of the ri+1 nodes, and not connected to other nodes. And the second item qr 1+1\nmeans that it is not connected to any of nodes, so the connection is randomly set, and the setted node\nis erased.\nAlso, RAG can retrieve some information about layer l, Assume that with probability p\u03b9, RAG\nretrieves information about node ui,i, then, the percentage of node erased by converge and RAG are\nti, n stands for the number of nodes in layer l + 1\np = pe + p\u03b9\u00b7 (1 \u2013 pe)\n((1 \u2212 ql+1)ql+1n\u2212rl+1n + ql+1nrl+1n) + \u03c1\u03b9 \u00b7 1\u2212 ((1 \u2212 ql+1)ql+1n\u2212rl+1n + ql+1nrl+1n)\n(1\u2212qt+1)ql+1n-ri+1 + q+1+1\n= q (q\u012b+1 \u2212 1 + \u22121) + \u03c1\u03b9 - \u03c1\u03b9\u03b1 (q+1 \u22121+  -1+\n = (\u03b1\u0390 \u2013 \u03c1\u03b9\u03b1) (q+1 \u2212 1 + +1) + \u03c1\u03b9\"\n    },\n    {\n      \"title\": \"A.2 PROOF OF THEOREM 2.5\",\n      \"content\": \"To assess whether t converges to t, we conducted a\nsimulation experiment with 10 repetitions, where p\u03b9 ~\nN(0.8 \u2013 0.061, 0.01), q\u03b9 ~ N(0.8 \u2013 0.061, 0.01), and\n\u03b9 ~ \u039d(16 \u2013 1.4l, 0.01), where l represents the layer\nindex. As depicted in figure 4, it is observed that t\nis quite close to the corresponding t, the critical point\nwhere the fission reaction stops, therefore, with high\nprobability t\u2248 t. Also we can notice that the vari-\nance in higher layers are quite large, this means that\nwith probability that we happen to retrieve some infor-\nmation quite close to the answer, then we can erase\nthe higher layers, but in normal cases, we can only\nreduce the lower layers. Also, the critical point de-\ncreases when the layer index increases, showing that\nwe can replace more nodes in the lower levels. But\ndue to the large number of nodes, replacing the whole\nlayer is also a tricky problem.\nAssumption A.2. The probability of erasing in layer l satisfies t\u2081 \u2248 z\u0131, z\u0131 is the first zero point of\ng(t).\nFor simplicity, we use z instead of z\u0131, and g(z) = 0,\n(qn \u2212 pqn) (q-nz \u2212 1 + z) + p \u2212 z = 0\n\u2013 (q\u00afnz \u2212 1 + z) q\"p + p + qn-nz \u2013 q\" + q\"z \u2212 z = 0\n(qn \u2013 qn-nz \u2013 qnz + 1) p + qn-nz \u2013 q\" + qnz \u2212 z = 0\n-\n(qn \u2013 qn-nz \u2013 qnz + 1) p = q\" \u2013 qn-nz \u2013 qnz + z\np = qn - qn-nz - qz + zqnqn-nz - qnz + 1\"\n    },\n    {\n      \"title\": \"B THE IMPACT OF NOISE\",\n      \"content\": null\n    },\n    {\n      \"title\": \"B.1 PROOF OF THEOREM 3.1\",\n      \"content\": \"Assumption B.1. The model conducts filtering before inference, and those information regarded\nas noise would not be used during inference. When there are 8 percent of relevant tokens and\n1 \u2013 8 percent of noise information are included in the inference, let X and Xn represents the\"\n    },\n    {\n      \"title\": \"C EXTRA LAYERS FOR FILTERING\",\n      \"content\": null\n    },\n    {\n      \"title\": \"C.1 PROOF OF THEOREM 4.1\",\n      \"content\": \"Fact C.1 (Set disjointness communication lower bound (Yao, 1979)). Suppose Alice and Bob are\ngiven inputs a, b \u2208 {0,1}n, respectively, with the goal of jointly computing DISJ(a, b) = maxi aibi\nby alternately sending a single bit message to the other party over a sequence of communication\nrounds. Any deterministic protocol for computing DISJ(a, b) requires at least n rounds of commu-\nnication.\n10 if \u2203a, b s.t. g(xi, xa, xb) = 0\nri ={\nelse\nIn normal cases, judging the value of ri requires calculating g(xi, xa, xb) for all a \u2208 [0, na) and b \u2208\n[nd, nd+ng). Here we simplify the question, and we consider the situation where g(xi, Xa, xb) = 0\nonly if b = a + nd and ng = nd. Apparently, this is a special case of the original problem, and if\none layer of self-attention fail to solve this, it is impossible for it to solve the original problem.\nIf we assume that the input is like,\nxi \u2208 if i = 0,\n[Xi\u2208 {0, Xa) if i \u2208 {1, ..., na \u2014 1},10, x} if i \u2208 {nd, ..., 2 \u00b7 na \u2014 1}.\n(17)\nGiven input (a,b) \u2208 {0,1}nd \u00d7 {0,1}nd, let xi = xa if and only if a\u2081 = 1 and let xi xb if and\nonly if bi-na = 1. In this way r\u2081 = 0 if and only if DISJ(a, b) = 1.\nFor simplicity, we use n = nd. Now consider a more complex situation where Alice and Bob each\nhold a matrix A\u2208 Rn\u00d7d, B\u2208 Rn\u00d7d. each row of the matrix contains w, so d = H(w). and we\nassume that x = [s, w].\nThen\nXi if i = 0,\n{Xi = [s, a\u017c] if i \u2208 {1, . . ., na \u2013 1},[s, bi] if i \u2208 {nd,..., 2nd 1}.\n(18)\nLet DISJ1(A, B) = maxi(g'(x, ai, bi)), where the function g' acts similar with g, but it takes a, b\nas input, and \u00e6 is a fixed constant as xi. So the calculation of DISJ1 requires n \u00d7 H(w) bits of\ncommunication.\nAlso similar to the setting of x, ri = 1 if and only if DISJ1(A, B) = 1.\"\n    },\n    {\n      \"title\": \"D EXPERIMENTS\",\n      \"content\": \"We conduct experiments using the Natural Questions (NQ) dataset (Kwiatkowski et al., 2019), which\nis a large-scale collection of real-world queries derived from Google search data. Each entry con-\nsists of a user query and the corresponding Wikipedia page that contains the answer. We utilize\nContriever to extract documents, treating those that do not contain the answer as distracting doc-\numents (Cuconasu et al., 2024). Due to the limited computational resource, we only sample 1000\nqueries to test the performance.\nwe use different task instruction when query is ahead or after the documents.\n\u2014 query ahead document: \u201cYou are given a question and you MUST respond with a short\nanswer (max 5 tokens) based on the provided documents. If none of the documents contain\nthe answer and you do not know the answer, please respond with NO-RES.": "n\u2014 query after document: You are given a question and you MUST respond with a short answer\n(max 5 tokens) based on the provided documents. If none of the documents contain the\nanswer and you do not know the answer, please respond with NO-RES. The question will\nbe presented both before and after the documents.\nAlso, in the query after document situation, we actually also put the query after the documents to\nmake LLM remember what the question is."}, {"title": "D.1 RAG BRINGS MORE SPARSITY", "content": "When we implement RAG, it may struggle to reduce many layers, but it effectively helps to decrease\nnumerous nodes, introducing some sparsity to LLMs. In Figure 1, Document 1 contains information\nabout 42,2, which can aid in reducing the calculations for nodes 41,2, U1,3, and so on. This allows\nfor the pruning of certain parameters used to compute these nodes, resulting in a sparser LLM when\nutilizing RAG.\nTo verify this, we conduct experiments on the NQ dataset, employing the Wanda pruning method\n(Sun et al., 2023). Specifically, the pruning parameters are assessed based on their score, which is\ndefined as\nsi,j = Wi,j | \u00b7 || xj ||2,\nwhere Wij represents the absolute value of the parameter, while X, denotes the input. The score\nis evaluated as the product of its magnitude and the norm of the corresponding input activations,\nwhich directly indicates the importance of each parameter for the inference.\nWe utilize the query along with one relevant document that directly contains information related to\nthe query as input. As expected, with increasing sparsity, conducting inference using only the query\nleads to worse performance. However, inference with RAG maintains similar performance until the\nmodel becomes too sparse to retrieve information from the documents. Also as sparsity increases,\nthe fine-tuned model gradually outperforms the vanilla model with RAG because the fine-tuned\nversion can stop calculating certain nodes and focus on other computations with limited parameters.\nWhile using RAG in LLMs can enhance performance, it does not fully leverage the sparse nature of\nRALM. Consequently, as sparsity increases, the fine-tuned RAG surpasses the regular RAG."}, {"title": "D.2 EXPERIMENTS ON QUERY DOCUMENT ORDER", "content": "In Table 3, we show the performance when the prompt is in regular order, which means that we put\nthe documents ahead of the query. We can observe that although DPrompt tuning performs best,"}, {"title": "E RELATED WORK", "content": "Many recent works have explored better RAG strategies. Shi et al. (2023b) treat the model as a\nblack box and design a retriever to enhance performance. However, Shi et al. (2023a) found that\ndistracting documents in the retrieved set significantly weaken performance, leading some to address\nthis by fine-tuning the LLM (Yoran et al., 2023; Zhang et al., 2024). Additionally, researchers have\nidentified that noise in the prompt can negatively impact performance, prompting efforts to eliminate\nthis noise and compress prompts (Jiang et al., 2023b;a; Pan et al., 2024). Xu et al. (2024) first\nhighlighted the duality of RAG, encompassing both benefits and detriments. Understanding how\nthese factors influence the reasoning capabilities of LLMs remains an open question.\nMerrill & Sabharwal (2023) proves that the expressive power of LLM is constrained to TC\u00ba, but\nFeng et al. (2024); Li et al. (2024) shows that by Chain of Thought prompting, we can infinitely stack\ntransformer layers to solve any DP problems, Zeng & Lee (2023) further prove the rank needed to\nadapt the model for other takss by LoRA, and Petrov et al. (2024) shows that prompt tuning can be a\nuniversal approximator with enough prompt length, while prompt tuning fail to change the attention\npattern and can only bias the output (Petrov et al., 2023). RAG is also one kind of prompting\ntechnique, and it also try to involve more information when conducting inference like CoT. But how\ndoes RAG helps to improve the expressive power of LLM has not been studied before."}]}