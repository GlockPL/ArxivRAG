{"title": "Do generative video models learn physical principles from watching videos?", "authors": ["Saman Motameda", "Laura Culpb", "Kevin Swerskyb", "Priyank Jainib", "Robert Geirhosb"], "abstract": "Al video generation is undergoing a revolution, with quality and realism advancing rapidly. These advances have led to a passionate scientific debate: Do video models learn \"world models\" that discover laws of physics-or, alternatively, are they merely sophisticated pixel predictors that achieve visual realism without understanding the physical principles of reality? We address this question by developing Physics-IQ, a comprehensive benchmark dataset that can only be solved by acquiring a deep understanding of various physical principles, like fluid dynamics, optics, solid mechanics, magnetism and thermodynamics. We find that across a range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical understanding is severely limited, and unrelated to visual realism. At the same time, some test cases can already be successfully solved. This indicates that acquiring certain physical principles from observation alone may be possible, but significant challenges remain. While we expect rapid advances ahead, our work demonstrates that visual realism does not imply physical understanding. Our project page is at Physics-IQ-website; code at Physics-IQ-benchmark.", "sections": [{"title": "Introduction", "content": "Can a machine truly understand the world without interacting with it? This question lies at the heart of the ongoing debate surrounding the capabilities of AI video generation models. While the generation of realistic videos has, for a long time, been considered one of the major unsolved challenges within deep learning, this recently changed. Within a relatively short period of time, the field has seen the development of impressive video generation models (1-3), capturing the imagination of the public and researchers alike. A major milestone towards general-purpose artificial intelligence is to build machines that understand the world, and if you cannot understand what you cannot create (as Feynman would say), then the ability of those models to create visually realistic scenes is an essential step towards that capability. However, the degree to which successful generation signals successful understanding is the subject of a passionate debate. Is it possible to understand the world without ever interacting with it? Phrased differently, do generative video models learn the physical principles that underpin reality from \"watching\" videos?\nProponents argue that the way the models are trained predicting how videos continue, a.k.a. next frame prediction- is a task that forces models to understand physical principles. According to this line of argument, it is impossible to predict the next frame of a sequence if the model has no understanding of how objects move (trajectories), that things fall down instead of up (gravity), and how pouring juice into a glass of water changes its color (fluid dynamics). As an analogy, large language models are trained in a similar fashion to predict the next tokens (characters or words) in a text; a task formulation that is equally simple but has proven sufficient to enable impressive capabilities and text understanding. Moreover, predicting the future is a core principle of biological perception, too: The brain constantly generates predictions about incoming sensory input, enabling energy-efficient processing of information (4) and building a mental model of the world as postulated by von Helmholtz (5) and later the predictive coding hypothesis (6). In short, successful prediction signals successful understanding.\nOn the other hand, there are also important arguments contra understanding through observation. According to the causality rationale, \"watching\" videos (or to be more precise, training models to predict how videos continue) is a passive process, with models unable to interact with the world. This lack of interaction means that a model cannot observe the causal effects of an intervention (as, for instance, children are able to when playing with toys).\nTherefore, a model is faced with the nearly impossible task of distinguishing correlation from causation if it is to succeed in understanding physical principles.\nFurthermore, video models that are touted as \"a promising path towards building general purpose simulators of the physical world\" (1) arguably experience a different world to begin with: the digital world as opposed to the real world that an embodied system (like a robot, or virtually all living beings) experience. As a consequence, skeptics argue that visual realism by no means signals true understanding: All it takes to produce realistic videos is to reproduce common patterns from the model's vast sea of training data shortcuts without understanding (7, 8).\nIn light of these two diametrically opposed perspectives, how can we tell whether generative video models indeed learn physical principles? To address this question in a quantifiable, tractable way, we created a challenging testbed for physical understanding in video models: the \"Physics-IQ\" benchmark. The core idea is to enlist video models to do what they do best: predict the continuation of a video. In order to test understanding, we designed a range of diverse scenarios where predicting the continuation requires a deep understanding of physical principles, going beyond pattern reproduction and testing out-of-distribution generalization. For instance, models are asked to predict how a domino chain falls normally, vs. when a rubber duck is placed in the middle of the chain; or how pillows react when a kettlebell vs. a piece of paper is dropped onto the pillow. The diverse"}, {"title": "Physics-IQ benchmark", "content": "Dataset. Our goal is to develop a dataset that tests the physical understanding capabilities of video generative models on different physical laws like solid mechanics, fluid dynamics, optics, thermodynamics, and magnetism. We therefore created the Physics-IQ dataset which consists of 396 videos each 8 seconds long covering 66 different physical scenarios. Each scenario in our dataset focuses on a specific physical law and aims to test a video generative model's understanding of physical events. These events include examples like collisions, object continuity, occlusion, object permanence, fluid dynamics, chain reactions, trajectories under the influence of forces (e.g., gravity), material properties and reactions, as well as lights, shadows, reflections, and magnetism.\nEach scenario was filmed at 30 frames per second (FPS) with a resolution of 3840 x 2160 (16:9 aspect ratio) from three different perspectives: left, center, and right using high-quality Sony Alpha a6400 cameras equipped with 16-50mm lenses. Each scenario was shot twice (take 1 and take 2) under identical conditions to capture the inherent variability of real-world physical interactions. These variations are expected in real-world due to factors like chaotic motion, subtle changes in friction, and variations in force trajectory. In this paper, we refer to the differences observed between these two recordings of the same scenario as physical variance. This results in a total of 396 videos (66 scenarios \u00d7 3 perspectives \u00d7 2 takes). All our videos are shot from a static camera perspective without camera motion. The setup for filming the videos is illustrated in fig. 8. The full dataset and code for evaluating model predictions is open-sourced here:\nEvaluation protocol. Physical understanding can be measured in different ways. One of the most stringent tests is whether a"}, {"title": "Models", "content": "We evaluate eight different video generative models on our benchmark: VideoPoet (both i2v and multiframe) (13), Lumiere (i2v and multiframe) (11), Runway Gen 3 (i2v)"}, {"title": "Metrics for physical understanding", "content": "Video generative models commonly use metrics for evaluating the visual quality and realism of the generated videos. These metrics include Peak Signal-to-Noise Ratio (PSNR) (41), Structural Similarity Index Measure (SSIM) (42), Fr\u00e9chet Video Distance (FVD) (43), and Learned Perceptual Image Patch Similarity (LPIPS) (44). These metrics are useful for comparing the appearance, temporal smoothness, and statistics of generated videos with the ground truth. Unfortunately, these metrics are not equipped to assess the understanding of physical laws by video models. For instance, both PSNR and SSIM evaluate pixel-level similarities but are not sensitive to the correctness of motion and interactions in a video; FVD captures overall feature distributions but does not penalize a model for physically implausible actions and LPIPS focuses on human-like perception of similarity rather than physical plausibility. While these metrics are great for measuring what they were designed for, they are not equipped to judge whether a model understands real-world physics.\nIn our benchmark, we use the following four metrics to track different aspects of physical understanding:\n\u2022 Where does action happen? Spatial IoU\n\u2022 Where & when does action happen? Spatiotemporal IoU\n\u2022 Where & how much action happens? Weighted spatial IoU\n\u2022 How does action happen? MSE\nThese four metrics-explained in detail below are then combined into a single score, the Physics-IQ score, by summing the individual scores (with a negative sign for MSE where lower better). This Physics-IQ score is normalized such that physical variance the upper limit of what we can reasonably expect a model to capture is at 100%.\nThe location of movement is an important indicator of physical \"correctness\". For instance, in the \"domino with duck interrupting the chain\" scenario from Figure 1, only the part of the chain that is to the right side of the duck should tumble, while the other part should remain unchanged. Similarly, the spatial trajectory of a moving ball is indicative of whether the movement is realistic. Our Spatial IoU metric compares generated videos against ground truth to determine whether the location of movement/action mirrors ground truth. Since the benchmark videos are filmed from a static perspective without camera movement, a simple threshold on pixel intensity changes across frames (see Algorithm 2 for pseudocode) easily identifies where movement happens. This leads to a binary $h \\times w \\times t$ \"motion mask video\" that highlights the regions of motion in a scene at any point in time. Spatial IoU then simply generates a binary $h \\times w$ spatial \"motion map\"-similar, in spirit, to a saliency map-by collapsing the masks across the time dimension with a max operation. A motion map thus simply has a 1 whenever action occurred, at any point in time, at a particular location; and a 0 otherwise. This motion map is compared against the motion map from the real, ground truth video, using Intersection over Union or IoU (a metric commonly used in object detection to measure overlap while penalizing areas that differ):\n$Spatial-IoU = \\frac{M_{binary, spatial}^{real} \\cap M_{binary, spatial}^{gen}}{M_{binary, spatial}^{real} \\cup M_{binary, spatial}^{gen}}$\nwhere $M_{spatial}^{real}$ and $M_{spatial}^{gen}$ are the motion maps based on real and generated videos, respectively. Spatial IoU measures whether the location where an action happens is correct.\nSpatiotemporal IoU goes a step further than Spatial IoU by also taking into account when an action occurs. Instead of collapsing across time as Spatial IoU does, Spatiotemporal IoU compares the two motion mask videos (based on real and generated videos) frame-by-frame, averaging across t:\n$Spatiotemporal-IoU(M_{real}, M_{gen}) = \\frac{M_{real} \\cap M_{gen}}{M_{real} \\cup M_{gen}}$\nwhere $M_{real}$ and $M_{gen}$ are the h\u00d7w\u00d7t binary motion masks for the real and generated videos, respectively. Spatiotemporal IoU thus tracks not only where an action occurs in a video, but also whether it occurs at the right time (when). If a model does well on Spatial IoU but poorly on Spatiotemporal IoU, this would therefore indicate that the model gets the location of the action right, but the timing wrong.\nWeighted spatial IoU is similar to Spatial IoU in the sense that it compares two h \u00d7 w \"motion maps\". However, instead of comparing binary motion maps (action occurred or did not occur), it also assesses how much action happens at any given location. This distinguishes between e.g. motion caused by a pendulum (showing repeated motion in an area) from motion by a rolling ball (which passes a location only once). Weighted spatial IoU is computed by taking the binary hx wxt motion mask video (described above in the section on Spatial IoU) and collapsing across the time dimension t in a weighted fashion (instead of taking the maximum). The weighting simply averages per-frame action. This weighted h \u00d7 w spatial \"motion map\" is then used to compute the metric by summing the pixel-wise minimum of two motion maps and dividing by the pixel-wise maximum:\n$Weighted-spatial-IoU = \\frac{\\sum_{i=1}^{n} min (M_{weighted, spatial}^{real,i} , M_{weighted, spatial}^{gen, i})}{\\sum_{i=1}^{n} max (M_{weighted, spatial}^{real,i}, M_{weighted, spatial}^{gen, i})}$\nwhere $M_{weighted, spatial}^{real}$ and $M_{weighted, spatial}^{gen}$ are the weighted motion maps representing how much activity/action happes at any location (based on real and generated videos, respectively). Weighted spatial IoU thus measures not only where an action occurs, but also how much action is happening.\nFinally, mean squared error (MSE) calculates the average squared difference between corresponding pixel values in two frames (e.g., a real and a generated frame). Given two frames $f_{real}$ and $f_{gen}$, the MSE is given by:\n$MSE(f_{real}, f_{gen}) = \\frac{1}{n} \\sum_{i=1}^{n} (f_{real,i} \u2013 f_{gen,i})^2$\nwhere n is the total number of pixels in the frame. MSE focuses on pixel-level fidelity; this is a very strict requirement that is sensitive to how objects look and interact. For instance, if a"}, {"title": "Results", "content": "Physical understanding. The goal of our Physics-IQ benchmark is to understand, and quantify, whether generative video models learn physical principles. Therefore, we test all eight models in our study on every scenario and for each camera position (left, center, right) in the benchmark dataset. These samples are visualized in Figure 1. We first report the aggregated Physics-IQ results across all metrics related to physical understanding (Spatial-IoU, Spatiotemporal-IoU, Weighted-spatial-IoU, MSE) in Figure 4. The main takeaway from the left part of this figure is that all the models show a massive gap to the physical variance baseline, with the best model scoring only 24.1% out of the possible 100.0%. As we mentioned in the previous section, each scenario was recorded twice (take 1 and take 2) to estimate the natural variability in real-world physical phenomena. This estimate is termed the physical variance; the figure is normalized such that pairs of real videos that differ only by physical randomness score 100.0%. The gap between model performance and real videos demonstrates a severe lack of physical understanding in current powerful video generative models. Across the different models, VideoPoet (multiframe) (13) ranks best; interestingly, VideoPoet is a causal model. For the two models that have both an image2video (i2v) and a version conditioned on multiple frames (multiframe), the multiframe variants do better than the i2v variants. This is to be expected given that in order to predict the future as we require models to do on our challenging Physics-IQ benchmark, having access to temporal information (as multiframe variants have) should help.\nZooming in on these overall results, Figure 6 breaks down model performance into different categories that include solid mechanics, fluid dynamics, optics, thermodynamics, and magnetism. While there is no category that can be considered \"solved\", performance varies across categories, with some showing promising indications and differences across models. Interestingly, all models perform much better on Spatial-loU, a metric that has the weakest requirement in the sense that it is only sensitive to where an action occurred, not whether it occurred at the right time (as Spatiotemporal-IoU would track) or whether it had just the right amount of action"}, {"title": "Visual realism: Multimodal large language model evaluation", "content": "Our results indicate a striking lack of physical understanding in current generative video models. Why, then, do samples from many of those models that are circulated online look so realistic? We decided to quantify the visual realism of model-generated videos by asking a capable multimodal large language model, Gemini 1.5 Pro (45), to identify the generated one out of a pair of two videos for each scenario in the Physics-IQ benchmark). The result of this experiment is presented in Figure 5 (left). The MLLM score evaluates a model's ability to generate videos that can deceive a multimodal large language"}, {"title": "Discussion", "content": "We introduced Physics-IQ, a challenging and comprehensive real-world benchmark to evaluate physical understanding in video generative models. We analyzed eight models on Physics-IQ and proposed metrics to quantify physical understanding."}, {"title": "Do video models learn physical principles?", "content": "We investigated the question whether the ability of video generative models to generate realistic-looking videos also implies that they have acquired an understanding of the physical principles that govern reality. Our benchmark shows that this is not the case: all evaluated models currently lack a deep understanding of physics. Even the highest-scoring model, VideoPoet (multiframe), only achieves a score of 24.1, falling significantly short of the best possible score of 100.0 obtained from the physical variance baseline, which quantifies the natural variability observed between real-world videos. That said, our results don't imply that future models cannot learn a better physical understanding through next frame prediction. It remains an open question whether simply scaling the same paradigm further might solve this, or whether alternative (and possibly more interactive) training schemes might be required. Given the success of scaling deep learning, we are optimistic that future-frame prediction alone could lead to a much better understanding: while successful prediction does not guarantee successful understanding, acquiring a better understanding should certainly help with successful prediction. Learning physics by predicting pixels may sound challenging, but language models are known to learn syntax and grammar from text prediction alone (46).\nIt may be worth pointing out that even though the models in our study often failed to generate physically plausible continuations, most current models were already successful on some scenarios. For example, the highest-ranking model, VideoPoet (multiframe), displayed remarkable physical understanding in certain scenarios such as accurately simulating paint smearing on glass. In contrast, many lower-ranking models exhibited fundamental errors, such as physically implausible interactions (e.g., objects passing through other objects). Separate studies (e.g. 39) based on synthetic datasets have shown that given a large enough dataset size, video models are able to learn specific physical laws. We consider it likely that as models are trained on larger and more diverse corpora of videos, their understanding of real-world physics will continue to improve. We hope that quantifying physical understanding, as we aim to do by open-sourcing the physics-IQ benchmark, might facilitate tracking progress in this area."}, {"title": "Visual realism doesn't imply physical understanding", "content": "We observed a striking discrepancy between visual realism and physical understanding: those two properties are not statistically significantly correlated (Figure 5), thus models that produce the most realistic-looking videos don't necessarily show the most physically-plausible continuations. For instance, in a scenario where a burning matchstick is lowered into a glass full of water (leading to the flame being extinguished), Runway Gen 3 generates a continuation where as soon as the flame touches the water, a candle spontaneously appears and is lit by the match. Every single frame of the video is high quality in terms of resolution and realism, but the temporal sequence is physically impossible. Such a tendency to hallucinate objects into existence is a drawback of many generative models and an active area of research in deep learning (47). In our"}, {"title": "Dataset biases are reflected in model generations", "content": "We observed that most models were consistent in producing videos that aligned with the scene and viewpoint provided. Models like Sora and Runway Gen 3 were particularly strong at understanding the given scene and generating subsequent frames that were consistent with respect to object placement and their attributes (shape, color, dynamics). Interestingly, many models also demonstrated biased generations depending on properties of their training. For example, in prototyping experiments we observed that when given a conditioning video of a red pool table where one ball hits another, as Lumiere starts generating, it immediately turned the red pool table to a green one, showing a bias to commonly occurring green pool tables. Similarly, videos generated by Sora often featured transition cuts, possibly suggesting a training paradigm optimized to generate artistic videos."}, {"title": "Metrics and their limitations", "content": "Popular metrics to test quality and measure realism of generated videos include PSNR (41), FVD (43), LPIPS (44) and SSIM (42). However, designing metrics that quantify physical understanding in generated videos is a challenging endeavor. We proposed a suite of metrics to measure the spatial, temporal and perceptual coherence of video models. While individually none of these metrics is perfect, the combined insights from these metrics and the Physics-IQ score that integrates a normalized score across these metrics provide a holistic assessment of the strengths and weaknesses of video models. That said, none of these metrics directly quantify any physical phenomena and instead serve as proxies. For instance, the MLLM metric provided a way to quantify how much generated videos 'deceive' a multimodal model. However, the metric is limited by the capability of the underlying multimodal large language model (MLLM) itself. In our analysis, we found that while the MLLM was frequently able to identify generated videos (except for videos generated by Sora), its explanations for the decision were often wrong. As another example, we observed that Stable Video Diffusion often generated videos with large amounts of hallucinations and implausible object motions; nonetheless, it was the best performing model on Weighted-spatial-IoU showing that no metric should be assessed in isolation. This is also evidenced by the fact that e.g. Runway Gen 3 did very well in terms of the spatial location of actions (Spatial-loU) while scoring poorly on temporal consistency (Spatiotemporal-IoU).\nWe intentionally designed Physics-IQ to be a challenging benchmark in order to provide useful signal for model development in the future; in this context it may be worth noting that our metrics may be on the conservative side by strongly penalizing object hallucinations, camera movement (which we prompted models not to do) or shot changes. For instance, Sora tends to show these more frequently than other models, leading to low scores on some metrics. This is not ideal, but we believe that in an area like deep learning where hype is omnipresent, scientific benchmarks should rather err on the side of caution."}, {"title": "Outlook", "content": "The obvious trends are better models trained on bigger datasets. Will these models essentially solve physical understanding or instead, hit a limit after which one can only improve one's understanding the world by interacting with it? The jury is still out on this question, but the benchmark and analyses introduced in this article might help quantifying this either way. In addition to future models, improvements could also come from inference-time compute, such as scaling the number of samples. If this would lead to strong results, it would raise the following question: from a model's perspective, is reality but one possibility among infinitely many others?"}]}