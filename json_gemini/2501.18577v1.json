{"title": "Prediction-Powered Inference with Imputed Covariates and Nonuniform Sampling", "authors": ["Dan M. Kluger", "Kerri Lu", "Tijana Zrnic", "Sherrie Wang", "Stephen Bates"], "abstract": "Machine learning models are increasingly used to produce predictions that serve as input data in subsequent statistical analyses. For example, computer vision predictions of economic and environmental indicators based on satellite imagery are used in down-stream regressions; similarly, language models are widely used to approximate human ratings and opinions in social science research. However, failure to properly account for errors in the machine learning predictions renders standard statistical procedures invalid. Prior work uses what we call the Predict-Then-Debias estimator to give valid confidence intervals when machine learning algorithms impute missing variables, as-suming a small complete sample from the population of interest. We expand the scope by introducing bootstrap confidence intervals that apply when the complete data is a nonuniform (i.e., weighted, stratified, or clustered) sample and to settings where an ar-bitrary subset of features is imputed. Importantly, the method can be applied to many settings without requiring additional calculations. We prove that these confidence in-tervals are valid under no assumptions on the quality of the machine learning model and are no wider than the intervals obtained by methods that do not use machine learning predictions.", "sections": [{"title": "1 Introduction", "content": "With increasingly rich data collection and a surge in open science initiatives, investigators now frequently rely on machine learning algorithms to predict quantities of interest based on large collections of related but indirect measurements. For example, in remote sensing studies, land cover is predicted from satellite images with computer vision algorithms. Similarly, millions of protein structures have been predicted from their amino acid sequences, but such structures are rarely measured directly because it is costly and difficult. This situation is pervasive\u2014investigators are now often faced with large data sets that partially consist of machine learning outputs. However, using error-prone predictions from machine learning models as input data for statistical analysis (e.g., calculating regression coefficients) can lead to considerable biases and misleading conclusions. This has led to a flurry of recent works that develop methods to resolve the conundrum of how to use machine-learning predictions in a statistical analysis. In particular, when the analyst has access to data in which the predictions and the ground truth measurements are jointly available, it is possible to correct the bias of the machine learning model without assumptions on the quality of the predic-tions. In such settings, we develop methods to construct confidence intervals that account for errors in the machine learning predictions."}, {"title": "1.1 Problem setup", "content": "We first introduce our setting and two simple baselines. We assume a distribution over data points X = (Xobs, Xmiss) \u2208 R\u00ba. The features Xobs are always observed, but measuring Xmiss is costly (e.g., it requires expert human labeling). As such, few complete data points are available. In addition, we also have access to a much larger data set of machine-learning predictions of Xmiss, which we denote by Xmiss. The reader should interpret Xmiss as a reason-ably accurate but imperfect proxy of Xmiss. Formally, we have access to a small complete (\u2764) sample (Xobs, Xmiss, Xmiss)i\u2208I\u2764 and a much larger incomplete (0) sample (Xobs, Xmiss)i\u2208I\u00ba, adding up to N data points total. Here, I\u2764 and I\u00b0 are disjoint sets of indices such that I\u2764 UI\u00b0 = {1, ..., N}. We denote by n the size of the complete sample, n = |I\u2764|. Our goal is to estimate some quantity \u03b8 \u2208 Rd describing the distribution of X. As our primary example in this work, we may wish to estimate the coefficients in a generalized linear model (GLM), obtained by regressing one component of X on the remaining compo-nents. Note that the population regression coefficient vector for a GLM is still a well-defined estimand of interest even when a GLM does not accurately describe the distribution of X. Typically an investigator will have access to a function (implemented in software) A(\u00b7) which takes a sample of data as input and returns an estimate of \u03b8 as an output. They can, therefore, readily deploy two natural and simple approaches to estimating \u03b8: 1. The naive approach: Act as if Xmiss = Xmiss and apply A(.) to all available samples imputed with machine learning predictions: \u03b8all = A((Xobs, Xmiss)i=1N). 2. The classical approach: Ignore the machine learning predictions and apply A(\u00b7) to the small complete sample only: \u03b8 = A((Xobs, Xmiss)i\u2208I\u2764). Both approaches have both advantages and limitations. In particular, \u03b8 targets the correct parameter \u03b8, but it does not leverage the potential power of machine-learning predictions. Meanwhile, the naive approach uses the abundant machine-learning predictions, but it tar-gets the wrong parameter; even with infinite data, \u03b8all will be biased, unless the proxy Xmiss comes from the exact same distribution as Xmiss. Consequently, confidence intervals based on the naive approach are invalid. We observe in our real-data experiments (Section 4) that the naive estimator is sometimes biased by a factor of 1/3 or more. Given the limitations of the classical and naive approaches and the growing prevalence of machine-learning-imputed datasets, there is a growing interest in developing methods that leverage all available data, aiming for the best of both worlds. In this paper, we build on an"}, {"title": "1.2 A flexible solution: Predict-Then-Debias (PTD) estimator", "content": "The PTD estimator takes the naive estimator based on the incomplete sample\u2014which is biased and then adds a bias correction term based on the complete sample. In particular, let \u03b8\u2070 = A((Xobs, Xmiss)i\u2208I\u00ba), \u03b8\u2764 = A((Xobs, Xmiss)i\u2208I\u2764), and \u03b8all = A((Xobs, Xmiss)i=1N); the first estimator is similar to the previously defined naive estimator and the second estimator is the classical estimator. The basic Predict-Then-Debias estimator is then defined as\n\u03b8PTD = \u03b8\u2070 + (\u03b8\u2764 \u2212 \u03b8all)."}, {"title": "1.3 Our contribution", "content": "We develop bootstrap-based approaches to construct confidence intervals for the PTD estimator, generalizing to new settings and providing precise technical conditions under"}, {"title": "1.4 Related work", "content": "Because the literature on methods for using proxies in a statistical analysis is vast, we restrict our attention to discussing methods that use a small complete subsample, some-times called internal validation data, where all variables of interest, including the proxies, are jointly measured. This has historically been investigated in the literature on measure-ment error (Carroll et al., 2006), missing data (Little and Rubin, 2019), and survey sam-pling (S\u00e4rndal et al., 2003).\nMeasurement error and missing data: Measurement error techniques such as regres-sion calibration and SIMEX (e.g., Carroll et al., 2006) are generally designed for cases where internal validation data is unavailable, so they rely on assumptions about the prediction er-ror. Such methods are sometimes used even when there is an internal validation sample (Spiegelman et al., 2001; Fong and Tyler, 2021), but they still rely critically upon the non-differential measurement error assumption, unlike the present work. Multiple imputation methods (e.g., Little and Rubin, 2019) are often used when internal validation data is avail-able (Carroll et al., 2006; Guo and Little, 2011; Proctor et al., 2023). Multiple imputation does not require the nondifferential measurement error assumption, but it instead requires Bayesian modelling assumptions about the distribution of the missing data (in our case the ground truth values) given the observed data (in our case the predictions and other widely available variables). As a result, confidence intervals for parameters of interest given by multiple imputation-based approaches do not have frequentist guarantees. This is a critical limitation in practice\u2014data-based experiments in Proctor et al. (2023) show that multiple imputation confidence intervals with nominal level 95% have coverage of less than 80% in some cases.\nSemiparametric and semisupervised inference: Approaches from the semiparamet-rics literature (Robins et al., 1994; Tsiatis, 2006), give a broad class of estimators for the setting of internal validation with asymptotically valid confidence intervals. As discussed in Chen and Chen (2000), the PTD estimator is a special case of the semiparametric estima-tor from Robins et al. (1994), and therefore the semiparametric estimator, with an optimal estimate of the nuisance function, can be more efficient than the PTD estimator. A recent body of work on semi-supervised inference considers methods for leveraging a large unla-beled dataset and a small labeled dataset and focuses on efficiency in high-dimensional and semiparametric regimes for specific types of estimators. In particular, recent works studied efficient estimation of means (Zhang et al., 2019; Zhang and Bradic, 2021; Zhang et al., 2023), linear regression parameters (Chakrabortty and Cai, 2018), quantiles (Chakrabortty et al., 2022), and quantile and average treatment effect estimates (Chakrabortty and Dai, 2022). While their efficiency is appealing, the complexity of semiparametric methods (which require estimating a nuisance function) makes them less accessible to users who do not have extensive statistical training.\nLoss-debiasing approaches: More recently, prediction-powered inference (PPI) (An-gelopoulos et al., 2023a), and other works that build upon or extend it (Angelopoulos et al.,"}, {"title": "2 Properties of the PTD estimator", "content": "We formalize the key properties of the tuned PTD estimator \u03b8PTD,\u03a9. Moreover, we present a couple of strategic ways to choose the tuning matrix \u03a9 and discuss the efficiency of the optimally tuned \u03b8PTD,\u03a9 relative to the classical estimator \u03b8, the PPI++ estimator"}, {"title": "2.1 Formal setting and assumptions", "content": "We start by introducing the formal setting, notation, and some assumptions. Let X = (X(1), ..., X(p)) \u2208 R\u00ba be a random vector drawn from a distribution Px denoting the actual data and X = (X(1),..., X(p)) \u2208 R\u00ba be a random vector drawn from a distribution P denoting the machine-learning-imputed proxy for X. The reader should think of the case where only a subset of the components of X are imputed with predictions-X = (Xobs, Xmiss) and X = (Xobs, Xmiss). The investigator has N proxy samples (Xi)i=1N and for a subset I C {1,..., N} of these samples the corresponding vector of actual data, Xi, is also available. The goal is to use the small number of samples of (X, X) and the larger number of samples of X to estimate some parameter \u03b8 = \u03c6(Px) \u2208 Rd, where \u03c6 is a function that maps probability distributions to a summary statistic of interest. Note that e can be any quantity describing the joint distribution of the entries of X. For example, e can be the population mean or quantile of a component of X, the population correlation between two components of X, the population regression coefficient in a generalized linear model (GLM) relating one component of X to other components of X. We denote by \u03b3 = \u03c6(P\u0303x) \u2208 Rd the corresponding parameter of the joint distribution of the proxy X. We now introduce assumptions about the sampling of I that are analogous to the missing-at-random and positivity assumptions commonly seen in the missing data and sample survey literatures, respectively. Let I\u00bf \u2208 {0,1} be the indicator taking on the value 1 if X is observed and 0 otherwise. We assume the following:\nAssumption 1 (Sampling and missingness assumption).\n(i) IID assumption: (Ii, Xi, X\u0303i)i=1N  iid P;\n(ii) Missing at random assumption: I\u22a5X|X;\n(iii) Known sampling probability: \u03c0(X) = P(I = 1|X) is known;\n(iv) Overlap assumption: For some a, b \u2208 (0,1), a \u2264 \u03c0(X) \u2264 b almost surely.\nFor two-phase labeling designs, where an investigator first starts with (X\u0303i)i=1N and subse-quently chooses the subset of samples where Xi is measured (i.e where Ii = 1), the investi-gator can easily ensure by design that parts (ii)-(iv) of Assumption 1 hold. Note that when X = (Xobs, Xmiss) and X = (Xobs, Xmiss), this means that \u03c0(X) is allowed to depend on the observable features Xobs.\nWe consider settings where \u03b8 and \u03b3 can be estimated using standard statistical software and an appropriate choice of weights. Specifically, let X \u2208 RN\u00d7p be the data matrix of the proxies whose i'th row is Xi and let X \u2208 RN\u00d7p be the data matrix of the actual data whose i'th row is Xi (which is observed only when Ii = 1). Further, define the weights\nWi = Ii/\u03c0(X\u0303i) and W\u0303i = (1 \u2212 Ii)/(1 \u2212 \u03c0(X\u0303i)),\ntypically used for inverse probability weighted estimators, and let W\u2022 = (W1,...,WN) \u2208 RN and W\u0303\u2022 = (W\u03031,...,W\u0303N) \u2208 RN. Let A : RN\u00d7p \u00d7 RN \u2192 Rd be a function that takes"}, {"title": "2.2 Asymptotic normality of the PTD estimator", "content": "We now show that the tuned PTD estimator can be well approximated by a normal distribution under certain regularity conditions. We begin with an assumption that holds for a large class of estimator functions A(\u00b7;\u00b7).\nAssumption 2 (Asymptotic weighted linearity of \u03b8, \u03b3, and \u03b3\u2070). There exist functions \u03a8 : RP \u2192 Rd and \u03a8\u0303 : RP \u2192 Rd such that each component of the random vector (\u03a8(X), \u03a8\u0303(X)) has mean 0 and finite variance, and such that, as N \u2192 \u221e,\n(i) N\u2211i=1NW\u0303i\u03a8(Xi) 0,\n(ii) \u221a(\u03b3 \u2212 \u03b3\u2070 \u2212 N\u2211i=1NW\u0303i\u03a8\u0303(Xi)) 0,\n(iii) \u221a(\u03b3 \u2212 \u03b3\u2070 \u2212 N\u2211i=1NW\u0303i\u03a8\u0303(Xi)) 0.\nWhile Assumption 2 is stated in an abstract form, it holds for a broad class of common statistical estimators and there is often a mathematical formula that can be used to derive the functions \u03a8 and \u03a8\u0303. For example, under certain regularity conditions, \u03a8(\u00b7) and \u03a8\u0303(\u00b7) can be found by calculating the influence function of \u03c6 with respect to the distributions Px and P\u0303x, respectively (Hampel, 1974; van der Vaart, 1998). Assumption 2 also holds for M-estimators under fairly mild regularity conditions. M-estimators include a broad class of estimators of interest such as sample means, sample quantiles, linear and logistic regression coefficients, and regression coefficients in quantile regression or robust regression, among others. See Appendix E.1, particularly Proposition E.1 and Assumptions 5 and 6, for further details justifying Assumption 2 for M-estimation."}, {"title": "2.3 Optimal tuning matrix", "content": "Ideally, the tuning matrix \u03a9 is chosen to minimize the (asymptotic) variance of each component of \u03b8PTD,\u03a9. In Equation (2), note that for each j \u2208 {1, ...,d}, [\u03a3PTD(\u03a9)]jj only depends on the jth row of \u03a9, which we denote by \u03a9j\u2022 \u2208 Rd. Further, [\u03a3PTD(\u03a9)]jj is a quadratic function in \u03a9j\u2022 that is minimized when \u03a9j\u2022 = (\u03a3\u2022 + \u03a3\u0303\u2022)\u22121[\u03a3\u0303\u2070,\u03b3]T ej, where ej is the jth canonical vector. Hence, setting \u03a9 = \u03a9opt, where\n\u03a9opt = \u03a3\u0303\u2070,\u03b3(\u03a3\u2022 + \u03a3\u0303\u2022)\u22121,\nsimultaneously minimizes each diagonal entry of \u03a3PTD(\u03a9). If one uses the tuning matrix\n\u03a9(diag) = diag([\u03a3\u0303\u2070,\u03b3(\u03a3\u2022 + \u03a3\u0303\u2022)\u22121]11,...,[\u03a3\u0303\u2070,\u03b3(\u03a3\u2022 + \u03a3\u0303\u2022)\u22121]dd),\nand \u03a3\u03030,\u03b3\u03a3\u2022\u03a3\u0303\u2022, \u03a3\u0303\u2022\u03a3\u2022\u03a3\u0303;, and \u03a3\u0303 \u03a3\u03b3\u03a3\u0303;, then \u03a9opt\u03a9(diag)opt and, by Proposition 2.1, we have a tuned PTD estimator \u03b8PTD,\u03a9(diag) opt with minimal asymptotic variance.\nBecause \u03a9opt has d\u00b2 tuning parameters, estimating \u03b8PTD,\u03a9opt can be unstable in small sample sizes. To address this, we also consider the optimal tuning matrix among the class of diagonal tuning matrices, reducing the number of tuning parameters from d\u00b2 to d. Note that, by minimizing d univariate quadratic equations, the asymptotic variance in Equation (2) is minimized across all diagonal choices of \u03a9 when letting \u03a9 = \u03a9(diag)opt, where \u03a9(diag)opt is the diagonal matrix with\n[\u03a9(diag)opt]jj = [\u03a3\u03030,\u03b3]j[\u03a3\u2022 + \u03a3\u0303\u2022]jj/[\u03a3\u0303\u03b3]jj.\nTherefore, selecting \u03a9 such that \u03a9 \u03a9(diag)opt minimizes the asymptotic variance of \u03b8PTD,\u03a9 among all possible diagonal choices of \u03a9."}, {"title": "2.4 Efficiency of the tuned PTD estimator", "content": "We next state that \u03b8PTD,\u03a9(diag) opt is more efficient than the classical estimator \u03b8: for each coordinate j, the asymptotic variance of [\u03b8PTD,\u03a9(diag) opt]j is smaller than that of \u03b8. This result has been established in similar settings (e.g., Chen and Chen (2000); Gronsbell et al. (2024))."}, {"title": "3 Bootstrap confidence intervals", "content": "We now develop bootstrap algorithms that construct confidence intervals based on \u03b8PTD,\u03a9. Algorithm 2 generalizes Algorithm 1 to non-uniformly weighted settings. We then introduce a computational speedup in Algorithm 3. We prove that both algorithms provide asymptot-ically valid confidence intervals under suitable assumptions. Finally, we discuss generaliza-tions of these bootstrap approaches to clustered and stratified sampling settings.\nThe bootstrap approaches presented in this section are more flexible than CLT-based approaches for constructing confidence intervals in the sense that they do not require the mathematical calculation of the asymptotic variance terms. Nonetheless, for completeness, we present a CLT-based approach in Appendix A."}, {"title": "3.1 Main bootstrap algorithm and its validity", "content": "We first introduce the necessary notation. Let Vi = (Wi, W\u0303i, Xi, X\u0303i) for each i. Further, let PN = N\u2211i=1N \u03b4Vi be the empirical distribution of Vi from the N samples, where \u03b4v assigns a point mass of 1 at v and 0 elsewhere. Next, define a single bootstrap draw of \u03b8\u2022, \u03b3\u2022, \u03b3\u2070,\u2217, and \u03b8PTD,\u03a9,\u2217 to be the version of that quantity with a starred superscript in the following procedure:\n1. Draw V\u22171,..., V\u2217N iid PN and set (W\u2217i, W\u0303\u2217i, X\u2217i, X\u0303\u2217i) = V \u2217i for each i \u2208 {1, ...,N}.\n2. Set W\u2022,\u2217 = (W\u22171, ..., W\u2217N), W\u0303\u2022,\u2217 = (W\u0303\u22171,..., W\u0303\u2217N) and X\u2217, X\u0303\u2217 \u2208 RN\u00d7p such that the i'th rows of X\u2217 and X\u0303\u2217 are X\u2217i and X\u0303\u2217i, respectively.\n3. Evaluate \u03b8\u2022,\u2217 = A(X\u2217; W\u2022,\u2217), \u03b3\u2217,\u2217 = A(X\u2217; W\u0303\u2022,\u2217), and \u03b3\u2070,\u2217 = A(X\u2217; W\u0303\u2070,\u2217).\n4. Set \u03b8PTD,\u03a9,\u2217 = \u03b3\u2070,\u2217 + (\u03b8\u2022,\u2217 \u2212 \u03a9\u03b3\u2217,\u2217).\nWith this in hand, Algorithm 2 computes confidence intervals at level a for each component of \u03b8PTD,\u03a9 by first taking B independent draws from the bootstrap distribution and returning the a/2 and 1 - a/2 empirical quantiles of each coordinate."}, {"title": "3.2 A faster bootstrap procedure", "content": "Algorithm 2 can be slow if the incomplete sample is large because it requires computing \u03b3\u2070\u2217 for B different bootstrap draws, which requires evaluating A on a large data set with each draw. (For the percentile bootstrap it is often recommended to choose B = 2,000 or larger (e.g., Little and Rubin (2019)).) Next, we propose a convolution-based speed-up (Algorithm 3) that replaces the computation of \u03b3\u2070,\u2217 with a Gaussian approximation. We show that this is valid when \u03b3\u2070 = A(X;W\u0303\u2022) is asymptotically Gaussian and when a consistent estimator of its asymptotic variance is readily available. This convolution-based speed-up exploits the fact that \u03b3 is asymptotically uncorrelated with \u03b8 and \u03b3."}, {"title": "3.3 Subroutines for estimating optimal tuning matrix", "content": "In this subsection, we present subroutines for Algorithms 2 and 3 to compute the tuning matrix \u03a9. The subroutines are designed to have the same computational complexity as the corresponding algorithm. The subroutines can easily be modified such that \u03a9 estimates the optimal d \u00d7 d tuning matrix rather than the optimal diagonal tuning matrix by modifying the last line to return \u03a9 = \u03a3\u0303\u2070,\u03b3(\u03a3\u2022 + \u03a3\u0303\u2022)\u22121; see Equation (4)."}, {"title": "3.4 Cluster and stratified bootstraps", "content": "In many applications of interest, it is more economical to measure X for entire clusters of samples (e.g., all samples in a geographical unit) and forgo measuring X entirely on the remaining clusters. Such cases will violate Assumption 1 and can render the confidence in-tervals from Algorithms 2 and 3 too narrow. These algorithms can readily be extended using a cluster bootstrap scheme to appropriately construct confidence intervals in such settings. The cluster bootstrap modification involves resampling entire clusters with replacement as opposed to resampling individual samples with replacement\u2014see Section B.1 and Algorithm 5 for details.\nAnother common setting is stratified sampling, which can often reduce the number of samples needed. In the stratified sampling that we consider, the population is partitioned into strata and a fixed number of incomplete samples and complete samples are drawn from each strata. In Appendix B.2 we present a modification of Algorithm 2 (Algorithm 6) that constructs confidence intervals for the PTD estimator that account for the stratified sampling scheme. We caution readers that Algorithm 6 is only designed to work in regimes where there is a small number of large strata and instead point readers to Section 6.2.4 of Shao and Tu (1995) for variants of the bootstrap for stratified samples that are designed to work in other regimes. Theoretical justifications of Algorithms 5 and 6 are out of scope for the current work, but we refer the reader to Davison and Hinkley (1997); Shao and Tu (1995) and references therein for details on these methods and Cheng et al. (2013) for a theoretical justification of the cluster bootstrap."}, {"title": "4 Experiments", "content": "In this section, we present a variety of experiments using four different real datasets to validate our method and compare it to the classical approach (i.e., only using the gold-standard data from the complete sample). We also consider a number of variations of the PTD approach that involve different algorithms for constructing confidence intervals (see Section 4.4) and different tuning matrix choices (see Section 4.5).\nOur experiments focus on regression tasks such as linear regression, logistic regression, and quantile regression, and some of them involve weighted, stratified, or clustered labelling schemes. Let Y and Z be the subvectors of X corresponding to the response variable and covariate vector, respectively, and define Y\u0303 and Z\u0303 as the analogous subvectors of X\u0303. Our regression experiments fall into 3 main categories:\n1. Error-in-response regressions: In an error-in-response regression, the investigator has access to a large incomplete sample with measurements of Y\u0303 and Z\u0303 and can obtain access to a much smaller complete sample with measurements of Y, Y\u0303 and Z\u0303. \n2. Error-in-covariate regressions: In an error-in-covariate regression, the investigator has access to a large incomplete sample with measurements of Y\u0303 and Z\u0303 and can obtain access to a much smaller complete sample with measurements of Y\u0303, Z, and Z\u0303.\n3. Error-in-both regressions: In an error-in-both regression, the investigator has ac-cess to a large incomplete sample with measurements of Y\u0303 and Z\u0303 and can obtain access to a much smaller complete sample with measurements of Y, Y\u0303, Z, and Z\u0303. Recent works on prediction-powered inference (Angelopoulos et al., 2023a,c; Miao and Lu, 2024; Zrnic, 2024) only test their methods for error-in-response regressions, so we focus on error-in-covariate and error-in-both regressions.\nThe experiments conducted are summarized in Table 1. We describe the experimental setup and datasets in more detail the following subsections."}, {"title": "4.1 Experimental setup", "content": "For each experiment we use the following validation procedure. We start with M samples where X and X\u0303 are jointly measured, where X = (Y\u0303, Z\u0303) and X\u0303 = (Y, Z). We calculate the \u201cground truth\" regression coefficients \u03b8 by regressing Y on Z using all M samples (blue lines in Figure 2) and the \u201cnaive\u201d regression coefficients \u03b8all by regressing Y\u0303 on Z\u0303 using all M samples (red lines in Figure 2). Even if the M samples are drawn from a superpopulation, we can treat \u03b8 estimated from the M samples as our estimand of interest because all simulations are based on these M empirical samples. We then conduct 500 simulations in which we\n1. Randomly select a subsample of the M samples of size N. A subset of the N samples are randomly assigned to the complete sample, where all variables are observed. For all samples not assigned to the complete sample, the Xmiss values are withheld.\n2. Compute the classical estimator of \u03b8 and the corresponding 90% confidence interval using only data from the complete sample. Confidence intervals are calculated using the sandwich estimator for Var(\u03b8).\n3. Run various PTD-based approaches using (X, X\u0303) from the complete sample and X\u0303 from the remaining samples to estimate \u03b8 and a corresponding 90% confidence interval. In particular, we consider a number of choices of tuning matrices and Algorithms 2-4, to construct 90% confidence intervals for \u03b8PTD,\u03a9. In some cases, we forgo Algorithm 4 or both Algorithms 3 and 4 because of lack of implemented analytic expressions. Unless otherwise specified, step 1 is done by uniform random sampling without replacement; however, in some of our experiments we use a weighted, stratified, or cluster sampling scheme.\nFinally, we calculate the coverage for each method by calculating the percentage of the 500 simulations in which the 90% confidence intervals contained \u03b8 (estimated from regressing Y on Z using all M samples)."}, {"title": "4.2 Datasets", "content": "We briefly describe the datasets used and experiments conducted. Further details are presented in Appendix G.\nAlphaFold Experiments: We used a dataset of M = 10,802 samples that originated from Bludau et al. (2022) and was downloaded from Zenodo (Angelopoulos et al., 2023b). Each sample had indicators ZAcet, ZUbiq \u2208 {0,1} of whether there was acetylation and ubi-quatination, and an indicator YIDR \u2208 {0,1} of whether the protein region was an internally disordered region (IDR) coupled with a prediction of YIDR based on AlphaFold (Jumper et al., 2021). We test Algorithms 2\u20134 when the estimands are the regression coefficients for a logisitic regression of YIDR on (ZAcet, ZUbiq, ZAcet \u00d7 ZUbiq). YIDR was withheld on all sam-ples outside from a randomly selected complete sample. The complete sample was selected according to a weighted sampling scheme such that for each of the 4 possible combinations of ZAcet and Zubiq, there were 250 complete samples in expectation.\nHousing Price Experiments: We used a dataset of gold-standard measurements and remote sensing-based estimates of economic and environmental variables from Rolf et al. (2021a,b) that was used in Proctor et al. (2023) to study multiple imputation methods. Each of the M = 46,418 samples corresponded to a distinct ~ 1km \u00d7 1km grid cell, and included grid cell-level averages of housing price, income, nightlight intensity, and road length. We considered settings where the estimand was the regression coefficient of housing price on income, nightlight intensity, and road length, and where outside of a small complete sample, gold-standard measurements of nightlights and road length were unavailable (but proxies based on daytime satellite imagery were available for all samples). In Experiment 2 the estimands were regression coefficients from a linear regression, and the estimands in Experiment 3 were regression coefficients for a quantile regression with q = 0.5.\nTree Cover Experiments: We used a dataset of M = 67,968 samples of ~ 1km\u00d71km grid cells taken from the previously mentioned data source (Rolf et al., 2021a,b). The variables included the percent of tree cover and grid cell-level averages of population and elevation. We considered settings where the estimands were the regression coefficients of tree cover on elevation and population, and where outside of a small complete sample, gold-standard measurements of tree cover and population were unavailable (but satellite-based proxies were available for all samples). In Experiments 4 and 5 the estimands were regression coefficients from a linear regression, while in Experiment 6 the estimands were regression coefficients from a logistic regression where the tree cover was binarized according to a meaningful threshold from a forestry perspective (Oswalt et al., 2019). In Experiment 5, the data and the complete samples were sampled via cluster sampling (where each cluster corresponded to a 0.5\u00b0\u00d7 0.5\u00b0 grid cell), and the cluster bootstrap method (Algorithm 5) was tested.\nCensus Experiments: We used a dataset of income, age, and disability status of M = 200,227 individuals from California taken from the 2019 US Census survey and downloaded via the Folktables interface (Ding et al., 2021). We considered a setting where the estimands are the regression coefficients of income on age and disability status, and where disability status was only measured on a small complete sample. Outside the complete sample we used predictions of disability status based on a machine learning model that we trained using the previous year's census data. In Experiment 7 the incomplete sample and complete sample were taken according to stratified random sampling (with K = 4 strata based on age buckets), and we tested Algorithm 6."}, {"title": "4.3 Point estimates and confidence interval size", "content": "For each experiment and regression coefficient, Figure 2 gives a violin plot across the 500 simulations of the point estimates and widths of the 90% confidence intervals for the classical approach and for the PTD approach. In this figure, we only present the results for the PTD approach when the tuning matrix \u03a9 is chosen to estimate the asymptotically optimal diagonal tuning matrix given in Equation (5) and when confidence intervals are calculated using the fully bootstrap approach (e.g., Algorithm 2). Figure 2 shows that in a fair number of cases the naive estimator has substantial bias, and that consistent with cautionary notes in the"}, {"title": "4.4 Comparing different confidence interval methods", "content": "In Figure 3, we present how the confidence interval widths and empirical coverage varied with the algorithm used to construct confidence intervals for \u03b8PTD,\u03a9. With the exception of the AlphaFold and the quantile regression experiments, in which mild overcoverage was observed, the confidence intervals for [\u03b8PTD,\u03a9]j had empirical coverages (across 500 simulations) that were close to the target coverage of 0.9. The overcoverage in the AlphaFold experiment can be explained by the fact that each simulation involved sampling 7,500 points without replacement from a dataset with 10,802 points. Had we instead sampled with replacement or used a larger dataset, our superpopulation inference approach would not substantially overestimate the simulation scheme-specific variance of \u03b8\u2070.\nThe 3 different approaches for constructing confidence intervals \u03b8PTD,\u03a9 yielded similar empirical coverages and confidence interval widths across the 7 experiments. Therefore, we recommend that investigators mainly consider runtime and ease-of-implementation when choosing between constructing CLT-based, convolution bootstrap-based, and fully bootstrap approaches to constructing confidence intervals for \u03b8PTD,\u03a9. Of these 3 approaches, CLT-based approaches are hardest to implement and generalize (requiring asymptotic variance calcu-lations or an existing software implementation) but have the lowest runtime. On the other end of the spectrum, fully bootstrap approaches can be implemented in a few lines of code and require no asymptotic variance calculations but have the longest runtime. The convo-lution bootstrap approach is a compromise that leverages existing software that calculates asymptotic variance approximations and has intermediate runtime."}, {"title": "4.5 Comparing different tuning matrix choices", "content": "The results presented in Figures 2 and 3 all use a tuning matrix \u03a9 that estimates the optimal diagonal tuning matrix \u03a9(diag)opt given in Equation (5). We next present additional results when using the tuning matrix \u03a9opt given in Equation (4) (which estimates the optimal tuning matrix among all d \u00d7 d matrices) and also when using the untuned PTD estimator (which has \u03a9 = Id\u00d7d). In Figure 4, for each regression coefficient, experiment, and tuning matrix choice we show the average and standard deviation of the 90% confidence interval widths across the 500 simulations. For ease of comparison across coefficients and experiments, the confidence interval widths are only presented for full percentile bootstrap approaches (e.g., Algorithms 2, 5, and 6) and are normalized by the average confidence interval width of"}, {"title": "C.1 Proof of Proposition 2.1", "content": "Fix \u03a9 such that \u03a9\u0303 \u03a9. Letting A\u03a9 = [Id\u00d7d \u2212\u03a9\u0303 \u03a9] and A\u0303\u03a9 = [Id\u00d7d \u2212 \u03a9\u0303 \u03a9], observe that A\u0303\u03a9 = A\u03a9 + op(1). Note that because \u03be = (\u03b8\u2070, \u03b3, \u03b3\u2070), \u03b6 = (\u03b8, \u03b3, \u03b3), and \u03b8PTD,\u03a9 = \u03b3\u2070 + (\u03b8\u2022 \u2212 \u03a9\u03b3\u0303), A\u0303\u03a9\u03be = ( \u03b8PTD,\u03a9) and A\u03a9\u03b6 = 0. Thus applying Lemma C.1 and Slutsky's lemma gives\nN(\u03b8PTD,\u03a9 \u2212 \u03b8) = A\u0303\u03a9(\u221a(\u03be \u2212 \u03b6)) = A\u03a9(\u221a(\u03be \u2212 \u03b6)) + op(1) =dist N(0, A\u03a9\u03a3\u2022AT\u03a9),\nwhere the second equality follows because \u221a(\u03be \u2212 \u03b6) = Op(1). By matrix multiplication and by Equation (7),\n= \u03a3\u2022 \u2212 \u03a3\u0303\u2070,\u03b3\u03a9T \u2212 \u03a9[\u03a3\u0303\u2070,\u03b3]T + \u03a9(\u03a3\u0303\u2070 + \u03a3\u0303\u2070)\u03a9T = \u03a3PTD(\u03a9).\nA\u03a9\u03a3\u2022AT\u03a9"}, {"title": "C.2 Proof of Proposition 2.2", "content": "First note that \u221a(\u03b8 \u2212 \u03b8) dist N(0, \u03a3\u2022), by considering the first d coordinates in the CLT from Lemma C.1. Now recall that by Formula (3), \u03a9opt = \u03a3\u0303\u2070,\u03b3(\u03a3\u2022 + \u03a3\u0303\u2022)\u22121 and by Formula (2), \u03a3PTD(\u03a9) = \u03a3\u2022 \u2212 \u03a3\u0303\u2070,\u03b3\u03a9T \u2212 \u03a9[\u03a3\u0303\u2070"}]}