{"title": "Model-Based Differentially Private Knowledge Transfer for Large Language Models", "authors": ["Zhaomin Wu", "Jizhou Guo", "Junyi Hou", "Bingsheng He", "Lixin Fan", "Qiang Yang"], "abstract": "As large language models (LLMs) become increasingly prevalent in web services, effectively leveraging domain-specific knowledge while ensuring privacy has become critical. Existing methods, such as retrieval-augmented generation (RAG) and differentially private data synthesis, often compromise either the utility of domain knowledge or the privacy of sensitive data, limiting their applicability in specialized domains. To address these challenges, we propose Llamdex, a novel framework that integrates privacy-preserving, domain-specific models into LLMs. Our approach significantly enhances the accuracy of domain-specific tasks, achieving up to a 26% improvement compared to existing methods under the same differential privacy constraints. Experimental results show that Llamdex not only improves the accuracy of LLM responses but also maintains comparable inference efficiency to the original LLM, highlighting its potential for real-world applications.", "sections": [{"title": "1 Introduction", "content": "The widespread adoption of large language models (LLMs) in web services has profoundly impacted various domains, yet their application in specialized domains - especially those handling sensitive data - faces significant hurdles. State-of-the-art LLMs, such as GPT-4 [29] and Gemini [41], are typically closed-source and owned by large companies (referred to as servers). These models, trained on extensive public datasets, frequently struggle to deliver accurate results in specialized areas like healthcare and finance, where precision is crucial. For example, a misdiagnosis in medical contexts can pose serious health risks, while erroneous financial forecasts can lead to substantial economic implications. The reluctance of servers to share their models due to commercial interests, combined with clients' inability to disclose sensitive data for privacy reasons, creates a significant barrier to developing LLMs that can effectively leverage domain-specific knowledge.\nExisting knowledge transfer methods have struggled to balance utility and privacy effectively. Approaches such as retrieval-augmented generation (RAG) [7, 49], transfer learning [9, 21], and parameter-efficient fine-tuning (PEFT) [10, 17, 18] offer promising utility but necessitate the direct sharing of domain-specific data, raising significant privacy concerns [47]. Privacy of domain-specific data is typically protected by sharing synthetic data [5, 30, 39] with differential privacy (DP) [4] mechanisms employed; however, the noise required to maintain differential privacy results in substantial utility loss, leading to a \"disparate impact\" [6] on the accuracy of models trained on synthetic data. Consequently, none of these methods effectively achieve knowledge transfer while ensuring strong differential privacy guarantees.\nTo address this issue, we propose a framework - Large language model with domain expert (Llamdex) - which utilizes differentially private models for domain knowledge transfer. These models, which can be considered summaries of data distributions, typically require much less noise to maintain the same level of differential privacy. The differentially private model is integrated into the intermediate layer of the LLM as a domain expert. The LLM learns to query the domain expert and leverages its outputs to generate responses based on the public schema of the domain data.\nAn overview of Llamdex's training and deployment process is illustrated in Figure 1 using the example of a financial inquiry. In this scenario, a bank, acting as the client, trains an expert model on"}, {"title": "2 Background", "content": "Transferring the expert model to the server could potentially be used to identify users from the private domain database. To mitigate this risk, we employ differential privacy to protect the privacy of the users. Differential privacy [4] is a rigorous framework for data privacy that provides strong probabilistic guarantees on the distinguishability of individuals in a dataset. It measures the privacy of a statistical query based on the divergence of its output distribution when a single data record is removed from the dataset. Formally,\nDEFINITION 2.1 (DIFFERENTIAL PRIVACY [4]). A randomized algorithm $M: D \\rightarrow O$ is said to be $(\\epsilon, \\delta)$-differentially private if, for any two datasets $D, D' \\in D$ that differ by a single record, and for all output sets $S \\subseteq O$,\n$Pr[M(D) \\in S] \\leq e^\\epsilon Pr[M(D') \\in S] + \\delta$.\n(1)\nThe parameter $\\epsilon$ represents the privacy budget, controlling the level of privacy; smaller values of $\\epsilon$ imply stronger privacy guarantees. The parameter $\\delta$ is a relaxation that allows the privacy guarantee to be violated with a small probability $\\delta$.\nDifferential privacy is typically achieved by introducing noise into the algorithm. One common method is the Gaussian mechanism, which adds Gaussian noise to the output to ensure differential privacy.\nTHEOREM 1 (GAUSSIAN MECHANISM [4]). Let $f$ be a function $f: D \\rightarrow \\mathbb{R}^d$, and assume the $L_2$-sensitivity of $f$ is defined as $\\Delta f := \\sup_{D \\approx D'} ||f(D) - f(D')||_2$, where $D$ and $D'$ are neighboring datasets differing by a single record. The Gaussian mechanism $M(D) = f(D) + Z$ is $(\\epsilon, \\delta)$-differentially private if $Z$ is a random variable drawn from a Gaussian distribution $N(0, \\sigma^2)$, where the standard deviation $\\sigma \\geq \\frac{\\Delta f \\sqrt{2 \\ln(1.25/\\delta)}}{\\epsilon}$.\nDifferential privacy can be utilized to generate synthetic data from private databases by adding noise, as demonstrated in methods like PATE-GAN [15]. Meanwhile, model parameters can also be protected in the context of differential privacy by adding noise to the gradients during each training step. The cumulative privacy loss over training is tracked using a technique known as the moments accountant [1]. A library called Opacus [45] supports differential privacy in deep learning and is used in our implementation."}, {"title": "3 Related Work", "content": "Existing approaches for domain knowledge transfer can be broadly classified into two categories: those prioritizing utility and those prioritizing privacy. Utility-focused methods, such as Retrieval-Augmented Generation (RAG) [16, 25, 46] and fine-tuning [10, 17,"}, {"title": "4 Problem Statement", "content": "Consider a collaboration between a server and a client. The server has a decoder-only large language model (LLM), denoted as $M$, which has been trained on a public text dataset $Z_p \\in A^{n \\times l}$, where $A$ represents the token set and $l$ is the sequence length. The client holds a private domain-specific dataset $X_d \\in \\mathbb{R}^{n \\times m}$. We assume that the client's data distribution $X_d$ is not fully dependent on the server's data distribution $X_p$, and the schema of the client's data $S_d$ is considered public. The primary goal in this scenario is for the server to modify $M$ to obtain a domain-aware model $M_d$, capable of generating accurate responses for the client's domain-specific data $X_d$ without direct access to $X_d$.\nThis study focuses on a model-based knowledge transfer approach. Specifically, the client trains an expert model, $\\mathcal{E}_d$, with parameters $\\theta_{\\mathcal{E}_d}$, on data $X_d$ to predict a target variable $y$. The training process utilizes the DPSGD optimizer [1], which ensures that the parameters $\\theta_{\\mathcal{E}_d}$ satisfy differential privacy [4], thus cannot be used to infer $X_d$. This is equivalent to adding Gaussian noise to the parameters. Formally, this can be expressed as:\n$\\theta_{\\mathcal{E}_d} = \\arg \\min_{\\theta_{\\mathcal{E}_d}} \\mathbb{E}_{(x,y) \\in X_d} [\\mathcal{L}(\\mathcal{E}_d(\\theta_{\\mathcal{E}_d}; x), y)] + N(0, \\sigma^2)$,\n(2)\nwhere $\\mathcal{L}$ denotes the task-specific loss function and $N(0, \\sigma^2)$ represents Gaussian noise with variance $\\sigma^2$.\nThe objective of this paper is to design an approach that integrates the expert model $\\mathcal{E}_d$ into the LLM to produce a domain-aware model, $M_d$, capable of generating accurate responses to domain-specific questions. These domain-specific questions, represented by $Z_d$, are assumed to be highly correlated with $X_d$. The training algorithm aims to minimize the following objective:\n$\\Theta_{M_d} = \\arg \\min_{\\Theta_{M_d}} \\mathbb{E}_{(z,y) \\in Z_d} [\\mathcal{L}(M_d(\\Theta_{M_d}; z, \\theta_{\\mathcal{E}_d}), y)]$.\n(3)\nAdditionally, we aim to keep the original LLM parameters, $\\Theta_M$, frozen during training, ensuring that the trainable parameters satisfy the condition $\\Theta_{M_d} \\cap \\Theta_M = \\emptyset$.\nSince tabular data is one of the most common formats in relational databases and can naturally be translated to and from text, with each feature being explainable in the schema, we focus on $X_d$ in tabular format in this paper. Extensions to other data formats, such as text, images, and videos, are discussed in Section 7 and are left for future work.\nThreat Model. Our primary focus in this paper is the privacy of the client's data, $X_d$. We assume that the server is semi-honest, meaning it will adhere to the protocol but may attempt to gather additional information. The server has access to the public dataset, $Z_p$, as well as the schema of the client's data, $S_d$. Furthermore, the server can access the expert model, $\\mathcal{E}_d$, and its parameters, $\\theta_{\\mathcal{E}_d}$. The primary risk is that the server may attempt to infer the client's data $X_d$ using the information it can access, particularly the expert model $\\mathcal{E}_d$."}, {"title": "5 Approach", "content": "In this section, we provide a detailed overview of our model design and algorithm. Section 5.1 describes the model architecture, followed by the training and inference processes in Section 5.2. The collaborative training process of Llamdex can be viewed as the transfer of domain knowledge from the client to the server. Two types of knowledge are shared: data schema knowledge and data distribution knowledge. The data schema is considered public information and can be directly shared with the server. However, the data distribution is sensitive and is shared via the expert model trained with differential privacy. The primary challenge in designing the training algorithm and model architecture is ensuring that the server can effectively utilize both the expert model and schema information to make accurate domain-specific predictions without accessing the real domain data."}, {"title": "5.1 Model Architecture", "content": "Overview. The Llamdex model integrates a domain expert into an intermediate layer of the LLM, as illustrated in Figure 2. The domain expert module receives question embeddings as input and generates corresponding answer embeddings based on domain knowledge, referred to as output embeddings. These output embeddings are appended to the question embeddings and passed to subsequent layers. Since many LLM architectures or their positional encodings (e.g. Mistral [14] with ROPE [40]) require a consistent sequence length across layers, Gaussian noise placeholders are used prior to the domain expert layer to maintain sequence length consistency with the expected output embeddings.\nExpert Encoder. The primary challenge of encoder design is that most modern LLMs (e.g., Mistral [14], GPT [29], Llama [43]) employ decoder-only structures that generate text auto-regressively - producing one word at a step based on prior context. These decoder-only LLMs are not suited for a single-step extraction of all required feature values from token embeddings. To address this, we utilize a pretrained encoder-only small language model (SLM) for feature extraction. Specifically, in our experiments, we employ a BERT-based model, ROBERTa [22]. The final $D_{in}$ hidden states from the last layer of ROBERTa are mapped into feature vectors using $D_{in}$ linear layers, respectively. To ensure stable training and avoid extreme feature values, all feature values are scaled to the range [0, 1] using a min-max scaler before training the expert model. The min-max scaler, derived from theoretical feature ranges (e.g., [0, 100] for age), and assumed to contain no sensitive information, is shared with the server and serves as the activation function for these linear layers.\nAnother challenge in using an SLM is the misalignment of token embeddings between the LLM and SLM, as they typically employ"}, {"title": "5.2 Training and Inference", "content": "The Llamdex training process has two stages: the client training stage and the server training stage. In client training stage, the client trains the expert model with differential privacy using the Opacus [45] library and transfers both the expert model and the data schema to the server. In server training stage, the server generates synthetic tabular data as well as synthetic questions based on the schema. The server then trains the expert encoder and decoder separately using the synthetic data and questions. Throughout the training, all the original LLM parameters remain frozen, with only the expert encoder and decoder being trained. Our experiments demonstrate that the expert encoder and decoder, trained on synthetic data, generalize well to real domain data. This is because the expert encoder and decoder learn how to extract features from text, rather than memorizing the underlying data distribution.\nTraining Expert Encoder. To address the absence of real data for training the expert encoder, we propose a training algorithm that uses server-generated synthetic data. First, synthetic tabular data is generated based on the client's schema using the sklearn library, with partial features marked as missing values. For each synthetic data row, a corresponding text-based question is generated using an"}, {"title": "6 Experiment", "content": "In this section, we present the experimental setup, which includes details on the datasets, the training process of expert models, the utilization of the LLM and SLM, the evaluation method, the baselines, and the environment used for the experiments.\nDataset. Our experiments incorporate four public real-world datasets: titanic [8], wine [3], bank [26], and nursery [34], with details provided in Table 2. For evaluation, all datasets are in tabular format with well-defined schema and meaningful column names. We split each dataset into training and test sets by an 8:2 ratio.\nExpert Model. The expert models are trained directly on the tabular datasets using PyTorch [31]. Each expert model is a multilayer perceptron (MLP) with two hidden layers consisting of 400 and 200 neurons, respectively. The ReLU [27] activation function is employed in the hidden layers, and the output layer is followed by a softmax function. Model training is performed using the AdamW optimizer [24], with a learning rate of $10^{-4}$ and a batch size of 64, over at most 30 epochs or until training loss converges. For models incorporating differential privacy, Opacus [45] library is used to add noise to the gradients during training, according to specified values of $\\epsilon$ and $\\delta$. The value of $\\delta$ is set to 1/n, following the convention in [4], where n represents the number of training instances.\nLLM and SLM. In our experiments, we employ the pretrained Mistral-7B [14], which contains 7 billion parameters, as the bottleneck LLM, and the pretrained Roberta-large [23], with 355 million parameters, as the SLM within the expert encoder. Under our Llamdex training paradigm, the LLM remains frozen throughout, while the SLM undergoes full-parameter fine-tuning. The expert"}, {"title": "6.2 Accuracy", "content": "In this subsection, we focus on the evaluation results without the addition of differential privacy noise. The accuracy of Llamdex and the baselines are presented in Table 3, with the highest accuracy marked in bold and the second highest underlined. We found that Llamdex significantly outperforms all baselines across every dataset. Notably, in the nursery dataset, Llamdex surpasses the second-best baseline, LoRA fine-tuning on real data, by 14%, despite not accessing the real domain data. This is because it is challenging for LLMs to learn the complex mapping from natural language questions to answers, whereas Llamdex is specifically trained to extract relevant features from natural language queries and utilize an accurate expert model to predict the answer. This demonstrates that Llamdex's synthetic data-based training paradigm generalizes well to domain-specific questions based on real data.\nEffect of Inserted Layer. We explore the accuracy of Llamdex when the domain expert is inserted at different layers of the LLM. The effect of the depth of the insertion layer is shown in Figure 5. From the figure, we observe that the best accuracy occurs when the domain expert is inserted in either the first few layers or the last few layers. The probable reason is that the initial and final layers are more closely aligned with natural language tokens, making the information easier to interpret, whereas the intermediate layers are more abstract and harder to map directly to natural language."}, {"title": "6.3 Privacy", "content": "In this subsection, we evaluate the privacy-utility trade-off by comparing the accuracy with differential privacy of various $\\epsilon$. The results are shown in Figure 4 and two key observations emerge. First, Llamdex continues to outperform the baselines even as more noise (smaller $\\epsilon$) is added. For example, Llamdex outperforms PATE-GAN by 26% when $\\epsilon$ = 2 on wine dataset. However, when excessive noise is introduced (e.g., $\\epsilon$ = 0.5 on the titanic dataset), Llamdex's accuracy slightly decreases. Second, we observe that PATE-GAN LORA provides accuracy closer to that of the original LLM on most datasets. This is because the synthetic data generated by PATE-GAN contains too much noise, limiting the useful information available to the LLM and leading to poorer performance. In summary, Llamdex strikes a balance between privacy and utility, maintaining high accuracy while ensuring a strong privacy guarantee."}, {"title": "6.4 Efficiency", "content": "In this subsection, we evaluate the efficiency of Llamdex and the baseline models in terms of both training time and memory consumption. The results are presented in Table 4 and Table 5.\nInference. The inference time and memory usage for Llamdex and the baselines are presented in Table 5. From the results, two key observations can be made: (1) Llamdex provides significantly faster inference than the Expert API, the only method with competitive accuracy to Llamdex. Specifically, Llamdex achieves an"}, {"title": "7 Discussion", "content": "Multi-Task Llamdex. In this paper, we successfully integrate a single expert model within an LLM. However, in real-world applications, more complex scenarios may arise where a single question requires the knowledge from multiple expert models. For instance, in the medical domain, a question about a patient's symptoms might necessitate inferencing various diagnostic models, such as a radiology model, a pathology model, and a clinical model. The primary challenge in extending Llamdex to support multiple tasks lies in token routing. Similar to Mixture of Experts (MoE) in LLMs, a gating module is required to determine which tokens should be routed to which expert model. To support multi-tasking, beyond the expert encoder and decoder design, Llamdex must also introduce a carefully designed gating module for efficient token routing. We leave this extension for future work.\nComplex Questions. In this paper, we focus on simple questions that can be answered directly by querying the expert model, where the model's output serves as the immediate answer. However, in real-world applications, questions are often more complex and may require multiple steps to reach an answer. For example, a question might first require inferring a diagnosis from a radiology image before using that diagnosis to formulate a response. Approaches like"}, {"title": "8 Conclusion", "content": "In this paper, we introduced Llamdex, a framework designed to facilitate model-based domain knowledge transfer to large language models while ensuring differential privacy. Our approach not only maintains high accuracy but also provides robust privacy guarantees, making it particularly applicable to sensitive domains such as healthcare and finance. Beyond its technical contributions, Llamdex represents a novel approach to transferring sensitive knowledge through models rather than data. We believe this framework will shape the future of LLMs by enabling better integration of domain-specific knowledge, leading to more tailored and privacy-aware web Al services."}]}