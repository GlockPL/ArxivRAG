{"title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?", "authors": ["Zhengbo Wang", "Jian Liang"], "abstract": "Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method for parameter-efficient fine-tuning foundation models by re-parameterizing the original matrix into the product of two low-rank matrices. Despite its efficiency, LoRA often yields inferior performance compared to full fine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning. We reveal that while LoRA employs low-rank approximation, it neglects to approximate the optimization process of full fine-tuning. To address this, we introduce a novel concept called the \"equivalent gradient.\" This virtual gradient makes the optimization process on the re-parameterized matrix equivalent to LoRA, which can be used to quantify the differences between LoRA and full fine-tuning. The equivalent gradient is derived from the gradients of matrices A and B. To narrow the performance gap, our approach minimizes the differences between the equivalent gradient and the gradient obtained from full fine-tuning during the optimization process. By solving this objective, we derive optimal closed-form solutions for updating matrices A and B. Our method constrains the optimization process, shrinking the performance gap between LoRA and full fine-tuning. Extensive experiments on natural language processing tasks validate the effectiveness of our method.", "sections": [{"title": "1 Introduction", "content": "Foundational models [Radford et al., 2021, Brown et al., 2020, Achiam et al., 2023, Kirillov et al., 2023, Rombach et al., 2022] have become the cornerstone of modern deep learning. By undergoing pre-training on massive datasets, these models typically exhibit excellent generalization and versatility. Remarkably, some foundation models even demonstrate emergent properties [Hoffmann et al., 2022, Kaplan et al., 2020]. As a result, foundation models have been widely applied to various downstream applications.\nDespite these advantages, the huge number of parameters in foundational models hinders their broader application. The substantial parameter count results in high fine-tuning costs for these tasks. To address this issue, recent research has focused on parameter-efficient fine-tuning (PEFT) methods [Hu et al., 2022, Houlsby et al., 2019, Lester et al., 2021, Zhou et al., 2022]. PEFT methods reduce the fine-tuning cost by keeping the foundation models frozen and only fine-tuning small, additional lightweight adapters. With the majority of parameters frozen, PEFT enables faster fine-tuning and requires fewer computational resources.\nLow-rank adaptation [Hu et al., 2022], also known as LoRA, is one of the most famous PEFT methods, which has been widely adopted across various domains. Inspired by previous works [Aghajanyan et al., 2021, Li et al., 2018], LoRA hypothesizes that the changes in weights during model adaptation exhibit a low-rank structure. To capture this, LoRA re-parameterizes these changes by expressing them as the product of two low-rank matrices: $W = W_o + \\Delta W \\approx W_o + sBA$, where s is a scaling factor, and $A \\in \\mathbb{R}^{r \\times n}$ and $B\\in \\mathbb{R}^{m \\times r}$ are low-rank matrices with rank $r < min(m, n)$. LoRA reduces the number of trainable parameters from $m \\times n$ to"}, {"title": "2 Related Work", "content": "Parameter-Efficient Fine-Tuning. Given the huge size of foundation models, recent research has focused on developing parameter-efficient fine-tuning methods [Hu et al., 2022, Liu et al., 2024, Ding et al., 2023, Houlsby et al., 2019, Liu et al., 2023, Lester et al., 2021]. These methods aim to reduce the cost of fine-tuning by adjusting only a small portion of the model's parameters. Generally, these methods fall into two main categories. The first category is adapter tuning [Houlsby et al., 2019, Sung et al., 2022, He et al., 2021, Zhang et al., 2024, Bapna and Firat, 2019, Hu et al., 2022], which involves inserting small neural network modules, called adapters, into specific layers of the model. During fine-tuning, we keep the model frozen and only fine-tune the lightweight adapter modules, significantly reducing the memory footprint for fine-tuning. The second category is prompt tuning [Lester et al., 2021, Zhou et al., 2022, Li and Liang, 2021, Liu et al., 2022]. Prompt tuning adapts the models to specific tasks by adding specially designed prompts or learnable tokens to the input data, rather than directly modifying the internal parameters of foundation models. In this paper, we focus on LoRA [Hu et al., 2022], a prominent method within the realm of adapter tuning.\nLow Rank Adaptation. Low-rank adaptation, initially referred to as LoRA [Hu et al., 2022], has evolved into a broad category encompassing parameter-efficient fine-tuning methods based on low-rank approximations [Hu et al., 2022, Liu et al., 2024, Hayou et al., 2024, Kalajdzievski, 2023, Zhang et al., 2023, Kopiczko et al., 2024, Hyeon-Woo et al., 2022, Zhang and Pilanci, 2024, Wang et al., 2024, Zhao et al., 2024]. LORA [Hu et al., 2022] assumes that the changes in the weights of pre-trained models exhibit a low-rank structure. Consequently, it re-parameterizes these changes as the product of low-rank matrices, thereby reducing the cost associated with fine-tuning.\nSeveral variants of LoRA have been proposed to address different aspects of this approach. For example, DORA [Liu et al., 2024] improves LoRA [Hu et al., 2022] by incorporating a learnable magnitude vector"}, {"title": "3 Method", "content": "In this section, we begin by revisiting LoRA [Hu et al., 2022] in Section 3.1. Following this, we conduct a comparison between LoRA and full fine-tuning from an optimization perspective in Section 3.2. Finally, in Section 3.3, we point out that LoRA falls short in approximating full fine-tuning during the optimization process, and we introduce LoRA-Pro as a solution to bridge this performance gap."}, {"title": "3.1 Revisit Low Rank Adaptation", "content": "First of all, let's dive back into Low-Rank Adaptation (LoRA) [Hu et al., 2022]. LoRA's core idea revolves around recognizing the low-rank structure of the change matrix AW in the standard fine-tuning process. This insight allows LoRA [Hu et al., 2022] to re-parameterize the change matrix into the product of two low-rank matrices,\n$W = W_o + \\Delta W = W_o + sBA$.\nHere, $W_o \\in \\mathbb{R}^{m \\times n}$ represents the pre-trained weight matrix, $B \\in \\mathbb{R}^{m \\times r}$ and $A \\in \\mathbb{R}^{r \\times n}$ are the low-rank matrices, and s is a scaling factor. For LoRA [Hu et al., 2022], $s = \\frac{1}{r}$, while for rsLoRA [Kalajdzievski, 2023], $s = \\frac{a}{\\sqrt{r}}$. Here, a is the hyper-parameter and $r < min(m,n)$ denotes the rank. Consequently, LoRA significantly reduces the number of fine-tuning parameters from $m \\times n$ to $r \\times (m + n)$."}, {"title": "3.2 LoRA v.s. Full Fine-tuning", "content": "Despite widespread applications across various domains, LoRA's performance still falls short when compared to full fine-tuning. In this part, we review and compare LoRA and full fine-tuning in the optimization process. In full fine-tuning, we utilize differential to analyze the relationship between changes in the loss and changes in the weights:\n$dL = <\\frac{\\partial L}{\\partial W}, dW>_F,$\nwhere $dL$ and $dW$ denotes the changes of the parameter W and the loss L, and $||\\cdot ||_F$ is the Frobenius norm. To minimize the loss function, we typically set $dW = \\frac{\\partial L}{\\partial W}g$ (omitting the learning rate for simplicity), which results in $dL = -||\\frac{\\partial L}{\\partial W}||^2 \\le 0$.\nIn LoRA optimization, given that $W = W_o + sBA$, we compute the differential using the chain rule:\n$dL = <\\frac{\\partial L}{\\partial W}, dW>_F$\n$=<\\frac{\\partial L}{\\partial W}, \\frac{\\partial W}{\\partial A} dA + \\frac{\\partial W}{\\partial B} dB>_F$\n$=<\\frac{\\partial L}{\\partial W}\\frac{\\partial W^T}{\\partial A}, dA>_F + <\\frac{\\partial L}{\\partial W}\\frac{\\partial W^T}{\\partial B}, dB>_F$\n$=<\\frac{\\partial L}{\\partial A}, dA>_F + <\\frac{\\partial L}{\\partial B}, dB>_F$.\nSimilarly, LoRA sets $dA = -\\frac{\\partial L}{\\partial A}g_{lora}^A$ and $dB = -\\frac{\\partial L}{\\partial B}g_{lora}^B$, and thus $dL = -||\\frac{\\partial L}{\\partial A}||^2 - ||\\frac{\\partial L}{\\partial B}||^2 \\le 0$. Moreover, employing the chain rule, we derive:\n$g_{lora}^A = \\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial W} \\frac{\\partial W}{\\partial A} = sB^T g$,\n$g_{lora}^B = \\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial W} \\frac{\\partial W}{\\partial B} = s g A^T.$"}, {"title": "3.3 Low-Rank Adaptation with Equivalent Gradient", "content": "In the context of LoRA optimization, we define the equivalent gradient as,\n$\\hat{g} = \\frac{\\partial W^T}{\\partial A} g^A + \\frac{\\partial W^T}{\\partial B} g^B = sBg^A + sg^BA,$\nwhere s is the scaling factor, and $g^A$ and $g^B$ are gradients with respect to A and B, respectively.\nIn this section, Equivalent Gradient. From Equation (3), we can see that changes in matrices A and B are inherently linked to changes in matrix W through the chain rule:\n$dW = \\frac{\\partial W^T}{\\partial A} dA + \\frac{\\partial W^T}{\\partial B} dB = -(sBg_{lora}^A + sg_{lora}^BA)$.\nIn comparison to full fine-tuning, this is equivalent to updating W using the gradient $g = sBg_{lora}^A + sg_{lora}^BA$. This critical relationship has been neglected in the LoRA optimization process. Hence, we hypothesize that by carefully adjusting the gradients of matrices A and B in such a way that $\\hat{g}$ under LoRA closely approximates the gradient g from full fine-tuning, we can effectively bridge the gap between LoRA and full fine-tuning.\nBased on this relationship, we define the concept of equivalent gradient in Definition 1. Equivalent gradient describes the gradient of the matrix W following low-rank adaptation, despite W not being a trainable parameter. To narrow the performance gap, our goal is to carefully select suitable $\\hat{g}^A$ and $g^B$ to minimize the distance between the equivalent gradient $\\hat{g}$ and the gradient under full fine-tuning g. Hence, our objective is:\n$\\min_{\\hat{g}^A, g^B} ||g - \\hat{g}||$\ns.t. $\\hat{g} = sBg^A + sg^BA$,\ndL \u2264 0.\nAssume matrices $B\\in \\mathbb{R}^{m\\times r}$, $A \\in \\mathbb{R}^{r\\times n}$ are both full rank. For the objective $\\min_{\\hat{g}^A,g^B} ||\\hat{g} - g||$, the solutions are given by:\n$\\hat{g}^A = (B^TB)^{-1} B^T g+XA = (B^TB)^{-1} g_{lora}^A + XA$\n$\\hat{g}^B = \\frac{1}{s^2}[I - B(B^TB)^{-1} B^T ]gA^T (AA^T)^{-1} - BX = \\frac{1}{s^2}[I - B(B^TB)^{-1} B^T ]g_{lora}^B (AA^T)^{-1} - BX.$\nHere, $X \\in \\mathbb{R}^{r \\times r}$ represents an arbitrary matrix."}, {"title": "4 Experimental Results", "content": "In this section, we evaluate our LoRA-Pro method across various natural language understanding datasets. To provide a comprehensive comparison, we include several baseline methods: 1) full fine-tuning and the standard LORA [Hu et al., 2022]. 2) LoRA variants maintaining the original structure, such as rsLoRA [Kalajdzievski, 2023], LoRA+ [Hayou et al., 2024], PiSSA [Meng et al., 2024], 3) ORA variants with modified structures, including DoRA [Liu et al., 2024] and AdaLoRA [Zhang et al., 2023].\nThe results are shown in Table 1. We fine-tune the T5-base model [Raffel et al., 2020] with the baseline methods on a subset of GLUE datasets. From Table 1, we observe that LoRA-Pro achieves the highest scores on 3 out of 5 datasets and the highest average score across all 5 datasets. Moreover, on average over 5"}, {"title": "5 Conclusion", "content": "In this paper, we introduce LoRA-Pro, a novel approach designed to bridge the performance gap between LORA and full fine-tuning. To bridge the performance gap, we introduce the concept of Equivalent Gradient, which allows us to quantify the difference in the optimization process between LoRA and full fine-tuning. By minimizing this discrepancy, we derive the optimal closed-form updated solutions for LoRA. Moreover, we prove that the solutions guarantee the loss decease during optimization. These solutions not only apply a low-rank approximation to the fine-tuning matrix but also maintain consistency with the optimization of full fine-tuning, enabling more effective fine-tuning. Finally, we validate the effectiveness of our method through extensive experiments on natural language processing tasks."}, {"title": "A Notations", "content": "In Table 2, we detail the notations utilized in our paper."}, {"title": "B Proof of Theoretical Results", "content": ""}, {"title": "B.1 Proof of Theorem 3.1", "content": ""}, {"title": "B.2 Proof of Theorem 3.2", "content": ""}, {"title": "B.3 Proof of Theorem 3.3", "content": ""}, {"title": "C Optimization Algorithms", "content": "In this section, we present the pseudo-codes for implementing our LoRA-Pro method using the SGD [Sutskever et al., 2013] and AdamW [Loshchilov and Hutter, 2019] optimizers. These are detailed in Algorithm 1 and Algorithm 2, respectively.\nIn the standard SGD algorithm, as illustrated in Algorithm 1, all we need to do is adjusting the gradients of matrices A and B with the solutions in Theorem 3.1.\nIn AdamW optimizer, the implementation becomes more complex. Several modifications are necessary. Firstly, in order to mimic full fine-tuning, after adjusting the gradients of matrices A and B, we need to compute the equivalent gradient,\n$\\hat{g} = sg^BA + sBg^A$.\nSubsequently, we calculate the first and second moments of this equivalent gradient to derive the corresponding AdamW gradient, $\\hat{g}_{AdamW}$. Secondly, we determine the gradients with respect to matrices A and B as follows:\n$g^A = sB^T \\hat{g}_{AdamW}, g^B = s \\hat{g}_{AdamW} A^T$.\nThirdly, the weight decay process must be adjusted. In line with full fine-tuning, the weight decay is given by:\n$W \\leftarrow (1 - \\gamma \\lambda)(W_0 + sBA)$"}]}