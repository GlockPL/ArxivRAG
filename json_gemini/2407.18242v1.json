{"title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?", "authors": ["Zhengbo Wang", "Jian Liang"], "abstract": "Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method for parameter-efficient\nfine-tuning foundation models by re-parameterizing the original matrix into the product of two low-rank\nmatrices. Despite its efficiency, LoRA often yields inferior performance compared to full fine-tuning. In\nthis paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning. We reveal that while LoRA\nemploys low-rank approximation, it neglects to approximate the optimization process of full fine-tuning.\nTo address this, we introduce a novel concept called the \"equivalent gradient.\" This virtual gradient makes\nthe optimization process on the re-parameterized matrix equivalent to LoRA, which can be used to quantify\nthe differences between LoRA and full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices A and B. To narrow the performance gap, our approach minimizes the differences between the\nequivalent gradient and the gradient obtained from full fine-tuning during the optimization process. By\nsolving this objective, we derive optimal closed-form solutions for updating matrices A and B. Our method\nconstrains the optimization process, shrinking the performance gap between LoRA and full fine-tuning.\nExtensive experiments on natural language processing tasks validate the effectiveness of our method.", "sections": [{"title": "1 Introduction", "content": "Foundational models [Radford et al., 2021, Brown et al., 2020, Achiam et al., 2023, Kirillov et al., 2023,\nRombach et al., 2022] have become the cornerstone of modern deep learning. By undergoing pre-training on\nmassive datasets, these models typically exhibit excellent generalization and versatility. Remarkably, some\nfoundation models even demonstrate emergent properties [Hoffmann et al., 2022, Kaplan et al., 2020]. As a\nresult, foundation models have been widely applied to various downstream applications.\nDespite these advantages, the huge number of parameters in foundational models hinders their broader\napplication. The substantial parameter count results in high fine-tuning costs for these tasks. To address\nthis issue, recent research has focused on parameter-efficient fine-tuning (PEFT) methods [Hu et al., 2022,\nHoulsby et al., 2019, Lester et al., 2021, Zhou et al., 2022]. PEFT methods reduce the fine-tuning cost by\nkeeping the foundation models frozen and only fine-tuning small, additional lightweight adapters. With the\nmajority of parameters frozen, PEFT enables faster fine-tuning and requires fewer computational resources.\nLow-rank adaptation [Hu et al., 2022], also known as LoRA, is one of the most famous PEFT methods,\nwhich has been widely adopted across various domains. Inspired by previous works [Aghajanyan et al., 2021,\nLi et al., 2018], LoRA hypothesizes that the changes in weights during model adaptation exhibit a low-rank\nstructure. To capture this, LoRA re-parameterizes these changes by expressing them as the product of two\nlow-rank matrices: \\(W = W_o + \\Delta W \\approx W_o + sBA\\), where s is a scaling factor, and \\(A \\in \\mathbb{R}^{r \\times n}\\) and \\(B \\in \\mathbb{R}^{m \\times r}\\) are\nlow-rank matrices with rank \\(r < min(m, n)\\). LoRA reduces the number of trainable parameters from \\(m \\times n\\) to"}, {"title": "2 Related Work", "content": "Parameter-Efficient Fine-Tuning. Given the huge size of foundation models, recent research has focused\non developing parameter-efficient fine-tuning methods [Hu et al., 2022, Liu et al., 2024, Ding et al., 2023,\nHoulsby et al., 2019, Liu et al., 2023, Lester et al., 2021]. These methods aim to reduce the cost of fine-tuning\nby adjusting only a small portion of the model's parameters. Generally, these methods fall into two main\ncategories. The first category is adapter tuning [Houlsby et al., 2019, Sung et al., 2022, He et al., 2021, Zhang\net al., 2024, Bapna and Firat, 2019, Hu et al., 2022], which involves inserting small neural network modules,\ncalled adapters, into specific layers of the model. During fine-tuning, we keep the model frozen and only\nfine-tune the lightweight adapter modules, significantly reducing the memory footprint for fine-tuning.\nThe second category is prompt tuning [Lester et al., 2021, Zhou et al., 2022, Li and Liang, 2021, Liu et al.,\n2022]. Prompt tuning adapts the models to specific tasks by adding specially designed prompts or learnable\ntokens to the input data, rather than directly modifying the internal parameters of foundation models. In\nthis paper, we focus on LoRA [Hu et al., 2022], a prominent method within the realm of adapter tuning.\nLow Rank Adaptation. Low-rank adaptation, initially referred to as LoRA [Hu et al., 2022], has evolved\ninto a broad category encompassing parameter-efficient fine-tuning methods based on low-rank approxima-\ntions [Hu et al., 2022, Liu et al., 2024, Hayou et al., 2024, Kalajdzievski, 2023, Zhang et al., 2023, Kopiczko\net al., 2024, Hyeon-Woo et al., 2022, Zhang and Pilanci, 2024, Wang et al., 2024, Zhao et al., 2024]. LORA [Hu\net al., 2022] assumes that the changes in the weights of pre-trained models exhibit a low-rank structure.\nConsequently, it re-parameterizes these changes as the product of low-rank matrices, thereby reducing the\ncost associated with fine-tuning.\nSeveral variants of LoRA have been proposed to address different aspects of this approach. For example,\nDORA [Liu et al., 2024] improves LoRA [Hu et al., 2022] by incorporating a learnable magnitude vector"}, {"title": "3 Method", "content": "In this section, we begin by revisiting LoRA [Hu et al., 2022] in Section 3.1. Following this, we conduct a\ncomparison between LoRA and full fine-tuning from an optimization perspective in Section 3.2. Finally, in\nSection 3.3, we point out that LoRA falls short in approximating full fine-tuning during the optimization\nprocess, and we introduce LoRA-Pro as a solution to bridge this performance gap."}, {"title": "3.1 Revisit Low Rank Adaptation", "content": "First of all, let's dive back into Low-Rank Adaptation (LoRA) [Hu et al., 2022]. LoRA's core idea revolves\naround recognizing the low-rank structure of the change matrix AW in the standard fine-tuning process.\nThis insight allows LoRA [Hu et al., 2022] to re-parameterize the change matrix into the product of two\nlow-rank matrices,\n\\[W = W_o + \\Delta W = W_o + sBA.\n\\]\nHere, \\(W_o \\in \\mathbb{R}^{m \\times n}\\) represents the pre-trained weight matrix, \\(B \\in \\mathbb{R}^{m \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times n}\\) are the low-rank matrices,\nand s is a scaling factor. For LoRA [Hu et al., 2022], \\(s = \\frac{1}{r}\\), while for rsLoRA [Kalajdzievski, 2023], \\(s = \\frac{\\alpha}{r}\\).\nHere, \\( \\alpha\\) is the hyper-parameter and \\(r < min(m,n)\\) denotes the rank. Consequently, LoRA significantly\nreduces the number of fine-tuning parameters from \\(m \\times n\\) to \\(r \\times (m + n)\\)."}, {"title": "3.2 LoRA v.s. Full Fine-tuning", "content": "Despite widespread applications across various domains, LoRA's performance still falls short when compared\nto full fine-tuning. In this part, we review and compare LoRA and full fine-tuning in the optimization\nprocess. In full fine-tuning, we utilize differential to analyze the relationship between changes in the loss\nand changes in the weights:\n\\[dL = <\\frac{\\partial L}{\\partial W}, dW>_F,\n\\]\nwhere \\(dL\\) and \\(dW\\) denotes the changes of the parameter W and the loss L, and \\(||\\cdot ||_F\\) is the Frobenius norm.\nTo minimize the loss function, we typically set \\(dW = \\frac{\\partial L}{\\partial W} g\\) (omitting the learning rate for simplicity),\nwhich results in \\(dL = -||\\frac{\\partial L}{\\partial W}||^2 \\leq 0\\).\nIn LoRA optimization, given that \\(W = W_o + sBA\\), we compute the differential using the chain rule:\n\\[\\begin{aligned}\ndL &= <\\frac{\\partial L}{\\partial W},dW>_F\\\\\n&= (\\frac{\\partial L}{\\partial W} \\frac{\\partial W}{\\partial A}, dA + \\frac{\\partial L}{\\partial W} \\frac{\\partial W}{\\partial B}, dB)_F\\\\\n&= <\\frac{\\partial L}{\\partial W} \\frac{\\partial W}{\\partial A},dA>_F + <\\frac{\\partial L}{\\partial W} \\frac{\\partial W}{\\partial B}, dB>_F\\\\\n&= <\\frac{\\partial L}{\\partial A}, dA>_F + <\\frac{\\partial L}{\\partial B}, dB>_F\n\\end{aligned}\n\\]\nSimilarly, LoRA sets \\(dA = -\\frac{\\partial L}{\\partial A} g_{lora}\\) and \\(dB = -\\frac{\\partial L}{\\partial B} g_{lora}\\), and thus \\(dL = -||\\frac{\\partial L}{\\partial A} g_{lora}|| - ||\\frac{\\partial L}{\\partial B} g_{lora}|| \\leq 0\\). Moreover,\nemploying the chain rule, we derive:\n\\[g^A_{lora} = \\frac{\\partial L}{\\partial W} \\frac{\\partial W}{\\partial A} = sB^T g, \\quad g^B_{lora} = \\frac{\\partial L}{\\partial W} \\frac{\\partial W}{\\partial B} = s g A^T.\n\\]"}, {"title": "3.3 Low-Rank Adaptation with Equivalent Gradient", "content": "In this section, Equivalent Gradient. From Equation (3), we can see that changes in matrices A and B are\ninherently linked to changes in matrix W through the chain rule:\n\\[\\Delta W = \\frac{\\partial W}{\\partial A} dA + \\frac{\\partial W}{\\partial B} dB = -(sB g^A_{lora} + sg^B_{lora} A).\n\\]\nIn comparison to full fine-tuning, this is equivalent to updating W using the gradient \\(g = sB g^A_{lora} + sg^B_{lora} A\\).\nThis critical relationship has been neglected in the LoRA optimization process. Hence, we hypothesize\nthat by carefully adjusting the gradients of matrices A and B in such a way that \\(\\tilde{g}\\) under LoRA closely\napproximates the gradient g from full fine-tuning, we can effectively bridge the gap between LoRA and full\nfine-tuning.\nBased on this relationship, we define the concept of equivalent gradient in Definition 1. Equivalent\ngradient describes the gradient of the matrix W following low-rank adaptation, despite W not being a\ntrainable parameter. To narrow the performance gap, our goal is to carefully select suitable \\(\\tilde{g}^A\\) and \\(\\tilde{g}^B\\) to\nminimize the distance between the equivalent gradient \\(\\tilde{g}\\) and the gradient under full fine-tuning g. Hence,\nour objective is:\n\\[\\min_{\\tilde{g}^A,\\tilde{g}^B} ||g - \\tilde{g}||_F\n\\\\\ns.t. \\quad \\tilde{g} = sB\\tilde{g}^A + s\\tilde{g}^B A,\n\\\\\ndL \\leq 0.\n\\]"}, {"title": "4 Experimental Results", "content": "In this section, we evaluate our LoRA-Pro method across various natural language understanding datasets.\nTo provide a comprehensive comparison, we include several baseline methods: 1) full fine-tuning and the\nstandard LORA [Hu et al., 2022]. 2) LoRA variants maintaining the original structure, such as rsLoRA [Kala-\njdzievski, 2023], LoRA+ [Hayou et al., 2024], PiSSA [Meng et al., 2024], 3) ORA variants with modified\nstructures, including DoRA [Liu et al., 2024] and AdaLoRA [Zhang et al., 2023].\nThe results are shown in Table 1. We fine-tune the T5-base model [Raffel et al., 2020] with the baseline\nmethods on a subset of GLUE datasets. From Table 1, we observe that LoRA-Pro achieves the highest scores\non 3 out of 5 datasets and the highest average score across all 5 datasets. Moreover, on average over 5"}, {"title": "5 Conclusion", "content": "In this paper, we introduce LoRA-Pro, a novel approach designed to bridge the performance gap between\nLORA and full fine-tuning. To bridge the performance gap, we introduce the concept of Equivalent Gradient,\nwhich allows us to quantify the difference in the optimization process between LoRA and full fine-tuning.\nBy minimizing this discrepancy, we derive the optimal closed-form updated solutions for LoRA. Moreover,\nwe prove that the solutions guarantee the loss decease during optimization. These solutions not only apply a\nlow-rank approximation to the fine-tuning matrix but also maintain consistency with the optimization of\nfull fine-tuning, enabling more effective fine-tuning. Finally, we validate the effectiveness of our method\nthrough extensive experiments on natural language processing tasks."}, {"title": "A Notations", "content": "In Table 2, we detail the notations utilized in our paper."}, {"title": "B Proof of Theoretical Results", "content": ""}, {"title": "B.1 Proof of Theorem 3.1", "content": ""}, {"title": "B.2 Proof of Theorem 3.2", "content": ""}, {"title": "B.3 Proof of Theorem 3.3", "content": ""}, {"title": "C Optimization Algorithms", "content": "In this section, we present the pseudo-codes for implementing our LoRA-Pro method using the SGD [Sutskever\net al., 2013] and AdamW [Loshchilov and Hutter, 2019] optimizers. These are detailed in Algorithm 1 and\nAlgorithm 2, respectively.\nIn the standard SGD algorithm, as illustrated in Algorithm 1, all we need to do is adjusting the gradients\nof matrices A and B with the solutions in Theorem 3.1.\nIn AdamW optimizer, the implementation becomes more complex. Several modifications are necessary.\nFirstly, in order to mimic full fine-tuning, after adjusting the gradients of matrices A and B, we need to\ncompute the equivalent gradient,\n\\[\\tilde{g} = s g^B g^A + s B g^A.\n\\]\nSubsequently, we calculate the first and second moments of this equivalent gradient to derive the corre-\nsponding AdamW gradient, \\(\\tilde{g}_{AdamW}\\). Secondly, we determine the gradients with respect to matrices A and B\nas follows:\n\\[g^A = s B^T \\tilde{g}_{AdamW}, \\quad g^B = s \\tilde{g}_{AdamW} A^T.\n\\]\nThirdly, the weight decay process must be adjusted. In line with full fine-tuning, the weight decay is given\nby:\n\\[W \\leftarrow (1 - \\gamma \\lambda)(W_0 + sBA).\n\\]"}]}