{"title": "MuseBarControl: Enhancing Fine-Grained Control in Symbolic Music Generation through Pre-Training and Counterfactual Loss", "authors": ["Yangyang Shu", "Haiming Xu", "Ziqin Zhou", "Anton van den Hengel", "Lingqiao Liut"], "abstract": "Automatically generating symbolic music-music scores tailored to specific human needs can be highly beneficial for musicians and enthusiasts. Recent studies have shown promising results using extensive datasets and advanced transformer architectures. However, these state-of-the-art models generally offer only basic control over aspects like tempo and style for the entire composition, lacking the ability to manage finer details, such as control at the level of individual bars. While fine-tuning a pre-trained symbolic music generation model might seem like a straightforward method for achieving this finer control, our research indicates challenges in this approach. The model often fails to respond adequately to new, fine-grained bar-level control signals. To address this, we propose two innovative solutions. First, we introduce a pre-training task designed to link control signals directly with corresponding musical tokens, which helps in achieving a more effective initialization for subsequent fine-tuning. Second, we implement a novel counterfactual loss that promotes better alignment between the generated music and the control prompts. Together, these techniques significantly enhance our ability to control music generation at the bar level, showing a 13.06% improvement over conventional methods. Our subjective evaluations also confirm that this enhanced control does not compromise the musical quality of the original pre- trained generative model.", "sections": [{"title": "1 Introduction", "content": "Symbolic music generation, which focuses on automatically creating music scores, has garnered increasing interest in recent years due to its intuitive readability and excellent editability [27, 23]. Noteworthy advancements, such as Music Transformer [2], Museformer [26] and MuseCoco [17], have captivated both researchers and enthusiasts. Utilizing extensive datasets and sophisticated transformer architectures, these developments not only generate highly valuable music scores but also facilitate easy modification and editing, thanks to the accessibility provided by the score format.\nSignificant progress has been made in symbolic music generation, but there are still important limita- tions to acknowledge. One major challenge is the granular control over the music produced. Previous studies [23, 18, 17] have primarily focused on generating music using broad, overarching descrip- tions, allowing limited manipulation of elements like tempo and style across entire compositions. This lack of fine-grained control at the level of individual bars restricts the detailed alteration of musical elements. The ability to manage music at the bar level would be advantageous, offering users greater creative freedom and also enhancing applications in automatic music composition. For instance, attributes from a specific bar could be extracted and applied to generate another piece, facilitating style imitation. Bar-level control could improve the alignment of lyrics and melody, ensuring that the music accurately reflects the emotional cues of the lyrics. Furthermore, attributes from favoured music pieces can be identified and utilized to create new pieces using bar-level control, allowing for customized musical creation.\nAn effective method for implementing bar-level control is to fine-tune a foundational model with newly introduced control signals. This approach is particularly valuable due to the wide range of necessary controls that are difficult to fully anticipate during the initial training of the foundational model. The capability of adapt a trained model to new control is crucial as it enables the integration of diverse and unexpected controls, enhancing the model's utility without the need for complete retraining. Specifically, we can utilize bar-level music attributes extracted from the training set's music scores as prefix control prompts to train an autoregressive model, with the objective of optimizing the likelihood of the training samples. However, we've found that models trained in this manner often fail to adhere to the guidance of the bar-level attributes. Our analysis suggests that the model struggles with interpreting the meanings of these new control prompts, leading to music that does not accurately align with these prompts. Furthermore, when the training data is limited, the model is prone to overfitting, focusing more on minimizing loss rather than effectively using the control prompt to steer music generation.\nTo address this limitation, we propose two strategies to improve bar-level controllability. The first strategy involves pretraining the control prompt and fine-tuning the model on an auxiliary task designed to promote accurate alignment between the control prompts and the generated music tokens. The second strategy introduces a counterfactual loss that penalizes the model for neglecting the bar-level guidance. By implementing these two techniques together, we significantly enhance the accuracy of bar-level control without compromising the quality of the music produced.\nIn sum, the key contributions of this work are outlined as follows:\n\u2022 We conducted the first study in achieving fine-grained control of symbolic music generation based on the existing foundation model.\n\u2022 We propose two innovative strategies, auxiliary task pre-training and counterfactual loss, to improve bar-level control in the foundation model."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Text-Driven Music Generation", "content": "Text-driven music generation, aimed at creating high-quality music from textual descriptions, has attracted considerable attention from researchers due to its user-friendly editing capabilities. However, the scarcity of paired text-music data presents a major challenge. To address this, some studies have employed diffusion-based methods [28, 21] with self-collected text-audio datasets to facilitate text-audio music generation. MuLan [10] tackles the issue of data scarcity. They use techniques similar to those in CLIP [19] to contrastively embed two modalities: music pieces and their textual annotations. Building on this, MusicLM [1] generates audio from MuLan's embeddings [10], enabling text-to-music conversion without the need for paired data. However, MusicLM's process of sampling to acquire fine-grained acoustic tokens is computationally intensive. Other efforts, like MeLoDy [14], seek to simplify music generation by efficiently translating conditioning tokens into sound waves. Furthermore, MusicGen [5] introduces a single-stage transformer LM framework that models multiple streams of acoustic tokens in parallel. Despite significant advancements in text-driven music generation, the methods are still relatively crude, limiting users' ability to edit musical elements within the generated audio. The controllability and editability of the outputs remain constrained."}, {"title": "2.2 Symbolic Music Generation", "content": "Compared to text-driven music generation, symbolic music provides easier editing capabilities, allowing users to manipulate specific musical elements more effectively. The development of solutions for this task has evolved significantly, from grammar rules-based methods [4, 8, 7] to probabilistic models and evolutionary computation [24, 16], and more recently to neural networks and deep learning [25, 3, 6]. The advent of transformer-based models, known for their successes"}, {"title": "3 Preliminaries", "content": "We begin by examining the MuseCoco model [17], which serves as the foundation for our method. MuseCoco is a text-to-music generative model that initially converts text instructions into a set of music attributes and then generates music tokens based on these attributes. Our approach builds upon this attribute-to-music generation model.\nIn this model, a series of prefix tokens $x = [x_1, x_2, ..., x_m]$ encodes the music attributes. Subsequently, the model generates a sequence of music tokens $y = [y_1, y_2, ..., y_n]$. For additional details on the tokenization design, we refer to [17]. The model is trained to maximize the log-likelihood of the ground-truth music sequences, as in the standard autoregressive model, namely:\n$\\mathcal{L} = - \\sum_{i=1}^n \\log p(y_i | y_{<i}, x_{1:m}),$\nwhere $y_{<i}$ indicates historic tokens before $i$.\nOne limitation of the MuseCoco model [17] is its focus solely on global music attributes, which describe the overall composition without supporting finer control, such as at the bar level. Fine- grained control is particularly valuable to both musicians and amateurs as it enables users to define specific properties for smaller segments of music. For instance, users can specify the chords\u00b3 in each bar and provide a chord progression, and then explore various musical outputs based on that progression. Figure 1 illustrates how different music pieces can share the same chord progression. Beyond its utility for human users, fine-grained bar-level control is also advantageous for automated composition. For example, bar-level music attributes from one piece could be used to conditionally"}, {"title": "4 Method", "content": "To attain precise control, we present MuseBarControl. This model overcomes the constraints of exist- ing music score generation models that largely produce music based on broad and vague descriptions. Our method consists of three components: (1) we refine the control prompt in MuseCoco [17] to facil- itate bar-level control instead of global control. (2) we incorporate an auxiliary task to pre-condition the model and the newly implemented control prompts. (3) we introduce a counterfactual loss to enhance the adherence of the generated music to the bar-level control prompts."}, {"title": "4.1 Control Prompt Augmentation", "content": "MuseCoco [17] only incorporates global musical attributes that define the overall character of the music, which cannot achieve fine-grained bar-level control of music generation. To facilitate the latter, we first introduce a scheme to specify the bar-level music attribute. Specifically, MuseBarControl processes a sequence of music attribute tokens $X = X_g, X_1,X_2, ...X_b$ as input, where $X_g = X_{g,1}, X_{g,2}, ...X_{g,|X_g|}$ comprises $|X_g|$ tokens representing global attributes, and $b$ denotes the number of music bars. Each $i$th bar $X_i = x_{i,1}, x_{i,2}, ..., x_{i,|X_i|}$ contains $|X_i|$ tokens representing the attributes at the bar level, e.g., token 24 could indicates the chord of the current bar is \"E:b\" (see Table 5 in Appendix). Those attributes can be extracted from the training music scores, i.e., from $Y_1, Y_2, ..., Y_n$. For example, there are existing algorithms 4 to extract the chord implies in a given bar based on the distribution of the note pitches in the bar. Position embeddings are added to the bar-level tokens within this sequence $X_1, X_2, ... X_b$, distinguishing each bar's tokens shown in Figure 2. Tokens corresponding to the same bar are assigned identical position embeddings. Conversely, position embeddings are not used for the global tokens $X_g$. As a result, the input sequence in our method is encoded as follows:\n$X_g, X_1, X_2,..., X_b, [SEP], Y_1, Y_2, \u2026, Y_n$.\nA straightforward approach involves fine-tuning the MuseCoco model by optimizing the likelihood of the ground-truth music sequence given the bar-level attributes derived from that same sequence, say:\n$L_{BFT} = - \\sum_{i=1}^n \\log p(Y_i | Y_{<i}, X),$\nwhere $L_{BFT}$ denotes the Bar-level Fine-Tuning loss. In practice, this process can be easily im- plemented via efficient parameter fine-tuning, such as LoRA [9]. However, as detailed in the experimental section (see Table 1), this method proves to be less effective. We find that the model tends to overlook the newly introduced control prompts in its efforts to maximize the likelihood. Consequently, although the loss decreases, the controllability of the new prompts does not improve. We hypothesize that this issue arises because the new control prompts are randomly initialized and they have not been supported by the foundation. As a result, the MuseCoco model struggles to adequately translate these controls into the music generation process. Furthermore, the model can more readily maximize likelihood by overfitting the data, such as by memorizing the training pieces, thereby diminishing any incentive for the network to adhere to the control signals.\nTo address this issue, this paper proposes two strategies to improve the controllability of the network."}, {"title": "4.2 Control Prompts Pre-Adaptation via an Auxiliary Task (PA)", "content": "The first strategy is to utilize an auxiliary task to pre-adapt the bar-level control mechanism of the MuseCoco model. In this context, \"pre-adapt\" refers to modifying the embeddings of the control prompts and the LoRA parameters. The auxiliary task is framed to meet two key criteria: firstly, it must be closely related to the task of bar-level controlled generation, ensuring that the parameters refined during the auxiliary task can be effectively transferred to the main generation task. Secondly,"}, {"title": "4.3 Improving Controllability via Counterfactual Loss (CF)", "content": "The second strategy incorporates the use of a counterfactual loss to verify that the generated to- kens are genuinely influenced by the bar-level control prompts. Our rationale is that if the music tokens of a particular bar are truly driven by its associated bar-level control prompt, then alter- ing the control prompt should result in a substantial decrease in the likelihood of those specific music tokens. Specifically, we randomly replace the bar-level attributes $X_i \\in {X_1, X_2, ..., X_b}$ with a different value within the attribute, denoted as $X_i$. We then measure the change of the negative log-likelihood of the $i$th bar's token, represented by the difference $I_2 - I_1$, where $I_1 = -\\sum_{i\\in bar_i} \\log p(y_i | y_{<i}, X_g, X_1, ..., X_i, ..., X_b)$ is the negative log-likelihood before the change in this bar, and $I_2 = -\\sum_{i\\in bar_i} \\log p(y_i | y_{<i}, X_g, X_1, ..., X_i, ..., X_b)$ indicates the negative log-likelihood after the change in this bar. $X_i$ represents a randomly selected attribute token. In our implementation, to enhance the model's recognition of the governance range of bar-level attributes, we assign $X_i$ the same value as $X_{i-1}$ whenever $X_i \\neq X_{i-1}$.\nThis approach is designed to reinforce the model's awareness of the influence exerted by bar-level attributes. The counterfactual loss is thus defined as:\n$L_{CF} = \\max{0, \\eta - (I_2 - I_1)},$\nwhere the counterfactual loss $L_{CF}$ is designed to promote a significant decrease in the log-likelihood when the alignment between the control prompt and the music tokens is disrupted after modifying $X_i$. $\\eta$ is a margin parameter specifying the desired log-likelihood change."}, {"title": "4.4 Inference", "content": "During the inference stage, music sequences are generated on a bar-by-bar basis. Within each bar, up to $K$ sampling attempts are allowed. If a sample accurately reflects the intended bar-level attributes, the generation continues from the subsequent bar. Conversely, if $K$ samples all fail to exhibit the correct attributes, the token with the highest probability is chosen to continue the sequence prediction. When $K = 1$, this process simplifies to a typical auto-regressive model sampling procedure. Generally, a larger $K$ enhances controllability at the expense of reduced sampling efficiency. It is noteworthy that in many cases, the model may produce valid music on the first attempt, implying that the actual number of sampling operations may increase sublinearly with $K$."}, {"title": "5 Experiments", "content": "In this section, we assess our method through a case study focused on bar-level chord control. In other words, the bar-level attribute corresponds to the chord for each bar. This is useful as in music composition, particularly in pop music, it is customary to first establish a chord progression pattern before composing the music. It's important to note that the objective of our experiment is to evaluate the effectiveness of the proposed methods in enhancing control based on a pre-trained music model. We are not aiming to optimize performance specifically for chord-controlled music generation."}, {"title": "5.1 Experimental Setting", "content": ""}, {"title": "5.1.1 Datasets", "content": "In our study, we use the POP909 dataset [22] to train and evaluate the proposed method. This dataset comprises multiple renditions of the piano arrangements for 909 popular songs, totalling approximately 60 hours of music. These arrangements are meticulously crafted by professional musicians and are provided in MIDI format. On average, each song consists of 270 bars. We chose this dataset for two reasons: (1) Pop music typically features distinct chord progression patterns, making it ideal for this study. (2) Pop piano music is more accessible for general audiences to evaluate its quality. Following [17], we randomly selected three 16-bar clips from each MIDI file. From these clips, global attributes are extracted from the entire clip, while bar-level attributes-chords, are extracted from each individual bar within the 16-bar length clip. The specific predefined musical attribute values, including global attributes sourced from the MuseCoco project [17] and chord attributes derived from MIDI files, are displayed in Appendix A.1. Details regarding the distribution of chords within this dataset are provided in Appendix A.3. To convert the MIDI files into token sequences, we employ a REMI-like representation method [11]. For training, validation, and testing purposes, we partition the songs into three sets, with a split ratio of 8:1:1 for training, validation, and testing, respectively."}, {"title": "5.1.2 Implementation details", "content": "We employ the Linear Transformer architecture as our backbone model [12], configured with a causal attention mechanism spanning 24 layers and utilizing 24 attention heads. The hidden size is set to 2048, while the feedforward network (FFN) hidden size is 8192. During the training phase, we began by initializing the MuseCoco-base weights [17] with fp16 precision and subsequently applied a fine-tuning approach. Within the attention layers, LoRA Adapters [9] was incorporated, with a rank size of $r = 8$. The maximum sequence length was set to 5120. We use validation performance to set the margin $\\eta$ to 0.05. To execute the fine-tuning process, we utilized 4 40GB-A100 GPUs, conducting 50 epochs for the first strategy (PA), and 40 epochs for the second strategy (BFT and CF). We utilize a batch size of 4 and employ the Adam optimizer [13] with a learning rate of 2 \u00d7 10\u20134, incorporating a warm-up step of 16,000 and an invert-square-root decay schedule. For inference, we consider the top 15 highest probabilities as potential prediction hypotheses and perform sampling $K = 15$ times."}, {"title": "5.1.3 Compared models", "content": "In this investigation, we conduct a comparative analysis between our proposed method and MuseC- oco [17] for symbolic music generation. Since MuseCoco is only fed with global musical attributes, we examine MuseCoco with Bar-level Fine-Tuning, represented as BFT which builds upon the MuseCoco model by incorporating bar-level training techniques. This approach involves aligning the music attributes with each bar chord in the input."}, {"title": "5.1.4 Objective evaluation", "content": "\u2022 Chord Accuracy: We use chord accuracy to evaluate the alignment between the prompted chords and the chords generated during the symbolic music generation process. This measure provides insight into the bar-level controllability afforded by our proposed method.\n\u2022 Global Attribute Accuracy: We use global attribute accuracy to evaluate the alignment between prompted global musical attributes and those generated during the inference process. This metric offers insight into the sample-level controllability of our method."}, {"title": "5.1.5 Subjective evaluation", "content": "\u2022 Musicality: measures the extent to which generated music resembles music created by humans.\nWe randomly selected 20 pieces of music generated by MuseCoco performed with piano and our model and created a survey. We then enlisted 16 piano teachers to evaluate which pieces resembled human-created music more closely. The survey options were 'Music 1', 'Music 2', or 'Similar', where Music 1 and Music 2 were randomly assigned to either the MuseCoco generation or ours 5. The degree of musicality was gauged based on the percentage of votes received for each option."}, {"title": "5.2 Compared with MuseCoco", "content": "Table 1 presents the results of both objective and subjective evaluations. The findings are as follows: 1) In terms of musicality, MuseBarControl achieves performance comparable to MuseCoco, with scores of 43.75% versus 40.63% respectively (the full survey statistic is shown in Figure 4.). This equivalence in scores demonstrates that MuseBarControl preserves the musicality inherent in MuseCoco. 2) Regarding chord accuracy, MuseCoco lacks the capability to generate specific chords aligned with each bar. Compared to BFT, MuseBarControl exhibits a significant improvement in chord accuracy by 13.06%, underscoring its enhanced controllability at the bar level. 3) Concerning average global attribute accuracy, MuseBarControl outperforms BFT by a slight margin of 1.01%, and both show a substantial improvement over MuseCoco. The data indicates that aligning music attributes with each bar chord in the input significantly enhances global attribute accuracy. This improvement suggests that the effective generation of bar-level attributes can positively influence global attribute generation."}, {"title": "5.3 Ablation Study", "content": ""}, {"title": "5.3.1 Component-wise analysis", "content": "In this section, we conduct ablation studies to evaluate the impact of each component within our method. The findings are detailed in Table 2. From the table, it can be seen that employing BFT alone only yields a chord accuracy of 65.27% during the inference stage. Notably, the inclusion of PA or CF significantly enhances performance, with improvements of 6.95% and 5.88%, respectively."}, {"title": "5.3.2 Impact of K - the number of sampling in inference", "content": "In our method, during the inference phase, we select music sequences bar-by-bar from the K sampling attempts. This section explores the impact of the value of K. Figure 5 displays the accuracy achieved with different K values. The results indicate that as K, the number of sampling attempts, increases, chord accuracy rises significantly. Additionally, compared to BFT, MuseBarControl consistently delivers superior performance across various K values, with the performance gap between BFT and MuseBarControl widening as K increases."}, {"title": "5.3.3 Impact of the parameter \u5165", "content": "To assess the impact of the parameter $\\lambda$, we conducted experiments using varying values of $\\lambda$. Table 3 presents the chord accuracy corresponding to five different settings of $\\lambda$. As observed, when $\\lambda$ is set to 0, the approach defaults to PA+BFT. As $\\lambda$ increases from 0 to 1e3, there is a gradual improvement in chord accuracy, reaching a peak of 78.33% when $\\lambda$ is approximately 1e3. Beyond this point, the performance begins to decline slightly."}, {"title": "5.4 Complexity Analysis", "content": "Assuming the length of the music sequence is $n$ and the number of sampling attempts per bar is $K$, the time complexity of the inference step in MuseCoco is $O(n)$, whereas in our method it is $O(Kn)$."}, {"title": "5.5 Comments from Musician and Composer", "content": "We invited musicians to provide feedback on the music created by our system and composers to experience how our system aids the creative process. Their guidance and comments are as follows:\nFrom Musician: \u201cI was truly impressed by the music produced by this system; its performance is remarkable. The quality of the music is very similar to that of human compositions, and some of the bar chord arrangements are astonishing\".\nFrom Composer: \u201cThis system for chord arrangement and music creation significantly reduces my composition time. I found it to be a great source of inspiration. For instance, when I wanted to arrange the next chord as an A:m chord, the system provided many options to choose from. However, it uses a lot of broken chords, which composers typically don't use as frequently in their compositions. It would be beneficial if this aspect could be improved in the future\u201d."}, {"title": "6 Limitation and Future Work", "content": "This work focuses on attribute-to-music generation, directly specifying attribute values to control the bar-level music generation process. However, this approach may not be user-friendly for those who prefer to use text descriptions for control. Therefore, in the future, we aim to develop a more user-friendly interface that allows bar-level music generation from text descriptions, enabling users to create and edit bars using natural language. Also, we plan to build a system incorporating more diverse control signals."}, {"title": "7 Conclusion", "content": "In this paper, we introduce MuseBarControl, a method that allows for finer detail control at the level of individual bars, significantly advancing the field of automated music score composition and alignment. This innovative approach offers substantial value and potential for both musicians and amateurs, enhancing creative efficiency and providing greater control over the composition process. Our solution contains two innovative strategies that enhance bar-level control without compromising the quality of the music produced. These designs enable the model to accurately adapt to the adjusted bar-level attributes as new control prompts, thereby achieving impressive bar-level controllability.\nOur research demonstrates the feasibility of bar-level editing in AI technologies. Successful results in chord control and generation, as shown in Appendix A.2, suggest that more bar-level attributes, such as melody trends, can be explored in the future. We hope that further inspiration for bar-level attributes will enhance the ability to edit and control bar generation, thereby improving music creation. It is possible to adopt MuseBarControl for more bar-level attribute edits like melody trends. We hope MuseBarControl will be even more useful with additional bar-level attributes, utilizing the proposed two training strategies."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Pre-defined Musical Attributes", "content": "Table 5 displays the global attributes and their values, as well as the bar-level chord attributes and their values."}, {"title": "A.2 Piano Rolls Analysis", "content": ""}, {"title": "A.2.1 Canon-Style Case Generation", "content": "We conducted experiments using the \u201cCanon chord progression\u201d, as shown in Figure 1, to generate new melodies. In addition, we applied different global attributes to generate distinct melodies while maintaining the Canon style. The global attributes for the first case are 4 octaves, intense intensity, moderato tempo, and minor key. For the second case, the attributes are 2 octaves, moderate intensity, fast tempo, and major key. The piano rolls for these two cases are displayed in Figure 6, and the"}, {"title": "A.2.2 Good Cases", "content": "We selected some successful cases from our testing where the generated chords perfectly align with the prompts at the bar level, as shown in Figure 7. This demonstrates the proposed method's strong ability to control each bar's generation with the correct chords."}, {"title": "A.2.3 Failure Cases", "content": "We also selected some failure cases to analyze and observe how the model failed to generate the correct chords in each bar, as shown in Figure 8. In these instances, we found that some prompt chords, such as A:m7, were incorrectly generated as A:m, and D:m7 was incorrectly generated as F. Additionally, the model frequently confused D:m, D:m7, and E:m7. This indicates that the model sometimes struggles to differentiate between similar chords (e.g., A:m: A-C-E vs A:m7: A-C-E-G; D:m7: D-F-A-C vs F: F-A-C)."}, {"title": "A.3 Chord Proportions.", "content": "The complete chord distribution is presented in Table 6. As shown, the common chords in the POP909 dataset include C, C:maj7, D, D:m, D:m7, E, E:m, E:m7, F, F:maj7, G, A:m, and A:m7. Some chords, however, never appear in the dataset, such as D:+, E:+, F:+, F:m7b5F#:+F#:7G:+, Ab:+, Ab:m7b5, A:+, Bb:+, Bb:dim, Bb:m7b5 B:+, and B:maj7. Given that the selected dataset primarily"}, {"title": "A.4 Human Evaluation", "content": "Figure 9 shows the voting interface."}]}