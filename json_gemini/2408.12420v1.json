{"title": "Dataset | Mindset = Explainable AI | Interpretable AI", "authors": ["Caesar Wu", "Rajkumar Buyya", "YuanFang Li", "Pascal Bouvry"], "abstract": "We often use \"explainable\" Artificial Intelligence (XAI)\" and \"interpretable AI (IAI)\" interchangeably when we apply various XAI tools for a given dataset to explain the reasons that underpin machine learning (ML) outputs. However, these notions can sometimes be confusing because interpretation often has a subjective connotation, while explanations lean towards objective facts. We argue that XAI is a subset of IAI. The concept of IAI is beyond the sphere of a dataset. It includes the domain of a mindset. At the core of this ambiguity is the duality of reasons, in which we can reason either outwards or inwards. When directed outwards, we want the reasons to make sense through the laws of nature. When turned inwards, we want the reasons to be happy, guided by the laws of the heart. While XAI and IAI share reason as the common notion for the goal of transparency, clarity, fairness, reliability, and accountability in the context of ethical AI and trustworthy AI (TAI), their differences lie in that XAI emphasizes the post-hoc analysis of a dataset, and IAI requires a priori mindset of abstraction. This hypothesis can be proved by empirical experiments based on an open dataset and harnessed by High-Performance Computing (HPC). The demarcation of XAI and IAI is indispensable because it would be impossible to determine regulatory policies for many AI applications, especially in healthcare, human resources, banking, and finance. We aim to clarify these notions and lay the foundation of XAI, IAI, EAI, and TAI for many practitioners and policymakers in future AI applications and research.", "sections": [{"title": "1. Introduction", "content": "The notion of eXplainable Artificial Intelligence (XAI) suggests the ability to clarify AI. It offers reasons, evidence, and contexts for the Artificial Intelligence/Machine Learning (AI/ML) results. It answers questions of \"why\" and \"how\". \"Why\u201d offers reasons, and \"how\" explains how the AI arrived at the result supported by reasons. It is a process of making sense of the AI/ML outputs.\nOn the other hand, Interpretable AI (IAI) also provides reasons, but it asks the question, \"Is the reason reasonable?\u201d If we consider IAI to be a high-level abstraction of XAI, IAI is the meta-XAI or criteria of XAI for satisfaction. The IAI process requires our mindset. Molnar [33] argued that our mindset is a perspective of the world when we construct a learning model. In computer programming, \"interpretation\u201d means interactively converting a high-level language into machine codes line-by-line. A programming language is a form of communication between programmers and other people. It also helps programmers organize and describe ideas for a computer. Similarly, IAI can be considered a form of communication among humans, while XAI is an engagement process of our mindset for a given dataset within a particular problem context.\nGuidotti et al.[1] argued that IAI is needed for AI models, while \u03a7\u0391\u0399 aims for prediction results. Miller[2] attempted to define XAI from different perspectives, including philosophy, cognitive psychology/science, and social psychology. Miller did not differentiate between interpretability (XAI) and explainability (IAI). Molnar[16] also prefers to use both terms interchangeably but differentiates between explainability/interpretability (degree to human understanding) and explanation(predictions). Lipton [3] defines \"interpretability\" from an ML algorithm perspective but finds the term is slippery. He argued that today's predictive (AI/ML) models are incapable of reason at all. Nevertheless, Lipton proposed two approaches to understanding XAI: the intrinsic (thing-in-itself) and the \"post-hoc\" methods. He concludes that understanding \"thing in itself\" is to interpret, while \"post-hoc\" is to explain.\nBenois-Pineau et al. [4] define \"explaining\" as the process of computing, while \"interpreting\" is the process of assigning meaning to the explanation,"}, {"title": "2. Literature Review", "content": "Many widely recognized XAI techniques can be classified as computational versus non-computational, statistical methods versus causal methods (under computational), global versus local (under statistical), and post-hoc versus a priori(Refer to Fig 4) if we assume XAI to be a subset of IAI from a top-down perspective. The meaning of \"post-hoc\" is to examine the result of the ML model after training [16] because we want to understand how the final ML model reaches its conclusions. Due to limited space, we only concentrate on eight XAI techniques marked in orange. We exclude the saliency map and sensitivity analysis from this study because we want to focus on the gradient-boosting machines (GBM) as an ML model. Besides the global (model-agnostic) post-hoc, we can also have global intrinsic models, such as LRP [29] and DTD [30] for neural networks. The meaning of intrinsic implies the model itself or opening a black box. In contrast, we can have extrinsic or causal models that can also be categorized locally and globally. Under the hood of the non-computational category, there are self-explanation scorecard (self-XSC) and stakeholder playbook (SPB) [32]. These XAI models are beyond the paper's scope."}, {"title": "2.2. XAI Techniques Briefing", "content": "The idea of Local Interpretable Model-agnostic Explanations or LIME [11] is to employ a local and simple surrogate model(fs) to replace the original and complex model (fm) known as local model-agnostic. The following equation can compute LIME:\n$\\xi(x) = arg min_{fsEF} L(f_m, f_s, \\pi_x) + \\Omega(f_s)$ (1)\nWhere \u00a7(x) is the error between fm (ML or target model) and fs (surrogate model) F is a class of potential explainable models, which $f_s \\in F$, \u03a9(fs) is the measurement of the complexity of fM. \u03c0\u03b1 defines a proximity measurement between an instance z and x as a locality around x.\nLIME is a very popular XAI tool. We can use it even as a debugging tool because it is simple and transparent. It can also be applied to many ML models. However, the quality of LIME depends on a surrogate model. The explainable results are often unstable. It gives rise to the Anchor model."}, {"title": "Anchor", "content": "Anchor is another local model-agnostic method presented by [12][13]. It aims to explain any black box model with high probability guarantees. The basic idea is to employ a decision rule (IF-THEN) for one or some instances while generalizing the rest. It does not aim to open the black box's architecture and understand the internal parameters of the model. Instead, it uses a similar approach as LIME to use high-precision rules called anchors for the target model. Thus, the anchor algorithm is universal and defined as follows:\n$E_{D(z|A)} [1_{f(x)=f(z)}] \\geq T, A(x) = 1$ (2)\nWhere x means the instance being explained, A(x): Anchor is a set of predictions. If A(x) = 1 when A is a sufficient condition for f(x) with a high probability. The function f(x) implies the classification model to be explained D(\u00b7|A) is the distribution of neighbours of a matching A. T specifies a precision threshold.\nComparing LIME, the Anchor approach provides the following advantages: 1.) The result is easier to comprehend due to IF-THEN rules; 2.) It can be applied to any model. It is less likely to underfit; 3.) It can also serve as a subset and cover some essential instances; 4.) It supports parallel computation. The disadvantages are 1.) The result heavily depends on the initial configuration; 2.) An outcome is too specific with low coverage; 3.) Building an anchor depends on many factors. It leads to the particular anchor of runtime varying widely; 4.) The explainable coverage is undefined in some domains.\nIn order to achieve the total coverage, we can select the Individual Conditional Expectations (ICE) curve showing an ML model profile because the ICE plot gives the output of N instances [18] for features. The ICE plot provides heterogeneous relationships. The disadvantage of the ICE plot is that it can only show one feature at a time. To overcome the limitation, we can leverage the Shapley value method [27]. The concept of this approach has its roots in game theory, particularly cooperative game theory [28]. The Shapley value only describes a very particular type of fairness that is equal based on the principle that all players deserve equal rights and opportunities.\nTo calculate a model's fairness, we can employ algorithmic fairness, which is one of the global techniques. It was proposed by Kozodoi et al. [19] with 11 fairness data metrics, including demographic, proportional, equalized odds, predictive rate, false positive, false negative rate, accuracy, negative"}, {"title": "predictive value, etc.", "content": "Given the limited space constraints, we only selected some metrics, such as accurate, precise, and predictive probability measurements. The advantages of accuracy and precision are that the prediction result can be intuitively simple. It improves the prediction results globally. However, it can be misleading in rare instances if the population size is immensely large, which explains the fairness regarding the population size.\nAnother global technique is the Partial Dependent Profile or Plot (PDP). It can demonstrate one or two features in the feature set S contributing to the final prediction result in marginal effects. The marginal effects mean that other features are excluded from the plot. The PDP can unveil the relationship [15] between the input feature and the output prediction, whether linear, monotonic, or complex. The mathematical relationship [31] can be expressed as follows:\n$f_s(x_s) = E_{x_c}[f(x_s,x_c)] = \\int f(x_s, x_c)dP(x_c)$ (3)\nWhere xs donates to a feature. xc stands for other features of the ML model f. The partial function fs is an estimated value resulting from the average training data. It is similar to the Monte Carlo simulation, which predicts possible outcomes from an uncertain event.\nThe advantages of the PDP plot are straightforward and easier to implement. If the selected feature is not correlated, the PDP can represent how this feature impacts the predicted result on average. The disadvantages are: 1.) the maximum number of features is two; 2.) Some PDPs do not include feature distribution, which can cause misinterpretation. 3.) If the features in C and S are correlated, the PDP result cannot be trusted.\nTo solve the correlation issue, we can employ Accumulate Local Effect Plots (ALE). The ALE [17] uses conditional distribution plus the effect of correlated features of interest rather than marginal distribution. The ALE concept can be formulated in the following equations:\n$f_{1,ALE}(z_1) = E_{f}[E_{f}(X_1, X_2)|X_1 = z_1]dz_2 \u2013 C_c$ (4)\n$ = \\int (P_{2/1}(X_2|z_1) f^{1}(Z_1, X_2)dx_2)dz_1 - C_c$ (5)\nWhere $f^{1}(x_1, x_2) = d(x_1, x_2)/dx_1$ represent the local effect of x\u2081 on f(\u00b7) at (X1, X2). Xmin,1 is the selected value near the lower bound of the effect support"}, {"title": "of p1(.).", "content": "Ce is the constant chosen to centre the plot vertically P2|1(X2|21) stands for conditional density probability. In essence, the ALE calculates the differences in predictions and only focuses on interesting features rather than all. The difference in predictions is a result of the feature's effects on all individual instances in a certain interval.\nThe advantages of the ALE are: 1) It still works if features are correlated; 2) The computation of the ALE is faster; 3) It is much easier and more transparent to explain results. The disadvantages of the ALE are: 1.) The result can be unstable at various intervals. There is no perfect solution to finding an ideal number of intervals; 2.) Unlike the PDP technique, we cannot use ICE as a complementary plot to check the ALE's heterogeneity in the feature effect; 3.) Although the second-order ALE estimates can present a changing stability across the feature space, it is not visible; 4.) Moreover, the second-order effect plot is hard to explain; 5.) Compared to the PDP, the ALE is much more complex in terms of implementation; 6.) The ALE can not exhibit more than two features.\nThe last technique is Variable Importance (VI) or Feature Importance (FI). It measures the importance of each variable or feature's contribution to the prediction result. We can permutate all variables or features and rank them in a list to visualize VI [14]. Breiman proposed permutation VI for the random forests algorithm [16]. The advantages of the VI are: 1.) It can be explained directly and efficiently; 2.) The VI provides an overview of all features. The disadvantages are 1.) If features are correlated, the interactive components between features cannot be added; 2.) The accuracy of the VI (or ranking order of VIs) heavily depends on the model's errors; and 3.) The VI is the result of the final model. All the above XAI techniques are also known as model-agnostic models that can be applied to any ML model."}, {"title": "3. Database and Experimental Setup", "content": "Prior to a series of XAI experiments, it is necessary to understand a dataset. We selected an open dataset regarding annual car insurance claims from Kaggle [20]. It comprises exactly 10,000 observations and 19 features. According to the data donor, most data is real, but the author changed some values. It is unclear which part of the data has been artificially modified. Furthermore, the dataset has 982 missing values in the \"credit score\" column"}, {"title": "3.2. Experimental Setup and Implementation", "content": "Once the data imputation was completed, we split the dataset into a 70/30 ratio. The large part is for training, and the smaller proportion is for testing. We can also split the dataset into 80/20 or 50/50 ratios. It depends on a given problem context. Chollet[37] suggested a 50/50 ratio for a time series problem because we try to predict the future given the past, not the reverse. We set the cross-validation value to five.\nThe basic idea of the experiments is to train some common ML models first, such as general linear model (GLM), random forests (RF), gradient boost machine (GBM) and extended gradient boosting machine (Xgbm) that"}, {"title": "4. Experiment Implementation, Results and Analysis", "content": "The experiment results are divided into three folds: 1.) Prediction results via GLM[23], RF, and GBM for comparison. 2.) Hyper-parameter searching for an optimizing solution via harnessing High-Performance Computing (or cloud computing) power. 3.) Explainable results by global and local X\u0391\u0399 techniques. By comparison, GBM appears to be the best model to fit with the dataset. However, the GBM result is not optimal. This leads to an optimization process via a hyperparameters search."}, {"title": "4.2. Optimization Results", "content": "The hyperparameter search begins with an initial random guess of the GBM model's parameters based on our intuition. These parameters include"}, {"title": "4.3. Explanation AI", "content": "The first XAI experiment is the variable importance (VI) also known as feature influence (FI). It is very straightforward. Each feature gain can be calculated based on the optimal Xgbm model. Next, we can"}, {"title": "The last XAI experiment", "content": "The last XAI experiment is the \"anchors\u201d technique, which uses a simple decision tree (IF-THEN) to explain the ML model. It means that the anchors use one or a few particular instances (or anchors) with decision rules to explain the ML model while generalizing to as many other instances as possible. The experiment uses five instances (anchors) to explain the ML model.  Notice that the selected case 5 only has 32.5% instances coverage, although the prediction precision is 100%, while case 1 can achieve 90.8% prediction with nearly 70% coverage."}, {"title": "All the above experiments", "content": "All the above experiments assume that features are not interactive with each other. However, this assumption could face a challenge. For example, the features of past accidents and annual mileage could be correlated because, intuitively, the higher the driving mileage, the more accidents could occur. We can verify our intuition or hypothesis with the ALE plot.  has approved that our intuition is right but only after the threshold level of annual mileage between 13,000 and 14,000. The decision of selecting which diagram to explain may rely on the researcher's interpretation and expertise. For this particular dataset, we only have 19 features. If the dataset has 100 or even 1000 features, the number of feature combinations could be significantly large. Consequently, we need a high-level abstraction or IAI to subjectively select the desired feature combination for XAI. It is apriori. That is, we want reason to be justified."}, {"title": "5. Results Discussion", "content": "The paper's primary research question is whether the XAI and IAI differ. If so, why is it so important? How can we distinguish between them? We solve these problems in three phases: data processing, ML modelling, and analysis of XAI techniques. Each phase has many subsequent questions or high-level abstractions for the goal of XAI. The approach is similar to Biecek and Burzykowski's [26] explanatory method. We can summarize the high-level abstraction into a 3X3 matrix that consists of 1) data, 2) ML modelling,"}, {"title": "For the data processing phase,", "content": "For the data processing phase, we performed a data imputation experiment. We believe the imputation data method is better than the omitted missing data approach for XAI. Our belief is a part of our intuition. We can ask many high-level reflected questions to interpret the given dataset for the ML problem.\nDuring the ML phase, three ML models are generated: GLM, RF, and GBM with its extension of GBM (or Xgbm) for comparison of prediction accuracy . We can also adopt a transformer model (deep learning architecture) for prediction. The choice of which ML model to use depends on the expected result of the prediction and subsequent selection of XAI methods. (e.g. global intrinsic or global post-hoc). It determines our mindset for a given dataset. When we build an ML model to fit with a"}, {"title": "dataset,", "content": "dataset, we must consider at least five issues for the future XAI: 1.) Which ML model should be selected for the context of the ML problem? 2.) What is the dataset ratio for training, testing, and validation? 3.) Which feature or features do I care about the most? 4.) How can the ML model be optimized? All these questions are within a framework of a meta-hyperparameter search that defines \u03a7\u0391\u0399 criteria.\nIn general, the ML model may provide some clues for adopting which XAI technique. For example, LRP and DTD are usually applied to neural networks. Some XAI techniques are universal and can be applied to any type of black box, such as LIME. As we showed above, the VI and PDP plots are often unstable. The variables' ranking order often changes if the RMSE is drifting. It may indicate that some features are correlated. The ALE plot is an alternative to the PDP. We show four pairs of correlation features in If we want to focus on the local explanation, LIME, ICE, Shapley value, and Anchor techniques offer solutions for XAI. However, the selection of which XAI technique depends on our mindset or IAI regarding the problem context."}, {"title": "6. Conclusion and Future Works", "content": "We argue that XAI and IAI differ. The demarcation of XAI and IAI is the duality of the reason because whether we want to explain or interpret AI, we must provide some reasons. However, reason has its duality, also known as outward and inward reasoning. When we reason outwards, we want a reason to make sense, which is governed by the law of nature, logic, algorithms, rationality, and dataset. When we reason inwards, we want reason to be happy. It is governed by the law of the heart that eventually leads to ethics, belief, and intuition or mindset. Our E2E explanation process demonstrates that many decision points or criteria of XAI are required based on our mindset and interaction with the XAI process. From a programming perspective, IAI is similar to one level-up of abstraction, while XAI is more like detailed commands that can get things done.\nThe implications of this demarcation clarify the notions between XA\u0399 and IAI. This clarification can help many practitioners and policymakers move beyond simple algorithmic explanations. This work's main contribution highlights the demarcation through various empirical experiments.\nThe limitation is that we are unable to test deep neural networks-related \u03a7\u0391\u0399 tools, such as LRP and DTD, as well as the causal methods for XAI."}]}