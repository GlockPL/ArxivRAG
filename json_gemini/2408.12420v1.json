{"title": "Dataset | Mindset = Explainable AI |\nInterpretable AI", "authors": ["CaesarWu", "Rajkumar Buyya", "YuanFangLi", "PascalBouvrya"], "abstract": "We often use \"explainable\" Artificial Intelligence (XAI)\" and \"interpretable\nAI (IAI)\" interchangeably when we apply various XAI tools for a given\ndataset to explain the reasons that underpin machine learning (ML) out-\nputs. However, these notions can sometimes be confusing because interpre-\ntation often has a subjective connotation, while explanations lean towards\nobjective facts. We argue that XAI is a subset of IAI. The concept of IAI\nis beyond the sphere of a dataset. It includes the domain of a mindset. At\nthe core of this ambiguity is the duality of reasons, in which we can reason\neither outwards or inwards. When directed outwards, we want the reasons\nto make sense through the laws of nature. When turned inwards, we want\nthe reasons to be happy, guided by the laws of the heart. While XAI and\nIAI share reason as the common notion for the goal of transparency, clarity,\nfairness, reliability, and accountability in the context of ethical AI and trust-\nworthy AI (TAI), their differences lie in that XAI emphasizes the post-hoc\nanalysis of a dataset, and IAI requires a priori mindset of abstraction. This\nhypothesis can be proved by empirical experiments based on an open dataset\nand harnessed by High-Performance Computing (HPC). The demarcation of\nXAI and IAI is indispensable because it would be impossible to determine\nregulatory policies for many AI applications, especially in healthcare, hu-\nman resources, banking, and finance. We aim to clarify these notions and\nlay the foundation of XAI, IAI, EAI, and TAI for many practitioners and\npolicymakers in future AI applications and research.", "sections": [{"title": "1. Introduction", "content": "The notion of eXplainable Artificial Intelligence (XAI) suggests the ability\nto clarify AI. It offers reasons, evidence, and contexts for the Artificial Intel-\nligence/Machine Learning (AI/ML) results. It answers questions of \"why\"\nand \"how\". \"Why\u201d offers reasons, and \"how\" explains how the AI arrived\nat the result supported by reasons. It is a process of making sense of the\nAI/ML outputs.\nOn the other hand, Interpretable AI (IAI) also provides reasons, but it\nasks the question, \"Is the reason reasonable?\u201d If we consider IAI to be a\nhigh-level abstraction of XAI, IAI is the meta-XAI or criteria of XAI for\nsatisfaction. The IAI process requires our mindset. Molnar [33] argued that\nour mindset is a perspective of the world when we construct a learning model.\nIn computer programming, \"interpretation\u201d means interactively converting a\nhigh-level language into machine codes line-by-line. A programming language\nis a form of communication between programmers and other people. It also\nhelps programmers organize and describe ideas for a computer. Similarly, IAI\ncan be considered a form of communication among humans, while XAI is an\nengagement process of our mindset for a given dataset within a particular\nproblem context.\nGuidotti et al.[1] argued that IAI is needed for AI models, while \u03a7\u0391\u0399\naims for prediction results. Miller[2] attempted to define XAI from different\nperspectives, including philosophy, cognitive psychology/science, and social\npsychology. Miller did not differentiate between interpretability (XAI) and\nexplainability (IAI). Molnar[16] also prefers to use both terms interchange-\nably but differentiates between explainability/interpretability (degree to hu-\nman understanding) and explanation(predictions). Lipton [3] defines \"inter-\npretability\" from an ML algorithm perspective but finds the term is slippery.\nHe argued that today's predictive (AI/ML) models are incapable of reason\nat all. Nevertheless, Lipton proposed two approaches to understanding XAI:\nthe intrinsic (thing-in-itself) and the \"post-hoc\" methods. He concludes that\nunderstanding \"thing in itself\" is to interpret, while \"post-hoc\" is to explain.\nBenois-Pineau et al. [4] define \"explaining\" as the process of computing,\nwhile \"interpreting\" is the process of assigning meaning to the explanation,"}, {"title": "2. Literature Review", "content": "Many widely recognized XAI techniques can be classified as computa-\ntional versus non-computational, statistical methods versus causal methods\n(under computational), global versus local (under statistical), and post-hoc\nversus a priori(Refer to Fig 4) if we assume XAI to be a subset of IAI from\na top-down perspective. The meaning of \"post-hoc\" is to examine the result\nof the ML model after training [16] because we want to understand how the\nfinal ML model reaches its conclusions. Due to limited space, we only con-\ncentrate on eight XAI techniques marked in orange. We exclude the saliency\nmap and sensitivity analysis from this study because we want to focus on\nthe gradient-boosting machines (GBM) as an ML model. Besides the global\n(model-agnostic) post-hoc, we can also have global intrinsic models, such as\nLRP [29] and DTD [30] for neural networks. The meaning of intrinsic implies\nthe model itself or opening a black box. In contrast, we can have extrinsic or\ncausal models that can also be categorized locally and globally. Under the\nhood of the non-computational category, there are self-explanation scorecard\n(self-XSC) and stakeholder playbook (SPB) [32]. These XAI models are\nbeyond the paper's scope."}, {"title": "2.2. XAI Techniques Briefing", "content": "The idea of Local Interpretable Model-agnostic Explanations or LIME\n[11] is to employ a local and simple surrogate model(fs) to replace the orig-\ninal and complex model (fm) known as local model-agnostic. The following\nequation can compute LIME:\n$\\xi(x) = arg min_{f_s \\in F}L(f_m, f_s, \\pi_x) + \\Omega(f_s)$   (1)\nWhere \u00a7(x) is the error between fm (ML or target model) and fs (sur-\nrogate model) F is a class of potential explainable models, which $f_s \\in F$,\n$\\Omega(f_s)$ is the measurement of the complexity of fM. $\\pi_x$ defines a proximity\nmeasurement between an instance z and x as a locality around x.\nLIME is a very popular XAI tool. We can use it even as a debugging\ntool because it is simple and transparent. It can also be applied to many ML\nmodels. However, the quality of LIME depends on a surrogate model. The\nexplainable results are often unstable. It gives rise to the Anchor model."}, {"title": "3. Database and Experimental Setup", "content": "Prior to a series of XAI experiments, it is necessary to understand a\ndataset. We selected an open dataset regarding annual car insurance claims\nfrom Kaggle [20]. It comprises exactly 10,000 observations and 19 features.\nAccording to the data donor, most data is real, but the author changed some\nvalues. It is unclear which part of the data has been artificially modified.\nFurthermore, the dataset has 982 missing values in the \"credit score\" column"}, {"title": "3.2. Experimental Setup and Implementation", "content": "Once the data imputation was completed, we split the dataset into a\n70/30 ratio. The large part is for training, and the smaller proportion is for\ntesting. We can also split the dataset into 80/20 or 50/50 ratios. It depends\non a given problem context. Chollet[37] suggested a 50/50 ratio for a time\nseries problem because we try to predict the future given the past, not the\nreverse. We set the cross-validation value to five.\nThe basic idea of the experiments is to train some common ML models\nfirst, such as general linear model (GLM), random forests (RF), gradient\nboost machine (GBM) and extended gradient boosting machine (Xgbm) that"}, {"title": "4. Experiment Implementation, Results and Analysis", "content": "The experiment results are divided into three folds: 1.) Prediction results\nvia GLM[23], RF, and GBM for comparison. 2.) Hyper-parameter searching\nfor an optimizing solution via harnessing High-Performance Computing (or\ncloud computing) power. 3.) Explainable results by global and local X\u0391\u0399\ntechniques. By comparison, GBM appears to be the best model to fit with\nthe dataset. (See Figure 6) However, the GBM result is not optimal. This\nleads to an optimization process via a hyperparameters search."}, {"title": "4.2. Optimization Results", "content": "The hyperparameter search begins with an initial random guess of the\nGBM model's parameters based on our intuition. These parameters include"}, {"title": "4.3. Explanation AI", "content": "The first XAI experiment is the variable importance (VI) also known as\nfeature influence (FI). It is very straightforward. Each feature gain can be\ncalculated based on the optimal Xgbm model. (Figure 7). Next, we can"}, {"title": "5. Results Discussion", "content": "The paper's primary research question is whether the XAI and IAI differ.\nIf so, why is it so important? How can we distinguish between them? We\nsolve these problems in three phases: data processing, ML modelling, and\nanalysis of XAI techniques. Each phase has many subsequent questions or\nhigh-level abstractions for the goal of XAI. The approach is similar to Biecek\nand Burzykowski's [26] explanatory method. We can summarize the high-\nlevel abstraction into a 3X3 matrix that consists of 1) data, 2) ML modelling,"}, {"title": "6. Conclusion and Future Works", "content": "We argue that XAI and IAI differ. The demarcation of XAI and IAI is\nthe duality of the reason because whether we want to explain or interpret AI,\nwe must provide some reasons. However, reason has its duality, also known\nas outward and inward reasoning. When we reason outwards, we want a rea-\nson to make sense, which is governed by the law of nature, logic, algorithms,\nrationality, and dataset. When we reason inwards, we want reason to be\nhappy. It is governed by the law of the heart that eventually leads to ethics,\nbelief, and intuition or mindset. Our E2E explanation process demonstrates\nthat many decision points or criteria of XAI are required based on our mind-\nset and interaction with the XAI process. From a programming perspective,\nIAI is similar to one level-up of abstraction, while XAI is more like detailed\ncommands that can get things done.\nThe implications of this demarcation clarify the notions between XA\u0399\nand IAI. This clarification can help many practitioners and policymakers\nmove beyond simple algorithmic explanations. This work's main contribution\nhighlights the demarcation through various empirical experiments.\nThe limitation is that we are unable to test deep neural networks-related\n\u03a7\u0391\u0399 tools, such as LRP and DTD, as well as the causal methods for XAI."}]}