{"title": "On the Transfer of Knowledge in Quantum Algorithms", "authors": ["Esther Villar-Rodriguez", "Eneko Osaba", "Izaskun Oregi", "Sebasti\u00e1n V. Romero", "Juli\u00e1n Ferreiro-V\u00e9lez"], "abstract": "The field of quantum computing is generating significant anticipation within the scientific and industrial communities due to its potential to revolutionize computing paradigms. Recognizing this potential, this paper explores the integration of transfer of knowledge techniques, traditionally used in classical artificial intelligence, into quantum computing. We present a comprehensive classification of the transfer models, focusing on Transfer Learning and Transfer Optimization. Additionally, we analyze relevant schemes in quantum computing that can benefit from knowledge sharing, and we delve into the potential synergies, supported by theoretical insights and initial experimental results. Our findings suggest that leveraging the transfer of knowledge can enhance the efficiency and effectiveness of quantum algorithms, particularly in the context of hybrid solvers. This approach not only accelerates the optimization process but also reduces the computational burden on quantum processors, making it a valuable tool for advancing quantum computing technologies.", "sections": [{"title": "I. INTRODUCTION", "content": "Traditionally, in the field of Artificial Intelligence (AI), problems have been addressed individually using methods that do not rely on any preliminary knowledge. This approach has proven effective in many real-world scenarios [1]. However, the increasing complexity of contemporary problems, driven by digital transformation, has created an ideal environment for researchers to explore alternative resolution paradigms.\nIn this context, AI is now going beyond basic reasoning, striving for genuine learning in its quest to reach the next level: Strong AI [2], an intelligence indistinguishable from the human mind. This human-level intelligence currently rests on two main pillars: (1) augmenting processing capacities based on human brain-inspired hardware and software, referred to as neuromorphic computing [3], to deliver skills closer to human cognition by emulating the nervous system's functioning; and (2) inferring complex relational knowledge as well as accepting, as input, more enriched data representations such as ontologies or graphs. Nevertheless, implementing real intelligence demands progress not only in knowledge acquisition and methods to exploit it but also in knowledge retention to become efficient in information processing [4].\nThis leads us inevitably to the reuse of knowledge: How much new information is truly novel? Are machines creating knowledge or simply solving complex mathematical problems? This is where the discipline that leverages previous knowledge to accelerate and enhance the quality of new knowledge comes into play: Transfer Learning (TL, [5]) in the field of Machine Learning (ML) and its counterpart in Optimization, known as Transfer Optimization (TO, [6]).\nOn this basis, and in both cases, the transfer of knowledge (ToK) is usually described as a mechanism to exploit the knowledge gained from completing previous tasks when facing new ones. However, this definition is not exhaustive. TL and TO also rely on multitasking schemes where all solvers are simultaneously running [7], [8], deriving and sharing knowledge in parallel. Whatever the strategy, transfer is likely to become the cornerstone of more efficient processing.\nIn turn, the field of Quantum Computing (QC, [9]) is raising great expectations in the scientific and industrial community. The potential benefits offered by this paradigm are intended to change the way computing is conceived. Nevertheless, QC is undergoing a slow democratization process [10]. Although democratization in quantum technology is often assumed, as there are increasing activities geared towards providing access through the cloud and supporting this access with complementary training, the real democratization of reaching more stakeholders with devices freely available is halfway through. In fact, the cost of quantum hardware is currently far from insubstantial. Extensive experimentation or industrial deployments entail huge expenses. Here, efficiency becomes even more urgent [11].\nAdditionally, new research avenues will coexist with the classical AI for several years due to compelling reasons. However, this coexistence presents an opportunity [12]. The advancements made by classical AI can now assist in providing not only the foundation for hybrid methods or innovative strategies but also valuable lessons for the quantum community. The potential for knowledge transfer in this context exemplifies the applicability of many classical transfer schemes. Furthermore, knowledge transfer in QC could be highly effective if we view hybridization as a bridge for knowledge exchange between mutually enriching classical AI and quantum approaches [13], [14]."}, {"title": "II. FUNDAMENTALS ON TRANSFER OF KNOWLEDGE", "content": "Knowledge transfer involves formulating principles to share knowledge by leveraging computed information for similar problems. These principles act as a bridge between what has already been learned or tested and what is currently being explored.\nAs mentioned in the introduction, two paradigms have principally arisen regarding the AI cores: TL in Machine Learning and TO in Optimization. Regardless of the discipline, there are three main research questions to be answered [5]:\n1) When to transfer, distinguishing situations that are advantageous from those that are counterproductive.\n2) What knowledge should be transferred between domains or tasks.\n3) How to transfer, i.e., the strategy to develop ToK.\nThe following three subsections (II-A, II-B, and II-C), elaborate on the core principles related to these questions. Finally, Subsection II-D presents some of the most recurrent trends in classical computation regarding TO and TL.\nAs a last mention in this introduction to fundamentals and as a preliminary for the subsections to come, let us define the two main concepts on which the theory of ToK is formulated in this work:\n\u2022 A task refers to the specific objective or problem that needs to be solved and its characterization, i.e., the elements that are initialized or formulated explicitly for the objective.\n\u2022 A domain refers to the environment or delimited space in which the task is mapped and the solving procedure will evolve."}, {"title": "A. When to transfer", "content": "Let us assume that we had built a deep learning model to classify people as either joyful or sad, and we were considering adding new mood categories. It would be perfectly sensible to exploit the knowledge extracted in the initial run. In fact, if we examined the first filters of the neural network, we would discover patterns that define postural deviations in facial components. This represents cross-domain knowledge in computer vision [21].\nMultiple examples of TL have been documented in the literature, particularly in the fields of Image Recognition and Natural Language Processing [22], [23]. However, not all similarities among tasks and domains are immediately apparent. Some similarities may be subtle and require preprocessing to become evident.\nWhen should we devote time to discovering similarities? When aware or having the intuition (drawn from experience) that a task will appear in the future in a similar form, with slight variations, or when there is a core of knowledge that remains valid across use cases, it is sensible to consider ToK. Violating this assumption, i.e., transferring knowledge when neither the domain nor the task is similar, will likely lead to encountering negative transfer between the source and target tasks (origin and destination of the transfer) [24]. As stated in [5]:\n\"When the source domain and target domain are not related to each other, brute-force transfer may be unsuccessful. In the worst case, it may even hurt the performance of learning in the target domain, a situation which is often referred to as negative transfer.\"\nBearing this in mind, the ToK may assist, particularly:\n\u2022 When facing very complex search spaces in optimization problems [25],\n\u2022 When there is a risk of overfitting, the model generalizes poorly, and the data set is relatively small in ML tasks (typically due to annotation expense or privacy concerns) [26], and\n\u2022 When speed in processing is a must-have [27].\nHaving said this, literacy in ToK has postulated some conditions that, in case of being met, turn the problem to be solved into a good candidate for knowledge transfer.\nIn ML, and given the definitions provided in Table I, when is proposed under this categorization [5]:\n\u2022 Inductive transfer learning: Given a source domain $D_s$ and a corresponding learning task $T_s$, a target domain $D_T$ and a learning task $T_T$, inductive transfer learning aims to improve the learning of the target predictive function $f(\\cdot; \\theta)_T$ in $D_T$ using the knowledge of $D_s$ and $T_s$ given that $T_s \\neq T_T$. A typical example of inductive transfer is sharing the feature representation layers in a neural network for two tasks in a multitask setting: a regression and a classification. This involves one backbone with two prediction heads.\n\u2022 Transductive transfer learning: Given a source domain $D_s$ and a corresponding learning task $T_s$, and a target domain $D_T$ and a corresponding learning task $T_T$, transductive transfer learning aims to improve the learning of the target predictive function $f(\\cdot; \\theta)_T$ in $D_T$ using the knowledge from $D_s$ and $T_s$, provided that $T_s = T_T$ and $D_s \\neq D_T$. Transductive learning is most commonly exemplified in situations where there is a large amount of unlabeled data in $D_T$, which hinders the training of $f(\\cdot; \\theta)_T$ with data from $D_T$ and also the retraining of a pre-trained predictor $f(\\cdot; \\theta)_s$ to adjust to the new domain $D_T$. To circumvent this situation, domain adaptation (DA) techniques are commonly applied to achieve a domain translation, i.e., a meaningful correspondence or alignment, between $D_s$ and $D_T$.\n\u2022 Unsupervised transfer learning: Given a source domain $D_s$ with a learning task $T_s$, a target domain $D_T$ and a corresponding learning task $T_T$, unsupervised transfer learning aims to help improve the learning of the target predictive function in $D_T$ using the knowledge in $D_s$ and $T_s$, where $T_s \\neq T_T$ and $Y_s$ and $Y_T$ are not available. For instance, assuming that somehow datasets from $D_s$ and $D_T$ share a common underlying structure that can be leveraged to perform transfer learning in a clustering or dimensionality reduction task."}, {"title": "B. What to transfer", "content": "The transversal knowledge, the gold, that boosts the learning or solving procedure and improves performance is categorized\n\u2022 Instance or individual transfer: Bringing samples or individuals (i.e. a subset of $X$) from $D_s$ with some adjustment in the transfer, via re-weighting or importance sampling in ML, or mapping the encodings in optimization.\n\u2022 Feature or search representation transfer: Inferring a (sometimes low-dimensional) common feature representation to be shared across tasks in ML or a common search space representation in optimization, mainly to accommodate multitasking schemes. This involves transferring $X$ in both cases.\n\u2022 Parameter transfer: Assigning the hyperparameters $\\theta$ in $f_s(\\cdot; \\theta)$ to the model $f_T(\\cdot; \\theta)$ in ML or helping set the prior distributions of them in both ML and optimization algorithms.\n\u2022 Relational knowledge. The motivation lies in the fact that there are relationships among the data $X_s$ that can be exploited in $T_T$ as the relational structure holds or there exists a common sub-structure [28]."}, {"title": "C. How to transfer", "content": "Once the rationale and main concepts are defined, it is time to design the strategy for knowledge sharing. In this regard, two main conceptualizations can be distinguished:\n\u2022 Sequential Transfer: Task $T_s$ precedes task $T_T$, meaning that the knowledge created in $T_s$ is subsequently imported into $T_T$.\n\u2022 Multitasking: Knowledge is created and shared during the parallel and simultaneous solving of a set of tasks. Therefore, multitasking relies on omnidirectional transfer, with tasks interchangeably playing the roles of source and target. In the field of optimization, when the pool of tasks represents different formulations of the same problem, this approach is known as Multiform Optimization [6]."}, {"title": "D. Current trends in classic computation", "content": "Once explained the main concepts of ToK, graphically represented, the following paragraphs will briefly outline the trends followed by the scientific community in both TL and TO fields. In this context, many variants can be crafted by blending the previously mentioned approaches and strategies."}, {"title": "III. QUANTUM ALGORITHMS: DISCOVERING POTENTIAL SYNERGIES", "content": "At the present time, there are two main accessible quantum computing paradigms: quantum annealing (QA) and universal gate-based quantum computing.\nFirst proposed in 1988, QA [58]\u2013[61] encodes problems using the Ising model, whose corresponding Hamiltonian can be used to map optimization problems or obtain lowest-energy samples [62]\u2013[65]. the QA process begins by representing the problem as an energy landscape. Initially, the quantum system is prepared in a superposition of all possible solutions, representing a high-energy state. As the system evolves, the Hamiltonian is adjusted, allowing the system to explore the energy landscape. The goal is to guide the system towards the minimum energy state, which corresponds to the optimal solution of the problem. Advances in quantum technologies have contributed to the construction of intermediate-scale Quantum Annealers for programmable use. In this context, the Canadian company D-Wave Systems has developed commercial devices based on QA, being the main provider of this technology. To date, these systems are the most widely used for optimization problems approached from a quantum perspective, with interesting research published in fields such as finance [66], logistics [67], and industry [68]. Currently, the most advanced computer from D-Wave Systems, called Advantage_system6.4, consists of 5,612 superconducting qubits arranged in a Pegasus topology [69]. However, the launch of a new system, named Advantage2, which will feature 7,430 qubits arranged in a Zephyr topology is scheduled for late 2025 [70].\nIn contrast to QA, gate-based quantum computing, inspired by classical logic gates, operates by driving the quantum system within Hilbert space through a finite sequence of unitary gates, enabling universal computation. This approach requires precise control over quantum states, necessitating high-fidelity gates to implement complex and deep algorithms effectively. However, current hardware limitations restrict the range of feasible algorithms, making fault-tolerant algorithms like Shor's and Grover's, which demand high-precision control, currently impractical.\nTo address these challenges, variational quantum algorithms (VQAs) have gained popularity in recent years [71]. These hybrid algorithms combine classical and quantum resources to exploit the quantum Hilbert space by parameterizing quantum gates. The parameters are optimized classically by minimizing a predefined cost function. By incorporating classical optimization into the process, hybrid algorithms enhance the robustness of the protocol. The classical optimization absorbs imperfections in the minimization process, making these algorithms more resilient to hardware-induced errors. Concrete examples of VQA are the Quantum Approximation Optimization Algorithm (QAOA) [72] and the Variational Quantum Eigensolver (VQE) [73]. The former was introduced to return approximate solutions for combinatorial optimization problems, whereas the latter was originally conceived to find the ground state energy of a given Hamiltonian, a central problem in quantum chemistry. In a more technical way:\n\u2022 QAOA [72] aims to solve combinatorial optimization problems encoded in the ground state of the Hamiltonian $H_c$. Starting from the ground state $|\\Psi_0\\rangle$ of the mixing Hamiltonian $H_B$, a parameterized circuit is applied to drive the system towards the target state $|\\gamma, \\beta \\rangle) = U_B(\\gamma_p)U_C(\\beta_p)...U_B(\\gamma_1)U_C(\\beta_1)|\\Psi_0\\rangle$, where $p$ denotes the algorithm's depth . Parameters $\\beta = {\\beta_i}_{i=1}^{p}$ and $\\gamma = {\\gamma_i}_{i=1}^{p}$ are tuned using classical optimization subroutines, maximizing the cost function $\\langle H_c \\rangle_{\\gamma, \\beta} = \\langle (\\gamma, \\beta)|H_C|(\\gamma, \\beta) \\rangle$.\n\u2022 VQE [73], aims to find the Hamiltonian ground state by driving the system from an easy-to-prepare initial state $|\\Psi_0\\rangle$ to the target state $|\\Psi(\\theta)) = U(\\theta) |\\Psi_0\\rangle$, where $U(\\theta)$ is the problem-specific ansatz . The parameters $\\theta = {\\theta_i}_{i=1}^{N}$ are optimized by minimizing the energy expectation value of the Hamiltonian, defined as $\\langle H \\rangle = \\langle \\Psi(\\theta)|H|\\Psi(\\theta) \\rangle$, thought classical optimization techniques."}, {"title": "A. What to start with? Preliminary experimentation", "content": "As part of the present research, we have conducted three different experiments to give the reader a slight idea of the potential of ToK in the field of QC. More specifically, we explore the materialization of ToK through three transfer designs to showcase diverse strategies for the three main aforementioned quantum algorithms: QA, QAOA, and VQE.\nTo facilitate reproducibility, Table IV summarizes the description of the three use cases (UC), highlighting their principal characteristics: the solving scheme as well as the what and how to transfer. Furthermore, the parameterization and the results obtained are also described. We now dive into each UC for the sake of completeness.\nUC1: QA-Reverse Annealing, Individual-based, Sequential Transfer. Framed in the QA paradigm, UC1 explores the efficiency of the reverse annealing (RA, [74]) for conducting sequential transfer of knowledge.\nIn a nutshell, RA is a method that allows the annealing process to go backward from a given state before moving forward to a new state. This involves performing a local search to refine a classical state. For individual-based transfer of knowledge to be utilized via RA, this classical state must be a solution obtained from a prior solver. Specifically, the main objective of this UC is to analyze the contribution and sensitivity of feeding RA with results obtained from solving similar problems.\nTo carry out UC1, we have utilized the well-known Maximum Cut problem (MCP, [75]), a problem widely employed in QC because of its simplicity. Specifically, the MCP is a graph partitioning problem whose objective is to find a cut such that the number of edges lying between the two subsets is maximized. In this case, the main instance to process is the 50-node instance available in the QOPTLib library [76].\nTo conduct the experimentation, a pool of similar instances, named MaxCut_50_X, is generated by eliminating a set of randomly selected X% edges from MaxCut_50. Next, all MaxCut_50_X instances are solved 10 times through forward annealing using D-Wave's Advantage_System6.4 as a quantum device. Eventually, RA is executed, taking the best solution of each MaxCut_50_X as the classical input to solve the MaxCut_50 instance.\nIn summary, in UC1 we explore how reusing the solutions (or individuals, as seen in Table III) of MaxCut_50_X through the RA paradigm can impact the resolution of MaxCut_50. More specifically, referring to , the knowledge is injected in the annealing step. The results of the objective function for each MaxCut_50-MaxCut_50_X combination after 10 independent runs are depicted and summarized in Table IV. To properly understand the results, it should be noted that the objective function used is the sum of the weights of the cut edges, so the bigger the value, the better the solution.\nIn a nutshell, analyzing both and Table IV, we can clearly see how the ToK among related tasks benefits the solving of MaxCut_50. In fact, the resolution of the problem improves even when using the instance that is the most different in terms of composition, which is MaxCut_50_50. Also, it is interesting to observe how the quality of the results improves significantly as the similarity of the problems increases. Accordingly, transferring the instance MaxCut_50_7 into instance MaxCut_50 yields better results than any other attempt.\nFinally, given this setup of experimentation and as described in Table II, UC1 is an example of ToK between partially different tasks with $T_s \\sim T_T$ and $X_s \\neq X_T$ in domains that completelly overlap with $D_s \\sim D_T$.\nUC2: QAOA, Individual-based multitasking In the second example, we explore the influence of applying the TO principles to QAOA in a multitasking scenario. Specifically, we address MCP by simultaneously optimizing a set of $k$ graphs. Our goal is to analyze the impact of ToK on both individual and overall performance. Thus, we have conceived an approach called Transfer-QAOA, which takes advantage of the hybrid nature of QAOA, exploiting the classical optimization layer to facilitate information exchange between graphs without compromising the performance of the quantum algorithm.\nTransfer-QAOA divides the entire optimization procedure into smaller $N_r$ sub-blocks, each functioning as a standard QAOA protocol. Thus, the knowledge transfer subroutine is executed between sub-blocks , and the information is introduced during state manipulation . The subroutine evaluates the costs of the different $k$ graphs within the optimization parameters of the other graphs to optimize. Therefore, the parameters are shared among tasks,\nIn summary, UC2 is a case of ToK between partially different tasks with $X_T \\sim X_s$ and $X_T \\neq X_s$ in domains that partially overlap $X_T \\sim X_s$ and $X_T \\neq X_s$.\nUC3: VQE, Parameter-based, Sequential Transfer. In the last of the practical examples, we study the impact of transferring previously obtained knowledge on computing the ground state energy of the $H_2$ molecule. This process is carried out sequentially for different bond lengths between two hydrogen atoms, with $r \\in [0.2, 2.85]$, \u00c5 in steps of $dr = 0.05$ \u00c5. To this end, we apply a VQE, where the State Initialization stage for a new instance is fed with the optimal, or best-found, ansatz parameters of the previous instance as the initial guess. Thus, the instance at a bond length $r + dr$ is initialized with the optimal parameters returned at a bond length $r$."}, {"title": "B. Preliminary Insights and Reflections", "content": "As a general note on the three use cases shown in the previous subsection, we can conclude that the ToK has proven beneficial in all the tests conducted. It is particularly interesting to mention that this benefit has materialized in different forms:\n\u2022 In UC1, the use of previous knowledge enhances the performance of the quantum annealer, leading to higher quality results and lower uncertainty in the solver's performance.\n\u2022 In UC2, we have demonstrated how ToK can be beneficial even in a multitasking environment by using the well-known QAOA. In this case, efficiency is represented by an improvement in the success rate.\n\u2022 In UC3, utilizing previously acquired experience improves the computational efficiency of the solver, enabling it to find the optimal solution in less time.\nFinally, the experimentation conducted has demonstrated that:\n\u2022 The sharing of knowledge can be beneficial in both computational paradigms (QA and gate-based models).\n\u2022 Both sequential and multitasking schemes can also be applied in quantum computing, allowing knowledge to be injected at different stages of execution.\n\u2022 The material to be shared can vary in nature, such as circuit parameters or a complete solution to a problem."}, {"title": "IV. CHALLENGES AND OPPORTUNITIES OF TRANSFER OF KNOWLEDGE IN QC", "content": "As previously discussed in this work, ToK is a mechanism to reduce the complexity and the cost of training classical ML models or solving optimization problems from scratch. Based on the main principles outlined in Section Section II, we present several scenarios where ToK could be beneficial in QC."}, {"title": "A. When to Transfer", "content": "The question of when to transfer in QC parallels that in classical computing: when it provides significant advantages in terms of algorithm performance, or when it reduces the need for extensive computation in tasks like the training of ML algorithms. However, it is important to note that QC algorithms involve different steps -depending on the paradigm-, such as state preparation, ansatz preparation and initialization, and optimization, where we can asses the potential usefulness of ToK. Moreover, for mid- or large-sized problems, extra tasks are devoted to circumventing the limited capacity of current QPUs by decomposing big problems into smaller sub-problems."}, {"title": "B. How and Point of transfer", "content": "When considering what to transfer and how, the answer becomes more extensive. However, we can again leverage techniques from classical TL and TO and adapt them to quantum computing. Regarding the pipeline of quantum algorithms mentioned above, the following are some interesting lines of investigation to pursue in addition to the ideas conducted in Section III.\n\u2022 Problem conceptualization: As previously mentioned, problem decomposition can be valuable when addressing real-world use cases in current QPUs. In this context, two decomposition techniques can be studied: i) decompose a problem into reusable pieces. Some problem formulations are, by their nature, a sum of independent components to be solved. This is the case of the Bin Packing Problem where each bin loading is, per se, a small piece of the overall problem that, once optimized, is a potential template to reuse for future bins. ii) Decompose a problem into similar pieces. If symmetries or approximated symmetries are identified within a complex problem, the number of run-per-subproblem may be reduced by solving one component and adapt it to the rest of the equivalents.\n\u2022 State initialization (or data embedding): In classical neural networks, ToK typically involves freezing the initial layers of a pre-trained deep neural network, as these layers capture general features. Drawing from this concept, the authors in [80] applied a pre-trained classical neural network to embed high-dimensional data into a quantum processor, followed by a variational quantum circuit to solve the target machine learning task. This method efficiently pre-processes the data by embedding a carefully selected set of informative features into the quantum processor, bypassing the need for retraining or direct access to the raw data.\n\u2022 Ansatz preparation and/or initialization: In kernel-based methods such as Support Vector Machines, ToK may involve reusing learned kernel functions, effectively transferring representations from one task to another. In the context of QC, kernels are defined through quantum embedding kernels, which map classical data into a quantum feature space to measure similarities between data points. To define efficient quantum embeddings (i.e., an appropriate ansatz) for classification purposes, the authors in [81] employed the kernel target alignment concept. This concept evaluates how well a kernel function captures the true relationships in the data by comparing it to an ideal target kernel that reflects the desired classification or prediction. Although this kernel definition is highly effective, training the parameterized circuit is resource-intensive. Therefore, transferring the embedded kernel from a source task can significantly reduce the training complexity of the target alignment.\n\u2022 Accelerating the optimization process: Accelerating the optimization process of hybrid algorithms can be incredibly beneficial. Depending on the optimizer used, another way to refine the optimization step is by decomposing the search space into subpopulations, thus speeding up the search process. It is important to note that VQAs can suffer from barren plateaus, a phenomenon where the gradient of the loss function diminishes exponentially as the size of the quantum system increases. This results in training stagnation because the gradients become too small for the optimizer to effectively update the parameters, leading to slow or no progress. TL can be a useful approach to mitigate barren plateaus in QC. Instead of starting with a large quantum system prone to barren plateaus, one could pre-train the quantum circuit on smaller systems or simpler tasks. The parameters from this pre-trained circuit can then be used as a starting point for the larger system, potentially avoiding the flat regions (barren plateaus) encountered with random initialization. Similarly, one can train a quantum circuit layer by layer. Parameters are optimized for the first layer and then frozen as subsequent layers are added and trained. This approach can prevent gradients from vanishing early in the process, providing a structured optimization path.\nLastly, the challenges associated with deciding how to transfer are entirely dependent on the frequency with which the problems to be addressed arise. Although the multitasking approach can offer advantages, as seen in UC2, it is complex to devise efficient schemes that maintain the quantum properties of the qubits. In other words, sequential transfer will be advisable as long as the tasks to be addressed do not appear simultaneously.\nAs can be seen, ToK can be a promising strategy to address the challenges regarding ML and optimization algorithms in the NISQ era. By leveraging previous results and well-established techniques, it is possible to enhance the efficiency and effectiveness of quantum schemes. This approach not only accelerates the optimization process but also reduces the computational burden on quantum processors, making it a valuable tool for advancing quantum computing technologies."}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "QC is, without a doubt, a promising computing paradigm that is expected to revolutionize AI. In order to live up to all expectations, QC will eventually need to prove advantageous in real world settings. Fortunately, classical AI has come a long way and provides us with many lessons learned. As such, ToK has made a mark by lightening computations in scenarios where problems are repetitive or bear significant similarity. Within this context, the objectives of this work have been twofold:\n\u2022 To gather and present the core theories and principles behind ToK in a brief, yet exhaustive, description and categorization. To unify the main concepts scattered throughout the vast bibliography related to this discipline, a homogenized mathematical notation for TO and TL has been proposed. This theoretical framework is the cornerstone for identifying opportunities to enhance the performance of hybrid solvers and increase the probability of success.\n\u2022 To encourage quantum researchers to consider and adopt, if applicable, ToK techniques by providing an overview of the most remarkable efforts made in classical computing and a practical demonstration of the advantages that could be gained in the field.\nLast but not least, and ascending to holistic thinking, knowledge transfer between members of multidisciplinary teams will also catalyze enormous advances in the QC field, where hybridization is now and will be in the future the only plausible approach."}]}