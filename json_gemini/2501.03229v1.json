{"title": "GAUSSIAN MASKED AUTOENCODERS", "authors": ["Jathushan Rajasegaran", "Xinlei Chen", "Rulilong Li", "Christoph Feichtenhofer", "Jitendra Malik", "Shiry Ginosar"], "abstract": "This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While\nreconstructive self-supervised learning frameworks such as MAE learns good se-\nmantic abstractions, it is not trained for explicit spatial awareness. Our approach,\nnamed Gaussian Masked Autoencoder, or GMAE, aims to learn semantic ab-\nstractions and spatial understanding jointly. Like MAE, it reconstructs the image\nend-to-end in the pixel space, but beyond MAE, it also introduces an intermediate,\n3D Gaussian-based representation and renders images via splatting. We show that\nGMAE can enable various zero-shot learning capabilities of spatial understand-\ning (e.g., figure-ground segmentation, image layering, edge detection, etc.) while\npreserving the high-level semantics of self-supervised representation quality from\nMAE. To our knowledge, we are the first to employ Gaussian primitives in an\nimage representation learning framework beyond optimization-based single-scene\nreconstructions. We believe GMAE will inspire further research in this direction\nand contribute to developing next-generation techniques for modeling high-fidelity\nvisual data. More details at https://brjathu.github.io/gmae.", "sections": [{"title": "INTRODUCTION", "content": "Vision systems, by nature, process raw, low-level observations of the world, but visual reasoning\nfrequently requires spatial understanding as well as higher-level semantic abstractions of the data.\nIn this work, we aim to learn the structure of the world, which is constructed from objects and their\nrelationships in 3D space. We learn these abstractions from raw image observations by learning\nmasked auto-encoders controlled by 3D Gaussians as their intermediate representations.\nWhat sort of spatial understanding does visual reasoning require? In this work, we take inspiration\nfrom Wang and Adelson (Wang & Adelson, 1994), who demonstrated that the simplest version of\na spatially-aware representation consisting of 2.1D layers is sufficient for representing objects that\ninherently move with respect to one another. Even in static images, layered representations allow\nus to learn more about the structure of the world (Tucker & Snavely, 2020; Zhou et al., 2018). We,\ntherefore, set ourselves the task of learning image representations that are both layered and at the\nlevel of abstraction of single objects.\nLearning high-level semantic abstractions can be achieved by supervised learning (Krizhevsky et al.,\n2012; He et al., 2015; Dosovitskiy et al., 2020) or by learning binding from large scale paired\ndatasets (Radford et al., 2021; Jia et al., 2021; Zhang et al., 2022; Rombach et al., 2022). However,\nself-supervised learning has recently emerged as the more promising approach in this direction (Oquab\net al., 2024; Grill et al., 2020; Bao et al., 2021; He et al., 2022; Wei et al., 2022). Notably, Masked\nAutoencoders (MAE) (He et al., 2022) demonstrated that self-supervised learning is an effective\nrepresentation learning mechanism by directly predicting the RGB values of masked image patches.\nHowever, while the leading methods, such as MAE (He et al., 2022) and DINOv2 (Oquab et al.,\n2024), learn higher-level representations of images, they are not trained for explicitly recovering the\nspatial structure of objects and scenes in the world.\nThis paper proposes jointly learning high-level semantic abstractions such as objectness, grouping,\nand semantic structure with 2.1D layering via self-supervised learning. Our idea is conceptually\nsimple: given MAE, a pixel-based self-supervised representation learning approach, we design\nmechanisms that can lead to desirable intermediate representations as learned latents. Specifically,\nour central insight is that 3D Gaussians (Kerbl et al., 2023) are a good candidate for intermediate\nimage representations that can lead to semantic and spatial understanding."}, {"title": "RELATED WORK", "content": "Self-supervised Learning: Over the years, self-supervised pre-training has proven effective in many\nareas, including language, vision, and robotics. In computer vision, there are two primary schools of\nthought: discriminative and reconstructive pre-training with visual data. Discriminative pre-training\ninvolves training an instance discrimination model to learn the similarity between different augmented\nimage versions. Wu et al. (2018) and SimCLR (Chen et al., 2020) showed that instance discrimination\ntraining can be used to learn strong discriminative features via contrastive learning. Later, notable\nworks such as MoCo (He et al., 2020) and DINO (Caron et al., 2021) have shown the effectiveness of\nsuch trained visual representations for various downstream tasks.\nReconstructive pre-training learns to model the data distribution by trying to reconstruct an image\nor a video from its noisy version. One of the most successful methods for such pre-training in\ncomputer vision has been the BERT (Devlin et al., 2018)-style masked modeling of images proposed\nby BEIT (Bao et al., 2021), and MAE (He et al., 2022). Compared to BERT, MAE uses asymmetric\nencoder-decoders, allowing it to be very efficient at training with high masking ratios. This style\nof reconstructive pre-training learns strong visual priors and shows impressive results on various\ndownstream tasks such as object detection (Li et al., 2022), pose estimation (Xu et al., 2022), and\nrobot tasks (Radosavovic et al., 2022).\nMid-level Representations: Image can be constructed by operating functions on some representations.\nOne line of approachs keep the representations in the latent spaces, and use a pretrained decoder\nnetwork to re-construct the image. VAE (Kingma, 2013) with image synthesis (Rombach et al., 2022;\nLi et al., 2023) are good examples of this case, along with MAE He et al. (2022) and BEiT Bao\net al. (2021). Other line of approaches follow a structured representations to represent an image.\nThere are various such options: super-pixels, Gaussians, SVG code and multi-plane images etc. For\nexample Super-pixel sampling networks (Jampani et al., 2018) learns to predict super-pixels as the\nrepresentation to reconstruct and to predict segmentations and flow. Multi-plane images is another\nway to represent an image (Tucker & Snavely, 2020), where an image is composed by multiple\nlayered planes, and can be learned end-to-end. There are hybrid approaches also exist. For example,\nslot-attention (Locatello et al., 2020), learns an intermediate representation for objects, by adding a\nbottleneck in the model architecture. Similarly Leopart (Ziegler & Asano, 2022) learns to cluster the\npatches based on self-supervised clustering. In this paper, we take another approach which uses 3D\nGaussians as intermediate representations to reconstruct an image.\nGaussian Splatting: Gaussian splatting (Kerbl et al., 2023) is a novel differentiable volume splatting\ntechnique using Gaussian primitives as the 3D representation, offering high optimization flexibility\nand high fidelity in reconstruction. This idea follows a long list of differentiable rendering techniques\nthat recently gained significant attention as a method to bridge the gap between the 3D world and 2D\nimages. Differentiable rendering allows for reconstructing 3D representations (e.g., meshes, point\nclouds) from 2D image signals by enabling gradient-based optimization. For example, (Liu et al.,\n2019) introduces a differentiable rasterizer for meshes using probability aggregation. (Lassner &\nZollh\u00f6fer, 2021) proposes an efficient and differentiable formula to render large sets of point clouds.\n(Mildenhall et al., 2021) and (Barron et al., 2022) apply differentiable volume rendering (Levoy,\n1990) to reconstruct 3D radiance fields from a handful of multi-view images.\nIn this paper, we propose that 3D Gaussians are a useful learned mid-level image representation\ndue to their non-uniformity properties. We take advantage of the splatting operation from (Kerbl\net al., 2023) that enables end-to-end training of mid-level representations with image-based losses.\nWe then use Gaussian primitives in a representation learning framework rather than a single-scene\noptimization-based 3D reconstruction as in (Kerbl et al., 2023), thus opening the door for using\nlearned 3D Gaussian representations in computer vision applications.\nThe key advantages of Gaussian splatting include its ability to adapt to scene complexity, efficient\nrendering, and high-quality reconstructions. Unlike uniform voxel grids, Gaussian primitives can\nvary in size and density, allowing for more compact and expressive representations of 3D scenes.\nThis adaptability makes them particularly suitable for a wide range of computer vision tasks, from\n3D reconstruction to novel view synthesis and beyond."}, {"title": "METHOD", "content": "We propose a method that reconciles pixel-based learning, the mainstream in self-supervised learning,\nand latent-based learning, which can impose extra properties on representations. Our key insight is\nthat end-to-end learnable 3D Gaussians are good candidates for mid-level image representations due\nto their non-uniform properties. Given a large collection of images, we train a Masked Autoencoder\n(MAE) (He et al., 2022) to reconstruct full images from their masked inputs. The MAE encoder is a\nViT (Dosovitskiy et al., 2020) that learns to encode the visible square patches of the masked images\ninto learned embeddings. However, rather than predicting patches of pixels directly as in MAE (He\net al., 2022), our ViT-based decoder predicts explicit 3D Gaussians (Kerbl et al., 2023) - their color,\n3D center location, scale, and orientation. We then render these Gaussians as images with a splatting\ndifferentiable renderer and train the entire model using an MSE loss in pixel space."}, {"title": "PRELIMINARIES", "content": "Self-supervised Masked Autoencoders. Masking autoencoders model the data distribution by\nrandomly masking parts of the data and predicting the masked parts. In language, BERT (Devlin\net al., 2018) is trained by masking part of the text tokens and predicting them using a transformer\nmodel (Vaswani et al., 2017). In vision, MAE (He et al., 2022) and BEiT (Bao et al., 2021) mask image\npatchs at the input and predict the masked regions. In MAE (He et al., 2022), a ViT (Dosovitskiy\net al., 2020) encoder encodes the visible patches, and a smaller decoder ViT model is used with\nmasked tokens to reconstruct the masked patches.\n3D Gaussian Primitives and Splatting. Our model learns a mid-level image representation using the\n3D Gaussian primitives originally proposed for optimization-based single-scene 3D reconstruction\nby (Kerbl et al., 2023). Each Gaussian is characterized by a 3D covariance matrix $\\Sigma \\in \\mathbb{R}^{3 \\times 3}$ and\na center location $p \\in \\mathbb{R}^3$. Additionally, each Gaussian is assigned a color $r \\in \\mathbb{R}^3$ and an opacity\n$o \\in \\mathbb{R}$ to encode the scene content. For image rendering, these Gaussian primitives are transformed\ninto camera coordinates and projected onto the image plane using volume splatting. Due to the\ndifferentiable nature of this process, the attributes of the Gaussian primitives can receive gradients\nfrom the rendered image. In our work, we adopt the standard approach of factorizing the covariance\nmatrix $\\Sigma = RSSTR^\\dagger$ into a scaling matrix $S = \\text{diag}(s) \\in \\mathbb{R}^{3 \\times 3}$ represented by a scale vector\n$s \\in \\mathbb{R}^3$, and a rotation matrix $R \\in \\mathbb{R}^{3 \\times 3}$ represented by a rotation quaternion $& \\in \\mathbb{R}^4$. Consequently,\neach Gaussian is parameterized by a 14-dimensional vector $g = \\{p, s, p, r, o\\} \\in \\mathbb{R}^{14}$."}, {"title": "OUR APPROACH", "content": "Our model has a ViT-based encoder model, a lightweight decoder model, and a differentiable renderer.\nFig. 2 shows a high-level overview of our method. For a given image, we first patchify it into $N$\npatchs and randomly mask them with a masking ratio $r$, resulting in $n$ visible patches. The ViT\nencoder model only sees the visible patches and encodes them from patches to latent embeddings,\n$X_i \\in \\mathbb{R}^{d_{enc}}, i \\in \\{1,2,3,...n\\}$.\nAssume the decoder has $k$ learnable query tokens $q_j \\in \\mathbb{R}^{d_{dec}}, j\\in \\{0,1,2, ...k\\}$. Note that $k$ can be\nany value irrespective of the number of masked tokens. We project the encoder latent to $\\hat{c}_i \\in \\mathbb{R}^{d_{dec}}$\nand concatenate it with the query tokens.\n$\\mathbb{X}_{dec} = \\{\\hat{c}_i\\}_{i=1}^k \\cup \\{q_j\\}_{j=1}^k $\n(1)\nThe decoder sees the $\\mathbb{X}_{dec}$ tokens and predicts $k$ Gaussians, one for each query token (we discard\nthe predictions for the latent tokens). Each Gaussian is parameterized by a 14-dimensional vector\n$g_j = \\{p, s, p, r, o\\} \\in \\mathbb{R}^{14}$.\nOnce we have $k$ predicted Gaussians, we splat them on a plane with a fixed camera projection and\nrender the splatted Gaussians to generate an image. We limit the size of the Gaussians by using an\neffective scale $c \\cdot \\text{sigmoid}(s)$. Here, $c$ controls a Gaussian's maximum size. After rendering, we use\na mean squared error loss to compare the reconstructed image with the input image on the originally\nmasked pixels.\nNote that since the Gaussians are the output of the decoder, they are effectively randomly initialized.\nThis is in contrast to the typical usages of Gaussian splatting for 3D reconstruction that rely on\npoint-cloud initialization. In this work, we do not use any prior knowledge. We directly learn all the\nGaussian properties from reconstructing the image."}, {"title": "EXPERIMENTS", "content": "First, we will explore various design spaces for pre-training our models. All our experiments in this\nsection are based on a ViT-base encoder and a lightweight decoder and measured by ImageNet (Deng\net al., 2009) classification performance. All the models are trained for 400 epochs. We use a base\nlearning rate of 1e - 4 with cosine decay with AdamW (Loshchilov & Hutter, 2017) optimizer. We\nevaluate our pre-trained models using linear probing and full finetuning.\nNumber of Gaussians: Unlike MAE, our de-\ncoder model is fully decoupled from encoder\ntokens. Therefore, we can use any number of\nGaussians for decoding. We train 4 models that\nlearn to decode 64, 128, 256, and 512 Gaussians,\nrespectively. Fig. 3 shows ImageNet classifica-\ntion performance under linear probing and full\nfine-tuning as a function of the number of Gaus-\nsians. With linear probing, performance mono-\ntonically increases as we increase the number of\nGaussians. With full fine-tuning, we see a sim-\nilar behavior at first, but it saturates after 256\nGaussians.\nGaussian Scale: Since the Gaussians are pre-\ndictions from the decoder, we can not control\ntheir initialization explicitly. We only have an\nactivation function after decoder predictions: for\nx,y,z we use tanh, and for scale and quaternions\nwe use sigmoid. Since we are learning the scales\nfrom randomly initialized Gaussians, we limit\nthe Gaussians from being too big by passing\nthem through the scaled sigmoid function be-\nfore rendering ($c \\times \\text{sigmoid}(scale)$). Here,\nwe study the effect of $c$ on the representation\nquality. shows how classification performance on ImageNet changes by varying the maxi-\nmum allowed scale values. This essentially controls how big each Gaussian can be and how many\npixels they can influence. This variable has only a small effect on the classification performance.\nHowever, small scale values greatly hinder the reconstruction quality"}, {"title": "DESIGN CHOICES", "content": "Masking Ratio: We study the dependence of our pre-trained models on the masking ratio in Table 1b.\nMAE (He et al., 2022) showed that higher masking rates allow the model to learn better representations\nand make the training much more efficient. Using differentiable rendering instead of transformer-\nbased rendering did not change this behavior.\nLoss: We also study how to apply the loss on patches. Should we apply on masked patches or all the\npatches? On pixel prediction or normalized pixel prediction? MAE (He et al., 2022) benefited from\npredicting normalized patches when applying the loss on masked patches. In our case, a normalized\npatch loss significantly hurts model performance, as shown in Table 1c. This is due to the fact that all\nthe Gaussians can influence a specific pixel value, a constraint that makes it harder to reconstruct\nlocally normalized patches. Finally, we have a similar observation as in MAE (He et al., 2022); when\nthe loss is applied only to the masked tokens, the model performs slightly better than the model\ntrained with loss applied to all the patches.\nFinal Model: Based on our findings from the above experiments, our final model is a ViT-base model\nthat is pre-trained with a 0.75 masking ratio for 400 epochs, with patch loss applied to the masked\ntokens and has 512 Gaussians at the decoder during per-training. We will use this model for the rest\nof our experiments. All our models are pre-trained on the ImageNet (Deng et al., 2009) 1k dataset."}, {"title": "SUPERVISED TASKS", "content": "To measure the representation quality of our pre-trained models, we evaluate our models on ImageNet-\n1k (Deng et al., 2009) classification and COCO (Lin et al., 2014) object detection. We use the ViT-base\nmodel, trained for 400 epochs with a masking ratio of 0.75, for fine-tuning on ImageNet and COCO.\nImage Recognition:  shows the performance of various pre-trained models on ImageNet\nclassification top-1 accuracy. We fine-tuned our model for 100 epochs, following the protocols of\nMAE (He et al., 2022). While achieving comparable performance to MAE, our models show an\ninteresting trend when increasing the number of Gaussians, Scaling this further without\nincreasing compute requirements would necessitate further modifications, which we leave for future\ndirections.\nObject Detection and Segmentation: As another important way to evaluate our encoder representa-\ntions, we transfer the representations learned with our pipeline via fine-tuning, to object detection\nand segmentation. We follow the protocol of ViTDet (Li et al., 2022) and evaluate on the COCO"}, {"title": "UNSUPERVISED TASKS", "content": "In this section, we study the properties of the decoder of our pre-trained models. Unlike MAE, we now\nhave access to an intermediate representation that we can edit and modify without re-training. Given\nan input image, we fully encode the image and generate Gaussians that we splat to reconstruct the\nimage. First, we evaluate the image reconstruction quality of our model. On the ImageNet validation\nset, our ViT-base model achieved 89.45 on reconstruction FID, while the MAE ViT-base model got\n98.12 (lower is better). This improvement in reconstruction quality is due to the non-uniformity of\nthe learned distribution of Gaussians, which allows GMAE to model high-frequency information, as\nshown in Fig. 5. As a result, our reconstructions can be used directly for other tasks without needing\nto add a GAN loss or an upsampling layer on top.\nAnother advantage of using a 3D Gaussian representation to represent 2D images is that it learns to\nseparate objects in the z direction. This may be due to the fact that with random initialization, the\npoints closer to the camera represent low-frequency information, while the points far from the camera\nmodel the high-frequency information (see Section 4.4). To segment an image along the z axis, we\nsimply sort the predicted Gaussians based on their depth value and group them into $L = 10, 11, ...la$\ngroups. To split an image into two layers, we try to render it as two images using $l_0, ..., l_n$ and\n$l_0, ..., l_n, l_{n+1}$. If the difference at a pixel is larger than a specific threshold $th$, we assign that pixel to\nthe layer $n + 1$. shows the layering effect of a Gaussian representation with 64 layers. We also\nshow the layering effect in pixel space\nZero-shot Figure-Ground Separation: The layer-wise rendering allows us to perform figure-ground\nsegmentation for free. We simply get the layer-wise rendered image and apply a threshold to obtain\nforeground-background segmentation. We evaluate our approach on the PASCAL dataset (Everingham\net al., 2015) on foreground segmentation and single object detection tasks The \u201ccopy\u201d baseline (Bar et al., 2022) predicts a random ground truth mask from\nthe training set. We note that GMAE performs better than other few-shot baselines despite being a\nzero-shot approach."}, {"title": "Zero-shot Edge detection:", "content": "In a similar fashion, as in zero-shot figure-ground segmentation, we can\nsimply take the layer-wise rendered image and find edges with a discontinuity in the z direction.\nAdditionally, the number of layers determines the granularity in the edge detection; for example, a\nlarge number of layers means we will detect more fine-grained edges. shows the detected edges\nof our zero-shot method with varying number of layers. We also quantitatively evaluate our method\non BSDS500 (Arbelaez et al., 2010). Table 4 shows that our method archives reasonable performance\nfor zero-shot detection without ever being trained for segmentation or detection. Fig. 8 shows that\nas we decrease the number of layers (e.g., increasing the width of each layer), the quality of edge\ndetection gets better, allowing us to have a hierarchy of edges."}, {"title": "QUALITATIVE RESULTS", "content": "Distribution of Gaussians in xy: shows how the Gaussians are arranged differently in space\nfor different images. In patch-based methods such as MAE, patches are arranged in a uniform tile.\nEven if we decrease the patch size to increase the number of patches, they will still be uniformly\nallocated across all regions. However, in our work, a Gaussian can go to any part of the image, which\nallows them to dynamically position themselves based on the input image. This property allows\nmodeling high-frequency regions with high fidelity. As shown in , our reconstructions can\ncapture high-frequency regions such as faces and intricate patterns.\nSize vs Depth: shows a clear trend: the Gaussians\nwith larger scale values lie closer to the camera, while the\nones with smaller scale values, lie away from the cam-\nera, on average. This distribution validates our previous\nhypothesis in Sec 4.3, that low-frequency blobs lie closer\nand cover larger regions, while high-frequency blobs lie\naway from the camera, on average. At the start, some\nGaussians will be closer to the camera, influencing more\npixels, therefore representing low-frequency regions. Our\nlayering results are the outcome of this property. In the real\nworld, backgrounds tend to have low-frequency regions\nwhile objects usually have high-frequency details. This\ncorrelation leads to our zero-shot results. We attribute this\nto, as all else, to divine benevolence."}, {"title": "DISCUSSION", "content": "This paper presents GMAE, a self-supervised image representation learning approach that extends\nMAE to include a learned intermediate Gaussian representation to jointly learn semantics and\nspatial understanding. We show that learning to represent images with 3D Gaussians has several\nbuilt-in advantages that stem from their non-uniform dynamical allocation of scale, location, and\ndistribution. Our method, therefore, lends itself to zero-shot capabilities of spatial understanding\nsuch as foreground-background segmentation, image layering, and edge detection. Along with these\nadvantages, we demonstrate that the representation learned by our method is still has good semantic\nabstractions, and is on par with MAE on standard supervised image recognition tasks and that it\ntransfers to downstream tasks such as detection and segmentation via fine-tuning.\nGMAE still exhibits several empirical limitations. For example, setting larger scale values at the start\nof training results in a more challenging optimization. Compared to the number of Gaussians typically\nused for 3D reconstructions (up to millions), the number of Gaussians we have used in GMAE is\nbottlenecked by compute, and increasing it to more than a thousand can cause major slow-downs for\npre-training. An interesting future direction is to further accelerate our pipeline.\nWe hope our exploration can inspire more work in this direction and truly unlock the next generation\nof techniques that effectively model visual data."}, {"title": "TRAINING DETAILS", "content": "Here we provide configurations used for training 5 and finetuning 6 our models. For pre-training, we\nfollow a similar recipe as in MAE (He et al., 2022). For linear probing we use AdamW instead of\nLARS"}, {"title": "MORE SAMPLES", "content": ""}]}