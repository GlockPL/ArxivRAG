{"title": "MTA: Multimodal Task Alignment for BEV Perception and Captioning", "authors": ["Yunsheng Ma", "Burhaneddin Yaman", "Xin Ye", "Feng Tao", "Abhirup Mallik", "Ziran Wang", "Liu Ren"], "abstract": "Bird's eye view (BEV)-based 3D perception plays a crucial role in autonomous driving applications. The rise of large language models has spurred interest in BEV-based captioning to understand object behavior in the surrounding environment. However, existing approaches treat perception and captioning as separate tasks, focusing on the performance of only one of the tasks and overlooking the potential benefits of multimodal alignment. To bridge this gap between modalities, we introduce MTA, a novel multimodal task alignment framework that boosts both BEV perception and captioning. MTA consists of two key components: (1) BEV-Language Alignment (BLA), a contextual learning mechanism that aligns the BEV scene representations with ground-truth language representations, and (2) Detection-Captioning Alignment (DCA), a cross-modal prompting mechanism that aligns detection and captioning outputs. MTA integrates into state-of-the-art baselines during training, adding no extra computational complexity at runtime. Extensive experiments on the nuScenes and TOD3Cap datasets show that MTA significantly outperforms state-of-the-art baselines, achieving a 4.9% improvement in perception and a 9.2% improvement in captioning. These results underscore the effectiveness of unified alignment in reconciling BEV-based perception and captioning.", "sections": [{"title": "1. Introduction", "content": "3D perception is a fundamental and crucial task for embodied AI applications such as robotics and autonomous driving [5, 14]. Among 3D perception methods, bird's eye view (BEV)-based methods have recently gained significant attention, particularly in the context of autonomous driving tasks [10, 11, 15, 39]. Unlike monocular frameworks which process each camera view separately, BEV provides a unified representation of a given scene by fusing information from multi-view camera images or other sensory inputs such as LiDAR scans [20, 24]. The generated BEV representations serve as the primary source of information for solving downstream autonomous driving tasks such as detection and tracking.\nIn recent years, transformer-based BEV methods have shown rapid progress, enabling the extraction of spatio-temporally holistic representations of the surrounding environment from multi-view camera images [20, 43]. These rich representations have facilitated achieving state-of-the-art 3D perception performance. The rise of foundation models, such as multimodal large language models (MLLMs) have led to the emergence of research on explainability and understanding of 3D scenes [4, 28, 33, 42]. This task is manifested as a captioning task which aims to describe the localization, context, and behavior of objects in the scene in the form of natural language. 3D captioning has been extensively investigated for various indoor applications [7, 8] and has more recently been extended to outdoor applications such as perception task in autonomous driving [16]. BEV-based 3D captioning extracts information from BEV and task heads such as 3D detection and uses it as the condition for caption generation.\nWhile there is an increasing number of research studies on BEV perception and captioning tasks, the joint alignment between modalities, which aims to enhance the performance of both modality tasks, has not been properly addressed. In particular, one stream of works focuses on BEV-based detection without considering the captioning performance [20], while another stream of works focuses on captioning performance without reporting performance on perception tasks such as 3D detection [16]. However, these two tasks are not disjoint and can complement each other by enforcing multimodal alignment strategies that have the potential to significantly advance the field of 3D perception and captioning in autonomous driving applications.\nTo bridge this gap, we introduce MTA, a multimodal task alignment approach for BEV perception and captioning. The proposed MTA approach presents two mechanisms for alignment, namely BEV-Language Alignment (BLA) and Detection-Captioning Alignment (DCA). BLA introduces a multimodal contextual learning mechanism that incorporates ground-truth caption representations to learn the alignment between the BEV visual representations of the scene and the natural language-based scene understanding. Rather than relying solely on the the language modeling objective, BLA provides additional supervision by aligning the BEV based contextual object queries with corresponding ground-truth linguistic representations obtained from pre-trained text encoders. DCA, on the other hand, aims to explicitly promote consistency between the perception outputs from the visual branch and captioning outputs from the language branch. It introduces a cross-modal prompting mechanism that encourages the MLLM to generate captions that are coherent with the predicted bounding boxes and class labels. DCA goes beyond relying solely on the gradients from single modality objectives (detection loss or language modeling loss) to optimize the task heads or the MLLM.\nMTA is a flexible framework that can be seamlessly integrated into existing BEV-based perception and captioning frameworks. Additionally, the proposed MTA modules are only used during training time to enforce alignment between modalities. Thus, MTA does not require any architectural changes or introduce any additional computational overhead during inference time, which is very critical for downstream tasks such as autonomous driving. In this paper, we evaluate the importance of MTA on the state-of-the-art frameworks using the challenging large-scale nuScenes [2] and TOD3Cap datasets [16]. Experimental results show that MTA outperforms the previous best baseline on both perception and captioning tasks. In particular, MTA provides an improvement of 4.9% and 9.2% in terms of perception and captioning metrics, respectively, over the corresponding state-of-the-art baseline. Moreover, qualitative results further confirm the quantitative findings, demonstrating that MTA not only achieves superior performance metrics but also reduces the occurrence of hallucinated captions, which is a fundamental factor for safety-critical applications like autonomous driving.\nOur main contributions can be summarized as follows:\n\u2022 We propose MTA, a novel multimodal task alignment framework that bridges the gap between BEV-based perception and captioning tasks.\n\u2022 MTA introduces two novel alignment modules, BEV-Language Alignment (BLA) and Detection-Captioning Alignment (DCA), which enforce alignment through multimodal contextual learning and cross-modal prompting mechanisms, respectively.\n\u2022 MTA seamlessly integrates into existing architectures and does not introduce any additional computational overhead during inference, as both of the MTA components are only active during training.\n\u2022 Extensive experiments on the challenging nuScenes and TOD3Cap datasets demonstrate that MTA consistently outperforms state-of-the-art methods and in both perception and captioning tasks."}, {"title": "2. Related Work", "content": "In recent years, BEV frameworks have utilized transformer architectures that generate high-quality BEV feature maps [20, 22, 24, 41, 43]. Among these works, BEVFormer has unraveled a new era in BEV perception by fusing information from multi-view camera images both spatially and temporally to acquire a spatio-temporally holistic representation of the scene [20]. Another notable work in this domain is BEVFusion, which presents a framework to fuse BEV feature maps from both camera and LiDAR sensors for efficient and robust BEV perception [24]. These advancements in BEV perception have laid the foundation for a more comprehensive understanding of 3D environments in autonomous driving applications."}, {"title": "2.2. 3D Captioning", "content": "3D captioning aims to provide natural language descriptions of the localization and behavior of objects in a given scene. Recently, the field of 3D captioning has witnessed significant progress, thanks to the rapid emergence of multimodal large language models and the release of many public datasets, primarily for indoor applications [6\u20138]. These advancements have prompted the embodied AI community to collect 3D captioning datasets and develop 3D captioning frameworks for outdoor applications such as autonomous driving [26, 31, 37]. A notable work in this direction is TOD3Cap [16], which has released a large captioning dataset for autonomous driving and proposed a framework for BEV-based 3D dense captioning. This framework utilizes information from BEV and 3D perception outputs as inputs to an MLLM for generating captions.\nDespite these advancements in BEV perception and 3D captioning, there remains a significant gap in jointly optimizing and aligning these two modalities to enhance the performance of both tasks, which we aim to address in this study through the proposed MTA framework."}, {"title": "2.3. Vision-Language Models", "content": "Vision-language models, trained on massive internet-scale data, have shown strong promise in learning good representations for downstream tasks. Pioneering works such as CLIP [32], ALIGN [13], and Florence [40, 44] pretrain mul-"}, {"title": "3. Methodology", "content": "Overview. The overall framework of multimodal task alignment (MTA), which strengthens the alignment between BEV perception and captioning tasks to achieve state-of-the-art performance on both tasks, is illustrated in Fig. 1. The methodology section is outlined as follows. In Sec. 3.1, we provide a background on BEV perception and captioning tasks. In Sec. 3.2 and Sec. 3.3, we provide details of the proposed MTA alignment mechanisms, namely BEV-Language Alignment and Detection-Captioning Alignment. Finally, in Sec. 3.4, we provide the overall loss function for training the MTA framework."}, {"title": "3.1. Preliminaries", "content": "BEV Perception Module. The BEV perception module D processes sensory inputs such as camera, LiDAR, or both to obtain a unified top-down representation of the surrounding environment. In the context of given camera sensors, multi-view camera images are processed through a backbone to obtain multi-view camera features. Subsequently, the resulting perspective-view features are fed into BEV encoders such as BEVformer, which lift these image features into BEV space by spatio-temporally fusing them [20].\nSubsequently, the generated BEV feature maps are fed into a downstream task head, such as a transformer decoder for 3D detection [38]. Due to a lack of ground-truth BEV maps, BEV perception is trained end-to-end with the objective of minimizing the task head loss function. The evaluation performance of the task head serves as a proxy for the quality of BEV perception.\nBEV Captioning Module. The BEV captioning module G aims to generate natural language descriptions of the localization and behavior of objects in the scene. It takes the BEV perception outputs, such as the BEV feature map and object proposals from the task head, as input. A relation query transformer (Q-Former) is generally employed to extract and transfer contextual information from the BEV perception to language space [16, 19, 37]. Formally, the Q-Former maps the embedding for each detected object to the"}, {"title": "3.2. BEV-Language Alignment", "content": "Our goal is to bridge the gap between the BEV-based scene representation used for 3D detection and the language-based scene understanding and reasoning capabilities of the MLLM. However, off-the-shelf MLLMs cannot directly comprehend and reason on BEV features, as they have not been exposed to such representations during their pre-training phase. Furthermore, the alignment gap between BEV features and the MLLM's language space is more pronounced compared to the visual tokens used in general-domain MLLMs [9].\nTo address this challenge, we introduce a novel BEV-Language Alignment (BLA) module that explicitly aligns the BEV perception features, which encompass visual contextual information for each object, with their corresponding ground-truth linguistic representations. By aligning the Q-Former's visual BEV features with ground-truth captioning features, we strengthen the alignment between the BEV perception and captioning modules, enabling the MLLM to better comprehend and reason over the BEV representation. Formally, the BLA module operates as follows. Given a ground-truth caption o, we compute its text embedding using a pretrained CLIP text encoder T [32]. We then extract the projected Q-Former features ($q_\theta$), where $q_\theta$ represents the hidden states from the l-th layer of the Q-Former, and $\\Phi^o$ is a trainable projection head parameterized as an MLP. The alignment is enforced using a mean squared error loss formulated as:\n$\\mathcal{L}_{BLA} (\\Phi^o, Q_{1:l}) := || \\Phi^o(q_\theta) \u2013 T(o) ||^2$. \nConceptually, the BLA-enhanced Q-Former can be viewed as a two-stage process. In the first stage (before the l-th layer), the Q-Former focuses on learning a context-aware representation of the object queries by attending to the BEV features. This stage allows the Q-Former to capture the spatio-temporal relationships and semantics encoded in the BEV representation with direct representation-based supervision. In the second stage (from the l-th layer onwards), the Q-Former maps the object query features into an MLLM-aligned space, making them more amenable to the MLLM's language-based reasoning capabilities."}, {"title": "3.3. Detection-Captioning Alignment", "content": "In current BEV-based perception and captioning frameworks, the 3D detection and captioning tasks are typically optimized independently, which may lead to suboptimal performance and a lack of coherence between the predicted bounding boxes and the generated captions. To address this limitation, we further propose a Detection-Captioning Alignment (DCA) module that aims to bridge the gap between the detection and captioning outputs. The main challenge here lies in the significant discrepancy between the modalities of the detection labels (class labels and bounding box coordinates) and the captioning logits (language tokens). Directly aligning these outputs can lead to a performance drop in both tasks.\nWe tackle this challenge by introducing a cross-modal prompting approach. We define a set of N learnable prompt tokens P = {p1, \u2026, pN} \u2208 $R^{N\u00d7D}$ that serves as a shared embedding space for aligning the detection and captioning outputs. Formally, let $\\hat{c}$ and $\\hat{b}$ denote the class labels and bounding box coordinates from the detection head, respectively. We project the concatenated detection outputs into the cross-modal prompt space via attention pooling:\n$P_{det} = \\frac{ \\sum_{i=1}^{N} [p_{\\phi^{cls}(\\hat{c})}, p_{\\phi^{box}(\\hat{b})} ]}{\\sum_{i=1}^{N}1}$,\n$p_{\\phi^{cls}(\\hat{c})} = \\sum_{i=1}^{N} \\frac{exp (x_{det}^T P_i)}{\\sum_{j=1}^N exp (x_{det}^T P_j)}$.\nwhere $\\phi(\\cdot)$ denotes trainable projection heads parameterized as MLPs.\nSimilarly, we project the captioning logits into the same prompt space:\n$P_{cap} = \\sum_{i=1}^{N} \\frac{exp (\\phi^{cap} (o))^T P_i}{\\sum_{j=1}^N exp (\\phi^{cap}(o))^T P_j}$,\nwhere o represents the captioning logits from the MLLM."}, {"title": "3.4. Training", "content": "The final loss function for training the proposed MTA framework is a weighted combination of the detection loss $L_{DET}$, the language modeling loss $L_{LM}$, the BEV-language alignment loss $L_{BLA}$, and the detection-captioning alignment loss $L_{DCA}$:\n$L_{MTA} := \\alpha L_{DET} + \\beta L_{LM} + \\lambda_1 L_{BLA} + \\lambda_2 L_{DCA}$.\nBy default, we do not tune and set (\u03b1, \u03b2) = (10, 1) following [16], and set (${\\lambda}_1$, ${\\lambda}_2$) = (1,10\u22122) to ensure a balanced magnitude."}, {"title": "4. Experiments and Results", "content": "We conduct a comprehensive evaluation of the proposed MTA framework, demonstrating its effectiveness in improving both 3D dense captioning and detection performance through novel alignment mechanisms. The experimental setup, including datasets, evaluation metrics, and implementation details, is described in Sec. 4.1. In Sec. 4.2, we compare MTA's performance against the baseline TOD3Cap network and other state-of-the-art methods, along with qualitative results. Finally, Sec. 4.3 presents an extensive ablation study to validate the effectiveness of MTA's alignment components."}, {"title": "4.1. Experimental Set-up", "content": "Datasets. We conduct comprehensive experiments on the nuScenes [2] and TOD3Cap [16] datasets. The nuScenes dataset is a widely used benchmark in autonomous driving, containing 700 training and 150 validation scenes. Each scene is captured for a duration of ~20s using six cameras covering the entire 360-degree field of view, with key samples annotated at a rate of 2Hz. The detection task contains 1.4M annotated bounding boxes from 10 object categories. The TOD3Cap dataset extends nuScenes with dense language captioning annotations, providing approximately 2.3M language descriptions, with an average of 2.7K descriptions per scene.\nPerception Metrics. For the BEV perception task, we report the standard 3D object detection metrics within the nuScenes dataset, including: mean Average Precision (mAP), Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE), Average Attribute Error (AAE), and nuScenes Detection Score (NDS). More details on detection metrics can be found in [2]. The reported results are calculated using the validation split for all experiments.\nCaptioning Metrics. For the BEV captioning task, we report the m@kIoU [8] metric. Let (bi, oz) denote each ground-truth box-caption pair, where bi and oz are the bounding box coordinates and the caption for the i-th object, respectively. The predicted box-caption pair is denoted as (bi, \u00f4i). The m@kIoU metric is formulated as:\n$m@kIoU := \\frac{1}{N} \\sum_{i=1}^{N} \\delta(o_i, \\hat{o_i}) 1 \\{IoU (b_i, \\hat{b_i}) \\geq k\\},$ \nwhere N indicates the number of ground-truth objects, and m denotes the standard image captioning metrics, including Bilingual Evaluation Understudy (BLEU-4 [30]), Metric for Evaluation of Translation with Explicit ORdering (METEOR [1]), Recall-Oriented Understudy for Gisting Evaluation (Rouge [23]), and Consensus-based Image Description Evaluation (CIDEr [36]), abbreviated as B-4, M, R, and C, respectively. 1 represents the indicator function that is set to 1 if the IoU value for the i-th box is bigger than the threshold k, otherwise 0.\nImplementation Details. For model configurations, we follow the setup in TOD3Cap [16] unless otherwise specified. We employ the commonly used BEVFormer-tiny [20] as the pretrained BEV perception module and the lightweight pretrained Llama-3.2-1B [25] as the LLM in the BEV captioning module. Llama-3.2-1B is the state-of-the-art model in the 1B parameter class for on-device use cases. We note that MTA shares the same architecture as TOD3Cap, with the exception of the BLA and DCA mechanisms. In all experiments, the models were trained with a learning rate of 2 \u00d7 10\u22124 for 10 epochs, with the Llama model frozen except for the adapter [46] parameters. This includes training the pretrained BEVFormer baseline for additional 10 epochs, ensuring a fair comparison."}, {"title": "4.2. Main Results", "content": "3D Dense Captioning Results. We compare the performance of the TOD3Cap network with that of the same models trained with the proposed MTA, as well as other state-of-the-art methods: Scan2Cap [8] uses a message passing graph module to facilitate learning object relation fea-"}, {"title": "4.3. Ablation Study", "content": "We perform extensive ablation studies to validate the key design choices in the proposed MTA framework. We note that additional ablation studies and comprehensive experimental tables are provided in the supplementary materials.\nBLA & DCA Module Analysis. To gain a better understanding of the individual contributions of the proposed BLA and DCA modules, we conduct an ablation study, as shown in Tab. 3. We analyze the performance of each module separately and in combination (MTA), comparing them to the baseline TOD3Cap model.\nNotably, both the BLA and DCA mechanisms independently contribute to significant performance improvements over the TOD3Cap baseline. The BLA module has a partic-"}, {"title": "Effect of l.", "content": "We also investigate the impact of attaching the BLA objective to different layers of the Q-Former, where the total number of layers is L = 8. The results in Tab. 4 demonstrate that aligning at the middle layer yields the best performance. We hypothesize that aligning at an early stage forces the detection embedding to directly mimic the text embedding without sufficient interaction with the BEV features, potentially hindering the detection performance. Conversely, aligning at a later stage of the Q-Former leaves little room for the remaining layers to map the query to the MLLM space, considering that the text embedding from the text encoder T differs from the MLLM, which can impede the captioning performance. Consequently, in other experiments, we apply BLA to align the first half of the Q-Former layers, striking a balance between allowing the detection embedding to interact with the BEV features and providing sufficient capacity for mapping to the MLLM space."}, {"title": "Alignment Objective.", "content": "We compare three training objectives for alignment: Mean Squared Error (MSE), Negative Cosine Similarity (Cos. Sim.), and the CLIP Contrastive Loss (CLIP) [32]. These functions are applied for ablating both the BLA and the DCA objectives.\nAs shown in Tab. 5, we empirically find that for the BLA objective, the MSE and CLIP objectives offer advantages over the Cos. Sim., with MSE showing marginal advantages over the CLIP loss across all captioning and detection metrics. Therefore, we opt for MSE as the objective for BLA. In contrast, for the DCA objective, as presented in Tab. 6, the CLIP objective consistently outperforms MSE and Cos. Sim. across all metrics.\nThe difference in optimal objectives for BLA and DCA arises from their distinct alignment goals and the nature of the modalities being aligned. BLA focuses on aligning BEV features with linguistic representations, where the direct correspondence enforced by MSE proves effective. In contrast, DCA aims to align detection and captioning outputs within a shared prompt space, where the key is to establish correspondences between information in the two modalities. For example, consider the caption, \"traffic cone about 7 meters away in the back left of the ego car.\" Here, the detection output should align the bounding box location with the spatial description \u201c7 meters away in the back left,\u201d while the classification label should correspond with the object category \"traffic cone\" mentioned in the caption. The shared prompt space, leveraging the contrastive learning objective, proves effective."}, {"title": "5. Conclusion", "content": "In this paper, we introduced MTA, a novel multimodal task alignment framework that bridges the gap between BEV perception and captioning tasks, significantly enhancing performance on both tasks. MTA consists of two novel mechanisms: BEV-Language Alignment (BLA) and Detection-Captioning Alignment (DCA). BLA harnesses a multimodal contextual learning mechanism to align BEV-based visual representations and scene understanding with ground-truth linguistic representations. DCA leverages a cross-modal prompting mechanism to align detection and captioning outputs. Through extensive quantitative and qualitative experiments, we demonstrated the effectiveness of MTA in achieving improved performance on both BEV perception and captioning tasks. Importantly, MTA's alignment mechanisms are active only during training, ensuring no additional computational cost during inference, which is a critical factor in autonomous driving applications."}]}