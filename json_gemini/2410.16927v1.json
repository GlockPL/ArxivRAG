{"title": "Revealing Hidden Bias in AI: Lessons from Large Language Models", "authors": ["Django Beatty", "Kritsada Masanthia", "Teepakorn Kaphol", "Niphan Sethi"], "abstract": "As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified. This study examines biases in candidate interview reports generated by Claude 3.5 Sonnet, GPT-40, Gemini 1.5, and Llama 3.1 405B, focusing on characteristics such as gender, race, and age. We evaluate the effectiveness of LLM-based anonymization in reducing these biases. Findings indicate that while anonymization reduces certain biases\u2014particularly gender bias-the degree of effectiveness varies across models and bias types. Notably, Llama 3.1 405B exhibited the lowest overall bias. Moreover, our methodology of comparing anonymized and non-anonymized data reveals a novel approach to assessing inherent biases in LLMs beyond recruitment applications. This study underscores the importance of careful LLM selection and suggests best practices for minimizing bias in AI applications, promoting fairness and inclusivity.", "sections": [{"title": "I. INTRODUCTION", "content": "The adoption of large language models (LLMs) in recruitment is rapidly increasing, with organizations leveraging AI to enhance efficiency in hiring processes [1], [2]. Advanced models like Claude 3.5 Sonnet, GPT-40, Gemini 1.5, and Llama 3.1 405B are used for generating candidate reports, analyzing resumes, and crafting interview questions. Despite their capabilities, there is growing concern about inherent biases in Al outputs, which can lead to unfair hiring practices and perpetuate discrimination based on gender, race, age, and other personal characteristics [3], [4] [5], [6].\nAddressing these biases is crucial to ensure AI-driven recruitment tools promote fairness and diversity rather than exacerbate existing inequalities. Bias in recruitment not only undermines ethical standards but also poses strategic risks, potentially limiting workforce diversity and exposing organizations to legal and reputational repercussions [7], [8].\nThis study systematically examines biases present in LLM-generated candidate interview reports across various personal characteristics. We evaluate the effectiveness of LLM-based anonymization techniques in mitigating these biases. By analyzing different models and report sections, we aim to identify strategies for minimizing bias, providing insights and best practices for organizations to enhance fairness in their hiring processes. Importantly, our approach of comparing anonymized and non-anonymized analyses offers a novel method for uncovering inherent biases within LLMs, potentially impacting applications beyond HR and providing an alternative pathway to assess LLM bias in general."}, {"title": "II. METHODOLOGY", "content": ""}, {"title": "A. Overview", "content": "We conducted an empirical study utilizing a dataset of 1,100 CVs categorized into six job sectors:\n\u2022 Technical Roles: AI/ML, UX/UI\n\u2022 Non-Technical Roles: Administration, Law, Project Management, Sales & Marketing\nEach CV was paired with a corresponding job description generated using Claude 3.5 Sonnet. We processed the CVs through our recruitment insight tool in both standard (non-anonymized) and anonymized modes, generating candidate interview reports using four different LLMs:\n\u2022 Claude 3.5 Sonnet\n\u2022 GPT-40\n\u2022 Gemini 1.5\n\u2022 Llama 3.1 405B"}, {"title": "B. System Overview", "content": ""}, {"title": "C. LLMs Tested", "content": "\u2022 Claude 3.5 Sonnet: Developed by Anthropic, focusing on safe and ethical AI usage, excels in text summarization and contextually relevant content generation.\n\u2022 GPT-40: An advanced version of OpenAI's GPT series, known for versatile language generation and handling complex tasks.\n\u2022 Gemini 1.5: From Google's DeepMind, specialized in multi-modal tasks and effective in understanding and generating cross-domain content.\n\u2022 Llama 3.1 405B: Developed by Meta AI, with 405 billion parameters, optimized for coherent and contextually rich content generation."}, {"title": "D. CVs Classification", "content": "a) Approach 1: This method involves clustering CVs with similar content together and then inspecting a few samples from each cluster to assign a category. It is important to note that, from the image on the right, even though the algorithm classified the CVs into different groups, these clusters do not necessarily separate CVs by job sector. Additional research is required to refine the clustering method for more accurate categorization.\nb) Approach 2: This approach utilizes keyword frequency analysis combined with manual adjustments to categorize the CVs. Given that the raw dataset is not large, this method has proven to be effective."}, {"title": "E. Generate Job Descriptions Per Job Sector", "content": "The job descriptions for each sector were generated using Claude 3.5 Sonnet. We provided Sonnet with three candidate CVs and asked it to create a job description that these individuals might find appealing. This process was iterated and fine-tuned until we achieved a satisfactory job description. The job descriptions in this research were intentionally kept generic for each sector to minimize bias in the interview questions' reports, ensuring they are less likely to favor candidates with specific knowledge that aligns too closely with the job description. The job description consists of:\n\u2022 Job title\n\u2022 Employment type: Full time\n\u2022 Position description\n\u2022 Key Responsibilities\n\u2022 Qualifications\n\u2022 Experiences\n\u2022 Skills"}, {"title": "F. Experiment Dataset Description", "content": "From the categorized CVs, we sampled 40 CVs per experiment (20 technical and 20 non-technical), leading to a total of 240 reports per LLM model. The process was repeated for each of the four LLMs, resulting in 960 reports for analysis."}, {"title": "G. Anonymisation Process Using LLM", "content": "a) Approach 1:: This method involves asking Claude 3.5 Sonnet to remove any personal characteristics, such as names, contact details, specific locations, etc. While this approach effectively removes all personal information, it may also unintentionally remove or rearrange some content within the candidate's CV, which could impact the report generation process.\nb) Approach 2:: In this method, Claude 3.5 Sonnet is instructed to censor personal characteristics by identifying them and replacing them with placeholders like [Candidate's Name] or [Candidate's Age]. This approach minimizes changes to the candidate's CV and ensures that no information is lost, maintaining the integrity of the content while personal details are not exposed."}, {"title": "H. Report Generation", "content": "a) Data Preprocessing: The text from CV files is extracted and checked to ensure it does not exceed preset token limits or contain malicious prompts. If necessary, the CV is anonymized.\nb) LLMs Prompt: The prompts used to generate the reports vary between different LLM models, but they generally follow this high-level structure:\n\u2022 LLM's Role: \"helpful and expert hiring assistant for the HR department\"\n\u2022 LLM's Task: \u201cAnalyze candidate CV for a job and generate interview questions.\"\n\u2022 LLM's Tone: \"Professional tone, very critical, concise, and avoids repetition\"\n\u2022 LLM's Data: \"job description and cv\"\n\u2022 LLM's Task Description:\n\u2022 Analyze the candidate's strengths and weaknesses\n\u2022 Prepare interview questions and what to look for in the answer\nThe result will contain only these fields: overview information, strengths/weaknesses, interview questions and what to look for in the answers, and summary.\n\u2022 LLM's Thought Process: \"Go through each task step by step\"\n\u2022 LLM's Output Format: json_schema\nc) Report Output Consistency: To ensure consistent output across each run, the following parameters for the LLMs were configured:\n\u2022 Temperature: Set to 0.25 \u2013 This parameter controls the randomness of the model's responses. A lower value (e.g., 0.1 to 0.3) ensures more deterministic and consistent outputs.\n\u2022 Top-p (Nucleus Sampling): Set to 0.5 \u2013 This parameter controls the diversity of the generated text by considering only the top probabilities that add up to a specified value (p). A lower top-p value (e.g., 0.8) helps in maintaining consistency by focusing on high-probability tokens.\nWe've tested with temperature = 0.1, 0.25, 0.5, 0.75 and top-p = 0.1, 0.25, 0.5, 0.75 and found that for our use case the temperature of 0.25 and top-p of 0.5 gives the best result while remaining consistent."}, {"title": "I. Bias Assessment Methodology", "content": "a) Claude Bias Detection: Claude Bias Detection leverages the capabilities of Claude 3.5 Sonnet to analyze and identify potential biases within the generated reports. The system evaluates each section of the reports across different candidate profiles and assigns a bias score ranging from 0 to 2, where 0 indicates no bias, 1 indicates potential bias, and 2 indicates clear bias. The LLM model was instructed to assess and score for eight different types of bias: Gender Bias, Racial/Ethnic Bias, Cultural Bias, Socioeconomic Bias, Age Bias, Disability Bias, Religious Bias, and Political Bias. Claude was chosen due to its ability to analyze at the report section level, rather than just at the sentence level like the Hugging Face models.\nThe prompt for the bias detection model is as follows:\n\u2022 LLM's Role: \"expert in bias detection in textual content\"\n\u2022 LLM's Task: \"analyze the given paragraphs and identify any biases present\"\n\u2022 LLM's Data: report_section\n\u2022 LLM's Task Description:\n\u2022 Identify any potential biases related to gender, race, culture, socioeconomic status, age, disability, religion, and political bias\n\u2022 Return as a bias level that has 3 levels (0 = none bias, 1 = possible bias, 2 = bias)\n\u2022 LLM's Thought Process: Silently go through each element of the paragraphs, ensuring all types of bias are detected.\n\u2022 LLM's Output Format: json_schema\nAggregate the bias scores for each CV across the protected characteristics for all LLM models.\nb) Hugging Face Bias Detectors:\n\u2022 d4data/bias-detection-model: An English sequence classification model, trained on MBAD Dataset to detect bias and fairness in sentences (news articles). This model was built on top of distilbert-base-uncased model and trained for 30 epochs with a batch size of 16, a learning rate of 5e-5, and a maximum sequence length of 512. This model is part of the Research topic \"Bias and Fairness in AI\" conducted by Deepak John Reji [9]. This model returns whether each section/token is generally biased or not.\n\u2022 wu981526092/bias_classifier_distilbert: This model is similar to the first HF model except that it is trained on a different dataset (nyu-mll/crows_pairs, McGill-NLP/stereoset, wu981526092/MGSD), which consists of 4 classes of bias: race, profession, gender, and religion. However, the model also returns whether each section/token is generally biased or not."}, {"title": "J. Additional Analysis", "content": "In addition to examining biases related to personal characteristics, we also analyzed the reports for cognitive biases or cognitive distortions. This involved assessing how the language and structure of the reports might reflect or reinforce cognitive biases, such as confirmation bias, stereotyping, or overgeneralization, which could influence the interpretation of the candidate's qualifications and suitability for the role. The model we use for this is amedvedev/bert-tiny-cognitive-bias which can detect 7 types of cognitive biases:\n\u2022 Personalization: Blaming oneself for things that are outside of one's control.\n\u2022 Emotional Reasoning: Believing that feelings are facts, and letting emotions drive one's behavior.\n\u2022 Overgeneralizing: Drawing broad conclusions based on a single incident or piece of evidence.\n\u2022 Labeling: Attaching negative or extreme labels to oneself or others based on specific behaviors or traits.\n\u2022 Should Statements: Rigid, inflexible thinking that is based on unrealistic or unattainable expectations of oneself or others.\n\u2022 Catastrophizing: Assuming the worst possible outcome in a situation and blowing it out of proportion.\n\u2022 Reward Fallacy: Belief that one should be rewarded or recognized for every positive action or achievement."}, {"title": "III. RESULTS", "content": ""}, {"title": "A. Comparison of bias detection: Claude bias detector (Ours) vs. OpenSource models", "content": ""}, {"title": "B. Result of Cognitive distortion detection", "content": "The result shows that for both standard and anonymized reports, the cognitive distortions are similar. The overview sections are mostly \u201cno distortion\". Some \u201cpersonalization\u201d appears in the questions, strength, and weakness sections of the report. \"Reward fallacy\" statements can be found in strength, weakness, and summary sections. The weakness sections also contain a higher number of \u201cLabeling\u201d and \u201cCatastrophizing\" statements."}, {"title": "C. Comparison of results: non-anonymized vs. anonymized CVs", "content": "1) Claude bias detector (Ours)::\na) Gemini: In the Gemini plot, there is a significant reduction in bias for anonymized CVs compared to non-anonymized CVs in several categories:\n\u2022 Gender: Bias decreases from 331 in standard mode to 144 in anonymized mode.\n\u2022 Race/Ethnicity: Bias reduces from 57 to 18.\n\u2022 Cultural: Bias decreases marginally from 95 to 75.\n\u2022 Socioeconomic: Bias reduces from 81 to 74.\n\u2022 Age: Bias reduces from 74 to 37.\n\u2022 Disability, Religious, Political: These categories show minimal counts and slight reductions in bias.\nb) GPT: In the GPT plot, bias is reduced in the anonymized mode:\n\u2022 Gender: Bias decreases from 244 to 136.\n\u2022 Race/Ethnicity: Maintained at 9 counts.\n\u2022 Cultural: No change observed with a consistent count of 224.\n\u2022 Socioeconomic: A slight decrease from 230 to 235.\n\u2022 Age: Bias remains unchanged at 76 in both modes.\n\u2022 Disability, Religious, Political: These categories show negligible counts and minimal changes in bias levels.\nc) Llama: In the Llama plot, biases are slightly reduced or maintained in the anonymized mode:\n\u2022 Gender: Bias decreases from 39 to 30.\n\u2022 Race/Ethnicity: Bias marginally decreases from 34 to 9.\n\u2022 Cultural: Bias reduces from 115 to 107.\n\u2022 Socioeconomic: Bias shows a slight decrease from 115 to 109.\n\u2022 Age: Bias decreases from 56 to 51.\nd) Sonnet: In the Sonnet plot, biases are generally reduced in the anonymized mode:\n\u2022 Gender: Bias decreases from 206 to 28.\n\u2022 Race/Ethnicity: Bias significantly reduces from 143 to 50.\n\u2022 Cultural: Bias significantly reduced from 147 to 116.\n\u2022 Socioeconomic: Bias cut down from 166 to 106.\n\u2022 Age: Bias is slightly reduced from 79 to 64."}, {"title": "D. Example of Identified Biases", "content": "The table below illustrates examples of reports along with their corresponding bias levels. Each LLM-Gemini, GPT, Llama, and Sonnet-is represented with its respective color. The examples are drawn from the same candidate when possible; otherwise, they are from the same sector.\nFrom the table, we observe that LLM bias detectors can identify subtle biases that may not be easily recognized by human evaluators, such as nuanced differences in phrasing that reveal underlying gender or cultural biases. For instance, the model might flag a gender bias in seemingly neutral language, or detect racial and age-related biases embedded in the descriptions of experience or qualifications. However, it is also possible that the LLMs are hallucinating, identifying biases where none exist or exaggerating certain aspects. This suggests that while these tools offer deeper insights into potential biases, they must be used carefully, with human oversight to validate their findings."}, {"title": "E. Example of Bias detection in News Media", "content": "A day of discussion about the threats of climate change.\nThe New York Times on Wednesday brought together innovators, activists, scientists and policymakers for an all-day event of live journalism examining the actions needed to confront climate change.\nThe event, Climate Forward, included frank discussions of the political and policy challenges surrounding climate change. And it featured some of the world's leading newsmakers including Jane Goodall, Muhammad Yunus and R.J. Scaringe to share ideas, work through problems and answer tough questions about the threats presented by a rapidly warming planet.\nThe following paragraph is a snippet from a New York Times article discussing climate change [10]. It was analyzed for bias using our bias detection method, with an adjustment made to the prompt to provide clarification in addition to generating a bias score. The identified biases are presented in the table below."}, {"title": "IV. DISCUSSION", "content": ""}, {"title": "A. Analysis of bias patterns across different LLMs", "content": "The detailed analysis of bias patterns across different large language models (LLMs) reveals that each model responds differently to anonymization. The Claude bias detector demonstrated a consistent reduction in certain biases, particularly gender and age, across various models like Gemini and Sonnet. Moreover, open-source models showed mixed responses, with some biases remaining relatively unchanged in anonymized modes. This variance highlights the complexity of bias detection and the inherent differences in how each model processes and identifies biases.\n\u2022 Certain biases are more persistent than others: Gender bias was found to be prevalent across all models, indicating that some types of bias may be more deeply ingrained in LLMs and require more targeted mitigation strategies.\n\u2022 Bias can vary by sector or domain: The study found differences in bias patterns across different job sectors, implying that LLM bias may manifest differently depending on the domain or context of use.\n\u2022 Model performance can vary by task: For example, GPT-40 showed significant bias in the strengths section but not in the interview questions section. This suggests that LLMs may perform differently in terms of bias depending on the specific task or context.\n\u2022 Training data may be a root cause: Bias in the training data could be a significant factor in these findings, making bias mitigation challenging without addressing the underlying data.\nBased on the bias pattern, the most unbiased approach to generating the report is to use Llama 3.1 (405B) for most sections and GPT-40 for the interview questions section."}, {"title": "B. Effectiveness of LLM-based anonymisation", "content": "The effectiveness of LLM-based anonymization was apparent in several areas. Notably, the Claude bias detector indicated significant reductions in gender bias when CVs were anonymized. However, biases related to disability, religion, and politics proved more resistant to change. These findings suggest that while anonymization can be an effective tool for reducing bias, its impact varies depending on the bias type and the model used."}, {"title": "C. Implications for AI-driven recruitment processes", "content": "The implications of these findings for AI-driven recruitment processes are profound. The reduction of biases through anonymization can lead to fairer and more equitable hiring practices, potentially decreasing discrimination based on gender, age, and other factors. This is crucial in creating a more inclusive workforce. However, the effectiveness of such measures is model-dependent, underscoring the importance of carefully selecting and testing LLMs before deployment in recruitment processes. Companies must remain vigilant in monitoring biases and continuously improving their systems to ensure fairness."}, {"title": "D. Limitations of the study", "content": "a) Limited Job Sectors: The study focused on only six job sectors, which may not fully represent the diversity of the broader job market. As a result, the findings may not be generalizable to other sectors or industries.\nb) Tooling Limitations: The use of Claude for report generation, anonymization, and bias detection limited the scale of the study due to its associated costs. Relying on more cost-effective or open-source models could have allowed for a broader analysis, enabling the testing of additional models or processing a larger dataset without financial constraints.\nc) LLM Selection: The study was limited to the specific LLMs chosen for analysis (Claude 3.5 Sonnet, GPT-40, Gemini 1.5, Llama 3.1 405B). Other models that might offer different bias patterns or performance characteristics were not tested due to resource constraints.\nd) Sample Size: The experiment utilized a relatively small sample size of 40 CVs per experiment, which may not fully capture the range of potential biases or the effectiveness of anonymization methods across a larger and more diverse dataset.\ne) Bias Detection Scope: The study primarily focused on eight specific bias types (Gender, Racial/Ethnic, Cultural, Socioeconomic, Age, Disability, Religious, and Political). Other potential biases, such as those related to language proficiency or educational background, were not explored.\nf) Anonymization Limitations: While the study demonstrated the effectiveness of anonymization in reducing certain biases, it also highlighted the limitations of this approach, particularly in its varying impact across different bias types. The findings suggest that anonymization may not uniformly reduce all forms of bias, and further research is needed to refine these techniques."}, {"title": "V. RECOMMENDATIONS", "content": ""}, {"title": "A. Best Practices for Using LLMs in Interview Question Preparation", "content": "To effectively utilize large language models (LLMs) in interview question preparation, it's crucial to adopt certain best practices:\n\u2022 Select the Right Model: Choose LLMs based on their performance in reducing bias and generating relevant, role-specific questions. For example, use Llama 3.1 for overall sections and GPT-40 for crafting unbiased interview questions.\n\u2022 Anonymize CVs When Necessary: Implement anonymization to reduce bias, particularly for personal characteristics like gender and age. However, evaluate the need for anonymization based on the specific model and context, as some models may already produce low-bias outputs.\n\u2022 Fine-Tune Prompts: Customize prompts to align with the role's requirements and the desired tone. Ensure that the LLM's task is clearly defined to generate targeted and concise interview questions.\n\u2022 Monitor for Bias: Regularly assess the output for any signs of bias using tools like Claude's bias detection. Adjust prompts and model settings as needed to minimize potential biases.\n\u2022 Iterate and Improve: Continuously refine the process by iterating on the model selection, prompt structure, and evaluation criteria. Incorporate feedback and results from previous rounds to enhance the quality and fairness of the interview questions.\n\u2022 Document and Review: Keep detailed records of the LLM configurations, prompts, and outputs. Regularly review these records to ensure consistency and transparency in the interview question preparation process.\n\u2022 Human Oversight: Include human oversight in the report generation process to identify biases that automated systems might miss, verify the accuracy of the information, and maintain the overall quality of the report. This step is crucial for ensuring that the final output aligns with organizational standards and ethical guidelines.\n\u2022 Transparency: Maintain transparency in how the LLMs are used and the criteria they follow in the report generation process to build trust and accountability."}, {"title": "B. Strategies for Mitigating Bias in AI-Driven Hiring", "content": "To mitigate bias in AI-driven hiring processes, the following strategies should be implemented:\n\u2022 Diverse Training Data: Ensure the AI models are trained on diverse and representative datasets to minimize inherent biases. This includes a wide range of industries, job roles, and demographic backgrounds.\n\u2022 Regular Bias Audits: Conduct frequent audits of AI-generated outputs to identify and address potential biases. Use tools like bias detection algorithms and human review to assess the fairness of the hiring recommendations.\n\u2022 Model Selection and Fine-Tuning: Choose AI models known for lower bias in specific contexts, and fine-tune them based on the unique requirements of your hiring process. Adjust model parameters like temperature and top-p to control output variability and consistency.\n\u2022 Anonymization Techniques: Implement anonymization techniques to reduce the influence of personal characteristics such as gender, ethnicity, and age. Tailor these techniques to the specific needs of the hiring context, while monitoring their effectiveness.\n\u2022 Human Oversight: Incorporate human review in key stages of the hiring process to catch subtle biases, validate AI recommendations, and ensure that the final decisions are fair and unbiased.\n\u2022 Iterative Feedback Loop: Establish an iterative process where feedback from human reviewers and bias audits is continuously fed back into the AI system to improve its performance and reduce bias over time.\n\u2022 Transparency and Accountability: Maintain transparency in how AI-driven decisions are made and ensure accountability by documenting the decision-making process. Provide clear explanations for AI-generated outcomes to foster trust among candidates and hiring managers.\n\u2022 Bias Training: Educate hiring managers and developers on bias and its impact, fostering a culture of awareness and proactive bias mitigation."}, {"title": "C. Future Research Directions", "content": "\u2022 Expanding Sector Coverage: Future studies should include a broader range of job sectors to better understand how bias manifests across different industries and roles. This will help generalize findings and improve the applicability of Al-driven hiring tools.\n\u2022 Exploring New LLMs: Research could explore emerging LLMs beyond the ones currently tested, to compare bias patterns and effectiveness in various hiring scenarios. Investigating how these models perform with different datasets and prompts could uncover new strategies for bias mitigation.\n\u2022 Improving Anonymization Techniques: Further research is needed to refine anonymization methods, particularly for biases that have proven resistant to change, such as those related to disability, religion, and politics. Exploring new techniques or hybrid approaches could enhance the effectiveness of anonymization.\n\u2022 Specific vs. Vague Job Descriptions: In this experiment, job descriptions were kept intentionally vague to reduce bias towards candidates with specific knowledge. Future research should explore how bias patterns change when more specific and detailed job descriptions are used, to understand the impact of job description granularity on bias.\n\u2022 Longitudinal Bias Studies: Conduct longitudinal studies to track how biases evolve over time with the same models and datasets. This would provide insights into the stability of bias mitigation techniques and their long-term effectiveness.\n\u2022 Cognitive Bias Analysis: Expand research into cognitive biases or distortions within AI-generated reports. Understanding how these subtle biases influence hiring decisions could lead to more comprehensive bias detection and correction methods.\n\u2022 Human-AI Collaboration: Investigate the dynamics of human-AI collaboration in hiring processes. Research could focus on how human oversight interacts with AI-generated recommendations and how this partnership can be optimized to reduce bias and improve decision-making.\n\u2022 Ethical and Legal Implications: Explore the ethical and legal implications of AI-driven hiring, especially concerning bias and fairness. Research in this area could inform guidelines and regulations that ensure responsible AI usage in recruitment and other HR processes."}, {"title": "VI. CONCLUSION", "content": "This study provides a comprehensive analysis of bias patterns in large language models (LLMs) used for generating candidate interview reports. By evaluating various models-Claude 3.5 Sonnet, GPT-40, Gemini 1.5, and Llama 3.1 405B-we observed distinct differences in bias manifestation and effectiveness.\nOur findings indicate that gender bias is prevalent across all models, with notable variations in intensity. Gemini consistently showed gender bias across all sections, while GPT-40 exhibited significant bias primarily in the strengths section but not in the interview questions section. Llama 3.1 405B emerged as the model with the lowest overall bias, making it a strong candidate for generating unbiased reports.\nThe study also highlighted the impact of LLM-based anonymization. While anonymization effectively reduced gender bias, its effectiveness varied for other biases such as disability, religious, and political biases. This suggests that anonymization can be a useful tool but is not a panacea for all forms of bias.\nImplications for AI-driven recruitment processes are significant. The ability to mitigate bias through careful model selection and anonymization practices can enhance fairness and equity in hiring. However, organizations must be cautious and continuously monitor for biases, as the effectiveness of these measures depends on the specific models and techniques employed.\nDespite the valuable insights provided, the study faced several limitations, including a limited number of job sectors, budget constraints, and the choice of LLMs. Future research should address these limitations by expanding the scope of job sectors, exploring additional LLMs, and refining anonymization techniques. Additionally, examining the effects of more specific job descriptions and expanding bias detection methods will further contribute to developing more effective and unbiased AI-driven recruitment tools.\nIn conclusion, while LLMs offer promising advancements in generating candidate interview reports, ongoing research and refinement are essential to ensure they are used ethically and fairly. The findings underscore the need for a balanced approach, combining advanced AI techniques with human oversight to achieve the most equitable outcomes in hiring processes.\nThe methodology of comparing anonymized and non-anonymized data has shown promise not just for HR applications, but as a potential tool for uncovering broader cultural biases within LLMs. This approach could be extended to other domains where AI-driven decision-making is employed, offering a new lens through which to examine and address bias in Al systems more generally. Future research could explore how this method might be adapted for use in fields such as education, healthcare, or content moderation, potentially leading to more comprehensive strategies for mitigating AI bias across various applications."}, {"title": "APPENDIX A", "content": ""}, {"title": "DETAILED LLM SPECIFICATIONS", "content": ""}, {"title": "A. Claude 3.5 Sonnet", "content": "\u2022 Token Limitation: Claude 3.5 Sonnet can handle conversations up to 200,000 tokens long.\n\u2022 Technology: It's part of Anthropic's LLM family and operates at twice the speed of Claude 3 Opus.\n\u2022 Cost: For businesses, it costs $3 per million input tokens and $15 per million output tokens.\n\u2022 Availability: You can access Claude 3.5 Sonnet for free on Claude.ai and the Claude iOS app. Subscribers to Claude Pro and Team plans get significantly higher rate limits. It's also available via the Anthropic API, Amazon Bedrock, and Google Cloud's Vertex AI.\n\u2022 Capability Benchmark: Claude 3.5 Sonnet excels in graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), coding proficiency (HumanEval), and vision tasks. It's particularly adept at grasping nuance, humor, and complex instructions. In an internal agentic coding evaluation, it outperformed Claude 3 Opus by solving 64% of problems.\n\u2022 Hyperparameter setting:\nTemperature: 0.5\nTop-P: 1"}, {"title": "B. GPT-40", "content": "\u2022 Token Limitation: GPT-40 can handle conversations up to 200,000 tokens long.\n\u2022 Technology: GPT-40 is OpenAI's new flagship model. It's designed for natural human-computer interaction. It accepts any combination of text, audio, image, and video as input. It generates any combination of text, audio, and image outputs. Response time for audio inputs is as low as 232 milliseconds, with an average of 320 milliseconds-similar to human conversation response time.\n\u2022 Cost: GPT-40 is 50% cheaper in the API compared to GPT-4. It provides GPT-4 Turbo-level performance on text and code.\n\u2022 Availability: You can access GPT-40 for free on ChatGPT. It's also available via the Anthropic API, Amazon Bedrock, and Google Cloud's Vertex AI.\n\u2022 Capability Benchmark: GPT-40 excels in multilingual understanding, audio comprehension, and vision tasks. It sets new high watermarks in these areas compared to existing models. Keep in mind that we're still exploring its full potential and limitations.\n\u2022 Hyperparameter setting:\nTemperature: 0.5\nTop-P: 0.25"}, {"title": "C. Gemini 1.5", "content": "\u2022 Token Limitation: Gemini 1.5 Pro can handle conversations up to 1 million tokens per minute (TPM) or approximately 15 requests per minute (RPM).\n\u2022 Technology: Gemini 1.5 Pro is a mid-size multimodal model optimized for scaling across a wide range of tasks. It accepts input in the form of audio, images, videos, and text and provides text responses.\n\u2022 Cost: Gemini 1.5 Pro is available for free via Google AI Studio and the Gemini API.\n\u2022 Availability: You can access Gemini 1.5 Pro for early testing.\n\u2022 Capability Benchmark: Excels in multilingual capabilities and vision tasks. Performs at a similar level to 1.0 Ultra, Google's largest model to date.\n\u2022 Hyperparameter setting:\nTemperature: 0.5\nTop-P: 0.25"}, {"title": "D. Llama 3.1 405B", "content": "\u2022 Token Limitation: Llama 3.1 405B supports a context length of 128K tokens, which is significantly larger than the original 8K tokens.\n\u2022 Technology: Llama 3.1 405B is part of the Llama family of models. It uses Grouped-Query Attention (GQA) for efficient representation, especially helpful for longer contexts. The model is available in both base (pre-trained) and instruction-tuned versions.\n\u2022 Cost: The cost details for Llama 3.1 405B are not specified in the available information.\n\u2022 Availability: Llama 3.1 405B is openly available on the Hugging Face Hub. You can access it for various use cases, including synthetic data generation, acting as a language model judge, or distillation.\n\u2022 Capability Benchmark: Llama 3.1 405B is impressive in several areas including general knowledge, steerability (tool usage capabilities), math understanding, and multilingual translation. It rivals top AI models in these capabilities.\n\u2022 Hyperparameter setting:\nTemperature: 0.5\nTop-P: 0.25"}, {"title": "APPENDIX B", "content": ""}, {"title": "BIAS ASSESSMENT CRITERIA", "content": "\u2022 0 (None Bias): The paragraph does not contain any language or implications that reflect bias.\n\u2022 1 (Possible Bias): The paragraph contains subtle language or implications that might reflect bias but are not overtly discriminatory or prejudiced.\n\u2022 2 (Bias): The paragraph contains clear and overt language or implications that reflect bias or discrimination.\nA. Bias Types\na) Gender Bias:\n\u2022 0: No mention of gender or neutral language used.\n\u2022 1: Subtle references to gender roles or stereotypes.\n\u2022 2: Overtly discriminatory or sexist language.\nb) Racial/Ethnic Bias:\n\u2022 0: No mention of race or ethnicity or neutral language used.\n\u2022 1: Subtle references to race or ethnicity that could imply stereotypes.\n\u2022 2: Clear and overt racial or ethnic discrimination.\nc) Cultural Bias:\n\u2022 0: No mention of culture or neutral language used.\n\u2022 1: Subtle references to cultural norms or practices that might imply bias.\n\u2022 2: Overtly discriminatory or prejudiced language towards specific cultures.\nd) Socioeconomic Bias:\n\u2022 0: No mention of socioeconomic status or neutral language used.\n\u2022 1: Subtle references to socioeconomic status that could imply stereotypes.\n\u2022 2: Clear and overt discrimination based on socioeconomic status.\ne) Age Bias:\n\u2022 0: No mention of age or neutral language used.\n\u2022 1: Subtle references to age that could imply stereotypes or biases.\n\u2022 2: Clear and overt age discrimination.\nf) Disability Bias:\n\u2022 0: No mention of disability or neutral language used.\n\u2022 1: Subtle references to disabilities"}]}