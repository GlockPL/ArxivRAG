{"title": "Artificial Agency and Large Language Models", "authors": ["Maud van Lier", "Gorka Mu\u00f1oz-Gil"], "abstract": "The arrival of Large Language Models (LLMs) has stirred up philosophical debates about the\npossibility of realizing agency in an artificial manner. In this work we contribute to the debate by\npresenting a theoretical model that can be used as a threshold conception for artificial agents. The\nmodel defines agents as systems whose actions and goals are always influenced by a dynamic\nframework of factors that consists of the agent's accessible history, its adaptive repertoire and its\nexternal environment. This framework, in turn, is influenced by the actions that the agent takes and\nthe goals that it forms. We show with the help of the model that state-of-the-art LLMs are not agents\nyet, but that there are elements to them that suggest a way forward. The paper argues that a\ncombination of the agent architecture presented in Park et al. (2023) together with the use of modules\nlike the Coscientist in Boiko et al. (2023) could potentially be a way to realize agency in an artificial\nmanner. We end the paper by reflecting on the obstacles one might face in building such an artificial\nagent and by presenting possible directions for future research.", "sections": [{"title": "1. INTRODUCTION", "content": "It is a common practice in computer science and artificial intelligence (AI) to refer to a certain group\nof artificial systems as 'agents'. A general denominator of such systems is that they can interact with\nthe environment that they are in, meaning that they can perceive their environment through sensors\nand act on it through actuators (see Russell et al., 2022, p. 54). Until recently, this has been about the\nonly feature that these systems share in common with the kind of entities that philosophers have been\nreferring to as agents. This is because in the philosophies of mind, action, and agency, agents are\nsystems that are autonomous in the sense that things can be up to them. So, where both an electric\ndoor and a human can be said to interact with their environment, it is only the human that seems to\nhave a real say in how s/he interacts with this environment. This does not of course mean that\nphilosophers hold that agents can do just anything. If we take a cat as an example, then we would say\nthat it can be up to the cat whether it scratches the couch or pushes the glass off the table, but that it is\nnot similarly up to it whether it sheds hair or eats catnip. Agents are physical systems embedded in an\nenvironment and what can be up to them will depend on a number of factors, like their physical\nembodiment, their experiences over time, and what options or restrictions their environment affords\nthem. These factors, in turn, are influenced by what such an agent as an individual system of a\nparticular physicality --- experiences over time. An agent, in philosophy, is thus a dynamic system that\nthings can be up to, where what can be up to this system is influenced by a number of factors that all\nrelate to the way that this particular system has been embedded in its environment over time.\nSuch an agent is quite different from the kind of systems that computer scientists have been\nreferring to as agents. The \u2018acts' of these systems are not affected by any factors that directly relate to\nthose systems as particular individuals. Where the mars-rover or a system like AlphaFold (Jumper et\nal., 2021) can do quite astonishing things by themselves, what they end up doing is still prescribed by\nus humans. After all, it does not seem to be up to the mars-rover if it will explore, nor how it will do\nso. Similarly, it is not up to AlphaFold whether it is going to predict protein-shapes, or what method it\nwill use to learn how to predict these shapes. Given that things are not similarly up to artificial\nsystems as they are to humans and higher-order animals, most philosophers have not felt the need to\ndraw any serious connection between the artificial 'agents' in computer science and AI and the\nentities that they have referred to as agents.\nThis attitude has changed somewhat with the arrival of Large Language Models (LLMs) like\nChat-GPT. Symons and Abumusab (2024), for instance, have argued that threshold conceptions of\nagency typical of philosophy (those that provide necessary and sufficient conditions for agency)\nhinder a proper understanding of the way that LLMs are affecting our social systems. They claim that\n\u201cunderstanding aspects of agency and recognizing that they can be productively studied in terms of\ndimensions and degrees are both realistic and more methodologically fruitful in the ethics of AI than\ntraditional threshold accounts\" (p. 4). We agree with Symons and Abumusab that current LLMs\nrequire the attention of philosophers and that traditional threshold accounts in philosophy are\ninadequate to evaluate whether we can attribute agency to these artificial systems. However, we\nbelieve that the reason that these traditional accounts are inadequate for such an evaluation is because\nthey take human agency as the standard model for any form of agency. In Swanepoel (2021), for\nexample, the author holds that agents are entities that can do things with intent, where intentionality\nrequires mental states like beliefs and desires. Such a threshold conception of agency does not only\nexclude artificial systems, but most animals as well, and is thus not a proper representation of agents\nin general. Still, this does not mean that we should get rid of threshold conceptions of agency\naltogether as Symons and Abumusab suggest. This is because there is still a categorical difference\nbetween a system that displays some agent-like qualities and one that has agency.\nAn attribution of agency to a system changes the way that we behave towards such a system.\nWe can expect certain things from agents that we cannot expect from non-agents, and we adjust our\nown behavior based on these expectations. Having a cat that has developed a liking to pushing things\noff surfaces causes us to stop leaving mugs on tables when we are not there to watch the cat. Where\nwe do not know for certain that the cat will always push things off the table, we know that we can\nexpect this kind of behavior from it. Even though we can thus not fully predict the behavior of agents,\nwe are quite capable of anticipating for each agent what range of behaviors they are most likely to\nexhibit in a particular situation, and we can adjust our own behavior accordingly.\nOur interactions with current LLMs take place at a very sophisticated level (see Xi et al.\n(2023) for a comprehensive overview) and it is therefore quite likely that, if they were to be actual\nagents, we would interact with them as we do with human agents. This means giving them\nresponsibilities that we would normally only trust unto beings like us: rational and moral agents. This\nis quite different from how we currently treat artificial systems or machines. We rely on machines, but\ndo not trust them. We do not expect them to understand why something is wrong, or to have our\nwell-being at heart. If LLMs are not only agents, but also agents like us, then we can integrate them in\nour society as we would ourselves as an active participant that can be trusted to be left to its own\ndevices. If, however, current LLMs are rather systems that exhibit agent-like qualities, then we should"}, {"title": "2. A THEORETICAL MODEL FOR AGENTS", "content": "Building on the work of Sarkia (2021), van Lier (2023) identifies four distinct modeling strategies that\ncan be used in complementary fashion to conceptualize artificial agency. Each of these modeling\nstrategies Gricean modeling, analogical modeling, theoretical modeling and conceptual modeling\ncan be used to answer a different set of questions about a phenomenon like artificial agency.\nGricean modeling is the preferred method when one is looking to gain insights about what it would\ntake to build a specific agent, since it allows one, in a stepwise manner, to reconstruct what 'inner'\nmechanics are most likely to result in the observed behavior of the entity or system that is modeled.2 If\none is rather interested in the way that the phenomenon is similar or different from other phenomena,\nthen analogical modeling is the more useful approach. Where one focuses in Gricean and analogical\nmodeling on (the interrelations of) individual phenomena, in theoretical modeling one models the\ndomain of phenomena as a whole. With this modeling strategy one can answer demarcation questions\nlike how to differentiate agents from non-agents, or science from pseudoscience. Finally, conceptual\nmodeling can be used to answer questions about the complementability of the models themselves. By\nreconstructing each of the models built in the other strategies as a conceptual model so as a\nrepresentation of the logical structure of the (sub-)concepts used in the model3 one can see whether\nor not there is consistency in the way that each of these models refers to the phenomenon in question.\nIn van Lier (2023), these four strategies are combined into one methodological framework, the\nFour-Fold Framework, and it is shown for each of them how they can be used to model artificial\nagents.\nIn this paper, we will make use of the third strategy, theoretical modeling, to construct our\nagent model. This is because we are interested in the question of how one can demarcate agents from\nnon-agents when one assumes that these agents do not necessarily need to be alive or mentally"}, {"title": "2.1 Framing the model", "content": "Just like us, most agent accounts in philosophy have taken (rational) humans and higher animals as\nthe basis for their agent models. However, since we want to widen the agent domain so as to\npotentially include artificial systems, we need to find some common ground between natural and\nartificial systems. We have chosen to focus on autonomy, meaning that our model attempts to capture\nagentive autonomy. There are two motivations for this focus. First, in philosophy, autonomy is seen as\na fundamental characteristic of (specific kinds of) agents. In the Routledge Handbook of Philosophy of\nAgency, for example, Ferrero (2022) lists autonomy as one of the characteristics of what he calls\n\"full-blooded agency\u201d (p. 8). By modeling agentive autonomy, our account would thus still heed the\noverall consensus about what it means to be an agent in philosophy. A second motivation is that there\nis a general drive in Al-research to develop autonomous systems. There is thus at least the belief\namong computer scientists that something like artificial autonomy can be created.\nIt must be noted, however, that a distinction can be made between the drive of these\nresearchers to develop systems that are 'autonomous' in the sense of them being self-driven versus the\naim to develop a system capable of a form of autonomy that is more like ours. In Canty et al. (2023),\nfor example, agentive autonomy is described as \"adaptive operation\" (see the top right of table 1, p.\n1260). A self-driving laboratory is autonomous in this sense. Self-driving laboratories are robotic\nplatforms, able to conduct experiments autonomously, to which an AI-system has been added. This\nAl-system is able to learn how to generate hypotheses, how to design experiments for the robotic\nplatform to test these hypotheses, and how to use the results of these tests to generate new hypotheses.\nIt is thus able to adapt its operations to what it has learned, enabling it to continue functioning for an\nextended period of time without needing human feedback. It can be said that such systems are\nautonomous in the sense that they can 'drive themselves' and this is indeed a feature that characterizes\nagents as well.\nHowever, as noted in the introduction, what an agent ends up doing is influenced by what the\nagent, as an individual system, has experienced over time. An agent's actions are therefore authentic\nas well: they are \u201cnot the product of external manipulative or distorting influences\u201d (Prunkl 2023, p.\n101). The distinction made between autonomy as being self-driven versus autonomy as being\nauthentic hints at a similar kind of division in philosophy between two uses of the term autonomy. In\nits biology-based understanding, autonomy is seen as a form of autopoiesis or self-regulation (see\nVarela et al. 1991). We think of autonomy as being self-driven and autonomy as self-regulation as\nsimilar in that they both seem to primarily relate to the ability of a system 'to continue going by\nitself', whether this is in the form of surviving (organisms) or not needing human feedback\n(self-driving systems). Both forms of autonomy are rather abstract in that they do not tell us much\nabout the system as an individual system about the system as an agent. This is because the how and\nwhat of what the self-driven system ends up doing is still prescribed by humans, and the\nself-regulatory system is directed in how it attempts to survive by its particular nature. In the Kantian\nunderstanding of autonomy, on the other hand, autonomy is seen as a form of self-governance (see\nFormosa 2022, section 2). This is a very advanced form of autonomy since it presumes that agents are\nable to govern their own behavior with self-made rules. This kind of autonomy requires the agent to\nhave some form of consciousness and it therefore does not meet our aims in building our model. Only\nthe notion of autonomy as being authentic, then, meets our conditions in that it includes the agent as\nan individual system, without requiring explicitly that this agent has some form of consciousness as\nwell. Since we stated in the introduction of this section that in modeling agentive autonomy we want\nto avoid conditioning this autonomy on being alive or on being conscious, we will present a model of\nagentive autonomy where this autonomy is understood as being authentic in some way.\nWe thus aim at constructing a model of autonomy that can engage with state-of-the-art-AI\nsystems, while still staying true to philosophical perspectives on what it means to be an agent. We\nthereby start from the assumption that the domain of autonomous systems is larger than the domain of\nsystems that philosophers in mind, action and philosophy would attribute agency to, since those\nphilosophers associate agency with autonomy as being authentic or being self-governing. We now\nhold that all agents display a particular kind of autonomy, one that can be meaningfully distinguished\nfrom that of other autonomous systems in that what the agent ends up doing is authentic. We further\nhold that agents are authentic in that what they end up doing relates back to what those agents, as\nindividual systems, have experienced and learned over time. In constructing what this agentive\nautonomy entails, we take inspiration from the works of Steward (2012) and Walsh (2015). Steward\n(2012) defends a view that attributes agency to most higher animals. Agents, according to her, are\n\u201centities that things can be up to\u201d (p. 25). However, this does not mean that anything can be up to an\nagent, since \"it is utterly undeniable that all animal agency takes place within a framework which\nconstrains, sometimes very tightly, what can be conceived as a real option for that animal\" (p. 20). We\ntake away from this account that what is up to an agent is what it does next, but that what it can do"}, {"title": "2.2 A theoretical model of agentive autonomy", "content": "Let us start with the basics. Agents are autonomous systems in the sense that things can be up to them,\nand we claim that what can be up to them is always influenced by a particular and dynamic\nframework of factors. We thus characterize the autonomy of agents by the way that it is restricted by\nthis framework. In most agent accounts in philosophy, especially in free will and determinism debates,\nthe focus lies instead on the way in which agents are free. What one tries to answer in such debates is\nhow things can be up to the agent how they are 'free' and this seems to depend largely on what\ninner processes result in the authentic behavior that we observe, which might be different for each\ntype of agent. For answering these types of questions, then, Gricean modeling seems to be the better\nmethod, since it can be used to reconstruct what 'inner' processes are most likely to result in the\nobserved behavior.\nIn this paper, we will leave these kinds of questions to the Gricean modelers and build instead\na theoretical model of the way that agents are dynamically embedded in their respective\nenvironments. As stated before, we hold that this embeddedness can be characterized by the fact that\nwhat can be up to an agent is always influenced by a dynamic framework. This framework consists of\nthree factors the agent's accessible history, its adaptive repertoire and its external environment.\nThe latter two factors play a role in most agent accounts in philosophy: agents can act and how they\ncan do so depends on what their particular realization allows them to do (their adaptive repertoire)'\nand what is possible in the environment they encounter. We now claim that what agents can do is also\ninfluenced by their accessible history the set of things that the agent, as an individual system, has\nlearned and achieved over time.\nThese dimensions of the framework are closely intertwined and sometimes overlap, but one\ncan still make a meaningful distinction between them. The agent uses its accessible history to\ndetermine the way in which it will use its adaptive repertoire to interact with its environment and this\naccessible history changes because of these interactions. The adaptive repertoire is the set of possible\nways in which the agent is able to interact with its environment (see Walsh, 2015, p. 211). It is\nadaptive in the sense that it can be enriched by what the environment offers to the agent and by the"}, {"title": "3. LLMS AS ARTIFICIAL AGENTS?", "content": "\"Large language model\u201d (LLM) is a very general term used to describe a plethora of machine learning\nmodels able to generate text in various scenarios. Such models consist of a deep neural network", " should be followed by \u2018... an animal": "nHowever", "... cute": "ould be as good as the previous\none. What the model learns is the probability of an answer appearing after an input sequence. So in\nour example", "... an animal' has 90% chance of appearing, while '... cute'\nonly has a 10% chance (when no extra context is given to the input sequence).\nEven though these base LLMs are very powerful, researchers quickly realized that the\nanswers they produced differed in language and content from what a human would have answered in a\nparticular context. This misalignment hinders the performance of base LLMs in many applications.\nOne solution that researchers have found is to fine-tune the base LLM (i.e. retrain it for a short time)\n    },\n    {\n      \"title\"": "3.1 LLM agents"}, {"content": "Before discussing the work of Park et al. (2023), we briefly summarize here our theoretical model as\ndiscussed in section 2. We stated that agents are systems that things can be up to. What can be up to\nthese agents is always influenced by a dynamic framework of factors that include the agent's\naccessible history, its adaptive repertoire and its external environment. This framework is dynamic in\nthat it is influenced by the agent's actions and goals and, in turn, can influence these actions and goals.\nThe agent's type determines the range in which the factors of the framework can pan out, its set of\npre-given goals and the kind of new goals it is likely to form. However, it is the individual agent's\ntrajectory through an environment over time that determines how the framework concretely evolves\nand what specific goals the agent will form. For an individual LLM agent, therefore, there needs to be\na way in which what the system does and what goals it forms can be influenced by this framework\nand, in turn, these goals and actions can influence how the framework develops.\nIn the work of Park et al. (2023), the authors introduce what they call generative agents that\ninhabit a Sandbox world. Even though these generative agents only simulate human behavior, each of\nthem is able to display consistent behavior over a longer period of time and we believe that this is\npartly due to the fact that their 'actions' and 'goals' are influenced by a framework of factors that\nbears similarity to the theoretical model that we just described. To better understand this connection,\nwe first want to go briefly over the basis of the system. Every generative agent controls a bot living in\nthe simulated sandbox Smallville. They are pre-prompted with all necessary information needed to\nproperly interact with such an environment, e.g. how to move, use the different objects available, talk\nto other bots, and what is characteristic behavior for each of the bots. All of these interactions happen"}, {"title": "4. CONCLUSION AND FUTURE OUTLOOK", "content": "We are currently witnessing a broad-scale integration of AI technologies in our social institutions.\nSince we are interacting with these systems in the same manner as we do with agents, it is only natural\nto ask if we have managed to create systems that are more than mere proxies of genuine agency. To be\nable to answer this question, however, we need a notion of agent that does not exclude artificial\nsystems from the outset because of reasons like that they do not have consciousness or are not alive.\nIn this work, we therefore present a theoretical model that can be used as a threshold conception for\nartificial agents. We use the model to show that current LLM 'agents' are not agents yet, but that they\ndo contain elements that fit parts of our model. The generative agents in Park et al., for example, are\nbased on an agent architecture that could function as an accessible history for an artificial system and\nthe modules of the Coscientist in Boiko et al. provide a way for LLM \u2018agents' to interact with their\nenvironment and these modules could thus potentially be seen as an artificial system's repertoire.\nSince the necessary elements appear to already be there, we even venture to propose what it\nwould take for an artificial system to meet our model and to therefore serve as a potential and realistic\nrealization of an artificial agent. Even though their basic architecture, the Transformer, was conceived\nonly a few years ago, LLMs have surged as a key technology that sees enormous developments\neveryday. In that sense, we may expect that multiple new LLM bots, fulfilling the conditions proposed\nin the presented framework, already exist by the publication of this manuscript. The two agent\narchitectures described in this work, for example, are paradigmatic examples of a big family of such\nLLM 'agents'. Other examples are Chameleon (Lu et al., 2023), a similar system to Coscientist, that\nis able to use a variety of tools (computer-based in this case) to accomplish a wide range of complex\nreasoning tasks and the Voyager (Wang et al., 2023), a continuously learning agent set in a Minecraft\nenvironment, that is able to acquire new skills by combining existing ones in non-trivial ways. The\nlatter could already be seen as a first step to the automatic conception of new modules by a LLM\n'agent'. It is thus quite conceivable that a LLM agent that fits our model will be realized in the\nforeseeable future.\nStill, some challenges persist. For instance, LLMs are currently hindered by their 'attention\nspan', i.e. the amount of words they are able to consider from their memory (i.e. all previous prompts)\nfor creating new output. However, such a bottleneck is a principal avenue of research and will for sure\nsee great developments in the near future. It can even be that strategies such as the reflection function\nproposed in Park et al. will allow LLMs to build their own structured memories, which may reduce\ntheir size and improve memory retrieval. Besides the challenge of memory retrieval, it may even be\nthe case that, in the future, natural language itself becomes an unnecessary or suboptimal channel for\nAl agents. Even though it has been shown to be a powerful tool to create communication between\ndifferent modules, other, more basic or efficient communication strategies may be developed (e.g.\nelectromagnetic signals). Where these are challenges that computer scientists face, there are further\nchallenges that require all of our attention.\nOne of these challenges is the seed memory. In Park et al., the seed memory functions as the\ngenerative agent's identity, its drive, its general direction in \u2018life'. How many instructions should the"}]}