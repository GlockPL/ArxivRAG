{"title": "Artificial Agency and Large Language Models", "authors": ["Maud van Lier", "Gorka Mu\u00f1oz-Gil"], "abstract": "The arrival of Large Language Models (LLMs) has stirred up philosophical debates about the\npossibility of realizing agency in an artificial manner. In this work we contribute to the debate by\npresenting a theoretical model that can be used as a threshold conception for artificial agents. The\nmodel defines agents as systems whose actions and goals are always influenced by a dynamic\nframework of factors that consists of the agent's accessible history, its adaptive repertoire and its\nexternal environment. This framework, in turn, is influenced by the actions that the agent takes and\nthe goals that it forms. We show with the help of the model that state-of-the-art LLMs are not agents\nyet, but that there are elements to them that suggest a way forward. The paper argues that a\ncombination of the agent architecture presented in Park et al. (2023) together with the use of modules\nlike the Coscientist in Boiko et al. (2023) could potentially be a way to realize agency in an artificial\nmanner. We end the paper by reflecting on the obstacles one might face in building such an artificial\nagent and by presenting possible directions for future research.", "sections": [{"title": "1. INTRODUCTION", "content": "It is a common practice in computer science and artificial intelligence (AI) to refer to a certain group\nof artificial systems as 'agents'. A general denominator of such systems is that they can interact with\nthe environment that they are in, meaning that they can perceive their environment through sensors\nand act on it through actuators (see Russell et al., 2022, p. 54). Until recently, this has been about the\nonly feature that these systems share in common with the kind of entities that philosophers have been\nreferring to as agents. This is because in the philosophies of mind, action, and agency, agents are\nsystems that are autonomous in the sense that things can be up to them. So, where both an electric\ndoor and a human can be said to interact with their environment, it is only the human that seems to\nhave a real say in how s/he interacts with this environment. This does not of course mean that\nphilosophers hold that agents can do just anything. If we take a cat as an example, then we would say\nthat it can be up to the cat whether it scratches the couch or pushes the glass off the table, but that it is\nnot similarly up to it whether it sheds hair or eats catnip. Agents are physical systems embedded in an\nenvironment and what can be up to them will depend on a number of factors, like their physical\nembodiment, their experiences over time, and what options or restrictions their environment affords\nthem. These factors, in turn, are influenced by what such an agent as an individual system of a\nparticular physicality --- experiences over time. An agent, in philosophy, is thus a dynamic system that\nthings can be up to, where what can be up to this system is influenced by a number of factors that all\nrelate to the way that this particular system has been embedded in its environment over time.\nSuch an agent is quite different from the kind of systems that computer scientists have been\nreferring to as agents. The \u2018acts' of these systems are not affected by any factors that directly relate to\nthose systems as particular individuals. Where the mars-rover or a system like AlphaFold (Jumper et\nal., 2021) can do quite astonishing things by themselves, what they end up doing is still prescribed by\nus humans. After all, it does not seem to be up to the mars-rover if it will explore, nor how it will do\nso. Similarly, it is not up to AlphaFold whether it is going to predict protein-shapes, or what method it\nwill use to learn how to predict these shapes. Given that things are not similarly up to artificial\nsystems as they are to humans and higher-order animals, most philosophers have not felt the need to\ndraw any serious connection between the artificial 'agents' in computer science and AI and the\nentities that they have referred to as agents.\nThis attitude has changed somewhat with the arrival of Large Language Models (LLMs) like\nChat-GPT. Symons and Abumusab (2024), for instance, have argued that threshold conceptions of\nagency typical of philosophy (those that provide necessary and sufficient conditions for agency)\nhinder a proper understanding of the way that LLMs are affecting our social systems. They claim that\n\u201cunderstanding aspects of agency and recognizing that they can be productively studied in terms of\ndimensions and degrees are both realistic and more methodologically fruitful in the ethics of AI than\ntraditional threshold accounts\" (p. 4). We agree with Symons and Abumusab that current LLMs\nrequire the attention of philosophers and that traditional threshold accounts in philosophy are\ninadequate to evaluate whether we can attribute agency to these artificial systems. However, we\nbelieve that the reason that these traditional accounts are inadequate for such an evaluation is because\nthey take human agency as the standard model for any form of agency. In Swanepoel (2021), for\nexample, the author holds that agents are entities that can do things with intent, where intentionality\nrequires mental states like beliefs and desires. Such a threshold conception of agency does not only\nexclude artificial systems, but most animals as well, and is thus not a proper representation of agents\nin general. Still, this does not mean that we should get rid of threshold conceptions of agency\naltogether as Symons and Abumusab suggest. This is because there is still a categorical difference\nbetween a system that displays some agent-like qualities and one that has agency.\nAn attribution of agency to a system changes the way that we behave towards such a system.\nWe can expect certain things from agents that we cannot expect from non-agents, and we adjust our\nown behavior based on these expectations. Having a cat that has developed a liking to pushing things\noff surfaces causes us to stop leaving mugs on tables when we are not there to watch the cat. Where\nwe do not know for certain that the cat will always push things off the table, we know that we can\nexpect this kind of behavior from it. Even though we can thus not fully predict the behavior of agents,\nwe are quite capable of anticipating for each agent what range of behaviors they are most likely to\nexhibit in a particular situation, and we can adjust our own behavior accordingly.\nOur interactions with current LLMs take place at a very sophisticated level (see Xi et al.\n(2023) for a comprehensive overview) and it is therefore quite likely that, if they were to be actual\nagents, we would interact with them as we do with human agents. This means giving them\nresponsibilities that we would normally only trust unto beings like us: rational and moral agents. This\nis quite different from how we currently treat artificial systems or machines. We rely on machines, but\ndo not trust them. We do not expect them to understand why something is wrong, or to have our\nwell-being at heart. If LLMs are not only agents, but also agents like us, then we can integrate them in\nour society as we would ourselves as an active participant that can be trusted to be left to its own\ndevices. If, however, current LLMs are rather systems that exhibit agent-like qualities, then we should\""}, {"title": "2. A THEORETICAL MODEL FOR AGENTS", "content": "Building on the work of Sarkia (2021), van Lier (2023) identifies four distinct modeling strategies that\ncan be used in complementary fashion to conceptualize artificial agency. Each of these modeling\nstrategies Gricean modeling, analogical modeling, theoretical modeling and conceptual modeling\ncan be used to answer a different set of questions about a phenomenon like artificial agency.\nGricean modeling is the preferred method when one is looking to gain insights about what it would\ntake to build a specific agent, since it allows one, in a stepwise manner, to reconstruct what 'inner'\nmechanics are most likely to result in the observed behavior of the entity or system that is modeled.2 If\none is rather interested in the way that the phenomenon is similar or different from other phenomena,\nthen analogical modeling is the more useful approach. Where one focuses in Gricean and analogical\nmodeling on (the interrelations of) individual phenomena, in theoretical modeling one models the\ndomain of phenomena as a whole. With this modeling strategy one can answer demarcation questions\nlike how to differentiate agents from non-agents, or science from pseudoscience. Finally, conceptual\nmodeling can be used to answer questions about the complementability of the models themselves. By\nreconstructing each of the models built in the other strategies as a conceptual model so as a\nrepresentation of the logical structure of the (sub-)concepts used in the model\u00b3 one can see whether\nor not there is consistency in the way that each of these models refers to the phenomenon in question.\nIn van Lier (2023), these four strategies are combined into one methodological framework, the\nFour-Fold Framework, and it is shown for each of them how they can be used to model artificial\nagents.\nIn this paper, we will make use of the third strategy, theoretical modeling, to construct our\nagent model. This is because we are interested in the question of how one can demarcate agents from\nnon-agents when one assumes that these agents do not necessarily need to be alive or mentally\nendowed (thereby leaving room for the option of an artificial agent). In theoretical modeling, one\nreasons \u201cabout the laws and regularities that are associated with a particular domain of phenomena\nwithout detailed reference to either particular entities that populate that domain (as in analogical\nmodeling) or particular mechanisms that maintain those laws and regularities (as in Gricean\nmodeling)\u201d (Sarkia, 2021, \u00a74). In this paper, we demarcate agents from non-agents on the basis of\nhow they are embedded and interact with their environment, and we will therefore model the laws and\nregulations that characterize the way that agents are influenced and interact with their environment.\nThere are two advantages to taking a top-down approach like theoretical modeling when\ndebating the potential agency of LLMs. As stated in the introduction, there is a certain resistance in\nphilosophy to attribute agency to artificial systems. One of the reasons for this resistance is that\nagency is often associated with a form of consciousness and/or a form of biological self-regulation.\nBoth consciousness and mechanisms like autopoiesis seem very difficult to realize in artificial form,\nand this naturally results in a skeptical attitude towards the possible realization of an artificial agent.\nBy building a theoretical model that models instead the way that agents are dynamically embedded in\ntheir respective environments, we are not pre-conditioning agency on being alive or on having some\nform of consciousness. A second advantage is that we do not have to start from a position where we\nalready assume that LLMs can display agency our model neither affirms that there can be artificial\nagents, nor affirms excludes it as a possibility, since we do not pre-condition agency on being alive or\non being conscious. Still, if one accepts that our theoretical model represents the domain of agents,\nand LLMs fit the model, then one can at least make the argument that agency could be attributed to\nthem.\nIn the following, we think of humans and most higher animals as paradigmatic examples of\nagents, and model the way that these agents are dynamically embedded in their environment. We\nthereby explicitly avoid conditioning these dynamics on them being alive or on having some form of\nconsciousness."}, {"title": "2.1 Framing the model", "content": "Just like us, most agent accounts in philosophy have taken (rational) humans and higher animals as\nthe basis for their agent models. However, since we want to widen the agent domain so as to\npotentially include artificial systems, we need to find some common ground between natural and\nartificial systems. We have chosen to focus on autonomy, meaning that our model attempts to capture\nagentive autonomy. There are two motivations for this focus. First, in philosophy, autonomy is seen as\na fundamental characteristic of (specific kinds of) agents. In the Routledge Handbook of Philosophy of\nAgency, for example, Ferrero (2022) lists autonomy as one of the characteristics of what he calls\n\"full-blooded agency\u201d (p. 8). By modeling agentive autonomy, our account would thus still heed the\noverall consensus about what it means to be an agent in philosophy. A second motivation is that there\nis a general drive in Al-research to develop autonomous systems. There is thus at least the belief\namong computer scientists that something like artificial autonomy can be created.\nIt must be noted, however, that a distinction can be made between the drive of these\nresearchers to develop systems that are 'autonomous' in the sense of them being self-driven versus the\naim to develop a system capable of a form of autonomy that is more like ours. In Canty et al. (2023),\nfor example, agentive autonomy is described as \"adaptive operation\" (see the top right of table 1, p.\n1260). A self-driving laboratory is autonomous in this sense. Self-driving laboratories are robotic\nplatforms, able to conduct experiments autonomously, to which an AI-system has been added. This\nAl-system is able to learn how to generate hypotheses, how to design experiments for the robotic\nplatform to test these hypotheses, and how to use the results of these tests to generate new hypotheses.\nIt is thus able to adapt its operations to what it has learned, enabling it to continue functioning for an\nextended period of time without needing human feedback. It can be said that such systems are\nautonomous in the sense that they can 'drive themselves' and this is indeed a feature that characterizes\nagents as well.\nHowever, as noted in the introduction, what an agent ends up doing is influenced by what the\nagent, as an individual system, has experienced over time. An agent's actions are therefore authentic\nas well: they are \u201cnot the product of external manipulative or distorting influences\u201d (Prunkl 2023, p.\n101). The distinction made between autonomy as being self-driven versus autonomy as being\nauthentic hints at a similar kind of division in philosophy between two uses of the term autonomy. In\nits biology-based understanding, autonomy is seen as a form of autopoiesis or self-regulation (see\nVarela et al. 1991). We think of autonomy as being self-driven and autonomy as self-regulation as\nsimilar in that they both seem to primarily relate to the ability of a system 'to continue going by\nitself', whether this is in the form of surviving (organisms) or not needing human feedback\n(self-driving systems). Both forms of autonomy are rather abstract in that they do not tell us much\nabout the system as an individual system\nabout the system as an agent. This is because the how and\nwhat of what the self-driven system ends up doing is still prescribed by humans, and the\nself-regulatory system is directed in how it attempts to survive by its particular nature. In the Kantian\nunderstanding of autonomy, on the other hand, autonomy is seen as a form of self-governance (see\nFormosa 2022, section 2). This is a very advanced form of autonomy since it presumes that agents are\nable to govern their own behavior with self-made rules. This kind of autonomy requires the agent to\nhave some form of consciousness and it therefore does not meet our aims in building our model. Only\nthe notion of autonomy as being authentic, then, meets our conditions in that it includes the agent as\nan individual system, without requiring explicitly that this agent has some form of consciousness as\nwell. Since we stated in the introduction of this section that in modeling agentive autonomy we want\nto avoid conditioning this autonomy on being alive or on being conscious, we will present a model of\nagentive autonomy where this autonomy is understood as being authentic in some way.\nWe thus aim at constructing a model of autonomy that can engage with state-of-the-art-AI\nsystems, while still staying true to philosophical perspectives on what it means to be an agent. We\nthereby start from the assumption that the domain of autonomous systems is larger than the domain of\nsystems that philosophers in mind, action and philosophy would attribute agency to, since those\nphilosophers associate agency with autonomy as being authentic or being self-governing. We now\nhold that all agents display a particular kind of autonomy, one that can be meaningfully distinguished\nfrom that of other autonomous systems in that what the agent ends up doing is authentic. We further\nhold that agents are authentic in that what they end up doing relates back to what those agents, as\nindividual systems, have experienced and learned over time. In constructing what this agentive\nautonomy entails, we take inspiration from the works of Steward (2012) and Walsh (2015). Steward\n(2012) defends a view that attributes agency to most higher animals. Agents, according to her, are\n\u201centities that things can be up to\u201d (p. 25). However, this does not mean that anything can be up to an\nagent, since \"it is utterly undeniable that all animal agency takes place within a framework which\nconstrains, sometimes very tightly, what can be conceived as a real option for that animal\" (p. 20). We\ntake away from this account that what is up to an agent is what it does next, but that what it can do\""}, {"title": "2.2 A theoretical model of agentive autonomy", "content": "Let us start with the basics. Agents are autonomous systems in the sense that things can be up to them,\nand we claim that what can be up to them is always influenced by a particular and dynamic\nframework of factors. We thus characterize the autonomy of agents by the way that it is restricted by\nthis framework. In most agent accounts in philosophy, especially in free will and determinism debates,\nthe focus lies instead on the way in which agents are free. What one tries to answer in such debates is\nhow things can be up to the agent how they are 'free' and this seems to depend largely on what\ninner processes result in the authentic behavior that we observe, which might be different for each\ntype of agent. For answering these types of questions, then, Gricean modeling seems to be the better\nmethod, since it can be used to reconstruct what 'inner' processes are most likely to result in the\nobserved behavior.\nIn this paper, we will leave these kinds of questions to the Gricean modelers and build instead\na theoretical model of the way that agents are dynamically embedded in their respective\nenvironments. As stated before, we hold that this embeddedness can be characterized by the fact that\nwhat can be up to an agent is always influenced by a dynamic framework. This framework consists of\nthree factors the agent's accessible history, its adaptive repertoire and its external environment.\nThe latter two factors play a role in most agent accounts in philosophy: agents can act and how they\ncan do so depends on what their particular realization allows them to do (their adaptive repertoire)'\nand what is possible in the environment they encounter. We now claim that what agents can do is also\ninfluenced by their accessible history the set of things that the agent, as an individual system, has\nlearned and achieved over time.\nThese dimensions of the framework are closely intertwined and sometimes overlap, but one\ncan still make a meaningful distinction between them. The agent uses its accessible history to\ndetermine the way in which it will use its adaptive repertoire to interact with its environment and this\naccessible history changes because of these interactions. The adaptive repertoire is the set of possible\nways in which the agent is able to interact with its environment (see Walsh, 2015, p. 211). It is\nadaptive in the sense that it can be enriched by what the environment offers to the agent and by the\nnext is constrained by a particular framework. Walsh (2015) defines agents as goal-directed systems\nthat, because of their adaptive repertoire, can experience their conditions as things that afford\nopportunities for, or impediments to, the pursuit of their goals (p. 163). We will integrate each of these\nkey concepts adaptive repertoire and goal in our model, since they help define both the drive of\nthe agent and its particular embeddedness in its environment. Having these preliminaries in place, we\nnow turn to our theoretical model."}, {"title": "3. LLMS AS ARTIFICIAL AGENTS?", "content": "\"Large language model\u201d (LLM) is a very general term used to describe a plethora of machine learning\nmodels able to generate text in various scenarios. Such models consist of a deep neural network,\nusually based on the Transformer architecture (see Vaswani et al., 2017), containing billions of\ntraining parameters (compared to the few millions commonly used for other machine learning tasks).\nThe training of such base LLMs can vary slightly from one to the other, but their core goal is always\nto predict the word that follows a given input sequence, usually referred to as the prompt. For\ninstance, the model learns that the sequence \u2018A cat is ...' should be followed by \u2018... an animal'.\nHowever, since these models are probabilistic, the answer '... cute' would be as good as the previous\none. What the model learns is the probability of an answer appearing after an input sequence. So in\nour example, the model might learn that \u201c... an animal' has 90% chance of appearing, while '... cute'\nonly has a 10% chance (when no extra context is given to the input sequence).\nEven though these base LLMs are very powerful, researchers quickly realized that the\nanswers they produced differed in language and content from what a human would have answered in a\nparticular context. This misalignment hinders the performance of base LLMs in many applications.\nOne solution that researchers have found is to fine-tune the base LLM (i.e. retrain it for a short time)\nfor use-cases. Chatbots, for example, have to be able to give answers that are relevant to the question\nbeing asked. To ensure that this is indeed the case, researchers have fine-tuned one of the most\nprominent base LLM, GPT-4 (Achiam et al., 2023), to output sentences that humans would consider\nto be good in a certain context (this is known as reinforcement learning from human feedback\n(Christiano et al., 2017; Lambert et al., 2022)).\nDue to their size, training base LLMs is very costly, and only few institutions and companies\nhave the resources to do so. Once they are trained, however, their deployment is much cheaper.\nNonetheless, even without any further training, researchers have realized that LLMs can learn new\ninformation just from text, turning them into zero-shot learners. For instance, they can learn to\nperform new tasks by means of some input text that prompts the model before any other prompt that\nwe input to the model (i.e. a pre-prompt). For instance, let's consider that an LLM has no previous\nknowledge of what a multiplication is. We can create a pre-prompt that says \u201cWhenever you are asked\nto do a multiplication between two numbers A and B, add A to itself B times\u201d. Now, each time we\ninput two numbers to the LLM, it first sees the pre-prompt and then uses that information to perform\nthe operation. Such a simple feature, which does not entail any further training, can be used in much\nmore complex scenarios, paving the way for the creation of complex LLM architectures that go\nbeyond text prediction.\nPerhaps the most promising of these architectures are those that are referred to as autonomous\n(Boiko et al., 2023) or self-managing (Firat & Kuleli, 2023) LLM \u2018agents' \u201d.These LLMs consist of a\nbase LLM (like e.g. GPT-4 (Achiam et al., 2023), LLaMa (Touvron et al., 2023), Mistral 7B (Jiang et\nal., 2023) or Gemini (Team Gemini, 2023), to name a few) that has access to various modules with\nwhich it can perform \u2018actions' that go beyond text generation. For instance, the base LLM can be\ncombined with a web search module to find new information or with a module that allows them to\ncontrol a robotic arm to perform a chemical experiment (like e.g. the Coscientist of Boiko et al.\n(2023)), or they can be combined with other machine learning models like an image generator (such\nas Dall-E (Ramesh et al., 2021)). All the information about the \u2018actions' the LLM can take, i.e. the\nmodules it can use and their functioning, is given as a pre-prompt to the system. This pre-prompt thus\ncontains information about what predefined commands the LLM can use to interact with each of the\nmodules. The model can then use plain text containing these predefined commands to interact with\nthese modules (e.g. the Coscientist in Boiko et al. used the text \u2018GOOGLE suzuki reaction conditions\noptimal' to activate the web search module and find information about a chemical reaction it later\nproduced in a real laboratory) and use the output of these modules to achieve the task set for it by the\ninitial human prompt (to return to the previous example, Boiko et al.'s Coscientist initial prompt was\n'You need to perform a Suzuki reaction using the available reagents').\nThe use of pre-prompts thus makes it possible for researchers to instruct LLMs on how to\ngain access to, and use, various modules. This has resulted in artificial systems that are capable of\nquite complex and independent ways of functioning. Where there still needs to be an initial prompt to\nset the system in motion, it has been shown in works like that of Park et al. (2023) and Boiko et al.\n(2023) that little to no human intervention is needed for the completion of the tasks that we set these\nadvanced LLM \u2018agents'. One could thus say that these \u2018agents' are capable of a form of autonomous\nfunctioning: they are able to learn (during training and fine-tuning, and with the help of\npre-prompting) and use what they have learned to continue to function without human intervention for\na prolonged period of time (until completion of the task). However, calling this a form of agentive\nautonomy seems, as of yet, a bridge too far. At the moment, LLM \u2018agents' are fine-tuned for a\nparticular context and instructed on how to use particular modules. They are prepared for use-cases,\nfor performing particular tasks like designing an experiment or holding a believable dialogue. They\nare meant to function as well, if not better, as we would in a task. For this, they only need to be a type\nof LLM 'agent', one that can proxy a number of human behaviors.\nIf we want to explore whether these LLMs can be more than our proxies, however, we will\nhave to think of a way in which we can create an individual LLM agent: one whose actions and goals\nare influenced by a dynamic framework of factors which, in turn, is influenced by what the individual\nLLM agent experiences and learns over time. Of course, this is already happening to an extent since\nchatbots are able to learn from their interactions with humans and they change their interactions with\nus on the basis of this. However, these changes are closely monitored so as not to result in unwanted\n(e.g. racist, sexist, biased or simply wrong) output (see Achiam et al. 2023). What is more, these\nchatbots are trained to proxy human forms of discourse, so where they can thus certainly learn from\ninteracting with us, the how and what that they should learn is still prescribed by us humans."}, {"title": "3.1 LLM agents", "content": "Before discussing the work of Park et al. (2023), we briefly summarize here our theoretical model as\ndiscussed in section 2. We stated that agents are systems that things can be up to. What can be up to\nthese agents is always influenced by a dynamic framework of factors that include the agent's\naccessible history, its adaptive repertoire and its external environment. This framework is dynamic in\nthat it is influenced by the agent's actions and goals and, in turn, can influence these actions and goals.\nThe agent's type determines the range in which the factors of the framework can pan out, its set of\npre-given goals and the kind of new goals it is likely to form. However, it is the individual agent's\ntrajectory through an environment over time that determines how the framework concretely evolves\nand what specific goals the agent will form. For an individual LLM agent, therefore, there needs to be\na way in which what the system does and what goals it forms can be influenced by this framework\nand, in turn, these goals and actions can influence how the framework develops.\nIn the work of Park et al. (2023), the authors introduce what they call generative agents that\ninhabit a Sandbox world. Even though these generative agents only simulate human behavior, each of\nthem is able to display consistent behavior over a longer period of time and we believe that this is\npartly due to the fact that their 'actions' and 'goals' are influenced by a framework of factors that\nbears similarity to the theoretical model that we just described. To better understand this connection,\nwe first want to go briefly over the basis of the system. Every generative agent controls a bot living in\nthe simulated sandbox Smallville. They are pre-prompted with all necessary information needed to\nproperly interact with such an environment, e.g. how to move, use the different objects available, talk\nto other bots, and what is characteristic behavior for each of the bots. All of these interactions happen\nvia natural language, which is why the generative agents only need to be equipped with an LLM\n(ChatGPT in this case). Humans can interact with the bots in various ways: by controlling other bots\nof the sandbox, by changing parts of the environment, or as an \u201cinner voice\u201d, i.e. changing or\nextending the pre-prompts the generative agents have access to.\nIn Park et al. (2023), each 'action' of the generative agents is always influenced by a dynamic\nframework of factors that is quite similar to the one described in our theoretical model. As\ncommented, every generative agent starts off with a pre-prompt that is called its seed memory: \u201cone\nparagraph of natural language description to depict each agent's identity, including their occupation\nand relationship with other agents\u201d (Park et al., 2023, \u00a73.1). This seed memory tells us something\nabout the type of generative agent that we are dealing with. It includes the generative agent's\npre-given goals (John Lin, for example, \u201cloves to help people\u201d and \u201cloves his family\u201d (\u00a73.1), while\nEddy Lin", "friendly, outgoing,\nhospitable\" (\u00a74.3), while his dad is \u201cpatient, kind, organized": "", "repertoire": "John Lin's \u2018actions' will relate to his work or his family, while Eddy\nwill do something with music. This initial seed memory is saved to the generative agent's memory\nstream and determines together with the current state that the generative agent is in (its environment)\nthe first 'action' of the bot once the simulation starts. 'Actions' are therefore influenced from the start\nby a framework of factors that is similar to the one in our theoretical model (accessible history,\nrepertoire, and environment).\nDuring the simulation, the 'actions' and 'goals' of the individual bots influence how their\nframework pans out and this dynamic framework, in turn, influences the \u2018actions' and 'goals' of the\nindividual bots. To see what we mean, let us start with the bot's 'actions'. Every 'action' a bot\nperforms and the effect this 'action' has on its environment are saved in the bot's memory stream,\nwhich is effectively appended to its original seed memory. This original seed memory thus evolves\nover time, based on the \u2018actions' the individual bot performs.12 Before performing any new \u2018action',\nthe bot accounts for its environment and its current memory stream. To filter the memory and avoid\nirrelevant information, the bots are equipped with a retrieval function that takes as input their current\nsituation and extracts the relevant pieces of memory. The bot then uses such refined memory to plan\nits next move. Additionally, the bots are able to reflect on the basis of their memory. This process,\nwhich is automatic but only takes place a few times", "day": "allows the bot to see its\nrecent memory stream and summarize its behavior. For example, if a bot has performed various\nactions related to music, a possible reflection would be: \u201cI like music\u201d. Such reflections are added into\nthe memory stream, improving the bot's understanding of its own behavior and guiding its next\n'actions'. The individual bot's 'actions' therefore influence how its memory stream (accessible\nhistory) is built up, and this memory stream together with the bot's current state (its environment)\ninfluences what actions (with its repertoire) the bot is able to perform.\nA similar kind of construction influences what \u2018goals' the individual generative agent is able\nto form and how these \u2018goals' in turn influence the \u2018actions' of the bot. Each generative agent starts\nthe day with a plan that is based on"}]}