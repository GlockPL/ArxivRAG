{"title": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey", "authors": ["LIANG CHEN", "ZEKUN WANG", "SHUHUAI REN", "LEI LI", "HAOZHE ZHAO", "YUNSHUI LI", "ZEFAN CAI", "HONGCHENG GUO", "LEI ZHANG", "YIZHE XIONG", "YICHI ZHANG", "RUOYU WU", "QINGXIU DONG", "GE ZHANG", "JIAN YANG", "LINGWEI MENG", "SHUJIE HU", "YULONG CHEN", "JUNYANG LIN", "SHUAI BAI", "ANDREAS VLACHOS", "XU TAN", "MINJIA ZHANG", "WEN XIAO", "AARON YEE", "TIANYU LIU", "BAOBAO CHANG"], "abstract": "Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming the multimodal information into tokens and predict the next one given the context. This survey introduces a comprehensive taxonomy that unifies both understanding and generation within multimodal learning through the lens of NTP. The proposed taxonomy covers five key aspects: Multimodal tokenization, MMNTP model architectures, unified task representation, datasets & evaluation, and open challenges. This new taxonomy aims to aid researchers in their exploration of multimodal intelligence. An associated GitHub repository collecting the latest papers and repos is available at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction.", "sections": [{"title": "1 INTRODUCTION", "content": "Humans' engagement with the universe is a tapestry, interwoven with the threads of various modalities. Humans can see and sketch paintings, read and write epics, listen and compose music, touch and sculpture heroes, ponder and make movements. These modalities \u2013 specific information types such as vision, sound, and language \u2013 are the channels through which humans interpret and respond to the world. This multifaceted interaction highlights the intertwined nature of perception and response in human experience. As a specialized area within Artificial Intelligence (AI) research, multimodal Learning focuses on creating systems capable of understanding and generating various multimodal information [16].\nA paradigm shift has emerged in the field of AI across multiple modalities, transitioning from specialized unimodal models trained for a single task to versatile multimodal ones dealing with a diverse array of tasks [150]. This shift is largely attributed to the advancement of Large Language Models (LLMs) in the Natural Language Processing (NLP) field such as GPT-3 [34], ChatGPT [300] and LLAMA [378], which unify multiple natural language understanding and generation tasks with a single Next Token Prediction (NTP) objective. The original task of NTP is to predict the next token (which can be a word, subword, or character) in a given sequence of text based on the context provided by preceding tokens. The NTP paradigm has been proven to be scalable given abundant data and computational resources in the lens of scaling law research [192, 472].\nSimultaneously, researchers have explored the incorporation of non-textual input and output modalities into large language models, sparking interest within the community to develop powerful Large Multimodal Models (LMMs) featuring capabilities to conduct tasks across different modalities [72, 448]. For a better understanding of the historical development of LMMs based on NTP, we demonstrate a timeline in Figure 1, categorized by models' understanding or generation ability and different modalities.\nIn Figure 2, we use the image modality as an example to illustrate the workflow of Multimodal Learning with NTP (MMNTP). The process can be divided into three key components: Tokenization, Modeling, and Training Objectives, which will be explained and discussed in details in the rest of the survey. For vision modality, image and video understanding capabilities have been demonstrated in large vision-language models such as GPT4-V [301], QwenVL [12], LLaVA [254], phi 3.5-Vision [1] and Gemini [370], while Emu [363] and Chameleon [369] show visual generation could be achieved in NTP manner. Similarly, end-to-end audio understanding and generation have been achieved in NTP-based models such as GPT4-0 and Moshi [105, 302].\nTo equip LLMs with visual understanding capabilities, pioneering research such as Flamingo [3], BLIP2 [227], GPT4V [301], MiniGPT4 [509] and LLaVA [254] has demonstrated that LLMs can be easily adapted to process multimodal inputs such as images and videos, by converting multimodal information into tokens with a straightforward tokenization module, such as a visual encoder like CLIP [321] or a simple linear projection [18]. Subsequently, these models perform multimodal instruction tuning based on image-query-answer triples using the same NTP objective."}, {"title": "1.1 Overall Structure of the Survey", "content": "The structure of the survey is shown in Figure 3. Section 2 focuses on Multimodal Tokenization, and highlights the importance of tokenization as the bridge between raw multimodal data and their representations, distinguishing between discrete tokens that use Vector Quantization and continuous tokens. Section 3 delves into the Multimodal Backbone Model for NTP, indicating that an auto-regressive model, often resembling a large language model, is employed to capture multimodal tokens, utilizing distinct attention masks for different modalities to account for their specific features. Section 4 covers Training with Unified Multimodal Task Representation, explaining the training objectives varying from discrete to continuous token prediction, enabling multimodal output through VQ decoders or directly generating conditions for models like diffusion or VAE. The section also covers prompt engineering techniques such as In-Context Learning and Chain-of-Thought reasoning of MMNTP models adopted from LLM research. Section 5 introduces datasets and evaluation metrics, noting the superior performance of NTP models over non-NTP models in both understanding and generation tasks. Lastly, Section 6 outlines unsolved challenges in MMNTP research, such as scaling up MMNTP, emergent abilities, modality-specific biases, modalities interference, and MMNTP as universal interfaces, and discusses approaches to mitigate these challenges. Table 1 outlines key tables and figures in our survey."}, {"title": "1.2 Related Work", "content": "Several recent works have reviewed Large Multimodal Models (LMMs) in multimodal learning. For instance, Yin et al. [448] delve into the understanding capabilities of early vision-language models. Similarly, Awais et al. [8], Bordes et al. [27], Ghosh et al. [130], Caffagni et al. [36], and Zhang et al. [477] take a step forward and explore recent progress in multimodal learning with a focus on model architecture, training strategies, datasets, evaluation metrics and more. In addition, several surveys have reviewed multimodal learning in vision-language tasks, including pre-training [36], transfer learning [479], reasoning [402], and reinforcement learning from human feedback (RLHF) [479]. Beyond the discussions on the general revolution of the LMMs, specialized surveys have investigated the application of LMMs in domains such as multimodal agents [224, 421] and autonomous driving [72]. Recent surveys have also tackled key issues in multimodal learning, such as hallucinations in LMMs [256, 334] and efficiency of LMMs [185, 429].\nDiverging from prior work that primarily focused on the understanding abilities of multimodal LLMs, our survey adopts a systematic perspective by integrating both understanding and generation in multimodal learning through the paradigm of next-token prediction. To the best of our knowledge, this is the first survey that reviews LMMs from the perspective of next token prediction, aiming to aid researchers in their exploration of multimodal intelligence.\nIn summary, in this survey, we aim to provide a holistic review on current multimodal models that rely on next token prediction. An associated GitHub link collecting the latest papers is at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction."}, {"title": "2 MULTIMODAL TOKENIZATION", "content": "Tokenization is the first and a fundamental step for multimodal sequential modeling under the next token prediction framework. It decomposes information from various sources, such as images, videos, and audio clips, into a sequence of minimal, manageable units known as tokens for the NTP model to learn. Table 2 provides an overview of the tokenizers used across various modalities in recent research.\nDespite being derived from various modalities, these tokenization methods can all be categorized into two prototypes: discrete tokenization and continuous tokenization. In this section, we will initially introduce the general definition and basics techniques of training multimodal tokenizers (\u00a7 2.1), then the fundamentals and applications of discrete tokens (\u00a7 2.2, 2.3) and continuous tokens (\u00a7 2.4, 2.5) in NTP framework."}, {"title": "2.1 Tokenization of Different Modalities", "content": "We first define the tokenization process as a function $f$ that maps a sample $x$ from the raw multimodal space $X$ to a representation $z$ in the tokenizer's output representation space $Z_f$.\n\\begin{equation}\nf(x) = z,\n\\end{equation}\nwhere $x \\in X$ and $z \\in Z_f$.\nAs illustrated in Fig. 4, tokenizers for multimodal information can be categorized into two types: discrete and continuous. This classification is based on how tokens are derived from the original data. Both tokenization methods encode the original information into a latent representation space, but they differ in their approach.\nDiscrete tokenization performs quantization on the latent space, utilizing a fixed-size, discrete space similar to the vocabulary of language models. In contrast, continuous tokenization does not involve quantization, resulting in a much larger representation space.\nIn Equation 1, a discrete token implies that the representation space $Z_f$ comprises a finite number of discrete symbols. The output space is called the codebook $C = \\{C_1, C_2, ..., C_N\\}$, where $c_i \\in R^D$, and each representation $z$ is composed of codes from this codebook, i.e., $z = \\{Z_1, Z_2, ..., Z_n\\}$ with $z_i \\in C$. Language tokens are inherently discrete because they originate from a finite vocabulary. Each word or subword unit is mapped to a unique token from this predefined set. In contrast, modalities such as audio and images exist in continuous, high-dimensional spaces. To process these modalities within the same framework (i.e., NTP) as for discrete language tokens, they need to be transformed into a discrete representation.\nQuantization is a process that maps values from a continuous space to a discrete space, typically resulting in a much smaller representation space. It is a default operation when a discrete representation is desired for tokenizing multimodal information. Quantization is often combined with auto-encoder techniques to reduce the size of the latent space. Typical examples include VQ-series tokenizers such as VQVAE [138] and VQGAN [112], which inherently feature discrete representations. Details of the quantization process are introduced in Section 2.2."}, {"title": "2.1.2 Features of Tokenizers", "content": "Before diving into different tokenization techniques, we summarize the basic two features (Representation and Reconstruction) that an ideal multimodal tokenizer should possess to achieve better understanding and generation capabilities in the NTP framework."}, {"title": "2.1.3 Training Methods for Tokenizers", "content": "The training methodologies for tokenizers can be categorized into four groups, based on their respective training objectives: Auto-Encoding, Denoising Auto-Encoding, Supervised Training, and Contrastive Learning, as depicted in Figure 5. Herein, we provide a summary of the core concepts associated with various tokenizers.\nAuto-Encoding. Auto-Encoder (AE) is a type of artificial neural network designed to learn efficient data representations. It consists of two main components: an encoder, which maps input data to a latent space with reduced dimensions, and a decoder, which reconstructs the input data from this latent representation. The training goal for an Auto-Encoder is to minimize the reconstruction error, ensuring the decoded output closely resembles the original input. Variants like Variational Auto-Encoders [195] (VAEs) use probabilistic approaches to generate more robust and informative embeddings. In multimodal generation models, tokenizers trained with auto-encoder methodologies are used to restore the multimodal input from the latent representation. A special case is diffusion models [90], which can also be viewed as Auto-Encoder, enabling generation in a non-autoregressive manner [233]. Discrete tokens are typically generated by quantizing [335] the continuous data representation within the latent space of auto-encoders.\nDenoising Auto-Encoding. A Denoising Auto-Encoder (DAE) builds on the basic auto-encoder concept by introducing noise into the input data and training the model to reconstruct the original, noise-free version. This approach encourages the model to learn robust features capable of handling data corruption, thereby improving its generalization capabilities. In transformer-based models, a common technique known as Masked Language Modeling [84] involves masking parts of the input tokens and training the model to predict them, which can be viewed as a special type of denoising auto-encoder. This method has become mainstream across various modalities, popularized in language by BERT [84], in vision by BEiT [17] and MAE [153], and in audio by HuBERT [160].\nSupervised Pretraining. Some tokenizers are pretrained on specific tasks using supervised learning, aiming to acquire task-specific representations through labeled datasets. These models are initially trained on large-scale datasets to capture specific features of the input data. In the vision modality, supervised tasks include semantic segmentation, object detection, and depth estimation. Models trained for these tasks, such as SAM [196, 392], ViTDet [240], and MiDaS [329], are later used in LMMs as tokenizers, like in DeepSeek-VL [268] and Cambrain-1 [376], to extract diverse visual features from input data. In the audio modality, Whisper [322] is trained with 680,000 hours of labeled audio data in a weakly supervised manner. Thanks to its robust and powerful speech feature extraction capabilities, Whisper is widely used in Speech LLMs [65, 162, 366] for extracting speech embeddings.\nContrastive Learning. Contrastive Learning is a self-supervised learning method that focuses on learning representations by distinguishing between positive and negative pairs. The core idea is to bring similar (positive) examples closer together in the representation space while pushing dissimilar (negative) examples further apart. The items in each pair can belong to the same or different modalities. For example, DINO [40] uses image-image pairs to enhance vision representation, while CLIP [321] employs text-image pairs to improve language alignment within vision representation.\nCurrently, LMMs that only feature multimodal understanding capabilities, such as Instruct-BLIP [74] and LLaVA [254], opt for tokenizers with superior representation abilities like CLIP [321], as they do not require reconstruction of the multimodal information. Conversely, LMMs supporting multimodal generation capabilities tend to choose VQVAE as the tokenizer, exemplified by models like Unified-IO [271], Chameleon [369], Emu3 [401], among others [128, 396, 405]."}, {"title": "2.2 Discrete Tokenization Basics", "content": "Unlike the language modality, which inherently comprises discrete symbols (e.g., tokens or words), most other modalities naturally exist in a continuous space. To bridge the gap, the core technique is Vector Quantization (VQ), which aims to map the original continuous information into a compressed, finite representation space, i.e. discrete tokens. The discrete tokens can have 2-dimensional or 3-dimensional structures for images and videos. These tokens are initially linearized based on a specific order, such as left to right and top to bottom, transforming them into a 1-dimensional sequence. This linearization allows for effective modeling usingthe next token prediction objective.\nIn this section, we will first elaborate on modern vector quantization techniques widely used as multimodal tokenizers, such as VQVAE (\u00a7 2.2.1) and its variants. Following that, we will introduce the specific optimizations of discrete tokenization in different modalities (\u00a7 2.3)."}, {"title": "2.2.1 Vector Quantization Methods", "content": "The origins of VQ method trace back to the 1950s at Bell Laboratories, where researchers endeavored to optimize signal transmission through the development of suitable discretization procedures [306]. In essence, quantization is the process of mapping an infinite set of continuous values to a smaller, discrete set of finite values. The primary objective of vector quantization is to reconstruct all the information in the original data as accurately as possible with a finite set of vectors, which is also called the codebook.\nVanilla VQ. The original VQVAE proposed by van den Oord et al. [385] is a milestone of many successive vector quantization methods. As shown in Figure 6, a VQVAE consists of three main components: the encoder, the quantizer, and the decoder. The encoder comprises the input data to a compact latent space, the quantizer select the nearest code vectors from the finite codebook to approximate the continuous latents, the decoder reconstruct the input data using the discrete codes. When training the VQVAE, three main loss components are crucial: reconstruction loss, codebook loss, and commitment loss [385]. The reconstruction loss, often implemented as mean squared error or binary cross-entropy, ensures accurate data reconstruction by minimizing differences between input and output. Codebook loss, or vector quantization loss, enables effective encoding by aligning encoder outputs with nearest codebook entries, ensuring discrete latent variables. Meanwhile, commitment loss acts as a regularizer, encouraging encoder outputs to stay close to codebook entries to maintain stable learning, preventing erratic mapping. As gradient can not pass the quantization operator (finding the nearest code), the straight-through estimator [21] is adopted to let the gradient flow normally.\nRecent advancements in vector quantization methods have focused on achieving better image reconstruction and enhancing generative capabilities. To improve reconstruction quality, both architectural innovations and codebook designs have been proposed. Transformer-based frameworks, such as ViT-VQGAN [450], Swin-MAE [433], Swin-Unet [38], and Efficient-VQGAN [39], replace traditional CNN encoders and decoders with more robust modules like ViT [96] and Swin-Transformer [264, 265], leading to better feature representations and reconstruction fidelity. Additionally, several methods such as LFQ [456] and FSQ [291] are proposed to address the significant challenge of codebook collapse during codebook learning, where a large portion of code embeddings are not used when enlarging the codebook size, causing a redundancy in the codebook and limiting the expressive power of the generative model [19]. For improved generative performance and efficiency, several approaches have been introduced. Tian et al. [374] propose Visual Autoregressive modeling, which facilitates image generation through \"next-scale prediction\", moving away from the traditional raster-scan \"next-token prediction\" used in standard VQVAE-based models. RQ-Transformer [216] employs residual quantization (RQ) to precisely approximate feature maps and reduce spatial resolution. RQ helps the RQ-Transformer to significantly reduce computational costs and effectively learn long-range interactions in inputs. RAR [459] introduces a randomness annealing strategy with a permuted objective, enhancing the model's ability to learn bidirectional contexts while retaining the autoregressive framework. TiTok [461] tokenizes images into 1D latent sequences, providing a more compact latent representation that is substantially more efficient and effective than conventional techniques. It greatly reduces the number of tokens required to encode an image compared to previous methods [39, 450].\nVQ with Auxiliary Losses. The primary goal of the vanilla VQVAE is to accurately reconstruct input data by minimizing the mean squared error loss. However, this auto-encoding objective doesn't always align with human perception of the quality of reconstructed data. For example, in the visual modality, the vanilla MSE loss often results in images with blurred details, particularly in human faces [210]. To address this issue, several approaches introduce higher-level training objectives aimed at improving the overall quality of the output data. In the realm of vision, perceptual loss [188] is widely used to enhance the quality of reconstructed images by leveraging a pre-trained CNN. VQGAN [39] incorporates a discriminator network to enhance image fidelity by adding an adversarial training objective. The role of the discriminator is to discern between the reconstructed and original images, while the VQ-VAE is optimized to deceive the discriminator, thereby improving the quality of the reconstructed images. In the audio modality, it is essential to decouple the audio into its acoustic and semantic components to achieve both powerful audio reconstruction quality and LLM modeling. SpeechTokenizer [486] and Mimi [78] introduce the loss of semantic distillation at the first layer of Residual VQ, using self-supervised models, such as HuBERT [160] and WavLM [56].\nResidual Vector Quantization. Residual vector quantization (RVQ) has been used for image [217] and audio [468] generation, where quantized codes are refined by storing additional quantized residuals. Lee et al. [216] propose the RQVAE that also introduces a residual quantization to recursively quantize the feature map in a coarse-to-fine manner, employing a fixed-size codebook to maintain both precision and code diversity.\nProduct Quantization. El-Nouby et al. [108] propose product quantization (PQ), to factor the codebook into a product of smaller codebooks, allowing for high-quality quantizers without the requirement of intractably large codebooks.\nMulti-scale Quantization. Tian et al. [374] introduce the Visual Autoregressive modeling (VAR), which develops a multi-scale quantization autoencoder that encodes images into K multi-scale discrete token maps using a shared codebook. It aids the model in generating images through \"next-scale prediction,\" instead of the raster-scan \"next-token prediction\" typically used in standard VQVAE-based models. The multi-scale quantization enables the model to learn visual distributions and demonstrates strong generalization capabilities.\nFinite Scalar Quantization. To generate concise and expressive tokens using a larger token vocabulary and avoid codebook collapse, Mentzer et al. [291] propose finite scalar quantization (FSQ). FSQ projects the VAE representation down to a few dimensions that can be quantized into fixed values, creating an implicit codebook.\nLook-up Free Quantization. LFQ [457] reduces the embedding dimension of the codebook to zero, effectively replacing the codebook with an integer set. It allows VQVAE to improve the quality of image reconstruction and generation by vastly increasing the vocabulary size by magnitudes. For example, the rFID on Imagenet decreases from 2.5 to 1.4 when the LFQ vocabulary size increases from 210 to 216 on ImageNet dataset.\nEmbedding-Free Quantization. Maskbit [406] explores an embedding-free tokenization approach that utilizes binary quantization. It projects latent embeddings into K dimensions and then quantizes them based on their sign values to produce bit token representations. The generated bit tokens exhibit highly structured semantic representations, which are crucial for generation tasks.\nGroup Vector Quantization. Unlike RVQ which models the information residually, Group Vector Quantization models the information across different dimensions. In the audio domain, HiFi-Codec [439] proposes a group-residual vector quantization technique to reduce the number of codebooks, while FACodec [189] disentangles speech into prosody information, content information, and acoustic details using three-factorized vector quantizers."}, {"title": "2.2.2 Evaluation of VQ Tokenizers", "content": "When evaluating VQVAEs, two critical metrics are commonly considered: reconstruction ability and generation ability.\nReconstruction ability refers to how well the VQVAE can reproduce the original input data after encoding and decoding. This metric evaluates the fidelity of the model in terms of how accurately it can reconstruct the input data from its latent representations. L2 distance, Peak Signal-Noise Ratio (PSNR), and reconstruction Fr\u00e9chet Inception Distance (rFID) are often applied to assess the reconstruction ability.\nGeneration ability assesses the model's capacity to generate new, plausible samples from the learned distribution in the codebook space. This metric evaluates the creativity and diversity of the VQVAE in producing new data that is consistent with the training data distribution. To quantitatively evaluate generation ability, metrics such as the Inception Score (IS) and generation Fr\u00e9chet Inception Distance (gFID) [157] are often used.\nrFIDs are often computed between ImageNet validation images and their reconstructed images. gFIDs are usually computed against the training set with ADM's evaluation suite [86]."}, {"title": "2.3 Discrete Tokenization for Different Modalities", "content": "Generic quantization methods provide basic ways to convert continuous data into discrete tokens. However, there isn't a single quantizer that works well for all modalities because each modality has unique characteristics. Therefore, it is important to create specific tokenizers for each modality. This section will explain the unique features of different modalities and showcase some examples of tokenizers for images, audio, and video, among others."}, {"title": "2.3.1 Image", "content": "Images can be tokenized into discrete symbols with the previously introduced VQVAE structure. Compared to text tokens, images diverge in three fundamental aspects that significantly impact how they should be tokenized:\nRich Information Granularity: Unlike text, which primarily encapsulates high-level semantic meaning, images are contain with a myriad of perceptual details. These encompass low-level visual elements such as colors, shapes, and textures, alongside more abstract concepts like objects and actions.\nDense Information: Images inhabit a densely packed representational realm, where each pixel, across multiple dimensions including height, width, and color channels (RGB being a common example), carries information. This stands in stark contrast to the discreteness of text in nature, characterized by sequentially arranged words.\nTwo-Dimensional Spatial Structure: Images are inherently structured in two dimensions, spread across a grid defined by height and width. This 2D layout differs fundamentally from the straightforward, one-dimensional sequence that characterizes textual data, introducing unique complexities in their processing and analysis.\nGiven these differences, bridging the gap between text and image modalities in the training of LLMs based on discrete image tokens requires a robust image tokenizer, which must balance the fusion of sufficient alignment with LLM's language ability (referred to as \u201crepresentation\u201d), the retention of rich original image information (referred to as \u201creconstruction\u201d), and the efficient use of tokens given the growing inference cost of transformer decoder (referred to as \u201ctoken efficiency\u201d). These factors possess a trade-off [127, 128, 357, 456], making it crucial for the construction of an image tokenizer to maintain equilibrium among these factors.\nIn terms of better representation, models like ViT [96] are commonly employed, often aligned with a text encoder through contrastive loss [312, 321], or aligned with text modalities through generative loss [451]. Additionally, modules like Q-Former [227] can also be used for image feature transformation [128, 227]. Consequently, the resultant image features integrate higher-level semantics and gradually compress high-dimensional images into lower-dimensional representations aligned with text. While the initial arrangement of image patches follows a raster order, preserving intrinsic sequential relationships, this configuration lacks causal semantics, posing challenges for language modeling.\nRegarding reconstruction ability, an image decoder is often layered atop the image encoder to reconstruct the original image from its representation, incorporating reconstruction loss into the training process [112, 128, 187, 309]. Training labels typically use the original images, but with advancements in diffusion models, more research is incorporating latents for diffusion models as reconstruction labels [128, 187].\nFor token efficiency, modules like selectors or mergers for image tokens are utilized to truncate their length (i.e., the number of tokens per image). For instance, SEED-LLaMA [128] compresses longer image features encoded by ViT into 32 continuous tokens using a Causal Q-Former and then discretizes them through quantization. LaViT [187] first predicts whether each patch token should be selected using a shared MLP, and then compresses the image length by employing selected patches as queries and unselected patches as keys and values in cross-attention blocks [128].\nBeyond these aspects, some studies also focus on the unique properties of specific image types or tasks. For example, VQ-IMG aims to enhance the modeling capabilities of image tokenizers for faces [124], while LVM integrates tasks like segmentation and object detection during the training of models based on VQGAN to enrich the representation of image tokens [13]. StrokeNVWA introduces a VQ-Stroke method to discretize vector graphic images into stroke tokens [367]."}, {"title": "2.3.2 Audio", "content": "Raw audios are typically stored as 16-bit integer values with a sampling rate that exceeds tens of thousands values per second, which leads to extremely long sequences and renders next token prediction training more difficult. Versatile quantization methodologies have been investigated for audio tokenization. Initially aimed at audio compression, these methodologies have more recently been developed to create compact semantic and acoustic representations in the context of NTP language modeling.\nAs a traditional companding algorithm, \u00b5-law/A-law algorithm is commonly employed in speech generative models such as WaveNet [383]. While this algorithm projects each audio frame to an 8-bit value, it does not reduce the sampling rate, thereby preserving overlong sequences. Self-supervised learned models have shown exceptional performance in various speech-related tasks, sparking interest in clustering their speech representations for speech quantization. The vq-wav2vec [10] uses either a Gumbel-Softmax or online k-means clustering to quantize the SSL-learned dense representation. HuBERT [160] is trained with a masked prediction task, whose targets are obtained through k-means clustering of learned features from earlier iterations. Utilizing quantized tokens learned with Self-Supervised Learning (SSL), GSLM [207] and VQTTS [99] demonstrate faster speed in speech generation tasks compared with WaveNet. Because SSL tokens are extracted with highly abstracted semantics while discarding low-level acoustic information, the reconstruction quality is relatively low, and speaker identity is lost [28]. Neural codec models typically apply a VQ-VAE on the raw audios with residual vector quantization, exemplified by SoundStream [467] and EnCodec [104]. They are originally designed for audio compression, have the capability to encode waveforms into discrete codes and faithfully reconstruct them back into high-quality waveforms. Recently, they are widely used in audio generation models such as AudioLM [28], VALL-E [391] and their variants [148, 354, 397], and reach new state-of-the-art performance on various tasks. Compared with traditional \u00b5-law/A-law algorithms, codec models can efficiently reduce the length of token sequences. It can also maintain multi-scale acoustic information indicating speaker identity compared with highly-abstracted SSL-learned discrete tokens such as HuBERT [160] tokens. Additionally, the codec models are typically off-the-shelf and lightweight.\nLatest works have attempted to impose additional supervision on the discrete codes extracted by codec models. The objective is to enhance their ability to extract and encode higher-level semantic information, thereby improving language modeling. SpeechTokenizer [486] is an RVQ-based codec model, where its first-layer codebook incorporates semantic information through the semantic distillation process, using HuBERT [160] representations as the semantic teacher. Mimi, used by Moshi [105], further improves upon this by replacing the semantic teacher from HuBERT with WavLM [56]. Additionally, it isolates the first-layer codebook from the RVQ process to achieve better semantic and acoustic disentanglement. To enhance the compression rate, WavTokenizer [178] is capable of quantizing one-second audio into 75 or 40 tokens with a single quantizer."}, {"title": "2.3.3 Video", "content": "Compared to images, videos introduce an additional temporal dimension that must be considered during the tokenization process. A straightforward strategy is to utilize an image-based VQVAE model to tokenize the video frame-by-frame. This approach is employed by several multimodal foundation models, such as LVM [13], LWM [257], and Unified-IO series [270, 271]. However, a significant drawback of frame-by-frame tokenization is its inability to compress video data over time, resulting in a high degree of token redundancy across frames\u2014particularly in long-form videos\u2014thereby imposing substantial computational demands [353]. Furthermore, using an image-based tokenizer fails to model temporal relationships between frames, leading to issues of temporal inconsistency.\nTo address token redundancy and enhance temporal modeling, several studies have proposed training a 3D tokenizer that compresses videos across spatial and temporal dimensions. For example, VideoGPT [437] applies a 3D-CNN architecture in the encoder and decoder of the video tokenizer. C-ViViT [388] uses a transformer architecture to split videos into 3D cubes, which are then discretized into token IDs.\nThere are two additional desirable features for a video tokenizer: (1) Joint Image-Video Tokenization. The MAGVIT series [456] enables tokenizing images and videos with a shared vocabulary. To achieve this, the number of frames in an input video, T, must satisfy $T = 1+ n\\times F_r$, meaning the video comprises an initial frame followed by n clips, each containing $F_r$ frames. When n = 0, the video contains only the initial frame, thus simplifying the video to an image. Accordingly, both the initial frame and each subsequent clip are discretized into a (1, H', W') token map, where H' and W' are the height and weight of the token map. (2) Temporal Causality. Compared to vanilla 3D architectures, using causal 3D architecture can ensure the tokenization and detokenization of each clip depend only on the preceding clips, facilitating autoregressive modeling along the temporal dimension."}, {"title": "2.3.4 More Modalities", "content": "Modeling various information as discrete tokens has gone far beyond the traditional text, image, video and audio modalities. In the computer vision field, we can unify the output spaces of tasks like object detection, semantic segmentation, and depth mapping into images. These can then be tokenized into discrete image tokens, allowing us to train a single NTP model to handle all these tasks [13, 396, 399]. In robotics and embodied Al domain, the robots actions in response to the environments can be coded into various discrete tokens and learn the policy in NTP manner as shown in recent studies such as VIMA [183], RT2 [32] and Locomotion NTP [324]. In AI4Science, by factorizing various proteins into DNA token sequences, protein language models are capable of learning from a wide array of sequences that span the evolutionary tree. These models have demonstrated their efficacy as powerful tools for sequence design and protein engineering, as highlighted in studies [281, 340]."}, {"title": "2.4 Continuous Tokenization Basics", "content": "Continuous tokens represent non-textual modalities in a continuous feature space, offering less information loss [51", "423": "."}]}