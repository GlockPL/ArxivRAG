{"title": "MVKTrans: Multi-View Knowledge Transfer for Robust Multiomics Classification", "authors": ["Shan Cong", "Zhiling Sang", "Hongwei Liu", "Haoran Luo", "Xin Wang", "Hong Liang", "Jie Hao", "Xiaohui Yao"], "abstract": "The distinct characteristics of multiomics data, including complex interactions within and across biological layers and disease heterogeneity (e.g., heterogeneity in etiology and clinical symptoms), drive us to develop novel designs to address unique challenges in multiomics prediction. In this paper, we propose the multi-view knowledge transfer learning (MVKTrans) framework, which transfers intra- and inter-omics knowledge in an adaptive manner by reviewing data heterogeneity and suppressing bias transfer, thereby enhancing classification performance. Specifically, we design a graph contrastive module that is trained on unlabeled data to effectively learn and transfer the underlying intra-omics patterns to the supervised task. This unsupervised pretraining promotes learning general and unbiased representations for each modality, regardless of the downstream tasks. In light of the varying discriminative capacities of modalities across different diseases and/or samples, we introduce an adaptive and bi-directional cross-omics distillation module. This module automatically identifies richer modalities and facilitates dynamic knowledge transfer from more informative to less informative omics, thereby enabling a more robust and generalized integration. Extensive experiments on four real biomedical datasets demonstrate the superior performance and robustness of MVKTrans compared to the state-of-the-art. Code and data are available at https://github.com/Yaolab-fantastic/MVKTrans.", "sections": [{"title": "I. INTRODUCTION", "content": "With recent technological advances in acquiring high-throughput omics data, multiomics integration is evolving as a rapidly growing research field [1], [2]. Compared to single omics, which can only reflect a part of biological complexity from a certain perspective, integrating multiple omics types holds the capacity to capture complementary information from diverse biological layers. Multiomics integration has exhibited promising performance in various biomedical tasks, such as clinical prediction and disease subtyping [3]. Traditional approaches often involve statistical and machine learning models, which may have limited capacity to capture the complex, non-linear relationships present in multiomics data. In recent years, applying deep learning (DL) models to multiomics studies has emerged for addressing these limitations [3]\u2013[6]. Well-designed DL methods are capable of identifying complex patterns inherent in individual omics and integrating information from various sources, thereby enriching the analytical insight into underlying biological processes.\nResearch on multiomics integration faces two main challenges: 1) the effective learning of feature representations and 2) the effective fusion of information derived from multiple omics. For the first challenge, different network architectures are employed to reduce redundancies and noises within high-dimensional omics data and produce meaningful and informative feature embeddings. A commonly used network is the encoder that compresses each modality into a non-linear latent space [1], [7], [8]. Autoencoder and its variants are also extensively used for learning efficient encodings of each omics type [9]\u2013[12]. Additionally, recent methodologies often represent omics data as graphs, leveraging graph neural networks to capture complex interactions effectively [13]\u2013[15]. It is worth noting that existing approaches often integrate feature representation with downstream tasks, such as clinical classification and survival prediction. While these task-guided strategies are designed to extract outcome-related features, their effectiveness would be influenced by factors such as disease complexity, inherent label bias, and sample heterogeneity. Furthermore, the intrinsic noisiness and inconsistent distribution characteristic of omics data impose limitations on the robustness and generalizability of existing methods.\nOn the other hand, effective modeling of complementary and interactive information among different omics can significantly improve the discriminative potential of fusion features [16]. However, the exploration in this specific research domain remains relatively limited. Regardless of the fusion stage\u2014early, intermediate, or late\u2014current approaches mainly use concatenation for fusing information from multiple sources [3], [12], overlooking the crucial inter-omics interactions. Recent strategies are developed to harness omics correlation in either intermediate or late fusion stages, demonstrating superior efficacy compared to simple concatenation [13], [17], [18]. For example, Mogonet [13] constructed the cross-omics tensor based on omics-specific classification probabilities and then employed the view correlation discovery network to incorporate label space omics correlations. Cross-modal attentions are also employed to capture inter-modality interactions [7], [17]. Alternatively, trustworthy fusion is proposed to evaluate the confidence level of each omics type and accordingly adapt to their quality differences [1], [19]. Despite the achievements of existing efforts, the intrinsic imbalances in informativeness and complex interactions among different omics can introduce irrelevant information during the fusion process, thus, in turn, hindering robust multiomics integration.\nBased on the above observations, we propose a multi-view knowledge transfer method (MVKTrans) for robust multiomics classification as illustrated in Fig. 1, which incorporates omics-specific pretraining (intra-view knowledge transfer (KT)) and cross-omics adaptive distillation (inter-view KT) to promote model stability and generalizability. Specifically, during the pretraining phase, we introduce omics-specific graph contrastive learning (GCL) to initialize the model parameters trained from unlabeled data, which facilitates a foundational understanding of latent structural patterns and interrelations within each omics type. Based on the pre-trained initializations, omics-specific graph attention networks (GAT) are built to generate initial predictions. Afterward, we introduce a cross-omics distillation (CD) module to facilitate adaptive knowledge transfer among disparate omics types, with a cross-omics attention module incorporated to model modality correlation and bridge the gap in label distributions. This endows the model with robustness for integrating heterogeneous omics sources. Our main contribution can be summarized as follows:\n\u2022 We model the multiomics data in a multi-view knowledge transfer framework to learn and pass the intra- and inter-omics knowledge in an adaptive manner. To the best of our knowledge, this is the first work to integrate both intra- and inter-modal transfer learning for robust multimodal classification.\n\u2022 To tackle real-world issues in multiomics data, such as label bias and omics informativeness imbalances, we specifically design an unsupervised pretraining module that employs graph contrastive learning to promote the learning of more general and unbiased representations, along with a CD module that adaptively transfers generalizable and reliable knowledge from information-rich to less informative modalities.\n\u2022 We conduct extensive experiments across four multiomics classification tasks to show the superiority of MVKTrans over the SOTA. Ablation and perturbation studies confirm the effectiveness and robustness of MVKTrans."}, {"title": "II. THE PROPOSED METHOD", "content": "In the following sections, we denote matrices as bold-face uppercase letters and vectors as boldface lowercase ones. Let $m\\in [1,..., M]$ denotes the m-th omics, $X^m = {x_1^m,...,x_n^m}\\in \\mathbb{R}^{n\\times d_m}$ represents the m-th feature matrix, and $y = [y_1,..., y_n] \\in \\mathbb{R}^{n}$ denotes the label vector, where n is the number of samples and $d_m$ is the number of features.\nWe now detail the proposed MVKTrans. The overall architecture and important modules are illustrated in Fig. 1."}, {"title": "A. Omics-specific graph construction", "content": "To formulate the functional relationships within samples and enable graph-based modeling, each omics data is transformed into a graph $G_m=(X_m, E_m)$, where $X_m\\in \\mathbb{R}^{n\\times d_m}$ is the feature matrix, and $E_m \\in \\mathbb{R}^{n \\times n}$ is the edge matrix. Specifically, $E_m$ is obtained by thresholding the adjacency matrix, which calculates sample-sample similarity:\n$E_{ij}^m = \\begin{cases} 1 & \\text{if } s(x_i^m, x_j^m) \\geq \\delta\\\\ 0 & \\text{otherwise} \\end{cases}$                                                                                                                                                                               (1)\nwhere $s(x_i^m, x_j^m) = \\frac{x_i^m\\cdot x_j^m}{\\left\\|x_i^m\\right\\|\\left\\|x_j^m\\right\\|}$ computes the cosine similarity between samples $x_i^m$ and $x_j^m$ and $\\delta$ is a hyperparameter for thresholding. We set $\\delta$ = 0.05 across experiments.\nConsequently, M sample similarity graphs $G = {G_m}_{m=1}^M$ are constructed to serve as model inputs, with nodes denoting samples and edges denoting the relationship between them."}, {"title": "B. Intra-omics KT: graph contrastive pretraining", "content": "In the context of graph-structured data, GCL is widely studied, which focuses on maximizing agreement between various graph augmentations to facilitate the capture of implicit semantic information of the original data, independent of external labels [20], [21]. Here, we utilize GCL-based pretraining to decipher the underlying molecular mechanisms inherent in each omics type. A typical GCL comprises four major components: graph augmentation, a backbone encoder, a projection head, and a contrastive loss function.\nGraph augmentation. Given a graph $G = (X, E)$, two corrupted graph views denoted as $G_1 = (X_1, E)$ and $G_2 = (X_2, E)$ are generated by randomly masking node features from G:\n$X_1 = t_1(X, p_1) = {x_1 \\odot p_1, x_2 \\odot p_1, ..., x_n \\odot p_1},$                                                                                                                                                                       (2)\n$X_2 = t_2(X, p_2) = {x_1 \\odot p_2, x_2 \\odot p_2, ..., x_n \\odot p_2},$\nwhere $p_1 \\in {0, 1}^{d_m}$ and $p_2 \\in {0, 1}^{d_m}$ are masking vectors, with each element independently sampled from Bernoulli distribution $B(p_1)$ and $B(p_2)$, respectively. The hyperparameters $p_1$ and $p_2$ are used to control the degree of corruption to generate $G_1$ and $G_2$. We tune $p_1$ and $p_2$ using a grid search within the range of 0.2 to 0.8 and set $p_1$ = 0.3 and $p_2$ = 0.2 in the implementation.\nEncoder. We use multi-head GAT as the encoder to represent node features, utilizing the graph view as input to produce node embeddings, denoted as: $H_1 = GAT(G_1)$ and $H_2 = GAT(G_2)$.\nProjection head. A multilayer perceptron (MLP) is used to map the embeddings to latent spaces $K_1$ and $K_2$ for applying the contrastive loss: $K_1 = MLP(H_1)$ and $K_2 = MLP(H_2)$.\nContrastive loss. The contrastive objective is to make the embeddings of each node from the two views close to each other (i.e., positive pairs) while being distinguishable from the representations of other nodes (i.e., negative pairs) [21]. As shown in Fig. 1(b), for any node $i$, its embeddings from two graph views form the positive pair $(\\mu_i, v_i)$, while all other $2(n - 1)$ nodes from these two views are defined as negative samples. Let $s(\\mu_i, v_i)$ calculate the cosine similarity between two node embeddings. Then, for each positive pair, the contrastive loss is defined as:\nl(\\mu_i, v_i)=-log\\frac{exp(s(\\mu_i, v_i)/\\tau)}{exp(s(\\mu_i, v_i)/\\tau) + \\sum_{j=1,j\\neq i}^{n} exp(s(\\mu_j, \\psi_j)/\\tau)}, \\psi_j\\in {\\mu_j\\cup v_j}                                                                                                                                                                                                                                                        (3)\nwhere $\\tau$ denotes a temperature parameter and is set to be 0.5. The final loss is computed across all positive pairs:\nL_{est}= \\sum_{i=1}^{n} (l(\\mu_i, v_i) + l(v_i, \\mu_i)).                                                                                                                                                                                                                                                                                                                                                                                          (4)\nThe parameters of GAT and MLP are updated by minimizing the contrastive loss during each training epoch. The optimized encoder parameters, denoted as $W_{GCL}$, are transferred as initial configurations for the subsequent classification task. Using these pre-trained parameters, omics-specific GAT is fine-tuned to generate initial prediction probabilities."}, {"title": "C. Inter-omics KT: cross-omics distillation", "content": "To address the omics disparity issue, we introduce graph-based knowledge distillation (KD) [22], [23] to conduct adaptive cross-omics distillation. This integration is employed to aid the cross-attention mechanism in automatically adapting and dynamically regulating disparities during the omics fusion process. Traditionally, KD [24] for two modalities involves transferring knowledge from the stronger to the weaker one. Considering two modalities $m_1$ and $m_2$ with the distillation direction from $m_1$ to $m_2$ (i.e., $m_1 > m_2$), the training loss of KD is composed of classification loss and distillation loss: $l = \\gamma_1 l_{cls} + \\gamma_2 l_{dis}$. Here, the distillation loss represents the information flowing from modality $m_1$ to $m_2$, typically quantified by the logit and/or representation distances between the source and target.\nYet, predetermining the direction and strength of distillation is challenging when handling multiple modalities. Moreover, a diverse KD is necessary due to the heterogeneity of diseases and omics types. As shown in Fig. 1(d), cross-omics distillation formulates a weighted, directed graph, denoted as $G = (V, E)$. The set V comprises |V| = M vertices with each vertex representing one omics type, while E is the edge matrix with each element $e_{k\\leftarrow i} \\in [0,1]$ denoting the distillation strength from omics j to k. Let $l_{k\\leftarrow i}$ denote the distillation loss from vertex $v_j$ to $v_k$ ($l_{k\\leftarrow i} \\neq l_{i\\leftarrow k}$). Here, we employ the logits loss because we use the late-fusion strategy in the model (as discussed in Sec. II-D). $L_1$ distance is used to measure the logits loss between source and target omics.\nLet $z_k^i$ denote the logits of sample i for modality k, the total CD loss flowing on graph G can be expressed as:\n$L_{CD} = \\sum_{i=1}^{M} \\sum_{k=1}^{M} \\sum_{j\\in N(k)} e_{k\\leftarrow j}^i l_{k\\leftarrow j}^i,$                                                                                                                                                                                                                                                                                              (5)\n$e_{j \\rightarrow k}^i = W_2[W_1z_j^i, W_1z_k^i],$                                                                                                                                                                                                                                                                                          (6)\n$l_{k\\leftarrow j}^i =||z_k^i- z_j^i||_1,$                                                                                                                                                                                                                                                                                                                                                                                                                                                 (7)\nwhere $W_1$ and $W_2$ are parameters to learn, $[,]$ is concatenation, $N(k)$ indicates the set of source vertices for node k."}, {"title": "D. MVKTrans model architecture", "content": "Now, we delineate the overall architecture of the proposed MVKTrans. As shown in Fig. 1(a), the framework integrates multiple omics types and predicts the diagnostic status of each participant. Generally, we employ the GAT as omics encoders and use the late-fusion strategy to integrate multiomics data. More specifically, the MVKTrans is designed as follows:\nGAT encoders. Using pre-trained parameters (denoted as $W_{GCL}^m$, $m\\in [1, ..., M]$) for initialization, encoders based on the GAT architecture are fine-tuned to generate omics-specific representations $F_m = GAT(G_m, W_{GCL}^m)$.\nAuxiliary classifiers. Each auxiliary classifier (AC) uses GAT-encoded representations and sample labels to perform mono-omics prediction:\n$\\hat{y}^m = MLP (F_m),$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (8)\n$\\mathcal{L}_{AC} = \\sum_{m=1}^M \\mathcal{L}_{om} = \\sum_{m=1}^M\\sum_{i=1}^n  L_{CE} (y_i, \\hat{y}_i^m).$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (9)\nOmics-specific self attention module. We incorporate the self-attention mechanism (Fig. 1(a,c)) to dynamically weigh the significance of logits from various omics types, ensuring the prioritization of the most salient omics information:\n$U^m = Attention (F^mW_Q, F^mW_K,F^mW_V).$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (10)\nCross-omics attention module. We introduce cross-omics attention to reconcile the distribution disparities between high-level heterogeneous features, thereby reinforcing further distillation. For example, omics $U^m$ can be enhanced by the other omics types:\n$Z^m =  \\underset{j=1,j\\neq m}{M}{||} Attention (U^mW_Q,W_K,U^jW_V).$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (11)\nCross-omics distillation module. Details of the CD module are discussed in Sec. II-C. As illustrated in Fig. 1(d), the distillation module is composed of FC layers and is optimized at the training time in an end-to-end manner.\nObjective optimization. The overall loss is composed of the auxiliary classification loss (Eq. 9), the CD loss (Eq. 5), and the cross-entropy loss of the final classification:\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_{AC} + \\lambda_2 \\mathcal{L}_{CD} + L_{Final},$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (12)\nwhere $\\lambda_1$ and $\\lambda_2$ are hyperparameters for adjusting different losses. We set $\\lambda_1$=1 and $\\lambda_2$=0.005 across all experiments."}, {"title": "III. EXPERIMENTS", "content": "Multiomics datasets. Our experiments are conducted on four real-world datasets [13], including ROSMAP for AD, LGG for low-grade glioma grading, BRCA for five breast cancer subtyping, and KIPAN for three renal cancer subtyping. Three omics types (mRNA, methylation, and miRNA) are available for each dataset. Details are listed in Table I.\nBenchmark methods. We compare the proposed method with 8 competitors, including three single-omics classifiers with early fusion (SVM, XGBoost, and NN), four concatenation-based methods (GRidge [25], BSPLSDA [26], CF [27], and gated multimodal fusion (GMU [28])), and three methods with advanced representation and fusion designs (Mogonet [13], MODILM [18] and Dynamics [1]).\nEvaluation metrics. To assess the performance of methods in binary classification, we employ accuracy (ACC), F1 score (F1), and the area under the receiver operating characteristic curve (AUC). For multiclass classification, metrics include ACC, the average weighted F1 (F1-w), and the macro-averaged F1 (F1-m). Experiments are conducted five times to ascertain the mean and standard deviation. T-tests are performed to evaluate the significance of the improvements.\nImplementation details. We use PyTorch to develop the model and use Adam as the optimizer. In GCL pre-training, we train the model for 2000 epochs with a learning rate (lr) of 1e-3. In fine-tuning, the model is trained for 5000 epochs, with the lr of GAT set at 5e-3 and the lr for inter-omics KT set at 3e-3. We implement all experiments on an RTX 3090 GPU with 24GB memory."}, {"title": "B. Comparison with the state-of-the-art", "content": "Table II presents the comparison results on four benchmark datasets. It can be observed that 1) the proposed MVKTrans outperforms other methods in terms of three metrics, and 2) MVKTrans consistently shows significant improvements (t-test p < 0.05) over the suboptimal results in the ROSMAP, LGG, and BRCA datasets. The superior performance is attributed to the particularly designed intra- and inter-omics KT modules. In subsequent evaluations, we exclude the KIPAN dataset due to its relatively straightforward classification."}, {"title": "C. Ablation study", "content": "We conduct ablation studies to evaluate the effectiveness of the intra-omics and inter-omics KT components. We accordingly remove the GCL pretraining and replace the cross-omics distillation with fully connected NN for comparison. Table III shows the results, from which we have the following observations: 1) incorporating GCL enhances prediction performance, affirming that unsupervised pretraining, which mitigates label bias, effectively captures latent structural patterns in omics data; 2) CD module (Fig. 1(d)) enables compensatory distillation to enhance discriminative abilities of each omics, subsequently boosting model performance."}, {"title": "D. Evaluations of omics contributions", "content": "Fig. 2 shows the results of applying our model in various omics combinations. We can observe that the integration of multiple omics surpasses their subsets, underscoring the distinct contribution of each omics and validating the capacity of our method in modeling cross-omics information."}, {"title": "E. Robustness study", "content": "To evaluate robustness against data perturbation, we set varying feature missing rates (from 20% to 80%) to the data and then compare the proposed model against others. We include the Mogonet, MODILM, and Dynamics in comparison, given their relatively good performances. Fig. 3 illustrates that the proposed MVKTrans consistently outperforms under various missing rates, exhibiting substantial robustness at both lower and higher levels of data loss. For example, in comparison to the suboptimal Dynamics model, our method experiences a smaller decline of 5.1% (vs. 9.0%) in F1-w on BRCA with a 40% missing rate, and shows a weaker decrease of 15.3% (vs. 16.9%) in ACC on LGG at an 80% missing rate."}, {"title": "F. Biomarker identification and explanation", "content": "Identifying biomarkers plays a crucial role in disease inference, offering essential insights that are instrumental in unraveling pathological mechanisms and facilitating novel treatments. Feature ablation experiments [29] are conducted to pinpoint key biomarkers, with the most significant ones presented in Table IV. These findings offer promising prospects for subsequent wet-lab validation."}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose an adaptive multiomics integration framework introducing multi-view knowledge transfer for robust diagnosis classification. Graph contrastive pretraining is introduced to derive intra-omics patterns in an unsupervised manner. Cross-omics distillation facilitates the automatic transfer of inter-omics information from stronger to weaker modalities. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach. Concurrently, we have identified biomarkers that significantly impact disease inference, offering vital insights for elucidating pathological mechanisms and developing novel therapeutic approaches."}]}