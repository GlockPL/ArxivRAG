{"title": "CHILLI: A data context-aware perturbation method for XAI", "authors": ["Saif Anwar", "Nathan Griffiths", "Abhir Bhalerao", "Thomas Popham", "Mark Bell"], "abstract": "The trustworthiness of Machine Learning (ML) models can be difficult to assess, but is critical in high-risk or ethically sensitive applications. Many models are treated as a 'black-box' where the reasoning or criteria for a final decision is opaque to the user. To address this, some existing Explain-able AI (XAI) approaches approximate model behaviour using perturbed data. However, such methods have been criticised for ignoring feature dependencies, with explanations being based on potentially unrealistic data. We propose a novel framework, CHILLI, for incorporating data context into XAI by generating contextually aware perturbations, which are faithful to the training data of the base model being explained. This is shown to improve both the soundness and accuracy of the explanations.", "sections": [{"title": "1. Introduction", "content": "Machine Learning (ML) and Artificial Intelligence (AI) are increasingly being used to tackle problems in a variety of domains because of their prodigious performance in automated decision-making. Some of these domains have high associated risks, such as financial systems (Ala'raj &\nAbbod, 2016; Byanjankar et al., 2015), healthcare (Lodhi\net al., 2017; Mikalsen et al., 2018) and criminal justice (Rig-ano, 2019). Incorrect decisions in these scenarios can have significant repercussions, and making decisions with intentional or inadvertent biases can lead to discrimination and other social consequences (Reuters, 2018; Sweeney, 2013).\nTherefore, it is essential that the decisions made by an ML model are trusted before being acted upon. The foundation of such trust is dependent on both developers and end users understanding the reasoning behind a model's decisions.\nDue to the complexity of many ML techniques, they are often treated as a 'black-box' where the reasoning for a prediction can be difficult to ascertain. Such understanding would allow users to better detect biases in data, assess the vulnerabilities of a model, and ensure a model meets any regulatory standards (Goodman & Flaxman, 2017) and societal expectations. Explainable AI (XAI) methods aim to increase confidence in AI systems, supporting their acceptance and wider adoption. While the use of XAI terminology varies, we define explainability as providing evidence or reasoning for all outputs via an explanation, interpretability is the notion that all explanations must be understandable to users, and faithfulness is a measure of how accurately an explanation reflects the behaviour of an AI system.\nWhile some ML models are inherently interpretable (Sudjianto & Zhang, 2021), e.g., decision trees, where the be-haviour of a model can implicitly be explained (Breiman, 2017), other ML techniques require explanations to be generated separately. Post-hoc XAI attempts to form explanations after a predictive model has been learnt. Such approaches are often model-agnostic and applicable to a range of ML techniques (Goldstein et al., 2014; Molnar, 2023; Ribeiro\net al., 2016b). However, evaluations of model-agnostic ap-proaches have shown that, just as ML models are adapted to their context, XAI systems should also be adapted to the appropriate deployment domain (Zhang et al., 2019; Sokol et al., 2019).\nIt is often challenging to interpret context from numerical data representing quantitative information, features and signal values. For example, a value of 0.5 may represent a probability of 50% or a value in the range [0,10]. This is a common problem in XAI, where feature values are used to explain predictions without contextual knowledge of the data (Sokol et al., 2019; Zhang et al., 2019). Earlier works (Lieberman & Selker, 2000; Selker & Burleson, 2000) discuss the importance of context sensitivity for computer systems, which is crucial for XAI in understanding complex ML models. An XAI framework requires underlying domain knowledge of numerical data, to incorporate the appropriate semantics into the explanation. In this paper, we explore the effects of incorporating contextual domain knowledge into XAI, highlighting its importance when explaining predictions. We demonstrate this by evaluating the interpretability and faithfulness of explanations in an intuitive and quantitative manner. The contributions of this paper are as follows."}, {"title": "2. Related Work", "content": "XAI methods either produce global or local explanations (Mohseni et al., 2021). The former explain model behaviour overall (Wang et al., 2014; Lakkaraju et al., 2016), whereas the latter focus on a small area of the decision space, such as around a particular data instance (Ribeiro et al., 2016b; 2018; Zeiler & Fergus, 2013; Baehrens et al., 2009).\n2.1. Inherently Interpretable Models\nThe structure of some ML models is inherently interpretable, e.g., linear regression where feature coefficients can be observed (Murphy, 2023), and decision trees where the decision path can be traced. In these cases, an explanation is the base model itself, which is of course completely faithful to its own behaviour. As a result, such model types are often favoured in high-risk scenarios (Rudin, 2019).\n2.2. Post-hoc Approaches\nIt is not always be possible to use an inherently interpretable model, for example if a pre-built model requires explaining. Moreover, for some applications the best performing mod-els are highly complex with large numbers of parameters (Simonyan & Zisserman, 2015) and are consequently not inherently interpretable (Wickramanayake et al., 2021). While there is increasing research into high performing inherently interpretable models (Sudjianto & Zhang, 2021), post-hoc XAI methods have been developed to explain existing mod-els. These are typically model-agnostic, using only input and output data to understand model behaviour. Some meth-ods use counterfactual explanations, by highlighting the consequence of modifying an input on a prediction (Verma et al., 2020), while others, such as Shapley values (Messalas et al., 2019) or proxy models, present contributions of features towards a prediction as an explanation.\nProxy models approximate the behaviour of a base model in an interpretable form, such as a decision tree (Schmitz et al., 1999) or linear regression model (Ribeiro et al., 2016b). The proxy model is used as an explanation without sacrificing predictive performance of the base model. This is achieved by fitting a simpler proxy model to the base model predic-tions. A global proxy model would approximate the base model behaviour in all areas to increase interpretability, how-ever this may not be sufficiently faithful because of oversimplification. It is generally accepted that there is a trade-off between faithfulness and interpretability (Do\u0161ilovi\u0107 et al., 2018). Therefore, some XAI approaches use a local proxy model fit to the neighbourhood of an instance being ex-plained. This reduces the coverage of the approximation, and so a more faithful explanation, yet with low complexity may be formed (Wood-Doughty et al., 2021).\n2.3. Perturbation Based Methods\nThe Local Interpretable Model-Agnostic Explanations (LIME) method (Ribeiro et al., 2016b) explains the pre-diction for a given instance by fitting a proxy model in its locality. Since there may not be sufficient training data in a locality to fit a proxy model, algorithms such as LIME fit a proxy model to a set of synthetic perturbations of the instance being explained.\nIn LIME, a set of perturbed inputs, Z, is generated in the lo-cality of an input instance, x, whose output prediction from some base model, f, is being explained. The base model is used to predict a set of target values, f(Z), for the perturba-tions. A local proxy model, g, is then fit to this perturbed dataset. Non-categorical features are perturbed in LIME by sampling from a Normal distribution with mean and stan-dard deviation estimated from the training data. Samples are taken from the center of the training data and then scaled around the instance (Garreau & von Luxburg, 2020). Cate-gorical features are perturbed by uniformly sampling from the distribution of feature values in the training data.\nWhen fitting the proxy model according to some loss func-tion, the loss contribution of each perturbation, z \u2208 Z, is weighted by a proximity measure, \u03c0x(z), according to some distance function, D(x, z), to ensure the explanation is lo-"}, {"title": "3. Contextually Enhanced Interpretable Local\nExplainable AI", "content": "In this section, we propose Contextually Enhanced In-tepretable Local Explainable AI (CHILLI), an XAI frame-work that combines the contextually aware proximity mea-sures and domain representative perturbation generation method presented below to explain base model behaviour using local proxy models. CHILLI aims to satisfy potential contextual constraints and consider limitations of numerical data. Explanations are fit to perturbed data that is representative of the base model training data and is local to the instance being explained. CHILLI is based on LIME (Ribeiro et al., 2016b), with modifications to the proximity calculations and perturbation generation methods.\n3.1. Contextually Aware Proximity Measures\nProximity in LIME is based on Euclidean distance (see above, Section 2.3), irrespective of the feature type. However, if for some features the absolute difference between two values does not reflect their distance, this proximity measure becomes invalid. This is the case if, for example, the units are not equidistant, or a feature is not measured linearly, such as magnitude recorded on a logarithmic scale. Such distance measures are also unsuitable for cyclic or tem-poral features e.g., hour of day where raw values for 23:00 and 00:00 appear to be far apart, but domain knowledge informs us that they are consecutive.\nWe propose that the context of features should be considered independently by incorporating the scale and bounds of each feature to ensure the calculated distance is truly representative. Consider the points p and q, represented by feature vectors of d dimensions. Instead of using a generic distance function, such as Euclidean distance, the distance between the points, D(p, q), is calculated individually for each fea-ture dimension, i, using a specified distance function, Di. The distance in each feature dimension is normalised, to allow for equal contribution, and averaged across all dimen-sions to give a single distance value. From this, a proximity measure can be calculated, as shown in Equation 2.\n\u03c0\u03c1(q)\n= exp\n( -(\u2211d\ni=1 Di(pi, qi))2Ed\n\u03c32\n) (2)\nSince the proximity measure is used when quantifying the performance of each explanation, g, an accurate proximity measure is essential to ensure the most faithful explanation model, g, is selected from the set of possible explanations, G.\nThe value of the locality hyperparameter, o, may be adjusted to vary the locality of an explanation in the model space. As \u03c3 increases, the proximity tends to 1, as shown in Figure 2 for a range of distance values. All perturbations for a high"}, {"title": "3.2. Domain Representative Perturbation Generation", "content": "Existing perturbation-based XAI methods, such as LIME, do not consider contextual knowledge when generating per-turbations, which are the foundation for creating an expla-nation (Ribeiro et al., 2016a;b; Zhang et al., 2019; Sokol et al., 2019). Perturbations in LIME are sampled uniformly from the entire feature space, and a proxy model fit to these will focus on all relationships in the feature space. Such an explanation is not local to the instance being explained, but is generalised to the overall model behaviour. Presenting this as a local explanation may be misleading regarding the behaviour of the model.\nSuch perturbations also ignore any bounds on feature values, such as 'Age' which can only take positive values. Without considering these bounds, perturbations may contain unrealistic values. Moreover, since features are perturbed independently, feature dependencies are ignored. For example, assume that as the population of a city grows, traffic congestion increases. Although these features are correlated, ignoring feature dependancies may result in a perturbation combining lowered congestion with increased population. The omission of such dependencies may result in an unre-alistic set of perturbations, leading to an explanation that does not describe the behaviour of the base model (Laugel et al., 2019). An XAI framework should generate perturba-tions that are representative of real-world data, such as the training data, and are local to the instance being explained.\nWe present a framework, CHILLI, for generating local and contextually conforming perturbations. Our method takes in-spiration from the SMOTE algorithm (Chawla et al., 2002). SMOTE is a well-regarded sampling technique used to generate synthetic data when there is class imbalance (Chawla et al., 2002). A random point is selected from the minority class and its k-nearest-neighbours are located, for a prede-termined number, k. One of these neighbours is selected uniformly at random and a synthetic datapoint is generated by linearly interpolating between the two points and uni-formly selecting a random point on the line joining the two. This is repeated for different instances from the minority class until a specified degree of over-sampling has been achieved.\nWe use this approach to generate perturbations, and to en-sure perturbations fall within realistic bounds, they are pro-duced by interpolating between an instance being explained, x, and some other randomly selected instance, x', in the training data. Each feature value is interpolated indepen-dently. For categorical features, the interpolated value is rounded to the nearest feature value.\nTo maintain the locality of the perturbations, the selection of x' is from a probability distribution calculated using proximity (Equation 2) which is normalised for all data points such that P(x' = xi) = \u03c0x(xi) and \u2211x;\u2208x P(x' =\nXi) = 1. As a result, perturbations are more likely to contain values in closer proximity to x. The process for generating a set of N perturbations is outlined in Algorithm 1."}, {"title": "Algorithm 1 Contextual Perturbation Generation", "content": "Input: Number of perturbations to generate, N; Data in-stance to perturb, x; Training dataset, X\nOutput: Set of perturbations Z\n1: F Features of x\n2: Initialise empty set of perturbations, Z []\n3: Calculate \u03c0\u03b1 (x\u00b2) for each x\u00b2 \u2208 X\n4: Assign a probability to each x\u00b2 where P(x' = x\u00b2) =\n\u03c0\u03b1 (x\u00b2)\nmax(x(xi)\u2200x\u00b2\u2208X)\n5: while Z < N do\n6: Uniformly select some value between 0 and 1 \u2192 I\n7: Select some x \u2208 X based on probability for each\nxix'\n8: for f in F do\n9: Zf = xf + I(x'f - xf)\n10: end for\n11: Z = ZU {z}\n12: end while"}, {"title": "4. Experimental Setup", "content": "We compare the functionality and performance of our pro-posed method, CHILLI, with that of LIME (Ribeiro et al., 2016b), to explore the effect of incorporating contextual information into XAI frameworks."}, {"title": "4.1. Datasets", "content": "Our evaluation uses the WebTRIS and MIDAS datasets. WebTRIS (National Highways, 2017), recorded by High-ways England, contains traffic data at 15 minute intervals for many motorway sites around England. We restrict the dataset to a single site (M6/7570A) between 01/01/2016 and 01/01/2017. A Support Vector Regressor (SVR) is trained on a subset of the available features describing date, time, average speed and number of vehicles of various sizes, to predict the 'Total Volume' of traffic per time interval. The data distribution in the individual feature dimensions is shown in Figure 3.\nMIDAS (Office, 2022), published by the UK Meteorological Office, records hourly weather observations at multiple lo-cations across the UK. A Recurrent Neural Network (RNN) is trained on data containing various weather features from a station located at Keswick and 3 neighbouring stations (St. Bees Head, Shap and Warcop Firing Range) to predict 'Air Temperature' at Keswick at a given time. It is expected that observations from surrounding areas will be related to the upcoming weather at Keswick, and therefore the data used from neighbouring stations is offset by 1 hour. The distribution of the training data is shown in Figure 4.\nWe can hypothesise about expected feature importance due to the linearity of the feature relationships against the target variable. Since LIME scales perturbations according to the covariance of the feature against the target variable, we explore the effect on explanation performance of removing generally linear features from the MIDAS data (namely, those describing relative humidity and dewpoint)."}, {"title": "4.2. Forming Explanations", "content": "Explanations containing a set of linear coefficients are pro-duced using CHILLI and LIME for predictions made by a base model. The magnitude of each coefficient indicates the contribution of the corresponding feature towards the base model prediction, whilst the sign indicates the direction of the correlation between the feature and the target variable.\nWe quantify the performance of an explanation using its er-ror, which represents its faithfulness towards the base model. Explanations for WebTRIS predictions are quantified using RMSE and MAE is used for MIDAS predictions. We com-pare the error for explanations produced using CHILLI and LIME over 25 instances, selected uniformly at random. The selected instances are shown in Figures 3 and 4."}, {"title": "5. Results & Discussion", "content": "In this section, we use LIME and CHILLI to fit a local proxy model which is used to explain a prediction produced by a base model for a given instance.\n5.1. Perturbation Generation\nFigure 5 shows explanations produced by CHILLI and LIME alongisde the perturbations used to fit them for a prediction made by the SVR base model, f, for a randomly selected instance, x, from the WebTRIS test dataset. The instance is shown as the black point in each of its feature dimensions against the true 'Total Volume' value, which is 47. The perturbations of the instance with the prediction of the target feature \u2018Total Volume' from the base model, f(z), are shown as orange points.\nFrom a visual inspection of Figure 5a, it can be seen that the perturbations of features generated by LIME do not follow the data distribution shown in Figure 3. Moreover, since each feature is perturbed independently, feature values in a single perturbation do not consider feature dependencies, which leads to unrealistic perturbations being generated. For example, a perturbation may have a 'Time Interval' of 03:00, but the value of '0-520cm' may correspond to the number of vehicles that would be observed at rush hour.\nThe bounds of features have also not been considered, as can be seen in Figure 5a where all non-categorical features exhibit perturbed values which fall outside the normalised range of [0, 1]. Negative values of \u20180-520cm', \u2018521-660cm' and '661-1160cm' imply a negative number of vehicles of the respective sizes passing in the corresponding time interval, which is not possible. This leads to a set of pertur-bations that do not represent real-world data, and therefore do not represent the training data. The negative impact of such inappropriate perturbations can be observed from the predicted values from the base model, which often predicts a negative volume of traffic flow, which is also not possi-ble. An explanation that is fit on such perturbations will not correctly represent the true behaviour of the base model.\nThe opacity of each perturbation, shown in Figure 5a, sig-nifies its calculated proximity weighting, \u03c0\u2081(z), to the in-stance being explained. Perturbations which are further from the instance are sometimes assigned a higher weighting than those which are closer. Since this indicates the contribution of each perturbation to the selection of the best fit linear proxy model, the produced explanation will not be locally focused around the instance being explained, and is instead a generalised explanation across all the perturbations.\nOn the other hand, the perturbations generated by CHILLI not only conform to the distribution of the training data shown in Figure 3, but are also realistic combinations of feature values that fall within the appropriate feature bounds. CHILLI also generates perturbations with greater density around the instance being explained, which can be seen in Figure 5a from the concentration of orange points around the instance. This is also the case for features of a cyclic nature, such as 'Time Interval' in WebTRIS, where 0 and 1"}, {"title": "5.2. Feature Contributions", "content": "The linear proxy model with the lowest error when fit to the set of perturbations, Z, and base model predictions, f(Z), is selected as the explanation for the instance being explained. The explanations shown in Figure 5b indicate that CHILLI produces explanations with greater disparity between fea-ture coefficients. It is expected for explanations produced by CHILLI to have larger feature coefficients than LIME, since the perturbations generated by LIME are based on a Normal distribution which naturally does not exhibit any linear correlation. Due to the covariance scaling of LIME perturbations towards the training data, some features, such as '0-520cm' in the WebTRIS data, exhibit a strong linear correlation with 'Total Volume' as shown in Figure 3. LIME recognises this and identifies it as the most significant fea-ture contribution in its explanation. Similarly, features in the MIDAS data containing 'dewpoint' and 'relative humidity' have a linear correlation with 'Keswick Air Temperature' across their range of values, as can be seen from Figure 4.\nFigure 6 shows the variation in feature contributions in ex-planations produced for the 25 instances shown in Figure 4. Again, only generally linear features have noticeable contribution in the explanations produced by LIME. This is unsuitable since general feature trends are not relevant in a locally focused explaination. CHILLI produced ex-planations that are fit to perturbations local to the instance being explained, and recognises general linear trends in cases where the linear relationship is also present locally. However, CHILLI also highlights contributions from other locally impactful features, although they are not as signifi-cant as the generally linear features.\nUpon removal of generally linear features, there is greater variation in the explanations produced by CHILLI, as shown in Figure 7. Since LIME cannot detect local behaviour, it performs poorly when locality is important, and does not identify any significant feature contributions due to the absence of general trends. CHILLI, on the other hand, is able to detect local trends and achieves significantly lower MAE, indicating that the explanations produced by CHILLI are more faithful to the true behaviour of the base model."}, {"title": "5.3. Explanation Faithfulness", "content": "As noted in Section 2.4, a lower error indicates a more faithful explanation. The explanation produced by CHILLI achieved a signifcantly lower RMSE than LIME on the perturbations shown in Figure 5. The explanation produced by CHILLI predicted the 'Total Volume' of traffic for the instance to be 81 whilst LIME's explanation predicted 93. The lower error of the CHILLI explanation, combined with a"}, {"title": "5.4. Locality Hyperparameter Exploration", "content": "The importance of accurate proximity measurements can be understood by observing the effect of varying \u03c3 on MAE. Figure 9 shows a comparison of the MAE achieved by LIME and CHILLI for a uniformly randomly selected instance from MIDAS, when using different values of \u03c3. The MAE achieved by LIME is similar across all values of \u03c3. Since LIME forms explanations that do not consider the local data context of the instance being explained, it is not ex-pected for them to vary based on locality size. CHILLI achieves lower MAE for lower values of o before stabilising at higher values. As the defined locality increases, perturba-tions are considered that are further from the instance being explained. Features which do not exhibit linear relationships on a broader scale are difficult to describe using a linear proxy model. This leads to a worse performing explanation, since it attempts to generalise behaviour rather than explain-ing local trends, as with LIME. While MAE increases when using CHILLI, it still outperforms LIME since the intuition regarding perturbation generation is sound."}, {"title": "6. Conclusion and Future Work", "content": "In this paper, we explored the effect of incorporating con-textual domain knowledge into a model-agnostic local perturbation-based XAI approach, namely LIME. We pro-posed a method for contextually aware proximity measures, to ensure locality is accurately defined and constrained. We also proposed a method for generating perturbations that consider the contextual limitations and dependencies of data features. These methods are combined into a new frame-work, CHILLI, for generating local explanations for black-box ML models, and we compared the functionality and performance of CHILLI with LIME.\nUsing the WebTRIS and MIDAS datasets, we demonstrated that LIME does not appropriately measure proximity be-tween instances, resulting in an explanation which is not local to the instance being explained. Explanations gen-erated by LIME were found to be generalised and only consider features with general linear trends. It was also found that LIME does not generate perturbations that are representative of the training data, and the perturbations contained unrealistic values.\nCHILLI was shown to generate perturbations that are repre-sentative of the base model training data and are local to the instance being explained. Therefore, CHILLI's explanations had relatively larger feature contributions compared to those produced by LIME. CHILLI consistently achieved a lower error, and therefore produced a more faithful explanation, across all explained instances compared to LIME.\nThrough empirical and intuitive evaluation of LIME and CHILLI, we conclude that incorporating contextual domain knowledge regarding data features used for generating ex-planations improves faithfulness, which may ultimately in-crease trust in both the explanation and explanation frame-work. In future work we will investigate how improving the performance of local explanations affects the overall trust in a model. We would also like to explore the efficacy of CHILLI when proxy models of a different form are used, such as decision trees or small-order polynomial regressors."}]}