{"title": "DO DEEPFAKE DETECTORS WORK IN REALITY?", "authors": ["Simiao Ren", "Hengwei Xu", "Tsang (Dennis) Ng", "Kidus Zewde", "Shengkai Jiang", "Ramini Desai", "Disha Patil", "Ning-Yau Cheng", "Yining Zhou", "Ragavi Muthukrishnan"], "abstract": "Deepfakes, particularly those involving faceswap-based manipulations, have sparked significant societal concern due to their increasing realism and potential for misuse. Despite rapid advancements in generative models, detection methods have not kept pace, creating a critical gap in defense strategies. This disparity is further amplified by the disconnect between academic research and real-world applications, which often prioritize different objectives and evaluation criteria. In this study, we take a pivotal step toward bridging this gap by presenting a novel observation: the post-processing step of super-resolution, commonly employed in real-world scenarios, substantially undermines the effectiveness of existing deepfake detection methods. To substantiate this claim, we introduce and publish the first real-world faceswap dataset, collected from popular online faceswap platforms. We then qualitatively evaluate the performance of state-of-the-art deepfake detectors on real-world deepfakes, revealing that their accuracy approaches the level of random guessing. Furthermore, we quantitatively demonstrate the significant performance degradation caused by common post-processing techniques. By addressing this overlooked challenge, our study underscores a critical avenue for enhancing the robustness and practical applicability of deepfake detection methods in real-world settings.", "sections": [{"title": "1 INTRODUCTION", "content": "The rise of artificial intelligence has benefited various fields from material design to energy[17, 15, 16, 12]. However, among those achievements, face-swap technology has emerged as a double-edged sword, showcasing remarkable advancements in artificial intelligence while simultaneously posing significant ethical and societal challenges [23]. By seamlessly superimposing one individual's face onto another's body in videos, this technology blurs the line between reality and fabrication, undermining the trust that forms the foundation of modern society. The malicious use of deepfake face-swapping to create deceptive media-ranging from non-consensual explicit content [5] to fraudulent political campaigns [14]\u2014has eroded public confidence in the authenticity of digital content. As these forgeries become increasingly indistinguishable from genuine media, they foster skepticism and paranoia, threatening interpersonal relationships, organizational credibility, and democratic processes.\nDespite the recognized dangers of deepfake face-swap technology, current detection mechanisms face significant limitations, particularly in real-world applications. While many detection algorithms achieve high accuracy in controlled laboratory conditions [24], their performance often degrades when applied to real-world data. Beautification filters and post-processing techniques, commonly applied to media in practical scenarios, exacerbate this challenge by obscuring the subtle artifacts that detection systems rely on. This gap between theoretical robustness and practical reliability"}, {"title": "2 Related work", "content": "The rapid development of public deepfake datasets, such as FaceForensics++ (FF++) [18], CelebDF [9], DFD [1], and DFDC [4], has provided diverse test grounds for evaluating deepfake detection methods. Recent benchmark studies [24, 6] have pointed out naive detectors directly train models on labeled datasets using architectures like Xception [3] and EfficientNet [21, 19] have comparable performances to spatial detectors.\nModel-based image upscaling harms spatial detectors that rely on blending artifacts [7, 13, 19, 8]. Unlike previous studies, we focus on the post-processing step where super-resolution models, such as CodeFormer [25], enhance deepfake outputs. While some detection methods [11, 20] target upscaling in encoder-decoder structures, our work highlights the challenge posed by post-processing upscaling, which introduces distribution shifts that significantly degrade detection performance.\nPrevious studies [10] have examined the adverse effects of post-processing on deepfake detection, particularly focusing on beautification filters commonly used on social media. However, our work diverges by addressing a more pervasive real-world phenomenon: model-based upscaling."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Real-world faceswap (RWFS) dataset creation", "content": "During our initial investigations, we observed that the quality of real-world deepfakes found on social media and in the news significantly surpasses that of those in existing academic benchmark datasets. This observation motivated us to create a new deepfake detection dataset that better reflects real-world settings. To achieve this, we introduced two key distinctions from traditional academic datasets in our creation process:\n\u2022 Utilizing the top online deepfake generators for real world application: While there is an abundance of face-swap repositories on GitHub and methodologies in academic literature, we deliberately avoided these sources for dataset creation. These methods are often more \"academic\" in nature and do not represent the tools used by the general public. In real-world scenarios, a bad actor seeking to generate deepfake imagery is more likely to rely on freely available online resources. To replicate this behavior, we identified and used the top eight websites returned by searching \"faceswap, free\" that allow users to generate faceswap images at no cost.\n\u2022 Matching race-gender-age pairs of our source and target faces: A common limitation of academic benchmark datasets is the low fidelity resulting from significant mismatches between the source and target facial features. In real-world scenarios, a bad actor is unlikely to swap between two drastically different faces. To address this, we implemented a \"Race-Gender-Age\" matching mechanism. Using race, age, and gender prediction models, we ensured that face swaps were performed only between individuals with matching"}, {"title": "3.2 Reverse engineer post processing step through self-swap", "content": "After collecting our real-world deepfake dataset, we observed intriguing artifacts that were absent in the input imagery. To further investigate how the processing in real-world datasets differs from that in academic datasets, we devised a \"self-swap\" test. This test involved reverse-engineering the post-processing pipeline of online deepfake generators by swapping a face with itself. Under the assumption that swapping a face with itself should produce identical imagery, we discovered that this assumption was violated across all tested online deepfake sites. This finding indicates the presence of additional post-processing transformations.\nThrough careful observation and unsuccessful attempts to replicate these transformations using facial smoothing and beautification filters, we identified that the applied transformations are model-based super-resolution techniques. These techniques introduce subtle yet significant changes to the imagery, distinguishing real-world deepfakes from their academic counterparts."}, {"title": "3.3 Quantitatively evaluation of super-resolution impact", "content": "To test the hypothesis that post-processing steps involving super-resolution contribute to the degradation of deepfake detection accuracy, we conducted a quantitative evaluation comparing the performance of state-of-the-art deepfake detectors on standard benchmark imagery versus super-resolved imagery. For this analysis, we selected multiple state-of-the-art deepfake detectors and super-resolution methods.\nWe chose GFPGAN [22] and CodeFormer [26] for our experiments due to their popularity and demonstrated robustness. GFPGAN has garnered over 10 million downloads on GitHub, reflecting its widespread adoption, while CodeFormer is the latest state-of-the-art method, achieving superior visual quality in several benchmarks. Both methods provide a strong basis for evaluating the impact of superfake detection performance."}, {"title": "4 Results", "content": ""}, {"title": "4.1 RWFS dataset Created", "content": "We created 848 real-world faceswap images using eight online faceswap websites, with the number of images from each site shown in Table 1. For the real images in the RWFS dataset, we randomly selected 900 from the Celeb dataset (the same source as the fake images).\nWe also evaluated popular deepfake detection algorithms on this dataset. While these detectors perform well on benchmark datasets like FF++ and CelebDF, they fail significantly on our RWFS dataset. We tested two pretrained networks that have shown strong performance in prior studies to demonstrate this.\nTo ensure a fair comparison, we reproduced their scores in our test environment and reported both the reproduced performance and the originally published performance. Figures 3 and 2 illustrate the stark contrast: academically successful networks fail to generalize to real-world image distributions. Specifically, pretrained models from FF++ achieve AUROC scores exceeding 0.92 on balanced academic test sets but perform as poorly as 0.53 (close to random guessing) on our real-world dataset. This discrepancy underscores the significant gap between what academia is optimizing for and the requirements of real-world applications."}, {"title": "4.2 Self-swap results", "content": "We first compared the self-swapped images with the original face images by plotting their differences, as shown in Figure 4. Upon closer inspection, we observed distinct changes in the self-swapped images. For instance, in the original image, the double eyelids were barely visible, and the eyelashes were clearly defined. However, in the self-swapped images, the double eyelids became significantly more pronounced, and the reflections in the eyes changed noticeably."}, {"title": "4.3 Super-resolution", "content": "With super-resolution identified as a major factor contributing to the degradation of deepfake detection performance, we applied super-resolution algorithms directly to the FF++ dataset and evaluated the resulting performance degradation. The results are presented in Figure 5 and Figure 6. From these figures, it is evident that the performance of deepfake detectors is significantly reduced by the application of super-resolution algorithms. Specifically, the AUROC scores dropped from over 0.9 to approximately 0.7 for both the self-blended images model and the Efficient-B4 naive model.\nMoreover, the extent of performance degradation varies depending on the specific super-resolution algorithm used. This discrepancy can be attributed to the fact that deepfake detector models are typically trained to identify specific artifacts, such as blending inconsistencies at face edges. When bad actors introduce an additional processing step, such as using another neural network-based super-resolution algorithm like GFPGAN or CodeFormer, the original model artifacts are significantly diminished or replaced with new traces introduced by the super-resolution model. This shift in artifact patterns confuses the deepfake detectors, leading to reduced performance.\nIt is important to note, however, that the performance degradation caused by super-resolution does not yet fully account for the challenges posed by our RWFS dataset. Upon closer examination of the super-resolution outputs, we observed"}, {"title": "5 Conclusions", "content": "Deepfake face-swap technology poses significant social risks by undermining trust in digital media. While detection algorithms perform well in controlled settings, their effectiveness diminishes in real-world scenarios due to post-processing techniques like super-resolution.\nIn this work, we introduced the Real-World Faceswap Dataset, the first to reflect public-facing deepfakes, and benchmarked state-of-the-art detectors on it. Our results highlight vulnerabilities in current detection methods, particularly the impact of super-resolution and post-processing on performance.\nBy providing this dataset and insights, we aim to bridge the gap between academic research and practical applications, advancing deepfake detection to mitigate their societal impact."}]}