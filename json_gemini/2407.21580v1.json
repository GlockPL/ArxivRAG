{"title": "Voxel Scene Graph for Intracranial Hemorrhage", "authors": ["Antoine P. Sanner", "Nils F. Grauhan", "Marc A. Brockmann", "Ahmed E. Othman", "Anirban Mukhopadhyay"], "abstract": "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs to support the clinical decision-making. The majority of early work focuses on the detection and segmentation of ICH, but do not model the complex relations between ICH and adjacent brain structures. In this work, we design a tailored object detection method for ICH, which we unite with segmentation-grounded Scene Graph Generation (SGG) methods to learn a holistic representation of the clinical cerebral scene. To the best of our knowledge, this is the first application of SGG for 3D voxel images. We evaluate our method on two head-CT datasets and demonstrate that our model can recall up to 74% of clinically relevant relations. This work lays the foundation towards SGG for 3D voxel data. The generated Scene Graphs can already provide insights for the clinician, but are also valuable for all downstream tasks as a compact and interpretable representation.", "sections": [{"title": "1 Introduction", "content": "Intracranial Hemorrhage (ICH) is a potentially life-threatening emergency in which rapid recognition and decision on further treatment is crucial for patient survival [8,9]. The 1-year mortality rate is up to 40% in various studies [2,17] and survivors retain significant persistent functional limitations in two thirds of cases [14]. Patient-centered individualized treatment decisions remain clinically challenging, especially considering that patient outcome deteriorates rapidly after ICH onset [1]. Given that CT imaging remains the main tool for diagnosis and planning, Deep Learning-based solutions can leverage this data to improve ICH patients' care, e.g. reducing the time to diagnosis.\nThe majority of early Deep Learning work focuses on the detection and segmentation of ICH [5,12], but completely ignore the related clinical complications. Indeed, hemorrhages can interact with other brain structures. Through hemorrhage expansion or involvement of the ventricular system, ICH can cause severe complications [12], which are therefore additional predictors for patient outcome"}, {"title": "2 Methodology", "content": "In this section, we give insights into how we structured Scene Graphs for ICH. We then introduce our two-stage method for SGG as visualized in Fig. 2.\nScene Graph for ICH: The first and most critical step is to define which structures are relevant, as well as a set of relations that can model possible sources of complications. We select the following interactions for our application based on possible clinical complications: 1) midline shifts, 2) blood flow to the ventricle system, and 3) swelling induced asymmetry of the ventricle system. As such, we have three classes of objects (\"Bleeding\", \"Ventricle System\" and \"Midline\"), and three bleeding-induced classes of relations. Additionally, we expect all patients to have exactly one midline. We model the ventricle system as a single object even when it is only sparsely visible, since the presence of blood can have severe complications independently of the location within the system.\nObject Detection: Localizing individual bleeding requires a multiscale approach, as they can have vastly different size, shape, and position. The 3D Retina-UNet coupled with a Feature Pyramid Network has already proved its usefulness for 3D medical imaging [10]. In particular, this architecture is very flexible and can fully leverage multiscale features. This is crucial as the volume of the bleeding can range from only 0.1 cm\u00b3 to more than 100 cm\u00b3. In opposition, the midline and the ventricle system only appear at a single scale, but pose their own set of challenges. Indeed, the 3D aspect ratio of the midline will heavily"}, {"title": null, "content": "depend on the head's orientation, especially in the axial plane. This both can cause issues with anchor matching and bounding box regression. In contrast, depending on the swelling or the presence of blood, the ventricle system may appear as multiple fragmented objects. These would need to be detected individually, as there would be a large overlap between the bleeding with the system and the system itself, rendering an accurate object matching impossible. For these reasons, we decide to leverage Retina-UNet's segmentation capabilities by detecting both anatomies from the predicted semantic segmentation. This simple but elegant solution, can solve the previously mentioned challenged, while also ensuring that only one object is predicted per class per image.\nRelation Prediction: Once objects are localized in the image, SGG methods focus on building a global context from individual object features (as defined by [18]). Inspired by Neural Motifs [21], and Iterative Message Passing [19], we design two variants for relation prediction for 3D voxel data (V-MOTIF and V-IMP).\nV-MOTIF uses bidirectional Long Short-term Memory Networks and iterates over detected objects. We choose to order these objects from top-to-bottom, as related objects in the cerebral scene are very likely to be at the same depth. Sorting by size, may be another viable solution, but may create a bias towards certain relation classes. In comparison, V-IMP builds a primal object graph, and its dual edge graph, then combines Gated Recurrent Units (GRU) with message passing iteratively to allow for information flow. Since each GRU receives multiple messages at each iteration, learnable weights are used to pool messages while only keeping the relevant information. Both models finally predict relations from the enriched features. Usually, these are also used to refine the classification of each object. Given the object matching challenges discussed previously, we decide to only use SGG methods for relation prediction.\nFinally, segmentation-grounding [11] involves computing the object features with the object segmentation, rather than with its bounding box, to leverage the finer localization. Since only a semantic segmentation in computed in the first step, we use it as a proxy by cropping it to the object's bounds and binarizing it based on the object's predicted class."}, {"title": "3 Experiments", "content": "In this section, we introduce the base images used and their annotation process. We then describe our evaluation setup for the different stages of our method.\nSource Images: For this study, we source two datasets. The first one is the publicly available INSTANCE2022 challenge dataset [13]. It contains 130 non-contrast head CTs of patients diagnosed with ICH. 10 images with significant streak artifacts were discarded, as they were deemed out-of-distribution. Additionally, we select a private cohort for the purpose of external validation. It is constituted of 18 non-contrast head CTs of patients diagnosed with ICH. We choose more severe cases (caused by trauma) compared to the first dataset."}, {"title": null, "content": "These cases are clinically more challenging, e.g. some patients were already operated on, and we want to test the robustness of our method against distribution shift (see Fig. 3).\nData Annotation: The patient's head position is first manually harmonized in all images. A senior neuroradiologist then segments a label map for each image using 3D Slicer [6], which takes 5 to 30 min per image. 3D bounding box information is automatically extracted from the label maps by computing the bounds of each segmented object. Currently, one of the main obstacles of Scene Graph application using voxel data is the lack of tools for relation annotation. In particular, the annotator needs to be able to easily scroll through the 3D volume to select individual objects as being part of a relation. We developed an internal tool specifically to this end. We plan to make it open-source in the near future.\nOn average, annotating relations takes under 2 min per image.\nModel Training: For bounding box detection, we split the official training images from the INSTANCE2022 dataset [13] for training and validation in an 80/20 fashion. The official validation cases are used for in-distribution testing. The private cohort is fully used for external validation. However, not all images contain relations. As such, we only keep cases with relations for training the relation detector, the splits remaining otherwise unchanged. Configuration files with hyperparameters and data splits will be made available with the code.\nEvaluation Metrics: For bleeding detection, we use the well established metrics for object detection, i.e. Average Recall (AR) and mean Average Precision (mAP). We choose a localization threshold of 30% Intersection Over Union (IoU), as it reflects a balance between the clinical need for coarse localization and producing sensible bounding boxes for larger bleeding [3]. Additionally, since no relation can get predicted when at least one object has not been detected, we also give an upper bound of how many relations can be recalled given the object detected in the first stage. For relation prediction, we compute the metrics based on the top-K predictions per image, with again a 30% IoU threshold for object"}, {"title": null, "content": "localization. We use the Recall@K (R@K), mean Recall@K (mR@K), and mean Average Precision@K (mAP@K). Given that an image has on average overall 2 relations (up to 7), we compute these metrics for K = 8. Our method is evaluated for both Predicate Classification and Scene Graph Generation tasks [18], i.e. respectively predicting relations from ground truth and predicted object localization. The results are averaged over 5 random seeds."}, {"title": "4 Results", "content": "In this section, we evaluate our two-stage pipeline, first for object detection and then for relation prediction (Predicate Classification and Scene Graph Generation). Results for ablations of individual method components can be found in the Supplementary Material."}, {"title": "4.1 Object Detection", "content": "Ventricle System & Midline Detection: We first evaluate the performance for detecting the ventricle system and the midline, as these structures are localized using the predicted semantic segmentation. As shown in Fig. 4, our solution can detect them robustly and precisely, with a detection rate of 96.4% for both structures in the INSTANCE2022 dataset. For the private cohort, it still has a detection rate of 100% and 83.3% respectively for the ventricle system and the midline."}, {"title": "Bleeding Detection", "content": "We then evaluate the detection performance for bleeding separately from the previously mentioned structures. In Table 1, we compare our method to state-of-the-art nnDetection [3]. Our method significantly outperforms it on both datasets, showing that careful method parametrization can outperform AutoML for this task. The private cohort contains proportionally many smaller bleedings, which are not present in the training data and predominantly have no associated relation. In this regard, our method still robustly detects relevant bleedings in the private cohort."}, {"title": "4.2 Scene Graph Prediction", "content": "Predicate Classification: We evaluate our Voxel Neural Motifs (V-MOTIF) and Voxel Iterative Message Passing (V-IMP) models for relation prediction, with and without segmentation grounding and report the results in Table 2. Given that datasets for natural scene-understanding [4] or even surgical data science [16] comprise thousands of scenes, one could assume that a large data quantity is required for training SGG methods. These results show that under 200 scenes are enough to generalize across medical centers for the ICH cerebral scene."}, {"title": "Scene Graph Generation", "content": "This experiment reflects the true performance of the entire prediction pipeline (Table 3). Qualitative results are shown in Fig. 5. Overall, the models recall relevant relations, with clinically interpretable information for decision support in treatment planning. However, this experiment shows that our complete method already offers a satisfying performance for in-distribution data."}, {"title": "5 Conclusion", "content": "The clinical scene for Intracranial Hemorrhage (ICH) goes beyond detecting bleedings, as these can interact with adjacent brain structures, and potentially"}, {"title": "6 Compliance with Ethical Standards", "content": "This study was performed in line with the principles of the Declaration of Helsinki. The retrospective evaluation of imaging data from the University Medical Center Mainz was approved by the local ethics boards (Project 2021-15948-retrospektiv). Ethical approval was not required, as confirmed by the license attached with the open access data."}]}