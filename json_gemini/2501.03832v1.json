{"title": "Three-dimensional attention Transformer for state evaluation in real-time strategy games", "authors": ["Yanqing Ye", "Weilong Yang", "Kai Qiu", "Jie Zhang"], "abstract": "Abstract-Situation assessment in Real-Time Strategy (RTS) games is crucial for understanding decision-making in complex adversarial environments. However, existing methods remain limited in processing multi-dimensional feature information and temporal dependencies. Here we propose a tri-dimensional Space-Time-Feature Transformer (TSTF Transformer) architecture, which efficiently models battlefield situations through three independent but cascaded modules: spatial attention, temporal attention, and feature attention. On a dataset comprising 3,150 adversarial experiments, the 8-layer TSTF Transformer demonstrates superior performance: achieving 58.7% accuracy in the early game (~4% progress), significantly outperforming the conventional Timesformer's 41.8%; reaching 97.6% accuracy in the mid-game (~40% progress) while maintaining low performance variation (standard deviation 0.114). Meanwhile, this architecture requires fewer parameters (4.75M) compared to the baseline model (5.54M). Our study not only provides new insights into situation assessment in RTS games but also presents an innovative paradigm for Transformer-based multi-dimensional temporal modeling.\n\nIndex Terms-real-time strategy games, situation assessment, tri- dimensional attention mechanism, temporal-spatial attention, feature attention", "sections": [{"title": "I. INTRODUCTION", "content": "REAL time strategy (RTS) games represent one of the most challenging domains for artificial intelligence research, characterized by enormous state and action spaces that dwarf even those of traditional board games like Go [1]. In these complex war simulation environments, players must simultaneously manage economies, control dozens of units, and make strategic decisions under severe time constraints [2]. Despite significant advances in game AI, particularly demonstrated by AlphaGo's triumph over human champions [1], developing strong AI systems for RTS games remains an elusive goal, with top human players still consistently outperforming the best artificial agents [3].\nA fundamental challenge in RTS game Al lies in the development of accurate state evaluation functions - methods that can assess the relative advantages of players in any given game state. Traditional approaches have relied primarily on material-based evaluations that consider unit counts, resources, and basic combat metrics [3, 4]. While such methods capture raw military and economic strength, they often fail to account for crucial spatial relationships between units and terrain features that experienced human players naturally recognize and exploit [5]. More sophisticated evaluation techniques based on combat simulation [6] and Lanchester attrition laws [7-10] have shown promise but remain limited in scope, focusing primarily on military aspects while neglecting other strategic elements.\nRecent breakthroughs in deep learning, particularly in computer vision [11] and game playing [1], suggest a promising new direction. Convolutional neural networks (CNNs) have demonstrated remarkable ability to learn complex spatial patterns and hierarchical features directly from raw input data [12]. Stanescu et al. [5] investigate the potential of deep CNNs to revolutionize RTS game state evaluation by learning to recognize complex tactical and strategic patterns. Unlike previous approaches that rely on hand-crafted features or focus solely on combat outcomes [13, 14], CNNs can potentially learn to identify and evaluate subtle positional advantages, strategic resource control, and tactical opportunities directly from raw game state representations [5]. Initial work has shown that CNN-based evaluation functions, despite being computationally more intensive, can significantly outperform traditional methods when incorporated into state-of-the-art search algorithms [5, 15, 16]. Their success in Go, where they enabled both accurate position evaluation and intelligent action selection, raises the intriguing possibility of applying similar techniques to the more complex domain of RTS games [5].\nRecent years have witnessed the remarkable success of Transformer architecture through its self-attention mechanism in natural language processing and computer vision [17-19]. The key advantages of Transformer include: (1) fewer inductive biases allowing richer feature representation learning from large-scale data [20], (2) direct modeling of long-range dependencies through self-attention rather than stacking multiple convolutional layers to expand receptive fields [21], and (3) a unified token-based processing paradigm for multi-modal inputs [22]. These characteristics make Transformer particularly suitable for handling the complex state spaces and long-term dependencies in RTS games.\nIn video understanding, TimeSformer [23] has achieved remarkable success by extending Transformer's self-attention mechanism to the spatiotemporal domain. Its key innovation lies in proposing \"divided attention\", which computes temporal and spatial attention separately to maintain modeling effectiveness while improving computational efficiency [24]. With this design, TimeSformer has achieved state-of-the-art performance on multiple video understanding benchmarks while requiring lower training and inference costs [25, 26]. For RTS game state evaluation tasks that require simultaneous understanding of spatial relationships and temporal evolution, TimeSformer's architecture offers inherent advantages over traditional convolutional approaches [5, 27].\nHowever, the conventional TimeSformer architecture is primarily optimized for video classification tasks, with temporal and spatial attention mechanisms mainly processing RGB pixel features [23]. In contrast, each grid position in RTS games contains richer feature information, including unit types, health points, ownership, and resources (neutral or occupied). These features exhibit complex interdependencies, and directly applying the TimeSformer architecture may result in loss of important feature relationship information. Therefore, this study proposes incorporating a Feature Attention (FA) module after spatiotemporal attention to model interactions between different feature dimensions. After modeling temporal and spatial dependencies, this module further processes the associations between feature planes to better capture the intrinsic structure of RTS game states [28]. This multi-dimensional attention mechanism design shows promise in enhancing the model's understanding and evaluation of game situations.\nHere, we present a novel channel attention enhanced TimeSformer architecture for RTS game state evaluation that combines the strengths of spatiotemporal attention with explicit feature relationship modeling, demonstrating superior performance in capturing the complex dynamics of RTS games while maintaining computational efficiency."}, {"title": "II. MATERIALS AND METHODS", "content": "We developed a comprehensive AI algorithm system for RTS games comprising 10 distinct strategies. These algorithms span from basic random strategies to sophisticated economic-military integrated strategies, enabling simulation of diverse gameplay pat-terns. Based on their characteristics, these algorithms are categorized into three main types: baseline strategies, tactics-oriented strategies, and integrated strategies.\nRandomBiasedAI serves as the baseline testing strategy, implementing a biased random decision mechanism. During each decision cycle, this strategy randomly selects potential actions based on predetermined probability distributions, while adjusting se-lection probabilities through bias terms. This design maintains strategic randomness while avoiding potentially irrational behaviors associated with purely random selection.\nTactics-oriented strategies encompass four distinct rush types: WorkerRush, RangedRush, LightRush, and HeavyRush. Specifically, WorkerRush focuses on rapid worker unit training, launching ear-ly-game attacks through numerical advantage; RangedRush prioritizes ranged combat units, achieving safe offensive positions through range advantages; LightRush empha-sizes light unit mobility, gaining battlefield advantages through speed and flexibility; HeavyRush centers on heavy units, which, despite slower development, provides signif-icant late-game advantages by maintaining continuous heavy unit production alongside economic development, though vulnerable to early harassment strategies like Worker-Rush.\nIntegrated strategies incorporate more complex decision-making mechanisms. EconomyRush prioritizes economic development, maximizing economic benefits while maintaining basic defense through precise resource collection and allocation algorithms; EconomyMilitaryRush combines economic development with military expansion, seek-ing balance between the two; CRush_V1 and CRush_V2 represent two distinct versions of early-game offensive strategies; PuppetNoPlan employs a reactive decision-making mechanism, responding to immediate battlefield situations rather than following prede-termined plans.\nWe employed a round-robin tournament system for adversarial testing. Specifically, 10 AI algorithms were paired for head-to-head competitions, with each pair engaging in 70 rounds of matches. To eliminate systematic bias potentially arising from turn order, opponents switched positions after 35 rounds, resulting in a total of $C_{10}^{2} \\times 70 = 3150$ matches. Experiments were conducted on a 16\u00d716 map. To prevent infinite game loops, we implemented a 1000-round maximum game limit, after which the winner was determined by the number of surviving units."}, {"title": "B. Neural Network Architecture", "content": "Given an RTS game situation sequence X as model input, belonging to a five-dimensional real tensor space $X \\in R^{{B \\times T \\times C \\times H \\times W}}$, where B represents batch size, T = 500 denotes discrete time steps, C = 5 corresponds to the number of multi-channel feature planes, and H = W = 16 represent spatial dimensions of the game map. Each feature plane independently encodes key game state information: entity type matrix $M_{type} \\in R^{{H \\times W}}$ with values in [1,7] representing 7 basic unit types, health value matrix $M_{health} \\in R^{{H \\times W}}$ with values in [0,10] indicating unit survival status, faction matrix $M_{faction} \\in \\{1,2\\}^{{H \\times W}}$ denoting unit affiliation, neutral resource matrix $M_{neutral} \\in [0,25]^{{H \\times W}}$ and faction resource matrix $M_{resource} \\in [0,25]^{{H \\times W}}$ representing resource distribution states. This multi-channel feature design constructs a complete representation space for game situations.\nTo enhance computational efficiency and capture local spatial patterns, we partition the input situation map into non-overlapping 4\u00d74 blocks. Each block is mapped to a D-dimensional embedding space (D = 155) through a learnable linear transformation matrix E, while incorporating positional encoding to preserve spatial and temporal information. This process is represented as:\n$Z_{p,t} = E X_{p,t} + e_{pos}$     (1)\nwhere $z_{p,t} \\in R^{D}$ is the initial feature representation for position p and time step t, $EX_{p,t}$ is the feature embedding obtained through linear transformation of input block $X_{p,t} \\in R^{{C \\times 16}}$ using learnable matrix $E \\in R^{{D \\times (C \\times 16)}}$, and $e_{pos} \\in R^{D}$ is a learnable positional embedding vector encoding spatiotemporal information in the input sequence. Here, p \u2208 {1,2, ..., N} denotes spatial position index (N = $\\frac{{H \\times W}}{{16}}$ = 16), t \u2208 {1,2, ..., T} represents time step index. Notably, a special token vector $z_{0,t}$ \u2208 RD is inserted at the sequence start, which aggregates sequence information for situation assessment in the network's final layer. Through this preprocessing, the original input is transformed into a feature sequence with shape [B, (T \u00d7 N + 1), D]."}, {"title": "B. Neural Network Architecture", "content": "We propose a tri-dimensional attention Transformer network comprising L = 12 sequentially stacked encoder layers. In each encoder layer $l \\in \\{1,2, ..., L\\}$, attention computation is decomposed into three independent but sequentially cascaded submodules: spatial attention (SA), temporal attention (TA), and feature attention (FA). Each encoder layer receives output features $z^{{l-1}} \\in R^{{B \\times (T \\times N+1) \\times D}}$ from the previous layer as input, where the first encoder layer (l = 1) takes patch embedding sequence $z^{0}$ as input.\nThe SA module initially models spatial dependencies within each time step. For input features $z^{{l}}$ of layer 1, spatial attention first computes query, key, and value matrices in the spatial dimension:\n$Q_s = W_Q z^{{l}} + b_Q$   (2)\n$K_s = W_K z^{{l}} + b_K$   (3)\n$V = W_V z^{{l}} + b_V$   (4)\nwhere $W_Q$, $W_K$, $W_V$ \u2208 $R^{{D \\times D}}$ are learnable weight matrices and $b_Q$, $b_K$, $b_V$ \u2208 $R^{D}$ are corresponding bias vectors. Spatial attention weights are then computed through scaled dot-product attention:\n$SA(z) = softmax(\\frac{{Q_s K_s^T}}{{\\sqrt{{d_h}}}}) V_s$.    (5)\nHere, $d_h$ = $\\frac{D}{{h}}$ = 31 represents the dimension per attention head, with h = 5 attention heads. During computation, input is reshaped to [(B \u00d7 T) \u00d7 N \u00d7 D], enabling direct attention calculation across N spatial positions within each time step. This multi-head design allows parallel capture of spatial dependencies in different subspaces.\nBuilding on spatial relationship modeling, the TA module captures temporal patterns in battle evolution. This module computes query, key, and value matrices in the temporal dimension based on spatial attention output:\n$Q_t = W_Q SA(z^{{l}}) + b_Q$   (6)\n$K_t = W_K SA(z^{{l}}) + b_K$   (7)\n$V_t = W_V SA(z^{{l}}) + b_V$   (8)\nwhere $W_Q$, $W_K$, $W_V$ \u2208 $R^{{D \\times D}}$ and $b_Q$, $b_K$, $b_V$ \u2208 $R^{D}$ are temporal attention weights and biases. Temporal attention follows a similar mechanism:\n$TA(SA(z^{{l}})) = softmax(\\frac{{Q_t K_t^T}}{{\\sqrt{{d_h}}}}) V_t$.    (9)\nCrucially, temporal attention reshapes feature tensors to [(B \u00d7 N) \u00d7 T \u00d7 D], allowing each spatial position to learn long-term dependencies across T time steps.\nWe introduce a feature attention (FA) module to effectively process the rich grid feature information in RTS tasks, which significantly differs from the RGB values in video pixel features. The FA module processes features after spatiotemporal dependency modeling:\n$Q_f = W_Q TA(SA(z^{{l}})) + b_Q$   (10)\n$K_f = W_K TA(SA(z^{{l}})) + b_K$   (11)\n$V_f = W_V TA(SA(z^{{l}})) + b_V$   (12)\nwhere $W_Q$, $W_K$, $W_V$ \u2208 $R^{{d' \\times d'}}$ and $b_Q$, $b_K$, $b_V$ \u2208 $R^{{d'}}$ are feature attention parameters, with d' = $\\frac{D}{{C}}$ = 31 representing per-feature channel dimension. Feature attention computation is expressed as:\n$FA (TA(SA(z^{{l}}))) = softmax(\\frac{{Q_f K_f^T}}{{\\sqrt{{d_h}}}}) V_f$.    (13)\nAt this stage, feature representations are reshaped to [(B \u00d7 T \u00d7 N) \u00d7 C \u00d7 d'], enabling the model to learn associations between different features such as entity types, health values, and resources.\nFor deep feature extraction, we combine the three attention modules sequentially to form basic Transformer blocks, constructing a deep network architecture with L=12 layers. The computation process for each Transformer block can be represented as:\n$z^{{l+1}} = LN (FA(TA(SA(z^{{l}}) + z^{{l}})))$   (14)\nwhere LN denotes Layer Normalization, used to normalize feature distributions and stabilize training. The addition symbol represents residual connections, mitigating gradient vanishing in deep networks. Specifically, the output $z^{{l+1}} \\in R^{{B \\times (T \\times N+1) \\times D}}$ of layer l inherits all feature information from layer 1-1 while incorporating new feature representations. This cascaded multi-layer design enables the model to construct hierarchical representations from tactical to strategic features: shallow layers focus on local tactical features (such as unit combinations and resource control), while deeper layers integrate global information for strategic understanding.\nAfter L layers of feature extraction, the model performs final situation assessment through nonlinear transformation of the special token vector:\n$y = \\sigma(W_2 \\phi (W_1 z_{{cls}} + b_1) + b_2)$   (15)\nwhere $z_{{cls}} \\in R^{D}$ represents the feature representation of the final layer token vector, $W_1 \\in R^{{D \\times 4D}}$ and $W_2 \\in R^{{4D}}$ are weight matrices of two fully connected layers, $b_1 \\in R^{{4D}}$ and $b_2 \\in R$ are corresponding bias vectors, $\\phi$ denotes the GELU activation function, and $\\sigma$ is the sigmoid activation function. The output y \u2208 [0,1] represents the predicted probability of victory for one side. This design enables the model to compress complex multi-dimensional feature information into a single situation assessment score.\n$Loss = - \\frac{1}{{M}} \\sum_{{i=1}}^{{M}} (y_i log(p_i) + (1 - y_i) log(1 - p_i))$   (16)\nThe model employs binary cross-entropy as the loss function to measure the discrepancy between predictions and ground truth labels:\nwhere M denotes batch size, $y_i \\in \\{0,1\\}$ is the true winning faction label for the i-th sample, and $p_i$ is the corresponding model prediction probability. For optimization, we employ the AdamW optimizer with weight decay coefficient set to 0.01, initial learning rate of 1e-4, momentum decay coefficients $$\\beta_1$$ = 0.9 and $$\\beta_2$$ = 0.999, and numerical stability constant $$\\alpha$$ = 1e-8. Training parameters for different depths of tri-dimensional attention Transformer (TSTF Transformer) and traditional Timesformer are shown in TABLE II."}, {"title": "III. RESULTS", "content": "To investigate the impact of network depth on situation assessment performance, we systematically analyzed the performance of Transformer architectures with different depths in battlefield situation assessment tasks. Specifically, we designed tri-dimensional attention Transformers (TSTF Transformer) with 6 and 8 layers and compared them with a 12-layer TimesFormer, aiming to validate the relationship between model depth and assessment performance while exploring whether comparable or superior performance could be achieved with fewer layers through optimized architectural design.\nModel depth (number of Transformer layers) significantly influences performance, primarily manifesting in feature extraction capability, computational efficiency, and convergence speed. Therefore, we tested TSTF Transformers of different depths (depth=6, depth=8) and compared them with traditional assessment methods to comprehensively evaluate the impact of model depth on situation assessment performance.\nFirst, Figure 3 shows the comparison of situation assessment results between Timesformer (depth=12) and TSTF Transformer (depth=6). This comparison not only reflects performance differences between models of different depths but also demonstrates the advantages of our proposed tri-dimensional attention mechanism in maintaining high performance while reducing model complexity.\nThe Simple eval method employs linear combination of features to calculate player strength:\n$E_p = W_{res} R_p + W_{work} \\sum_{{u \\in W_p}} R_u + W_{unit} \\sum_{{u \\in C_p}} \\frac{{HP_u}}{{MaxHP_u}}$    (17)\nwhere $E_p$ represents the total strength score of player p, $R_p$ denotes accumulated resources, $W_p$ represents the worker unit set, $R_u$ indicates resources carried by each worker, $C_u$ represents unit cost coefficient, and $\\frac{{HP_u}}{{MaxHP_u}}$ reflects the current health ratio of each unit. Weights $W_{res}$, $W_{work}$, $W_{unit}$ are built-in microRTS parameters.\nThe Lanchester eval method incorporates Lanchester's combat laws, considering force concentration effects in military confrontations:\n$E_p = W_{res} R_p + W_{work} \\sum_{{u \\in W_p}} a_w R_u + W_{base} \\frac{{HP_{base}}}{{MaxHP_{base}}}$ + $W_{barracks} \\frac{{HP_{barracks}}}{{MaxHP_{barracks}}}$ + $C_u \\sum_{{u \\in C_p}} \\frac{{HP_u}}{{MaxHP_u}} + N_p^{0.7}$    (18)\nwhere $\\alpha_u$ represents unit-specific strength coefficients and $N_p$ denotes total combat units. The strength coefficients, weights, and exponent 0.7 are built-in microRTS parameters optimized for force concentration effects across various combat scenarios. Both methods process game states represented as 16\u00d716\u00d75 feature matrices, encoding entity types (7 categories), health values (0-10), faction affiliation (1-2), neutral resources (0-25), and accumulated resources (0-25).\nFor outcome prediction, we calculate E1-E2 for Simple eval or E1-E2 for Lanchester eval, where positive values predict player 1's victory. Comparison with traditional methods helps quantify the advantages of our proposed neural network architecture in capturing complex nonlinear relationships within RTS game states."}, {"title": "D. Model Evaluation Metrics", "content": "To objectively evaluate the performance of the tri-dimensional attention Transformer model, we employed four standard metrics enabling comprehensive assessment across all dimensions of predictive performance.\nAccuracy is defined as the ratio of correct predictions to total predictions:\n$Accuracy = \\frac{{Number\\ of\\ Correct\\ Predictions}}{{Total\\ Number\\ of\\ Predictions}}$    (19)\nThis provides a basic measure of overall model performance, ranging from 0 to 1, where 1 indicates perfect accuracy and 0 indicates complete inaccuracy.\nPrecision evaluates the accuracy of positive predictions and assesses false positive occurrences:\n$Precision = \\frac{{True\\ Positives}}{{True\\ Positives + False\\ Positives}}$   (20)\nThis metric is particularly valuable when false positives carry high costs. Like accuracy, precision ranges from 0 to 1, where 1 indicates perfect precision (no false positives) and 0 indicates all positive predictions are incorrect.\nRecall, also known as sensitivity, measures the model's ability to detect all actual positive cases:\n$Recall = \\frac{{True\\ Positives}}{{True\\ Positives + False\\ Negatives}}$    (21)\nValues range from 0 to 1, where 1 indicates all true positives are correctly identified and 0 indicates no true positives are detected.\nF1 Score represents the harmonic mean of precision and recall:\n$Recall = \\frac{{True\\ Positives}}{{True\\ Positives + False\\ Negatives}}$  (22)\nThis provides balance when both false positives and false negatives carry significant, equal costs. The score ranges from 0 to 1, with 1 indicating perfect precision and recall.\nOverall Performance (OP): To reflect comprehensive performance, we introduce the OP index:\nOP = Accuracy + Precision + Recall + F1 Score.  (23)\nHigher OP scores indicate better performance across all key aspects of binary classification. This composite metric provides a holistic view of model performance, reflecting balance among individual metrics. Notably, \"optimal\" OP refers to best overall performance rather than necessarily achieving the highest score in each individual metric."}, {"title": "IV. DISCUSSION", "content": "The experimental results demonstrate several significant findings regarding the effectiveness of our proposed TSTF Transformer architecture in RTS game situation assessment. Here we analyze these findings from multiple perspectives and discuss their broader implications.\nThe superior performance of TSTF Transformer-8 can be attributed to several key technical innovations:\n1. Integrated Feature Processing: The incorporation of feature attention mechanisms alongside spatiotemporal attention enables more comprehensive state representation. Unlike traditional Timesformer architectures that primarily process spatial and temporal dimensions, our model's dedicated feature attention module effectively captures the complex interactions between different game state features (unit types, health values, resources, etc.).\n2. Efficient Architecture Design: Despite having fewer layers than the baseline Timesformer (8 vs 12), TSTF Transformer-8 achieves superior performance while requiring fewer parameters (4.75M vs 5.54M). This suggests that our tri-dimensional attention mechanism more efficiently processes the structured information present in RTS game states.\n3. Superior Performance with Limited Input: The model's ability to achieve 58.7% accuracy with only 4% of game progression demonstrates remarkable effectiveness in early situation assessment. This represents a significant improvement over all other models including Timesformer's 41.8% accuracy at the same stage and highlights the architecture's capability to extract meaningful patterns from limited information.\n4. Stability and Consistency: The low performance variation (standard deviation 0.114) during mid-game stages indicates robust and reliable assessment capabilities, crucial for practical applications in strategic decision-making.\nThis study explores the application of transformer architectures to RTS game state evaluation through decomposing attention mechanisms into specialized components (spatial, temporal, and feature). The experimental results indicate that TSTF Transformer-8 achieves reliable early-game assessment capabilities while maintaining computational efficiency through reduced parameter count. This performance advantage is particularly valuable in RTS games' opening phases, where traditional evaluation methods often struggle to provide dependable assessments.\nThe model's efficient architecture and ability to process multi-dimensional feature information suggest potential practical applications. In RTS contexts, the reduced parameter count enables real-time processing under computational constraints. Beyond RTS games, similar principles might benefit other domains requiring concurrent analysis of spatial, temporal, and feature relationships, such as autonomous systems or strategic planning.\nHowever, several challenges remain. While more efficient than Timesformer, the tri-dimensional attention mechanism still incurs notable computational overhead. Future research could explore optimization techniques and pruning methods to further reduce computational requirements, as well as investigate the adaptation of these concepts across different applications."}, {"title": "V. CONCLUSION", "content": "Experimental results demonstrate a clear performance hierarchy: in terms of accuracy, TSTF Transformer-8 > Timesformer > TSTF Transformer-6; in terms of model complexity, Timesformer > TSTF Transformer-8 > TSTF Transformer-6. While the shallower TSTF Transformer (depth=6) outperforms Timesformer (depth=12) during low-information phases (approximately first 10% of game progress), its performance gradually deteriorates relative to Timesformer as information density increases with game progression. TSTF Transformer-8 achieves an optimal balance between accuracy and model complexity, surpassing traditional Timesformer in model efficiency while maintaining superior accuracy throughout the game progression. In contrast, both Timesformer and TSTF Transformer-6 perform comparably or slightly worse than traditional models (Simple eval. and Lanchester eval.) during early stages when input information is limited."}]}