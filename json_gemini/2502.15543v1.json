{"title": "PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning", "authors": ["Pengcheng Huang", "Zhenghao Liu", "Yukun Yan", "Xiaoyuan Yi", "Hao Chen", "Zhiyuan Liu", "Maosong Sun", "Tong Xiao", "Ge Yu", "Chenyan Xiong"], "abstract": "Knowledge-Augmented Generation (KAG) has shown great promise in updating the internal memory of Large Language Models (LLMs) by integrating external knowledge. However, KAG inevitably faces knowledge conflicts when the internal memory contradicts external information. Current approaches to mitigating these conflicts mainly focus on improving external knowledge utilization. However, these methods have shown only limited effectiveness in mitigating the knowledge conflict problem, as internal knowledge continues to influence the generation process of LLMs. In this paper, we propose a ParametrIc Pruning-based Knowledge-Augmented Generation (PIP-KAG) approach, which prunes internal knowledge of LLMS and incorporates a plug-and-play adaptation module to help LLMs better leverage external sources. Additionally, we construct the CoConflictQA benchmark based on the hallucination of LLMs to better evaluate contextual faithfulness during answering questions. Experimental results on CoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts and improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by 13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes are available at https://github.com/OpenBMB/PIP-KAG.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023) and LLaMA (Touvron et al., 2023), have demonstrated remarkable performance across numerous NLP tasks by leveraging their parametric knowledge (Wei et al., 2022b; Zhao et al., 2023). However, the parametric knowledge within LLMs is inherently static, making it susceptible to becoming outdated over time. This limitation often leads to hallucinations (Huang et al., 2023; Elazar et al., 2021) and inaccurate responses (Ji et al., 2023; Shuster et al., 2021), significantly hindering their applicability in real-world scenarios.\nTo address this issue, Knowledge-Augmented Generation (KAG) has been introduced, which integrates external knowledge from tools, such as retrievers and search engines, allowing LLMs to access up-to-date information during generation. Nevertheless, this approach gives rise to the knowledge conflict problem (Longpre et al., 2021; Xu et al., 2024), where external knowledge may contradict the internal memory of LLMs. These conflicts degrade the reliability and effectiveness of KAG systems (Yu et al., 2023b; Chen et al., 2024a).\nTo mitigate these conflicts, various strategies have been proposed, such as encouraging mod-"}, {"title": "2 Related work", "content": "Knowledge-Augmented Generation (KAG) has proven effective by enhancing LLMs with external knowledge sources, such as Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Ram et al., 2023; Shi et al., 2024b; Yao et al., 2023) and Tool-Augmented Generation (Schick et al., 2023; Qin et al., 2024). These approaches update the internal memory of LLMs by incorporating external knowledge as context, thereby facilitating a wide range of knowledge-intensive tasks (Zheng et al., 2023; Lin et al., 2024; Li et al., 2024). However, the integration of external knowledge can lead to knowledge conflicts when it contradicts the model's parametric knowledge, undermining the reliability of KAG systems (Chen et al., 2022; Longpre et al., 2021; Yu et al., 2023a; Xie et al., 2024; Bi et al., 2024c).\nNumerous studies focus on understanding and evaluating knowledge conflicts by constructing specialized benchmarks. Recent works (Longpre et al., 2021; Jin et al., 2024) synthesize conflicts by substituting entities in the context, demonstrating that LLMs tend to over-rely on their internal memory. In contrast, other researchers argue that LLMs may excessively rely on external evidence, even when it is counterfactual context synthesized by LLMs (Xie et al., 2024; Tan et al., 2024). Additionally, existing research (Kortukov et al., 2024; Xie et al., 2024) indicates that LLMs often prioritize contextual cues aligned with their existing memory when confronted with multiple conflicting sources.\nAnother line of research addresses knowledge conflicts by improving the external knowledge integration ability of LLMs. Some work focuses on prompt engineering, where context-aware instructions explicitly guide models to prioritize external knowledge over parametric memory (Zhou et al., 2023; Wang et al., 2023). Another approach explores training-based adaptations (Xue et al., 2023; Bi et al., 2024a), such as fine-tuning models on knowledge-augmented datasets to enhance contextual grounding answering ability of LLMs during the training phase (Li et al., 2023; Fang et al., 2024; Mo et al., 2024; Neeman et al., 2023). Additionally, contrastive decoding strategies amplify differences between context-aware and default outputs (Shi et al., 2024a; Bi et al., 2024b; Jin et al., 2024), explicitly reinforcing external knowledge reliance to improve the faithfulness. Different from these"}, {"title": "3 Methodology", "content": "As illustrated in Figure 2, this section introduces our ParametrIc Pruning-based Knowledge-Augmented Generation (PIP-KAG) model, which helps LLMs to utilize external knowledge c to answer the question q. Specifically, PIP-KAG prunes parameters to uninstall internal knowledge of LLMs (Sec. 3.1) and installs a plug-and-play KAG adaptation module to facilitate the knowledge usage (Sec. 3.2)."}, {"title": "3.1 Uninstalling Internal Knowledge via Parametric Pruning", "content": "To mitigate knowledge conflicts, PIP-KAG first uninstalls the internal knowledge of LLMs by pruning parameters using a neuron activation-based approach. Specifically, we first introduce the concept of neuron activation and then explain how internal knowledge is uninstalled by pruning knowledge-related parameters in transformer layers that become inhibited after knowledge augmentation."}, {"title": "3.1.1 Preliminary of Neuron Activation", "content": "This section provides a technical foundation for our parametric pruning method by examining neuron activation mechanisms in transformer-based LLMs. Modern LLMs typically comprise multiple stacked decoder blocks, each containing a multi-head attention (MHA) mechanism and a feed-forward network (FFN). The FFN sub-layer is widely viewed as a key-value memory mechanism, storing the majority of the parametric knowledge (Geva et al., 2021) through two parameter matrices K, V \u2208 \u211d^{d_m \\times d}, where d_m denotes the intermediate hidden dimension. For the i-th token X_i \u2208 X in the input sequence, the FFN sub-layer processes its representation x_i \u2208 \u211d^d from the last layer through linear transformations. Formally, the computation in the l-th FFN sub-layer can be expressed as a key-value memory:\nFFN(x) = (\u03c3(Kx))^TV',\nwhere \u03c3 is the activation function. Consequently, K^l and V^l can be expressed as sets of d_m parameter vectors: K^l = {k_1, ..., k_{d_m}} and V^l"}, {"title": "3.1.2 Parametric Pruning via Knowledge Augmented Neuron Inhibition", "content": "To identify layers that store a higher proportion of parametric memory, which may contribute to knowledge conflicts, we propose a layer selection algorithm based on neuron inhibition.\nSpecifically, for a query q and external context c, we calculate the neuron inhibition ratio AR by measuring the change in activated neuron ratios before and after knowledge augmentation with c:\n\u2206R^l = R^l(q) \u2013 R^l(c \u2295 q),\nwhere \u2295 denotes the concatenation operation. A larger \u2206R^l indicates greater inhibition of neurons after incorporating external knowledge c. We then define \\mathcal{H}_{pruning} as the set of layers selected for pruning, aiming to uninstall internal knowledge:\n\\mathcal{H}_{Pruning} = {h_i | \u2206R^l \u2265 \u03b1}.\nHere, \u03b1 represents the threshold that specifies the minimum activation ratio difference required for a layer to be pruned. After identifying the layers for pruning \\mathcal{H}_{Pruning}, we prune the FFN sub-layers in these layers, as they store the majority of the knowledge in LLMs (Geva et al., 2022, 2021; Meng et al., 2022; Mela et al., 2024)."}, {"title": "3.2 Knowledge-Augmented Adaptation through Preference Optimization", "content": "After pruning, we further incorporate a plug-and-play KAG adaptation module (Hu et al., 2022) and optimize it to recalibrate the model's knowledge utilization preferences, enabling more effective usages of external knowledge:\nL = \u03bb_1 * L_{KAT} + \u03bb_2 * L_{KPO},\nwhere \u03bb_1 and \u03bb_2 are hyperparameters that control the balance between the two objectives. The Knowledge-Augmented Training (L_{KAT}) and Knowledge Preference Optimization (L_{KPO}) objectives guide the pruned model to both generate accurate answers and calibrate its knowledge usage preference towards external knowledge.\nKnowledge-Augmented Finetuning. Following Lin et al. (2024), we maximize the likelihood of generating the ground truth answer y^* based on both query q and external knowledge c:\nL_{KAT} = \u2211_{q\u2208Q} -log P(y^* | q, c),"}, {"title": "4 CoConflictQA: A Consistency-Filtered Conflict Knowledge QA Dataset", "content": "In this section, we introduce the Consistency-filtered Conflict knowledge QA (CoConflictQA) dataset, specifically designed to analyze LLMs' behavior in real-world knowledge conflict scenarios. We begin by highlighting the key differences between CoConflictQA and existing works, followed by an overview of our knowledge conflict construction method, and conclude with a detailed description of the dataset construction process.\nMotivations of CoConflictQA. Previous studies typically simulate knowledge conflicts by synthesizing counterfactual contexts that contradict accurate parametric knowledge (Longpre et al., 2021; Si et al., 2023; Xie et al., 2024), thereby forcing LLMs to accept these pseudo facts provided in the input contexts. In contrast, our approach collects knowledge conflicts from the hallucinations and inaccurate memories of LLMs, rather than relying on these synthesized counterfactuals. Additionally, we ensure data quality by applying a self-consistency based data filtering method and utilizing a GPT-4o-mini based conflict verification mechanism, distinguishing our method from previous works (Yuan et al., 2024; Kortukov et al., 2024).\nKnowledge Conflict Construction. We propose a consistency-based knowledge conflict filtering method to identify and collect queries that induce conflicts between the parametric memory of LLMs and external information, thereby constructing a"}, {"title": "5 Experimental Methodology", "content": "In this section, we describe the datasets, evaluation metrics, baselines and implementation details used in our experiments.\nDatasets. For our experiments, we use the Co-ConflictQA dataset for both training and evaluation. In addition, we utilize the ConFiQA (Bi et al., 2024a) dataset during evaluation, which serves as an out-of-domain test scenario to assess the generalization ability of different models. ConFiQA is designed to evaluate the context faithfulness of LLMs in adhering to counterfactual contexts. It consists of three subsets: Question-Answering, Multi-hop Reasoning, and Multi-Conflicts, each containing 6,000 instances.\nEvaluation. Following previous work (Longpre et al., 2021; Zhou et al., 2023), we employ multiple evaluation metrics to assess the generated responses from two aspects: correctness and context faithfulness. To ensure more accurate evaluations, we normalize both responses and answers according to the method described by Li et al. (2024).\nFor accuracy assessment, we use EM(\u2191), which measures whether the generated responses exactly match the ground truth answers. To evaluate context faithfulness, we adopt two metrics: the recall of context (ConR\u2191) and the recall of memory (MemR\u2193). Specifically, ConR assesses whether the generated responses align with the provided context, while MemR assesses the alignment with parametric memories. Additionally, we adopt the memorization ratio MR(\u2193) =  \\frac{MemR}{MemR + ConR},\nwhich captures the tendency to rely on internal memory.\nBaselines. We evaluate PIP-KAG against five baselines, which are categorized into three groups: (1) Prompt-based approaches, including the attributed prompt (Attrprompt) and the combined opinion-based and instruction-based prompt (O&Iprompt) from Zhou et al. (2023); (2) Fine-tuning methods, consisting of standard Supervised Fine-Tuning (SFT) and Knowledge Aware Fine-Tuning (KAFT) (Li et al., 2023). KAFT enhances context faithfulness through counterfactual data augmentation; and (3) the Context-DPO (Bi et al., 2024a) utilizes DPO method (Rafailov et al., 2023) to strengthen context-grounded responses while penalizing those relying on parametric memory.\nImplementation Details. In our experiments, we use LLaMA3-8B-Instruct as the backbone for"}, {"title": "6 Experiment Results", "content": "In this section, we first present the overall performance of PIP-KAG. We then conduct ablation studies to evaluate the contribution of each individual component in PIP-KAG. Finally, we analyze the knowledge usage of PIP-KAG and examine the neuron inhibition in LLMs when provided with external knowledge."}, {"title": "6.1 Overall Performance", "content": "This experiment evaluates PIP-KAG on CoConflictQA to assess its overall performance. Additionally, we test PIP-KAG on the ConFiQA dataset, which represents an out-of-domain setting.\nAs shown in Table 2, PIP-KAG significantly outperforms baseline models on CoConflictQA, demonstrating its effectiveness in producing more accurate and contextually faithful responses. Compared to the vanilla KAG model, PIP-KAG achieves an average improvement of 52.36% in EM and about 5% in other evaluation metrics, highlighting its effectiveness in helping LLMs mitigate knowledge conflicts and use external knowledge. The evaluation results show that these prompt-based methods, such as Attrprompt and O&Iprompt, are effective in reducing the model's reliance on parametric knowledge, but they degrade the performance of the vanilla KAG model in terms of answer correctness. In contrast, fine-tuning-based approaches, including SFT, KAFT, and DPO, provide a more effective method to guide LLMs in"}, {"title": "6.2 Ablation Study", "content": "The ablation studies are conducted to investigate the contribution of different modules in PIP-KAG and to assess the performance of PIP-KAG using various parameter pruning strategies.\nAs shown in Table 4, we first compare PIP-KAG with PIP-KAG w/o Uninstall and PIP-KAG w/o Adaption models, in order to analyze the roles of different modules in PIP-KAG. The results indicate that removing the knowledge uninstallation or adaptation modules causes a performance drop of approximately 0.6% in ConR, highlighting their importance in helping LLMs effectively leverage external knowledge. Specifically, compared to PIP-KAG, PIP-KAG w/o Uninstall shows a 1.16% decrease in MR score, while PIP-KAG w/o Adaption exhibits a smaller decrease of 0.15% in MR score.\nThese findings demonstrate the effectiveness of PIP-KAG in calibrating the behavior of LLMs with respect to their reliance on parametric knowledge and in mitigating knowledge conflicts.\nNext, we evaluate the performance of PIP-KAG using three structured pruning strategies and one unstructured pruning method. The structured pruning strategies are as follows: (1) removal of FFN sub-layers (PIP-KAG), (2) elimination of MHA sub-layers (PIP-KAG w/ MHA), and (3) pruning of entire transformer layers (PIP-KAG w/ Layer). Additionally, we compare an advanced unstructured pruning approach (PIP-KAG w/ Param) (Lee et al., 2019), which identifies important parameters using a gradient-based importance scoring method and then prunes the less important parameters with the predefined pruning ratio. Among all the methods, PIP-KAG achieves the best performance when pruning FFN sub-layers, showing its effectiveness in reducing reliance on internal memory. The primary reason may lie in the fact that FFN layers play a critical role in knowledge storage, which aligns with findings in previous studies (Geva et al., 2022; Dai et al., 2022)."}, {"title": "6.3 Effectiveness of PIP-KAG in Leveraging Contextual Knowledge", "content": "In Figure 3, we further evaluate the ability of different LLMs to utilize contextual knowledge. We compare the performance of three models: the vanilla LLM, the Uninstall model (PIP-KAG w/o Adaption), and our PIP-KAG.\nFirst, we compute the semantic similarity between the outputs of different models and two knowledge sources-parametric knowledge from the model (Figure 3(a)) and contextual answers (Figure 3(b))\u2013to analyze their knowledge preference. Additionally, the performance of vanilla KAG model is provided as a reference. As shown in Figure 3(a), compared to vanilla LLM, the Uninstall model exhibits the lowest similarity with parametric knowledge among all models, indicating that the knowledge uninstallation process effectively reduces the LLM's reliance on internal mem-"}, {"title": "6.4 Neuron Activation in LLMs", "content": "As shown in Figure 4, we visualize the ratio of activated neurons and the absolute inhibition ratio |AR| (Eq. 5) in LLaMA3-8B-Instruct. The neuron activation ratios of different LLMs are provided in Appendix A.8.\nThe results reveal a significant reduction in overall neuron activation levels when external context is provided. This reduction likely suggests that certain neurons associated with parametric knowledge become inhibited in the presence of external knowledge. We further observe that these inhibited neurons are predominantly concentrated in the upper layers of the model, which aligns with prior findings that factual knowledge is predominantly stored in the upper layers of transformer-based models (Geva et al., 2021; Wang et al., 2024b). While parametric knowledge plays a crucial role in generating responses, it may introduce risks when it is outdated or conflicts with external information provided by KAG, potentially degrading the KAG performance. This work explores a pruning-based approach to mitigate the impact of parametric memory by removing these neurons that are inactive after feeding contextual knowledge, offering a new perspective on mitigating knowledge conflicts."}, {"title": "7 Conclusion", "content": "In this paper, we propose PIP-KAG, a novel paradigm for mitigating knowledge conflicts in LLM-based KAG systems. Our approach employs a neuron activation-based pruning method to selectively remove parametric knowledge, followed by the integration of a plug-and-play module that adjusts the model's preference toward external information."}, {"title": "Limitation", "content": "This paper uninstalls knowledge by removing entire FFN layers after identifying those with greater neuron inhibition. While PIP-KAG demonstrates promising results, the pruning process may possibly affect some non-knowledge-related neurons. Regarding evaluation, we have employed ConR and MemR metrics, consistent with established practices in the field. Although these rule-based metrics provide valuable insights, we acknowledge that more sophisticated evaluation methodologies could offer more comprehensive evaluations."}, {"title": "Ethics Statement", "content": "Our data construction process involves prompting LLMs to elicit their parametric knowledge for studying knowledge conflicts. This process may result in some hallucinated content. We commit to the careful distribution of the data generated through our research, ensuring it is strictly used for research purposes. Our goal is to encourage responsible use of LLMs while advancing understanding of knowledge conflicts. Additionally, no personally identifiable information or offensive content is included in our dataset. We adhere to ethical guidelines for responsible AI research and data sharing.\nWe also employed human evaluation to assess the reliability of GPT-4o-mini in identifying knowledge conflicts. Evaluation data was carefully distributed to human evaluators solely for research purposes, ensuring it adheres to ethical standards and contains no content that violates these standards."}, {"title": "A Appendix", "content": null}, {"title": "A.1 License", "content": "We present the licenses of the datasets used in this study: Natural Questions (CC BY-SA 3.0 license), NewsQA (MIT License), SearchQA and TriviaQA (Apache License 2.0), HotpotQA and SQUAD (CC BY-SA 4.0 license).\nAll these licenses and agreements permit the use of their data for academic purposes."}, {"title": "A.2 Details of Data Constructing", "content": "In this section, we detail the two main steps in constructing CoConflictQA. The dataset sizes at each stage of the pipeline are shown in Table 5.\nParametric Knowledge Elicitation. First, we elicit the LLM's parametric knowledge by prompting it in a closed-book setting (i.e., without any context). To ensure the reliability of the elicited knowledge, we apply a consistency-based filtering method. Specifically, for each query, the LLM is prompted five times, and the frequency of each response is recorded. The response with the highest frequency is identified as the majority answer. Queries where the majority answer appears fewer than three times are discarded, in order to filter out inconsistent responses and enhance data quality. The following prompt is used to instruct the LLM:"}, {"title": "A.3 Assessing the Reliability of GPT-4o-mini in Knowledge Conflict Identification", "content": "In this subsection, we conduct the human evaluation to assess the reliability of GPT-4o-mini in identifying knowledge conflicts, which is a critical task in our data construction process to guarantee the data quality.\nWe randomly sampled 100 examples from each of the six subsets of CoConflictQA, yielding a total of 600 samples. Six senior computational linguistics researchers were then asked to evaluate whether a knowledge conflict was present in each example. For each instance, the evaluators were provided with the question, the contextual answer, the model-generated response, and the corresponding supporting evidence. The results were classified into three categories: No Conflict, Somewhat Conflict, and High Conflict. The detailed annotation instructions are as follows:"}, {"title": "A.4 Evaluating the Effectiveness of Our Consistency-Based Filtering Method", "content": "In this subsection, we evaluate the effectiveness of our consistency-based knowledge conflict filtering method. As described in Appendix A.2, for each query, we prompt the model five times and record the most frequently generated answer along with its occurrence frequency. Based on this frequency, we divide the data into sub-datasets, where all queries within each sub-dataset share the same answer frequency. We then apply \"Conflict Data Selection\" to each sub-dataset, retaining only instances where knowledge conflicts occur. Finally, we evaluate ConR and MemR on these sub-datasets.\nAs shown in Figure 5, a clear trend emerges: as answer frequency increases, ConR consistently decreases, while MemR increases. This pattern indicates that as answer frequency rises, the model becomes increasingly reliant on its internal knowledge. Notably, for data with an answer frequency of 1, MemR is only 3%, indicating minimal dependence on internal knowledge. Retaining only high-answer-frequency data improves the quality of CoConflictQA. This data construction approach distinguishes our methodology from previous studies (Longpre et al., 2021; Xie et al., 2024)."}, {"title": "A.5 Additional Implementation Details of Our Experiments", "content": "This subsection outlines the training prompt, describes more details of the training data, and provides details of the experimental setup used in our experiments.\nTraining Prompts. We adopt a simple QA-format training prompt following Zhou et al. (2023) for all methods except Attrprompt and O&Iprompt.\nTraining Datasets. During PIP-KAG, we randomly sample 32,580 instances from the training set of the MRQA 2019 benchmark (Fisch et al., 2019) to construct our training data.\nExperimental Setup. In this work, all models are trained for 2,100 steps with a total batch size of 32 and a learning rate of 1e-4. To enhance training efficiency, we implemented PIP-KAG with LORA (Hu et al., 2022), setting both the rank r and scaling factor alpha to 64. For PIP-KAG, we set \u03b1 to 0.1 (Eq. 6), which determines the minimum activation ratio difference required for a layer to be pruned. Additionally, we adopt a dynamic \u03b3 in LKC (Eq. 9), which linearly transitions from an initial margin (\u03b3_0 = 1) to a final margin (\u03b3_1 = 5) as training progresses. This adaptive strategy gradually reduces the model's reliance on internal parametric knowledge, encouraging it to rely more on external knowledge provided by the KAG system."}, {"title": "A.6 Implementation Details of Baselines", "content": "This subsection describes the implementation details of all baseline methods.\nWe adopt two prompt-based baselines: the attributed prompt (Attrprompt) and a combination of opinion-based and instruction-based prompts (O&Iprompt). The corresponding prompt templates are as follows:"}, {"title": "A.7 Extending PIP-KAG to More LLMs", "content": "We extend PIP-KAG to a diverse range of LLMs, encompassing multiple model families and sizes."}]}