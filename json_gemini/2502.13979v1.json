{"title": "Utilizing Effective Dynamic Graph Learning to Shield Financial Stability from Risk Propagation", "authors": ["Guanyuan Yu", "Qing Li", "Yu Zhao", "Jun Wang", "YiJun Chen", "Shaolei Chen"], "abstract": "Financial risks can propagate across both tightly coupled temporal and spatial dimensions, posing significant threats to financial stability. Moreover, risks embedded in unlabeled data are often difficult to detect. To address these challenges, we introduce GraphShield, a novel approach with three key innovations: Enhanced Cross-Domain Information Learning: We propose a dynamic graph learning module to improve information learning across temporal and spatial domains. Advanced Risk Recognition: By leveraging the clustering characteristics of risks, we construct a risk recognizing module to enhance the identification of hidden threats. Risk Propagation Visualization: We provide a visualization tool for quantifying and validating nodes that trigger widespread cascading risks. Extensive experiments on two real-world and two open-source datasets demonstrate the robust performance of our framework. Our approach represents a significant advancement in leveraging artificial intelligence to enhance financial stability, offering a powerful solution to mitigate the spread of risks within financial networks.", "sections": [{"title": "1 Introduction", "content": "In financial markets like the networked-guarantee loan market, entities such as small and medium-sized enterprises (SMEs) are integrated into the same ecosystem. Within this network, the default risk of one SME can be influenced by the financial health and behavior of its peers. This interconnectedness leads to a phenomenon known as financial risk propagation [Ali and Hirshleifer, 2020]. Without adequate management, this can lead to widespread cascading risks, potentially destabilizing the entire financial system [Eisenberg and Noe, 2001; Billio et al., 2012; Elliott et al., 2014].\nAs shown in Fig. 1, financial risks can propagate through both temporal and spatial domains, which are tightly coupled. For example, the risk status of a node (e.g., $u_t$) is influenced by both its immediate neighbors and its prior state ($u_{t-1}$). This interdependence complicates the understanding of their propagation mechanisms. Recent studies have utilized dynamic graph neural networks to represent the financial system as a graph, where entities are nodes and their interconnections are edges [Cheng et al., 2022]. Such models typically capture structural features using graph neural networks, such as GCNs and GATs. They then proceed to learn temporal features through time-series models, including gated recurrent units, temporal attention layers, and iTransformer. Representative models include AddGraph [Zheng et al., 2019], TRACER [Cheng et al., 2020], StrGNN [Cai et al., 2021], and RisQNet [Lu et al., 2024]. It has been demonstrated that processing spatial and temporal information separately within such hybrid frameworks can lead to information loss across temporal and spatial domains, resulting in suboptimal outcomes [Liu et al., 2021].\nAs the scope and nature of real-world financial businesses continue to evolve, the patterns and forms of financial risks dynamically change [Hanley and Hoberg, 2019]. This evolution gives rise to new risks that may emerge stealthily and remain undetected or unlabeled, posing significant challenges to risk management strategies. As illustrated in Fig. 1, identifying some risk nodes hidden within the unlabeled data is particularly challenging yet crucial for preventing risk propagation. For instance, the risk status of $u_2$ impacts both its immediate neighbors and its future vulnerabilities. Failure to effectively identify and address these hidden risks can severely hinder efforts to control their spread.\nIn this study, we introduce GraphShield, a novel and effective dynamic graph learning approach designed to protect financial stability from the propagation of risks. This approach can achieve the following three main functionalities: (a) To enhance information learning across spatial and temporal domains, which are tightly coupled, we integrate both spatial and temporal operations simultaneously into a single layer of the dynamic learning module. This integration structure ensures that it captures the structural information of nodes while concurrently learning their temporal information. (b) To enhance the identification of hidden risks, we extend beyond the use of risk labels by exploiting the clustering tendencies of risk samples. These samples frequently group together, as visually demonstrated in Fig. 1 and empirically validated in the study [Lu et al., 2024]. We hypothesize that risk samples follow a Gaussian mixture distribution and employ a fully-connected neural network to construct the risk recognizing module, which can reduce the over-reliance on labels. (c) Furthermore, we offer a financial risk propagation visualization analysis tool capable of quantifying and validating the impact between risks. This tool aids in pinpointing and quantifying key factors and entities that trigger widespread cascading risks, exploring the interactions among various factors and entities that generate risk, and devising strategies to effectively mitigate or manage these risks. In summary, our study presents the following three unique contributions.\n(a) We propose a novel dynamic graph learning module to enhance information learning across spatial and temporal domains by integrating both spatial and temporal operations into a single layer. Besides, We are pioneering research into identifying hidden yet critical risk entities in the financial risk propagation process by leveraging their clustering characteristics.\n(b) We conduct a rigorous evaluation of our proposed approach by comparing it with existing benchmarks across various datasets, achieving state-of-the-art performance. Additionally, beyond mere risk identification, we offer an in-depth visualization analysis of financial risk propagation and demonstrate that our approach can prevent significant financial losses.\n(c) Our method represents an advancement in leveraging artificial intelligence to enhance financial stability. It offers a strong framework to mitigate the spread of risk throughout financial networks. By doing so, it strengthens financial stability, promotes economic growth, and aligns with sustainable development goals."}, {"title": "2 Related Work", "content": "Recent studies have utilized machine learning and deep learning technologies to capture and analyze complex data patterns, offering new perspectives and methods for risk assessment and management. For example, TRACER [Cheng et al., 2020], iConReg [Cheng et al., 2022], SCRPF [Cheng et al., 2023], RisQNet [Lu et al., 2024] utilize graphs to depict the loan-guarantee relationships among small and medium-sized enterprises in the networked loan market, and construct effective deep graph neural networks to identify and curb the propagation of financial defaults.\nIn this paper, acknowledging the dynamic propagation of financial risks across spatial and temporal dimensions, and their evolving patterns, we introduce a novel and effective dynamic graph learning model designed for the recognition and analysis of financial risks."}, {"title": "3 Preliminary", "content": "Financial dynamic graphs can be represented as $G = \\{G^S, G^T\\}$. The spatial domain $G^S$ comprises a series of directed heterogeneous graphs $G = \\{A_t,E_t, B_t, R_t\\}$ observed at discrete times $t = \\{1,...,T\\}$. Here, $A_t$ and $E_t$ denote the sets of nodes and edges at time t, respectively. Each node $u \\in A_t$ and each edge $e \\in E_t$ are linked to their respective types through the type mapping functions $\\eta(\\cdot)$ and $\\iota(\\cdot)$, associating nodes with types $\\eta(u) \\in B_t$ and edges with types $\\iota(e) \\in R_t$. The temporal domain $G^T$ is defined as $\\{G^T : u \\in A\\}$, where $A$ includes all node types. For each node u, $G^T$ is a directed homogeneous graph that captures the evolution of node u across the time points $t = \\{1,...,T\\}$. This graph consists of nodes $A_u = \\{u_1, u_2, ..., u_T\\}$ and edges $E_u = \\{e(u_i, u_j) : e(u_i, u_j) = 1$ if $j = i + 1$ and $e(u_i, u_j) = 0$ otherwise; $i, j = 1, . . ., T\\}$."}, {"title": "4 Our Proposed GraphShield Approach", "content": "As illustrated in Fig. 2, our proposed GraphShield approach can achieve the following three functionalities: (a) dynamic graph learning, (b) financial risk recognition, and (c) visualization analysis of risk propagation."}, {"title": "4.1 Dynamic Graph Learning Module", "content": "To effectively learn from dynamic graphs, neural network models must integrate both spatial structure and temporal dynamics. Typically, these two types of information are intertwined and must be processed simultaneously to enhance financial risk detection. A critical design consideration for dynamic graph encoders is how to simultaneously account for spatial and temporal information. Existing models involve using hybrid networks that combine spatial and temporal modules. These modules independently capture spatial and temporal data. For example, in the StrGNN model [Cai et al., 2021], a Graph Convolutional Network (GCN) acts as the spatial module, while a Gated Recurrent Unit (GRU) processes the GCN outputs across different timestamps to handle temporal dynamics. However, this separation can lead to the loss of information across spatial and temporal domains, resulting in suboptimal performance [Liu et al., 2021]. To address this issue, we introduce a novel dynamic graph learning module that interleaves spatial and temporal operations in a sandwich-like structure. Additionally, we implement both spatial and temporal operations using a multi-head attention mechanism enhanced by a separable kernel function, effectively reducing the time complexity from quadratic to linear."}, {"title": "Separable Kernel Function Based Spatial Operation", "content": "Here, we apply the separable kernel function based multi-head attention mechanism to construct spatial operations on a graph. Specifically, given a graph $G$ at timestamp t, let $u_t$ represent the target node. Let $v \\in N(u_t)$ denote the neighbors of $u_t$. In the h-th head of the multi-head attention mechanism, we apply a node type-specific linear transformation $Q-Linear_{\\eta(u)}^{(h)}(\\cdot)$ to the target features $H_u^{(l-1)}$, converting them into a query matrix Q. Similarly, for each neighbor $v \\in N(u_t)$, we employ $K-Linear_{\\eta(v)}^{(h)}(\\cdot)$ and $V-Linear_{\\eta(v)}^{(h)}(\\cdot)$ to map $H_v^{(l-1)}$ into key and value matrices $K$ and $V$, respectively. These transformations are tailored to the node types, enhancing the ability of our model to capture and utilize the structural and feature diversity within the graph.\n$Q = Q-Linear_{\\eta(u)}^{(h)}(H_u^{(l-1)}) \\in \\mathbb{R}^{N \\times d}$,\n$K = K-Linear_{\\eta(v)}^{(h)}(H_v^{(l-1)}) \\in \\mathbb{R}^{N \\times d}$,\n$V = V-Linear_{\\eta(v)}^{(h)}(H_v^{(l-1)}) \\in \\mathbb{R}^{N \\times d}$,\n(1)\nh = {1, 2, ..., H}.\nNotably, the canonical attention mechanism can be formulated as $QK^T/\\sqrt{d/H}$, which exhibits quadratic time complexity. To achieve a linear time complexity, we employ attention mechanisms based on separable kernel functions. Specifically, we calculate the i-th row of the weighted message head $M_u^{(h)}$ via leveraging the following equation,\nM_u^{(h)} = \\frac{\\sum_{j=1}^{|N|} \\varphi(\\langle Q_i, K_j \\rangle) V_j}{\\sum_{j=1}^{|N|} \\varphi(\\langle Q_i, K_j \\rangle)} = \\frac{\\mathbb{E}(\\varphi(Q) | \\varphi(K)) V}{\\mathbb{E}(\\varphi(Q) | \\varphi(K))},\n(2)\nIn the above equation, the kernel function is defined as $\\varphi(x) = ReLU(x) + 1$. Given that both $\\mathbb{E}(\\varphi(Q) | \\varphi(K)) V_j$ and $\\mathbb{E}(\\varphi(Q) | \\varphi(K))$ can be precomputed, cached, and reused, the complexity of calculating $M_u^{(h)}$ can be reduced from quadratic to linear. This optimization significantly enhances the efficiency of the process.\nThen, the updated node representation $H_u^{(l)}$ is computed as $H_u^{(l)} = \\tau_1 H_u^{(l-1)} + (1 - \\tau_1) ||_{h=1}^H M_u^{(h)}$. Here, $||_{h=1}^H M_u^{(h)}$ denotes the concatenation of H message heads, and $\\tau_1$, a trainable parameter, lies within the interval (0,1)."}, {"title": "Separable Kernel Function Based Temporal Operation", "content": "To construct temporal operations, we employ H heads of multi-head attention mechanisms based on separable kernel functions. Initially, we incorporate rotary position encoding [Su et al., 2024] prior to the temporal operation to mark the temporal order and relevance of the node sequence {$u_t$}$_{t=1}^T$. For $H_u^t$ at timestamp t, the rotated position embedding $H_{u,rot}^t$ is computed as follows,\nH_{u,2i}^{(1)} = H_{u,2i}^{(1)} cos(\\Theta_{\\frac{t}{10000^{2i/d}}}) + H_{u,2i+1}^{(1)} sin(\\Theta_{\\frac{t}{10000^{2i/d}}}),\n(3)\nH_{u,2i+1}^{(1)} = -H_{u,2i}^{(1)} sin(\\Theta_{\\frac{t}{10000^{2i/d}}}) + H_{u,2i+1}^{(1)} cos(\\Theta_{\\frac{t}{10000^{2i/d}}}),\n(4)\nwhere t = {1,2,..., T} and i = {1, 2, . . ., d}. At each layer, the input vectors are multiplied by their corresponding rotation vectors. In different layers, these input vectors undergo various rotational encodings. Consequently, the shallower layers primarily focus on information from neighboring positions, while the deeper layers concentrate on information from more distant locations. This hierarchical rotation allows the temporal operation to capture positional information from multiple perspectives, thereby enhancing its understanding of the global structure and context of the sequential data.\nIn the h-th attention head, we derive $M_u^{(h)}$ through the following linear transformation,\n$Q = Q-Linear^{(h)}(\\hat{H}_u) \\in \\mathbb{R}^{T \\times H}$,\n$K = K-Linear^{(h)}(\\hat{H}_u) \\in \\mathbb{R}^{T \\times H}$,\n$V = V-Linear^{(h)}(\\hat{H}_u) \\in \\mathbb{R}^{T \\times H}$,\n(5)\nM_u^{(h)} = \\frac{\\varphi(Q_i) \\sum_{j=1}^{|N|} \\varphi(K_j)^T V_j}{\\varphi(Q_i) \\sum_{j=1}^{|N|} \\varphi(K_j)^T}\nIn the equation above, the kernel function $\\varphi(x) = ELU(x) + 1$. The terms $\\sum_{j=1}^{|N|} \\varphi(K_j)^T V_j$ and $\\sum_{j=1}^{|N|} \\varphi(K_j)$ can be precomputed, cached, and reused, enabling linear computational complexity. Finally, the update for $H_u^{(l+1)}$ is given by $\\tau_2 H_u^{(l)} + (1 - \\tau_2) ||_{h=1}^H M_u^{(h)}$, where $\\tau_2 \\in (0, 1)$ is a trainable parameter."}, {"title": "Graph Reconstruction Constraint", "content": "C_{rec}(\\mathcal{G},\\hat{\\mathcal{G}}) = \\sum_{t=1}^T {\\frac{1}{|D(G)|} \\sum_{(u,v) \\in D(G)} |D_{uv}(G) - D_{uv}(\\hat{G})| + \\frac{1}{|D(\\hat{G})|} \\sum_{(u,v) \\in D(\\hat{G})}|D_{uv}(G) - D_{uv}(\\hat{G})|},\\\\\nwhere $\\hat{\\mathcal{G}} = {\\hat{G^S},\\hat{G^T}}$ represents the predicted graph, and D(\u00b7) computes the adjacency matrix. Minimizing $C_{rec}(\\mathcal{G},\\hat{\\mathcal{G}})$ optimizes the model parameters."}, {"title": "4.2 Risk Recognizing Module", "content": "To enhance the identification of hidden risks, we move beyond traditional reliance on risk labels. Instead, we leverage the inherent clustering tendencies of risk samples, which naturally group together. This is visually demonstrated in Fig. 1 and empirically validated in the study [Lu et al., 2024", "gamma_{i,K}": "Softmax(h^{(L)}) \\in \\mathbb{R}^{K}.\n(7)\nNext, we calculate the estimated expectation $\\mu_k$, component probability $\\varpi_k$, and covariance $\\Sigma_k$ for the k-th data group,\n\\mu_k = \\frac{\\sum_{i=1}^N \\gamma_{i,k} z_i}{\\sum_{i=1}^N \\gamma_{i,k}}, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"}]}