{"title": "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework", "authors": ["Krishna Aswani", "Huilin Lu", "Pranav Patankar", "Priya Dhalwani", "Iris Tan", "Jayant Ganeshmohan", "Simon Lacasse"], "abstract": "Recent advancements in prompt engineering strategies, such as Chain-of-Thought (CoT) and Self-Discover, have demonstrated significant potential in improving the reasoning abilities of Large Language Models (LLMs). How-ever, these state-of-the-art (SOTA) prompting strategies rely on single or fixed set of static seed reasoning modules like \"think step by step\" or \"break down this problem\" intended to simulate human approach to problem-solving. This constraint limits the flexibility of mod-els in tackling diverse problems effectively. In this paper, we introduce Auto-Evolve, a novel framework that enables LLMs to self-create dynamic reasoning modules and downstream action plan, resulting in significant improve-ments over current SOTA methods. We evalu-ate Auto-Evolve on the challenging BigBench-Hard (BBH) dataset with Claude 2.0, Claude 3 Sonnet, Mistral Large, and GPT 4, where it con-sistently outperforms the SOTA prompt strate-gies. Auto-Evolve outperforms CoT by up to 10.4% and on an average by 7% across these four models. Our framework introduces two innovations: a) Auto-Evolve dynamically gen-erates reasoning modules for each task while aligning with human reasoning paradigm, thus eliminating the need for predefined templates. b) We introduce an iterative refinement compo-nent, that incrementally refines instruction guid-ance for LLMs and helps boost performance by average 2.8% compared to doing it in a single step.", "sections": [{"title": "1 Introduction", "content": "LLMs have demonstrated significant potential in various Natural Language Processing (NLP) ca-pabilities such as understanding, generating, and reasoning (Brown et al., 2020; Chowdhery et al., 2022; Anil et al., 2023; OpenAI, 2023b). Despite the impressive progress, LLMs continue to have challenges in solving multi-step reasoning tasks that require systematic thinking and planning. In-creasing model size alone is not enough to solve these issues, emphasizing the necessity for devel-oping novel techniques to improve LLMs' reason-ing capabilities (Srivastava et al., 2022; Rae et al., 2021).\nVarious prompting strategies have been devel-oped to guide and facilitate the reasoning capabili-ties of LLMs. CoT (Wei et al., 2022) has emerged as a prominent approach, encouraging LLMs to generate step-by-step explanations mimicking hu-man reasoning.\nSubsequent research efforts have focused on re-fining the generation process thereby enhancing the quality and consistency of the rationales (Kojima et al., 2022; Fu et al., 2023; Zhou et al., 2022; Wang et al., 2022). Self-Discover (Zhou et al., 2024) im-proves models' reasoning capabilities over CoT by allowing models to select the most appropri-ate reasoning path from a fixed set of reasoning modules. However, our analysis of the seed mod-ules in Self-Discover revealed that a subset of fixed seed modules dominated the usage, limiting the framework's reasoning coverage and performance on diverse tasks (Appendix: Fig. 9). CoT's and Self-Discover's reliance on a limited set of rea-soning seed modules such as \"think step by step\" or \"break down this problem\" constrains the ap-proaches to tackling a problem, negatively affect-ing their ability to generalize over diverse tasks.\nOur framework, Auto-Evolve, builds upon the strengths of previous prompting approaches while addressing their limitations. Rather than relying on a fixed set of seed modules, Auto-Evolve creates custom reasoning modules on-the-fly for each task, allowing LLMs to come up with a wider range of reasoning structures (instruction guidance in JSON format that LLMs can follow to solve a task step by step) that are better suited to handle the spe-"}, {"title": "2 Related work", "content": "There are two key model optimization techniques, Model Prompting and Model Fine-Tuning. Model Prompting Methods enhance the reasoning capa-bilities of LLMs by providing carefully designed prompts that guide the model towards generating the desired output, without modifying the underly-ing model parameters. On the other hand, Model Fine-Tuning Methods involve updating the model's parameters by training on a relevant dataset to spe-cialize the model for a particular task or domain, which can be computationally expensive. While both methods have their advantages and disadvan-tages, our framework Auto-Evolve, is closely re-lated to Model Prompting Methods.\nModel Prompting Methods such as COT prompting (Wei et al., 2022) encourages models to generate intermediate reasoning steps that lead to the final desired answer. CoT has been shown to boost performance on arithmetic, commonsense, and symbolic reasoning tasks. Subsequent work has extended CoT by selectively sampling ratio-nales (Kojima et al., 2022), improving rationale consistency (Wang et al. (2022); Self-Consistency), generating more structured reasoning paths (Fu et al., 2023), and having models first plan the rea-soning before solving the problem (Wang et al. (2023); Plan-and-Solve). Self-Discover (Zhou et al., 2024), introduces a three-stage process where LLMs select relevant reasoning modules, adapt them to the specific task, and implement them into a coherent reasoning structure. Self-Discover out-performs CoT (Wei et al., 2022), Self-Consistency"}, {"title": "3 Auto-Evolve Framework", "content": "Auto-Evolve framework is inspired by two fun-damental principles. (1) Higher interpretabil-ity associated with JSON structure: LLM's rea-soning capabilities and performance are enhanced by JSON structure's higher interpretability (Zhou et al., 2023; OpenAI, 2023b,a). (2) LLMs have inbuilt diverse reasoning abilities: LLMs pos-sess an inherent grasp of diverse thinking styles and essential reasoning modules crucial for tack-ling variety of tasks since they were trained on enormous data, typically measured in petabytes. SOTA Self-Discover adheres to the first principle, but it overlooks the key aspect of the second princi-ple. Instead of leverage knowledge hidden within LLMs, Self-Discover supplies LLMs with a fixed set of initial human-designed reasoning modules such as \u201cUse critical thinking\u201d and \u201cLet's think step by step\u201d. On the flip side, Auto-Evolve advo-cates for LLMs' intrinsic ability to independently discern and utilize relevant reasoning strategies for different tasks.\nAuto-Evolve comprises of two stages as illustrated in Fig. 1. Stage 1 dynamically generates intrin-sic task-related reasoning modules and structure (JSON instructions) by leveraging task examples and three meta-prompts, thereby guiding LLMs to solve tasks without needing static human-designed seed modules and further training. Stage 1 operates at task-level, i.e., one run for each task category. Stage 2 uses the finalized reasoning structure pro-duced as an output of Stage 1 to solve individual task instances by asking the model to follow the instruction step by step. Given the straightforward and uncomplicated nature of Stage 2, we focus rest of this section on further elaborating the three com-ponents of Stage 1 that are illustrated in Fig. 2. We also present a graphical representation in Fig. 3 that's accompanied by mathematical notations to elucidate the procedure of Stage 1. Left half of this figure showcases the GENERATE and IMPLE-MENT components, while right half showcases the REFINE components. The mathematical no-tations are explained in the following subsections. Prompts details are included in Appendix Fig. 7."}, {"title": "3.1 Reasoning Module Generator (GENERATE)", "content": "The primary function of the GENERATE com-ponent is to dynamically create task-specific rea-soning modules and descriptions. Unlike the Self-Discover approach that relies on a predetermined set of 39 static reasoning modules (Appendix Fig. 9) for problem solving, GENERATE em-braces adaptability and responsiveness by creating modules dynamically. E.g., the reasoning mod-ules in Appendix Fig. 10 for Boolean Expression and Disambiguation QA tasks are generated using Auto-Evolve.\nFor the tasks under the same domain, given only a few task examples without labels $t_i \\in T$, GENERATE first creates a set of task-specific reasoning modules R by using a model M and a meta-prompt PG:\n$R = M(P_G||t_i)$.\nBy assessing the unique attributes and demands of each task, GENERATE orchestrates the creation of a task-specific set of reasoning modules, ensur-ing a nuanced and tailored approach to problem-solving. The dynamic generation process enables our framework to continually evolve and adapt to new challenges and task domains, facilitating more effective and contextually relevant reasoning pro-cesses."}, {"title": "3.2 Reasoning structure initializer (IMPLEMENT)", "content": "IMPLEMENT serves as a starting point for gen-erating task-specific reasoning structure. IMPLE-MENT uses only the first reasoning module from GENERATE for building the initial reasoning structure. This lays the groundwork for subse-quent refinement steps and ensures the initial rea-soning structure aligns closely with the context of the given task.\nGiven the same task examples without labels $t_i \\in T$, Reasoning Structure Initializer implements an initial key-value reasoning plan S by using the first reasoning module $R_1$ generated from previous component, an action plan of another task E and a meta-prompt $P_1$:\n$S = M(P_1||t_i||R_1||E)$."}, {"title": "3.3 Reasoning structure evolver (REFINE)", "content": "Finally, given the initial reasoning structure S, RE-FINE component iteratively distills the initial rea-"}, {"title": "4 Experiments", "content": "We evaluate Auto-Evolve using a diverse and large-scale reasoning benchmarking dataset: BIG Bench Hard (BBH) (Suzgun et al., 2022). It is designed to evaluate the performance and reasoning capa-bilities of language models. It consists of 23 com-plex reasoning tasks, totaling 5,511 task instances. (Appendix Table 3) spanning across 4 domains: (1) Algorithmic and Multi-Step Arithmetic Rea-soning (11 tasks, e.g., Boolean Expressions Eval-uation, Object Counting), (2) Natural Language Understanding (7 tasks, e.g., Snarks, Disambigua-tion QA), (3) Use of World Knowledge (5 tasks, e.g., Movie Recommendation, Date Understand-ing), and (4) Multilingual Knowledge and Rea-soning (Salient Translation). We use accuracy as the evaluation metric to measure the model perfor-mance on BBH."}, {"title": "4.2 Models", "content": "We use four LLMs to showcase the generalizability of Auto-Evolve framework: Claude 2.0 (Anthropic, 2023), Claude 3 Sonnet (Anthropic, 2024), Mistral Large (AI, 2024) and GPT-4 (gpt-4-turbo-preview) (OpenAI, 2023b). In our experiments, LLMs ex-hibited non-determinism even with temperature set to 0*. To ensure robustness in our evaluations, we run all experiments three times and average the results. Table 1, Table 2, Fig. 4 and Fig. 5 in the next section show the performance of Auto-Evolve compared to other prompt strategies."}, {"title": "4.3 Baselines", "content": "We compare Auto-Evolve with Direct, CoT and Self-Discover frameworks for evaluating LLM rea-soning capabilities:\nDirect Prompting, where language models pro-duce the answer without the need for intermediate reasoning stages.\nCoT (Wei et al., 2023; Kojima et al., 2022), where language models are prompted to produce a logical sequence of steps resulting in the final solution.\nSelf-Discover (Zhou et al., 2024), where a set of thinking styles are provided to guide LLMs to pro-duce a logical path for solving problems, much like the approach a human expert might take."}, {"title": "4.4 Experiments setup and evaluation", "content": "LLM Inputs: For Direct Prompting, we only pro-vide task instance as the prompt, while for CoT, we add an additional sentence \"Thinking step-by-step\" to the prompt fed into the LLMs. For Self-Discover, the prompt includes a set of 39 thinking styles for LLMs to select and adapt to the tasks. For Auto-Evolve, however, we purely rely on LLMs to dynamically generate the task-specific reason-ing modules and structures. During the steps for generating the task-specific reasoning modules and reasoning structures, we randomly select two task instances without the target labels from the task set as the examples fed to LLMs. For the step-by-step plan example which is applied in Reasoning Struc-ture Initializer component of Stage 1, we use the model-discovered JSON structure generated from another task.\nLLM Response Evaluation: We meticulously examine the results obtained from the LLMs with automatic and manual evaluation procedures. Since LLMs do not always produce consistent format of outputs when they follow the reasoning instruc-tions, we programmatically extract answers/labels by examining the model responses. For the outputs that can not be programmatically extracted, we em-"}, {"title": "5 Results and Discussion", "content": "Auto-Evolve demonstrates significant performance improvement across the 23 diverse tasks in the BBH dataset (Suzgun et al., 2022). As shown in Table 1, Auto-Evolve achieves an average abso-lute 8.1% and 2.7% improvement across 23 di-"}, {"title": "5.1 Performance", "content": "verse tasks over CoT and Self-Discover respec-tively when using Mistral Large. With Claude 2.0, the improvement is even more substantial, with 10.4% and 6.7% gains over CoT and Self-Discover. We observe the same trends for GPT-4 in Table 2, where Auto-Evolve improves GPT-4's performance over CoT and Self-Discoverwith absolute gains of 6.3% and 2.6%. The performance improvements on Claude 3 Sonnet are less significant, achiev-ing an average absolute 2.5% and 3.1% improve-ment over CoT and Self-Discover respectively. It is likely due to Claude 3 Sonnet's already advanced reasoning capabilities, which enable the model to perform exceptionally well even with a direct ap-proach, without the aid of prompting techniques. This leaves less room for enhancement through external reasoning frameworks like Auto-Evolve. These results highlight the effectiveness of Auto-Evolve's dynamic and adaptive reasoning approach compared to frameworks that rely on static seed modules."}, {"title": "5.2 Efficiency", "content": "Auto-Evolve Framework is designed with effi-ciency and inference call costs in mind. For each task, the framework requires 1 call for GENER-ATE, 1 call for IMPLEMENTand on average 4-5 calls for REFINE. These one-time calls enable ef-ficient processing of large datasets, with only 1 call per data point required once the reasoning struc-ture is defined. Appendix Fig. 14 compares the efficiency of Auto-Evolve with other prompting framework (data from (Zhou et al., 2024)), demon-strating that it achieves similar or better perfor-mance than Self-Consistency and Majority Voting while requiring 10-40 times fewer inference calls."}, {"title": "5.3 Themes: Improvement across categories", "content": "Auto-Evolve demonstrates performance improve-ments across all four categories of the BBH dataset (Suzgun et al., 2022) as shown in Fig. 5. The most notable improvements are observed in the Algo-rithm category, where the complex reasoning struc-tures generated by the Auto-Evolve prove partic-ularly effective. We believe these types of tasks require much more complex reasoning structures because of which our framework outperformed Self-Discover."}, {"title": "5.4 Ablation", "content": "The ablation study in Fig. 6 highlights the indi-vidual contributions of the GENERATE + IM-PLEMENT and REFINE components in the Auto-Evolve framework. These results are compared to the CoT and Self-Discover across fours tasks with Claude 2.0. We chose to conduct ablation study using Claude 2.0 as it had the most pronounced dif-ference between results for Self-Discover and Auto-Evolve across the evaluated tasks (6.7%), allowing us to clearly highlight the individual impacts of Auto-Evolve's components.\nGENERATE + IMPLEMENT components alone for all the BBH tasks achieve 62.6% per-formance. While with the refine step included it achieves 65.4% performance, giving a perfor-mance boost of 2.8%. It outperforms CoT and Self-Discover on all four tasks with avg. improvement of 7.25% for CoT and 4.75% for Self-Discover. With GENERATE + IMPLEMENT we see the most improvement in arithmetic task, 17% on CoT, 13% on Self-Discover. REFINE gives an avg. boost of 15% for CoT and 12.75% for Self-Discover."}, {"title": "5.5 Deep Dive Analysis", "content": "Fig. 10 in Appendix showcases reasoning modules generated by Claude 2.0 using the Self-Discover and Auto-Evolve frameworks for two distinct tasks: Boolean and Disambiguation QA. In the case of the Boolean Expressions task, Auto-Evolve generates a"}, {"title": "5.5.1 Deep Diving into Auto-Evolve Reasoning Modules", "content": "highly pertinent module: \"Identify and understand logical operators (not, and, or, etc.)\", which di-rectly addresses the core aspects of the task. On the other hand, Self-Discover uses more generic mod-ules such as \"Critical Thinking\" and \"Let's think step by step\", which lack the task-specific focus needed for optimal performance. Similarly, for the Disambiguation QA task, Auto-Evolve generates a module that captures the essence of the task: \"Mem-ory Module: Maintain awareness of noun phrases mentioned earlier in the passage or conversation to determine if the pronoun refers back to one of those\". This module encapsulates the key aspects of pronoun resolution and antecedent identification, which are crucial for disambiguating references in the given context. In contrast, Self-Discover's modules remain more general, even after the adapt stage, where they are refined to \"Identify the pro-noun. Find all possible antecedents based on noun phrases\". While this refinement improves the rele-vance of the modules, they still lack the complexity and specificity offered by Auto-Evolve. The en-"}, {"title": "5.5.2 Deep Diving into Auto-Evolve Reasoning Structures", "content": "In Appendix Fig. 11, we showcase Auto-Evolve generated reasoning structures for Hyperbaton rea-soning task using GPT-4. Auto-Evolve reason-ing structure is tailored to its task, incorporating task-specific reasoning modules such as \"Linguis-tic Analysis\", \"Adjective Order Rules\", \"Recall rules and examples\" and etc. Through Linguistic Analysis reasoning module, Auto-Evolve is able to recognizes the standard English conventional or-der of adjectives, and derives the correct answer. Additionally, Appendix Fig. 11 contrasts reason-ing processes from Self-Discover. Self-Discover's reasoning modules emphasize simplification and decomposition of problems into manageable parts, as well as consideration of human behavior nu-ances. Correspondingly, the generated reasoning structure breaks down sentences into constituent adjectives and simplifies grammatical rules to fa-cilitate understanding. While Self-Discover also presents an action plan, it fails to recognize the task requirement of adjective ordering and yield incorrect answer."}, {"title": "5.6 Transferability and Generalizability to OpenSource Models", "content": "One of the main challenges in using open-source models is achieving the same reasoning ability and accuracy as larger proprietary models. In our experiments on the disambiguation question-answering task from the BBH dataset, Llama 3.1 70B achieved only 22.4% accuracy with direct prompting. However, with Auto-Evolve, which dy-namically generates reasoning structures, the accu-racy surged to 72.0%, outperforming Self-Discover (56.8%) and CoT (60.4%) as well. We observed similar improvements on the causal judgement task, where Auto-Evolve (65.3%) outperformed direct prompting (27.3%), CoT (62.6%), and Self-Discover (64.7%).\nSmaller models like Llama 3.1 8B typically struggle to generate complex reasoning plan au-tonomously. This limitation can be addressed by using larger models to create these reasoning struc-tures. When we applied reasoning structures gen-erated by Llama 3.1 70B to Llama 3.1 8B, the model's accuracy improved significantly. With Auto-Evolve, Llama 3.1 8B achieved 62.4% accu-racy compared to 45.2% with direct prompting and 54.4% with CoT. For the causal judgement task, Auto-Evolve (58.3%) again outperformed direct prompting (49.7%) and CoT (56.1%).\nThese results highlight that while smaller models struggle to generate complex reasoning structures independently, they can perform well when guided by reasoning structures from larger models, demon-strating the transferability of reasoning strategies across model architectures. This approach provides an efficient solution for resource-constrained en-vironments while still benefiting from advanced reasoning capabilities. It also opens opportunities for further research on optimizing transferability and balancing performance and efficiency across models of different sizes."}, {"title": "6 Conclusion and Future Work", "content": "Auto-Evolve introduces a novel framework that dy-namically generates task-specific reasoning struc-tures, eliminating the need for static seed modules and enabling more effective reasoning across di-verse problem domains. By seamlessly integrating dynamic prompt generation and iterative refine-ment, Auto-Evolve surpasses state-of-the-art meth-ods like CoT prompting, achieving performance improvements up to 10.4% and an average gain of 6.8% when evaluated with GPT-4, Claude 2.0, Claude 3 Sonnet and Mistral Large models. The framework's ability to transfer reasoning structures from larger models to smaller ones, as demon-strated with models like Llama 3.1 8B, highlights its broader utility and adaptability across architec-tures.\nThe broader implications of Auto-Evolve ex-tend beyond the performance enhancement, as the framework has the potential to advance the devel-opment of more interpretable and transparent AI systems by generating dynamic problem specific reasoning modules and explicit reasoning struc-tures. Our experimentation has highlighted the pivotal role played by JSON reasoning structures in solving tasks effectively. In future iterations, we aim to explore the potential of incorporating feedback mechanisms to iteratively improve these reasoning structures, further refining and enhanc-ing the framework's capabilities."}, {"title": "Limitations", "content": "While the proposed Auto-Evolve framework demonstrates promising results in enhancing large language models' reasoning capabilities, we ac-knowledge the following limitations:\nApplicability to Smaller Models: Our experi-ments demonstrate that large models like Llama 3.1 70B can directly benefit from Auto-Evolve, in-dependently generating and utilizing sophisticated reasoning structures. However, smaller models such as Llama 3.1 8B struggle to create these struc-tures autonomously. We found that applying rea-soning structures generated by larger models (e.g., Llama 3.1 70B) to guide smaller models signif-icantly enhances their performance. This com-bined approach enables resource-efficient models to leverage advanced reasoning capabilities. Future research will focus on optimizing this transfer pro-cess, exploring methods to effectively scale reason-ing capabilities across models of varying sizes and architectures, with particular emphasis on enhanc-ing smaller, more efficient models using insights from their larger counterparts.\nIncreased Complexity in Reasoning Structures: Auto-Evolve's reasoning structures can become overly complex due to the cyclic incorporation of insights from multiple reasoning modules. This complexity, while beneficial for certain tasks de-manding elaborate reasoning, is not universally nec-essary and can be an overhead for simpler tasks. Based on our experience we suggest readers to in-corporate all reasoning modules in a single step as a starting point and then use iterative part of the framework as a optional step for problems that can't be solved with single step.\nModel Determinism: During our experiments, we observed non-deterministic behavior even when the temperature was set to be 0. Slight variations in the generated reasoning modules led to significant disparities in the downstream reasoning structures and outputs. To address this, we ran multiple trials and reported average performance, which added computational overhead."}, {"title": "Ethics Statement", "content": "Bias Propagation and Amplification: While Auto-Evolve is designed to enhance the reasoning abili-ties of large language models (LLMs), we acknowl-edge the potential for the generated reasoning mod-ules to propagate or even amplify biases present in the underlying model's training data. If the train-ing data contains cultural, societal, or linguistic biases, these biases may manifest in the reasoning modules and structures produced by Auto-Evolve. To mitigate this risk, it is crucial to incorporate human-in-the-loop feedback mechanisms or other guardrails to ensure that the final outputs align with user values and ethical considerations."}, {"title": "A Auto-Evolve Prompt details", "content": "The meta-prompt templates for the GENERATE, IMPLEMENT and REFINE components in the first stage of Auto-Evolve are shown in Fig. 7."}, {"title": "B Performance on BBH dataset", "content": "Table 3 contains BBH per-task performance of Claude 2.0, Claude 3 Sonnet and Mistral Large over 4 prompt strategies comparing to human performance. Compared to human average performance, Mistral Large with Auto-Evolve framework outperforms on 19 out of 23 tasks, Claude 2.0 with Auto-Evolve outperforms on 11 out of 23 tasks, and Claude 3 Sonnet with Auto-Framework outperforms on 11 out of 23 tasks."}, {"title": "C Analyzing Reasoning Processes", "content": "The comparison between reasoning modules generated using Self-Discover and Auto-Evolve reveals distinct approaches to problem-solving. Self-Discover's generated reasoning modules emphasize simplifi-cation and decomposition of problems into manageable parts, as well as consideration of human behavior"}, {"title": "D Reasoning Module Analysis", "content": "Frequency plot Fig. 9 showcases that Self-Discover only uses a few reasoning seed modules in solving the BBH tasks (out of 39). The inherent gravitation of LLMs towards utilizing only a subset of the provided"}, {"title": "E Auto-Evolve Reasoning Module Comparison", "content": "Fig. 10 shows deep analysis on reasoning module generation comparisons across two different prompt strategies (Self-Discover and Auto-Evolve). For both Boolean and Disambiguation tasks, Auto-Evolve represents a significant advancement over Self-Discover by implementing more detailed, task-specific reasoning modules. This approach allows for greater flexibility and adaptability, enhancing the model's performance in complex reasoning tasks. The specific focus on logical operations, detailed syntax and grammar analysis, and memory retention provides a more comprehensive framework for improving LLM reasoning capabilities."}, {"title": "F Auto-Evolve Reasoning Structure Comparison", "content": "In Fig. 11 example, it showcases the LLMs follow the reasoning structures using Auto-Evolve and Self-Discover framework on a Hyperbaton task. LLMs are able to follow the Auto-Evolve's guidance integrated with task-specific instructions (keys and sub-keys) and derive the final answer correctly. In this specific Hyperbaton task, while Self-Discover relies on a broad and generic analysis, Auto-Evolve employs a detailed and structured approach that includes linguistic analysis, pattern recognition, and comparative evaluation. This comprehensive method allows Auto-Evolve to accurately apply grammatical rules and critically assess sentence structures, leading to more reliable and correct outcomes. The Auto-Evolve framework's ability to dynamically adapt its reasoning structures based on the specific task at hand demonstrates a significant improvement in handling this complex linguistic challenges."}, {"title": "G Auto-Evolve Performance Comparison", "content": "In the Fig. 12, it displays the accuracy differences of Auto-Evolve over Direct Prompt, CoT and Self-Discover on GPT-4 for BBH 23 tasks. The green bars show the absolute percentage improvement, and yellow bars show the absolute percentage decrease. Auto-Evolve outperforms 22/23 tasks over Direct Prompt, and outperforms 17/23 tasks over CoT and Self-Discover on GPT-4. By using GPT-4 with our proposed framework Auto-Evolve, it improves most on complex tasks such as Web of Lies, Multistep Arithmetic, Shuffled Object and etc."}, {"title": "H Efficiency Comparison", "content": "In Fig. 14, it displays the number of inference calls per task instance. Below, we give an example of total number of calls by task level (aggregate level). Example: For one task which includes 250 questions, below are the number of inference calls to the LLMs for different prompting strategies compared to Auto-Evolve. Direct Prompting: 250 calls / per task. Chain-of-Thought: 250 calls / per task."}]}