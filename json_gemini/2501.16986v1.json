{"title": "Generative quantum combinatorial optimization by means of a novel conditional generative quantum eigensolver", "authors": ["Shunya Minami", "Kouhei Nakaji", "Yohichi Suzuki", "Al\u00e1n Aspuru-Guzik", "Tadashi Kadowaki"], "abstract": "Quantum computing is entering a transformative phase with the emergence of logical quantum processors, which hold the potential to tackle complex problems beyond classical capabilities. While significant progress has been made, applying quantum algorithms to real-world problems remains challenging. Hybrid quantum-classical techniques have been explored to bridge this gap, but they often face limitations in expressiveness, trainability, or scalability. In this work, we introduce conditional Generative Quantum Eigensolver (conditional-GQE), a context-aware quantum circuit generator powered by an encoder-decoder Transformer. Focusing on combinatorial optimization, we train our generator for solving problems with up to 10 qubits, exhibiting nearly perfect performance on new problems. By leveraging the high expressiveness and flexibility of classical generative models, along with an efficient preference-based training scheme, conditional-GQE provides a generalizable and scalable framework for quantum circuit generation. Our approach advances hybrid quantum-classical computing and contributes to accelerate the transition toward fault-tolerant quantum computing.", "sections": [{"title": "Introduction", "content": "We are at the dawn of the era of fault-tolerant quantum computation. Logical qubits have been demonstrated across several quantum computing architectures [1, 2, 3, 4, 5, 6]. Recent advances in error-correcting codes [7, 8] further accelerate the shift toward early fault-tolerant systems in the relatively near future. While these developments enable substantially more quantum operations than systems without error correction, executing fault-tolerant quantum algorithms for practically significant problems remains a distant goal. Thus, building hybrid quantum-classical algorithms that work with quantum devices in the early-fault-tolerant regime is a practical and reasonable focus [9, 10] for near-term quantum computing applications.\nOne widely studied methodology over the past decade is the variational quantum algorithm (VQA) [11, 12, 13, 14]. Applications of VQA, such as quantum machine learning [15, 16, 17, 18], often require upload-ing classical data into the circuit. The most common strategy is to embed the data into the rotation angles of gates in a parameterized quantum circuit (PQC). However, this approach faces limitations in express-ibility, as the Fourier components of the expectation function are constrained to specific wave numbers [19, 20, 21]. Moreover, embedding classical knowledge or inductive bias into the PQC structure remains challenging [22], despite the critical role of inductive bias in successful optimization [23]. Addressing these limitations requires innovative strategies that lead to the next-generation hybrid quantum-classical computation.\nThis paper explores an alternative approach based on the recently proposed generative quantum eigen-solver (GQE) [24]. GQE is a hybrid quantum-classical algorithm that uses a classical generative model to construct quantum circuits, where circuit components are sequentially generated from a predefined gate pool, similar to sentence generation in natural language processing. Unlike VQAs, no parameters are embedded in the quantum circuit; all parameters are contained within a classical generative model (see Fig. 1). These parameters are iteratively updated to minimize a particular objective. In a proof-of-concept experiment [24], the generative model is implemented using GPT-2 [25] architecture, referred to as the generative pre-trained transformer quantum eigensolver (GPT-QE), and its effectiveness is demonstrated in the ground state search of molecular Hamiltonians. A key feature of GQE is its ability to incorporate classical variables directly into the neural network, allowing for a non-trivial influence on the generated quantum circuits. Additionally, inductive biases can be seamlessly integrated, much like classical convolutional neural networks in computer vision [26, 27, 28] and graph neural networks in ma-"}, {"title": "Results", "content": "terials informatics [29, 30, 31]. While the potential of incorporating classical variables into the generative model has been previously discussed in the context of quantum chemistry [24], specific methods for its implementation have not yet been detailed.\nBased on the concept of GQE, this paper introduces conditional-GQE (Fig. 1c), an input-dependent quantum circuit generation. To generate circuits from given inputs, we adopt an encoder-decoder Trans-former architecture [32], making the model applicable across different contexts. We apply this conditional-GQE approach to combinatorial optimization and develop a new hybrid quantum-classical method called Generative Quantum Combinatorial Optimization (GQCO). By incorporating a graph neural network [33] into the encoder to capture the underlying graph structure of combinatorial optimization problems, our model is trained to generate quantum circuits to solve combinatorial optimization problems with up to 10 qubits, achieving about 99% accuracy on new test problems. Notably, for 10-qubit problems, the trained model finds the correct solution faster than brute-force methods, simulated annealing (SA) [34], or the quantum approximate optimization algorithm (QAOA) [35].\nMany of the existing works for quantum circuit design [36, 37] often rely on labeled datasets, which limits their scalability as classical simulation becomes infeasible for large circuits. Although some re-cent approaches explore reinforcement learning for circuit optimization [38, 39], they typically require computing intermediate quantum states to guide gate selection. Consequently, both these supervised and reinforcement learning methods become impractical for large-scale quantum systems where classical simulation of the quantum algorithm is not feasible. To address these challenges, we introduce a dataset-free, preference-based algorithm. Specifically, this work uses direct preference optimization (DPO) [40] to update the circuit parameters by comparing the expected values of generated circuits. Unlike many supervised or reinforcement learning-based methods, our DPO-based strategy does not rely on prior labeling; it only requires the final measurement results of the generated circuits, thereby substantially reducing computational overhead.\nWhile we illustrate the potential of conditional-GQE by focusing on combinatorial optimization, our broader goal is to present a novel, scalable, and generalizable workflow for quantum circuit generation across diverse domains, which is accelerated with the help of high-performance computing systems [10]. This work is expected to support practical quantum computation in the early fault-tolerant era and advance quantum technology's real-world application."}, {"title": "Generative quantum eigensolver (GQE)", "content": "Large language models (LLMs) generate sequences of token indices, each corresponding to a word or subword, which, in turn, form a sentence together. Analogously, GQE generates quantum circuits by mapping each index to a component of a quantum circuit, such as a gate or a gate combination. The generated sequence of indices results in a composition of quantum gates, forming a quantum circuit.\nGiven a fixed initial state $|\\emptyset_{ini}\\rangle$, GQE uses classical machine learning to generate a quantum circuit $U$ that minimizes the expectation value $ \\langle O \\rangle_U := \\langle \\phi_{ini} | U^{\\dagger} O U | \\phi_{ini} \\rangle$ of an observable $O$. In many quantum computing applications, observables can be expressed as the function $O(x)$ of certain variables $x$, such as coefficients of the Ising Hamiltonian representing combinatorial optimization problems. However, similar to many VQAs, GPT-QE\u2014the original demonstration of GQE\u2014does not incorporate $x$ into the generative model but instead uses a separate model for each context, as illustrated in Fig. 1a-b. We believe that incorporating contextual inputs into the generative model can yield significantly different results compared to previous algorithms. This study presents the context-aware GQE, which aims to train a generative model with contextual inputs, generating a circuit that minimizes the energy $(O(x))_U$ in response to a given input $x$. In contrast to GPT-QE, which utilizes a decoder-only Transformer, we employ a Transformer architecture that includes both an encoder and a decoder. The details of GQE and our approach are provided in the Methods section.\nIn previous work by some of us [41], we suggested a way of training a parameterized quantum circuit $U(\\theta, x)$ depending on the variables $x$. In this algorithm, the variables $x$ are embedded into the circuit, and"}, {"title": "Conditional quantum circuit generation for combinatorial optimization", "content": "the parameters $\\theta$ are optimized so that $\\langle O \\rangle_{U(\\theta, x)}$ is minimized for each $x$. However, embedding classical data into a parameterized quantum circuit faces the challenge of expressibility [19, 20, 21], meaning that the functional form of $U(\\theta, x)$ for $x$ is severely restricted. In contrast, in GQE, we are not restricted by these expressibility issues. The variables $x$ are incorporated into the classical neural network alongside trainable parameters, and they affect non-trivially the generated quantum circuit.\nAs a very important practical application, we focus on solving combinatorial optimization problems with conditional-GQE, which we call Generative Quantum Combinatorial Optimization (GQCO). The schematic diagram of the entire workflow is shown in Fig. 2.\nCombinatorial optimization problems can always be mapped to a corresponding Ising Hamilto-nian [42], which serves as an observable. We use the Hamiltonian coefficients as input to a generator, embedding them into graph structures with feature vectors that capture domain-specific information. A graph encoder with Transformer convolution [43] then produces an encoded representation. Using this, a Transformer decoder generates a sequence of token indices that defines a quantum circuit. The solution is identified by selecting the bit sequence corresponding to the computational basis state with the highest observation probability from the generated quantum circuit. Fig. 2 presents the schematic of this process, with further details provided in the Methods section.\nCircuit component pools must be predefined, allowing for the incorporation of domain knowledge and inductive bias. For example, since GPT-QE [24] aims to search a ground state for a given molecule, the operator pool is composed of unitary coupled-cluster singles and doubles (UCCSD) ansatz [44] derived from the target molecule. In this study, we use basic 1- and 2-qubit gates (Hadamard gate, rotation gates, and CNOT gate) and the QAOA-inspired $R_{zz}$ rotational gate, i.e., an Ising-ZZ coupling gate acting on two target qubits. The target qubit(s) of each quantum gate and, if necessary, the control qubit, are available in all configurations, and there are six possible rotation angles of $\\{\\frac{\\pi}{7}, \\frac{\\pi}{14}, \\frac{\\pi}{7}\\}$ for the rotation"}, {"title": "Solving combinatorial optimization via GQCO", "content": "gates. By using basic gates rather than components suitable for many-body physics such as the UCCSD ansatz, this work aims to study whether we can train the model successfully without prior knowledge of an optimal or intuitively useful operator pool.\nWhile a detailed description of our training strategy is provided in the Methods section, we summarize it here to highlight our scalable, broadly applicable framework. Scaling circuit size is critical for fault-tolerant quantum computing; however, most prior works [36, 37] rely on supervised learning methods that struggle to produce high-quality training data at large scales. In contrast, GPT-QE employs an alternative training approach called logit matching. This method does not require any pre-existing dataset; instead, it trains the generative model to approximate a Boltzmann distribution derived from the expectation value of a given Hamiltonian. In this work, to further increase the probability around the preferred circuits beyond what is computed by the Boltzmann distribution, we use a preference-based strategy called direct preference optimization (DPO) [40]. DPO compares candidate circuits based on their computed costs and updates the model parameters to increase the likelihood of the most favourable circuit. Crucially, it relies solely on expectation values from the generated circuits, eliminating the need for labelled datasets and therefore it facilitates the treatment of large-scale quantum systems. In other words, the model is trained by exploring the space of solutions rather than relying on previously-gathered ground truth. To manage the diversity arising from different problem sizes, we introduce a qubit-based mixture-of-experts (MoE) architecture [45, 46, 47]. This module comprises specialized model sublayers called experts, and the model switches between layers depending on the number of qubits required. We further accelerate model training through curriculum learning [48], starting with smaller circuits and increasing task complexity step by step, then we proceed to fine-tune each expert for the respective problem size. Our preference-based curriculum training with MoE modules enhances the model's expressive power and scalability, facilitating the efficient integration of larger quantum circuits.\nWe trained a GQCO model capable of generating quantum circuits with 3 to 10 qubits. All computations during the training were performed on classical hardware (CPUs and GPUs), and quantum calculations were conducted using a classical simulator. Specifically, multiple NVIDIA V100 GPUs were used for the GPU computations. Details of the training and hardware are provided in the Method section and the Supplementary Information.\nFig. 3a compares the accuracy of GQCO with two other solvers\u2014simulated annealing (SA) [34] and the quantum approximate optimization algorithm (QAOA) [35] \u2014\u2014\u2014\u2014\u2014\u2014on 1,000 randomly generated combinatorial optimization problems for each problem size. After training the GQCO model, we selected the circuit with the lowest expected value among 100 sampled circuits for each test problem. SA and QAOA were initialized and performed independently for each problem; in particular, in QAOA, the circuit parameters were trained from scratch for each problem and the solution was determined based on the optimized circuit. The GQCO model consistently achieved a high accuracy of approximately 99% across all problem sizes, whereas the other two solvers gradually decreased accuracy as the problem size increased. Notably, QAOA failed to exceed 90% accuracy even for a 3-qubit task, and its accuracy declined to about 30% for a 10-qubit task. This performance drop reflects the limited expressive power and trainability of the canonical QAOA approach. Achieving over 90% accuracy with QAOA would require a much deeper parameterized circuit, making, at the current time, stable training infeasible. In contrast, GQCO addresses these limitations by leveraging the high expressive power of classical neural networks and by employing a large number of parameters on the classical side of the computation. The performance gap observed here indicates the advantages of the generative quantum algorithm approach over variational algorithms.\nA notable difference between our numerical exploration of GQCO and SA, was the runtime of the two algorithms. Fig. 3b shows the time required for each method to reach 90% accuracy. To adjust runtime, we varied the hyperparameters\u2014namely, the number of sampled circuits for GQCO, the number of sweeps for SA, and the number of iteration layers for QAOA. The total runtime includes all steps, from submitting a test problem to identifying the answer. For GQCO, this runtime encompasses both"}, {"title": "Error analysis of GQCO solutions", "content": "model inference and quantum simulation; for QAOA, it includes parameter optimization and quantum simulation. SA and brute-force calculations were performed on CPUs, while the other computations, including quantum simulation, were conducted on GPUs. The gray dashed line indicates the runtime of brute-force search, which grows exponentially with problem size; SA exhibits a similar exponential trend. In contrast, the increase in GQCO's runtime was surprisingly restrained. Although it took a certain amount of computation even for small problem sizes due to the need for Transformer inference, GQCO surpassed the brute-force method when the problem size exceeded 10 qubits. In terms of computational complexity, the brute-force method for a problem size n requires a runtime on the order of $O(2^n)$. In contrast, GQCO's complexity depends on both Transformer inference and the quantum computation of the generated circuit. The former depends on sequence length [49] (i.e., circuit depth) and scales on the order of $O(n^2)$, while the latter can potentially benefit from exponential speedup on quantum devices. In other words, GQCO can be expected to provide polynomial acceleration compared to brute-force, though GQCO does not guarantee to reach 100% accuracy. It is important to note that, in this performance evaluation, the quantum computations were performed using a GPU-based simulator, so any speedup that could be gained from a quantum approach would not be present in any of these results. Nevertheless, a clear reduction in the growth rate of the runtime even for these classical simulations observed.\nFig. 3c illustrates the detailed relationship between runtime and accuracy. Generally, performance improved as execution time increased. However, for QAOA, increasing the number of training iterations did not consistently enhance the performance of the algorithm, especially with an increasing number of qubits. This behaviour is attributed to the training difficulties inherent in VQAs. In contrast, GQCO outperformed the other solvers, demonstrating greater performance gains as the execution time grew. This advantage arises from the processing power of GPUs, which enables additional samplings at little additional wall-clock cost, thereby boosting performance.\nBecause the runtime baseline depends on a device's computational power, the problem size at which the advantage emerges may differ across devices. However, the difference in computational complexity is independent of the device used.\nDuring the performance evaluation, we identified one incorrect answer in each size-3 and size-4 problem set and two in the size-5 problem set. Fig. 4 shows the corresponding coefficient matrices, generated quantum circuits, and resulting quantum states. As mentioned above, we sampled 100 circuits for each problem, with the circuit yielding the lowest expected value among them identified as the GQCO answer. Note that lower-cost configurations may exist outside this finite sampling; indeed, for the four problems in Fig. 4, sampling 100 circuits alone did not produce a correct solution. In each case, the second-best solution had a cost that was very close to the optimal value, causing the GQCO model to become trapped in a near-suboptimal solution. This likely occurred because the Transformer cannot fully capture the discrete nature of combinatorial optimization, where slight fluctuations in the Hamiltonian coefficients can lead to discontinuous changes in the solution. Increasing training time or using more"}, {"title": "Characteristics of generated circuits and limitations of GQCO", "content": "precise floating-point calculations may help in reducing such errors. Another effective approach is to increase the number of circuit samplings; indeed, the four problems in Fig. 4a were correctly solved by GQCO when 1600, 300, 400, and 700 circuits were sampled, respectively. In natural language processing, the inference scaling law [50, 51, 52] states that increasing the inference time improves the quality of model outputs. A similar phenomenon appears to apply to quantum circuit generation as well. However, because generative models are inherently stochastic, theoretically guaranteeing perfect accuracy for circuit generation remains challenging.\nExamining the structure of circuits generated by GQCO offers insight into how GQCO solves problems. Fig. 5a-b show the average circuit depth and the number of CNOT gates for circuits generated by the GQCO model and a single-layer QAOA circuit, respectively. Both values were obtained after transpiling the circuits using Qiskit [53] with optimization level 1 (light optimization). Notably, the GQCO-generated circuits are shallower and include fewer CNOT gates than those produced by QAOA. Because the GQCO algorithm does not use the Hamiltonian directly in its circuit design, it does not require the extensive entanglement operations that QAOA does.\nIn this work, we did not impose an explicit restriction on the number of CNOT gates, although the maximum circuit depth for GQCO was set to twice the number of qubits. Certainly, the cost function and model structure are flexible enough to incorporate additional constraints on circuit depth or CNOT gate count. Further constraints that lead to shallower circuits could help generate circuits more robust to noise. Furthermore, the model can address device-related constraints. Because many quantum devices have restricted physical connectivity [54, 55], compilation is often needed to map circuits onto the hardware. However, GQE-based quantum circuit generators can bypass this process by excluding gates that do not satisfy the device's physical constraints. This flexibility in generating hardware-efficient circuits is a key advantage of the GQE-based approach.\nFig. 5 shows a typical 3-qubit quantum circuit generated by our GQCO model. In this circuit, six gates are used to transform the initial state $|000\\rangle$ to state $e^{-1} |001\\rangle$. Notably, the three successive $R_\\gamma(\\frac{\\pi}{3})$ gates placed in the middle of the circuit are primarily responsible for obtaining the final state.\nThe matrix representation of the composition of three $R_\\gamma(\\frac{\\pi}{3})$ gates is given by $\\begin{bmatrix}0 & -1\\\\1 & 0\\end{bmatrix}$, which corresponds to a bit flip from $|0\\rangle$ to $|1\\rangle$ (or from $|1\\rangle$ to \u2013$|0\\rangle$). The remaining three gates (one $R_z$ gate and two $R_{zz}$ gates) only change the global phase for the computational basis states and have no direct effect on the final solution. These observations suggest that GQCO differs substantially from quantum-"}, {"title": "Solving combinatorial optimization using a quantum device", "content": "oriented methods such as QAOA in that the GQCO model does not acquire a quantum mechanics-based logical reasoning capability. Instead, much like many classical machine learning models, GQCO appears to generate circuits by interpolating memorized instances. GQCO's circuit-generation ability relies on a data-driven approach rather than any logical understanding of quantum algorithms. However, it should be noted that combinatorial optimization problems are not inherently quantum, and the ground truth is represented by one of the computational basis state. Because such problems can be solved with simple bit-flip circuits, it is natural that GQCO converges on generating such trivial circuits. For more distinctly quantum tasks, such as molecular ground state searches, conditional-GQE is expected to acquire the ability to generate physically interpretable circuits.\nAll of the circuits generated during the performance comparison (Fig. 3) are non-Clifford circuits and are generally expected to be difficult to simulate classically. However, as noted above, many of these circuits primarily perform bit flips. If we remove gates that affect only the global phase (e.g., the first $R_{zz}$ gate in Fig. 5c), most GQCO-generated circuits become Clifford, allowing them to be classically simulable. Consequently, our findings do not demonstrate a quantum advantage [54] or the quantum utility [55] of GQE-based circuit generation. Nevertheless, even if the trained model produces circuits that can be classically simulated, non-Clifford circuits are still generated during training. In other words, the entire circuit space\u2014including circuits that are computationally hard to simulate classically\u2014must be explored to obtain the trained model, highlighting the benefits of incorporating quantum computation into the overall workflow. Moreover, for applications beyond combinatorial optimization, solutions often involve more complex quantum states, and the circuits generated by the trained model are expected to be classically unsimulable. Since our model can be trained without explicitly determining whether the generated circuits are classically simulable, the GQCO workflow applies equally well to problems that rely on superposition or entanglement. However, training generators for such problems entails a more intricate cost landscape, making it challenging to train using simple gate pools or vanilla DPO loss, as done in this study. Future research focusing on more carefully designed workflows would therefore be promising.\nAt the end of the performance analyses of GQCO, we examined its behavior on a physical quantum device. The target problem was the 10-variable max-cut problem illustrated in Fig. 6a. For comparison, we used a two-layer QAOA circuit whose parameters were optimized with a classical simulator. The resulting circuits (Fig. 6b) were then executed on the IonQ Aria quantum processor. Fig. 6c presents the sampling results for varying numbers of shots alongside the state vector computed by the classical simulator.\nA key characteristic of GQCO-generated circuits is that resulting quantum state exhibits a distinct observation probability peak at a single computational basis state. In contrast, because QAOA discretely approximates time evolution from a uniform superposition, its resulting quantum state is more complex and less likely to yield a clear peak, particularly when the circuit depth is limited. Consequently, QAOA required more than 100 shots to identify the correct answer in this study, whereas GQCO was able to find it with just a single shot. This disparity in the number of required shots is expected to grow as the number of qubits increase.\nAnother notable aspect of GQCO becomes evident in cases where the ground state is degenerate. In principle, our GQCO model cannot account for degeneracy because the training process relies solely on circuit sampling, focusing on identifying a solution without considering the underlying quantum me-chanics of the input Ising Hamiltonian. In a max-cut problem, the system is inherently degenerate, particularly the target problem in this section is doubly degenerate. As illustrated in Fig. 6c, while GQCO identified only one of the two solution candidates, QAOA exhibited observation probability peaks for both degenerated ground states. Originating in adiabatic quantum computation, QAOA is theoret-ically expected to yield a non-trivial probability distribution over degenerate ground states. GQCO's inability to capture degeneracy will require future work on model architectures and training approaches that incorporate quantum mechanical principles."}, {"title": "Discussion", "content": "We developed a novel quantum-classical hybrid technique for context-aware quantum circuit gener-ation and then applied it to combinatorial optimization problems. Our approach, which we have named conditional-GQE, extends GQE by integrating contextual encoding, and employs the cutting-edge methodologies such as DPO-based training and qubit-based curriculum learning to yield a scalable workflow. This strategy enabled us to successfully build the quantum circuit generator for combinatorial optimization, a high-performance solver that outperforms conventional solvers for problems with up to 10 variables. Although this work is still a prototype, the results suggest the potential for more prac-tical, larger-scale implementations toward foundation models of combinatorial optimization. Moreover, we highlight the capacity of classical neural networks to generate flexible, high-quality quantum circuits, paving the way for advanced quantum-classical hybrid technologies.\nThe conceptual workflow described in this study can be extended beyond combinatorial optimization to any problem formulated as observable expectation value minimization. For example, in molecular ground-state searches, representing molecular structures as graphs allows direct adoption of our graph-based encoding. By replacing the encoder, the GQE-based approach also generalizes to quantum machine learning and partial differential equation solvers. We thus view GQE-based quantum circuit generation as a next step following VQAS.\nHowever, several limitations remain as open problems. A major obstacle is the significant classical computational resources required to achieve scalability. While our findings indicate the computational advantage over classical solvers, fully realizing this advantage demands extensive classical training be-forehand. Overcoming these challenges will likely involve designing an encoder architecture guided by domain knowledge, refining training strategies (including developing pre-training methods), and using the high-performance computing resources [10]. Additionally, careful gate pool design will play a crucial role. Machine learning-based approaches for identifying suitable gates or gate representation learning may offer a promising direction.\nThis research provides a novel pathway for quantum computation by leveraging large-scale machine"}, {"title": "Methods", "content": "learning models. It underscores the growing role of AI in the advancement of next-generation quantum computing research activities. We believe that our work will serve as a catalyst for accelerating the development of quantum applications across diverse domains and facilitating the democratization of quantum technology."}, {"title": "GQE, GPT-QE, and conditional-GQE", "content": "Let $t = \\{t_1,..., t_N \\}$ be a generated token sequence of length $N$, where each token index $t_k$ is an integer satisfying $1 \\le t_k < V$, with $V$ being the vocabulary size. Each token index $t_k$ corresponds to a quantum circuit component $U_k$ selected from a predefined operator pool $\\{U_l\\}_{l=1}^V$. These components collectively form a quantum circuit $U = U_{t_N} \\dots U_{t_1}$. Let $p_\\theta(U)$ denote the generative model of quantum circuits, where $p_\\theta(U)$ is a probability distribution over unitary operators $U$, and $\\theta$ is the set of optimizable parameters. In GQE, the parameters $\\theta$ are iteratively optimized so that circuits sampled from $p_\\theta(U)$ are more likely to minimize the expectation value of an observable:\n$\\langle O \\rangle_U := \\langle \\phi_{ini} | U^{\\dagger} O U | \\phi_{ini} \\rangle,$\nwhere $O$ is an observable and $|\\phi_{ini} \\rangle$ is a fixed input state. In particular, for an $n$ qubit system, we use $|\\phi_{ini} \\rangle = |0\\rangle^n$. The quantum computation is involved in the estimation of $\\langle O \\rangle_U$. Notably, unlike in VQAs, all optimizable parameters are embedded in the classical generative model $p_\\theta(U)$ rather than in the quantum circuit itself (see Fig. 1b).\nAs discussed in the Result section, the observable can be expressed as a function of certain variables $x$. Let us denote such an observable as $O(x)$. The quantum circuit $U$ that minimizes $\\langle O(x) \\rangle_U$ also depends on the variable $x$. In original GQE approach (including GPT-QE), parameters are set and optimized for each specific target problem, much like in VQAs. More precisely, GPT-QE aims to obtain a decoder-only Transformer $P_{\\theta^*(x)}(U)$ for each $x$, where $\\theta^*(x)$ is the solution for the following minimization problem:\n$\\theta^*(x) = \\underset{\\theta}{\\text{arg min}} \\mathbb{E}_{U \\sim p_\\theta(U)} \\langle O(x) \\rangle_U,$", "latex": ["\\langle O \\rangle_U := \\langle \\phi_{ini} | U^{\\dagger} O U | \\phi_{ini} \\rangle", "\\theta^*(x) = \\underset{\\theta}{\\text{arg min}} \\mathbb{E}_{U \\sim p_\\theta(U)} \\langle O(x) \\rangle_U"]}, {"title": "Direct preference optimization", "content": "where $\\mathbb{E}_{p(X)}[f(X)]$ denotes the expectation value of $f(X)$ with respect to the random variable $X$ over the sample space $\\Omega$, where $X$ is drawn from the distribution $p(X)$, i.e., $\\mathbb{E}_{X \\sim p(X)}[f(X)] := \\int_{\\Omega} f(X) p(X) dX$. By utilizing $x$ as context (i.e., input), conditional-GQE aims to train a generative model $p_\\theta(U|x)$ that generates circuits minimizing $\\langle O(x) \\rangle_U$. The function $p_\\theta(U|x)$ provides the conditional probability of generating the unitary operator $U$ when the input $x$ is given. In Transformer-based generative models, the probability $p_\\theta(U|x)$ is expressed as follows:\n$p_\\theta(U|x) = \\prod_{i=1}^N p_\\theta(U_{t_i} | U_{t_0}, ..., U_{t_{i-1}}, x) = \\prod_{i=1}^N \\frac{\\text{exp} \\{ z_i(U_{t_0}, ..., U_{t_{i-1}}, x; \\theta) / T \\}}{\\sum_{l=1}^V \\text{exp} \\{ z_i(U_{t_0}, ..., U_{t_{i-1}}, x; \\theta) / T \\}},$\nwhere $t_0$ is the start-token index, chosen such that $U_{t_0} = \\mathbb{I}^n$ in this work. $z_i$ denotes the logit for $i$-th token, that is, the corresponding output from the model before applying the sigmoid function. $T$ is the sampling temperature; in this work, we set $T = 1.0$ for training and $T = 2.0$ for evaluation, thereby enhancing randomness in the evaluation phase. Then, we can realize a generative model $p_{\\theta^*}(U|x)$ through the following optimization:\n$\\theta^* = \\underset{\\theta}{\\text{arg min}} \\mathbb{E}_{x \\sim p(x)} \\mathbb{E}_{U \\sim p_\\theta(U|x)} \\langle O(x) \\rangle_U,$\nwhere $p(x)$ denotes the probability distribution of inputs $x$ in the target domain.\nSolving the optimization problems in Eq. (1) or Eq. (2) is challenging and thus requires surrogate objective functions. GPT-QE employs a logit-matching approach, whereas this study utilizes DPO [40] loss. Further details of DPO are provided in the subsequent section."}, {"title": "Qubit-based mixture-of-experts", "content": "Traditionally, supervised learning with labeled dataset has been widely used in machine learning appli-cations to quantum information processing. Examples include quantum circuit compilation [38, 63, 39], ansatz generation for VQAs [64, 65], and diffusion-based quantum circuit generation [36]. However, this approach faces some significant challenges. The most notable issue is the scalability. Preparing training data using classical computation quickly becomes infeasible for large-scale circuits exceeding 50 qubits. Furthermore, for complex tasks, it is difficult to prepare ground-truth circuits, or the circuit structure may not be uniquely defined. Consequently, preparing high-quality training datasets remains problematic.\nDepending on the number of variables in a combinatorial optimization problem, quantum computation requires a corresponding number of qubits. To effectively handle the diversity of tasks resulting from variations in problem size, we employ the mixture-of-experts (MoE) architecture [45, 46, 47], which is commonly used in LLMs. As illustrated in Fig. 2 and Fig. 7b-c, each feed-forward module in the model is partitioned into specialized submodules, referred to as \"experts.\" The gating mechanism dynamically selects layers based on the number of qubits, forming what we term a qubit-based MoE. This design balances the need for diverse model representations while limiting the growth of model parameters."}, {"title": "Curriculum learning", "content": "The qubit-based MoE enhances model scalability. By incorporating additional experts and restarting training, circuit generators can be trained for varying numbers of qubits without the need to retrain the entire model from scratch. Leveraging this capability, we adopt curriculum learning [48], a method that incrementally increases task complexity by starting with simpler problems and gradually progressing to more challenging ones.\nTraining begins with randomly generated combinatorial optimization problems involving 3 qubits. Performance is monitored regularly, and training continues until the model achieves an accuracy exceeding 90% on randomly generated test problems. Once this threshold is met, size-4 optimization problems are introduced as training candidates, along with the integration of a new expert module within the MoE layers. Then, performance is continuously monitored, and the maximum problem size in the training dataset is gradually increased.\nEven when the maximum problem size is $n_{max}$, problems involving fewer qubits ($< n_{max}$) are still generated as part of the training data. The probability of generating a problem of size $n$ when the current maximum size is $n_{max}$ is defined as:\n$p(n | n_{max}) = \\begin{cases}0 & \\text{if } n \\leq n_{max} < 3 \\\\\\frac{1}{N_{max}-3} & \\text{if } 3 < n < n_{max},\\\\\\frac{0.5}{N_{max}-3} & \\text{if } 3 < n < n_{max}.\\end{cases}$", "latex": ["p(n | n_{max}) = \\begin{cases}0 & \\text{if } n \\leq n_{max} < 3 \\\\\\frac{1}{N_{max}-3} & \\text{if } 3 < n < n_{max},\\\\\\frac{0.5}{N_{max}-3} & \\text{if } 3 < n < n_{max}.\\end{cases}"]}, {"title": "Hardware configuration", "content": "In brief, when the number of qubits is larger than three, the probability of generating the largest problem size is 0.5, and the remaining 0.5 probability is equally divided among all smaller sizes. Notably, the probability of generating previously trained problem sizes is not set to zero. This strategy mitigates catastrophic forgetting [6"}]}