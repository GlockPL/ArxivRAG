{"title": "Implicit assessment of language learning during practice as accurate as explicit testing", "authors": ["Jue Hou", "Anisia Katinskaia", "Anh-Duc Vu", "Roman Yangarber"], "abstract": "Assessment of proficiency of the learner is an essential part of Intelligent Tutoring Systems (ITS). We use Item Response Theory (IRT) in computer-aided language learning for assessment of student ability in two contexts: in test sessions, and in exercises during practice sessions.\nExhaustive testing across a wide range of skills can provide a detailed picture of proficiency, but may be undesirable for a number of reasons. Therefore, we first aim to replace exhaustive tests with efficient but accurate adaptive tests. We use learner data collected from exhaustive tests under imperfect conditions, to train an IRT model to guide adaptive tests. Simulations and experiments with real learner data confirm that this approach is efficient and accurate.\nSecond, we explore whether we can accurately estimate learner ability directly from the context of practice with exercises, without testing. We transform learner data collected from exercise sessions into a form that can be used for IRT modeling. This is done by linking the exercises to linguistic constructs; the constructs are then treated as \"items\" within IRT.\nWe present results from large-scale studies with thousands of learners. Using teacher assessments of student ability as \"ground truth,\" we compare the estimates obtained from tests vs. those from exercises. The experiments confirm that the IRT models can produce accurate ability estimation based on exercises.", "sections": [{"title": "1 Introduction", "content": "An important goal in intelligent tutoring systems (ITS) is to support the learner's progress through personalized tutoring. ITSs have been shown to be effective in various subject do- mains, [1-3]. We present work in a sub-area of ITS, namely on computer-aided language learning (CALL), used in a real-world learning environment. We present experiments with the CALL system Revita, [4, 5]. This work is the result of a collaborative international effort, involving teachers who use the system in their curricula at several universities; the approach is targeting students who have passed beyond the beginner level.\nThe main aspect of Revita's approach to CALL is that the intelligent tutor supports the student during out-of-class practice. The student may attend lectures, receive various learning content from the teacher, etc.; the tutor engages the student in practice, to leverage the student's time outside the classroom toward improving mastery.\nA central requirement for personalized tutoring in ITS and CALL is accurate assessment of the current proficiency of the learner. Assessment is needed for two main reasons: the \"external\" and the \"internal.\" Externally, assessment is needed to communicate to the learner- and to the teacher-what the learner has/has not mastered. This supports planning future lectures, individual attention, etc. More crucially, internally, the assessment drives the choice of exercises presented to the learner during practice. The ITS contains an Instruction Model, which is the component responsible for selecting the most appropriate exercises for the learner, given the learner's current state of proficiency.\nIn the widely-accepted Vygotskian theory of educational psychology, the Zone of Proximal Development (ZPD) is defined as those skills in the learning domain, which the learner is best prepared to learn next, see Figure 1 [6, 7]. Skills falling outside the ZPD are either already mastered, or they are too far beyond the learner's reach at present. If the exercises offered by the tutor to the learner focus excessively on skills already mastered-it will bore the learner. If we focus excessively on skills far out of reach-it will frustrate the learner. In either case, the learner will be demotivated and is likely to quit learning. Therefore the ITS should concentrate its efforts on finding the ZPD, to make sure that most of the time the exercises are exactly at the right level. Identifying the ZPD requires accurate assessment of the learner's ability.\nWe must distinguish from the outset two different contexts (environments) in which the learner can work with the tutor: test sessions vs. practice sessions with exercises. In this learning setting, we will explore three types of assessment: (A) assessment via exhaustive test,"}, {"title": "2 Prior work", "content": "Learning analytics:\nSeveral approaches for modeling learning are currently in use. Bayesian Knowledge Tracing (BKT) attempts to model student proficiency as a latent variable in a Hidden Markov Model, which is updated by observing learner responses to test items [8]. Surveys of BKT research may be found, e.g., in [9, 10]. BKT variants train separate parameters for initial knowledge and learning; they also model \"guess\" and \"slip\": guess refers to when the student answers a question correctly although she has not mastered the skill; conversely, a slip happens when the learner has mastered a skill, but gives a wrong answer accidentally. BKT models may estimate guess and slip for each skill; or may compute them each time a student attempts to answer a new question, based on learned models of guess and slip, e.g., [11]. We turn to these topics in Section 4.2.2.\nKnowledge Space Theory (KST), models the domain as a \"knowledge space\" a graph of knowledge states, in which a student may be at the present time [12]. Each state contains a subset of the skills in the domain, mastered by the student so far. The student has mastered the domain once she reaches the state containing all skills. KST models not only the student knowledge, but also learning paths, starting from the empty set toward the full set of concepts. One approach to building a knowledge space is Formal Concept Analysis (FCA) [13].\nItem Response Theory (IRT) is a popular approach to modeling the student's state of ability, [14]. IRT is applied in many settings (besides ITS), including stress testing, psychological and medical testing, etc. Depending on the specific application domain, the latent trait under consideration can encompass various factors such as anxiety, neurosis, authoritarian personality, and so on. Several studies have focused on the application of Item Response Theory (IRT) in the field of language learning. For instance, these studies have involved modeling language proficiency using test data [15], validating the reliability and difficulty of language tests using IRT [16], and measuring the difficulty of Thai language topics for learners [17]. Furthermore, Polat [18] conducted a comparison of language proficiency performance measures based on test data using both IRT and Classical Test Theory (CTT), with IRT demonstrating superior capabilities in modeling language skills. We apply IRT to language learning, where the latent trait is the learner's ability, or language proficiency. IRT has an information-theoretic basis; in that it is related to Elo rating systems of ability [19]-the Elo formulas have been adapted to the context of ITS, [20]. The language-learning domain is much more complex than many domains where IRT is used, because the learning constructs to be mastered by the learner are relatively numerous, and have complex relationships among them. A linguistic construct"}, {"title": "3 Data", "content": "This work builds on learner data collected through a collaborative effort with language teachers at several universities, with learners at various levels on the CEFR scale, ranging from Al to C2, [24]. The platform collects learner data from several languages and multiple contexts: grammar exercises, vocabulary exercises, tests, etc. Here we focus on data collected from tests and grammar exercises from learners of Russian as a second language (L2).\n3.1 Learner data from tests\nSeveral thousand students took many exhaustive tests. Each test session consists of items sampled from a bank of 3390 multiple-choice questions. The question bank is compiled by experts in language pedagogy. Each question is linked to one of a set of linguistic constructs (140 at present); which constructs are involved in each question is also determined by the experts. Each teacher can define a template, selecting which constructs to test (or use a \"default\" template with 300 questions). All of these items are dichotomous: a learner's response to an item is either correct or wrong.\nThe experiments we describe use a data set of over 750 000 responses to test items from approximately 1800 learners. For each question, we record to which construct the question relates, whether the answer was correct, and the timestamp.\nManual assignment of difficulty and proficiency:\nFor each item (test question) the pedagogy experts assigned a difficulty score on the CEFR scale for the linguistic constructs linked to the item. Also, the teachers assigned a CEFR level of proficiency to over 200 students, based on overall performance in their language courses.\nThe estimates of difficulty made by the experts for many of the questions in the item bank were considered questionable by other experts. In our experiments, we aim to evaluate the quality of the judgements of difficulty and proficiency by comparing them with the estimates from models trained on learner data (RQ3)."}, {"title": "3.2 Learner data from exercises", "content": "All exercises are automatically generated by the CALL system, Revita, based on authentic texts chosen by the learners from arbitrary sources on the web. The system creates a number of exercise types; here we focus on fill-in-the-blank (\"cloze\") and multiple-choice exercises. In a cloze exercise, the system hides certain words or phrases, and the learner receives the lemma (dictionary form) of the hidden word or phrase as a hint. The learner's task is to insert the correct surface forms, based on the context of the cloze. In a multiple-choice exercise, the learner is given several options to choose from, the options are generated automatically.\nThe learner can make multiple attempts to answer each exercise. In case of an incorrect answer, on subsequent attempts the system gives hints, which gradually guide the learner toward the correct answer. The hints start out as more general and become progressively more specific on subsequent attempts. Each exercise is attached to one or more linguistic constructs.\nThe system analyzes the student's response, the requested hints and the number of attempts to answer to compute the performance of each linguistic construct. In general, a student's answer may be correct with respect to some constructs, but wrong with respect to others; for example, the student may have entered a correct tense for a verb but an incorrect person.\nWe have collected 214K exercises, done by approximately 1500 students. The exercises are linked to over 200 linguistic constructs, [4]. We have over 50 students, who have done over 100 exercises and who have their CEFR levels assessed by their teachers. We evaluate the quality of the model trained on all students, by comparing the estimated proficiency of these students with their teacher-assessed CEFR levels."}, {"title": "3.3 Challenges relating to the data", "content": "Tests:\nAlthough the exhaustive test is developed by experts in language pedagogy, it is problematic in several aspects. First, the test is too long. Students reported being stressed and frustrated by the end of the test. Second, the test originally allowed only 15 seconds per question, which is too short, especially when the examinee is at the lower CEFR levels. Because of these problems, the data which we collected from the exhaustive test may give a skewed reflection of students' true proficiency. This poses a challenge-can robust models for testing be trained on this imperfect learner data (RQ2) ?\nExercises:\nEvaluating the exercises is also very challenging. The challenge comes with assignment of credit and penalty to the student answers. In terms of assigning credit and penalty, a major challenge is that language constructs are not directly judged, as test questions are. Each test question corresponds in a clear way to one item, and assigning credit/penalty is straightforward for test questions: it is unambiguous and there is a clear judgment of the answer-correct or wrong. In the case of exercises, the credit standard is not clear because the link from exercise to linguistic constructs is one-to-many. This requires a more sophisticated approach to credit and penalty assignment. We will discuss our approach in more detail in Section 4. Overall, these challenges pose a similar question: can we use the data (possibly noisy) from exercises to construct robust models (RQ4)?"}, {"title": "4 Method", "content": "4.1 Item Response Theory\nItem Response Theory (IRT) provides ways to evaluate and compare the difficulty of question items and the proficiency of a learner-which in our setting should be mapped to the CEFR levels.\n3PL model: Since our test items are multiple-choice questions with one correct answer, we focus on the 3PL variant (three-parameter logistic model) of IRT [14]. In 3PL, we model the probability that a student s whose current ability estimate is 0s will give a correct answer to Qi- Question item i. The probability function is expressed as:\n$P(0_s, Q_i) = c_i+ (1 - c_i) \\cdot \\frac{1}{1+exp(-a_i(0_s - b_i))}$\nwhere the parameters-the properties of Qi-are:\n\u2022 a\u2081: discrimination factor,\n\u2022 b\u2081: estimate of difficulty,\n\u2022 ci: probability that a random guess is correct.\nInformation Function: IRT defines two types of information functions: item information and test information. Item information measures the amount of information a question Qi yields, based on the learner's current ability estimate 0s. The Information function is used during the adaptive test to select the most informative item, for the given value of ability 0s. It is computed as:\n$I(0_s, Q_i) = a_i^2 \\frac{1 - c_i}{P(0_s, Q_i)} [\\frac{P(0_s, Q_i) - c_i}{1 - c_i}]^2$\nTest information is the sum of information over all items. It provides a measure of the informativeness of the test as a whole; i.e., it measures how accurate the test result will be.\n4.2 Test Analytics\nWe next describe the experimental setup: evaluating the effectiveness of IRT under various conditions.\n4.2.1 Test simulations\nWe simulate the adaptive test, to check the effectiveness of IRT trained from learner data-for testing future unseen learners.\nThe Simulation procedure: iteratively selects successive question items as follows:\n1. Choose true this is the true ability of the learner (hidden from the model).\n2. Initialize On, an estimate of ability true. For time step n = 0, the initial 00 is drawn from the normal distribution with \u00b5 = 0, \u03c3 = 0.5; i.e., initially the student is \"about average.\"\n3. Item selection: given the current value of ability On, find the next \"best\" question-the most informative item Qn according to Equation 2.\n4. \"The learner answers\u201d the selected question at time n: in the simulation, the probability of a correct answer is sampled using the true ability Otrue and Equation 1."}, {"title": "4.2.2 Slip and Compensation", "content": "Slip: Although Equation 1 includes the notion of a lower bound for random guess, the simulation procedure does not include the possibility of a slip, which may happen randomly, even if the learner knows the correct answer. Therefore, we introduce an extra step for slip when a correct learner answer has been sampled from Equation 1, in step 4.\nResearch has shown that a participant's willingness or capability to attend to the sequence of test questions is an important contextual factor: motivated participants may process information more deeply [25-27]. This suggests that a slip is related to the examinee's attitude toward the test, and it may be unrealistic to estimate the probability of a slip \u025bslip. In our experiments, we expect Eslip to be fairly low-we assume that the student is trying to concentrate on the test to the best of her ability. In our simulations, we use Eslip = 0.05.\nExploration: We expect that the presence of slips will slow down the adaptive test process. The mechanism of IRT will try to compensate for the \"drop\" in ability estimate 0 and will select questions at a lower difficulty, which will lead to a longer test. To speed up the process, we introduce a capacity for exploration before step 3 (item selection). With probability &expl, we add a factor \u03b1 \u03c4\u03bf \u03b8\u03b7, according to its trend over the previous several time steps: if the trend is downward, a is negative, and if it is upward, a is positive. In our simulations, the probability of exploration &expl = 0.2.\nImplementing exploration requires a reliable estimation of the trend in theta over the recent N iterations. Therefore, we do not apply exploration at the beginning of the test (since it takes some time to establish a trend).\nOn the other hand, we see, for example, in Figure 7 that after 25 (or more) iterations the adaptive test may already be quite close to the true target estimate of ability.\nTherefore, as the trend becomes flat, we believe we should stop exploration. The goal of exploration is to speed up the convergence, not to delay the convergence. But we expect that"}, {"title": "4.2.3 Termination Criteria", "content": "We explore how termination criteria affect the estimation at the end of the simulation. We mostly follow the termination criteria evaluated in [29], although our context and setup are somewhat different. [29] performed a comprehensive evaluation over four criteria: fixed-length, Standard Error of Measurement (SEM), change in 0, and remaining minimum information. They also evaluate combinations of these criteria. Inspired by this work, we apply the first three criteria, as the fourth one did not perform well in their evaluation.\nFixed-length: The simplest approach is to terminate a simulation after a fixed number of iterations. This criterion can give a general idea of the performance of the test, when the rest of the test does not offer more information and the predicted 0 is reasonably close to the true level. This also serves as a baseline for the other termination criteria.\nSEM: This variable-length termination criterion depends on the confidence of 0. The iterations will go on until the test reaches a level of confidence, which is measured by standard error- the variance of the ability estimate in the 3PL model. It can be estimated as the reciprocal of the test information function at the current ability estimate 0:\n$SEM(0) = \\sqrt{\\frac{1}{I(0)}} = \\sqrt{\\frac{P(0, Q_i) [1 - P(0, Q_i)]}{\\sum a_i^2 \\frac{1 - c_i}{1 - c_i} [P(0, Q_i) - c]^2}}$\nEarlyStop: This variable-length termination criterion is based on the change in 0. We expect 0 to change more at the beginning of the test, when each question offers much information about"}, {"title": "4.3 Exercise Analytics", "content": "Alongside with experiments above on adaptive and exhaustive test data, as mentioned in the Introduction, we aim to explore how much information about the student's ability can be inferred from her performance of exercises-collected over practice sessions, without explicit testing. A good teacher can form reliable estimates of the student's ability without explicit testing; the AI tutor aims to do the same.\nThe key question is: to apply IRT, we need a notion of an \u201citem\u201d in the exercise setting. The item cannot be an exercise-since all students work with different texts, chosen by them individually, the exercises they receive are also different!\nTo model the exercise learner data, we propose to consider the linguistic constructs as the items for the IRT model. To calculate the performance of students on the constructs, we need to analyze the learner's answers and compare them to the expected answers. Each exercise involves more than one construct in general. When a learner answers an exercise, she may get some of the constructs correcte.g., the tense of the verb and other incorrecte.g., the person of the verb. We assign credit for the constructs that the learner answered correctly, and penalty for constructs for which the expected answer and learner's answer differ.\nNote: the hints and feedback that the learner requests are also linked to constructs: \"use perfect tense,\u201d \u201cuse second person,\" etc. We also penalize those linguistic constructs that are linked to hints that the learner requested: since the learner has asked for a hint, we can assume that she needed help-has not yet mastered the corresponding construct. Based on the counts of credits and penalties for each linguistic construct, we define the performance of every student S on every construct C as the rate of correctness of S's answers on C.\nWe assign the guess factor as 0.0 to \"cloze\" exercises, and 0.25 to \"multiple-choice\" exercises."}, {"title": "5 Experiments and results", "content": "The main question for the simulations is whether the IRT models trained on our learner data work, and if they work, what do the simulations reveal? We explore these questions next."}, {"title": "5.1 Experiments with test data", "content": "5.1.1 Estimation of student ability\nTo evaluate the ability estimates that the IRT-based test provides in real life, we conduct a simulation with test data from 200+ real learners. These learners were assigned by the teachers manually to a CEFR level (A1 to C2), and each learner took the exhaustive test at least once (300+ questions).\nThe exhaustive test follows a fixed template: the questions in the test are not necessarily the most informative-as the IRT-based adaptive test would select. Therefore, in the simulation, rather than selecting the most informative question overall, we select the most informative question only from among those for which we have an actual response from the learner. Lastly, we limit the length of this simulation to a maximum of 100 steps.5\nFigure 2 and Figure 3 show the results with real learner data. Figure 3 shows the distribution of the test lengths-the vast majority of the tests converge in 60 questions or less. Figure 2, shows very strong correspondence between the IRT model's prediction and the teachers' CEFR judgements. This matches with the results from the artificial simulations, and confirms that the IRT-based adaptive test is effective-much shorter test length and accurate its predictions of ability correspond well with the teachers' judgements (RQ1).\nNote that the range of the observed ability scores over the students in this group is [-3,3]. Although examinees with a higher CEFR score have a higher ability estimate, Figure 2 shows a less clear boundary between some levels, especially levels A2 and B1 show some overlap. This may be because our examinees are not evenly distributed across the CEFR levels. At levels B1 and B2 in our dataset has fewer learners-23 and 14 examinees, respectively; larger numbers of examinees at A1 and A2, with 87 and 55 examinees, respectively. We can expect clearer boundaries between levels when more data are collected and more examinees are labeled with CEFR levels."}, {"title": "5.1.2 Estimation of question difficulty", "content": "The next simulation compares the quality of A. the parameters of question items learned from learner data-specifically the difficulty level b\u2081 of each item-against B. the experts' judgements of the difficulty of the items. Each question was judged individually on the CEFR scale by expert teachers, as mentioned in Section 3.\nIn this simulation, rather than using the parameters learned by IRT from data- discrimination a and difficulty b-we apply a\u2081 = 1 uniformly, and replace b; with the experts' judgement of the difficulty of question i.\nThe expert judgements of difficulty are mapped from the CEFR scores to the difficulty scale as follows. We take the range of the difficulty values b learned from data for all items- from the minimum to the maximum-and split this range of difficulties evenly into 6 bins of equal width. The CEFR level of the question (assigned manually by experts) is mapped to the center of each of the 6 bins on the difficulty scale.\nFigures 4 and 5 show the results of this simulation. The examinees' true CEFR levels correspond much less to ability predicted by using \"manual\" estimates of difficulty, compared to Figure 2, where the means of the boxes increase linearly with ability. Also, Figure 5 shows that the mean length of the test is much higher, uniformly distributed, and many tests fall into the rightmost bin, which is the maximum number of iterations allowed in this simulation-these tests were forced to terminate at 100 iterations, and may not have converged.\nLastly, to give the \"full benefit of the doubt\" to the exhaustive test, we apply the IRT model to the entire session (containing answers to 300 questions), rather than using item selection according to Equation 2, and visualize the result in Figure 6. The result is similar to Figure 4. The correspondence is still far worse than in Figure 2. This indicates that the question levels labeled by experts are not nearly as accurate as the item parameters learned from data (RQ3). Further, as mentioned, the size of the exhaustive test was 300-five times longer than most of the adaptive test sessions need to converge. Figures 6, 4 and 5 show that although the exhaustive test process was imperfect, IRT was still able to use learner data collected from"}, {"title": "5.1.3 Slip and exploration in adaptive test", "content": "As mentioned in Section 4.2.2, the IRT-based adaptive test is affected by random slips and exploration. We investigate the exploration parameter settings in Appendix A.\nFigure 7 shows simulations conducted with \"artificial\u201d learners, free of slip and exploration. At each iteration, we sample an item from the top 5 most informative questions according to Equation 2. Each artificial learner is placed at one of 5 \u201ctrue\u201d proficiency levels, which are represented by the solid horizontal lines; each true level is represented by its own color. On each level, we simulate 3 artificial learners-represented by a dotted line, dashed line, and dash- dot line. In these simulations, we model convergence by the EarlyStop termination criterion with hyper-parameters N 10 steps and 8 = 0.05 (see Section 4.2.3). To observe the progress of the simulation with this criterion, we continue each simulation for additional 10 iterations beyond convergence, with the point of convergence marked by a dot on each simulated line.\nAs seen from Figure 7, all 15 simulations converge quite near to their respective targets-to the solid lines. Most simulations converge well before 60 steps; a few simulations converged between 60 and 80 steps-a much more favorable length, compared to the original 300.\nIn practice, convergence of the adaptive test is strongly affected by aberrant responses early on in the test. Therefore, we explore specifically simulations with early aberrant responses. We increase the slip rate to Eslip = 0.6 at the beginning, to simulate frequent early aberrant responses, and restore to normal \u025bslip = 0.1 after the first 10 questions.\nThe result can be seen in Figure 8. Early aberrant responses have an asymmetric effect on the final ability estimate. For high-ability students, simulations converge to significantly underestimated scores. For low-ability students, simulations produce a relatively accurate estimate.\nTo compensate for this effect, we introduce a \u201cwarm-up\u201d phase-initial wrong responses are discarded and are not fed into the model. A simulation with a warm-up phase of 10 steps, with the same setup as Figure 8, is shown in Figure 9."}, {"title": "5.2 Experiments with exercise data", "content": "We turn to the final research question: Can we model learner ability reliably based on the learners' responses to exercises, without explicit testing?\nThe setup of experiments with exercises is described in Section 4.3. We study how the quality of assessment is affected by two factors: A. the amount of exercises done by the student, and B. the amount of answers collected per construct. For A: we train the IRT model only on learner data from students who have done a substantial number of exercises; we consider students who have done only a few exercises unreliable for collecting statistics. The parameter minexer is the minimum number exercises that must be done by student S in order to use data from S. For B: once the IRT model has been trained, we estimate the ability of each student S using her data only for those constructs for which S has answered at least minconstr exercises.\n(We assume that if S has done too few exercises with a construct C, her statistics for that constructs are less reliable.)\nThe plots in Figure 10 show how \"real\" CEFR estimates (from teachers) compare to estimated ability scores from IRT based on their exercise performance. The x-axes represent the \"real\" CEFR and the y-axes represent the estimated ability scores. For each CEFR score, the figures show the distribution of IRT estimated scores as box plots for students corresponding to that CEFR score. The figures on the left side use minexer = 50, whereas those on the right side use minexer = 100.\nFor each box plot, the orange line indicates the median and the box indicates the interquar- tile range which includes 50% (25% fall between the first quartile and median, 25% fall between the third quartile and median) of the students of that level. Outside the interquartile"}, {"title": "6 Conclusions and Future Work", "content": "In order to support personalized tutoring, one of the most essential tasks of ITS is to access the learner's proficiency. We present the application of Item Response Theory, to assess students in two contexts: test and exercise.\nOur aim is for a more efficient and accurate assessment on student's proficiency, so the tutor can offer better items during tests, and better exercises during practice, based on the assessment.\nAssessment of ability through testing:\nOur goal is to build an adaptive test as a component of the ITS (to replace the exhaustive testing used previously), that not only gives accurate estimates of the examinee's ability, but is also quicker.\nWe have introduced our data sources and the bank of test items. Then, we introduce an approach to fitting an IRT model, and simulation procedures. The IRT model has randomness, including random initialization, random item selection, slip and exploration; we discussed the motivation for randomness in the testing process.\nThe experiments in the context of testing show:\n1. In spite of an imperfect initial data collection process, the parameters learned from the learner data, are usable to train an IRT model that results in a viable adaptive test.\n2. Models that use difficulty estimates learned from data far outperform the models that use difficulty estimates assigned to items by pedagogy experts. This agrees with the findings of other researchers, e.g., [31].\n3. Slips generally increase the length of a test. However, introducing an Exploration step is able to slightly compensate for the delay in convergence caused by slips.\n4. The IRT model can give an accurate prediction at a very early stage of a test. Additionally, more questions will generally allow the IRT model to achieve a better prediction.\nWe expect that over time the adaptive tests will yield more learner data: since the test does not stress the learners, they will be more willing to use it periodically to check their ability"}, {"title": "EarlyStop:", "content": "We next explore the third stopping criterion. The idea behind this criterion is to stop the test once the ability value is fluctuating within a small range 8 over the last N iterations. We vary 8 from 0.05 to 0.35 with an increment of 0.1, while N varies from 6 to 12 with an increment of 2. Higher values of N indicates a stricter criterion, while lower values of 8 indicates a stricter criterion. The combination of these two parameters gives 16 simulations, which is visualized in Figure C5. The simulations appear sorted by N and 8.\nAs seen from Figure C5, except for the worst two, the MAE is mostly below 0.35, which suggests a low error rate of < 5% over the observed range of 0 (-3.5 <0 <+3.5, in Figure 2). For a given N, we see that the higher 8, the higher the MAE and the SD of error. This is expected because the 8 suggests a less restricted constraint. The test length required for convergence decreases as 8 becomes higher. On the other hand, for a given 8, we can observe a similar behaviour as N. A greater value of N means the MAE and the SD of error becomes lower.\nAt the same time, the SD of test length also decreases with a greater value of 8 or smaller value of N. This is a similar situation as in Figure C4. It is easy to converge when the constraint is relaxed, but it will be difficult with more uncertainty with a stricter constraint."}]}