{"title": "Benchmarking Counterfactual Interpretability in Deep Learning Models for Time Series Classification", "authors": ["Ziwen Kan", "Shahbaz Rezaei", "Xin Liu"], "abstract": "The popularity of deep learning methods in the time series domain boosts interest in interpretability studies, including counterfactual (CF) methods. CF methods identify minimal changes in instances to alter the model predictions. Despite extensive research, no existing work benchmarks CF methods in the time series domain. Additionally, the results reported in the literature are inconclusive due to the limited number of datasets and inadequate metrics. In this work, we redesign quantitative metrics to accurately capture desirable characteristics in CFs. We specifically redesign the metrics for sparsity and plausibility and introduce a new metric for consistency. Combined with validity, generation time, and proximity, we form a comprehensive metric set. We systematically benchmark 6 different CF methods on 20 univariate datasets and 10 multivariate datasets with 3 different classifiers. Results indicate that the performance of CF methods varies across metrics and among different models. Finally, we provide case studies and a guideline for practical usage.", "sections": [{"title": "Introduction", "content": "As the deep learning methods continue to thrive in many data modalities, including time series, their applications, such as classification, forecasting, and anomaly detection underscore the need for interpretability. The deep learning models, often considered as black boxes, require explainability methods to enhance trustworthiness. Various explainability techniques have been proposed for time series data, which can be categorized into several types. Attribution-based methods determine the importance of time series features to model predictions. Prototype-based methods aim to generate representative examples for datasets.\nApart from these approaches, numerous CF studies have been proposed for time series classification and anomaly detection. Given a target class, CF methods are instance-based explanations aiming to find minimal changes in instances to alter the model predictions. While attribution-based methods indicate the contribution of different features, CF methods provide information about the changes in features altering predictions. The information can be used to identify the decision boundaries and indicate the difference between classes.\nDespite numerous CF approaches being proposed, no work has a systematic evaluation of CFs in time series. Existing overview studies in time series focus on benchmarking attribution-based methods or providing a general overview of explainability. Additionally, evaluations of existing CF studies suffer from the inconsistent and limited selection of datasets and a single arbitrary choice of classifiers. Also, some evaluation metrics are inadequate to capture the desirable characteristics of CFs. For example, as shown in Figure 1, the CF appears to change only 2 time steps. However, the value of Lo metric is large because it includes imperceptible changes, contradicting human observations. This huge discrepancy highlights the need to reevaluate vanilla Lo as an evaluation metric.\nIn this work, we redesign evaluation metrics and conduct a systematic benchmarking on CF methods in the time series. To the best of our knowledge, this is the first work to address this topic. Our contributions are listed below:\n\u2022 Redefine evaluation metrics to accurately evaluate desirable characteristics. We redesign metrics for sparsity and plausibility and introduce a new metric for consistency and combine them with validity, proximity, and generation time to form a comprehensive set of metrics.\n\u2022 Conduct systematic and extensive experiments on 6 CF methods on 20 univariate and 10 multivariate datasets with 3 classifiers.\n\u2022 Demonstrate that no CF method outperforms others across all metrics, highlight classifiers' impact on CF performance, and provide case studies and a guideline for practical usage."}, {"title": "Related Work", "content": "Counterfactual Methods\nCF explanation methods can be further categorized into heuristic methods and optimization-based methods. Heuristic methods generate CFs by predefined rules or heuristic choices, while optimization-based methods create CFs by optimizing loss functions.\nHeuristic method Native Guide (NG) identifies the nearest unlike neighbor (NUN) of an instance and subsequently employs Class Activation Map (CAM) to guide CF generation by replacing a subsequence from the NUN. Similarly, COMTE specifically tackles CF generation on multivariate datasets by replacing several channels from the NUN. SETS utilizes Shapelets by removing the original instance's Shapelets and introducing the target class Shapelets. Similarly, MG-CF utilizes Shapelets with specified lengths to guide replacement. Following the idea, TeRCE mines Allen-rules between Shapelets to guide replacement.\nOptimzation-based method Wacther Counterfactuals (WCF) iteratively minimizes a loss function to find CFs with a Manhattan distance constraint. LASTS generates exemplar, counter-exemplar instances, and Shapelet rules utilizing the latent space of autoencoders. TSEvo combines time series transformation with the evolutionary algorithm to generate CF explanations under the constraints of proximity and sparsity. Similarly, Sub-SpaCE utilizes a genetic algorithm leveraging a reconstruction autoencoder to add extra constraints in latent space. CounTS proposes a variational Bayesian deep learning model to generate CFs. Sulem et al. propose a CF method using a loss function with a smoothness term. TimeX modifies the most significant segment identified by Dynamic Barycenter Averaging with a constraint on sparsity. Similarly, Glacier also tunes on either latent space or raw time series with the guide of an importance vector generated by LIMESegment.\nDespite more than 150 datasets in the UCR and UEA archives, CF studies often select only 5 to 10 datasets and one single deep learning classifier. Additionally, some metrics used by those studies may not be sufficient to capture the desirable characteristics of CFs. These factors collectively undermine the conclusions of the existing results.\nExplanability Benchmarking and Survey\nThere are several benchmarking or reviewing works in the time series domain. TSR mainly focuses on evaluating general attribution-based methods and adapting them from the general domain into time series. Exathlon focuses on explanation discovery in anomaly detection, providing an anomaly dataset and the benchmarking methodology for this purpose. Theissler et al. present a review paper focusing on the taxonomy and methodology of general explainable methods in time series. XTSC-Bench is a time series explanation benchmarking paper. It involves CFs but focuses more on attribution methods, and compares CFs along with attribution methods on metrics originally designed for attribution methods. There are also CF surveys and benchmarking papers in the general domain. Verma et al. present a survey covering CF algorithms, metrics, and challenges in the general domain. Guidotti performs a benchmarking study focusing on CF methods in tabular data.\nDespite several surveys proposed in the time domain, there is no benchmarking paper on CF methods. Similarly, the general domain CF survey methods primarily focus on other modalities rather than time series. Given the extensive CF studies proposed in time series modality, there is a need for a benchmarking paper focusing on CF explanations."}, {"title": "Methodology", "content": "Let $x \\in \\mathbb{R}^{N \\times T}$ be a time series, where N represents the number of channels and T represents time steps length, with a predicted label y from a classifier $f(\\cdot)$. CF methods alter a corresponding CF instance $\\hat{x}$ trying to make the prediction $f(\\hat{x})$ the same as a given target label $\\hat{y}$. In this work, we use feature points to refer to all the features $N \\times T$, considering channels and time steps together.\nCounterfactual Explanability Method Selection\nWe select the CF methods that support PyTorch classifiers. Based on code availability, we select 6 CF methods that cover both heuristic and optimization-based methods, as well as univariate and multivariate scenarios, as shown in Table 1."}, {"title": "Evaluation Metrics", "content": "Several characteristics for CFs are considered desirable, including validity, proximity, sparsity, segment sparsity, plausibility, and generation time. In this work, we also introduce consistency characteristics across the models.\nValidity An effective CF method should generate valid CF explanations for a large portion of the dataset. We define $Valid$ as the proportion of valid CFs over the test instances.\nProximity A desirable CF should be close to the original instance. Following the evaluation of NG, we employ $L_1$, $L_2$, and $L_{inf}$ norms to evaluate proximity. Unlike other studies with a single metric for proximity, NG's metrics provide a comprehensive comparison. Notably, $L_{inf}$ norm measures the maximum difference. A higher $L_{inf}$ value indicates more abrupt changes.\nSparsity and Sensitivity Sparsity is often considered a valuable metric, as CFs altering a few features are generally more interpretable. Many studies use Lo norm divided by the number of feature points to measure sparsity, represented in the following equation:\n$L_0(x, \\hat{x}) = \\frac{1}{N \\times T} \\sum_{c=1}^{N} \\sum_{i=1}^{T} 1(x_{i,c} \\neq \\hat{x}_{i,c})$\nwhere 1 is an indicator function that outputs 1 when condition fulfills, and $x_{i,c}$ and $\\hat{x}_{i,c}$ are the i-th time step value in c-th channel of original instance x and CF $\\hat{x}$, respectively. In a modality with a few feature points , changes can be presented in a list. However, visualizations are preferred in time series to represent the changes. Unlike lists, minor changes might be imperceptible in a visualization. That causes the huge difference shown in Figure 1. Here the impact of minor changes is important yet remains unclear. If minor changes are irrelevant to model predictions, the vanilla Lo becomes an overestimate of feature alterations. Conversely, if those changes impact model predictions, then visualizations convey misleading information because end users interpret CFs only with perceptible changes in visualizations. But without minor changes, the CFs are no longer valid. An example is provided in the Appendix. To address this issue, we propose a modified sparsity metric $Thresh L_0$ to consider only the changes exceeding a predefined threshold $ \\tau * range(x)$ and a sensitivity metric Sens to assess the impact of those minor changes. The formal definitions are as follows:\n$ThreshL_0(x, \\hat{x}) = \\sum_{c=1}^{N} \\sum_{i=1}^{T} 1_{i,c},$\nwhere $1_{i,c} = 1 (|x_{i,c} - \\hat{x}_{i,c}| >= \\tau * range(x))$.\n$Sens = 1 (f(x') \\neq f(x))$\nwhere $x'$ matches the CF's values at time points where the change exceeds the threshold; otherwise, it retains the original instance's values. We predict $x'$ with the classifier. If the prediction changes, we consider the CF to be sensitive to minor changes. In the implementation, we set the threshold $\\tau$ as 0.25% of the original instance range and justify it empirically in the Appendix.\nSegment sparsity Some studies suggest that a CF should also change only a few subsequences rather than distributed, and discrete time steps for the latter are hard to interpret. It can be considered as a complement to sparsity. Previous studies measure it by the number of segments, $NumSeg$. We observe that directly calculating it yields counterintuitive results where CF appears to change several segments but the vanilla $NumSeg$ is only 1, as shown in Figure 1. That is because wCF alters all time steps with most changes being imperceptible. To address this, we only consider differences exceeding the threshold $\\tau * range(x)$. However, applying the threshold brings another issue. The CFs of NUN_CF and NG each are a continuous change, but they are cut off into multiple subsequences by some similar points, resulting in large $NumSeg$ numbers. An example is provided in the Appendix. We assume that even if a subsequence contains a few cut-off points, end users still consider it as a single segment and apply a 1% time length tolerance $\\tau_{tol}$.\nNotably, segment sparsity assumes that the distinguishing regions lie in continuous subsequences which may not hold for every dataset. Possibly, they may appear as one-point peaks in datasets. For example, the Earthquake dataset in the UCR archive only consists of discrete peaks representing geological activities and zeros. In this study, we include segment sparsity for comprehensiveness.\nPlausibility A realistic and plausible CF explanation instance is considered to be more interpretable. Evaluating plausibility involves assessing if the CFs fall within the dataset distribution. Some methods assess it by detecting outliers , checking kNN neighbor labels , or computing autoencoders reconstruction error. Specifically, the metric can measure the distribution of an entire dataset or the distribution of each target class. Existing studies only consider either one of them and it remains unclear if they exhibit similar patterns. Also, the distribution of a dataset may not be accurately represented by the distance metric in the time domain thus the neighbor distance or the outlier detection in the time domain may be biased. Additionally, due to the often small dataset sizes, training autoencoders require delicate tuning and may not be scaled up efficiently. Instead, we use a latent representation to evaluate plausibility. We extract the last layer weights in classifiers of a CF as its representation $\\hat{r}$ and compute the distance to the representation $r_i$ of training instance i. Smaller latent distances indicate that CFs are closer to the distributions. We propose $Dist_{all}$ to evaluate if the CF is in dataset distribution and $Dist_{class}$ to evaluate if it is in target class distribution. We first compute, the average distance to the nearest k training representations, $Dist_{nbr}$, defined as the following equation:\n$Dist_{nbr}(c, r) = \\frac{1}{k} \\sum_{j \\in kNN(r)} (r - r_j)^2$\nwhere $kNN(r)$ is the k nearest neighbor of representation r among all training instances with the predicted label c. $Dist_{all}$ and $Dist_{class}$ are formulated as follow equations:\n$Dist_{all} = \\frac{\\frac{1}{m_{all}}\\sum_{i} Dist_{nbr}(all, r)}{\\frac{1}{m_{all}}\\sum_{i} Dist_{nbr}(all, r_i)}$\n$Dist_{class} = \\frac{\\frac{1}{m_{c}}\\sum_{i} Dist_{nbr}(c, r)}{\\frac{1}{m_{c}}\\sum_{i} Dist_{nbr}(c, r_i)}$\nwhere $m_{all}$ and $m_{c}$ is the size of the entire training set and the size of the training set with the class label c, respectively. We empirically set the neighborhood size k to 5.\nGeneration Time A practical CF method should be efficient in terms of time. We report the average generation time $G_{time}$ for each dataset.\nConsistency In general, consistency measures how much explanations differ between different deep learning models. We define a consistent CF as the one keeping validity when the classifier is altered and argue such consistency enhances its generalizability. Due to the page limit, the experiments of consistency can be found in the Appendix.\nWe use all the metrics above for evaluation. In practice, the set of metrics may be adjusted for different interests. For proximity, $L_1$ and $L_2$ usually exhibit similar patterns, while $L_{inf}$ is important when the abrupt changes are undesirable in the time series. For sparsity, $ThreshL_0$ ignores tiny changes below a threshold and measures the perceptible changes. Meanwhile, Sens measures if those tiny changes affect model predictions. Segment sparsity represents another aspect of sparsity by evaluating the number of changed subsequences but it may not be applicable to all datasets. Notably, the threshold $\\tau*range(x)$ and tolerance $tol$ can be adjusted according to specific interests. For plausibility, we observe that $Dist_{all}$ and $Dist_{class}$ exhibit similar patterns. Since the entire distribution has more important information than the class distribution, we suggest $Dist_{all}$ as the more important metric if only one is chosen."}, {"title": "Experimental design", "content": "We select 20 univariate datasets from UCR datasets, and 13 multivariate datasets from UEA datasets. The dataset details can be found in the Appendix. We select univariate datasets based on three criteria: 1) popularity of usage, 2) strong classifier performance, and 3) coverage of all nine UCR categories. We observe that some CF methods reach the time limit on multivariate datasets with a large number of feature points or channels which is discussed in Section in detail. We eventually report results for 10 multivariate datasets, each with a smaller number of feature points and channels. We select 3 deep learning classifiers: FCN, MLP and InceptionTime. For CF methods, we maintain default hyperparameters unless specified. For efficiency, we conduct a stratified sampling of 160 samples for wCF and TSEvo and set up a time limit on CF generation. The details of the experimental design can be found in the Appendix."}, {"title": "Experimental results", "content": "The average rankings of the evaluation metrics using univariate and multivariate datasets with classifiers are shown in Figure 2 and 3. For a metric, the length of the bar in the radar, from the innermost to the outermost, represents the average ranking ranging from 5 to 1.\nUnivariate Results\nNG is omitted in MLP as it is not applicable. The HandOutlines dataset is excluded in MLP and InceptionTime for a fair comparison because wCF times out on every sample. The critical difference diagrams for each metric are provided in the Appendix.\nValidity NUN_CF, NG, and TSEvo always generate valid CFs, as guaranteed by their algorithm design, whereas wCF and SETS do not. We observe that wCF stops after 500 iterations in almost every dataset but that occurs less frequently in FCN and InceptionTime. We speculate that SETS's validity might depend on its hyperparameters. One hyperparameter controls the number of Shapelets stored for each class during mining. We observe that in some datasets like Computers, after ignoring Shapelets in multiple classes, no Shapelet remains for certain classes, resulting in failing in all instances targeted at those classes. Increasing the number of Shapelets stored might mitigate this issue. Additionally, we observe TSETS might be too tight for many datasets, resulting in few Shapelets detected in test samples.\nProximity In terms of proximity, $L_1$ and $L_2$ norms exhibit similar results. WCF performs well, likely because its loss function only has a single constraint term based on the L1 norm. NG ranks second in FCN and InceptionTime; showing that despite a simple strategy, it is relatively effective. SETS and NUN_CF perform poorly in $L_1$ and $L_2$ norms. The distances of NUN_CF are large as expected as it replaces the entire instance with another. As shown in Figure 4d, we observe sometimes the minimum and maximum values of the subsequence and Shapelets are not aligned during the scaling, resulting in poor performance for SETS. The Linf norm ranking varies among classifiers. And most methods do not exhibit a significant performance difference. See the critical difference diagram in the Appendix for details. Still, some patterns can be observed like NG ranks first in FCN and InceptionTime and SETS performs poorly in all classifiers. Interestingly, NUN_CF ranks first in MLP, indicating that other methods generate CFs with abrupt changes even greater than replacing them with entire instances, which is undesirable.\nSparsity and Sensitivity wCF and TSEvo perform well in sparsity. TSEvo's performances might be attributed to one of its loss terms, $L_0$. Interestingly, wCF slightly outperforms TSEvo despite only having a $L_1$ constraint. SETS performs poorly in sparsity, probably because it usually needs to replace several Shapelets to generate a valid CF. In terms of sensitivity, heuristic methods like NUN_CF and SETS perform well. Since a large portion of changes are imperceptible in wCF, its CFs are expected to be sensitive. Surprisingly, TSEvo's imperceptible changes also impact predictions. Our conjecture is that heuristic methods create fewer imperceptible changes and their CFs are further from the decision boundary. In contrast, optimization-based methods make more imperceptible changes, and their CFs are close to the decision boundary and sensitive to those small changes.\nSegment sparsity NUN_CF and NG perform well in segment sparsity. Both change only one subsequence algorithmically, having the most rigorous constraint regarding $NumSeg$. Since we use a threshold to count visible segments, even with a tolerance setup, a continuous change may still be divided into segments by parts resembling the original instance. And NG is slightly better than NUN_CF because it changes a smaller portion of the instance, resulting in a smaller probability of being divided. SETS ranks in the middle because its changes are continuous subsequence replacement. TSEvo and wCF have no constraints on segments and thus generate CFs with a large $Numseg$.\nPlausibility In terms of plausibility, $dist_{all}$ and $dist_{class}$ exhibit similar patterns. NUN_CF utilizes a real instance from the training set and ranks first in all models as expected. NG ranks second in FCN and InceptionTime, likely because its CFs are combinations of two real instances, which have high plausibility. SETS performs poorly in FCN but relatively well in MLP and InceptionTime. This is probably because while SETS attempts to generate counterfactuals using the same set of Shapelets, classifiers capture different features and form various latent spaces. wCF's performance varies across models; it ranks just after NG in FCN but performs worse in MLP and InceptionTime. TSEvo performs poorly across all classifiers. We speculate that its mutation stage randomly replaces sequences from training instances with different labels, resulting in out-of-order instances.\nGeneration Time NUN_CF is the most efficient as it simply selects the nearest instance with a different label. NG also performs well for its straightforward searching approach. Notably, SETS relies on a time-consuming Shapelet mining preprocessing step. It is not included in Gtime because it has an adjustable time limit and is performed once per dataset. However, this overhead should be considered in practice. Additionally, wCF's computational cost increases significantly with feature points, leading to time-outs on the HandOutLines dataset in MLP and InceptionTime."}, {"title": "Multivariate Results", "content": "In multivariate datasets, we observe more time-out events with different CF methods. We exclude wCF since it times out in 6 of 13 datasets, which is a major limitation in multivariate datasets. We also exclude three datasets for fair comparison since some methods time out in them. We observe similar patterns of results between FCN and InceptionTime. To save space, we place the results of InceptionTime in the Appendix. Critical difference diagrams for each metric are also provided in the Appendix.\nValidity NUN_CF, COMTE, and TSEvo can always generate valid CFs, while SETS falls when it runs out of Shapelets. In some datasets, SETS can only generate a few valid CFs. We speculate that this happens because the mining time is equally allocated to each channel for multivariate datasets, leaving less time to find high-quality Shapelets in each channel and restricting SETS to be only capable of making limited changes.\nProximity In terms of $L_1$ and $L_2$ norms, TSEvo and SETS perform well. Unlike the bad performance in univariate datasets, we observe that SETS ranks first in multivariate datasets. However, this likely stems from that SETS is only capable of making limited changes and generates a small portion of valid CFs. Since COMTE does not penalize replacing fewer than 3 channels, its CFs are as distant as expected. In terms of $L_{inf}$ norm, the rankings vary among classifiers and no method significantly outperforms others. See the critical difference diagram in the Appendix for detailed information.\nSparsity and Sensitivity In terms of sparsity, the pattern is similar to $L_1$ and $L_2$ norms and we suspect the same underlying reason. In terms of sensitivity, similar to univariate datasets, heuristic methods like NUN_CF, COMTE, and SET perform well.\nSegment sparsity SETS and COMTE perform well in $NumSeg$. SETS only changes a few Shapelets to flip the models and COMTE replaces several channels entirely.\nPlausibility Similar to the univariate datasets, NUN_CF is the most plausible. Similar to NG, COMTE performs well, likely because its CFs are a combination of two real instances. SETS and TSEvo perform poorly in plausibility.\nGeneration Time Similarly, NUN_CF ranks first, followed by SETS and COMTE, while TSEvo performs poorly. Notably, SETS times out on the Heartbeat and NATOPS datasets, which have 61 and 24 channels, respectively. It is likely because SETS tries combinations of all channels when it fails with just one, leading to an exponential increase in computational cost with the number of channels. TSEvo times out in the EigenWorms dataset with more than 100,000 feature points. We exclude Heartbeat, NATOPS, and TSEvo for fair comparisons. The detailed generation time and time-out with FCN, is provided in the Appendix."}, {"title": "Case study", "content": "Discussion\nWe select the GunPoint dataset as a case study as an example of how the CF methods and evaluation metrics work. The GunPoint dataset records the hand location on the X-axis when actors draw guns or point fingers. The details of the GunPoint dataset and another case study of the Chinatown dataset can be found in the Appendix.\nThe CFs with evaluation results are shown in Figure 4. As shown in Figure 4a, wCF generates the CF with a zigzag in the decreasing edge. The change is small in terms of $L_1$ and $L_2$ but relatively abrupt with a large Linf. Notably, while the vanilla Lo norm is 1.0, only 2 points are changed perceptibly. Thresh Lo results in 0.01 and is consistent with the visualization. Despite being close to the original instance, the CF exhibits large latent distances because no such jagged sequence at the decreasing edge is observed in the training samples. As shown in Figure 4b, NG replaces the entire peak, resulting in larger $L_1$ and $L_2$ and Thresh Lo. But the change is smooth with relatively low Linf and latent distances. Although only one subsequence is replaced in the algorithm, the change is cut by a relatively long resembling part at the peak in visualization, resulting in Num Seg being 2. As shown in Figure 4c, TSEvo creates several changes, including a small bump in the decreasing edge and an abrupt change around time step 110. Despite relatively low proximity and sparsity, it is hard to understand the CF because this CF changes many segments. Additionally, no such bump is observed in the training samples, and it's unlikely an actor would abruptly move a hand and immediately return it. Thus this CF is implausible with large latent distances. As shown in Figure 4d, SETS introduces a Shapelet of a ditch which is supposed to be an interpretable feature. However, the scaling process makes the CF distant from the original instance since the minimum and maximum values of this Shapelet do not align with those of the corresponding subsequence. Additionally, the high value after the ditch and the gap of this magnitude around time step 135 are not observed in the training samples, resulting in poor plausibility."}, {"title": "Performance Summary and Practical Guideline.", "content": "Heuristic method NUN_CF and NG consistently generate CFs. NUN_CF provides evaluation baselines for CF methods. For example, in MLP its Linf performance shows that other methods perform poorly by creating abrupt changes. NG is a simple but effective approach, performing well in proximity, plausibility, segment sparsity, and efficiency in univariate datasets. However, it can't work in multivariate datasets or in MLP. Importantly, it requires access to model weights, resulting in the most amount of information of the methods. SETS replaces Shapelets to generate CFs. Its validity depends on hyperparameters and mining time. For proximity or sparsity, SETS doesn't perform well in univariate datasets. The low proximity and sparsity in multivariate datasets might be because SETS can only make limited changes per channel and generate only a few valid CFs. It relies on a time-consuming preprocessing step and times out on datasets with a large number of channels. However, SETS performs well regarding sensitivity and segment sparsity. COMTE specializes in multivariate datasets and ensures validity. Since it replaces entire channels, it shows weakness in proximity and sparsity. Similar to NG, COMTE performs well in segment sparsity and plausibility.\nOptimization-based method The validity of wCF is not always guaranteed. wCF performs well on L1 and L2. However, it usually creates abrupt changes and performs poorly on Linf. Additionally, it tends to make imperceptible changes to which its CFs are likely to be sensitive. wCF has no constraint other than proximity and thus didn't perform well in segment sparsity and plausibility. Additionally, wCF's computational cost increases fast as the number of feature points increases. It times out on the HandOutlines dataset and a large number of multivariate datasets. TSEvo shares similarities with the wCF. Its computational cost is higher in simpler datasets but grew slower than wCF. Still, it times out when the number of feature points is very large. TSEvo always generates valid CFs and it achieves relatively good performance in proximity and sparsity. Like wCF, it struggles with sensitivity, segment sparsity, and plausibility.\nClassifier impact The choice of classifiers impacts different metrics variably. While the performances of L1 and L2 norms, sparsity metrics, and generation time remain relatively consistent, performances of validiy, Linf norm, and plausibility differ among classifiers. More differences are observed in univariate datasets than in multivariate datasets.\nGuideline The heuristic methods perform well in the segment sparsity and plausibility. They might be a good choice when generating interpretable CFs. For univariate datasets, if model weights are accessible, NG can be an efficient and effective choice. For multivariate datasets, COMTE is an alternative option. When time series channels are few, SETS might generate plausible results, though its validity and computationally expensive preprocess remain concerns. Optimization-based methods, like as wCF and TSEvo, are advantageous for their proximity to indicate the decision boundary which might be helpful in revealing spurious features the classifier learns. When using wCF and TSEvo, we suggest only using them on datasets with fewer feature points and being cautious about imperceptible changes and sensitivity issues."}, {"title": "Summary", "content": "To the best of our knowledge, we conduct the first comprehensive benchmarking work focusing on CF explanations in time series covering 6 CF methods on 20 univariate and 10 multivariate datasets using 3 classifiers. We reevaluate and redesign metrics to better capture the desirable characteristics of CFs. We observe no single method outperforms all others across all metrics and classifiers impact the performance. We provide case studies, and offer practical guidelines for using CF methods."}, {"title": "Details of Experiment Design", "content": "Dataset Statisitics\nWe choose 20 univariate datasets in UCR that cover all 9 subcategories, for multivariate datasets, we first chose 13 datasets. Due to some methods time out on 3 multivariate datasets, we end up reporting results on 10 multivariate datasets with a smaller number of feature points and channels. The details of those datasets are shown in Table 2 and 3.\nClassifiers and Training\nFCN is chosen for its relatively strong performance and simple structure, as reported in a classification benchmarking survey. InceptionTime is recognized as a state-of-the-art deep learning classification model , and MLP is included to provide results that extend beyond CNN-based models. The high-performance variance is reported among the same architecture with different random initialization. To ensure a relatively satisfying performance, we repeat the training process five times and select either the best test accuracy or the first instance to reach 95% of reported accuracy. To obtain the CAM, we only use one InceptionTime instead of an ensemble of five classifiers. We utilize the implementation of TSAI for deep learning classifiers.\nHyperparameter Settings\nWithout specification, all the hyperparameters for CF methods are default by original papers. Notably, we adjust the regularization parameter \\(\\lambda\\) for wCF to 10, as it typically requires a significantly higher value than the default of 0.1 to generate valid CFs. For NG, there is a limitation to stop when subsequence length reachs 500 by default. Since the original paper's datasets are much shorter, it won't cause any trouble. However, as we evaluates the performance on longer datasets, the limitation prevents generating CFs. Since NG's algorithm defines no limitation on the subsequence length in the original paper, we increase this to match the length of the time series in our dataset. To maintain consistency in CF target classes, we design the class with the second highest probability as the target class. And we set the stopping criterion of every methods as 0.5. We utilize the implementation of TSInterpret for"}, {"title": "Consistency Evaluation", "content": "Different models even with the same architecture might capture different features. Consistent CFs are robust to this difference. To assess consistency, we compute the percentage of valid CFs across different models. To simplify, we use binary classification datasets and only consider high-performing models, defined as those with over 90% test accuracy on univariate datasets. We train another group of FCN models from scratch with different random initializations and only use datasets that have over 90% test accuracy on both FCNs. Four datasets, namely, GunPoint (GP), PowerCons (PC), Strawberry (SB), and Wafer (WF), are included. Additionally, only include instances that are correctly predicted by both models.\nSpecifically, we compute consistBC which is the percentage of consistent CFs among the total number of correctly predicted instances. Additionally, we calculate consistBV which is the percentage of consistent CFs among the valid CFs in the correctly predicted instances. The former metrics evaluate the ability to generate consistent consistent CFs given a dataset. The latter metric evaluates the ability to generate consistent CFs among those that are valid.\nThe results are shown in Table 5. When ranking the consistency, we ignore NUN_CF since it uses a training instance as a CF and it is expected that its consistency metric to be very high. In the SB dataset, NUN_CF doesn't achieve 1.0 consistency. That might be because NUN_CF uses a few training samples that have different predictions between two FCN classifiers. In general, NG and SETS generate CFs that have high consistency across the models. NG wins in the consistBC, showing it generates the largest amount of consistent CFs in those four datasets. SETS wins in consistBV, and although it can't always generate valid CFs, the valid ones it creates are usually consistent between models. wCF and TSEvo perform poorly in consistency, their CFs are only valid in the model they are applied to and can hardly be generalized into other classifiers."}, {"title": "Discussion of Sparsity, Sensitivity and Segment Sparisity", "content": "We observe a large portion of imperceptible changes in wCF's results. If these minor changes are irrelevant to model prediction, then the vanilla LO gives an overestimation of the number of changed features. As shown in Figure 26a, the CF from the GunPoint dataset appears very sparse and shows only two positions of change at time steps 50 and 82. However, the computed vanilla Lo result is 1.0, indicating that all time steps are changed, which contradicts the observation. After we utilize Thresh Lo with 0.1% data range as the threshold, only the two points are counted as changes, and the sparsity is correctly represented as 0.013.\nWhen those minor changes have an impact on model prediction, then the CF becomes totally uninterpretable and visualization conveys misleading information. As shown in Figure 26b, the CF appears to change the decreasing edges and another subsequence at the end. However, there is a minor change at time step 35 which is imperceptible. When we use the original instance value at time step 35 in CF then the prediction is back to the original prediction, making it no longer a valid CF. This instance is therefore considered sensitive and misleading.\nThe threshold of threshold LO is set at 0.25% of the original data range since approximately a default figure generated by the matplotlib package is 4.8 inches with 300 dpi resulting in 1440 pixels in total, and the default line width is 2 pixels. If the difference is less than 0.25%, the difference in the figure is approximately only less than 4 pixels, and the instance lines will overlap with each other. This is relatively conservative choice since in practice, end users prefer bolder lines for better visualizations.\nIn terms of NumSeg, if we directly computed the number of segments, the instance in Figure 26a only has one segment since all the points are changed and they are connected. However, we can observe two locations of changes. Thus, we only considered the changes greater than the threshold and computed NumSeg based on that. This resulting the correct NumSeg as 1. However, this action causes another issue to bring too many segments that it should be. For example in Figure 26c, NUN_CF changes the whole sequence thus NumSeg should be 1. But some points resemble the original instance and cut off the CF, resulting NumSeg as 3. We assume that even if a few consecutive points are not changed in the time series, people will still consider both sides as one segment. Thus, we mitigate this issue by giving a 1% time length tolerance. With this tolerance, the NumSeg is 1."}, {"title": "Supplemental Case Studies"}]}