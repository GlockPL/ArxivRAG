{"title": "Benchmarking Counterfactual Interpretability in Deep Learning Models for Time Series Classification", "authors": ["Ziwen Kan", "Shahbaz Rezaei", "Xin Liu"], "abstract": "The popularity of deep learning methods in the time series domain boosts interest in interpretability studies, including counterfactual (CF) methods. CF methods identify minimal changes in instances to alter the model predictions. Despite extensive research, no existing work benchmarks CF methods in the time series domain. Additionally, the results reported in the literature are inconclusive due to the limited number of datasets and inadequate metrics. In this work, we redesign quantitative metrics to accurately capture desirable characteristics in CFs. We specifically redesign the metrics for sparsity and plausibility and introduce a new metric for consistency. Combined with validity, generation time, and proximity, we form a comprehensive metric set. We systematically benchmark 6 different CF methods on 20 univariate datasets and 10 multivariate datasets with 3 different classifiers. Results indicate that the performance of CF methods varies across metrics and among different models. Finally, we provide case studies and a guideline for practical usage.", "sections": [{"title": "Introduction", "content": "As the deep learning methods continue to thrive in many data modalities, including time series (Theissler et al. 2022; Bodria et al. 2023), their applications, such as classification (Wang, Yan, and Oates 2017; Ismail Fawaz et al. 2019), forecasting (Torres et al. 2021; Benidis et al. 2022), and anomaly detection (Choi et al. 2021; Bl\u00e1zquez-Garc\u00eda et al. 2021) underscore the need for interpretability. The deep learning models, often considered as black boxes, require explain- ability methods to enhance trustworthiness (Arrieta et al. 2020; Guidotti 2022; Guidotti et al. 2018).\nVarious explainability techniques have been proposed for time series data, which can be categorized into several types. Attribution-based methods (Guillem\u00e9 et al. 2019; Crabb\u00e9 and Van Der Schaar 2021; Parvatharaju et al. 2021; Doddaiah et al. 2022) determine the importance of time series fea- tures to model predictions. Prototype-based methods (Gee et al. 2019; Ghods and Cook 2022; Ghosal and Abbasi-Asl 2021; Guidotti et al. 2020) aim to generate representative examples for datasets.\nApart from these approaches, numerous CF studies (De- laney, Greene, and Keane 2021; Ates et al. 2021; Bahri, Boubrahimi, and Hamdi 2022; Li, Boubrahimi, and Hamdi 2022; Bahri et al. 2022; Guidotti et al. 2020; H\u00f6llig, Kulbach, and Thoma 2022) have been proposed for time series classification and anomaly detection. Given a target class, CF methods are instance-based explanations aiming to find minimal changes in instances to alter the model predictions. While attribution-based methods indicate the contribution of different features, CF methods provide information about the changes in features altering predictions (Wachter, Mittelstadt, and Russell 2017; Yan and Wang 2023). The infor- mation can be used to identify the decision boundaries and indicate the difference between classes.\nDespite numerous CF approaches being proposed, no work has a systematic evaluation of CFs in time series. Ex- isting overview studies in time series focus on benchmark- ing attribution-based methods (Ismail et al. 2020; H\u00f6llig, Thoma, and Grimm 2023) or providing a general overview of explainability (Jacob et al. 2020; Theissler et al. 2022). Additionally, evaluations of existing CF studies (Delaney, Greene, and Keane 2021; Ates et al. 2021; Bahri et al. 2022; H\u00f6llig, Kulbach, and Thoma 2022) suffer from the inconsis- tent and limited selection of datasets and a single arbitrary choice of classifiers. Also, some evaluation metrics are in- adequate to capture the desirable characteristics of CFs. For example, as shown in Figure 1, the CF appears to change only 2 time steps. However, the value of Lo metric is large because it includes imperceptible changes, contradicting hu- man observations. This huge discrepancy highlights the need to reevaluate vanilla Lo as an evaluation metric.\nIn this work, we redesign evaluation metrics and conduct a systematic benchmarking on CF methods in the time se- ries. To the best of our knowledge, this is the first work to address this topic. Our contributions are listed below:\n\u2022 Redefine evaluation metrics to accurately evaluate desir- able characteristics. We redesign metrics for sparsity and"}, {"title": "Related Work", "content": "Counterfactual Methods\nCF explanation methods can be further categorized into heuristic methods and optimization-based methods (Guidotti 2022). Heuristic methods generate CFs by pre- defined rules or heuristic choices, while optimization-based methods create CFs by optimizing loss functions.\nHeuristic method Native Guide (NG) (Delaney, Greene, and Keane 2021) identifies the nearest unlike neighbor (NUN) of an instance and subsequently employs Class Ac- tivation Map (CAM) (Zhou et al. 2016) to guide CF gen- eration by replacing a subsequence from the NUN. Simi- larly, COMTE (Ates et al. 2021) specifically tackles CF gen- eration on multivariate datasets by replacing several chan- nels from the NUN. SETS (Bahri, Boubrahimi, and Hamdi 2022) utilizes Shapelets by removing the original instance's Shapelets and introducing the target class Shapelets. Sim- ilarly, MG-CF (Li, Boubrahimi, and Hamdi 2022) utilizes Shapelets with specified lengths to guide replacement. Fol- lowing the idea, TeRCE (Bahri et al. 2022) mines Allen- rules (Allen 1983) between Shapelets to guide replacement.\nOptimzation-based method Wacther Counterfactuals (WCF) (Wachter, Mittelstadt, and Russell 2017) iteratively minimizes a loss function to find CFs with a Manhattan distance constraint. LASTS (Guidotti et al. 2020) generates exemplar, counter-exemplar instances, and Shapelet rules utilizing the latent space of autoencoders. TSEvo (H\u00f6llig, Kulbach, and Thoma 2022) combines time series trans- formation with the evolutionary algorithm to generate CF explanations under the constraints of proximity and sparsity. Similarly, Sub-SpaCE (Refoyo and Luengo 2023) utilizes a genetic algorithm leveraging a reconstruction autoencoder to add extra constraints in latent space. CounTS proposes (Yan and Wang 2023) a variational Bayesian deep learning model to generate CFs. Sulem et al. (2022) propose a CF method using a loss function with a smoothness term. TimeX (Filali Boubrahimi and Hamdi 2022) modifies the most significant segment identified by Dynamic Barycenter Averaging with a constraint on sparsity. Similarly, Glacier (Wang et al. 2024) also tunes on either latent space or raw time series with the guide of an importance vector generated by LIMESegment (Sivill and Flach 2022).\nDespite more than 150 datasets in the UCR and UEA archives (Bagnall et al. 2018; Dau et al. 2019), CF studies often select only 5 to 10 datasets and one single deep learn- ing classifier (Delaney, Greene, and Keane 2021; Ates et al."}, {"title": "Explanability Benchmarking and Survey", "content": "There are several benchmarking or reviewing works in the time series domain. TSR (Ismail et al. 2020) mainly fo- cuses on evaluating general attribution-based methods and adapting them from the general domain into time series. Exathlon (Jacob et al. 2020) focuses on explanation discov- ery in anomaly detection, providing an anomaly dataset and the benchmarking methodology for this purpose. Theissler et al. (2022) present a review paper focusing on the tax- onomy and methodology of general explainable methods in time series. XTSC-Bench (H\u00f6llig, Thoma, and Grimm 2023) is a time series explanation benchmarking paper. It involves CFs but focuses more on attribution methods, and compares CFs along with attribution methods on metrics originally designed for attribution methods. There are also CF surveys and benchmarking papers in the general do- main. Verma et al. (2020) present a survey covering CF algorithms, metrics, and challenges in the general domain. Guidotti (2022) performs a benchmarking study focusing on CF methods in tabular data.\nDespite several surveys proposed in the time domain, there is no benchmarking paper on CF methods. Similarly, the general domain CF survey methods primarily focus on other modalities rather than time series. Given the extensive CF studies proposed in time series modality, there is a need for a benchmarking paper focusing on CF explanations."}, {"title": "Methodology", "content": "Let $x \\in \\mathbb{R}^{N \\times T}$ be a time series, where N represents the number of channels and T represents time steps length, with a predicted label y from a classifier $f(\\cdot)$. CF methods alter a corresponding CF instance $\\hat{x}$ trying to make the prediction $f(\\hat{x})$ the same as a given target label $\\hat{y}$. In this work, we use feature points to refer to all the features $N \\times T$, considering channels and time steps together.\nCounterfactual Explanability Method Selection\nWe select the CF methods that support PyTorch classifiers. Based on code availability, we select 6 CF methods that cover both heuristic and optimization-based methods, as well as univariate and multivariate scenarios, as shown in Table 1."}, {"title": "Evaluation Metrics", "content": "Several characteristics for CFs (Delaney, Greene, and Keane 2021; Guidotti 2022) are considered desirable, including va- lidity, proximity, sparsity, segment sparsity, plausibility, and generation time. In this work, we also introduce consistency characteristics across the models.\nValidity An effective CF method should generate valid CF explanations for a large portion of the dataset. We define $Valid$ as the proportion of valid CFs over the test instances.\nProximity A desirable CF should be close to the original instance. Following the evaluation of NG (Delaney, Greene, and Keane 2021), we employ L1, L2, and Linf norms to evaluate proximity. Unlike other studies with a single metric for proximity, NG's metrics provide a comprehensive com- parison. Notably, Linf norm measures the maximum differ- ence. A higher Linf value indicates more abrupt changes.\nSparsity and Sensitivity Sparsity is often considered a valuable metric, as CFs altering a few features are gener- ally more interpretable. Many studies use Lo norm divided by the number of feature points to measure sparsity, repre- sented in the following equation:\n$\\mathcal{L}_{0}(x, \\hat{x})=\\frac{1}{N \\times T} \\sum_{c=1}^{N} \\sum_{i=1}^{T} \\mathbb{1}(x_{i, c} \\neq \\hat{x}_{i, c})$\nwhere 1 is an indicator function that outputs 1 when condi- tion fulfills, and $x_{i,c}$ and $\\hat{x}_{i,c}$ are the i-th time step value in c-th channel of original instance x and CF $\\hat{x}$, respectively. In a modality with a few feature points (Guidotti 2022), changes can be presented in a list. However, visualizations are preferred in time series to represent the changes. Un- like lists, minor changes might be imperceptible in a vi- sualization. That causes the huge difference shown in Fig- ure 1. Here the impact of minor changes is important yet remains unclear. If minor changes are irrelevant to model predictions, the vanilla Lo becomes an overestimate of fea- ture alterations. Conversely, if those changes impact model predictions, then visualizations convey misleading informa- tion because end users interpret CFs only with perceptible changes in visualizations. But without minor changes, the CFs are no longer valid. An example is provided in the Ap- pendix. To address this issue, we propose a modified sparsity metric Thresh Lo to consider only the changes exceeding a predefined threshold $\\tau * range(x)$ and a sensitivity metric"}, {"title": "Consistency", "content": "In general, consistency measures how much explanations differ between different deep learning models. We define a consistent CF as the one keeping validity when the classifier is altered and argue such consistency enhances its generalizability. Due to the page limit, the experiments of consistency can be found in the Appendix.\nWe use all the metrics above for evaluation. In practice, the set of metrics may be adjusted for different interests. For proximity, $L_1$ and $L_2$ usually exhibit similar patterns, while $L_{inf}$ is important when the abrupt changes are unde- sirable in the time series. For sparsity, $Thresh_{L_0}$ ignores tiny changes below a threshold and measures the perceptible changes. Meanwhile, Sens measures if those tiny changes affect model predictions. Segment sparsity represents an- other aspect of sparsity by evaluating the number of changed subsequences but it may not be applicable to all datasets. Notably, the threshold $\\tau * range(x)$ and tolerance tol can be adjusted according to specific interests. For plausibility, we observe that $Dist_{all}$ and $Dist_{class}$ exhibit similar patterns. Since the entire distribution has more important information than the class distribution, we suggest $Dist_{all}$ as the more important metric if only one is chosen."}, {"title": "Experimental results", "content": "The average rankings of the evaluation metrics using uni- variate and multivariate datasets with classifiers are shown in Figure 2 and 3. For a metric, the length of the bar in the radar, from the innermost to the outermost, represents the average ranking ranging from 5 to 1.\nUnivariate Results\nNG is omitted in MLP as it is not applicable. The Hand- Outlines dataset is excluded in MLP and InceptionTime for a fair comparison because wCF times out on every sample. The critical difference diagrams for each metric are provided in the Appendix.\nValidity NUN_CF, NG, and TSEvo always generate valid CFs, as guaranteed by their algorithm design, whereas wCF and SETS do not. We observe that wCF stops after 500 iter- ations in almost every dataset but that occurs less frequently in FCN and InceptionTime. We speculate that SETS's va- lidity might depend on its hyperparameters. One hyperpa- rameter controls the number of Shapelets stored for each class during mining. We observe that in some datasets like Computers, after ignoring Shapelets in multiple classes, no Shapelet remains for certain classes, resulting in failing in all instances targeted at those classes. Increasing the number of Shapelets stored might mitigate this issue. Additionally, we observe TSETS might be too tight for many datasets, re- sulting in few Shapelets detected in test samples.\nProximity In terms of proximity, L\u2081 and L2 norms exhibit similar results. WCF performs well, likely because its loss function only has a single constraint term based on the L1 norm. NG ranks second in FCN and InceptionTime; showing that despite a simple strategy, it is relatively effective. SETS and NUN_CF perform poorly in L\u2081 and L2 norms. The dis- tances of NUN_CF are large as expected as it replaces the entire instance with another. As shown in Figure 4d, we ob- serve sometimes the minimum and maximum values of the subsequence and Shapelets are not aligned during the scal- ing, resulting in poor performance for SETS. The Linf norm ranking varies among classifiers. And most methods do not exhibit a significant performance difference. See the critical difference diagram in the Appendix for details. Still, some patterns can be observed like NG ranks first in FCN and InceptionTime and SETS performs poorly in all classifiers. In- terestingly, NUN_CF ranks first in MLP, indicating that other"}, {"title": "Sparsity and Sensitivity", "content": "wCF and TSEvo perform well in sparsity. TSEvo's performances might be attributed to one of its loss terms, Lo. Interestingly, wCF slightly outperforms TSEvo despite only having a L\u2081 constraint. SETS performs poorly in sparsity, probably because it usually needs to re- place several Shapelets to generate a valid CF. In terms of sensitivity, heuristic methods like NUN_CF and SETS per- form well. Since a large portion of changes are impercep- tible in wCF, its CFs are expected to be sensitive. Surpris- ingly, TSEvo's imperceptible changes also impact predic- tions. Our conjecture is that heuristic methods create fewer imperceptible changes and their CFs are further from the decision boundary. In contrast, optimization-based methods make more imperceptible changes, and their CFs are close to the decision boundary and sensitive to those small changes.\nSegment sparsity NUN_CF and NG perform well in seg- ment sparsity. Both change only one subsequence algo- rithmically, having the most rigorous constraint regarding NumSeg. Since we use a threshold to count visible seg- ments, even with a tolerance setup, a continuous change may still be divided into segments by parts resembling the orig- inal instance. And NG is slightly better than NUN_CF be- cause it changes a smaller portion of the instance, resulting in a smaller probability of being divided. SETS ranks in the middle because its changes are continuous subsequence re- placement. TSEvo and wCF have no constraints on segments and thus generate CFs with a large Numseg.\nPlausibility In terms of plausibility, distall and distclass exhibit similar patterns. NUN_CF utilizes a real instance from the training set and ranks first in all models as expected. NG ranks second in FCN and InceptionTime, likely because its CFs are combinations of two real instances, which have high plausibility. SETS performs poorly in FCN but rela- tively well in MLP and InceptionTime. This is probably be- cause while SETS attempts to generate counterfactuals us- ing the same set of Shapelets, classifiers capture different features and form various latent spaces. wCF's performance varies across models; it ranks just after NG in FCN but per- forms worse in MLP and InceptionTime. TSEvo performs poorly across all classifiers. We speculate that its mutation stage randomly replaces sequences from training instances with different labels, resulting in out-of-order instances."}, {"title": "Multivariate Results", "content": "In multivariate datasets, we observe more time-out events with different CF methods. We exclude wCF since it times out in 6 of 13 datasets, which is a major limitation in multi- variate datasets. We also exclude three datasets for fair com- parison since some methods time out in them. We observe similar patterns of results between FCN and InceptionTime. To save space, we place the results of InceptionTime in the Appendix. Critical difference diagrams for each metric are also provided in the Appendix.\nValidity NUN_CF, COMTE, and TSEvo can always gen- erate valid CFs, while SETS falls when it runs out of Shapelets. In some datasets, SETS can only generate a few valid CFs. We speculate that this happens because the min- ing time is equally allocated to each channel for multivari- ate datasets, leaving less time to find high-quality Shapelets in each channel and restricting SETS to be only capable of making limited changes.\nProximity In terms of L\u2081 and L2 norms, TSEvo and SETS perform well. Unlike the bad performance in univariate datasets, we observe that SETS ranks first in multivariate datasets. However, this likely stems from that SETS is only capable of making limited changes and generates a small portion of valid CFs. Since COMTE does not penalize re- placing fewer than 3 channels, its CFs are as distant as ex- pected. In terms of Linf norm, the rankings vary among classifiers and no method significantly outperforms others. See the critical difference diagram in the Appendix for de-"}, {"title": "Detailed information", "content": "Sparsity and Sensitivity In terms of sparsity, the pattern is similar to L\u2081 and L2 norms and we suspect the same un- derlying reason. In terms of sensitivity, similar to univari- ate datasets, heuristic methods like NUN_CF, COMTE, and SET perform well.\nSegment sparsity SETS and COMTE perform well in NumSeg. SETS only changes a few Shapelets to flip the models and COMTE replaces several channels entirely.\nPlausibility Similar to the univariate datasets, NUN_CF is the most plausible. Similar to NG, COMTE performs well, likely because its CFs are a combination of two real in- stances. SETS and TSEvo perform poorly in plausibility.\nGeneration Time Similarly, NUN_CF ranks first, fol- lowed by SETS and COMTE, while TSEvo performs poorly. Notably, SETS times out on the Heartbeat and NATOPS datasets, which have 61 and 24 channels, respectively. It is likely because SETS tries combinations of all channels when it fails with just one, leading to an exponential increase in computational cost with the number of channels. TSEvo times out in the Eigen Worms dataset with more than 100,000 feature points. We exclude Heartbeat, NATOPS, and TSEvo for fair comparisons. The detailed generation time and time- out with FCN, is provided in the Appendix."}, {"title": "Consistency Evaluation", "content": "Different models even with the same architecture might cap- ture different features. Consistent CFs are robust to this difference. To assess consistency, we compute the percent- age of valid CFs across different models. To simplify, we use binary classification datasets and only consider high- performing models, defined as those with over 90% test accuracy on univariate datasets. We train another group of FCN models from scratch with different random initializa- tions and only use datasets that have over 90% test accu- racy on both FCNs. Four datasets, namely, GunPoint (GP), PowerCons (PC), Strawberry (SB), and Wafer (WF), are included. Additionally, only include instances that are cor- rectly predicted by both models.\nSpecifically, we compute consistBC which is the per- centage of consistent CFs among the total number of correctly predicted instances. Additionally, we calculate consistBV which is the percentage of consistent CFs among the valid CFs in the correctly predicted instances. The former metrics evaluate the ability to generate consis- tent consistent CFs given a dataset. The latter metric evalu- ates the ability to generate consistent CFs among those that are valid.\nThe results are shown in Table 5. When ranking the con- sistency, we ignore NUN_CF since it uses a training instance as a CF and it is expected that its consistency metric to be very high. In the SB dataset, NUN_CF doesn't achieve 1.0 consistency. That might be because NUN_CF uses a few training samples that have different predictions between two FCN classifiers. In general, NG and SETS generate CFs that have high consistency across the models. NG wins in the consist BC, showing it generates the largest amount of con- sistent CFs in those four datasets. SETS wins in consistBV, and although it can't always generate valid CFs, the valid ones it creates are usually consistent between models. wCF and TSEvo perform poorly in consistency, their CFs are only valid in the model they are applied to and can hardly be gen- eralized into other classifiers."}, {"title": "Discussion of Sparsity, Sensitivity and Segment Sparisity", "content": "We observe a large portion of imperceptible changes in wCF's results. If these minor changes are irrelevant to model prediction, then the vanilla LO gives an overestimation of the number of changed features. As shown in Figure 26a, the CF from the GunPoint dataset appears very sparse and shows only two positions of change at time steps 50 and 82. However, the computed vanilla Lo result is 1.0, indicating that all time steps are changed, which contradicts the obser- vation. After we utilize Thresh Lo with 0.1% data range as the threshold, only the two points are counted as changes, and the sparsity is correctly represented as 0.013.\nWhen those minor changes have an impact on model pre- diction, then the CF becomes totally uninterpretable and vi- sualization conveys misleading information. As shown in Figure 26b, the CF appears to change the decreasing edges and another subsequence at the end. However, there is a mi- nor change at time step 35 which is imperceptible. When we use the original instance value at time step 35 in CF then the prediction is back to the original prediction, making it no longer a valid CF. This instance is therefore considered sensitive and misleading.\nThe threshold of threshold LO is set at 0.25% of the origi- nal data range since approximately a default figure generated by the matplotlib package is 4.8 inches with 300 dpi result- ing in 1440 pixels in total, and the default line width is 2 pixels. If the difference is less than 0.25%, the difference in the figure is approximately only less than 4 pixels, and the instance lines will overlap with each other. This is relatively conservative choice since in practice, end users prefer bolder lines for better visualizations.\nIn terms of NumSeg, if we directly computed the num- ber of segments, the instance in Figure 26a only has one seg- ment since all the points are changed and they are connected. However, we can observe two locations of changes. Thus, we only considered the changes greater than the threshold and computed NumSeg based on that. This resulting the correct NumSeg as 1. However, this action causes another"}, {"title": "Supplemental Case Studies", "content": "The GunPoint dataset is firstly provided by (Ratanama- hatana and Keogh 2004) and serves as a binary-class prob- lem. The two classes are class Gun-Draw and Point. For the Gun-Draw class, the actors first have their hands by their sides, then draw a replicate gun from a hip-mounted holster, point it at a target for approximately one second, and then re- turn the gun to the holster and their hands to their sides. For the Point class, the actors point with their index fingers to a target for approximately one second and then return their hands to their sides. The values in the time step track the ac- tors' hand X-axis. Class 0 is Gun-Draw and Class 1 is Point.\nThe Chinatown dataset records the pedestrian activity in the Chinatown of the City of Melbourne, Australia in 24 hours. Class O is the traffic on weekends and class 1 is on weekdays.\nThe visualization with quantitative metrics is shown in Figure 27. As for the visualization of the Chinatown dataset, in Figure 27a, wCF creates an abrupt increase at 19 o'clock. The proximity (L1 and L2) and sparsity metrics are good. Given that the training samples include no instance with such high traffic at that time and the change is abrupt, re- sulting in large Linf and latent distances. In Figure 27b, NG replaces the traffic from 0 to 4 o'clock, providing a clear message that compared with weekdays, weekends have more pedestrian activities at midnight which gradually de- creases to nearly zero. NG achieves a relative performance of proximity, and plausibility in this instance. In Figure 27c, TSEvo changes the values at midnight but also modifies the values from 12 o'clock to late at night, and the great amount of changes results in large proximity and latent distances. In Figure 27e, SETS replace the time from 1 o'clock to 10 o'clock with a Shapelet of decreasing. In Chinatown, time series are perfectly aligned by the hour in nature, so the Shapelet replacement went smoothly and shows a similar pattern as NG, resulting in good plausibility performance."}]}