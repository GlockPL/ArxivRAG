{"title": "Virtual Compiler Is All You Need For Assembly Code Search", "authors": ["Zeyu Gao", "Hao Wang", "Yuanda Wang", "Chao Zhang"], "abstract": "Assembly code search is vital for reducing the burden on reverse engineers, allowing them to quickly identify specific functions using natural language within vast binary programs. Despite its significance, this critical task is impeded by the complexities involved in building high-quality datasets. This paper explores training a Large Language Model (LLM) to emulate a general compiler. By leveraging Ubuntu packages to compile a dataset of 20 billion tokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC), capable of compiling any source code of any language to assembly code. This approach allows for virtual compilation across a wide range of programming languages without the need for a real compiler, preserving semantic equivalency and expanding the possibilities for assembly code dataset construction. Furthermore, we use ViC to construct a sufficiently large dataset for assembly code search. Employing this extensive dataset, we achieve a substantial improvement in assembly code search performance, with our model surpassing the leading baseline by 26%.", "sections": [{"title": "Introduction", "content": "Reverse engineering frequently entails the challenging task of swiftly locating specific functions within extensive binary files, such as those associated with malicious behavior. Traditionally, reverse engineers rely on the tedious approach of searching for unique strings (Hex-rays, 2023a) or constants (polymorf, 2020) to locate these functions. This practice often leads to inefficiency as it heavily relies on experience or heuristic algorithms, frequently consuming considerable time.\nWe observe that the specific search requirements articulated by reverse engineers could often be distilled into natural language descriptions. This realization sparked the idea of employing a code search (Lu et al., 2021; Feng et al., 2020) perspective to refine the extant methodologies. Code search has achieved substantial success in high-level programming languages, garnering wide commercial application (GitHub, 2018). Assembly code search enables users to search the assembly (i.e. assembly code) function with natural language, enhancing user interaction with binary executable files through natural language.\nThe construction of assembly code search datasets presents considerable difficulty. Unlike high-level programming languages where datasets can be readily created by parsing the docstring of function, constructing an assembly code search dataset entails an obligatory compilation step from source code to assembly code. It is exemplified by the work conducted in Nova+ (Jiang et al., 2023). Despite attempts to compile over 4 million C programs and 1 million C++ programs, only 32,774 C programs and 40,087 C++ programs were successfully compiled. Numerous compilation failures stem from the complex compilation environment, including diverse dependencies and variations in optimization levels and compilers, greatly complicating dataset construction. Previous efforts target the compilation challenges of single-source C functions through type inference or neural compilation (da Silva et al., 2021; Guo and Moses, 2022; Armengol-Estap\u00e9 and O'Boyle, 2021). Yet, the practical utility of these advancements is constrained by the scarce availability of code search datasets for C language.\nInspired by Meta's use of Large Language Models (LLMs) for compiler optimization (Cummins et al., 2023), we postulate a novel modeling paradigm-let LLMs emulate a general compiler and understand the comprehensive compilation process, including optimization options and assembly code generation. This method not only enables us to utilize the previous code search dataset in C/C++ language like CCSD (Liu et al., 2021) but also demonstrates the capability to generalize across languages such as Python and Golang, despite only being trained on C/C++ source and assembly code pairs. This flexibility suggests the potential of applying extensive datasets from prior works, like CodeSearchNet (Husain et al., 2020), to the realm of assembly code search, addressing the historical constraints of dataset scarcity in this field.\nAddressing the challenges in assembly code search and the shortcomings of current methodologies, our work makes the following contributions:\n\u2022 Introduction of the virtual compiler (ViC). We introduce a novel approach, ViC for creating an assembly code search dataset. By employing the virtual compiler, we generate a diverse and robust dataset that circumvents the traditional barriers related to compilation challenges, vastly enriching the resources available for assembly code search tasks.\n\u2022 Enhanced assembly code search performance. We constructed a high-quality assembly code dataset using a virtual compiler, and the model trained on this dataset achieved a 26% performance improvement in assembly code search tasks over existing state-of-the-art (SOTA) solutions.\n\u2022 We release our models and datasets to facilitate future research\u00b9."}, {"title": "Background and Related Works", "content": "2.1 Assembly Code Analysis\nThe process of transforming high-level, human-readable source code (such as C or C++) into assembly code (also known as binary code) that a CPU can execute directly is known as compilation.\nBinary reverse engineering is a commonly used methodology in binary code analysis. It entails analyzing binary files without the source code to reconstruct the function functionality and partial program logic. In this complex process, one of the challenges is locating the specific function with certain functionality, such as encryption and authentication. The common methods include searching for representative strings (Hex-rays, 2023a), analyzing function import and export, looking for pattern matching (polymorf, 2020), static analysis and dynamic debugging. These methods, while useful, can be time-consuming and imprecise. To overcome these limitations, the development of assembly code search models marks a significant advancement, offering a more efficient and accurate method of navigating binary files.\n2.2 Assembly Code Modeling\nWith high-level logical structure replaced by the bare jump instructions, understanding and modeling the assembly code has quite different challenges compared to the high-level language. To effectively address these challenges, researchers have developed various techniques to model the assembly code for the downstream tasks such as binary code similarity detection (BCSD). This technique is pivotal for assessing the similarity between different assembly codes which can be affected by varying aspects like compiler versions and optimization levels and identifying the paired assembly code compiled from the same source code from an amount of candidates, useful for clone detection and supply chain analysis. CodeCMR (Yu et al., 2020) integrates GNN, DPCNN, and LSTM to extract the semantic features of source code and binary code. PalmTree (Li et al., 2021) pre-trains on unlabeled binary corpus through self-supervised training to capture various characteristics of assembly languages. Trex (Pei et al., 2021) models micro traces with Transformers, while jTrans (Wang et al., 2022) uses unique jump-aware representation method preserves control flow information. These models, especially the Transformer-based models, have a partial ability to capture the semantics in the function to support the code search task.\n2.3 Code Search\nCode search plays a pivotal role in software development, enabling developers to query and retrieve valuable code snippets. This process leverages either pattern matching or, more recently, natural language queries, thanks to advancements in semantic code search. The most code search works hinges on a pipeline that begins with pre-training models on extensive corpora consisting of unpaired code and natural language and accompanies by fine-tuning the model on specialized code search datasets, which sharpens their ability to understand and execute code search queries accurately (Wang et al., 2023, 2021; Lu et al., 2021; Feng et al., 2020; Guo et al., 2021)."}, {"title": "Overview", "content": "We present our workflow in Figure 2, with three phases to advance assembly code search capabilities. Initially, we compile a vast collection of packages from Ubuntu to obtain a mapping between source and assembly code. Following this, we train the CodeLlama model on this massive dataset to emulate compiler behavior (Section 4). Lastly, leveraging this model, we perform virtual compilations on existing code search datasets to produce an augmented assembly code search dataset. This enriched dataset then informs the training of an assembly code encoder, yielding a refined model for effective assembly code search (Section 5)."}, {"title": "Virtual Compiler", "content": "4.1 Code Dataset Construction\nIn our pursuit to train a model to compile the source code of the individual function to its corresponding assembly code, we compile over 6,000 C/C++ packages from Ubuntu using two types of x86-64 compilers, GCC and Clang, each in three different versions (GCC-{7,9,11} and Clang-{9,11,12}) and five optimization levels (O{0-3} and Os). Then, we extract the source and assembly code for the model training. The model exclusively provides the source code of the function body and is required to predict the corresponding assembly code. To reduce the hallucinations of the model, we exclude assembly code containing inline functions due to compiling optimization to ensure a one-to-one correspondence between the source code and assembly. Additionally, we omit functions shorter than 5 lines. These often represent class constructors, which will generate disproportionately large volumes of assembly code. However, for simplicity in source code processing, global variables, and macro definitions are not provided to the model. Ultimately, we amass a dataset consisting of 15 million source-to-assembly pairs, exceeding 20 billion tokens. We filter out source functions with fewer than four lines in the function body, remove function comments, and prefer unmangled function names in the assembly code. A validation set of 1,000 unseen function source codes paired with corresponding assembly code is segregated to assess the output quality.\n4.2 Model Training\nContrasting with Meta's work (Cummins et al., 2023) that begins with random initialized weight, we employ Codellama 34B (Rozi\u00e8re et al., 2023) as our foundational model, anticipating a more rapid convergence (Chen et al., 2021). Subsequent experiment shows that leveraging Codellama also provides enhanced comprehension of various programming languages. The model is trained using Supervised Fine-Tuning (SFT). Besides the function source code, the prompt includes the compiler, optimization level, and whether the assembly code should be stripped, denoting the removal of symbol information, such as local variable names.\nConsidering the length of the vast majority of inputs in the CodeSearchNet, we opt for a 4,096 context length during training instead of the 16,384 natively supported by CodeLlama, aiming for a faster training process. We initially focus on training sets with less than 2,048 tokens, followed by a training mix that includes a 3:1 ratio of 0-2,048 and 2,048-4,096 token-length data for better training efficiency. The model is trained on 64 NVIDIA A100 GPUs, exceeding a total of 1,000 GPU days, employing a cosine learning rate schedule with 1% of warm-up steps. The initial phase operates at a peak learning rate of 3e-5 with a batch size of 3,072, while the subsequent phase uses a peak learning rate of 1.5e-5 and a batch size of 2,048. Each dataset underwent a single epoch."}, {"title": "Code Search Contrastive Learning", "content": "5.1 Dataset\nWe use two datasets to conduct the code search contrastive learning. The first dataset we utilize is CCSD (Liu et al., 2021), which comprises function summarization from over 95,000 functions from 300 different projects with an extensive deduplication and cleaning process. Furthermore, we apply virtual compilation to CodeSearchNet (Husain et al., 2020), a challenge introduced by GitHub featuring a dataset and a corresponding benchmark. This dataset is the bedrock of contemporary code search research nowadays. CodeSearchNet spans multiple programming languages - Python, JavaScript, Ruby, Go, Java, and PHP - none of which are directly applicable to assembly code search due to their higher-level nature.\nIn constructing our virtual assembly dataset for model training, each function from CCSD and CodeSearchNet is subjected to a randomly chosen combination of compiler and optimization levels, thus generating their virtual assembly codes. This approach allows us to expand the applications of these comprehensive, multi-language datasets into the realm of assembly code search.\n5.2 Model Architecture\nBuilding on the revelations from jTrans (Wang et al., 2022), we acknowledge the need for specialized treatment of textual and assembly code representations. To this end, we decouple the text encoder and assembly code encoder, adopting a bespoke architecture for the assembly code encoder, allowing each encoder to become adept at handling the intricacies of its respective data modality.\nFor the assembly code encoder, we use a roformer-base (Su et al., 2022) model that incorporates shared parameters (Wang et al., 2022) with 110M parameters to better integrate the inductive bias of control flow in assembly code. And we keep string literals in assembly code to better preserve semantic information through Byte-Pair Encoding instead of normalizing them, which is a common simplification in previous assembly modeling work (Li et al., 2021; Pei et al., 2021; Wang et al., 2022). On the other side, the text encoder is initialized by the well-established sentence-transformers (Reimers and Gurevych, 2019).\n5.3 Assembly Code Encoder Training\nDuring the training of the assembly code encoder, we employ the commonly used in-batch negative sampling strategy and use InfoNCE loss as the training loss, shown as Equation 1 and 2. This method aims to increase the similarity between positive pairs within a batch while reducing the similarity across negative pairs. The text encoder is initialized from sentence-transformers, and the assembly encoder is pre-trained with MLM (Masked Language Model) and JTP (Jump Target Prediction) tasks. With the text encoder well-established, we set the learning rate to 1e-6 to reduce the disturbance, and the learning rate for assembly code is set to 2e-5 empirically (Wang et al., 2022). We take the model with the best in-dataset evaluation result for further evaluation.\nGive a batch of positive assembly code and text pairs $B = \\{(a_1, t_1), (a_2, t_2),..., (a_n, t_n)\\}$. We treat each pair $(a_i, t_i)$ as positive pair, and $(a_i, t_j)_{i \\neq j}$ as negative pairs. The text-to-assembly contrast loss is defined as:\n$L_1 = -\\frac{1}{n} \\sum_{i=0}^{n} log \\frac{exp(t_i a_i / T)}{\\sum_{j=1}^{n} exp(t_i a_j / T)}$\nThe auxiliary loss, assembly to text contrast loss, is inversely defined as:\n$L_2 = -\\frac{1}{n} \\sum_{i=0}^{n} log \\frac{exp(a_i t_i / T)}{\\sum_{j=1}^{n} exp(a_i t_j / T)}$\nwhere T is the temperature, which we set to 0.07 empirically (Radford et al., 2021; He et al., 2020). Then the train loss is defined as $L = L_1 + L_2$."}, {"title": "Evaluation", "content": "6.1 Evaluation Setup\nWe implement ViC using Pytorch 2.1 (Paszke et al., 2019). We use IDA Pro 8.3 (Hex-rays, 2023b) to disassemble and extract the functions from the binary executable file in all of the experiments. Our training and experiments are conducted on several servers to accelerate training. The CPU setup is 128 cores with 2TB RAM for each server. The total GPU setup is 32 NVIDIA Tesla A100.\n6.2 Evaluation of Virtual Compiler\nWe endeavor to appraise the quality of assembly code yielded by the virtual compiler, with an emphasis on the model's capability to generate assembly code for augmenting the code search dataset.\nWe employ two distinct approaches for evaluating the model's capability. For code with ground truth, namely, C/C++ source code with corresponding assembly code, we assess quality using various similarity metrics in Section 6.2.1-6.2.3. For high-level languages without direct assembly code ground truth, such as PHP and Golang from the CodeSearchNet dataset, we use the downstream task in Section 6.3 and a case study in Section A to demonstrate the model's capabilities.\nDuring the dataset construction phase, we preserve 1,000 C/C++ functions with their source code and assembly code that the model has not previously trained in the training process for evaluation.\n6.2.1 Sequence Similarity\nA concise evaluation of the generated assembly code is carried out using the BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR score (Banerjee and Lavie, 2005), which are employed to show the similarity between generated assembly code and the reference ground truth. The scores are shown in Figure 3, revealing a clear trend in the data that as the number of tokens processed during model training increased, there is a commensurate rise in the scores for each metric.\n6.2.2 Runtime Similarity\nWe further study the runtime characteristics of the assembly code generated by the virtual compiler. We implement an enforced execution of assembly code instructions using Keystone (Keystone, 2024) and Unicorn (Unicorn, 2024), executing any assembly code by randomly initializing the CPU's registers and memory state. We execute assembly code generated by the real and virtual compilers, respectively, and record their memory reads and writes. To avoid infinite loops, we set a maximum executed instruction count of 2,000.\nWe compare the function return value, the local stack of the function, and the sequence of memory accesses as indicators for studying the runtime characteristics of functions. Specifically, we use a very strict evaluation scheme to compare whether the rax register, which holds the return value, is equal at the end of execution, whether the stack registers rsp and rbp are equal, and whether the sequence of memory reads and writes are equal. We denote the average of these three indicator as RUNTIME in Figure 3. As the number of training tokens increases, the runtime characteristics of the virtual assembly become increasingly similar to those of the ground truth.\n6.2.3 Semantic Similarity\nComplementing the BLEU score, we adopt a more contextualized assessment method, Binary Code Similarity Detection (BCSD), aligned with our code search task. In this task, we use CLAP (Wang et al., 2024) for evaluation. As shown in Figure 3, after training on only 0.5 billion tokens, the virtual compiler is already capable of producing assembly code achieving a BINSIM score above 0.8, and it nears convergence after processing 5 billion tokens. In the context of BCSD tasks, a BINSIM score exceeding 0.8 typically indicates a strong positive pair of functions, with their variations likely attributable to different compilers or optimization options. This signifies that the assembly code generated by our model is perceived as being remarkably close to the ground truth.\n6.2.4 Case Study\nGiven the complexity inherent in the virtual compilation process, perfect replication of compiler-generated results is a considerable challenge. This complexity is primarily due to the extensive presence of addresses, global variables, and the use of structure offsets in the code, which add intricate layers to the compilation task. While we utilize quantitative measures like BLEU to gauge the similarity between the model-generated and compiler-generated assembly code, we recognize that such metrics might not capture the full spectrum.\nIn Figure 4, we present a segment of a large function to showcase the comparison between the model-generated assembly code and the real compiler-generated counterpart. A cursory glance reveals a multitude of differences. However, a thorough manual inspection reveals several distinct categories of mismatches, which we have highlighted in the figure using different colors for clarity.\nThe mismatches highlighted in green signify differences in the addresses. These discrepancies arise from the model's inability to obtain addresses during the virtual compilation. Cyan highlights indicate alternative expressions of the same operation. For example, the ground truth assembly executes the operation rdi = 8 * (rax+1) using lea edi, [rax+1]; shl rdi, 3, whereas the virtual compiler opts for lea rdi, ds:8[rax*8]. Although semantically equivalent, such variations are not captured by string-matching metrics like BLEU.\nThe third category of mismatches, marked in yellow, corresponds to differences in register allocation during code generation. The actual compiler may choose a different set of registers compared to the virtual compiler. Grey highlights denote inaccuracies in stack allocation and usage due to the model's unawareness of stack variables' sizes.\nFinally, an unresolved category of divergence, not explicitly marked in the figure, reveals additional instructions provided by one compiler that are absent in the other's output, and vice versa. This discrepancy underscores the inherent unpredictability and complexity when comparing outputs from two different compilers."}, {"title": "Evaluation of Assembly Code Search", "content": "In this section, we validate the enhancement of the model's code search capabilities brought by the virtual compiler. Moreover, we aim to indirectly demonstrate the quality of the model-generated virtual assembly code and the model's capability to handle high-level languages.\n6.3.1 Training Dataset\nSimilar to other code search datasets, we use the docstrings collected from the source code for comparison and apply cleaning steps in Section B. Out of 6,000 packages and 5 million source code functions, we extracted 400,000 docstrings, indicating that only a small fraction of functions are accompanied by docstrings. We also use the virtual assembly code in Section 5.1, including the code search dataset from CodeSearchNet and CCSD. We show their statistics for each dataset in Table 1. We denote the assembly code dataset generated by ViC as VirtualAssembly, denote the extracted docstring dataset as UbuntuDocstring, and denote the mixture of these two datasets as HybridAssembly.\n6.3.2 Evaluation Dataset\nTo construct the evaluation dataset of assembly code search that can represent the real-world scenario, we collect multiple popular binaries from the real world that have no overlap with our training set, from three different platforms: Mac, Windows, and Linux. Notably, although the model is trained using assembly code exclusively from Linux, the x86 instruction set assembly code has the same syntax across different operating systems, thereby enabling the model's cross-OS transferability. Then we perform reverse engineering on these software applications. During this process, by leveraging the expertise of reverse engineering professionals, we write code search queries for functions that exhibit relatively independent functionalities. These manually crafted queries, derived from real-world reverse engineering efforts, reflect the practical requirements encountered during actual reverse engineering tasks. This approach underscores our commitment to ensuring that the evaluation closely mimics real-world scenarios. Finally, we obtain an evaluation set containing 257 queries and relevant function pairs.\n6.3.3 Baselines\nPreviously in binary analysis research, the concept of assembly code search has not been explicitly proposed. Research efforts predominantly revolve around binary code similarity detection (BCSD), which only targets to compare the similarity between two assembly codes, instead of the natural language level functionality.\nConsequently, we use some general embedding models for comparison, including sentence-transformers (Reimers and Gurevych, 2019)\u00b2, the initialization model for the text encoder in Section 5.3; GTE (Li et al., 2023)\u00b3, recognized as one of the premier open-access encoders on the MTEB leaderboard; Voyage AI models\u2074, including the voyage-code-2 model optimized for code retrieval; and the OpenAI embedding models (Neelakantan et al.)\u2075.\n6.3.4 Metric\nDuring our manual reverse engineering process, we often encounter scenarios where a single function can be described in multiple ways, or multiple functions implement similar functionalities. In such situations, the recall is defined as follows. Suppose {a\u1d62} is the set of relevant functions, and {b\u1d62} is the retrieved functions. And there are q queries trying to retrieve the desired functions. The the recall@k is defined as follow:\n$recall@k = \\frac{1}{q} \\sum_{i=1}^{q} \\frac{|\\{a_i\\} \\cap \\{b_i\\}|}{min(|\\{a_i\\}|, |\\{b_i\\}|)}$\nwhen $|\\{b_i\\}| = k$.\nMoreover, we adopt MAP (Mean Average Precision) to better measure the ranking capability of the model, which is defined as:\n$AP = \\frac{1}{q} \\sum_{i=1}^{q} \\sum_{k} P@k \\times rel@k$\nwhere $P@k$ refers to precision@k:\n$P@k = \\frac{|\\{a_i\\} \\cap \\{b_i\\}|}{|\\{b_i\\}|}$, when $|\\{b_i\\}| = k$\n6.3.5 Main Result\nIn Table 2, our comparative analysis highlights the performance gap between our method and baseline models, notably sentence-transformers, GTE, and voyage-2. These general models tend to underperform in code search tasks involving assembly code due to their limited coding data during training, particularly the complex assembly code which needs an understanding of control flows. Their reliance on textual snippets solely within assembly code for semantic cues, missing the control flow information, often leads to suboptimal results in both recall and MAP metrics. In contrast, the voyage-code-2 and voyage-large-2 models demonstrate better performance, attributed to their more robust training on diverse code datasets compared to voyage-2.\nModels trained on the HybridAssembly dataset, starting from an MLM (Masked Language Modeling) pre-trained base model, still show significant improvements over OpenAI's models. This underscores a critical gap in the capability of existing code search models to effectively support assembly code search, indicating that these models lack comprehensive training with extensive assembly code datasets necessary to achieve a functional level for aiding binary code analysis.\n6.3.6 Virtual v.s. Real Assembly Code\nWe analyze the effectiveness of virtual assembly code against actual assembly code in assembly code search tasks. By virtually compiling source code from UbuntuDocstring into assembly code, we train two models using the real and virtual assembly code respectively, combined with docstrings, and compared their performance. The result is detailed in Table 3, along with the result of excluding small functions from both sets of data (denoted as w/o Small), which follows our approach during the training phase in Section 4.1.\nThe performance analysis reveals that excluding small functions, both sets of assembly code demonstrate similar outcomes, indicating that the quality of virtual assembly code is on par with real assembly code. Not training the model on small functions is intended to reduce model hallucinations, but this leads to minimal performance gains and poses a risk of generating irrelevant code when processing shorter source code. Conversely, the actual compiler-generated assembly for small functions aligns well with the corresponding docstrings, providing useful insights albeit not ideal for training purposes. This slight performance dip in the UbuntuDocstring dataset without small functions underscores the nuanced impact of training data selection on model behavior.\n6.3.7 Dataset Contributions\nThis section evaluates how each dataset influences the assembly code search performance, as shown in Table 4. Training solely with the VirtualAssembly dataset underperformed compared to using the UbuntuDocstring dataset. Detailed analysis of specific failures highlighted the representation challenge in assembly language. For instance, low-level operations like the \"keccakf function\" in SHA3 or \"destroying a linked list\" are broadly abstracted in higher-level languages through library calls, and are missing in broader datasets like CodeSearchNet. The absence of these concepts affects performance finally.\nFurther evaluations involve removing language components from the VirtualAssembly dataset, as detailed in Table 4 with rows prefixed with w/o. Generally, omitting any single language dataset negatively impacts the results. Surprisingly, excluding the PHP dataset improved performance, likely due to suboptimal PHP handling by the virtual compiler, with the balanced performance in the HybridAssembly dataset suggesting compensatory benefits from better-matched docstrings.\nAdditionally, the analysis identifies a significant drop in performance using the CodeSearchNet dataset alone, especially without C (CCSD). This degradation highlights the importance of incorporating low-level concepts like \"linked list\" and \"keccakf function\" in training sets for assembly code search, even if these assembly codes are not compiled from a real compiler."}, {"title": "Limitation & Discussion", "content": "7.1 Code Search Dataset Quality\nThe quality of the assembly code search dataset we construct is intrinsically tied to the underlying code search datasets, which are predominantly derived from docstrings, such as the CodeSearchNet dataset utilized in this study. In the evaluation in CodeSearchNet, 32.8% of docstrings were found to be irrelevant to the source code. In our evaluation, a random sample of docstrings extracted from the Ubuntu source code exhibited a comparable proportion of loosely related docstrings. This observation underlines a significant limitation: relying solely on docstrings as the basis for code search datasets may introduce challenges due to the potential lack of strong relevance or association. It's conceivable to generate code search datasets directly from source code using LLMs. However, with the capability to compile different programming languages, we can greatly augment the assembly code search dataset without bias.\n7.2 Broader Impacts\nWith the emergence of Large Language Models, code search has gained prominence as a crucial component within Retrieval-Augmented Generation (RAG) (Chase, 2024; Gao et al., 2023). With assembly code search as an RAG module, it helps the LLM to rapidly locate the desired functions with a large binary for faster reverse engineering.\nWhile reverse engineering can be vital for security experts in vulnerability detection and analysis, it also poses risks of exploitation by malicious actors for software cracking and exploiting vulnerabilities. This duality highlights the necessity for ethical considerations and responsible use.\n7.3 Future Work\nWe acknowledge that the evaluation dataset is relatively small, limited by the intensive labor and extensive analysis required for reverse engineers. Future work will focus on expanding and diversifying the evaluation dataset to enable a more comprehensive assessment of assembly code search.\nIn the model training in Section 5.3, we train the assembly code model from scratch. However, with better model initialization from previous works such as CLAP (Wang et al., 2024) and PalmTree (Li et al., 2021), we can achieve better assembly code search performance and faster convergence."}, {"title": "Conclusion", "content": "In this study, we introduce a pioneering method to improve assembly code search by training an LLM to function as a virtual compiler, ViC, effectively addressing the challenge of compiling difficulties and enhancing dataset quality for assembly code. This approach not only broadens the scope of languages compilable to assembly but also significantly boosts the performance of assembly code search tasks, outperforming all the existing solutions. Our results demonstrate the potential of virtual compilers to revolutionize reverse engineering processes, offering a promising direction for future advancements in software engineering and security analysis."}]}