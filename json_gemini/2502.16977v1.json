{"title": "Convergence of Shallow ReLU Networks on Weakly Interacting Data", "authors": ["L\u00e9o Dana", "Loucas Pillaud-Vivien", "Francis Bach"], "abstract": "We analyse the convergence of one-hidden-layer ReLU networks trained by gradient flow on $n$ data points. Our main contribution leverages the high dimensionality of the ambient space, which implies low correlation of the input samples, to demonstrate that a network with width of order $\\log(n)$ neurons suffices for global convergence with high probability. Our analysis uses a Polyak-\u0141ojasiewicz viewpoint along the gradient-flow trajectory, which provides an exponential rate of convergence of $\\frac{1}{n}$. When the data are exactly orthogonal, we give further refined characterizations of the convergence speed, proving its asymptotic behavior lies between the orders $\\frac{1}{\\sqrt{n}}$ and $\\frac{1}{n}$, and exhibiting a phase-transition phenomenon in the convergence rate, during which it evolves from the lower bound to the upper, and in a relative time of order $\\log(n)$.", "sections": [{"title": "1 Introduction", "content": "Understanding the properties of models used in machine learning is crucial for providing guarantees to downstream users. Of particular importance, the convergence of the training process under gradient methods stands as one of the first issues to address in order to comprehend them. If, on the one hand, such a question for linear models and convex optimization problems (Bottou et al., 2018; Bach, 2024) are well understood, this is not the case for neural networks, which are the most used models in large-scale machine learning. This paper focuses on providing quantitative convergence guarantees for a one-hidden-layer neural network.\nTheoretically, such global convergence analysis of neural networks has seen two main achievements in the past years: (i) the identification of the lazy regime, due to a particular initialization, where convergence is always guaranteed at the cost of being essentially a linear model (Jacot et al., 2018; Arora et al., 2019; Chizat et al., 2019), and (ii) the proof that with an infinite amount of hidden units a two-layer neural network converges towards the global minimizer of the loss (Mei et al., 2018; Chizat and Bach, 2018; Rotskoff and Vanden-Eijnden, 2018). However, neural networks are trained in practice outside of these regimes, as neural networks are known to perform feature learning, and experimentally reach global minimum with a large but finite number of neurons. Quantifying in which regimes neural networks converge to a global minimum of their loss is still an important open question."}, {"title": "", "content": "This article aims to identify such a regime, characterized by low-correlated inputs, under which we can rigorously prove the convergence of shallow neural networks trained via gradient flow. The previous assumptions needed for convergence relied on special initialization scales (Chizat et al., 2019; Boursier et al., 2022), on an infinite number of neurons (Jacot et al., 2018; Chizat and Bach, 2018), or on the exact orthogonality of the input data (Boursier et al., 2022; Frei et al., 2023). In this article, we aim to simply fix the dimension $d$, and ask how many randomly sampled examples can be interpolated by the network with high probability. This high dimensionality regime, corresponding to $d$ being larger than $n^2$, is our main assumption to prove convergence of shallow ReLU neural networks. In addition, we are interested in understanding the general dynamics of the system, and in particular the system's convergence speed to a global minimizer of the loss.\nWe summarize our contributions in the analysis of the learning dynamics of a one-hidden-layer ReLU network on a finite number of data $n$ via gradient flow.\n\u2022 Our main contribution is that shallow neural networks in high dimension $d > Cn^2$ interpolates exactly random whitened data with high probability as soon as the neural network has more that $\\log(n)$ neurons, for any initialization scale. We also show that the loss converges to zero exponentially fast with a rate at least of order $\\frac{1}{n}$.\n\u2022 Then, when the inputs are orthogonal, we refine our analysis in order to characterize the range of possible asymptotic speeds, which we find to be at most of order $\\frac{1}{\\sqrt{n}}$. Moreover, we conjecture that this speed is always of the highest order $\\frac{1}{\\sqrt{n}}$ with high probability and verify empirically this claim.\n\u2022 Finally, for orthonormal inputs and a special initialization of the network, we highlight a phase transition in the convergence rate during the system's evolution, and compute the associated cut-off time and transition period."}, {"title": "2 Problem Setup", "content": "Notations. We use $||v||$ to denote the euclidean norm of a vector $v$, $(\\cdot|\\cdot)$ its scalar product, and $||M||$ for the operator norm associated with $||\\cdot||$ of a matrix $M$. Moreover, let $v = \\frac{v}{||v||}$.\nLoss function. Let $(x_i, y_i)_{i=1:n} \\in (\\mathbb{R}^d \\times \\mathbb{R})^n$ be a sample of input vectors and real outputs. Let $d \\in \\mathbb{N}^*$ be the dimension of the vector space and $n \\in \\mathbb{N}^*$ the number of data points. In order to learn the regression problem of mapping $x_i$ to $y_i$, we use one-hidden-layer ReLU neural networks\u00b9, which we write:\n$h_\\theta(x) = \\frac{1}{p} \\sum_{j=1}^p a_j \\sigma((w_j|x)),$  (1)\nwhere $p \\in \\mathbb{N}^*$ is the number of units, $\\sigma(x) = \\max{0, x}$ for $x \\in \\mathbb{R}$ is the rectified linear unit (ReLU), and the parameters are gathered in $\\theta = (a_j, w_j)_{1<j<p} \\in (\\mathbb{R} \\times \\mathbb{R}^d)^p$. To"}, {"title": "", "content": "simplify the ReLU notation, we define $\\sigma((w_j|x_i)) = (w_j|x_i)_+$ and $\\mathbb{1}_{(w_j|x_i)>0} = \\mathbb{1}_{j,i}$. When mentioning neurons of the network, we refer to $(w_j|x_i)_+$, while second layer neurons refer to $a_j$. Neurons can be activated if $(w_j|x_i)_+ > 0$, and are correctly activated if moreover $a_jy_i > 0$. Upon this prediction class and data, we analyse the regression loss with square error,\n$\\mathcal{L}(\\theta) := \\frac{1}{2n} \\sum_{i=1}^n (y_i - h_\\theta(x_i))^2.$ (2)\nAs soon as $d \\geq n$, $(x_i)_{i=1:n}$ can form a free family, in which case the set of minima of $\\mathcal{L}$, which consists of all interpolators, is non-empty.\nGradient flow. In order to understand a simplified version of the optimization dynamics of this neural network, we study the continuous-time limit of gradient descent. We initialize $\\theta_{t=0} = \\theta_0$ and follow for all $t > 0$ the ordinary differential equation\n$\\frac{d}{dt} \\theta_t = - p \\nabla_\\theta \\mathcal{L}(\\theta_t),$ (3)\nwhere we choose a particular element of the sub-differential of the ReLU $\\sigma'(x) = \\mathbb{1}_{x>0}$, for any $x \\in \\mathbb{R}$. This choice is motivated by both prior empirical work from Bertoin et al. (2021) and theoretical work from Boursier et al. (2022, Proposition 2) and Jentzen and Riekert (2023). We also decided to accelerate the dynamics by a factor $p$ as only this scaling gives a consistent mean field limit for the gradient flow when the number of neurons tends to infinity (see Definition 2.2 by Chizat and Bach (2018)).\nWeight invariance. The 1-homogeneity of the ReLU provides a continuous symmetry in the function $\\theta \\mapsto h_\\theta$ and hence the loss\u00b2. This feature is known to lead automatically to invariants in the gradient flow as explained generally by Marcotte et al. (2024). The following lemma is not new (Wojtowytsch, 2020, p.11), and shows that, from this invariance, we deduce that the two layers have balanced contributions throughout the dynamics.\nLemma 1. For all $j \\in [1, p]$, for all $t \\geq 0$, $|a_j(t)|^2 - ||w_j(t)||^2 = |a_j(0)|^2 - ||w_j(0)||^2$, and thus, if $|a_j(0)| \\geq ||w_j(0)||$, then $a_j(t)$ maintains its sign and $|a_j(t)| \\geq ||w_j(t)||$.\nInitialization. Throughout the paper, we initialize the network's weights $w_j$ and $a_j$ from a joint distribution where both marginals are non-zero, centered, rotational-invariant, have finite covariance, and we take norms of $a_j$ and $w_j$ independent of $d, n, p$. Moreover, we need an assumption of asymmetry of the norm at initialization.\nAssumption 1 (Asymmetric norm initialization). We assume that the weights of the network at initialization satisfy $\\forall j \\in [1, p], |a_j(0)| \\geq ||w_j(0)||$.\nArticles by Boursier and Flammarion (2024a,b) already used this assumption to study two-layer neural networks in order to use the property described in Lemma 1\u00b3."}, {"title": "", "content": "Data. We define the data matrix $X = (x_1, ..., x_n) \\in [\\mathbb{R}^d]^n$. Denote $C_F = \\min_i ||x_i||$ and $C_y = \\min_i |y_i|$; in what follows, we suppose that $C_F > 0$ and $C_y > 0$, i.e., the input and output data are bounded away from the origin. Similarly, we also let $C_F^+ = \\max_i ||x_i||$ and $C_y^+ = \\max_i |y_i|$. We note $Cty$ to refer to the set of these constants. Finally, we introduce the following hypothesis on the low correlation between the inputs.\nAssumption 2 (Low correlated inputs). We assume that the data satisfy\n$||X^T X - D_X|| < \\frac{(C_F^-)^2 C_y^-}{\\sqrt{n} C_y^+},$ (4)\nwhere $D_X$ denotes the diagonal matrix with coefficients $||x_i||^2$.\nThe term $||X^T X - D_X||$ is a control on the magnitude of the correlations $((x_i, x_j))_{i\\neq j}$. As an extreme case, when it equals zero, the inputs are orthogonal. This assumption is purely deterministic at this stage. Later, we show that this weak interaction between the inputs is highly likely to occur for random whitened vectors in high dimensions (see Corollary 1).\nDimensions. Throughout the paper, even if the results provided are all non-asymptotic in nature, the reader can picture that the numbers $n, p, d$ (respectively data, neurons and dimension) are all large. Moreover, they verify the following constraint: $n$ is less than $d$, and $p$ can be thought of the order $\\log(n)$, meaning only a \u201clow\u201d number of neurons is required."}, {"title": "2.1 Related works", "content": "Convergence of neural networks. Neural networks are known to converge under specific data, parameter, or initialization hypotheses, among which: the neural tangent kernel regime studied by Jacot et al. (2018); Arora et al. (2019); Du et al. (2019); Allen-Zhu et al. (2019), that has been shown to correspond in fact to a lazy regime where there is no feature learning because of the initialization scale. Another field of study is the mean-field regime, where feature learning can happen but where the optimization has been shown to converge only in the infinite width case (Mei et al., 2018; Chizat and Bach, 2018; Rotskoff and Vanden-Eijnden, 2018). Note that it is also possible to produce generic counter examples, where convergence does not occur (Boursier and Flammarion, 2024b). Beyond these, there have been attempts to generalize convergence results under local PL (or local curvature) conditions as shown by Chatterjee (2022); Liu et al. (2022); Zhou et al. (2021), but they remain unsatisfactory to explain the good general behavior of neural networks due to the constraint it imposes on the initialization. Convergence theorems similar in spirit to Theorem 1 can be found in an article by Chen et al. (2022). The main difference relies on two features: only the inner weights are trained and their result necessitates a large value of outer weights when $n$ is large, which is the regime of interest of the present article. Finally, it is worth mentioning other works on neural networks dynamics, e.g., the study of the implicit bias either for regression (Boursier et al., 2022) or classification (Lyu and Li, 2020; Ji and Telgarsky, 2020), or sample complexity to learn functions in a specific context (Glasgow, 2023)."}, {"title": "3 Convergence in high dimension", "content": "In this first section, our goal is to understand when the gradient flow converges toward a global minimizer of the loss. Note that the parametrization of the prediction function $h_\\theta$ by a neural network often implies the non-convexity of the objective $\\mathcal{L}$ and prevents any direct application of convex tools in order to ensure global convergence. Generally speaking, even if gradient flows are expected to converge to critical points of the parameter space (Lee et al., 2016), such that $\\nabla_\\theta\\mathcal{L}(\\theta) = 0$, they might become stuck in local minimizers that do not interpolate the data."}, {"title": "3.1 Local PL-curvature", "content": "Convexity is not the only tool that provides global convergence: as known in the optimization community, showing that $\\frac{||\\nabla \\mathcal{L}(\\theta_t)||^2}{\\mathcal{L}(\\theta_t)}$ is uniformly lower bounded suffices. As mentioned in Section 2.1, this is known as the Polyak-Lojasiewicz condition (Polyak, 1964). Taking a dynamical perspective on this, we define a trajectory-wise notion of this \"curvature\" condition which we name the local-PL curvature of the system, and define for all $t \\geq 0$,\n$\\mu(t) := p \\frac{||\\nabla \\mathcal{L}(\\theta_t)||^2}{\\mathcal{L}(\\theta_t)} = \\frac{\\frac{d}{dt} \\mathcal{L}(\\theta_t)}{\\mathcal{L}(\\theta_t)},$ (5)\nwith the second equality being a property of the gradient flow. Intuitively, this coefficient describes the curvature in parameter space that $\\theta_t$ \u201csees\u201d at time $t > 0$. The following lemma is classical and shows how it can be used to prove the global convergence of the system, as well as a quantification on the rate.\nLemma 2. Let $\\langle\\mu(t)\\rangle := \\frac{1}{t} \\int_0^t \\mu(u) du$ the time average of the local-PL curvature, which we name the average-PL curvature. We have $\\mathcal{L}(\\theta_t) = \\mathcal{L}(\\theta(0))e^{-\\langle\\mu(t)\\rangle t}$.\nHence, if the total average-PL curvature $\\langle\\mu_\\infty\\rangle := \\lim_{t\\rightarrow\\infty} \\langle\\mu(t)\\rangle$ is strictly positive, we can deduce an upper bound on the loss and convergence to 0 at the exponential speed $\\langle\\mu_\\infty\\rangle$. This shows that the average-PL curvature is actually the instantaneous exponential decay rate of the loss, and thus controls the speed at which the system converges."}, {"title": "3.2 Global convergence of neural networks for weakly correlated inputs", "content": "We are ready to state the main theorem of the paper on the minimization of the loss.\nTheorem 1. Let $\\varepsilon > 0$, $p > 4\\log(\\frac{n}{\\varepsilon})$, and suppose Assumption 1. We fix the data $(x_i, y_i)_{1<i<n}$ and suppose it satisfies Assumption 2. Then with probability at least $1 - \\varepsilon$ over the initialization of the network, the loss converges to 0 with $\\langle\\mu_\\infty\\rangle >= \\frac{C}{n}$, where we define $C = \\frac{(C_F^-)^2 C_y^-}{C_y^+}$. Moreover, for any $t \\geq 0$, we have the lower bound\n$\\mu(t) \\geq \\frac{C}{n} \\min \\{1 - \\frac{r_i(t)}{y_i}\\},$ (6)\nNote that, at best, the number of neurons required in Theorem 1 is logarithmic. This finiteness stands in contrast with the infinite number required in the mean-field regime, and the polynomial dependency typical of the neural tangent kernel (NTK) regime (Jacot et al., 2018; Allen-Zhu et al., 2019). In the orthogonal case, the ReLU makes the $\\log(n)$ dependency necessary and sufficient, as shown in Lemma 5, as the residual $r_i$ goes to zero if and only if a neuron gets initialized as $a_jy_i > 0$ and $(w_j|x_i) > 0$ for each $i$.\nAssumption 2 is crucial for this proof: it means that the examples are insufficiently correlated with each other for the weights to collapse onto a single direction. As proved by Boursier and Flammarion (2024a, Theorem 1), the direction $w^* = \\arg \\min_{\\{\\theta = \\{w,a\\}\\}} \\mathcal{L}(\\theta)$ will attract all neurons if it is accessible from anywhere on the initialization landscape\u2074. This phenomenon known as early alignment and first described by Maennel et al. (2018), will prevent interpolation if examples are highly correlated (Boursier and Flammarion, 2024a, Theorem 2). The fact that our result holds for any initialization scale shows that near-orthogonal inputs prevent accessibility to $w^*$ and make the early alignment phenomenon benign, as found by Boursier et al. (2022); Frei et al. (2023).\nNote finally that our norm-asymmetric initialization (Assumption 1) is sufficient for global convergence with high probability, but may not be necessary. That said, we present in Appendix C.1 a detailed example of interpolation failure when the assumption is not satisfied."}, {"title": "", "content": "Convergence in high dimension. In this paragraph we assume that the data $(x_i, y_i)_{i=1:n}$ are generated i.i.d. from some distribution $P_{x,y}$. We first show that, with high probability, Assumption 2 is almost always valid if the dimension is larger than the square root of the number of data points. Additionally, we assume that the law anti-concentrates at the origin. These two features are gathered in the following lemma.\nLemma 3. Let $(x_i, y_i)_{1<i<n}$ be generated i.i.d. from a probability distribution $P_{x,y}$ such that the marginal $P_x$ is sub-Gaussian, has zero-mean, and satisfy $E_{x\\sim P_x}[xx^T] = \\lambda I_d$ for some $\\lambda$ independent of $d, n$, while, on $\\mathbb{R}^*$, the marginal law $P_y$ has compact support.\u2075 There exists $C > 0$ depending only on the constants $Cty$ and the initialization weights, such that, if $d > C \\left(n^2 + n \\log(\\frac{1}{\\varepsilon})\\right)$, then, with probability $1 - \\varepsilon$, Assumption 2 is satisfied."}, {"title": "3.3 Sketch of Proof", "content": "The proof of convergence relies on three key points: (i) the loss strictly decreases as long as each example is activated by at least a neuron, (ii) for a data point, if there exists a neuron which is activated at initialization, then at least one neuron remains activated throughout the dynamics, (iii) At initialization, condition (ii) is satisfied with large probability. Let us detail shortly how each item articulates with one another.\n(i). First, Lemma 6, stated and proved in Appendix, shows that, by computing the derivatives of the loss, we get a lower bound on the curvature\n$\\mu(t) \\geq \\frac{2}{n} \\left(\\frac{(C_F^-)^2}{C_y^+} - ||X^T X - D_X||\\right) \\min_i \\{\\frac{1}{p} \\sum_{j=1}^p |a_j|^2 \\mathbb{1}_{j,i}\\}$ (7)\nTo prove the strict positivity, one needs to show that $||X^T X - D_X||$ is small enough, and that for each data $i$, there exists $j$ such that $|a_j|^2 \\mathbb{1}_{j,i}$ is strictly positive. Thanks to the initialization of the weights, $|a_j|^2 \\geq |a_j(0)|^2 - ||w_j(0)||^2 > 0$, and to Assumption 2, $\\frac{(C_F^-)^2}{\\sqrt{2} C_y^+} > ||X^T X - D_X||$. Thus, we have convergence if at any time, for any data input, one neuron remains active, i.e., formally, for all $t > 0$, and all $i \\in [1, n]$, there exists $j \\in [1, p]$ such that $(w_j(t)|x_i)_+ > 0$. Hence, the loss decreases as long as one neuron remains active per data input. We see next how to show this crucial property.\n(ii). Let us fix the data index $i \\in [1, n]$, and $y_i > 0$ without loss of generality. Let us define $j^* = \\arg \\max_{a_j y_i > 0} \\langle w_j(t)|x_i\\rangle$ the index of the largest correctly initialized neuron. Since $a_j$ cannot change sign thanks to Assumption 1, $\\langle w_{j^*}(t)|x_i\\rangle$ is continuous, and has a derivative over each constant segment of $j^*$. The strict positivity of this neuron is an invariant of the dynamics: if $r_i \\geq y_i$, the derivative of the neuron shows it increases, and if $r_i < y_i$, the residual has decreased, which implies that the $\\langle w_i(t)|x_i\\rangle$ is strictly positive. Thus, if a neuron is correctly initialized for the data point $i$, a neuron stays active throughout the dynamics.\n(iii). Finally, Lemma 5 shows $P\\left(\\forall i, \\exists j, \\langle w_j(0)|x_i\\rangle > 0 \\cap a_j y_i > 0\\right) \\geq 1 - n \\left(\\frac{3}{4}\\right)^p$, which implies that for $p > 4 \\log(\\frac{n}{\\varepsilon})$, the network is well initialized with probability at least $1 - \\varepsilon$."}, {"title": "4 Orthogonal Data", "content": "In this section, we go deeper on the study of the gradient flow, assuming that the input data are perfectly orthogonal, or equivalently that $||X^T X - D_X|| = 0$. Since most of the intuition for the convergence is drawn from the orthogonal case, it offers stronger results which we detail. In particular, we are able to closely understand the local-PL curvature $(\\mu(t))_{t\\geq 0}$ evolution and asymptotic behaviour."}, {"title": "4.1 Asymptotic PL curvature", "content": "Theorem 1 has shown that the local-PL curvature is lower bounded by a term of order $\\frac{1}{n}$, allowing us to show an exponential convergence rate of this order. The following proposition shows that in the orthogonal case the curvature can also be upper bounded.\nProposition 1. Let $\\varepsilon > 0$. Given orthogonal inputs, Assumption 1, $d$ large enough, and $p \\geq 4 \\log(\\frac{n}{\\varepsilon})$, with probability $1 - \\varepsilon$, we have an upper-bound on the local-PL curvature: there exists $C > 0$ depending only on the initialization and the constants $Cty$ such that for all $t > C\\frac{n}{P}$,\n$\\mu(t) \\leq C \\max_i \\{\\frac{\\mathbb{P} }{n} \\sqrt{1 - \\frac{r_i(t)}{y_i}} + \\frac{C}{n}\\}$ (8)\nThis upper bound uses two properties that are characteristic of the orthogonal case. First, once a neuron is inactive on some data input, then, it can never re-activate again. The second property is that for an initialization scale independent on $n$, there is a phase during which correctly initialized neurons increase while the others decrease to 0. This extinction phase, proved in Lemma 7, is short in comparison to the time needed to fit the residuals, and leaves the system decoupled between positive and negative outputs $y_i$.\nIn the limit where $n$ goes to infinity, Proposition 1 shows that the network does not learn since the local-PL is 0. This is an artifact of the orthogonality of the inputs: the interaction between inputs should accelerate the dynamics. However, although all quantities have well defined limits as $n \\rightarrow +\\infty$, the limits cannot be understood as a gradient descent in an infinite dimensional space\u2077.\nProposition 1 is in fact valid for $p$ fixed, and an initialization of the weights for which every data is correctly initialized by a neuron. In that case, Proposition 1 shows that the"}, {"title": "", "content": "asymptotic curvature cannot be larger than the order $\\frac{1}{n}$. While the local-PL curvature is between the order $\\frac{1}{\\sqrt{n}}$ and $\\frac{1}{n}$, the next proposition shows that any intermediate order $\\frac{1}{n^{\\alpha}}$, for $\\alpha \\in [\\frac{1}{2}, 1]$, can be reached asymptotically, with strictly positive probability, using a particular initialization of the network.\nGroup initialization. In the following, we use $p_n$ to denote the number of neurons, and partition the $n$ data points in $p_n$ groups of cardinality $k_n$ (note that $p_nk_n = n$). We re-index the examples per group as by $(x^q_i, y^q_i) = (x_{i+(j-1)k_n}, y_{i+(j-1)k_n})$, for all $i \\in [1, k_n]$ and $j \\in [1, p_n]$. Moreover, we use a special initialization of the network such that for all $j, q \\in [1, p_n], i \\in [1, k_n]$,\n$\\begin{cases} \\langle w_j|x^q\\rangle > 0 \\text{ if } j = q \\\\ \\langle w_j|x^q\\rangle \\leq 0 \\text{ if } j\\neq q\\end{cases}$ and $a_j = s_j||w_j||,$ (9)\ni.e., $w_j$ is correctly activated on the group $j$ only.\nProposition 2. Suppose the group initialization described above, with orthonormal inputs, i.e., $X^T X = I$, and that the signs of all outputs of the group $j$ are equal to $s_j$. We fix $k_n = n^{2(1-\\alpha)}$ with $\\alpha \\in [\\frac{1}{2}, 1]$. Then, for $t > Cn^{3\\alpha-1}\\log (Cn)$, the local-PL curvature satisfies\n$\\frac{K_1}{n^{\\alpha}} \\leq \\mu(t) \\leq \\frac{K_2}{n^{\\alpha}},$ (10)\nwhere $C = \\max(C_y^+, (\\frac{\\mathbb{P}}{\\sqrt{2}})\\, K_1 = 2C \\min\\{\\frac{min_j |a_j(0)|^2}{\\Sigma_{i=1}^n ||x_i||^2},\\frac{ ||w_j(0)||^2}{\\Sigma_{i=1}^n ||x_i||^2}\\}$ and $K_2 = 4C_y^+$.\nProposition 2 states that any asymptotic value $(\\mu_\\infty) \\in [\\frac{1}{\\sqrt{n}}, \\frac{1}{n}]$ can be achieved with strictly positive probability using group initialization. But of what order is the most likely limit of the curvature for standard initialization? Experiment 5.2 in Section 5.2 suggest that, with high probability, the asymptotic curvature is always of the order $\\frac{1}{\\sqrt{n}}$.\nConjecture 1. Let $\\varepsilon > 0$. There exist $C_1, C_2 > 0$ depending only on the data and the initialization, such that for $p > C_1\\log(\\frac{n}{\\varepsilon})$ and for orthogonal examples, with probability at least $1 - \\varepsilon$ over the initialization of the network, we have convergence of the loss to 0 and\n$\\langle\\mu_\\infty\\rangle = \\frac{C_2}{\\sqrt{n}}$ (11)"}, {"title": "4.2 Phase transition in the PL curvature", "content": "In the previous section, we emphasized the asymptotic order of the local-PL curvature with respect to $n$ and hypothesized that it is of the order $\\frac{1}{\\sqrt{n}}$ in most cases. In this section, we are interested in the evolution of the local-PL curvature during the dynamics. Lemma 4 below computes the local-PL curvature at initialization in the large $p$ regime, and shows that initially it is of order $\\frac{1}{n}$.\nLemma 4. At initialization, the local-PL curvature $\\mu(0)$ is a random variable which satisfy $\\sqrt{p}(\\mu(0) - \\beta_0) \\longrightarrow N(0, \\gamma_0)$, and with $\\beta_0, \\gamma_0$ depending only on the data and the distributions of the network's neurons."}, {"title": "5 Experiments", "content": "In this Section, we aim to perform deeper experimental investigations on the system, which we could not do formally. Precisely, we want to answer two questions:\n1. What is the probability that the loss reaches 0 for $n$ data points in dimension $d$, under the distributional hypotheses of Lemma 3 (sub-Gaussian, zero-mean and whitened data)? What is the maximum $n$ for a fixed $d$ such that global convergence holds with high probability ?\n2. In the orthogonal case, is the asymptotic exponential convergence rate of order $\\frac{1}{\\sqrt{n}}$ (on average over the initialization) as stated in Conjecture 1?\nThe data and weights distribution which have been used for the experiments below can be found in Appendix B, and the code is available on GitHub."}, {"title": "5.1 Probability of Convergence", "content": "This section aims to test the limit in which Corollary 1 holds when the number of data points increase. Intuitively, as the number of examples $n$ grows, the neural network becomes less and less overparametrized, and hence is expected to fail to globally converge. Knowing if and when this occurs with high probability is important for us to understand how much our current threshold $C\\sqrt{d}$ can be improved. We thus plot the probability of convergence, as well as the loss at convergence to obtain additional information when the probability is zero. We train 500 one-layer neural networks with the normalization presented in Section 2, dimension $d = 100$, $n$ ranging from 2500 to 3500, and $p_n = C\\log(n)$ neurons. Additional details on the training procedure can be found in Appendix B."}, {"title": "5.2 Empirical asymptotic local-PL curvature", "content": "In this section we test Conjecture 1, and to do so we measure $\\mu(t)$ during the dynamics, and mostly at the end of the dynamics, since we know by Lemma 4 that near 0 the local-PL curvature is of order $\\frac{1}{n}$. To provide the strongest evidence for the conjecture, we measured the order of the local-PL curvature in three ways: by directly measuring the local-PL $\\mu(t_{\\infty}) = \\log \\left(\\frac{\\mathcal{L}(t_{\\infty} - 1)}{\\mathcal{L}(t_{\\infty})}\\right)$ at the last epoch $t_{\\infty}$, by measuring the average-PL curvature $\\langle\\mu_\\infty\\rangle = \\frac{1}{t_{\\infty}} \\log\\left(\\frac{\\mathcal{L}(t_0)}{\\mathcal{L}(t_{\\infty})}\\right)$, and finally by mesuring the lower and upper bounds on the local-PL given in Lemma 6.\nFollowing Conjecture 1, all approximations should likely be decreasing in $\\frac{1}{\\sqrt{n}}$ as $n$ increases. To show this, we plot the log-log graph of each measure above. We train 500 networks in dimension $d = 2000$, with $n$ ranging from 1000 to 2000, and $p_n = C\\log(n)$. All resulting plots appear linear in the log-log scale, with a slope close to $-\\frac{1}{2}$ (see Figure 5 in Appendix B), meaning that the scalings are indeed in $\\frac{1}{\\sqrt{n}}$. This empirically confirms our conjecture that the local PL curvature has order $\\frac{1}{\\sqrt{n}}$ asymptotically."}, {"title": "6 Conclusion", "content": "We have studied the convergence of the gradient flow on a one-hidden-layer ReLU networks on finite datasets. Our analysis leverages a local Polyak-\u0141ojasiewicz viewpoint on the gradient-"}]}