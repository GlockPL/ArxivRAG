{"title": "Benchmarking Robust Self-Supervised Learning Across Diverse Downstream Tasks", "authors": ["Antoni Kowalczuk", "Jan Dubi\u0144ski", "Atiyeh Ashari Ghomi", "Yi Sui", "George Stein", "Jiapeng Wu", "Jesse C. Cresswell", "Franziska Boenisch", "Adam Dziedzic"], "abstract": "Large-scale vision models have become integral in many applications due to their unprecedented performance and versatility across downstream tasks. However, the robustness of these foundation models has primarily been explored for a single task, namely image classification. The vulnerability of other common vision tasks, such as semantic segmentation and depth estimation, remains largely unknown. We present a comprehensive empirical evaluation of the adversarial robustness of self-supervised vision encoders across multiple downstream tasks. Our attacks operate in the encoder embedding space and at the downstream task output level. In both cases, current state-of-the-art adversarial fine-tuning techniques tested only for classification significantly degrade clean and robust performance on other tasks. Since the purpose of a foundation model is to cater to multiple applications at once, our findings reveal the need to enhance encoder robustness more broadly. Our code is available at github.com/layer6ai-labs/ssl-robustness.", "sections": [{"title": "1. Introduction", "content": "Foundation models trained through self-supervised learning (SSL) have become the backbone of many applications due to their versatility; one foundation model can be adapted to many downstream tasks with a small amount of data and training (or fine-tuning). Foundation models in the vision domain have even outperformed dedicated models on several tasks (Caron et al., 2021; He et al., 2022; Oquab et al., 2024). Despite their broad utility, the adversarial robustness of these models has only been explored for classification tasks with linear probing (Naseer et al., 2020; Jiang et al., 2020; Fan et al., 2021; Zhang et al., 2022; Luo et al., 2023) while other common downstream tasks, such as semantic segmentation (Long et al., 2015) and depth estimation (Godard et al., 2019), remain unexplored. Recently, Li et al. (2023) showed that non-robust features extracted from adversarial examples for supervised models (and useful for classification) become largely useless when transferred to self-supervised learning paradigms. They advocated for a cross-paradigm examination of robustness, yet focused their analysis solely on classification. A major outstanding question is whether adversarial robustness transfers across downstream tasks.\nWe present an in-depth empirical evaluation of the adversarial robustness of self-supervised vision encoders (Chen and He, 2021; Caron et al., 2021) for downstream tasks beyond classification. We use attacks that operate in the encoder's embedding space (EmbedAttack) and those that leverage direct access to the downstream task outputs (PGDAttacks), e.g., PGD for classification (Madry et al., 2018) or SegPGD for semantic segmentation (Gu et al., 2022). Our main observation is that the state-of-the-art adversarial full fine-tuning of encoders (Zhang et al., 2022): (1) substantially lowers clean performance, (2) increases robustness only against the EmbedAttack, and (3) remains ineffective in improving robustness against the task-specific PGDAttacks. We observe only a slight improvement against the PGDAttacks for classification when the adversarial fine-tuning dataset and downstream dataset come from the same distribution. This indicates a need to rethink what it means for a foundation model to be robust. Finally, we offer potential approaches to bolster the cross-task robustness of SSL encoders."}, {"title": "2. Background and Related Work", "content": "Self-Supervised Learning. SSL aims to extract a representation of data which is useful for downstream tasks specified at test-time (Balestriero et al., 2023). In many frameworks, an input x is first modified by two semantic-preserving augmentations yielding x1 and x2, which are subsequently passed to an encoder f. The training objective aligns the output representations by minimizing a distance metric d (e.g., Euclidean distance) as L(f,x) = d(f(x1), f(x2)) (Chen et al., 2020). Once trained, the encoder's representations are then used for various downstream tasks, such as classification, semantic segmentation, or depth estimation by fine-tuning adaptor networks. In this work, we focus on a state-of-the-art SSL framework, DINO (Caron et al., 2021). DINO utilizes two encoder networks, the teacher ft, and student fs. The student network is optimized to minimize the cross-entropy between fs(x1) and the soft labels ft(x2), as a form of knowledge distillation (Hinton et al., 2015). To prevent collapse, the gradients are only passed through fs. Parameters of ft are updated using the moving average of the student's parameters. DINOv2 (Oquab et al., 2024) improves over DINO in terms of scale and efficiency of training, rather than proposing a new SSL method. Oquab et al. (2024) showed substantial improvements on dense (pixel-wise) downstream tasks like semantic segmentation and depth estimation compared to DINO encoders.\nAdversarial Robustness in SSL. In this work, we focus on the state-of-the-art Decoupled Adversarial Contrastive Learning (DeACL) framework by (Zhang et al., 2022) to obtain robust SSL encoders. For an overview on other methods for robust SSL and a thorough discussion on the advantages of DeACL, see Appendix B.1. DeACL fine-tunes existing encoders for increased robustness using knowledge transfer from a pre-trained encoder to a robust one. The objectives for the distillation are to: (1) match the distilled encoder representations to those of the pre-trained encoder (high cosine similarity), and (2) bring the distilled encoder's representations of adversarial examples (i.e., examples generated with the pre-trained encoder that maximize the distance to their original samples) close to their clean counterparts. By decoupling the encoder pre-training from increasing its robustness, DeACL provides high computational efficiency in comparison to prior methods and obtains state-of-the-art robust performance.\nDownstream Tasks. To evaluate the quality of representations learned by SSL methods, we consider three common downstream tasks. (1) Linear Classification assesses the quality of the learned representations by training a downstream classifier and measuring classification performance. (2) Semantic Segmentation is a common computer vision task that categorizes every pixel in an image into a class or object. While downstream-agnostic adversarial examples against SSL encoders can be used to fool segmentation models, Gu et al. (2022) show with SegPGD that tailoring the attack to the segmentation task is even more effective. SegPGD aims at manipulating all pixel classifications of an image by introducing a weighted loss term between correctly classified and misclassified pixels. (3) Depth Estimation is another prevalent computer vision task aimed at estimating distances of objects in an image relative to the camera location, where each pixel is assigned a depth value. Targeted adversarial attacks against depth estimation can lead to strong deviations between actual and predicted depth (Wong et al., 2020). At the same time, they can also be leveraged for depth estimation-specific adversarial training to improve robustness (Cheng et al., 2022)."}, {"title": "3. Attack and Defense Methods", "content": "We propose a framework to assess the robustness of foundation models at both the embedding level and for downstream tasks, as described in Section 2. The goal of benchmarking the robustness of foundation models across diverse downstream tasks restricts our possible selection of encoder models. Specifically, the encoder must generate representations that are applicable to a variety of tasks beyond classification. In our preliminary experiments, we evaluated the performance of SimCLR (Chen et al., 2020), SimSiam (Chen and He, 2021), and DINO encoders. We observed that the representations produced by SimCLR and SimSiam were insufficient to achieve high-quality downstream segmentation or depth estimation. For that reason, we use the foundation models DINO and DINOv2 as examples, and train a linear adaptor for each downstream task. For the embedding attack, we target the model at the representation layer. For downstream attacks, we evaluate three different tasks: classification, semantic segmentation, and depth estimation. Each attack is detailed in the following sections."}, {"title": "3.1. Embedding-level Attack", "content": "The EmbedAttack operates directly on the underlying encoder's embeddings (Kim et al., 2020; Jiang et al., 2020; Fan et al., 2021; Luo et al., 2023). The objective behind the approach is to make imperceptibly small modifications to an input image such that the resulting representation from the SSL encoder is changed substantially. More concretely, for a clean input image x, we find its adversarial perturbation xadv = x + \u03b4 such that ||\u03b4||\u221e < \u03b5, where \u03b5 is the maximum allowed input distortion measured in the l\u221e-norm. Given an encoder f, the objective is to find xadv such that the l2 distance between the representations from"}, {"title": "3.2. Downstream Attacks", "content": "Classification. For the standard classification tasks, we use the PGD attack (Madry et al., 2018) with settings similar to those above: \u03b5 = 8/255, 20 steps with step size 2/255, and initialization from randomly perturbed images. The target is to maximize cross-entropy loss for the perturbed images.\nSemantic Segmentation. To attack semantic segmentation we leverage the SegPGD attack (Gu et al., 2022) which calculates a weighted average of the loss over correctly and incorrectly classified pixels,\nL(fseg(xadv), y) = \\frac{1}{HW} \u03a3_{j \u2208 P^T} A_t L_j + \\frac{1}{HW} \u03a3_{k \u2208 P^F} L_k. (1)\nHere Lj represents the cross-entropy loss, At is a hyperparameter, H and W denote the height and width of the image, while PT and PF are the sets of correctly and incorrectly classified pixels respectively. PGD is used to find adversarial examples with this loss, and we use similar settings as mentioned previously. The weight At starts from zero and increases linearly each iteration. The main insight behind the SegPGD attack is to fool correctly classified pixels in the first attack iterations and then treat the correct and incorrect pixel classifications roughly equally in the later iterations. As a result, the SegPGD attack can achieve similar attack effectiveness as PGD but with substantially fewer iterations.\nDepth Estimation. Similarly to semantic segmentation, we compute the average loss per pixel and then apply a PGD attack targeting this loss, referred to as DepthPGD. The loss terms used for depth estimation and its attack are akin to those in Oquab et al. (2024), incorporating the multi-scale gradient matching loss (Li and Snavely, 2018) and pixel-wise depth loss (Farooq Bhat et al., 2021). For more details, refer to Appendix C."}, {"title": "3.3. DeACL Defense", "content": "We combat the above attacks using the state-of-the-art method of obtaining robust encoders, DeACL (Zhang et al., 2022). We select DeACL for several compelling reasons. Firstly, unlike many other methods aimed at enhancing robustness, it does not rely on any specific downstream task and instead improves robustness at the representation layer in a self supervised manner. Besides, it is a state-of-the-art method with superior robustness compared to other techniques. Lastly, the proposed adversarial fine-tuning approach is significantly more computationally efficient compared to training models from scratch using traditional adversarial training methods. These advantages make DeACL feasible and practical, particularly given the substantial computational resources required to train state-of-the-art encoder models. We start from a pretrained encoder f and create its robust version fR using fine-tuning with the following objective:\nL(fR, f) = d(fR(x), f(x)) + \u03b3d(fR(xadv), fR(x)). (2)\nHere we set d as the standard cosine similarity, and x as the input image. Equation (2) aims to preserve representation quality, and improve robustness against adversarial examples. \u03b3 = 2 is a parameter used to balance the impact of each goal on the final objective function."}, {"title": "4. Empirical Evaluation", "content": "4.1. Setup\nWe present the results for encoders trained using the DINO and DINOv2 SSL frameworks, utilizing ViT (Dosovitskiy et al., 2020) backbones. The underlying encoders are either Standard, i.e., provided by the SSL frameworks, or DeACL, further fine-tuned to enhance robustness. We present the hyperparameters that we use to train the linear layers for the various of downstream tasks. These hyperparameters are uniform across encoders and datasets, and vary only between different types of tasks, i.e., classification, semantic segmentation, and depth estimation. Full insights are presented in Appendix C.\nClassification. We use a learning rate of 0.5, batch size 16, and train the linear classifiers for 5 epochs using the Adam (Kingma and Ba, 2015) optimizer. As a train-time augmentation we use random horizontal flips.\nSemantic segmentation. We follow the setup from the DINOv2 framework, and use a learning rate of 0.0001, batch size 16, weight decay 0.001, and train for 50 epochs using the AdamW (Loshchilov and Hutter, 2018) optimizer. For training as well as evaluation on non-uniformly sized images (e.g., PASCAL VOC 2012) we utilize sliding window inference, i.e., we divide the image into parts of uniform size, compute logits for all of the parts, and then combine them into one final logit map. Overlap between the parts is handled by averaging the logits in the overlap regions. We use random cropping, and random horizontal and vertical flips as training-time augmentations.\nDepth estimation. Since DINOv2 has achieved state-of-the-art performance in depth estimation, we adopt their settings. For training, we use their combination of gradient matching loss and pixel-wise depth loss. For the remaining hyperparameters, we use a learning rate of 0.0001, batch size 128, weight decay 0.01, and train for 20 epochs using AdamW. All hyperparameters are listed in Appendix C.1."}, {"title": "4.2. Results", "content": "Classification. We follow the widely used linear evaluation protocol (Chen et al., 2020; Chen and He, 2021), where a linear classifier is trained on top of the frozen base SSL encoder, and test accuracy is used as a proxy for representation quality. We compare the classification accuracy after linear probing for the standard vision benchmarks: CIFAR10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al., 2009), and STL10 (Coates et al., 2011). The evaluation is presented in Table 1. Contrary to the results shown by Zhang et al. (2022), we observe no improvement in robustness against tailored PGD attacks (right column) for the encoder fine-tuned using DeACL, with the only exception being on the STL10 dataset. We argue that the discrepancy in our results and ones reported by Zhang et al. (2022) stems from the underlying training sets of the fine-tuned encoder. Zhang et al. (2022) utilized encoders trained on CIFAR10, then fine-tuned and evaluated them on CIFAR10 as well. In contrast, we focus on ImageNet-trained encoders, use ImageNet for fine-tuning, and evaluate them on various datasets including CIFAR10. We assume that the discrepancy between training, fine-tuning, and evaluation sets leads directly to the inefficacy of DeACL in obtaining robust encoders against stronger adversarial attacks than EmbedAttack, like PGD. This idea is supported by the improved adversarial accuracy against PGD attacks on STL10 with the fine-tuned encoder, as it is a subset of ImageNet. We observe an increase (above random guessing) in accuracy compared to the standard encoder (see second last and the last row of Table 1, rightmost column), from 0 to 0.23.\nSemantic segmentation. Similarly to classification, a single linear layer is trained on patch embeddings, to obtain a low-resolution logit map. Next, we interpolate the logits to obtain a logit map of a resolution matching the size of x. The minimized objective is a pixel-wise cross-entropy loss. We evaluate encoders on ADE20k (Zhou et al., 2017; 2019), CityScapes (Cordts et al., 2016), and PASCAL VOC 2012 (Everingham et al., 2010), and report mean Intersection over Union (mIoU\u2191) scores in Table 2. EmbedAttack proves to be a potent downstream task-agnostic method of obtaining adversarial examples for the segmentation task, achieving mIoU of 0 for all clean encoders across all datasets. Similarly to the linear classification task, we note that fine-tuning with DeACL improves robustness against EmbedAttack, however, it fails to achieve significant improvements for the downstream attack SegPGD.\nDepth estimation. For depth estimation, following Oquab et al. (2024), we extract the final layer of the frozen transformer and concatenate the CLS token with each patch token. Then we apply bilinear upsampling to the tokens to enhance the resolution. Finally, we train a linear layer on top to estimate the depth of each pixel. We evaluate quality of the depth estimation using the standard Root Mean Square Error (RMSE) metric on the NYU-Depth-v2 dataset (Silberman et al., 2012). Our results in the Table 3 show that the EmbedAttack and DepthPGD attacks significantly increase the RMSE. The only instance where the RMSE remains below 1 after an attack is with DeACL fine-tuning against the EmbedAttack; however, this fine-tuning fails to provide a notable improvement in robustness against the DepthPGD attack, similarly to the classification and semantic segmentation tasks.\nEvolution of Robustness During DeACL Fine-Tuning. Figure 2 presents the dynamics of model robustness for different downstream tasks during DeACL fine-tuning. Notably, robustness against PGD-based attacks exhibits minimal improvement, remaining unchanged during this process. The only exception is the improvement in robustness of linear classification on the STL10 dataset (which is a subset of ImageNet) observed during the first 20 epochs of training. We also observe that a relatively short period of fine-tuning-around 10 epochs-leads to noticeable improvements in robustness against EmbedAttack. However, further fine-tuning iterations show diminishing returns, with robustness metrics plateauing. Performance on clean data remains relatively stable throughout fine-tuning after a drop during the first 10 epochs. The simultaneous increase in robustness against EmbedAttack and decrease in performance on clean data observed at the start of the fine-tuning process confirms the trade-off between clean and adversarial model performance. The observed dynamics hold true across all downstream tasks. Our findings indicate that the adversarial fine-tuning method proposed by Zhang et al. (2022) exerts its greatest impact during the initial epochs, with little to no benefit from prolonging training to a larger (e.g. 100) number of epochs."}, {"title": "5. Discussion and Conclusions", "content": "SSL encoders are foundation models leveraged for a myriad of downstream vision tasks in critical domains, like autonomous driving (Liu et al., 2021) or medical imaging (Jiang et al., 2018). This motivates the necessity of ensuring the encoders' robustness. In this work, we argue that prior work on SSL encoder robustness mainly evaluates downstream classification tasks while leaving other popular tasks, such as semantic segmentation or depth estimation under-explored. Through our experimentation, we show that encoders are highly vulnerable to adversarial attacks on multiple downstream tasks, which pose a significant risk. Our results also highlight that the defenses that were developed with downstream classification in mind also harm the downstream performance on classification and other tasks. This suggests that more fundamental work is required to make foundational SSL encoders robust and effective for a wide variety of tasks.\nFuture directions for improving robustness. We observe that defenses against adversarial examples in SSL are effective only for a single attack type, namely EmbedAttack. However, they remain ineffective for other perturbations, especially task-specific attacks like PGD, SegPGD, and DepthPGD. To train SSL models that are simultaneously robust to multiple perturbation types, a potential solution is to apply multi-perturbation adversarial training, similar to the approach used for enhancing robustness in supervised models against various perturbations (Tram\u00e8r and Boneh, 2019), which involved concurrent adversarial training with first-order l1, l2, and l\u221e attacks. Therefore, to enhance the robustness of SSL encoders, we should not only fine-tune them on adversarial examples in the embedding space but also potentially perform robust tuning for each intended downstream task."}, {"title": "A. Societal Impact", "content": "Prior work on SSL encoder robustness has primarily focused on classification tasks, leading to a false sense of security among users. Our findings reveal that encoders are also susceptible to attack on other downstream tasks, underscoring the need for more comprehensive defenses. This paves the way for the development of robust solutions, thereby enhancing the trustworthiness and reliability of foundational SSL encoders for broader societal applications."}, {"title": "B. Extended Related Work", "content": "B.1. Adversarial Robustness in SSL\nFor supervised tasks, adversarial attacks produce imperceptible changes \u03b4 to an input x that result in the model predicting an incorrect label y (Biggio et al., 2013; Szegedy et al., 2013). To increase robustness, adversarial training (Goodfellow et al., 2014) incorporates the perturbed data with the correct label into the training data. Since SSL operates without labels, this approach is not directly applicable. The initial method towards robust SSL proposed by Naseer et al. (2020) introduces a purifier network to defend against adversarial examples, which attempts to recover the original input from an adversarially perturbed version before inputting it to the encoder. Robust contrastive learning (RoCL) (Kim et al., 2020) instead aims to make the encoder itself robust by maximizing the similarity between a random augmentation of a data point and its instance-wise adversarial perturbation. RoCL translates instance-level robustness to class-level robustness, at the cost of substantial degradation in clean performance.\nHo and Nvasconcelos (2020) propose adversarial examples specifically designed to challenge contrastive learning methods. Using these adversarial examples, they develop a novel adversarial training algorithm for self-supervised learning, which they call Contrastive Learning with Adversarial Examples (CLAE). Compared to standard contrastive learning, CLAE creates more difficult positive pairs by using adversarial examples. Additionally, by optimizing over all images in a batch, CLAE produces more challenging negative pairs through adversarial training. In essence, CLAE strengthens contrastive learning models by exposing them to tailored adversarial attacks during training.\nJiang et al. (2020) introduce adversarial contrastive learning (ACL) to improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. They extend SimCLR (Chen et al., 2020) to learn robust representations by maximizing feature consistency between differently augmented views. Fan et al. (2021) build on top of ACL and propose AdvCL, which leverages labels in addition to instance-level robustness to further boost robust performance. Luo et al. (2023) propose Dynamic Adversarial Contrastive Learning (DYNACL) as an extension that uses pseudo-labels directly generated by the pre-trained encoder. All these methods require retraining the large SSL encoders from scratch to improve robustness which is highly impractical and computationally expensive. To solve the problem, (Zhang et al., 2022) propose a two-stage framework called Decoupled Adversarial Contrastive Learning (DeACL) which fine-tunes existing encoders for increased robustness. Therefore, the knowledge of a pre-trained encoder is distilled to a robust one. The objective for the distillation are to: (1) match the distilled encoder representations to those of the pre-trained encoder, and (2) bring the distilled encoder's representations of adversarial examples close to their clean counterparts. Closeness is defined by cosine similarity, and adversarial examples are just those examples generated on the original trained encoder to maximize the distance to the original samples. A compelling aspect is that the decoupling approach of DeACL is not limited to contrastive learning - the original encoder could potentially leverage other self-supervised learning (SSL) methods. Only the distillation loss may need adaptation for SSL frameworks like MAE (He et al., 2022), where cosine similarity may not be optimal. Through this approach, DeACL sets a new state-of-the-art by effectively and efficiently improving encoder robustness. This is achieved by decoupling the SSL pre-training stage from the adversarial fine-tuning stage. The flexibility of DeACL leaves room for exploring different SSL methods in the first pre-training stage. Given the many advantages of DeACL demonstrated thus far, we focus our evaluation on this approach."}, {"title": "C. Hyperparameters", "content": "C.1. Further Insights on Depth Estimation\nThe multi-scale gradient matching loss (Li and Snavely, 2018) encourages smoother transitions in depth predictions and penalizes differences in log-depth gradients across multiple scales:\nLgrad = \\frac{1}{n} \u03a3_k \u03a3_i |\u2207_x R_i^k| + |\u2207_y R_i^k|. (3)\nThe loss is computed at multiple scales where R represents the value of the log-depth difference at position i and scale k. \u2207x and \u2207y denote the gradients in the x and y directions, respectively.\nThe pixel-wise depth loss (Farooq Bhat et al., 2021) measures the difference between the predicted and ground truth depth values in a scale-invariant manner:\nLpixel = \u03b1 \\sqrt{ \\frac{1}{n} \u03a3_i g_i^2 - \\frac{\u03b2}{n^2} (\u03a3_i g_i)^2}. (4)\nWhere gi = log di - log di, with di representing the predicted depth and di the ground truth depth. The parameters \u03b1 and \u03b2 are set to 1 and 0.85 in our experiments.\nThe final loss we use is \\frac{1}{2} Lgrad + Lpixel\u00b7"}, {"title": "C.2. DeACL fine-tuning", "content": "In this section, we describe the hyperparameters we adopt to perform the adversarial fine-tuning proposed by (Zhang et al., 2022) on DINOv1 with ViT B/16 backbone. We use a learning rate of 0.05 with a cosine scheduler and 10 epochs of warmup. We fine-tuned the model for 100 epochs with a SGD optimizer (momentum 0.9) and batch size of 128. The adversarial perturbation budget \u03b5 was set to 4/255. We did not use weight decay. We employed random crops, and random horizontal and vertical flips as training-time augmentations."}]}