{"title": "Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning", "authors": ["Eric Pasewark", "Kyle Montgomery", "Kefei Duan", "Dawn Song", "Chenguang Wang"], "abstract": "We present a new method for large language models to solve compositional tasks. Although they have shown strong performance on traditional language understanding tasks, large language models struggle to solve compositional tasks, where the solution depends on solving smaller instances of the same problem. We propose a natural approach to solve compositional tasks recursively. Our method, Re-Tuning, tunes models to break down a problem into subproblems, solve those subproblems, and combine the results. We show that our method significantly improves model performance on three representative compositional tasks: integer addition, dynamic programming, and parity. Compared to state-of-the-art methods that keep intermediate steps towards solving the problems, Re-Tuning achieves significantly higher accuracy and is more GPU memory efficient.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLM) have obtained the state-of-the-art performance on a wide set of tasks (Brown et al., 2020; Taylor et al., 2022; Chowdhery et al., 2022; Anil et al., 2023; OpenAI, 2023; Touvron et al., 2023a,b). However, recent studies (Anil et al., 2022; Dziri et al., 2023; Zhou et al., 2023b) show these models struggle to generalize to compositional tasks, where the solution depends on solutions to smaller instances of the same problem. An example task, integer addition, is shown in Figure 1a. When calculating '1234 + 4567', we first break the problem into a smaller subproblem '234 + 567'. After obtaining the solution to this subproblem, the original problem is partially solved. Similarly, to solve '234 + 567', we first sum '34 + 67'. This recursion is the fundamental operation to solve compositional tasks. However, no existing approach has explicitly captured the recursive nature of compositional tasks.\nIn this paper, we propose a recursion-based method for LLMs to better solve compositional tasks. More specifically, we adopt a top-down approach to solve problems recursively. We train LLMs to recursively call themselves on subproblems of reduced size, recognize and solve the base case directly, and combine the solutions up the associated call stack to obtain the solution to the original problem (Figure 1a). The above procedure is referred to as recursive tuning (or Re-Tuning in short).\nThe basic idea behind Re-Tuning is motivated by two lines of work. First, recent work (Nye et al., 2021; Anil et al., 2022; Dziri et al., 2023) shows that training LLMs on high-quality scratchpad data, which includes intermediate steps towards solving a problem, can improve performance on certain compositional tasks such as integer addition and parity. Instead of using the intermediate steps to train models, which is computationally costly, Re-Tuning breaks down the problems into smaller and smaller subproblems. Each subproblem runs independently within its own context in the associated call stack. The solution to each subproblem is then propagated up the call stack to produce the final solution. Since each level of the call stack only includes the information necessary to solve the current subproblem, models can more easily attend to the relevant context, improving the accuracy of solving each subproblem. Second, our tuning process is reminiscent of recent works that incorporate tool use in LLMs (Schick et al., 2023; Paranjape et al., 2023). Similar to how these models call a tool and resume generating output based on the output of the tool, with Re-Tuning, the models call themselves on a subproblem and resume generating"}, {"title": "2 Approach", "content": "We present Re-Tuning in this section. Re-Tuning recursively tunes LLMs to solve compositional tasks. Specifically, the method involves (1) recursively decreasing the size of the problem, (2) solving the base case, and (3) passing the solutions up the recursion stack, solving subproblems of increasing complexity along the way.\nFirst, with Re-Tuning, an LLM recursively calls itself on subproblems of decreasing length or complexity. For example, when adding 1234 + 5678, the LLM calls itself to add 234 + 678. This call is then sent to a new context in which the LLM calls itself to add 34 + 78, which is again sent to a new context where the LLM calls itself to add 4 + 8.\nNext, the base case is solved. The base cases are easy enough to be solved directly in the same context. For the integer addition problem, the base case is to add the two least-significant digits together (e.g., adding 4 + 8).\nFinally, the subproblem solutions are passed up the recursive call stack. Specifically, subproblem solutions are appended directly after the associated call in the context one level up the call stack."}, {"title": "3 Experiments", "content": "We consider three tasks: integer addition, a dynamic programming problem, and the parity problem. For each task, we train 3 types of models: baseline, scratchpad, and Re-Tuning. The baseline models were trained to simply output the solution to the problem. The scratchpad models were trained to generate a scratchpad (Nye et al., 2021) containing intermediate reasoning steps before generating the final solution to the problem. The Re-Tuning models are as described above.\nDuring evaluation, we consider both in-distribution and out-of-distribution (OOD) data. The in-distribution data are those with problem lengths that were seen in training and the OOD data are those with problem lengths longer than seen in training. For example, on the integer addition task, the training data consists of numbers with lengths up to 15 digits. Evaluation examples with 1-15 digits are considered in-distribution and examples with 16 or more digits are considered OOD.\nWe train LLaMA 7B and 13B (Touvron et al., 2023a) using Low-Rank Adapters (Hu et al., 2022). See Appendix A.2 for additional details on the training setup. Additionally, we provide results on the smaller Galactica (Taylor et al., 2022) 125m and 1.3B parameter models, in Appendix B."}, {"title": "3.1 Experimental Setup", "content": "We consider 3 representative compositional problems: integer addition, a dynamic programming problem, and the parity problem. Here, we describe each problem in detail, as well as how the data was constructed. Additional details are provided in Appendix A.1 and examples are provided in Appendix C.\nInteger addition This problem challenges LLMs to add two integers. The input to the model is simply a prompt to add 2 numbers. For example, '45 + 97'. Pretrained language models have some capability to perform addition without any training, but it seemingly disappears as the numbers grow in size. Nye et al. (2021) used a scratchpad to teach language models addition, and more recently Liu and Low (2023) taught LLaMA 7B to add numbers up to 15 digits. In both cases, there is remarkable performance degradation when adding inte-"}, {"title": "3.2 Main Results", "content": "Here we share our main results on LLaMA 7B and LLaMA 13B, across all three tasks. Results are shown in Figure 2, and discussed in detail in the proceeding paragraphs. Across all problems and model sizes, the Re-Tuning method outperforms the baseline and scratchpad methods, with the clearest difference being on integer addition. In particular, Re-Tuning exhibits significantly better OOD generalization compared to the baseline or scratchpad methods. We find this to be true even on language models with very few parameters, including Galactica 125M and Galactica 1.3B (see Appendix B).\nInteger addition The Re-Tuning method considerably outperforms the baseline and scratchpad methods. The scratchpad method performs the worst, achieving 0% accuracy on every problem longer than those seen during training on both LLaMA 7B and LLAMA 13B. The baseline method has non-zero OOD accuracy for problems up to length 20, but accuracy falls to 0% on longer problems with both models. In contrast, the Re-Tuning method maintains near-perfect accuracy in regimes where the baseline and scratchpad models have 0% accuracy, only falling below 90% accuracy on problems of length 40 and 45 for LLaMA 7B and LLAMA 13B respectively. Astonishingly, with Re-Tuning, both models maintain near 50% accuracy on adding up to 60 digit numbers. The model by Liu and Low (2023), which is also trained on addition up to 15 digits, has similar OOD performance to our baseline models, and falls to 0% accuracy when adding 21-digit numbers.\nDynamic programming Again, Re-Tuning outperforms both the baseline and scratchpad approaches, though the gap between Re-Tuning and baseline is narrower for LLAMA 13B than it is for LLaMA 7B. Still, with Re-Tuning, both models achieve near 90% accuracy on problems of length 10, twice as long as the longest examples in the training data. Moreover, on problems of length 15, LLaMA 7B achieves 40% accuracy with Re-Tuning and 0% accuracy with the baseline and scratchpad methods. Dziri et al. (2023) trains and evaluates GPT3 models with and without scratchpad. Both reach 0% accuracy on problems of length"}, {"title": "4 Analysis and Further Discussion", "content": "In this section, we conduct additional experiments in order to better understand the effects of various mechanisms behind Re-Tuning."}, {"title": "4.1 Ablation Study", "content": "For all tasks, as the problem size grows, so does the number of unique possible problems. For example, there are more combinations of 10-digit addition problems than there are 2-digit addition problems. If we randomly sample problems from the space of all possible problems up to some length, then the distribution of problems will be skewed toward longer problem instances. Due to Re-Tuning's recursive design, it's important that an appropriate"}, {"title": "4.6 Why is Re-Tuning so Effective?", "content": "As discussed in Section 4.4, Re-Tuning exhibits significantly higher sample efficiency than the scratchpad or baseline methods. However, there are other factors at play that also contribute to the success of Re-Tuning.\nWith Re-Tuning, an LLM generates a recursive call with a subproblem to be solved in a separate recursive context. Once the subproblem is solved, the solution is returned to the original context. This"}, {"title": "4.7 Efficiency Comparison", "content": "Due to Re-Tuning's high degree of sample-efficiency (see Figure 5) and shorted training sequences relative to the scratchpad method (see Figure 1b), Re-Tuning is an efficient and effective training paradigm to improve the performance of LLMs on compositional tasks. Still, generation takes longer with Re-Tuning than with baseline or"}, {"title": "5 Related Work", "content": "Several works have explored the length generalization ability of LLMs on compositional problems. Dziri et al. (2023) suggests that LLMs solve compositional tasks via \u201clinearized subgraph matching\" and thus fail to learn the underlying algorithm necessary to solve more complex problem instances. Anil et al. (2022) showed that training on a combination of in-context learning and scratchpad prompting could enable better performance. Similarly, Re-Tuning involves training pretrained LLMs to make recursive calls in order to improve performance on compositional tasks. Other works have studied length generalization on small, purpose-built transformer models. Lee et al. (2023) and Zhou et al. (2023b) showed that training small transformer models from scratch on scratchpad data could enable better length generalization. Recently, McLeish et al. (2024) showed that transformers can achieve strong OOD performance on"}, {"title": "6 Conclusion", "content": "We study the problem of solving compositional tasks with large language models. We propose a new tuning paradigm that decomposes the original compositional problem into smaller and smaller instances of the same type, solves each, and combines the results to produce the final answer. To the best of our knowledge, our method is the first to utilize the recursive property of compositional tasks. Experimental results on three representative compositional tasks demonstrate the effectiveness of our method. Our method not only significantly outperforms standard training and state-of-the-art methods, especially on out-of-distribution problem instances, but is also more memory efficient during training. We hope our method can be applied to more tasks where recursive computation is inherent and computational resources are limited."}, {"title": "Limitations", "content": "Re-Tuning has shown better accuracy and better sample efficiency than standard methods. However, it does have some disadvantages. Re-Tuning takes longer to generate responses than standard prompting because it generates recursive calls in addition to generating the final answer. The inference procedure for Re-Tuning is also more complex than standard inference since we need to extract text from contexts, check if there is a generated prompt in the text, and recursively generate using the generated prompts."}, {"title": "A Experimental Details", "content": "In this section, we provide additional experimental details, including details related to the synthetic construction of the data, training process, and inference pipeline."}, {"title": "A.1 Data Construction", "content": "We highlight our pipeline for synthetic data construction in this section.\nSeed data To construct the training data, we start by randomly generating a collection of seed data. On the dynamic programming and parity problems, this seed data is exhaustive (e.g., all possible binary arrays up to length 20). On the integer addition task, we randomly generate 304,000 pairs of numbers up to 15 digits long. Next, we generate the recursive solution, including the solutions to the recursive sub-problems, for each instance of the seed data. The union of the seed data and recursive sub-problems from the training data, which we format according to the method (baseline, scratchpad, and Re-Tuning).\nResampling In general, we upsample examples with smaller lengths and downsample those with larger lengths in our training data. There are 2 reasons for this. First, examples with larger lengths are more numerous than examples with smaller lengths (there are many more examples of adding 2 15-digit numbers than there are adding 2 1-digit numbers). Second, since Re-Tuning generates calls to all examples except the base case, it has trouble learning what to do in the base case if there are not enough examples. Without resampling, the base cases for each problem would be far less than 1% of the training data. We do not follow any specific methodology for resampling. We simply try to bring the training data distribution closer to uniform than it would be without resampling. Figure 6 displays the distributions of the training data before and after resampling with respect to length for the"}, {"title": "A.2 Training Details", "content": "Training and evaluation were done on NVIDIA H100, A100, and RTX A6000 GPUs, depending on the compute requirements of the job. Rather than train the full model, we train using low-rank adapters (Hu et al., 2022). Hyperparameters for training jobs are in Table 4. These hyperparameters apply to training all models across all tasks, with two notable exceptions: (1) when training parity baselines, we used a slightly higher learning rate of 5e-4 for better stability, and (2) the scratchpad training job for the dynamic programming problem on LLAMA 13B used a batch size of 64, along with 64 gradient accumulation steps, so that the training job could be done on a single A100 GPU. Our training code is a heavily modified version of the code from Rafailov et al. (2023)."}, {"title": "A.3 Inference Pipeline", "content": "For baseline and scratchpad methods, our evaluation procedure is rather standard: we sample at a low temperature (0.01) and impose no additional context limitations beyond those of the models themselves, in which case the input is truncated from the left. With Re-Tuning, we use a recursive wrapper around the same generation procedure, the pseudocode for which is shown in Algorithm 1.\nTo better understand the recursive generation procedure of Re-Tuning, let's consider the following integer addition example: \u201c687 + 891\\nSolution: \". With Re-Tuning, the model is trained to return the following subproblem call \u201cCall: 87 + 91\\n\". In this case, we would extract the text \"87 + 91\" and prompt for the solution to this subproblem in a new context. Once we have the solution to this subproblem, it's returned to the main context \"687 + 891\\nSolution: Call: 87 + 91\\nReturn: 178\\nAnswer: \", and we again call the model to generate the final answer."}, {"title": "B Additional Results", "content": "In this section, we share results on two additional models: Galactica 125M and Galactica 1.3B (Taylor et al., 2022). Results across our 3 tasks are shown in Figure 7. In general, we observe that Re-Tuning enables Galactica 1.3B to maintain higher accuracy on more complex problem instances and Galactica 125M to perform as good, if not better, than either the baseline or scratchpad methods."}, {"title": "C Example Problems", "content": "In this section, we provide additional details and examples of the training data for all three tasks (integer addition, dynamic programming, and parity) with all three methods (baseline, scratchpad, and"}]}