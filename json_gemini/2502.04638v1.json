{"title": "Learning Street View Representations with Spatiotemporal Contrast", "authors": ["Yong Li", "Yingjing Huang", "Gengchen Mai", "Fan Zhang"], "abstract": "Street view imagery is extensively utilized in representation learning for urban visual environments, supporting various sustainable development tasks such as environmental perception and socio-economic assessment. However, it is challenging for existing image representations to specifically encode the dynamic urban environment (such as pedestrians, vehicles, and vegetation), the built environment (including buildings, roads, and urban infrastructure), and the environmental ambiance (such as the cultural and socioeconomic atmosphere) depicted in street view imagery to address downstream tasks related to the city. In this work, we propose an innovative self-supervised learning framework that leverages temporal and spatial attributes of street view imagery to learn image representations of the dynamic urban environment for diverse downstream tasks. By employing street view images captured at the same location over time and spatially nearby views at the same time, we construct contrastive learning tasks designed to learn the temporal-invariant characteristics of the built environment and the spatial-invariant neighborhood ambiance. Our approach significantly outperforms traditional supervised and unsupervised methods in tasks such as visual place recognition, socioeconomic estimation, and human-environment perception. Moreover, we demonstrate the varying behaviors of image representations learned through different contrastive learning objectives across various downstream tasks. This study systematically discusses representation learning strategies for urban studies based on street view images, providing a benchmark that enhances the applicability of visual data in urban science.", "sections": [{"title": "1. Introduction", "content": "In recent years, unsupervised learning has demonstrated outstanding performance on various downstream tasks. By leveraging methods such as contrastive learning (He et al., 2020; Chen et al., 2020, 2021) and masked image modeling (He et al., 2022; Xie et al., 2022), it has achieved efficient image representation and exhibited excellence in classical computer vision tasks such as image classification (Radford et al., 2021), object detection (He et al., 2022), and semantic segmentation (Wang et al., 2020a), surpassing the vast majority of traditional supervised learning only approaches. However, current unsupervised learning and self-supervised learning aim to encode as much semantic and structural information of objects and environments in a scene as possible (Park et al., 2023; Huang et al., 2024). This is not suitable for all downstream tasks in tasks such as street view-based urban environment understanding. For instance, in place recognition tasks (Lowry et al., 2015), the features are expected to focus only on place-invariant information, such as buildings and roads, filtering out dynamic information like lighting conditions, pedestrians, vehicles, and vegetation. In contrast, in tasks related to human perception of places (Dubey et al., 2016; Zhang et al., 2018), these dynamic elements are important. Moreover, tasks like socioeconomic prediction (Wang et al., 2020b) emphasize the spatially consistent expression of neighboring scenes.\nIn image representation learning, selectively encoding dynamic and static information in urban environments and the ambiance they create is highly important but inherently challenging (Cordts et al., 2016). Achieving precise encoding of such information typically requires separately labeling dynamic and static elements"}, {"title": "2. Related work", "content": "2.1. Self-Supervised Representation Learning\nSelf-supervised representation learning leverages the inherent structure within data to generate supervisory signals, thereby mitigating the need for extensive labeled datasets. A prominent approach in this field is contrastive learning, which has demonstrated significant success in learning robust representations. Methods such as InstDis (Wu et al., 2018), SimCLR (Chen et al., 2020), and the MoCo series (He et al., 2020; Chen et al., 2021) focus on contrasting positive pairs of similar instances against negative pairs of dissimilar instances to learn effective features. In contrast, BYOL (Grill et al., 2020), SimSiam (Chen and He, 2021),"}, {"title": "2.2. Spatiotemporal Contrastive Learning in Vision Tasks", "content": "Spatiotemporal contrastive learning enhances traditional contrastive learning by integrating both spatial and temporal information, enabling models to capture underlying relationships in unlabeled data that vary over space and time.\nTemporal contrastive learning excels in sequential data by differentiating between related and unrelated frames. For example, Contrastive Predictive Coding (CPC) (van den Oord et al., 2019) applies temporal contrastive learning by using consecutive video frames as positive pairs and shuffled or temporally distant frames as negative pairs, helping models learn temporal coherence. SeCo (Manas et al., 2021) and GeoSSL (Ayush et al., 2021) use multi-season remote sensing images for self-supervised pre-training, enhancing model performance in remote sensing tasks."}, {"title": "2.3. Street View Representation Learning for Downstream Tasks", "content": "Street view imagery has been widely used in various urban downstream tasks, such as road defect detection (Chacra and Zelek, 2018), urban function recognition (Huang et al., 2023), and socioeconomic prediction (Fan et al., 2023). However, existing research on street view representation often relies on supervised models trained on datasets like Places365 (Zhou et al., 2017) or directly uses the pixel proportions of semantic segmentation results. These approaches fail to fully capture the rich semantic information embedded in street view imagery. Unlike natural images, street view imagery not only contains complex visual semantics but also encodes valuable spatiotemporal information in its metadata. Effectively representing this dual semantic nature - both visual and spatiotemporal \u2013 remains a significant challenge for improving its use in urban downstream tasks. Although a few studies have explored spatiotemporal self-supervised learning approaches to represent street view imagery (Stalder et al., 2024), these methods still have limitations. For instance, Urban2Vec (Wang et al., 2020b) incorporates spatial information into self-supervised training by constructing positive sample pairs based on nearest neighbors, while KnowCL (Liu et al., 2023) integrates knowledge graphs with contrastive learning to align locale and visual semantics, improving the accuracy of socioeconomic prediction using street view imagery. However, these approaches fail to explore the natural meanings of the spatiotemporal attributes of street view imagery and how to leverage these attributes to construct self-supervised methods suitable for various downstream tasks."}, {"title": "3. Method", "content": "The real world undergoes continuous changes across both temporal and spatial dimensions, yet these changes exhibit a certain level of continuity. In the temporal dimension, it is important to capture the invariant characteristics of a location as they evolve over time. Meanwhile, in the spatial dimension, the focus is on maintaining the consistency of the overall ambiance within a specific spatial range. These temporal and spatial invariances are crucial for enhancing performance in various downstream tasks. In this section, we introduce the proposed spatiotemporal contrastive learning framework in detail (Figure 1)."}, {"title": "4. Experiments and Results", "content": "To validate our hypothesis, we first pre-train the models using datasets specifically designed for self-supervised contrastive learning, spatial contrastive learning, and temporal contrastive learning, respectively. We then evaluate the models on three distinct downstream tasks that reflect the characteristics of these contrastive learning models: visual place recognition (VPR), socioeconomic indicator prediction, and safety perception. Additionally, we conduct interpretability analyses on the features learned by the different contrastive models to gain a deeper understanding of the information the models focus on and how this impacts performance on urban downstream tasks."}, {"title": "4.1. Pre-training Datasets and Experiment Setup", "content": "To obtain street view imagery for both self-supervised model training and socioeconomic indicator prediction, we first sourced road network data for each city using the OSMnx library (Boeing, 2017) from OpenStreetMap. We then generated query points along these road networks at regular intervals of 15 meters. The Google Street View (GSV) Application Programming Interface (API) was subsequently utilized to retrieve and download street view images.\nSince the VPR and safety perception datasets include a wide range of street view images from different cities, while the socioeconomic prediction task focuses more on local city characteristics, we constructed two separate datasets \u2013 a global version and a local version \u2013 for testing on different downstream tasks.\nFor the global version, to capture a broad spectrum of urban environments, we trained our self-supervised models on data collected from ten diverse and representative global cities including Amsterdam, Barcelona, Boston\u2013Cambridge-Medford-Newton (Boston), Buenos Aires, Dubai\u2013Sharjah (Dubai), Johannesburg, Los Angeles, Melbourne, Seoul, and Singapore. These cities were carefully selected to encompass a variety of geographical locations, cultural backgrounds, and urban forms, ensuring the diversity and richness of our training dataset. We collected historical images of ten global cities from the Google Street View (GSV) API which resulted in a total of over 42 million street view images used for pre-training.\nFor the local version, we selected street view images from Los Angeles to construct different contrastive datasets tailored to the specific needs of the socioeconomic prediction task in that city. The construction methods of datasets are similar to the global version."}, {"title": "4.2. Visual Place Recognition", "content": "Visual place recognition (VPR) is a crucial urban task that aims to identify specific locations based on visual input. This task requires the removal of temporal disturbances to focus on stable information that does not change over time, demanding feature extraction that effectively distinguishes constant characteristics in the environment to improve recognition accuracy.\nTo evaluate the model's performance in VPR tasks, we used several benchmark datasets: CrossSeason (Mans Larsson et al., 2019), Essex (Zaffar et al., 2021), Pitts250k, Pitts30k (Arandjelovi\u0107 et al., 2018), SPED (Chen et al., 2018), and MapillarySLS (Warburg et al., 2020) datasets. Detailed information about these benchmark datasets are described in Appendix A.2. The models was tested by freezing the backbone of the pre-trained ViT and extracting the [CLS] token for VPR tasks. We assessed performance using the Recall@K metric, measuring the models' ability to correctly identify query image locations among the top-k most similar database images.\nIn Figure 2, the GSV-Temporal model demonstrates exceptional performance on the CrossSeason dataset, achieving a recall value of 100% across all K values. This indicates its robust capability in cross-season VPR tasks. In contrast, GSV-Self and ImageNet-Self exhibit significantly lower performance, suggesting their inability to effectively capture temporal invariant features. On the Essex dataset, GSV-Temporal maintains a recall value exceeding 75%, with values of 99.05% for both K=20 and K=25. This highlights its not sensitivity to dynamic changes in the environment, outperforming other models in this context. In the Pitts250k dataset, GSV-Temporal consistently outperforms GSV-Self and ImageNet-Self in recall values, the GSV-Temporal model also excels on the Pitts30k dataset, achieving a recall value of 90.23% at K=15. underscoring its suitability for complex urban environments in VPR tasks. For the SPED dataset, GSV-Temporal displays superior recall values compared to other models, particularly with a notable performance at K=5. In the MapillarySLS dataset, GSV-Temporal showcases its outstanding performance again, with a recall value of 77.57% at K=15.\nIn summary, the GSV-Temporal model consistently outperforms other models across multiple datasets, particularly in VPR tasks. Its not sensitivity to temporal"}, {"title": "4.3. Socioeconomic Indicator Prediction", "content": "The socioeconomic indicator prediction task aims to use street view images to infer the socioeconomic status of urban areas. It emphasizes learning the overall ambiance of a region rather than specific geometric features, highlighting the need for feature extraction to focus on similarities between regions to better understand economic conditions and developmental dynamics.\nIn the downstream task of predicting socioeconomic indicators, we utilized the socioeconomic dataset published by Fan et al. (2023), which contains 18 socioeconomic indicators across seven major cities in the United States (Table A1). We take the socioeconomic indicator prediction of Los Angeles as an example. Detailed descriptions are provided in Section Appendix A.1. We first extracted street view features from the images using the pre-trained models of the local version. These features were then aggregated using the mean values at the block group level. The aggregated features were used as input features to predict socioeconomic indicators for each block group.\nFor prediction model training and evaluation, we split each city's dataset into a training set (70%) and a testing set (30%). We used LASSO as the regressor to evaluate the predictive performance of the image features extracted by the different pre-trained models. Additionally, we applied 5-fold cross-validation to ensure robust evaluation. This approach allows for a fair comparison of the different contrastive learning models in capturing visual features that are meaningfully correlated with socioeconomic indicators."}, {"title": "4.4. Safety Perception", "content": "The safety perception task involves using street view imagery to estimate how safe people perceive a given scene to be. To make accurate estimates, this task requires analyzing all relevant elements within the scene, as each can contribute to the overall perception of safety, particularly elements such as trees and vehicles (Zhang et al., 2018).\nWe selected the PlacePlus 2.0 (Dubey et al., 2016) dataset for the downstream task of human environmental perception, filtering out over 1,144 images with safety perception scores below 3.5 and above 6.5, with 80% of the data used for training and 20% for testing. The model was trained using a linear binary classification approach for 20 epochs to effectively distinguish between low and high safety perception environments.\nThese findings suggest that spatial contrastive pre-training effectively captures the overall ambiance of urban areas, enabling more precise predictions of regional socioeconomic information. Additionally, temporal contrastive pre-training filters out random factors and dynamic elements in the images, enhancing the reliability of socioeconomic predictions."}, {"title": "4.5. Analysis of Differences in Spatio-Temporal Contrastive Features", "content": "This section explores the differences in feature representation and retrieval tasks by comparing the Self, Spatial, and Temporal contrastive methods. We use street view images from Chicago, analyzing the performance of these three contrastive methods in pre-trained models. For each model pre-trained with a unique contrastive learning objective, we extract a 768-dimensional feature vector representing the characteristics of street view images. In our experiment, to comprehensively evaluate the retrieval performance of these methods, we randomly selected 500 street view images from different locations in Chicago as query images, ensuring that each image originated from a distinct spatial location. For each query image, we used the Nearest Neighbors method in feature space to retrieve the top five street view images with the closest Cosine distance. This process generated a total of 500 sets of query and retrieval pairs, with five results for each query. Specifically, we used the Euclidean distance as a similarity measure to rank and obtain the top five retrieval results.\nFinally, we randomly selected one set of query and retrieval results for visualization. By comparing the year, heading, and feature distance of the retrieved results with the query image, we visually demonstrated the significant differences in retrieval characteristics among the three contrastive methods. Figure 3 shows the retrieval results for a given query street view image using GSV-Self, GSV-Spatial, and GSV-Temporal contrastive methods. On the left, the query image is displayed, including information about the year of capture (June 2018), heading (90\u00b0), geographic location (42.3951, 71.1217), and city (Chicago). The retrieval results are arranged in three rows, corresponding to the GSV-Self, GSV-Spatial, and GSV-Temporal methods (Figure 2). Each row shows the top five most similar retrieval results, ranked from left to right (1st to 5th)."}, {"title": "4.6. What do GSV-Temporal and GSV-Spatial Contrastive Objectives Learn from GSV?", "content": "Our experimental results reveal that different contrastive learning methods excel in different tasks: Temporal contrastive performs exceptionally well in VPR tasks, Spatial contrastive shows better results in macroeconomic prediction tasks, and Self contrastive achieves the best performance in safety perception tasks, confirming our hypothesis that street view images captured at the same location over time enable contrastive learning tasks to uncover the temporal-invariant characteristics of the built environment. Similarly, spatially proximate street view images from the same period facilitate learning tasks to capture the spatial-invariant neighborhood ambiance, such as socioeconomic overal ambiance."}, {"title": "5. Conclusion", "content": "In this work, we propose a self-supervised urban visual representation learning framework based on street view images, capable of selectively extracting and encoding dynamic and static information and their ambiance in urban environments according to the requirements of different downstream tasks. By leveraging the unique spatiotemporal attributes of street view imagery, we have developed three contrastive learning strategies: temporal invariance representation, spatial invariance representation, and global information representation. Experimental results demonstrate that these strategies can effectively learn task-specific features suitable for their respective downstream applications, significantly enhancing performance in urban environment understanding tasks. Furthermore, we conducted an in-depth analysis of the reasons behind the performance of different contrastive methods, further emphasizing the importance of targeted learning strategies. This study systematically explores representation learning strategies based on street view images, provides a valuable benchmark for the application of visual data in urban science, and enhances their practical applicability."}, {"title": "6. Acknowledgements", "content": "This work was supported by the High-performance Computing Platform of Peking University. We also acknowledge the financial support from the National Natural Science Foundation of China (Grant No. 42371468)."}]}