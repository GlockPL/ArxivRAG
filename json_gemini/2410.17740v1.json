{"title": "Emotion Recognition with Facial Attention and Objective Activation Functions", "authors": ["Andrzej Miskow", "Abdulrahman Altahhan"], "abstract": "In this paper, we study the effect of introducing channel and spatial attention mechanisms, namely SEN-Net, ECA-Net, and CBAM, to existing CNN vision-based models such as VGGNet, ResNet, and ResNetV2 to perform the Facial Emotion Recognition task. We show that not only attention can significantly improve the performance of these models but also that combining them with a different activation function can further help increase the performance of these models.", "sections": [{"title": "1 Introduction", "content": "The most recent breakthrough in emotion recognition is the idea of using attention to improve the accuracy of the deep learning model. The methodology behind visual-attention-based models was inspired by how humans inspect a scene at first glance. [1] has found that humans retrieve parts of the scene or objects sequentially to find the relevant information. Since neural networks attempt to mimic how the human brain works to complete the desired task, various methods were developed to imitate human attention. The discovery of these attention mechanisms helped improve the accuracy of emotion recognition models. In this work, we aim to discover the effect of introducing an attention mechanism to existing deep learning models to recognise facial expressions and how their performance can be further boosted via simple but effective changes to their architectures. Additionally, the new architectures will be further improved by modifying their activation functions from ReLU to ELU activation functions to solve the issue of bias shift. The paper proceeds as follows. In the next section, we present related work, while in section 3, we show the methodology, and in section 4, we show the results."}, {"title": "2 Attention", "content": "When processing a complex visual scene, human vision does not process the entire image at once. Instead, we tend to only focus on a subset of the image while ignoring the rest to speed up the visual analysis process. This process of selecting"}, {"title": "2.1 Attention in Facial Expression Recognition Context", "content": "In computer vision, attention mechanisms can be treated as a selection process that weighs an input dimension and its features according to their importance to the task. Each dimension defines a different input domain that contributes to the task to a different extent. Furthermore, each portion of the input domain has varied importance to the task.\nFrom a deep learning perspective, attention can be infused into a CNN model by further distinguishing higher-level features from low-level features and assigning higher weights to crucial features, i.e., by attracting the model's attention to these features [4]. From this perspective, attention mechanisms can be divided into three distinct categories; channel, spatial, and temporal attention. Additionally, these categories can be combined to form other hybrid attention mechanisms, namely: channel & spatial attention and spatial & temporal attention. Temporal attention will not be discussed as the work focuses on recognition from static images, not sequential data.\nSince the introduction of attention modules and their easy integration with CNN's models, researchers have switched their focus to using CNN classification with attention for FER applications. In [5], authors proposed using attention for FER tasks. They implemented spatial attention with a CNN and improved the model's performance by focusing less on irrelevant parts of the image and training the model on \"where\" to find the needed information. Additionally, [6] has explored using channel attention which defines \"what\" to look for in the model by placing a higher value on more informative features. The combination of spatial and channel attention for FER applications is envisaged to achieve state-of-the-art results."}, {"title": "3 Methodology", "content": "We use three different CNN image processing models as our base model and add attention to them to boost their performances. The models that we use are VGGNet, Resnet, and Resnet V2. These models are considered a good fit for our problem due to their resilience to noise and ability to deal with degradation and vanishing gradient problems. Each one of these models has its strengths and weaknesses, and we want to study what happens when we add attention to them in the context of FER.\nIn addition, we vary the depth of these architectures to study the effect of different attention mechanisms on the depth of the architecture and whether they aggravate or alleviate some of the issues associated with the depth of the"}, {"title": "3.1 Face Detection and Pre-processing", "content": "We start by detecting the face in the image and removing the insignificant background pixels. Without this step, unwanted features in the image may be extracted and classified along with important information resulting in errors. Facial detection can be achieved using standard object detection methods. This paper uses a state-of-the-art facial detector built on top of the YOLO framework [7]. YOLO was chosen due to its efficient one-stage object detection capability comparable to the performances of two-stage detectors while offering significantly better computational performance [8].\nThe default yolov5s weights were chosen due to their high performance and accuracy after experimenting with different weights on a subset of the dataset. More importantly, the original YOLO architecture was modified to ensure the output images had a fixed image size of 80 \u00d7 80 pixels. Since faces bounding boxes can have different proportions, cropped faces must be re-sized, so they all have the same size. This stage can be considered an external attention layer for our model."}, {"title": "3.2 Activation Functions", "content": "ReLU activation function has helped to solve the vanishing gradient problem, and hence it was utilised by the architectures discussed earlier. This is because the gradients of the ReLU activation follow the identity function for positive arguments and zero otherwise, meaning that large gradient values are still used, and negative values are discarded. On the other hand, since ReLU is non-negative, it has a mean activation larger than zero. As a result, neurons with a non-zero mean activation act as a bias for the next layer causing a bias shift for the next layer. The shift in bias causes weight variance, leading to the activation function being locked to negative values, and the affected neuron can no longer contribute to the network learning. Consequently, two activation functions have been proposed that tackle the problem of bias shift differently while also solving the vanishing gradient problem.\nELU function was proposed that allows negative gradient values, resulting in the mean of the unit activations being closer to zero than ReLU. Like ReLU, ELU applies the identity function for positive values, whereas it utilises the exponential function if the input is negative. For this reason, ELU achieves faster learning, and significantly better generalization performance than ReLU on networks with more than five layers [9].\nSELU function [10] was proposed to solve the issue of bias-shift through self-normalization. Through this property, activations automatically converge to a zero mean and unit variance. This convergence property makes SELU ideal for networks with many layers and further improves the ReLU activation function."}, {"title": "4 Attention Modules", "content": "Attention modules are designed to be integrated with CNN models to improve them further. First, we discuss how attention is implemented in each module, the benefits of each implementation, and possible improvements. Subsequently, we show how the attention modules integrate within the implemented CNN architectures.\nSEN-Net The SEN-Net architecture was the first implementation of channel attention in computer vision tasks [4]. The block improved the representational ability of the network by modelling the interdependencies between the channels of a convolutional layer. This is done through a feature re-calibration operation split into two sequential operations: squeeze and excitation.\nA set of experiments was conducted on the CK+ dataset using ResNet-50 as the backbone to find the optimal value of r in table 2."}, {"title": "ECA-Net", "content": "ECA-Net [11] was developed to improve channel attention used in SEN-Net. In SEN-net, the excitation module uses dimensionality reduction via two fully connected layers to extract channel-wise relationships. The channel features are mapped into a low-dimensional space and then mapped back, making the channel connection and weight indirect. Consequently, this negatively affects the direct connections between the channel and its weight, reducing the model's performance. Furthermore, empirical studies show that the operation of dimensional reduction is inefficient and unnecessary for capturing dependencies across all channels [11]. The ECA-Net attempts to solve the issue of dimensionality reduction while improving the efficiency of the excitation operation by introducing an adaptive kernel size within its excitation operation.\n$k = \\psi(C) = \\left|\\frac{\\log_2(C)}{b} + \\frac{1}{2}\\right|_{odd}$ (1)\nA 1D convolutional layer performs the excitation operation with kernel size k. The value of k is adaptively changed based on the number of channels. With this operation, ECA captures channel-wise relationships by considering every channel and its k neighbours. Therefore, instead of considering all relationships that may be direct or indirect, an ECA block only considers direct interaction between each channel and its k-nearest neighbours to control the model's complexity. Table 3 shows the effect of utilising a static value of k over the adaptive, confirming that the adaptive kernel size is the best option for FER applications."}, {"title": "CBAM", "content": "The last attention module implemented in this paper is the Convolutions Block Attention Module (CBAM) [12]. CBAM proposed utilising both spatial and channel attention to improve the model's performance, unlike the previous attention modules, which only utilised channel attention. The motivation behind the CBAM stemmed from the fact that convolution operations extract informative features by cross-channel and spatial information together. Therefore, emphasising meaningful features along both dimensions should achieve better results.\nCBAM channel attention consists of squeeze and excitation operations inspired by the implementation of channel attention from SEN-Net[4]. However, CBAM modifies the original squeeze operation from SEN-net to include average and max pooling to capture channel-wise dependencies. The idea behind utilising both pooling operations stems from the fact that all spatial regions contribute to the average pooling output, whereas max-pooling only considers the maximum values. Consequently, combining both should improve the representation power of relationships between channels. The two pooling operations are used simultaneously and are passed to a shared network consisting of two fully connected layers (W\u2081 and W2), which perform the excitation operation (following the exact implementation from SEN-Net). After the output of each pooling operation is passed through the shared MLP, the resultant feature vectors are merged using element-wise summation.\nThe design of the CBAM spatial attention module follows the same idea as the CBAM channel attention module. To generate a 2D spatial attention map, we compute a 2D spatial descriptor that encodes channel information at each pixel over all spatial locations. This is done via applying average-pooling and max-pooling along the channel axis, after which their outputs are concatenated. This is because pooling along the channel axis effectively detects informative regions as per [13]. The spatial descriptor is then passed to a convolution layer with a kernel size of 7, which outputs the spatial attention map. The choice of the large kernel size is necessary since a large receptive field is usually helpful in deciding spatially important regions. The output is passed through a sigmoid function to normalize the output.\nLike SEN-Net, the reduction ratior allows us to vary the capacity and computational cost of the channel attention block, as shown in a set of experiments that we conducted on the CK+ and summarised in table 4."}, {"title": "4.1 Integration of Different Attention Mechanisms with Different Deep Vision-Based Models", "content": "As mentioned, we integrate the three attention mechanisms discussed earlier with three types of vision-based deep learning architectures. The chosen attention modules are versatile and are designed to be easily integrated within CNN models.\nIntegration with VGGNet The creators of SEN-Net stated that the SE block could be integrated into standard architectures such as VGGNet by the insertion after the activation layer following each convolution. Through research in the classification of medical images, it was shown that authors had used three different ways to integrate attention in VGGNet: (1) placing attention as described by SEN-Net [14], (2) placing the attention module before the last fully connected layers [15] and (3) placing the attention modules at layers 11 and 14 [16]. Method 2 achieved the best performance for the emotion recognition task as shown in table 5.\nIntegration with ResNet Even though ResNet is a more complicated architecture, the creators of SEN-Net provided the most optimal way to integrate their block within the residual block, where the attention module is added before summation with the identity branch. Through research and experimentation, we did not find more optimal ways to integrate attention within ResNet; therefore, ECA-Net and CBAM followed the same integration method."}, {"title": "5 Results", "content": null}, {"title": "5.1 FER Datasets", "content": "It is necessary to have datasets with emotions that are correctly labeled and contain enough data to train the model optimally. For this reason, this paper uses three datasets of different sizes, widely used in FER research. Extended Cohn-Kanade Dataset CK+ dataset [17] is an extension of the CK dataset. It contains 593 video sequences and still images of eight facial emotions; Neutral, Angry, Contempt, Disgusted, Fearful, Happy, Sad, and Surprised. The dataset"}, {"title": "5.2 Evaluation of CNN-Based Models with an ELU Activation Function", "content": "This section shows the results of applying the previously discussed CNN-based models with a different activation function, ELU. This is necessary to establish ground truth and isolate the effect of changing the activation function from adding attention (discussed in the next section). Table 6 displays the final evaluation accuracies of the CNN models on the three datasets. The evaluations for CK+ and JAFFE were executed three times to ensure the results' correctness; with smaller datasets, evaluation accuracies fluctuate between the runs. Out of the three executions, the highest value was chosen.\nAnalysing the results, we see that VGG-19 achieved the best accuracy on CK+ and FER2013, while ResNetV2-50 achieved the best accuracy on the JAFFE dataset. This was an unexpected result as the initial assumption was that the deeper ResNet models should outperform VGGNet, which was not the case. We conclude that this is due to the modification of the activation function from ReLU to ELU in the CNN models. This change improved the VGG-19 accuracy from 87.91% to 90.66% on the CK+ dataset, significantly better than the deeper ResNet models for the same modification. This finding indicates that residual"}, {"title": "5.3 CNNs with Different Attention Mechanisms", "content": "This section shows the results of augmenting the previously discussed CNN-based architectures with different attention mechanisms."}, {"title": "6 Conclusion", "content": "In this paper, we studied the effect of infusing three different attention mechanisms, SEN-Net, ECA-Net, and CBAM, into three CNN-based deep learning architectures, namely the VGGNet, ResNet, and ResNetV2, with different depths to classify the seven basic human emotions on three datasets, namely CK+, JAFFE, and FER2013. In addition, we have replaced their internal activation function from RELU to ELU. As a result, there was a significant improvement in their performances. We studied the effect of changing the activation function first, then infused the resultant architectures with attention. We also showed that the new residual blocks presented in ResNetV2 perform significantly better than the original ResNet on smaller datasets and slightly improve on mid-sized and larger-sized datasets. Our results show that these amendments refined the extracted features and improved the generalisation capabilities of these models. The attention module hyperparameters were modified through experimentation to maximize the models' performance on emotion recognition tasks.\nOur work verified the attention mechanism's effect on the performance of CNNs. We have shown that each attention module outperformed the baseline models on each dataset. Consequently, attention modules could successfully improve the generalisation ability and refine the extracted features regardless of the problem size. Furthermore, our work confirmed that utilising ResNet V2 with attention modules yields better results than the original ResNet when attention modules and ELU are applied. In the future, we intend to conduct a comprehensive study on the effect of simplifying the transformation operations used in attention to speed up training time without losing competency."}]}