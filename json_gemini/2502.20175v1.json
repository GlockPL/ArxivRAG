{"title": "AN EXTENSIVE EVALUATION OF PDDL CAPABILITIES IN OFF-THE-SHELF LLMS", "authors": ["Kaustubh Vyas", "Damien Graux", "S\u00e9bastien Montella", "Pavlos Vougiouklis", "Ruofei Lai", "Keshuang Li", "Yang Ren", "Jeff Z. Pan"], "abstract": "In recent advancements, large language models (LLMs) have exhibited proficiency in code generation and chain-of-thought reasoning, laying the groundwork for tackling automatic formal planning tasks. This study evaluates the potential of LLMs to understand and generate Planning Domain Definition Language (PDDL), an essential representation in artificial intelligence planning. We conduct an extensive analysis across 20 distinct models spanning 7 major LLM families, both commercial and open-source. Our comprehensive evaluation sheds light on the zero-shot LLM capabilities of parsing, generating, and reasoning with PDDL. Our findings indicate that while some models demonstrate notable effectiveness in handling PDDL, others pose limitations in more complex scenarios requiring nuanced planning knowledge. These results highlight the promise and current limitations of LLMs in formal planning tasks, offering insights into their application and guiding future efforts in AI-driven planning paradigms.", "sections": [{"title": "1 INTRODUCTION", "content": "Automated planning has long been a cornerstone of artificial intelligence, traditionally relying on explicit domain knowledge encoded in formal languages such as PDDL. In recent years, the rapid evolution of large language models (LLMs) has sparked considerable interest in their ability to bridge the gap between natural language descriptions and formal planning representations.\nEarly studies by Zuo et al. (2024) and Oswald et al. (2024) demonstrated that LLMs are capable of translating natural language descriptions into syntactically valid PDDL representations. However, these pioneering works also revealed significant gaps, as the generated planning domains frequently diverge from gold-standard models, both syntactically and semantically. This observation has spurred further research into the underlying reasoning capabilities of LLMs and their potential role in executing complete planning tasks\nAdvancement in LLMs fuelled recent efforts that looked into how these multi-billion parameter models can be best employed as agents Huang et al. (2024). Building on this momentum, several strategies have been proposed to map user instructions into PDDL problems Pallagani et al. (2023); Liu et al. (2023a); Dagan et al. (2023); Gestrin et al. (2024); Zhang et al. (2024), without however providing conclusive evidence for the feasibility of the task in the general domain. These studies underscore both the promise and the challenges inherent in leveraging LLMs for complex planning and reasoning tasks, where transforming natural language into an executable agentic workflow remains a non-trivial endeavor.\nIn this study, we step back to examine the fluency of twenty LLMs from seven major families in the PDDL language, focusing on their ability to parse, generate, and reason with PDDL. Specifically, we leverage the Planetarium benchmark Zuo et al. (2024) alongside the dataset introduced by Oswald et al. (2024) to assess how well these models understand and generate actions, problems, and plans. By analyzing a randomly sampled subset of over 13,000 (NL-instruction, PDDL-problem) pairs, our results show that although some models demonstrate moderate proficiency in handling PDDL, the majority struggle to convert natural language instructions into fully correct PDDL representations. This challenge is especially evident in smaller LLMs, which often fail to produce parsable PDDL."}, {"title": "2 EXTENSIVE PDDL CAPABILITY EVALUATION", "content": "From a high-level point of view, PDDL involves three types of elements: the domains to represent the possible actions available in a certain space, the problems which roughly encode the premise and the goal of a real world operation to be performed in a defined space (i.e. domain) and finally the plans that represent the effective set of actions to be run to perform the real world operation, achieving the goal.\nTherefore, practically, we stressed the considered LLMs to generate all or part of the aforementioned elements, while maintaining a wide set of evaluation scores across the involved steps to fuel the discussion and draw conclusions."}, {"title": "2.1 ACTION GENERATION", "content": "Task Signature = [input: NL instruction, PDDL domain predicates; output: PDDL action]\nWe rely on the benchmark proposed by Oswald et al. (2024) to evaluate the action generation capabilities: given a seed domain file and the NL description of an action, we let the LLM generate it in proper PDDL syntax. In their article, the authors shared a set of 32 NL-to-Action instructions distributed across 9 popular PDDL domains. We enriched these by generating 4 NL-variations for each NL-to-instruction pair to obtain a dataset of 160 [(1 + 4) \u00d7 32] instructions. To assess the results, we score along the following dimensions: Parsable: Determines if the output conforms to correct PDDL syntax. Solvable: Measures how well the action integrates into the target domain (e.g. the action may be syntactically correct but involving type mismatches, wrong number of variables for some predicates,...). Equivalent: Syntactically valid PDDL that integrates with the desired domain under the domain equivalence heuristic.\nTo measure the similarity between the generated action and the gold standard, we calculate the normalised differences in their preconditions and effects, and then subtract this value from one to derive a similarity score.\nSimilarity = $1 - \\frac{|A_{pre} \\Delta A_{pre}| + |A_{ef} \\Delta A_{ef}|}{|A_{pre} \\cup A_{pre}| + |A_{ef} \\cup A_{ef}|}$\nWhere $A_{pre}, A_{ef}$ are preconditions and effects in the gold action and $A_{pre}, A_{ef}$ are preconditions and effects in the LLM generated action."}, {"title": "2.2 PROBLEM GENERATION", "content": "Task Signature = [input: NL instruction, PDDL domain; output: PDDL problem]\nWe choose the Planetarium benchmark to evaluate the problem generation capabilities of models Zuo et al. (2024). The benchmark was primarily selected due its size that enabled a comprehensive evaluation on our side. In particular, we randomly selected 10% of the full dataset, resulting in a test set consisting of 13 203 (NL-instruction, PDDL-problem) pairs. Metrics for this set of experiments are as above (with a slight difference): Parsable: generated PDDL adheres to the syntactic rules of the language, Solvable: the generated problem can be effectively processed by existing PDDL planners, reflecting its practical utility, Equivalent: matches the gold standard in both structure and semantics.\nWhen it comes to measure the similarity between the gold and the generated PDDL problem, we use ChrF Popovi\u0107 (2015) as it is a standard metric to evaluate code generation tasks Evtikhiev et al."}, {"title": "2.3 PLAN GENERATION", "content": "Task Signature = [input: PDDL domain, PDDL problem; output: Plan]\nFinally, although LLMs are not expected to outperform conventional planners since their reasoning capabilities rely on intrinsic parametric knowledge rather than explicit logical reasoning we also aimed to assess their ability to plan in PDDL when provided with pairs of domain and problem. For this purpose, we selected domain-problem pairs from the Planetarium benchmark to prompt the models for plan generation. To evaluate generalisation, we categorised these problems based on their level of abstractness, classifying descriptions as either explicit or abstract. Explicit descriptions are direct propositions found in the PDDL problem (e.g., \u201cblock1 is on block2", "all blocks are in a single tower": ".", "categories": 1}, {"title": "2.4 CONSIDERED LLMS", "content": "To review the capabilities of language models to deal with PDDL, we utilised LLMs from several leading organisations, ensuring that both general-purpose and specialist models (i.e. chatting, code generation or instruction-following modes) are considered. Our set of models includes LLMs from\n\u2022 OpenAI (GPT-3.5-turbo, GPT-40-mini, GPT-40),\n\u2022 Anthropic (Claude-3-Haiku and Claude-3-Sonnet),\n\u2022 Google (Gemini-1.5-Pro, Gemini-1.5-Flash and Gemma-2-9B-it),\n\u2022 Meta (LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, and LLaMA3.1-405B-Instruct),\n\u2022 Mistral (Large2, 7B-Instruct, and Codestral),\n\u2022 DeepSeek (Coder-V2 and Chat-V2),\n\u2022 Alibaba (Qwen2-1.5B-Instruct, Qwen2.5-7B-Instruct, Qwen2.5-Coder-7B-Instruct, and Qwen2.5-72B-Instruct).\nOverall, this set involves members of 7 distinct providers, including commercial and open LLMs. In addition, this set allows us to compare behaviors and performances across different parameter numbers and specialities."}, {"title": "2.5 RESULTS", "content": "Domain (Fig.1A) We review the performance of 20 distinct LLMs in populating PDDL domain files with new actions based on NL instructions. While most LLMs perform well in generating correct actions, a notable decline in performance is observed in their ability to produce equivalent actions. Among the models, GPT-40, Qwen2.5-72B-instruct, and Mistral-Large2 stand out as the top performers across all three metrics for action generation. In contrast, some models consistently fail to adhere to the required action syntax. Notably, the entire LLaMA family performs very poorly, irrespective of the number of parameters. It is worthwhile noting that all the models have their respective parsable and solvable values very close, in other words the difference between them for a\nmini which has the bext score for parsable ends up being in the worst when it comes to equivalent, a similar behaviour goes for Codestral too.\nPlan (Fig.1C) Investigating the plan generation, as expected, we find that given a PDDL domain and problem, the models struggle to generate PDDL plans. This was tested on 160 data points, with Gemini-1.5-pro performing the best, yet achieving a valid plan in only 16.87% of the cases. In contrast, the BFWS-FF planner succeeded in generating a conclusive plan 86.25% of the time. Additionally, we explored the plan generation relative to the abstractness of the initial state and goal of the PDDL problem. Across all LLMs, we observe that they perform better when the initial state of the PDDL problem is abstract, though, no such pattern is noticed regarding the abstractness of the goal. Once again, the models are not performing similarly across the different tasks. Typically, our smallest model in the mix (Qwen2-1.5B-Instruct) which had scores almost all null for the domain and problem tasks, happens to be in the top-5 for the plan generation."}, {"title": "2.6 DISCUSSIONS", "content": "Our results indicate that while most LLMs can generate syntactically correct PDDL, they largely lack the capacity to generate effective problems and plans for addressing the input instructions. This behaviour becomes increasingly evident as the complexity of the experimental setup rises.\nParameter Number With the exception of the LLaMa family, an increased number of parameters in LLMs does not consistently lead to better PDDL fluency. This suggests that current pre- and post-training approaches are not effectively scaling these models to meet the demands of complex PDDL generation, including, but not limited to, long-horizon planning tasks. A clear challenge emerges with models like the LLaMA family (see Fig.1), which, while effective in problem generation, struggle significantly with action generation. This issue stems from a syntactic bias: for example, instead of the correct keyword\": precondition\", they generate \u201c:preconditions\", making actions unparsable. Similar patterns are observed in Gemma-2-9B-it, Mistral-7B-Instruct, or Qwen2.5-72B-Instruct, which incorrectly output \"(action...)\" rather than \u201c(:action...)\". Similarly, while generating problems, models often struggle with maintaining correct action sequences in the goal, occasionally placing semantically similar actions in the goal that are not defined in the domain, leading to mismatches.\nLLMs as copilots However, our findings, in Fig.2, reveal that despite these challenges, LLM-generated PDDL actions and problems demonstrate a high degree of closeness to the gold standard. This suggests that, while the models (off-the-shelf) may not yet be fully reliable for independent use, they hold strong potential as supportive tools. By generating near-accurate PDDL structures, these models can serve as co-pilots, streamlining the drafting process and allowing experts to focus on refinement and optimization rather than building from the ground up. The position articulated in Kambhampati et al. (2024) underscores that, although LLMs may not inherently plan effectively, they can nonetheless play a significant supportive role in LLM-modulo planning frameworks. This"}, {"title": "3 RELATED WORK PDDL LLM", "content": "The generation of PDDL domains and problems has recently garnered significant attention as a means to enhance planning via large language models (LLMs) Strobel & Kirsch (2020); Silver & Chitnis (2020); Silver et al. (2022); Vyas et al. (2025). In parallel, the advent of sophisticated prompting techniques has unlocked new applications for LLMs Liu et al. (2023b); Graux et al. (2024). Nonetheless, while LLMs have demonstrated planning capabilities Huang et al. (2024), they continue to struggle with long-horizon planning, uncertainty in generated plans, and generalisation to unseen domains Sermanet et al. (2023). Consequently, several works have aimed to bridge the gap between the probabilistic nature of LLMs and the deterministic requirements of PDDL-based planners. For instance, Collins et al. (2022) compared the out-of-distribution robustness of PDDL-augmented LLMs with human reasoning, highlighting clear limitations in current LLM approaches.\nIn many settings, LLMs have proven more effective at translating natural language into formal representations rather than performing the planning itself, as noted in works such as Alford et al. (2009); Helmert (2009); Xie et al. (2023). This observation has spurred strategies that decompose the problem into translating user instructions into PDDL problems, solving these problems via formal logic within the PDDL framework, and then translating the resulting plans back into natural language Pallagani et al. (2023); Liu et al. (2023a); Dagan et al. (2023); Silver et al. (2024); Gestrin et al. (2024); Mahdavi et al. (2024); Zhang et al. (2024).\nMore recent contributions have further refined the dialogue between LLMs and planning. Hao et al. (2023) propose that reasoning with a language model can be reinterpreted as planning with an integrated world model, while Rossetti et al. (2024) explore the learning of general policies for planning directly via GPT models. In addition, benchmark efforts such as PlanBench introduced by Valmeekam et al. Valmeekam et al. (2023) and critical investigations into LLM planning abilities Valmeekam et al. (2023) provide valuable insights into the performance and limitations of current models.\nNovel benchmarks such as PlanBench Valmeekam et al. (2023), AutoPlanBench Stein et al. (2024), Planetarium Zuo et al. (2024), and the domain benchmark from Oswald et al. (2024) have been introduced to assess LLMs' planning capabilities using PDDL. However, to the best of our knowledge, the recent families of foundational models have not yet been extensively benchmarked to reveal their inherent robustness and reliability in handling PDDL generation. In this study, we explore the capacity of these foundational models to generate both PDDL domains and problems, thereby extending prior evaluations and situating our work alongside the latest advances in planning with LLMs."}, {"title": "4 CONCLUSION", "content": "In this study, we experimentally reviewed the PDDL capabilities of a large panel of language models: twenty in total, representing multiple dimensions of the current state-of-the-art. Our evaluations show that (some) LLMs can be used to generate actions to complete PDDL domains, they may also be used to assist in the task of generating PDDL problems from NL instructions. However, as expected, they reveal being poor planners and it is better to rely on \"real\" planners which have been developed by the community for decades. Surprisingly also, behaviours given a specific model are not uniform across tasks as good performers (even leaders) for a certain PDDL aspect may turn out to be among the worst ones later. Overall, we hope to pave the road to future efforts in AI-driven planning challenges."}, {"title": "A EXAMPLES OF 0-SHOT PROMPTS", "content": "In order to give a better idea on the prompts we triggered at the language models, we provide in this Appendix an example for each of the tested task, i.e. Action creation, Problem generation and Plan creation. We refer the reader to the main body of the article for more details, see e.g. Figure 1 (A, B and C) for details of the results\u2074 . In particular, here we provide:\n\u2022 The action generation (put-down) for the BLOCKS domain;\n\u2022 A problem generation in blocksworld domain (init=holding_one);\n\u2022 A plan generation in blocksworld domain(init=on_table,goal=tower).\nGiven a description of an action in some domain, convert it to Planning Domain Definition Language (PDDL) action. You may only use the allowed predicates for each action.\nAllowed Predicates:\n(handempty): the hand is empty.\n(clear ?x - block): block x is clear.\n(holding ?x - block): block x is held.\n(on ?x - block ?y - block) : block x is on block y.\n(ontable ?x - block): block x is on the table.\nInput: The action, \u201cput-down\u201d will have the hand put down a block.\nPDDL Action:\n(:action put-down\n:parameters (?x - block)\n:precondition (holding ?x)\n:effect (and (not (holding ?x)) (clear ?x) (handempty) (ontable ?x))\n)\nProvide me with the complete, valid problem PDDL file that describes the following planning problem directly without further explanations or texts. The domain for the planning problem is:\n(define (domain blocksworld) (:requirements :strips) (:predicates (arm-empty) (clear ?x) (holding ?x) (on ?x ?y) (on-table ?x)) (:action pickup :parameters (?ob) :precondition (and (clear ?ob) (on-table ?ob) (arm-empty)) :effect (and (holding ?ob) (not (clear ?ob)) (not (on-table ?ob)) (not (arm-empty)))) (:action putdown :parameters (?ob) :precondition (holding ?ob) :effect (and (clear ?ob) (arm-empty) (on-table ?ob) (not (holding ?ob))) ) (:action stack :parameters (?ob ?underob) :precondition (and (clear ?underob) (holding ?ob)) :effect (and (arm-empty) (clear ?ob) (on ob underob) (not (clear ?underob)) (not (holding ?ob)))) (:action unstack :parameters (?ob ?underob) :precondition (and (on ?ob ?underob) (clear ?ob) (arm-empty)) :effect (and (holding ?ob) (clear ?underob) (not (on ?ob ?underob)) (not (clear ?ob)) (not (arm-empty)))))\nPlanning problem:\nYou have 17 blocks. You are holding bl. b2 is clear. b2 is on the table. b3 is clear. b3 is on the table. b4 is clear. b4 is on the table. b5 is clear. b5 is on the table. b6 is clear. b6 is on the table. b7 is clear. b7 is on the table. b8 is clear. b8 is on the table. b9 is clear. b9 is on the table. b10 is clear. b10 is on the table. b11 is clear. b11 is on the table. b12 is clear. b12 is on the table. b13 is clear. b13 is on the table. b14 is clear. b14 is on the table. b15 is clear. b15 is on the table. b16 is clear. b16 is on the table. b17 is clear. b17 is on the table.\nYour goal is to stack the blocks into a towers of heights 1, 1, 1, 2, 3, 9.\nProblem PDDL:"}, {"title": null, "content": "(:requirements :strips)\n(:objects b1 b10 b11 b12 b13 b14 b15 b16b17 b2 b3 b4 b5 b6 b7 b8 b9)\n(:init (clear b10) (clear b11) (clear b12) (clear b13) (clear b14) (clear b15) (clear b16)\n(clear b17) (clear b2) (clear b3) (clear b4) (clear b5) (clear b6) (clear b7) (clear b8) (clear\nb9) (holding bl) (on-table b10) (on-table b11) (on-table b12) (on-table b13) (on-table b14)\n(on-table b15) (on-table b16) (on-table b17) (on-table b2) (on-table b3) (on-table b4) (on-\ntable b5) (on-table b6) (on-table b7) (on-table b8) (on-table b9))\n(:goal (and (arm-empty) (clear b1) (on-table bl) (clear b2) (on-table b2) (clear b3) (on-\ntable b3) (clear b4) (on b4 b5) (on-table b5) (clear b6) (on b6 b7) (on b7 b8) (on-table b8)\n(clear b9) (on b9 b10) (on b10 b11) (on b11 b12) (on b12 b13) (on b13 b14) (on b14 b15)\n(on b15 b16) (on b16b17) (on-table b17)))\n)\nGiven a PDDL domain and a PDDL problem file, come up with the plan associated with the\nproblem. The domain describes the possible actions and their effects, while the problem file\ndetails the specific scenario to be solved. Do not generate anything but the correct plan\nDomain PDDL:\n(define (domain blocksworld) (:requirements :strips) (:predicates (arm-empty) (clear ?x)\n(holding ?x) (on ?x ?y) (on-table ?x)) (:action pickup :parameters (?ob) :precondition (and\n(clear ?ob) (on-table ?ob) (arm-empty)) :effect (and (holding ?ob) (not (clear ?ob)) (not (on-\ntable ?ob)) (not (arm-empty)))) (:action putdown :parameters (?ob) :precondition (holding\n?ob) :effect (and (clear ?ob) (arm-empty) (on-table ?ob) (not (holding ?ob)))) (:action stack\n:parameters (?ob ?underob) :precondition (and (clear ?underob) (holding ?ob)) :effect (and\n(arm-empty) (clear ?ob) (on ?ob ?underob) (not (clear ?underob)) (not (holding ?ob)))) (:action\nunstack :parameters (?ob ?underob) :precondition (and (on ?ob ?underob) (clear ?ob)\n(arm-empty)) :effect (and (holding ?ob) (clear ?underob) (not (on ?ob ?underob)) (not (clear\n?ob)) (not (arm-empty)))))\nProblem PDDL:\n(define (problem on_table_to_tower_1_1_1_1_3_13)(:domain blocksworld)(:requirements\n:strips)(:objects b1 b10 b11 b12 b13 b14 b15 b16 b17 b18 b19 b2 b20 b3 b4 b5 b6 b7\nb8 b9)(:init (arm-empty) (clear b1) (clear b10) (clear b11) (clear b12) (clear b13) (clear b14)\n(clear b15) (clear b16) (clear b17) (clear b18) (clear b19) (clear b2) (clear b20) (clear b3)\n(clear b4) (clear b5) (clear b6) (clear b7) (clear b8) (clear b9) (on-table bl) (on-table b10)\n(on-table b11) (on-table b12) (on-table b13) (on-table b14) (on-table b15) (on-table b16)\n(on-table b17) (on-table b18) (on-table b19) (on-table b2) (on-table b20) (on-table b3) (on-\ntable b4) (on-table b5) (on-table b6) (on-table b7) (on-table b8) (on-table b9))(:goal (and\n(arm-empty) (clear b1) (on-table b1) (clear b2) (on-table b2) (clear b3) (on-table b3) (clear\nb4) (on-table b4) (clear b5) (on b5 b6) (on b6 b7) (on-table b7) (clear b8) (on b8 b9) (on b9\nb10) (on b10 b11) (on b11 b12) (on b12 b13) (on b13 b14) (on b14 b15) (on b15 b16) (on\nb16 b17) (on b17b18) (on b18b19) (on b19b20) (on-table b20))))\n)"}]}