{"title": "Reasoning Does Not Necessarily Improve Role-Playing Ability", "authors": ["Xiachong Feng", "Longxu Dou", "Lingpeng Kong"], "abstract": "The application of role-playing large language models (LLMs) is rapidly expanding in both academic and commercial domains, driving an increasing demand for high-precision role-playing models. Simultaneously, the rapid advancement of reasoning techniques has continuously pushed the performance boundaries of LLMs. This intersection of practical role-playing demands and evolving reasoning capabilities raises an important research question: \"Can reasoning techniques enhance the role-playing capabilities of LLMs?\" To address this, we conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing strategies, comparing the effectiveness of direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, large models still lack proficiency in advanced role-playing, and Chinese role-playing performance surpasses English role-playing performance. Furthermore, based on extensive experimental results, we propose two promising future research directions: Role-aware CoT for improving role-playing LLMs and Reinforcement Learning for role-playing LLMs, aiming to enhance the adaptability, consistency, and effectiveness of role-playing LLMs for both research and real-world applications.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), leveraging their strong foundational capabilities and advanced instruction-following abilities, have become the cornerstone of role-playing AI models (Chen et al., 2024b; Tseng et al., 2024). These role-playing LLMs present new opportunities in both research and commercial applications. In the academic domain, they offer novel possibilities for social simu-"}, {"title": "2 Evaluation Framework", "content": "The evaluation framework for LLM-based role-playing agents primarily consists of the following components: the LLM itself (M), character profile (P), evaluation task (T), evaluation metric (E), and, when applicable, a reference standard answer (A). In this study, we focus on whether reasoning techniques (R) can enhance the role-playing capabilities of LLMs. Therefore, the overall evaluation process can be formalized as follows:\nGiven an LLM M, a predefined character profile P, an evaluation task T, and an evaluation metric E, the role-playing performance of M is assessed by generating responses conditioned on P and T. The generated outputs are then evaluated using E, which quantifies alignment with the intended role. Additionally, reasoning techniques R are incorporated into M to examine their impact on enhancing role-playing abilities. Formally, the evaluation process can be expressed as: $S = E(M(P,T,R), A)$ where S represents the final performance score, capturing the effectiveness of M in role-playing under the given conditions. A higher S indicates stronger role-playing capabilities. If A is unavailable or unnecessary, the evaluation metric E can be adapted to rely on alternative assessment criteria, such as LLM-as-a-Judge."}, {"title": "3 Experimental Setting", "content": "In this section, we introduce the benchmarks, models, metrics, and reasoning methods used in our experiments. All relevant code has been integrated into the OpenCompass (Contributors, 2023) library to facilitate efficient replication by researchers."}, {"title": "3.1 Role-playing Benchmarks", "content": "We select six role-playing benchmarks to ensure the reliability of our experimental results, including: RoleBench (Wang et al., 2023b): Generates responses to both general and role-specific ques-tions based on role information. HPD (Chen et al.,"}, {"title": "3.2 Backbone LLMs", "content": "Our experiments include a total of 24 models, comprising 2 closed-source models and 22 open-source models. These models originate from six different companies, including state-of-the-art models such as GPT-4-Turbo and DeepSeek-R1. The detailed list of models is provided in Appendix A."}, {"title": "3.3 Evaluation Metrics", "content": "Our evaluation incorporates two types of metrics: automated metrics and LLM-as-a-Judge, both"}, {"title": "3.4 Role-playing Methods", "content": "In this study, we employ three approaches to guide LLMs in performing role-playing tasks: R1: Direct zero-shot role-playing using an LLM, where the model generates responses without reasoning steps. R2: Role-playing with chain-of-thought (CoT) reasoning, where an LLM explicitly engages in step-by-step reasoning before executing the role-playing task. R3: Role-playing using reasoning-optimized LLMs, such as QwQ-32B-Preview and DeepSeek-R1, which autonomously engage in deep reasoning before generating responses."}, {"title": "4 Results and Findings", "content": "In this section, we present our experimental analysis and key findings."}, {"title": "4.1 CoT May Reduce Role-Playing Performance", "content": "To investigate whether reasoning techniques enhance the role-playing capabilities of LLMs, we conduct extensive experiments. Specifically, we select 17 models and evaluate their role-playing performance using both zero-shot and chain-of-thought (CoT) approaches across six standardized and widely used benchmarks. All experimental results are presented in the Appendix C. Figure 1 provides an aggregated overview of the results. Specifically, for each model, its final performance on a given benchmark is computed as the average of its performance across all sub-datasets.\nAs shown in Figure 1, employing CoT reasoning is more likely to degrade role-playing performance on four benchmarks: CroSS-MR, HPD, SocialBench, and CharacterEval. In contrast, on In-Character and RoleBench, CoT reasoning enhances the role-playing capabilities of LLMs. To further understand why CoT reduces the role-playing capabilities of LLMs, we select Qwen2.5-7B-Instruct as the experimental model. We then sample 50 test cases from each of the six benchmarks where CoT performance is lower than Zero-shot performance for detailed analysis.\nOur findings indicate that the primary reasons for CoT-induced degradation in role-playing are: (1) \"Attention Diversion\": The model must simultaneously engage in reasoning and role-playing modes, which dilutes its focus on the role-playing task. (2) \"Linguistic Style Drift\u201d: Reasoning responses tend to be structured, logical, and formal, whereas effective role-playing requires a vivid, expressive, and character-consistent linguistic style."}, {"title": "4.2 Reasoning-optimized LLMs Are Unsuitable for Role-Playing", "content": "The most advanced models available today are undoubtedly reasoning-optimized models, including OpenAI's o series, DeepSeek's R1 model, and various distilled versions derived from DeepSeek-R1. Compared to CoT reasoning, these models leverage pretraining and reinforcement learning techniques to cultivate intrinsic reasoning capabilities, making them inherently more adept at reflection, verification, and other cognitive processes.\nTo investigate whether reasoning-optimized"}, {"title": "4.3 Reasoning Ability Disrupts the Role-Playing Scaling Law", "content": "Figure 3 presents the results of our three experimental settings: direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. First, we observe that, with the exception of the InCharacter benchmark, the other five benchmarks generally follow the scaling law, where larger models exhibit stronger role-playing capabilities. However, we also find that the role-playing scaling law is not particularly pronounced the performance gains from increasing model size remain relatively modest and inconsistent across benchmarks. Furthermore, introducing reasoning capabilities, whether through CoT or reasoning-optimized LLMs, further weakens the benefits of scaling, leading to more pronounced fluctuations, increased instability, and greater variability in model performance across different role-playing tasks and datasets, making the scaling trend less predictable and consistent."}, {"title": "4.4 The Qwen Series Is Well-Suited for Role-Playing Tasks", "content": "To provide guidance on model selection for future role-playing tasks, we conduct a comprehensive and systematic comparative analysis across different model scales (e.g., 1B, 3B, 7B, 14B, 32B, 72B). The results are shown in Figure 3. Our findings indicate that the Qwen2.5 series consistently outperforms the Llama, Gemma, and Mistral series across various size ranges in terms of role-playing accuracy, persona consistency, linguistic expressiveness, and contextual coherence. Notably, we recommend Qwen2.5-7B-Instruct as the most cost-effective and well-balanced model for role-playing applications, as it offers a strong trade-off between performance, computational efficiency, and adaptability across diverse role-playing scenarios."}, {"title": "4.5 Chinese Role-Playing Performance Surpasses English Role-Playing Performance", "content": "The role-playing benchmarks HPD, SocialBench, InCharacter, and RoleBench provide both Chinese and English versions, enabling a direct comparison of multilingual role-playing performance. Interestingly, an analysis of Figure 3 reveals that current LLMs exhibit stronger role-playing capabilities in Chinese than in English across multiple benchmarks and evaluation metrics. This observation contradicts the common conclusion that LLMS generally have a stronger foundation in English compared to other languages, particularly in reasoning and knowledge-intensive tasks. We hypothesize that this phenomenon may be attributed to the following factor: Due to the extensive training on English data, models have to some extent internalized generalized character information. When performing precise role-playing tasks, this internalized generalization can interfere with context-sensitive role-playing, leading to performance degradation."}, {"title": "4.6 Large Models Still Lack Proficiency in Advanced Role-Playing", "content": "To further investigate the limitations of current LLM-based role-playing models, we conduct an analysis using the CharacterEval benchmark. Specifically, CharacterEval provides a pretrained reward model that assigns scores across 12 role-playing evaluation dimensions. We analyze the performance of the Qwen2.5 series, with the results presented in Figure 4. All detailed results are presented in Appendix D. First, consistent with our previous findings, direct zero-shot role-playing outperforms other approaches in most cases. Furthermore, a fine-grained comparison of evaluation metrics reveals that current LLMs still exhibit deficiencies in advanced role-playing dimensions, such as character knowledge exposition, personality expression, and linguistic diversity. These aspects require further enhancement in future research to improve the overall role-playing capability of LLMs."}, {"title": "5 Related Works", "content": "In recent years, large language models (LLMs) have revolutionized the paradigm of artificial intelligence, achieving human-like performance in mathematics (Liu et al., 2023; Shao et al., 2024), coding (Luo et al., 2023; Roziere et al., 2023; Guo et al., 2024; Zhu et al., 2024), embodied intelligence (Wang et al., 2023a; Liu et al., 2024), game intelligence (Hu et al., 2024; Feng et al., 2024), and advanced reasoning (Chu et al., 2023; Xu et al., 2025). The comprehensive enhancement of foundational capabilities in these models has opened new opportunities for the development of role-playing agents (Chen et al., 2024b), resulting in a proliferation of such applications based on LLMs. On one hand, this has catalyzed innovations in fields such as psychological counseling, anthropomorphic companionship, and game NPCs (Tseng et al., 2024). On the other hand, it has provided fresh avenues for advancing social simulations (Mou et al., 2024). Concurrently, reasoning techniques in LLMs have flourished, with methods like chain-of-thought reasoning (Wei et al., 2022), tree-of-thought reasoning (Yao et al., 2024), and o1-style reasoning (Jaech et al., 2024) progressively maximizing the potential of these models, particularly achieving remarkable outcomes in tasks involving mathematics and coding (Guo et al., 2025; Team et al., 2025). While prior research has predominantly focused on role-playing applications and their evaluation, this study is positioned at the intersection of LLM-based role-playing agents and reasoning techniques. It seeks to address the scientific question of whether reasoning techniques enhance the role-playing capabilities of LLMs. By doing so, this work aims to provide guidance for developing more realistic and reliable role-playing agents powered by LLMs."}, {"title": "6 Future Directions", "content": "Based on our experimental findings, we propose two potential research directions that integrate role-playing and reasoning techniques for future exploration."}, {"title": "Role-aware Chain-of-Thought (CoT) for Improving Role-playing LLMs", "content": "One promising direction for enhancing the role-playing ability of LLMs is Role-aware CoT reasoning. While standard CoT enables step-by-step logical inference, it often disregards the persona-specific constraints that are essential for consistent role-playing. A role-aware CoT approach would integrate persona attributes, narrative constraints, and character-specific perspectives into the reasoning process, ensuring that logical deductions align with the character's predefined traits. For instance, a historical figure simulated in an LLM should reason within the knowledge and biases of their era rather than applying contemporary logic. This requires incorporating dynamic memory structures that retain persona information throughout multi-turn interactions, mitigating the risk of breaking character or adopting inconsistent reasoning patterns."}, {"title": "Reinforcement Learning for Role-playing LLMs", "content": "DeepSeek-R1 has demonstrated that by defining precise rule-based rewards, reinforcement learning alone can induce emergent reasoning and cognitive capabilities. This suggests an important research direction: investigating whether carefully designed role-playing task rewards can enable models to autonomously develop intrinsic, role-specific reasoning and thinking abilities, thereby enhancing their role-playing performance. A critical consideration in reward design is ensuring that it simultaneously accounts for the accuracy of the role-playing task while effectively guiding the model to develop role-specific reasoning abilities. The reward function should prevent the model from relying on shortcuts to generate final responses, instead encouraging it to engage in authentic character-driven reasoning and decision-making."}, {"title": "7 Conclusion", "content": "This study aims to address the research question: \"Can reasoning techniques enhance the role-playing capabilities of large language models (LLMs)?\" To this end, we conduct extensive experiments using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing methods. Our experimental results lead to the following key findings: CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, the Qwen series is well-suited for role-playing tasks, Chinese role-playing performance surpasses English role-playing performance, and large models still lack proficiency in advanced role-playing. All code is integrated into OpenCompass, ensuring reproducibility and facilitating further research. We hope that our findings provide new perspectives for future studies on role-playing LLMs."}, {"title": "Limitations", "content": "Although we have made every effort to comprehensively cover the most commonly used models and role-playing benchmarks, our selection may still be incomplete. Certain niche models or specialized benchmarks may not have been included, which could introduce potential biases in our findings. As a result, while our conclusions provide valuable insights into the role-playing capabilities of large language models, their robustness could be further enhanced through broader model coverage, additional benchmark evaluations, and more extensive experimental validations. Future studies should explore a wider range of datasets and model architectures to ensure greater generalizability and reliability of the findings."}]}