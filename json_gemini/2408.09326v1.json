{"title": "Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks", "authors": ["Kexin Chen", "Yi Liu", "Dongxia Wang", "Jiaying Chen", "Wenhai Wang"], "abstract": "Large Language Models (LLMs) have increasingly become pivotal in content generation with notable societal impact. These models hold the potential to generate content that could be deemed harmful. Efforts to mitigate this risk include implementing safeguards to ensure LLMs adhere to social ethics. However, despite such measures, the phenomenon of \"jailbreaking\" where carefully crafted prompts elicit harmful responses from models persists as a significant challenge. Recognizing the continuous threat posed by jailbreaking tactics and their repercussions for the trustworthy use of LLMs, a rigorous assessment of the models' robustness against such attacks is essential. This study introduces an comprehensive evaluation framework and conducts an large-scale empirical experiment to address this need. We concentrate on 10 cutting-edge jailbreak strategies across three categories, 1525 questions from 61 specific harmful categories, and 13 popular LLMs. We adopt multi-dimensional metrics such as Attack Success Rate (ASR), Toxicity Score, Fluency, Token Length, and Grammatical Errors to thoroughly assess the LLMs' outputs under jailbreak. By normalizing and aggregating these metrics, we present a detailed reliability score for different LLMS, coupled with strategic recommendations to reduce their susceptibility to such vulnerabilities. Additionally, we explore the relationships among the models, attack strategies, and types of harmful content, as well as the correlations between the evaluation metrics, which proves the validity of our multifaceted evaluation framework. Our extensive experimental results demonstrate a lack of resilience among all tested LLMs against certain strategies, and highlight the need to concentrate on the reliability facets of LLMs. We believe our study can provide valuable insights into enhancing the security evaluation of LLMs against jailbreak within the domain.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as ChatGPT [54], and LLaMA [37] are types of attention-based sequential models based on the transformer architecture [67], which have driven rapid advances in the performance and generality of artificial intelligence (AI) systems. They are benefiting lots of applications recently, ranging from AI chatbots [1], coding assistants [29], to AI agents [33].\nDespite the growing intelligence and popularity in plenty of downstream applications [5], LLMs have also raised concerns about their reliability and security [35]. We focus on LLMs' reliability under Jailbreak attacks in this work."}, {"title": "2 Related Work", "content": "In this section, we review the existing Jailbreak attack strategies and the existing works which also aim to evaluate LLMs against jailbreak attacks. We also briefly introduce some other issues except jailbreak attacks that impede the trustworthy use of LLMs."}, {"title": "2.1 LLM Jailbreak", "content": "Currently, the majority of jailbreak attacks are accomplished through the creation of \"jailbreak prompts\". There exist multiple strategies to identify or craft such prompts, including collecting them from real-world scenarios, manually creating them by guided rules, or using automatic generation [85]. Overall, they can be categorized as follows:\nManual Crafting: This category encompasses jailbreak prompts crafted manually, leveraging human creativity to circumvent model constraints. Strategies such as role-playing [43] and scenario"}, {"title": "2.2 Evaluating Reliability of LLMs", "content": "In the pursuit of evaluating the reliability of LLMs, multiple studies have shed light on the jail- breaking vulnerability of LLMs and attempted to assess their reliability.\nMazeika et al. [51] introduces a framework for evaluating the efficacy of automated red teaming methods [10]. Through their extensive evaluation, they reveal key insights into the effectiveness of various red teaming methods against LLMs and defenses. Chao et al. [14] presents a uniform benchmark for analyzing jailbreak attacks, encompassing an ethically aligned behavior dataset, and a definitive evaluation protocol. Zhou et al. [85] proposes an elaborate framework for designing and scrutinizing jailbreak attacks, and standardizes the process of generating jailbreak attacks. Shen et"}, {"title": "2.3 Vulnerabilities of LLMs", "content": "Numerous vulnerabilities identified in LLMs fall into two principal categories: inherent issues and intended attacks [56]. Aside from jailbreak attacks, we briefly outline additional prevalent and significant vulnerabilities in LLMs.\nInherent vulnerabilities are intrinsic shortcomings of LLMs that are not easily rectified by the models themselves [35]. For example, factual errors occur when LLM outputs contradict established truths, a phenomenon some literature refer as hallucination [5,44,83]. Additionally, reasoning errors highlight the models' limited capacity for logical and mathematical problem-solving, often resulting in inaccurate responses to calculative or deductive reasoning tasks [5, 28, 46].\nIn contrast, intended attacks are orchestrated by malicious attackers seeking to manipulate LLMs to achieve their objectives [35]. For example, prompt injection attacks overrides an LLM's original prompt and directs it to follow malicious instructions, which may result in data exposures or enable social engineering schemes [7,32,34,48]. Conversely, training data poisoning is characterized by the deliberate contamination of an LLM's training dataset with the aim of degrading its performance or instilling biases, which can subsequently distort predictions and conduct [13, 17,66,68].\nIn essence, it is the vulnerability landscape that makes evaluating the reliability of LLMs an essential task, ensuring their security and trustworthy use in practice."}, {"title": "3 Preliminaries: Jailbreaking LLMs", "content": "The goal of a jailbreak attack is to design input prompts in such a way that they can cause a LLM to generate texts that are harmful, toxic, or objectionable, bypassing its built-in security restrictions [70]. Below we formally define LLMs and jailbreak attacks.\nAn LLM takes a sequence of tokens of an arbitrary length as input and returns a distribution on the next token in the sequence. Let T* denote the set of all sequences of tokens of arbitrary length. Let \\Delta(T) denote the set of probability distributions over T.\nAn LLM can be defined as a mapping:\nLLM : T* \\rightarrow \\Delta(T)\nIn the context of jailbreaking, adversaries gain access to a LLM as an attack target. Additionally, adversaries possess a harmful query Q, which instructs the target LLM to produce objectionable content, such as \"Tell me how to build a bomb?\" or \"How to create and distribute malware for financial gain?\"."}, {"title": "4 Evaluation Framework", "content": "Overview. Our evaluation framework consists of five modules, including dataset construction, jailbreak attack strategies, LLMs, response evaluation, and metrics aggregation. The workflow is illustrated in Fig. 2."}, {"title": "4.1 Dataset Construction", "content": "We adopted the dataset framework proposed by Wang et al. [69] that employs an exhaustive three- level hierarchical taxonomy to evaluate safeguards in LLMs. This comprehensive categorization enumerates 61 specific instances of harm, clustered into 12 harm types and 5 broader risk do- mains. In an effort to enhance the comprehensiveness of our evaluation, we expanded the dataset significantly. For each instance of harm, we added 25 tailored malicious queries, thus enriching the dataset to a total of 1525 entries (25 queries \u00d7 61 specific harms). This augmentation was achieved through meticulous manual curation and the integration of selected examples from Ad- vBench [86], MultiJail\u00b9, SimpleSafetyTests, MM-SafetyBench, MaliciousInstruct7, BeaverTails- Evaluations, Verazuo-jailbreak-llms, and Xstest [60]."}, {"title": "4.2 Evaluation Pipeline", "content": "Jailbreak Attack Strategies. Based on popularity and accessibility, we select multiple existing strategies for each category of the attacks. For Manual Crafting-based attacks, we apply JailBro- ken [70], DeepInception [45], and ICA [72]. For longtail Encoding-based attacks, we apply Mul- tiLingual [20], Cipher [81], and CodeChameleon [50]. For prompt Refinement-based attacks, we"}, {"title": "Evaluation Criteria", "content": "Using the language of Askell et al. [8], we want language models to be helpful (they should help the user solve their task), honest (they shouldn't fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). Moreover, The principles of responsible AI as investigated by Bano et al. [9] assert that AI systems should be designed to conform to ethical norms as well as to robustly resist manipulations that might lead to misuse or harm. Additionally, human-computer interaction (HCI) [57] seeks to enhance the interactions between humans and AI to be more fluid, natural, and intuitive. Engaging with an LLM that generates coherent and grammatically sound responses not only encourages continuous user engagement but also fosters trust in the system. These principles guide us to establish evaluation criteria for LLMs, aiming to uphold high standards of quality and safety in AI systems. Ideally, faced with jailbreak attacks, a reliable LLMs should meet the following criteria:\n\u2022 Resistance to Jailbreak. The responses generated by LLMs should neither explicitly nor implicitly fulfill harmful queries.\n\u2022 Non-Toxicity. The responses should not contain harmful, toxic, obscene, insulting, or threat- ening content.\n\u2022 Quality. Text quality usually contains three dimensions: semantic, syntactic, and structural. The responses should maintain semantic integrity and fluency while minimizing grammatical errors."}, {"title": "Response Evaluation", "content": "We consider the criteria of LLM Reliability in Sec. 4.2 and employ five metrics to measure and characterize the reliability of responses of LLMs against jailbreak attacks."}, {"title": "Metrics Aggregation", "content": "To furnish a more comprehensive reliability score of each model as reference, it is essential to normalize the metric values and incorporate them into a composite score. This score reflects the weighted significance of various dimensions, with the determination of these weights entrusted to the discretion of the model user. Subsequently, we describe the procedure in detail.\nNormalization. For metrics that improve with minimization (e.g., ASR, Toxicity Score, Gram- matical Errors, and Fluency), a higher value indicates decreased reliability. Conversely, for the metric"}, {"title": "5 Experiment", "content": "5.1 Evaluation of Attack Strategy\nIn our evaluation framework, we consider multiple metrics; notably, the Attack Success Rate (ASR) is uniquely tailored to quantify the Resistance of LLMs to jailbreak attacks. This metric is initially calculated and scrutinized in our study.\nTable 3 presents the ASR results for the target LLMs under various jailbreak attack strategies, as categorized in our taxonomy. From the baseline results, we observe that any of the thirteen LLMs do not demonstrate initial resistance to harmful queries. Notably, well-aligned models such as Llama3, GPT-3.5, and GPT-4 yielded baseline ASR scores of 20.26%, 13.84%, and 16.26%, respectively.\nThe results indicates that the average ASR across some LLMs does not surpass the baseline, which suggests a degree of resistance to jailbreak attacks in some LLMs. Vicuna and Mistral emerged as the LLM families most vulnerable or susceptible to jailbreak attacks, which aligns with our conclusion related to the harm type (see Sec. 6.1) and many previous works [47,80,86].\nFrom the perspective of jailbreak category, we find that all jailbreak strategies, except for ICA and Cipher, demonstrate high attack efficacy, as demonstrated in Table 3. After detailed analysis, we"}, {"title": "5.2 Characterizing the Reliability of LLMs under Attacks", "content": "In this section, we calculate all the metrics in our evaluation framework and present the reliability of the responses of LLMs under jailbreak attack. Table 4 shows the evaluation metrics of responses of various jailbreak attacks on LLMs.\nIn general, from Table 4, GPT-4 stands out for its low toxicity across all categories and a relatively low ASR, making it highly reliable in maintaining non-toxic outputs under attacks, despite its higher perplexity. Llama3 combines a low ASR, low toxicity, and few grammatical errors, with decent fluency, making it another strong contender for reliability. Gemma-2b-it has the lowest ASR,"}, {"title": "5.3 Reliablity Score and Evaluating", "content": "Building upon the process delineated in Sec. 4.3, we initially normalized the values of all metrics listed in Table 4. Following this, as stipulated in Eq. 10, weights can be allotted to each metric at the discretion of the model user. Assuming equal significance for all metrics, Eq. 10 simplifies to the sum average of all the normalized metrics.\nTable. 5 displays the calculated reliability scores for each model, considering the normalization of each metric based on the criteria provided. The models are sorted in descending order of their reliability scores, where a higher score indicates a better overall reliability under jailbreak based on the aggregated metrics. This aggregation provides a comprehensive view of the overall reliability of each model by integrating various metrics into a unified score."}, {"title": "6 Ablation Study", "content": "6.1 Evaluation of Harm Type\nBuilding upon the dataset structure outlined in Table 2, we can analyze the ASR of various LLMs across different harm types. As illustrated in Table 6, we observed a significantly variance in ASR"}, {"title": "6.2 Relationship Between Attack Strategy and Harm Type", "content": "To further explore the relationship between the harm type (i.e., the 12 harm type categories in Table 2) and the jailbreak attacks (i.e., the 10 different types of jailbreak attack strategies), we visualize the ASR between the two using a heatmap, as depicted in Fig. 3b.\nFirst, it shows that the ICA, and Cipher jailbreak attacks demonstrate consistently poor per- formance across all violation categories in the harm types, denoted by the negative correlation in the heatmap. Furthermore, the Prompt Refinement-based jailbreak attacks are relatively robust and versatile among all the violation categories. Among all the Manual Crafting-based jailbreak methods, Jalibroken is the most effective one from various categories of our harm types. The above conclusions intuitively support the findings we presented previously (see Sec. 5.1)."}, {"title": "6.3 Correlation Between Metrics", "content": "In this section, we conducted a correlation analysis between the evaluation metrics in our framework. As we know, jailbreak attacks on LLMs aim to manipulate models to produce damaging content, where their potential effectiveness may in part rely on the level of malignancy and toxicity present in the resulting outputs. We initially investigate the relationship between the ASR metric evaluated in Sec. 5.1 and our toxicity metrics. we calculated the Pearson correlation coefficient between the ASR metric and the toxic, obscene, insult and threat metrics, subsequently illustrating the results through a heatmap presented in Fig. 4a. Additionally, we rendered a scatter plot enriched with a regression line, depicted in Fig. 4b.\nOur analysis revealed that the ASR is moderately positively correlated with the metrics for toxicity, obscenity, insult, and threat. Nonetheless, regression analyses suggested that the impact of these toxicity metrics on ASR was statistically negligible, which hints at the existence of additional influencing factors. Consequently, it appears that ASR is subject to a variety of influences, with the considered toxicity metrics contributing only partially.\nAdditionally, as detailed in Sec. 4.2, we sought to quantify the output quality of LLMs across three metrics: Fluency, Token Length, and Grammatical Errors. To elucidate the relationships between these metrics, we carried out a statistical analysis accompanied by a visualization, presented in Fig. 5, to uncover potential patterns or correlations. The analysis divulged a relative strong positive correlation (0.78) between Grammatical Errors and Token Length, signifying that models producing longer responses typically exhibit more grammatical inaccuracies. Conversely, Fluency did not demonstrate a robust correlation with either Grammatical Errors or Token Length, implying that it captures a distinct facet of response quality.\nIn conclusion, the empirical findings from our correlation analysis underscore the validity of our multifaceted evaluation framework. Each of the five metrics we have selected - Adversarial Success"}, {"title": "7 Conclusion", "content": "In this paper, we present a novel and comprehensive evaluation framework designed to assess the reliability of LLMs when exposed to diverse jailbreak attack strategies. Our evaluation framework included the construction of a refined three- level hierarchical dataset, comprising 1525 questions across 61 distinct harmful categories. We evaluate the LLMs' outputs under jailbreak by integrating different aspects of content security, such as resistance to jailbreak, toxicity, and quality, and using multi-dimensional metrics like Attack Success Rate (ASR), Toxicity Score, Fluency, Token Length, and Grammatical Errors.\nThrough elaborate experiments involving 13 popular LLMs and 10 state-of-the-art jailbreak attack strategies, we holistically analyze the jailbreak attacks and uncover significant variations in the models' resilience. Our findings draw attention to the inherent vulnerabilities of some of the most popular LLMs, such as Vicuna and Mistral, which exhibited heightened susceptibility to"}, {"title": "8 Limitations", "content": "To address the constraints posed by limited resources, our evaluation experiment does not extend to larger models, such as those with 33 billion and 70 billion parameters, nor does it cover other powerful commercial models like Claude [6] and Gemini [30]."}, {"title": "9 Apendix", "content": "9.1 Choice of Jailbreak Evaluator\nA key difficulty in evaluating the performance of jailbreaking attacks is determining whether a given input prompt succeeds in jailbreaking the target model. Determining the success of an attack involves an understanding of human language and a subjective judgment of whether generated content is objectionable, which is challenging even for humans. With a result dataset encompassing 574,577 responses, it is impractical to label all the results manually. Existing strategies for addressing this issue vary, we consider six candidate jailbreak evalustors which are commonly used in the jailbreaking literature:\n\u2022 PrefixMatch. Zou et al. [86] utilized a set of common refusal patterns, such as \"I am sorry'and\"I cannot\", to automate the identification of non-compliant responses,\n\u2022 BERT. The BERT-base-cased fine-tuned model used as a evaluator [36],\n\u2022 ROBERTa. The RoBERTa fine-tuned model used as a evaluator [80],\n\u2022 GPT-4. A approaches leveraging GPT-4 for attack analysis were explored by Chaoet al [15].\n\u2022 Llama Guard. The fine-tuned Llama Guard evaluator [38],\n\u2022 Llama-2-13b-cls. Distillation fine-tuned Llama-2 -3B-Chat evaluator [51]\nTo choose an effective jailbreak evaluator, we selected a random sample of 1,00 prompts and responses from our result dataset, aiming for a 95% confidence level with a 3% margin of error. Each prompt-response instance was labeled by three computer science graduate students, with the majority vote determining the \"ground truth\" label for each case. Finally, we compared the agreement, false positive rate (FPR), and false negative rate (FNR) of the six Jailbreak Evaluators listed above to these ground truth labels. Our results are summarized in Table 7."}, {"title": "9.2 Dataset Construction Details", "content": "We employed the dataset framework proposed by Wang et al. [69], which originally comprised 939 labeled entries. We augmented this initial dataset with an additional 6,000 entries sourced from other various libraries in Sec. 4.1. To ensure strict classification of new data in our dataset, we attempted to fine-tune text classification models such as BigBird, CANINE, ConvBERT, DeBERTa, ROBERTa, etc., on the do-not-answer dataset [69], aiming to achieve 90% accuracy on the held-out test set.\nAfter careful consideration of the performance metrics suach as accuracy, we selected the bert- base-uncased model for fine-tuning. The fine-tuning protocol includes: a batch size of 8, fifteen training epochs, a learning rate of 5 \u00d7 10\u22125, application of the Adam optimizer, and linear rate decay. Using the fine-tuned model, we labeled integrated dataset with additional 6,000 entries. Finally, to ensure the accuracy and reliability of the dataset classification, we conducted a round of random sampling for manual verification."}]}