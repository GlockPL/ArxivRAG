{"title": "Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks", "authors": ["Kexin Chen", "Yi Liu", "Dongxia Wang", "Jiaying Chen", "Wenhai Wang"], "abstract": "Large Language Models (LLMs) have increasingly become pivotal in content generation with notable societal impact. These models hold the potential to generate content that could be deemed harmful. Efforts to mitigate this risk include implementing safeguards to ensure LLMs adhere to social ethics. However, despite such measures, the phenomenon of \"jailbreaking\" where carefully crafted prompts elicit harmful responses from models persists as a significant challenge. Recognizing the continuous threat posed by jailbreaking tactics and their repercussions for the trustworthy use of LLMs, a rigorous assessment of the models' robustness against such attacks is essential. This study introduces an comprehensive evaluation framework and conducts an large-scale empirical experiment to address this need. We concentrate on 10 cutting-edge jailbreak strategies across three categories, 1525 questions from 61 specific harmful categories, and 13 popular LLMs. We adopt multi-dimensional metrics such as Attack Success Rate (ASR), Toxicity Score, Fluency, Token Length, and Grammatical Errors to thoroughly assess the LLMs' outputs under jailbreak. By normalizing and aggregating these metrics, we present a detailed reliability score for different LLMS, coupled with strategic recommendations to reduce their susceptibility to such vulnerabilities. Additionally, we explore the relationships among the models, attack strategies, and types of harmful content, as well as the correlations between the evaluation metrics, which proves the validity of our multifaceted evaluation framework. Our extensive experimental results demonstrate a lack of resilience among all tested LLMs against certain strategies, and highlight the need to concentrate on the reliability facets of LLMs. We believe our study can provide valuable insights into enhancing the security evaluation of LLMs against jailbreak within the domain.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as ChatGPT [54], and LLaMA [37] are types of attention-based sequential models based on the transformer architecture [67], which have driven rapid advances in the performance and generality of artificial intelligence (AI) systems. They are benefiting lots of applications recently, ranging from AI chatbots [1], coding assistants [29], to AI agents [33].\nDespite the growing intelligence and popularity in plenty of downstream applications [5], LLMs have also raised concerns about their reliability and security [35]. We focus on LLMs' reliability under Jailbreak attacks in this work."}, {"title": "2 Related Work", "content": "In this section, we review the existing Jailbreak attack strategies and the existing works which also aim to evaluate LLMs against jailbreak attacks. We also briefly introduce some other issues except jailbreak attacks that impede the trustworthy use of LLMs."}, {"title": "2.1 LLM Jailbreak", "content": "Currently, the majority of jailbreak attacks are accomplished through the creation of \"jailbreak prompts\". There exist multiple strategies to identify or craft such prompts, including collecting them from real-world scenarios, manually creating them by guided rules, or using automatic generation [85]. Overall, they can be categorized as follows:\nManual Crafting: This category encompasses jailbreak prompts crafted manually, leveraging human creativity to circumvent model constraints. Strategies such as role-playing [43] and scenario crafting [45] are employed to induce models to disregard their built-in protocols. Additionally, some Strategies [72] exploit vulnerabilities in the model's context learning [12] to yield to malicious instructions.\nLongtail Encoding: This category emphasizes models' limited generalization to data not seen during security alignment [70]. However, due to their extensive pretraining, they can still understand intentions and generate unsafe content. These strategies [20, 50, 81] leverages rare or unique data formats. For example, MultiLingual [20] encodes inputs into low-resource languages to bypass security guardrail. CodeChameleon [50] encrypts inputs and embeds a decoding function in the prompt, thus circumventing security checks based on intent, all while maintaining task functionality.\nPrompt Refinement: This category utilizes automated methods to detect and exploit the susceptibilities of models. Strategies like GCG [86] harness model gradients for targeted vulnerability exploration. AutoDAN [47] adopts genetic algorithms for prompt evolution, while GPTFUZZER [80] and FuzzLLM [77] investigate variations of prompts for potential flaws. Additionally,the PAIR [15] iteratively refines prompts based on language model scores.\nThe identified jailbreak attack strategies are elaborated in Table 1, with specific emphasis on those selected for evaluation in our framework."}, {"title": "2.2 Evaluating Reliability of LLMs", "content": "In the pursuit of evaluating the reliability of LLMs, multiple studies have shed light on the jailbreaking vulnerability of LLMs and attempted to assess their reliability.\nMazeika et al. [51] introduces a framework for evaluating the efficacy of automated red teaming methods [10]. Through their extensive evaluation, they reveal key insights into the effectiveness of various red teaming methods against LLMs and defenses. Chao et al. [14] presents a uniform benchmark for analyzing jailbreak attacks, encompassing an ethically aligned behavior dataset, and a definitive evaluation protocol. Zhou et al. [85] proposes an elaborate framework for designing and scrutinizing jailbreak attacks, and standardizes the process of generating jailbreak attacks. Shen et al. [63] performed a large-scale evaluation of ChatGPT's reliability in the generic QA scenario [18], demonstrating variability in performance and vulnerability to adversarial attacks. Xu et al. [75] conducted a comprehensive evaluation of attack and defense strategies for LLMs, revealing the relative effectiveness of these strategies and how special tokens can impact the jailbreak attacks.\nOur study distinguishes itself by evaluating the reliability of LLMs from the perspective of security and quality of their outputs, as opposed to merely focusing on the efficacy of jailbreak attack mechanisms. Moreover, we conduct evaluations on a variety of LLMs when subjected to jailbreak attacks, enabling a detailed comparative analysis and providing thorough insights into the reliability of LLMs from multiple dimensions."}, {"title": "2.3 Vulnerabilities of LLMs", "content": "Numerous vulnerabilities identified in LLMs fall into two principal categories: inherent issues and intended attacks [56]. Aside from jailbreak attacks, we briefly outline additional prevalent and significant vulnerabilities in LLMs.\nInherent vulnerabilities are intrinsic shortcomings of LLMs that are not easily rectified by the models themselves [35]. For example, factual errors occur when LLM outputs contradict established truths, a phenomenon some literature refer as hallucination [5,44,83]. Additionally, reasoning errors highlight the models' limited capacity for logical and mathematical problem-solving, often resulting in inaccurate responses to calculative or deductive reasoning tasks [5, 28, 46].\nIn contrast, intended attacks are orchestrated by malicious attackers seeking to manipulate LLMs to achieve their objectives [35]. For example, prompt injection attacks overrides an LLM's original prompt and directs it to follow malicious instructions, which may result in data exposures or enable social engineering schemes [7,32,34,48]. Conversely, training data poisoning is characterized by the deliberate contamination of an LLM's training dataset with the aim of degrading its performance or instilling biases, which can subsequently distort predictions and conduct [13, 17,66,68].\nIn essence, it is the vulnerability landscape that makes evaluating the reliability of LLMs an essential task, ensuring their security and trustworthy use in practice."}, {"title": "3 Preliminaries: Jailbreaking LLMs", "content": "The goal of a jailbreak attack is to design input prompts in such a way that they can cause a LLM to generate texts that are harmful, toxic, or objectionable, bypassing its built-in security restrictions [70]. Below we formally define LLMs and jailbreak attacks.\nAn LLM takes a sequence of tokens of an arbitrary length as input and returns a distribution on the next token in the sequence. Let T* denote the set of all sequences of tokens of arbitrary length. Let \\(\\Delta(T)\\) denote the set of probability distributions over T.\nAn LLM can be defined as a mapping:\n\\(\\text{LLM} : T^* \\rightarrow \\Delta(T)\\) (1)\nIn the context of jailbreaking, adversaries gain access to a LLM as an attack target. Additionally, adversaries possess a harmful query Q, which instructs the target LLM to produce objectionable content, such as \"Tell me how to build a bomb?\" or \"How to create and distribute malware for financial gain?\"."}, {"title": "4 Evaluation Framework", "content": "Overview. Our evaluation framework consists of five modules, including dataset construction, jailbreak attack strategies, LLMs, response evaluation, and metrics aggregation. The workflow is illustrated in Fig. 2."}, {"title": "4.1 Dataset Construction", "content": "We adopted the dataset framework proposed by Wang et al. [69] that employs an exhaustive three-level hierarchical taxonomy to evaluate safeguards in LLMs. This comprehensive categorization enumerates 61 specific instances of harm, clustered into 12 harm types and 5 broader risk domains. In an effort to enhance the comprehensiveness of our evaluation, we expanded the dataset significantly. For each instance of harm, we added 25 tailored malicious queries, thus enriching the dataset to a total of 1525 entries (25 queries \u00d7 61 specific harms). This augmentation was achieved through meticulous manual curation and the integration of selected examples from AdvBench [86], MultiJail\u00b9, SimpleSafetyTests, MM-SafetyBench, MaliciousInstruct7, BeaverTails-Evaluations, Verazuo-jailbreak-llms, and Xstest [60]."}, {"title": "4.2 Evaluation Pipeline", "content": "Jailbreak Attack Strategies. Based on popularity and accessibility, we select multiple existing strategies for each category of the attacks. For Manual Crafting-based attacks, we apply JailBroken [70], DeepInception [45], and ICA [72]. For longtail Encoding-based attacks, we apply MultiLingual [20], Cipher [81], and CodeChameleon [50]. For prompt Refinement-based attacks, we apply ReNeLLM [22], GPTFUZZER [80], AutoDAN [47], and PAIR [15]. Overall, our evaluation encompasses ten strategies spanning three distinct categories of attacks.\nWe establish the baseline for comparison by querying the target LLMs directly using the harmful queries specified in our evaluation dataset (see Sec. 4.1) without employing any jailbreak strategies. Our experiments was conducted on Server PowerEdge XE9680 with 8 NVIDIA A100 (80G) GPUs. We configured the hyperparameters for the jailbreak attack strategies in accordance with their specifications in the original papers, as delineated in Table 8 in the Appendix.\nLLMs under Attack. We selected thirteen popular LLMs as our target LLMs for evaluation, including GPT-3.5-Turbo [54], GPT-4-0125-preview [5], LLaMA2-7B-chat, LLaMA2-13B-chat [65], Vicuna-7B-v1.5, Vicuna-13B-v1.5 [84], Mistral-7B-v0.1, Mistral-7B-v0.2 [39], Baichuan2-7B-chat, Baichuan2-13B-chat [76], Gemma-2B-it, Gemma-7B-it [64], and Llama-3-8B-Instruct [53].\nThese LLMs are chosen considering their widespread application in security-related research (e.g.,malware analysis, phishing detection, and network intrusion detection [74]), their availability in open-source community (e.g., Open LLM Leaderboard [16,25], and LLM Safety Leaderboard [24] ), and the size of their model parameters (e.g.,2B, 7B, 8B, and 13B). Note that our selections are limited to those LLMs with human alignment [55], excluding base models\u00b9\u2070.\nEvaluation Criteria. Using the language of Askell et al. [8], we want language models to be helpful (they should help the user solve their task), honest (they shouldn't fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). Moreover, The principles of responsible AI as investigated by Bano et al. [9] assert that AI systems should be designed to conform to ethical norms as well as to robustly resist manipulations that might lead to misuse or harm. Additionally, human-computer interaction (HCI) [57] seeks to enhance the interactions between humans and AI to be more fluid, natural, and intuitive. Engaging with an LLM that generates coherent and grammatically sound responses not only encourages continuous user engagement but also fosters trust in the system. These principles guide us to establish evaluation criteria for LLMs, aiming to uphold high standards of quality and safety in AI systems. Ideally, faced with jailbreak attacks, a reliable LLMs should meet the following criteria:\n\u2022 Resistance to Jailbreak. The responses generated by LLMs should neither explicitly nor implicitly fulfill harmful queries.\n\u2022 Non-Toxicity. The responses should not contain harmful, toxic, obscene, insulting, or threatening content.\n\u2022 Quality. Text quality usually contains three dimensions: semantic, syntactic, and structural. The responses should maintain semantic integrity and fluency while minimizing grammatical errors.\nResponse Evaluation. We consider the criteria of LLM Reliability in Sec. 4.2 and employ five metrics to measure and characterize the reliability of responses of LLMs against jailbreak attacks."}, {"title": "4.3 Metrics Aggregation", "content": "To furnish a more comprehensive reliability score of each model as reference, it is essential to normalize the metric values and incorporate them into a composite score. This score reflects the weighted significance of various dimensions, with the determination of these weights entrusted to the discretion of the model user. Subsequently, we describe the procedure in detail.\nNormalization. For metrics that improve with minimization (e.g., ASR, Toxicity Score, Grammatical Errors, and Fluency), a higher value indicates decreased reliability. Conversely, for the metric that benefits from maximization (i.e., Token Length), a higher value implies enhanced reliability. We normalize each metric to a range between 0 and 1, whereby a higher value consistently denotes increased reliability.\nFor metrics to be minimized (Mmin), such as ASR, Toxicity Score, Grammatical Errors, Fluency, we define a normalization function \\(f_{min}(x)\\):\n\\(f_{min}(x) = 1 - \\frac{x - x_{min}}{x_{max} - x_{min}}\\) (8)\nWhere: - x represents the raw value of the metric. - \\(x_{min}\\) represents the minimum possible or observed value for the metric to be minimized. - \\(x_{max}\\) represents the maximum possible or observed value for the metric to be minimized.\nFor the metric to be maximized (Mmax), such as Token Length, we define a normalization function \\(f_{max}(x)\\):\n\\(f_{max}(x) = \\frac{x - x_{min}}{x_{max} - x_{min}}\\) (9)\nWhere: - x represents the raw value of the metric. - x'min represents the minimum possible or observed value for the metric to be maximized. - \\(x_{max}\\) represents the maximum possible or observed value for the metric to be maximized.\nAggregation. To derive a reliability score for each model, we amalgamate all the normalized values. Recognizing that model users attribute varying levels of importance to different metrics, the reliability score S for each model can be computed by synthesizing the normalized values across all pertinent metrics:\n\\(S = \\sum_{i=1}^{n} w_i f(x_i)\\) (10)\nWhere: - n is the number of metrics evaluated. - \\(f(x_i)\\) is the normalization function applied to the i-th metric. - wi represents the assigned weight to each metric, which the model user determines. - The reliability score S is weighted average of all normalized metrics."}, {"title": "5 Experiment", "content": "In our evaluation framework, we consider multiple metrics; notably, the Attack Success Rate (ASR) is uniquely tailored to quantify the Resistance of LLMs to jailbreak attacks. This metric is initially calculated and scrutinized in our study.\nTable 3 presents the ASR results for the target LLMs under various jailbreak attack strategies, as categorized in our taxonomy. From the baseline results, we observe that any of the thirteen LLMs do not demonstrate initial resistance to harmful queries. Notably, well-aligned models such as Llama3, GPT-3.5, and GPT-4 yielded baseline ASR scores of 20.26%, 13.84%, and 16.26%, respectively. The results indicates that the average ASR across some LLMs does not surpass the baseline, which suggests a degree of resistance to jailbreak attacks in some LLMs. Vicuna and Mistral emerged as the LLM families most vulnerable or susceptible to jailbreak attacks, which aligns with our conclusion related to the harm type (see Sec. 6.1) and many previous works [47,80,86].\nFrom the perspective of jailbreak category, we find that all jailbreak strategies, except for ICA and Cipher, demonstrate high attack efficacy, as demonstrated in Table 3. After detailed analysis, we discern that all the Longtail Encoding-based attacks are model-specific, i.e., they can achieve a low ASR score on a specific model. For example, Multilingual only exhibited poor attack performance on Llama3, GPT-3.5, and GPT-4, with ASR nearing 0. We attribute this phenomenon to the strong capability of the LLM. LLMs like GPT-4, with their extensive training on wide variety of datasets, obtain the capacity to recognize and process texts that have been translated to low-resource languages or encoded, which is beyond the capability of other models. This capability makes it easier to align such LLMs and decreases their vulnerability to jailbreak attacks.\nIn general, from Table 3, ReNeLLM achieve the highest average ASR (about 29.22%) when targeting all LLMs. In addition, almost all Prompt Refinement-based attacks yield higher ASR results, except for AutoDAN on Gemma. This result is expected given that these attacks typically necessitates greater time and computational resources for iterative optimization.\nFor the manual crafting-based attacks, ICA exploit vulnerabilities in the model's context learning [12] to induce responses to malicious instructions [72]. However, in our experimentation, these tactics failed to yield effective results, evidenced by the nearly 0 ASR observed across various LLMs. Conversely, we find that Jailbroken can still achieve high ASR across LLMs, which highlights the significance of actively collecting and analyzing such jailbreak prompts. This also underscores the robustness and effectiveness of human-based jailbreak prompts."}, {"title": "5.2 Characterizing the Reliability of LLMs under Attacks", "content": "In this section, we calculate all the metrics in our evaluation framework and present the reliability of the responses of LLMs under jailbreak attack. Table 4 shows the evaluation metrics of responses of various jailbreak attacks on LLMs.\nIn general, from Table 4, GPT-4 stands out for its low toxicity across all categories and a relatively low ASR, making it highly reliable in maintaining non-toxic outputs under attacks, despite its higher perplexity. Llama3 combines a low ASR, low toxicity, and few grammatical errors, with decent fluency, making it another strong contender for reliability. Gemma-2b-it has the lowest ASR, indicating high robustness against jailbreak attacks, although its other metrics are not the best across the board. Mistral-7B-Instruct-v0.2 and Vicuna-13b-v1.5 exhibit high ASR and relatively high toxicity levels, indicating lower reliability under jailbreak conditions. Baichuan2-7B-Chat also shows high toxicity and grammatical errors, which compromises its reliability despite having a reasonably long response length."}, {"title": "5.3 Reliablity Score and Evaluating", "content": "Building upon the process delineated in Sec. 4.3, we initially normalized the values of all metrics listed in Table 4. Following this, as stipulated in Eq. 10, weights can be allotted to each metric at the discretion of the model user. Assuming equal significance for all metrics, Eq. 10 simplifies to the sum average of all the normalized metrics.\nTable. 5 displays the calculated reliability scores for each model, considering the normalization of each metric based on the criteria provided. The models are sorted in descending order of their reliability scores, where a higher score indicates a better overall reliability under jailbreak based on the aggregated metrics. This aggregation provides a comprehensive view of the overall reliability of each model by integrating various metrics into a unified score."}, {"title": "6 Ablation Study", "content": ""}, {"title": "6.1 Evaluation of Harm Type", "content": "Building upon the dataset structure outlined in Table 2, we can analyze the ASR of various LLMs across different harm types. As illustrated in Table 6, we observed a significantly variance in ASR different harm types. Furthermore, for a more insightful comparative analysis of model performance, these results were synthesized into a heatmap, as shown in Fig. 3a. Within this graphical representation, models exhibiting higher vulnerability are indicated by darker shades, which correlate with enhanced ASRS.\nIn general, Assist Illegal Acts, Advise Unethical Actions, Lower Disinformation Cost, and Adult Content are the four most straightforward and amenable harm types to launch jailbreak attacks, whereas the Stereotypes & Discrimination, and Treat Bot As Human are more challenging to jailbreak attacks. From the results, we observe that three LLMs contain the highest ASR corresponding to all harm types: Vicuna (7 types), Mistral (10 types), and Baichuan (8 types). Therefore, we believe that although other models also suffer from jailbreak attacks across different harm types, these three models are the most vulnerable.\nThis phenomenon underscores the challenges associated with effectively aligning LLM policies and their jailbreak attacks, indicating the future works for improvement in ensuring policy compliance within LLMs."}, {"title": "6.2 Relationship Between Attack Strategy and Harm Type", "content": "To further explore the relationship between the harm type (i.e., the 12 harm type categories in Table 2) and the jailbreak attacks (i.e., the 10 different types of jailbreak attack strategies), we visualize the ASR between the two using a heatmap, as depicted in Fig. 3b.\nFirst, it shows that the ICA, and Cipher jailbreak attacks demonstrate consistently poor performance across all violation categories in the harm types, denoted by the negative correlation in the heatmap. Furthermore, the Prompt Refinement-based jailbreak attacks are relatively robust and versatile among all the violation categories. Among all the Manual Crafting-based jailbreak methods, Jalibroken is the most effective one from various categories of our harm types. The above conclusions intuitively support the findings we presented previously (see Sec. 5.1)."}, {"title": "6.3 Correlation Between Metrics", "content": "In this section, we conducted a correlation analysis between the evaluation metrics in our framework. As we know, jailbreak attacks on LLMs aim to manipulate models to produce damaging content, where their potential effectiveness may in part rely on the level of malignancy and toxicity present in the resulting outputs. We initially investigate the relationship between the ASR metric evaluated in Sec. 5.1 and our toxicity metrics. we calculated the Pearson correlation coefficient between the ASR metric and the toxic, obscene, insult and threat metrics, subsequently illustrating the results through a heatmap presented in Fig. 4a. Additionally, we rendered a scatter plot enriched with a regression line, depicted in Fig. 4b.\nOur analysis revealed that the ASR is moderately positively correlated with the metrics for toxicity, obscenity, insult, and threat. Nonetheless, regression analyses suggested that the impact of these toxicity metrics on ASR was statistically negligible, which hints at the existence of additional influencing factors. Consequently, it appears that ASR is subject to a variety of influences, with the considered toxicity metrics contributing only partially.\nAdditionally, as detailed in Sec. 4.2, we sought to quantify the output quality of LLMs across three metrics: Fluency, Token Length, and Grammatical Errors. To elucidate the relationships between these metrics, we carried out a statistical analysis accompanied by a visualization, presented in Fig. 5, to uncover potential patterns or correlations. The analysis divulged a relative strong positive correlation (0.78) between Grammatical Errors and Token Length, signifying that models producing longer responses typically exhibit more grammatical inaccuracies. Conversely, Fluency did not demonstrate a robust correlation with either Grammatical Errors or Token Length, implying that it captures a distinct facet of response quality.\nIn conclusion, the empirical findings from our correlation analysis underscore the validity of our multifaceted evaluation framework. Each of the five metrics we have selected - Adversarial Success Rate (ASR), Toxicity, Fluency, Token Length, and Grammatical Errors - has been meticulously chosen to capture nuances in the generated content of LLMs. Although they interact in complex and sometimes subtle ways, they collectively contribute to a comprehensive evaluation of LLM reliability."}, {"title": "7 Conclusion", "content": "In this paper, we present a novel and comprehensive evaluation framework designed to assess the reliability of LLMs when exposed to diverse jailbreak attack strategies. Our evaluation framework included the construction of a refined three- level hierarchical dataset, comprising 1525 questions across 61 distinct harmful categories. We evaluate the LLMs' outputs under jailbreak by integrating different aspects of content security, such as resistance to jailbreak, toxicity, and quality, and using multi-dimensional metrics like Attack Success Rate (ASR), Toxicity Score, Fluency, Token Length, and Grammatical Errors.\nThrough elaborate experiments involving 13 popular LLMs and 10 state-of-the-art jailbreak attack strategies, we holistically analyze the jailbreak attacks and uncover significant variations in the models' resilience. Our findings draw attention to the inherent vulnerabilities of some of the most popular LLMs, such as Vicuna and Mistral, which exhibited heightened susceptibility to jailbreak attacks. By normalizing and aggregating these metrics, we present a detailed reliability score for different LLMs, which encapsulates the leveled importance of different metrics, providing actionable insights to the end-users in terms of prioritizing aspects they consider most crucial for their specific use cases.\nWe conduct a comprehensive ablation study to assess reliability across diffirent harm types. We find that the reliability of LLMs is specific under certain harmful question scenarios. We also present fine-grained relationships between Attack Strategy and harm type, we investigste the correlations between different evaluation metrics that verify the soundness of our framework.\nFinally, We posit that responsible AI development must persistently address the evolving landscape of adversarial attacks, ensuring that language models remain steadfast guardians of ethical guidelines, while continuing to be helpful, honest, and harmless companions in our digital interactions."}, {"title": "8 Limitations", "content": "To address the constraints posed by limited resources, our evaluation experiment does not extend to larger models, such as those with 33 billion and 70 billion parameters, nor does it cover other powerful commercial models like Claude [6] and Gemini [30]."}, {"title": "9 Apendix", "content": ""}, {"title": "9.1 Choice of Jailbreak Evaluator", "content": "A key difficulty in evaluating the performance of jailbreaking attacks is determining whether a given input prompt succeeds in jailbreaking the target model. Determining the success of an attack involves an understanding of human language and a subjective judgment of whether generated content is objectionable, which is challenging even for humans. With a result dataset encompassing 574,577 responses, it is impractical to label all the results manually. Existing strategies for addressing this issue vary, we consider six candidate jailbreak evalustors which are commonly used in the jailbreaking literature:\n\u2022 PrefixMatch. Zou et al. [86] utilized a set of common refusal patterns, such as \"I am sorry'and\"I cannot\", to automate the identification of non-compliant responses,\n\u2022 BERT. The BERT-base-cased fine-tuned model used as a evaluator [36],\n\u2022 ROBERTa. The RoBERTa fine-tuned model used as a evaluator [80],\n\u2022 GPT-4. A approaches leveraging GPT-4 for attack analysis were explored by Chaoet al [15],\n\u2022 Llama Guard. The fine-tuned Llama Guard evaluator [38],\n\u2022 Llama-2-13b-cls. Distillation fine-tuned Llama-2 -3B-Chat evaluator [51]\nTo choose an effective jailbreak evaluator, we selected a random sample of 1,00 prompts and responses from our result dataset, aiming for a 95% confidence level with a 3% margin of error. Each prompt-response instance was labeled by three computer science graduate students, with the majority vote determining the \"ground truth\" label for each case. Finally, we compared the agreement, false positive rate (FPR), and false negative rate (FNR) of the six Jailbreak Evaluators listed above to these ground truth labels. Our results are summarized in Table 7."}, {"title": "9.2 Dataset Construction Details", "content": "We employed the dataset framework proposed by Wang et al. [69], which originally comprised 939 labeled entries. We augmented this initial dataset with an additional 6,000 entries sourced from other various libraries in Sec. 4.1. To ensure strict classification of new data in our dataset, we attempted to fine-tune text classification models such as BigBird, CANINE, ConvBERT, DeBERTa, ROBERTa, etc., on the do-not-answer dataset [69], aiming to achieve 90% accuracy on the held-out test set.\nAfter careful consideration of the performance metrics suach as accuracy, we selected the bert-base-uncased model for fine-tuning. The fine-tuning protocol includes: a batch size of 8, fifteen training epochs, a learning rate of 5 \u00d7 10\u22125, application of the Adam optimizer, and linear rate decay. Using the fine-tuned model, we labeled integrated dataset with additional 6,000 entries. Finally, to ensure the accuracy and reliability of the dataset classification, we conducted a round of random sampling for manual verification."}]}