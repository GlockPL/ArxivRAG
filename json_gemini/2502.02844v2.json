{"title": "Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning", "authors": ["Sunwoo Lee", "Jaebak Hwang", "Yonghyeon Jo", "Seungyul Han"], "abstract": "Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering system-wide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL.", "sections": [{"title": "1. Introduction", "content": "Multi-agent Reinforcement Learning (MARL) has gained attention for solving complex problems requiring agent cooperation (Oroojlooy & Hajinezhad, 2023) and competition, such as drone control (Yun et al., 2022), autonomous navigation (Chen et al., 2023), robotics (Orr & Dutta, 2023), and energy management (Jendoubi & Bouffard, 2023). To handle partially observable environments, the Centralized Training and Decentralized Execution (CTDE) framework (Oliehoek et al., 2008) trains a global value function centrally while agents execute policies based on local observations. Notable credit-assignment methods in CTDE include Value Decomposition Networks (VDN) (Sunehag et al., 2017), QMIX (Rashid et al., 2020), which satisfies the Individual-Global-Max (IGM) condition ensuring that optimal joint actions align with positive gradients in global and individual value functions, and QPLEX (Wang et al., 2020b), which encodes IGM into its architecture. However, CTDE methods face challenges from exploration inefficiencies (Mahajan et al., 2019; Jo et al., 2024) and mismatches between training and deployment environments, leading to unexpected agent behaviors and degraded performance (Moos et al., 2022; Guo et al., 2022). Thus, enhancing the robustness of CTDE remains a critical research focus.\nTo improve learning robustness, single-agent RL methods have explored strategies based on game theory (Yu et al., 2021), such as max-min approaches and adversarial learning (Goodfellow et al., 2014; Huang et al., 2017; Pattanaik et al., 2017; Pinto et al., 2017). In multi-agent systems, simultaneous agent interactions introduce additional uncertainties (Zhang et al., 2021b). To address this, methods like perturbing local observations (Lin et al., 2020), training with adversarial policies for Nash equilibrium (Li et al., 2023a), adversarial value decomposition (Phan et al., 2021), and attacking inter-agent communication (Xue et al., 2021) have been proposed. However, these approaches often target a single agent per attack, overlooking interdependencies in cooperative MARL, making them vulnerable to scenarios where multiple agents are attacked simultaneously.\nTo overcome the vulnerabilities posed by coordinated adversarial attacks in MARL, we propose the Wolfpack adversarial attack framework, inspired by wolf hunting strategies. This approach disrupts inter-agent cooperation by targeting a single agent and subsequently attacking the group of agents assisting the initially targeted agent, resulting in more devastating impacts. Experimental results reveal that traditional robust MARL methods are highly susceptible to such coordinated attacks, underscoring the need for new defense mechanisms. In response, we also introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, a robust policy training approach specifically designed to counter the Wolfpack Adversarial Attack. By fostering system-wide collaboration and avoiding reliance on specific agent subsets, WALL enables agents to defend effectively against coordinated attacks. Experimental evaluations demonstrate that WALL significantly improves robustness compared to existing methods while maintaining high performance under a wide range of adversarial attack scenarios.\nThe key contributions of this paper in constructing the Wolfpack Adversarial Attack are summarized as follows:\n\u2022 A novel MARL attack strategy, Wolfpack Adversarial Attack, is introduced, targeting multiple agents simultaneously to foster stronger and more resilient agent"}, {"title": "2. Related Works", "content": "Robust MARL Strategies: Recent research has focused on robust MARL to address unexpected changes in multi-agent environments. Max-min optimization (Chinchuluun et al., 2008; Han & Sung, 2021) has been applied to traditional MARL algorithms for robust learning (Li et al., 2019; Wang et al., 2022). Robust Nash equilibrium has been redefined to better suit multi-agent systems (Zhang et al., 2020b; Li et al., 2023a). Regularization-based approaches have also been explored to improve MARL robustness (Lin et al., 2020; Li et al., 2023b; Wang et al., 2023; Bukharin et al., 2024), alongside distributional reinforcement learning methods to manage uncertainties (Li et al., 2020; Xu et al., 2021; Du et al., 2024; Geng et al., 2024).\nAdversarial Attacks for Resilient RL: To strengthen RL, numerous studies have explored adversarial learning to train policies under worst-case scenarios (Pattanaik et al., 2017; Tessler et al., 2019; Pinto et al., 2017; Chae et al., 2022). These attacks introduce perturbations to various MDP components, including state (Zhang et al., 2020a; 2021a; Everett et al., 2021; Li et al., 2023c; Qiaoben et al., 2024), action (Tan et al., 2020; Lee et al., 2021; Liu et al., 2024), and reward (Wang et al., 2020a; Zhang et al., 2020c; Rakhsha et al., 2021; Xu et al., 2022; Cai et al., 2023; Bouhaddi & Adi, 2023; Xu et al., 2024; Bouhaddi & Adi, 2024). Adversarial attacks have recently been extended to multi-agent setups, introducing uncertainties to state or observation (Han et al., 2022; He et al., 2023; Zhang et al., 2023; Zhou et al., 2023), actions (Yuan et al., 2023), and rewards (Karde\u015f et al., 2011). Further research has applied adversarial attacks to value decomposition frameworks (Phan et al., 2021), selected critical agents for targeted attacks (Yuan et al., 2023; Zhou et al., 2024), and analyzed their effects on inter-agent communication (Xue et al., 2021; Tu et al., 2021; Sun et al., 2023; Yuan et al., 2024).\nModel-based Frameworks for Robust RL: Model-based methods have been extensively studied to enhance RL robustness (Berkenkamp et al., 2017; Panaganti & Kalathil, 2021; Curi et al., 2021; Clavier et al., 2023; Shi & Chi, 2024; Ramesh et al., 2024), including adversarial extensions (Wang et al., 2020c; Kobayashi, 2024). Transition models"}, {"title": "3. Background", "content": "3.1. Dec-POMDP and Value-based CTDE Setup\nA fully cooperative multi-agent environment is modeled as a decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek et al., 2016), defined by the tuple \\(M = (N, S, A, P, \\Omega, O, R, \\gamma)\\). \\(N = 1, . . ., n\\) is the set of agents, \\(S\\) the global state space, \\(A = A^1 \\times \u00b7\u00b7\u00b7 \\times A^n\\) the joint action space, \\(P\\) the state transition probability, \\(\\Omega\\) the observation space, \\(R\\) the reward function, and \\(\\gamma\\in [0, 1)\\) the discount factor. At time \\(t\\), each agent \\(i\\) observes \\(o_t^i = O(s_t, i) \\in \\Omega\\) and takes action \\(a_t^i \\in A\\) based on its individual policy \\(\\pi^i(\\cdot|\\tau_t^i)\\), where \\(\\tau^i_t\\) is the agent's trajectory up to \\(t\\). The joint action \\(a_t = (a_t^1, ..., a_t^n)\\) sampled from the joint policy \\(\\pi := (\\pi^1,......, \\pi^n)\\) leads to the next state \\(s_{t+1} ~ P(\\cdot|s_t, a_t)\\) and reward \\(r_t := R(s_t,a_t)\\). MARL aims to find the optimal joint policy that maximizes \\(\\sum_{t=0}^\\infty \\gamma^t r_t\\). As noted, this paper adopts the centralized training with decentralized execution (CTDE) setup, where the joint value \\(Q^{tot}(s_t, a_t)\\) is learned using temporal-difference (TD) learning. Through credit assignment, individual value functions \\(Q^i(\\tau_t^i, a^i)\\) are learned, guiding individual policies \\(\\pi^i\\) to select actions that maximize \\(Q^i\\), i.e., \\(\\pi^i := arg \\, max_{a \\in A} Q^i (\\tau^i, \\cdot)\\).\n3.2. Robust MARL with Adversarial Attack Policy\nAmong various methods for robust learning in MARL, Yuan et al. (2023) defined an adversarial attack policy \\(\\pi^{adv}\\) and implemented robust MARL by training multi-agent policies to defend against attacks executed by \\(\\pi^{adv} : S\\times A\\times N \\rightarrow A\\). A cooperative MARL environment with an adversarial policy \\(\\pi^{adv}\\) can be described as a Limited Policy Adversary Dec-POMDP (LPA-Dec-POMDP) \\(M\\), defined as follows:\nDefinition 3.1 (Limited Policy Adversary Dec-POMDP). Given a Dec-POMDP \\(M\\) and a fixed adversarial policy \\(\\pi^{adv}\\), we define a Limited Policy Adversary Dec-POMDP (LPA-Dec-POMDP) \\(M = (N, S, A, P, K, \\Omega, O, R, \\gamma)\\), where \\(K\\) is the maximum number of attacks, \\(\\pi^{adv} (S_t, a_t, k_t)\\) executes joint action \\(a_t\\) to disrupt the original action \\(a_t\\) chosen by \\(\\pi\\), and \\(k_t < K\\) indicates the number of remaining attacks.\nHere, if \\(\\pi^{adv}\\) selects an attack action \\(a_t'\\) different from the original action \\(a_t\\), the remaining number of attacks \\(k_t\\) decreases by 1. Once \\(k_t\\) reaches 0, no further attacks can be"}, {"title": "4. Methodology", "content": "4.1. Motivation of Wolfpack Attack Strategy\nExisting adversarial attackers typically target only a single agent per attack, without coordination or relationships between successive attacks. In a cooperative MARL setup, such simplistic attacks enable non-targeted agents to learn effective policies to counteract the attack. However, we observe that policies trained under these conditions are vulnerable to coordinated attacks. As illustrated in Fig. 1(a), a single agent is attacked at time t. In Fig. 1(b), at the next step t + 1, responding agents adjust their actions, such as healing or moving to guard, to protect the initially attacked agent. In contrast, Fig. 1(c) demonstrates a coordinated attack strategy that targets the agents responding to the initial attack. Such coordinated attacks render the learned policy ineffective, preventing it from countering the attacks entirely. This highlights that coordinated attacks are far more detrimental than existing attack methods, and current robust policies fail to defend effectively against them.\nAs depicted in Fig. 1(c), targeting agents that respond to an initial attack aligns with the Wolfpack attack strategy, a tactic widely employed in traditional military operations, as discussed in Section 1. To adapt this concept to a cooperative multi-agent setup, we define a Wolfpack adversarial attack as a coordinated strategy where one agent is attacked initially, followed by targeting the group of follow-up agents that respond to defend against the initial attack, as shown in Fig. 1(c). Leveraging this approach, we aim to develop robust policies capable of effectively countering Wolfpack adversarial attack, thereby significantly enhancing the overall resilience of the learning process.\n4.2. Wolfpack Adversarial Attack\nIn this section, we formally propose the Wolfpack adversarial attack, as introduced in the previous sections. The Wolfpack attack consists of two components: initial attacks, where a single agent is targeted at a specific time step \\(t_{init}\\), and follow-up group attacks, where the group of agents responding to the initial attack is selected and targeted over the subsequent steps \\(t_{init} + 1,..., t_{init} + t_{WP}\\). Over the course of an episode, a maximum of \\(K_{wp}\\) Wolfpack attacks can be executed. Consequently, the total number of attack steps is given by \\(K = K_{WP} \\times (t_{wp} + 1)\\). The Wolfpack adversarial attacker \\(\\pi_{adv}^{WP}\\) can then be defined as follows:\nDefinition 4.1 (Wolfpack Adversarial Attacker). A Wolfpack adversarial attacker \\(\\pi_{adv}^{WP} : S\\times A\\times N \\rightarrow A\\) is defined as \\(a'_t = \\pi_{adv}^{WP}(S_t, a_t, k_t)\\), where \\(a^i = arg \\, min_{a \\in A^i} Q^{tot} (s_t, a^i, a^{-i})\\) for all \\(i \\in N_{t,attack}\\), and \\(a^i = a^i\\) otherwise. Here, \\(a^{-i}\\) represents the joint actions of all agents excluding the i-th agent, and \\(N_{t,attack}\\) denotes the set of agents targeted for adversarial attack, defined as\n\n\\begin{equation}\nN_{t,attack} = \\begin{cases}\n\\emptyset & \\text{if } k_t = 0,\\\\\n\\{i\\} & \\text{else if } t = t_{init}, i \\sim Unif(N),\\\\\nN_{follow-up} & \\text{else if } t = t_{init} + 1,..., t_{init} + t_{WP},\\\\\n\\emptyset & \\text{otherwise},\n\\end{cases}\n\\end{equation}\n\nwhere Unif(.) is the Uniform distribution, \\(N_{follow-up} := \\{i_1,...,i_m\\} \\subset N\\) is the group of agents selected for follow-up attack, and m is the number of follow-up agents.\nHere, note that \\(k_t\\) decreases by 1 for every attack step such that \\(a_t' \\ne a_t\\), as in the ordinary adversarial attack policy, and the total value function \\(Q^{tot}\\) is used for the attack instead of \\(Q^i\\). The proposed Wolfpack adversarial attacker \\(\\pi_{adv}^{WP}\\) is a special case of the adversarial policy defined in Definition 3.1. Consequently, the proposed attacker forms an LPA-Dec-POMDP \\(M\\) induced by \\(\\pi_{adv}^{WP}\\), and as demonstrated in Yuan et al. (2023), the convergence of MARL within the LPA-Dec-POMDP can be guaranteed. The proposed Wolfpack attack involves two key issues: how to design the group of follow-up agents \\(N_{follow-up}\\) and when to select \\(t_{init}\\). The following sections address these aspects in detail.\n4.3. Follow-up Agent Group Selection Method\nIn the Wolfpack adversarial attacker, we aim to identify the follow-up agent group \\(N_{follow-up}\\) that actively responds to the initial attack \\(\\pi_{adv} (S_{tinit}, a_{tinit}, k_{tinit})\\) and target them in subsequent steps. To do this, we define the difference between the Q-functions from the original action and the initial attack at time t as:\n\\(\\Delta Q_t^{tot} = Q^{tot}(s_t, a_t) - Q^{tot}(s_t, \\bar{a}_t),\\)\nwhere \\(\\Delta Q^{tot} \\ge 0\\) for all \\(t\\) such that \\(N_{t,attack} \\ne \\emptyset\\), because \\(\\bar{a}_t\\) minimizes \\(Q^{tot}\\) for the agent indices selected by \\(\\pi_{adv}^{WP}\\). Assuming the i-th agent is the target of the initial attack, updating \\(Q^{tot}\\) based on \\(\\Delta Q^{tot}\\) adjusts each agent's individual value function \\(Q^i\\) to increase \\(Q^{tot}\\) for all \\(j \\ne i\\in N\\), in accordance with the credit assignment principle in CTDE algorithms (Sunehag et al., 2017; Rashid et al., 2020), as shown below:\n\\(Q_i(\\tau_{tinit}, a_{tinit}) = Q_i(\\tau_{tinit}) - \\alpha_{lr} \\Delta Q^{tot} \\frac{\\partial Q^{tot}(\\tau_{tinit}, a_{tinit})}{\\partial Q_i(\\tau_{tinit}, a_{tinit})},\\) (1)\nwhere \\(\\alpha_{lr}\\) is the learning rate. As agents select actions based on \\(Q\\), changes in \\(Q^i\\) indicate adjustments in their policies in response to the initial attack. Agents with the largest changes in \\(Q^i\\) are identified as follow-up agents, while the i-th agent is excluded as it is already under attack and cannot respond immediately.\nTo identify the follow-up agent group, the updated \\(Q\\) and original \\(Q\\) are transformed into distributions using the Softmax function Soft(.). This transformation softens the deterministic policy \\(\\pi^j\\), which directly selects an action to maximize \\(Q^j\\), making distributional differences easier to compute. The follow-up agent group is determined by selecting the m agents that maximize the Kullback-Leibler (KL) divergence \\(D_{KL}\\) between these distributions:\n\\(N_{follow-up} = arg\\, max_{\\substack{N'\\subset N,\\\\ |N'|=m,j\\in N',j\\neq i}} \\sum_{j\\in N'} D_{KL}(Soft(Q^{j}(\\tau_{tinit}))||Soft(\\bar{Q}^{j}(\\tau_{tinit}))).\\) (2)\nUsing the proposed method, the follow-up agent group is identified as the agents whose policy distributions experience the most significant changes following the initial\n4.4. Planner-based Critical Attacking Step Selection\nIn the proposed Wolfpack adversarial attacker \\(\\pi_{adv}^{WP}\\), the follow-up agent group is defined, leaving the task of determining the timing of initial attacks \\(t_{init}\\), executed \\(K_{WP}\\) times within an episode. While Random Step Selection involves choosing time steps randomly, existing methods show that selecting steps to minimize the rewards of the execution policy \\(\\pi\\) leads to more effective attacks and facilitates robust learning (Yuan et al., 2023). However, in coordinated attacks like Wolfpack, targeting steps that cause the greatest reduction in the Q-function value \\(\\Delta Q^{WP}\\) ensures a more devastating and lasting impact on the agents' ability to recover and respond. Thus, we propose selecting initial attack times based on the total reduction in \\(\\Delta Q^{WP}\\), defined as:\n\\(\\Delta Q_t^{WP} = \\sum_{l=t}^{t+t_{WP}} Q^{tot}_l,\\)\nwhere the Wolfpack attack is performed from \\(t\\) (initial attack) to \\(t + 1,...,t + t_{WP}\\) (follow-up attacks). Initial attack time steps \\(t_{init}\\) are chosen to maximize \\(\\Delta Q^{WP}\\), which captures the total Q-value reduction caused by the attack over \\(t_{WP} + 1\\) steps, enhancing the criticalness of the attack. However, computing \\(\\Delta Q^{WP}\\) for every time step is computationally expensive as it requires generating attacked samples through interactions with the environment. To mitigate this, a stored buffer is utilized to plan trajectories of future states and observations for the attack.\nFor planning, we employ a Transformer (Vaswani, 2017), commonly used in sequential learning, which leverages an attention mechanism for efficient learning. As shown in Fig. 3, the Transformer learns environment dynamics \\(P\\) using replay buffer trajectories to predict future states and observations \\((\\hat{s}_{t+1}, \\hat{o}_{t+1},\u00b7\u00b7\u00b7, \\hat{s}_{t+t_{WP}}, \\hat{o}_{t+t_{WP}})\\), where \\(\\hat{o}_t\\)\n4.5. WALL: A Robust MARL Algorithm\nSimilar to other robust MARL methods, we propose the Wolfpack-Adversarial Learning for MARL (WALL) framework, a robust policy designed to counter the Wolfpack attack by performing MARL on the LPA-Dec-POMDP M with the Wolfpack attacker \\(\\pi_{adv}^{WP}\\). We utilize well-known CTDE algorithms, including VDN (Sunehag et al., 2017), QMIX (Rashid et al., 2020), and QPLEX (Wang et al., 2020b). Detailed implementations, including loss functions\n\\(P_{t,attack} = Soft(\\Delta Q_t^{WP}/\\tau,..., \\Delta Q_{t+L-1}^{WP}/\\tau\\)), (3)\nwhere \\(x_i\\) indicates the i-th element of x, and \\(\\tau > 0\\) is the temperature. In this paper, we set \\(L = 20\\) as it provides an appropriate attack period. After selecting \\(K_{wp}\\) initial attacks, no further attacks are performed. Fig. 4 shows how step probabilities are distributed for different \\(\\tau\\) values (\\(\\tau = 0.1, 1, 10\\)). At each time t, the planner predicts \\(\\Delta Q^{WP}\\) for t to t+L-1, forming soft initial attack probabilities. A larger \\(\\tau\\) results in more uniform probabilities, while a smaller \\(\\tau\\) increases the likelihood of targeting critical steps where \\(\\Delta Q^{WP}\\) is highest. These critical steps are selected with the highest probabilities for initial attacks. In Section 5, we analyze the effectiveness of this method in delivering more critical attacks compared to Random Step Selection and examine the impact of \\(\\tau\\) on performance in practical environments. Since the proposed method involves planning at every evaluation, we also train a separate model to predict \\(\\Delta Q^{WP}\\), significantly reducing computational complexity. Details of this approach and the Transformer training loss functions are provided in Appendix B.1."}, {"title": "5. Experiments", "content": "In this section, we evaluate the proposed methods in the Starcraft II Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) environment, a benchmark in MARL research. Specifically, we compare: (1) the impact of the proposed Wolfpack adversarial attack against other adversarial attacks, and (2) the robustness of the WALL framework in defending against such attacks compared to other robust MARL methods. Also, an ablation study analyzes the effect of the proposed components and hyperparameters on robustness. All results are reported as the mean and standard deviation (shaded areas for graphs and \u00b1 values for tables) across 5 random seeds.\n5.1. Environmental Setup\nThe SMAC environment serves as a challenging benchmark requiring effective agent cooperation to defeat opponents. We evaluate the proposed method across six sce-i = arg min_{i} min_{a_j} Q^{tot} (S_{tinit}, a_{tinit}, a^{-i}), instead of\nComponent Evaluation: To evaluate the impact of each proposed component on attack severity and policy robustness, we consider five setups: 'Default', which uses all proposed components as designed; 'Init. agent (min)', where the initial target agent i is selected to minimize Qtot, i.e.,\n'Random Step Selection (Step (Random)), which uses random step selection instead of the proposed selection method, while keeping the same total number of attacks; and 'Agents & Step (Random)', which randomly selects both m follow-up agents and attack steps.\nFor each setup, we train the Wolfpack adversarial attack\n5.2. Performance Comparison\nTable 1 presents the average win rates over the last 100 episodes of MARL policies against attacker baselines. The results show that the proposed Wolfpack adversarial attack is significantly more powerful than existing methods like EGA"}, {"title": "5.3. Visualization of Wolfpack Adversarial Attack", "content": "To analyze the superior performance of the Wolfpack attack, we provide a visualization of its execution in the SMAC environment. Fig. 7 illustrates a scenario where the proposed step selector identifies t = 6 as a critical initial step to initiate the attack. Prior to t = 6, all setups are assumed to follow the same trajectory. Fig. 7(a) shows Vanilla QMIX in a Natural scenario without attack, where our agents successfully defeat all enemy agents, achieving victory. Fig. 7(b) demonstrates Vanilla QMIX under the Wolfpack adversarial attack, with follow-up agents targeted during t = 7 to t = 9. This leaves other agents unable to effectively defend against the adversarial attack, resulting in defeat"}, {"title": "5.4. Ablation Study", "content": "To evaluate the impact of each component and hyperparameter in the proposed Wolfpack adversarial attack, we conduct an ablation study focusing on the following aspects: component evaluation, step selection temperature T, and the number of follow-up agents m. The ablation study is conducted in the 8m and MMM environments, where the performance differences are most pronounced. Additionally, more ablation studies on other hyperparameters, such as the total number of Wolfpack attacks Kwp and the attack duration twp, are provided in Appendix F."}, {"title": "6. Conclusions", "content": "In this paper, we propose the Wolfpack adversarial attack, a coordinated strategy inspired by the Wolfpack tactic used in military operations, which significantly outperforms existing adversarial attacks. Additionally, we develop WALL, a robust MARL method designed to counter the proposed attack, demonstrating superior performance across various SMAC environments. Overall, our WALL framework enhances the robustness of MARL algorithms."}]}