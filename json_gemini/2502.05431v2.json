{"title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding", "authors": ["Xinyu Yang", "Tianqi Chen", "Beidi Chen"], "abstract": "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (APE), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5\u00d7 speedup by reducing 28\u00d7 prefilling time for a 128K-length context.", "sections": [{"title": "1 Introduction", "content": "Recent advances in context-augmented generation (CAG) techniques, particularly retrieval-augmented generation (RAG) (Gupta et al., 2024; Gao et al., 2023) and in-context learning (ICL) (Dong et al., 2022; Wei et al., 2022), have been widely adopted in large language models (LLMs) (Dubey et al., 2024; Achiam et al., 2023), improving their ability to generalize to unseen tasks with contextual information, as demonstrated in Figure 1 (top). These techniques employ a sequential encoding process to ground LLM inputs with knowledge from external sources: concatenating the retrieved texts into one sequence, and encoding the sequence into key-value (KV) states as the context for subsequent queries. While this new, significantly longer input improves performance, the increased latency in context prefilling becomes a bottleneck in tasks that require long inputs but generate short outputs (Bai et al., 2023; Agarwal et al., 2024; Jiang et al., 2024b). For example, prefilling a 128K context takes 17 seconds, whereas generating 256 tokens requires only 6 seconds. This discrepancy leaves significant room to improve the practical efficiency of CAG systems in real-world deployments (Liu, 2022; Chase, 2022).\nSince texts for CAG are typically stored independently in external databases (Zayarni et al., 2024; Douze et al., 2024), pre-caching all these texts for direct loading during inference offers a brute-force approach to accelerate CAG. However, for autoregressive LLMs, the KV states are inherently context-dependent. This dependency makes naive pre-caching impractical, as it would require caching all possible context permutations, leading to factorial growth in memory requirements as the database size increases. For instance, caching all permutations of just ten 256-token text chunks for the LLAMA-3-8B model would demand an impractical 22 PB of memory.\nTo address this issue, parallel encoding (Ratner et al., 2022; Yen et al., 2024; Li et al., 2024; Sun et al., 2024) is introduced to encode each context into KV states separately, ensuring that tokens from different contexts cannot attend to each other during encoding. Next, the on-the-fly generation starts by prefilling user queries, which can attend to the cached KV states from all contexts without re-encoding, offering two benefits:\nPre-caching Contexts for Fast Inference: Texts from external sources can be pre-computed and cached"}, {"title": "2 Background and Related Work", "content": "This work explores CAG problems using LLMs, where user queries are enhanced with additional contexts from external databases. CAG typically involves two scenarios: RAG (Asai et al., 2024; Gupta et al., 2024; Gao et al., 2023), which focuses on directly retrieving relevant information, and ICL (Dong et al., 2022; Wei et al., 2022; Agarwal et al., 2024), which emphasizes further acquiring emergent capabilities from in-context examples."}, {"title": "2.1 Context-Augmented Generation", "content": "This work explores CAG problems using LLMs, where user queries are enhanced with additional contexts from external databases. CAG typically involves two scenarios: RAG (Asai et al., 2024; Gupta et al., 2024; Gao et al., 2023), which focuses on directly retrieving relevant information, and ICL (Dong et al., 2022; Wei et al., 2022; Agarwal et al., 2024), which emphasizes further acquiring emergent capabilities from in-context examples."}, {"title": "2.2 Parallel Encoding", "content": "Next, we present the formulation of using parallel encoding in LLMs for CAG settings. Let $S$ represent the input sequence including $N$ contexts $C_1,..., C_N$ and one query $Q$. Formally, this can be denoted as:\n$S = {SC_{1,1}, ..., SC_{1,l_1}, SC_{2,1}, ..., SC_{2,l_2}, ..., SC_{N,1}, ..., SC_{N,l_N}, SQ_{1}, ..., SQ_{l_Q}} .$\nFor simplicity, we can express this as: $S = {SC_1, SC_2,...,SC_N, SQ}$. Given two models $\\Theta_{Enc}$ and $\\Theta_{Dec}$ (which may be the same model), a response $R$ is generated to the input $S$ using parallel encoding in two steps:\nPre-caching Contexts. The first step is to encode and cache the KV states for each context independently using $Enc$. For a given context $Sc_i$, we compute its KV states offline as $(Kc_i, Vc_i) = Enc(SC_i)$ and store them for direct loading during inference. Specifically, we denote $Kc_i = {kc_{i,1},..., kc_{i,l_i} }$ and $Vc_i = {VC_{i,1},..., VC_{i,l_i}}$.\nGenerating Response. Next, the user query is augmented by all relevant pre-cached KV states to generate the response: $R = \\Theta_{Dec}(SQ, Kc, Vc)$, where $Kc, Vc$ are subsets of ${KC_1, ..., KC_N }$ and ${VC_1, ..., VCN }$, respectively.\nParallel encoding significantly improves efficiency compared to sequential encoding by reducing the complexity of prefilling from $O((l_1 + ... + l_N + l_Q)^2)$ (i.e., quadratic) to linear concerning the total context length. With pre-caching, the cost becomes $O((l_1 + ... + l_N + l_q) \\cdot l_q)$. In the absence of pre-caching, the complexity is $O(max(l_1, ..., l_n) + ((l_1 + ... + l_n + l_q) \\cdot l_q)$, which remains efficient for multiple contexts of similar length.\nPrior parallel encoding approaches vary in their design of $\\Theta_{Enc}$ and $\\Theta_{Dec}$. Parallel Context Windows (PCW) (Ratner et al., 2022) directly employs pre-trained LLMs as both, resulting in significant performance drops. Block-Attention (Sun et al., 2024) further fine-tunes the model, successfully recovering performance in RAG tasks.\nAlternatively, CEPE (Yen et al., 2024) and FocusLLM (Li et al., 2024) train new Transformer-based encoders using encoder-only and decoder-only architectures, respectively. These methods also differ in Dec: CEPE trains additional cross-attention layers for processing contexts, whereas other methods directly input the context into original self-attention layers. While these trainable methods show promising results in RAG tasks, challenges remain regarding their training overheads and generalization abilities to more complex ICL scenarios. Moreover, applying parallel encoding in CAG can be viewed as a kind of memory-augmented neural networks (Burtsev et al., 2020; De Jong et al., 2021; F\u00e9vry et al., 2020), where external memory is directly stored into KV states."}, {"title": "2.3 Attention Mechanism", "content": "In a standard Softmax attention, we attend the query to all past KV states using the following formula:\n$O = Softmax(\\frac{Q K^T}{\\sqrt{d}})V \\quad Q \\in \\mathbb{R}^{n \\times d}, K,V \\in \\mathbb{R}^{m \\times d},$\nwhere $Q$ is the query state, and $K$ and $V$ denote the key and value states, respectively. Previous research has revealed several significant insights into the distribution of attention weights (i.e., $Softmax(QK^T)$).\nAttention Sink. StreamingLLM (Xiao et al., 2023) identifies the presence of an \"attention sink\" in LLMs, a token that receives a significantly higher attention score than other tokens but provides limited semantic information. It observes that the attention sink exists in the initial token and influences the following tokens.\nPosition Embedding. To effectively process sequential input, LLMs require position embeddings, such as absolute position embeddings (Vaswani, 2017; Devlin, 2018) and relative position embeddings (Su et al., 2024; Press et al., 2021). However, the introduction of position embedding not only limits the context window to the training length (Chen et al., 2023) but also results in the \"lost in the middle\" (Liu et al., 2024a) issue, where LLMs struggle to produce correct answers when relevant information locates in the middle of the context."}, {"title": "3 Observations", "content": "In Section 3.1, we evaluate sequential encoding, parallel encoding, and CEPE-Distilled (CEPED) (Yen et al., 2024) using the LLAMA-2-7B-CHAT model\u00b9. Figure 2 presents our findings on various RAG and ICL tasks, highlighting the limitations of trainable approaches in generalizing to complex reasoning tasks. Next, we explore the alignments and misalignments between parallel encoding and sequential encoding in Section 3.2, providing insights into why parallel encoding remains effective and identifying opportunities for further improvement."}, {"title": "3.1 Trainable Approaches are only Effective for Easy Tasks.", "content": "In Figure 2, we compare the performance of different context encoding methods on RAG and ICL tasks, with detailed setups described in Appendix A. Our analysis of the long-context RAG capability on LongBench (Bai et al., 2023) is showcased in Figure 2a. Despite accessing more passages, CEPED only surpasses the sequential baseline in two of the three QA tasks, and it even notably underperforms parallel encoding in the summarization task (MultiNews), which requires synthesizing information from the entire context. We hypothesize that CEPED cannot process complex tasks since the encoder and decoder are only trained on the unlabeled pre-training corpus without instruction-tuning on high-quality QA samples. This conclusion is further supported by the results of ICL tasks (see Figure 2b), where CEPED performs on par with the 1-shot sequential encoding baseline"}, {"title": "3.2 Comparing Parallel Encoding and Sequential Encoding.", "content": "In Figure 2, we observe that parallel encoding still holds promise, as it can generate reasonable responses without further modifications. This finding is non-trivial as contexts are encoded into KV states separately without guarantee that these states can be compared or combined. However, our analysis reveals that the attention mechanism naturally builds alignments between KV states from different positions in independent contexts similar to sequential encoding. To clarify this, Figure 3 focuses on the impact of the attention sink (Xiao et al., 2023), where we visualize the direction of KV states for different samples and positions. In Figure 4, we further visualize the distribution of various components in the Softmax attention, resulting in several findings.\nKey states from different contexts are similar. In Figure 3a and 3b, we measure the cosine similarity between the key states of different initial tokens for the LLAMA-3-8B-INSTRUCT and MISTRAL-7B-INSTRUCT-V0.3 models, which consistently yields a value close to 1. This observation indicates that the direction of the initial key state remains invariant mainly across different inputs. Figure 3c and 3d further analyze the similarity between the initial key states and their subsequent states, where we observe comparable negative values from different positions. Therefore, the angles between the initial key states and their subsequent states are similar and significantly larger than the angles between different initial key states, as demonstrated in Figure 5. It suggests that the direction of key states remains relatively consistent across contexts, as they are primarily decided by the initial key states, which exhibit similar directions across examples. These findings, combined with the small variance in key state magnitudes across examples in Figure 4b, indicate"}, {"title": "4 Adaptive Parallel Encoding", "content": "With all the lessons learned in Section 3, we will design our APE to address the residual misalignments. APE enables a seamless shift to parallel encoding without requiring training while maintaining most of the model's capabilities. Our approach adaptively aligns the distribution of attention weights between sequential and parallel encoding via three steps as illustrated in Figure 1, thereby boosting efficiency and performance."}, {"title": "4.1 Prepending Shared Prefix.", "content": "Figure 4 shows that the distribution of the first few tokens differs significantly from that of subsequent tokens. This discrepancy poses a challenge when encoding contexts in parallel due to duplicating these abnormal KV states. To address this issue, we propose a simple yet effective solution: prepending a shared prefix to all contexts. This approach ensures that these KV states appear only once in each generation step. In practice, the choice of prefix varies with the model and task. We use existing system prompts and instructions as the shared prefix when available. Otherwise, we will insert a few newline characters (i.e., \u201c\\n\") before all contexts.\nAlthough the later positions are not identified as abnormal, we still observe instability at the start of LLM inputs. To mitigate this issue, we may also consider extending the existing prefix with more newline characters."}, {"title": "4.2 Adjusting Attention Temperature.", "content": "In Figure 4d, the value of QK dot products increases as the relative distance decreases, with a notably sharper rise when the distance approaches zero. To show its impact on parallel encoding, we set a 50-token prefix and query, encoding the remaining 900 tokens either sequentially or in five parallel chunks, with attention distributions shown in Figure 7. Comparing Figure 7b with 7a, duplicating neighboring KV states in parallel encoding will disperse the query's attention to multiple contexts, resulting in a more uniform attention distribution. We adjust the attention temperature $T$ to a value less than 1 to refocus on the most relevant tokens, sharpening the distribution after the Softmax operation. The comparison between different $T$ is shown in Figure 7c and 7d."}, {"title": "4.3 Adding Scaling Factor.", "content": "While adjusting the temperature sharpens the attention distribution among context tokens, it will also alter the overall attention allocated to the whole context, as indicated by the LogSumExp value in Figure 8. Specifically, when the sum of the original QK dot product values in a given layer is significantly greater than 0, reducing temperature amplifies these positive values, resulting in an increased, positive LogSumExp value. Conversely, when the sum is closer to 0, lowering temperature has a stronger effect on the negative QK dot products, leading to a decreased, negative LogSumExp value. These effects generally increase the absolute value of $LogSumExp(QK)$. To compensate for these changes, we introduce a scaling factor $S < 1$ to reduce this absolute value."}, {"title": "4.4 Formulation.", "content": "Given these three steps, we can formulate the modified attention in APE. We begin with the standard Softmax attention, where Q, K, and V are the query, key, and value states, respectively. We use the subscript Ci for elements from the context Ci, while those without a subscript correspond to user queries or generated texts.\n$O = Softmax(\\frac{Q[K_{C_1},..., K_{C_N}, K^T]}{\\sqrt{d}}) \\times [V_{C_1},..., V_{C_N}, V]$\n$O = \\frac{[A_{C_1},..., A_{C_N}, A]}{\\sum_{i=1}^{N} \\sum_{j=1}^{l_{c_i}} a_{C_{i,j}} + \\sum_{j=1}^{l_{q}} a_{j}} \\times [V_{C_1},..., V_{C_N}, V],$\nwhere $A_{C_i} = [exp(\\frac{Qk_{C_{i,1}}}{\\sqrt{d}}),...., exp(\\frac{Qk_{C_{i,l_{C_i}}}}{\\sqrt{d}})]$ and $a_{C_{i,j}} = exp(\\frac{Qk_{C_{i,j}}}{\\sqrt{d}})$. Similar for A and $a_j$.\nAfter incorporating the proposed changes, the formula for our refined attention calculation becomes:\n$O' = \\frac{[A_P, A'_{C_1},..., A'_{C_N}, A]}{\\sum_{j=1}^{l_p} a'_p + (\\sum_{i=1}^{N} \\sum_{j=1}^{l_{c_i}} a'_{c_{i,j}})^S + \\sum_{j=1}^{l_q} a'_{j}} \\times [V_P, V_{C_1},..., V_{C_N}, V],$\nwhere $A'_{C_i} = [exp(\\frac{Qk_{C_{i,1}}}{T\\sqrt{d}}),..., exp(\\frac{Qk_{C_{i,l_{C_i}}}}{T\\sqrt{d}})]$ ,$a'_{C_{i,j}}= exp(\\frac{Qk_{C_{i,j}}}{T\\sqrt{d}})S$ and $a'_{j} = exp(\\frac{Qk_{j}}{T\\sqrt{d}})S$\n$A_P$ represents the attention weights for the shared prefix while A denotes that for query and generated tokens. The attention temperature $T$ and the scaling factor $S$ for the context are less than 1. Appendix C provides a detailed deduction of this formula for better understanding. All these modifications are compatible with fast attention implementations such as flash attention (Dao et al., 2022) by computing the context and non-context KV states separately and merging them into the attention output. This process only incur a negligible overhead.\nFor the choice of hyperparameters, we conduct a greedy search over a small validation set. If no prefix is provided, we begin by adding two \"\\n\u201d and increase the prefix length by 10, 20, and 40. S and T are searched in the ranges [0.1, 1.0] using 0.1 step sizes. We use $S \\cdot T$ instead of S as the scaling factor to simplify our search."}, {"title": "5 Experiments", "content": "Empirically, we present the effectiveness and efficiency of APE in CAG scenarios such as RAG and ICL. Since we focus on context encoding problems, we do not include comparisons with long-context LLMs. Specifically,\n\u2022\nIn Section 5.1, APE can maintain 98% of the accuracy on ChatRAG-Bench compared to sequential encoding. Furthermore, it improves 3.3% performance for RAG on LongBench by retrieving more and longer contexts.\n\u2022\nIn Section 5.2, APE outperforms parallel encoding by 7.9% on average in three ICL tasks. Moreover, APE can maintain 93% of the accuracy achieved by sequential encoding when using the same number of examples.\n\u2022\nIn Section 5.3, APE can scale to many-shot CAG tasks, effectively encoding hundreds of texts in parallel.\n\u2022\nIn Section 5.4, APE achieves 4.5\u00d7 faster inference for 128k context through 28\u00d7 reduction in prefilling time."}, {"title": "5.1 Retrieval-Augmented Generation.", "content": "In the context of RAG tasks, we validate that APE retains most of the sequential encoding capability while accommodating more and longer contexts, mitigating retrieval errors, and outperforming encoding baselines."}, {"title": "5.1.1 Retrieval for Multi-turn Question Answering.", "content": "Setup. APE is evaluated on five conversational QA tasks using ChatRAGBench (Liu et al., 2024b). For each query, we prepare about 100 text chunks. Three retrievers of varying quality are employed to retrieve up to the top-5 chunks for evaluation, including Contriever (Izacard et al., 2021), GTE-base Li et al. (2023), and Dragon-multiturn Liu et al. (2024b). We use LLAMA3-CHATQA-1.5-8B as the base model. To fairly measure performance drop after our modifications, the same retrieved texts are used for APE and sequential encoding."}, {"title": "5.1.2 Retrieval for Long-context Understanding.", "content": "Setup. Our evaluation involves eight tasks on LongBench (Bai et al., 2023). Given the long context, we split it into chunks with a size of M words, employ Contriever (Izacard et al., 2021) to compute the embeddings of all chunks and the query and retrieve the top-N chunks according to the cosine similarity of their embeddings to the query embedding. M and N vary across different methods. We compare APE with sequential encoding with and without RAG, and PCW, using LLAMA-3-8B-INSTRUCT (Dubey et al., 2024), MISTRAL-7B-INSTRUCT-v0.3 (Jiang et al., 2023), GEMMA-2-9B-IT (Team et al., 2024), and LLAMA-3.1-8B-INSTRUCT as base models.\nResults. In Table 2, APE consistently improves performance across all models, achieving a 5.6% average gain over sequential encoding without RAG. It also outperforms sequential RAG baselines by 3.3% by retrieving more and longer contexts. The superior performance over PCW further showcases the effectiveness of our modifications in APE. Notably, APE surpasses the 128K-context variant of the LLAMA-3.1-8B-INSTRUCT model by placing retrieved texts within the 8K context window, mitigating the \"lost in the middle\" phenomenon."}, {"title": "5.2 In-context Learning", "content": "Setup. We evaluate APE on three ICL tasks using the LM Evaluation Harness (Gao et al., 2024) codebase: GSM8K (8-shot) (Cobbe et al., 2021a), TriviaQA (5-shot) (Joshi et al., 2017), and MMLU (5-shot) (Hendrycks et al., 2020a). Experiments are conducted using the same base models as in our LongBench evaluations. We compare parallel encoding (PCW) to show the improvement of APE. Sequential encoding with varying numbers of shots (i.e., 1-shot, half-shots, and full-shots) is also employed to measure the gap from the ideal scenarios.\nResults. In Figure 9, APE surpasses parallel encoding with average improvements of 15.4% on GSM8K, 4.7% on TriviaQA, and 3.5% on MMLU. When compared with the 1-shot sequential baseline with similar context length, our method consistently yields superior results. Moreover, APE performs better than half-shot sequential encoding in 8/12 settings and preserves 93% accuracy compared to full-shot sequential encoding. Additionally, the LLAMA family exhibits enhanced compatibility with parallel encoding, potentially due to the stronger directional alignment of initial tokens from different contexts (see Figure 3a). Across different tasks, the performance gap between APE and full-shot sequential encoding is the largest on GSM8K. This finding suggests that while APE keeps most capabilities, its effectiveness may decrease as task complexity increases."}, {"title": "5.3 Many-shot Context-Augmented Generation", "content": "Setup. We evaluate the scalability of APE on four RAG and ICL tasks from the LOFT benchmark (Lee et al., 2024), each involving hundreds of additional texts. We employ LLAMA-3.1-8B-INSTRUCT as our base model to compare APE with sequential encoding, both applied to the same many-shot inputs. The total context lengths for the RAG and ICL tasks are 128K and 32K, respectively. We also include the zero-shot, few-shot (\u2264 5), and half-shot sequential encoding baselines. For metrics, F1 score and EM are used in RAG and ICL tasks.\nResults. In Table 3, APE achieves performance comparable to sequential encoding when processing the same many-shot long-context inputs, showing its ability to encode hundreds of texts in parallel efficiently. Notably, it outperforms sequential encoding on ArguAna and FEVER for RAG tasks. While APE is expected to reduce performance, it recovers this drop by positioning all texts close to the query, mitigating the \"lost in the middle\" problem in long-context LLMs. For ICL tasks, APE can learn from examples as effective as sequential encoding."}, {"title": "5.4 Efficiency Evaluation", "content": "Setup. We measure the latency for sequential encoding, MInference (Jiang et al., 2024a), and APE usingLlama-3.1-8B-Instruct (Dubey et al., 2024) on an H100 GPU with batch sizes of 1 and 4. The query and generation lengths are fixed at 256 tokens, while the context lengths range from 2K to 128K tokens. We employ VLLM (Kwon et al., 2023) as our inference engine and measure both prefilling time and total inference time.\nResults. Comparing to sequential encoding and MInference, APE can accelerate inference up to 4.5\u00d7 and"}, {"title": "6 Analysis", "content": "This section presents analyses to answer the following research questions: RQ1: Can APE improve performance for real-world RAG applications? RQ2: How does each component in APE contribute to the performance? RQ3: Can APE extend LLM context window size in long-context scenarios without RAG?"}, {"title": "6.1 Can APE improve performance for real-world RAG applications?", "content": "In Table 4, we evaluate APE in real-world RAG scenarios using the CRAG benchmark (Yang et al., 2024). Task 1 augments the model with five webpages, while Task 2 provides an additional knowledge graph as another retrieval source. In our experiments, the sequential encoding baseline is limited to retrieving 4K tokens, whereas APE can process 20 parallel segments of 4K tokens each. By incorporating significantly more external texts,"}, {"title": "6.2 How does each component in APE contribute to the performance?", "content": "In Table 5, we conduct an ablation study to examine each component in APE, including the shared prefix (P), attention temperature (T), and scaling factor (S). We present results averaged across the four base models evaluated in Figure 9. Our findings indicate that incorporating each of these components can improve performance for all tasks, with average improvements of 5.19%, 0.59%, and 2.07%, respectively. Among them, adding a shared prefix leads to the largest improvement, while adjusting the attention temperature yields minimal accuracy gains without the complementary effect of the scaling factor."}, {"title": "6.3 Can APE extend context lengths in long-context scenarios without RAG?", "content": "Table 6 evaluates the effectiveness of APE in handling a single long-context input using the LLAMA-3-8B-INSTRUCT model on the LongBench dataset (Bai et al., 2023). To accommodate the long context within our APE, we split it into multiple segments of less than 7,500 tokens. Additionally, we append the last 500 tokens to the query for two code completion tasks. Our results indicate that APE enhances performance across 10/11 tasks, yielding an average improvement of 6.6% compared to the sequential encoding baseline with limited context window size. More baseline results of long-context LLM approaches are provided in Appendix D."}, {"title": "7 Conclusion", "content": "This work explores the potential of parallel encoding in CAG scenarios, which can pre-cache KV states for fast inference and re-use positions for long context but lead to worse performance. To address this, we propose APE, a training-free method to enable accurate, fast, and long CAG systems. APE achieves this by aligning the attention weight distribution of parallel encoding with sequential encoding via three steps: shared prefix, adaptive temperature, and scaling factor. Empirically, we show that APE improves accuracy and efficiency in various RAG and ICL tasks while successfully scaling to process hundreds of chunks in parallel for both settings."}, {"title": "8 Limitations", "content": "While APE shows the effectiveness and efficiency of parallel encoding with only inference-time modification in the attention distribution, it remains sensitive to hyperparameter selection, particularly the attention temperature T and scaling factor S. In real-world applications, where contexts vary in length, quantity, and content, aligning the distribution between sequential and parallel encoding automatically presents a significant challenge."}]}