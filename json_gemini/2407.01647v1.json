{"title": "Optimizing PM2.5 Forecasting Accuracy with Hybrid Meta-Heuristic and\nMachine Learning Models", "authors": ["Parviz Ghafariasl", "Masoomeh Zeinalnezhad", "Amir Ahmadishokooh"], "abstract": "Timely alerts about hazardous air pollutants are crucial for public health. However,\nexisting forecasting models often overlook key factors like baseline parameters and missing data,\nlimiting their accuracy. This study introduces a hybrid approach to address these issues, focusing\non forecasting hourly PM2.5 concentrations using Support Vector Regression (SVR). Meta-\nheuristic algorithms, Grey Wolf Optimization (GWO) and Particle Swarm Optimization (PSO),\noptimize SVR Hyper-parameters \"C\" and \"Gamma\" to enhance prediction accuracy. Evaluation\nmetrics include R-squared (R2), Root Mean Square Error (RMSE), and Mean Absolute Error\n(MAE). Results show significant improvements with PSO-SVR (R2: 0.9401, RMSE: 0.2390,\nMAE: 0.1368) and GWO-SVR (R2: 0.9408, RMSE: 0.2376, MAE: 0.1373), indicating robust and\naccurate models suitable for similar research applications.", "sections": [{"title": "1. Introduction", "content": "Historically, when the amount of produced data was limited, many managers and decision-makers\nunderstood the concepts behind them with a superficial look and manual separation of the data\n(Zeinalnezhad et al., 2019). Due to the great importance of data and the progress of data mining,\nthis study has encountered the production of a large amount of them. The science of data mining\nhas provided a platform that can be used to classify, analyse and extract the hidden concepts in the\ndata by using new technologies, such as artificial intelligence and machine learning, appropriate\nfor specific goals and to use them to make critical decisions (Goodarzi et al., 2022). Various\nbusinesses around the world generate enormous data sets (Chofreh et al., 2021). These data sets\ninclude sales transactions, marketing data, product information, advertisements, company records\nand reports, and customer feedback (Alavi et al., 2022). In meteorology and air pollution, relevant\norganisations and researchers have been trying to identify and produce appropriate data to predict\nthe intensity of pollutants and protect human health from them (Kong et al. 2024).\nThe increase in the earth's temperature, climate change, and the rise of the sea level are among the\nconsequences of the high consumption of fossil energy and the release of pollutants and\ngreenhouse gases (Lopes et al. 2024). The depletion of fossil energy resources and the projected\nrise in prices have prompted legislators to devise regulations and policies for environmental\nmanagement. Simultaneously, academics are actively pursuing the development of cleaner and\nrenewable energy sources as potential alternatives to the existing energy infrastructure. The\nprediction of carbon dioxide (CO2) emissions has the utmost importance due to the substantial\nimpact it has on climate change and global warming, with the grave hazards it poses to human\nhealth (Shabani et al. 2021).\nThe provision of air quality forecasting serves as an efficient means of safeguarding public health\nby offering timely alerts regarding the presence of detrimental air pollutants (Bai et al. 2018). The\nongoing concern of governments and individuals is focused on the degradation of air quality, the\nfrequent occurrence of air contaminants, and the consequent health impacts (Zeinalnezhad et al.\n2021). There is a pressing need for the development of suitable and efficient forecasting tools\nwithin the realm of scientific study. Forecasting models are subject to continuous improvement\nand expansion. Various methodologies and approaches can be explored in the context of time\nseries data analysis to effectively address air pollution concerns and mitigate the occurrence of\nsevere pollution episodes (Liu et al. 2021). Various algorithms and methods, such as time series\nmodels, machine learning algorithms, neural networks, deep learning, and heuristic and meta-\nheuristic algorithms, have been used to predict pollutants (Zeinalnezhad et al. 2020) (Lai et al.\n2023). According to these contents, this study aims to introduce a better possible algorithm with a\nminor error to predict the emission of pollutants and obtain results with wider dimensions and the\nclosest to the real world.\nThe correlation between energy usage and the release of pollutants and greenhouse gases is widely\nacknowledged. The utilisation of fossil fuels, encompassing petroleum, petroleum byproducts, and\nnatural gas, results in the emission and augmentation of greenhouse gases, namely carbon dioxide\n(Ghafariasl et al. 2024). Developing nations significantly contribute to the production and\ndissemination of greenhouse gas emissions. China, the United States of America, Russia, India,\nJapan, Germany, Canada, Great Britain, South Korea, and Iran are internationally acknowledged\nas the foremost 10 global participants in carbon dioxide emissions (Qiao et al. 2021).\nDue to the severe dangers of air pollution to human health and natural ecosystems, various studies\nhave predicted these pollutants as an essential policy tool. Boznar et al. (Boznar, Lesjak, and\nMlakar 1993) were the first to model hourly sulfur dioxide (SO2) concentration in polluted\nindustrial areas of Slovenia with a neural network. Their study used the input parameters of\ntemperature, wind speed, direction, solar radiation, and time of day. Ultimately, the results showed\nthe usefulness of using neural networks in modeling. Juhos et al. (Juhos, Makra, and T\u00f3th 2008)\npredicted NO and NO2 concentration values for the next four days using MLP neural networks and\nSVR along with the PCA preprocessing method and showed that both SVR and MLP neural\nnetwork models can predict NO and NO2 pollutants.\nIn their study, Pao and Tsai (2011) conducted an analysis on the interconnections among pollutant\nemissions, energy consumption, and output in Brazil from 1980 to 2007. The gray forecasting\nmodel (GM) forecasts three variables during 2008-2013. The non-linear gray Bernoulli model\n(NGBM) predicts three indicators of carbon dioxide emissions, energy consumption, and actual\noutputs (Pao, Fu, and Tseng 2012). A numerical iterative approach is proposed for the optimisation\nof the NGBM parameter. The present study examines the \"full breakdown\" technique to assess the\nintensity of CO2 emissions and its constituent components across 36 economic sectors throughout\nthe period spanning from 1996 to 2009 (Robaina Alves and Moutinho 2013). A novel accounting\nmethodology was employed, using forecast error variance decomposition and shock response\nfunctions to analyse the elements that contribute to the decomposition of emission intensity. Yang\nand Zhao (2014) conducted a study whereby they analysed the temporal correlations between\neconomic growth, energy consumption, and carbon emissions in India from 1970 to 2008. The\nresearchers utilised sophisticated methodologies, including out-of-sample Granger causality tests\nand directed acyclic graphs (DAG), to examine these relationships.\nIn an independent investigation, Wu et al. (2015) undertook an examination of the interrelationship\nbetween energy consumption, urban population, economic growth, and CO2 emissions among the\nBRICS nations (Brazil, Russia, India, China, and South Africa) throughout the period spanning\nfrom 2004 to 2010. The researchers employed a New Multivariate Grey Revisited model for their\ninvestigation. This study aims to examine the various impacts via which carbon dioxide (CO2)\nemissions in the tourist sector can be analysed. Specifically, it seeks to investigate the evolution\nof these effects over time and determine which of them play a more significant role in determining\nthe overall emissions. (Robaina-Alves, Moutinho, and Costa 2016). In this study, the analysis\ntechnique employed was the logarithmic average division index, which was applied to examine\nfive distinct sub-sectors within the tourism industry in Portugal during the period from 2000 to\n2008. In their study, Wang and Ye (2017) proposed a multivariate grey model that incorporates\nthe power exponential expression of key factors as exogenous variables to forecast carbon dioxide\nemissions resulting from fossil energy consumption. Two non-linear programming models are\nformulated with the objective of minimising the average absolute percentage of error. The purpose\nof these models is to determine the values of the unknown parameters in the non-linear grey\nmultivariate model. Moreover, for the purpose of improving the suitability of the Grey model for\ndatasets with large sample sizes, the data about China's Gross Domestic Product (GDP) and carbon\nemissions arising from the consumption of fossil energy between the years 1953 and 2013 is\ndivided into fifteen separate stages.\nIn their study, Fand et al. (2018) introduced an enhanced Gaussian process regression technique\nfor predicting carbon dioxide emissions. This approach incorporates a modified particle swarm\noptimisation (PSO) algorithm to efficiently optimise the parameters of the covariance function in\nGaussian process regression. The authors also conducted experiments using their enhanced\nParticle Swarm Optimization-Gaussian Process Regression (PSO-GPR) technique using\ncomprehensive data pertaining to total carbon dioxide (CO2) emissions in the United States, China,\nand Japan for the period spanning from 1980 to 2012. The authors conducted a comparative\nanalysis of the predictive performance of their proposed methodology in relation to Gaussian\nProcess Regression (GPR) and the original Backpropagation (BP) neural networks. The evaluation\nwas carried out using datasets sourced from the United States, China, and Japan. Hosseini et al.\n(2019) utilised Multiple Linear Regression (MLR) and Polynomial Regression (MPR)\nmethodologies to predict the levels of carbon dioxide (CO2) emissions in Iran for the year 2030.\nThe researchers examined two specific scenarios, referred to as Business As Usual (BAU) and the\nSixth Development Plan (SDP). In their study, Acheampong and Boateng (2019) utilised an\nArtificial Neural Network (ANN) approach to develop predictive models for carbon emission\nintensity in five specific countries, namely Australia, Brazil, China, India, and the United States\nof America. The researchers utilised a collection of nine attributes, specifically economic growth,\nenergy consumption, research and development, financial development, foreign direct investment,\ntrade openness, industrialization, and urbanisation, as input variables. The aforementioned\nelements play a pivotal role in influencing the level of carbon emission intensity.\nWu et al. (2020) performed an analysis on the carbon dioxide emissions of the BRICS countries,\nnamely Brazil, Russia, India, China, and South Africa. The researchers utilised a conformable\nfractional non-homogeneous grey model in their investigation. The solutions of the novel model\nhave been derived utilising mathematical methodologies and grey theory. Additionally, the ant\nlion optimizer, a meta-heuristic algorithm, has been employed to explore the optimal fractional\norder. Machine learning predictive models for predicting particulate matter concentrations in\natmospheric air on a Taiwan air quality monitoring dataset obtained from 2012 to 2017 have been\ninvestigated by Doreswamy et al. (Doreswamy et al. 2020).\nThe study conducted by Qiao et al. (2021) employed the Discrete Grey Forecasting Model (DGM)\nto predict carbon dioxide (CO2) emissions in the member nations of the Asia-Pacific Economic\nCooperation (APEC) for the period of 2020-2023. The model utilised data from 2014 to 2019, and\nits performance was evaluated based on the Mean Absolute Percentage Error (MAPE) metric. In\na separate study, Rehman and colleagues (2021) conducted an investigation of the effects of carbon\ndioxide emissions on many aspects, including forest productivity, crop production, animal\nproduction, energy consumption, population increase, temperature, and rainfall in the context of\nPakistan. In their study, Yan et al. (2021) endeavoured to construct a predictive model for Beijing\nair quality that encompasses several locations and sites. To achieve this, they employed deep\nlearning network models that incorporated spatial and temporal clustering techniques.\nFurthermore, the researchers conducted a comparative analysis between these models and a neural\nnetwork known as BPNN. Espinosa et al. (2021) introduced a novel approach that utilises accuracy\nand robustness as key criteria for evaluating and contrasting various pollutant prediction models\nand their respective attributes. This study examines various deep learning models, including\nDCNN1, GRU, and LSTM, as well as regression models, such as random forest regression, lasso\nregression, and support vector machines, using different window widths. Salam et al. (2021)\nintroduced a novel model called LSTM-SDAE (CLS) loop, which utilises deep learning techniques\nto forecast particulate matter levels. The model also uncovers the relationship between particulate\nmatter and meteorological parameters.\nThe primary concentration of carbon dioxide (CO2) emissions connected to energy production is\nobserved in metropolitan regions. The significance of undertaking quantitative research on the\nassociation between carbon dioxide (CO2) emissions and economic growth at both the municipal\nand sub-municipal levels is of utmost relevance. The study conducted by Shi et al. (2022)\ninvestigated the extent to which CO2 emissions were disconnected from economic growth in 16\nregions of Beijing throughout the period of 2006 to 2017. The Tapio decoupling model was utilised\nfor this analysis. Chong et al. (2022) provides an overview of the latest advancements in several\ngrowing energy industries, with a particular focus on the importance of achieving carbon neutrality\nand ensuring energy sustainability in the period following the Covid-19 pandemic.\nThe significance of precise forecasting of air pollution levels is underscored in the existing body\nof literature, as it serves as a crucial component within the early warning system. Nonetheless, this\nissue continues to present a formidable obstacle, mostly stemming from the scarcity of available\ndata regarding the emission source, as well as the substantial level of uncertainty surrounding the\nintricate dynamic processes involved (Kim et al. 2021). Analysis and prediction of the emission\nof pollutants and greenhouse gases are of double necessity in making decisions and preventing\nenvironmental destruction.\nPredictive models based on artificial intelligence (AI) and machine learning (ML) techniques have\nbeen extensively employed and suggested for the estimation of pollutant parameters, with a\nprimary focus on predicting PM2.5 levels. As previously said, this discipline necessitates the\nincorporation and suggestion of innovative hybrid predictive models that are founded on the\nprinciple of an SVR model, which has been enhanced by the utilisation of robust optimisation\ntechniques. The main contribution of this research is the utilisation and enhancement of innovative\nhybrid support vector regression (SVR)-based models in the field of pollutant prediction, with a\nspecific focus on PM2.5. The choice was made to incorporate two well-established and highly\nregarded optimisation methodologies, specifically Particle Swarm Optimisation (PSO) and Grey\nWolf Optimisation (GWO), into hybrid Support Vector Regression (SVR) models. This paper\npresents a novel methodology that involves the utilisation of two support vector regression (SVR)\nmodels, specifically Particle Swarm Optimization-SVR (PSO-SVR) and Grey Wolf Optimization-\nSVR (GWO-SVR), in order to forecast PM2.5 levels. The utilisation of the Particle Swarm\nOptimisation (PSO) and Grey Wolf Optimisation (GWO) algorithms is implemented in order to\noptimise the hyperparameters 'C' and 'gamma' of the Support Vector Regression (SVR) model.\nThe goal is to enhance the predictive capabilities of the model and achieve improved performance.\nThe work demonstrates innovation through the introduction and implementation of hybridization\ntechniques in SVR models for PM2.5 forecasting.\nThe paper is structured in the following manner. Section 2 encompasses the materials and methods\nemployed in this investigation, encompassing the study area and data description, the models\nutilised, and the performance measures employed. Section 3 encompasses the results and\ndiscussion. Section 4 entails the conclusions."}, {"title": "2. Materials and methods", "content": "This section describes the implemented models and discusses the SVR-based optimization\ntechniques and validation criteria."}, {"title": "2.1. Study area and data description", "content": "The present work utilised the dataset obtained from the UCI website in order to validate our model.\nIn January 2013, Beijing implemented an air pollution monitoring network as an integral\ncomponent of its nationwide monitoring network. In Beijing, there exists a total of 36 sites\ndedicated to the monitoring of air quality. Among these sites, 35 are operated by the Beijing\nMunicipal Environmental Monitoring Centre (BMEMC), while the remaining site is managed by\nthe United States embassy located in Beijing (Zhang et al. 2017). The dataset under consideration\nencompasses six prominent air contaminants and six associated meteorological variables observed\nat multiple locations throughout Beijing. The dataset comprises hourly measurements of air\npollutants collected from 12 stations dedicated to monitoring air quality. The air quality data was\nacquired from the Beijing Municipal Environmental Monitoring Centre. The meteorological data\nobtained at each air quality site aligns with the meteorological station located nearby, which is\nunder the operation of the China Meteorological Administration. The period of time being\nexamined extends from March 1, 2013, to February 28, 2017 (UCI Machine Learning Repository:\nBeijing Multi-Site Air-Quality Data Data Set, n.d.). In this study, the data from Aotizhongxin for\nthe years 2013 and 2014 have been selected for the purpose of model validation.\nThe American Environmental Protection Agency (EPA) has selected six primary pollutants as\nstandard pollutants to measure the level of air pollution. Additionally, the data has been classified\ninto two distinct categories: primary and secondary. Primary pollutants are compounds that are\ndirectly emitted into the ambient air from sources. They include CO, NO2, SO2, PM, and PB\npollutants, except for the latter one found in our dataset. Secondary pollutants refer to the things\nthat arise from reactions in the earth's atmosphere, and O3 can be mentioned in this category.\nMeteorological conditions have a significant effect on air pollution. The issue of air pollution can\nbe analysed in terms of meteorological factors, which can be classified into two main categories:\nprimary and secondary. The primary characteristics encompass wind direction (WD), wind speed,\ntemperature, while the secondary parameters encompass precipitation, humidity, radiation, and\nvisibility. The aforementioned metrics exhibit a substantial correlation with latitude, seasonality,\nand topographic characteristics. The degree of pollution is influenced by weather conditions, and\nconversely, air pollution has an impact on weather conditions. As an illustration, the presence of\nair pollution has the potential to diminish visibility, intensify the occurrence and length of dense\nfogs, and diminish the amount of solar energy reaching the Earth's surface. The levels of rainfall\nand relative humidity in urban areas have the potential to both rise and fall."}, {"title": "2.2. Implemented models", "content": "Different algorithms are introduced and used to build a prediction model at this stage. These\nalgorithms are implemented in Python software, and the accuracy of each one is obtained in order\nto choose the best method."}, {"title": "2.2.1. Support vector regression", "content": "The support vector machine (SVM) was first proposed by Vapnik in 1999, based on the concepts\nof statistical learning theory. The method in question is widely acknowledged within the academic\ncommunity as a form of guided learning. The kernel function possesses the capability to convert\ninput vectors that are non-linear into a space with multiple features. A hyperspace is created within\nthe feature space in order to effectively distinguish and separate the two distinct data kinds. The\ndistinctive attributes of the decision level guarantee that Support Vector Machines (SVM) possess\na strong capacity for generalization (Rui et al. 2019). Furthermore, support vector machines have\nbeen employed for the analysis of time series and regression tasks in many research and scenarios\n(Gao, Qi, and Yang 2024). The SVM algorithm can be categorised into two main variants: support\nvector classification machine and support vector regression machine. The former typically pertains\nto tasks involving the categorization of data and is employed for the purpose of making predictions.\nRaj (Raj 2020) mentioned that although support vector regression is rarely used, it has certain\nadvantages, as listed below: (i) The algorithm exhibits robustness against outliers. (ii) The decision\nmodel may be readily modified and updated. (iii) The algorithm demonstrates strong generalisation\ncapabilities, resulting in accurate predictions. (iv) The implementation of the algorithm is\nstraightforward and uncomplicated.\nThe SVR function possesses the ability to demonstrate both linear and non-linear behaviour. The\nSupport Vector Regression (SVR) model employs a series of linear functions, characterised by the\nequation f(x) = (w.x) + b, in order to generate predictions. The equation presented herein involves\nthe utilisation of variables x, w, and b, which respectively denote the input vector, weight vector,\nand bias term. The incorporation of a loss function is a fundamental component of this\nmethodology, as it functions to quantify the permissible degree of disparity between the predicted\nvalues and the actual values (Drucker et al., 1996). Hence, the following equations are utilised to\nminimise the optimisation problem.\nMinimize: $\\frac{1}{2}||W||^2 + C \\sum_{i=1}^{n}(\\xi_i^* + \\xi_i)$ (1)\nSubject to: $\\begin{cases}\nY_i \u2013 (W.X_i + b) \\leq E + \\xi_i \\\\\n(W.X_i + b) \u2013 Y_i \\leq E + \\xi_i^* \\\\\n\\xi^*,\\xi \\geq 0\n\\end{cases}$ (2)\nIn the context of a loss function, the symbol & denotes the permissible error, while \u03be and \u03be*\nrepresent the variables that approach their respective limits. Additionally, C denotes the penalty\nparameter. It is important to acknowledge that the efficacy of Support Vector Regression (SVR)\nis contingent upon the appropriate configuration of certain parameters, including C, E, and the\nrelevant kernel parameters (Paryani et al., 2021).\nThe optimisation issue mentioned above can be transformed into a quadratic dual optimisation\nproblem by using the Lagrange coefficients a\u2081 and ai*. Upon successfully solving the dual\noptimisation problem, the resultant parameter vector w is acquired in equation (3). The support\nvector regression (SVR) function is derived as equation (4).\nW* = $\\sum_{i=1}^{n}(\\alpha_i \u2212 \\alpha_i^*)(X_i)$ (3)\nf(\u03a7, \u03b1\u0390, \u03b1\u0390) = $\\sum_{i=1}^{n}(\\alpha_i \u2212 \\alpha_i^*)K(X_i, X_j) + b$ (4)\nThe Lagrange coefficients, denoted as a\u2081 and a\u2081*, are utilised in conjunction with the kernel function\nK(Xi, Xj) to facilitate non-linear mapping. Various kernels are employed in the Support Vector\nRegression (SVR) model. According to Hamzeh et al. (Hamzeh et al. 2017), some common kernels\nare:\nK(X1,X2) = $X_1^TX_2$ Linear kernel (5)\nK(X1,X2) = $(X_1 X_2 + y)^d$ y, d > 0 Polynomial kernel (6)\nK(X1,X2) = exp (\u2212y||X1 \u2013 X2||2) \u03b3 > 0 Radial Basis Function (RBF) (7)\nK(X1,X2) = tanh (yX1 X2 + r) \u03b3,\u03b3 > 0 Sigmoid kernel (8)\nThe kernel parameters are denoted by r, y, and d. The performance, generalizability, and accuracy\nof SVR models are contingent upon the optimal selection of parameters such as y, r, C, and d."}, {"title": "2.2.2. Particle swarm optimization", "content": "The Particle Swarm Optimisation (PSO) algorithm is considered to be a highly effective approach\nfor addressing optimisation problems, particularly when compared to other evolutionary search\nmethods that mimic the behaviour of fish schools and bird colonies (Kennedy and Eberhart, 1995).\nConsequently, the researchers endeavour to enhance the accuracy of pollutant prediction outcomes\nby integrating the aforementioned approach with the Support Vector Regression (SVR) technique.\nIn Particle Swarm Optimisation (PSO), a collection of particles is metaphorically represented by\na flock of birds, while a food source symbolises a functional objective. Once the pertinent details\nregarding the spatial separation between the avian assemblages and the sustenance origin have\nbeen conveyed, the precise whereabouts of the sustenance origin can be ascertained through the\ncongregations of avian groups. This collaborative behaviour enables the entire group of avian\norganisms to effectively communicate and determine the most accurate details on the whereabouts\nof the nourishment site, ultimately resulting in their collective convergence towards the food\nsource. By employing these procedures, it is possible to furnish the most prevalent source of\nsustenance (Li et al. 2021).\nIn the Particle Swarm Optimisation (PSO) algorithm, the initialization phase involves assigning\nnumerical values to the particles. Each particle is then considered as a potential candidate solution\nto the specific problem, with an equal likelihood of being picked. Subsequently, it is vital to\nprecisely ascertain two crucial attributes of every particle, specifically the revised velocity (V) and\nthe unchanging position (X) (Poli et al. 2007). The fitness function assesses the fitness of\nindividual particles, and the positions of the particles' masses are adjusted according to the fitness\nfunction's evaluation outcomes. Through successive iterations, the particle swarm algorithm\nconverges towards the optimal position that maximises the predefined goal function as determined\nby the users (Li et al. 2021). The relevant parameters in Particle Swarm Optimisation (PSO) are\nupdated in the following manner, allowing for the determination of the new position and velocity.\n$\\begin{cases}\nVt+1 = wV^t + c_1rand(A)(P_{best} - X^t) + c_2rand(B)(G_{best} \u2013 X^t) \\\\\nX^{t+1} = X^t + V^{t+1}\n\\end{cases}$ (9)\nWhere t be the current iteration number, and w rand(A) and rand(B) represent random numbers\nselected from the interval (0, 1). Pbest and Gbest represent the best separate particle and whole\nparticle positions. c1 and c2 remain constants that control particle acceleration (Zhou et al. 2013).\nThe symbol o is used to denote the inertia weight, which plays a crucial role in determining the\nequilibrium between global and local optimization (Shi and Eberhart 1998). In general, the value\nof o decreases in each iteration. It can be determined as follows:\nwt+1 = \u03c9max\n@max-min\nIterationmax\n(10)\nwmax denotes the greatest value of the inertia weight, whereas w^min indicates the minimum\nvalue. Additionally, Iterationmax signifies the maximum number of iterations or repetitions. The\nParticle Swarm Optimisation (PSO) algorithm is utilised within the Support Vector Regression\n(SVR) framework to optimise two significant meta-parameters, specifically C and y. The\nachievement of global optimisation in particle swarm is ultimately realised through the iterative\nprocedure of updating the velocity and position of all particles inside the swarm. The overall\nprocedure of Particle Swarm Optimisation (PSO) can be elucidated by the visual representation\nprovided in Figure 1 (Li et al. 2021)."}, {"title": "2.2.3. Grey wolf optimization", "content": "The GWO algorithm is a biologically inspired optimisation algorithm that emulates the social\nhierarchical leadership and hunting techniques observed in grey wolves (Mirjalili et al. 2014). The\nGWO algorithm yielded notable outcomes in comparison to other established algorithms. The\noutcomes pertaining to unimodal and multimodal functions provide evidence of the enhanced\nefficacy of the Grey Wolf Optimisation (GWO) algorithm. The outcomes of the integrated\nfunctions exhibit a significant tendency to avoid local optima, and the examination of Grey Wolf\nOptimisation (GWO) convergence verifies the convergence of this technique. The outcomes of\nengineering design challenges further demonstrate that the Grey Wolf Optimisation (GWO)\nalgorithm exhibits exceptional performance when operating in unfamiliar and demanding search\ndomains.\nThe Generalised World Optimisation (GWO) algorithm possesses various advantageous\ncharacteristics when applied to non-linear and multivariate functions. These include simplicity,\nflexibility, and the ability to avoid local optima, as highlighted by (Song et al. 2015). Grey wolves\nhave a preference for residing in social groups consisting of 5 to 12 members (Emary, Zawbaa,\nand Hassanien 2016). Every individual wolf within the pack is assigned distinct responsibilities\nthat the leader of the pack determines. Consequently, these entities are categorised into four\ndistinct classifications, namely \u03b1, \u03b2, \u03b4, and w. The GWO algorithm is founded upon a hierarchical\nstructure. Once a random solution (population) has been generated, the values of a, \u1e9e, and 8 are\ndecided based on the most appropriate solutions. The determination of the value of o during the\nremaining solutions is based on the equations provided by(Balogun et al. 2021):\nX (t + 1) = $\\frac{X_1+X_2+X_3}{3}$ (11)\n$\\begin{cases}\nX_1=X\u03b1 -A_1x(Da)\\\\\nX_2 = X\u03b2 \u2212A_2x (D\u03b2), \u1ea2= 2 x\u1ea3x\u012b \u2013 \u1ea3, D= |cxxp (t) \u2212x(t), x(t+1)= |Xp (t) \u2212\u00c3\u00d7D|\\\\\nX_3 = X\u03b4 -A_3x (D\u03b4)\n\\end{cases}$ (12)\n$\\begin{cases}\nDa= |C_1xX\u03b1-X|\\\\\\nDp= |C_2xX\u03b2-X|, C= 2 xr\\\\\\nD\u03b4= |C_3xX\u03b4-X|\n\\end{cases}$ (13)\n\u2611 and t represent the position of the wolf and the number of iterations. Xp is the position vector of\nthe prey A and C represent the coefficient vectors and components which decrease linearly\nbetween 0 and 2 in each iteration (Tu, Chen, and Liu 2019). r\u2081 and r2 are random vectors\ngenerated for the range [0,1] (Gupta and Deep 2019) (Figure 2). Hunting is also completed when\na takes values between -1 and 1 when an attack occurs (Balogun et al. 2021)."}, {"title": "2.3 SVR-based optimization techniques", "content": "The present study utilises Particle Swarm Optimisation (PSO) and Grey Wolf Optimisation\n(GWO) algorithms for the purpose of hyperparameter optimisation in a prediction model that is\nbased on Support Vector Regression (SVR). After conducting numerous experiments, it has been\nobserved that during each optimisation process, the computational time of the model tends to\nincrease as the population sizes become larger with an increase in the number of iterations. The\nstability of fitness values is expected to be higher in populations with small sizes. In this paper,\nthe decision was made to utilise a population size of 150 in the optimisation model for the purpose\nof generating models.\nThe hybrid model incorporating Support Vector Regression (SVR) utilises the Particle Swarm\nOptimisation (PSO) and Grey Wolf Optimisation (GWO) techniques to effectively optimise the\nhyperparameters 'C' and 'gamma' associated with the SVR model. Typically, the parameters are\nassigned values within the range of (0.01, 100). The fundamental procedure for optimising support\nvector regression (SVR) parameters utilising particle swarm optimisation (PSO) and grey wolf\noptimisation (GWO) approaches is outlined as follows:\n(1) Data preparation: The dataset is partitioned into training and testing sets using a suitable 80%\nand 20% ratio."}, {"title": "2.3.1. PSO-SVR model", "content": "One limitation of Support Vector Regression (SVR) is that it imposes certain constraints that may\nrestrict its applicability in academic and industrial settings. The researcher must define some free\nparameters, namely the SVR hyperparameters and SVR kernel parameters. The efficacy of SVR\nregression models is contingent upon the appropriate configuration of its parameters.\nConsequently, practitioners face the primary challenge of determining the optimal parameter\nvalues to achieve favourable generalisation performance when applying SVR to a specific training\ndataset. The pseudocode for the PSO-SVR algorithm is presented in Table 2."}, {"title": "2.3.2. GWO-SVR model", "content": "The major focus of prior research has been on utilising the Genetic Algorithm (GA) and Particle\nSwarm Optimisation (PSO) methods to optimise parameters inside the Support Vector Regression\n(SVR) model. However, it should be noted that these optimisation strategies often demonstrate\nslow convergence rates, complex parameter settings, or a tendency to get stuck in local optima.\nTherefore, the current work utilises the Grey Wolf Optimisation (GWO) algorithm to optimise the\nparameters of the Support Vector Regression (SVR) model. The GWO approach demonstrates a\ndecreased quantity of parameters and contains a significant level of global search capability. The\nexecution of this approach is uncomplicated and efficiently governs the local search range of the\nalgorithm, so attaining a harmonious equilibrium between its global search capacity and local\nsearch capacity. The schematic representation of the GWO-SVR model is depicted in Figure 4.\nAccording to previous research, it has been demonstrated that larger sample sizes are associated\nwith improved model performance and greater convergence when utilising the RBF and Sigmoid\nkernel functions, as opposed to the polynomial kernel function. In situations when the sample size\nis constrained and the number of features significantly surpasses the number of samples, it is\nplausible for the linear kernel function to provide performance that is on par with the radial basis\nfunction (RBF) kernel. Therefore, it can be obtained. Regardless of the presence of various\ncharacteristics, such as small features, multiple samples, or small sample sizes, it is evident that\nthe RBF kernel function has excellent performance in modelling and boasts a strong capability for\nnon-linear mapping. In this study, the RBF kernel function has been chosen as the kernel function\nfor the training prediction model of Support Vector Regression (SVR). The pseudocode of the\nGWO-SVR method is displayed in Table 3."}, {"title": "2.4 Performance metrics", "content": "Three mathematical evaluation metrics are adopted for the validation of the proposed models. In\ngeneral, the optimal prediction performance is indicated by RMSE and MAE values of zero,\nwhereas R2 values of 100. Various optimisation algorithms yield distinct prediction outcomes,\nmaking it possible to employ these values in order to ascertain the most effective optimisation\ntechnique. In the present study, a comprehensive evaluation methodology is utilised to analyse the\noverall performance of the three algorithms under consideration.\nR2 = 1 -$\\frac{\\sum_{i=1}^{M}(y_"}]}