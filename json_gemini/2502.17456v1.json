{"title": "Survey on Recent Progress of AI for Chemistry: Methods, Applications, and Opportunities", "authors": ["Hu Ding", "Pengxiang Hua", "Zhen Huang"], "abstract": "The development of artificial intelligence (AI) techniques has brought revolutionary changes across various realms. In particular, the use of AI-assisted methods to accelerate chemical research has become a popular and rapidly growing trend, leading to numerous groundbreaking works. In this paper, we provide a comprehensive review of current AI techniques in chemistry from a computational perspective, considering various aspects in the design of methods. We begin by discussing the characteristics of data from diverse sources, followed by an overview of various representation methods. Next, we review existing models for several topical tasks in the field, and conclude by highlighting some key challenges that warrant further attention.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) refers to a series of techniques that can perform tasks typically requiring human intelligence, such as speech recognition, language translation, path planning, and so on. This concept originated at the 1956 Dartmouth conference, and over the decades, AI technologies have become pervasive in a variety of areas and achieved remarkable results. In addition to applications in commerce or industry, people have been exploring how AI can be utilized to accelerate scientific research and discovery Stevens et al. [2020]. For example, the AlphaFold developed by DeepMind can accurately predict the three-dimensional structure of proteins based on amino acid sequence, revolutionizing the field of structural biology Jumper et al. [2021]. George et al. combined neural networks with physical laws expressed as PDEs, enabling efficient solutions to complex high-dimensional problems in fluid dynamics, quantum mechanics, and more Raissi et al. [2019]. AI for Science has become a new paradigm in scientific research, and has the potential to surpass the previous heuristics-based experimental design and discovery.\nChemistry is the fundamental science that drives technological innovation by unlocking the secrets of matter and enabling the discovery of groundbreaking materials Garcia-Martinez and Serrano-Torregrosa [2011]. Traditional chemistry research often faces significant challenges, such as the time-consuming trial-and-error processes, the complexity of modeling molecular interactions, and the vast number of possible reaction combinations. To accelerate the process of chemical research, there has been a growing exploration of the potential applications of AI in the field of chemistry. Developed at Stanford University in the mid-1960s, \"DENDRAL\" was one of the earliest and most influential chemical expert systems Buchanan et al. [1969]. By encoding the knowledge of experienced chemists into a series of rules and heuristics, the system was successfully applied to identify molecular structures from mass spectrometry data. Subsequently, several other expert systems emerged for predicting the reaction outcomes Salatin and Jorgensen [1980], Funatsu and Sasaki [1988], Satoh and Funatsu [1995], but all relied on predefined rules and were limited in adapting to new or complex scenarios.\nRecent advances in computational power and the development of automated synthesis technologies have led to a growing interest in developing machine learning (ML) methods for chemistry. For example, Ahneman et al. conducted around 4600 \"Pd-catalyzed Buchwald-Hartwig\" reactions using high-throughput equipment, and validated the yield prediction performance of several ML models such as linear regression and random forest Ahneman et al. [2018]. One of the most fascinating techniques in machine learning today is large language models (LLMs) Brown [2020], Achiam et al. [2023], Touvron et al. [2023], Zhao et al. [2023], and meanwhile LLMs for chemistry is gradually gaining attention. For instance, several LLMs enabled agent systems for chemical research have been proposed M. Bran et al. [2024], Boiko et al. [2023]. The powerful encoding and feature extraction abilities of language models are also utilized to improve downstream tasks Schwaller et al. [2021b, 2020b].\nTo provide a thorough introduction to the advancement of AI for Chemistry from the lens of computer science, it is essential to first clarify the paradigm of AI research, particularly in machine learning. The goal of ML is to extract knowledge from data to assist machines in performing certain tasks. Generally, ML involves three important components: data, representation and model (Fig. 1). Data serves as the foundation for ML, enabling the identification of patterns and relationships. For evaluating the accuracy and generalization of ML models, high-quality and diverse data is crucial. The collected raw data should be transformed into an appropriate machine-recognizable format, referred to as representation. An effective representation captures the essential features of the data while filtering out irrelevant or noisy information. The model, which is typically a mathematical framework equipped with construction algorithms, learns patterns from the training data and applies them to make predictions or decisions in new scenarios. Models can range from simple approaches, such as polynomial or tree-based models, to complex neural networks Liu et al. [2017b], Pouyanfar et al. [2018] and large language models. Moreover, how to select an appropriate model also heavily depends on the task at hand.\nThe remainder of this article is organized as follows: First, we introduce various data sources in the field of chemistry and discuss their characteristics. Next, we present the commonly used representation methods in machine learning. We then summarize the existing approaches for key application scenarios, including prediction tasks, molecular design, retrosynthesis, and AI-driven chemical robots. Following that, we explore recent studies on LLMs in chemistry. Finally, to support future advancements in this field, we highlight several key challenges in the application of AI techniques to chemistry."}, {"title": "2 Datasets and Descriptions", "content": "Machine learning is a data-driven discipline, and its rapid progress has been largely propelled by the increasing availability of data Zhou et al. [2017]. In the field of chemistry, addressing various data challenges is particularly demanding Strieth-Kalthoff et al. [2022]. High-quality, sufficiently large datasets are essential for machine learning techniques to uncover and understand chemical principles. However, as an experimental science, chemistry often relies on data derived from laboratory experiments or computational simulations, which can result in issues such as data scarcity and inconsistent quality Feller [1996]. Moreover, data are typically recorded in the individual laboratory, making it difficult to establish large-scale, publicly available datasets Liu et al. [2023]. Fortunately, significant advancements in technology and research over the years have led to substantial improvements in chemical datasets, providing a great support for machine learning applications in this field. Chemical datasets generally fall into two categories: molecular-level datasets, which record molecular properties and features, and reaction-level datasets, which focus on reaction conditions and outcomes (Fig. 2). This section will introduce several widely used datasets in the field, and a summary of these datasets is shown in Table 1."}, {"title": "2.1 Descriptions", "content": "Chemical datasets contain rich information, including molecular properties and reaction yields. However, to fully capture and utilize this information, we need first identify molecular entities and translate abstract molecular concepts into machine-readable formats. Therefore, before introducing the datasets, we will first discuss some common molecular identifiers and descriptors."}, {"title": "2.1.1 Molecular Identifiers", "content": "Molecular identifiers are symbols, strings, or codes that uniquely represent chemical molecules. They enable the systematically representation, storage, sharing, and retrieval of chemical substances. Traditionally, molecules are identified using identifiers such as the Chemical Abstracts Service (CAS) registry Number or database retrieval IDs. In the context of machine learning, however, a chemical identifier not only serves as a unique label for a molecule but also encodes basic chemical information. There are several common examples of these molecular identifiers, such as the Simplified Molecular Input Line Entry System (SMILES) Weininger [1988] and the International Chemical Identifier (InChI) Heller et al. [2015]. These identifiers describe a molecule's structure, composition, and other key features, and are widely used in machine learning models as inputs for various tasks."}, {"title": "SMILES", "content": "SMILES is a notation system designed to represent the structure of molecules in a human-readable text format (Fig. 3). Its primary goal is to offer a simple way to encode molecular structures while ensuring compatibility with computational tools. In SMILES, atomic symbols represent elements, with bonds typically implied or denoted by specific symbols such as '=', '#'. This system efficiently captures branching and ring structures and can encode stereochemistry and isotopes when necessary.\nAs a human-readable identify, SMILES is ideal for molecular data storage and retrieval; due to its compact nature and ease of use, it has been widely adopted in cheminformatics and computational chemistry. However, SMILES can encounter challenges when representing certain complex molecules. Additionally, it lacks a standard approach for representing stereochemical information, and in some cases, SMILES identifiers may not be unique. To address these limitations, alternative or improved linear representation methods, such as SMILES Arbitrary Target Specification (SMART) Systems and Self Referencing Embedded String (SELFIES) Krenn et al. [2020], have been proposed."}, {"title": "InChI", "content": "InChI is a standardized molecular identifier developed by the International Union of Pure and Applied Chemistry (IUPAC) to uniquely represent chemical substances in a machine-readable format. It encodes the structural information of a molecule in a layered format, detailing aspects such as atomic connectivity, stereochemistry, isotopic composition, and charge states Heller et al. [2015]. The InChI format ensures that each chemical substance is represented uniquely, making it an essential tool for chemical databases and research. Its machine-readable nature facilitates seamless data exchange across different systems, enhancing consistency and accessibility.\nInChI has become essential in global chemical research, particularly in drug discovery, as it standardizes data formats across various databases and platforms. However, it has limitations in representing complex structures compared to other identifiers like SMILES, and it is also more difficult for humans to interpret. The InChI standard is still under continuous development by IUPAC and the InChI Trust, with ongoing efforts to enhance and expand its applicability."}, {"title": "2.1.2 Molecular Fingerprints", "content": "Molecular fingerprints are widely used in cheminformatics to enable the comparison and classification of molecules based on their structural features. These fingerprints typically represent molecular information as binary or integer vectors, where each bit or value indicates the presence or absence of specific structural features, such as functional groups, atoms, or bonds. Different types of molecular fingerprints exist, each capturing distinct aspects of a molecule, from basic atomic features to more complex topological and stereochemical information. The following are some common types of molecular fingerprints.\nAtomPair Fingerprints capture the relationships between pairs of atoms in a molecule, considering both atom types and their distances Carhart et al. [1985]. The Molecular ACCess System (MACCS) keys, developed by Durant et al., consist of a set of predefined substructures (typically 166), encoding the presence or absence of each substructure within a molecule Durant et al. [2002]. Pharmacophore Fingerprints, proposed by Wpolber et al., focus on the spatial arrangement of functional groups or pharmacophores, which are key factors in determining a molecule's biological activity Wolber et al. [2008]. Extended-Connectivity Fingerprints (ECFP), also known as Morgan fingerprints, are circular fingerprint generated by iteratively exploring the environment around each atom Rogers and Hahn [2010]. ECFPs are highly customizable in terms of radius and bit size, which allows them to capture detailed molecular information. These types of fingerprints are widely applied in similarity searching Cereto-Massagu\u00e9 et al. [2015], virtual screening Melville et al. [2009], drug discovery and molecular property prediction Sandfort et al. [2020], Walters and Barzilay [2021]."}, {"title": "2.2 Molecular-Level Datasets", "content": "Molecular-level datasets are fundamental to the application of machine learning in fields like chemistry, drug discovery, and materials science. These datasets typically include molecular structures, quantum mechanical properties, physicochemical properties, biological activity data, and other relevant information. They are sourced from various outlets, including scientific publications, patents, or commercial databases. Such datasets can be used to evaluate a model's ability to predict molecular properties and can also serve as training data for developing molecular representations."}, {"title": "2.2.1 Several Common Public Databases", "content": "Based on centuries of continuous exploration and discovery, we witness that several well-established chemical databases have been created and are now publicly accessible. These datasets contain millions of molecular records, with the information of molecular structure, chemical formula, SMILES and InChI. Prominent examples include ChEMBL Zdrazil et al. [2023], PubChem Kim et al. [2024] and ZINC Irwin et al. [2020]. ChEMBL is one of the largest and most well-established bioactivity databases, offering a comprehensive resource to the scientific community. It provides extensive data on the bioactivity of small molecules, focusing on their interactions with biological targets such as proteins, enzymes, and receptors. ZINC specializes in providing researchers with a vast collection of commercially available compounds, ideal for virtual screening. PubChem, maintained by the National Center for Biotechnology Information (NCBI), is a comprehensive chemical database that offers detailed information on the biological activities, structures, and properties of chemical compounds."}, {"title": "2.2.2 Quantum Mechanics Datasets", "content": "Quantum Mechanics (QM) is a fundamental theory in physics that describes the behavior of matter and energy at the atomic and subatomic levels Dirac [1981]. In computational chemistry, QM methods are employed to predict the electronic structure of molecules and materials, providing valuable insights into their properties and behaviors. These methods have become indispensable tools for studying molecular interactions, chemical reactions, and material design Szabo and Ostlund [1996], McQuarrie and Simon [1997], Jensen [2017]. However, QM methods typically require substantial time and computational resources. For instance, performing a detailed electronic structure calculation using techniques such as Density Functional Theory (DFT) or coupled-cluster theory can take hours or even days on high-performance computing systems, especially for large molecules. As a result, using machine learning methods to efficiently and accurately predict quantum chemical properties holds significant practical value.\nTo support the development of these machine learning methods, several quantum chemistry datasets have been publicly released. Quantum-machine qua is an open project that provides a large collection of datasets with the goal of developing a \u201cquantum machine\" for fast and accurate quantum chemical simulations. The project has produced several popular quantum mechanics datasets, including QM7, QM7b, QM9, and QM8 Blum and Reymond [2009], Rupp et al. [2012], Montavon et al. [2013], Ruddigkeit et al. [2012], Ramakrishnan et al. [2014, 2015]. These datasets contain a weath of valuable molecular quantum chemical properties. For example, the QM9 dataset includes key properties such as energy, dipole moments, and free energies, making it a valuable resource for developing new computational techniques and exploring structure-property relationships in organic molecules."}, {"title": "2.2.3 Molecular Dynamics Datasets", "content": "Molecular dynamics (MD) is a computational simulation technique used to model the physical movements of atoms and molecules over time Alder and Wainwright [1959]. By solving Newton's equations of motion, MD simulations provide valuable insights into the behavior of molecules at the atomic level, helping to understand their structures, interactions, and dynamics. In machine learning, incorporating molecular dynamics data as input can enhance model performance in predicting related properties such as energy Riniker [2017]. How to leverage ML techniques to accelerate MD predictions is an important issue that warrants attention, and there already exist several machine learning models that integrate with MD computational methods Chmiela et al. [2017, 2018, 2020]. As an example, the MD17 dataset Chmiela et al. [2017] is a benchmark collection designed for training and evaluating machine learning models in molecular dynamics. It includes data for molecules such as benzene, toluene, naphthalene, ethanol, uracil, and aspirin Chmiela et al. [2017]. The dataset provides molecular dynamics trajectories generated via ab initio methods, which include atomic coordinates, potential energies, and atomic forces."}, {"title": "2.2.4 Molecular Properties Datasets", "content": "Molecular properties, such as solubility, toxicity, and biological activity are also key factors in chemical research. For example, the Blood-Brain Barrier Penetration (BBBP) dataset Sakiyama et al. [2021] is used to predict whether a molecule can cross the blood-brain barrier, which is a key factor in designing drugs that affects the central nervous system. The FreeSolv dataset Mobley and Guthrie [2014] provides solvation free energy data for molecules in water, offering valuable insights into how a molecule will interact in aqueous environment. This information aids in drug design by predicting the solubility of compounds. The Tox21 dataset Mayr et al. [2016], Huang et al. [2016] contains data on the toxicity of various compounds, specifically focusing on their effects on biological systems.\nThese molecular property datasets are widely studied and commonly used as benchmarks for evaluating molecular prediction models. However, since obtaining such data often requires conducting experimental works in chemical labs, these datasets are generally smaller in size compared to QM and MD datasets."}, {"title": "2.3 Reaction-Level Datasets", "content": "Reaction-level datasets typically document reaction conditions and components, sometimes also including selectivity or yield information. In general, these datasets can be constructed from three sources: existing literature and patents, laboratory experimental records (i.e. electronic laboratory notebooks (ELNs)), and specifically designed high-throughput experiments (HTE) Saebi et al. [2023].\nTo describe reaction-level datasets, we must first introduce the concept of \"reaction space\", which is fundamental in chemistry. Reaction space refers to the multidimensional space that encompasses all possible reaction combinations, including reactants, products and reaction conditions. The complexity and diversity of reaction space often lead to varying degrees of data insufficiency Stocker et al. [2020], Williams et al. [2021]."}, {"title": "2.3.1 USPTO", "content": "The USPTO reaction dataset contains millions of reactions extracted through text mining from United States patents published between 1976 and September 2016 Marco et al. [2015]. Each entry records a reaction in formats such as Chemical Markup Language (CML) or SMILES, including the details for reactants, reagents, and conditions. Although the dataset contains over one million reactions, it suffers from issues such as data duplication, poor quality, and inconsistencies. As a result, data cleaning is often necessary before using it for training purposes. Additionally, the USPTO dataset has been applied in the development of retrosynthesis models. It also provides valuable insights into reaction conditions, catalysts, and reagents, thereby contributing to more efficient chemical processes and advancing computational chemistry Chen et al. [2020a]."}, {"title": "2.3.2 Reaxys", "content": "Reaxys is a commercial chemical database that provides extensive reaction data sourced from patents and journals Elsevier, Goodman [2009]. It offers detailed information on reaction conditions and product yields, which is of great importance for synthetic planning and reaction optimization. Reaxys plays a crucial role in both academic and industrial research, assisting in the design of synthetic pathways, optimization of processes, and discovery of new reactions. The database is regularly updated and has supported a variety of applications from reaction prediction to material and drug discovery Elsevier."}, {"title": "2.3.3 HTE Datasets", "content": "High-Throughput Experimentation (HTE) accelerates the exploration and optimization of chemical processes through automation, reducing both the time and resource usage Shevlin [2017], Krska et al. [2017]. Several HTE datasets, such as the Suzuki coupling dataset Perera et al. [2018] and Buchwald-Hartwig dataset Ahneman et al. [2018], provide great amounts of reaction data under varying conditions, supporting tasks like yield prediction and reaction optimization. These reaction datasets typically account for multiple variables in the reactions, generating a reaction space with thousands of data points."}, {"title": "2.4 Additional Remarks on Chemical Data", "content": "In general, the lower the cost of acquiring data, the larger the available dataset. Low-cost data often consists of simple, basic descriptions of molecules and is typically used for representation learning. In contrast, high-quality data, which is harder to obtain, captures more macroscopic and direct properties. Such data is highly valuable for prediction tasks in chemistry, as models generally require a sufficient amount of high-value data for effective training Strieth-Kalthoff et al. [2022].\nTo address data scarcity, machine learning methods like transfer learning Shim et al. [2022] and active learning Shields et al. [2021], Gong et al. [2021] can be applied. Transfer learning involves transferring knowledge from a pre-trained model on one task to a related task and is particularly useful when the new task only has limited training data Pan and Yang [2009]. Active learning is a machine learning technique that iteratively identifies the most informative data points from an unlabeled dataset for labeling Ren et al. [2021]. Instead of randomly selecting data points, the model queries for labels on the most \u201cuncertain\u201d ones, and therefore yields a more efficient learning process. Moreover, acquiring large volumes of high-quality data through HTE Shevlin [2017], Krska et al. [2017] or leveraging ELN data Saebi et al. [2023] could also serve as effective strategies to mitigate the issue of data scarcity. Nonetheless, solving the challenges surrounding data for chemistry remains one of the most critical issues today."}, {"title": "3 Representation Learning Methods for Chemical Objects", "content": "Effectively representing data is a critical challenge in machine learning Bengio et al. [2013]. In the field of chemistry, this challenge is even more pronounced due to the complexity and diversity of chemical properties. A well-designed representation should be capable of distinguishing different molecular properties, meaning the distribution in the representation space should strongly correlate with the downstream task Talanquer [2022]. This requires that training data should contain sufficient chemical information, such as the topological relationships between atoms, 3D conformation and electronic distribution Kearnes et al. [2016].\nIn the previous section, we discussed simple molecular representations such as molecular fingerprints. While these methods are effective and align well with the intuition of chemists, they often fall short in tasks that demand higher generalizability in representation. In the domain of AI for chemistry, graph neural networks (GNNs) Gilmer et al. [2017], Zhou et al. [2020], Jiang et al. [2021] are widely adopted as the foundational model for representation learning, given the natural relevance of molecular graphs to this architecture. Additionally, the Transformer architecture Vaswani [2017], which has shown significant promise in AI, is also increasingly used to process SMILES strings to obtain better molecular representations Raghunathan and Priyakumar [2022]. Below, we delve into these two approaches in greater details."}, {"title": "3.1 GNN Based Representation Learning", "content": "Graph neural networks (GNNs) are specialized neural networks designed to handle graph-structured data Wu et al. [2020]. The core principle of GNNs lies in \"message passing\", where the nodes iteratively update their representations by exchanging information with neighboring nodes Scarselli et al. [2008]. Given that molecular structures naturally map to graph structures, GNNs are a fitting choice for molecular modeling.\nIn this approach, atoms and bonds are mapped to nodes and edges, respectively (Fig. 4). Each node (atom) is associated with a feature vector encoding properties such as atom type, charge, etc., while edges (bonds) can also carry features, such as bond type or length. During training, a GNN learns to represent graph-structured data by applying multiple layers of message passing, where each node updates its representation based on information from its neighboring nodes and connecting edges.\nAfter several iterations of message passing, the model produces rich representations for each node. For graph-level tasks, such as molecular property prediction, these node embeddings are aggregated to form a comprehensive graph-level representation of the molecule. This enables GNNs to effectively capture both local and global structural information, making them powerful tools for molecular modeling.\nBuilding on the commonly used GNN architectures, such as Graph Convolutional Networks (GCN) Kipf and Welling [2017], Graph Attention Networks (GAT) Veli\u010dkovi\u0107 et al. [2017] and\nGraph Sample and Aggregation (GraphSAGE) Hamilton et al. [2017], various models have been developed to represent molecular structures Kojima et al. [2020], Liu et al. [2022], LIU [2023]. Their effectiveness has been extensively validated using benchmark datasets such as QM9 and BBBP Zhou et al. [2023], further demonstrating their utility in advancing AI-driven chemistry research.\nThe methods introduced above are designed for 2D molecular structures, whereas \"3D-GNNs\" more efficiently leverage molecular 3D information by incorporating spatial relationships derived from the atomic coordinates Reiser et al. [2022], Godwin et al. [2022]. Instead of solely relying on the connectivity defined by chemical bonds, these models integrate geometric features such as interatomic distances, angles, and torsions. By embedding this spatial information, 3D-GNNs capture finer details about molecular interactions and conformations, making them particularly suitable for tasks involving quantum chemical properties, protein-ligand interactions, and materials discovery.\nGNNs can also leverage large amounts of unlabeled data to learn representations with better generalization capabilities. Contrastive Learning, a type of self-supervised learning technique, focuses on learning meaningful representations by comparing similar and dissimilar pairs of data points Chen et al. [2020b]. The central idea is to encourage the model to bring similar instances (called \"positive\" pairs) closer in the representation space while pushing dissimilar instances (called \"negative\" pairs) further apart. The construction of positive and negative molecular pairs allows GNNs to more accurately capture the underlying patterns in molecular space. Feng et al. Feng et al. [2024] introduced \u201cUnicorn\u201d, a unified pre-training framework for molecular representation. Unicorn addresses the limitations of existing pre-training approaches, which are often tailored to specific downstream tasks, by providing a more versatile solution. It employs contrastive learning to align molecular views at three levels: 2D graph masking, 3D graph denoising, and 2D-3D mapping, offering a comprehensive understanding of molecule structures. This unified approach achieves state-of-the-art performance across a wide range of quantum, physicochemical, and biological tasks, showcasing its effectiveness as a general-purpose model for molecular representation."}, {"title": "3.2 Transformer Based Representation Learning", "content": "Originally developed for natural language processing, Transformers have been adapted for chemical representation using sequential input of molecules Mao et al. [2021]. Transformers excel at handling sequential data, making them particularly effective for processing SMILES, which are linear encodings of molecular structures. These models can capture long-range dependencies within sequences, which is important for understanding complex chemical properties. Typically, Transformer models are pre-trained on large datasets and then fine-tuned for specific tasks such as property prediction or molecule generation Wang et al. [2019], Schwaller et al. [2019] (Fig. 5).\nGenerative models based on the Transformer architecture can perform SMILES-to-SMILES tasks and, after fine-tuning for specific downstream applications, excel in tasks such as retrosynthesis and molecular design Tetko et al. [2020], Mazuz et al. [2023]. Leveraging frameworks like Bidirectional Encoder Repre-sentations from Transformers (BERT) Devlin et al. [2019], Transformer can also handle various practical classification and prediction tasks in chemistry, such as reaction yield prediction Schwaller et al. [2021b, 2019]. Schwaller et al. Schwaller et al. [2021a] proposed the Reaction Fingerprint (RXNFP), a model that generates fingerprints for reactions via BERT. Their model employs attention mechanisms to focus on key atoms and reagents relevant to each reaction class. Moreover, these reaction fingerprints are independent of the number of molecules involved in the reactions, making them versatile and applicable across diverse reaction datasets. In this article, the authors further utilized RXNFP to embed and visualize reaction representations from the public USPTO-50k dataset Schneider et al. [2015], which comprises 50000 reactions categorized into 10 reaction classes."}, {"title": "3.3 Additional Remarks on Chemical Representation", "content": "Chemical representation plays a vital role in the application of machine learning to chemistry, as it directly imapcts a model's ability to understand the chemical world. Traditional chemical representation methods often rely on chemists' prior knowledge, which can limit the diversity and richness of the representation. In contrast, learning-based representation methods aim to uncover underlying patterns from large volumes of unlabeled molecular data, leading to improved performance across various tasks.\nHowever, representations derived through training are often not explicitly linked to chemical structures or properties, leading to challenges in interpretability. As AI continues to advance in the field of chemistry, we envision future representations combining the strengths of both traditional and learning-based approaches. The development of interpretable representation learning methods holds significant potential to accelerate progress in chemical research and enhance our understanding of complex molecular systems."}, {"title": "4 ML Models for Different Applications", "content": "The primary purpose of chemical research is to explore the structure, properties, and transformations of matter. However, traditional experimental and analytical methods in chemistry are often costly, posing significant challenges to the development of new materials and drugs. Here, we introduce several specific AI-driven applications in chemistry, including various prediction tasks, molecular design, and retrosynthesis. The integration of AI technologies has proven to significantly enhance the efficiency of these traditionally labor-intensive and time-consuming processes."}, {"title": "4.1 Prediction Problems", "content": "Many tasks in chemistry involve the prediction of certain features (Fig. 6). For example, in drug screening, chemists evaluate properties such as the activity, toxicity, and half-life of candidate compounds. In chemical synthesis, chemists prioritize reactions with high yield and selectivity. From a ML standpoint, these tasks can be framed as problems of \"classification\u201d (discrete output) or \"regression\" (continuous output), both of which are closely related to the representation of input data. Traditional \"non-deep learning\" methods, such as random forests or support vector machines Ahneman et al. [2018], typically feed molecular fingerprints or descriptors into their machine learning models. In contrast, recent advancements in the field increasingly leverage neural networks to learn molecular or reaction representations directly. These learned representations are then used to make predictions, either through additional network layers or traditional machine learning models Probst et al. [2022].\nThere are two primary approaches for modeling inputs in this realm. The first approach models each molecule as a graph, and represents each reaction as a set of graphs, encompassing reactants, products, and other related components; then it learns the mappings from inputs to outputs via specific graph-based methods. Coley et al. Coley et al. [2019] adopted the Graph Convolutional Neural Network (GCN) to predict potential molecular sites of reactivity by calculating likelihood scores for each bond change between each atom pair. Candidate products are generated by enumerating the most likely changes, and then they are ranked by using another GCN to identify the final predicted product species. Zang et al. Zang et al. [2023] proposed a hierarchical GNN framework for property prediction, capturing multi-scale information at the levels of atoms, motifs and entire molecules. Kwon et al. Kwon et al. [2022] combined GNN with uncertainty-aware learning, enabling the simulatneous prediction of the mean and variance of reaction yields. Saebi et al. Saebi et al. [2023] developed a hybrid framework that combines GNN with descriptor-based models; the predictions from both sources are integrated using an additional linear layer, and then the framework can produce the final predicted yield.\nThe second approach leverages the SMILES of molecules as inputs and employs sequence-based methods. Wang et al. Wang et al. [2019] proposed a two-stage transform-based model called SMILES-BERT. This model was pre-trained on a masked SMILES recovery task using large-scale unlabeled data, and later fine-tuned for different property prediction tasks, such as predicting molecular activity. Similar strategies have also been applied to reaction-level predictions, such as yield prediction, where the input consists of the concatenated SMILES strings of reaction components. Yield-BERT, developed by Schwaller et al. Schwaller et al. [2021b], was a pioneering work that utilized BERT Devlin et al. [2019] to encode reactions and make predictions. Later, their subsequent studies revealed that the performance could be further enhanced through data augmentations, achieved by permuting the order of reaction components Schwaller et al. [2020b]. Yin et al. Yin et al. [2024] extended this line of research by introducing an additional contrastive learning module into the BERT framework, leading to a more robust understanding of the input SMILES strings.\nFor prediction tasks, uncovering the specific relationships between molecular structures and chemical properties provides valuable insights into the underlying principles of chemistry. For example, Zheng et al. Zheng et al. [2019b] adopted the attention mechanism to identify these relationships. By analyzing the weights in the attention matrix, their proposed method can detect which segments in SMILES strings are prioritized and which are disregarded. This enables the identification of molecular fragments that influence specific properties. In their experiments for toxicity prediction, this approach successfully identified the functional groups, such as phosphoric acid esters, aliphatic halides, that contribute to toxicity. Recently, Wong et al. Wong et al. [2024] developed a Monte Carlo Tree Search (MCTS) based approach to identify substructures associated with high antibiotic activity. They first trained an ensemble of predictive models, provided by Chemprop package Heid et al. [2023], to predict the activity of a given substructure. The substructure was then iteratively pruned using MCTS, with deletions selected based on high prediction scores. The identified substructures offer promising avenues for exploring novel structural classes, and thus the method has the potential to play a pivotal role in developing antibiotics to address the growing resistance crisis."}, {"title": "4.2 Molecular Design", "content": "The discovery and design of molecules with desired properties is one of the fundamental tasks of chemical research. For instance, researchers aim to develop molecules that can selectively bind to specific biological targets or design catalysts with tailored structures. Early studies predominantly focused on virtual screening, where the properties are predicted for molecules within large chemical libraries, followed by experimental validation of the most promising candidates Shoichet [2004]. For example, Stokes et al. Stokes et al. [2020] performed virtual screening on more than 107 million molecules from several chemical libraries, which led to the discovery of the first AI-discovered antibiotic, \"halicin\". However, the effectiveness of virtual screening is inherently constrained by the coverage of chemical libraries. Expanding library size may improve coverage but significantly increases time costs for property prediction. To address these limitations, de novo molecular design has emerged as a compelling alternative. Unlike virtual screening, this approach does not rely on existing molecular libraries but instead generates novel molecules tailored to specific properties. Current methods for de novo molecular design can be broadly categorized into two main approaches: deep generative methods and combinatorial optimization methods (Fig. 7)."}, {"title": "4.2.1 Deep Generative Methods", "content": "Recent advances in deep generative models have yielded many exciting results Achiam et al. [2023", "2024": "making them an increasingly popular approach for de novo molecular design. Drawing from the advancements in neural language processing, a widely explored strategy is to generate SMILES sequences for molecules. For example, Segler et al. Segler et al. [2018a"}]}