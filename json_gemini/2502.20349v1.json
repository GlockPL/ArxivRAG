{"title": "Naturalistic Computational Cognitive Science", "authors": ["Wilka Carvalho", "Andrew Lampinen"], "abstract": "Artificial Intelligence increasingly pursues large, complex models that perform many\ntasks within increasingly realistic domains. How, if at all, should these developments in\nAl influence cognitive science? We argue that progress in Al offers timely opportunities\nfor cognitive science to embrace experiments with increasingly naturalistic stimuli, tasks,\nand behaviors; and computational models that can accommodate these changes. We first\nreview a growing body of research spanning neuroscience, cognitive science, and Al that\nsuggests that incorporating a broader range of naturalistic experimental paradigms (and\nmodels that accommodate them) may be necessary to resolve some aspects of natural\nintelligence and ensure that our theories generalize. We then suggest that integrating\nrecent progress in Al and cognitive science will enable us to engage with more natural-\nistic phenomena without giving up experimental control or the pursuit of theoretically\ngrounded understanding. We offer practical guidance on how methodological practices\ncan contribute to cumulative progress in naturalistic computational cognitive science, and\nillustrate a path towards building computational models that solve the real problems of\nnatural cognition\u2014together with a reductive understanding of the processes and principles\nby which they do so.", "sections": [{"title": "Introduction", "content": "Cognitive scientists build models to make our theories concrete \u2014 which offers testable\npredictions, eliminates ambiguities (Guest and Martin, 2021), and can reveal unexpected\nproperties of cognition and behavior (McClelland, 2009). Cognitive models can range from\nsimplified models of a high-level process (e.g. Frank and Goodman, 2012; Cohen et al.,\n1990), to task-performing models of a particular domain (Newell, 1973), or beyond (Newell,\n1994). Many cognitive paradigms\u2014connectionism (McClelland et al., 1986), bayesian infer-\nence (Tenenbaum and Griffiths, 2001), and cognitive architectures (e.g. Ritter et al., 2019;\nLaird, 2019)-\u2014have sought generalizable models that can explain a broad range of behavior\nfrom a simpler set of principles. Ultimately, these approaches could aspire to build towards\n\"unified theories of cognition\u201d that can explain and predict natural behavior across many\ntasks and domains (Newell, 1994). However, we are not there yet. In recent years there have\nbeen substantial debates about the generalizability of our theories (Yarkoni, 2022; Eckstein\net al., 2021), whether our models and theories are adequately constrained (e.g. Jones and\nLove, 2011; Rahnev and Denison, 2018), and calls to accommodate a broader scope of nat-\nuralistic phenomena (Wise et al., 2023; Cisek and Green, 2024). Our goal in this paper is to\nmotivate and outline a path that we believe will begin to address many of these challenges.\nThe path we propose is motivated by recent practical and conceptual developments in Al.\nIn the last ten years Al has progressed substantially in building models that can perform a\nwide range of tasks in various domains. We now have vision models that can classify and\nsegment real-world images in objects without external supervision (He et al., 2020; Caron\net al., 2021), we have reinforcement learning models that can learn new tasks in a domain\nwith human-like efficiency (Team et al., 2023), and we have language models which can\nproduce human-like language and solve many language-specified tasks (Radford et al., 2019;\nWei et al., 2022). Surprisingly, such models produce internal representations that capture\nmany features of brain representations (Yamins et al., 2014; Schrimpf et al., 2021). These\nfindings have led to discussion of how such complex models can be explanatory (Cao and\nYamins, 2024a), and how they should alter our theories (Hasson et al., 2020; Perconti and\nPlebe, 2020; Piantadosi, 2023). Yet they have also led to debate over how we test our models\nand interpret the results (Bowers et al., 2023; DiCarlo et al., 2023; Dentella et al., 2024).\nThus, it remains an open question if and how the progress in Al can contribute to addressing\nthe challenges of cognitive science.\nIn this work, we attempt to weave a thread of arguments that synthesizes these literatures,\nspanning from cognitive science's motivation and theory development to the practicalities\nof model engineering and reproducibility. We articulate a research strategy that we believe\nwill be helpful in enabling theory-driven cognitive science to achieve deeper understanding\nof the full range of natural intelligence. We present an overview of this framework and our\narguments in Figure 1.\nOur paper proceeds as follows. First, in \u00a72, we unpack what we mean by \u201cnaturalistic\u201d\ncomputational cognitive science, by providing a pragmatic outline of the features that we\nbelieve deserve more emphasis in our experimental paradigms and models. In particular, we\nargue that researchers should expand their task paradigms to to more closely approximate the\nbreadth of the settings in which their theoretical constructs would be expected to generalize by\nboth adding relevant variables that may interact with those in question, and by incorporating\nmore variability within existing parameters. Likewise, researchers should expand their models"}, {"title": "What is \u201cnaturalistic\u201d computational cognitive science?", "content": "Naturalistic computational cognitive science is a research strategy for theory-driven cognitive\nscience that aims to predict human behavior across increasingly naturalistic stimuli and tasks.\nThis approach gradually increases the ecological validity of experimental designs while ensuring\nmodels can accommodate both simplified and more natural conditions. As demonstrated in\nmachine learning (\u00a75), this strategy has enabled models that can work with real-world data,\nalbeit imperfectly."}, {"title": "The benefits of naturalistic experimental settings", "content": "In this section, we outline the benefits of naturalistic experimental settings for cognitive\nscience. Specifically, we argue that naturalistic experimental settings can engage mechanisms\nthat are qualitatively different than those engaged in simpler settings \u2014 and thereby change\nthe scientific inferences we draw. We review examples where naturalistic experimental settings\nhave changed our understanding of a system, and have helped to disentangle competing\nmodels that simpler settings could not.\nAs an orienting conceptual example, imagine that we are doctors studying heart function,\nand we chose to only study it while the participants are lying down and resting. We could\ncertainly learn interesting things thereby; the resting setting offers high test-retest reliability,\nand a highly-controlled environment to study the processes like how respiratory cycles affect\nheart rate. However, it offers this control and reliability precisely by removing the many\nfactors of natural variation that interact with heart rate, and indeed for which heart function\nevolved \u2014 such as rapid adaptation to movement, stress, etc. Studying heart function across\na broader range of naturalistic settings would be necessary to understand why the system is\nthe way it is, and how all the physiological (and psychological) processes involved interact.\nFor example, if we experimentally manipulated breathing, but did so across a range of different\nnaturalistic activities (sitting, speaking, running, etc.) we would more effectively determine\nhow breathing affects heart function, and how that effect depends on the state of the overall\nsystem. That is, by studying phenomena across a range of naturalistic settings we elaborate\nour understanding of the system, and can build more complete theories of its function."}, {"title": "Behavior can be different in naturalistic experiments", "content": "The starting point for many cognitive analyses is behaviors on a task designed to isolate\nsome cognitive capability. However, there are many cases in which task-relevant behavior\ncan be altered by seemingly-orthogonal features of the task\u2014thus raising the risk that our\ntheories will be overfit to a restricted task setting unless we also consider a broading range\nof variation. In this section, we review some examples illustrating changes in behavior or\ncapabilities in naturalistic settings.\nExample 1: moral behavior can change when moving from textual vignettes to virtual\nreality. Recent research has shown that behavioral choices can change when moving from\ntext to virtual reality as it can mark a shift from moral judgment (i.e. deciding whether\nthings are moral) to moral action (Francis et al., 2017). When presented with the classic\ntrolley dilemma in text form, only 40% of participants say they would pull a switch to divert\na trolley that would kill one person to save five others. However, when the same scenario\nis presented in virtual reality where participants must simulate the action, 55% choose to\npull the switch. This difference becomes even more pronounced in the \u201cfootbridge\u201d variant\nof the dilemma. In the text version, only 10% of participants say they would push someone\noff a bridge to save five others. Yet when able to simulate this action in virtual reality, 65%\nof participants choose to push the person. These findings demonstrate that people's stated\nmoral preferences in hypothetical scenarios can differ substantially from their actions in more\nnaturalistic settings.\nExample 2: memory performance differs between naturalistic search and explicit mem-\norization. When participants are explicitly instructed to memorize objects in a 3D home\nenvironment, their subsequent recall accuracy was significantly lower than when they inciden-\ntally encounter the same objects during visual search tasks (Helbing et al., 2020). This effect\nextends to spatial memory as well. Not only do participants better remember the identity of\nobjects encountered during search, they also show more accurate memory for object loca-\ntions-placing objects closer to their original positions when they were initially seen during\nsearch versus explicit memorization. These findings demonstrate that tasks focused explicitly\non memory may not capture all aspects of how memory naturally operates during everyday\nbehavior. Additionally, it suggests that computational models of memory may need to be\nrevised to account for these differences in naturalistic settings. This is supported by research\nshowing that people display detailed memory of object and spatial information in real-world\nscenes (Bainbridge et al., 2019).\nExample 3: mice can learn 1000\u00d7 faster during natural behavior, compared to two-\nalternative forced choice tasks. Recent research has shown that learning rates can dra-\nmatically differ between simplified laboratory tasks and more naturalistic settings (Rosenberg\net al., 2021). When mice are tested in traditional two-alternative forced choice (2AFC)\ntasks, they typically require 10,000 trials over 3-6 weeks of training to reach asymptotic per-\nformance of only about 67% accuracy. However, when allowed to freely explore a labyrinth"}, {"title": "Neural systems can operate differently under naturalistic conditions", "content": "In addition to behavioral differences like those reviewed above, naturalistic conditions can\nresult in changes to neural coding and computation. Thus, in order to arrive at a complete\nunderstanding of neural function, we need to consider the responses of the system across\na range of simpler and more naturalistic settings. In this section, we highlight some exam-\nples of different computations or computational roles that arise when moving from standard\nparadigms to more naturalistic ones.\nExample 1: the model of superior colliculus shifted from a model of eye movement to\none of integrating sensory inputs for body control. Cisek (1999) suggests that the focus\non computational behavior as an input-output mapping neglects the fundamental fact that\nnatural intelligence evolved not to produce single responses, but for closed-loop control in an\nenvironment. This setting yields a rather different interpretation of the system's representa-\ntions and processes. More generally, many researchers have argued that cognition cannot be\nunderstood completely outside its embodiment and the environment in which cognitive pro-\ncesses are instantiated (e.g., Newen et al., 2018). For example, Cisek and Green (2024) ar-\ngues that as we increased naturalism from head-fixed to head-free to body-free settings when\nstudying monkeys, we expanded our theory of superior colliculus from controlling saccadic eye\nmovement to generally integrating multimodal cues to guide bodily action-selection (Cisek\nand Green, 2024). That is, as we increased the naturalism of the experimental conditions\nthat we used to study monkeys, we arrived at a more complete model of superior colliculus.\nExample 2: visual processing systems operate differently during natural viewing com-\npared to passive viewing paradigms. Traditional research has shown that one of the earliest\nvisual responses in the brain (known as P100/M100) occurs about 100ms after a person's\neyes land and fixate on a new location (Vinje and Gallant, 2000). However, this finding comes\nprimarily from experiments where participants passively view images while keeping their eyes\nstill. In contrast, natural vision involves actively moving our eyes multiple times per second\nto sample information from different parts of a scene. When the researchers examined brain\nresponses during this more natural active viewing, they found that the P100/M100 response\nactually begins when the eye movement (saccade) starts, not when it ends at the new fixation"}, {"title": "Naturalistic experimental paradigms can expose computational chal-lenges that engage mechanisms differently", "content": "Finally, naturalistic paradigms can likewise introduce challenges that are important to un-\nderstanding functioning of the system as a whole. In this section, we highlight examples\nwhere the challenges posed by increasing naturalism can engage computational mechanisms\ndifferently\u2014which can be useful for disentangling underlying mechanisms, and is important\nfor achieving generalizable understanding.\nExample 1: Different learning mechanisms only yield distinct predictions under work-\ning memory load. Overly-simplified tasks can yield aliasing of different solutions, where\nmany different computational approaches yield the same behavior. Collins (2024) presents\nan example, by showing that what seems superficially to be implemented as a standard rein-\nforcement learning algorithm, on closer examination may instead be implemented through a\ncombination of working memory and simpler value-free associative learning.\u00b9 However, the\ndifference between these two implementations cannot be identified in standard task settings,\nas Collins notes: \u201cEven simple tasks designed to elicit a target process (such as bandit tasks\nfor RL) recruit multiple other processes, but those processes may be unidentifiable in such\ntasks. Disentangling multiple processes requires considering more complex tasks to elicit dif-\nferentiable behavior.\u201d The more complex tasks in question simply increase the stimulus set\nsize within learning blocks to a handful of objects, rather than restricting to two or three\u2014i.e.,\n\u00b9Whether this alternative counts as an alternative implementation of RL or not is orthogonal to our argument;\nthe important point is that the two models yield distinct predictions in some regimes but not others."}, {"title": "The (surprising) benefits of learning with naturalistic data", "content": "The standard lesson in science is that we must simplify an experimental setting so that we can\nbetter arrive at a causal conclusion. However, we have already seen examples illustrating that\nmore is different (Anderson, 1972) \u2014certain properties of intelligence may only emerge in more\n\"complicated\u201d naturalistic settings. In this section, we detail examples where naturalistic data\nitself seems to play an importance role in producing the empirical phenomena of intelligence\nthat we wish to study.\nMany of the findings we will discuss here will be modern results from Al. One common trend\nis that there is a positive interaction between learning-based systems and naturalistic data\u2014\nlearning-based systems can accommodate naturalistic data, and reciprocally, learning from\nnaturalistic data results in qualitatively better results than learning in simpler settings. While"}, {"title": "Learning enables models that can operate on natural data", "content": "Perhaps the first surprising benefit of (deep) learning is that it enables a system to operate\nover natural data. Until AlexNet (Krizhevsky et al., 2012), the prevailing wisdom was that\none had to design useful representations of natural data for models. However, AlexNet and\nsubsequent work showed that a model could learn useful representations for naturalistic data\nsimply by being trained to make predictions about this data. Surprisingly, these models learn\nrepresentations that have notable correspondence to the internal representations in visual\ncortex (Yamins et al., 2014; Khaligh-Razavi and Kriegeskorte, 2014). Here, we review some\nof this work and how learning-based methods can enable more flexible cognitive models that\noperate on natural data.\nLearning-based vision models can operate on natural data and predict neural data (sur-\nprisingly) well. There is a rich history of learning-based methods revealing how neural net-\nworks can develop internal representations from pixel-based images that strongly correspond\nto neural data. A seminal example comes from Olshausen and Field (1996), who showed that\nV1-like Gabor filters could emerge from a convolutional neural network optimized to transmit\ninput while minimizing overall activation (essentially, a sparse autoencoder). Prior to this, V1\nactivations were modeled using analytically defined Gabor filters. This breakthrough demon-\nstrated that V1-like receptive fields could arise naturally from optimization to environmental\nstatistics of edges at varying frequencies and orientations. Building on this foundation, Lee\net al. (2007) demonstrated that V2-like convolutional filters could emerge from unsupervised\noptimization of sparse deep belief networks. A significant advance came when Yamins and\nDiCarlo (2016) showed that task-optimized deep convolutional neural networks trained for\nobject classification developed intermediate layers that matched neural representations in V4\nand IT better than any previous models. Recent research has revealed an even more striking\nfinding: these task-optimized networks represent images in a latent space whose geometry\nhas remarkable correspondence to brain topography (Doshi and Konkle, 2023). When fitting\na 2D sheet to image representations such that neighboring points correspond to nearby points\nin the original high-dimensional space, the resulting sheet shows striking similarities to actual\nbrain topography. Thus, learning-based approaches, particularly neural networks, not only\nenable effective visual processing but also provide surprisingly accurate predictions of brain\norganization and function."}, {"title": "Learning from naturalistic data can improve generalization", "content": "Studying how systems generalize is a key method in cognitive science and Al. Conceptual\narguments often rely on empirical studies of generalization; however, these generalization\nproperties are often studied in simple settings, with minimal models learning from minimal\nfeatures (e.g. Marcus, 1998). A fundamental assumption underlying this approach is that\nthe simplifications do not alter the generalization problem. However, here we review studies\nfrom Al and neuroscience showing that the variability of experience according to seemingly-\northogonal variables can alter how systems learn and generalize\u2014thus implying that studying\nmodels within more restricted settings may be misleading about the naturalistic computational\nproblem.\nExample 1: Compositional generalization of image classifiers vs. agents. Naturalistic\ntasks can fundamentally change what models learn and how they generalize. Hill et al. (2020)\ntrained two models to do a vision-language grounding task, and tested their compositional\ngeneralization to held-out language instructions. Both models were trained on the same\nlanguage examples, and tested on the same held-out examples. However, one model was\ntrained as an agent that interacts with a simulated environment, while the other was a simple\nimage-language classifier trained on screenshots from that environment. The authors found\nthat the interacting agent exhibited perfect compositional generalization to the novel language\nutterances, while the image classifier was substantially worse in novel settings. The authors\nalso explored a range of other settings, including generalization benefits of more naturalistic\n3D (rather than 2D) environments, or egocentric embodiments. Their results illustrate how\nricher, more naturalistic settings can enhance the generalizability of the solutions a system\nlearns. Thus, if we are interested in understanding how a system generalizes, we may need"}, {"title": "Learning with naturalistic data can yield good performance acrossa range of seemingly disparate tasks", "content": "One interesting finding from the deep learning literature is that when deep learning archi-\ntectures with many parameters are trained with a lot of naturalistic data and an appropriate\n\"basic\" learning objective, these architectures can develop mechanisms that go beyond the\nlearning objective they were trained on. Crucially, these learning paradigms can allow models\nto transfer well (through initial performance or accelerated learning) on novel tasks beyond\nthe training distribution.\nHuman learning may likewise benefit from transfer among the disparate tasks we learn. We ar-\ngue that these Al findings motivate computational cognitive science research studying whether\n\"down-stream human behavior\u201d on a set of tasks can be recapitulated by a model which is\ntrained on a large set of naturalistic tasks or stimuli representative of some of aspect of\nhuman experience\u2014as in meta-learning approaches (cf. Wang, 2021). Below, we provide ex-\namples of this kind of transfer spanning computer vision, reinforcement learning, and natural\nlanguage processing.\nExample 1: Computer Vision. One of the earliest successes from deep learning came in\ncomputer vision. Soon after AlexNet (Krizhevsky et al., 2012) achieved strong results on\nthe ImageNet dataset (Deng et al., 2009), researchers showed that the features discovered\nby AlexNet could be repurposed to novel tasks ranging from scene recognition to medical"}, {"title": "Learning from naturalistic data allows us to ask new questionsabout the origins of knowledge", "content": "The points outlined above have an important consequences; using models that learn from\nnaturalistic data can change our theoretical conclusions in cognitive science. For example,\nif a model fails to generalize when trained on simple data, we cannot be sure if the model"}, {"title": "Building generalizable models", "content": "ML research has excelled in developing generalizable models. While we focus on their utility for\nworking on naturalistic stimuli and tasks, we believe cognitive science would broadly benefit\nfrom improved practices for developing generalizable models. In this section, we highlight\nthree key practices of empirical ML research that we believe computational cognitive science\ncan adopt. The first is frictionless reproducibility: the practice of developing research artifacts\nwhich can be reused, and repurposed with minimal effort (\u00a75.1). The second is a focus on\ndeveloping generalizable models that can learn successfully across many datasets (\u00a75.2). We\nargue that these two offer the foundation for having groups of research collectively work on\nimportant research problems through the utilization of benchmarks (\u00a75.3).\nBefore we continue, we acknowledge that modern empirical ML is a young field which, like\nother fields, has faced challenges in rigor and reproducibility (Melis et al., 2017; Lucic et al.,\n2018; Riquelme et al., 2018; Henderson et al., 2018; Recht et al., 2019; Agarwal et al.,\n2021). Despite these challenges, we are optimistic that some of the practical knowledge ML\nhas developed is valuable for improving rigor and reproducibility, as we highlight below.\nSection summary. We envision a future where cognitive scientists identify a cognitive phe-\nnomenon of interest (e.g., theory of mind, working memory, or object recognition) and create"}, {"title": "Frictionless reproducibility: a superpower", "content": "Within cognitive science and psychology, there is currently substantial friction when using data\nor code of prior work (Nosek et al., 2015; Hardwicke et al., 2018). Researchers may not share\ndata in a standard or easy to use format, often don't fully report their analysis methods, or\nprovide code that contains errors (Poldrack et al., 2017). In a large-scale replication analysis\nof open code and data, researchers found that 38% of code was not usable and only 31%\nof workflows had reproducible results (Hardwicke et al., 2018). These factors may have\ncontributed to the replication crisis (Lilienfeld, 2017; Shrout and Rodgers, 2018).\nIn recent years, data-centric sciences are experiencing a profound transition to what David\nDonoho calls \u201cfrictionless reproducibility\" (Donoho, 2024; cf. Recht, 2024). Donoho identi-\nfies three key pillars to frictionless reproducibility: data sharing, code sharing, and competitive\nchallenges. We will focus on the first two in this section, and competitive challenges in \u00a75.3.\nCritically, with frictionless reproducibility, the research artifacts that are produced allow fu-\nture researchers to exactly re-execute the same complete workflow with minimal effort. With\nthis, good ideas are spread, adopted, and improved with little friction. Donoho argues that\nML is ostensibly the most successful at frictionless reproducibility. Below we review some\nsupporting strategies that may aid cognitive science.\""}, {"title": "Recommendations for cognitive science", "content": "R1. Use a standardized evaluation protocol where models can be compared via a com-\nmon interface. Machine learning's foundation of standardized evaluation protocols traces\nback to 1959, when Highleyman created the first alphanumeric dataset and the first ever\n\"train-test\u201d split for comparing pattern recognition (Highleyman and Kamentsky, 1959).\nThis strategy continued with the famous MNIST alphanumeric dataset (LeCun et al., 1998).\nMNIST simplified data-sharing and evaluation by pre-processing data, fixing the train and\ntest distributions, and sharing the data online. This standardization and usage of a common\ninterface enabled fair comparisons between diverse approaches, from boosted stumps (K\u00e9gl\nand Busa-Fekete, 2009) to support vector machines (Cristianini and Scholkopf, 2002) to\nnearest neighbor methods (Khan, 2017).\nStandardized evaluation protocols serve a critical function: mitigating personal bias in model\nassessment. The evolution from MNIST through Caltech 101 (Fei-Fei et al., 2004) to Im-\nageNet (Deng et al., 2009) demonstrates this principle, as increasingly naturalistic datasets\nenabled fair comparisons between diverse model families. This enabled AlexNet (Krizhevsky\net al., 2012) to validate deep learning in an era when many researchers doubted its potential.\nThus, we recommend that cognitive scientists establish clear hold-out data for either model\ndevelopment or data analysis. This is especially critical for confirming exploratory results (Pol-\ndrack et al., 2017). Combined with standardized evaluation protocols, this will enable fair\ncomparison between diverse theoretical frameworks ranging from program synthesis to deep\nneural networks to reinforcement learning and active inference.\nR2. Make data accessible with a single line of code. Fully processed research data\nshould be programmatically accessible with a single line of code, requiring no permissions.\nWhile uploading data to platforms like the Open Science Framework (OSF) is valuable, we\nadditionally recommend uploading data to platforms like Hugging Face that offer version\ncontrol, free hosting, authentication-free access, the ability to download data with one line\nof code, and to upload data with a few lines of code. This removes friction from reusing and\nupdating datasets.\nR3. Adopt the \"single file\" philosophy. Ideally, key workflows (e.g. model or evaluation\ndefinititions) should be defined in a single file. This has emerged as a powerful force for\nreproducibility in ML research (see Table 1 for examples). While this approach contradicts\ntraditional software engineering principles of modularity and abstraction, it offers distinct ad-\nvantages for research code: (1) complete understanding of an important component (e.g.\nhow a model works) requires reading only one file, (2) minimal abstraction layers and depen-\ndencies, and (3) easy adaptation of either complete implementations or specific components.\nOne can simply copy this single file into another codebase. Removing friction from future\nreuse also benefits us\u2014as we can see from Table 1, these methods get a large number of\ncitations."}, {"title": "Developing models with an eye towards generalizability", "content": "Task-performing computational models have been a goal of cognitive science since Newell\n(1973). This sentiment still exists today (Kriegeskorte and Douglas, 2018; Almaatouq et al.,\n2024) and is exemplified in conferences that explicitly seek to integrate cognitive science and\nneuroscience with Al (Naselaris et al., 2018). Cognitive scientists are well aware that they\ncan't rely on a single paradigm for the studying an effect if they want to achieve generaliz-\nable conclusions (Holzmeister et al., 2024; Yarkoni, 2022). Ideally, the models we develop\ngeneralize beyond the setting (e.g. stimulus set or task-specification) they were designed\nfor. This requires two things. First, that the model can operate over new stimulus sets and\ngeneralize its behavior to new tasks. Second, that the behavioral (and potentially neural)\npredictions made by our models capture human behavior on new tasks and new stimulus sets.\nClearly, the first is a requirement for the second. Here, we argue that machine learning, with\nits emphasis on frictionless reproducibility, has made significant strides in developing models\nthat can generalize. We detail recommendations for cognitive science below."}, {"title": "Recommendations for cognitive science", "content": "R4. Develop models with simplified stimluli but ensure they work with increasingly\nnaturalistic stimuli. The frame problem (Shanahan, 2004) and the \u201csimulation is doomed to\nsucceed", "development": "nthe evaluation tasks we design often fail to capture the full complexity of the phenomenon\nwe care about. Empirical machine learning addresses this by developing models that must\nsucceed in both simplified and increasingly complex settings. Below we detail two examples.\nExample 1: Generative Models. One of the most successful models of machine learning\nhas been the variational autoencoder (Kingma, 2013), a generative model that learns to\ninfer latent variables which describe an observation. When this model was released they\nshowed results inferring latent variables correctly on both \u201csimplified\u201d MNIST digits and more\ncomplex images of real-world faces. Importantly, in this more complicated setting where the\nlatent dimensions did not have obvious correspondence, they performed a qualitative analysis\nshowing their model had learned a face manifold capturing semantically meaningful dimensions\nof faces such as sentiment and facial direction.\nExample 2: Reinforcement Learning. The history of RL is one of progress towards increasing\nnaturalism: initial environments were grid-worlds (Sutton and Barto, 2018), later 2D video\ngames like atari (Bellemare et al., 2013), then virtual 3D home environments (Szot et al.,\n2021; Li et al., 2021b), and now an emphasis on open-world environments (Fan et al., 2022;\nMatthews et al., 2024; Hughes et al., 2024). Despite the increased emphasis on naturalistic\nsettings, there is still emphasis on the importance of toy settings for studying specific aspects\nof your model (Osband et al., 2019; Obando Ceron et al., 2024). Indeed, the strategy is\nalmost always to first develop your model for simplified settings, but one should always then\nsee that your model continues to \u201cwork\u201d with more naturalistic settings. Collectively, this\nhas allowed RL researchers to both advance a theoretical understanding of how these models\nwork (Dadashi et al., 2019; Grimm et al., 2020; Lyle et al., 2022), an empirical understanding\nof how they work (Obando Ceron et al., 2024; Farebrother et al., 2024), but also develop\nperformant models for a wide range of \u201ccomplex\u201d settings including beating the world's best\nplayers in games like Go (Silver et al., 2017) and controlling robots in the real world (Levine\net al., 2016; Cheng et al., 2024).\nR5. Develop models that work with many stimulus sets. The ultimate goal of ML\nhas always been generalization (Highleyman and Kamentsky, 1959; Hardt and Recht, 2022;\nRecht, 2024). Initially, this meant models that generalize to new data. Now, it means\nboth models and learning procedures that generalize to new settings. Thus, the practice\nof evaluating models across diverse datasets has become foundational in machine learning.\nThis has manifested with research papers where ML models are evaluated on numerous\ndatasets to showcase their generality. Early examples include", "Deep Q-Network": "npaper (Mnih et al., 2015), which showed that a single neural network architecture with fixed\nhyperparameters\u00b3 could master 50 different Atari games. The field has since evolved to\ngroup environments by the cognitive capabilities they test (e.g., exploration, generalization,\nmanipulation) (Patterson et al., 2023), with researchers developing models on one set of\ntasks and validating on entirely different ones to ensure robust generalization (Machado\net al., 2018).\n3Hyperparameters are parameters that are set for a model and algorithm before training, such as the number\nof layers in a neural network or the learning rate of the optimizer"}, {"title": "Benchmarks: catalysts for progress and innovation", "content": "Another challenge that cognitive science faces is fragmentation. This has been true since\nNewell's seminal critique (Newell, 1973), where his observation that piece-wise investigation\nof brain components would not yield a cohesive understanding remains relevant forty years\nlater (Schrimpf et al., 2020; DiCarlo et al., 2023). Current experimental paradigms often\nproduce disconnected, incommensurate findings that resist integration across studies, even\nwithin the same domain (Almaatouq et al., 2024). This fragmentation manifests not only in\nmethodology but also in how researchers distribute their attention across different problems,"}, {"title": "Recommendations for cognitive science", "content": "R7. Embrace the leaderboard principle. A common concern with benchmarks is that re-\nsearchers will \u201coverfit\u201d to test sets, which theoretically should underestimate true test error\n(Hastie et al., 2009). Yet despite widespread iteration on benchmarks in machine learning\u2014\na practice that ostensibly violates statistical principles\u2014the field has produced models with\nstrong performance on held-out data. This apparent paradox is partially explained in a recent\nmachine learning textbook (Hardt and Recht, 2022) by a phenomenon known as the \u201cleader-\nboard principle\u201d. Researchers typically only publish and build upon models that demonstrate\nsubstantial improvements over prior state-of-the-art results, rather than minor variations.\nThis selective pressure towards meaningful advances effectively constrains the degree of adap-\ntation to test sets, as researchers focus on substantial improvements rather than exhaustively"}]}