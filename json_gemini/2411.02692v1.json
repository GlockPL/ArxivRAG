{"title": "JPEC: A Novel Graph Neural Network for Competitor Retrieval in Financial Knowledge Graphs", "authors": ["Wanying Ding", "Santosh Chikoti", "Manoj Cherukumalli", "Vinay K. Chaudhri"], "abstract": "Knowledge graphs have gained popularity for their ability to organize and analyze complex data effectively. When combined with graph embedding techniques, such as graph neural networks (GNNs), knowledge graphs become a potent tool in providing valuable insights. This study explores the application of graph embedding in identifying competitors from a financial knowledge graph. Existing state-of-the-art(SOTA) models face challenges due to the unique attributes of our knowledge graph, including directed and undirected relationships, attributed nodes, and minimal annotated competitor connections. To address these challenges, we propose a novel graph embedding model, JPEC(JPMorgan Proximity Embedding for Competitor Detection), which utilizes graph neural network to learn from both first-order and second-order node proximity together with vital features for competitor retrieval. JPEC had outperformed most existing models in extensive experiments, showcasing its effectiveness in competitor retrieval.", "sections": [{"title": "1 INTRODUCTION", "content": "Competitor retrieval is one of the most crucial use cases for financial organizations. Traditionally, it is mostly driven by multiple manual tasks involving collecting data and converting factors like revenue, products, pricing, marketing, and industry distributions. While manually gathered market data offer vital insights, they have limitations in terms of applicability and scalability. On the other hand, knowledge graphs can provide competitive clues by revealing meaningful connections, such supply-chain, between companies. Combined with graph embedding techniques, knowledge graphs can offer a structured and efficient approach for automatic and intelligent competitor retrieval. However, most SOTA graph embedding methods are sub-optimal for our task due to the complex structure of a real world knowledge graph(described in Section2). This paper introduces a novel graph neural network, JPEC, for competitor detection from a financial knowledge graph with various types of edges but limited labeled data."}, {"title": "2 PROBLEM AND PROPOSED METHOD", "content": ""}, {"title": "2.1 Problem Definition", "content": "In our knowledge graph G = (V, S, C, X), each node in the node-set V represents a real-world company, and X contains attributes associated with each node. The directed edge set S signifies supply chain connections between companies, while the undirected edge set C denotes mutual competitor relationships. Notably, our knowledge graph lacks numerous competitor edges, resulting in a significantly smaller volume for C compared to S. Our objective is to leverage the limited competitor edges, combined with the extensive company node attributes and supply chain graph structure, to identify additional competitors for a given company."}, {"title": "2.2 Model Description", "content": "We introduce JPEC (shown as in Figure1) model for competitor detection. This model leverages two orders of proximity for effective competitor pattern capture."}, {"title": "2.2.1 First Order Proximity: Laplacian Eigenmap on Competitor Network", "content": "Although competitor edges are sparse in our graph, learning from these connections is natural and essential. We employ first-order proximity to characterize the local connection and use it as supervised information that constrains the similarity of latent representations between pairs of competitors[10]. The weight $w_{ij}$ along the competitor edge between node i and j is 1 if they are known competitors, -1 if they are non-competitors, otherwise 0. We applied Laplacian Eigenmap[1] to enforce the first proximity. Laplacian Eigenmap aims at constructing a Laplacian matrix L to ensure nodes connected as competitors stay as close as possible after embedding. We designed two Laplacian Eigenmaps to learn from positive samples (Equation 1) and negative samples (Equation 2) respectively. It is worth noting that Laplacian Eigenmap only accepts non-negative weights, so we used the opposite edge weight for non-competitors, -$w_{ij}$, in Equation 2.\n$L_{pos} = \\sum_{i,j=1}^{N} w_{ij}||y_i - y_j|| = 2tr(Y^TL^+Y)$ (1)\n$L_{neg} = \\sum_{i,j=1}^{N} -w_{ij}||y_i - y_j|| = 2tr(Y^TL^-Y)$ (2)\nwhere $L^+$ is the Laplacian matrix for nodes in positive samples, and $L^-$ is for nodes in negative samples, tr() is the trace of a matrix, and Y, indicating the node embeddings. We'll elaborate on how to generate Y in the next section.\nFinally, we utilized a pairwise ranking loss function (Equation 3) to minimize the distance between positive pairs and simultaneously maximizing the distance between negative pairs, where m denotes the margin, which is a hyper-parameter controlling the desired separation space between positive and negative pairs.\n$L_{1st} = L_{pos} + max(0, m \u2013 L_{neg})$ (3)"}, {"title": "2.2.2 Second Order Proximity: Directed GCN Autoencoder on Supply Chain Network", "content": "In this section, we explain how we obtain node embeddings Y for the aforementioned objective function. Since each node has associated attributes, GCN is a straightforward option to utilize and learn graph structure and attributes simultaneously. GCN is naturally designed for undirected graphs, and we change the GCN's propagation function $\u010e^{-1/2}\u00c3\u010e^{-1/2}$ to $\u010e^{-1}\u00c3$, to apply it into a directed supply-chain graph[8, 9]. By changing the normalization function, the propagation rule of GCN can be rewritten as Equation 4\n$\\Upsilon^{(l+1)} = \\sigma(\\tilde{D}^{-1}\\tilde{A}\\Upsilon^{(l)}W^{(l)})$ (4)\nwhere A = A + I is the adjacency matrix. I is the identity matrix, \u010e is the degree matrix, and $W^{(l)}$ is layer-specific trainable weight matrix, $\\sigma(\\cdot)$ denotes an activation function. $\\Upsilon^{(l)} \\in R^{N \\times D}$ is hidden representations in the $l^{th}$ layer.\nSince many competitor edges are missing in our graph, a decoder is necessary to enhance the model's ability to extract information from the supply chain graph. Since GCN is a Laplacian smoothing process, we employ a Laplacian sharpening process[6] to reverse the encoding process. Similarity, we made adaptions to accommodate the characteristics to a directed graph(shown as Equation 5).\n$\\Upsilon^{(m+1)} = \\sigma((\\tilde{D}^{-1}\\tilde{A})\\Upsilon^{(m)}W^{(m)})$ (5)\nThe loss function for the second order proximity is to minimize the difference between the original node feature vectors and the reconstructed ones, which can be formulated as Equation 6:\n$L_{2nd} = ||X \u2013 \\hat{X}||^2$ (6)\nThe ultimate objective function of our model integrates the loss function derived from both the first-order and second-order proximity, and can be mathematically represented as Equation 7.\n$L = L_{1st} + \\beta L_{2nd} + \\lambda W^2$ (7)\nwhere \u03b2 is a hyper parameter to balance the first-order and second-order losses, $ \\lambda W^2$ is the regularization term."}, {"title": "3 EXPERIMENTAL SETTINGS", "content": ""}, {"title": "3.1 Datasets", "content": "We possess a large-scaled financial knowledge graph offering a comprehensive overview of financial entities (e.g., companies, investors, bankers) and their relationships (e.g., supply chain, investment). From this knowledge graph, we extracted a subgraph G consisting of 133, 812 company nodes and 611, 812 associated supply chain edges. Within this sample graph, 39, 755 nodes, equivalent to 30% of the total nodes, have competitors, resulting in a total of 213, 071 competitor edges. To facilitate a comprehensive experiment, we generated two evaluation datasets: a regular test dataset R and a zero-shot test dataset Z."}, {"title": "3.1.1 Zero-Shot Test Dataset Preparation", "content": "With 70% of companies' competitors absent in our knowledge graph, assessing a model's generalization capability for predicting competitors not in the graph becomes crucial. We selected a subset of 7, 952 nodes (20% of all nodes) and extracted COMPETE_WITH edges around them. We then removed all COMPETE_WITH connections between these nodes and the rest of the graph to ensure these nodes are unseen in the training competitor data. But their supply chain relationships are retained and utilized for competitor prediction. In testing, we exclusively consider companies with a minimum of 5 competitors, yielding a subset of 201 companies."}, {"title": "3.1.2 Regular Test Dataset Preparation", "content": "We performed random sampling on the remaining dataset after creating the zero-shot test dataset. In contrast to the zero-shot test dataset, the regular test dataset retains all nodes but randomly removes some COMPETE_WITH edges from the graph. It worth noting that a node is still visible in the training competitor data since it still has other competitors in addition to the removed ones. We extracted 20% of the edges out as test data. Similarly, we curated a subset of companies with a minimum of 5 competitors, resulting in a final set of 795 companies for the regular test.\nWe randomly designated certain unconnected nodes in the competitor network as non-competitors. This method may introduce errors since some sampled nodes may be competitors with missing COMPETE_WITH edge. Nevertheless, we are dedicated to refining our negative sampling methodology to mitigate such issues."}, {"title": "3.2 Tasks and Evaluation Metrics", "content": "In the context of competitor retrieval, our objective is to identify the competitors of a given company. We compare models' performances with three ranking metrics. Hits calculates the rate of correct items appearing in each instance list's top K entries. MRR stands for Mean Reciprocal Rank. It tries to measure where the first correct item is in the predicted list. MAP stands for Mean Average Precision, which measures the average precision across a set of items."}, {"title": "3.3 Baselines and Settings", "content": "We employ two types of baselines: Human and Graph Embedding. For the Human baseline, we collaborated with our business team to manually detect competitors from the knowledge graph, translating their expertise into proprietary graph queries. This approach, constrained to utilizing information solely within the knowledge graph, yielded satisfactory outcomes as confirmed through a review of sample results. Regarding graph embedding, we evaluated various methods, including structure embedding (Node2Vec, Metapath2Vec, TransE, and SDNE), attributed embeddings (GCN, GAT, DGCN, and DGAE), and hetergeneous methods (HAN and KBGAT)."}, {"title": "4 RESULTS AND DISCUSSIONS", "content": ""}, {"title": "4.1 Default GCN vs. Directed GCN", "content": "Before examining the model performance on two tasks, we briefly validate the directed GCN used in this paper. We implemented three methods: GCN, GAT, and DGCN. Figure 2 shows DGCN's superior performance compared to the others. GCN's limitations stem from its reliance on symmetric adjacency matrices, while GAT performs better than GCN but worse than DGCN due to it neglects of edge directions, potentially causing errors."}, {"title": "4.2 Discussion on Competitor Retrieval", "content": "Competitor Retrieval is a very important task in our business operations. Given a company, our goal is to retrieve a roaster of potential competitors. Figure 3 displays the performance of each model evaluated on our regular test datasets, whereas Figure 4 exhibits the performance on our zero-shot test dataset. The red solid line in each figure represents the performance of human queries. The to-be-ranked candidate pool is all the 133, 812 companies."}, {"title": "4.2.1 Regular Testing", "content": "Figure 3 shows most machine learning-based methods outperforming human queries, highlighting graph topology's importance in competitor detection. HAN's limitations stem from significant information loss due to its extraction method, while DGCN, DGAE, and KBGAT excel by considering node attributes. KBGAT's shortcomings arise from noises accumulated from n-hop information aggregation, whereas DGAE's performance is limited by its unlearnable decoder. JPEC and TransE rank high in identifying competitors, but TransE struggles to prioritize true competitors over non-competitors in terms of much lower MRR and MAP."}, {"title": "4.2.2 Zero-Shot Testing", "content": "Figure 4 tests each model's generalization capability through zero-shot testing. The red line, representing human queries, remains consistent across both figures. Structure embedding methods like Node2Vec, MetaPath2Vec, SDNE, and TransE perform worse than attributed embedding methods like DGCN and DGAE. TransE's significant decline indicates a cold-start problem, difficult to make accurate predictions on unseen nodes. Attributed embedding methods, considering both graph structure and node attributes, exhibit more stability, similar to recommender systems. JPEC consistently outperforms other methods on both regular and zero-shot test datasets, demonstrating its effectiveness and superiority in competitor discovery."}, {"title": "5 RELATED WORK", "content": "Graph embedding techniques, including Graph Neural Networks (GNN), transform nodes, edges, and attributes into lower-dimensional vector spaces while preserving graph properties. This paper applies graph embedding to competitor detection, categorizing methods into Structure Embedding, Attributed Embedding, and Heterogeneous Embedding. Structure Embedding models, such as DeepWalk [7], Node2Vec[3], Metapath2Vec [2], TransE[12], and SDNE [10], focus on graph topology. Attributed Embedding, including GCN[13], GAE[4], and GALA[6], emphasizes node attributes. Heterogeneous Embedding, with methods like HAN [11] and KBGAT[5], represents diverse node and edge types. The proposed JPEC model, inspired by SDNE[10] and GALA [6], employs a directed graph convolutional network (DGCN) and Laplacian sharpening for semi-supervised competitor detection, outperforming existing models in the financial domain."}, {"title": "6 CONCLUSION", "content": "In summary, this paper introduces JPEC, a novel graph embedding model designed for competitor detection from a financial knowledge graph. Leveraging two orders of node proximity and essential features, JPEC surpasses most SOTA graph embedding models and human queries from various extensive experiments. Our evaluations of various graph embedding models offer valuable insights into their strengths and limitations for real-world competitor detection tasks. JPEC represents a significant contribution to the field, showcasing the potential of knowledge graphs and graph embedding techniques in unveiling patterns within complex networks in practical business scenarios."}]}