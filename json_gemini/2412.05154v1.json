{"title": "Towards Flexible 3D Perception: Object-Centric Occupancy Completion Augments 3D Object Detection", "authors": ["Chaoda Zheng", "Feng Wang", "Naiyan Wang", "Shuguang Cui", "Zhen Li"], "abstract": "While 3D object bounding box (bbox) representation has been widely used in autonomous driving perception, it lacks the ability to capture the precise details of an object's intrinsic geometry. Recently, occupancy has emerged as a promising alternative for 3D scene perception. However, constructing a high-resolution occupancy map remains infeasible for large scenes due to computational constraints. Recognizing that foreground objects only occupy a small portion of the scene, we introduce object-centric occupancy as a supplement to object bboxes. This representation not only provides intricate details for detected objects but also enables higher voxel resolution in practical applications. We advance the development of object-centric occupancy perception from both data and algorithm perspectives. On the data side, we construct the first object-centric occupancy dataset from scratch using an automated pipeline. From the algorithmic standpoint, we introduce a novel object-centric occupancy completion network equipped with an implicit shape decoder that manages dynamic-size occupancy generation. This network accurately predicts the complete object-centric occupancy volume for inaccurate object proposals by leveraging temporal information from long sequences. Our method demonstrates robust performance in completing object shapes under noisy detection and tracking conditions. Additionally, we show that our occupancy features significantly enhance the detection results of state-of-the-art 3D object detectors, especially for incomplete or distant objects in the Waymo Open Dataset.", "sections": [{"title": "1 Introduction", "content": "In autonomous driving, accurate and robust 3D scene perception is crucial for safe and efficient navigation. Conventional perception systems primarily adopt 3D object bounding boxes as the perception representation [25, 6, 15, 16]. However, the limitations of 3D bounding boxes (bboxes) are becoming increasingly pronounced as the demands for perception accuracy continue to escalate. Since a 3D bbox is essentially a cuboid that encapsulates the object, it fails to capture the precise details of the object's shape, particularly for objects with irregular geometries. As shown in Fig. 1 (a),"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 3D Occupancy Prediction and Shape Completion", "content": "3D semantic occupancy prediction (SOP) [16, 29, 34, 30, 12] has become a critical task in vision-centric autonomous driving, where algorithms primarily perceive the environment using RGB cameras. These vision-centric models typically discretize the surrounding environment into a volumetric grid"}, {"title": "2.2 3D Object Detection with Long Sequences", "content": "As demonstrated in [38, 41, 24], a single-frame detector can directly benefit from temporal information by taking the concatenation of several history frames as inputs. Although such a simple multi-frame strategy shows noticeable improvements, the performance becomes easily saturated as the number of input frames increases (e.g.,2 ~ 4 frames). Besides, the computational cost grows significantly as the number of input frames increases, which is not ideal for real-time applications. To remedy this issue, [9] employs a residual point probing strategy to remove redundant points in the multi-frame inputs. Besides, [3] opts for an object-centric approach that conducts the temporal aggregation at the level of tracklet proposals, which allows for longer sequences (i.e.,16 frames) to be processed with lower computational costs. Furthermore, [21, 8] demonstrate human-level detection performance by leveraging past and future information of entire object tracks. However, they are limited to offline applications since they require access to future frames. More recently, MoDAR[14] improves detection by augmenting LiDAR point clouds using motion forecasting outputs, which consist of future trajectory points predicted from long history subsequences (i.e., 90 frames). Compared to MODAR[14], our method is able to aggregate all the historical information via the compact implicit latent embeddings. Besides, our method goes beyond detection by predicting the complete object-centric occupancy for each proposal."}, {"title": "2.3 Implicit Neural Representation", "content": "Implicit shape representation [13] represents 3D shapes with a continuous function. Compared to traditional explicit representations (e.g., point clouds, meshes, volumetric grids), implicit representations can describe shape structure in continuous space, and are more memory-efficient. Rather than manually designing the implicit function, recent works [18, 19, 39, 22] propose to learn the implicit function from data. Specifically, they employ neural networks to approximate the implicit function, which can be trained in a data-driven manner. These neural functions typically take continuous 3D coordinates as inputs and output the related shape attributes at the queried positions (e.g., color, density, signed distance, etc.) For example, [19] learns a signed distance function (SDF) from high-quality 3D meshes for better shape representation. While [18] learns a neural radiance field from multi-view images to achieve better view synthesis. Our implicit shape decoder shares similarities with DeepSDF introduced in [19]. However, instead of predicting the signed distance at a queried position, we predict its occupancy probability."}, {"title": "3 Object-Centric Occupancy Dataset", "content": "High-quality datasets are critical for learning-based methods. However, existing datasets do not satisfy our requirements for object-centric occupancy perception due to unaligned coordinate systems and reduced resolutions. We discuss these limitations and introduce our automated annotation pipeline in the following subsections."}, {"title": "3.1 Object-Centric vs. Scene-Level Occupancy", "content": "Occupancy representation discretizes the 3D space into a volumetric grid, wherein each voxel is classified as occupied or free. Given our objective is to more accurately represent complex object structures, background elements despite their extensive coverage are not our primary focus. Therefore, we define object-centric occupancy as a 3D grid centered on the object's coordinate. As different object instances vary in size, their corresponding occupancy resolutions also vary, if given a predefined voxel size. In contrast, existing scene-level occupancy datasets [29, 34, 33] use an occupancy volume to represent an entire scene centered at the ego vehicle's coordinate system. Since all scenes are bounded by a fixed range, the occupancy resolution remains constant when the voxel size is given."}, {"title": "3.2 Dataset Generation Pipeline", "content": "Similar to previous scene-level approaches [29, 33], we can construct object-centric occupancy annotations based on any existing 3D detection datasets [27, 2]. However, instead of generating an occupancy volume for the entire scene, we create it for each annotated object instance under its local coordinate system.\nFor each designated object, we gather points within its annotated bounding boxes over time, transform these points from sensor coordinates to the bounding box coordinates and aggregate them into a dense point cloud. After that, we directly voxelize it under the local object coordinate system, yielding the object-centric occupancy grid.\nAdditionally, we perform occlusion reasoning to classify unoccupied voxels as either free or unobserved by comparing each voxel center's range value to raw range images from LiDAR scans. This strategy is significantly faster than traditional ray-casting [29]. After finishing the annotation, every tracked object within the detection dataset is associated with an object-centric occupancy grid. This grid is centered at the local coordinate and has a size determined by the object's size and the desired resolution. Please refer to Appendix A.1 for more details about the dataset generation pipeline."}, {"title": "4 Sequence-based Occupancy Completion Network", "content": "Fig. 4 illustrates the architecture of our object-centric occupancy completion network. Our method utilizes an object sequence as input, formulated as a {$(P_t, B_t)$}$_{t=0}^T$, where $P_t \\in \\mathbb{R}^{N \\times 3}$ is the point cloud at timestamp t and $B_t \\in \\mathbb{R}^7$ is the corresponding noisy 3D object bbox. The input sequence can be generated using off-the-shelf 3D detection [38, 6] and tracking [32] systems. Our main objective is to predict the complete object-centric occupancy grid for each proposal in the trajectory. Additionally, we use the occupancy features to further refine the detection results of the 3D detector."}, {"title": "4.1 Dynamic-Size Occupancy Generation via Implicit Decoding", "content": "Our network primarily focuses on Regions of Interest (RoIs) defined by object proposals. Given that different objects have varying sizes and proposals for the same object may also vary due to inaccurate detection, efficiently decoding the occupancy volume from feature space for each dynamic-sized proposal poses a significant challenge. Conventional scene-level occupancy perception approaches [30, 34] typically apply dense convolution layers to decode the occupancy volume. However, this strategy encounters several limitations in the context of dynamic-size object-centric occupancy. First, since we require feature interaction across timestamps, the features for different proposals are better if in the same size. However, decoding a dynamic-sized volume from a fixed-size feature map is non-trivial for convolution. Secondly, the dense convolution operation becomes computationally expensive for high occupancy resolution. One alternative is sparse convolution [5, 10], however, it cannot fill the unoccupied voxels with the correct occupancy status.\nDrawing inspiration from the recent success of implicit shape representations [18, 19], we tackle the aforementioned challenge through an implicit shape decoder D. This decoder is capable of predicting the occupancy status of any position within the RoI based on its corresponding latent embedding. Specifically, the decoder takes in the latent embedding z along with a query position $q \\in \\mathbb{R}^3$ at the Rol coordinate, and subsequently outputs the occupancy probability at q:\n$p = D(z, q),$\nwhere D : $\\mathbb{R}^e \\times \\mathbb{R}^3 \\rightarrow \\mathbb{R}_{[0,1]}$ is implemented as an MLP. The latent z $\\in \\mathbb{R}^e$ is a fixed-length embedding depicting the geometrics within the RoI. The latent z and query position q are concatenated before being sent to D. Besides enabling flexible feature interaction and efficient computation, the implicit shape decoder also allows for easier occupancy interpolation or extrapolation with continuous query positions."}, {"title": "4.2 Dual Branch RoI Encoding", "content": "Having the implicit shape decoder in place, the next step is to obtain a latent embedding z that accurately represents the complete object shape within the RoI. To achieve accurate shape completion and detection, two information sources are essential: 1) the partial geometric structure of each Rol, and 2) the motion information of the object over time. To make different RoIs share the same embedding space, we encode each RoI under a canonical local coordinate system. However, transforming the RoI to the local coordinate system inevitably loses the global motion dynamics of the object, reducing the network's ability to handle detection drifts. Therefore, we encode each Rol using two separate encoders: $E_{local}$ that encodes the RoI in the local coordinate system and $E_{global}$ in the global coordinate system.\nSpecifically, we employ the sparse instance recognition (SIR) module in FSD[6] as our RoI encoder. SIR is a PointNet-based network [20] characterized by multiple per-point MLPs and max-pooling layers. Drawing inspiration from LiDAR R-CNN [15], we additionally enhance the point cloud with size information of the RoI. This augmentation involves decorating each point within the RoI"}, {"title": "4.3 Feature Enhancement via Temporal Aggregation", "content": "After RoI encoding, we use the motion information from $Z_g$ to enrich the local shape latent embeddings $Z_l$. First, we employ a transformer mechanism [31] to $Z_g$ to enable feature interaction across timestamps. To ensure online applications, we restrict each RoI feature in $Z_g$ to only attend to its historical features, thereby preventing information leakage from future timestamps:\n$Z_g' = CausualAttn(Z_g + \\gamma(T) + \\phi(B)),$\nwhere CausualAttn is a causal transformer that restricts the attention to the past timestamps. $\\gamma(\\cdot)$ is a sinusoidal positional encoding [31] that encodes the temporal timestamp T $\\in \\mathbb{R}^{T \\times 1}$. $\\phi(\\cdot)$ is a learnable MLP that encodes the bbox information B $\\in \\mathbb{R}^{T \\times 7}$ in the global coordinate system.\nNext, we fuse the enriched global latents $Z_g'$ with the local latents $Z_l$ to obtain the final latent embeddings $Z \\in \\mathbb{R}^{T \\times c}$:\n$Z = MLP(Concat(Z_l, Z_g')),$\nwhere 'Concat' denotes the concatenation operation, and 'MLP' is a multi-layer perceptron that projects the concatenated features to the desired dimension c."}, {"title": "4.4 Occupancy Completion and Detection Refinement", "content": "Given the final latent embeddings Z, we can predict the complete object-centric occupancy volume for each proposal by querying the implicit shape decoder D at different positions. During training, we randomly sample a fixed number of query positions within each RoI to compute the loss. During inference, we query the decoder at all voxel centers within the RoI to obtain the complete occupancy volume. Since Z now encodes information of complete object shapes, it provides more geometric information for better detection. To retain motion information, we additionally fuse Z with the global Rol feature $Z_g$:\n$Z_{det} = MLP(Concat(Z, Z_g)).$\nThe fused feature $Z_{det}$ is then fed into a detection head for bbox and score refinement (Fig. 4)."}, {"title": "4.5 Loss functions", "content": "The overall training loss consists of three components: the occupancy completion loss $L_{occ}$, the bbox loss $L_{det}$, and the objectness loss $L_{score}$:\n$L = L_{occ} + \\lambda_{det} L_{det} + \\lambda_{score} L_{score},$\nwhere $\\lambda_{det}$ = 2 and $\\lambda_{score}$ = 1 are hyperparameters that balance the three losses. We use the binary cross-entropy loss for $L_{occ}$ and $L_{score}$, and the L1 loss for $L_{det}$."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Implementation Details", "content": "Position Query Sampling. During training, we randomly sample 1024 voxel centers and corresponding occupancy statuses from each annotated occupancy as the position queries. To ensure the occupancy prediction is not biased, we adopt a balanced sampling strategy, where 512 points are sampled from the occupied voxels and 512 from the free voxels. For an RoI that matches a ground-truth (GT) bbox, we transform the corresponding query set to its coordinate system using the relative pose between the RoI and the bbox. These position queries are then sent to the implicit"}, {"title": "5.2 Dataset and Evaluation Metrics", "content": "Dataset. Our method is evaluated on the Waymo Open Dataset (WOD)[27]. We use the official training set, comprising 798 sequences for training, and 202 sequences for evaluation. We apply our automatic pipeline on WOD to construct the object-centric occupancy annotations with the voxel size set to 0.2m. All experiments are conducted on rigid objects (i.e., vehicles) to ensure accurate evaluation of shape completion using our annotated ground-truths.\nEvaluation Metrics. For shape completion, we adopt the widely-used intersection-over-union (IoU) to evaluate the quality of the predicted occupancy volumes. Due to the object-centric nature of our method, we cannot calculate the IoU directly between the predicted and the ground-truth occupancy volumes because they are in different coordinate systems and may have different sizes (noisy RoI vs. GT box). To overcome this issue, we employ a two-step process as illustrated in Fig. 5. Firstly, we transform the ground-truth (GT) box to the coordinate system of the RoI using the relative pose. This transformation aligns the GT box with the RoI, enabling a consistent comparison. Subsequently, we determine the predicted occupancy status of each voxel center within the transformed GT box. For voxels falling inside the RoI (hit), their occupancies are determined by the corresponding predicted occupancies within the RoI. On the other hand, voxels located outside the RoI (missed) are considered as free. By applying this process, we construct a predicted occupancy volume within the GT box. Finally, we compute the IoU by comparing the predicted occupancy volume in the GT box with the ground-truth occupancy volumes. During the IoU calculation, we ignore unobserved voxels in the GT volume for a fair assessment. Besides, RoIs that do not intersect with any GT boxes are excluded from the evaluation. We also report mean IoU that is respectively averaged at track and box levels to provide a more detailed evaluation.\nFor object detection, we adopt the official 3D detection metrics in WOD [27], including Average Precision (AP) and Average Precision Weighted by Heading (APH) at IoU thresholds of 0.7 for vehicles. Meanwhile, based on the number of points contained within each object, the data is divided into two difficulty levels: LEVEL 1, where the number of points is greater than 5, and LEVEL 2, where the number of points is between 1 and 5."}, {"title": "5.3 Shape Completion Results", "content": "Comparison against Baseline. Since object-centric occupancy is a novel task, no learning-based methods can be used for comparison as far as we are acknowledged. We compare our method with the baseline that directly accumulates and voxelizes the history point clouds within the noisy tracklet proposals. We evaluate the shape completion performance on three types of tracklet inputs: ground-truth (GT) tracklets, tracklets generated by CenterPoint (CP) [38], and tracklets generated by FSD [6]. As shown in Tab. 1, the shape completion performance is strongly correlated with the quality of the input tracklets, where better tracklets lead to better shape completion. In all cases, our method outperforms the baseline, even when the input tracklets are noise-free GTs. This is because our method can effec-"}, {"title": "5.4 Object Detection Results.", "content": "Main Results Tab. 2 presents the 3D detection results on the WOD validation set. Significant improvements are observed when applying our methods to the tracklet proposals generated by CenterPoint [38] and FSD [6]. Compared to the previous state-of-the-art MoDAR [14], our method achieves notably greater enhancements on 1-frame CenterPoint (e.g., 8.6% vs. 3.2% improvement in L1 AP). Applying our method to a more advanced detector, 1-frame FSD [6], still results in a noticeable improvement. This enhancement is more significant compared to adding MoDAR to a detector with similar performance (i.e., 3-frame SWFormer [28]). Furthermore, we achieve new state-of-the-art online detection results by applying our method to 7-frame FSD, attaining 83.3% AP and 75.7% APH on L1 and L2, respectively. This indicates our method's effectiveness in aggregating long-sequence information for object detection in addition to shape completion. Moreover, our method can be seamlessly integrated with other state-of-the-art detectors without requiring retraining on their respective tracklets in the training data. For example, applying our method (trained on CP and FSD tracklets) to FSDv2 [7] yields significant improvements, showcasing the strong generalization capability of our approach.\nRange Breakdown. Distant objects are more challenging to detect due to their sparsity. We further analyze the detection performance across different distance ranges. As shown in Tab. 3, our improvements over the base detector be-"}, {"title": "5.5 Model Analysis.", "content": "In this section, we evaluate different design choices in our method and analyze their impact on the shape completion and detection performance. All the results are based on 1-frame FSD [6] tracklets.\nSingle Branch vs. Dual Branch. We first evaluate the performance when using only a single branch for RoI encoding. In this setting, only a local encoder $E_{local}$ is used to encode the Rol in the local coordinate system. The encoded features are enhanced by the causal transformer and then used to generate occupancy and detection outputs. As shown in Tab. 4, the single-branch model is inferior to our dual-branch model in both shape completion and detection. This indicates that the motion information from the global branch is essential for accurate shape completion and detection refinement.\nExplicit vs. Implicit. We then attempt to refine detection results using the explicit occupancy predictions. Specifically, we sample occupied voxel centers from each predicted occupancy volume and apply the RoI encoder $E_{global}$ to generate the final feature used for detection (more details are in the Appendix A.2). However, as demonstrated in Tab. 4, this strategy leads to a significant performance drop. Due to the non-differentiable nature of the occupancy sampling process, the detection errors cannot be back-propagated to other components when relying on explicit occupancy predictions, resulting in unstable training. In contrast, our implicit shape representation allows for joint end-to-end training of shape completion and detection, leading to better performance."}, {"title": "Occupancy Helps Detection.", "content": "Finally, we evaluate the impact of the occupancy task on detection performance. We removed the OCC head from our full model and retrained it using only the detection loss. As shown in the last row of Tab. 4, the absence of the occupancy decoder results in a noticeable decline in detection performance. This suggests that the occupancy completion task not only explicitly enriches the object shape representation but also enhances detection by contributing additional geometric information to the latent space.\nTraining & Testing Length. Tab. 5 shows how the sequence lengths affect the performance of our method. We retrain our method using 8-frame and 16-frame tracklets, respectively. As indicated in the first 3 rows in Tab. 5, using longer sequences for training leads to better results. However, the performance improvement diminishes as the sequence length doubles. To strike a balance between performance and computational cost, we set our default training length to 32. Even trained with 32-frame tracklets, our method is flexible to handle various-length tracklets during inference. By default, we leverage all history frames to generate pre-"}, {"title": "Computational Efficiency.", "content": "Tab. 6 shows the time and GPU memory cost of the proposed shape decoder. Since object tracklets vary in length, our method's running time may also vary with different inputs. Additionally, the dimension of the decoded object-centric occupancy depends on the detected bounding box. To ensure fair testing of running time, we standardized the input length to 32 and set the number of decode queries to 4096. As demonstrated in Tab. 6, the shape decoder only introduces a slight increase in computational cost, demonstrating its efficiency."}, {"title": "Limitations", "content": "Technically speaking, our automatic occupancy annotation relies on the rigid-body assumption, which may not be accurate for deformable objects. Consequently, our experiments focus on vehicle objects since they are rigid. Although our method can be applied to other deformable object categories, accurate evaluation for deformable objects cannot be guaranteed due to considerable noise in the ground-truth data."}, {"title": "Conclusion", "content": "In this work, we introduce a novel task, object-centric occupancy, which extends the traditional object bounding box representation to provide a more detailed description of the object shape. Compared to its scene-level counterpart, object-centric occupancy enables higher voxel resolution in large scenes by focusing on foreground objects. To facilitate object-centric occupancy learning, we build an object-centric occupancy dataset using LiDAR data and box annotations from the Waymo Open Dataset (WOD). We further propose a novel sequence-based occupancy completion network that learns from our dataset to complete object shapes from noisy object proposals. Our method achieves state-of-the-art performance on both shape completion and object detection tasks on WOD. We believe that our work will inspire future research in perception tasks in the context of autonomous driving."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Dataset Generation Pipeline", "content": "Our annotation pipeline is illustrated in Fig. 6. Leveraging LiDAR scans and detection annotations from a base 3D dataset, our pipeline probes dense occupancy grids by aggregating multi-frame LiDAR point clouds and then executes occlusion reasoning to discriminate between free and unobserved voxels. Compared to ego-centric approaches [29, 33], our methodology primarily differs by focusing on annotated objects instead of the entire scene. For each designated object, we gather points within its annotated bounding boxes over time, transform these points from sensor coordinates to the bounding box coordinates and aggregate them into a dense point cloud. Unlike scene-level occupancy, we do not transform the densified object point cloud back to each ego-vehicle coordinate for occupancy construction. Instead, we directly voxelize it under the local object coordinate system, maintaining object-centric precision. While densified object point clouds encode better shape information than a single LiDAR scan, it's important to note that unoccupied voxels (grey voxels in Fig. 6) does not necessarily indicate free space; they may be unobserved by the LiDAR due to occlusion. Hence, an occlusion reasoning process is required to distinguish between voxels that are truly free and those that are unobserved. Basically, an unoccupied voxel is considered free if it is traversed trough by a LiDAR ray, and unobserved otherwise. Instead of the time-consuming ray-casting operation used in [29], we adopt a more efficient approach by leveraging range information from raw range images. Specifically, for each unoccupied voxel, we first convert its center to the range image format using sensor intrinsics and extrinsics at a specific timestamp t, yielding a 2D-pixel index ($u_t$, $v_t$) and a range value $r_t$. Next, we decide its status by comparing its range with the original range image at timestamp t:\nif $r_t$ < $R_t[u_t, v_t]$ :\nfree\nelse:\nunobserved\nwhere $R_t$ $\\in$ $\\mathbb{R}^{H \\times W}$ is the raw range image captured by the LiDAR sensor at timestamp t. We do this for all timestamps to decide the final status of the voxel. And a voxel is considered unobserved only if it is not traversed by any LiDAR ray at any timestamp).\nFig. 7 shows some examples of our object-centric occupancy annotations."}, {"title": "A.2 Alternative Design Choices", "content": "Fig. 8 and Fig. 9 illustrate the pipelines of the single-branch model and the model using explicit occupancy for detection, respectively. The two global RoI encoders $E_{global}$ in Fig. 9 share the same weights. We additionally add an extra channel to each point feature to indicate whether it is from raw point clouds or from the predicted occupancy volume."}]}