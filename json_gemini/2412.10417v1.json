{"title": "LEVERAGING AUDIO AND TEXT MODALITIES IN MENTAL HEALTH: A STUDY OF LLMS PERFORMANCE", "authors": ["Abdelrahman A. Ali", "Aya E. Fouda", "Radwa J. Hanafy", "Mohammed E. Fouda"], "abstract": "Mental health disorders such as depression and PTSD are increasingly prevalent worldwide, creating an urgent need for innovative tools to support early diagnosis and intervention. This study explores the potential of Large Language Models (LLMs) in multimodal mental health diagnostics, specifically for detecting depression and PTSD (Post Traumatic Stress Disorder) through text and audio modalities. Using the E-DAIC dataset, we compare text and audio modalities to investigate whether LLMs can perform equally well or better with audio inputs, assessing their effectiveness in capturing both vocal cues and linguistic patterns. We further examine the integration of both modalities to determine if this can enhance diagnostic accuracy, which generally results in improved performance metrics. Our analysis specifically utilizes custom-formulated metrics\u2014Modal Superiority Score (MSS) and Disagreement Resolvement Score (DRS)\u2014to evaluate how combined modalities influence model performance. The Gemini 1.5 Pro model achieves the highest scores in binary depression classification when using the combined modality, with an F1 score of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full dataset. These results represent an increase of 3.1% over its performance with the text modality and 2.7% over the audio modality, highlighting the effectiveness of integrating modalities to enhance diagnostic accuracy. Similarly, with a BA of 77% and an F1 score of 0.68, the GPT-40 mini demonstrates significant success in classifying PTSD. Notably, all results are obtained in zero-shot inferring, highlighting the robustness of the models without requiring task-specific fine-tuning. To explore the impact of different configurations on model performance, we conduct binary, severity, and multiclass tasks using both zero-shot and few-shot prompts, examining the effects of prompt variations on performance. The results reveal that models such as Gemini 1.5 Pro in text and audio modalities, and GPT-40 mini in the text modality, often surpass other models in balanced accuracy and F1 scores across multiple tasks. This study highlights the promising role of LLMs in clinical settings for mental health assessment, emphasizing the need for advancements in LLM-based diagnostics for multimodal mental health applications.", "sections": [{"title": "1 Introduction", "content": "In recent years, mental health disorders have become increasingly prevalent, with conditions like depression and PTSD affecting a significant portion of the global population. According to the World Health Organization (WHO), over one billion people currently live with a mental disorder, with cases of depression and anxiety rising by more than 25% during the first year of the COVID-19 pandemic. PTSD, which impacts individuals exposed to traumatic events, continues to pose significant long-term risks to well-being. In response to these escalating numbers, artificial intelligence (AI), particularly machine learning[1], has emerged as a vital tool, aiding in the detection and diagnosis of mental health conditions. Al models now analyze patterns in speech, behavior, and medical data, allowing for earlier intervention and improved treatment outcomes.\nIn recent years, models like BERT[2] have advanced rapidly, demonstrating significant capabilities in tasks ranging from natural language processing to decision-making in specialized domains.\" An Overview of Large Language Models (LLMs)\" (2023)[3], this paper highlights the capabilities of LLMs as they are trained on vast and diverse corpora, enabling them to capture complex patterns in language. Their training allows them to generate coherent and contextually relevant text, making them highly adaptable across various applications. In healthcare, for example, LLMs can assist in diagnosing diseases, summarizing patient records, and even providing support for therapeutic interventions. Their ability to generalize across tasks has made them valuable in different fields, significantly enhancing efficiency and scalability.\nLLMs show a remarkable ability to process and analyze vast amounts of text data, identifying and predicting psychiatric conditions by detecting patterns and subtle linguistic cues in patient communication. They excel at providing timely, scalable, and personalized assessments, aiding mental health professionals in diagnosis and treatment planning. LLMs provide scalable, efficient, and objective assessments, enhancing diagnostic accuracy and personalization of treatment. However, they must be used carefully, as they may generate inaccurate responses or lead to over-reliance, requiring continuous monitoring to ensure safety and effectiveness[4][5].\nRecent LLM advancements have enabled models to process not only text but also audio inputs directly[6], providing valuable insights into mental health. Some models convert audio to text via speech recognition system [7], analyzing the linguistic content for symptoms of mental disorders. However, newer models can directly analyze audio by detecting differences in tone, cadence, and speech patterns, which can reflect emotional states more accurately than text alone [8]. This approach allows for deeper insights into a patient's mental health, potentially leading to more precise and early diagnosis of conditions like depression or anxiety.\nIn this paper, we compare two approaches for mental illness detection: text modality and audio modality, using different models to analyze each. Models processing text evaluate written transcripts to identify linguistic patterns, while models processing audio analyze direct raw speech to capture features such as tone and cadence. Additionally, we explore the integration of both modalities to determine if this enhances model performance. To quantitatively assess how the combined modality either increases or decreases performance, we utilize our specially formulated metrics: the MSS and DRS. By evaluating both individual and integrated modalities, we aim to provide a comprehensive overview of the potential that audio-based inputs and multimodal approaches hold for LLMs.\nAlso in this study, we utilized few-shot learning with the three most consistent models to evaluate their performance across text and audio modalities. Few-shot learning was applied to text-based prompts for both modalities, but inference was conducted separately for text and audio inputs. This setup allows us to compare the models' adaptability and performance when handling text versus audio in few-shot conditions, offering insights into how each modality responds to minimal training data within the few-shot framework.\nTo our knowledge, there are no papers that directly input raw audio interviews into an LLM; however, some studies have constructed transformer models that can process audio inputs. Despite this, no research has yet leveraged pre-trained LLMs designed to handle audio directly for mental illness detection. Current literature lacks models capable of processing long-form audio inputs, and there has not been a comprehensive review of how small prompt changes impact LLM performance. Many studies rely on preprocessing audio data and using specific audio features for their models. In contrast, our approach aims to explore the use of raw audio inputs directly with LLMs. Furthermore, while there are LLMs that can process audio, most are restricted to segments no longer than 30 seconds. Only a few can handle slightly longer durations, but they are still not sufficient for analyzing full-length interviews effectively.\nThis work focuses on addressing these gaps:\n\u2022 Zero-shot (ZS) preprocessing and ZS inference for LLMs.\n\u2022 A setup of various tasks, including a multi-label classification task.\n\u2022 Comparison of prompts and their effects on model performance.\n\u2022 Analysis of results between audio and text modalities, their combination, and their evaluation through custom-formulated metrics (MSS and DRS).\n\u2022 Comparison of different models from various families and sizes.\n\u2022 Evaluation of the effects of few-shot (FS) learning on different tasks and models to assess its impact on performance.\nIn this paper, we will also address some limitations of the models, including how different prompts can impact their performance and accuracy. Variations in input phrasing may lead to differing results [9], which is an important consideration when evaluating LLMs. In the methodology 4 section, this approach for comparing models will be outlined, focusing on text-based and audio-based inputs. In the Experimental Setup 4.2, we will discuss the various"}, {"title": "2 Related Work", "content": "tasks conducted to evaluate the models' performance. Lastly, in the Results and Discussion 5 section, we will analyze and discuss how the models performed across different metrics, highlighting their strengths and weaknesses.\nIn most studies utilizing the DAIC-WOZ dataset[10], researchers leverage various modalities, such as text, audio, and video, to provide a comprehensive analysis of psychological states. Each modality offers distinct insights: text data can reveal linguistic patterns, audio can capture speech characteristics, and video can provide visual cues. Depending on the study's objectives, researchers may focus on a single modality or combine multiple ones to capture a more holistic view of the participant's mental health. The following figure (Figure 1) represents the workflow of most studies written on the DAIC-WOZ dataset, illustrating how these modalities are processed and integrated for prediction tasks.\nTo process these modalities, researchers employ techniques from machine learning (ML), deep learning (DL), and increasingly, large language models (LLMs). ML methods are often used for structured data and straightforward predictions, as detailed in Section 2.2.1, while DL techniques, like neural networks, can handle unstructured data such as raw audio or video, as detailed in Section 2.2.2. LLMs are particularly useful for analyzing text data, as they can understand and generate human language in a context-aware manner, making them highly effective for assessing linguistic and semantic features within mental health interviews, as detailed in Section2.2.3.\nCombining modalities is common practice to enhance prediction accuracy and robustness. By integrating information from text, audio, and video, models can capture a broader spectrum of emotional and behavioral signals, leading to more reliable and nuanced assessments. This multimodal approach allows the model to compensate for the weaknesses of individual modalities and strengthen its ability to detect mental health conditions.\nThe goal of these approaches is typically to predict either a binary classification (e.g., whether a participant is depressed or not) or to provide a severity score for conditions like depression or PTSD. multimodal systems combined with advanced ML, DL, and LLM techniques help deliver more accurate and comprehensive predictions, ultimately improving mental health diagnostics.\nAll the scores presented in Table 1 are reported on the DAIC-WOZ dataset, except for those that explicitly mention the E-DAIC dataset."}, {"title": "2.1 Modality Preprocessing", "content": "Preprocessing is an essential step in working with multimodal data from the E-DAIC and DAIC-WOZ datasets. Each modality: text, audio, and visual requires different techniques to ensure the data is clean and properly formatted for model training. Preprocessing helps to remove noise, extract relevant features, and standardize the inputs across modalities. In some cases, preprocessing techniques are combined to improve the model's ability to learn relationships between different types of data, such as synchronizing audio and visual features for better context understanding."}, {"title": "2.1.1 Text Preprocessing", "content": "In working with textual data from multimodal datasets, several preprocessing techniques are commonly employed to ensure the text is clean and structured before further analysis. Below are the primary preprocessing techniques applied to textual data, along with the papers that have utilized these techniques:\n\u2022 Basic Text Cleaning: This includes the removal of irrelevant annotations, such as speaker tags, hardware syncing notes, and non-verbal cues (e.g., laughter). Text is often lowercased to ensure uniformity, and punctuation is standardized to maintain semantic context. Papers such as those by Rohan Kumar Gupta et al. (2024)[14] and Ping-Cheng Wei et al. (2022)[25] have implemented this step, ensuring that the cleaned text is ready for further processing.\n\u2022 Tokenization and Removal of Stop Words: Tokenization is a crucial step in breaking down transcriptions into words or smaller linguistic units. Stop words (common words such as \"the\" or \"and\") are often removed to focus on more meaningful terms in the dataset. Several papers, including those by Clinton Lau et al. (2023)[24] and Giuliano Lorenzoni et al. (2024)[21], applied tokenization and stop word removal to improve model performance by focusing on more significant features within the text.\n\u2022 Feature Extraction using Embeddings: After cleaning and tokenization, the textual data is often transformed into feature vectors using pre-trained language models or embeddings such as BERT, GloVe, or Word2Vec. This allows for capturing the deeper semantic meaning of the text. The papers by Avinash Anand et al. (2024)[16] and Bakir Hadzic et al. (2024)[20] employed BERT embeddings, while other papers, such as Xiangyu Zhang et al. (2024)[22], utilized GloVe and Word2Vec embeddings to capture contextual and lexical features from the transcriptions.\nThese preprocessing steps are essential to ensuring that the textual data is in a format suitable for downstream machine learning models, allowing them to accurately detect and predict mental health conditions based on language patterns."}, {"title": "2.1.2 Audio Preprocessing", "content": "Audio data in multimodal datasets undergoes various preprocessing steps to ensure high-quality inputs for different models. These steps include cleaning the audio, extracting meaningful features, and normalizing the data. Below are the primary preprocessing techniques applied to audio data, along with the papers that have utilized these techniques:\n\u2022 Resampling and Noise Removal: To standardize the audio data, many studies resample it to a consistent frequency, often 16 kHz, and remove noise, including long pauses and irrelevant sounds. For example, David Gimeno-G\u00f3mez et al. (2024)[11] resampled audio data and used feature extraction tools to focus on relevant sound signals. Similarly, Xiangsheng Huang et al. (2024)[17] used noise removal techniques to clean the audio before processing.\n\u2022 Feature Extraction with MFCCs and Spectrograms: Mel-frequency cepstral coefficients (MFCCs) and log-mel spectrograms are commonly extracted from the audio data to capture speech and acoustic features. Papers like Jinhan Wang et al. (2024)[12] and Rohan Kumar Gupta et al. (2024)[14] utilized MFCCs to capture essential features from the raw audio signals, while Xiangsheng Huang et al. (2024)[17] extracted log-mel spectrograms for further analysis.\n\u2022 Advanced Feature Extraction using pre-trained Models: In some studies, pre-trained models such as HuBERT and wav2vec 2.0 are used to extract higher-level audio features. For instance, Avinash Anand et al. (2024)[16] used HuBERT-large to extract 1024-dimensional features from the audio, while Xu Zhang et al. (2024)[18] applied wav2vec 2.0 for frame-level feature extraction, enhancing the model's ability to analyze complex audio patterns.\nThese preprocessing steps are crucial for transforming raw audio data into meaningful inputs, ensuring that different models can effectively analyze speech patterns, acoustic features, and other relevant audio signals for detecting mental health conditions."}, {"title": "2.1.3 Visual Preprocessing", "content": "Visual data, particularly facial expressions and body language, plays a significant role in multimodal datasets. Various preprocessing steps are applied to extract meaningful features from visual data, such as facial landmarks and action units, which are then used for mental health predictions. Below are the primary preprocessing techniques applied to visual data, along with the papers that have utilized these techniques:\n\u2022 Facial Landmark Detection and Normalization: Facial landmarks, including key points such as eye, nose, and mouth positions, are extracted to understand emotional expressions. Normalization techniques are often used to ensure uniformity across different participants. For example, Ping-Cheng Wei et al. (2022)[25] extracted and normalized facial landmarks for consistency in facial expressions, and Xiangsheng Huang et al. (2024)[17] applied similar techniques to capture important facial features.\n\u2022 Facial Action Units (FAUs) Extraction: Facial Action Units (FAUs) capture muscle movements that reflect various emotions, making them essential for predicting mental states. The OpenFace toolkit is commonly used to extract FAUs. Studies such as Avinash Anand et al. (2024)[16] and Rohan Kumar Gupta et al. (2024)[14] used FAUs as a key visual feature for their models, focusing on facial expressions linked to emotional and mental health states.\n\u2022 Pose and Head Movement Features: In addition to facial features, head pose and body movement features are extracted to analyze non-verbal behavior. These features help to capture body language and gaze direction. Giuliano Lorenzoni et al. (2024)[21] and Clinton Lau et al. (2023)[24] both employed techniques to capture head pose and movement features, improving their models' ability to interpret visual cues related to mental health.\nThese preprocessing techniques are essential for extracting rich, meaningful features from visual data, which are then used to predict mental health conditions by analyzing facial expressions, body language, and other non-verbal cues."}, {"title": "2.2 Processing Techniques", "content": "Once the textual, audio, and visual data are preprocessed, different Processing techniques are applied to predict mental health conditions like depression and PTSD. These models aim to leverage the cleaned and extracted features from each modality, learning patterns that can indicate the presence of psychological distress. The most commonly used approaches include Machine Learning (ML), Deep Learning (DL), and more recently, Large Language Models (LLMs), each of which contributes uniquely to the field."}, {"title": "2.2.1 Machine Learning (ML)", "content": "Machine learning techniques primarily focus on structured feature extraction from preprocessed data. Models such as random forests, support vector machines (SVMs), and XGBoost are often employed to analyze features from text, audio, and visual modalities. For instance, Giuliano Lorenzoni et al. (2024)[21] used Random Forest and XGBoost models to process text features like sentiment analysis and word frequency, achieving high accuracy in detecting mental illness. Similarly, Shanliang Yang et al. (2024)[23] implemented multi-task learning and knowledge transfer techniques (RLKT-MDD) to improve their multimodal depression diagnosis system. Xiangyu Zhang et al. (2024)[20] employed machine learning techniques, specifically focusing on the integration of acoustic landmarks with language models to enhance their mental health predictions. These machine learning models are highly interpretable and work well with small to medium-sized datasets, leveraging relationships between structured features to predict mental health outcomes like depression and PTSD."}, {"title": "2.2.2 Deep Learning (DL)", "content": "Deep learning models, especially convolutional neural networks (CNNs), recurrent neural networks (RNNs), and other advanced architectures, are widely used for processing unstructured data, such as raw audio and visual inputs. These models are known for their ability to extract complex hierarchical patterns from the data. For example, Xiangsheng Huang et al. (2024)[17] applied a CNN-based architecture to analyze log-mel spectrograms from audio data, achieving excellent accuracy in binary classification for depression. Similarly, Xu Zhang et al. (2024)[18] integrated Wav2Vec 2.0 with CNNs and attention pooling to fuse audio and visual modalities, demonstrating the power of DL in handling multimodal data.\nOther studies, such as Rohan Kumar Gupta et al. (2024)[14], utilized LSTM networks to process sequential audio data, capturing temporal patterns that are indicative of depression. LSTM and RNN models are particularly effective for analyzing speech data over time, allowing for the detection of subtle emotional cues across audio sequences.\nAdditionally, David Gimeno-G\u00f3mez et al. (2024)[11] focused on a multimodal temporal model that processes non-verbal cues from various modalities, utilizing deep learning architectures to improve predictions in mental health detection. The combination of multiple inputs such as audio, visual, and text through deep learning models allows for more comprehensive analyses of participant behaviors."}, {"title": "2.2.3 Large Language Models (LLMs)", "content": "Large Language Models (LLMs) like BERT, GPT-3.5, and GPT-4 have become essential tools in analyzing text data, particularly when working with transcriptions from clinical interviews. LLMs excel at understanding the deeper semantic context and patterns within language, making them highly effective for predicting mental health conditions from text-based data.\nFor instance, Avinash Anand et al. (2024)[16] integrated LLMs with multimodal data, including textual and audio-visual modalities, to achieve better contextual understanding of patient responses. The use of BERT-based embeddings in this study enhanced the model's ability to extract meaning from text and fuse it with non-verbal cues like facial expressions and vocal tones.\nClinton Lau et al. (2023)[24] applied prefix-tuning to large language models, such as GPT-4, to fine-tune their performance for specific tasks like depression severity estimation. By leveraging the contextual power of LLMs, they were able to capture subtle emotional cues from the patient transcripts that might be missed by traditional models.\nBakir Hadzic et al. (2024)[20] performed a comparison of several NLP models (BERT, GPT-3.5, GPT-4) in predicting mental health conditions, finding that transformer-based models can capture linguistic nuances in patient interviews with high precision.\nThese studies demonstrate the unique ability of LLMs to handle large and complex text sequences, improving the overall accuracy of predicting mental health conditions through text analysis, especially when combined with other data modalities."}, {"title": "3 Datasets", "content": "The Extended Distress Analysis Interview Corpus (E-DAIC) is an enhanced version of the DAIC-WOZ, designed to study psychological conditions such as anxiety, depression, and PTSD through semi-clinical interviews. The interviews are conducted by a human-controlled virtual agent named \"Ellie\" in a wizard-of-Oz (WoZ) setting or by an autonomous AI agent, both aiming to detect verbal and nonverbal indicators of mental illnesses. Developed as part of the DARPA Detection and Computational Analysis of Psychological Signals (DCAPS) program, this dataset is specifically crafted to advance the understanding and detection of psychological stress signals, with a particular focus on depression. The dataset is available through the University of Southern California's Institute for Creative Technologies (USC ICT) and can be accessed by researchers through a data use agreement, ensuring ethical compliance and protection of participant data. Approval for the use of this dataset was obtained from USC ICT, emphasizing its adherence to institutional guidelines for studying psychological health conditions.\nThe dataset contains 275 samples, which are systematically divided into training, development, and test sets. This division ensures a balanced representation of participants in terms of age, gender, and depression severity, as measured by the PHQ-8 scores, with the test set consisting exclusively of sessions conducted by the AI-controlled agent. This unique structure provides an invaluable resource for evaluating autonomous interaction models in the context of mental health diagnostics.\nEach session directory is structured to include various files:\n\u2022 Audio recordings (WAV format)\n\u2022 Transcripts (CSV format)\n\u2022 Feature sets derived from audio and visual data:\n\u2022 Audio features like eGeMAPS and MFCCs processed into a bag-of-words model.\n\u2022 Visual features including Pose, Gaze, and Action Units (AUs) summarized over set intervals.\n\u2022 Deep representations from CNN models like ResNet and VGG, and Densenet for spectral images converted from audio.\nThe E-DAIC dataset's comprehensive structure supports Multiple viewpoints on depression by offering extensive behavioral, acoustic, and visual cues. This rich combination of data modalities allows researchers to develop and test diagnostic models that can autonomously assess psychological distress with greater accuracy. For instance, the deep learning models trained on this dataset can leverage the diverse and detailed features to identify subtle indicators of depression, enhancing the potential for early and more reliable detection of mental health issues. This approach is particularly valuable in clinical simulations and real-world applications, where automated systems can provide consistent and unbiased assessments."}, {"title": "3.1 Data Analysis", "content": "The E-DAIC dataset includes four distinct labels used to classify mental health conditions, focusing on both depression and Post-Traumatic Stress Disorder (PTSD). These labels provide a comprehensive analysis by offering both binary classification and severity scores for each condition.\n\u2022 PHQ_Binary: This label classifies individuals based on depression using the PHQ-8, a standard questionnaire for assessing depression. In this binary classification, individuals are labeled as \"Negative\" or \"Positive\" for depression. A score of 10 or higher on the PHQ_Score corresponds to the \"Positive\" label, indicating the presence of depressive symptoms.\n\u2022 PHQ_Score: This is a continuous score derived from the PHQ-8, ranging from 0 to 24, and it represents the severity of depression. Individuals with a score of 10 or higher are considered to have clinically significant depression. The PHQ_Binary classification is directly based on this score, with a cutoff point at 10.\n\u2022 PCL-C (PTSD): This binary label indicates whether an individual meets the criteria for PTSD based on the PCL-C (Post-Traumatic Stress Disorder Checklist \u2013 Civilian Version). Similar to the PHQ_Binary, individuals are classified as \"Negative\" or \"Positive\" based on their PTSD severity score.\n\u2022 PTSD Severity: This is a continuous score that assesses the severity of PTSD symptoms. A score higher than 44 indicates the presence of PTSD. The binary PCL-C classification is derived from this severity score, with 44 serving as the threshold for diagnosis.\nTable 2 below summarizes the count of individuals classified as \"Negative\" or \"Positive\" for both depression and PTSD, providing a binary overview of these conditions within the dataset:"}, {"title": "3.1.1 Data Preprocessing", "content": "\u2022 Label Correction: In the E-DAIC dataset, an issue with incorrect labeling was identified in the PHQ_Binary classification. Specifically, there are 20 instances where the PHQ_Score is 10 or higher, indicating that the participants should be classified as \"Positive\" for depression. However, the PHQ_Binary label was incorrectly assigned as 0 (Negative) instead of 1 (Positive). The IDs of the incorrect samples ID = [320, 325, 335, 344, 352, 356, 380, 386, 409, 413, 418, 422, 433, 459, 483, 633, 682, 691, 696, 709]. This mislabeling can lead to inaccuracies in model training and prediction if not corrected during the data preprocessing stage.\n\u2022 Severity Mapping:\nDepression\nBased on the PHQ-8 depression scale explained in the referenced paper [30][Kroenke et al.], we derived the severity mapping for depression scores ranging from 0 to 24. However, the labels associated with these categories were not explicitly provided in the referenced paper. We applied standard clinical terminology to label the ranges as seen in the figure.\nTable 3 summarizes the count of participants falling within each severity label. The labels are mapped as follows: 0 refers to a PHQ_Score between 0-4, 1 refers to scores from 5-9, and so on.\nPTSD\nBased on the PCL-C PTSD scale explained in the referenced paper Garc\u00eda-Valdez et al. (2024) [31], we derived the severity mapping for PTSD symptoms. According to the paper, the labels are used to categorize PTSD severity as follows:\n* 0: little to no severity\n* 1: Moderate severity\n* 2: High severity\nThe PCL-C score intervals are chosen based on the understanding of the used LLM and are detailed in figure 9. The Results & Discussion (Section 5) discuss the scoring system and compare it to existing intervals from the literature."}, {"title": "4 Methodology", "content": "In this section, we discuss the proposed methodology including processing pipelines, prompt engineering and LLMs under evaluation."}, {"title": "4.1 Evaluation Pipeline for Audio-Based Data", "content": "The proposed evaluation pipeline for audio-based data, as illustrated in Figure 2, begins with raw audio inputs. These audio samples can be directly provided to the model, or first transcribed into text using the Whisper Large-V3 model. In addition, the pipeline supports integrating both modalities-raw audio and transcribed text-together. After determining the preferred input format (audio only, text only, or a combination of both), a prompt engineering step is conducted. Here, carefully crafted task-specific prompts guide the model toward binary classification, severity classification, or multi-label classification tasks. These prompts are designed to ensure that the model receives clear instructions, aligned with the chosen input modality or modalities.\nOnce the input (audio, transcription, or both) is combined with the tailored prompts, the resulting prompt is passed to large language models (LLMs) for evaluation. This approach enables the assessment of the model's zero-shot capabilities-evaluating how well it can perform classification tasks without prior fine-tuning or preprocessing. By examining LLM responses across different modalities and tasks, this pipeline provides insights into the model's inherent ability to generalize, adapt, and accurately interpret a variety of input formats and instructions.\nIn this setup, all 275 samples from the E-DAIC dataset are used in their entirety as a test set, ensuring a comprehensive evaluation of model performance. By comparing models across modalities- raw audio, transcribed text, and combined inputs -the evaluation highlights which modality performs better under specific conditions and tasks. This methodology helps identify optimal configurations and provides valuable insights into how the models adapt to varying input formats and task requirements."}, {"title": "4.2 Experimental Setup", "content": "In this study, a comprehensive experimental setup was established to evaluate the effectiveness of Large Language Models in predicting mental health conditions, specifically depression and PTSD, using both text and audio modalities. The main objective of this experiment is to compare the performance of LLMs when processing textual data, such as transcriptions from interviews, against their performance in analyzing audio data that includes vocal features. By"}, {"title": "4.2.1 Task 1: Binary Classification", "content": "The first task in the experimental setup involves binary classification, where participants are categorized into two groups: depressed or not depressed, and PTSD-positive or PTSD-negative. For depression, the PHQ-8 (Patient Health Questionnaire-8) scores are used as a reference, with participants classified as depressed if their score meets or exceeds a predefined threshold of 10. Similarly, for PTSD, the PCL-C (Post-Traumatic Stress Disorder Checklist) scores are used, with a score threshold of 44 indicating PTSD positivity."}, {"title": "4.2.2 Task 2: Severity Classification", "content": "In this task, the focus was on classifying the severity of depression and PTSD, by categorizing the severity of depression into multiple levels based on the PHQ-8 score. The severity levels range from minimal (0-4), mild (5-9), moderate (10-14), moderately severe (15-19), to severe (20-24). Additionally, PTSD severity is classified into three categories: little or no severity, moderate severity, and high severity. This granularity enables a more detailed understanding of an individual's mental health, facilitating tailored treatment plans corresponding to each severity level."}, {"title": "4.2.3 Task 3: Multiclass Classification", "content": "In this task, we extend the classification approach by combining the binary classifications of depression and PTSD to create a multiclass framework. Participants are categorized into one of several classes based on their mental health status: no disorder, depression only, PTSD only, or both depression and PTSD. This multiclass setup allows us to evaluate the performance of Large Language Models (LLMs) in predicting whether a participant has one or more mental health disorders or none at all.\nTo assess the effectiveness of LLMs, we compare both the text and audio modalities, as well as their combination. This comprehensive comparison aims to identify how well each modality and their combination perform in simultaneously classifying multiple disorders, providing insights into the potential benefits of a multimodal approach in mental health diagnostics."}, {"title": "4.3 Audio Handling", "content": "For the audio handling process, we directly utilized the raw audio files from the E-DAIC dataset without applying any preprocessing or cleaning techniques. The average interview duration in the dataset is approximately 16 minutes. These unaltered audio files were essential inputs for both the analysis and transcription stages. This raw data was then used during the transcription process, ensuring that all acoustic nuances were preserved and processed by the Whisper model, as described in the transcription process section. By working with the original files, we aimed to evaluate the models in a real-world scenario, where audio imperfections such as background noise and variability in speech could impact model performance."}, {"title": "4.4 Transcription Process", "content": "For the transcription process, we used the Whisper [32] model, specifically the Large-V3 version, to transcribe the entire interview data, including both the interviewer's prompts and the participant's responses. The transcription provided in the dataset contained only the answers given by the participants, omitting the interviewer's questions. By transcribing the whole interaction, we were able to capture crucial contextual information from the interviewer's prompts (e.g., Ellie's prompts), which can provide significant insights. These prompts often contain information that the models can exploit to classify the participants more effectively, as highlighted in previous research by Sergio Burdisso et al. (2024).\nOne of the key features of Whisper's architecture is its ability to handle multilingual transcription, background noise, and various accents with high precision. Whisper processes audio data in chunks, typically by converting the audio into spectrograms (a visual representation of sound) that can then be interpreted by the neural network. This process"}, {"title": "4.5 LLMs Under Evaluation", "content": "In this study, we evaluated a diverse set of large language models (LLMs) to analyze their capabilities in handling both text and audio data. The models were selected based on factors such as parameter size, source, accessibility via APIs, and support for different data modalities. as well as those excluded, based on findings and recommendations from the referenced paper [9]. These criteria ensured a comprehensive comparison of models from various providers, including both proprietary and open-source options.\nTable 4 summarizes the characteristics of the selected models, which vary significantly in terms of parameter size, ranging from lightweight models like Phi-3.5-mini (3.8 billion parameters) to much larger models such as Llama 3 70B. Additionally, the sources of the models encompass major technology companies like Google and Microsoft, as well as specialized AI firms such as Mistral AI and Meta. API accessibility is provided by multiple platforms, including Nvidia NIM, OpenAI, Groq, and Google's Gemini API, enabling diverse deployment options for text and audio processing tasks.\nFor the audio analysis, Whisper was employed to transcribe audio files into text before inputting the results into text-focused models like Llama 3 and GPT-40 mini. In contrast, models supporting multimodal data, such as Gemini 1.5 Flash and Pro version 2, were directly fed audio data to evaluate their performance in handling both audio and text tasks. This approach allowed for a direct comparison of text-only versus multimodal model capabilities.\nSeveral models in this evaluation, such as Phi-3.5-mini and Phi-3.5-MoE, were chosen due to their emerging relevance in multimodal and multilingual tasks. The inclusion of models with different quantization strategies, such as those used in Llama 3 70B and Mistral NeMo, highlights the trade-offs between model complexity and computational efficiency. Quantization, in some cases, helped optimize model performance, particularly in audio-capable models."}, {"title": "4.5.1 Model Exclusions and Limitations", "content": "In evaluating models for the study", "requirements": "n\u2022 Qwen/Qwen2: This model was excluded due to a technical limitation related to the size of audio files it can process. Specifically", "Flamingo": "Although Flamingo is a powerful multimodal model that excels in combining image and text processing, it was excluded because its audio capabilities were not robust enough for the study's focus. The research aimed to evaluate models designed for handling audio and text data, while Flamingo is more oriented toward tasks involving visual and textual data. Its strength lies in few-shot learning"}]}