{"title": "RAMer: Reconstruction-based Adversarial Model for Multi-party Multi-modal\nMulti-label Emotion Recognition", "authors": ["Xudong Yang", "Yizhang Zhu", "Nan Tang", "Yuyu Luo"], "abstract": "Conventional Multi-modal multi-label emotion\nrecognition (MMER) from videos typically as-\nsumes full availability of visual, textual, and acous-\ntic modalities. However, real-world multi-party set-\ntings often violate this assumption, as non-speakers\nfrequently lack acoustic and textual inputs, lead-\ning to a significant degradation in model perfor-\nmance. Existing approaches also tend to unify\nheterogeneous modalities into a single representa-\ntion, overlooking each modality's unique charac-\nteristics. To address these challenges, we propose\nRAMer (Reconstruction-based Adversarial Model\nfor Emotion Recognition), which leverages adver-\nsarial learning to refine multi-modal representa-\ntions by exploring both modality commonality and\nspecificity through reconstructed features enhanced\nby contrastive learning. RAMer also introduces a\npersonality auxiliary task to complement missing\nmodalities using modality-level attention, improv-\ning emotion reasoning. To further strengthen the\nmodel's ability to capture label and modality in-\nterdependency, we propose a stack shuffle strategy\nto enrich correlations between labels and modality-\nspecific features. Experiments on three bench-\nmarks, i.e., MEmoR, CMU-MOSEI, and M\u00b3ED,\ndemonstrate that RAMer achieves state-of-the-art\nperformance in dyadic and multi-party MMER sce-\nnarios. The code will be publicly available at\nhttps://github.com/Sootung/RAMer", "sections": [{"title": "1 Introduction", "content": "Emotion recognition from videos is crucial for advancing\nhuman-computer interaction and social intelligence. Multi-\nmodal, multi-label emotion recognition (MMER) leverages\nvisual, textual, and acoustic signals to identify multiple emo-\ntions (e.g., happy, sad) simultaneously [Zhang et al., 2020;\nZhang et al., 2021a]. Traditional MMER methods, as shown\nin Figure 1(a), typically focus on monologue or dyadic set-\ntings, assuming all modalities are fully available. However,\nreal-world conversations often involve multiple participants\n(i.e., multi-party scenarios) with incomplete modality data for\nnon-speakers who always lack acoustic and textual signals.\nMulti-party MMER, a more complex and practical set-\nting, introduces three key challenges. Firstly, handling in-\ncomplete modalities is a significant challenge. Most exist-\ning approaches [Zhang et al., 2021a; Zhang et al., 2022; Ge\net al., 2023] assume full modality availability and indepen-\ndently encode each modality, ignoring missing data. While\nsome methods [Ghosal et al., 2019; Hu et al., 2021] model\nconversational context by capturing speaker dependencies,\nthey struggle in multi-party scenarios where non-speakers\noften lack key modalities, leading to poor emotion recog-\nnition performance. Secondly, representing diverse modal-\nities effectively remains challenging. Current fusion strate-\ngies, such as aggregation-based methods (e.g., concatenation,\naveraging)[Shen et al., 2020] and hybrid approaches [Man-\nzoor et al., 2023], project modalities into a shared subspace,\noften neglecting their unique characteristics and reducing dis-\ncriminative ability. Recent methods [Zhang et al., 2022] at-\ntempt to separate modality-specific and shared features but\noften suffer from information loss due to inadequate handling\nof inter-modal correlations. Similarly, methods preserving\nmodality-specific information [Peng et al., 2023] may over-\nlook cross-modal commonalities, limiting their ability to fully\ncapture inter-modal relationships. Finally, multi-label learn-\ning presents challenges in modeling robust label correlations\nand capturing complex interdependency between modali-\nties and labels. Existing approaches [Cisse et al., 2013;\nMa et al., 2021] often fail to fully exploit collaborative la-\nbel relationships. Moreover, emotions vary across modalities,\nand different emotions rely on distinct modality features, fur-\nther complicating the task.\nTo address these issues, we propose RAMer, a novel\nframework designed to tackle the challenges of the Multi-\nparty MMER problem. RAMer integrates multimodal rep-\nresentation learning with multi-label modeling to effectively\nhandle incomplete modalities in multi-party settings.\nAs illustrated in Figure 1(b), RAMer addresses the chal-\nlenges of multi-party MMER by following techniques. To\naddress the challenge of incomplete modalities, we propose\nan auxiliary task that incorporates external knowledge, such\nas personality traits, to complement the existing modalities.\nLeveraging this, we employ modality-level attention mech-\nanisms to capture both inter- and intra-personal features. A\nreconstruction-based network is utilized to recover the fea-\ntures of any modality by leveraging information from the\nother modalities.\nTo represent diverse modalities effectively and capture dis-\ncriminative features, we design an adversarial network that\nextracts commonality across modalities while amplifying the\nspecificity inherent to each one. This helps ensure minimal\ninformation loss during the fusion process.\nAdditionally, to model robust interconnections between\nmodalities and labels, we propose a novel modality shuffle\nstrategy. This strategy enriches the feature space by shuffling\nboth samples and modalities, based on the commonality and\nspecificity of the modalities, improving the model's ability to\ncapture label correlations and modality-to-label relationships.\nIn summary, the contributions of this work are:\n\u2022 A Novel Model for the Multi-party MMER Problem. We\npresent RAMer, a new model designed to address the\nMulti-party Multi-modal Multi-label Emotion Recogni-\ntion problem. RAMer uses adversarial learning to cap-\nture both commonality and specificity across multiple\nmodalities, improving emotion recognition even with in-\ncomplete modality data.\n\u2022 Optimization Techniques. To enhance the robustness of\nmulti-party emotion recognition, RAMer employs con-\ntrastive learning to enrich reconstructed features and in-\ntegrates a personality auxiliary task to capture modality-\nlevel attention. We also propose a stack shuffle strat-\negy, enhancing the modeling of label correlations and\nmodality-to-label relationships by leveraging the com-\nmonality and specificity of different modalities.\n\u2022 Extensive Experiments. We conduct comprehensive ex-\nperiments on three benchmarks, i.e., MEmoR, CMU-\nMOSEI, and M\u00b3ED, across various conversation sce-\nnarios. Results show that RAMer surpasses existing"}, {"title": "2 Related Work", "content": "Multi-modal Representation Learning. Emotion recogni-\ntion has progressed from uni-modal approaches [Huang et al.,\n2021; Saha et al., 2020], which rely on a single modality, to\nmulti-modal methods [Mittal et al., 2020; Lv et al., 2021;\nZhang et al., 2022] that exploit complementary features\nacross modalities. While uni-modal approaches often face\nrecognition biases [Huang et al., 2021], multi-modal learning\nhas gained significant attention, with a key challenge being\nthe effective integration of heterogeneous modalities. Early\nfusion methods, such as concatenation [Ngiam et al., 2011a],\ntensor fusion [Liu et al., 2018a], and averaging [Hazirbas\net al., 2017], struggle with modality gaps that hinder ef-\nfective feature alignment. To address this, attention-based\nmethods [Ge et al., 2023; Tsai et al., 2019] leverage cross-\nattention mechanisms to dynamically align features in the\nl latent space, while contrastive learning [Chen et al., 2020;\nPeng et al., 2023] further improves robustness. However,\nmost attention-based methods consolidate modalities into a\njoint embedding, often overlooking the unique characteristics\nof each modality.\nMulti-label Emotion Recognition in Videos. Multi-label\nemotion recognition in videos involves assigning multiple\nemotion labels to a target individual in a video sequence.\nEarly methods treated multi-label classification as indepen-\ndent binary tasks [Boutell et al., 2004], but recent ad-\nvancements explore label correlations using techniques like\nAdjacency-based Similarity Graph Embedding(ASGE) [You\net al., 2020], Graph Convolutional Network (GCN) [Chen et\nal., 2019], and multi-task pattern [Tsai and Lee, 2020] to ex-\nplore label correlations. Some noteworthy strategies [Zhang\net al., 2021b; Zhang et al., 2022] focus on modeling label-\nfeature correlations through label-specific representations en-\nabled by visual attention [Chen et al., 2019] and transform-\ners [Zhang et al., 2022]. Beyond monologue settings, MMER\nin conversations has gained interest [Poria et al., 2018], using\nGCN [Ghosal et al., 2019] and memory networks [Hazarika\net al., 2018] to model dynamic speaker interactions. How-\never, multi-party MMER presents significant challenges as it\nextends beyond recognizing emotions for individual speak-\ners to handling multiple characters with incomplete modali-\nties. Additionally, the influence of individual personalities on\nlabel-feature correlations remains underexplored.\nAdversarial Training. Adversarial training (AT) [Goodfel-\nlow et al., 2014], involves two models: a discriminator that\nestimates the probability of samples, and a generator that cre-\nates samples indistinguishable from actual data. This setup\nforms a minimax two-player game, enhancing the robustness\nof the model. The technique has since been adapted for CV\nand NLP applications [Wang et al., 2017]. For instance, Miy-\nato et al. [Miyato et al., 2016] extended AT to text categoriza-\ntion by introducing perturbations to word embeddings. Wu\net al. [Wu et al., 2017] applied it within a multi-label learn-\ning framework to facilitate relationship extraction. Addition-\nally, AT has been used to learn joint distributions between"}, {"title": "3 Problem Formulation", "content": "In this section, we introduce the notations used and for-\nmally define the Multi-party Multi-modal Multi-label Emo-\ntion Recognition (Multi-party MMER) problem.\nNotations. We use lowercase letters for scalars (e.g., \u03c5),\nuppercase letters for vectors (e.g., Y), and boldface for ma-\ntrices (e.g., X). A data sample is represented by the tuple\n(V, Pt, Sr, Et,r), where:\n\u2022 V = ( {P_t}_{t=1}^T , {S_r}_{r=1}^R ) is a video clip containing T\npersons and R semantic segments.\n\u2022  {P_t}_{t=1}^T refers to the set of target persons, and  {S_r}_{r=1}^R\nrepresents the target segments, each annotated with an\nemotion moment.\n\u2022 Et,r denotes the labeled emotion for person Pt in Sr.\nEach sample is characterized by multiple modalities, in-\ncluding visual (v), acoustic (a), textual (t), and personality\ntraits (p).\nFor each modality m\u2208 {v, a, t, p}, the corresponding fea-\ntures are represented as (X_1^m, X_2^m,...,X_l_k^m), where X^m \u2208\n\u211d^{l_k\u00d7d_k} represents the feature space of the k-th modality.\nHere: lk denotes the sequence length, and dk denotes the di-\nmension of the modality.\nLet Y = {Y1,Y2,\u00b7\u00b7\u00b7, y_c} represent a label space with c\npossible emotion labels.\nMulti-party MMER Problem. Given a training dataset\nN\nD = {(X_1^{1,2,...,m}, Y_t)}_{t=1}^N with N data samples, where:\n(1) Xm \u2208 Xm represents the features for each modality m\nin sample t, and (2) Y\u2081 = {0,1}^Y is a multi-hot vector in-\ndicating the presence (1) or absence (0) of emotion labels,\nwhere Y = 1 indicates that sample 7 belongs to class v, and\nY = 0 otherwise. The goal of the Multi-party MMER prob-\nlem is to learn a function F : X_1 x X_2 x . . x X_m \u2192 Y that\npredicts the target emotion Et,r for person Pt in segment Sr,\nleveraging contextual information from multiple modalities.\nDiscussion. It is important to note that the target person\nPt may have incomplete modality information, meaning they\nmay not simultaneously possess visual, textual, or acoustic\nrepresentations. This introduces uncertainty in the modality\nof the target segment Sr, making the prediction task more\nchallenging."}, {"title": "4 Methodology", "content": "Figure 2 shows the framework of RAMer, which con-\nsists of three components: auxiliary uni-modal embedding,\nreconstruction-based adversarial Learning, and stack shuffle\nfeature augmentation."}, {"title": "4.1 Auxiliary Uni-modal Embedding", "content": "To extract contextual information from each modality, we\nemploy four independent transformer encoders [Vaswani et\nal., 2017], each dedicated to a specific modality m.Each en-\ncoder consists of nm identical layers to ensure consistent and\ndeep representation. For real-world multi-party conversa-\ntion videos with T participants and incomplete modalities,\nwe introduce an optional auxiliary task leveraging person-\nality to complement missing modalities. Specifically, we\nconcatenate personality embedding Xp with each modality\nxm \u2208 {v,t,a} to enrich the feature space. We then apply\nthe scaled dot-product attention to compute inter-person at-\ntention across the person dimension within each segment, and\nintra-person attention along the segment dimension for each\nindividual [Shen et al., 2020]. This modality-level attention\nmechanism is designed to enhance the model's emotion rea-\nsoning ability by effectively capturing both interpersonal dy-\nnamics and temporal patterns within the data. In this way, we\nobtain personality enhanced representation Xm \u2208 \u211d^{l\u00d7d}."}, {"title": "4.2 Reconstruction-based Adversarial Learning", "content": "The second component focuses on leveraging multiple\nmodalities by capturing inter-modal commonalities while en-\nhancing the unique characteristics of each modality. To ad-\ndress the limitations of adversarial networks [Goodfellow et\nal., 2014; Zhang et al., 2022], which can result in infor-\nmation loss and difficulty in learning modality-label depen-\ndencies in incomplete modality scenarios, we introduce a\nreconstruction-based approach. This method uses contrastive\nlearning to create modality-independent but label-relevant\nrepresentations. By reconstructing missing modalities during\ntraining, the model ensures more robust performance.\nAdversarial Training. Considering the significance of\nboth specificity and commonality, we employ adversarial\ntraining to extract discriminative features. The uni-modal em-\nbeddings Xm are fed to three fully connected networks fm\nto extract specificity Sm,m \u2208 {v, a, t}. In parallel, Xm are\nalso passed through a reconstruction network, which is cou-\npled with a contrastive learning network, followed by a gen-\nerator G (; 0) to derive the commonality Cm. Both speci-\nficity and commonality are then passed through linear lay-\ners with softmax activation in the discriminator D (; 0D) that\nis designed to distinguish which modality the inputs come\nfrom. The generator captures commonality representation\nC'm by projecting different reconstructed embedding Xm\ninto a shared latent subspace, ensuring distributional align-\nment across modalities. Consequently, this architecture en-\ncourages the generator G(\u00b7; \u03b8G) to produce outputs that chal-\nlenge the discriminator D (\u00b7; 0D) by obscuring the source\nmodality of the representations Cm. That means the gener-\nator and discriminator are trained simultaneously in a game-\ntheoretic scenario, which intends to enhance the robustness of\nthe generated features against modality-specific biases. Both\nthe commonality adversarial loss Le and the specificity ad-\nversarial loss Ls are calculated by cross-entropy loss.\nIn the shared subspace, it is advantageous to employ a uni-\nfied representation of various modalities to facilitate multi-\nlabel classification. This representation is designed to elimi-"}, {"title": null, "content": "nate redundant information and extract the elements common\nto the different modalities, thereby introducing a common se-\nmantic loss defined as,\nL_{cml} = - \\sum_{m \\in \\{v,t,a\\}} \\frac{1}{N} \\sum_{r=1}^{N} \\sum_{v=1}^{c} y_{v,m} \\log \\hat{y}_{v,m} + (1 - y_{v,m}) \\log (1 - \\hat{y}_{v,m}),\nwhere \\hat{y}_{v,m} is predicted with Cm and y is the ground-truth\nlabel. In an effort to encode diverse aspects of multi-modal\ndata, an orthogonal loss L_{orth} is induced to ensure that the\nencoded subspaces for commonality Cm and specificity Sm\nrepresentations maintain distinctiveness by minimizing over-\nlap.\nL_{orth} = \\sum_{m \\in \\{v,t,a\\}} \\frac{1}{N} \\sum_{r=1}^{N} ||(C^m_r)^T S^m_r||_F,\nwhere ||\u00b7|| F is Frobenius norm. Hereby, the objective of ad-\nversarial training Ladv is\nL_{adv} = \\lambda_{da} (L_c + L_s) + \\lambda_{do} L_{orth} + \\lambda_{c} L_{cml},\nwhere \u03bb\u03b1, \u03bb\u03bf and Ae are trade-off parameters.\nMulti-modal Feature Reconstruction. To reconstruct the\nfeatures of any modality by leveraging information from the\nother modalities. We employ a reconstruction network that\nis composed of modality-specific encoders \\epsilon_m, decoders dm,\nand a two-level reconstruction process utilizing multi-layer\nlinear networks g(\u00b7). Given input Xm from different modal-\nity, three encoders \\epsilon_m that consist of MLPs are utilized to\nproject Xm into latent embedding Zm within the latent space\nS\". Subsequently, three corresponding decoders dm trans-\nform these latent vectors into the decoded vectors X. At the\nfirst level of reconstruction network, the intrinsic vector Dm\nthat derived from contrastive learning network and semantic\nfeatures x{v,t,a}m are concatenated to form the input, which\nis processed to produce X used for the second-level recon-\nstruction network. Hereby, the reconstruction network can be\nformulated as,\n\\hat{X}^m = g(g(d_m (\\epsilon_m (X^m ; \\theta_m)), D^m)).\nThe obtained three embedding XM, X, and Xm from three\ndistinct feature spaces are fed into fully connected network\nfollowed by max pooling. We can formulate the reconstruc-\ntion loss Lrec and classification loss Llar as,\nL_{rec} = \\sum_{m=1}^M (||X^m - \\hat{X}^m||_F + ||X^m - \\hat{X}^m||_F),\nL = \\lambda_\\alpha \\mathcal{L}_B (S^v, Y) + \\lambda_\\beta \\mathcal{L}_B (S^a, Y) + \\lambda_\\gamma \\mathcal{L}_B (S^t, Y),\nwhere ||||F is the Frobenius norm, \u03bb\u03b1,\u03b2,\u03b3 are trade-off pa-\nrameters, LB is the binary cross entropy (BCE) loss.\nTo capture the feature distributions of different modal-\nities and use them to guide the restoration of incomplete\nmodalities, intrinsic vectors \u0189m obtained through a su-\npervised contrastive learning network [Khosla et al., 2020]\nare incorporated into the reconstruction network. The en-\ncoders \u025bm project input Xm to contrastive embeddings Zm,\""}, {"title": null, "content": "{z_{\\sigma_i}^m}, \\sigma \\in {\\alpha,\\beta,\\gamma}. Given a contrastive embedding set Z =\nZ^{\\{v,t,a\\}},\nan anchor vector zi \u2208 Z and assuming the pro-\ntotype vector updated during the training process based on\nthe moving average is u_{j,k}^m, where modality m \u2208 {v,t, a},\nlabel category j \u2208 [S], label polarity k \u2208 {pos, neg}, then\nthe intrinsic vector D^m can be derived from:\n\\delta_{j,k}^m = \\frac{exp(z_i \\cdot u_{j,k}^m)}{\\sum_{O_{j',k'}\\{pos, neg\\}} exp(z_i \\cdot u_{j',k'}^m)}\nD^m = d_m ([\\delta_m,..., \\delta_m]; \\theta_m).\nThe loss of contrastive learning network is defined as,\nL_{scl}(i, \\mathcal{Z}) = \\sum_{i\\in \\mathcal{Z}} \\frac{1}{P(i)} \\log \\frac{\\exp (z_i \\cdot z_{p/n})}{\\sum_{r \\in A(i)} \\exp(z_i \\cdot z_{r/n})},\nwhere P(i) is the positive set, \u03b7 \u2208 \u211d+ is a temperature pa-\nrameter, and A(i) = Z \\ {i}."}, {"title": "4.3 Stack Shuffle for Feature Augmentation", "content": "To construct more robust correlations among labels and\nmodel the complex interconnections between modalities and\nlabels, we propose a multi-modal feature augmentation strat-\negy that incorporates a stack shuffle mechanism. As shown\nby Figure ?? in Appendix, after obtaining the commonality\nand specificity representations, we perform sample-wise and\nmodality-wise shuffling processes sequentially on a batch of\nsamples. To strengthen the correlations between labels, we\nfirst apply a sample-wise shuffle. The features derived from\nC and Sm are split into k stacks along the sample dimension,\nwith the top elements of each stack cyclically popped and ap-\npended to form new vectors. Next, a modality-wise shuffle\nis introduced to help the model capture and integrate infor-\nmation across different modalities. For each sample, features\nare divided into stacks along the modality dimension, and it-\nerative pop-and-append operations are applied. Finally, the\nshuffled samples V are used to fine-tune the classifier cs with\nthe binary cross-entropy (BCE) loss.\nC_{suf} = \\frac{1}{N} \\sum_{m \\in \\{v,t,a\\}} \\sum_{r=1}^{N} -\\mathcal{L}og (Y_m \\log (c(V))),\nCombing the Eq.(3), Eq.(5) ~ Eq.(9) and Eq.(10), the final\nobjective function L is formulated as,\nL = C_{suf} + Cls + ArLrec + AsLscl + Ladv,\nwhere \u03bb, \u03bb\u03c2 are trade-off parameters."}, {"title": "5 Experiments", "content": "Datasets and Metrics. We evaluated RAMer on three mul-\ntimodal, multi-label benchmark datasets: MEmoR [Shen et\nal., 2020], a multi-party conversation dataset that includes\npersonality, and CMU-MOSEI [Zadeh et al., 2018] and\nM\u00b3ED [Zhao et al., 2022], which are dyadic conversation\ndatasets that do not include personality information. The"}, {"title": null, "content": "evaluation is conducted under the protocols of these datasets.\nFor CMU-MOSEI and M\u00b3ED, we employed four commonly\nused evaluation metrics: Accuracy (Acc), Micro-F1, Preci-\nsion (P), and Recall (R). Due to data imbalance in MEmoR,\nwe followed the benchmark's protocol and used Micro-F1,\nMacro-F1, and Weighted-F1 metrics.\nBaselines. For the MEmoR dataset, we compare RAMer\nwith multi-party conversation baselines, including MDL\nMDAE [Ngiam et al., 2011b], BiLSTM+TFN [Zadeh\net al., 2017], BiLSTM+LMF [Liu et al., 2018b], Dia-\nlogueGCN [Ghosal et al., 2019], DialogueCRN [Hu et al.,\n2021], and AMER [Shen et al., 2020]. We also evaluate its\nrobustness against recent models designed for dyadic con-\nversations, such as CARAT [Peng et al., 2023] and TAI-\nLOR [Zhang et al., 2022]. For the CMU-MOSEI and M\u00b3ED\ndatasets, we test three categories of methods. 1) Classic meth-\nods. CC [Read et al., 2011], which concatenates all avail-\nable modalities as input for binary classifiers. 2) Deep-based\nmethods. ML-GCN [Chen et al., 2019], using Graph Con-\nvolutional Networks to map label representations and cap-\nture label correlations. 3) Multi-modal multi-label methods.\nThese include MulT [Tsai et al., 2019] for cross-modal inter-\nactions, MISA [Hazarika et al., 2020] for learning modality-\ninvariant and modality-specific features, and methods like\nMMS2S [Zhang et al., 2020], HHMPN [Zhang et al., 2021a],\nTAILOR [Zhang et al., 2022], AMP [Ge et al., 2023], and\nCARAT [Peng et al., 2023]."}, {"title": "5.2\nComparison with the state-of-the-arts", "content": "We present the performance comparisons of RAMer on the\nMEMOR, CMU-MOSEI, and M3ED datasets in Table 1, Ta-\nble 2, and Table 3, respectively, with following observations.\n1) On the MEmoR dataset, RAMer outperforms all base-\nlines by a significant margin. While TAILOR achieves a high\nweighted-F1 score in the fine-grained setting, its overall per-\nformance is weaker due to biases toward frequent and easier-\nto-recognize classes. RAMer consistently delivers strong re-\nsults across all settings, demonstrating its ability to learn\nmore effective representations. 2) On the CMU-MOSEI and\nM3ED datasets, RAMer surpasses state-of-the-art methods\non all metrics except recall, which is less critical compared\nto accuracy and Micro-F1 in these contexts. 3) Deep-based\nmethods outperform classical ones, highlighting the impor-\ntance of capturing label correlations for improved classifica-\ntion performance. 4) Multimodal methods like HHMPN and\nAMP significantly outperform the unimodal ML-GCN, em-\nphasizing the necessity of multimodal interactions. 5) Models\noptimized for dyadic conversations, such as CARAT, experi-\nence a notable performance drop in multi-party settings with\nincomplete modalities. In contrast, RAMer excels in both\nscenarios, achieving substantial improvements in Macro-F1\nscores on the MEmoR dataset, outperforming CARAT by\n0.178 and 0.209, respectively."}, {"title": "5.3 Ablation Study", "content": "To better understand the importance of each component of\nRAMer, we compared various ablated variants.\nAs shown in Table 4, we make the following observations:"}, {"title": null, "content": "\u2022 The specificity and commonality enhance MMER per-\nformance. Variants (1), (2), and (3) exhibit an approx-\nimately 0.05 decrease in Micro-F1 performance com-\npared to variant (11). This indicates that jointly learn-\ning specificity and commonalities yields superior perfor-\nmance, underscoring the importance of capturing both\nmodality-specific specificity and shared commonality.\n\u2022 Contrastive learning benefits the MMER. The inclusion\nof loss functions Lscl in adversarial training leads to\nprogressive performance improvements, as evidenced by"}, {"title": null, "content": "the superior results of (4).\n\u2022 Feature reconstruction net benefits MMER. Variants (5),\n(6), (7) are worse than (11), and (8) shows an 0.045\ndecrease in Micro-F1, which indicates that feature re-\nconstruction can improve model performance. When\nthe entire reconstruction process is omitted, the perfor-\nmance of (8) declines even more compared to (6) and\n(7), confirming the effectiveness of multi-level feature\nreconstruction in achieving multi-modal fusion.\n\u2022 Changing the fusion order leads to poor performance,\nvariants (9) and (10) perform worse than (11). It vali-\ndates the rationality and optimality of feature fusion."}, {"title": "5.4 Qualitative Analysis", "content": "Visualization of Learned Modality Representations.\nTo evaluate the effectiveness of reconstruction-based adver-\nsarial training in generating distinguishable representations,\nwe used t-SNE to visualize the commonality representations\nC{v,a,t} and specificity representations S{v,a,t} learned in\nthe aligned CMU-MOSEI dataset. In figure 3(a), without\nadversarial training, specificity and commonality are loosely\nseparated, but their distributions overlap in certain areas, such\nas the lower-right corner. In contrast, Figure 3(b) shows that\nwith adversarial training, commonality and specificity are\nclearly separated in latent subspaces, forming distinct bound-\naries and effectively distinguishing emotions across modal-\nities. Figure 3(c) demonstrates that without the reconstruc-"}, {"title": null, "content": "tion net (RN) and contrastive learning net (CLN), represen-"}, {"title": null, "content": "tations of different modalities are distinguishable, but emo-\ntion labels within the same modality remain intermixed. In\ncontrast, Figure 3(d) shows that embeddings of both differ-\nent modalities and labels are distinctly separable, highlight-\ning that the reconstruction-based module effectively enhances\nmodal specificity. Overall, RAMer accurately captures both\nthe commonality and specificity of different modalities.\nVisualization of Modality-to-Label Correlations.\nTo explore the relationship between modalities and labels, we\nvisualized the correlation of labels with their most relevant\nmodalities. As shown in Figure 4, regardless of the presence\nof adversarial training, different emotion label is influenced\nby different modalities. For instance, surprise is predomi-\nnantly correlated with the acoustic modality, while anger is\nprimarily associated with the visual modality. This indicates\nthat each modality captures the distinguishable semantic in-\nformation of the labels from distinct perspectives."}, {"title": "Case Study", "content": "To demonstrate RAMer's robustness in complex scenarios,\nFigure 5 shows an example of multi-party emotion recogni-\ntion on MEmoR dataset where specific target persons have\nincomplete modality signals. The top three rows display\ndifferent modalities from a video clip, segmented semanti-\ncally with aligned multi-modal signals. Key observations\ninclude: 1) The target moment requires recognizing emo-\ntions for both the speaker (e.g., Howard) and non-speakers\n(e.g., Penny and Leonard). While the speaker typically has\ncomplete multi-modal signals, non-speakers often lack cer-\ntain modalities. TAILOR, relying on incomplete modalities,\nproduced partial predictions as its self-attention mechanisms\nstruggled to align labels with missing features. 2) Limita-"}, {"title": null, "content": "tions of single or incomplete modalities. A single modality,\nsuch as text, is often insufficient for accurate emotion infer-\nence (e.g., only Howard's Joy is detectable from text alone).\nAlthough CARAT attempts to reconstruct missing informa-\ntion, it fails to capture cross-modal commonality, leading to\nincorrect predictions. 3)The importance of inter-person inter-\nactions and external knowledge is evident in emotion recogni-\ntion, where inter-person attention enables individuals with in-\ncomplete modalities to gain supplementary information from\nothers. Moreover, integrating external knowledge, such as\npersonality traits, enhances emotion reasoning across partic-\nipants and contexts, emphasizing the synergy between user\nprofiling and emotion recognition. Experimental results vali-\ndate that RAMer demonstrates superior robustness and effec-\ntiveness in these challenging, real-world scenarios."}, {"title": "6 Conclusion", "content": "In this paper, we proposed RAMer, a framework that re-\nfines multi-modal representations using reconstruction-based\nadversarial learning to address the Multi-party Multi-modal\nMulti-label Emotion Recognition problem. RAMer captures\nboth the commonality and specificity across modalities using\nan adversarial learning module, with reconstruction and con-\ntrastive learning enhancing its ability to differentiate emotion\nlabels, even with missing data. We also introduce a personal-\nity auxiliary task to complement incomplete modalities, im-\nproving emotion reasoning through modality-level attention.\nFurthermore, the stack shuffle strategy enriches the feature\nspace and strengthens correlations between labels and modal-\nities. Extensive experiments on three datasets demonstrate\nthat RAMer consistently outperforms state-of-the-art meth-\nods in both dyadic and multi-party MMER scenarios."}]}