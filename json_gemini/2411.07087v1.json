{"title": "OCMDP: Observation-Constrained Markov Decision Process", "authors": ["Taiyi Wang", "Jianheng Liu", "Jiaye Li", "Zhihao Wu", "Yu Wu"], "abstract": "In many practical applications, decision-making processes must balance the costs of acquiring information with the benefits it provides. Traditional control systems often assume full observability, an unrealistic assumption when observations are expensive. We tackle the challenge of simultaneously learning observation and control strategies in such cost-sensitive environments by introducing the Observation-Constrained Markov Decision Process (OCMDP), where the policy influences the observability of the true state. To manage the complexity arising from the combined observation and control actions, we develop an iterative, model-free deep reinforcement learning algorithm that separates the sensing and control components of the policy. This decomposition enables efficient learning in the expanded action space by focusing on when and what to observe, as well as determining optimal control actions, without requiring knowledge of the environment's dynamics. We validate our approach on a simulated diagnostic task and a realistic healthcare environment using HeartPole. Given both scenarios, the experimental results demonstrate that our model achieves a substantial reduction in observation costs on average, significantly outperforming baseline methods by a notable margin in efficiency.", "sections": [{"title": "1 Introduction", "content": "In traditional control systems, it is often assumed that all necessary information is readily available, which is seldom the case in practical scenarios [1, 29, 28, 27, 26]. The need to actively decide which observations to make adds a layer of complexity to the decision-making process, as it requires balancing the benefits of additional information against the costs of acquiring it [10, 12]. Additionally, tasks in virtual environments, such as those running on simulators, often disregard observation costs because the optimization goals\u2014such as maximizing rewards or achieving specific objectives\u2014are not inherently aligned with the expenses involved in acquiring observations [19, 9]. This disconnect allows for the assumption of complete knowledge of the environment, but ignoring observation costs in such models makes reinforcement learning applications diverge from real-world practice.\nIn many real-world applications, particularly in healthcare, decision-making processes must account for the costs associated with actively obtaining observations. Medical assessments, diagnostic tests, and patient monitoring not only require financial resources but also demand significant time from healthcare professionals and patients alike. This inherent cost associated with information gathering necessitates strategies that judiciously balance the need for information with the resources available [3, 32]. Medical assessments can be regarded as sequential decision-making problems, where treatments are administered based on the patient's current health states. These health states are inferred from observations, including physical examinations and clinical metrics. The challenge lies in making optimal decisions with limited and costly observations, which is critical for both patient outcomes and resource management [13, 21, 4].\nThis paper addresses the challenge of simultaneously learning an observation strategy and a control strategy in environments where observations are costly. In such settings, there is a fundamental trade-off between the cost of acquiring observations and the benefit they provide in making informed control decisions. We consider the simplest case of observation action design, where each observation action is a binary decision: whether to acquire a specific observation or not. This formulation leads to a total action space of $2^{|O|} \\times |A_{control}|$, where $|O|$ is the number of possible observations to make and $A_{control}$ is the number of control actions. This exponential growth in the action space introduces the curse of dimensionality, making the learning process significantly more complex than when learning a control policy alone.\nTo tackle this challenge, we propose an iterative, model-free deep reinforcement learning approach that decomposes the sensing and control policies. By separating these two aspects, the learning algorithm can more efficiently navigate the enlarged action space. The model focuses on learning when and what to observe, alongside determining the optimal control actions, without requiring a complete model of the environment's dynamics [23, 11].\nWe demonstrate the effectiveness of our method on two fronts of medical practice: a simulated Diagnostic Chain task designed to capture the essential elements of the problem, and a realistic healthcare environment using HeartPole [17]. These experiments showcase the model's ability to make cost-effective observation decisions while achieving desirable control outcomes."}, {"title": "2 Problem Formalism", "content": "We address environments where the agent must actively decide not only on control actions but also on whether to acquire observations, each potentially incurring a cost. This scenario extends the classic Partially Observable Markov Decision Process (POMDP) by allowing the agent's policy to influence the observability of the environment's state. We formalize this setting as an Observation Constrained Markov Decision Process (OCMDP).\nAn OCMDP is defined by the tuple $M = (S, A, O,T, Z,R, C, \\gamma)$, where: $S$ is the full state space, which can be any measurable set (not necessarily a subset of $R^n$). $A = A^c \\times A^o$ is the composite action space, consisting of control actions $A^c$ and observation actions $A^o$. $O$ is the set of possible observations, augmented with a null observation (\u00d8 indicating no observation. $T : S \\times A^o \\rightarrow \\mathcal{M}(S)$ is the state transition function, where $\\mathcal{M}(S)$ denotes the set of probability distributions over $S$. $Z : S \\times A^o \\rightarrow \\mathcal{M}(O)$ is the observation function, dependent on both the state and the observation action. $R : S \\times A^o \\rightarrow \\mathbb{R}$ is the reward function. $C : A^o \\rightarrow \\mathbb{R}_{\\geq o}$ is the cost function associated with making observations. $\\gamma \\in [0, 1)$ is the discount factor.\nAt each time step $t$, the agent proceeds as follows: First, the agent selects an observation action $a_t^o \\in A^o$ using the observation policy $\\pi^o$, based on its current history $h_t$ or belief state $b_t$: $a_t \\sim \\pi^o(a | h_t)$. Then, the agent receives an observation $o_t$ based on the observation function, which depends on the current full state $s_t$ (might be unobservable) and the selected observation action $a_t^o$: $O_t \\sim Z(O_t | S_t, a_t^o)$. The agent updates its history $h_t$ to include the new observation $o_t$, potentially updating its belief state $b_t$. Next, the agent selects a control action $a_t^c \\in A^c$ using the control policy $\\pi^c$, conditioned on the updated history or belief state: $a_t^c \\sim \\pi^c(a | h_t, O_t)$. The environment"}, {"title": "3 Related Work", "content": "POMDP Generally speaking, Partially Observed Markov Decision Process (POMDP, [24, 15]) is the special case of the setting considered in this work, as the latent system state (e.g., the state of the patient) can only be inferred based on partial observations, However, distinguished from vanilla POMDP problems where the observation space is fixed, we consider the observation as an active decision that can varies across states. From this perspective, our problem setting reduces to POMDP when a trivial always observe strategy is applied.\nActive Sensing Active sensing is a framework where an agent actively decides which observations to acquire to improve task performance while minimizing the associated costs [31]. In environments where observations are costly or resource-intensive, active sensing enables agents to selectively focus on the most informative data. Our work relates to active sensing in that setting the discount factor $\\gamma$ to zero and defining the reward function R as a classifier for a downstream task reduces our learning objective in Eqn. (1) to the traditional active sensing task. However, our OCMDP solver extends beyond conventional active sensing by incorporating sequential decision-making over time. Unlike standard active sensing, which typically involves single-step decisions in static environments, our approach addresses the challenges of making both action and observation decisions in dynamic environments across multiple time steps. By considering a positive discount factor $\\gamma > 0$, we balance immediate observation costs with long-term benefits, enabling a strategy that accounts for the future impact of current decisions. This integration allows us to tackle more complex scenarios where traditional active sensing methods may not be feasible.\nModel-Based Methods Model-based reinforcement learning (RL) approaches incorporate a model of the environment's dynamics into the policy learning process, enabling the agent to plan and make informed decisions based on predicted future states [25]. Recently, several studies have integrated sensing or observation strategies into the policy learning framework. Additionally, Yin et al. [30] introduces the task of Active Feature Acquisition, which emphasizes representation learning. Their approach utilizes Variational Autoencoders (VAEs) for imputing missing features and inferring latent states, operating within the broader category of model-based RL. These model-based methods typically require accurate models of the environment or observation processes, which can be difficult to obtain in complex or high-dimensional settings. In contrast, our approach adopts a model-free reinforcement learning methodology, circumventing the need for explicit environment models. By leveraging recent advances in model-free deep RL, our method can be more scalable and adaptable to diverse and intricate environments where modeling the dynamics is impractical or infeasible. This allows our OCMDP solver to effectively handle the uncertainties and complexities inherent in partially observable settings without relying on predefined models.\nML-enhanced Decision Making in Healthcare Machine learning (ML) has significantly advanced decision-making processes in healthcare by enabling more accurate diagnoses, personalized treat- ment plans, and efficient resource allocation [6, 20]. Supervised learning techniques, such as deep neural networks, have been employed to analyze medical imaging data, leading to breakthroughs in detecting conditions like diabetic retinopathy and skin cancer with performance comparable to"}, {"title": "4 The Model-Free Approach", "content": "We propose a model-free framework to address the Active Observation Markov Decision Process (AOMDP) problem. This framework involves two distinct policies: an observation policy $\\pi^o$ and a control policy $\\pi^c$, which together form the combined policy $\\pi = (\\pi^o, \\pi^c)$.\n4.1 Trajectory-Based Action-Value Function\nWe define the trajectory-based action-value function $Q^{\\pi}(h_t, a_t)$ as:\n$Q^{\\pi}(h_t, a_t) = \\mathbb{E}_{\\tau \\sim \\pi} \\Big[ \\sum_{k=0}^{\\infty} \\gamma^k (r(s_{t+k}, a_{t+k}) - c_{a_{t+k}}^o) \\Big]$ (2)\n$h_t$ is the history up to time $t$, $a_t$ is the action taken at time $t$, $\\gamma \\in [0, 1)$ is the discount factor, $r(s, a)$ represents the reward for taking control action $a^c$ in state $s$, $c$ is the cost vector associated with observation actions, $a^o$ denotes the observation actions. This function captures the expected cumulative reward minus the cumulative observation cost over an infinite horizon. For analytical convenience, we decompose $Q^{\\pi}(h_t, a_t)$ into two components:\n$Q^{\\pi}(h_t, a_t) = R^{\\pi}(h_t, a_t) - C^{\\pi}(h_t, a_t)$,\nwhere $R^{\\pi}(h_t, a_t) = \\mathbb{E}_{\\tau \\sim \\pi} \\Big[ \\sum_{k=0}^{\\infty} \\gamma^k r(s_{t+k}, a_{t+k}^c) \\Big]$,\n$C^{\\pi}(h_t, a_t) = \\mathbb{E}_{\\tau \\sim \\pi} \\Big[ \\sum_{k=0}^{\\infty} \\gamma^k c_{a_{t+k}}^o \\Big]$\nHere, $R^{\\pi}(h_t, a_t)$ represents the expected cumulative reward, and $C^{\\pi}(h_t, a_t)$ represents the expected cumulative observation cost.\n4.2 Iterative Optimization\nOur approach employs a model-free, iterative optimization framework that alternates between refining the control and observation policies to achieve optimal performance. This methodology leverages the interplay between control actions and observation strategies, ensuring that each policy enhancement step contributes to the overall improvement of the system's behavior.\nInitially, we focus on optimizing the control policy while keeping the observation policy fixed. By holding the observation policy $\\pi^o$ constant, we isolate the control dynamics and aim to maximize the expected cumulative reward. The control-specific action-value function is defined as:\n$Q_c^{\\pi}(h_t, a_t) = \\mathbb{E}_{\\pi, \\pi^o} \\Big[ \\sum_{k=0}^{\\infty} \\gamma^k r_o(s_{t+k}, a_{t+k}^c) \\Big]$ (3)\nwhere the modified reward incorporates observation costs:$r_o(s, a^c) = r(s, a^c) - c \\overline{a^o}$."}, {"title": "4.3 Policy Gradient Optimization", "content": "Here, $\\overline{a^o}$ represents the observation actions dictated by the fixed observation policy $\\pi^o$. By optimizing the control policy $\\pi^c$, we ensure that the agent selects actions that not only maximize rewards but also account for the costs associated with observations, effectively embedding the observation strategy within the control dynamics.\nSubsequently, we shift our focus to optimizing the observation policy while keeping the control policy fixed. The objective here is to minimize the expected cumulative observation costs without compromising the reward structure established by the control policy. The observation-specific action-value function is articulated as:\n$Q_o^{\\pi}(h_t, a_t) = \\mathbb{E}_{\\pi^o, \\overline{\\pi^c}} \\Big[ \\sum_{k=0}^{\\infty} \\gamma^k r_c(s_{t+k}, a_{t+k}^c) \\Big]$ (4)\nwhere the reward is adjusted based on the control actions: $r_c(s, a^o) = r(s, \\overline{a^c}) - c \\overline{a^o}$.\nIn this context, $\\overline{a^c}$ denotes the control actions determined by the fixed control policy $\\pi^c$. Optimizing the observation policy $\\pi^o$ under these conditions ensures that the agent selectively acquires infor- mation, balancing the trade-off between the cost of observations and the benefits they provide in enhancing decision-making.\nThis iterative process of alternating between control and observation policy optimization is motivated by the need to decouple the complexities inherent in each policy type. By isolating the optimization steps, we can more effectively navigate the high-dimensional policy spaces, ensuring that improve- ments in one policy component do not inadvertently degrade the performance of the other. Moreover, this separation facilitates targeted enhancements, allowing for more nuanced adjustments that align with the specific objectives of control and observation strategies.\nTo operationalize the iterative policy optimization framework, we adopt policy gradient methods, which are well-suited for optimizing parameterized policies in continuous and high-dimensional spaces. We parameterize the control policy with parameters $\\theta$ and the observation policy with parameters $\\phi$, enabling flexible and scalable policy representations.\nThe optimization objectives for both policies are defined to align with their respective goals of maximizing rewards and minimizing observation costs. For the control policy, the objective is to maximize the expected cumulative reward, which is formalized as:\n$J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}, \\pi_{\\phi}} [Q_c^{\\pi}(h_t, a_t)]$. (5)\nThe gradient of this objective with respect to $\\theta$ is derived using the policy gradient theorem:\n$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}, \\pi_{\\phi}} [\\nabla_{\\theta} log \\pi_{\\theta}(a | h_t) \\cdot Q_c^{\\pi}(h_t, a_t)]$. (6)\nThis gradient expression guides the adjustment of $\\theta$ in a direction that increases the expected cumulative reward, effectively enhancing the control policy's performance.\nConversely, the observation policy aims to minimize the expected cumulative observation costs while maintaining the reward structure influenced by the control policy. This objective is encapsulated as:\n$J(\\phi) = - \\mathbb{E}_{\\pi_{\\theta}, \\pi_{\\phi}} [C^{\\pi}(h_t, a_t)]$. (7)\nThe negative sign indicates the minimization of observation costs. The corresponding gradient with respect to $\\phi$ is given by:\n$\\nabla_{\\phi} J(\\phi) = - \\mathbb{E}_{\\pi_{\\theta}, \\pi_{\\phi}} [\\nabla_{\\phi} log \\pi_{\\phi}^o(a | h_t) \\cdot Q_o^{\\pi}(h_t, a_t)]$. (8)\nThis gradient facilitates the reduction of observation costs by adjusting $\\phi$ in a direction that decreases the expected cumulative costs, thereby optimizing the observation policy's efficiency.\nThe integration of these policy gradient updates within the iterative optimization framework ensures that both control and observation policies are incrementally improved in a coordinated manner. This dual optimization strategy not only enhances the agent's ability to make informed and cost-effective decisions but also fosters a balanced trade-off between reward maximization and cost minimization. The resulting policies are thus finely tuned to navigate the complexities of the environment, leveraging observations judiciously to achieve superior performance."}, {"title": "4.4 Theoretical Analysis of Iterative Policy Optimization", "content": "As discussed in section 4.2, our COMDP solver employs a sequential optimization strategy, grounded in dynamic programming principles, to iteratively refine both control and observation strategies. The following sections provide the theoretical foundation supporting this approach.\nLemma 1 (Value Function Contraction). Consider the Bellman optimality operator T defined as:\n$TQ(h_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}_{s_{t+1}} \\Big[ \\max_{a_{t+1}} Q(h_{t+1}, a_{t+1}) \\Big]$.\nUnder the supremum norm $||Q||_{\\infty} = sup_{h_t, a_t} |Q(h_t, a_t)|$, the operator T acts as a contraction mapping with a contraction factor of $\\gamma$. Repeated application of T guarantees convergence to the unique optimal action-value function $Q^*$.\nProof. To establish that T is a contraction, consider any two value functions $Q_1$ and $Q_2$. We aim to show that:\n$||TQ_1 - TQ_2||_{\\infty} \\leq \\gamma ||Q_1 - Q_2||_{\\infty}$.\nStarting from the definition of the supremum norm:\n$||TQ_1 - TQ_2||_{\\infty} = sup_{h_t, a_t} |TQ_1(h_t, a_t) - TQ_2(h_t, a_t)|$\n$= \\gamma sup_{h_t, a_t} \\mathbb{E}_{s_{t+1}} \\Big[ max_{a_{t+1}} Q_1(h_{t+1}, a_{t+1}) - max_{a_{t+1}} Q_2(h_{t+1}, a_{t+1}) \\Big]$\n$\\leq \\gamma sup_{h_t, a_t} \\mathbb{E}_{s_{t+1}} \\Big[ max_{a_{t+1}} |Q_1(h_{t+1}, a_{t+1}) - max_{a_{t+1}} Q_2(h_{t+1}, a_{t+1})| \\Big]$\n$\\leq \\gamma ||Q_1 - Q_2||_{\\infty}$.\nThe first inequality follows from the linearity of expectation and the properties of the supremum norm. The second inequality leverages the fact that the maximum difference between $Q_1$ and $Q_2$ across all actions bounds the expectation. This demonstrates that T reduces the distance between any two value functions by at least a factor of $\\gamma$. By Banach's Fixed Point Theorem, this contraction property ensures that repeated application of T will converge to the unique fixed point $Q^*$, the optimal action-value function."}, {"title": "4.5 Implementing the Iterative Approach", "content": "To operationalize our sequential optimization strategy, we introduce a learning framework that decouples the optimization of the control policy from the representation learning, specifically the belief state inference. This framework leverages a shared belief state module to facilitate efficient and scalable learning. The architecture comprises three neural networks parameterized by $\\psi$, $\\phi$, and $\\theta$, corresponding to belief state extraction, observation policy, and control policy, respectively.\nAt time t, the belief state $b_t$ is derived from the history $h_t$ through the function $b_t = f_{\\psi}(h_t)$. The observation action $a_t^o$ is then determined by the observation policy $\\pi_{\\phi}^o$, parameterized by $\\phi$, and conditioned on the belief state: $a_t^o \\sim \\pi_{\\phi}^o(\\cdot | b_t)$. The observation action $a_t^o$ specifies which components of the environment to observe, resulting in an observation $o_t$, which is a function of the true state $s_t$ and the observation action $a_t^o$: $o_t = O(s_t, a_t^o)$, where $O$ represents the observation function that returns the observed components of the state based on $a_t^o$. The control action $a_t^c$ is then determined by the control policy $\\pi_{\\theta}^c$, parameterized by $\\theta$, and conditioned on the observed state $o_t$: $a_t^c \\sim \\pi_{\\theta}^c(\\cdot | o_t)$. This process ensures that the control actions are based on the most recent observations, which have been acquired according to the observation policy. The observation policy aims to acquire information that is most beneficial for decision-making while minimizing the associated costs.\nThe optimization process proceeds iteratively, following a structured sequence of updates. Initially, the observation policy is initialized using a curiosity-driven approach, which encourages the gathering of comprehensive information without immediate consideration of costs. This initialization ensures that the belief state extractor has access to rich information, crucial for accurately inferring the state. Subsequently, we optimize the parameters $\\psi$ and $\\theta$ to maximize the expected return using the currently accessible information, resulting in the optimized parameters $\\psi_1$ and $\\theta_1$, and the most accurate accessible state representation $b^* = f_{\\psi}(h_t)$. The control policy $\\pi_{\\theta}^c$ is thus optimized based on the observations $o_t$ resulting from the observations acquired via the observation policy.\nIn the next iteration, while keeping the control policy fixed, we optimize $\\psi$ and $\\phi$ to minimize state inference costs. Since the control policy remains constant, these optimized components should recover the optimal belief state $b^*$ as accurately as possible while minimizing observation costs. This involves updating the observation policy $\\pi_{\\phi}^o$ to select observation actions that are most informative for the control policy while incurring minimal cost. Once this is achieved, we can fine-tune the control policy. This iterative process continues until convergence, with each iteration refining the observation and control policies to balance the trade-off between observation costs and control performance."}, {"title": "5 Experiments", "content": "5.1 The Diagnostic Chain\nDesign of the Environment We validate our approach with a simple and carefully-designed Diagnostic Chain task in which an agent is required to transition a patient to a target health state within a sequential chain of health states. Each node in this chain represents a distinct health condition of a patient. For the purposes of this experiment, we assume that all patients consistently begin at the baseline health state, depicted as the leftmost node in the chain. The agent has access to multiple treatment options, which can be interpreted as different medical prescriptions. Specifically, we consider two possible treatment actions, denoted as $A_T = {Treatment1, Treatment2}$, representing the control actions available to the agent. Additionally, the agent can choose between two observation actions, $A^o = {Observe, Not Observe}$, which determine whether the agent will assess the effectiveness of the administered treatment.\nEach treatment action incurs a cost, represented by $-C_T$, where $C_T > 0$, while choosing to perform an observation incurs a cost of $-C_O$, where $C_O > 0$. Successfully reaching the target health state yields a positive reward, denoted by $R_T$, where $R_T > 0$. The effectiveness of each treatment varies across different patients. For each episode, one of the two treatment actions will successfully transition the patient to the next health state in the chain with an equal probability of 50%. Conversely, there is a 50% probability that the treatment will not alter the patient's current health state.\nThe optimal strategy for the agent involves selectively performing observations to determine the effectiveness of a treatment before deciding whether to continue with the same treatment or switch to an alternative. Initially, the agent selects a treatment action without performing an observation. After administering the first treatment, the agent may choose to observe the outcome. If the observation action is selected (Observe), the agent incurs the associated cost $-C_O$ and gains information about the treatment's effectiveness. An effective treatment leads the agent to continue with the same treatment, whereas an ineffective treatment prompts the agent to switch to the alternative treatment. This approach mirrors real-world healthcare decision-making, where clinicians may perform diagnostic tests to evaluate treatment efficacy before making subsequent treatment decisions.\nIf the agent chooses not to perform an observation action (Not Observe), it forgoes the additional cost and does not receive explicit information about the treatment's effectiveness. In this scenario, the agent must rely on blurred state information, meaning that its perception of the current health state is noisy or incomplete. This blurred state ensures consistency in the input state representations, even in the absence of direct observations, thereby testing the agent's ability to make informed decisions under uncertainty.\nThe agent continues to alternate between treatment and observation actions, adhering to its policy until it successfully reaches the target health state. Upon reaching the target, the agent receives the reward $R_T$, concluding the episode. This setup challenges the agent to balance the costs associated with treatments and observations against the benefits of achieving optimal patient health outcomes. By incorporating uncertainty in treatment effectiveness and the option to perform costly observations,"}, {"title": "5.2 Experiments on the HeartPole Healthcare Simulator", "content": "In this study, we employ the HearPole environment [17] for our experimental evaluations. HearPole is a simplified, rule-based healthcare simulation developed to facilitate the benchmarking of rein- forcement learning (RL) algorithms in medical contexts. Designed as an OpenAI Gym environment, HearPole serves as an introductory platform for testing RL techniques before applying them to more complex clinical datasets.\nHearPole models a healthcare scenario where the agent's objective is to maximize productivity while maintaining the patient's health. The action space consists of four discrete actions: do nothing, drink coffee, drink beer, and sleep. These actions influence the patient's state, which is represented by six continuous observation variables: alertness, hypertension, intoxication, time since last sleep, time elapsed, and work done. The agent receives a reward of +1 for each unit of"}, {"title": "5.3 Experimental Results", "content": "The experimental results are shown in Figures 5, 6 and 4. Figure 4 illustrates the evolution of observation policies in our simulated Diagnostic Chain tasks across different training steps. The visualization reveals how the policy progressively learns to balance observation costs against potential rewards, developing a strategic approach to achieve the final targets efficiently.\nFigures 5 and 6 illustrate the performance of our proposed Observation-Constrained MDP (OCMDP) framework across two tasks: the Diagnostic Chain Task and the HearPole Task. The results demon- strate significant improvements in both control performance and observation efficiency, validating the versatility and effectiveness of OCMDP across different complex environments. In the Diagnostic Chain Task (Figure 5), our method achieves a relative improvement of 71% in expected cumulative reward over the baseline model-free control policies. This improvement reflects the effectiveness of our control policy optimization in maximizing rewards while accounting for observation costs. Furthermore, the optimization of the observation policy reduces observation costs by 50% compared to fixed and continuous observation strategies, as shown in Figure 5.(b), indicating that OCMDP successfully balances the trade-off between acquiring valuable information and minimizing associated costs.\nIn the Heartpole Task (Figure 6), OCMDP outperforms several baseline reinforcement learning algorithms, with a relative improvement of approximately 75% in episodic return over the next best- performing algorithm, PPO. This improvement highlights OCMDP's ability to effectively balance observation costs with control rewards, particularly in diagnostic and decision-making tasks with high complexity. Figure 6.(b) compares different observation strategies within OCMDP, including optimal observation (our adaptive strategy), always observe, and never observe. The optimal observation strategy yields the highest episodic return, achieving approximately 80% higher return compared"}, {"title": "6 Conclusion and Future Work", "content": "In conclusion, we have introduced a novel approach that effectively balances the costs of informa- tion acquisition with the benefits of informed decision-making in cost-sensitive environments. By defining the Observation-Constrained Markov Decision Process and implementing a decomposed deep reinforcement learning algorithm, our approach efficiently manages the expanded action space inherent in simultaneous sensing and control tasks. The successful application of our method to both simulated diagnostic scenarios and the HeartPole healthcare environment highlights its capability to significantly reduce observation costs while maintaining high levels of control performance. These results demonstrate the practicality and superiority of our strategy over existing baseline methods, paving the way for more resource-efficient decision-making systems in real-world applications where the judicious use of observations is paramount.\nWhile our approach shows promising results, there are several avenues for future research to further enhance its capabilities. One potential direction is to extend the Observation-Constrained MDP frame- work to incorporate multi-agent systems, allowing for collaborative observation and control strategies across distributed agents. This could be particularly beneficial in environments where multiple entities need to make coordinated decisions with shared information costs. Another area of future work is to explore adaptive observation cost functions that dynamically adjust based on task complexity or environmental uncertainty. This would enable the model to prioritize critical observations in more complex or volatile scenarios, further optimizing resource use without compromising decision quality. Additionally, integrating our framework with real-world sensory data and testing in practical applications, such as healthcare monitoring, autonomous vehicles, and industrial IoT systems, could provide valuable insights into its performance under more diverse and realistic conditions. These efforts could pave the way for the development of robust, cost-effective decision-making systems that are tailored to operate effectively in dynamic, resource-constrained environments."}]}