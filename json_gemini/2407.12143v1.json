{"title": "False Consensus Biases AI Against Vulnerable Stakeholders", "authors": ["Mengchen Dong", "Jean-Fran\u00e7ois Bonnefon", "Iyad Rahwan"], "abstract": "The deployment of Al systems for welfare benefit allocation allows for accelerated decision- making and faster provision of critical help, but has already led to an increase in unfair benefit denials and false fraud accusations. Collecting data in the US and the UK (N = 2 449), we explore the public acceptability of such speed-accuracy trade-offs in populations of claimants and non-claimants. We observe a general willingness to trade off speed gains for modest accuracy losses, but this aggregate view masks notable divergences between claimants and non-claimants. Although welfare claimants comprise a relatively small proportion of the general population (e.g., 20% in the US representative sample), this vulnerable group is much less willing to accept Al deployed in welfare systems, raising concerns that solely using aggregate data for calibration could lead to policies misaligned with stakeholder preferences. Our study further uncovers asymmetric insights between claimants and non-claimants. The latter consistently overestimate claimants' willingness to accept speed-accuracy trade-offs, even when financially incentivized for accurate perspective-taking. This suggests that policy decisions influenced by the dominant voice of non-claimants, however well-intentioned, may neglect the actual preferences of those directly affected by welfare Al systems. Our findings underline the need for stakeholder engagement and transparent communication in the design and deployment of these systems, particularly in contexts marked by power imbalances.", "sections": [{"title": "1 Introduction", "content": "The use of Artificial Intelligence (AI) is becoming commonplace in government operations [1-4]. In the United States alone, a 2020 survey of 142 federal agencies found that 45% were using or planning to use machine learning algorithms to streamline their operations, increase their capaci- ties, or improve the delivery of their public services [2]. In the specific context of providing welfare benefits, the main promise of Al is to speed up decisions [4, 5]. For many individuals and families, welfare benefits provide critical assistance in times of financial hardship or emergency. Using AI to speed up decisions can avoid delays that would exacerbate these hardships, and decrease the period of uncertainty and anxiety during which applicants are waiting for a decision. There is, however, a documented risk that since welfare Al systems often focus on fraud detection, their speed gains come with a biased accuracy loss, increasing the acceptable trade-offs between the speed and accuracy of welfare allocations rate at which people are unfairly denied the welfare benefits they are entitled to [6-11].\nAs a result, government agencies that seek to deploy welfare Al systems must strike a careful balance between speed gains and accuracy losses, and this balance must be informed by public preferences [12, 13], for at least two reasons. First, we know that people who lose trust in the AI used by one government agency also lose trust in the AI used by other government agencies- if welfare Al systems ignore public preferences when balancing speed and accuracy, they risk creating distrust that can bleed into perceptions of other government services [14, 15]. Second, and more immediately, the wrong balance of speed gains and accuracy losses could erode the trust of people who need welfare benefits, and make them less likely to apply, for fear of being wrongly accused of fraudulent claims [14], especially when the Al system is labeled with foreboding names like 'FraudCaster' [16] or described as a 'suspicion machine' in the media [8, 17]. In sum, it is important for welfare Al systems to trade off speed and accuracy in a way that is aligned with the preferences of the general public as well as with the preferences of potential claimants.\nGreat efforts have been made to understand people's attitudes toward and concerns about welfare Al systems, often focusing on the opinions of the general public [14, 18] or vulnerable populations directly affected by welfare Al systems [11, 19]. Qualitative evidence has also been accumulated regarding the divergent preferences of different stakeholders involved in Al gov- erning systems [8, 20], contributing to long-lasting philosophical and regulatory discussions on"}, {"title": "2 Results in the US", "content": "Participants in the US (N = 987, representative on age, gender, and ethnicity, 20% self-declaring as welfare claimants) indicated their preference between human and Al welfare decisions. We varied the information about speed gains (1/2/3/4/5/6 weeks faster, as compared to a baseline waiting time of 8 weeks if handled by public servants) and accuracy losses (5/10/15/20/25/30% more false rejections than public servants) within a realistic range, based on governmental reports and third-party investigations [9, 12, 25, 26], yielding 36 trade-offs (as illustrated in Figure 1). In each trade-off condition, participants indicated their preference on a scale ranging from 0 = definitely a public servant to 100 = definitely the Al program. Participants were randomly assigned to respond from their own perspective as claimants or non-claimants, or to adopt the oppo- site perspective. The US study was not preregistered, hence all analyses should be considered exploratory.\nWhen participants responded from their own perspective (N = 506), their willingness to let Al make decisions was influenced both by speed gains (\u03b2 = 0.19, p < .001) and accuracy losses (\u03b2 = 0.40, p < .001). Overall (see Figure 2), they traded off a 1-week speed gain for a 2.4 percentage points loss of accuracy. Among these US participants, 21% self-declared as welfare claimants. For all the 36 trade-offs, these claimants (vs. non-claimants) showed greater average aversion to letting Al make welfare decisions (\u03b2 = \u22120.19 p < .001). The average difference between the responses of claimants and non-claimants was 5.9 points (range: 0.3 to 12.8, see Figure 3A).\nFigure 3B displays the biases of claimants and non-claimants when trying to predict the answers of the other group, across the 36 tradeoffs. Here we calculate the bias for each trade-off condition by subtracting participants' actual preference (e.g., claimants taking a claimant per- spective) from the other groups' insights through perspective taking (e.g., non-claimants taking a claimant perspective). We then compare the bias scores with zero to determine their statistical significance, using the formula below:\n$$bias_{ij} = \\beta_0 + \\mu_{0j} + \\epsilon_{ij}$$"}, {"title": "3 Results in the UK", "content": "To replicate and extend the results obtained from the US representative sample, we collected data from N = 1462 participants in the UK with a balanced composition of claimants and non- claimants. Such a balanced sample can help consolidate our pre-registered test on the asymmetry in perspective-taking. In addition, we implemented the following changes:\n1. We examined preferences about a specific benefit in the UK (the Universal Credit) and tar- geted a balanced sample between Universal Credit claimants (48%) and non-claimants (52%). The UK government has recently announced the deployment of Al for the attribution of this benefit, raising concerns that the Al system may be biased against some claimants [7].\n2. We adopted a different range of speed (0/1/2/3 weeks faster, as compared to a baseline wait- ing time of 4 weeks if handled by public servants) and accuracy (0/5/10/15/20% more false rejections than public servants) parameters, resulting in 20 trade-offs. Notably, when wel- fare Al demonstrates comparable performance (i.e., 0 week faster and 0% more error), people were still in favor of humans making welfare decisions (M = 45.4, SD = 28.7; t = 4.36, p < .001).\n3. We added financial incentives for participants to correctly predict the preferences of the other group, that is, when non-claimants predict claimants' preference and claimants predict non-claimants' preference. We also asked non-claimants whether they had claimed welfare benefits in the past, whether they thought they may claim benefits in the future, and whether they were acquainted with people who were welfare claimants, to assess whether these circumstances made it easier to adopt the perspective of claimants.\n4. For each trade-off, we additionally asked participants whether their trust in the government would decrease or increase (from 0 = decrease a lot decrease a lot to 100 = increase a lot) if the government decided to replace public servants with the Al program they just considered.\n5. Finally, we added a treatment that made explicit the existence of a procedure to ask for redress in case a claimant felt their claim was unfairly rejected. Even though participants in the human redress condition believed in the chance to appeal (\u03b2 = 0.37, p < .001; vs. the redress condi- tion), this clarification did not impact trade-off preferences (\u03b2 = 0.03, p = .210). Therefore, we pool the data from this treatment with that of the baseline treatment. Full analyses of this treatment are presented in the Supplementary Information.\nAgain, when participants responded from their own perspective (N = 739), their willingness to let Al make decisions was influenced both by speed gains (\u03b2 = 0.34, p < .001) and accuracy losses (\u03b2 = 0.44, p < .001). Overall (see Figure 4), they traded off a 1-week speed gain for a 5 percentage point loss of accuracy. Among these UK participants, 47% self-declared as current claimants of the Universal Credit. As in the US study, for all 20 trade-offs, welfare claimants showed greater average aversion to letting Al make welfare decisions (\u03b2 = -0.09, p = .008), with an average difference of 5.7 points (range: 0.1 to 8.7, see Fig. 5A). In both groups, we observe a strong correlation across trade-offs between the aversion to letting the Al make decisions, and the loss of trust in the government that would deploy this AI (r = .77 for claimants, and r = .84 for non-claimants).\nFigure 5B displays the biases of claimants and non-claimants when trying to predict the answers of the other group, across the 20 trade-offs. As in the US study, we calculated the perspective-taking biases for claimants and non-claimants, respectively. On average, claimants provide an unbiased estimate of the answers of non-claimants (p = .323), with an underestima- tion of 0.9 points and a 95%-confidence interval including zero, [-2.7, 0.9]. Non-claimants, how- ever, overestimate the preferences of claimants by 4.2 points (p < .001), with a 95%-confidence interval of [2.6, 5.7]. These asymmetrical insights between claimants and non-claimants are con- sistent with our preregistered prediction. To explore whether some life experiences may reduce bias in the predictions of non-claimants, we recorded whether they had past experience as claimants of other benefits, whether they were acquainted with current claimants, and their per- ceived likelihood of becoming claimants in the near future. We found no credible evidence for any of these effects.\nIn sum, results from our targeted UK sample consolidate and extend results from our repre- sentative sample. The average willingness to trade a 5-point accuracy loss for a 1-week speed gain hides heterogeneity in responses, with welfare claimants being systematically more averse to AI than non-claimants. We also find strong evidence for asymmetrical insights between claimants and non-claimants: claimants are well-calibrated when predicting the answers of non-claimants, but non-claimants overestimate the willingness of claimants to let Al make decisions. Finally, lower acceptance of the AI system for welfare allocation is strongly linked to decreased trust in the government among both welfare claimants and non-claimants."}, {"title": "4 Discussion", "content": "One primary advantage of using Al for welfare benefit allocation is quicker decision-making, allowing claimants to receive support faster [4, 5]. However, these systems often result in an accu- racy loss, potentially leading to unfair denials or false fraud accusations [6-11]. Governments must carefully balance these trade-offs to maintain public trust [14, 27]. Indeed, we found that the acceptability of this balance to participants was closely tied to their resulting trust in the government.\nIn both the US and UK, our data suggested that participants would trade a one-week speed gain for a 2.5 to 5 percentage point accuracy loss. However, we also found that averaging across participants masked strong divergences between claimants and non-claimants. Though the dif- ference between the two groups varied across trade-offs, welfare claimants were systematically less amenable to Al deployment than non-claimants. In parallel with the comparison between welfare claimants and non-claimants, we also conducted latent profile analysis to explore the underlying patterns in the preference data without relying on existing labels (e.g., claimant sta- tus). Since we did not find strong support for a particular number of profiles, we report the results in the Supplementary Information. In summary, using aggregate data to calibrate Al in welfare systems could backfire, as average responses may not reflect the divergent preferences of stake- holders. This finding aligns with recent calls in behavioral science to focus on heterogeneity when informing policy [28], as well as to consider the positionality of Al models [29], that is, their social and cultural position with regard to the stakeholders with which they interface.\nData revealed a further complication: asymmetric insight between claimants and non- claimants. Claimants could provide unbiased estimates of the preferences of non-claimants, but non-claimants failed to do the same, even in the presence of financial incentives. These find- ings echo laboratory results suggesting that participants who are or feel more powerful struggle to take the cognitive perspective of others [30-33], as well as sociological theories positing that marginalized groups have greater opportunities and motivations to develop an understanding of the thoughts and norms of dominant groups [34-36].\nIn the context of welfare Al, asymmetric insights create the risk that the perspective of claimants may be silenced even when non-claimants seek to defend the interests of claimants. These well-intentioned non-claimants may use their dominant voice to shape public opinion and policy without realizing that they do not in fact understand the preferences of claimants, result- ing in Al systems that are misaligned with the preferences of their primary stakeholders. Our results thus underline the need to actively engage with claimants when building welfare Al sys- tems, rather than to assume that their preferences are well-understood or can be understood through empathetic perspective-taking.\nOur results also highlight the need for transparent communication about welfare Al sys- tems' design choices. Given the significant heterogeneity in public preferences, and the close link between meeting these preferences and trust in government, clear justifications are crucial. Despite increasing technical attempts to align Al with pluralistic values and diverse perspec- tives [37-39], there are inevitably situations where agreement or reconciliation cannot be easily achieved (e.g., when non-claimants fail to estimate welfare claimants' aversion to Al, but not vice versa). In this case, our study sheds light on the possibility of evidence-based public communi- cation when human-centered Al designs need to de-emphasize the preferences of the general population but optimize toward a particular subgroup of people. Our core findings, heterogene- ity and asymmetric insights, may also hold in other cases where Al is deployed in a context of power imbalance \u2013 conducting behavioral research on these cases in advance of Al deployment may help avoid the scandals that marred the deployment of welfare AI."}, {"title": "5 Methods", "content": "Both of the US and UK studies were approved by the ethics committee at the Max Planck Insti- tute for Human Development, and obtained informed consent from all participants. Data were collected in February 2022 and September 2022, respectively. All participants were recruited on Prolific for a study named \u201cArtificial Intelligence in Social Welfare\u201d, and were paid \u00a31.6 upon com- pletion. Participants in the UK study who had to predict the answers of the other group received an additional \u00a30.03 for each response that fell within 5 points of this other group's average.\nBoth studies were hosted on Qualtrics. After providing informed consent and basic demo- graphic information (age, gender, education, income, and political ideology), participants were instructed to take a claimant or non-claimant perspective. To familiarize themselves with the stimuli and response scale, they were first shown two extreme trade-offs in the survey, as train- ing examples. They answered these two examples, and had a chance to review and change their answers. Then the survey started, and all targeted trade-offs were shown in random order, not including the two trade-offs that were shown as examples during the training phase. Complete descriptions of our design materials, and survey questions are included in the Supplementary Information."}, {"title": "5.1 The US study", "content": "Participants. We had N = 987 participants from the United States, who were representative on age (M = 45.3, SD = 16.3), gender (473 males and 514 females), and ethnicity (77.8% White, 11.4% Black, 6.1% Asian, 2.5% Mixed, and 2.1% other), and 20.4% of them self-reported as welfare claimants at the time of the study. The sample size was determined based on the recent recommendation of around 500 people for latent profile analysis [40]. We aimed for an almost doubled sample size given our two-condition perspective-taking manipulation.\nDesign and procedure. The US study employed a mixed design, with one between-subjects and two within-subjects factors. First, participants were asked for their basic demographic infor- mation, and were randomly assigned to take either a claimant (\"You are applying for a social benefit\") or a controlled taxpayer (\"Someone else in your city is applying for a social bene- fit\") perspective. We then manipulated the information about welfare Al's speed (6 conditions: 1/2/3/4/5/6 weeks faster than a public servant) and accuracy (6 conditions: 5/10/15/20/25/30% more false rejections than a public servant). The presented speed (an average of 8 weeks) and accuracy (at most 40% errors) baselines referred to realistic information from some governmental reports and third-party investigations [9, 12, 25, 26].\nAfter knowing the perspective they should take, participants went through two training examples, reading two extreme cases of welfare AI (bad case: 0 week faster + 50% more false rejec- tions; good case: 7 weeks faster + 1% more false rejections) and answering the same question \"To what extent do you prefer a public servant or the Al program to handle your/the person's wel- fare application?\" on a 100-point scale (from 0 = definitely a public servant to 100 = definitely the Al program). They then had a chance to review and calibrate their answers in the two cases before moving to the 36 official test rounds - which did not allow revisions anymore. In each of the 36 test rounds, they read information about their perspective, Al speed, and Al accuracy in three consecutive cards (see Fig. S1 in the Supplementary Information for an illustration of the cards in different experiment conditions). After reading the three cards in each round, participants answered the same question about their preference for welfare Al versus public servants."}, {"title": "5.2 The UK study", "content": "Participants. We performed a simulation-based power analysis for multilevel regression mod- els, which suggested that a sample of N = 800 would allow us to detect the interaction effect of Al performance, claimant status, and perspective-taking with higher than 80% power at an alpha level of 0.05 (see the pre-registration at https://tinyurl.com/welfareAlregistration). We therefore aimed for N = 1600 participants in the United Kingdom given our additional between-subjects human redress manipulation. As pre-registered, we filtered out participants who provided differ- ent answers to one identical welfare status question (\"Are you a recipient of Universal Credit?\"; Answer: \"Yes/No\"), which was embedded both in the Prolific system screener and our own sur- vey. After the screening, we eventually had N = 1 462 participants (age: M = 37.6, SD = 11.1; ethnicity: 88.4% White, 3.0% Black, 5.6% Asian, 2.7% Mixed, and 0.3% other), with relatively bal- anced compositions of males and females (42.7% male, 55.9% female, 1.4% other), and welfare claimants (47.9%) versus non-claimants (52.1%).\nDesign and procedure. Study 2 examined a real-life social benefits scheme in the UK \u2013 Uni- versal Credit (https://www.gov.uk/universal-credit). We employed a mixed design with three between-subjects and two within-subjects factors. As between-subjects factors, we recruited both Universal Credit claimants and non-claimants, and randomly assigned them to take a Univer- sal Credit claimant or a controlled taxpayer perspective. They were then randomly assigned to a no redress or a human redress condition, which differed on whether claimants could appeal to public servants. As within-subject factors, we manipulated information about welfare Al's speed (0/1/2/3 weeks faster, as compared to a baseline waiting time of 4 weeks if handled by public servants), and accuracy (0/5/10/15/20% more false rejection).\nBefore starting the 20 rounds of official tradeoff evaluations, as in the US study, participants went through two training examples with a chance of revision, reading two extreme cases of welfare AI (bad case: 0 week faster + 40% more false rejections; good case: 3 weeks faster + 1% more false rejections). In each example, they answered two questions on a 100-point scale: \"To what extent do you prefer a public servant or the Al program to handle your/the person's wel- fare application?\u201d (0 = definitely a public servant to 100 = definitely the AI program) and \u201cIf the UK government decided to replace some public servants with the Al program in handling welfare applications, would your trust in the government decrease or increase?\" (0 = decrease a lot to 100 = increase a lot). They then had a chance to review and change their answers in the two cases before moving to the 20 official test rounds \u2013 which did not allow revisions anymore. In each of the 20 test rounds, they read information about their perspective, Al speed, Al accuracy, and human redress condition in three consecutive cards (see Fig. S2 in the Supplementary Information for an illustration of the cards in different experiment conditions). After reading the three cards in each round, participants answered the same two question about their preference for welfare Al versus public servants, and their trust in the government.\nTo increase the motivation of perspective taking, participants were informed and incen- tivized to take the opposite perspective, for each accurate answer that fell within \u00b15 points of the other group's average. At the end of the 20 official rounds, as a manipulation check, participants indicated the extent to which they believed that \"you/the person can appeal to public servants if you/they are not satisfied with the welfare decision made by the Al program?\" (0 = not at all to 100 = very much)."}, {"title": "5.3 Data availability", "content": "All anonymized data can be found on Open Science Framework (at https://tinyurl.com/ welfareAl; and will be made publicly accessible upon acceptance of the work)."}, {"title": "5.4 Code availability", "content": "All code necessary to reproduce all analyses can be found on Open Science Framework (at https: //tinyurl.com/welfareAl; and will be made publicly accessible upon acceptance of the work)."}, {"title": "End notes", "content": "\u2022 Acknowledgements: Agence Nationale De La Recherche ANR-19-PI3A-0004 (JFB); Agence Nationale De La Recherche ANR-17-EURE-0010 (JFB); The research foundation TSE- Partnership (JFB)\n\u2022 Authors' contributions: Conceptualization: MD, JFB, IR; Methodology: MD, JFB; Investiga- tion: MD; Visualization: JF; Funding acquisition: IR; Project administration: MD; Supervision: IR; Writing - original draft: MD, JFB; Writing \u2013 review & editing: MD, JFB, IR\nMaterials & Correspondence: Correspondence and requests for materials should be addressed to M.D.\n\u2022 Additional information: Supplementary Information is available for this paper."}]}