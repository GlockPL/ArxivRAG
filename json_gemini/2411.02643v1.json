{"title": "A Comparative Analysis of Counterfactual Explanation Methods for Text Classifiers", "authors": ["Stephen McAleese", "Mark Keane"], "abstract": "Counterfactual explanations can be used to interpret and debug text classifiers by producing minimally altered text inputs that change a classifier's output. In this work, we evaluate five methods for generating counterfactual explanations for a BERT text classifier on two datasets using three evaluation metrics. The results of our experiments suggest that established white-box substitution-based methods are effective at generating valid counterfactuals that change the classifier's output. In contrast, newer methods based on large language models (LLMs) excel at producing natural and linguistically plausible text counterfactuals but often fail to generate valid counterfactuals that alter the classifier's output. Based on these results, we recommend developing new counterfactual explanation methods that combine the strengths of established gradient-based approaches and newer LLM-based techniques to generate high-quality, valid, and plausible text counterfactual explanations.", "sections": [{"title": "1. Introduction", "content": "The rapid advancement of machine learning (ML) and deep learning (DL) models has led to their widespread application in real-world tasks. Due to algorithmic improvements, increased computational power, and dataset sizes, the performance of models powered by deep neural networks has surpassed traditional ML algorithms such as logistic regression and support vector machines (SVMs) on many tasks such as speech recognition, language translation, summarization, and sentiment classification [1, 2]. Today, many high-performing NLP models such as BERT [3] and GPT-4 [4] are large language models (LLMs) consisting of dozens of transformer layers and millions or billions of parameters.\nWhile these models have achieved remarkable performance across a wide range of tasks, their complexity and scale decreases their transparency and explainability. Consequently, many leading NLP models are considered to be \"black boxes\", providing outputs to users without explaining how or why they were produced [5, 6]. Their lack of interpretability and explainability hinders their trustworthiness, fairness and the ability of model developers to identify potential flaws [7, 8]. For example, a black-box model may contain hidden biases or rely on spurious features to make decisions. Furthermore, the lack of transparency of these models may impede their deployment in sensitive domains such as healthcare, finance, and legal applications [6].\nThe problems associated with black-box models have motivated the development of the fields of interpretability and explainable AI (XAI) which aim to develop techniques to understand the inner workings of these models and generate explanations for model outputs [9, 10]. Among the various techniques introduced for explaining ML models, research activity on counterfactual explanations has increased significantly since 2019 due to their strong support from psychological research and other benefits [11].\nCounterfactual explanations demonstrate how an input can be minimally altered to change a classifier's output in order to show which parts of the input are important for determining the model's output [12]. For example, given the sentence \"I liked the movie\" classified as positive sentiment, a counterfactual input \"I hated the movie\" classified as negative sentiment helps users understand the classifier's behavior by highlighting the influence of the verb on the model's output. Since their introduction, counterfactual explanations have been widely applied to traditional ML problems involving explicit features and tabular"}, {"title": "2. Background", "content": "Our research addresses the challenge of explaining the outputs from a binary classifier powered by a BERT-base neural language model [3]. Two broad approaches to explaining the predictions of deep neural networks are explaining the processing of data or explaining the representation of data inside the neural network [9]. The former approach seeks to answer the question, \"Why does this particular input lead to that particular output?\" while the latter aims to answer the question \"What information does the network contain?\". Our work focuses on the first question, the problem of explaining why the model produced a particular output. While explanations of network representations such as feature visualizations [17] for individual neurons can offer valuable information, they provide limited insight into the reasons why a model produced a particular output. Therefore, we focus on explaining the processing of data to gain a more high-level understanding of model behavior.\nSome popular methods for understanding ML classifiers include LIME [18] and saliency maps [19, 20, 21]. LIME explains models by creating a simple, interpretable linear model that approximates the more complex model locally. Saliency maps highlight which components of the input have the most significant influence on the output typically by calculating the partial derivative of the output with respect to each input component. Although LIME has proven useful for explaining various classifiers, approximating the behavior of a language model with millions of parameters which accepts unstructured text as input is challenging. Saliency maps can also be helpful but provide non-contrastive explanations, which can limit the intuitiveness of their explanations for users [22, 23].\nTo explain our black-box classifier, we instead turn to counterfactual explanations which provide model-agnostic, contrastive explanations which do not require a detailed understanding of the model's internal mechanisms [24]."}, {"title": "2.2. Counterfactual explanations", "content": "The concept of explanation is closely linked to causality and counterfactual events are a key feature of causal reasoning. An event C can be said to have caused E if, under some hypothetical counterfactual case the event C did not occur, E would not have occurred [25].\nResearch from psychology and the social sciences has found that people generally seek out contrastive explanations for events. This means that instead of asking why event P occurred, people tend to seek explanations that explain why event P happened rather than some alternative event Q [25]. Furthermore, people are selective and tend to select one or two primary causes from many possible potential causes.\nThese findings have motivated the development of methods for generating counterfactual explanations which provide contrastive explanations for model outputs [12]. Counterfactual explanations are usually given in the form of a counterfactual instance or counterfactual: given an input x and a classifier f: x \u2192 y, a counterfactual is a minimally altered input x' that produces a different outcome y' from the classifier. The task of generating a counterfactual input x' for a classifier can be formulated mathematically as the following optimization problem:\n$$argmind(x, x') subject to f(x') = y'$$"}, {"title": "3. Methods", "content": "This section provides detailed explanations of several methods developed in recent years for generating counterfactual explanations for text classifiers. These methods are also included in our comparative study which empirically compares the effectiveness of each method. To organize the past literature, we group the methods into three categories: adversarial methods, substitution methods, and LLM methods."}, {"title": "3.1. Adversarial methods", "content": "Adversarial attack methods are designed to test the adversarial robustness of classifiers by making small, imperceptible changes to the input such as changing a single character or word to induce a model to make incorrect predictions [27]. In contrast, counterfactual explanation methods are designed to produce noticeable, realistic, and understandable changes in order to provide useful explanations of a model's behavior.\nWhile adversarial methods were not originally designed to generate counterfactuals, these methods can be repurposed to generate them due to their ability to make minimal input modifications that change model outputs. However, unlike methods for generating counterfactual explanations, adversarial methods generally do not prioritize producing realistic or natural changes and consequently counterfactuals generated using these methods tend to score poorly on metrics measuring plausibility.\nIntroduced by Ebrahimi et al. in 2018, HotFlip [28] is a gradient-based adversarial method which generates adversarial examples or counterfactuals by replacing one or more tokens in the text. To generate a counterfactual, first several candidate substitutions are generated for each position of the original text. The value of each candidate substitution is determined by calculating the partial derivative of the classifier's output with respect to each candidate substitution token. Using beam search, HotFlip explores the space of possible substitutions, prioritizing those with the highest estimated impact on the model's output. The search process continues until the classifier's output is changed using a sufficient number of substitutions or until all positions have been considered. HotFlip is designed to flip the output of the classifier with as few edits as possible and therefore the resulting counterfactuals tend to score highly on validity and sparsity. However, since HotFlip does not consider the plausibility of its substitutions, the generated counterfactuals may score poorly on measures of plausibility and appear unnatural to human readers."}, {"title": "3.2. Substitution methods", "content": "Both substitution-based counterfactual methods, such as CLOSS [14] and adversarial methods such as HotFlip [28] identify important words in the original sentence in order to minimize the number of substitutions needed to change the output of the classifier. However, unlike adversarial methods,"}, {"title": "3.3. LLM methods", "content": "Recent advancements in large language models (LLMs) have led to the development of new methods for generating counterfactual explanations for text classifiers. While several substitution-based methods such as CLOSS use LLMs to generate plausible word substitutions, these approaches often involve complex, task-specific architectures that limit their generalizability. Additionally, since LLMs have a limited role in the generation of counterfactuals, these methods fail to fully leverage the potential of LLMs for generating high-quality, plausible text counterfactuals.\nRecently, several methods for generating text counterfactuals have been developed that leverage LLMs more extensively in the process of generating counterfactuals. These methods can be classified into two categories: controlled generation and general-purpose LLM methods. Early controlled generation methods such as Polyjuice [15] and GYC [32] use fine-tuned LLMs such as GPT-2 [33] to generate counterfactuals. In contrast, general-purpose LLM methods such as FIZLE [16] use a custom system prompt to generate counterfactuals, eliminating the need for task-specific fine-tuning."}, {"title": "4. Experimental setup", "content": "In this section, we describe our experiment to compare the effectiveness of several methods for generating counterfactual explanations for a BERT text classifier. We outline the datasets used, the text classification model to be explained, the counterfactual methods tested, and our evaluation criteria. Our experiment evaluates five methods for generating counterfactual explanations across two datasets using three evaluation metrics 1."}, {"title": "4.1. Classifier and datasets", "content": "The black-box model to be explained is a BERT-base [3] binary classification model trained by TextAttack [34] and fine-tuned on either the SST-2 or QNLI dataset depending on which dataset is being tested. The model has 12 transformer layers, 110 million parameters, and is uncased. The input to the model can be any text sentence and the output is always a binary label. For example, given a sentence x such as \"I liked the movie\", the model f : x \u2192 y classifies the sentence as either 0 for negative sentiment or 1 if positive.\nOur two datasets are SST-2 [35], a binary sentiment classification dataset, and QNLI [36], a binary natural language inference dataset. The SST-2 dataset consists of approximately 70,000 short sentences describing movie reviews where each sentence is classified as positive or negative sentiment. The QNLI dataset contains approximately 100,000 rows. Each row consists of a question, a sentence, and a label which is either \"entailment\" if the sentence contains the answer to the question or \"not_entailment\" otherwise. We sample 1000 datapoints from each dataset and pre-process each sentence by converting all characters to lowercase and removing unnecessary spaces."}, {"title": "4.2. Methods", "content": "We evaluate and compare five methods for generating counterfactual explanations for our text classifier. The following list describes the implementation details and hyperparameters for each method:"}, {"title": "4.3. Evaluation metrics", "content": "Given an input sentence, each method for generating counterfactual explanations outputs a minimally edited counterfactual sentence that changes the output of the classifier. We use three evaluation metrics to evaluate the average quality of the counterfactuals generated by each method. These metrics are designed to measure the key qualities high-quality counterfactuals should have including high validity, sparsity, and plausibility.\n$$LFS = \\frac{1}{n} \\sum_{i=1}^{n} 1[f(x_i) \\neq f(x'_i)]$$"}, {"title": "5. Results", "content": "Our experimental results, summarized in Table 2 and Figure 1, demonstrate the performance of five counterfactual generation methods on the SST-2 and QNLI datasets, with each method evaluated using our three evaluation metrics: label flip score (LFS), mean normalized Levenshtein similarity, and perplexity.\nOn the SST-2 dataset, CLOSS achieves the highest Label Flip Score (LFS) (0.96), demonstrating its superior ability to generate valid counterfactuals that change the model's prediction. HotFlip achieves an LFS of 0.63 while also having the highest similarity score for the dataset (0.86). FIZLE-naive and FIZLE-guided also achieve high LFS scores (0.88 and 0.85 respectively), which is remarkable given that they are black-box methods without any access to the classifier's internal gradients. While most methods achieve high LFS and similarity scores, Polyjuice fails to achieve a high LFS (0.35) or similarity"}, {"title": "6. Conclusion", "content": "In this work, we evaluated five methods for generating counterfactual explanations across two datasets, evaluated using three metrics. We aimed to address two primary research questions: 1) Which methods for generating counterfactual explanations for text classifiers are most effective? 2) Do newer, simpler methods that prompt general-purpose LLMs to generate counterfactuals outperform older, more specialized methods?\nOur findings suggest that the effectiveness of each counterfactual method varies across evaluation metrics and datasets. For example, both FIZLE-naive variants achieve high validity, sparsity, and plausibility scores on the SST-2 dataset but fail to maintain high validity scores on the QNLI dataset. Conversely, CLOSS consistently achieves high validity and similarity scores but underperforms LLM-based methods on measures of plausibility. These results suggest that the most effective method depends on which dataset is used and which evaluation metric is prioritized.\nRegarding the second question, we find that newer LLM-based methods such as FIZLE and Polyjuice consistently produce more plausible and natural counterfactuals than substitution-based methods."}]}