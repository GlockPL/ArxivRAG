{"title": "A REVIEW OF HANDCRAFTED AND DEEP RADIOMICS IN NEUROLOGICAL DISEASES: TRANSITIONING FROM ONCOLOGY TO CLINICAL NEUROIMAGING", "authors": ["Elizaveta Lavrova", "Henry Woodruff", "Hamza Khan", "Eric Salmon", "Philippe Lambin", "Christophe Phillips"], "abstract": "Medical imaging technologies have undergone extensive development, enabling non-invasive visu- alization of clinical information. The traditional review of medical images by clinicians remains subjective, time-consuming, and prone to human error. With the recent availability of medical imaging data, quantification have become important goals in the field. Radiomics, a methodology aimed at extracting quantitative information from imaging data, has emerged as a promising approach to uncover hidden biological information and support decision-making in clinical practice. This paper presents a review of the radiomic pipeline from the clinical neuroimaging perspective, providing a detailed overview of each step with practical advice. It discusses the application of handcrafted and deep radiomics in neuroimaging, stratified by neurological diagnosis. Although radiomics shows great potential for increasing diagnostic precision and improving treatment quality in neurology, several limitations hinder its clinical implementation. Addressing these challenges requires collaborative efforts, advancements in image harmonization methods, and the establishment of reproducible and standardized pipelines with transparent reporting. By overcoming these obstacles, radiomics can significantly impact clinical neurology and enhance patient care.", "sections": [{"title": "1 Introduction", "content": "Since the discovery of X-rays [1], medical imaging has advanced significantly. However, the conventional manual review of medical images by clinicians is subjective, time-consuming, and costly. With the increasing availability of medical imaging data, there is a growing opportunity for quantitative analysis in this field.\nRadiomics is a methodology aimed at retrieving quantitative information from imaging data [2]. It is based on extracting numerous descriptors from medical images and finding a link between features and clinical outcomes. Handcrafted radiomics utilizes termed features, which are mathematically defined during the pipeline development, whereas deep radiomics utilizes features created by the artificial neural network during the model training process. The radiomics approach hypothesizes that medical imaging data contains hidden, complementary biological information that can be used for decision support in clinical practice [3]. Therefore, this method is of high interest for application in individualized diagnosis and treatment.\nAs handcrafted radiomics workflow requires a segmented region of interest (ROI), this methodology has been extensively developed in the oncological field where tumors and organs are routinely delineated for treatment planning purposes [2]. Since its inception, pioneer studies have revealed the connection between imaging biomarkers and histology [4] and have matured to produce externally validated and clinically relevant predictive models [5, 6].\nWhereas in oncology a large amount of segmented imaging data is accumulated mostly for radiotherapy needs, other branches of medicine have collected imaging data and could potentially benefit from the application of radiomics. Thus, it is essential to perform an early diagnosis of neurological diseases since symptoms appear after the disease progresses considerably. Often there are no formal reliable biomarkers, and the diagnosis is based on the regularly reviewed diagnostic criteria [7]. Therefore, the differential diagnosis between the diseases and handling the atypical cases might be challenging.\nThus, radiomics is an emerging methodology in medical imaging research expanding from oncology to other branches of medicine. However, the review of radiomics in non-oncological neurology is needed to analyze the current state of the art, identify the methodological pitfalls, and suggest possible solutions for the future progress of quantitative clinical neuroimaging. In this review, we present a typical workflow of radiomics analysis regarding neuroimaging. We provide a broad overview of the currently published works stratified by neurological diagnosis. We discuss the current limitations of radiomics in neurology, suggesting potential improvements."}, {"title": "2 Workflow", "content": "The following section considers the practical implementation of radiomics in the neuroimaging field combining some common steps of radiomics and neuroimaging workflows (illustrated in Figure 1). After the steps are described, the list of the corresponding software is provided."}, {"title": "2.1 Data curation", "content": "In hospitals, the data is saved in Picture Archiving and Communications Systems (PACS) in Digital Imaging and Communications in Medicine (DICOM) format [8, 9]. It stores imaging data together with metadata. In research, open file formats are preferred, such as Nifti, Analyse, MNC, and NRRD [10].\nTo read and write the imaging and metadata, an application programming interface (API) for the currently relevant programming languages is recommended to make the pipeline fully automated and avoid manually introduced mistakes.\nClinical and metadata need to be anonymized or pseudo-anonymized, considering the possible need for follow-up acquisitions [11]. Brain scans usually include the facial features of the patient or teeth. Since facial features or teeth can be used to identify a person, it is necessary to remove them as well. A simple procedure for defacing is skull stripping [12].\nA good practice is a sanity check of the data. It might include linking imaging and non-imaging samples to reveal missing or unwanted data, acquisition time point check for longitudinal studies and image quality check.\nIn neuroimaging research, the Nifti format is preferred as it is standardized and constructed specifically for neuroimaging data. In the case of data conversion to Nifti with a custom code, it is important to correctly transfer the geometrical parameters of the scan, as described in https://nipy.org/nibabel/coordinate_systems.html.\nFor automated data analysis, maintaining a uniform data structure is crucial. Data structure for different patients, imaging modalities, and potential acquisition timeframes should be established together with naming conventions for files and folders. For neuroimaging specifically, the Brain Imaging Data Structure (BIDS) [13] is recommended. It offers a standardized approach that is suited for multi-modal data and its derivatives and is supported by the community."}, {"title": "2.2 Data pre-processing", "content": "After the proper curation, data is considered ready for use. The next step is image pre-processing which is described in [14].\nSince many imaging modalities or image acquisition time points can be combined in neurology, brain scan co-registration is needed. It means that multiple brain scans should be co-aligned to achieve the closest spatial position. The data to be co-registered can belong to different imaging modalities or sequences. Besides co-registration at the patient level, registration to the tissue probability maps in the standardized space can be performed [15]. Co-registration can be performed in both rigid (only the head position and orientation are changed) and non-rigid (additional scaling and elastic deformations) ways. Even though non-rigid co-registration allows for the best correspondence of the anatomy and regions of interest, it changes visualized tissue texture. Therefore, for clinical tasks, mostly rigid co-registration on a patient level is applied.\nImage re-shaping is required to obtain the same voxel shape within the dataset. It allows for the same input image shape in the pipeline. While changing the voxel size, it is important to consider the interpolation effects introduced. In [16], different interpolation methods are described. The detailed recommendations are given in [14].\nBrain scans contain intensity inhomogeneities due to the presence of the bias field. In MRI, the bias field is caused by the MR field inhomogeneity of the scanner originating from the equipment [17] and the patient disturbing the magnetic field. To reduce the effect of the bias field, bias field correction (BFC) can be applied. While performing BFC, it is important to consider that it might reduce the contrast and remove critical abnormality information. The most popular method is N4 BFC [18]. However, there are recent works on deep learning-based BFC [19]. CT is not affected by bias because it represents attenuation of the X-ray beam through the body, therefore, in general, this procedure is not recommended for CT scans.\nSince CT images are expressed in HU, with a well-defined range, more advanced reliable pre-processing is possible. Knowing the characteristic HU for the tissue of interest, it is possible to exclude all the objects on the scan that are not relevant to the analysis. Signal clipping can be applied to the intensities outside of the range of interest [20].\nEven though \"hard pre-processing\" is not recommended in the quantitative image analysis to prevent a signal loss [14], some filtering can be applied to decrease the noise level. The most popular filters among smoothing filters are Gaussian and median filters. Gaussian filter is effective in removing high-frequency noises whereas a median filter is applied to remove impulse noise [21]."}, {"title": "2.3 Image segmentation", "content": "In neurology, ROIs can be anatomically or physiologically derived and vary from application to application. Since brain structures have a systematic organization and traditional computer vision techniques can be applied for segmen- tation, there are many computer vision-based auto-segmentation tools recognized by the neuroimaging community. Nevertheless, the development of neural networks brings new solutions which are gaining more interest. The deep learning models are trained on different data and do not contain mathematical constraints about anatomy or the expected distribution of intensities.\nBrain extraction narrows the image size and removes the surrounding tissues. In some studies, radiomics analysis was performed over the whole brain mask [22]. But since the brain includes different structures, whole-brain radiomics do not give comprehensive information about particular shapes and textures. Nevertheless, this kind of analysis is prospective in the discovery of healthy and pathological brain signatures for screening. In some works, features are extracted from the right and left hemispheres to be compared [23].\nA lower level of defining the ROIs is presented with the nervous tissues. The human brain is composed of white and gray matter. Analysis of the radiomics features extracted from the separate brain tissues is closer to the in-vivo histology and allows for the interpretation of texture and density abnormalities.\nMany studies focus on analyzing certain areas of the cortex and deep gray matter. In this case, it is possible to build connections between imaging data and functional outcomes since particular cortical areas are responsible for the specific functions. Moreover, analyzing certain areas can lead to early disease diagnosis before more severe symptoms appear [24].\nROIs described above can contain both healthy and pathological tissues. In most cases, neurological pathology appears in textural changes. But when the pathological tissue is compact (hematoma or tissue lesion), it can be selected as ROI. Physiologically-derived ROIs are obtained from functional imaging such as PET or fMRI. They represent the delineated tracer or function activity areas. No manual delineation is needed, and ROI contours depend on the selected binarization method."}, {"title": "2.4 Feature extraction", "content": "Radiomic features are usually divided into shape-, intensity-, and texture-based features [4]. The first category contains mathematical descriptors of ROI geometry, both 2D and 3D. The second category contains intensity statistics and histogram-derived descriptors. The third category contains descriptors of the spatial distribution of image intensity values and their mutual orientation. Shape-based features are not relevant in most neuroimaging studies since the shape of the analyzed ROIs is either standardized or complicated. Nevertheless, volume is always important since it can represent the dystrophy of brain structures, lesion load, or size of the affected area. Intensity- and texture-based features describe tissue properties including tissue homogeneity and density, therefore these groups of features are used in neurological studies.\nAdditional features to the ones obtained from the original image can be extracted when the same values are calculated from the transformed images. Image transformations may include, but are not limited to, square, square root, logarithm, exponential, Gaussian, Laplacian, Laplace of Gaussian, wavelet, local binary pattern, and Gabor filters.\nWhile performing feature extraction, besides image and ROI mask, feature extraction parameters are needed. Different feature extraction tools provide different levels of customization. IBSI has some recommendations on the most common feature extraction parameters. This includes intensities re-scaling with normalization or z-scoring. Intensity re-scaling for the images expressed in arbitrary units (MRI) is recommended but it is not for (semi) quantitative images, such as CTs. Another feature extraction parameter is intensity discretization before extracting texture features [25]."}, {"title": "2.5 Data analysis", "content": "After the feature extraction step, data analysis as well as model development and validation are performed. For this, there are a large number of publications on good practices in AI [26, 27]. Therefore, in this section, we will focus on radiomics-specific steps.\nEvery case of missing data raises a question of feature or patient elimination, or data imputation. In [28], a histological data imputation approach was suggested relying on the present features. Excluding patients will limit the population. Excluding features will limit the amount of diagnostic information. However, crucial clinical or demographic infor- mation missing should lead to record exclusion. Since the radiomic features are highly intercorrelated, in case of the absence of a radiomic feature, it can be both eliminated or imputed.\nWhile developing a radiomics signature, it is important to assess feature stability and exclude non-reproducible features. To detect reproducible features, test-retest studies should be performed [29, 30]. Additionally, stability does not mean informativity, therefore, further steps on feature selection are needed.\nAfter the data is split into train and test sets, the test set should be kept apart and used only in model evaluation so that they do not interfere with the model building process. In some cases, when the data size is not large or to show model robustness to data deviation, cross-validation is performed. This means that the data is split in one of the common cross-validation schemes multiple times and the whole training process is performed from scratch for every split. To show model generalizability, a good practice is to perform external validation to demonstrate model performance on external data coming from different acquisition equipment or hospitals.\nTo get rid of the redundant information in the data, inter-correlated features should be excluded preserving the informa- tion content. Additionally, it is necessary to exclude features with zero and low variance since they may contain little signal. To detect non-variant features, the standard deviation is usually calculated followed by scaling to the mean value. This approach gives unstable results if feature values have significantly different ranges and mean values. One of the popular methods is implemented as a nearZeroVar function of Caret package (https://topepo.github.io/caret/) in R.\nThe final step before modeling is feature selection to only retain the informative features. Usually, feature selection steps are model-based. Therefore, for different machine learning models, different features might be selected. The feature set should be reported together with the model performance: if the resulting model performs with low scores, we cannot conclude that the features are strongly linked to the outcome. Feature selection can be performed based on the univariate feature performance [31], feature weights in the model, or \u201crecursive feature elimination\u201d (RFE) based on recursively decreasing the feature set size and comparing model performances [32]. The number of the features in the final feature set can be estimated based on the sample size using several rules of thumb [33, 34], or empirically based on the saliency point of the dependency of the model score from the number of features.\nDimensionality reduction methods such as principal component analysis (PCA) [35], independent component analysis (ICA) [36], or linear discriminant analysis (LDA) [37] can also be used to decrease the model complexity. This group of methods is based on the features decomposition resulting in the input matrix transformation into a lower-dimensionality matrix containing only distinctive information. However, it is not commonly used in radiomics since it transforms transparent radiomic features into abstract values losing the interpretability of the final method.\nIn the case of deep radiomics, feature reduction, and selection procedures are performed by the neural network in a data-driven manner.\nAt this stage, overfitting as one of the major problems in machine learning should be considered. Comparison of the training and testing set scores enables assessment of an overfitting effect. A way to examine the model for overfitting is a permutation test [38]. To prevent overfitting, several techniques can be applied. These techniques are based on introducing random components into the data or the model and include data augmentation, regularization, ensembling, early stopping, or dropout layer for deep radiomics."}, {"title": "2.6 Harmonization", "content": "After the whole radiomics pipeline is established, the next steps are larger multi-center studies or clinical trials. One of the challenges here is feature instability caused by variations in population or acquisition equipment. It leads to situations where models are performing poorly on the data from the unseen domain. Harmonization is data alignment ensuring its compatibility and consistency. As it was shown in [39], harmonization can be performed at different steps of the radiomics pipeline, but globally in the image or feature domain.\nIn multi-center studies, data harmonization can start at the beginning of the study by standardizing image acquisition. Nevertheless, even when the protocols are standardized, the intensity distribution in scans can vary, especially while dealing with MRI data. To identify stable features, test-retest and phantom studies can be performed [30].\nIf the raw sensor data is available, scans can be reconstructed with the same parameters [40]. It is possible to implement traditional image processing methods (intensity normalization, z-scoring) as well as deep-learning-based style transfer. Nevertheless, while changing the appearance of the scans and their intensity distribution, it is not clear whether it will improve feature analysis. In [41], the U-Net is trained to produce the MRI scans with consistent contrast. In [42], the generative adversarial network was trained to harmonize MRI scans aiming to preserve the consistency of feature values.\nFinally, feature values can be harmonized. The most popular feature harmonization method is ComBat originally developed for harmonization of the gene expression data [43]. It is an empirical Bayesian method aimed at removing batch-specific bias and preserving the influence of biologically significant components. For the neuroimaging data, DeepCombat combining conditional variational autoencoder architecture with ComBat methodology is suggested [44]."}, {"title": "2.7 Handcrafted vs. deep radiomics", "content": "Deep radiomics automatically learns representative image features from the high-dimensional data by using non-linear modules of the neural network [45]. Handcrafted radiomics represents a \u201chard-coded\u201d version of deep radiomics. Whereas in handcrafted radiomics ROI, feature formulas, and mathematical models are defined by the user, in deep radiomics, these instances are learned from the data. Therefore, feature extraction and selection can be replaced with the neural network. However, a neural network can be a supplementary component for the handcrafted workflow to segment the ROI. In the most general case, the whole scan can be used as a neural network input, as illustrated in Figure 2. In this case, the model will learn to identify the informative features of the scan.\nBoth handcrafted and deep radiomics have advantages and limitations. Handcrafted radiomics can be trained with less data and is more transparent due to the interpretable features. However, it is limited in capturing complex patterns, not robust to variations of imaging parameters, and requires image preprocessing and segmentation. Deep radiomics learns relevant features automatically, captures more complex dependencies, and shows impressive results. It can adapt to the different imaging modalities and tasks with minimal changes in architecture. Moreover, additional domain knowledge can be utilized with transfer learning. However, deep radiomics is greedy for training data and computational resources and is challenging to interpret [46, 47].\nNot all of the handcrafted features are necessarily linked to the outcome. In contrast, deep radiomics generates features during the training process. Recently attention in AI has shifted towards self-supervised learning and foundation models [48, 49]. Self-supervised learning aims to provide pseudo-labels to the data by deriving supervisory signals from the data. After pre-training, models can be fine-tuned for specific downstream tasks. This approach allows for training with a substantial amount of unlabeled data and enables utilization of the same pseudo-labels for multiple downstream tasks creating a new concept of imaging features."}, {"title": "2.8 Neuroimaging software packages", "content": "To sum up, the most acknowledged open-source neuroimaging software packages are presented in Table 1. At the moment, the most common language for open-source research software is Python. Table 2 presents Python packages for neuroimaging."}, {"title": "3 Applications", "content": "The number of papers in non-oncological neurology is growing starting from the first publication in 2017 [9], which explicitly mentions radiomics. Figure 3 shows the number of papers per year found in the PubMed database with the search query 'radiomics AND neurology NOT oncology'. In this section, we will review the non-oncological neurological studies utilising radiomics."}, {"title": "3.1 Alzheimer's disease", "content": "Alzheimer's disease (AD) is the leading cause of dementia worldwide, and dementia is the leading cause of disability among the elderly population [73]. Since research is extensively performed in this field, and much experience and data have been accumulated, the first neurological radiomic studies were performed in this area.\nAs the disease develops gradually, causing different levels of disability, the first work [74] classified disease stages with deep radiomics using data from Alzheimer's Disease Neuroimaging Initiative (ADNI) database [75]. This first study pointed out the need for more data and for establishing the connection between the radiomic features and pathological processes. There are some later works on AD stages classification as well [76]. Many later works performed binary classification between AD patients and normal controls (NC) to show that MRI- and PET-derived features can be associated with AD [77, 78, 79, 80, 81, 82, 83, 84, 85]. Whereas in AD, pathological changes might be well visible in scans and be accompanied by strong clinical symptoms, mild cognitive impairment (MCI) is harder to diagnose. Moreover, MCI individuals can be confused with AD patients and NC. There are works on distinguishing between NC and MCI patients [86, 87]. Most of the AD radiomics studies are performed on classification between NC, MCI, and AD patients utilizing either pair-wise binary or multi-class classification [88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99].\nAs MCI is considered as an early stage of AD, for these patients it is crucial to know whether their impairment will progress to AD. In [100], the classification model is trained to estimate amyloid positivity status in MCI patients. In a number of works, classification models are built to directly predict the conversion from MCI to AD [101, 102, 103, 104, 105, 106, 107]. There were also attempts to predict the speed of disease progression [108]. Nevertheless, AD is not the only cause of dementia, and focusing on AD patients only will lead to the lack of specificity of the methods. There are works on the classification of the different dementia diagnoses: dementia with Lewy bodies vs AD [108, 109] and idiopathic normal pressure hydrocephalus vs. AD [110].\nNevertheless, besides clinical radiomics, there are traditional neuroimaging features to characterize the brain. Functional connectivity plays an important role here. Even though a number of studies relied on fMRI [111, 87, 76, 105, 83], the connection between these biomarkers has yet to be revealed. In [111], the correlation between connectivity and radiomic features is studied."}, {"title": "3.2 Multiple sclerosis", "content": "Multiple sclerosis (MS) is the leading cause of disability among the young population [112]. The disease progression is fast, therefore early diagnosis and prognosis for the patient are important.\nHowever, the first deep radiomic study on inflammatory degeneration was performed on survival classification of the amyotrophic lateral sclerosis patients [113]. MS-related studies were performed later and covered simple binary classification between MS and NC [114, 115, 116]. These works show the utility of the approach but lack the specificity of MS among other inflammatory neurodegenerative diseases. Neuromyelitis optica-spectrum disorder (NOSD) can be easily confused with MS therefore a precise diagnosis is needed to enable the correct treatment. There are some works on binary classification between MS and NOSD [117, 118, 119, 120]. There are studies where the classification models are built to distinguish between MS and other diagnoses such as neuropsychiatric systemic lupus erythematosus [121] and ischemic vasculopathy [122]. The multi-class classifier was built to distinguish between MS, NOSD, migraine, and vasculitis [123]. Besides diagnosis, it is essential to grade the disease severity within the MS cohort. In [124], the radiomics model was trained to estimate the relapse rate in MS. In [125], EDSS score was predicted with deep radiomics. In [126], MS types were classified as well as NC.\nSeveral models are trained to characterize the neurodegeneration process. In [127], the deep learning model is built to perform the detection of the demyelinated voxels on PET. In [128], MS lesion classification is performed. In [129, 130], lesion rim status classification is performed. In [131], a handcrafted radiomic model was built to predict lesion growth."}, {"title": "3.3 Parkinson's disease", "content": "Parkinson's disease (PD) is a degenerative condition of the central nervous system, which develops disability faster than any other neurological disorder [132]. Since it develops gradually, exploration of the early and specific biomarkers is essential.\nThe first work was on the motor function assessment in PD [9]. To show the utility of radiomics, the models were built to distinguish between PD patients and NC [133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]. As PD is heterogeneous in terms of its clinical phenotype, for treatment and severity estimation, PD variants classification is important. The models to classify the patients between parkinsonism subtypes were built in a number of works [145, 146, 147, 148, 149, 150, 151, 152]. In PD there is a need for differentiation from other similar diseases. In [153, 154], binary classification models were trained to distinguish between PD and multiple system atrophy and progressive supranuclear palsy. Radiomics approach was also used for the treatment response prediction [155].\nTransfer learning attempts are performed in the PD field trying to fine-tune the AD diagnostic model for PD diagnosis [156]. Diagnostic support solutions are suggested as in [155], where the nigrosome 1 abnormalities detector is trained."}, {"title": "3.4 Stroke", "content": "Stroke is the second leading cause of death, and third leading cause of disability worldwide [157]. The main task in stroke management is patient outcome prediction.\nAs a first task in stroke management, stroke areas should be identified. In [158], the deep learning model was trained to detect the stroke area on non-contrast CT scans. In [159], the model was trained to distinguish between hyperperfusion areas from normal ones. In [160], primary and secondary hemorrhages were classified. The other works were devoted to the prognosis of stroke area development [161, 162, 163, 164]. Besides the pathology development prediction, there were studies focused on the prediction of the biological recovery processes. In [165], collateral circulation was classified. In [166, 167], models were trained to predict recanalization. Most works were focused on the treatment outcome prediction, including functional and cognitive, for both thrombolysis and mechanical thrombectomy [168, 169, 170, 171, 172, 173]. There are studies on stroke onset time estimation with radiomics [174]. Finally, some studies are performed to train the radiomic models to predict post-stroke events such as recurrent stroke or epilepsy [175, 176, 177]."}, {"title": "3.5 Epilepsy", "content": "Epilepsy is the most common neurological disease [178]. In [179], the model was trained to predict epilepsy laterality. In [180], the epileptic foci detection method was suggested. In [181], the radiomic model was trained to detect focal cortical dysplasia lesions. In [182], the binary classifier was built to distinguish between Juvenile Myoclonic Epilepsy and NC."}, {"title": "3.6 Mental disorders", "content": "Mental disorders affect behavior and quality of life significantly and are observed in 12.5% of the population [183]. Most studies in radiomics in mental disorders are devoted to the binary classification between the NC and schizophrenia patients [184, 185, 186, 187, 188, 189, 190, 191, 192], bipolar disorder patients [193], or first episode psychosis [194, 195, 196]. In [195], a more difficult deep radiomics classifier was built to distinguish between schizophrenia patients, major depressive disorder, and NC. In [196], the classifier was built to distinguish between first-episode psychosis, bipolar disorder, and NC. Finally, there are some works on radiomics implementation for treatment response prediction [197, 198]."}, {"title": "3.7 Neurodevelopmental disorders", "content": "The most common neurodevelopmental disorders are autism spectrum disorder (ASD) and attention-deficit/hyperactivity disorder (ADHD). They affect cognitive and behavioral functions and might require life-long care and support. In most radiomic studies, binary classification models were built to distinguish between NC and ASD [199], and ADHD [200, 201]. In [202], ASD-linked radiomic features were revealed."}, {"title": "3.8 Open-source datasets", "content": "We believe that to enable extensive research in some diagnostic areas it is essential to have access to a sufficient amount of medical imaging data."}, {"title": "4 Discussion", "content": "In this review, we gave an overview of the radiomics pipeline in clinical non-oncological neuroimaging and gave some recommendations on each step of the pipeline. The variety of open-source tools for neuroimaging analysis as well as the expanding amount of radiomics studies are bringing optimism in the development of artificial intelligence (AI) in clinical neurology. Moreover, the amount of accumulated data is still growing, pushing the quantitative neuroimaging development forward. Therefore, the approaches, that previously existed in oncology, can be landed in neurology. However, there are some limitations present in the majority of the papers and characterizing the current challenges in the field."}, {"title": "4.1 Data availability", "content": "Many current studies are cross-sectional and performed on small private datasets. These datasets usually are not sufficient to demonstrate pathological patterns and represent the target cohort. One-center training severely decreases the generalizability of the model. To enable clinical implementation of the model, it needs to confirm its performance on various demographics as well as acquisition equipment set-ups.\nExternal validation on samples coming from the different data domains is recommended. It is desirable to have external validation data to be prospectively collected to show robustness to the potential data drift. However, external validation results should be interpreted carefully. Sample size, heterogeneity, and data balance should be considered. External validation performance should be explainable considering the sample properties. The external validation does not give absolute information about model generalizability. There are some industry-inspired suggestions to perform regular and recurrent validation every time the model is deployed to evaluate the generalizability of the predictive model [214, 215].\nAnother consequence of the limited data accessibility is the low reproducibility of the published studies. Experiments on private data cannot be repeated. Additionally, it is not possible to check the labeling correctness. Moreover, if one wants to compare some models, he needs to test them on exactly the same cohort. Nevertheless, there are established datasets used by multiple research groups. These are 1) open-source datasets, such as ADNI, 2) challenge datasets (https://grand-challenge.org/) which are much smaller and usually do not have extensive multi-modal data, 3) clinical trials datasets (for example, [216]). However, using a single dataset without any external data introduces overfitting across the community, which limits the usefulness of the dataset itself. To make the data and data-driven solutions sustainable and therefore trustful, the following four principles have to be maintained: Findability, Accessibility, Interoperability, and Reusability (FAIR) [217].\nIn the last years, attention has been drawn to the latest generations of AI models \u2013 visual transformers and foundation models. They have a huge potential in medicine. Visual transformers demonstrate high performance while solving visual tasks such as image classification or segmentation [218, 219]. Foundation models will be able to solve complex problems on multi-modal data with a high variety of particular downstream tasks [49, 220]. However, the main challenge in their implementation is the high demand for the amount of the training data. This fact gives an additional motivation for data sharing and aggregation."}, {"title": "4.2 Data harmonization", "content": "Another limitation resulting from limited data availability is the lack of data harmonization. It is affecting the radiomic methods themselves because of the heterogeneity of the population as well as acquisition equipment. If the development data is not population and equipment representative, every new inference data point can be out of the distribution which leads to the wrong model outcomes. The CT data is presented in HU which gives it quantification and stability. MRI data is expressed in arbitrary units and acquired with a large variety of MR sequences and hardware. One of the promising directions in the MRI research is in multi-echo qMRI maps reconstruction. It gives stability and quantification to the MRI data, but at the moment this approach is far from being used in the clinical set-up. Another approach is in implementation of the AI-based methods such as generative adversarial networks [42] to harmonize the data."}, {"title": "4.3 Clinical relevance of the data", "content": "Reliable and stable imaging biomarkers for neurological disorders should be not only sensitive but also specific for every neurological condition. In the current studies, most of the models are developed to distinguish between the disorder and NC. Therefore, these approaches are not applicable in clinical practice, where more than one neurological condition can be suspected. Moreover, co-existing conditions are also possible. Therefore, broader studies and intra-diagnosis tests are needed to develop disease-specific radiomic signatures.\nMost neurological diseases develop gradually, and the patients are diagnosed in already chronic stages of the disease. This brings bias to the study data that almost does not contain non-symptomatic patients at early stages. These patients are highly important to develop methods for early diagnosis and disease prevention. However, there are some longitudinal studies (for example, ADNI-based) that are performed with the early-stage cases."}, {"title": "4.4 Study design", "content": "Current studies are mostly proof-of-concept. Therefore, the study design is highly simplified. Prediction tasks are solved as classification tasks in most cases and the outcomes are limited by the predicted event timeframe.\nImaging modality should be selected correctly, based on domain knowledge, existing clinical protocols, and its availability.\nThe fusion of data of different natures should not be performed before the predictive power of every baseline data source is studied. However, data fusion can be justified by showing the added value in model performance obtained with the fused data compared to the separate baseline models.\nFor handcrafted radiomics, ROI selection should be based on domain knowledge. In the currently published papers, this justified approach was demonstrated, and ROIs were selected based on the brain areas affected the most by the corresponding diseases."}, {"title": "4.5 Pipeline implementation", "content": "Different research groups perform radiomic pipeline steps differently and in different order. This results in inconsistency and low reproducibility of the results. To overcome this issue"}, {"title": "4.5 Pipeline implementation", "content": "Different research groups perform radiomic pipeline steps differently and in different order. This results in inconsistency and low reproducibility of the results. To overcome this issue, transparent reporting is essential following TRIPOD [221] and RQS [222]. Recently, the CheckList for Evaluation of Radiomics Research (CLEAR) checklist for radiomics was out [223].\nAdditionally, code sharing has become a common practice in scientific reporting in the last few years making the results transparent and reproducible [224].\nAnother challenge is caused by the fact that multiple data processing tools are implemented in different platforms and environments breaking the consistency of the data flow. However, for every common neuroimaging tool multiple APIs exist enabling a single infrastructure for the study implementation.\nTo justify the selection of the model, its design, and hyperparameters in a reproducible environment, experiment tracking tools such as MLFlow (www.mlflow.org) are useful. They do not only inform the researcher about the best-performing setup but also the protocol of all the experiments.\nAnother implementation challenge is related to image segmentation which is traditionally performed manually. Since intra- and inter-reader agreement is never absolute, development and improvement of the automated segmentation methods is needed.\nFor the clinical application of AI in medical imaging, FUTURE-AI guiding principles are developed [225] to ensure that AI solutions are effective, trustworthy, ethical, and safe."}, {"title": "4.6 Interpretation", "content": "Even though the reported models perform with high scores, there is still a lack of interpretation. For this, behavioral analysis should be performed and aligned with the clinical knowledge. Connections between the predictive and stable radiomic features and clinical parameters should be studied. Additionally, pathological mechanisms are not revealed by the radiomic studies, and large work should be done in this field supported by extensive clinical and histological data.\nSince medical imaging analysis involves high-stakes decisions, information is needed about which influence inputs have on a final decision of the model. Due to a simple implementation, handcrafted radiomics is more transparent compared to deep radiomics. However, for acceptance in clinical practice, implementation of explainable AI (XAI) is needed for overcoming a \"black box problem\" [226, 227].\nWhile every AI model is accompanied by its performance scores, which provide insights into its efficiency and facilitate comparisons with other models [228], it is imperative to remember that the significance lies not in the AI scores themselves but in the impact on clinical outcomes. Consequently, for more advanced models, the inclusion of supplementary metrics is vital to elucidate how they enhance the clinical pipeline."}, {"title": "5 Conclusion", "content": "We gave a review of the radiomic pipeline from the clinical neuroimaging perspective. The amount of collected data and the high performance of the published models have shown that the application of radiomics in neuroimaging will increase diagnostic precision and quality of treatment. Development of the new AI methods will increase the performance of the deep radiomics as well as will enable solutions for more complex and multi-modal tasks. However, some important limitations are preventing the implementation of this methodology in clinical practice for high-level diagnostic support rather than image quantification. To overcome these limitations, it is necessary to set data exchange and collaborations, work on data harmonization methods, and implement reproducible pipelines and transparent reporting."}]}