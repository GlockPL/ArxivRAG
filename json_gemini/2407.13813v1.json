{"title": "A REVIEW OF HANDCRAFTED AND DEEP RADIOMICS IN\nNEUROLOGICAL DISEASES: TRANSITIONING FROM ONCOLOGY\nTO CLINICAL NEUROIMAGING", "authors": ["Elizaveta Lavrova", "Henry Woodruff", "Hamza Khan", "Eric Salmon", "Philippe Lambin", "Christophe Phillips"], "abstract": "Medical imaging technologies have undergone extensive development, enabling non-invasive visu-\nalization of clinical information. The traditional review of medical images by clinicians remains\nsubjective, time-consuming, and prone to human error. With the recent availability of medical\nimaging data, quantification have become important goals in the field. Radiomics, a methodology\naimed at extracting quantitative information from imaging data, has emerged as a promising approach\nto uncover hidden biological information and support decision-making in clinical practice. This paper\npresents a review of the radiomic pipeline from the clinical neuroimaging perspective, providing a\ndetailed overview of each step with practical advice. It discusses the application of handcrafted and\ndeep radiomics in neuroimaging, stratified by neurological diagnosis. Although radiomics shows great\npotential for increasing diagnostic precision and improving treatment quality in neurology, several\nlimitations hinder its clinical implementation. Addressing these challenges requires collaborative\nefforts, advancements in image harmonization methods, and the establishment of reproducible and\nstandardized pipelines with transparent reporting. By overcoming these obstacles, radiomics can\nsignificantly impact clinical neurology and enhance patient care.", "sections": [{"title": "1 Introduction", "content": "Since the discovery of X-rays [1], medical imaging has advanced significantly. However, the conventional manual\nreview of medical images by clinicians is subjective, time-consuming, and costly. With the increasing availability of\nmedical imaging data, there is a growing opportunity for quantitative analysis in this field.\nRadiomics is a methodology aimed at retrieving quantitative information from imaging data [2]. It is based on extracting\nnumerous descriptors from medical images and finding a link between features and clinical outcomes. Handcrafted\nradiomics utilizes termed features, which are mathematically defined during the pipeline development, whereas deep\nradiomics utilizes features created by the artificial neural network during the model training process. The radiomics\napproach hypothesizes that medical imaging data contains hidden, complementary biological information that can\nbe used for decision support in clinical practice [3]. Therefore, this method is of high interest for application in\nindividualized diagnosis and treatment.\nAs handcrafted radiomics workflow requires a segmented region of interest (ROI), this methodology has been extensively\ndeveloped in the oncological field where tumors and organs are routinely delineated for treatment planning purposes [2].\nSince its inception, pioneer studies have revealed the connection between imaging biomarkers and histology [4] and\nhave matured to produce externally validated and clinically relevant predictive models [5, 6].\nWhereas in oncology a large amount of segmented imaging data is accumulated mostly for radiotherapy needs, other\nbranches of medicine have collected imaging data and could potentially benefit from the application of radiomics.\nThus, it is essential to perform an early diagnosis of neurological diseases since symptoms appear after the disease\nprogresses considerably. Often there are no formal reliable biomarkers, and the diagnosis is based on the regularly\nreviewed diagnostic criteria [7]. Therefore, the differential diagnosis between the diseases and handling the atypical\ncases might be challenging.\nThus, radiomics is an emerging methodology in medical imaging research expanding from oncology to other branches\nof medicine. However, the review of radiomics in non-oncological neurology is needed to analyze the current state\nof the art, identify the methodological pitfalls, and suggest possible solutions for the future progress of quantitative\nclinical neuroimaging. In this review, we present a typical workflow of radiomics analysis regarding neuroimaging. We\nprovide a broad overview of the currently published works stratified by neurological diagnosis. We discuss the current\nlimitations of radiomics in neurology, suggesting potential improvements."}, {"title": "2 Workflow", "content": "The following section considers the practical implementation of radiomics in the neuroimaging field combining some\ncommon steps of radiomics and neuroimaging workflows (illustrated in Figure 1). After the steps are described, the list\nof the corresponding software is provided."}, {"title": "2.1 Data curation", "content": "In hospitals, the data is saved in Picture Archiving and Communications Systems (PACS) in Digital Imaging and\nCommunications in Medicine (DICOM) format [8, 9]. It stores imaging data together with metadata. In research, open\nfile formats are preferred, such as Nifti, Analyse, MNC, and NRRD [10].\nTo read and write the imaging and metadata, an application programming interface (API) for the currently relevant\nprogramming languages is recommended to make the pipeline fully automated and avoid manually introduced mistakes.\nClinical and metadata need to be anonymized or pseudo-anonymized, considering the possible need for follow-up\nacquisitions [11]. Brain scans usually include the facial features of the patient or teeth. Since facial features or teeth can\nbe used to identify a person, it is necessary to remove them as well. A simple procedure for defacing is skull stripping\n[12].\nA good practice is a sanity check of the data. It might include linking imaging and non-imaging samples to reveal\nmissing or unwanted data, acquisition time point check for longitudinal studies and image quality check.\nIn neuroimaging research, the Nifti format is preferred as it is standardized and constructed specifically for neuroimaging\ndata. In the case of data conversion to Nifti with a custom code, it is important to correctly transfer the geometrical\nparameters of the scan, as described in https://nipy.org/nibabel/coordinate_systems.html.\nFor automated data analysis, maintaining a uniform data structure is crucial. Data structure for different patients,\nimaging modalities, and potential acquisition timeframes should be established together with naming conventions for\nfiles and folders. For neuroimaging specifically, the Brain Imaging Data Structure (BIDS) [13] is recommended. It\noffers a standardized approach that is suited for multi-modal data and its derivatives and is supported by the community."}, {"title": "2.2 Data pre-processing", "content": "After the proper curation, data is considered ready for use. The next step is image pre-processing which is described in\n[14].\nSince many imaging modalities or image acquisition time points can be combined in neurology, brain scan co-registration\nis needed. It means that multiple brain scans should be co-aligned to achieve the closest spatial position. The data to\nbe co-registered can belong to different imaging modalities or sequences. Besides co-registration at the patient level,\nregistration to the tissue probability maps in the standardized space can be performed [15]. Co-registration can be\nperformed in both rigid (only the head position and orientation are changed) and non-rigid (additional scaling and\nelastic deformations) ways. Even though non-rigid co-registration allows for the best correspondence of the anatomy\nand regions of interest, it changes visualized tissue texture. Therefore, for clinical tasks, mostly rigid co-registration on\na patient level is applied.\nImage re-shaping is required to obtain the same voxel shape within the dataset. It allows for the same input image shape\nin the pipeline. While changing the voxel size, it is important to consider the interpolation effects introduced. In [16],\ndifferent interpolation methods are described. The detailed recommendations are given in [14].\nBrain scans contain intensity inhomogeneities due to the presence of the bias field. In MRI, the bias field is caused by\nthe MR field inhomogeneity of the scanner originating from the equipment [17] and the patient disturbing the magnetic\nfield. To reduce the effect of the bias field, bias field correction (BFC) can be applied. While performing BFC, it is\nimportant to consider that it might reduce the contrast and remove critical abnormality information. The most popular\nmethod is N4 BFC [18]. However, there are recent works on deep learning-based BFC [19]. CT is not affected by\nbias because it represents attenuation of the X-ray beam through the body, therefore, in general, this procedure is not\nrecommended for CT scans.\nSince CT images are expressed in HU, with a well-defined range, more advanced reliable pre-processing is possible.\nKnowing the characteristic HU for the tissue of interest, it is possible to exclude all the objects on the scan that are not\nrelevant to the analysis. Signal clipping can be applied to the intensities outside of the range of interest [20].\nEven though \"hard pre-processing\" is not recommended in the quantitative image analysis to prevent a signal loss [14],\nsome filtering can be applied to decrease the noise level. The most popular filters among smoothing filters are Gaussian\nand median filters. Gaussian filter is effective in removing high-frequency noises whereas a median filter is applied to\nremove impulse noise [21]."}, {"title": "2.3 Image segmentation", "content": "In neurology, ROIs can be anatomically or physiologically derived and vary from application to application. Since\nbrain structures have a systematic organization and traditional computer vision techniques can be applied for segmen-\ntation, there are many computer vision-based auto-segmentation tools recognized by the neuroimaging community.\nNevertheless, the development of neural networks brings new solutions which are gaining more interest. The deep\nlearning models are trained on different data and do not contain mathematical constraints about anatomy or the expected\ndistribution of intensities.\nBrain extraction narrows the image size and removes the surrounding tissues. In some studies, radiomics analysis was\nperformed over the whole brain mask [22]. But since the brain includes different structures, whole-brain radiomics\ndo not give comprehensive information about particular shapes and textures. Nevertheless, this kind of analysis is\nprospective in the discovery of healthy and pathological brain signatures for screening. In some works, features are\nextracted from the right and left hemispheres to be compared [23].\nA lower level of defining the ROIs is presented with the nervous tissues. The human brain is composed of white and\ngray matter. Analysis of the radiomics features extracted from the separate brain tissues is closer to the in-vivo histology\nand allows for the interpretation of texture and density abnormalities.\nMany studies focus on analyzing certain areas of the cortex and deep gray matter. In this case, it is possible to build\nconnections between imaging data and functional outcomes since particular cortical areas are responsible for the specific\nfunctions. Moreover, analyzing certain areas can lead to early disease diagnosis before more severe symptoms appear\n[24].\nROIs described above can contain both healthy and pathological tissues. In most cases, neurological pathology appears\nin textural changes. But when the pathological tissue is compact (hematoma or tissue lesion), it can be selected as ROI.\nPhysiologically-derived ROIs are obtained from functional imaging such as PET or fMRI. They represent the delineated\ntracer or function activity areas. No manual delineation is needed, and ROI contours depend on the selected binarization\nmethod."}, {"title": "2.4 Feature extraction", "content": "Radiomic features are usually divided into shape-, intensity-, and texture-based features [4]. The first category contains\nmathematical descriptors of ROI geometry, both 2D and 3D. The second category contains intensity statistics and\nhistogram-derived descriptors. The third category contains descriptors of the spatial distribution of image intensity\nvalues and their mutual orientation. Shape-based features are not relevant in most neuroimaging studies since the shape\nof the analyzed ROIs is either standardized or complicated. Nevertheless, volume is always important since it can\nrepresent the dystrophy of brain structures, lesion load, or size of the affected area. Intensity- and texture-based features\ndescribe tissue properties including tissue homogeneity and density, therefore these groups of features are used in\nneurological studies.\nAdditional features to the ones obtained from the original image can be extracted when the same values are calculated\nfrom the transformed images. Image transformations may include, but are not limited to, square, square root, logarithm,\nexponential, Gaussian, Laplacian, Laplace of Gaussian, wavelet, local binary pattern, and Gabor filters.\nWhile performing feature extraction, besides image and ROI mask, feature extraction parameters are needed. Different\nfeature extraction tools provide different levels of customization. IBSI has some recommendations on the most common\nfeature extraction parameters. This includes intensities re-scaling with normalization or z-scoring. Intensity re-scaling\nfor the images expressed in arbitrary units (MRI) is recommended but it is not for (semi) quantitative images, such as\nCTs. Another feature extraction parameter is intensity discretization before extracting texture features [25]."}, {"title": "2.5 Data analysis", "content": "After the feature extraction step, data analysis as well as model development and validation are performed. For this,\nthere are a large number of publications on good practices in AI [26, 27]. Therefore, in this section, we will focus on\nradiomics-specific steps.\nEvery case of missing data raises a question of feature or patient elimination, or data imputation. In [28], a histological\ndata imputation approach was suggested relying on the present features. Excluding patients will limit the population.\nExcluding features will limit the amount of diagnostic information. However, crucial clinical or demographic infor-\nmation missing should lead to record exclusion. Since the radiomic features are highly intercorrelated, in case of the\nabsence of a radiomic feature, it can be both eliminated or imputed.\nWhile developing a radiomics signature, it is important to assess feature stability and exclude non-reproducible features.\nTo detect reproducible features, test-retest studies should be performed [29, 30]. Additionally, stability does not mean\ninformativity, therefore, further steps on feature selection are needed.\nAfter the data is split into train and test sets, the test set should be kept apart and used only in model evaluation so that"}, {"title": "2.6 Harmonization", "content": "After the whole radiomics pipeline is established, the next steps are larger multi-center studies or clinical trials. One\nof the challenges here is feature instability caused by variations in population or acquisition equipment. It leads to\nsituations where models are performing poorly on the data from the unseen domain. Harmonization is data alignment\nensuring its compatibility and consistency. As it was shown in [39], harmonization can be performed at different steps\nof the radiomics pipeline, but globally in the image or feature domain.\nIn multi-center studies, data harmonization can start at the beginning of the study by standardizing image acquisition.\nNevertheless, even when the protocols are standardized, the intensity distribution in scans can vary, especially while\ndealing with MRI data. To identify stable features, test-retest and phantom studies can be performed [30].\nIf the raw sensor data is available, scans can be reconstructed with the same parameters [40]. It is possible to implement\ntraditional image processing methods (intensity normalization, z-scoring) as well as deep-learning-based style transfer.\nNevertheless, while changing the appearance of the scans and their intensity distribution, it is not clear whether it will\nimprove feature analysis. In [41], the U-Net is trained to produce the MRI scans with consistent contrast. In [42],\nthe generative adversarial network was trained to harmonize MRI scans aiming to preserve the consistency of feature\nvalues.\nFinally, feature values can be harmonized. The most popular feature harmonization method is ComBat originally\ndeveloped for harmonization of the gene expression data [43]. It is an empirical Bayesian method aimed at removing\nbatch-specific bias and preserving the influence of biologically significant components. For the neuroimaging data,\nDeepCombat combining conditional variational autoencoder architecture with ComBat methodology is suggested [44]."}, {"title": "2.7 Handcrafted vs. deep radiomics", "content": "Deep radiomics automatically learns representative image features from the high-dimensional data by using non-linear\nmodules of the neural network [45]. Handcrafted radiomics represents a \u201chard-coded\u201d version of deep radiomics.\nWhereas in handcrafted radiomics ROI, feature formulas, and mathematical models are defined by the user, in deep\nradiomics, these instances are learned from the data. Therefore, feature extraction and selection can be replaced with"}, {"title": "2.8 Neuroimaging software packages", "content": "To sum up, the most acknowledged open-source neuroimaging software packages are presented in Table 1. At the\nmoment, the most common language for open-source research software is Python. Table 2 presents Python packages\nfor neuroimaging."}, {"title": "3 Applications", "content": "The number of papers in non-oncological neurology is growing starting from the first publication in 2017 [9], which\nexplicitly mentions radiomics. Figure 3 shows the number of papers per year found in the PubMed database with\nthe search query 'radiomics AND neurology NOT oncology'. In this section, we will review the non-oncological\nneurological studies utilising radiomics."}, {"title": "3.1 Alzheimer's disease", "content": "Alzheimer's disease (AD) is the leading cause of dementia worldwide, and dementia is the leading cause of disability\namong the elderly population [73]. Since research is extensively performed in this field, and much experience and data\nhave been accumulated, the first neurological radiomic studies were performed in this area.\nAs the disease develops gradually, causing different levels of disability, the first work [74] classified disease stages with\ndeep radiomics using data from Alzheimer's Disease Neuroimaging Initiative (ADNI) database [75]. This first study\npointed out the need for more data and for establishing the connection between the radiomic features and pathological\nprocesses. There are some later works on AD stages classification as well [76]. Many later works performed binary\nclassification between AD patients and normal controls (NC) to show that MRI- and PET-derived features can be\nassociated with AD [77, 78, 79, 80, 81, 82, 83, 84, 85]. Whereas in AD, pathological changes might be well visible\nin scans and be accompanied by strong clinical symptoms, mild cognitive impairment (MCI) is harder to diagnose.\nMoreover, MCI individuals can be confused with AD patients and NC. There are works on distinguishing between NC\nand MCI patients [86, 87]. Most of the AD radiomics studies are performed on classification between NC, MCI, and\nAD patients utilizing either pair-wise binary or multi-class classification [88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99].\nAs MCI is considered as an early stage of AD, for these patients it is crucial to know whether their impairment will\nprogress to AD. In [100], the classification model is trained to estimate amyloid positivity status in MCI patients. In a"}, {"title": "3.2 Multiple sclerosis", "content": "Multiple sclerosis (MS) is the leading cause of disability among the young population [112]. The disease progression is\nfast, therefore early diagnosis and prognosis for the patient are important.\nHowever, the first deep radiomic study on inflammatory degeneration was performed on survival classification of\nthe amyotrophic lateral sclerosis patients [113]. MS-related studies were performed later and covered simple binary\nclassification between MS and NC [114, 115, 116]. These works show the utility of the approach but lack the specificity\nof MS among other inflammatory neurodegenerative diseases. Neuromyelitis optica-spectrum disorder (NOSD) can be\neasily confused with MS therefore a precise diagnosis is needed to enable the correct treatment. There are some works\non binary classification between MS and NOSD [117, 118, 119, 120]. There are studies where the classification models\nare built to distinguish between MS and other diagnoses such as neuropsychiatric systemic lupus erythematosus [121]\nand ischemic vasculopathy [122]. The multi-class classifier was built to distinguish between MS, NOSD, migraine,\nand vasculitis [123]. Besides diagnosis, it is essential to grade the disease severity within the MS cohort. In [124],\nthe radiomics model was trained to estimate the relapse rate in MS. In [125], EDSS score was predicted with deep\nradiomics. In [126], MS types were classified as well as NC.\nSeveral models are trained to characterize the neurodegeneration process. In [127], the deep learning model is built to\nperform the detection of the demyelinated voxels on PET. In [128], MS lesion classification is performed. In [129, 130],\nlesion rim status classification is performed. In [131], a handcrafted radiomic model was built to predict lesion growth."}, {"title": "3.3 Parkinson's disease", "content": "Parkinson's disease (PD) is a degenerative condition of the central nervous system, which develops disability faster than\nany other neurological disorder [132]. Since it develops gradually, exploration of the early and specific biomarkers is\nessential.\nThe first work was on the motor function assessment in PD [9]. To show the utility of radiomics, the models were\nbuilt to distinguish between PD patients and NC [133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144]. As PD\nis heterogeneous in terms of its clinical phenotype, for treatment and severity estimation, PD variants classification\nis important. The models to classify the patients between parkinsonism subtypes were built in a number of works\n[145, 146, 147, 148, 149, 150, 151, 152]. In PD there is a need for differentiation from other similar diseases. In\n[153, 154], binary classification models were trained to distinguish between PD and multiple system atrophy and\nprogressive supranuclear palsy. Radiomics approach was also used for the treatment response prediction [155].\nTransfer learning attempts are performed in the PD field trying to fine-tune the AD diagnostic model for PD diagnosis\n[156]. Diagnostic support solutions are suggested as in [155], where the nigrosome 1 abnormalities detector is trained."}, {"title": "3.4 Stroke", "content": "Stroke is the second leading cause of death, and third leading cause of disability worldwide [157]. The main task in\nstroke management is patient outcome prediction.\nAs a first task in stroke management, stroke areas should be identified. In [158], the deep learning model was\ntrained to detect the stroke area on non-contrast CT scans. In [159], the model was trained to distinguish between\nhyperperfusion areas from normal ones. In [160], primary and secondary hemorrhages were classified. The other works\nwere devoted to the prognosis of stroke area development [161, 162, 163, 164]. Besides the pathology development\nprediction, there were studies focused on the prediction of the biological recovery processes. In [165], collateral\ncirculation was classified. In [166, 167], models were trained to predict recanalization. Most works were focused on the\ntreatment outcome prediction, including functional and cognitive, for both thrombolysis and mechanical thrombectomy\n[168, 169, 170, 171, 172, 173]. There are studies on stroke onset time estimation with radiomics [174]. Finally, some\nstudies are performed to train the radiomic models to predict post-stroke events such as recurrent stroke or epilepsy\n[175, 176, 177]."}, {"title": "3.5 Epilepsy", "content": "Epilepsy is the most common neurological disease [178]. In [179], the model was trained to predict epilepsy laterality.\nIn [180], the epileptic foci detection method was suggested. In [181], the radiomic model was trained to detect focal\ncortical dysplasia lesions. In [182], the binary classifier was built to distinguish between Juvenile Myoclonic Epilepsy\nand NC."}, {"title": "3.6 Mental disorders", "content": "Mental disorders affect behavior and quality of life significantly and are observed in 12.5% of the population [183].\nMost studies in radiomics in mental disorders are devoted to the binary classification between the NC and schizophrenia\npatients [184, 185, 186, 187, 188, 189, 190, 191, 192], bipolar disorder patients [193], or first episode psychosis\n[194, 195, 196]. In [195], a more difficult deep radiomics classifier was built to distinguish between schizophrenia\npatients, major depressive disorder, and NC. In [196], the classifier was built to distinguish between first-episode\npsychosis, bipolar disorder, and NC. Finally, there are some works on radiomics implementation for treatment response\nprediction [197, 198]."}, {"title": "3.7 Neurodevelopmental disorders", "content": "The most common neurodevelopmental disorders are autism spectrum disorder (ASD) and attention-deficit/hyperactivity\ndisorder (ADHD). They affect cognitive and behavioral functions and might require life-long care and support. In\nmost radiomic studies, binary classification models were built to distinguish between NC and ASD [199], and ADHD\n[200, 201]. In [202], ASD-linked radiomic features were revealed."}, {"title": "3.8 Open-source datasets", "content": "We believe that to enable extensive research in some diagnostic areas it is essential to have access to a sufficient amount\nof medical imaging data. Table 3 presents some popular open medical imaging datasets containing neuroimaging data."}, {"title": "4 Discussion", "content": "In this review, we gave an overview of the radiomics pipeline in clinical non-oncological neuroimaging and gave some\nrecommendations on each step of the pipeline. The variety of open-source tools for neuroimaging analysis as well as\nthe expanding amount of radiomics studies are bringing optimism in the development of artificial intelligence (AI) in\nclinical neurology. Moreover, the amount of accumulated data is still growing, pushing the quantitative neuroimaging\ndevelopment forward. Therefore, the approaches, that previously existed in oncology, can be landed in neurology.\nHowever, there are some limitations present in the majority of the papers and characterizing the current challenges in\nthe field."}, {"title": "4.1 Data availability", "content": "Many current studies are cross-sectional and performed on small private datasets. These datasets usually are not\nsufficient to demonstrate pathological patterns and represent the target cohort. One-center training severely decreases\nthe generalizability of the model. To enable clinical implementation of the model, it needs to confirm its performance\non various demographics as well as acquisition equipment set-ups.\nExternal validation on samples coming from the different data domains is recommended. It is desirable to have external\nvalidation data to be prospectively collected to show robustness to the potential data drift. However, external validation\nresults should be interpreted carefully. Sample size, heterogeneity, and data balance should be considered. External\nvalidation performance should be explainable considering the sample properties. The external validation does not give\nabsolute information about model generalizability. There are some industry-inspired suggestions to perform regular and\nrecurrent validation every time the model is deployed to evaluate the generalizability of the predictive model [214, 215].\nAnother consequence of the limited data accessibility is the low reproducibility of the published studies. Experiments\non private data cannot be repeated. Additionally, it is not possible to check the labeling correctness. Moreover,\nif one wants to compare some models, he needs to test them on exactly the same cohort. Nevertheless, there are\nestablished datasets used by multiple research groups. These are 1) open-source datasets, such as ADNI, 2) challenge\ndatasets (https://grand-challenge.org/) which are much smaller and usually do not have extensive multi-modal\ndata, 3) clinical trials datasets (for example, [216]). However, using a single dataset without any external data\nintroduces overfitting across the community, which limits the usefulness of the dataset itself. To make the data and\ndata-driven solutions sustainable and therefore trustful, the following four principles have to be maintained: Findability,\nAccessibility, Interoperability, and Reusability (FAIR) [217].\nIn the last years, attention has been drawn to the latest generations of AI models \u2013 visual transformers and foundation\nmodels. They have a huge potential in medicine. Visual transformers demonstrate high performance while solving\nvisual tasks such as image classification or segmentation [218, 219]. Foundation models will be able to solve complex\nproblems on multi-modal data with a high variety of particular downstream tasks [49, 220]. However, the main challenge\nin their implementation is the high demand for the amount of the training data. This fact gives an additional motivation\nfor data sharing and aggregation."}, {"title": "4.2 Data harmonization", "content": "Another limitation resulting from limited data availability is the lack of data harmonization. It is affecting the radiomic\nmethods themselves because of the heterogeneity of the population as well as acquisition equipment. If the development"}, {"title": "4.3 Clinical relevance of the data", "content": "Reliable and stable imaging biomarkers for neurological disorders should be not only sensitive but also specific for every\nneurological condition. In the current studies, most of the models are developed to distinguish between the disorder and\nNC. Therefore, these approaches are not applicable in clinical practice, where more than one neurological condition can\nbe suspected. Moreover, co-existing conditions are also possible. Therefore, broader studies and intra-diagnosis tests\nare needed to develop disease-specific radiomic signatures.\nMost neurological diseases develop gradually, and the patients are diagnosed in already chronic stages of the disease.\nThis brings bias to the study data that almost does not contain non-symptomatic patients at early stages. These\npatients are highly important to develop methods for early diagnosis and disease prevention. However, there are some\nlongitudinal studies (for example, ADNI-based) that are performed with the early-stage cases."}, {"title": "4.4 Study design", "content": "Current studies are mostly proof-of-concept. Therefore, the study design is highly simplified. Prediction tasks are\nsolved as classification tasks in most cases and the outcomes are limited by the predicted event timeframe.\nImaging modality should be selected correctly, based on domain knowledge, existing clinical protocols, and its\navailability.\nThe fusion of data of different natures should not be performed before the predictive power of every baseline data\nsource is studied. However, data fusion can be justified by showing the added value in model performance obtained\nwith the fused data compared to the separate baseline models.\nFor handcrafted radiomics, ROI selection should be based on domain knowledge. In the currently published papers,\nthis justified approach was demonstrated, and ROIs were selected based on the brain areas affected the most by the\ncorresponding diseases."}, {"title": "4.5 Pipeline implementation", "content": "Different research groups perform radiomic pipeline steps differently and in different order. This results in inconsistency\nand low reproducibility of the results. To overcome this issue, transparent reporting is essential following TRIPOD\n221] and RQS [222]. Recently, the CheckList for Evaluation of Radiomics Research (CLEAR) checklist for radiomics\nwas out [223].\nAdditionally, code sharing has become a common practice in scientific reporting in the last few years making the results\ntransparent and reproducible [224].\nAnother challenge is caused by the fact that multiple data processing tools are implemented in different platforms and\nenvironments breaking the consistency of the data flow. However, for every common neuroimaging tool multiple APIs\nexist enabling a single infrastructure for the study implementation.\nTo justify the selection of the model, its design, and hyperparameters in a reproducible environment, experiment tracking\ntools such as MLFlow (www.mlflow.org) are useful. They do not only inform the researcher about the best-performing\nsetup but also the protocol of all the experiments.\nAnother implementation challenge is related to image segmentation which is traditionally performed manually. Since\nintra- and inter-reader agreement is never absolute, development and improvement of the automated segmentation\nmethods is needed.\nFor the clinical application of AI in medical imaging, FUTURE-AI guiding principles are developed [225] to ensure\nthat AI solutions are effective, trustworthy, ethical, and safe."}, {"title": "4.6 Interpretation", "content": "Even though the reported models perform with high scores, there is still a lack of interpretation. For this, behavioral\nanalysis should be performed and aligned with the clinical knowledge. Connections between the predictive and stable\nradiomic features and clinical parameters should be studied. Additionally, pathological mechanisms are not revealed by\nthe radiomic studies, and large work should be done in this field supported by extensive clinical and histological data.\nSince medical imaging analysis involves high-stakes decisions, information is needed about which influence inputs have"}, {"title": "5 Conclusion", "content": "We gave a review of the radiomic pipeline from the clinical neuroimaging perspective. The amount of collected data\nand the high performance of the published models have shown that the application of radiomics in neuroimaging\nwill increase diagnostic precision and quality of treatment. Development of the new AI methods will increase the\nperformance of the deep radiomics as well as will enable solutions for more complex and multi-modal tasks. However,\nsome important limitations are preventing the implementation of this methodology in clinical practice for high-level\ndiagnostic support rather than image quantification. To overcome these limitations, it is necessary to set data exchange\nand collaborations, work on data harmonization methods, and implement reproducible pipelines and transparent\nreporting."}]}