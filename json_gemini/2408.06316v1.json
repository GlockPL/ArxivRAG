{"title": "Body Transformer: Leveraging\nRobot Embodiment for Policy Learning", "authors": ["Carmelo Sferrazza", "Dun-Ming Huang", "Fangchen Liu", "Jongmin Lee", "Pieter Abbeel"], "abstract": "In recent years, the transformer architecture has become the de facto\nstandard for machine learning algorithms applied to natural language process-\ning and computer vision. Despite notable evidence of successful deployment\nof this architecture in the context of robot learning, we claim that vanilla trans-\nformers do not fully exploit the structure of the robot learning problem. There-\nfore, we propose Body Transformer (BoT), an architecture that leverages the\nrobot embodiment by providing an inductive bias that guides the learning pro-\ncess. We represent the robot body as a graph of sensors and actuators, and rely\non masked attention to pool information throughout the architecture. The re-\nsulting architecture outperforms the vanilla transformer, as well as the classical\nmultilayer perceptron, in terms of task completion, scaling properties, and com-\nputational efficiency when representing either imitation or reinforcement learn-\ning policies. Additional material including the open-source code is available at\nhttps://sferrazza.cc/bot_site.", "sections": [{"title": "1 Introduction", "content": "For most of their correcting and stabilizing actions, physical agents exhibit motor responses that\nare spatially correlated with the location of the external stimuli they perceive [1]. This is the case\nof a surfer, where the lower body, i.e. feet and ankles, is mostly responsible for counteracting the\nimbalance induced by the wave under the board [2]. In fact, humans present feedback loops at the\nlevel of the spinal cord's neural circuitry that are specifically responsible for the response of single\nactuators [3]."}, {"title": "", "content": "Corrective localized actuation is a main factor for efficient locomotion [4]. This is particularly\nimportant for robots too, where however, learning architectures do not typically exploit spatial inter-\nrelations between sensors and actuators. In fact, robot policies have mostly been exploiting the same\narchitectures developed for natural language or computer vision, without effectively leveraging the\nstructure of the robot body.\nThis work focuses on transformer policies, which show promise to effectively deal with long se-\nquence dependencies and seamlessly absorb large amount of data. The transformer architecture [5]\nhas been developed for unstructured natural language processing (NLP) tasks, e.g., language trans-\nlations, where the input sequences often map to reshuffled output sequences. In contrast, we propose\nBody Transformer (BoT), an architecture that augments the attention mechanism of transformers by\ntaking into account the spatial placement of sensors and actuators across the robot body.\nBoT models the robot body as a graph with sensors and actuators at its nodes. Then, it applies a\nhighly sparse mask at the attention layers, preventing each node from attending beyond its direct\nneighbors. Concatenating multiple BoT layers with the same structure leads to information being\npooled throughout the graph, thus not compromising the representation power of the architecture.\nOur contributions are listed below:\n\u2022 We propose the BoT architecture, which augments the transformer architecture with a novel\nmasking that leverages the morphology of the robot body.\n\u2022 We incorporate this novel architecture in an imitation learning setting, showing how the\ninductive bias provided by BoT leads to better steady-state performance and generalization,\nas well as stronger scaling properties.\n\u2022 We show how BoT improves online reinforcement learning (RL), outperforming MLP and\nvanilla transformer baselines.\n\u2022 We analyze the computational advantages of BoT, by showing how reformulating the scaled\ndot product in the computation of the attention operation leads to near-200% runtime and\nfloating point operations (FLOPs) reduction."}, {"title": "2 Related Work", "content": "Transformers in robotics. Originally developed for NLP applications [5], transformers have been\nsuccessfully applied across domains, for example in computer vision [6] and audio processing [7].\nSeveral works have shown applications of transformers as a means to represent robot policies [8,\n9, 10], demonstrating its core advantages in this setting, i.e., variable context length, handling long\nsequences [11] and multiple modalities [12, 13]. However, these approaches use transformers as\noriginally developed for unstructured or grid-like inputs, such as language or images, respectively.\nIn this work, we leverage the robot embodiment by appropriately adapting the transformer attention\nmechanism.\nGraph Neural Networks (GNNs). GNNs [14] are a class of learning architectures that can pro-\ncess inputs in the form of a graph [15]. While early versions of these architectures featured ex-\nplicit message-passing schemes along the graph [16, 17], more recent architectures mostly feature\nattention-based approaches. In fact, the vanilla transformer, with its variable context length, inher-\nently supports fully connected graphs. However, state-of-the-art performance on graph interpretation\nbenchmarks is only achieved via modifications of the original transformer architecture, for example\nby means of learned graph encodings and attention biases [18]. A contemporaneous work, Buterez\net al. [19], following a similar idea as in the work from Veli\u010dkovi\u0107 et al. [20], utilizes masked atten-\ntion layers, where each node only attends to its neighbors, and interleaves such layers with unmasked\nattention layers. In this work, we exploit masked attention in a policy learning setting, by addition-\nally proposing an architecture that only comprises layers where each can attend to itself and its direct\nneighbors, resulting in naturally growing context over layers, i.e., the outputs of the first layers are\ncomputed using more local information compared to those of the last layers."}, {"title": "", "content": "Exploiting body structure for policy learning. Graph neural networks have been explored by\nseveral works as a way to obtain multi-task RL policies that are effective across different robot\nmorphologies. Earlier works focused on message passing algorithms [21, 22], and were later out-\nperformed by vanilla transformers [23, 24] and transformer-based GNNs that make use of learned\nencoding and attention biases [25]. All these approaches were only demonstrated in simulated\nbenchmarking scenarios and not applied to a real-world robotics setting. Compared to previous\nwork, we additionally show that introducing bottlenecks in the attention mask fully exploits the em-\nbodiment structure and benefits policy learning also for tasks achieved by a single agent, leading to\nbetter performance and more favorable scaling."}, {"title": "3 Background", "content": ""}, {"title": "3.1 Attention Mechanisms in Transformers", "content": "Transformer, a foundational architecture in modern machine learning applications as well as in our\nwork, is powered by the self-attention mechanism [5]. Self-attention weighs the values correspond-\ning to each element of the sequence with a score that is computed from pairs of keys and queries\nextracted from the same sequence. Thus, it is able to identify relevant pairs of sequence elements in\nthe model output.\nConcretely, the self-attention output vector is computed through the following matrix operation:\nAttention(Q, K, V) = softmax(QKT/\u221adk)V,\nwhere Q, K, V (respectively query, key, and value matrices) are learnable linear projections of\nthe sequence elements' embedding vectors, and dk is the dimension of the embedding space. As\nembedding pairs with higher correspondence will have a higher score (from the computation of\nQKT), the corresponding value vector of an embedding's associated key will receive a higher weight\nin the attention mechanism output."}, {"title": "3.2 Transformer-based GNNS", "content": "In this work, we model the agent embodiment as a graph whose nodes are sensors and actuators,\nand their connecting edges reflect the body morphology. While message-passing GNNs are suitable\ninductive biases for this formulation, they tend to suffer from oversmoothing and oversquashing of\nrepresentations, preventing effective long-range interactions and discouraging network depth [26].\nMore recently, self-attention was proposed as an alternative to message-passing [18, 23]. While\nthe standard self-attention mechanism amounts to modeling a fully connected graph, a popular\ntransformer-based GNN, Graphormer [18], injects node-edges information through graph-based po-\nsitional encodings [27, 28, 18, 25] and by biasing the scaled dot-product, i.e.,\nAttention(Q, K, V) = softmax(QKT/\u221adk + B)V,\nwhere B is a learnable matrix that depends on graph features, e.g., shortest path or adjacency matrix,\nand can effectively encode the graph structure in a flexible manner."}, {"title": "3.3 Masked Attention", "content": "The attention mechanism can be altered [5] with a binary mask M \u2208 {0,1}n\u00d7n (where n is the\nsequence length), which is equivalent to replacing the elements of B in (1):\nBi,j = { 1 Mi,j = 1\n\u2212\u221e Mi,j = 0'\nwhere i and j denote row and column indices. This operation effectively results in zeroing out the\ncontribution of the pairs indicated with zeros in the mask M to the computation of the attention."}, {"title": "4 Body Transformer", "content": "Robot learning policies that employ the vanilla transformer architecture as a backbone typically\nneglect the useful information provided by the embodiment structure. In contrast, here we leverage\nthis structure to provide a stronger inductive bias to transformers, while retaining the representation\npower of the original architecture. We propose Body Transformer (BoT), which is based on masked\nattention, where at each layer in the resulting architecture, a node can only attend to information from\nitself and its direct neighbors. As a result, information flows according to the graph structure, with\nthe upstream layers reasoning according to local information and the downstream layers pooling\nmore global information from the farther nodes."}, {"title": "", "content": "We present below the various components of the BoT architecture (see also Figure 1): (1) a tokenizer\nthat projects the sensory inputs into the corresponding node embedding, (2) a transformer encoder\nthat processes the input embeddings and generates output features of the same dimension, and (3) a\ndekokenizer that decodes the features to actions (or values, for RL critic's training).\nTokenizer. We map the observation vector to a graph of local observations. In practice, we assign\nglobal quantities to the root element of the body, and local quantities to the nodes representing the\ncorresponding limbs, similarly to previous GNN approaches [22, 23, 24, 25]. Then, a linear layer\nprojects the local state vector into an embedding vector. Each node's state is fed into its node-specific\nlearnable linear projection, resulting in a sequence of n embeddings, where n represents the number\nof nodes (or sequence length). This is in contrast to the existing works [23, 24, 25] that use a single\nshared learnable linear projection to deal with varying number of nodes in multi-task RL.\nBoT Encoder. We use a standard transformer encoder [5] with several layers as a backbone, and\npresent two variants of our architecture:\n\u2022 BoT-Hard masks every layer with a binary mask M that reflects the structure of the graph.\nSpecifically, we construct the mask as M = In + A, where In is the identity matrix of\ndimension n, and A is the adjacency matrix corresponding to the graph (see Figure 2 for\nan example). Concretely, this allows each node to attend to itself and its direct neighbors,\nand introduces considerable sparsity in the problem, which is particularly appealing from a\ncomputational perspective as highlighted in Section 5.4.\n\u2022 BoT-Mix interleaves layers with masked attention (constructed as in BoT-Hard) with layers\nwith unmasked attention. This is similar to the concurrent work in Buterez et al. [19],\nwith the distinctions that, I) we find it more effective in our experimental setting to have a\nmasked attention layer as the first layer, II) our mask M is not equivalent to the adjacency\nmatrix, allowing a node to additionally attend to itself at every layer of the architecture."}, {"title": "", "content": "Detokenizer. The output features from the transformer encoder are fed into linear layers that project\nthem to the actions associated with the node's limb, which are assigned based on the proximity of\nthe corresponding actuator with the limb. Once again, these learnable linear projection layers are\nseparate for each of the nodes. When BoT is employed as a critic architecture in the RL setting, as in\nthe experiments presented in Section 5.2, the detokenizers output values rather than actions, which\nare then averaged across body parts."}, {"title": "5 Experiments", "content": "We assess the performance of BoT across imitation learning and reinforcement learning settings. We\nkeep the same structure as in Figure 1 and only replace the BoT encoder with the various baseline\narchitectures to single out the effect of the encoder. Particularly, across the various experiments\nlisted in this section, we present the following baselines and variations: (i) an MLP that stacks all\nembedding vectors as its input, (ii) a vanilla unmasked transformer encoder, (iii) BoT-Hard that\nonly uses masked self-attention layers, (iv) BoT-Mix that alternates between masked and unmasked\nself-attention layers. All comparisons are made across models with a similar number of trainable\nparameters.\nWith the following experiments, we aim to answer the following questions:\n\u2022 Does masked attention benefit imitation learning in terms of performance and generaliza-\ntion?\n\u2022 Does BoT exhibit a positive scaling trend compared to a vanilla transformer architecture?\n\u2022 Is BoT compatible with the RL framework, and what are sensible design choices to maxi-\nmize performance?\n\u2022 Can BoT policies be applied to a real-world robotics task?\n\u2022 What are the computational advantages of masked attention?"}, {"title": "5.1 Imitation Learning Experiments", "content": "We evaluate the imitation learning performance of the BoT architecture in a body-tracking task\ndefined through the MoCapAct dataset [29], which comprises action-labeled humanoid state tra-\njectories with over 5M transitions, spanning a total of 835 tracking clips. For each architecture,\nwe train a deterministic behavioral cloning (BC) policy. We evaluate mean returns normalized by\nthe length of a clip, in addition to the normalized length of an episode, which terminates when the\ntracking error goes beyond a threshold. We run the evaluations both on the training and the (unseen)\nvalidation clips.\nWe report results in the table shown in Figure 3a, where BoT consistently outperforms the MLP and\ntransformer baselines. Remarkably, the gap with these architectures further increases on the unseen\nvalidation clips, demonstrating the generalization capabilities provided by the embodiment-aware\ninductive bias. We also report the performance on the training set obtained by a tailored multi-clip\npolicy presented in Wagener et al. [29] with the MoCapAct dataset. While the multi-clip policy is\ncompetitive with the vanilla transformer baseline, it is strongly outperformed by our architecture.\nThis is a particularly remarkable result, as the comparison presents conditions more favorable to\nthe baseline, which features a more flexible stochastic policy, was optimized in a recurrent fashion\ntailored to the tracking task, and was trained on a larger set of rollouts.\nAs shown in Figure 3b, we also find that BoT-Hard exhibits strong scaling capabilities, as its per-\nformance keeps improving with the number of trainable parameters compared to the transformer\nbaseline, both on the training and validation clips. This further indicates a tendency for BoT-Hard to\nnot overfit to the training data, which is induced by the embodiment bias. Additional comparisons\nare reported in Appendix E, including experiments on a dexterous manipulation benchmark, see\nFigure 4."}, {"title": "5.2 Reinforcement Learning Experiments", "content": "We evaluate the RL performance of BoT and baselines using PPO [30] on 4 robotic control tasks in\nIsaac Gym [31]: Humanoid-Mod, Humanoid-Board, Humanoid-Hill, and A1-Walk.\nAll humanoid environments build on top of the classical Humanoid environment in Isaac Gym,\nwhere we modify the observation space to increase the number of distributed sensory information\n(see details in Appendix A) and include contact forces at all limbs. Humanoid-Mod features the\nclassical running task on flat ground, while in Humanoid-Hill we replaced the flat ground with an\nirregular hilly terrain. Humanoid-Board is a newly designed task, where the task is for the humanoid\nto keep balancing on a board placed on top of a cylinder. Finally, we adapt the A1-Walk environment,\nwhich is part of the Legged Gym repository [32], where the task is for a Unitree Al quadruped robot\nto follow a fixed velocity command.\nFigure 5 presents the average episode return of evaluation rollouts during training for MLP, Trans-\nformer, and BoT (Hard and Mix). The solid curve corresponds to the mean, and the shaded area to\nthe standard error over five seeds. The result shows that BoT-Mix consistently outperforms both the\nMLP and vanilla transformer baselines in terms of sample efficiency and asymptotic performance,\nhighlighting the efficacy of integrating body-induced biases into the policy network architecture."}, {"title": "", "content": "Meanwhile, BoT-Hard performs better than the vanilla transformer on simpler tasks (A1-Walk and\nHumanoid-Mod), but shows relatively inferior results in hard-exploration tasks (Humanoid-Board\nand Humanoid-Hill). Given that the masked attention bottlenecks information propagation from\ndistant body parts, BoT-Hard's strong constraints on information communication may hinder effi-\ncient RL exploration: In Humanoid-Board and Humanoid-Hill, it may be useful for information\nabout sudden changes in ground conditions to be transmitted from the toes to the fingertips in the\nupstream layers. For such tasks, BoT-Mix strikes a good balance between funneling information\nthrough the embodiment graph and enabling global pooling at intermediate layers to ensure efficient\nexploration. In contrast, in Al-Walk or Humanoid-Mod, the environment's state changes more reg-\nularly, thus the strong body-induced bias can effectively reduce the search space, enabling faster\nlearning with BoT-Hard."}, {"title": "5.3 Real World Experiments", "content": "The Isaac Gym simulated locomotion environments are widely popular for sim-to-real transfer of\nRL policies without requiring adaptation in the real-world [32]. To verify that our architecture is\nsuitable for real-world applications, e.g., running in real time, we deploy one of the BoT policies\ntrained above to a real-world Unitree A1 robot, adapting the codebases in Zhuang et al. [33] and\nWu et al. [34]. This is showcased in the supplementary video, demonstrating feasibility of our\narchitecture for real-world deployment. We note that for simplicity we did not make use of teacher-\nstudent training or memory mechanisms [35] as common in the locomotion literature, which are\nknown to further improve the transfer by resulting in more natural gaits."}, {"title": "5.4 Computational Analysis", "content": "Connections between body parts of a physical agent are often sparse, and so are the pre-computable\nmasks M for embodiment graphs in BoT. Masked attention mechanisms can benefit from this\nsparsity, as their computational cost can be reduced by ignoring unnecessary computations of\nthose matrix elements that will eventually be masked out. Large-purpose deep learning libraries\nsuch as PyTorch feature largely optimized matrix multiplication and attention routines (e.g.,\nFlashAttention [36]), but do not leverage possible sparsity in the masked attention mechanism,\nascribed by Buterez et al. [19] to missing use cases so far. For a fair computational comparison,\nwe re-implement the scaled dot product in Equation (1) using CPU-based NumPy and evaluate on a"}, {"title": "6 Conclusion", "content": "In this work, we presented a novel graph-based policy architecture, Body Transformer, which ex-\nploits the robot body structure as an inductive bias. Our experiments show how masked attention,\nwhich is at the core of BoT, benefits both imitation and reinforcement learning algorithms. Addition-\nally, the architecture exhibits favorable scaling and computational properties, making it appealing\nfor applications on high-dimensional systems.\nHere, we used transformers to process sequences of distributed sensory information from the same\ntimestep. However, transformers have been shown to excel at processing information across time\ntoo. We leave the extension of BoT to the temporal dimension as future work, as it promises to\nfurther improve real world deployment of robot policies, such as the one demonstrated on the Unitree\nAl robot."}, {"title": "A Details on RL environments", "content": "We adapt the IsaacGym humanoid environment for the three humanoid-related tasks, by modifying\nthe observation space to include the vertical position of the torso, root coordinates and angular\nvelocity, joint positions and velocities, and per-limb contact forces. We leave the reward for the\nHumanoid-Mod and Humanoid-Hill unchanged, while we adapt the reward for Humanoid-Bob by\nforcing the forward target velocity to zero, and appropriately adjusting the target and termination\nheights to take the balancing board into account. For the A1-Walk task, we adapt the codebase\nin Zhuang et al. [33] and train the policies using proprioception only for the actor, and additional\nsimulation parameters for the critic. We define the task to mantain a target velocity of 0.5 m/s on\nan irregular terrain."}, {"title": "B Real-World Deployment", "content": "We deployed the RL policy trained for A1-Walk\ntask to a real-world Unitree A1 Robot. Three\nmain components \u2013 which are standard for sim-\nto-real locomotion [33] \u2013 were required for suc-\ncessful transfer, namely I) terrain randomiza-\ntion during training, similar to Zhuang et al.\n[33], II) higher stiffness in the joint controllers,\nand III) a low-pass action filter. The experi-\nments were run on flat ground, with offboard\ncomputation on CPU. Commands were sent via\nWiFi or Ethernet connection.\nThe policy was evaluated through 5 different\nrollouts, which were considered successful as\nlong as the robot walked for 10 seconds (based\non the available experimental space) without\nfalling. All five rollouts were successful.\nWe attach a supplementary video that demon-\nstrates the real-world deployment. A frame"}, {"title": "C Positional Encodings", "content": "For the reinforcement learning experiments presented in Section 5.2, we found that the use of po-\nsitional encodings improves the performance of BoT architectures. Specifically, we compute po-\nsitional encodings through an embedding layer that maps indices \u2013 up to n to encoding vectors,\nwhich are then added to the tokenizers' outputs. While this is beneficial for the reinforcement learn-\ning setting, we did not report a considerable improvement in the imitation learning setting, which\nwe present without the use of positional encodings. In fact, these are not strictly necessary, as in the\nBoT architecture tokenizers do not share weights across body parts, and may in principle replace the\nrole of positional encodings."}, {"title": "D Allocating Observations and Actions to Nodes", "content": "We follow simple rules to allocate observations and actions to the different nodes of the graph, each\nof which represents a robot limb and is mapped to a row (or column) of the mask M. We distinguish\nbetween two types of nodes:\n\u2022 A root node, to which we assign global observations (e.g., robot position and orientation)\nand environment observations (e.g., door handle angle, etc.).\n\u2022 Non-root nodes, to which we assign local observations (e.g., joint angles) and local actions\n(e.g., joint commands)."}, {"title": "", "content": "As a rule of thumb, observations or actions that spanned multiple nodes were assigned to the closest\nnode in the graph, or to either of the nodes involved.\nWe present an example allocation for the A1-Walk task below:\n\u2022 base - observations: orientation and angular velocity, actions: none.\n\u2022 front-left-hip \u2013 observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command.\n\u2022 front-left-thigh observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command.\n\u2022 front-left-calf observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command.\n\u2022 front-right-hip observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command.\n\u2022 front-right-thigh observations: corresponding joint angle, joint velocity, and previous\njoint command, actions: corresponding joint command.\n\u2022 front-right-calf observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command.\n\u2022 rear-left-hip observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command.\n\u2022 rear-left-thigh observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command.\n\u2022 rear-left-calf observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command.\n\u2022 rear-right-hip observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command.\n\u2022 rear-right-thigh observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command.\n\u2022 rear-right-calf observations: corresponding joint angle, joint velocity, and previous joint\ncommand, actions: corresponding joint command."}, {"title": "E Additional Imitation Learning Experiments", "content": ""}, {"title": "F Additional Reinforcement Learning Ablations", "content": ""}, {"title": "F.1 Effect of Body-Induced Masking in BoT", "content": "BoT relies on masked attention with its mask determined by the embodiment structure. We conduct\nan additional experiment in the RL setting to further demonstrate the effect of the body-induced\nmasking in this setting. We compare with BoT-Hard/Random and BoT-Mix/Random, where the\nattention mask M is given by a randomly sampled symmetric binary matrix with the same degree\nof sparsity (\u03b2 \u2248 0.82 for the IsaacGym humanoid). The results are presented in Figure 10. Overall,\nBoT with random masking (dotted lines) underperforms BoT with body-induced masking (solid\nlines) in both a simpler task (Humanoid-Mod) and a hard-exploration task (Humanoid-Board), which\nhighlights that the use of body-induced masking is crucial for the performance of BoT."}, {"title": "F.2 Effect of Per-Limb Tokenizer vs. Shared Tokenizer", "content": ""}, {"title": "", "content": "The existing works using Transformer-based policies [23, 24, 25] for multi-task RL adopt shared\nlinear projections for tokenizers and detokenizers to deal with the varying number of limbs, i.e.,\nper-limb observation features are projected into embedding vectors by the single shared tokenizer\nnetwork, and the per-limb hidden vectors are transformed to per-limb actions via the single shared\ndekokenizer network. In contrast, our BoT is designed for tasks with a single morphology, thus we\nadopt per-node linear projections for tokenizer and detokenizer. We conduct an additional experi-\nment to investigate the effect of this design choice, and the results are demonstrated in Figure 11.\nIn Figure 11, the solid lines denote the results of using per-node tokenizers/detokenizers, and the\ndotted lines present the results of using a shared tokenizer/detokenizer (which can be understood\nas representatives of the existing methods [23, 24, 25]). Overall, Transformer/BoT with per-node\n(de)tokenizers significantly outperform their shared (de)tokenizer counterparts in both a simpler\ntask (Humanoid-Mod) and a hard-exploration task (Humanoid-Board). This shows that the use of\ntokenizers shared across different limbs for Transformer-based policies hinders efficient learning."}, {"title": "GTraining Details", "content": "The training parameters of the experiments detailed in Section 5.1 and Section 5.2 are as summarized\nin Tables 12a, 12b, and 12c."}, {"title": "H FLOP Derivation for Custom Masked Attention Implementation", "content": "Below, we comparatively analyze an asymptotic bound for the amount of floating-point operations\nrequired in one scaled dot product (see Equation (1)) call between the vanilla and the masked ap-\nproach. From hereon, let n denote the sequence length and dk the input and output dimension of our\nattention mechanism.\nComputing QKT\u221adk . Considering Q \u2208 Rn\u00d7dk (and similarly for K), the computation of QKT will\ngenerally require d\u2081 multiplications and dk \u2013 1 additions for all of n\u00b2 pairs. Division by \u221adk results\nin n\u00b2 divisions and one constant factor c\u2081 of FLOPs for computing \u221adk. The total amount of flops\nis 2n2dk + C1.\nMasked computation of QKT\u221adk . Exploiting sparsity, we ignore all inner product computations for\nzero entries in M, computing only \u1e9en\u00b2 pairs of multiplications. This results in a reduction to\n2\u1e9en2dk + c\u2081 FLOPs.\nComputing Softmax(S). A softmax for one vector of dimension n requires n exponentiations,\n1 additions, and n divisions, performed for n rows. Let exponentiations require C2 FLOPs per\nelement, then a total of (2 + c2)n\u00b2 n FLOPs is performed.\nMasked computation of Softmax(S). As a result of sparsity, there is instead a total of \u1e9en2\nexponentiations \u1e9en\u00b2 divisions, and Bn2 n additions to compute, reducing our demand to (2 +\nc2)\u03b2\u03b72 n FLOPs.\nComputing the multiplication Softmax(S)V. A total of ndk pairs are multiplied, where each\npair requires 2n \u2013 1 operations to complete. The total amount of FLOPs is 2n2dk \u2013 ndk. Following\na similar reasoning with previous writing, a total of 2n2dk \u2013 ndk FLOPs are performed.\nAssuming that our physical agent provides a graph-induced mask M \u2208 {0,1}n\u00d7n of sparsity \u03b2\u2208\n[1, 1] (such that there are \u1e9en\u00b2 > n nonzero entries), then the amount of FLOPs required by a vanilla\nmasked self-attention implementation is 4n2dk + (2+c2)n\u00b2 \u2013 ndk n + c\u2081, while that of a custom\nmasked implementation is (2\u1e9e + 2)n\u00b2dk+ (2+c2)\u1e9en\u00b2 \u2013 ndk - n + c\u2081. Therefore, the performance\ngap between the vanilla and masked implementations is determined by the sparsity coefficient \u03b2, that\nis, the number of FLOPs that a vanilla approach requires will be c(n) times the number of FLOPs a"}, {"title": "I Computation Analysis vs Sparsity", "content": ""}]}