{"title": "Body Transformer: Leveraging Robot Embodiment for Policy Learning", "authors": ["Carmelo Sferrazza", "Dun-Ming Huang", "Fangchen Liu", "Jongmin Lee", "Pieter Abbeel"], "abstract": "In recent years, the transformer architecture has become the de facto standard for machine learning algorithms applied to natural language processing and computer vision. Despite notable evidence of successful deployment of this architecture in the context of robot learning, we claim that vanilla transformers do not fully exploit the structure of the robot learning problem. Therefore, we propose Body Transformer (BoT), an architecture that leverages the robot embodiment by providing an inductive bias that guides the learning process. We represent the robot body as a graph of sensors and actuators, and rely on masked attention to pool information throughout the architecture. The resulting architecture outperforms the vanilla transformer, as well as the classical multilayer perceptron, in terms of task completion, scaling properties, and computational efficiency when representing either imitation or reinforcement learning policies. Additional material including the open-source code is available at https://sferrazza.cc/bot_site.", "sections": [{"title": "1 Introduction", "content": "For most of their correcting and stabilizing actions, physical agents exhibit motor responses that are spatially correlated with the location of the external stimuli they perceive [1]. This is the case of a surfer, where the lower body, i.e. feet and ankles, is mostly responsible for counteracting the imbalance induced by the wave under the board [2]. In fact, humans present feedback loops at the level of the spinal cord's neural circuitry that are specifically responsible for the response of single actuators [3].\nCorrective localized actuation is a main factor for efficient locomotion [4]. This is particularly important for robots too, where however, learning architectures do not typically exploit spatial inter-relations between sensors and actuators. In fact, robot policies have mostly been exploiting the same architectures developed for natural language or computer vision, without effectively leveraging the structure of the robot body.\nThis work focuses on transformer policies, which show promise to effectively deal with long sequence dependencies and seamlessly absorb large amount of data. The transformer architecture [5] has been developed for unstructured natural language processing (NLP) tasks, e.g., language translations, where the input sequences often map to reshuffled output sequences. In contrast, we propose Body Transformer (BoT), an architecture that augments the attention mechanism of transformers by taking into account the spatial placement of sensors and actuators across the robot body.\nBoT models the robot body as a graph with sensors and actuators at its nodes. Then, it applies a highly sparse mask at the attention layers, preventing each node from attending beyond its direct neighbors. Concatenating multiple BoT layers with the same structure leads to information being pooled throughout the graph, thus not compromising the representation power of the architecture.\nOur contributions are listed below:\n\u2022 We propose the BoT architecture, which augments the transformer architecture with a novel masking that leverages the morphology of the robot body.\n\u2022 We incorporate this novel architecture in an imitation learning setting, showing how the inductive bias provided by BoT leads to better steady-state performance and generalization, as well as stronger scaling properties.\n\u2022 We show how BoT improves online reinforcement learning (RL), outperforming MLP and vanilla transformer baselines.\n\u2022 We analyze the computational advantages of BoT, by showing how reformulating the scaled dot product in the computation of the attention operation leads to near-200% runtime and floating point operations (FLOPs) reduction."}, {"title": "2 Related Work", "content": "Transformers in robotics. Originally developed for NLP applications [5], transformers have been successfully applied across domains, for example in computer vision [6] and audio processing [7]. Several works have shown applications of transformers as a means to represent robot policies [8, 9, 10], demonstrating its core advantages in this setting, i.e., variable context length, handling long sequences [11] and multiple modalities [12, 13]. However, these approaches use transformers as originally developed for unstructured or grid-like inputs, such as language or images, respectively. In this work, we leverage the robot embodiment by appropriately adapting the transformer attention mechanism.\nGraph Neural Networks (GNNs). GNNs [14] are a class of learning architectures that can process inputs in the form of a graph [15]. While early versions of these architectures featured explicit message-passing schemes along the graph [16, 17], more recent architectures mostly feature attention-based approaches. In fact, the vanilla transformer, with its variable context length, inherently supports fully connected graphs. However, state-of-the-art performance on graph interpretation benchmarks is only achieved via modifications of the original transformer architecture, for example by means of learned graph encodings and attention biases [18]. A contemporaneous work, Buterez et al. [19], following a similar idea as in the work from Veli\u010dkovi\u0107 et al. [20], utilizes masked attention layers, where each node only attends to its neighbors, and interleaves such layers with unmasked attention layers. In this work, we exploit masked attention in a policy learning setting, by additionally proposing an architecture that only comprises layers where each can attend to itself and its direct neighbors, resulting in naturally growing context over layers, i.e., the outputs of the first layers are computed using more local information compared to those of the last layers.\nExploiting body structure for policy learning. Graph neural networks have been explored by several works as a way to obtain multi-task RL policies that are effective across different robot morphologies. Earlier works focused on message passing algorithms [21, 22], and were later outperformed by vanilla transformers [23, 24] and transformer-based GNNs that make use of learned encoding and attention biases [25]. All these approaches were only demonstrated in simulated benchmarking scenarios and not applied to a real-world robotics setting. Compared to previous work, we additionally show that introducing bottlenecks in the attention mask fully exploits the embodiment structure and benefits policy learning also for tasks achieved by a single agent, leading to better performance and more favorable scaling."}, {"title": "3 Background", "content": "3.1 Attention Mechanisms in Transformers\nTransformer, a foundational architecture in modern machine learning applications as well as in our work, is powered by the self-attention mechanism [5]. Self-attention weighs the values corresponding to each element of the sequence with a score that is computed from pairs of keys and queries extracted from the same sequence. Thus, it is able to identify relevant pairs of sequence elements in the model output.\nConcretely, the self-attention output vector is computed through the following matrix operation:\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}}) V,$\nwhere Q, K, V (respectively query, key, and value matrices) are learnable linear projections of the sequence elements' embedding vectors, and dk is the dimension of the embedding space. As embedding pairs with higher correspondence will have a higher score (from the computation of $Q K^T$), the corresponding value vector of an embedding's associated key will receive a higher weight in the attention mechanism output.\n3.2 Transformer-based GNNS\nIn this work, we model the agent embodiment as a graph whose nodes are sensors and actuators, and their connecting edges reflect the body morphology. While message-passing GNNs are suitable inductive biases for this formulation, they tend to suffer from oversmoothing and oversquashing of representations, preventing effective long-range interactions and discouraging network depth [26].\nMore recently, self-attention was proposed as an alternative to message-passing [18, 23]. While the standard self-attention mechanism amounts to modeling a fully connected graph, a popular transformer-based GNN, Graphormer [18], injects node-edges information through graph-based positional encodings [27, 28, 18, 25] and by biasing the scaled dot-product, i.e.,\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}} + B) V,$\nwhere B is a learnable matrix that depends on graph features, e.g., shortest path or adjacency matrix, and can effectively encode the graph structure in a flexible manner.\n3.3 Masked Attention\nThe attention mechanism can be altered [5] with a binary mask $M \\in \\{0,1\\}^{n \\times n}$ (where n is the sequence length), which is equivalent to replacing the elements of B in (1):\n$B_{i,j} = \\begin{cases}1 & \\text{if } M_{i,j} = 1 \\\\-\\infty & \\text{if } M_{i,j} = 0\\end{cases}$"}, {"content": "where i and j denote row and column indices. This operation effectively results in zeroing out the contribution of the pairs indicated with zeros in the mask M to the computation of the attention."}, {"title": "4 Body Transformer", "content": "Robot learning policies that employ the vanilla transformer architecture as a backbone typically neglect the useful information provided by the embodiment structure. In contrast, here we leverage this structure to provide a stronger inductive bias to transformers, while retaining the representation power of the original architecture. We propose Body Transformer (BoT), which is based on masked attention, where at each layer in the resulting architecture, a node can only attend to information from itself and its direct neighbors. As a result, information flows according to the graph structure, with the upstream layers reasoning according to local information and the downstream layers pooling more global information from the farther nodes.\nWe present below the various components of the BoT architecture (see also Figure 1): (1) a tokenizer that projects the sensory inputs into the corresponding node embedding, (2) a transformer encoder that processes the input embeddings and generates output features of the same dimension, and (3) a detokenizer that decodes the features to actions (or values, for RL critic's training).\nTokenizer. We map the observation vector to a graph of local observations. In practice, we assign global quantities to the root element of the body, and local quantities to the nodes representing the corresponding limbs, similarly to previous GNN approaches [22, 23, 24, 25]. Then, a linear layer projects the local state vector into an embedding vector. Each node's state is fed into its node-specific learnable linear projection, resulting in a sequence of n embeddings, where n represents the number of nodes (or sequence length). This is in contrast to the existing works [23, 24, 25] that use a single shared learnable linear projection to deal with varying number of nodes in multi-task RL.\nBoT Encoder. We use a standard transformer encoder [5] with several layers as a backbone, and present two variants of our architecture:\n\u2022 BoT-Hard masks every layer with a binary mask M that reflects the structure of the graph. Specifically, we construct the mask as $M = I_n + A$, where $I_n$ is the identity matrix of dimension n, and A is the adjacency matrix corresponding to the graph (see Figure 2 for an example). Concretely, this allows each node to attend to itself and its direct neighbors, and introduces considerable sparsity in the problem, which is particularly appealing from a computational perspective as highlighted in Section 5.4.\n\u2022 BoT-Mix interleaves layers with masked attention (constructed as in BoT-Hard) with layers with unmasked attention. This is similar to the concurrent work in Buterez et al. [19], with the distinctions that, I) we find it more effective in our experimental setting to have a masked attention layer as the first layer, II) our mask M is not equivalent to the adjacency matrix, allowing a node to additionally attend to itself at every layer of the architecture."}, {"title": "5 Experiments", "content": "We assess the performance of BoT across imitation learning and reinforcement learning settings. We keep the same structure as in Figure 1 and only replace the BoT encoder with the various baseline architectures to single out the effect of the encoder. Particularly, across the various experiments listed in this section, we present the following baselines and variations: (i) an MLP that stacks all embedding vectors as its input, (ii) a vanilla unmasked transformer encoder, (iii) BoT-Hard that only uses masked self-attention layers, (iv) BoT-Mix that alternates between masked and unmasked self-attention layers. All comparisons are made across models with a similar number of trainable parameters.\nWith the following experiments, we aim to answer the following questions:\n\u2022 Does masked attention benefit imitation learning in terms of performance and generalization?\n\u2022 Does BoT exhibit a positive scaling trend compared to a vanilla transformer architecture?\n\u2022 Is BoT compatible with the RL framework, and what are sensible design choices to maximize performance?\n\u2022 Can BoT policies be applied to a real-world robotics task?\n\u2022 What are the computational advantages of masked attention?\n5.1 Imitation Learning Experiments\nWe evaluate the imitation learning performance of the BoT architecture in a body-tracking task defined through the MoCapAct dataset [29], which comprises action-labeled humanoid state trajectories with over 5M transitions, spanning a total of 835 tracking clips. For each architecture, we train a deterministic behavioral cloning (BC) policy. We evaluate mean returns normalized by the length of a clip, in addition to the normalized length of an episode, which terminates when the tracking error goes beyond a threshold. We run the evaluations both on the training and the (unseen) validation clips.\nWe report results in the table shown in Figure 3a, where BoT consistently outperforms the MLP and transformer baselines. Remarkably, the gap with these architectures further increases on the unseen validation clips, demonstrating the generalization capabilities provided by the embodiment-aware inductive bias. We also report the performance on the training set obtained by a tailored multi-clip policy presented in Wagener et al. [29] with the MoCapAct dataset. While the multi-clip policy is competitive with the vanilla transformer baseline, it is strongly outperformed by our architecture. This is a particularly remarkable result, as the comparison presents conditions more favorable to the baseline, which features a more flexible stochastic policy, was optimized in a recurrent fashion tailored to the tracking task, and was trained on a larger set of rollouts.\nAs shown in Figure 3b, we also find that BoT-Hard exhibits strong scaling capabilities, as its performance keeps improving with the number of trainable parameters compared to the transformer baseline, both on the training and validation clips. This further indicates a tendency for BoT-Hard to not overfit to the training data, which is induced by the embodiment bias. Additional comparisons are reported in Appendix E, including experiments on a dexterous manipulation benchmark, see Figure 4.\n5.2 Reinforcement Learning Experiments\nWe evaluate the RL performance of BoT and baselines using PPO [30] on 4 robotic control tasks in Isaac Gym [31]: Humanoid-Mod, Humanoid-Board, Humanoid-Hill, and A1-Walk.\nAll humanoid environments build on top of the classical Humanoid environment in Isaac Gym, where we modify the observation space to increase the number of distributed sensory information (see details in Appendix A) and include contact forces at all limbs. Humanoid-Mod features the classical running task on flat ground, while in Humanoid-Hill we replaced the flat ground with an irregular hilly terrain. Humanoid-Board is a newly designed task, where the task is for the humanoid to keep balancing on a board placed on top of a cylinder. Finally, we adapt the A1-Walk environment, which is part of the Legged Gym repository [32], where the task is for a Unitree Al quadruped robot to follow a fixed velocity command.\nFigure 5 presents the average episode return of evaluation rollouts during training for MLP, Transformer, and BoT (Hard and Mix). The solid curve corresponds to the mean, and the shaded area to the standard error over five seeds. The result shows that BoT-Mix consistently outperforms both the MLP and vanilla transformer baselines in terms of sample efficiency and asymptotic performance, highlighting the efficacy of integrating body-induced biases into the policy network architecture.\nMeanwhile, BoT-Hard performs better than the vanilla transformer on simpler tasks (A1-Walk and Humanoid-Mod), but shows relatively inferior results in hard-exploration tasks (Humanoid-Board and Humanoid-Hill). Given that the masked attention bottlenecks information propagation from distant body parts, BoT-Hard's strong constraints on information communication may hinder efficient RL exploration: In Humanoid-Board and Humanoid-Hill, it may be useful for information about sudden changes in ground conditions to be transmitted from the toes to the fingertips in the upstream layers. For such tasks, BoT-Mix strikes a good balance between funneling information through the embodiment graph and enabling global pooling at intermediate layers to ensure efficient exploration. In contrast, in Al-Walk or Humanoid-Mod, the environment's state changes more regularly, thus the strong body-induced bias can effectively reduce the search space, enabling faster learning with BoT-Hard.\n5.3 Real World Experiments\nThe Isaac Gym simulated locomotion environments are widely popular for sim-to-real transfer of RL policies without requiring adaptation in the real-world [32]. To verify that our architecture is suitable for real-world applications, e.g., running in real time, we deploy one of the BoT policies trained above to a real-world Unitree A1 robot, adapting the codebases in Zhuang et al. [33] and Wu et al. [34]. This is showcased in the supplementary video, demonstrating feasibility of our architecture for real-world deployment. We note that for simplicity we did not make use of teacher-student training or memory mechanisms [35] as common in the locomotion literature, which are known to further improve the transfer by resulting in more natural gaits.\n5.4 Computational Analysis\nConnections between body parts of a physical agent are often sparse, and so are the pre-computable masks M for embodiment graphs in BoT. Masked attention mechanisms can benefit from this sparsity, as their computational cost can be reduced by ignoring unnecessary computations of those matrix elements that will eventually be masked out. Large-purpose deep learning libraries such as PyTorch feature largely optimized matrix multiplication and attention routines (e.g., FlashAttention [36]), but do not leverage possible sparsity in the masked attention mechanism, ascribed by Buterez et al. [19] to missing use cases so far. For a fair computational comparison, we re-implement the scaled dot product in Equation (1) using CPU-based NumPy and evaluate on a"}, {"title": "6 Conclusion", "content": "In this work, we presented a novel graph-based policy architecture, Body Transformer, which exploits the robot body structure as an inductive bias. Our experiments show how masked attention, which is at the core of BoT, benefits both imitation and reinforcement learning algorithms. Additionally, the architecture exhibits favorable scaling and computational properties, making it appealing for applications on high-dimensional systems.\nHere, we used transformers to process sequences of distributed sensory information from the same timestep. However, transformers have been shown to excel at processing information across time too. We leave the extension of BoT to the temporal dimension as future work, as it promises to further improve real world deployment of robot policies, such as the one demonstrated on the Unitree Al robot.\nA mask with sparsity $\u03b2$ has $\u03b2n^2$ zero elements. When $\u03b2 = 1 - \\frac{1}{n}$, the mask reduces to the identity $I_n$.\nIn practice, the maximum degree of a vertex (or node) in robots will be approximately constant, making the computational complexity of masked attention grow linearly with the number of nodes."}, {"title": "A Details on RL environments", "content": "We adapt the IsaacGym humanoid environment for the three humanoid-related tasks, by modifying the observation space to include the vertical position of the torso, root coordinates and angular velocity, joint positions and velocities, and per-limb contact forces. We leave the reward for the Humanoid-Mod and Humanoid-Hill unchanged, while we adapt the reward for Humanoid-Bob by forcing the forward target velocity to zero, and appropriately adjusting the target and termination heights to take the balancing board into account. For the A1-Walk task, we adapt the codebase in Zhuang et al. [33] and train the policies using proprioception only for the actor, and additional simulation parameters for the critic. We define the task to mantain a target velocity of 0.5 m/s on an irregular terrain."}, {"title": "B Real-World Deployment", "content": "We deployed the RL policy trained for A1-Walk task to a real-world Unitree A1 Robot. Three main components \u2013 which are standard for sim-to-real locomotion [33] \u2013 were required for successful transfer, namely I) terrain randomization during training, similar to Zhuang et al. [33], II) higher stiffness in the joint controllers, and III) a low-pass action filter. The experiments were run on flat ground, with offboard computation on CPU. Commands were sent via WiFi or Ethernet connection.\nThe policy was evaluated through 5 different rollouts, which were considered successful as long as the robot walked for 10 seconds (based on the available experimental space) without falling. All five rollouts were successful.\nWe attach a supplementary video that demonstrates the real-world deployment. A frame overlay representing the robot motion is also shown in Figure 7."}, {"title": "C Positional Encodings", "content": "For the reinforcement learning experiments presented in Section 5.2, we found that the use of positional encodings improves the performance of BoT architectures. Specifically, we compute positional encodings through an embedding layer that maps indices \u2013 up to n to encoding vectors, which are then added to the tokenizers' outputs. While this is beneficial for the reinforcement learning setting, we did not report a considerable improvement in the imitation learning setting, which we present without the use of positional encodings. In fact, these are not strictly necessary, as in the BoT architecture tokenizers do not share weights across body parts, and may in principle replace the role of positional encodings."}, {"title": "D Allocating Observations and Actions to Nodes", "content": "We follow simple rules to allocate observations and actions to the different nodes of the graph, each of which represents a robot limb and is mapped to a row (or column) of the mask M. We distinguish between two types of nodes:\n\u2022 A root node, to which we assign global observations (e.g., robot position and orientation) and environment observations (e.g., door handle angle, etc.).\n\u2022 Non-root nodes, to which we assign local observations (e.g., joint angles) and local actions (e.g., joint commands).\nAs a rule of thumb, observations or actions that spanned multiple nodes were assigned to the closest node in the graph, or to either of the nodes involved.\nWe present an example allocation for the A1-Walk task below:\n\u2022 base - observations: orientation and angular velocity, actions: none.\n\u2022 front-left-hip \u2013 observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 front-left-thigh observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 front-left-calf observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 front-right-hip observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 front-right-thigh observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 front-right-calf observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 rear-left-hip observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 rear-left-thigh observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 rear-left-calf observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 rear-right-hip observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 rear-right-thigh observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command.\n\u2022 rear-right-calf observations: corresponding joint angle, joint velocity, and previous joint command, actions: corresponding joint command."}, {"title": "E Additional Imitation Learning Experiments", "content": "In this section we provide several ablations on the MoCapAct dataset, in addition to those presented in Section 5.1, as well as results on different tasks in the Adroit Hand benchmark [38, 39].\nMoCapAct. We compare (i) BoT-Hard, (ii) BoT-Mix, (iii) BoT-Soft, which \u2013 similarly to [25] learns the matrix B in (1) as a function of the graph's shortest path matrix, and (iv) BoT-Hard/Random with a randomly sampled mask, i.e. having ones on its diagonal and the same sparsity as the mask M used for the correct implementation of BoT-Hard. The table in Figure 9a shows the result of this comparison, with BoT-Hard outperforming all baselines on most of of the metrics. The bottlenecks introduced by the masked attention result in better performance compared both to a mixed approach (BoT-Mix) and an approach that also accounts for structure but does not prevent long-range communication (BoT-Soft). As expected, simply sampling a random mask without properly accounting for the embodiment structure deteriorates performance. While this work focused on deterministic policies, we also implemented a stochastic version of BoT-Hard, which uses a zero-mean Gaussian policy with fixed variance of 0.01. This variant shows that explicitly accounting for stochasticity can marginally improve performance further.\nIn Figure 9b, we show BoT-Hard performance as a function of the number of masked attention layers. Despite requiring 14 layers (equal to the embodiment graph diameter) for full information propagation across the graph, the plots show how our architecture may already exhibit good performance for a smaller number of layers.\nAdroit Hand. We compare our BoT-Hard architecture with baselines on three dexterous manipulation tasks, which are part of the Adroit Hand benchmark. Specifically, we restrict the experiments to a low-data regime, where transformers notably struggle and tend to overfit [6]. The results in Figure 9c show how BoT-Hard consistently outperforms the vanilla Transformer, while being competitive or outperforming the MLP baseline across all three tasks."}, {"title": "F Additional Reinforcement Learning Ablations", "content": "F.1 Effect of Body-Induced Masking in BoT\nBoT relies on masked attention with its mask determined by the embodiment structure. We conduct an additional experiment in the RL setting to further demonstrate the effect of the body-induced masking in this setting. We compare with BoT-Hard/Random and BoT-Mix/Random, where the attention mask M is given by a randomly sampled symmetric binary matrix with the same degree of sparsity (\u03b2 \u2248 0.82 for the IsaacGym humanoid). The results are presented in Figure 10. Overall, BoT with random masking (dotted lines) underperforms BoT with body-induced masking (solid lines) in both a simpler task (Humanoid-Mod) and a hard-exploration task (Humanoid-Board), which highlights that the use of body-induced masking is crucial for the performance of BoT.\nF.2 Effect of Per-Limb Tokenizer vs. Shared Tokenizer\nThe existing works using Transformer-based policies [23, 24, 25] for multi-task RL adopt shared linear projections for tokenizers and detokenizers to deal with the varying number of limbs, i.e., per-limb observation features are projected into embedding vectors by the single shared tokenizer network, and the per-limb hidden vectors are transformed to per-limb actions via the single shared detokenizer network. In contrast, our BoT is designed for tasks with a single morphology, thus we adopt per-node linear projections for tokenizer and detokenizer. We conduct an additional experiment to investigate the effect of this design choice, and the results are demonstrated in Figure 11.\nIn Figure 11, the solid lines denote the results of using per-node tokenizers/detokenizers, and the dotted lines present the results of using a shared tokenizer/detokenizer (which can be understood as representatives of the existing methods [23, 24, 25]). Overall, Transformer/BoT with per-node (de)tokenizers significantly outperform their shared (de)tokenizer counterparts in both a simpler task (Humanoid-Mod) and a hard-exploration task (Humanoid-Board). This shows that the use of tokenizers shared across different limbs for Transformer-based policies hinders efficient learning."}, {"title": "GTraining Details", "content": "The training parameters of the experiments detailed in Section 5.1 and Section 5.2 are as summarized in Tables 12a, 12b, and 12c."}, {"title": "H FLOP Derivation for Custom Masked Attention Implementation", "content": "Below, we comparatively analyze an asymptotic bound for the amount of floating-point operations required in one scaled dot product (see Equation (1)) call between the vanilla and the masked approach. From hereon, let n denote the sequence length and dk the input and output dimension of our attention mechanism.\n\u221adk\nComputing OKT. Considering Q \u2208 Rn\u00d7dk (and similarly for K), the computation of QKT will generally require dk multiplications and dk \u2013 1 additions for all of n\u00b2 pairs. Division by \\(\\sqrt{d_k}\\) results in n\u00b2 divisions and one constant factor c1 of FLOPs for computing \\(\\sqrt{d_k}\\). The total amount of flops is 2n2dk + c1.\n\u221adk\nMasked computation of OKT. Exploiting sparsity, we ignore all inner product computations for zero entries in M, computing only \u03b2n\u00b2 pairs of multiplications. This results in a reduction to 2\u03b2n2dk + c1 FLOPs.\nComputing Softmax(S). A softmax for one vector of dimension n requires n exponentiations, n additions, and n divisions, performed for n rows. Let exponentiations require C2 FLOPs per element, then a total of (2 + c2)n\u00b2 n FLOPs is performed.\nMasked computation of Softmax(S). As a result of sparsity, there is instead a total of \u03b2n2 exponentiations \u03b2n\u00b2 divisions, and $\\frac{\\beta n^2}{n}$ additions to compute, reducing our demand to (2 + c2)\u03b2n\u00b2\n$\\frac{n}{n}$ FLOPs.\nComputing the multiplication Softmax(S)V. A total of ndk pairs are multiplied, where each pair requires 2n \u2013 1 operations to complete. The total amount of FLOPs is 2n2dk \u2013 ndk. Following a similar reasoning with previous writing, a total of 2n2dk \u2013 ndk FLOPs are performed.\nAssuming that our physical agent provides a graph-induced mask $M \\in \\{0,1\\}^{n \\times n}$ of sparsity \u03b2\u2208 [1, 1] (such that there are \u03b2n\u00b2 > n nonzero entries), then the amount of FLOPs required by a vanilla masked self-attention implementation is 4n2dk + (2+c2)n\u00b2 \u2013 ndk n + c1, while that of a custom masked implementation is (2\u03b2 + 2)n\u00b2dk+ (2+c2)\u03b2n\u00b2 \u2013 ndk - n + c1. Therefore, the performance gap between the vanilla and masked implementations is determined by the sparsity coefficient \u03b2, that is, the number of FLOPs that a vanilla approach requires will be c(n) times the number of FLOPs a"}, {"title": "I Computation Analysis vs Sparsity", "content": "Figure 13 shows a comparison in terms of runtime between masked and unmasked attention for varying degrees of sparsity. The plots highlight how BoT-Hard retains its computational advantages across a wide range of sparsity degrees, covering the most common scenarios encountered in a robotics setting."}]}