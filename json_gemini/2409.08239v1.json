{"title": "Source2Synth: Synthetic Data Generation and Curation\nGrounded in Real Data Sources", "authors": ["Alisia Lupidi", "Carlos Gemmell", "Nicola Cancedda", "Jane Dwivedi-Yu", "Jason Weston", "Jakob Foerster", "Roberta Raileanu", "Maria Lomeli"], "abstract": "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Devlin et al., 2019; Chowdhery et al., 2022; Brown et al., 2020; Vaswani et al., 2017) have risen to popularity due to their remarkable ability to digest and generate human-like text (Radford et al., 2018). However, LLMs still struggle with more complex tasks such as multi-step reasoning, tool use and manipulating or processing structured data. For many of these tasks there exists source data, such as existing structured data on the web, but little data of how to use it to solve a task.In principle, one can achieve performance improvements during fine-tuning by collecting human annotated data of such tasks. However, this is an expensive and time-consuming process (Touvron et al., 2023).\nIn this paper, we propose Source2Synth, a general approach to generate synthetic data grounded in external real-world sources. Grounding the data generation process in real-world sources steers the examples to be more realistic, diverse, and factually correct. We showcase our method on two challenging tasks: multi-hop questions based on sources from the web, and tabular question answering using SQL as a tool. In both cases it achieves improved performance without relying on human annotations, resulting in a scalable data generation method for complex tasks.\nSource2Synth consists of three stages: Dataset Generation and Dataset Curation, followed by Model Finetuning, see Figure 1. At the Data Generation stage, we start by selecting a data source (such as tables on the web, or related Wikipedia articles) to ground our synthetic data generation in realistic information for a specific task. Then, to generate a given example, our method first selects a seed topic to condition the generation - for example a specific entity in a Wikipedia article or a factual statement about a table. Given the seed topic, the method then generates the full example: the instruction (e.g., question), the reasoning chain to arrive at the answer (e.g., the steps of multi-hop question answering, or tool use) and the answer itself.\nAt the Data Curation stage, the constructed synthetic dataset is split into two slices: the first slice is used to fine-tune the LLM, resulting in an intermediate fine-tuned model. We use this model to curate the second slice of data via imputation and the use of a filtering step by rejection sampling. For imputation, we blank some parts of a given example and accept the example if the model can fill in the blanks. For filtering, we reject examples that cannot produce the correct answer in k trials. This provides a higher quality curated dataset for the final fine-tuning stage on the second slice, resulting in a better performing model on a given task.\nTo demonstrate the generality of our approach, we apply it to two different domains:\n\u2022 answering tabular-based questions by learn-"}, {"title": "2 Related Work", "content": "Synthetic Data Generation using LLMs A number of works propose different strategies to generate synthetic datasets leveraging pre-trained language models. Some of these works rely on probing the knowledge contained in the LLM by first providing a prompt and letting the model either generate the continuation of a prefix or predict missing words in a close-style template (Schick and Sch\u00fctze, 2020; Schick and Sch\u00fctze, 2021; Petroni et al., 2019; Jiang et al., 2019). Other works introduce a variety of ways to improve the quality of synthetic data by using model-based or human filtering (Schick and Sch\u00fctze, 2021; Liu et al., 2022; Li et al., 2024; Thoppilan et al., 2022). Our method however does not rely on human annotations, and we improve the quality of the synthetic data by leveraging the LLM itself. Furthermore, our selection of the seed topic is automated and we use real data as a starting point. We note that some recent work also leverages real-world data for specific cases, such as a corpus from the web to construct high-quality synthetic data (Nguyen et al., 2024) or open-source code snippets to generate diverse instruction data for code generation (Wei et al., 2024; Dubey et al., 2024). In our case, we do not require a back-translation approach or an initial finetuning to generate the seed to digest the data. Our work proposes a general framework which can be applied across tasks. See Liu et al. (2024) for a thorough overview of synthetic data research and references therein.\nTeaching LLMs to Use Tools Enabling LLMs to use different tools can augment their abilities beyond text generation and towards manipulating structured data, retrieving information from exter-"}, {"title": "3 Method", "content": "Source2Synth produces high-quality synthetic examples grounded in external real-world data sources, and this resulting synthetic data is provided as step-by-step examples to the LLM for fine-tuning. Source2Synth is composed of three stages: Dataset Generation, Dataset Curation, and Model fine-tuning."}, {"title": "3.1 Dataset Generation", "content": "Data source selection The generation process begins by selecting a data source. This can be an already existing dataset re-purposed for a given task, a collection of existing data points that we would like to leverage to construct a new dataset, or structured information (e.g. graphs, tables). There is no need for human annotations on the entries, as Source2Synth will enrich it with extra instructions.\nSeed In order to create a given example of our new synthetic dataset, we first generate a seed topic as the initial trigger for the generation process, which is chosen conditioned on a randomly selected portion of the source data. The seed inspires the creation of the entry and dictates how the source data will be used. In addition, the randomness of the seed ensures variety in the generated data.\nDataset construction In order to tackle complex tasks, LLMs can leverage a step-by-step approach (Wei et al., 2022) that divides reasoning into smaller sub-tasks plus instructions on how to merge back each step into the final one. In Source2Synth, we leverage the seed to build synthetic data step-by-step, decomposing into such intermediate steps in order to arrive at an answer for a given question. This reasoning chain can then be used as supervision by providing it as the target in the synthetically generated training examples."}, {"title": "3.2 Dataset Curation", "content": "The Dataset Generation process yields an augmented dataset grounded in real data. At the Dataset Curation step, we employ a model-based approach to automatically refine the dataset to enhance its quality, while avoiding the need for human supervision. In particular, we prune the newly-built dataset of all the entries that have been incorrectly crafted or that are deemed low quality. This is achieved by slicing the dataset in two and using one slice to fine-tune the LLM (LLMSynth). During curation, LLMSynth is then used to improve the quality of the second slice of the dataset using imputation plus a filtering step. After these steps, we obtain the final curated dataset (shown in purple in Figure 1).\nData filtering During filtering, LLMSynth is used to predict the output of the given synthetic example using k tries. If the output cannot be predicted at least once, it is assumed the example is low quality and is not included in the final curated dataset.\nData Imputation We also consider an imputation process, which involves blanking parts of the augmented data points and using the LLM to fill in the blanks, to replace those fields. This is to provide cleaner data which is less unnatural."}, {"title": "3.3 Model fine-tuning", "content": "At this stage, we fine-tune on the curated synthetic dataset, initializing from a base version or instruction-tuned version of the LLM. We use our dataset for supervised training of both the reasoning chain and the final answer given an input. The resulting model LLMCurated is then ready to perform the desired task."}, {"title": "4 Applying Source2Synth to Special Cases", "content": "The general pipeline described above can be used to produce examples for the task at hand and to teach LLMs new skills. To demonstrate the impact of Source2Synth, we apply it to two challenging tasks where LLMs struggle, which are both areas of great interest for the community: multi-hop question answering and tabular question answering."}, {"title": "4.1 Multi-hop question answering", "content": "In multi-hop question answering (MHQA), we generate a dataset of multi-hop question-answer pairs, in addition to the reasoning chain that is used to"}, {"title": "4.1.1 Dataset Generation", "content": "Data source selection For multi-hop question answering, we pick English Wikipedia (Wikipedia contributors, 2004) as the data source, since it contains articles in natural language as well as additional meta-information like links to related articles. The data generation process starts by randomly selecting an initial article, denoted as D1, among all available Wikipedia articles. For each D1 we collect n \u2265 2 related articles.\nSeed An MHQA seed topic corresponds to an entity E retrieved from D1. The seed in MHQA doubles also as the \u201chop\" in the multi-hop question Q that we aim to generate, since E links the n = 2 subquestions that compose Q. For exam-"}, {"title": "4.1.2 Dataset Curation", "content": "Data filtering We check if the predicted answer matches the answer in the synthetically generated example, and if after k tries the LLM has not sup-"}, {"title": "4.2 Tabular question answering", "content": "In Tabular question answering (TQA) we generate a question-answer dataset where each question is based on a given (real) table from the data source. Generated training examples are hence enriched with tables and annotations which are built from automatically-generated interesting facts retrieved the table.\nData source selection In the TQA case, we use unlabeled tables in the train split of the WikiSQL"}, {"title": "4.2.1 Dataset Construction", "content": "We next generate an SQL-statement by zero-shot prompting the LLM: we provide the table and the seed (factual statement) as context, see Figure 12 for the exact prompt. Given the produced SQL statement, it is then executed using the Python library sqlite3 to obtain an SQL answer formatted as a table. If the generated statement is invalid, we discard it and re-generate."}, {"title": "4.2.2 Dataset Curation", "content": "Data filtering We check if the predicted answer of LLMSynth fine-tuned on slice 0 matches the answer in the synthetically generated example, and if after k tries the model has not supplied the correct answer we filter out the entry entirely. See Figure 5 for an example of model inference."}, {"title": "5 Experimental Setup", "content": "We test our method on two domains: tabular question answering and multi-hop question answering. For each, we use Source2Synth to generate and curate a high quality dataset suitable for fine-tuning, and compare our method to a number of baselines."}, {"title": "5.1 Multi-Hop QA Setup", "content": "Data To evaluate multi-hop question answering abilities, we evaluate our Source2Synth method on HotPotQA (Yang et al., 2018). HotPotQA is a benchmark based on Wikipedia containing 113,000 examples of multi-hop question-answer pairs and is split in train, test, and validation sets. Each entry in HotPotQA is constructed such that: 1) each question requires finding and reasoning over multiple supporting documents in order to answer; 2) each entry provides sentence-level supporting facts for strong supervision and explainability of the prediction; 3) each question can be classified as either a comparison or bridge question.\nA comparison question entails comparing the same concept between n objects (e.g. \"Who is the tallest student in class?\"), while a bridge question builds on a logical and/or causal link and requires deriving one statement to get to the answer (e.g. \"What is the height of the student that topped the entry exam?\" - this requires first identifying the student that topped the exam. The hop length is the number of comparison objects for comparison questions or the number of links for bridge questions. In our case, we chose n = 2 to be consistent with HotPotQA. The test set consists of 7,405 entries, split evenly between bridge and comparison questions.\nWe only generate synthetic data for bridge questions, since they pose a bigger challenge to current LLMs. In order to counterbalance this disparity, we include 500 comparison questions from HotPotQA's training dataset in our fine-tuning dataset.\nMetrics We measure the performance using soft exact match (soft-EM) as the metric. Soft-EM is 1 if the generated output contains the golden answer and 0 otherwise.\nModel The base model used in the experiments in MHQA is Llama-2 70B-Chat. We hence fine-tune Source2Synth and various other baseline methods initializing from this model. Source2Synth is trained with 1250 synthetic examples, unless noted"}, {"title": "5.2 Tabular QA Setup", "content": "Data We conduct evaluations with the WikiSQL (Zhong et al., 2017) dataset validation split. The WikiSQL dataset consists of a corpus of 80,654 hand-annotated examples of natural language questions, SQL queries, and SQL tables created from 24,241 tables extracted from Wikipedia. The validation split contains 7,857 examples after removing non-executable SQL tables, see Appendix B for more details.\nMetrics We measure performance using the exact match (EM) and the soft-EM metrics. The EM metric equals 1 if the golden answer is equal to the generated answer and 0 otherwise.\nModel For TQA, we use the Starchat-beta language model (Li et al., 2023b) from Huggingface as the initial language model (batch size 32, 100 steps, Ir 0.0001, linear warm-up). The Starchat model is an instruction-tuned LLM with 16 billion parameters trained to act as a helpful coding assistant. This model is a fine-tuned version of StarCoder (Li et al., 2023b), a LLM which was pre-trained and then fine-tuned on a large code corpus, which contains SQL statements, and successively fine-tuned on 35B Python tokens. For our Source2Synth generated data, the initial number of synthetic examples per slice is 8k (so 16k in total). After curation, we keep 2160 of the examples in slice 2 (27%)."}, {"title": "6 Results", "content": "6.1 Multi-Hop question answering\nOverall performance of Source2Synth on MHQA We report the experimental results in Table 1. We include the baselines of the vanilla instruction-tuned LLM, a fine-tuned LLM using only the HPQA 500 examples from the train split (second row), and LLMSynth which only uses the uncurated synthetic data for fine-tuning (third row). All fine-tuned methods outperform the instruction-tuned model (first row). Using only synthetic data or only HotPotQA data for fine-tuning demonstrates worse performance than when combined, whether the synthetic data is curated (fifth row) or not as in LLMSynth (fourth row). Once we use the full Source2Synth pipeline to obtain the curated synthetic dataset for fine-tuning we see further performance improvements LLMCurated (fifth row) over not curating the data (fourth row).\nAnalysis of performance on different question types and levels of difficulty We study the"}, {"title": "Scaling performance", "content": "We also report scaling performance in Figure 6. We study how performance evolves when adding more synthetic data in the fine-tuning data mix - that already includes 500 samples from the HPQA train split. We perform the analysis on LLMSynth and LLMCurated to show the impact of the curation technique. In both cases"}, {"title": "6.2 Tabular question answering", "content": "We report the experimental results for Tabular question answering in Table 3. Firstly, they indicate that providing no context about the table when prompting the instruction-tuned StarChat language model has very poor performance (first row), with an EM metric of 0.25%. This is expected since the WikiSQL benchmark questions require information contained in the table, and the model does not have any other information to answer the question except for the general knowledge stored in its parameters. However, even if we pass the table as part of the prompt, the performance does not improve much. For example, passing in a zero-shot fashion (second row) only has an EM metric of 1.83%. This may be challenging for the model as the information in the table is presented in a form that is not easy for the LLM to naturally handle (i.e. a table rather than natural language). While passing an example of table usage in a one-shot fashion (third row) improves the soft-EM metric, the EM metric is still very low (2.03%). Hence, this is still very challenging for the model. Thirdly, the performance increases once we provide a one-shot example containing the relevant table and SQL query (fourth row), with an EM of 12.3%. The ability to use the SQL tool improves the performance markedly.\nWe obtain a significant increase in performance when we fine-tune the StarChat model using the Source2Synth curated data (last row), with an EM of 34.5%. Our full method performs significantly better than fine-tuning the StarChat language model using synthetic data without curation, LLMSynth (second to last row) which has an EM of 23.86%, although that still outperforms the other baselines by a large margin as well, indicating the utility of our Source2Synth synthetic data generation scheme."}, {"title": "7 Conclusion", "content": "In this paper, we introduce Source2Synth, a new method for generating and curating high-quality synthetic data grounded in real data sources. We demonstrate its utility on two tasks that pose significant challenges for LLMs: multi-hop reasoning and tabular question answering with SQL. We believe our work could also be beneficial in other low-data regimes, and future work could explore our approach on other tasks and in diverse fields, for example, in to biology, chemistry and medicine."}, {"title": "8 Limitations", "content": "In this paper, our applications use a single seed to derive two-hop questions in the case of MHQA or SQL on a single table in TQA. However, Source2Synth can be extended to more complex questions e.g. with more hops, and even more complex tool-use, e.g. the use of multiple tables. This could be done by looping the dataset generation steps and feeding the result of the previous step as input to the next one. Our approach provides a way to sample the related articles graph and corresponding entities within the articles based on simple rejection sampling but we believe that our method could be improved with more clever sampling techniques. We consider this to be an interesting avenue of future research."}]}