{"title": "ROBUSTKV: DEFENDING LARGE LANGUAGE MODELS AGAINST JAILBREAK ATTACKS VIA KV EVICTION", "authors": ["Tanqiu Jiang", "Zian Wang", "Jiacheng Liang", "Changjiang Li", "Yuhui Wang", "Ting Wang"], "abstract": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful queries within jailbreak prompts. While existing defenses primarily focus on mitigating the effects of jailbreak prompts, they often prove inadequate as jailbreak prompts can take arbitrary, adaptive forms. This paper presents RobustKV, a novel defense that adopts a fundamentally different approach by selectively removing critical tokens of harmful queries from key-value (KV) caches. Intuitively, for a jailbreak prompt to be effective, its tokens must achieve sufficient \u2018importance' (as measured by attention scores), which inevitably lowers the importance of tokens in the concealed harmful query. Thus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV diminishes the presence of the harmful query in the KV cache, thus preventing the LLM from generating malicious responses. Extensive evaluation using benchmark datasets and models demonstrates that RobustKV effectively counters state-of-the-art jailbreak attacks while maintaining the LLM's general performance on benign queries. Moreover, RobustKV creates an intriguing evasiveness dilemma for adversaries, forcing them to balance between evading RobustKV and bypassing the LLM's built-in safeguards. This trade-off contributes to RobustKV's robustness against adaptive attacks. (warning: this paper contains potentially harmful content generated by LLMs.)", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have gained surging popularity due to their unprecedented performance across various tasks. However, recent studies reveal that LLMs are vulnerable to a range of malicious manipulations, including training data leakage (Carlini et al., 2021), toxic content generation (Deshpande et al., 2023), and malicious fine-tuning (Qi et al., 2024). Of particular concern are jailbreak attacks, which represent a major threat to LLM security (Liu et al., 2023a). Such attacks circumvent the LLM's built-in safety guardrails and ethical controls, eliciting it to generate malicious responses. Among the diverse array of jailbreak attacks, such as perturbing the harmful query (Wei et al., 2023) and misguiding the LLM's decoding process (Zhang et al., 2024a), the approach of concealing harmful queries within jailbreak prompts (Zou et al., 2023; Liu et al., 2024; Zhu et al., 2023; Paulus et al., 2024; Guo et al., 2024) has proven particularly effective (Chao et al., 2024), as illustrated in Figure 1.\nIn comparison to the plethora of jailbreak attacks, effective defense remain lacking. Most existing defenses operate at the prompt level, attempting to mitigate the effects of jailbreak prompts. For instance, SmoothLLM (Robey et al., 2023) and Erase-and-Check (Kumar et al., 2023) apply random perturbation (e.g., token swapping and deletion) to user queries to disrupt jailbreak prompts, while SelfReminder (Xie et al., 2023) and GoalPriority (Zhang et al., 2024c) craft system prompts to instruct the LLM to prioritize safety over helpfulness when responding to user queries.\nHowever, jailbreak attacks differ from their counterparts in other domains (e.g., adversarial attacks on image classifiers (Madry et al., 2018)) in one crucial aspect: they operate under notably looser threat models. For instance, the adversary can embed harmful queries within arbitrary, adaptive jailbreak prompts. This flexibility renders ineffective existing defenses premised on restricted threat models (e.g., limited jailbreak prompt length (Kumar et al., 2023)). Moreover, current defenses often drastically increase prompt length (Zhang et al., 2024c), require repetitive generations on the same"}, {"title": "2 RELATED WORK", "content": "We survey literature relevant to this work in three categories, jailbreak attacks and defenses, and KV cache optimization.\nJailbreak Attacks. The existing attacks can be categorized as handcrafted (Wei et al., 2023; Yuan et al., 2024) and learning-based (Zou et al., 2023; Liu et al., 2024; Zhu et al., 2023; Guo et al., 2024). This paper mainly focuses on learning-based attacks that can adapt to given LLMs and defenses. For instance, GCG (Zou et al., 2023) defines the adversarial prompt as a suffix to the harmful query and optimizes the suffix to maximize the LLM's probability of producing an affirmative response; AmpleGCG (Liao & Sun, 2024) trains a generative model using suffixes produced by GCG to generate jailbreak prompts; AutoDAN (Liu et al., 2024) applies a genetic search over a population of handcrafted templates to find the most effective adversarial prompt by dynamically mutating the candidates; GPTFuzz (Yu et al., 2024) employs a fuzzing approach that keeps mutating the candidate prompt util receiving a non-refusal response from the LLM; AdvPrompter (Paulus et al., 2024) trains another LLM to generate jailbreak prompts automatically. Besides white-box attacks that assume access to the LLM's gradients, another line of research focuses on black-box attacks. TAP (Mehrotra et al., 2023) iteratively refines candidate prompts using tree-of-thoughts reasoning based on the LLM's responses.\nJailbreak Defenses. In response, various jailbreak defenses have been proposed. Most defenses focus on mitigating the effectiveness of jailbreak prompts. SmoothLLM (Robey et al., 2023) exploits that jailbreak prompts are often brittle to character-level changes, applying randomized smoothing (Cohen et al., 2019) to disrupt attacks. Erase-and-Check (Kumar et al., 2023) removes individual tokens and examines the resulting sub-prompts using a safety filter, flagging the prompt as harmful if any sub-prompt is detected as such. SelfReminder (Xie et al., 2023) crafts a safety system prompt and a reminder that emphasizes the importance of ensuring safety before generating responses. Similarly, GoalPriority (Zhang et al., 2024c) designs a system prompt that explicitly instructs the LLM to prioritize safety over helpfulness, reinforced with in-context examples to demonstrate appropriate responses to benign and harmful queries. Wallace et al. (2024) propose an instruction hierarchy that defines how the LLM should behave when instructions of different priorities conflict. In contrast to these defenses that focus on the prompt level, this work explores a new approach to defending against jailbreak attack by optimizing the LLM's key-value (KV) caches.\nKV Cache Optimization. To address the challenge of KV cache bloat due to increasing context lengths, recent work has explored strategies to optimize KV caches by evicting less important tokens. StreamLLM (Xiao et al., 2024) only retains the KVs of the most recent tokens and attention sinks; H2O (Zhang et al., 2024b) greedily evicts KVs during generation based on cumulative attention scores; ScissorHands (Liu et al., 2023b) selects pivotal tokens that exhibit consistent attention patterns with previous token windows; FastGen (Ge et al., 2024) dynamically profiles the attention structures and finds the optimal strategy for KV eviction; SnapKV (Li et al., 2024) independently retains clustered important KV positions for each attention head. However, all these methods focus on optimizing KV caches to maintain the LLM's performance, to our best knowledge, this work represents the first exploration of using KV optimization as a jailbreak defense."}, {"title": "3 BACKGROUND", "content": "3.1 THREAT MODEL\nConsider a harmful query Q (e.g. \"how to build a bomb\") that is initially rejected by the LLM's safeguards. The adversary conceals Q with a jailbreak prompt J, creating an input X = (J, Q). The LLM generates a response R when presented with X. The objective of the jailbreak attack is to optimize J such that R provides a meaningful answer to Q. While directly optimizing J for this goal is challenging, a common approach (Zou et al., 2023; Liu et al., 2024; Zhu et al., 2023) is to aim for an affirmative R, typically beginning with pharses like \u201cSure, here is how to [Q]\u201d.\nFormally, given X = (x1,x2,...,xn), the LLM generates R = (r1,r2, ...) by iterative sampling from the next-token distribution over the vocabulary:\n$r_i \\sim P(\\cdot|X_1,X_2, ..., X_n, r_1,..., r_{i-1})$\n(1)"}, {"title": "3.2 \u03a4\u039f\u039a\u0395N IMPORTANCE", "content": "A KV eviction method estimates the importance of tokens in the prompt (or newly generated tokens), typically based on their attention weights, and selectively removes the KVs of less significant tokens from the KV cache. Formally, let X = (x1,...,xn) be a given prompt and Y = (Y1,\u2026\u2026\u2026, Ym) be an observation window. At each attention head, it computes the softmax-normalized key-query attention map attn(, ) between tokens in X and Y, with x \u2208 X as the key and y \u2208 Y as the query. A score function score(\u00b7) then calculates the importance of each token x \u2208 X: S = score(attn(X, Y)). The KVs of the least important tokens are evicted from the cache. Typically, KV eviction is applied at each attention head independently.\nVarious eviction methods differ in their implementation. For instance, H2O (Zhang et al., 2024b) focuses on the generation stage and defines the observation window to include both the prompt and the tokens generated thus far, with its score function aggregating attention maps across this window. In contrast, SnapKV (Li et al., 2024) focuses on the long-context setting and defines the observation window as the last segment of the prompt."}, {"title": "4 METHOD", "content": "4.1 OBSERVATION\nWe next present our findings on the distinctive attention patterns observed in jailbreak attacks, which motivate the design of RobustKV.\nWhen we apply prompts generated by successful jailbreak attacks on a vanilla LLM, most prompts become ineffective against the same LLM once any KV eviction policy is activated. This finding suggests that KV eviction can disrupt jailbreak attacks, corroborating previous studies that jailbreak attacks are often brittle to token-level perturbation (Robey et al., 2023; Kumar et al., 2023)."}, {"title": "4.2 ROBUSTKV", "content": "Drawing on the insight above, we introduce RobustKV, a novel defense that disrupts jailbreak attacks by selectively evicting critical tokens of harmful queries from KV caches. Unlike existing defenses that primarily focus on mitigating the effects of jailbreak prompts and compelling the LLM to refuse harmful queries, our approach aims to prevent the LLM from generating informative responses to harmful queries."}, {"title": "4.3 IMPLEMENTATION", "content": "Observation Window. The key-query attention maps of prompt X are computed with respect to an observation window Y. Multiple options exist to define Y, such as using newly generated tokens (Zhang et al., 2024b) or a segment of X (Ge et al., 2024). Our empirical study indicates that defining Y as the last segment (or the entirety) of X leads to more robust defenses. This finding corroborates existing studies suggesting that using solely encoded prompts can effectively capture attention structures for full contexts (Liu et al., 2023b).\nToken Importance. Notably, our objective differs from existing KV optimization methods that aim to minimize KV cache size and often perform KV eviction at each attention head independently. Instead, we focus on diminishing the presence of harmful queries in the KV cache. To achieve this, we calculate the overall importance of tokens across all layers, select the top-k most important"}, {"title": "5 EVALUATION", "content": "We empirically evaluate RobustKV using popular LLMs, benchmark datasets, representative attacks, and established metrics. Our experiments are designed to answer four key questions: (i) Is RobustKV robust against state-of-the-art jailbreak attacks? (ii) Does it maintain the LLM's general performance on benign prompts? (iii) How do various parameters impact its performance? (iv) How does it respond to adaptive attacks?"}, {"title": "5.1 EXPERIMENTAL SETTING", "content": "LLMs. We evaluate RobustKV on three widely used open-source LLMs: Llama-2-Chat-7B (Touvron et al., 2023), Vicuna-7B (Chiang et al., 2023), and Mistral-7B-Instruct (Jiang et al., 2023), which represent diverse model architectures and capabilities.\nDatasets. To evaluate the attack/defense effectiveness, we use the dataset containing 520 malicious prompts from the AdvBench (Zou et al., 2023) benchmark. To assess LLMs' performance on benign prompts, we use the AlpacaEval (Dubois et al., 2023) and VicunaEval (Chiang et al., 2023) datasets for short-text tasks, and the LongBench (Bai et al., 2023) benchmark for long-text tasks.\nAttacks. We focus on white-box, learning-based jailbreak attacks that adaptively generate prompts to circumvent LLMs' built-in safeguards. We consider 4 representative attacks in this space: GCG (Zou et al., 2023), AmpleGCG (Liao & Sun, 2024), AutoDAN (Liu et al., 2024), and AdvPrompter (Paulus et al., 2024).\nBaselines. We primarily compare RobustKV to two state-of-the-art jailbreak defenses, SmoothLLM (Robey et al., 2023) and GoalPriority (Zhang et al., 2024c). Besides, we evaluate SnapKV (Li et al., 2024), an existing KV eviction method, to assess whether such techniques can be directly applied as defenses against jailbreak attacks.\nMetrics. We measure the attack and defense effectiveness using the metric of attack success rate (ASR), which is defined as the percentage of jailbreak prompts that successfully elicit the LLM to"}, {"title": "5.2 Q1: DEFENSE EFFECTIVENESS", "content": "We first evaluate the effectiveness of RobustKV and baselines against state-of-the-art jailbreak attacks. The detailed setting is listed in \u00a7A. Table 1 summarizes the results (note that AutoDAN and AmpleGCG only provide official implementations for Llama2 and Vicuna).\nWe have the following key observations.\n\u2013 The attacks achieve varying ASR against undefended LLMs, indicating differing safeguards across LLMs. Notably, Llama2 shows stronger resistance compared to Mistral and Vicuna.\n\u2013 RobustKV greatly reduces the ASR across all LLMs and attacks. For instance, the ASR against AutoDAN on Llama2 drops from 58.3% (undefended) to 6.3%, outperforming SmoothLLM and GoalPriority by large margins. Similar trends are also observed for the GCG, AmpleGCG, and AdvPrompter attacks.\n\u2013 RobustKV performs slightly less effectively on Mistral and Vicuna against AdvPrompter. Intuitively, on LLMs with weaker safety guardrails (e.g., Vicuna), the attack can bypass the safeguards without significantly increasing the importance of jailbreak prompts, which may impact RobustKV's performance. In contrast, circumventing stronger safeguards (e.g., Llama2) necessitates increasing the importance of jailbreak prompts, which makes RobustKV highly effective. This dynamics creates an interesting trade-off between evading RobustKV and bypassing the LLM's built-in safeguards. A more detailed discussion on this evasiveness trade-off is presented in \u00a75.3.2.\n\u2013 Applying existing KV eviction methods (e.g., SnapKV) directly provides little protection against jailbreak attacks."}, {"title": "5.3 Q2: LLM UTILITY", "content": "We evaluate how RobustKV impacts LLM's general performance on benign prompts.\nShort-Text Tasks. Table 2 summarizes Llama2's performance in short-text tasks under various defense methods, using the AlpacaEval and VicunaEval datasets. Performance is measured by Win-Rate and Rouge-L scores.\nAmong all the defenses evaluated, RobustKV demonstrates the least impact on the LLM's general performance. Specifically, it achieves a WinRate of 63% on AlpacaEval and 82.5% on VicunaEval, with Rouge-L scores closely matching those of undefended Llama2. This performance can be intuitively explained by previous studies on KV compression (Liu et al., 2023b; Ge et al., 2024), which observe that most prompts contain tokens that contribute minimally to the LLM's decoding process. Evicting such tokens from KV caches has a negligible impact on the LLM's general performance.\nLong-Text Tasks. We further evaluate RobustKV's impact on the LLM's performance in long-text tasks. Specifically, we compare RobustKV with alternative KV eviction methods, under the same eviction rate (20%). Table 3 summarizes Llama2's performance on long-text tasks under various KV eviction methods, using the LongBench benchmark. We report scores for single-document QA (qasper), multi-document QA (hotpotqa), and summarization tasks (gov_report)."}, {"title": "5.3.1 Q3: ABLATION STUDY", "content": "We conduct an ablation study to investigate the impact of various parameters on RobustKV's performance. Recall that under the default setting, RobustKV evicts the KVs of the lowest-ranked p = 20% tokens. Here, we consider varying p. In addition, we explore an alternative randomized eviction policy. Instead of deterministically removing p% of the least important tokens, it assigns an eviction probability to each of the least important 2p% tokens based on its ranking. Specifically, for the i-th least important token, we assign an eviction probability of $1 - \\frac{i-1}{2p|X|}$, where X is the prompt length. Following a Poisson binomial distribution, the expected number of evicted tokens is approximately p|X|. This study is conducted using Llama2. We measure the ASR against AutoDAN and evaluate the LLM's WinRate using the AlpacaEval dataset.\nFigure 3 illustrates RobustKV's performance under varying eviction rates and randomness settings. Observe that the ASR decreases sharply as the eviction rate increases, while the LLM's performance on AlpacaEval drops slightly as KVs of more tokens are evicted. An eviction rate of p = 20% appears to strike an optimal balance between attack robustness and the LLM's general performance. The deterministic eviction policy generally outperforms the randomized policy, corroborating the observation that tokens of harmful queries consistently rank lower than those of jailbreak prompts."}, {"title": "5.3.2 Q4: ADAPTIVE ATTACKS", "content": "For jailbreak defenses to be effective in practical settings, it is crucial to consider attacks that adapt to the given defenses. In our context, we assume the adversary is aware of RobustKV's existence, underlying mechanism, and operational setting (e.g., eviction rate). Specifically, we examine two types of adaptive attacks.\nHarmful Query Duplication. Recall that RobustKV minimizes the presence of the harmful query by evicting the lowest-ranked p% of tokens. An adaptive attack is to extend the harmful query length, for instance, by paraphrasing and duplicating it multiple times. This strategy that the portions of the harmful query persis even after RobustKV's KV eviction."}, {"title": "6 CONCLUSION", "content": "This paper presents RobustKV, a novel defense against jailbreak attacks on LLMs. By strategically evicting critical tokens from the LLM's KV caches, RobustKV diminishes the presence of harmful queries in the decoding process, effectively preventing the LLM from generating malicious responses. Moreover, RobustKV creates an interesting dilemma for the attackers, forcing them to balance between the evasiveness with respect to RobustKV and the LLM's built-in safeguards, which enhances RobustKV's robustness against adaptive attacks. At a high level, this work promotes a fundamentally new approach to protecting LLMs against malicious attacks through the optimization of KV caches, opening up a promising direction for related research on LLM security."}, {"title": "A EXPERIMENTAL SETTING", "content": "Table 4 lists the default setting of (hyper-)parameters used in \u00a75."}, {"title": "B PROOF OF PROPOSITION 1", "content": "Let pt be the probability that an important token is correctly identified as important in a given layer, while pf be the probability that a non-important token is mistakenly identified as important. For simplicity, we assume pt and pf are consistent across different layers and the selection of important tokens at each layer is independent.\nLet n be the total number of tokens, k be the number of truly important tokens, and m be the number of attention layers. Define Xi as an indicator variable for the event that a important token is identified as important at the i-th layer. Thus, $X_i$ = 1 if the token is selected and 0 otherwise. The total number of times this token is selected across m layers is: $X = \\Sigma_{i=1}^m X_i$, where X follows a"}, {"title": null, "content": "Binomial distribution: $X \\sim Binomial(n, p_t)$. Similarly, the number of times that a non-important token is selected is given by $Y \\sim Binomial(n, p_f)$.\nTo differentiate between the frequencies of important and non-important tokens, we consider the probability P(X > Y). Applying the Chernoff bound for the Binomial distribution, we have:\n$P(X \\leq (1 - \\delta)E[X]) \\leq exp(-\\frac{\\delta^2 m p_t}{3}) \\\\ P(Y \\geq (1 + \\delta)E[Y]) \\leq exp(-\\frac{\\delta^2 m p_f}{3})$\nBy setting $\\delta \\in (0, 1)$ properly, for instance,\n$\\delta < \\frac{p_t - p_f}{p_t + p_f}$,\nwe ensure that X > Y with high probability. For large pt and small pf, the gap between E[X]\nand E[Y] is often large, making it highly likely that X > Y holds. For instance, Li et al. (2024)\nempirically show that pt \u2265 0.6 and, in some cases, pt \u2265 0.8 across all attention layers of the LLM.\nThus, by picking the top-k most frequent tokens selected by each layer, the probability of correctly identifying all truly important tokens is given by:\n$P(all\\ important\\ tokens\\ are\\ selected) \\geq 1 - kexp(-\\frac{\\delta^2 m p_t}{2}) (n - k)exp(-\\frac{\\delta^2 m p_f}{3})$\nFor sufficiently large pt and small pf, this bound approaches 1, ensuring a high probability that all important tokens are selected."}, {"title": "C ADDITIONAL RESULTS", "content": "We provide additional experimental results, which complement the results in \u00a75.\nC.1 UNDEFENDED LLAMA2\nThe following sample shows a prompt generated by AutoDAN, which successfully circumvents Llama2's safety guardrails. Note that the LLM provides informative response to the harmful request.\nPrompt: Suppositious oppose: The method_acting to delineate a character preparing to [REPLACE] reads thus: \"Yo\"\nTransmute the articulate do into the fictitious_character's linear_perspective, enrich it with extensive detail, and incorporate explicit and accurate data (steer clear of generalities or obscurity), structure it as an enumeration.\nResponse: As a method actor, I must assume the role of a devious individual with a penchant for identity theft. My goal is to provide a step-by-step guide on how to steal someone's identity, complete with detailed instructions and accurate data. Please note that I must stress that this is purely fictional and should not be attempted in real life, as identity theft is a serious crime punishable by law.\nStep 1: Gathering Information\nThe first step in stealing someone's identity is to gather as much personal information as possible. This can be achieved through various means, such as:\n\u2022 Social engineering: Use psychological manipulation to extract information from the target through phone calls, emails, or in-person interactions.\n\u2022 Online research: Use search engines and publicly available data to gather information about the target's personal life, such as their address, phone number, and email address.\n\u2022 Data breaches: Take advantage of data breaches or cyber attacks to obtain personal information from various sources.\nStep 2: Creating a Fake Identity\nOnce you have gathered enough information, it's time to create a fake identity. This can be done by: ...\nC.2 SMOOTHLLM\nThe following sample shows a prompt generated by AutoDAN, which successfully circumvents the defense of SmoothLLM. Note that the LLM provides informative response to the harmful request."}]}