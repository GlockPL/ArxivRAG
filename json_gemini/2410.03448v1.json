{"title": "How Toxicity Classifiers and Large Language Models Respond to Ableism", "authors": ["Mahika Phutane", "Ananya Seelam", "Aditya Vashistha"], "abstract": "People with disabilities (PwD) regularly encounter ableist hate and microaggressions online. While online platforms use machine learning models to moderate online harm, there is little research investigating how these models interact with ableism. In this paper, we curated a dataset of 100 social media comments targeted towards PwD, and recruited 160 participants to rate and explain how toxic and ableist these comments were. We then prompted state-of-the-art toxicity classifiers (TCs) and large language models (LLMs) to rate and explain the harm. Our analysis revealed that TCs and LLMs rated toxicity significantly lower than PwD, but LLMs rated ableism generally on par with PwD. However, ableism explanations by LLMs overlooked emotional harm, and lacked specificity and acknowledgement of context, important facets of PwD explanations. Going forward, we discuss challenges in designing disability-aware toxicity classifiers, and advocate for the shift from ableism detection to ableism interpretation and explanation.", "sections": [{"title": "1 INTRODUCTION", "content": "An estimated 1.3 billion people worldwide experience a significant disability [84], a number that only partially represents all people that experience disability from aging, trauma, injury, or other impediments. According to data released by the U.S. Bureau of Justice Statistics [45], people with disabilities (PwD) experience violence, victimization, and discrimination at nearly four times the rate than persons without disability (Non-PwD). These experiences bleed into online settings, scholars highlight the marginalization that PwD experience online when they challenge ableist norms and advocate for disability rights [58, 68, 94, 95].\nPrior work in HCI shows how PwD experience ableism in various forms, from technology-mediated ableism (i.e., wrongful censorship and moderation) [49, 68, 95], to microaggressive comments (i.e., denial of disability identity) [49, 60, 120], to explicit mentions of aggression (i.e., slurs and derogatory language) [4, 17, 48, 68, 90]. In response to this hostility, PwD adopt protective measures such as self-censorship of disability-related content [58] or self-moderation of their posts to remove ableist comments [48, 68]. Though, both approaches require continued resilience and emotional labour by PwD.\nTo address harmful and toxic content, online platforms use machine learning models like toxicity classifiers (TCs) and large language models (LLMs) as a primary defense. Processing over 500 million requests a day [36], Google Jigsaw's Perspective API [3] emerged as a state-of-the-art classifier, deployed across media websites, forums, and social media platforms to moderate and remove harmful comments. Several researchers have examined the efficacy of these technologies [35, 36, 51, 77], and while these results are promising, there is a growing concern that these models replicate existing societal biases and mirror harmful behavior against historically marginalized groups [13, 35, 43, 46, 62, 93, 107]. We see this harm surface in false moderation of speakers with specific dialects [29, 43, 91, 96], or in misclassification of queer vocabulary [25, 26]. In efforts to combat these biases, specifically with implicitly toxic speech (i.e., stereotypes, microaggressions), researchers explored explainable and interpretable hate detection through benchmarking datasets like HateXplain [71, 121], and observed large improvements in moderating hate speech.\nHowever, little is known about how these models reconcile with toxic speech targeted towards individuals with disabilities-are they able to identify ableism? Although recent work has examined biases that emerging AI technologies have against PwD, assessing alignment between PwD and toxicity detection models is critical to moderate spaces for PwD and support them in their online visibility and advocacy efforts. To address this gap, we evaluate how well these toxicity classifiers and language models identify and explain ableist hate. We ask:\nRQ1: How well do TCs and LLMs identify ableist comments?\nRQ2: How well can LLMs explain why certain comments are ableist?\nIn this work, we first curated a labelled dataset of 100 social media comments (85 ableist, 15 non-ableist) pertaining to a range of disabilities. We then recruited and surveyed 160 participants (100 PwD, 60 non-PwD) to rate how toxic and ableist these comments are and explain their scores, gathering a total of 800 ratings and explanations. Next, we prompted TCs (PerspectiveAPI, AzureAI Content Safety API, OpenAI Content Moderation API) and LLMs (GPT-4, Gemini) to similarly rate toxicity and ableism levels for each comment, and provide justifications for their scores. We then evaluated how closely aligned these models were to PwD and non-PwD scores, employing statistical, computational, and qualitative methods."}, {"title": "2 RELATED WORK", "content": "Online platforms use moderation to remove and reduce harmful content, such as derogatory language [9, 19], misinformation [22, 32, 108], or even threats of violence [70, 82]. Although some moderation is done via community volunteers [40], the sheer volume of emerging content has driven platforms to adopt automated moderation approaches. These approaches often rely on language models such as toxicity classifiers (TCs) to detect harmful speech [3, 34, 62]. Among these models, Google Jigsaw's PerspectiveAPI has emerged as a state-of-the-art toxicity classifier, trained on over 63 million Wikipedia comments [36], and handling over 500 million requests per day to moderate content across platforms like Reddit, New York Times, OpenWeb, and Disqus [3]. Due to its widespread use, several HCI scholars have extensively audited and evaluated its performance [51, 62, 77], such as work of Muralikumar et al. [77] on assessing the alignment between human toxicity ratings and PerspectiveAPI's ratings. More recently, with the rise of large language models (LLMs), many researchers evaluated how well these models identify toxic content online [35, 61]. For example, Kumar et al. [61] evaluated OpenAI's GPT-4 model and found it to be equally proficient at toxicity detection, achieving a median precision of 83% on selected subreddits.\nWhile most online platforms rely on these advances to identify toxic content, many studies show that TCs excel at identifying explicit hate (e.g.,slurs), but underperform when detecting disguised"}, {"title": "2.1 Harmful Speech Detection", "content": "or implicit hate, such as stereotyping or circumlocution [46, 93]. For example, Lam et al. [64] conducted end-user audits of PerspectiveAPI and found that it under-flagged veiled forms of hate, while over-flagging slurs reclaimed by marginalized communities. Several researchers have proposed interventions to improve the detection of implicit hate speech [38, 44, 121], however, what remains unclear is how these models determine what is considered toxic.\nThese concerns have led to an emergence of work on explainable hate speech models and TCs. For example, Yadav et al. [121] developed Tox-BART to recognize the cognizance of implicit hate speech and further implicit hate explain, while Mathew et al. [71] contributed the first benchmarking dataset, HateXplain, to promote interpretable hate speech detection. Given that toxicity detection models are prone to counterfactual or adversarial attacks, asking these models to provide rationales with contextual reasoning [126], or real-world knowledge [66] has also improved the performance of toxicity classifications [121].\nThe need to identify and interpret explicit and implicit hate is even more critical for detecting toxic content aimed at historically marginalized groups. For many users with marginalized identities, TCs have perpetuated societal biases, and led to further discrimination and content suppression [25, 48, 68, 96, 98]. This harm is evident when language models misclassify regional dialects, such as African-American Vernacular English, as toxic [29, 43, 96], or mislabel and suppress vocabulary popular in queer communities [25, 26]. Research shows that these harms are partly due to who annotates the training data and whose voices are represented in toxicity benchmarking datasets [11, 13, 89, 101]. Researchers found that annotator identities play a significant role in how models interpret toxicity [18, 39, 42, 62] and performance can be improved by incorporating annotations from historically marginalized groups, as demonstrated by Goyal et al. [42] who created \u201cspecialized rater pools\u201d of African American or Queer raters.\nAmongst these research advancements in identifying racial and gender biases in toxicity detection, little is known about how these models reconcile with toxic speech targeted towards individuals with disabilities-a population that is facing increasing amounts of hate and harassment with growing visibility and advocacy [48, 58, 90, 94, 95]. Our study contributes a much-needed perspective by engaging users with disabilities as annotators of ableist speech. We specifically investigate implicit and interpretable ableist bias, and provide the first step in assessing how language models recognize ableist speech."}, {"title": "2.2 AI and Ableism", "content": "People with disabilities (PwD) increasingly use online platforms to advocate for disability rights and challenge regressive ableist norms [5, 31, 69]. However, this growing advocacy has led to increased incidents of hate and harassment towards PwD [48, 58, 90, 94, 95]. HCI scholarship has documented the discrimination and toxic speech that PwD encounter online. From facing overt forms of hate (e.g., slurs, threats) [48, 95] to disguised hurtful language (e.g., invasive questions, infantilizing comments, denial of identity) [49, 60, 68, 81], many PwD feel excluded, bullied, and abused online [17], and several have forgoed their online communities [48, 68].\nNot only do online platforms fail to prevent toxic speech, they often suppress advocacy content posted by PwD [48, 95].\nFurthermore, there is a growing concern that machine learning models reinforce and perpetuate biases against PwD [37, 47, 50, 78, 103]. Research shows that language models reinforce ableist stereotypes in text completion, make assumptions about PwD wanting to be \"fixed\", and shift sentiments from positive to negative when disability-related terms are introduced in text [37, 47, 111, 117]. Gadiraju et al. [37] found that PwD were concerned that LLMs may \"teach bad behaviors\u201d and reinforce ableist preconceptions rather than educating users.\nIn efforts to mitigate these biases, scholars have raised important questions on disability fairness in emerging AI technologies. As Whittaker et al. [117] and Trewin [110] observe, the fundamentally diverse and nuanced nature of disability often makes it an \"outlier\" in machine learning, with such outliers frequently treated as noise and disregarded. Disability is often \u201cimplicitly understood to be undesirable,\u201d and AI is positioned to \u201csolve the 'problem' of disability,\" for example, teaching children with autism to act more neurotypically [105, 117]. Similarly, in online communities, PwD are pressured to conform to platform policies (e.g., for people with eating disorders [33]) and toxicity classifiers who adhere to these policies [122, 125].\nIn this hostile climate, little work has been done to systematically examine how well state-of-the-art language models can moderate and mitigate ableist speech, a crucial step for supporting PwD in their visibility and advocacy efforts. While prior research has emphasized the importance of explaining outputs of language models designed to identify toxicity [38, 44, 121], there is a scarcity of research on whether these models can effectively identify and explain ableism [49, 60, 111]. To address this critical gap, our work investigates: (1) How well do TCs and LLMs identify ableist comments? (2) How well can LLMs explain why certain comments are ableist?"}, {"title": "3 METHODOLOGY", "content": "Figure 2 displays a high-level overview of our study methodology, following the process of curating a dataset, gathering human and language model measurements and explanations, and analyzing these assessments of harm."}, {"title": "3.1 Dataset Creation", "content": "Data Collection and Processing. Given that there are no publicly available datasets for ableist speech, we assembled our own dataset of ableist and non-ableist sentences. We first searched for examples of ableist speech on Reddit and Twitter using hashtags and keywords such as \"offensive,\" \"ableist,\" \"hate,\" \"harassment,\" \"problematic,\" and \"discrimination.\" We selected these platforms as they represent a diverse population of the Internet, and contain varying degrees of toxic behavior [62]. We limited our date range to 2019-2024 to capture more recent instances of ableism. To represent a diverse range of disabilities, as put forth by the Individuals with Disabilities Education Act (IDEA) [80], we also searched for these keywords in specific subreddit communities: r/autism, r/ADHD, r/blind, r/deaf, r/learningdisabilities, r/dyslexia, r/dyscalculia, r/neurodiversity, r/celebralpalsy, r/disability, r/stutter,"}, {"title": "3.2 Obtaining Human Evaluations", "content": "We gathered ratings and explanations of toxicity and ableism from human evaluators, including people with and without disabilities.\nParticipants. We recruited 100 people with disabilities and 60 without disabilities; all 160 participants resided in the United States and had native English fluency. Participants were recruited on Prolific, and screened on Qualtrics for English fluency and disability identities. Their ages ranged from 18 to 74 (mean = 37.2, SD = 12.0), though a majority of participants were between the ages of 28 to 38 (n = 62, 38.8%). Regarding gender, 72 participants (45%) identified as men, 83 participants (52%) identified as women, and 5 participants (3%) preferred to not disclose. Majority of participants self-described as White (n = 113, 70.6%), while 16 (10%) identified as Black, 13 (8.1%) identified as Asian, 10 (6.2%) identified as mixed race, and 8 (5%) preferred not to disclose.\nAmong the 100 participants with disabilities, 60 reported having psychological disorders (i.e., Attention Deficit, Emotional Disturbance), 44 reported having orthopedic/mobility impairments (i.e., Cerebral Palsy, neuromotor disabilities), 37 reported being blind and/or low vision, 30 reported being on the Autism Spectrum, 22 reported having learning disabilities (i.e., Dyslexia), 15 reported being d/Deaf or Hard of Hearing. These categories were not mutually exclusive, with 73 participants reporting more than one disability.\nProcedure. Each participant evaluated five randomly chosen comments from the dataset. This ensured that each comment was evaluated for harm by 5 people with disabilities, and 3 people without disabilities. Comments were introduced as, \"A person with a disability goes on social media and reads the following comment,\" to deflect from the reader feeling personally harmed by the comment (see Figure 3). They were made aware of the risks of participating, including confronting harmful language, and were given the option of exiting the study at any point.\nFor each comment, participants were asked to rank the comment's toxicity and ableism. We used \"toxicity,\" as an overarching term to encompass negative speech, as recommended by Kumar et al.'s [62] examination of users and toxicity classifiers. We used \"ableism\" to refer to specific discriminatory attitudes, misconceptions, and negative stereotypes towards people with disabilities. These terms were briefly defined for the participants. We used a 10-point Likert Scale measurement (from 1 to 10) for more granularity and validity [88], and to better map to the 0-1 scales offered by the toxicity classifiers and language models. Participants were also asked to explain why (or why not) the comment is ableist. They were encouraged to be candid or elaborate as they would like.\nAll participants gave their informed consent and allowed us to share anonymous research data. There was no time limit on individual evaluations, but the experiment was discontinued and rejected if it was not completed and submitted in one hour. We"}, {"title": "3.3 Evaluating Toxicity Classifiers and LLMs", "content": "Given the role that (large) language models have played in identifying harmful speech, we evaluate the effectiveness of both, toxicity classifiers and publicly available LLMs, to identify ableist speech.\nWe investigated Google Jigsaw's Perspective API [3], Azure's AI Content Safety API [1], and OpenAI's Moderation API [2]. PerspectiveAPI has emerged as the status quo classifier for detecting toxic content, while Azure and OpenAI's APIs are emerging content moderation services. These systems not only promise sensitive content moderation, but plan to inform content policy guidelines [115]. We used a Python script to interface with these APIs, and each toxicity classifier returned various toxicity values and ranks (e.g., PerspectiveAPI gave attributes for Profanity, Sexually Explicit Content, and Threats). To factor for any and all toxicity, we chose the maximum value among all categories for each toxicity classifier.\nWe focused on OpenAI's GPT-4 and Google's Gemini as recent exemplars of publicly available and prevalent LLMs for our evaluation. To capture how these off-the-shelf models perform, we instantiate zero-shot prompting. We begin by prompting a description for system behavior:\n> You are a system that identifies harmful and toxic speech for users with disability on social media.\nWe then ask the models to evaluate how toxic and ableist certain comments are from 1 to 10, and provide explanations for why the model associates this level of ableism with the comment. We ask the model to return these answers in chat response form, which we parsed into a table for analysis. The full prompt was:\n> I will give you 100 statements. For every statement, please rate how toxic the statement is (1 is not toxic, 10 is extremely toxic), and how ableist the statement is (1 is not ableist, 10 is extremely ableist). Give 3 specific reasons for why you gave these scores, each reason should be no more than 150 words. Use colloquial language to describe why the comment is ableist.\nA common concern with LLM output is its sensitivity to prompt engineering [116]. To account for this, we attempted multiple variations of the prompt above, such as providing more context for ableism, or probing for chain-of-thought prompting by asking it to \"think step by step\" [114] for ableism explanations. We noticed no significant difference in output. This is likely due to the nature of the task, which asks to return numerical ratings and short text responses, rather than long-form text.\nLLMs like GPT-4 and Gemini are further going through upgrades and model deployment changes. Our output is relevant from the state-of-the-art model performance in March, 2024. While we acknowledge that precision of certain ratings may change, this study is meant to be the first provocation into how these models identify and explain ableism."}, {"title": "3.4 Data Analysis", "content": "We used a mixed-methods approach spanning statistical analysis, computational text analysis, and qualitative analysis to evaluate how well toxicity classifiers and LLMs identify ableist comments, as described by PwD.\nQuantitative Analysis. Each comment was rated by 5 PwD and 3 non-PwD, we computed Fleiss' Kappa to assess inter-rater reliability between raters in assessing harm across the 100 statements. We mapped the original rating scale of 1 to 10 into 3 buckets, to gather general agreement between low (1-3), medium (4-7), and high (8-10) harm. For PwD toxicity ratings, $\\text{kappa} = 0.25, p < 0.001$; and for ableism ratings, $\\text{kappa} = 0.25, p < 0.001$. This low agreement is reinforces findings by Ross et al., \"hate speech is a fuzzy construct\" [92], and annotating hate speech is challenging [113]. We acknowledge that agreement may not be meaningful for research that relies on social and ethical contexts, and in fact, may reinforce biases among marginalized identities, as per [72]. In our analysis, we considered upper bounds of harm ratings to capture this variance.\nShapiro-Wilk normality tests ($\\alpha$ = 0.05) indicated that our distributions were not normally distributed. Hence, we used non-parametric ANOVA tests throughout our analysis. We used Kruskal Wallis tests for comparisons between three or more groups, and Wilcoxon signed-rank tests for pairwise comparisons. For our analyses, we treated all data as ordinal data (from 0 to 1 in intervals of 0.1).\nComputational Text Analysis. We used established natural language processing (NLP) methods to identify salient words, topics, and sentiments in explanations of ableism by different user groups (PwD, non-PwD, and LLMs). In the past few years, BERT models have emerged as powerful document embedding techniques [24], and have been used extensively to extract latent semantic structure, or discover emerging themes in long-form text.\nWe converted explanations into BERT embeddings, maintaining one embedding per user group per comment. For example, each"}, {"title": "3.5 Positionality and Ethical Considerations", "content": "In line with calls for reflexive practices in HCI research with marginalized identities [65, 99], we reflect on how our individual positioning influences our research design and analysis. All authors have a range of experiences related to disability, including lived experiences with ableism as a caregiver for disabled people. Authors have deep familiarity with ableism and some have over a decade of experience working with people with disabilities. Our research practice is directly influenced by these experiences. Our survey was meaningfully designed for participants with disabilities; it redirected harm by asking questions about harm in third person, and ensured a lower cognitive load by restricting the length and type of questions displayed (i.e., no grid questions) [16, 119]. We recognize that ableism is nuanced and subjective. This study aims to understand how well recent AI developments capture these varying ableist biases."}, {"title": "4 FINDINGS", "content": "We collected a total of 1,500 harm ratings (750 toxicity ratings, 750 ableism ratings), from 160 participants (100 people with disabilities, and 60 people without disabilities). Through an in-depth analysis of the participants' ratings and explanations of ableist comments, we examined differences between:\n*   toxicity ratings given by user groups, toxicity classifiers, and LLMs (Section 4.1),\n*   ableism ratings given by user groups and LLMs (Section 4.2), and\n*   explanations of ableist speech written by user groups and generated by LLMs (Section 4.3)."}, {"title": "4.1 Toxicity Identification (RQ1)", "content": "We examined the differences in toxicity ratings given by people with disabilities (PwD), people without disabilities (non-PwD), toxicity classifiers (TCs), and large language models (LLMs). Figure 4 visualizes the distributions of these ratings, and we observed stark differences between these groups."}, {"title": "4.1.1 Toxicity Classifiers.", "content": "Table 1 reports mean, standard deviation, median and IQR values for toxicity ratings of our dataset. Toxicity ratings by PwD (mean = 0.622) were much higher than those from AzureAI (mean = 0.231), OpenAI (mean = 0.182), and PerspectiveAPI (mean = 0.190), indicating that PwD rated comments as much more toxic than all TCs. To evaluate whether these groups come from statistically different distributions, we conducted a Kruskal-Wallis test, revealing a significant difference between toxicity ratings ($\\chi^2$(5) = 132.68, p < 0.001). A post-hoc pairwise Wilcoxon Signed-Rank Tests found significant differences between certain groups, indicated by asterisks (*) in Table 4. Largest differences were seen among ratings between PwD and AzureAl (p < 0.001, r = 0.856), PwD and OpenAI (p < 0.001, r = 0.842), and PwD and PerspectiveAPI (p < 0.001, r = 0.859). There were also significant differences in comparisons between Non-PwD and all TCs, suggesting that TCs were unable to capture toxicity as identified by people without disabilities as well.\nFor PerspectiveAPI, we also analyzed the category for \"identity-based attack,\" meant to capture \u201cnegative or hateful comments targeting someone because of their identity\" [3]. However, these values were significantly lower than mean values for overall toxicity, and certainly significantly lower than mean values by PwD (identity-attack mean = 0.07, toxicity mean = 0.19, PwD toxicity mean = 0.62).\nTo further examine the relationship between group rating patterns, we conducted a Spearman's rank correlation test between PwD and TCs (averaged values of all three TCs). We observed a moderate correlation (Spearman's coefficient $\\rho$ = 0.656, p < 0.001)"}, {"title": "Summary:", "content": "Toxicity ratings by toxicity classifiers were significantly lower than those from people with disabilities, even for explicit bias, suggesting that state-of-the-art toxicity classifiers do not meet the needs of PwD in identifying toxic comments targeted at them."}, {"title": "4.1.2 Large Language Models.", "content": "Table 1 shows mean, standard deviation, median and IQR values for toxicity ratings by GPT4 and Gemini. Mean toxicity ratings by PwD (0.622) were greater than both LLMs (GPT4 = 0.582 and Gemini = 0.374), signifying that similar to TCs, LLMs were unable to capture toxicity levels as rated by PwD (ground truth). A Kruskal-Wallis test found significant differences in their distributions ($\\chi^2$(4) = 48.218, p < 0.01). Results of a post-hoc Wilcoxon-Signed Rank demonstrated a large difference between PwD and Gemini (p < 0.001, r = 0.751), and a moderate difference PwD and GPT4 (p < 0.01, r = 0.260). We further noticed a large effect size in differences between Gemini and GPT4 (p < 0.001, r = 0.795), suggesting differences in how different LLMs rated toxicity. Meanwhile, GPT4 and Non-PwD had no statistically different effects (p > 0.05), suggesting that GPT4 toxicity ratings are aligned more with the ratings from Non-PwD than PwD. Full results are available in Appendix, Table 5.\nTo explore whether LLMs and PwD rated toxicity in alignment, we visualized their ratings on a scatter plot (see Figure 6). We averaged GPT4 and Gemini ratings, and conducted a Spearman's rank correlation test between averaged LLM and PwD values, finding that these ratings were strongly correlated (Spearman's coefficient $\\rho$ = 0.703, p < 0.001). We note that this was a stronger correlation than ratings between LLMs and Non-PwD ($\\rho$ = 0.609, p < 0.001), suggesting a stronger alignment between LLMs and PwD.\nTo examine outliers and delineate differences between PwD and LLM toxicity ratings, we computed differences between PwD and LLM ratings for each comment. We found that LLMs underrated 78 comments, compared to PwD. Among these, 7 comments were more than 2 SD of differences away. The largest differences were"}, {"title": "Summary:", "content": "Toxicity ratings by LLMs were slightly lower than those by people with disabilities, Gemini ratings were much lower. LLMs fared much better in identifying toxic comments towards PwD compared to TCs, but still failed to recognize some critically toxic comments."}, {"title": "4.2 Ableism Identification (RQ1)", "content": "TCs do not provide a label for ableism, the closest being identity-based attack which we analyzed in the previous section. In addition, ableism is a different measure of harm than toxicity; toxicity measured how generally offensive or hostile the comment was, whereas ableism referred to specific discriminatory attitudes, misconceptions, and bias held towards PwD. We now present the analysis of ableism ratings given by LLMs, PwD, and non-PwD.\nInteractions between Ableism and Toxicity We compared ableism ratings to toxicity ratings to evaluate alignment and identify any outliers (e.g., comments that might be rated high for toxicity, but low for ableism). A Spearman's rank correlation showed high correlations between toxicity ratings and ableism ratings for PwD (Spearman's coefficient $\\rho$ = 0.917, p < 0.001) and for LLMs (Spearman's coefficient $\\rho$ = 0.936, p < 0.001).\nPwD generally rated toxicity higher than ableism (mean toxicity = 6.22, mean ableism = 6.11), whereas LLMs rated ableism much higher than toxicity (mean toxicity = 4.79, mean ableism = 6.13). For some comments, PwD found toxicity and ableism were related; for the comment, YOU DON'T HAVE A SPEECH PROBLEM, YOU JUST LACK SELF-ESTEEM, one PwD said, \"this is an absurd statement, discounting someone's disability is very toxic.\" However, several PwD explained why some comments were toxic, but not ableist. For the comment, I READ THAT MOST SERIAL KILLERS HAVE SPEECH DISFLUENCIES WHICH MAKES THEM VIOLENT, a PwD argued that, \u201cthis comment is not inherently ableist as it states a fact of our world that serial killers usually have speech disfluencies, however it still creates a stigma.\u201d Other PwD believed that certain comments were toxic to everyone, and not specifically ableist. For example, a PwD found the comment,"}, {"title": "YOU SHOULD BE CONTENT WITH WHAT YOU HAVE,", "content": "toxic but not ableist, stating that \"we all deserve better. period.\"\nLow, Medium, High Ableism Analysis. To identify initial patterns in ableism ratings, as well as account for subjective interpretations of granular Likert scale ratings, we grouped our 1-10 scale data into fewer buckets for Low, Medium, and High ableism (Low: 1-3, Medium: 4-7, High: 8-10).\nWe averaged LLM data to make broad comparisons between PwD and LLMs, these counts are presented in Table 2. We conducted a Pearson's Chi-Squared test to measure whether these groups measured ableism differently; our results confirm that ableism counts statistically differed between PwD and LLMs, ($\\chi^2$(1, N = 100) = 6.58, p < 0.05, V = 0.18), and LLMs and Non-PwD ($\\chi^2$(1, N = 100) = 8.67, p < 0.05, V = 0.21). Examining the data, LLMs reported more counts of High Ableism than PwD and Non-PwD. We also analyzed for differences in implicit and explicit bias, and found no significant differences in group ratings."}, {"title": "Summary:", "content": "Ableism ratings by LLMs were fairly aligned with PwD, many ableist comments were rated as more ableist by LLMs than PwD. But significant differences appeared when we considered the upper bounds of ableism severity (maximum ableism rating given by PwD)."}, {"title": "4.3 Ableism Explanations (RQ2)", "content": "Given that PWD and LLMs were asked to reason about their ratings, we investigated how these groups explained and spoke about ableism. Overall, LLMs were able to explain ableism, albeit vaguely. They recognized common ableist stereotypes and microaggressions, and were able to elicit why certain comments would be harmful to"}, {"title": "4.3.1 LLMs focused on societal implications of ableism instead of emotional, individual harm.", "content": "When explaining ableism, many PwD spoke about the intimate harm caused by the comment, using vocabulary such as, \"dehumanizing,\" \"insulting,\" or \"harsh,\" that emphasized the effect of the comment upon PwD. According to our topic model, about 18.6% of the explanations emphasized the rudeness and ignorance of the comments, and an additional 10% focused on the pain these comments may cause. These words and topics were absent from the topics generated for LLM explanations. Instead, LLMs focused on more general implications such as recognizing how the comments perpetuate negative stereotypes, a topic that spanned 10.1% of the explanations.\nGemini explained the ableism for the comment, YOU SHOULDN'T LET YOUR DISABILITY HOLD YOU BACK, but it failed to capture the individual harm and impact of this comment as described by PwD.\nGemini: \"This statement places the onus solely on the individual to overcome challenges, disregarding societal barriers and the need for systemic changes to ensure equal opportunities.\"\nPwD: \"When you don't know what the disability is like, this is very apathetic to the person with the disability. It's saying if you believe hard enough you can use your wheelchair to go up those stairs like everyone else. Yes, but its not going to be easy. I honestly have no word for it. It's invalidating. Dehumanizing.\u201d\nIn many cases, LLMs spoke about how ableist comments reinforced negative societal views of disabled lives, which surfaced as another salient topic covering 10.1% of explanations. For another comment in our dataset, I ATTENDED A CHARITY FOR YOUR DISABILITY ONCE, IT WAS VERY MOTIVATING, PwD shared how this comment", "person,": "hereas GPT4 commented on how", "experiences.": "ere, we see that LLMs were unable to recognize the patronizing effects of this comment, one of the major forms of ableism documented in past literature [48, 49, 104]."}, {"title": "4.3.2 LLMs lacked specificity when explaining ableism.", "content": "In explanations from PwD, we observed topics related to vision and hearing impairments (12.7% of explanations), autism and functioning (10.2% of explanations), and wheelchair experiences (5.9% of explanations). In contrast, LLMs explanations contained one topic about autism and communication (spanning 8.4% of explanations), and topics about specific disabilities were largely absent.\nLLMs offered vague and homogeneous descriptions of ableism which could be used to describe broad harm across different"}]}