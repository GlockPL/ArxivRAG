{"title": "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models", "authors": ["Yukang Yang", "Declan Campbell", "Kaixuan Huang", "Mengdi Wang", "Jonathan Cohen", "Taylor Webb"], "abstract": "Many recent studies have found evidence for emergent reasoning capabilities in large language models, but debate persists concerning the robustness of these capabilities, and the extent to which they depend on structured reasoning mechanisms. To shed light on these issues, we perform a comprehensive study of the internal mechanisms that support abstract rule induction in an open-source language model (Llama3-70B). We identify an emergent symbolic architecture that implements abstract reasoning via a series of three computations. In early layers, symbol abstraction heads convert input tokens to abstract variables based on the relations between those tokens. In intermediate layers, symbolic induction heads perform sequence induction over these abstract variables. Finally, in later layers, retrieval heads predict the next token by retrieving the value associated with the predicted abstract variable. These results point toward a resolution of the longstanding debate between symbolic and neural network approaches, suggesting that emergent reasoning in neural networks depends on the emergence of symbolic mechanisms.", "sections": [{"title": "1. Introduction", "content": "Large language models have become the dominant paradigm in artificial intelligence, but there is significant ongoing debate concerning the limits and reliability of their capabilities. One major focus of this debate has been the question of whether they can reason systematically in a abstract or human-like manner. Many studies have documented impressive performance on various reasoning tasks (Wei et al., 2022; Mirchandani et al., 2023), even rivaling human performance in some cases (Webb et al., 2023; Musker et al., 2024; Webb et al., 2024a), but other studies have questioned these conclusions (Wu et al., 2023; McCoy et al., 2023; Lewis & Mitchell, 2024). In particular, language models appear to perform more poorly in some reasoning domains, such as mathematical reasoning (Dziri et al., 2024) or planning (Momennejad et al., 2024); and, even in domains in which they have shown strong performance such as analogical reasoning (Webb et al., 2023), some studies have questioned the robustness of these capabilities (Lewis & Mitchell, 2024).\nThese conflicting findings raise the question: are language models genuinely capable of abstractly structured human-like reasoning, or are they merely mimicking this capacity by statistically approximating their training data? One way to answer this question is to look at the internal mechanisms that support this capacity. It has long been hypothesized that innate symbol-processing mechanisms are required to support human-like abstraction (Marcus, 2001; Dehaene et al., 2022; Wong et al., 2023). It has also been demonstrated that neural networks are capable, at least in principle, of implementing some of the key properties of symbolic systems (Smolensky, 1990; Hummel & Holyoak, 2003; Kriete et al., 2013), and that the incorporation of these properties as architectural inductive biases can support data-efficient acquisition of abstract symbolic reasoning (Webb et al., 2020; Altabaa et al., 2023; Webb et al., 2024b). It remains unclear, however, in the case of transformer language models that do not obviously possess such strong inductive biases, what mechanisms support their emergent capability for abstraction.\nHere, we report evidence for a set of emergent symbolic mechanisms that support abstract reasoning in large language models. We focus on a simple but paradigmatic abstract reasoning task-induction of algebraic rules-and identify an emergent three-stage architecture that supports performance of this task in the open-source language model Llama3-70B (Dubey et al., 2024). Notably, this architecture can be straightforwardly interpreted as performing a form of symbol processing. Specifically, we find evidence to support a three-stage procedure in which: 1) input tokens are converted to abstract variables (i.e., symbols) based on the relations between those tokens, 2) sequence induction is"}, {"title": "2. Approach", "content": "Figure 1 depicts the proposed architecture and rule induction task. In this work, we focused on an algebraic rule induction task involving sequences governed by one of two identity rules, ABA or ABB. For each problem, two in-context examples were presented, followed by an incomplete third example. The model was expected to generate the token that completes this third example. We instantiated rules using tokens randomly sampled from Llama3's vocabulary (ensuring that in-context examples within the same problem instance did not share tokens). We found that Llama3-70B displayed a 2-shot accuracy of 95% on this task.\nAlthough this task is relatively simple, especially when compared with some of the tasks that have been featured in recent debates over LLM reasoning (e.g., matrix reasoning tasks (Webb et al., 2023)), it nevertheless offers a paradigmatic case of relational abstraction. In particular, the use of completely arbitrary tokens ensures that the task cannot be solved by relying on statistical patterns specific to the tokens or associations among them, and for this reason it has previously been used to argue for the presence of symbol-processing mechanisms in human cognition (Marcus et al., 1999), and to evaluate systematic generalization of abstract rules in neural networks (Webb et al., 2020). Accordingly, the ability to reliably solve this task is already strongly suggestive of the presence of some form of symbol-processing. In the following sections, we describe a specific mechanistic hypothesis for how symbol-processing might be carried out in this model."}, {"title": "2.1. Symbol Abstraction Heads", "content": "Our hypothesis consists of three stages. In the first stage, input tokens are converted to symbolic representations. The inspiration for this hypothesis comes from the abstractor architecture (Altabaa et al., 2023), a variant of the transformer that implements a strong relational inductive bias (Webb et al., 2024b). In that architecture, a modified form of attention (termed relational cross-attention) is employed in which the values consist of a standalone set of learned embeddings, rather than being conditioned on the input tokens as in standard self attention. As a result, the output of this attention operation is completely abstracted away from the identity of the input tokens, and instead only reflects the pattern of relations among those tokens (as encoded by the pattern of inner products between query and key embeddings). These outputs can therefore be viewed as a form of learned, distributed symbolic representations.\nHere, we hypothesize that an emergent form of this relational attention operation is implemented by attention heads in early layers of the model. We refer to these heads as symbol abstraction heads. Concretely, the keys and query embeddings in these heads represent the input tokens, and the inner product between keys and queries represents the relations between these tokens. It is natural to interpret this operation as representing similarity relations (and this is the relevant type of relations in our task), but it is also possible for this operation to represent a broader class of relations (Altabaa & Lafferty, 2024). Importantly, we hypothesize that the value embeddings in these heads do not carry information about the specific identity of the input tokens, but instead represent only their position. More precisely, we hypothesize that the value embeddings represent the relative position of a token within an in-context example, as this is precisely the information that's needed to compute the abstract variable associated with that token (e.g., the fact that the first token and the third token are the same in an ABA rule is precisely what determines that they share the same variable). Given that these conditions are met, the self-attention operation is equivalent to relational cross-attention (Altabaa et al., 2023), and the output of such an attention head will be analogous to an abstract variable."}, {"title": "2.2. Symbolic Induction Heads", "content": "In the second stage, we hypothesize that sequence induction is performed over the abstract variables computed in the first stage. This hypothesis is inspired by previous work on induction heads, an emergent circuit that supports in-context learning in transformers (Elhage et al., 2021; Olsson et al., 2022). As originally formulated, this circuit performs a simple sequence induction mechanism: given a sequence that ends with a particular token, an induction head will look for previous instances of that token, and retrieve the token that succeeded it. Although this mechanism performs induction based only on in-context bigram statistics, subsequent work has identified heads that also compute higher-order n-gram statistics (Aky\u00fcrek et al., 2024). Here, we use the term 'induction' to refer to the more general process of predicting the next token based on in-context transition probabilities (i.e., beyond bigram statistics).\nWe hypothesize that a symbolic variant of this mechanism is responsible for performing induction over sequences of symbols rather than literal tokens. We refer to the attention heads that carry out this mechanism as symbolic induction heads. Unlike standard induction heads, which operate over direct representations of the input tokens, symbolic induction heads operate over representations of abstract variables (computed by symbol abstraction heads in previous layers). The output of symbolic induction heads is a prediction of the abstract variable associated with the next token. Empirically, we find that symbolic induction heads are distinct from standard induction heads (section 3.5)."}, {"title": "2.3. Retrieval Heads", "content": "Finally, in the third stage, we hypothesize that a separate mechanism is used to convert the abstract variables (symbols) to their associated tokens (values), by performing a simple form of retrieval. We refer to the attention heads that perform this retrieval operation as retrieval heads. The key and query embeddings in these heads represent abstract variables, and the value embeddings represent the corresponding input tokens. Retrieval heads perform the inverse of the relational attention operation performed by symbol abstraction heads. Given an input embedding representing an abstract variable (the prediction computed by symbolic induction heads in previous layers), this variable is matched with previous instances, and the associated token is retrieved. This can be viewed as a form of the 'dereferencing' operation that is central to symbolic computing, wherein a variable (i.e., a pointer to a particular location in memory) is used to retrieve the value associated with it (i.e., the data stored at that location)."}, {"title": "3. Results", "content": ""}, {"title": "3.1. Causal Mediation Analyses", "content": "We performed causal mediation analysis (Pearl, 2022; Meng et al., 2022; Wang et al., 2022; Todd et al., 2023) to isolate the hypothesized attention heads. In this analysis, embeddings from one context are patched into another context. This approach can be used to estimate the causal effect of an embedding at a particular layer, position, or attention head.\nOur analysis had two conditions. In one condition, intended to isolate representations of abstract variables (i.e., symbols), we created two contexts in which the same token is associated with two different variables. Given a set of tokens \n$A_1, B_1, ...A_N, B_N$\n(where N 1 represents the number of in-context examples), we created one context $c^{abstract}$ that instantiated an ABA rule, and another context $c^{abstract}$ that instantiated an ABB rule:\n$c^{abstract} = A_1, B_1, A_1, ..., A_N, B_N$  (1)\n$c^{abstract} = B_1, A_1, A_1, ..., B_N, A_N$  (2)\nImportantly, in this analysis, the final token in each in-context example, $A_N$, is identical for both contexts, but it is associated with a different variable, based on its relations to the other tokens within the same in-context example.\nWe contrast this with another condition that is intended to isolate representations of literal tokens. Given the same set of tokens used for the previous condition, we created the following two contexts:\n$c^{token} = A_1, B_1, A_1, ..., A_N, B_N$   (3)\n$c^{token} = A_1, B_1, B_1, ..., B_N, A_N$ (4)\nIn this condition, both contexts involve the same abstract rule, but the tokens used in the final example ($A_N$ vs. $B_N$) are swapped. We also perform a version of both analyses using the ABB rule and average the results for the two rules.\nTogether, these analyses allow for a double dissociation between representations of abstract variables (abstract causal mediation) vs. the tokens associated with those variables (token causal mediation). For each analysis, given two contexts $C_1$ and $C_2$, and a pretrained lanuage model f(c) that outputs logits for all possible next tokens, we computed the causal mediation score developed by Wang et al. (2022):\n$s = (f(c_1) [y_{c_1}] - f(c_1) [y_{c_1}]) - (f(C_1) [y_{c_1}*] - f(C_1)[y_{c_1}]),$ (5)\nwhere $f(c_1) [y]$ is the logit for answer y in context $c_1$; $f(c^*_1) [y]$ is the logit for answer y in the patched context $c^*_1$, for which activations are patched from $c_2$ to $c_1$ at specific layers, positions, and/or attention heads; $y_{c_1}$ is the"}, {"title": "3.2. Attention Analyses", "content": "We analyzed attention patterns to better understand the behavior of the identified attention heads. Figure 3(a) shows the attention patterns for symbol abstraction heads. Our hypothesis predicts that attention should be directed from the third item in each in-context example to the first item for ABA rules (top), and should be directed from the third item to the second item for ABB rules (bottom). The results largely confirmed this hypothesis (the attention patterns for these positions are highlighted with red dashed lines). Interestingly, the pattern became more focused for the second in-context example, suggesting that the symbol abstraction heads benefit from in-context learning.\nFigure 3(b) shows the results for symbolic induction heads. Our hypothesis predicts that attention should be directed from the final position in the entire sequence (the separation token at the end of the incomplete example) to positions of the final items in each in-context example, as these are the positions where the previous instances of the to-be-predicted abstract variable are located. The results confirmed this hypothesis. Again, the pattern was stronger for the second in-context example, consistent with a general effect of in-context learning, and consistent with the more focused pattern of attention observed for the symbol abstraction heads in the second in-context example.\nFigure 3(c)) shows the results for retrieval heads. Our hypothesis predicts that attention should be directed from the final position in the sequence to the positions corresponding to the tokens that will appear next (i.e., within the incomplete example). For ABA rules (top), we predict that attention should be directed to the first item in the example (corresponding to the variable A), and for ABB rules (bottom) we predict that attention should be directed to the second item in each example (corresponding to the variable B). This prediction too was confirmed by our analyses."}, {"title": "3.3. Representational Similarity Analyses", "content": "We also performed representational similarity analyses (Kriegeskorte et al., 2008) to better understand the representations that were produced by each type of attention head. In this analysis, representations are modeled in terms of their similarity with one another. For each set of tokens $A_1, B_1, ...A_N, A_N$, we created four prompts, intended to dissociate representations of abstract variables (i.e., symbols) vs. literal tokens. The first two prompts were the same as the $c^{abstract}$ and $c^{abstract}$ contexts used for causal mediation analysis, in which the same token plays different abstract roles. The other two prompts were based on $c^{abstract}$ and $c^{abstract}$, but the final instances of $A_N$ and $B_N$ were swapped. The resulting set of prompts predict one pattern of pairwise similarity for abstract variables, and a different pattern of similarity for literal tokens.\nFigure 4(a) shows the predicted pattern of pairwise similarities for representations of abstract variables, with all pairs involving two instances of the variable A forming one block of high similarity, and all pairs involving two instances of B forming another block. By contrast, Figure 4(b) shows the predicted pattern of pairwise similarities for representations of literal tokens. This pattern displays 3 diagonal bands, corresponding to pairs of the same token (regardless of whether this token plays the same role).\nFigure 4(c) shows the pattern of similarity observed for the output of symbol abstraction heads at the third position in both in-context examples (averaged across the two examples). The pattern closely resembles the abstract variable similarity matrix, indicating that the output of symbol abstraction heads have a representational structure similar to abstract variables. Figure 4(d) shows the pattern observed for symbolic induction heads at the final sequence position. This pattern also resembles the pattern predicted for abstract variables. Figure 4(e) shows the patten observed for retrieval heads at the final sequence position. This pattern closely resembles the token similarity matrix.\nInterestingly, although symbol abstraction heads and symbolic induction heads primarily resemble the pattern predicted for abstract variables, they also show the diagonal bands predicted by the token similarity matrix, suggesting that they preserve some degree of token identity, and thus do not represent perfectly abstract variables. Nevertheless, the patterns of similarity displayed by these three distinct types of attention heads have a strikingly close correspondence to the predictions of the hypothesized architecture, with symbol abstraction heads and symbolic induction heads both generating outputs that primarily resemble abstract variables, and retrieval heads generating outputs that primarily resemble literal tokens."}, {"title": "3.4. Ablation Analyses", "content": "The causal mediation analyses in section 3.1 demonstrate that the identified attention heads are causally sufficient, in the sense that perturbing their outputs alters the model's responses in a predictable manner. We also performed ablation analyses to test whether these heads are necessary for the model to perform the task.\nFor each type of attention head, we performed a cumulative ablation analysis in which the heads with the top h causal mediation scores were ablated. This was performed for the full range of h = 1...H heads in the entire model. We also performed a control experiment in which each head ablated in the previous experiment was replaced by the head in the same layer with the lowest causal mediation score. We measured the effects of these ablations in terms of the probability assigned to the correct answer.\nWe found that these ablation experiments had a dramatic effect for all three types of heads (Figures 5(a)-5(c)). In the ablation condition, the probability assigned to the correct answer rapidly fell to zero as more heads were ablated, whereas in the control condition it was necessary to ablate almost all attention heads to have such an effect. These"}, {"title": "3.5. Comparison with Induction Heads", "content": "We investigated the relationship between symbolic induction heads and the standard induction heads identified in previous work (Olsson et al., 2022). In that work, it was proposed that induction heads not only perform literal sequence induction, but may also perform a fuzzy or abstract form of induction. This raises the question of whether symbolic induction heads are merely standard induction heads.\nFor each attention head, we computed the prefix matching score previously used to identify induction heads (Olsson et al., 2022), and compared this with the causal mediation score for symbolic induction heads. We found that these scores were very weakly correlated (r = 0.13), thus appearing largely orthogonal (Figure 7(a)). These results suggest that, despite the conceptual similarity between these two mechanisms, they are implemented by disjoint sets of attention heads."}, {"title": "3.6. Comparison with Function Vectors", "content": "We also investigated the relationship between symbolic induction heads and function vectors (Todd et al., 2023), representations of an in-context task that are generated by a subset of attention heads. The symbolic induction heads identified in the present work have many similarities to the attention heads that generate function vectors, including: 1) they are found in intermediate layers of the model, 2) they primarily attend to the last item in each in-context example, and 3) they are causally implicated in in-context learning for relational tasks.\nTo address this, we computed the average indirect effect for each attention head, which represents a measure of the extent to which its outputs constitute function vectors (Todd et al., 2023), and compared this with the causal mediation score for symbolic induction heads. This analysis revealed that these scores are indeed highly correlated (Figure 7(b); r = 0.82), suggesting that they are essentially the same set of attention heads. That is, the output of symbolic induction heads can be thought of as function vectors. This result provides a novel perspective on function vectors, suggesting"}, {"title": "4. Related Work", "content": "There is a rich history of work illustrating how various aspects of symbol processing might be implemented in neural networks. Work on the tensor product representation (Smolensky, 1990) and binding-by-synchrony (Hummel & Holyoak, 2003) illustrated how dynamic variable-binding can be performed in neural networks. Kriete et al. (2013) demonstrated how indirection, the use of one variable to refer to another, can be implemented in a biologically plausible neural network. More recently, a series of studies illustrated how a relational bottleneck (Webb et al., 2024b)-a strong inductive bias to perform relational processing-can enable data-efficient learning of abstract reasoning capabilities in deep learning systems (Webb et al., 2020; Kerg et al., 2022; Altabaa et al., 2023). The primary contribution of our work, relative to these previous studies, is to demonstrate empirically that symbolic mechanisms can emerge in a large-scale neural network, and to illustrate how they operate to support abstract reasoning. Notably, the symbol abstraction heads identified in this work implement an emergent version of the abstractor architecture that was previously proposed to support relational learning (Altabaa et al., 2023)\nThere has also been much recent work investigating the internal mechanisms that support various forms of abstract and structured task processing in language models. This work has identified key primitives such as induction heads (Olsson et al., 2022), function vectors (Todd et al., 2023), binding IDs (Feng & Steinhardt, 2023), and other mechanisms that play a role in relational processing (Merullo et al., 2023). We build on this previous work by identifying an integrated architecture that brings together multiple mechanisms. These include newly identified mechanisms \u2013 symbol abstraction and symbolic induction heads - that, respectively, carry out the processes of abstraction and rule induction needed to implement an emergent form of symbol processing that supports abstract reasoning in a neural network."}, {"title": "5. Discussion", "content": "In this work, we have identified an emergent architecture consisting of several newly identified mechanistic primitives, and illustrated how these mechanisms work together to implement a form of symbol processing. These results have major implications both for the debate over whether language models are capable of genuine reasoning, and for the broader debate between traditional symbolic and neural network approaches in artificial intelligence and cognitive science.\nOn the one hand, the emergent architecture identified here, that supports abstract reasoning via an intermediate layer of symbol processing, is strikingly at odds with characterizations of language models as mere stochastic parrots (Bender et al., 2021) or 'approximate retrieval' engines (Wu et al., 2023). These results are also at odds with claims that neural networks will need innately configured symbol processing mechanisms in order to perform human-like abstract reasoning (Marcus, 2001; Dehaene et al., 2022; Wong et al., 2023). On the other hand, these results can be viewed as a vindication of longstanding claims that symbol-processing mechanisms of some form (whether they be innate or learned) are a necessary component supporting human cognitive abilities (Fodor & Pylyshyn, 1988), insofar as they suggest that neural networks can acquire these abilities by developing their own form of symbol processing."}, {"title": "", "content": "It is interesting to consider the extent to which the identified mechanisms are truly emergent vs. dependent on innate aspects of the model. The transformer architecture (Vaswani et al., 2017) does not obviously possess the strong relational inductive biases that characterize the abstractor (Altabaa et al., 2023) or other architectures designed to perform relational abstraction (Webb et al., 2024b). However, transformers do have some inductive biases that seem relevant, including: 1) an innate mechanism for computing in-context similarity via the inner product between keys and queries, and 2) a form of indirection, in the sense that the keys and queries that are used to select information for retrieval are distinct from the values that are retrieved. In future work, it would be interesting to investigate the extent to which these or other inductive biases contribute to the development of emergent symbol processing mechanisms.\nFinally, it should be noted that our results do not suggest the language model precisely implements the hypothesized architecture, but more likely that it implements an approximate version of these computations. For instance, the symbol abstraction head and symbolic induction head outputs do not appear to represent perfectly abstract variables, but rather contain some information about the specific tokens that are used in each problem. This might explain some of the 'content effects' that have been observed in language models (Dasgupta et al., 2022), in which reasoning performance is not entirely abstract, but depends on the specific content over which reasoning is performed. These effects are also well documented in human psychology (Wason, 1968), which suggests that a similar form of approximate symbol processing may be implemented by the human brain. Whether and how these systems can account for the reasoning mechanisms employed by the brain is an interesting and important question to explore in future work."}, {"title": "A. Code and Hardware", "content": "All code was written in Python using the TransformerLens and HuggingFace libraries. Experiments were conducted on two NVIDIA 80G H100 GPUs.\nAll code and data will be made publicly available upon acceptance."}, {"title": "B. Implementation Details", "content": ""}, {"title": "B.1. Task Performance Evaluation", "content": "We randomly selected English tokens from LLaMa-3 vocabulary to form 2000 prompts to evaluate the generation accuracy. The prompt format used in experiments (2-shot) is:\n$A_1^B_1^A_1\\nA_2^B_2^A_2\\n A_3^B_3^$"}, {"title": "B.2. Causal Mediation Analysis", "content": "Algorithm 1 Causal Mediation\n1: Input: context pair ($C_1,C_2$), language model $f(c) \\in R^A$: outputs the logits for all possible next tokens at the last position of prompt c, vocabulary size A, the i-th value of vector $f(c)$: $f(c)[i]$, the correct answer for $C_1$: $y_{c_1}$, the expected answer for the patched context $c_1$: $y_{c1}^*$.\n2: For $c_1$, measure the difference between the output logit for $y_{c1}^*$ and $y_{c_1}$, i.e., $Af_{c1}$ = $f (c_1)[y_{c1}^*] \u2212 f(c_1)[y_{c1}]$.\n3: Cache the inner activations after feeding $c_2$ into the model.\n4: Replace the activations of the entire attention block or individual attention head in certain layer with the activations from $c_2$ at specific token positions to intervene on processing of $c_1$, and retrieve the logit difference:\n$Af_{c1}^* = f(c^*_1)[y_{c1}^*] - f(C_1)[y_{c1}]$ (6)\nThe change in the logit difference is a measure of the causal mediation effect:\n$s = \u2206f_{c*} - Af_{c_1} = (f(c^*_1)[y_{c1}^*] \u2212 f(c^*_1)[y_{c1}]) \u2212 (f(C_1)[y_{c1}^*] \u2212 f(C_1)[y_{c1}]),$ (7)\n5: Output: Causal Mediation Score s\nAlgorithm 1 explains the causal mediation procedure on one context pair ($C_1, C_2$). We randomly selected 20 prompts, restricting the analysis to prompts that the model answered correctly, and calculated the average score over both ABA and ABB tasks.\nContexts. For ($c^{abstract}$, $c^{abstract}$), $y_{c1}$ = $A_N$ (Eq. 1 and Eq.2). After activation patching, $y_{c1}^*$ should be $B_N$ based on our hypothesis that the output embeddings of symbol abstraction heads and symbolic induction heads represent abstract variables. For ($c^{token}$, $c^{token}$), $y_{c1}^*$ should be $B_N$ according to the hypothesis that the output of retrieval heads represent the actual token.\nto\nActivations. We focus on two types of activations, the output of the entire attention block and the output embeddings of the individual attention head. The attention block includes attention heads followed by an MLP. The output embedding of the attention heads is the average of the value embeddings weighted by the attention scores.\nPositions. We replaced the activations at the final items of the in-context examples ($A_n$ or $B_N$) or the final token right before the generated tokens (the final separation token)."}, {"title": "B.3. Attention Analyses", "content": "For each individual head, the attention map was averaged over 1378 prompts each for ABA and ABB tasks, again limiting the analysis to prompts that the model answered correctly."}, {"title": "B.4. Representational Similarity Analyses", "content": "For one set of tokens $[(A_n, B_n)]_N$, there are four different contexts, i.e.,\n$A_1, B_1, A_1, ..., A_N, B_N$  (8)\n$A_1, B_1, A_1, ..., B_N, A_N$\n$B_1, A_1, A_1, ..., B_N, A_N$\n$B_1, A_1, A_1, ..., A_N, B_N$\nWe randomly selected 40 token sets formed in different contexts to measure the similarity of the activations at certain positions."}, {"title": "B.5. Cumulative Ablation Analyses", "content": "We randomly selected another set of 20 prompts, which do not overlap with the prompts used in causal mediation analyses. Starting from the heads with highest causal mediation scores, we gradually increase the number of ablated heads, and evaluate the effect on the probability of the correct answer. As a control, we performed the same analysis, but replaced each ablated head with the ablation of the head in the same layer with the lowest causal mediation score. The curves in Fig. 5 are averaged over both ABA and ABB tasks."}, {"title": "B.6. Induction Heads", "content": "Following (Olsson et al., 2022), we used the prefix matching score as a measure for induction heads. We created a prompt involving a repeated sequence of 50 random tokens. The prefix matching score is defined as the average attention score from each token to the tokens that directly follow the previous instance of the same token. We averaged results over 4 random seeds."}, {"title": "B.7. Function Vectors", "content": "As described in (Todd et al., 2023), function vectors are aggregated over heads with a high causal mediation score. The answers for each in-context example are shuffled to form a corrupted prompt and the causal indirect effect (CIE) is defined as the recovery of the probability for correct answers. The average indirect effect (AIE) was taken over 50 prompts each for ABA and ABB tasks."}, {"title": "C. Additional Experimental Results", "content": ""}, {"title": "C.1. Comparison of Symbol Abstraction Heads, Symbolic Induction Heads, and Retrieval Heads", "content": ""}]}