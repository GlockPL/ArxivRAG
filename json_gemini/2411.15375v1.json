{"title": "ADAMZ: AN ENHANCED OPTIMISATION METHOD FOR NEURAL NETWORK TRAINING", "authors": ["Ilia Zaznov", "Atta Badii", "Julian Kunkel", "Alfonso Dufour"], "abstract": "AdamZ is an advanced variant of the Adam optimiser, developed to enhance convergence efficiency in neural network training. This optimiser dynamically adjusts the learning rate by incorporating mechanisms to address overshooting and stagnation, that are common challenges in optimisation. Specifically, AdamZ reduces the learning rate when overshooting is detected and increases it during periods of stagnation, utilising hyperparameters such as overshoot and stagnation factors, thresholds, and patience levels to guide these adjustments. While AdamZ may lead to slightly longer training times compared to some other optimisers, it consistently excels in minimising the loss function, making it particularly advantageous for applications where precision is critical. Benchmarking results demonstrate the effectiveness of AdamZ in maintaining optimal learning rates, leading to improved model performance across diverse tasks.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of machine learning has witnessed significant advancements, particularly in the development of optimisation algorithms that enhance the efficiency and effectiveness of training deep neural networks. Among these algorithms, the Adam optimiser has gained widespread popularity due to its adaptive learning rate capabilities, which enable more efficient convergence compared to traditional methods such as stochastic gradient descent. However, despite its advantages, Adam is not without its limitations, particularly when it comes to handling issues such as overshooting and stagnation during the training process.\nTo address these challenges, we introduce AdamZ as an advanced variant of the Adam optimiser. AdamZ is specifically designed to dynamically adjust the learning rate responsive to the characteristics of the loss function, thereby improving both convergence stability and model accuracy. This novel optimiser integrates mechanisms to detect and mitigate overshooting, at the point where the optimiser has stepped too far into the parameter space, and stagnation at points, where progress has started to stall despite ongoing training. By introducing hyperparameters such as overshoot and stagnation factors, thresholds, and patience levels, AdamZ provides a more responsive approach to learning rate adaptation than obtained through Adam."}, {"title": "2 Related Work", "content": "The development of optimisation algorithms has been pivotal in advancing the field of machine learning, particularly in training deep neural networks. This section reviews several key optimisers that have influenced the design and functionality of AdamZ.\nStochastic Gradient Descent (SGD) [1] is a foundational optimisation algorithm introduced in 1951. It updates parameters iteratively by moving in the direction of the negative gradient of the loss function. The update rule is given by:\n$\\theta_{\\tau+1} = \\theta_{\\tau} \u2013 \\eta \\nabla_{\\theta}J(\\theta)$\nwhere \u03b7 is the learning rate. This method is commonly used in training machine learning models, especially in deep learning. It is simple to implement and computationally efficient for large datasets. However, it requires careful tuning of the learning rate and can get stuck in local minima.\nAveraged Stochastic Gradient Descent (ASGD) [2], introduced in 1992, enhances convergence by averaging the sequence of iterates. The update rule is:\n$\\Theta_t = \\frac{1}{t}\\sum_{i=1}^{t} \\theta_i$\nThis approach is effective in reducing variance in updates and is used in scenarios where reducing the variance of SGD updates is crucial, such as in online learning. However, the averaging process can slow down convergence in the initial stages.\nAdagrad (2011) [3] adapted the learning rate for each parameter based on historical gradients. The update rule is:\n$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla J(\\theta)$\nwhere $G_t$ is the sum of squares of past gradients and \u03f5 is a small constant. It is suitable for sparse data and problems with sparse gradients, such as natural language processing tasks. However, it accumulates squared gradients, which can result in a continually decreasing learning rate.\nRMSprop (2012) [4] modified Adagrad by introducing a decay factor to control the accumulation of past gradients:\n$E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma)g^2_t$\n$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla J(\\theta)$\nThis is popular in training recurrent neural networks and models with non-stationary objectives. It requires tuning of the decay factor and learning rate.\nAdam (2014) [5] combined the advantages of Adagrad and RMSprop by using moving averages of the gradient and its square:\n$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$\n$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g^2_t$"}, {"title": "3 Implementation of AdamZ Optimiser", "content": "The AdamZ optimiser is an advanced variant of the traditional Adam optimiser, designed to provide more adaptive learning rate adjustments during the training process. This optimiser addresses two common issues faced in optimisation: overshooting and stagnation. Overshooting occurs when the learning rate is too high, causing the optimiser to miss the optimal point, while stagnation happens when the learning rate is too low, resulting in slow convergence or getting stuck in local minima.\nThe motivation behind AdamZ is to enhance the flexibility and robustness of the learning process by dynamically adjusting the learning rate based on the behaviour of the loss function. Traditional Adam, while effective, can be deficient in responsively adapting the learning rate given with dynamically changing landscapes of the loss function, leading to inefficient convergence. AdamZ introduces mechanisms to detect and mitigate these issues by adjusting the learning rate in response to overshooting and stagnation, thereby improving the optimiseradaptability and efficiency.\nThus, AdamZ incorporates additional hyperparameters that enable it to respond to the training dynamics:\n\u2022 Overshoot Factor ($\\gamma_{over}$): Reduces the learning rate once overshooting has been detected, preventing the optimiser from overshooting the minimum.\n\u2022 Stagnation Factor ($\\gamma_{stag}$): Increases the learning rate once loss has started to plateau thus indicating the onset of stagnation is detected, helping the optimiser to escape local minima."}, {"title": "4 Experimental Results", "content": "4.1 Objectives\nThe primary objective of these experiments was to evaluate the performance of the proposed optimiser, AdamZ, in comparison with other popular optimisation algorithms. We aimed to assess its effectiveness across two different datasets, including a synthetic dataset generated using make_circles (from sklearn.datasets) and the widely-used MNIST dataset (from torchvision).\nThese experiments provided insights into the optimiser's performance, including loss, accuracy of prediction, training duration, and overall applicability in neural network training.\n4.2 Experiment 1: Synthetic Dataset Using make_circles with a shallow neural network\nThis experiment utilised the make_circles function from sklearn.datasets to generate a synthetic dataset for binary classification tasks. The controlled nature of this dataset enabled a clear assessment of the optimiser's performance.\nThe make_circles function enabled several parameters to be specified, such as:\n\u2022 n_samples: This parameter defined the total number of samples to be generated.\n\u2022 noise: This parameter specified the standard deviation of Gaussian noise to be added to the data, which can help simulate real-world conditions.\n\u2022 factor: This was the scale factor between the inner and outer circle, determining the relative size of the circles.\nUsing this dataset, one can effectively test and visualise the performance of classification algorithms that are designed to handle non-linear decision boundaries.\nThe performance of AdamZ was compared against well-established optimisers such as Adam, SGD, and RMSprop, providing a benchmark for evaluating improvements in terms of model classification accuracy, training time, and loss\n4.3 Experiment 2: MNIST Dataset with a deep neural network\nThe MNIST dataset is a cornerstone in the field of machine learning and computer vision, widely used for training various image processing systems. \nThese experiments spanned five epochs of training, each comprising of 1,000 steps, with 100 simulations to account for randomness in parameter initialisation. These tests were conducted on a high-performance computer cluster equipped with four A100 GPUs, ensuring robust computational support for the experiments.\nAs depicted in Figure 8 and summarised in Table 3, of all the optimisers tested, AdamZ achieved the highest classification accuracy. However, it required slightly more training time, of the order of seconds, compared to the other methods. This trade-off highlights the effectiveness of AdamZ in enhancing accuracy, albeit with a marginal increase in computational time."}, {"title": "4.4 Analysis of Results", "content": "The experimental findings indicate that AdamZ achieved higher model accuracy by more effectively minimising the loss, particularly on more challenging tasks such as MNIST. However, AdamZ incurred higher computational costs, indicating a trade-off that must be considered in practical applications.\nOverall, these findings suggest that AdamZ is a promising candidate for applications requiring high accuracy and reliability in model predictions at the expense of a marginal increase in latency which could be tolerated in most applications. Future development of AdamZ could explore further optimisation of hyperparameters and potential enhancements in computational efficiency to reduce training time without compromising accuracy. The results from these experiments not only validate the effectiveness of AdamZ but also pave the way for its application to more diverse and challenging machine-learning tasks."}, {"title": "5 Conclusions and Future Work", "content": "5.1 Conclusions\nThe development and evaluation of the AdamZ optimiser have demonstrated its potential as a robust tool for enhancing neural network training. By dynamically adjusting the learning rate to limit overshooting and stagnation, AdamZ effectively improves convergence stability and model accuracy. The experimental results underscore the superior performance of this optimiser in minimising loss and achieving higher accuracy, particularly in complex datasets such as MNIST. Despite its slightly longer training times, the ability of AdamZ to maintain optimal learning rates positions it as a valuable asset in applications where precision is critical.\nThe comparative analysis with well-established optimisers, such as Adam, SGD, and RMSprop, demonstrated the AdamZstrengths in navigating the intricate landscapes of neural network training. The advanced mechanisms of the optimiser for a learning rate adjustment, guided by hyperparameters such as overshoot and stagnation factors, thresholds, and patience levels, provide a dynamically responsive but tightly controlled approach that enhances its adaptability and efficiency.\n5.2 Future Work\nFuture research will focus on several key areas to further enhance the capabilities of AdamZ. Firstly, optimising the computational efficiency of AdamZ is crucial to reduce training times without compromising optimisation performance. This might require exploring alternative strategies for dynamic learning rate adjustment or integrating more advanced computational techniques.\nAnother promising direction is the exploration of adaptive hyperparameter tuning. Developing methods to automatically adjust hyperparameters in response to the evolving dynamics of the training process could further improve the performance and ease of use of the optimiser.\nAdditionally, expanding the application of AdamZ to more diverse and challenging machine learning tasks will be a priority. This includes testing its effectiveness in different neural network architectures and across various domains, such as natural language processing and computer vision, to validate its generalisability and robustness.\nFinally, integrating AdamZ with emerging technologies, such as reinforcement learning frameworks or hybrid optimisa- tion models, could open new avenues for innovation. By leveraging the strengths of AdamZ in conjunction with other optimisation strategies, it may be possible to achieve even greater improvements in model training and performance.\nIn conclusion, AdamZ represents a significant advancement in optimisation techniques, offering a more responsive and effective approach to learning rate adjustment. Continued research and development will ensure its relevance and utility in the ever-evolving landscape of neural network training."}]}