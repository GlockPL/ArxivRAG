{"title": "TRIFFID: Autonomous Robotic Aid For Increasing\nFirst Responders Efficiency", "authors": ["Jorgen Cani", "Panagiotis Koletsis", "Konstantinos Foteinos", "Ioannis Kefaloukos", "Lampros Argyriou", "Manolis Falelakis", "Iv\u00e1n Del Pino", "Angel Santamaria-Navarro", "Martin \u010cech", "Ond\u0159ej Severa", "Alessandro Umbrico", "Francesca Fracasso", "AndreA Orlandini", "Dimitrios Drakoulis", "Evangelos Markakis", "Georgios Th. Papadopoulos"], "abstract": "The increasing complexity of natural disaster inci-\ndents demands innovative technological solutions to support first\nresponders in their efforts. This paper introduces the TRIFFID\nsystem, a comprehensive technical framework that integrates\nunmanned ground and aerial vehicles with advanced artificial\nintelligence functionalities to enhance disaster response capabil-\nities across wildfires, urban floods, and post-earthquake search\nand rescue missions. By leveraging state-of-the-art autonomous\nnavigation, semantic perception, and human-robot interaction\ntechnologies, TRIFFID provides a sophisticated system com-\nposed of the following key components: hybrid robotic platform,\ncentralized ground station, custom communication infrastruc-\nture, and smartphone application. The defined research and\ndevelopment activities demonstrate how deep neural networks,\nknowledge graphs, and multimodal information fusion can enable\nrobots to autonomously navigate and analyze disaster environ-\nments, reducing personnel risks and accelerating response times.\nThe proposed system enhances emergency response teams by\nproviding advanced mission planning, safety monitoring, and\nadaptive task execution capabilities. Moreover, it ensures real-\ntime situational awareness and operational support in complex\nand risky situations, facilitating rapid and precise information\ncollection and coordinated actions.", "sections": [{"title": "I. INTRODUCTION", "content": "PHYSICAL phenomena can lead to catastrophic events\nthat severely impact human lives [1]. With rising urban-\nization and climate change, these events become even more\ndestructive. Most catastrophe-related deaths occur in urban\nsettings like cities, towns, and suburbs [2]. To address this,\nthe proposed system aims to enhance preparedness and timely\ninformation diffusion to First Responders (FRs). Recently,\nthe incorporation of Autonomous Mobile Robots (AMRs) for\nsupporting human efforts, minimizing personnel exposure to\ndanger, and delivering vital crisis information to FRs promptly\nhas shown promising results.\nModern robots are not only capable of enduring harsh\nsituations, but they are also expendable; a luxury not affordable\nby human operators. However, a sufficient amount of auton-\nomy is required for the robots to be useful to FRs in a real\nscenario. Additionally, a remote operation mechanism is also\ncritical in cases where the autonomous mechanism is unable\nto navigate through a rough situation. Robots with this set of\ncapabilities could shield FRs from hazards during missions,\nthereby speeding up response times.\nAccording to recent initiatives and experimentation, Un-\nmanned Ground Vehicles (UGVs) and Unmanned Aerial\nVehicles (UAVs) can improve situational awareness through\nsensors. This raw data is typically processed at a centralized\nstation for FR evaluation. The Ground-Station manages off-\nboard processing using advanced deep learning (DL) models\nwith the robots' sensors. FRs at the station can then use\nan Augmented Reality (AR) interface, like smart glasses,\nto access near real-time disaster site information in a 3D\nreconstructed map. Key semantics may be annotated on the\nmap, providing a safer and clearer method for operators to\nassess a crisis situation.\nIntegrating autonomous robots into FR tasks is challenging.\nAny relevant system, including the proposed robotic one,\nrequires well-defined training activities, such as organized ses-\nsions, specialized workshops, and expert knowledge exchange,\nto advance this initiative. Additionally, a comprehensive ex-\namination of the technological, operational, organizational,\nand policy aspects of the current FR ecosystem is essential\nfor effective training. In parallel, public awareness must be\ncarefully assessed through educational programs and seminars\nto help citizens understand how to interact with AMRs during\ncrises, emphasizing that robots are meant to assist, not to\nreplace, human responders. Building a cooperative relationship\nbetween the general public and advanced technologies can\nfoster trust, ultimately contributing to a more resilient society\nin disasters.\nIn this paper, an AMR-based system is introduced, termed\n'Autonomous Robotic Aid For Increasing First Responders"}, {"title": "II. RELATED WORK", "content": "A. Research Studies\nUrban Search And Rescue (USAR) systems employ semi-\nautonomous control schemes, which incorporate hierarchical\nreinforcement learning and utilized sensors comparable to\nthose in the proposed system. These systems also include\nmanual and tele-operated fall-back mechanisms [3].\nThe literature on the use of aerial AMRs focuses on\nan extensive examination of several topics, including plat-\nforms, sensors on board, Simultaneous Localization And Map-\nping (SLAM) methodologies, terrain coverage techniques, au-\ntonomous navigation strategies, and human-swarm interfaces\n[4]. The detection of damaged infrastructure is also a subject of\nresearch, with studies investigating the use of UGV platforms\nto detect damage in simulated earthquake scenarios [5].\nIn parallel, researchers investigate Multi-Robot Task Al-\nlocation (MRTA) problems in the context of flood response\nscenarios, assessing the performance of multiple UAV robots\nsubject to range and payload constraints [6]. The development\nof custom-made robots for earthquake scenarios is also an\nactive area of research, with proposals for modular and snake-\nlike robots that are lightweight and compact in size [7],\n[8]. Moreover, researchers have evaluated the deployment\nof Unmanned Surface Vehicles (USVs) alongside UAVs and\nUGVs, which require sophisticated frameworks to manage\ndisaster scenarios [9].\nIndoor post-disaster human detection is crucial for FRs,\nand Micro-Aerial Vehicles (MAVs) possess the advantage\nof maneuvering in confined spaces. MAVs equipped with\nthermal cameras and real-time algorithms can precisely detect\nsurvivors, as well as map the scene for additional analysis\n[10].\nB. International Projects\nApart from dedicated and task-oriented research works,\nmore large-scale and integrated robotic systems for disaster\nmanagement have recently been launched.\nSILVANUS is an ongoing project focused on developing\nan integrated wildfire management platform, using UAV and\nUGV robots, public Internet data, and IoT devices [12].\nCARMA is an active project that uses off-road tracked and\nlegged UGVs to assist FRs in their operations during disaster\nscenarios. These scenarios include urban zones affected by\nearthquakes, underground parking garage fires, cargo inci-\ndents, and Chemical, Biological, Radiological, Nuclear, and\nExplosive (CBRN-E) incidents [11].\nThe CURSOR project develops a search and rescue kit that\nincludes drones, minified robotic equipment, and advanced\nsensors. This kit is designed to reduce the time required for\ndetection and rescue of victims trapped under debris, while\nenhancing the personal safety of the search and rescue team\n[13].\nWhile the above projects have substantially contributed to\nthe successful integration of robotics to FR procedures, the\nTRIFFID system proposes a harmonized solution of both"}, {"title": "III. PROPOSED SYSTEM", "content": "The 'Autonomous robotic aid for increasing first responders\nefficiency' system (entitled TRIFFID), whose overall func-\ntional architecture is illustrated in Fig. 1, is composed of the\nfollowing main components:\n1) A comprehensive robotic platform, composed of a hy-\nbrid autonomous UGV and a UAV robot,\n2) A centralized ground-station, which is equipped with an\nadvanced AR interface to be interacted with by a human\nsystem operator,\n3) A custom wireless communication infrastructure, which\nwill interconnect at low latency the robots, the FR field\ncrew and the Ground-Station, and\n4) A smartphone app to be carried by each member of the\nhuman FR field crew\nThe system is set up with a ground-station, a UAV launch\npad, a UGV, and a remote system operator located near the\nlocal FR Base-of-Operations (BoO) at a mission's disaster\nsite. The UGV accompanies the field crew within the disaster\nscene. The ground-station reports to the local Disaster Scene\nCommander (DCS), who directly supervises the BoO. Each\nfield crew member carries a smartphone equipped with the\nsystem's application, offering different functionalities for: i)\nthe field crew leader and ii) multiple field crew members.\nThe unified TRIFFID system aims to be seamlessly inte-\ngrated into FR operational procedures. Its initial deployment\nbegins when the FR team arrives at the disaster site and\nestablishes a BoO. Immediately, local private wireless commu-\nnications infrastructure is set up, deploying network hotspots\nin strategic locations to cover the disaster area wirelessly. This\ntriggers the following four events:\n1) The operator interacting with the TRIFFID ground-\nstation initiates the system, which receives and analyzes\nEarth Observation (EO) data in order to construct a\npreliminary, semantically annotated area map, if Internet\nconnectivity is available at the BoO. This provides prior\nknowledge for understanding the damage and delimiting\nthe concerned area. If no Internet connectivity is avail-\nable, this preliminary map is manually provided by the\nresponsible authorities.\n2) The FR field crew activates their TRIFFID smartphone\napp, which is updated with the preliminary area map as\nsoon as it is ready.\n3) The ground-station operator sets any high-level mission\ndetails (e.g., flight path, including waypoints, terrain\navoidance, no fly zones, etc.) to be issued to the robots.\n4) The UAV is deployed to aerially survey the disaster area\nand, thus, to update the semantic area map with real-time\ninformation.\nAt this stage, a preliminary map is developed for au-\ntonomous navigation, while the system operator plans UAV\ntasks through high-level commands at the ground-station.\nSimultaneously, the FR field crew prepares for deployment.\nAfter the UAV returns for battery recharging and the FR crew\ndeploys to the disaster site with the autonomous UGV, the\nground-station starts receiving data from the UGV's sensors.\nA centralized 3D semantic area map is gradually created from\nthis data, with updated versions periodically broadcasted to\nthe TRIFFID smartphone app used by the FR field crew. The\nUGV typically follows the FR crew, but they can issue high-\nlevel commands from a predefined set, such as autonomously\nmapping, delivering supplies, searching for civilians, or check-\ning for hazards. Human-Robot Interaction (HRI) is mate-\nrialized via gestures or speech, with the smartphone app\nas a backup. The FR crew continues operations, while the\nUGV autonomously pursues its goal. If the UGV faces path\nchallenges due to debris, it alerts the ground-station, prompting\nthe remote operator to dispatch the UAV for a targeted aerial"}, {"title": "IV. ROBOT-POWERED POST-DISASTER RECONNAISSANCE", "content": "This section provides detailed information on the main\ncomponents of the proposed system. To facilitate autonomous\nnavigation and communication with the Base of Operations\n(BoO), advanced algorithms will be explored. The semantic\nperception of the deployed robots, as well as their interactions\nwith FRs and victims, will be analyzed. The system's overall\ntechnical architecture is depicted in Fig. 2.\nA. Navigation and Communication\nDisaster scenarios, including wildfires, floods, and earth-\nquakes, necessitate strong navigation-related modules. These\ninclude i) mission and task planning, ii) autonomous navi-\ngation techniques, iii) safety management to prevent harm\nand to ensure fault recovery, and iv) a low-latency, private,\nand heterogeneous wireless communications infrastructure for\ncontinuous interaction among the system's components.\n1) Mission and Task Planning: The initial module, 'Mis-\nsion and task planning,' aims to autonomously organize the\ntask plan for the system's robots, based on disaster site in-\nformation and high-level mission details provided by humans.\nThese details may include goals, no-fly areas, waypoints, and\nterrain avoidance zones, and can be communicated via the\nBOO, direct HRI, or the FR smartphone app. The module\nwill dynamically decompose goals into smaller tasks and will\nadapt the plan if necessary or if goals change. A three-layer\nstructure will be supported: i) a top layer for mission planning,\nmanaging the current mission according to the FR strategy and\nspecifications, ii) a global motion planning layer of each robot,\nintegrating mission planner requests with robot capabilities\nto define exploration, search, and coverage behaviors in the\nassigned zone, and iii) a traversability and local navigation\nsub-module [14] shall analyze terrain traversability and ve-\nlocities. Pre-defined concepts of operations (CONOPS) for\nemergency response will optimally exploit the robotic platform\nand inserted into the ground station's core one. UGV tasks\nwill be managed through behavior trees [15] to enable on-\nboard decisions based on the current scenario belief, while\nalso considering external inputs from the FRs or the operator.\n2) Autonomous Robot Navigation: The second module,\n'Autonomous Robot Navigation,' will implement navigation\nalgorithms for the system's robots drawing inspiration from\nprevious work [16], [17] but, this time, considering the\ndynamic nature of disaster sites. It will execute the task\nplan derived by the 'Mission and Task Planning' module\nat a low level. Methodologically, it will build on previous\napproaches [18], [19] to create a reliable, real-time navigation\nsubsystem integrating perception, planning, control, and local-\nization. This includes: i) one-shot harsh terrain traversability\nmodeling using sensor data and local 3D mapping with the\nUGV's LiDAR, extending the method from [20] to account\nfor noise, terrain elevation probability distribution, and neural\nnetwork classification of similar ground types. ii) Dynamic\npath planning and obstacle avoidance by adapting a sliding-\nwindow non-linear Model Predictive Control (MPC) method\nfor trajectory prediction, incorporating obstacles as constraints\n[21], to ensure safe UVG navigation in harsh environments. iii)\nHigher-level robot behavior structuring using behavior trees\n[22] to integrate navigation with high-level task management,\nsuch as HRI tasks. The current scene representation will be\nobtained from the 'Non-visual Data Analysis and Information\nFusion' module. The UGV robot will be the most challenging\nimplementation, as navigation is more straightforward for the\nUAV; thus, off-the-shelf components shall be adopted and\ncustomized for the UAV. Both UGV and UAV methods will be\ntuned for harsh conditions (e.g., smoke, dust, humidity, high\ntemperature, etc.).\n3) Safety Monitoring and Fault Recovery: The 'Safety\nmonitoring and fault recovery' module evaluates if a UGV or\nUAV can achieve a goal or should be aborted and returned to\nits base. It will include fault diagnostics and failsafe routines,\nextending previous methodologies [23] to address redundancy\nand heterogeneity in sensing and task execution. The module\nwill anticipate and detect failures, adapting behavior for safety.\nThis will involve: i) monitoring robot health via hardware\ndrivers, ii) detecting failures by comparing with nominal\ndata, triggering alarms based on severity, and iii) recover-\ning modules through resiliency logic. Redundancy will be\nused for switching problematic modules to functional ones\nand for reinitializing the failed system. Component status\nwill be communicated to the 'Mission and task planning'\nand 'Autonomous Robot Navigation' modules for mission\nadjustments. Decisions and health reports will be sent to the\nground station for remote checks. In case of serious errors with\ncommunication loss, locally executed behavioral fail-safes will\nbe activated. In this way, FRs and the operator are notified of\nerrors to switch to manual teleoperation, if desired.\n4) Communications: The fourth module, 'Communica-\ntions', will create a private low-latency wireless network.\nA heterogeneous architecture will integrate 4G, Wi-Fi, and\npossibly 5G, using existing disaster site infrastructure for\nredundancy. Network hotspots will be strategically deployed as\nneeded. Security will be prioritized by design, and low latency\nwill be improved with dynamic load balancing algorithms [24],\nprioritizing real-time video over non-critical data. The network\nwill switch to alternative options if a network type becomes\nunavailable, with real-time management and monitoring tools\nfrom the ground-station. Scalability for new technologies will\nbe a key consideration, optimizing communications efficiency\nthrough diverse connectivity options.\nB. Perception and Human-Robot Interaction\nThis component aims to improve real-time deep neural net-\nworks (DNNs), knowledge graphs (KG), and augmented real-\nity (AR) techniques for robot perception [25]. These enhance-\nments will facilitate HRI, human-computer interaction (HCI),\nand centralized information fusion. Consequently, both robots\nand the ground station operator will achieve a semantically rich\nand precise understanding of the disaster site. The following\nmodules will be developed: i) visual semantic environment\nmapping using sensors and Earth Observation (EO) analysis,\nii) non-visual sensor data analysis, denoising, geolocalization,\nand multimodal information fusion at the ground station, iii)\nverbal communication and gesture recognition for HRI in the\nfield with FRs and civilians, suitable for harsh conditions, and\niv) advanced AR-based interactive visualization for the ground\nstation operator.\n1) Visual Scene Analysis and EO Data Analysis: This\nmodule, 'Visual scene analysis and EO data analysis,' will\ncover techniques for 3D visual SLAM using LiDAR data\nand semantic 3D SLAM from RGB camera inputs [24].\nModels for entities will facilitate the SLAM subsystem to\nreconstruct the scene [26], supporting the mission planning\nmodule. Generative approaches, like diffusion models [27],\nwill be assessed for completing missing areas within the\nderived 3D point cloud under various conditions, occlusions,\nand limited visibility. Scene semantics will be extracted by\nspecialized object detection and recognition DNNs from RGB\nand thermal inputs, relevant to use cases, such as trees, water,\nfire, smoke, vehicles, debris, victims, FRs, buildings, and pow-\nerlines. For 3D SLAM from LiDAR data, existing estimation\nframeworks, e.g., WOLF [28], based on factor graphs, will\nbe extended to aid 3D LiDAR scan fusion. In parallel, this\nmodule will also concentrate on creating DNNs for semantic\nsegmentation of wildfire burnt zones, smoke clouds, flooded\nareas, and earthquake-damaged city regions in EO data. The\nmodels will feature novel architectural changes and training\nobjectives that will adopt various learning paradigms, includ-\ning supervised, unsupervised, adversarial, and reinforcement\nlearning. This strategy seeks to extract more information from\na fixed-size dataset, by either improving test-time accuracy or\nachieving the same accuracy with a smaller, more efficient\nmodel. Moreover, in order to tackle the scarcity of large-\nscale datasets and the evolving nature of post-disaster sites,\ndata augmentation, self-supervised visual learning [29], out-of-\ndistribution detection, and domain generalization techniques\nwill be applied.\n2) Non-visual Data Analysis and Information Fusion:\nThe second module, 'Non-visual data analysis and informa-"}, {"title": "V. USE-CASES", "content": "Recent research [42] shows that FRs need credible disaster\nscene information quickly. Common natural disaster risks\ninclude floods, extreme weather, wildfires, epidemics, and\nearthquakes [1]. Given urbanization risks, the system will\ntion fusion,' will generate, update, and maintain a merged,\nmultiview, semantically annotated 3D area map. It will use\nKalman filtering to integrate LiDAR- and RGB-based maps\nfrom each robot, considering non-simultaneous deployment.\nThe final map will be annotated with unused semantic outputs\nand will include geolocalized non-visual measurements from\nrobot sensors and FR smartphones, updated in near-real-time.\nAdditionally, this module will advance DNNs for real-time\nenvironmental sound analysis and processing, adapting speech\nrecognition for disaster site conditions. Audio cues will aid in\nsemantic annotation of the unified 3D area map, with speech\nrecognition capabilities employed in the third module.\n3) HRI and Human Behavior Analysis: The third module,\n'HRI and human behavior analysis,' will utilize real-time\nDNNs for semantic recognition of FR and civilian emotions,\nactivities, and gestures [30] under disaster site conditions, like\nocclusions and low visibility. Multimodal generative learning\nstrategies will address these issues, focusing on distress signs.\nNearby human motion and intention will be predicted for\nlong-term horizons [31] using a stratified Transformer DNN\nfor efficient spatiotemporal analysis [32]. Sophisticated train-\ning objectives and architectural modifications (Transformer,\nLSTM, CNN) will be explored to enhance accuracy [33], [34].\nOutputs will integrate with the current semantic 3D area map\nto populate a Knowledge Graph (KG) using a novel ontology,\nfacilitating real-time KG reasoning for contextualized robot\nbehavior near humans [35]. To achieve this, the IEEE RAS\nontology [36] shall be extended by formalizing representations\nof knowledge about health state and needs of target humans,\nphysiological state or current operational status of the FRs.\nThe KG engine will adapt robot responses to HRI inputs via\nvisual gesture recognition [37] and LLM-enabled verbal com-\nmunication. A pretrained LLM will be fine-tuned for disaster\nsite conditions [38], allowing natural-language instructions to\nthe UGV by FR crew members [39] and verbal assessments\nof stranded civilians' needs. Moreover, the KG will selectively\nenrich/update the 3D map online and vice versa, extending\nprevious work [40].\n4) Intuitive AR-based Human-Computer Interfaces: The\nfinal module, 'Intuitive AR-based human-computer interfaces,'\nwill provide the fully annotated 3D semantic area map from\nthe 'Non-visual data analysis and information fusion' module\nto the ground-station operator. This will be performed through\na real-time interactive visualization tool offering multiple\nviews, accessible via an advanced mobile AR interface and\na backup desktop Graphical User Interface (GUI). Both will\nmaintain connections to updated maps, with the AR interface\noptimizing situational awareness and cognitive load manage-\nment for efficiency. Super-resolution approaches in the AR\nsystem will enhance visual fidelity [41].\nbe demonstrated through three use-cases for European urban\npopulations. In this context, TRIFFID is using a robotic plat-\nform as a force multiplier to support FRs, improve situational\nawareness, and enhance operational efficiency. This approach\nreduces risks by minimizing human exposure and optimizing\nresource allocation. Fig. 3 shows representative images for\neach use case that the TRIFFID system will be designed to\naddress.\nA. Wildfire in Suburban Environments\nA forest fire is nearing a highly explosive industrial facility\nwith three active fronts and a path towards the plant. The\nsituation is worsened by smoke and winds over 70 km/h.\nFirefighters need a near-real-time semantic map from the\nproposed system, which highlights access paths, roads, tem-\nperature, plant dehydration, and wind speed and direction.\nReconnaissance occurs in two stages: a UAV collects aerial\ninformation, and a UGV, part of the FR field crew, operates au-\ntonomously for collecting detailed data. The TRIFFID mission\nmust be executed quickly before the fire reaches the facility.\nThe semantic map, generated in near-real-time, is accessible\nat the BoO for the remote operator and through a smartphone\napp for the FR crew.\nB. Urban Flood\nAn ongoing dual challenge of urban flooding and a crisis\nat a chemical plant, worsened by flood-compromised cooling\nsystems, is occurring. There is a risk of industrial spills\nof hazardous gases. Initially, the UAV surveys flooded ar-\neas, identifying hazards, like submerged vehicles, collapsed\nstructures, and downed power lines. It also detects stranded\ncitizens and highlights the urgent situation at the chemical\nplant, by identifying cooling water loss. The BoO relays\nrelevant near real-time updates and instructions to the FRs\nvia their smartphone app. In the second stage, the UGV,\nwith a secondary FR field crew, safely delivers supplies to\nthe primary FR crew, including an emergency cooling system\ncomponent, using the aerial overview. The primary FRs then\nnavigate within the flooded areas to reach both the affected\ncommunity and the chemical plant. They periodically send\nthe UGV for autonomous advanced reconnaissance to detect\npotential spilled hazardous gases and stranded citizens. As\nthey proceed, the FRs rescue victims and restore the cooling\nsystems using the UGV's supplies. Throughout this stage,\nthe UAV is occasionally redeployed in a targeted manner to\nprovide updates on emerging hazards.\nC. USAR after Earthquake\nA team of FRs is dispatched to conduct an urban search and\nrescue operation in the chaotic and hazardous aftermath of an\nearthquake. The FR field crew deploys the UGV to semanti-\ncally map a critical city area by navigating through debris and\nselecting the safest and most efficient route, while avoiding\nobstacles. Concurrently, the UAV performs autonomous aerial\nsurveillance to identify survivors and to assess damages,\nfacilitating and enabling mapping requests for specific areas.\nCritical objects, such as injured individuals, fire, water sources,\nand obstacles requiring removal, are automatically identified.\nThe semantic map, dynamically created in near real-time, is\naccessible at the BoO for the operator's interaction, but it\nis also available to the FR crew through their smartphone\napplication."}, {"title": "VI. CONCLUSION", "content": "This paper presents a comprehensive system for integrat-\ning autonomous mobile robots into FR operations, focusing\non enhancing reconnaissance during disasters. The TRIFFID\nsystem combines unmanned aerial and ground vehicles with\nadvanced AI tools to create a unified technical framework\nthat significantly boosts situational awareness and operational\nefficiency. By integrating state-of-the-art technologies in nav-\nigation, perception, and human-robot interaction, the system\nenables autonomous reconnaissance, while maintaining hu-\nman oversight. The supported use cases of wildfires, urban\nfloods, and post-earthquake scenarios demonstrate the system's\nadaptability and potential impact across various disasters.\nTRIFFID will provide FRs with real-time semantic mapping,\nautonomous navigation, and augmented reality interfaces, ad-\ndressing operational needs and reducing personnel exposure\nto hazards.\nRegarding future steps, successful implementation of this\nsystem could mark a significant advancement in disaster\nresponse capabilities. The modular architecture ensures adapt-\nability to different scenarios, while the focus on human-robot\ninteraction and safety monitoring guarantees reliability for\nreal-world deployment. As climate change and urbanization\nincrease the frequency and severity of natural disasters, sys-\ntems like TRIFFID will become increasingly valuable. Future\nresearch should focus on extensive field testing, refinement\nof human-robot interaction interfaces, and development of\nadditional use cases. The integration of robotic platforms into\nstandard emergency response protocols signifies a promising\ndirection for enhancing the effectiveness and safety of FR\noperations in challenging disaster scenarios."}]}