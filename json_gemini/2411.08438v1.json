{"title": "Towards Optimizing a Retrieval Augmented Generation using Large Language Model on Academic Data", "authors": ["Anum Afzal", "Juraj Vladika", "Gentrit Fazlija", "Andrei Staradubets", "Florian Matthes"], "abstract": "Given the growing trend of many organizations integrating Retrieval Augmented Generation (RAG) into their operations, we assess RAG on domain-specific data and test state-of-the-art models across various optimization techniques. We incorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble Retriever, and In-Context-Learning, to enhance the functionality and performance in the academic domain. We focus on data retrieval, specifically targeting various study programs at a large technical university. We additionally introduce a novel evaluation approach, the RAG Confusion Matrix designed to assess the effectiveness of various configurations within the RAG framework. By exploring the integration of both open-source (e.g., Llama2, Mistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer valuable insights into the application and optimization of RAG frameworks in domain-specific contexts. Our experiments show a significant performance increase when including multi-query in the retrieval phase.", "sections": [{"title": "1 INTRODUCTION", "content": "Given the recent surge of Large Language Models (LLMs), they have found application in many industrial use cases. Practitioners in various domains are increasingly eager to harness the power of LLMs in their work and operations. However, the use of LLMs in a domain-specific context is not straightforward, as these models are predominantly trained on publicly available data, making them less suited for specialized domains. This challenge led to the emergence of Retrieval Augmented Generation (RAG) frameworks, which offer a solution by enabling the integration of domain-specific data with the expansive knowledge base of LLMs, thereby enhancing their utility and relevance in various applications.\nWe opt for a study program use-case targeting university students. The data distribution of this use case is potentially different from what most LLMs are normally trained on, hence making it a good test bed for testing RAG performance as well as its various optimizations. The rapid development in all areas of research has given rise to many interdisciplinary and multifaceted university study programs, students are often left overwhelmed with many different programs to"}, {"title": "2 RELATED WORK", "content": "Early approaches to RAG involved relatively simple methods of retrieving relevant documents based on the query and then processing these documents to come up with a response to a given question [6]. However, recent advancements have seen more sophisticated integration of retrieval and generation processes. Notable implementations of RAG have demonstrated the ability to dynamically retrieve and incorporate relevant information during the generation process, thereby significantly enhancing the quality and relevance of the generated text [15]. These advancements have been facilitated by improvements in both the retrieval mechanisms, which have become more efficient and effective at finding relevant information, and the generative models, which have become better at integrating and contextualizing the retrieved information [5]. A recent survey [9] separates RAG approaches into naive RAG and advanced RAG. The naive RAG approach follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a \u201cRetrieve-and-Read\u201d framework [29]. On one hand, advanced RAG introduces specific improvements to enhance the retrieval quality, by employing pre-retrieval and post-retrieval strategies. Pre-retrieval strategies include query rewriting with an LLM [19] or query expansion methods like HyDE [8], which generates a hypothetical response to the query first and then uses the responses to search the database. On the other hand, post-retrieval methods focus on selecting the most essential information from retrieved documents, since feeding all of them at once to the LLM can lead to information overload and noisy context. This includes reranking the retrieved documents with neural models [10] or summarizing the context of retrieved documents before passing them as context [3]. Our study builds on top of these advancements by conducting a comprehensive evaluation of the usefulness of recent retrieval augmentations like multi-query, child-parent, and ensemble retrieval, as well as the generation technique in-context learning.\nRetrieval-augmented generation systems can be deployed for a wide variety of NLP tasks involving text generation. The most popular is open-domain question answering [23], where the task is to answer a given question with no provided context, i.e., the system has to first search through large knowledge bases in order to find an answer. Beyond QA, RAG can also be used for generative tasks like machine translation by retrieving example sentences from a corpus in target language [4], or"}, {"title": "3 DATASET CREATION", "content": "The dataset consisted of study program descriptions and requirements by scraping and cleaning 72 university study program descriptions to be used a representative of a domain-specific dataset. We also generated an evaluation set consisting of question-answer and Context triplets.\nThe origin of documents used for corpus creation are the official websites of a large technical university in Germany. Every document describes a certain study program at the bachelor's or the master's level, covering various fields like natural sciences, engineering, and management. The documents contain information such as the name and overview of the study program, admission prerequisites, coursework, examinations, graduation requirements, credits earned, study fees, deadlines, etc. All the study program descriptions were scraped from the official university website with the Python library BeautifulSoup and saved in a JSON format. In total, there were 72 different study programs, with parallel text written in both German and English.\nOnce the websites were scraped and a corpus of documents for each study program was created, a representative test set of questions that covered university students' information needs was constructed. This process was done semi-automatically by combination of GPT-4 and human-in-the-loop by scrapping frequently asked questions and also generation some new questions with the help of GPT-4. The whole process resulted in 500 QA pairs that served as the evaluation set. To ensure the quality and relevance of the generated QA pairs, the two authors manually went over all the questions and filtered the dataset down to 200 QA pairs. The questions that were kept aimed to cover a diverse range of students' personalized information needs [17]. Additionally, we consulted with students from both bachelor's and master's levels and inquired about the most common questions they have asked over the years related to their program's regulations. All 200 answers were checked and any observed errors that stem from GPT generation were manually corrected.\nThe questions cover the most common information needs of the students, focusing on factual knowledge such as duration of the study program, total credits required to graduate, and admission requirements to programs; but also the more complex questions that require reasoning over multiple parts like future job prospects, knowledge gained in the program, or the difference in scope between to programs."}, {"title": "4 RAG FRAMEWORK", "content": "This Framework is designed to explicate both the theoretical underpinnings and practical implementations of the RAG framework, encompassing both open-sourced and closed-sourced models in English and German. Key modules such as the Multi-Query module (Generation Phase), Child-Parent Retriever and Ensemble Retriever (Retrieval Phase), and In-context-learning (Generation Phase) are discussed to highlight their roles in enhancing the framework's performance. Each module's inclusion reflects strategic choices made to enhance the framework's efficiency and effectiveness in generating contextually relevant answers."}, {"title": "4.1 Models", "content": "We incorporated both open-source models such as Llama 2 [24] including llama2:7b-chat and 1lama2:13b-chat, and Mistral AI [13] specifically the mistral:7b-instruct version. Given the success of closed-source OpenAI models, we included gpt-3.5-turbo-1106and GPT-4-0125-preview in our RAG framework due to their performance in language comprehension, creativity, and complex query handling."}, {"title": "4.2 Optimization Modules", "content": ""}, {"title": "4.2.1 Pre-retrieval Phase", "content": "We follow a two-step approach for this such that we first try to find the respective study program and then the topic (Costs, Required Language Proficiency, Type of Study, etc) matching the student's query. We Identify both through an LLM-based filtering mechanism, that is later used in tandem with the Retriever. The selection of a study program is executed by correlating the user's query with a predefined list of programs. Combining the user query and the possible study programs in the following prompt template yields the answer."}, {"title": "4.2.2 Retrieval Phase", "content": "We use Multi-Query 1, a module utilized to generate variations of the user's original question, aiming to explore different formulations that could prompt the LLM to provide a broader range of topics as context during the Generation Phase. In our implementation, the study program remains constant; only the user's question is rephrased. We use a prompt-based approach with GPT-4 to generate similar queries. This strategy ensures we can elicit more detailed or varied information relevant to the same study program without altering its core focus. Secondly, through the Child-Parent Retriever 2, we distinguish between two types of vector stores: the \"memory store\" for child chunks and the vectorstore for parent chunks, each tailored for different granularities of data. The memory store ingests data segmented into 300-character chunks, while"}, {"title": "4.3 Generation Phase", "content": "We use In-context learning, which allows LLMs to learn new tasks on run-time through examples passed as part of the input prompt. We explore three-shot learning in our experiments to evaluate if it would improve the performance of an RAG framework. A considerable challenge was selecting three Question-Answer pairs that exhibit a broad semantic diversity to prevent model bias towards the domains of the examples provided."}, {"title": "4.4 Evaluation", "content": "The growing importance of RAG-based applications requires a detailed evaluation method that separates Retrieval Quality and Generation Quality."}, {"title": "4.4.1 HIT Rate", "content": "Hit Rate measures the proportion of queries for which the correct answer is found within the top-5 retrieved documents. Scoring a hit implies not only detecting the correct document (study program) but also the topic of interest within that document."}, {"title": "4.4.2 Reference-based Evaluation", "content": "We also employed some reference-based evaluation metrics like ROUGE[16] metric, which compares generated answers with a reference text, and assesses content overlap through n-grams and other sequences. Unfortunately, ROUGE's approach may miss the mark in recognizing semantically accurate or contextually fitting answers that don't exactly replicate reference phrasing, pointing to a need for metrics that evaluate semantic and contextual accuracy in RAG systems. Hence, we also calculate the contextual similarity between the reference and generated answer through BERTScore[27]"}, {"title": "4.4.3 LLM-based Evaluation", "content": "State-of-the-art research revolves around evaluating LLM-generated text across multiple features [25, 28]. We explore the concept of using LLM as an evaluator as described in the G-eval[18] on a scale of 1 - 5 across Relevance, Coherence, Fluency, and Faithfulness."}, {"title": "4.4.4 RAG Confusion Matrix", "content": "The traditional Confusion Matrix is a fundamental tool in machine learning for evaluating the performance of classification models. In the context of RAG systems, the Confusion Matrix concept is adapted to assess both the retrieval and generation phases of these models. We derive the confusion matrix for the same features as the ones evaluated by LLM. We consider an answer to be acceptable if the evaluation score assigned is above a threshold. Thus, the RAG Confusion Matrix evaluates:\nTrue Positive (TP): Relevant do is retrieved and an acceptable response is generated,\nTrue Negative (TN): Relevant document is not retrieved and an acceptable response is not generated\nFalse Positive (FP): Relevant document is not retrieved and an acceptable response is generated\nFalse Negative (FN): Relevant document is retrieved and an acceptable response is not generated"}, {"title": "5 RESULTS AND DISCUSSION", "content": "Given the information related to a study program, we asked LLM to generate question-answer pairs from it. After manual curation, 200 pairs were used for evaluation involving 96 different experiments, using a variety of LLMs, in both German and English.3"}, {"title": "5.1 HIT Rate", "content": "We measure the effectiveness of a model in extracting relevant textual information from a dataset by calculating the Hit Rate for each experiment on a subset of 82 samples. Table 2 shows that any module iteration excluding the Multi-Query feature performs significantly worse than those that incorporate it, highlighting the critical role of Multi-Query in enhancing retrieval effectiveness. Furthermore, only the closed-sourced models show competence in the German language while all open-source models except Mistral struggle with it."}, {"title": "5.2 LLM as an evaluator", "content": "LLM-based evaluation metrics are known to perform on par with human evaluation [2]. Since it is costly to perform human evaluation over such a wide range of heuristics, we employ GPT-4 for a proxy evaluator to narrow down the search space and hence find the optimal configurations for RAG. To keep our analysis practical and cost-effective, we included a sample of 20 answers, half with matches to the correct context (Match) and the other half to incorrect context (No Match)."}, {"title": "5.2.1 Faithfulness over others", "content": "For our use case, Faithfulness holds slightly more weight than other metrics. As seen in Table 3, configurations incorporating the Ensemble Retriever demonstrated superior performance, with several configurations achieving average scores above 4, in both English and German. The most consistently high-performing combinations involve the Multi-Query and Ensemble Retriever, with or without the addition of ICL."}, {"title": "5.2.2 Confusion Matrix", "content": "The proposed confusion matrix helps us decouple LLM's generation abilities from the retriever and provides a deeper insight into both modules of RAG. Such an analysis helps us understand which parts of the retriever or generator needs to be improved. Based on"}, {"title": "5.3 Further Enhancements and Evaluation of RAG", "content": "Having found the top 3 optimal RAG configurations in the previous set of experiments, we tried to optimize them. For the retriever part, our optimization efforts revolved around the filtering steps in the pre-retrieval phase. We discovered that allowing the LLM to freely predict the study program instead of constraining it to a predefined list helped increase the hit rate. We compute LLM-based evaluation scores, ROUGE, and BERTScores in Table 5 which shows the leading performance of GPT-4. ROUGE and BERTScore reflect GPT-4's its strong ability to generate responses that closely align with the reference texts, both in detail and semantic relevance, indicating proficiency in producing contextually accurate and engaging content. In general, all models produce somewhat fluent and coherent answers with some discrepancies in Faithfulness and Relevance. An answer can be faithful without being relevant but not the other way around. An example of this can seen in Mistral 7B which has a higher Faithfulness score but comparatively low Relevance scores. Due to the limited data sample, definitive conclusions are challenging to draw. GPT-4, however, maintained similar performance levels across all metrics. This consistency is noteworthy, especially when considering the divergence in evaluations between human evaluators and GPT-4 for the optimized RAG frameworks. This final interpretation aligns well with the findings that the performance of Llama 2 and Mistral dropped compared to the original RAG framework."}, {"title": "5.3.1 Correlation between LLM-based and Human Evaluation", "content": "We provide comparison of the evaluation scores between the normal/improved framework with GPT-4/human evaluation in Table 5 calculated on a subset of 30 samples. Llama 2 and Mistral didn't align with human preferences as well as GPT-4. This could indicate that GPT-4, having processed vast amounts of data, can present information in a manner more appealing to humans."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "During our experiments, we investigated various optimizations over a RAG framework while employing Faithfulness as the most prominent evaluation criterion. We evaluated the effectiveness of the Multi-Query, Child-Parent-Retriever, Ensemble Retriever, and In-context learning. Additionally, we introduce two novel ideas; first, the usage of LLM-based filtering in the pre-retrieval phase that aids in the identification of the correct study program & topic, and second the adaptation of the confusion matrix for RAG evaluation. Our initial experiments show a significant improvement in Hit Rate with the incorporation of Multi-Query techniques. Conversely, as per the LLM-based evaluation of the generated answer, Ensemble Retriever showed remarkable effectiveness in terms of Faithfulness, particularly when paired with the Multi-Query. While GPT-4 is a clear winner, there are notable competitors such as open-source Llama 2 13B for the Retrieval Phase, leveraging Multi-Query, In-Context-Learning, and Ensemble Retriever. For the Generation Phase, the Original Mistral 7B with the same module iteration outperforms other models. Lastly, we found discrepancies between Human and LLM-based evaluation as the human annotators as the humans almost"}]}