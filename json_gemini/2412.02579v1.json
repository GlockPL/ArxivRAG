{"title": "FACTORED SPACE MODELS: TOWARDS CAUSALITY BETWEEN\nLEVELS OF ABSTRACTION", "authors": ["Scott Garrabrant", "Magdalena Wache", "Sam Eisenstat", "Matthias G. Mayer", "Leon Lang", "Holger Dell"], "abstract": "Causality plays an important role in understanding intelligent behavior, and there is a wealth of\nliterature on mathematical models for causality, most of which is focused on causal graphs. Causal\ngraphs are a powerful tool for a wide range of applications, in particular when the relevant variables\nare known and at the same level of abstraction. However, the given variables can also be unstructured\ndata, like pixels of an image. Meanwhile, the causal variables, such as the positions of objects in the\nimage, can be arbitrary deterministic functions of the given variables. Moreover, the causal variables\nmay form a hierarchy of abstractions, in which the macro-level variables are deterministic functions\nof the micro-level variables. Causal graphs are limited when it comes to modeling this kind of\nsituation. In the presence of deterministic relationships there is generally no causal graph that satisfies\nboth the Markov condition and the faithfulness condition. We introduce factored space models as an\nalternative to causal graphs which naturally represent both probabilistic and deterministic relationships\nat all levels of abstraction. Moreover, we introduce structural independence and establish that it is\nequivalent to statistical independence in every distribution that factorizes over the factored space.\nThis theorem generalizes the classical soundness and completeness theorem for d-separation.", "sections": [{"title": "1 Introduction", "content": "Learning causal relationships plays a central role in human intelligence [24] [12], and learning a causal model is\nnecessary for any system that can generalize out of distribution [21]. Therefore, a rigorous mathematical theory of\ncausality is key to understanding intelligent behavior.\nThere is a large body of literature about the mathematical foundations of causality [17], a highly successful area of\nresearch, most of which is based on causal graphs. An important feature of causal graphs is that they can encode\nstatistical independence in their structure via d-separation [15]. It is commonly assumed that a graph G is a useful model"}, {"title": "2 Related work", "content": "In this section we review the related work on causal abstractions, and on representing deterministic relationships\nbetween variables. We also compare FSMs with causal graphs."}, {"title": "2.1 Causal abstractions", "content": "The causal abstractions framework [3] [1] [22] [27] shares our broad aim, that of understanding relationships between\nvariables not well modeled by causal graphs, and in particular when different variables reside at different levels of\nabstraction. In the causal abstractions framework, different levels of abstractions are conceived as different causal\nmodels. Transformations between such models are then used to relate them.\nIn our work, we seek to provide a language that can simultaneously do the work of causal models and d-separation, and\nalso express the sort of relations between variables that can occur when we speak of microscale and macroscale variables\ntogether. For example, we may describe a gas both on a very detailed level in terms of the kinetic energy of each gas\nparticle, and on a more abstract level in terms of the overall temperature of the gas. In particular, a representation for\ndifferent levels of abstractions should include deterministic relations\u2014we may want to model a macroscale variable\nas being determined by the microscale variables. One way to regard the division that exists in causal abstractions\nframework between the causal models on the one hand and the transformations on the other is that it stems from an\ninability to place variables that stand in such deterministic relations together in one causal graph. This is because no\nsuch graph can be a perfect map once we account for these deterministic relations, as we show in Section 3. We do not\ndevelop this here, but we are interested in the application of such ideas to neural network interpretability. The causal\nabstraction framework has been used in this way [10] [9]."}, {"title": "2.2 Representing deterministic relationships", "content": "In a causal graph, the nodes are random variables, and the independences between those variables are represented\nby d-separation of the nodes. To represent independences between deterministic functions of variables, we need to\nrepresent these functions of variables as their own nodes. However, as illustrated in Section 3, when we allow variables\nthat are functions of other variables, d-separation becomes incomplete. There have been various attempts to model\ncausality with variables that are deterministically related. There are approaches like Causal Constraints models [4]\nand an axiomatization of causal models with constraints [2], which extend causal models to include variables that are\ndeterministically related by a constraint.\nHowever, these approaches only consider an additional set of functional constraints on a fixed set of given variables\nrather than modeling any deterministic function of variables as its own variable. Moreover, both of these approaches\ndo not provide a criterion for independence in the same way that d-separation provides an independence criterion\nnodes in a causal graph. In contrast, structural independence on an FSM, our analog of d-separation, is a criterion for\nindependence that correctly represents independence even when variables are deterministically related.\nThere is work which does consider the independence between deterministically related variables [16], which provides a\ncausal discovery method based on independence of functions of variables. This type of independence between functions\nof variables is what we represent in FSMs. However, the existing work only applies to the special case of 2 binary\nvariables, and does not provide a general model for how to represent such independences. In contrast, FSMs can model\nsituations with an arbitrary number of discrete variables. The closest to an independence criterion for variables which\ncan have deterministic relationships is D-separation [11] (note the capital 'D' to distinguish from d-separation). But just\nlike for d-separation there are simple counterexamples for variables that are independent but not D-separated, as shown\nin Section 3."}, {"title": "2.3 Causal graphs and FSMS", "content": "Like a causal graph, an FSM models a set of probability distributions on some variables, and implies certain conditional\nindependence relations between them. Other research has also examined ways of representing probability distributions.\nIn particular, the factors of an FSM can be regarded as factors of a factor graph [26]. FSMs generalize causal graphs;\nwe can construct an FSM from a causal graph in order to represent the same independence properties, as we demonstrate\nin Section 5.2. Here, the factors are independent, which is in line with the spirit of causal modeling, as expressed by the\nprinciple of independent mechanisms [20]."}, {"title": "2.3.1 Soundness and completeness of d-separation", "content": "The theory of causal graphs provides us with the d-separation criterion [15], which characterizes which conditional\nindependence relations are implied by a causal graph. Given three sets of variables X, Y, Z corresponding to sets of\nnodes of a causal graph, X and Y are conditionally independent given Z in all probability distributions that factorize\naccording to the graph if and only if X and Y are d-separated by Z in the graph."}, {"title": "3 Causal graphs cannot capture deterministic relationships", "content": "Methods based on causal graphs are highly successful in modeling independence, intervention and counterfactuals.\nHowever, as we illustrate in this section, when there is a deterministic relationship between random variables, there\nis generally no perfect map of the probability distribution. That is, there is no graph G such that d-separation in G is\nequivalent to statistical independence.\nLet $X := (X_1, ..., X_n)$ be a vector-valued random variable, and let $Y := \\sum_{i=1}^n X_i$ be the mean of X. One can think\nof Y as an abstraction of X. Further, let Z be a variable that is causally downstream of Y, for example $Z := Y + N$\nwherein N is independent noise. Then X and Z are independent given Y (denoted $X \\perp Z | Y$), since when Y is\nknown, X provides no further information about Z. Moreover, $Y \\perp Z | X$ holds, since knowing X fully determines\nY, so Z provides no further information about Y. Furthermore, we have $Z \\perp (Y, X)$.\nThese three independence statements together contradict the intersection axiom [19] of d-separation (d-sep(X,Y |\nZ) $\\wedge$d-sep(X, Z | Y) $\\Rightarrow$ d-sep(X, (Y, Z))). Therefore, there is no graph in which the d-separations are equivalent to\nthe statistical independencies. In particular, the graph in Figure 2 is not a perfect map, because it does not reflect that\n$Y\\perp Z | X$ holds.\nEven D-separation [11], which extends d-separation to include nodes that are deterministic functions of their parents,\ndoes not cover this case."}, {"title": "4 Factored space models", "content": "In this section, we formally introduce the factored space model (FSM) framework. We define the building blocks of the\nframework, including derived variables, history, structural independence and structural time.\nDerived variables. Random variables are a central object of study in statistics and causality. Formally, a random\nvariable on a sample space \u03a9 is a measurable function $X : \\Omega \\rightarrow Val(X)$ from \u03a9 to a value space Val(X). When we\nspeak of variables in this paper, we always mean discrete random variables. Any subset of \u03a9 is called event.\nOur contribution is a better way to represent variables that are deterministically related. That is, some variables are a\ndeterministic function of other variables. We define a variable being a deterministic function of another variable as\nfollows:\nDefinition 4.1 (Derived Variable). Let $X : \\Omega \\rightarrow Val(X)$ and $Y : \\Omega \\rightarrow Val(Y)$ be two random variables, and let $C \\subseteq \\Omega$\nbe an event. Then, we say that Y is derived from X on C, or that it is a deterministic function of X on C if there is a\nfunction f: Val(X) \u2192 Val(Y), such that for all w \u2208 C, it holds that $Y (w) = f(X(w))$. We also write this as $X \\triangleright_C Y$.\nWe write $X \\triangleright Y$ as a shorthand for $X \\triangleright_{\\Omega} Y$, and say that Y is derived from X or a deterministic function of X.\nGiven multiple variables $X_1,..., X_n$, we define the variable $(X_1,..., X_n) : \\Omega \\rightarrow Val(X_1) \\times ... \\times Val(X_n)$ by\n$\\omega \\rightarrow (X_1(\\omega), ..., X_n(\\omega))$. Then, if $Y < (X_1, . . ., X_n)$, we say that Y is a deterministic function of the variables\n$X_1,..., X_n$.\nOperations on indexed families. As we make heavy use of indexed families, we introduce some operations on\nindexed families and probability distributions over sets of indexed families. Formally, an indexed family a with the\nindex set I, written as a = $(a_i)_{i\\in I}$ is a function $f_a: I \\rightarrow T$ from an index set I to a target set T of possible values.\nHowever, rather than being viewed as a function, a is treated as a collection of indexed elements $a_i := f_a(i)$. For\nexample, when we write $x \\in a$, that means there is an i \u2208 I with $x = a_i$. We define the following operations."}, {"title": "4.1 Factored spaces and factored space models", "content": "We now define two central mathematical structures of our framework - factored spaces and factored space models.\nDefinition 4.2 (Factored Space). A finite sample space \u03a9 is called a factored space if there is a finite index set I and\nfinite sets $N_i$ for all i \u2208 I such that $\\Omega = \\underset{i\\in I}{\\times} N_i$ holds. The sets $N_i$ are called the factors of \u03a9. Moreover, the random\nvariables $U_i : \\Omega \\rightarrow \\Omega_i$ with $U_i(\\omega) = \\pi_i(\\omega)$ are called the background variables of N, and we write $U = (U_i)_{i\\in I}$.\nThe elements of \u03a9 are indexed families of the form $\\omega = (\\omega_i)_{i \\in I}$ with $\\omega_i \\in \\Omega_i$. When using factored spaces for\nmodeling probability distributions, we consider those distributions that factorize over \u03a9.\nDefinition 4.3 (Factorizing Distribution). Let P be a probability distribution on a factored space \u03a9. We say that P\nfactorizes over \u03a9 if $P(\\omega) = \\underset{i\\in I}{\\Pi} P(\\omega_i)$ holds for all $\\omega \\in \\Omega$, wherein $P(\\omega_i) := P(\\pi_i^{-1}(\\omega_i))$.\nNote that P factorizes over 2 if and only if all background variables $U_i$ are mutually independent under P. As illustrated\nin Figure 3, one can think of the factors as axes, and the sample space as a hyper-rectangle along those axes. We\nremark that $U : \\Omega \\rightarrow \\Omega$ is the identity function on \u03a9, so any variable $X : \\Omega \\rightarrow Val(X)$ is a deterministic function of U.\nThis is analogous to every variable in a structural causal model [17] being a deterministic function of the independent\nbackground variables U.\nNext, we formally define the notion of a factored space model for a probability distribution P on a finite set Obs of\npossible observations.\nDefinition 4.4 (Factored Space Model). Let Obs be a finite set and let P be a distribution on Obs. Furthermore, let\n$O : \\Omega \\rightarrow Obs$ be a random variable on a factored space \u03a9. Then we say that the tuple $M := (\\Omega, O)$ is a factored space\nmodel for P if there is a distribution $P^{\\Omega}$ that factorizes over \u03a9 and satisfies $P^{\\Omega}(O = o) = P(o)$ for all $o \\in Obs$.\nWe call a variable X on \u03a9 observed if it is a deterministic function of O, and unobserved otherwise. We remark that\nevery probability distribution P over Obs has a trivial factored space model with a single factor (where \u03a9 = Obs and\n|I| = 1), but it may have many other factored space models."}, {"title": "4.2 History", "content": "In the following, we build up to a notion of structural independence of variables on factored space models which is an\nanalog of d-separation and which applies to arbitrary variables. To this end, we first define the history $H(X | C)$ of a\nrandom variable X given an event $C \\subseteq \\Omega$. The letter C indicates that this is the event we condition on. We would like\nto define the history such that it is the set of indices i of the background variables $U_i$ that X depends on if we condition\non C in a distribution that factorizes over \u03a9. One might think that the set of background variables that X depends on\nshould be defined as the smallest set of variables $U_J$ such that $U_J \\triangleright_C X$. However, this condition is not enough, since\nconditioning on C may make the background variables dependent. To see this, consider the following example.\nLet $\\Omega_1 := \\Omega_2 := \\{0,1\\}$ be the outcomes of two independent coin flips, and let P be the joint distribution over the\nfactored space $\\Omega = \\Omega_1 \\times \\Omega_2$. Then, the background variables $U_1$ and $U_2$ are the results of first and second coin\nrespectively. Since the coins are independent, P factorizes over \u03a9. Further, let $C := \\{00, 11\\}$ be the event that both\ncoins have the same result. Note that given $C$, we have that $U_1$ and $U_2$ are dependent in P. Thus, the set of background\nvariables that $U_1$ depends on given $C$, is $\\{U_1, U_2\\}$, rather than just $\\{U_1\\}$. When C is such that it does not introduce a\ndependence between $U_J$ and $U_{I\\backslash J}$ for $J \\subseteq I$, we say that J disintegrates C.\nDefinition 4.5 (Disintegration). Let $\\Omega = \\underset{i\\in I}{\\times} N_i$ be a factored space, let $C \\subseteq \\Omega$ be an event and let $J \\subseteq I$. Then J\ndisintegrates C if $C = C_J \\times C_{I\\backslash J}$.\nNote that in the previous example, $\\{1, 2\\}$ trivially disintegrates C but $\\{1\\}$ does not disintegrate C. We can now formally\ndefine the history.\nDefinition 4.6 (History, Generation). Let $\\Omega = \\underset{i\\in I}{\\times} N_i$ be a factored space with the background variables U. Let\n$J \\subseteq I$, let $X : \\Omega \\rightarrow Val(X)$, and let $C \\subseteq \\Omega$. Then J generates X given C if $U_J \\triangleright_C X$ and $C = C_J \\times C_{I\\backslash J}$. The\nhistory $H(X | C)$ of X given C is the intersection of all $J \\subseteq I$ that generate X given C. That is,\n$H(X | C) := \\underset{J \\subseteq I : J generates X given C}{\\cap} \\{J \\subseteq I : J generates X given C\\} .$\nNote that, for any event C, we have $C = C_I \\times C_{\\emptyset}$ and thus the index set I trivially disintegrates C. Moreover, for\nany variable X, the index set I trivially generates X because $U_I$ is the identity function on \u03a9 and X is a function\n$\\Omega \\rightarrow Val(X)$, so X is derived from $U_I$ on C in the sense of Definition 4.1. Therefore, the set on the right side of (1)\ncontains at least the set I.\nLemma 4.7 (History is minimal generating set). For a factored space $\\Omega = \\underset{i\\in I}{\\times} N_i$, let $X : \\Omega \\rightarrow Val(X)$, and let\n$C \\subseteq \\Omega$. Then, $H(X | C)$ is the unique minimal set which generates X given C.\nProof (sketch). We first show that generation is closed under intersection, see Lemma A.2 in the appendix. It follows that\n$H(X | C)$ generates X given C. As I is finite, $H(X | C)$ is the unique minimum of $\\{J \\subseteq I | J generates X given C\\}$.\nTherefore, $H(X | C)$ can be equivalently defined as the minimal set that generates X given C.\nShorthand notation for the history. For an event $A \\subseteq \\Omega$, we write H(A | C) as a shorthand for $H(1_A | C)$, wherein\n$1_A$ is the variable that is 1 for w \u2208 A, and 0 otherwise. When x and y are values of the random variables X and Y, we\nuse the notation H(x | C), H(X | y), H(A | y) or H(x | y). Here, x is a shorthand for the event $\\{\\omega \\in \\Omega | X(\\omega) = x\\}$\nwhich is a common abbreviation when denoting probabilities, such as $P(x | y)$. We write H(X), H(A) and H(x) as a\nshorthand for the unconditional history $H(X | \\Omega)$, $H(A | \\Omega)$ and $H(x | \\Omega)$ respectively.\nWe remark that the disintegration condition is vacuous for the unconditional history, as \u03a9 always satisfies $\\Omega =$\n$\\Omega_J \\times \\Omega_{I\\backslash J}$.\nHistory of joint variables. The following lemma formalizes the fact that if a variable $X : \\Omega \\rightarrow Val(X)$ depends on\nexactly the positions $J_1 \\subseteq I$ in \u03a9 and a variable $Y : \\Omega \\rightarrow Val(Y)$ depends on exactly the positions $J_2 \\subseteq I$, then the\njoint variable (X, Y) depends on exactly the positions $J_1 \\cup J_2$.\nLemma 4.8 (History of joint variable). Let X and Y be variables defined on a factored space $\\Omega = \\underset{i\\in I}{\\times} N_i$. For all\nevents $C \\subseteq \\Omega$, we have\n$H((X, Y) | C) = H(X | C) \\cup H(Y | C) .$\nThe proof is a straightforward application of Lemma 4.7 and can be found in Appendix A.2. Next, we introduce a\nlemma which establishes the relationship between the history of a variable X and the histories of its values $x \\in Val(X)$."}, {"title": "Lemma 4.9 (History of a variable is the union of the histories of events).", "content": "Let X be a random variable defined on a\nfactored space \u03a9. For all events $C \\subseteq \\Omega$, we have\n$H(X | C) = \\underset{x\\in Val(X)}{\\cup}H(x | C)$.\nThe proof is an inductive application of Lemma 4.8 and can be found in Appendix A.3."}, {"title": "4.3 Structural independence", "content": "In the following, we define two variables $X : \\Omega \\rightarrow Val(X)$ and $Y : \\Omega \\rightarrow Val(Y)$ to be structurally independent if they\ndepend on disjoint factors of the factored space $\\Omega = \\underset{i\\in I}{\\times} N_i$. We use our notion of history to formalize this definition\nand extend it to the situation of conditioning on a third variable. This is our analog of d-separation in Bayes nets.\nDefinition 4.10 (Structural Independence). Let X, Y, and Z be random variables in a factored space \u03a9. Then, X and\nY are structurally independent in \u03a9, denoted as $X \\perp^\\circ Y$, if $H(X) \\cap H(Y) = \\emptyset$. Moreover, X and Y are structurally\nindependent given Z, denoted as $X \\perp^\\circ Y | Z$, if we have\n$H(X | z) \\cap H(Y | z) = \\emptyset$ for all $z \\in Val(Z)$.\nWe choose the name structural independence because it represents those statistical independences which come from a\nstructure in the process that generated the distribution. For example, when throwing two fair coins $X_1$ and $X_2$, then $X_1$\nis statistically independent of the XOR variable $X_\\oplus = X_1 \\oplus X_2$, because $P(x_1, x_\\oplus) = 0.25 = P(x_1)P(x_\\oplus)$ holds\nfor all $x_1, x_\\oplus \\in \\{0,1\\}$. However, this independence is not structural because it relies on the exact parameters of the\nprocess. If one coin is slightly unfair and $P(X_1 = 0) = 51\\%$, then $X_1$ and $X_\\oplus$ are not independent anymore. In\ncontrast, $X_1$ and $X_2$ are still independent, no matter the distribution of $X_1$ and $X_2$. In that sense, the independence of\n$X_1$ and $X_2$ is a property of the process which generates a class of distributions rather than a property of a particular\ndistribution. The formal connection between statistical independence and structural independence is established in\nSection 6, where we prove that X and Y are independent given Z in all product probability distributions over 2 if and\nonly if X and Y are structurally independent given Z in \u03a9."}, {"title": "4.4 Structural time", "content": "Let X and Y be two random variables. In the following, we formalize a notion of X being before Y in the sense that X\nis always determined before Y in the process that generated X and Y.\nDefinition 4.11 (Structural Time). The structural time $\\lessdot^\\circ$ of a factored space \u03a9 compares two variables X and Y on\n\u03a9. We say that X is before Y in \u03a9, denoted $X \\lessdot^\\circ Y$, if $H(X) \\subseteq H(Y)$. We say that X is strictly before Y, denoted\n$X <^\\circ Y$ if $H(X) \\subset H(Y)$.\nWe can interpret the history H(X) as the set of sources of randomness that X depends on. If $H(X) \\subseteq H(Y)$ holds,\nthen X depends on a subset of the sources of randomness that Y depends on, which fits with the intuition that X is\ndetermined before Y. More precisely, in any process in which the values of $U_i$ become known one after the other, the\nvalue of X will always be determined before the value of Y, or at the same time.\nIn Section 5.2 we show that when we construct an FSM from a causal graph, then for variables that are nodes in a\ncausal graph, the structural time is equivalent to the ancestor relation. Structural time is more general than the ancestor\nrelation, since it is defined on all variables on \u03a9, not only the ones which are nodes in the graph.\nIn the following lemma, we show that the structural time can be equivalently expressed in terms of structural indepen-\ndence. We hold that by this lemma the name structural time is philosophically justified, but this topic is outside the\nscope of this paper.\nLemma 4.12 (Structural time and structural independence). Let X and Y be random variables on \u03a9. Then X is\nstructurally before Y if and only if $Y \\perp^\\circ Z \\Rightarrow X \\perp^\\circ Z$ holds for all variables Z on \u03a9.\nThe lemma follows from the definition of structural independence, and the proof can be found in Appendix A.4."}, {"title": "5 Bayesian networks and factored space models", "content": "In this section, we compare factored space models (FSMs) with Bayesian Networks. In Section 5.1 we compare the\nproperties of d-separation and structural independence in terms of the graphoid axioms they fulfill. In Section 5.2 we\nconstruct an FSM from a causal graph, and show that node variables in the graph are d-separated if and only if they are\nstructurally independent in the corresponding FSM."}, {"title": "5.1 Graphoid and semigraphoid axioms", "content": "Structural independence is our analog of d-separation. In this section, we compare the properties of d-separation,\nwhich forms a compositional graphoid and structural independence, which forms a compositional semigraphoid. This\ncomparison also provides an intuition as to why structural independence represents deterministic relationships correctly\nwhile d-separation does not.\nDefinition 5.1 (Compositional Semigraphoid). A semigraphoid is a set of triplets (X, Y, Z), usually denoted as\n$X \\perp Y | Z$, that satisfy the symmetry, decomposition, weak union, and contraction axioms, as listed in Table 1. A\ngraphoid [19] is a semigraphoid that also satisfies the intersection axiom. A compositional graphoid or semigraphoid\nadditionally satisfies the composition axiom.\nProposition 5.2. Structural independence is a compositional semigraphoid.\nThe proof is straightforward and we defer it to Appendix B.1.\nComposition. The composition axiom $(X \\perp Y | W) \\wedge (X \\perp Z | W) \\Rightarrow (X \\perp Y, Z | W)$ illustrates that structural\nindependence is not the same as independence: Structural independence satisfies this axiom intuitively, because if X\nand Y are influenced by disjoint sources of randomness, and X and Z are influenced by disjoint sources of randomness,\nthen X and Y, Z are influenced by disjoint sources of randomness as well. To see that statistical independence does\nnot satisfy the composition axiom, consider the following example: Let X be a message, Y be a string of uniformly\ndistributed bits, and let Z be their bitwise XOR: Z = X + Y. Then X and Y are independent, and X and Z are\nindependent, but X and (Y, Z) are dependent, so composition does not hold. This situation could be modeled with an\nFSM as follows. The variables X and Y are structurally independent, but as Z is computed from X, the histories of\nX and Z overlap and X and Z are structurally dependent, so composition is not violated. Independence is about the\nquestion \"Does X provide information about Y?\" while structural independence is about the question \"Are X and Y\ninfluenced by the same source of randomness?\u201d.\nIntersection. The intersection axiom $(X \\perp Y | Z, W) \\wedge (X \\perp Z | Y, W) \\wedge (Y \\neq Z) \\Rightarrow (X \\perp Y, Z | W)$ is\nparticularly interesting. Statistical independence does not in general satisfy the intersection axiom, though intersection\ndoes hold in those cases where P(x, y, z, w) is never zero [18]. However, we want to allow $P(x,y,z,w) = 0$.\nIn particular, if any variable is a deterministic function of another variable, there must be values x, y, z, w with\nP(x, y, z, w) = 0. The fact that d-separation satisfies intersection means that d-separation cannot represent the\nindependence relations among a set of variables, some of which are deterministically related. It is therefore an important\nproperty of structural independence that the intersection axiom does not hold.\nIn the next section, we continue our comparison of d-separation and structural independence by constructing an FSM\nfrom a causal graph."}, {"title": "5.2 From Bayesian networks to factored space models", "content": "In this section, we construct a factored space from a directed acyclic graph (DAG), and we construct an FSM from\na Bayesian network. We show that a distribution P factorizing over a DAG is equivalent to our construction being a\nfactored space model of P. We also show that in our construction, structural independence applied to node variables\nis equivalent to d-separation, and structural time applied to node variables is equivalent to the ancestor relationship.\nWe also introduce the notion of a perfect map, and use it to show that factored space models are more expressive than\nBayesian networks. We start by formally introducing Bayesian networks.\nBayesian networks. Let G = (V, E, Val) be a directed acyclic graph (DAG), wherein V is the vertex set, and E is the\nedge set. Moreover, each vertex v \u2208 V is associated with a set of possible values $Val_v$ with at least 2 elements, and\n$Val = \\underset{v\\in V}{\\times} Val_v$ is the set of all combinations of values for V.\nFor a vertex v, we write $pa(v) \\subseteq V$ for the set of parents of v, and $an(v)$ for the set of ancestors of v. If P is a\ndistribution over Val and $x_v \\in Val_v$ is a value of v, we write $P(x_v)$ as a shorthand for the probability $P(\\{ (y_u)_{u\\in V} \\in\nVal_y : x_v = y_v \\})$. We say that P factorizes over G, if for all $x = (x_v)_{v\\in V} \\in Val$, we have\n$P(x) = \\underset{v\\in V}{\\Pi} P(x_v | x_{pa(v)}).$\nFollowing the notation in [15], a pair B = (G, P) where P factorizes over G is called Bayesian network or Bayes net.\nIn the following, we show how to construct a factored space \u03a9 and a factored space model $M_G$ from G. Factors in this\nconstruction are closely related to factor graphs [26]."}, {"title": "5.2.1 Our construction of a factored space model from a Bayesian network", "content": "Given the DAG G = (V, E, Val), we now construct a factored space model $M_G = (\\Omega, O)$ for the observation space\nObs = Val. We first construct a factored space \u03a9. To this end, we define the index set I via $I := \\underset{v\\in V}{\\cup} I_v$, where for\nall v \u2208 V, we let\n$I_v := \\{ (v, x_{pa(v)}) : x_{pa(v)} \\in Val_{pa(v)} \\} .$\nIn other words, the index set I is partitioned into the sets $I_v$, and each element of $I_v$ corresponds to a possible value for\nthe vertices in pa(v). If v is a root, we remark that pa(v) = \u00d8 holds and that $Val_{pa(v)}$ contains the empty tuple () as its\nonly element. Finally, for each $i = (v, x_{pa(v)}) \\in I$, we define the set $N_i$ via $N_i := Val_v$. Then $\\Omega := \\underset{i\\in I}{\\times} N_i$ is the\nfactored space constructed from G.\nWe now define the random variable $O : \\Omega \\rightarrow Val$. To this end, we first define $X = (X_v)_{v\\in V}$, wherein $X_v : \\Omega \\rightarrow Val_v$\nis a random variable associated with each node v. We define $X_v$ recursively as\n$X_v(\\omega) := \\omega_{(v, x_{pa(v)} (\\omega))}.$ \nIn particular, for roots v we have $X_v(\\omega) = \\omega_{(v,())} \\in Val_v$. Since G is acyclic, $X_v$ is well-defined also for non-roots v.\nNote that $Val(X_v) = Val_v$ and $Val(X) = Val$. We call $M_G = (\\Omega, O)$ with $O = X$ the factored space model\nconstructed from G. This concludes the construction.\nTo show that $M_G$ is in fact a factored space model of a distribution P over Val in the sense of Definition 4.4 when P\nfactorizes over G, we first state the following lemma about the relationship between G and $M_G$."}, {"title": "Lemma 5.3.", "content": "Let $\\triangle*(G)$ be the set of distributions on Val that factorize over G and let $\\triangle^\\otimes(\\Omega)$ be the set of distributions\non $\\Omega$ that factorize over $\\Omega$. Let $\\tau: \\triangle^\\otimes(\\Omega) \\rightarrow \\triangle*(G)$ be the function defined as follows for all"}]}