{"title": "Causality-Driven Audits of Model Robustness", "authors": ["Nathan Drenkow", "Chris Ribaudo", "Mathias Unberath"], "abstract": "Robustness audits of deep neural networks (DNN) provide a means to uncover model sensitivities to the challenging real-world imaging conditions that significantly degrade DNN performance in-the-wild. Such conditions are often the result of the compounding of multiple factors inherent to the environment, sensor, or processing pipeline and may lead to complex image distortions that are not easily categorized. When robustness audits are limited to a set of pre-determined imaging effects or distortions, the results cannot be (easily) transferred to real-world conditions where image corruptions may be more complex or nuanced. To address this challenge, we present a new alternative robustness auditing method that uses causal inference to measure DNN sensitivities to the factors of the imaging process that cause complex distortions. Our approach uses causal models to explicitly encode assumptions about the domain-relevant factors and their interactions. Then, through extensive experiments on natural and rendered images across multiple vision tasks, we show that our approach reliably estimates causal effects of each factor on DNN performance using observational domain data. These causal effects directly tie DNN sensitivities to observable properties of the imaging pipeline in the domain of interest towards reducing the risk of unexpected DNN failures when deployed in that domain.", "sections": [{"title": "1. Introduction", "content": "A persistent challenge in the development and use of vision-based Al systems is dealing with the diversity of possible real-world operating conditions. Safety- and cost-critical applications require robust and reliable algorithms capable of maintaining their performance across the range of conditions they will face in their deployment environment. Recent studies [10, 18, 48, 52] have shown that despite significant developments in deep learning methods, DNNs remain susceptible to performance degradation due to challenging imaging conditions that occur naturally. Robustness audits thus play a critical role in identifying vision model sensitivities and vulnerabilities to such conditions before models are deployed in high-stakes applications. A compounding challenge is that the most demanding conditions may arise as a result of multiple interacting causes due to the external environment (e.g., lighting, weather, shadows), sensor (e.g., exposure, ISO, F-stop), and processing pipeline (e.g. auto white balance, tone mapping, denoising).\nIllustrative example: Consider the well-understood \"exposure triangle\" in natural photography where exposure time, aperture size (or F-stop), and ISO settings are adjusted to achieve the desired photographic appearance depending on lighting conditions. In limited illumination settings, captured images may contain combinations of effects such as limited depth of field (large aperture), blurring due to sensor/object motion (long exposure times), and higher noise (high ISO to account for low photon counts). Because it is not always possible to find an ideal compromise, all three distortions may occur simultaneously and in varying degrees of severity depending on the sensor settings (Fig. 1). Furthermore, attempting to offset one effect (e.g., exposure time) during imaging by adjusting one of the other settings (e.g., ISO) may only result in a change in the nature of the distortion (e.g., slightly reduced motion blur but higher noise). Similarly, changes in environmental factors, such as time of day, weather, or location, will subsequently cause exposure, ISO, and F-stop settings to have varying impact on the overall image quality. Given this potential range of complexity in real-world imaging conditions, we ask: how can we identify which imaging factors have the strongest effect on DNN robustness?"}, {"title": "2. Related work", "content": "Robustness research in deep learning for computer vision largely falls into two categories: adversarial and natural/non-adversarial robustness. We focus here on natural robustness where there is no explicit attacker and performance of the DNN is measured against challenging, naturally-occurring conditions [14]. Robustness audits in this context are often framed around distribution shift [2, 10, 38, 50, 52], out-of-distribution (OOD) [56, 61], or common corruption robustness [11, 22, 24, 33, 39, 41].\nConventional robustness auditing A majority of the natural robustness auditing methods center around the common corruptions framework [24] which initially proposed 15 classes of corruptions simulated independently at five discrete levels of severity. This approach was later expanded to other domains [12,22] and vision tasks [1,39,41]. Related methods proposed a similar evaluation on a subset of common image perturbations [32,33]. The use of metrics like mean Corruption Error [24] or robustness score [32] are useful when distortions occur independently but are not suitable for measuring the impact of individual factors of the IGP in the presence of other interacting factors. The discrete and independent nature of these simulated corruptions and robustness evaluations limits the generalizability of the results to domains where multiple corruptions often co-occur, thus increasing the risk of unexpected DNN failures in deployment.\nCausal inference for computer vision Causal inference methods have shown promise for analyzing [4,5,29,42, 55] and generating [15, 19] complex, large-scale datasets. We adopt a similar perspective here by focusing on specifying and using causal models of the IGP for robustness evaluation in contrast to methods that use causal models for representation learning [6, 26, 37,44,47,54,58-60]."}, {"title": "3. Methods", "content": "We now provide details on auditing robustness through the lens of causal inference. We first describe the specification of causal models for domain-specific image generating processes. We then provide two methods for analyzing DNN sensitivities to factors of the image generating process using the tools of causal inference."}, {"title": "3.1. Preliminaries: Causal models of the image generating process", "content": "The basis for our robustness audit is the specification of a Structural Causal Model (SCM) which explicitly codifies assumptions about the image generating process of a domain D. Formally, we start by specifying the set of major imaging factors V in the domain. Each factor $V \\in V$ corresponds to an observable property of the environment (e.g., time of day, weather, or location), sensor (e.g., exposure, f-stop, ISO), or other aspect of the imaging pipeline (e.g., white balance, compression) that impacts the formation and quality of the image X (see Fig. 2b).\nThe SCM is specified in the form of a directed acyclic graph (DAG) $G_D = (V,E)$ with variables V and directed edges $E$ where edges encode pairwise causal relationships between these variables. The existence of directed edges $(V, W) \\in E$ between any pair of factors $V, W \\in V$ is tied explicitly to the assumptions and knowledge of the domain (e.g., relationship between how exposure time influences the ISO setting in natural photography).\nUnder the Markov assumption, a factor is independent of its ancestors conditioned on its direct parents $(V \\perp anc(V) | pa(V)$ where $anc(V)$ and $pa(V)$ are the ancestors and parents of V respectively). With this assumption, any factor is determined by $V = f_V(pa(V), U_V)$ where $f_V(\\cdot)$ is the causal mechanism and $U_V$ is a latent term representing measurement noise or other independent stochasticity. For our causal audits, we require no explicit assumptions on the nature of $f_V$ or $U_V$."}, {"title": "3.2. Causality-driven robustness auditing", "content": "The objective of CDRA is to estimate the causal effects that individual factors $(V \\in V)$ of the IGP have on the performance of the DNN using image data collected (or simulated) under realistic conditions. We first choose a performance metric (e.g., accuracy) $M: \\mathcal{Y} \\times \\mathcal{\\hat{Y}} \\rightarrow \\mathbb{R}$ for labels $\\mathcal{Y}$. The prediction $\\mathcal{\\hat{Y}} = f_\\mathcal{\\hat{Y}}(X,U_X;\\theta)$ is determined by the task DNN $f_\\mathcal{\\hat{Y}}$ parameterized by $\\theta$ and trained to approximate $P(Y|X)$. We assume that $\\theta$ is fixed so that $f_\\mathcal{\\hat{Y}}$ is deterministic and the latent noise term can be omitted. We augment our causal graph by adding edges from $X \\rightarrow \\mathcal{\\hat{Y}}$, $X \\rightarrow Y$, $\\mathcal{\\hat{Y}} \\rightarrow M$ and $Y \\rightarrow M$ (as in Fig. 2b). The metric calculation is given as $M = f_M(Y, \\mathcal{\\hat{Y}}, U_M)$, and similarly, $U_M$ can be dropped since M is typically deterministic.\nGiven the final DAG (including X and M), our goal is to estimate for each factor V the average causal effect (ACE) of that factor on M. We formulate this as\n$ACEM(V) = E[M|do(V = \\tilde{v})] \u2013 E[M|do(V = v)] \tbinom{1}{}"}, {"title": "3.3. Root cause analysis for distribution shifts", "content": "A primary benefit of specifying the DAG is that it represents an explicit statement about the nature of the IGP driven by knowledge of the underlying imaging physics and domain itself. We often expect distribution shifts in the real-world to be driven by changes in the causal mechanisms rather than the DAG topology itself (i.e., a change in an edge weight is more likely than the removal/addition of the edge altogether). Assuming the DAG structure remains fixed across data collections, we aim to detect and isolate the likely root cause(s) of any changes in the metric distribution relative to the factors in the DAG. In many real-world situations, these types of near-distribution shifts are common since images from multiple data collections may look qualitatively similar yet small differences may lead to measurable performance disparities [45, 46].\nFormally, we start from a source domain $(D_0)$ dataset $S_0 = {(X, Y, V)}$ generated by a SCM with DAG $G_0$. We then consider the case where a new dataset $S_1$ is collected from a similar but (potentially) shifted domain $(D_1)$. We assume that the DAG for the new domain is structurally equivalent to the source domain such that $G_0 = G_1$. However, the joint distributions over factors, images, and labels are assumed to be different where $P_{G_0}(X,Y,V) \\neq P_{G_1}(X, Y, V)$. Additionally, we assume that there is a shift in the causal mechanism for only a small subset of factors (e.g., a single sensor setting is changed between data collections).\nGiven the samples $S_0, S_1$, we aim to identify which factor/mechanism in the causal DAG is the root cause of the change in the DNN performance. We use the method of [3] which combines the use of Shapley values and the causal graph to determine how the distribution of M is influenced by the shift in the marginal of other variables in the model (i.e., V \u2208 V)."}, {"title": "4. Experiments", "content": "In the following experiments, we use synthetically-corrupted image data in order to ensure that we have ground truth knowledge of the SCM DAG, causal mechanisms, and underlying distributions. We create multiple SCMs to simulate hypothetical domains whereby we can determine the efficacy of our CDRA approach to uncover DNN sensitivities beyond what average performance metrics capture.\nGenerating complex image corruptions for evaluating CDRA A primary benefit of the causal perspective on the IGP is that it enables a sparse factorization of complex distributions over factors that affect image quality. When we invert this property, we gain the ability to precisely control the generation of complex image distributions which we can use for evaluating the efficacy of CDRA. These complex distributions allow for simulating a wide range of imaging conditions whereby corruptions compound to yield complex distortions.\nTo generate such distributions, we first define the domain D using the DAG $G_D = (V,E)$. In the case of image generation for these hypothetical domains, we must also make assumptions about the set of causal mechanisms $F = {f_V(pa(V),U_V)|\\forall V \\in V}$ and latent noise distributions. Underlying each domain is a joint distribution over all factors and images, $P_D(V, X) = \\Pi_{V\\in V}P(V|pa(V))$, which we can sample to generate evaluation datasets for our simulated domain. We use the SCM definition in conjunction with one of two corruption generation processes (compositing and rendering) to generate our evaluation datasets.\nCompositing In the compositing setting (adapted from [15]), we specify a set of image corruption functions [24] to be applied in sequence according to the specified SCM DAG. In this setting, factor variables V in the SCM correspond to normalized severities for an associated set of corruption functions $C = {c_V | V \\in V}$ where $c_V(x, V) = x$. We first sample the severity values using the causal model (i.e., $V = f_V(pa(V), U_V)$ for all $V \\in V$). We then apply the corresponding corruption functions to the original image by following a topological ordering from $G_D$ and using the image output by the previous function in the ordering as input to the next corruption.\nThis approach builds on the well-established common corruption framework (i.e., [24]) but now allows for compounded corruptions that yield more complex imaging effects. This approach is similar conceptually to the Aug-Mix [25] augmentation strategy but where the corruption ordering and severity here is determined and sampled according to the causal model representing the knowledge about the domain.\nRendering In this work, we also extend beyond [15] by introducing a rendering approach which uses the causal model to directly guide physics-based rendering. Here, factors of the causal model correspond directly with settings of the rendering engine (i.e., Blender [7]). These settings typically also mirror the types of measurements that are available when capturing image datasets in real-world settings. For instance, the SCM may contain factors such as exposure, ISO, and focal length, all of which are easily mapped to settings in Blender and similarly available in EXIF tags in real world scenarios.\nTo render corrupted images, we sample the factor values $v \\sim P_G(V)$ and update the Blender settings directly with these values for a single scene. Then the physics-based Cycles engine renders the scene under the sampled conditions. The rendering process directly captures the full set of effects of the sampled factor values on the lighting, materials, scene geometry and dynamic properties of the scene."}, {"title": "4.1. Experiment 1: CDRA for images containing multiple corruptions", "content": "We aim to first determine whether CDRA allows us to estimate per-corruption model sensitivities from images containing multiple compounded corruptions. We first create a set of hypothetical domains where the causal DAG describes the factor interactions. We choose all factor distributions and SCM parameters in order to ensure a diversity of imaging conditions and visually inspect the generated images to verify that they are not overly-corrupted. The SCMs used in this experiment are intended only to provide a means to analyze how well CDRA can isolate the effects of individual types of corruptions even when they are compounded in the images and these SCMs do not model specific real-world imaging domains.\nUsing the compositing approach, we define a set of three-node SCMS (Figure 3) where each node refers to a corruption function as defined in [24]. Corruption severities for the two parent nodes are sampled from a Beta $\\beta(2,5)$ distribution which is biased towards low-severity corruptions in order to prevent rendering overly corrupted images. For the collider (center) node $V_c$ in each variant DAG, the severity is computed as $V_c = max(min(\\Sigma_{v \\in pa(V_c)} a_{V_c}V + U_v, 1), 0)$ where $U_{v} \\tilde N(0, 0.1)$ simulates a small amount of measurement noise on $V_c$ and the edge weight of $a = 0.8$ is chosen to prevent over-corruption of the final image. The severity is clipped to the range [0, 1] such that 0 corresponds to no corruption and 1 corresponds to the maximum severity used in [24]. Using the compositing approach, we generate a corrupted version of the ImageNet validation set to yield 50k total images per SCM variant. For these variants, each image is uniquely corrupted according to the causal DAG and the severity settings sampled from the underlying factor distribution.\nWe evaluate three popular baseline DNN architectures pretrained on ImageNet using standard data augmentation (provided by the torchvision package [40]), ResNet50 [23], Swin-S [35], and ConvNext-S [36], on each SCM variant dataset. We choose these DNNs because they represent common and diverse architectural strategies (e.g., convnet vs. transformer). Across all experiments in this paper, 95% confidence intervals (CI) are obtained via bootstrapping with 1000 resamples. Note that in all of our experiments, our primary goal is to assess CDRA as an auditing method rather than to make definitive or absolute statements about the robustness of particular DNN architectures or training strategies.\nResults: We perform CDRA by measuring the ACE of each factor in the SCM on the DNN accuracy."}, {"title": "4.2. Experiment 2: Sensitivity of CDRA across image distributions", "content": "Next, we test whether CDRA allows for analysis of DNN robustness for more complex SCMs involving more factors and interactions. As in Experiment 1, we define multiple SCMs corresponding to hypothetical domains where we can precisely control differences in the model structure and distributions. The SCMs used here capture common real-world dependencies as described in the example in Sec. 1 yet vary in their DAG topology and/or causal mechanisms. We do not intend to align these models with specific domains of interest, but use their differences in structure and form to assess the ability of CDRA to isolate task DNN sensitivities to individual factors of each SCM.\nWe define a series of more complex causal models and apply the same generation process as in Sec. 4.1. We specify an SCM (Table 3 - right) which represents a causal model for a natural image domain as in the illustrative example of Sec. 1. For this natural domain (DN) SCM, the image brightness is controlled by a gamma correction with $G = 2 * (B^a \u2013 1)$ where $B_G ~ B(1.2,1.2)$ and G > 0 produces low-brightness images. Mimicking the effect of low contrast in extreme lighting conditions, contrast severity is determined as $C = f_c(G) = a_C \u00b7 (1 \u2013 cos(\\pi \u00b7 G)) + U_C$ where $a_C$ is a scaling factor (set to 0.5) and $U_C ~ N(0,0.05)$. Similarly, defocus blur severity increases when brightness and contrast severity are high, using $D = f_D(G,C) = a_D \u00b7 0.5(|G| + C) \u00b7 1(G > 0) + U_D$ and $U_D ~ N(0,0.05)$. Lastly, image noise severity increases as contrast and/or defocus severity increase (i.e., $N = f(C, D) = a_N\u00b7 0.5 \u00b7 (C + D) + U_N$ where $U_N ~ N(0,0.05)$). We also specify causal DAGs for alternative hypothetical natural domains (DY, DD where subscript refers to the DAG shape - {D: diamond, Y: Y-shape}). We use the same causal mechanisms as described for the models of Sec. 4.1 for the new DAG topologies and provide additional implementation details and results in Appendix B. We apply CDRA to estimate the per-factor ACEacc for each SCM."}, {"title": "4.3. Experiment 3: Identifying sources of distribution shift", "content": "We next ask whether we can detect sources of distribution shift given only observational data containing complex image corruptions. As in the previous experiments, we use multiple SCMs to simulate the hypothetical domains where we have full knowledge of the SCM DAG, mechanisms, and underlying distributions. We use {DD,DY,DN} from Sec. 4.2 as the base domains in this experiment.\nTo create subtle distribution shifts, we create additional variants of these base SCMs by increasing the edge weight/scale a of a single edge for each variant (i.e., rescale by 1.1 for the DD SCM, 1.2 for the Dy, DN SCMs). The rescaled values are chosen to ensure the resulting images are not overly corrupted and differences in parameters reflect differences in the overall DAG structure. However, for all base SCMs, the increase in a directly causes an increase in the overall corruption severity and the effect on DNN performance for each variant will depend on which edge weight in the DAG is modified. For each edge in the base causal DAGs, we create a new dataset by shifting a as described above and corrupting the ImageNet validation set accordingly.\nWe aim to determine whether we can identify the source of distribution shift in these complex image distributions using knowledge of the SCM DAG and the model performance alone. We use the base and a-shifted datasets to represent the pre-/post-distribution shift data. For each possible distribution shift, we evaluate each of the DNN architectures and then use the root cause analysis approach from Section 3.3 to estimate the contribution of each factor in the SCM DAG to the shift in accuracy.\nResults: Figure 4 shows that both ACEacc and mean accuracy change as a result of the causal mechanism shifts. The decrease in accuracy for each mechanism shift reflects the fact that the shifts were implemented to increase the image distortions. However, the difference in mean accuracy or ACE between datasets is insufficient to isolate the shifted mechanism. Using the root cause analysis method, we see in Figure 5 that for the DD SCM, the measured contribution of each factor to accuracy when compared across the shifted and original datasets is clearly predictive of the mechanism that affected the distribution.\nDiscussion: The proposed causal approach is sufficiently sensitive to identify the causes of performance differences due to distribution shift while using only data from the pre-/post-distribution. Such near-distribution shifts occur often in real-world settings where the IGP does not change yet small differences between data collections could occur due to, e.g., changes in time of day, location, or sensor configuration. The sensitivity of this root cause analysis enables explaining small changes in DNN performance that result from otherwise hard to detect distribution shifts (e.g., [45,46]), especially when changes to the imaging conditions may not be easily identified qualitatively."}, {"title": "4.4. Experiment 4: Robustness audits for additional vision tasks", "content": "Lastly, we test the generalizability of CDRA to other vision tasks using data rendered with Blender and SCMs for sampling the underlying physics-based rendering settings."}, {"title": "4.4.1 Object-Centric Learning", "content": "We first consider applying CDRA for an emerging class of vision algorithms for doing object-centric learning (OCL). We render corrupted versions of the CLEVR [28] dataset consisting of photo-realistic synthetically-rendered images of simple objects with varying colors, sizes, and materials. Given the causal model in Fig. 6c (simulating a hypothetical imaging domain similar to the example in Sec. 1), we sample rendering settings and use the Blender Cycles rendering engine to generate evaluation data under a wider range of imaging conditions than those found in the original CLEVR benchmark. Details of the causal mechanisms for determining the rendering factor values are provided in Appendix F.\nWe evaluate several baseline OCL algorithms including EffMORL [16], GENESISv2 [17], GNM [27], IODINE [21], SPACE [34], and SPAIR [8]. All models were trained on images similar to the original CLEVR benchmark. We measure task performance using mean Intersection over Union (mIoU) between the predicted segmentation masks and ground truth. We apply CDRA and estimate the ACEMIOU for each factor of the SCM on the task performance of the model.\nResults: Figure 6a shows that causal effects expose sensitivities that are not obvious when looking at average performance alone as in Figure 6b. In particular, SPAIR and SPACE are the top performing models and have similar mIoU, yet SPACE appears to be more robust to changes in IGP factors as evidenced by the lower ACE values for factors of the SCM.\nDiscussion: These results emphasize the value of CDRA since the audit is performed directly on the synthetic domain data rather than trying to predict domain performance based on evaluation of pre-defined IID distortions. These results further underscore that both mean performance and ACE estimates are collectively necessary since mIoU measures the performance of the DNN directly while ACE estimates sensitivity of the metric to changes in factor values. This is particularly evident in the case where GENESISv2 shows low ACE values indicating high robustness to all factors of the IGP, yet the mIoU is also low indicating that the model performs poorly on average across all conditions. Similarly, SPACE and SPAIR achieve high mIoU, yet SPACE appears to be more robust due to lower magnitude ACE values across most factors."}, {"title": "4.4.2 Optical Flow", "content": "We apply CDRA to assess the robustness of optical flow (OF) methods for natural distortions. As in the OCL case, we find that even when top-performing baselines achieve similar average endpoint error (EPE), CDRA exposes sensitivities to IGP factors that differ between models. More detail on the experimental design and results are found in Appendix D."}, {"title": "5. Discussion and Conclusion", "content": "We present here a novel perspective on robustness auditing which uses causal inference to measure the sensitivity of DNN performance to causes of distortion in the image generating process. We find that even when average performance is similar between DNN models, the ACE estimates may differ measurably and thus yield more granular insights into how model performance may change as a function of individual causes of image distortion.\nOur approach is not free of limitations. First, our method is based on the ability to specify a plausible causal graph. While we believe domain knowledge is often sufficient to specify reasonable models, we recognize that verifying the accuracy of those models may be challenging. We also acknowledge that our experiments use simulated domains modeling only some real-world dependencies but did not address the question of how to verify SCMs for specific real-world domains. While CDRA can be applied to real-world data without modification to the method itself, we were compelled to rely on simulated data due to the lack of public benchmarks containing sufficient metadata from the image capture process (such as camera exposure, aperture, F-stop, ISO settings, or other EXIF tag information). Our focus in this work was on evaluating the efficacy of our CDRA approach (where simulated ground truth data was necessary), and we believe that methods for specifying and aligning SCMs to specific domains of interest is a fruitful direction for future research.\nOverall, our CDRA method paves the way for enabling DNN robustness audits directly on domain-specific data containing complex and challenging imaging conditions. By measuring DNN sensitivities relative to causes in the IGP rather than effects of it, the derived insights are actionable and can be used to develop new mitigation methods including targeted data collection/augmentation, architecture design, or SCM-driven adaptation."}]}