{"title": "Who's the (Multi-)Fairest of Them ALL: Rethinking Interpolation-Based Data Augmentation Through the Lens of Multicalibration", "authors": ["Karina Halevy", "Karly Hou", "Charumathi Badrinath"], "abstract": "Data augmentation methods, especially SoTA interpolation-based methods such as Fair Mixup, have been widely shown to increase model fairness. However, this fairness is evaluated on metrics that do not capture model uncertainty and on datasets with only one, relatively large, minority group. As a remedy, multicalibration has been introduced to measure fairness while accommodating uncertainty and accounting for multiple minority groups. However, existing methods of improving multicalibration involve reducing initial training data to create a holdout set for post-processing, which is not ideal when minority training data is already sparse. This paper uses multicalibration to more rigorously examine data augmentation for classification fairness. We stress-test four versions of Fair Mixup on two structured data classification problems with up to 81 marginalized groups, evaluating multicalibration violations and balanced accuracy. We find that on nearly every experiment, Fair Mixup worsens baseline performance and fairness, but the simple vanilla Mixup outperforms both Fair Mixup and the baseline, especially when calibrating on small groups. Combining vanilla Mixup with multicalibration post-processing, which enforces multicalibration through post-processing on a holdout set, further increases fairness.", "sections": [{"title": "1 Introduction", "content": "Algorithmic fairness has become increasingly important with the ubiquitous application of machine learning (ML). Unfairness can arise from many sources (Huang et al. 2022), including unequal representation of protected groups in data (Guo et al. 2022). For example, people of color can be underrepresented in clinical trials due to access barriers, lack of information, and discrimination (Allison, Patel, and Kaur 2022), leading ML models to have trouble predicting treatment outcomes for non-white patients. One way to mitigate underrepresentation is data augmentation, which creates synthetic individuals from the original data (Chuang and Mroueh 2021; Iosifidis and Ntoutsi 2018; Chawla et al. 2002; Sharma et al. 2020). A particularly promising form of augmentation is Mixup (Zhang et al. 2017) and its fairness-oriented counterpart Fair Mixup (Chuang and Mroueh 2021), which linearly interpolate individuals with features in between majority and minority group attributes.\nHowever, existing augmentation literature measures fairness through binary metrics like demographic parity and equalized odds (Chuang and Mroueh 2021), which accumulate loss even when predictors lean toward correct labels. These metrics can be misleading because data often does not include all predictive features, so some notion of uncertainty is appropriate in a good predictor but would be penalized. Furthermore, the methods in Chuang and Mroueh (2021) only assess and optimize fairness for one minority group, but a fair predictor should work well on multiple multi-dimensional intersecting groups.\nThe metric of multicalibration (MC) (Hebert-Johnson et al. 2018) accounts for this uncertainty and for the presence of multiple groups by comparing predicted probabilities to true probabilities, averaging over groups of interest, and considering subsets of a predictor's support separately. Hebert-Johnson et al. (2018) also introduce an algorithm, with runtime inversely proportional to the size of the smallest group, to post-process a predictor using a holdout set and guarantee a maximum MC violation. Barda et al. (2020) then use this algorithm to learn prediction adjustments from a holdout set and apply those adjustments to test predictions. However, such post-processing subtracts a substantial amount of holdout data from available training data, resulting in even less representation of underrepresented groups in initial training. Moreover, with runtime inversely proportional to group size, enforcing MC for very small groups can be slow. The guarantees of MC enforcement and upper bounds of the overall accuracy tradeoffs proven in Hebert-Johnson et al. (2018) also only apply to the post-processed holdout set, not to unseen test data.\nThis work examines whether we can combine the desirable properties of MC and data augmentation to supplement the binary outcome insights that demographic parity and equalized odds provide. We ask:\n1. RQ1: Under what conditions can Fair Mixup mitigate MC violations of neural network predictors on minority groups while preserving binary classification accuracy?\n2. RQ2: When can (Fair) Mixup serve as an alternative to and/or increase the efficiency of MC post-processing?\n3. RQ3: What aspects of Fair Mixup contribute to its success or failure in improving MC-based fairness?"}, {"title": "2 Preliminaries", "content": "This section defines calibration (Hebert-Johnson et al. 2018; Chouldechova 2017), multicalibration (Hebert-Johnson et al. 2018), multiaccuracy (Hebert-Johnson et al. 2018), and the data augmentation methods we later expand on."}, {"title": "2.1 Notation", "content": "Throughout this paper, X represents a universe of individuals, xi represents an individual with index i, S \u2286 X is a subset of individuals, C \u2286 2X is a set of subsets of individuals, f is a predictor that maps individual xi to outcome probability fi, p is the true outcome probability of xi, and Yi \u2208 {0, 1} is the binarized true outcome for xi."}, {"title": "2.2 Calibration", "content": "For a maximum violation \u03b1 \u2208 [0, 1], f is \u03b1-calibrated w.r.t. S if \u2203S' \u2286 S with |S'| \u2265 (1 \u2212 \u03b1)|S| such that \u2200v \u2208 [0, 1],\n$|E_{x_i \\sim S'}[f_i - p]| \\leq \\alpha$,\nwhere S = {xi : fi = v}. In most classification tasks, we only see the binary outcome yi for xi. Thus, we use a modification called observable calibration (Hebert-Johnson et al. 2018), where yi replaces p in Eq. 1.\nFor example, a tumor malignancy classifier is 0.05-observably calibrated for v = 0.6 on Latine patients if of all Latine patients for which it predicts a 60% chance of malignancy, 55% to 65% of these patients have a malignant tumor. The classifier is 0.05-observably calibrated on Latine patients if this holds for all v-of all Latine patients for which it predicts a v chance of malignancy, between v and v + 5% of these patients have a truly malignant tumor."}, {"title": "2.3 Multicalibration", "content": "f is (C, \u03b1)-multicalibrated if it is \u03b1-calibrated w.r.t. all S \u2208 C (Hebert-Johnson et al. 2018). We define MC as in Hebert-Johnson et al. (2018), but we require S = S' (calibration on all of S rather than any 1\u2212\u03b1 of it, explained in Appendix C). For computational feasibility over datasets with millions of prediction probabilities, we also discretize the predicted probabilities. For integer d > 0, the d-discretized version of S splits S into d + 1 subsets, where\n$S_v = \\{x_i: \\frac{v}{d} \\leq f_i < \\frac{v+1}{d} \\} \\text{ for } v \\in [0, 1, ..., d]$.\nContinuing with the tumor malignancy classifier example, the subset of the 10-discretized S = Latine patients with v = 6 would be all Latine patients with a predicted chance of at least 60% but less than 70% malignancy. Suppose all patients in this subset have a prediction of 63%."}, {"title": "2.4 Multiaccuracy", "content": "Multiaccuracy (MA) (Hebert-Johnson et al. 2018) is a looser version of MC. f is (C, \u03b1)-multiaccurate if \u2200S \u2208 C,\n$|E_{x_i\\sim S}[f_i - p]| < \\alpha$.\nRather than requiring the expected prediction error within each S and predicted probability to be < \u03b1, MA only requires this error to be \u2264 \u03b1 in S overall. Thus, for C = {Black patients, Latine patients}, (C, 0.05)-multiaccuracy means that the average prediction for Black patients is within 5% of the true proportion of Black patients that have a malignant tumor, and likewise for Latine patients. In the rest of this paper, when we say f has an MC or MA violation of \u03b1 on C, we mean that \u03b1 is the smallest value for which f is (C, \u03b1)-multicalibrated or multiaccurate."}, {"title": "2.5 Mixup", "content": "Mixup was proposed to improve the generalizability of neural networks (NN) by training on linear combinations of example pairs, with the intuition that the NN would learn how predictions differ as inputs move continuously between feature sets (Zhang et al. 2017). For training batch size b, mixup draws (x1, y1), ..., (xb, yb) and (x'1, y'1), ..., (x'b, y'b) without replacement from the training data. Let t \u223c Beta(\u03b5, \u03b5) where \u03b5 \u2208 (0, 0). Mixup constructs one synthetic point per i \u2208 [1, ..., b]:\n$(x'_i, y'_i) = (tx_i + (1 - t)x'_i, ty_i + (1 - t)y'_i)$\nand trains an NN on (x'1, y'1), ..., (x'b, y'b) instead of the original batch. Zhang et al. (2017) showed that mixup decreased test error on CIFAR-10 and CIFAR-100."}, {"title": "2.6 Fair Mixup", "content": "Chuang and Mroueh (2021) adapted mixup toward the goal of fairness. Fair Mixup (FM) samples (x1, y1), ..., (xb, yb) from minority group S and (x'1, y'1), ..., (x'b, y'b) from S' = \u00acS. Mixup is then performed on these samples as in Section 2.5 to create synthetic points. The loss function applies the standard Binary Cross Entropy (BCE) loss function to the original points, applies the gradient RMS of a pairwise fairness penalty M between S and S' to the synthetic points, and adds \u03bb times the fairness penalty to the BCE. Fair Mixup creates better tradeoffs between average precision and the fairness metrics of demographic parity and equalized odds (Chuang and Mroueh 2021)."}, {"title": "3 Related Work", "content": "There are several other data augmentation methods for fairness. In oversampling, minority group samples are duplicated until equal in number to majority group samples (Iosifidis and Ntoutsi 2018). Another method, SMOTE, creates minority group members through linear interpolation among existing minority group members (Chawla et al. 2002). More recently, Sharma et al. (2020) introduce \u201cIdeal World\": for each original point, a new sample is created with the same features and label, but the protected attribute is flipped, making both statistical parity difference and average odds difference decrease while preserving accuracy. Outside of structured data, Wadhwa et al. (2022) apply identity pair replacement, identity term blindness, and identity pair swap on text classification. Yucer et al. (2020) introduce data augmentation that improves facial recognition on minority groups.\nWe focus on structured data classification to minimize the confounding factor of unstructured data featurization. We also choose Fair Mixup as a basis because it minimizes data distribution changes and treats protected attributes as predictive features. Ideal World takes away the predictive information of protected attributes. Oversampling, SMOTE, and Ideal World create additional minority individuals, changing the frequency and composition of minority groups. In contrast, Fair Mixup creates individuals that are neither minority nor majority group members, but rather some interpolated in-between. Thus, while the data distribution may change, the members of boolean circuit-defined groups do not."}, {"title": "3.2 Extensions of MC", "content": "Hebert-Johnson et al. (2018) devise algorithms that could enforce MC \u03b1's to be below an arbitrary threshold. A related post-processing algorithm, designed for multiaccuracy, is MULTIACCURACY BOOST, which requires a trained auditor on top of a holdout set (Kim, Ghorbani, and Zou 2019). Applying the results of Hebert-Johnson et al. (2018) empirically, Barda et al. (2020) transfer learned post-processing updates to a COVID-19 mortality rate forecasting task. We test this application in Section 4.4.\nA few works extend (multi-)calibration to more nuanced metrics that handle complex notions of uncertainty. Kumar, Sarawagi, and Jain (2018) add calibration optimization to the training loss function, clamping overconfident predictions while minimizing penalties on true confident predictions. Wald et al. (2021) propose multi-domain calibration to evaluate model generalization to out-of-distribution data, suggesting both isotonic regression post-processing and a training regime that includes calibration from Kumar, Sarawagi, and Jain (2018). Jung et al. (2021) extend MC to higher moments, measuring moment consistency in a way that computes groupwise error inversely proportionally to group size (Jung et al. 2021). Other work extends MC to conformal prediction, which generates prediction sets rather than point estimates (Jung et al. 2023; Foygel Barber et al. 2020). This framework generalizes MC to quantiles of the label's support rather than individual values and is useful for categorical or continuous labels, unlike binary labels, for which MC is already a probabilistic extension. Gopalan et al. (2024) connect MC to multi-group loss minimization.\nThe most comprehensive investigation of MC post-processing to our knowledge is Hansen et al. (2024), which finds that baseline predictors on tabular data are often decently multicalibrated already, and post-processing does not improve worst-group calibration error for multi-layer perceptrons, Random Forests, and Logistic Regression but does benefit Support Vector Machines, Decision Trees, and Naive Bayes. When worst-group calibration error improves, there is an overall accuracy tradeoff. They further find that MC enforcement is hyperparameter-sensitive and most effective with huge amounts of data (found in image and language data but not tabular data). They find that calibration algorithms like Platt scaling and isotonic regression sometimes perform nearly on par with MC enforcement while being more efficient. These findings are consistent with previous works suggesting that empirical risk minimization may inevitably yield multicalibrated baseline predictors (B\u0142asiok et al. 2023, 2024). We refer the reader to Hansen et al. (2024) for a more comprehensive MC literature review and for image and language experiments.\nThis work extends Hansen et al. (2024) in three ways. First, expanding upon their maximum of 15 groups that are all at least 0.5% of their corresponding population, we stress-test our methods on MC w.r.t. up to 81 groups at a time, up to 55 of which are smaller than 0.25% of their corresponding population. We also select these groups in five different ways to investigate effects of group set size on MC. Second, expanding upon their examination of income prediction from folktables on Californian residents from 2018, we evaluate our methods on each permutation of the 10 most populous US states and the four most recent American Community Survey data collection years, yielding 40 datasets. We additionally test employment status prediction on these 40 datasets, for a total of 80 tasks considered. Third, while their work and much of the current MC literature considers data-reductive post-processing methods, our work takes inspiration from their finding that post-processing works best on huge datasets and instead focuses on data augmentation to maximize the amount of original data that can be used for initial training."}, {"title": "4 Methods", "content": "We test 13 NN training methods to determine the effects of particular features of FM that contribute to its performance and fairness. FM has 3 distinguishing components:\n1. C1: Training batches are balanced across membership in the minority group for which we wish to ensure fairness.\n2. C2: Synthetic data is created by linearly interpolating original points. If C1 is implemented, each synthetic data point is the interpolation of a minority group member and a majority group member. If not, the original points are split in half and paired at random for interpolation.\n3. C3 (can only be done if C2 is also implemented): A fairness penalty is added to the loss function for predictions on synthetic points, minimizing a weighted sum of the standard loss and the fairness penalty during training.\nPost-processing is distinguished by the following:\n4. C4: A post-processing algorithm learns prediction update rules from post-processing data (subtracted from initial training data), and it applies those update rules to the validation and test data during evaluation and deployment.\nWith these insights (summarized in Fig. 2), this section describes each method mathematically. We motivate each method by explaining how it implements a subset of {C1, C2, C3, C4}, thus isolating the effects of specific components of FM to answer RQ3. C4 also helps answer RQ2 (FM vs. post-processing). Method names are starred if they contain substantial novel elements that we introduce on top of existing work. Implementation details are in Appendix C."}, {"title": "4.1 Baselines", "content": "BASE trains an NN with mini-batch gradient descent using Binary Cross Entropy loss, over several epochs and batch selection iteration. We report test-time balanced accuracy for the epoch with the best validation-time balanced accuracy. BASE does not implement C1, C2, C3, or C4.\n*FAIRBASE modifies BASE by balancing training data groupwise (C1). Suppose we have minority groups C to optimize for fairness and n iterations of gradient descent per training epoch in BASE. Then, FAIRBASE conducts n \u00b7 |C| iterations of gradient descent. Each iteration centers around one S\u2208 C: we construct a batch by selecting one sub-batch from S and one sub-batch from its complement \u00acS. We subsample the larger sub-batch to be equal in size to the smaller sub-batch to ensure balance across membership in S."}, {"title": "4.2 Variants of Mixup", "content": "MIXUP is as defined in Section 2.5, implementing C2.\n*MIXUPEO modifies FAIRBASE. Consider minority groups Cand n \u00b7|C| iterations of gradient descent as in FAIRBASE. MIXUPEO conducts 2n\u00b7|C| iterations of gradient descent, each centered around one pair (S, y) \u2208 (C, {0,1}). We construct a batch by selecting one sub-batch of members of Sy (members of S whose true label is y) and one sub-batch of members of \u00acSy (members of \u00acS whose true label is y). Next, we perform mixup by pairing each member of Sy with a member of S within the batch and interpolating each pair. Our loss is a weighted sum of Binary Cross Entropy applied to the original batch and the same loss applied to the interpolated points. MIXUPEO implements C1, C2, and a control version of C3 (standard loss instead of pairwise fairness, but number of groups under consideration for this loss is adjustable, as elaborated on in Section 4.3). Thus, we can compare it to FAIRBASE to isolate the effect of C2."}, {"title": "4.3 Variants of Fair Mixup", "content": "FM implements C1, C2, and C3. Though Chuang and Mroueh (2021) introduce two versions of FM (with M as demographic parity difference and equalized odds difference), their framework generalizes to any pairwise fairness metric. We first show how to modify FM to accommodate multiple minority groups simultaneously. Then, we define the two versions of FM from Chuang and Mroueh (2021), followed by two versions with new metrics. Specifically, our extensions try to incorporate some notion of MC in the fairness penalty, as we aim to minimize MC violations. These methods test the effects of varying the metric in C3.\nConsider metric M, its group gradients $R^{mixup}_{Ms1} ... R^{mixup}_{Ms|C|}$ . For k \u2208 {1, ..., |C|}, the penalty is the mean of the k highest group gradients. We take means because preliminary experiments show that sums produce higher MC as. We make k adjustable to prevent overfitting. The rest of the computation proceeds as in Chuang and Mroueh (2021). Given Ms, the first step is to transform it into an integral via the Fundamental Theorem of Calculus so it can be computed for interpolated data (Appendix A.1 in Chuang and Mroueh (2021)). Then, we differentiate the integral to get $R^{mixup}_{Ms}$. We list the equations for Ms below, with full formulae in Appendix D.\nFMDP is the first version of FM in Chuang and Mroueh (2021). Ms is demographic parity, the difference between the average fi on members of S vs. its complement S':\n$M_s = \\Delta DP_S(f) = |E_{x_i \\sim S}[f_i] - E_{x_i \\sim S'}[f_i]|$.\nFMEO is the second version of FM, with the equalized odds difference (Hardt, Price, and Srebro 2016) that modifies DP by only considering one true outcome at a time:\n$M_s = \\Delta EO_S(f) = \\sum_{y \\in \\{0,1\\}} |E_{x_i \\sim S_y}[f_i] - E_{x_i \\sim S'_y}[f_i]|$,\nwhere Sy = {xi \u2208 S : yi = y}, and S'y = \u00acSy.\n*FMMA is our first extension of FM, with a version of MA modified to be pairwise. We measure the mean difference in prediction errors ei = fi - p between S and S':\n$M_s = \\Delta MA_S(f) = |E_{x_i \\sim S}[e_i] - E_{x_i \\sim S'}[e_i]|$,\n*FMMC is our second extension of FM, with a pairwise modification of MC, which modifies MA by considering one interval $S_d = \\{x \\in S: f_i \\in [\\frac{v}{d}, \\frac{v+1}{d}) \\}$ at a time:\n$M_s = \\Delta MC_S(f) = \\sum_{v=0}^d |E_{x_i \\sim S^v_d}[e_i] - E_{x_i \\sim S'^v_d}[e_i]|$.\n4.4 Post-Processing\nWe test whether MC and MA enforcement (implementing C4) improve test performance as in Barda et al. (2020).\nENFORCEMA post-processes predictions to minimize MA violations. We feed (1) predictions on a holdout post-processing set and (2) a set of minority groups C as inputs to Algorithm 3.1 in Hebert-Johnson et al. (2018). However, we augment Algorithm 3.1 with a list of rules mapping each SEC to a float as to be added to predictions on members of"}, {"title": "5 Experiments", "content": "This section describes our data and experimental settings."}, {"title": "5.1 Datasets", "content": "We test two prediction tasks from folktables (Ding et al. 2021), a superset of Adult Income data (Becker and Kohavi 1996) collected from the American Community Survey. We have p\u2208 {0,1}, but fi \u2208 [0,1]. Table 2 summarizes the data. Full data statistics are at http://tiny.cc/mfm-stats.\nEMPLOYMENT The task is to predict whether an individual is employed. Table 4 specifies the exact input features.\nINCOME The task is to predict whether an individual's annual income is higher than the median income for that year in their state of residence according to Data Commons (Google 2024). Table 5 lists input features.\nWe run 40 datasets each for EMPLOYMENT and INCOME: the 10 most populous US states \u00d7 the 4 most recent years, providing substantial geographic and temporal variation. We choose these tasks based on experiments in Jung et al. (2023). Additionally, we seek problems with reasonable baseline performance (\u2265 80% balanced accuracy on CA \u00d7 2022) to focus on improving fairness on useful classifiers."}, {"title": "5.2 Experimental Settings", "content": "To measure the effects of |C| and |S|, we run all combinations of datasets and training methods on five settings:\nALL C = U {all n computationally possible racial groups, disabled people, disabled members of each racial group}. A racial group is computationally possible if for all random seeds, at least one disabled member of that group is in each of the train, validation, and test splits. |C| = 2n + 1.\nBIG C = U {b racial groups each comprising > 0.25% of the total dataset, disabled people, disabled members of each of the b racial groups.} |C| = 2b + 1, b << n.\nSMALL C = \u222a {s racial groups each comprising \u2264 0.25% of the total dataset, disabled people, disabled members of each of the s racial groups}. |C| = 2s + 1, s << n.\nDIS This setting is closest to what FM has already been tested on: C = {disabled individuals}, so |C| = 1.\nDLFR C = disabled people, members of the least frequent (computationally possible) racial group (LFR), and disabled members of the LFR, hence |C| = 3."}, {"title": "6 Results", "content": "To capture both fairness and overall performance, we compute the mean across all 40 (state, year) pairs of the following quantities for each experiment: (1) % increase in balanced accuracy over BASE for the corresponding state, year, and task and (2) % decrease over BASE in worst (highest) individual group MC violation \u03b1. Table 3 reports these mean percentages, showing that for all (task, setting) pairs except for DIS (both tasks), (EMPLOYMENT, DLFR), and (INCOME, SMALL), MIXUPENFORCEMC shows the biggest average balanced accuracy and MC \u03b1 improvement. The other best methods are MIXUPMA for (EMPLOYMENT, DIS), MIXUP for (INCOME, DIS) and (INCOME, SMALL), ENFORCEMC for (EMPLOYMENT, DLFR) (though MIXUPENFORCEMC is close), but FMDP has the best \u03b1 for (EMPLOYMENT, DIS) according to Table 8. If we consider only methods that perform post-processing or augmentation/data balancing (i.e. all methods except MIXUPENFORCEMC), the best method is ENFORCEMC, except (EMPLOYMENT, DIS) (MIXUPMA was best), (INCOME, SMALL), and (INCOME, DIS) (MIXUP was best). One note is that for 28 of 40 datasets, we had s = 0 and thus C = just disabled people, so (INCOME, SMALL) results may be more characteristic of single-group calibration. We also note that all methods except for BASE had negative mean increases in balanced accuracy (up to -1.25%), so positive values in Table 3 indicate fairness improvements.\nExamining FM, we see that except (EMPLOYMENT, DIS), all FM variants worsened fairness. For (EMPLOYMENT, DIS), FM improved fairness while largely preserving balanced accuracy, confirming the result in Chuang and Mroueh (2021) that FM works on one larger group.\nComparing MIXUPENFORCEMC and ENFORCEMC, we observe that while MIXUPENFORCEMC outperforms ENFORCEMC in many cases, it sometimes makes the ENFORCEMC component of MIXUPENFORCEMC less efficient. On the EMPLOYMENT dataset, the number of iterations to convergence of the ENFORCEMC post-processing algorithm increased by a percentage in the range (0.34%, 5.8%), with the greatest percentage increase for SMALL (+5.8%) and the greatest decrease for BIG (-1.58%). For INCOME, all methods took fewer iterations, in the range (-3.68%, -0.08%).\nFinally, we analyze correlations between results and data statistics. We largely find either no correlation or low correlations, with some exceptions. One exception is that the mean MC \u03b1 across groups > 0.25% of the population on ALL has a moderate correlation with total dataset size (lower violations for bigger datasets) for all non-BASE methods"}, {"title": "7 Discussion", "content": "Our results reveal the importance of stress-testing fairness optimization on multiple groups of varying sizes and on metrics that capture uncertainty. To answer RQ1, the only condition under which FM improves MC is the condition it was designed for: fairness for one minority group (DIS) on a truly binary problem (EMPLOYMENT). This holds irrespective of the particular train-time fairness penalty. This leads to an answer to RQ2: under a single-group truly-binary condition, FM (especially FMDP) outperforms post-processing in ensuring fairness for disabled people. Based on the raw \u03b1s in Table 8, this could be because disabled people are a relatively big, non-monolithic group for which the BASE NN is already much more calibrated than the more fine-grained racial groups. Thus, to further improve upon the BASE \u03b1, it may be more effective to examine more disabled individuals and their full feature sets during training (as in FM) rather than apply a fixed adjustment to disabled individuals unconditionally (as in post-processing). However, this fixed post-processing adjustment may work well for smaller racial groups because the smaller sizes of racial groups make race more informative than disability.\nRegular MIXUP presents a robust alternative to ENFORCEMA in nearly all settings and to ENFORCEMC when considering one group in a continuous-to-binary prediction problem as in (INCOME, DIS) or part of (INCOME, SMALL). More powerfully, combining MIXUP and ENFORCEMC through MIXUPENFORCEMC enhances performance of ENFORCEMC alone in the majority of settings, especially when more than one group is under consideration. However, it is inconclusive whether this enhancement is accompanied by efficiency improvement for the ENFORCEMC component.\nFor RQ3, we see that MIXUP is the overall best post-processing-free method. Comparing MIXUP with MIXUPEO, MIXUPMA, MIXUPMC, and FAIRBASE, we observe that using interpolated data contributes more to fairness improvements than groupwise balancing of training batches. Looking at FAIRBASE, MIXUPEO, MIXUPMA, and MIXUPMC, we further suggest that data balancing may adversely affect performance and fairness, since the key factor that sets MIXUP apart from worse-performing methods of FM, MIXUPEO, MIXUPMA, and MIXUPMA is C1. This may be because having limited minority instances means we learn less about majority instances as well (and since groups intersect, some instances that are minorities in one way but majorities in another are seen less). Finally, comparing MIXUPEO, MIXUPMA, and MIXUPMC to FM variants, we see that C3 effects (train-time fairness penalty) are inconclusive, as outcomes fluctuate by method and setting. Thinking more generally about why MIXUP outperforms FM so often, we hypothesize that in addition to the adverse effect of data balancing in FM, MIXUP has a more manageable amount of"}, {"title": "8 Conclusion", "content": "We conduct the first investigation of how data augmentation via interpolation affects MC-based fairness on multiple minority groups of multiple sizes for binary tabular data classification. We find that while Fair Mixup is not so fair on multiple groups, regular mixup mitigates MC violations across many groups, both by itself and together with MC post-processing. Our investigation opens several avenues of future work, with our evaluation pipeline being easily extensible to data augmentation on probabilistic fairness in other modalities (e.g. vision, language) and ML problems (e.g. continuous/categorical labels)."}, {"title": "A Dataset Details", "content": "Table 4 summarizes the input features of the EMPLOYMENT dataset, and Table 5 summarizes the features for INCOME."}, {"title": "A.1 Hyperparameter Search", "content": "We explore a few choices of d, k, and \u03bb for our MIXUP and Fair Mixup implementations. For Fair Mixup, we try each of the 24 combinations of d\u2208 {10,55,100}, k\u2208 {1,3, 40, 100}, and \u03bb\u2208 {0.25,0.5} on the subsets of each prediction task from California from the year 2022 (while running these combinations on all 40 subsets would be ideal, this one search took us over a week and thus would be intractable to replicate). We determine the best (d, k, \u03bb) triple for each (dataset, method) combination and use that triple on the other 39 (state, year) subsets for that dataset and method. We measure \"best\u201d via the highest average of (1) percent increase in balanced accuracy over BASE and (2) percent decrease in mean individual-group MC violation over BASE.\nFor MIXUP, we fix d = 10, state = CA, and year = 2022 and search over the 8 combinations of k \u2208 {1, 3, 40, 100} and \u03bb\u2208 {0.25, 0.5}, as d = 55 and d = 100 always dramatically worsened both efficiency and performance in our hyperparameter search for Fair Mixup. We measure and determine the best (k, \u03bb) for each (dataset, method) tuple as we do in our Fair Mixup search, and we apply these hyperparameters across all states and years within each dataset and method.\nOne final hyperparameter is determining whether to create batches by sampling without replacement or by sampling the smaller group with replacement until it is equal to the specified batch size. We experiment with both options on California \u00d7 2022 \u00d7 ALL for all FM methods and determine that the best performance and fairness results from the following choices: (1) if the group is smaller than 183617.4 (approximately 0.25% of the EMPLOYMENT dataset size, multiplied by the training split percentage) and the fairness metric is demographic parity or multiaccuracy, then we take a sample of size b, with replacement if b is bigger than the size of the group, without replacement otherwise; (2) otherwise, we take a sample of size min(group size, b), without replacement."}, {"title": "C Implementation Details", "content": "The NN we used for our experiments was directly taken from Chuang and Mroueh (2021). It consists of 3 hidden layers of size 200 using ReLU activation after the first and second layers, and an output layer of size 1 with a sigmoid activation. We use the Adam optimizer with learning rate 0.001, as in Chuang and Mroueh (2021) as well. We use binary cross-entropy loss and train for 10 epochs with n = 100 iterations of mini-batch selection (b = 500) each. We use the PyTorch\u00b9 library."}, {"title": "C.2 (Fair) Mixup", "content": "For every mini-batch in the Mixup and Fair Mixup variants, we generate a fresh t ~ Beta(\u03b5,\u03b5), with \u03b5 = 1, as in Chuang and Mroueh (2021). Also following"}]}