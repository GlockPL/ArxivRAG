{"title": "EasyST: A Simple Framework for Spatio-Temporal Prediction", "authors": ["Jiabin Tang", "Wei Wei", "Lianghao Xia", "Chao Huang"], "abstract": "Spatio-temporal prediction is a crucial research area in data-driven urban computing, with implications for transportation, public safety, and environmental monitoring. However, scalability and generalization challenges remain significant obstacles. Advanced models often rely on Graph Neural Networks to encode spatial and temporal correlations, but struggle with the increased complexity of large-scale datasets. The recursive GNN-based message passing schemes used in these models hinder their training and deployment in real-life urban sensing scenarios. Moreover, long-spanning large-scale spatio-temporal data introduce distribution shifts, necessitating improved generalization performance. To address these challenges, we propose a simple framework for spatio-temporal prediction -- EasyST paradigm. It learns lightweight and robust Multi-Layer Perceptrons (MLPs) by effectively distilling knowledge from complex spatio-temporal GNNs. We ensure robust knowledge distillation by integrating the spatio-temporal information bottleneck with teacher-bounded regression loss, filtering out task-irrelevant noise and avoiding erroneous guidance. We further enhance the generalization ability of the student model by incorporating spatial and temporal prompts to provide downstream task contexts. Evaluation on three spatio-temporal datasets for urban computing tasks demonstrates that EasyST surpasses state-of-the-art approaches in terms of efficiency and accuracy. The implementation code is available at: https://github.com/HKUDS/EasyST.", "sections": [{"title": "1 Introduction", "content": "Spatio-temporal prediction is the ability to analyze and model the complex relationships between spatial and temporal data. This involves understanding how different spatial features (e.g., location, distance, and connectivity) and temporal features (e.g., time of day, seasonality, and trends) interact with each other to produce dynamic patterns and trends over time. By accurately predicting these patterns and trends, spatio-temporal prediction enables a wide range of applications in urban computing. For example, in transportation, it can be used to predict traffic flow and congestion patterns, optimize traffic signal timing, and improve route planning for public transit systems [41]. In public safety, it can be used to predict crime hotspots and allocate police resources more effectively [26]. In environmental monitoring, it can be used to predict air and water quality, monitor the spread of pollutants, and predict the impact of climate change [33].\nTraditional spatio-temporal forecasting techniques often overlook spatial dependencies present in data [17, 19, 31, 32]. The emergence of Graph Neural Network (GNN)-based models [7, 11, 24, 34] are motivated by the need to capture high-order spatial relationships between different locations, thereby enhancing the forecasting accuracy. By incorporating multiple graph convolutional or attention layers with recursively message passing frameworks, these models can model the interactions among spatially connected nodes [9]. However, two key challenges hinder the performance of existing solutions in GNN-based spatio-temporal forecasting:\nScalability. Spatio-temporal prediction often involves large-scale datasets with complex spatial and temporal relationships. However, the computational complexity of GNNs can become prohibitive in such cases. Specifically, GNN-based models for spatio-temporal prediction can be computationally demanding and memory-intensive due to the large-scale spatio-temporal graph they need to handle.\nGeneralization. Spatio-temporal prediction models need to generalize well to unseen data and adapt to distribution shifts that occur over time due to various factors, such as changes in the environment, human behavior, or other external factors [43]. These distribution shifts can lead to a significant decrease in the performance of spatio-temporal prediction models [40]. Therefore, it's"}, {"title": "2 Related Work", "content": "Spatio-Temporal Forecasting. In recent years, there have been significant advancements in spatio-temporal prediction within the domain of urban intelligence. This field enables accurate forecasting of complex phenomena such as traffic flow, air quality, and urban outliers. Researchers have developed a range of neural network techniques, including convolutional neural networks (CNNs) [36, 37], as well as graph neural networks (GNNs) [10, 11, 42]. Moreover, recent self-supervised spatio-temporal learning methods (e.g., ST-SSL [14] and AutoST [38]) have shown great promise in capturing complex spatio-temporal patterns, especially in scenarios with sparse data. However, SOTA approaches still face challenges in terms of scalability and computational complexity when dealing with large-scale spatio-temporal graphs. Additionally, it is crucial for spatio-temporal prediction models to adapt well to distribution\nKnowledge Distillation on General Graphs. Research on knowledge distillation (KD) for graph-based models has gained significant attention in recent years [39]. The proposed paradigms of knowledge distillation can be grouped into two categories: i) Logits Distillation involves using logits as indicators of the inputs for the final softmax function, which represent the predicted probabilities. In the context of graph-based KD models, the primary objective is to minimize the difference between the probability distributions or scores of a teacher model and a student model. Noteworthy works that leverage logits in knowledge distillation for graphs include TinyGNN [27], CPF [28], and GFKD [6]. ii) Structures Distillation aims to preserve and distill either local structure information (e.g., LSP [30], FreeKD [8], GNN-SD [3]) or global structure information (e.g., CKD [23], GKD [29]) from a teacher model to a student model. Notable examples in this category include T2-GNN [13], SAIL [35], and GraphAKD [12]. Drawing upon prior research, this study capitalizes on the benefits of KD to improve spatio-temporal prediction tasks. The objective is to streamline the process by employing a lightweight yet effective model. A significant contribution of this work lies in the novel integration of the spatio-temporal information bottleneck into the KD framework. By doing so, the model effectively mitigates the impact of noise through debiased knowledge transfer."}, {"title": "3 Preliminaries", "content": "Spatio-Temporal Units. Different urban downstream tasks may employ varying strategies for generating spatio-temporal units. For instance, in the domain of crime forecasting, the urban geographical space is often partitioned into  N = I \\times J  grids, where each grid represents a distinct region  r_{i,j} . Spatio-temporal signals, such as crime counts, are then collected from each grid at previous  T  time intervals. On the other hand, when modeling traffic data, spatio-temporal traffic volume signals are gathered using a network of sensors (e.g.,  r_i ), with data recorded at specific time intervals ( t \u0454 T ).\nSpatio-Temporal Graph Forecasting. The utilization of a Spatio-Temporal Graph (STG)  G(V, E, A, X)  provides an effective means of capturing the relationships among different spatio-temporal units. In this context,  V  is the collection of nodes (e.g., regions or sensors) and  &  denotes the set of edges that connect these nodes. The adjacency matrix,  A \u2208 R^{N \\times N}  (where  N = |V| ), captures the relationships between the nodes in the spatio-temporal graph.  X \u2208 R^{T \\times N \\times F}  represents the STG features, which encompass spatio-temporal signals such as traffic flow or crime counts. Here,  T  signifies the number of time steps, while  F  denotes the number of features associated with each node. This graph-based structure allows for an efficient characterization of spatial and temporal relationships, enabling a comprehensive analysis of the underlying urban dynamics. Our goal in STG prediction is to learn a function, denoted as  f , that can forecast the future STG signals (i.e.,  \\hat{Y} \u2208 R^{T' \\times N \\times F} ) for the next  T'  steps based on the available information from  T  historical frames.\n\\hat{Y}_{t:t+T'-1} = f(G(V, &, A, X_{t-T:t-1}))\n(1)"}, {"title": "4 Methodology", "content": "In this section, we present our EasyST along with its technical details, as shown in Figure 1. Throughout this section, subscripts are used to represent matrix indices, while superscripts are employed to indicate specific distinguishing labels, unless stated otherwise."}, {"title": "4.1 Knowledge Distillation with Spatio-Temporal GNNs", "content": "The effectiveness of spatio-temporal GNNs heavily relies on complex network models with recursive message passing schemes. In our EasyST, we aim to overcome this complexity by transferring the soft-label supervision from a large teacher model to a lightweight student model, while still preserving strong performance in spatio-temporal prediction. The teacher spatio-temporal GNN provides supervision through spatio-temporal signals (i.e.,  Y \u2208 R^{T' \\times N \\times F} ), and it generates predictive labels (i.e.,  Y_T \u2208 R^{T' \\times N \\times F} ). Our goal is to distill the valuable knowledge embedded in the GNN teacher and effectively transfer it to a simpler MLP, enabling more efficient and streamlined learning.\nL = L_{pre} (\\hat{Y}, Y) + \\lambda L_{kd}(\\hat{Y}, Y_T)\n(2)\nThe prediction of the student MLP is denoted as  \\hat{Y} \u2208 R^{T\u00b4 \\times N \\times F} . We introduce the trade-off coefficient  \\lambda  to balance the two terms in our objective. The first term,  L_{pre} , represents the predictive MAE-based or MSE-based loss function used in the original STG forecasting tasks. However, when it comes to knowledge distillation, the second term,  L_{kd} , which aims to bring the student's predictions closer to the teacher's results, requires careful reconsideration, especially for regression tasks. In the following subsection, we will present our well-designed objective that addresses this issue."}, {"title": "4.2 Robust Knowledge Transfer with Information Bottleneck", "content": "In the context of spatio-temporal predictions, the presence of two types of noise can indeed have a detrimental impact on the effectiveness of the knowledge distillation process. The predictions produced by the teacher model can be prone to errors or inconsistencies, which can misguide the knowledge transfer paradigm during the distillation process. Additionally, the presence of data distribution shift between the training and test data can pose a challenge for knowledge distillation. This can result in the student model struggling to identify relevant information for the downstream prediction task. As a result, addressing bias in the teacher model's predictions and handling data distribution shift are important considerations for successful spatio-temporal knowledge distillation.\nTo address the above challenges, we enhance our spatio-temporal knowledge distillation paradigm with Information Bottleneck principle (IB), to improve the model generalization and robustness. In particular, our objective of our framework in information compression is to generate compressed representations of input data that retains the invariant and most relevant information while discarding unnecessary or redundant information. Formally, we aim to minimize the objective by considering the student's predictions, denoted as  \\hat{Y} , the teacher's predictions, denoted as  Y_T , the ground-truth result, denoted as  Y , and the input spatio-temporal features,"}, {"title": "4.2.1 Variational Bounds our IB Mechanism", "content": "Since directly computing the mutual information terms  I(Y, Z) ,  I(Y_T, Z) , and  I(X, Z)  is intractable, we resort to using variational bounds to estimate each term in the objective, as motivated by the work [1]. Concerning the lower bound of  I (Y, Z) +I(Y_T, Z) , its formalization can be expressed as follows:\nI(Y, Z) + I(Y_T, Z)\n= E_{y,z} [log \\frac{P(YZ)}{P(Y)}] + E_{y_t,z} [log \\frac{P(Y_TZ)}{P(Y_T)}]\n(4)\nAs we always have  KL[P(Y|Z)||Q_1 (Y|Z)], KL[P(Y_T|Z)||Q_2(Y_T|Z)] \u2265 0 , we can obtain that:\nI(Y, Z) + I(Y_T, Z)\n> E_{y,z} [log Q_1 (Y|Z)] + E_{y_t,z} [log Q_2 (Y_T|Z)]\n(5)\nThe variational approximations  Q_1 (Y|Z)  and  Q_2(Y_T|Z)  are used to approximate the true distributions  P(Y|Z)  and  P(Y_T|Z) , respectively. These approximations aim to closely match the ground-truth result  Y  and mimic the behavior of the teacher model  Y_T  based on the hidden embeddings  Z . As for the upper bound of  I(X, Z) , we can express it as follows:\nI(X, Z) = E_{x.z}[log \\frac{P(ZX)}{P(Z)}]\n(6)\nSince we always have  KL [P(Z) ||Q_3 (Z)] \u2265 0 , the following equation can be derived:\nI(X, Z) \u2264 E_x [KL(P(Z|X) ||Q_3 (Z))]\n(7)\nThe variational approximation  Q_3 (Z)  is used to approximate the marginal distribution  P(Z) . In our spatio-temporal IB paradigm, the objective to be minimized is given by Equation 3.\nmin (E_{y,z} [log Q_1 (Y|Z)] + E_{y_1,z} [log Q_2 (Y_T |Z)])\nP(ZX)\n+ (\\beta_1 + \\beta_2) E_x [KL(P(Z|X)||Q_3(Z))]\n(8)"}, {"title": "4.2.2 Spatio-Temporal IB Instantiating", "content": "To instantiate the objective in Eq 8, we characterize the following distributions:  P(Z|X), Q_1 (Y|Z), Q_2 (Y_T|Z) , and  Q_3(Z) . These distributions play a crucial role in defining and instantiating the objective in Eq 8, allowing us to optimize the model based on the information bottleneck principle.\nEncoder with P(ZX). To obtain the mean and variance matrices of the distribution of  Z  from the input feature  X , we employ a Multilayer Perceptron (MLP) encoder  F_e . The formulation is:\n(\\mu_z, \\sigma_z) = F_e(X)\n(9)\nDecoder with  Q_1 (Y|Z)  and  Q_2(Y_T|Z) . After obtaining the distribution of  Z  with mean (\\mu_z) and variance (\\sigma_z) matrices, we utilize the reparameterization trick to sample from this learned distribution and obtain the hidden representation  Z . The reparameterization is given by  Z = \\epsilon\\sigma_z + \\mu_z , where  \\epsilon  is a stochastic noise sampled from a standard normal distribution (N(0, 1)). Subsequently, we decode the obtained  Z  using an MLP decoder  F_d  to generate the final prediction  \\hat{Y} :\n\\hat{Y} = F_d(Z)\n(10)\nFor tasks involving discrete predictions, such as classification, the cross-entropy loss is commonly used to maximize the likelihood in the first term of Equation 3. On the other hand, for regression tasks with continuous predictions, Equation 2 is employed, utilizing mean squared error (MSE) or mean absolute error (MAE) to maximize the likelihood. This choice of loss function depends on the nature of the prediction task and the type of output being considered.\nMarginal Distribution Control with  Q_3 (Z) . In our approach, we assume the prior marginal distribution of  Z  to be a standard Gaussian distribution N(0, 1). This choice is inspired by the spirit of variational auto-encoders (VAE) as discussed in the work [16]. Consequently, for the KL-divergence term in Equation 3, we can"}, {"title": "4.2.3 Teacher-Bounded Regression Loss", "content": "To effectively control the knowledge distillation process for regression tasks, a teacher-bounded regression loss  L_t  is employed as the knowledge distillation loss  L_{kd} . The purpose of this approach is to prevent the student model from being misled by deterministic yet erroneous regression results generated by the teacher model. The formulation of the teacher-bounded regression loss  L_t  is:\nL_{kd}(\\hat{Y}, Y_T) = L_b (\\hat{Y}, Y_T, Y)\n= \\begin{cases}\nl(\\hat{Y}, Y), & \\text{if } l(\\hat{Y}, Y) + \\delta \u2265 l(Y_T, Y) \\\\\n0, & \\text{otherwise}\n\\end{cases}\n(12)\nThe symbol  l  represents any standard regression loss, such as mean absolute error (MAE) or mean squared error (MSE). The threshold  \\delta  is used to control the knowledge transfer process. The vectors  \\hat{Y} ,  Y_T , and  Y  correspond to the predictions of the student, the teacher, and the ground truth, respectively. In detail, the student model does not directly take the teacher's predictions as its target but instead treats them as an upper bound. The objective of the student model is to approach the ground truth results and closely mimic the behavior of the teacher model. However, once the student model's performance surpasses that of the teacher model by a certain degree (exceeding the threshold  \\delta ), it no longer incurs additional penalties for knowledge distillation. To conclude, we extend the original KD loss, which is constrained by the proposed spatio-temporal IB principle, resulting in a robust and generalizable KD framework."}, {"title": "4.3 Spatio-Temporal Context Learning with Prompts", "content": "To infuse the spatio-temporal contextual information into the student model from downstream tasks, we leverage spatio-temporal prompt learning as a mechanism to impart task-specific knowledge to the compressed model. These prompts serve as explicit cues that guide the model in capturing data-specific spatial and temporal patterns. We incorporate the following spatio-temporal prompts:\nSpatial Prompt. The diverse nodes present in the spatio-temporal graph showcase distinct global spatial characteristics, which are closely linked to the functional regions (e.g., commercial and residential areas) they represent in urban geographical space. To effectively model this essential feature, we introduce a learnable spatial prompt denoted as  E^{(\\alpha)} \u2208 R^{N\\times D} , where  N  denotes the number of nodes (e.g., regions, sensors) within the spatio-temporal graph. This spatial prompt enables us to incorporate and encode the unique spatial characteristics associated with each spatial units.\nTemporal Prompt. To further enhance the student's temporal awareness, we incorporate two temporal prompts into the model, taking inspiration from previous works [18, 25]. These prompts include the \"time of day\" prompt, represented by  E^{(ToD)} \u2208 R^{T_1\\times d} , and the \"day of week\" prompt, represented by  E^{(DoW)} \u2208 R^{T_2\\times d} . The dimensionality of the \"time of day\" prompt is set to  T_1 = 288 , corresponding to 5-minute intervals, while the \"day of week\" prompt has a dimensionality of  T_2 = 7  to represent the seven days of the week.\nSpatio-Temporal Transitional Prompt. The spatial and temporal dependencies among nodes in the spatio-temporal graph can vary across different time periods, often reflecting daily mobility patterns, such as peak traffic during morning and evening rush hours in residential areas due to commuting. Consequently, it becomes crucial to learn spatio-temporal context with transitional prompts for different timestamps. However, this task can be time-consuming and resource-intensive, particularly when dealing with large-scale datasets. Taking inspiration from the work[11], we tackle this challenge by scaling all timestamps to represent a single day. We then employ Tucker decomposition [22] to learn the dynamic spatio-temporal transitional prompt for each node at all timestamps within a day, denoted as  N_t .\nE^{(\\beta)} = \\sum_{p=1}^{d} \\sum_{q=1}^{d} E_{kp}^{Es}E_{tn}^{Et}\nE_{t,n}^{FS} =  \\frac{exp(E^{(\\beta)'})}{\\sum_{m=1}^{dd} exp(E^{(\\beta)')}}\n(14)\nLet  E_k \u2208 R^{d\\times d\\times d}  represent the Tucker core tensor with a Tucker dimension of  d . We define  E_t \u2208 R^{N+\\times d}  to represent the temporal prompts, and  E_s \u2208 R^{N\\times d}  to represent prompts for spatial locations. Additionally,  E^{(\\beta)'} \u2208 R^{N_1\\times N\\times d}  and  E^{(\\beta)} \u2208 R^{N\\times N \\times d}  indicate the"}, {"title": "4.4 In-depth Discussion of our Proposed EasyST Framework", "content": "4.4.1 Rationale Analysis of EasyST's Robustness. Previous methods for knowledge distillation (KD) on vanilla graphs have mainly focused on robustness in handling noise. For example, NOS-MOG [21] uses adversarial training to ensure that the student model is resilient to feature noise during KD. Similarly, GCRD [15] uses self-supervised contrastive learning to enhance robustness. However, our model takes a unique approach by prioritizing information control to achieve robust KD. The information control process within our KD framework plays a crucial role in determining the inherent robustness of KD. In our proposed spatio-temporal IB principle, our EasyST aims to achieve simultaneous alignment of the encoded hidden representations  Z  with both the ground-truth  Y  and the teacher's predictions  Y_T  while reducing their correlation with the input spatio-temporal graph (STG) features  X . We posit that the input STG features are prone to noise originating from various sources, such as sensor malfunctions and inherent spatio-temporal distribution shifts. By mitigating the correlation between the hidden representations and the input features, our EasyST effectively captures environment-invariant information during the student encoding and teacher distillation process, thereby facilitating robust learning [1]. During the training stage, we optimize the loss function expressed in Equation 13. The first term of the loss function aims to minimize the discrepancy between the predicted outputs of the student model and the ground-truth labels. The second term of the loss function minimizes the difference between the student's predictions and the teacher's predictions, promoting knowledge transfer from the teacher model to the student. The third term aims to reduce the correlation with the input spatio-temporal features. By jointly optimizing these terms, our model achieves robust KD by aligning the hidden representations with both the desired outputs and the teacher's knowledge while reducing their dependence on noisy input features.\n4.4.2 Model Complexity Analysis. In this analysis, we compare the time complexity of our EasyST with other state-of-the-art"}, {"title": "4.5 Learning Process of the EasyST", "content": "We present detailed learning process of our EasyST in Algorithm 1."}, {"title": "5 Evaluation", "content": "To assess the effectiveness of our EasyST model, our experiments are designed to address the following research questions:\n\u2022 RQ1: How does the proposed EasyST framework perform compare to state-of-the-art baselines on different experimental datasets?\n\u2022 RQ2: To what extent do the various sub-modules of the proposed EasyST framework contribute to the overall performance?\n\u2022 RQ3: How scalable is our EasyST for large-scale spatio-temporal prediction?\n\u2022 RQ4: What is the generalization and robustness performance of our EasyST?\n\u2022 RQ5: How does EasyST perform with different teacher STGNNs?\n\u2022 RQ6: How do various hyperparameter settings influence EasyST's performance?\n\u2022 RQ7: How is the model interpretation ability of our EasyST?"}, {"title": "5.1 Experimental Settings", "content": "5.1.1 Experimental Datasets. To evaluate the effectiveness of our model in large-scale spatio-temporal prediction, we employ urban sensing datasets for three distinct tasks: traffic flow prediction, crime forecasting and weather prediction. i) Traffic Data. PEMS is a traffic dataset collected from the California Performance of Transportation (PeMS) project. It consists of data from 1481 sensors, with a time interval of 5 minutes. The dataset spans from Sep 1, 2022, to Feb 28, 2023. ii) Crime Data. CHI-Crime is a crime dataset obtained from crime reporting platforms in Chicago. For this dataset, we divide the city of Chicago into spatial units of size 1 km \u00d7 1 km, resulting in a total of 1470 grids. The time interval for this dataset is 1 day, covering the period from Jan 1, 2002, to Dec 31, 2022. ii) Weather Data. This is a weather dataset released by [44]. It comprises data from 1866 sensors, with a temporal resolution of 1 hour. The dataset spans from Jan 1, 2017, to Aug 31, 2021. To show the superiority of our EasyST more intuitively, we also evaluate it on the public dataset PEMS-4.\n5.1.2 Evaluation Protocols. To ensure a fair comparison, we divided the three datasets into a ratio of 6:2:2 for training, validation, and testing, respectively. For traffic prediction, we specifically focused on the flow variable to perform our predictions. For crime forecasting, we select four specific crime types for our analysis. In the task of weather prediction, our attention was directed towards the vertical visibility variable. To evaluate the performance of our model, we utilized three commonly adopted evaluation metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n5.1.3 Compared Baseline Methods. We conducted a comparative analysis of our model against 12 state-of-the-art baselines. The baseline models include: (1) Statistical Approach: HI [4]; (2) Conventional Deep Learning Models: MLP, FC-LSTM [20]; (3) GNN-based Methods: STGCN [34], GWN [25], StemGNN [2],"}, {"title": "5.2 Performance Comparison (RQ1)", "content": "Table 1 presents the comparison results of our EasyST with state-of-the-art baselines on traffic, crime and weather information, evaluating its effectiveness. The best-performing model's results are highlighted in bold for each dataset. Based on these results, we have the following observations:\n\u2022 Overall Superiority of our EasyST. Overall, our EasyST has consistently demonstrated superior performance compared to various baselines, validating the effectiveness of our approach in modeling spatio-temporal correlations. The design of our IB-based spatio-temporal knowledge distillation paradigm enables the student MLP to inherit rich spatio-temporal knowledge from the teacher STGNN while avoiding erroneous guidance and potential noise from the teacher.\n\u2022 Comparing to State-of-the-arts. Compared to GNN-based models like STGCN, GWN, StemGNN and MTGNN, our EasyST achieves significant improvements in predictive performance. The IB-constraint knowledge distillation architecture via teacher-bounded regression loss extracts valuable spatio-temporal correlations from the teacher STGNN, filtering out task-irrelevant information. The performance gap with dynamic graph-based model (e.g., DMSTGCN) highlights the effectiveness of leveraging the in-context spatio-temporal prompts to capture static spatial and temporal correlations as well as dynamic spatio-temporal"}, {"title": "5.3 Model Ablation Study (RQ2)", "content": "To verify the effectiveness of the designed modules, we perform comprehensive ablation experiments on key components of our model. The experimental results on three datasets are presented in Table 2. Accordingly, we have the following observations:\n\u2022 Spatio-Temporal Prompt Learning. We conduct experiments to remove the spatial, temporal and transitional prompts and generate three variants: \"w/o-S-Pro\", \"w/o-T-Pro\", \"w/o-Tran-Pro\", respectively. The results of these experiments show that all three types of prompts improve the model performance by injecting informative spatio-temporal contexts from the downstream tasks.\n\u2022 Spatio-Temporal IB. We exclude the spatio-temporal IB module to create a model variant: \"w/o-IB\". Upon comparing the results across the three datasets, we note that the presence of our IB module enables the student model to extract and filter significant information in assisting the downstream spatio-temporal predictions, thereby improving generalization during the encoding and knowledge distillation. This effect is particularly pronounced in the sparse crime data.\n\u2022 Teacher-Bounded Regression Loss. We substitute the bounded loss with the regular KD loss, specifically using the MAE loss (L_{kd}(\\hat{Y}, Y_T) ), to create a model variant called \"w/o-TB\". Upon evaluation, we have observed a notable decrease in the performance of our EasyST. This outcome suggests that our teacher-bounded loss for alignment can effectively alleviate to transfer erroneous information from the teacher model to the student model.\n\u2022 Spatio-Temporal Knowledge Distillation. To assess the effectiveness of our KD paradigm, we generate a model variant called \"w/o-KD\" by removing the knowledge distillation component. Upon evaluation, we have observed a significant decrease in the model's performance. This observation further solidifies the effectiveness of our proposed framework, highlighting the importance of the spatio-temporal knowledge transfer process in improving the model's performance."}, {"title": "5.4 Model Scalability Study (RQ3)", "content": "In order to evaluate the effectiveness and efficiency of our EasyST in addressing large-scale spatio-temporal prediction, we conduct a"}, {"title": "5.5 Generalization and Robustness Study (RQ4)", "content": "To further validate the robustness and generalization ability of our model, we compare it with baselines under the conditions of noisy and missing data over the PEMS traffic data. Performance w.r.t Data Noise: We artificially introduce noise to the input STG features  X  by modifying the features as  X = (1-\\gamma)X+\\gamma\\epsilon , where  \\gamma  is the noise coefficient, and  \\epsilon  is sampled from a Gaussian distribution. We gradually increase the noise coefficient from 0 (original input) to 0.3 (with an increment of 0.05) and compare our model with STGCN, DMSTGCN, and MLP. The results, shown in Figure 4 (top), demonstrate that as the noise coefficient increases, the performance gap between DMSTGCN, MLP, and our model widens. Within the 0-0.2 range, the performance gap between STGCN and our model also continues to increase. This reflects the strong noise resilience of our model, where our spatio-temporal IB paradigm filters out task-irrelevant information. Performance w.r.t Data Missing: We manually set a certain proportion of the input STG features  X  to zero, simulating the data missing problem in real-world scenarios. The missing ratio is denoted as  \\gamma = \\frac{M}{TXNXF} , where"}, {"title": "5.6 Model-agnostic Property Study (RQ5)", "content": "Our EasyST framework is model-agnostic, allowing it to be applied to different teachers. To validate its adaptability, we apply it to 4 STGNN models: STGCN, MTGNN, DMSTGCN, and StemGNN. The results on the traffic dataset are presented in Table 3. It can be observed that with our framework, the performance of all teacher models is improved, reaching the state-of-the-art level. This improvement can be attributed to our spatio-temporal IB and teacher-bounded loss, which effectively transfer task-relevant spatio-temporal knowledge to the student while filtering out noisy and misleading guidance. As a result, the positive effects of STGNN are maximized within our KD framework."}, {"title": "5.7 Hyperparameter Investigation (RQ6)", "content": "To analyze the impact of different hyperparameter configurations, we perform additional experiments where we modify a specific hyperparameter while keeping the others at their default values. We focus on four critical hyperparameters and present our experimental findings and observations based on the results obtained from the Weather dataset. The results are illustrated in Figure 6. Here are our detailed experiments and observations: i) We conduct a search for the coefficient  \\beta = \\beta_1 = \\beta_2  in the proposed IB principle, as defined in Equation 13. The search is performed within the range of  {1e - 2, 5e 3, 1e 3, 5e 4, 1e - 4} . We find that the best performance is achieved when  \\beta = \\beta_1 = \\beta_2 = 1e - 3 , which corresponds to the midpoint position of the coefficient range. ii) We explore the impact of varying the threshold  \\delta  in the bounded Knowledge Distillation (KD) loss, as defined in Equation 12. The threshold is varied within the range  {0, 0.1, 1, 5, 25}  and the optimal performance is achieved when  \\delta = 25 . iii) We investigate the influence of the coefficient  \\lambda  in controlling the loss term defined in Equation 13. The range of values for our experimental search is set to 0.1, 0.2, 0.3, 0.4 and the optimal performance is achieved when the coefficient is set to its midpoint,  \\lambda = 0.3 . iv) We conduct a search for the dimension  d  of hidden representations in the student MLP, with a range of 16, 32, 64, 128. We find that the model performs best when the dimension  d  is set to 64."}, {"title": "5.8 Model Interpretation Evaluation with Case Study (RQ7)", "content": "To provide further insights into the learned intermediate embeddings of our EasyST and other comparative models, namely DMST-GCN and StemGNN, we visualize these embeddings in Figure 5. The visualization process involves compressing the learned embeddings into a 2-dimensional space using t-SNE dimension reduction. Subsequently, a scatter plot is generated and smoothed using Gaussian kernel density estimation (KDE) to estimate the distribution of the embeddings. Figure 5 (a) illustrates the results of our EasyST which effectively allocates different spatial regions or nodes into larger and more distinct sub-spaces. On the other hand, the baseline methods heavily rely on iterative graph information propagation, which leads to over-smoothing of node embeddings and makes them more similar. Upon examining the visualizations of the baseline methods, we observe that the STGNNs tend to over-smooth the spatial region embeddings to a significant extent, resulting in the division of regions into multiple disconnected subspaces that lack cohesion."}, {"title": "6 Conclusion", "content": "In our research, we focus on addressing two crucial challenges in large-scale spatio-temporal prediction: efficiency and generalization. To overcome these challenges, we introduce a novel and versatile framework called EasyST, which aims to encode robust and generalizable representations of spatio-temporal graphs. Our framework incorporates the IB principle to enhance the knowledge distillation process by filtering out task-irrelevant noise in the student's encoding and alignment during knowledge transfer. Moreover, we introduce a spatio-temporal prompt learning component that injects dynamic context from the downstream prediction task. Through extensive experiments, we show that our EasyST surpasses state-of-the-art models in both performance and efficiency."}]}