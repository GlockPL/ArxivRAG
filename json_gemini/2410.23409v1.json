{"title": "TPP-Gaze: Modelling Gaze Dynamics\nin Space and Time with Neural Temporal Point Processes", "authors": ["Alessandro D'Amelio", "Giuseppe Cartella", "Vittorio Cuculo", "Manuele Lucchi", "Marcella Cornia", "Rita Cucchiara", "Giuseppe Boccignone"], "abstract": "Attention guides our gaze to fixate the proper location of the scene and holds it in that location for the deserved amount of time given current processing demands, before shifting to the next one. As such, gaze deployment crucially is a temporal process. Existing computational models have made significant strides in predicting spatial aspects of observer's visual scanpaths (where to look), while often putting on the background the temporal facet of attention dynamics (when). In this paper we present TPP-Gaze, a novel and principled approach to model scanpath dynamics based on Neural Temporal Point Process (TPP), that jointly learns the temporal dynamics of fixations position and duration, integrating deep learning methodologies with point process theory. We conduct extensive experiments across five publicly available datasets. Our results show the overall superior performance of the proposed model compared to state-of-the-art approaches.", "sections": [{"title": "1. Introduction", "content": "Gaze, the act of directing the eyes toward a location in the visual world, is considered a good measure of overt attention and, more generally, a window to the observer's thoughts, intentions, and emotions. It is no surprise that research spanning decades has struggled to produce several computational models aiming at effectively predicting attention towards regions or events within the landscape of visual and multimodal stimuli. With roots in psychology and neuroscience, these approaches have gained traction in the computer vision and pattern recognition fields since the seminal Itti et al. [32] model; more recently, state-of-the-art approaches rely on machine learning advancements, typically employing deep neural architectures to the purpose (but see [37] or [13] for an in-depth review). As a matter of fact, the vast majority of works in the field has focused on the computational modelling of spatial saliency in the shape of saliency maps, namely, a topographic map representing the likelihood of fixating a given location of the scrutinised stimulus, a fixation being defined as the period of time during which a part of the visual stimulus (the patch) on the screen is gazed at. Nevertheless, a growing number of models (i.e., scanpath models) are addressing the prediction of a sequence of fixations \u2013 namely, the scanpath where the gaze shift from one fixation to the next represents a saccade. Beyond the salience representation, these models explicitly unfold the dynamics of overt attention allocation over a stimulus [38]. It is worth remarking, though, that barely predicting the spatial sequence of fixations, does not entail proper modelling of the temporal evolution of attention. By and large, most scanpath models predict an ordered sequence of events while neglecting their continuous timestamp information. As a result, these models are able to tell where to look and in what order, but fail in answering when. In many respects, this is not an innocent flaw: human actions often rely on visual information, therefore it is important to direct attention to the right place at the right time [52]. Practically, modelling when to perform a saccade translates to devising scanpath models able to predict the sequence of both fixations position and corresponding duration. Albeit recently few approaches have successfully dealt with such problem [15, 16, 18, 45, 51] via fully engineered approaches, only a marginal subset of them has tackled it in a mathematically principled way [6, 21, 52]. This has typically resulted in a weaker generality of the methods which are tailored to specific contexts or applications. Under such circumstances, the chief concern of the present work is to introduce a fresh, general and simple view on the problem of scanpath modelling: in brief, we consider a scanpath as the realisation of a point process in space and time, precisely that of a Neural Temporal Point Process."}, {"title": "2. Background and Related Work", "content": "Temporal Point Processes (TPPs) are probabilistic generative models designed for continuous-time event sequences. Neural TPPs [44, 48, 64, 65] integrate key concepts from point process literature with deep learning methodologies, facilitating the creation of adaptable and effective models. Notably, the modelling assumptions of Neural TPPs align perfectly with the structure of scanpath data. A scanpath consists of a series of events (saccades) occurring at irregular intervals (fixation durations), which is exactly what Neural TPPs are designed to model. While the psychological and neuroscience literature has used traditional point processes for eye movement analysis [5, 25, 60], these tools are not well-suited for scanpath prediction due to their inability to handle stimuli. In other words, traditional TPPs are effective for studying the observer but fall short when addressing Computer Vision tasks related to attention allocation prediction. In contrast, Neural TPP-based models offer the best of both worlds: they combine the robust theoretical framework of TPPs with the flexibility and power of modern neural networks. Nevertheless, this is the first attempt to adopt them for the scanpath modelling problem.\nOur key contributions can be summarised as follows: 1) We propose a novel scanpath model able to jointly learn the temporal dynamics of both fixations position and duration. 2) We extend recent Neural TPP models to deal with visual data (i.e., images) and connect scanpath modelling and prediction to point process theory."}, {"title": "2.1. Neural Temporal Point Processes (TPPs)", "content": "Consider a sequence of generic events happening irregularly over time, TPPs model the next arrival time of an event by conditioning on the past events. Specifically, denote $H_t = {t_n \\in T : t_n < t}$ (with T representing the sequence of strictly increasing arrival times of events) the history of arrival times of all events up to time t, the relation between the current arrival time t and the history, is typically determined by the conditional intensity function $\\lambda^*(t) = \\lambda(t|H_t)$, whose functional form determines the properties of the TPP. Equivalently, the sequence of strictly positive inter-event times $T_n = t_n - t_{n-1}$ can be considered. Knowing the conditional intensity function allows to recover the conditional probability of the inter-arrival time of an event:\n$p^*(t_n) = p(T_n|H_{t_n})\n= \\lambda^*(t_{n-1} + \\tau_{n}) exp\\left(-\\int_{0}^{T} \\lambda^*(t_{n-1} + s)ds\\right)$.\n(1)\nFor instance, under the the assumptions of no dependence on the history and constancy over time (i.e., $\\lambda^*(t) = k$, with $k \\geq 0$), the homogeneous Poisson process is recovered, with inter-event times distributed according to the exponential distribution. Choosing more complex functional forms for $\\lambda^*(t)$ allows to recover many well known TPPs such as Hawkes or self-correcting processes [27,31]. Clearly, restricting $\\lambda^*(t)$ to a specific parameterisation limits the general applicability of TPPs. For this reason, most recent solutions resorted to neural approaches (Neural TPPs) implementing learnable parametric forms of the intensity function, $\\lambda(t)$ [24, 30]. As an example, early Neural TPPs, such as the Neural Hawkes Process [44], used RNNs to model the intensity function of the process. More recently, self-attention mechanisms have been employed to the same purpose [64, 65]. The choice of the parametric form for the intensity function has to take into account the necessity of a closed form solution of the integral in Eq. (1), thus practically restricting the expressiveness of the model. More complex parametric forms would require Monte Carlo approximation of the integral [44]. To overcome such limitations, Shchur et al. [48] recently proposed to directly learn the parametric conditional distribution $p_\\theta(\\tau)$ of the inter-arrival times rather than the conditional intensity function $\\lambda(t)$, thus recasting learning Neural TPPs as a density estimation problem.\nMarked TPPs. The basic mathematical formalism of TPPs allows to naturally handle the dynamics of arrival times of events. However, the distribution of time until the next event might depend on factors other than the history. Event data is often accompanied with some kind of covariate indicating the nature of the specific event being predicted. In the realm of TPPs, such covariate are called marks. More formally, a marked TPP is a random process whose realisations consists of a sequence of discrete events localised in time, ${r_{F_n},t_n}$, with the timing $t_n \\in \\mathbb{R}^+$ and the mark $r_{F_m} \\in \\mathbb{M}$. The mark $r_F$ is typically modelled as an integer representing the type of event, however other kinds of marks (e.g., $\\mathbb{M} = \\mathbb{R}^2$) can be eventually adopted. Specifying a marked-TPP involves the definition of the joint conditional density function of the next event, with inter-event time $T_n$ and mark $r_{F_m}$, given"}, {"title": "2.2. Scanpath Modelling", "content": "Modelling scanpaths involves defining a mapping from visual data, $I$ (raw image data representing either a static picture or a stream of images), to a sequence of time-stamped gaze locations $S = {(r_{F_1}, t_1), (r_{F_2}, t_2),... (r_{F_N}, t_N)}$. Here $r_{F_m} \\in \\mathbb{R}^2$ represents the two-dimensional vector of spatial coordinates of the n-th fixation on the stimulus $I$, while $t_n \\in \\mathbb{R}^+$ represents its arrival time. Eventually, a perceptual representation of the input stimuli, $Z$, is computed, with the aim of locating the relevant objects inside the scene:\n$I \\rightarrow Z \\rightarrow {(r_{F_1}, t_1), (r_{F_2}, t_2),...(r_{F_N}, t_N)}$.\n(3)\nHere we assume that no specific external task or goal is given to the observer (i.e., free-viewing condition). Notably, the dynamics of the attentive process, which unrolls as a sequence of fixations location with corresponding duration/arrival time, is characterised by an inherent randomness which likely stems from internal stochastic fluctuations affecting sensory and information processing, movement planning, and execution [54], in both fixations location and corresponding duration. Notably, many scanpath models proposed in the recent literature [2, 3, 39, 50] get rid of fixations' timestamp information by rearranging the sequence ${(r_{F_1}, t_1), (r_{F_2}, t_2), ...}$ as ${r_{F(1)}, r_{F(2)},\u2026\u2026}$, thus assuming $(r_{F_n}, t_n) = r_{F(n)}$.\nA handful of solutions [6, 21, 52] have dealt with this problem in its entirety by starting from specific theoretical frameworks. In [52] Tatler et al. modelled saccades timings as an evidence accumulation process with clear neurobiological significance. In a similar vein, in [21] a Langevin-type SDE race model [8] was adopted to predict fixations and their duration in socially relevant contexts, while in [6] fixation duration was equated to the patch residence time of a forager searching for nourishment. Conversely, the vast majority of recent methods [15, 16, 18, 45, 51] simply model fixation duration by employing specific neural architectural choices that aim at associating each fixation to its corresponding duration.\nIn a different vein, this work recasts the whole visual attention allocation process in the mathematical framework of point process theory [20]. This emphasises the central role of visual attention's spatio-temporal dynamics by explicitly modelling scanpaths as sequences of discrete events happening at irregular intervals. Specifically, we conceive a scanpath as a realisation of a random process whose events happen at strictly increasing arrival times $T = {t_1,...,t_n}$. Fixations duration can be recovered by resorting to inter-event times $T_n = t_n - t_{n-1}$, while their locations can be represented as the two-dimensional continuous mark associated to the n-th event. Under this assumption, (Neural) Temporal Point Processes (TPPs) represent the natural choice for modelling this kind of data."}, {"title": "3. Proposed Method", "content": "Given a stimulus (image) $I_j$, an ensemble of $N_{obs}$ observers performs a sequence of fixations and saccades (scanpath) on it, thus obtaining a set of sequences $C_j = {S^1,..., S^{N_{obs}}}$. Each scanpath $S^i$ is a sequence of pairs (events) $S^i_n = (r_{F_n}, t_n)$ each composed by a fixation position (marker) $r_{F_n} \\in \\mathbb{R}^2$, and a corresponding arrival time $t_n \\in \\mathbb{R}^+$. At the most general level, we are interested in modelling the stochastic generative process that given a semantic representation of the image $Z_j$ and the history of past events $H_t$, simulates the next fixation position and duration. More formally:\n$S_{n+1} \\sim p_\\theta(r_{F_{n+1}},t_{n+1}|H_t, Z_j)$,\n(4)\nwhere $p_\\theta()$ represents the parametric joint conditional distribution of a Neural TPP [48]."}, {"title": "3.1. Architecture", "content": "In the following, we present the architecture of TPP-Gaze, implementing a scanpath model on an image as a Neural TPP."}, {"title": "Representing Scene Semantics.", "content": "As outlined in Eq. (3), the sequence of events composing a scanpath depends not only on the history of past events, but on a perceptual representation of the input stimulus $I_j$, encoding scene semantics and relevant objects location. We extract the perceptual representation of the input image via a CNN architecture inspired by [39]. Specifically, the input image is first processed by a pre-trained DenseNet201 CNN [29]. Activation maps from various convolutional layers (as reported in [39]) are extracted, thus obtaining a 2,048 channels volume, each representing the location of semantic features inside the scene. It is worth noticing that learning to predict fixations location (i.e., marks) involves a mapping between coordinates in Cartesian space, a task in which standard convolutions have been reported to fail [41]. In the vein of [43, 50], we adopt a CoordConv layer [41] to give convolutions access to their own input coordinates. This results in a 2,051 channels volume which is fed as input to 3 layers of 1 \u00d7 1 convolutions, followed by a linear layer mapping to $z_j$ acting as our semantic representation."}, {"title": "Representing History.", "content": "Neural TPPs employ either Recurrent Neural Networks (RNNs) and their variants (e.g., LSTM, GRU) [24, 48, 55] or Transformer encoders [64, 65] to model the nonlinear dependency over both the markers and the timings from past events [49]. As shown in Fig. 2, the pair $(r_{F_n}, T_n)$ representing the event occurring at the time $t_n$ with fixation position $r_{F_n}$ and duration $T_n = t_n - t_{n-1}$, is fed as the input into either a GRU or a Transformer encoder as described in [65]. The Transformer/GRU state embedding $h_n$ represents the influence of the history up to the n-th fixation. Hence, can be employed as a vector space representation of $H_{t_n}$. Taking into account the semantic representation $z_j$ and the history embedding $h_n$, Eq. (4) can be rewritten as:\n$S_{n+1}\\sim p_\\theta(r_{F_{n+1}}, t_{n+1}|h_n, z_j)$.\n(5)"}, {"title": "Fixation Duration Generation.", "content": "We model the conditional dependence of the distribution $p_\\theta (T_{n+1}|h_n, z_j)$ on both past events and stimulus by concatenating the history embedding and semantic vectors into a context vector $c_{j,n} = [h_n||z_j]$. In the vein of [48], the latter is employed to learn the parameters of a Log-Gaussian Mixture Model (LGMM) via an affine transform:\n$\\mathbf{w} = \\text{softmax}(V_w c_{j,n}) \\qquad \\mathbf{s} = \\text{exp}(V_s c_{j,n})$\n$\\mathbf{m} = V_m c_{j,n}$\n(6)\nwhere $\\mathbf{w} \\in \\mathbb{R}^K$ are the mixture weights, $\\mathbf{m} \\in \\mathbb{R}^K$ are the mixture means, and $\\mathbf{s} \\in \\mathbb{R}^K$ are the standard deviations. $K$ represents the number of mixture components. The fixation duration for the n-th event can be generated by sampling from the LGMM defined by:\n$p_\\theta(T_n |c_{j,n}) = p(T_n|\\mathbf{w}, \\mathbf{m}, \\mathbf{s})\n= \\sum_{k=1}^K w_k \\frac{1}{T_n s_k \\sqrt{2\\pi}} \\text{exp}\\left(-\\frac{(\\text{log }T_n - m_k)^2}{2 s_k^2}\\right)$.\n(7)"}, {"title": "Fixation Position (Mark) Generation.", "content": "Similarly, given the context vector $c_{j,n}$, we define the conditional probability of the next mark (fixation position), $p_\\theta(r_{F_{n+1}}|h_n, z_j)$, as a 2D Gaussian Mixture Model (GMM) whose parameters are obtained via another affine projection:\n$\\omega_g = \\text{softmax}(R_\\omega c_{j,n}) \\qquad \\Sigma_g = \\text{diag}(\\text{exp}(R_\\Sigma c_{j,n}))$\n$\\mu_g = R_\\mu c_{j,n}$\n(8)\nwhere $\\omega_g \\in \\mathbb{R}^G$ are the mixture weights, $\\mu_g \\in \\mathbb{R}^2$ are the mixture means, and $\\Sigma_g \\in \\mathbb{R}^{2\\times 2}$ are the diagonal covariance matrices of G bi-variate Gaussian distributions. The x and y coordinates of the n-th fixation can be generated by sampling from the GMM defined by:\n$p_\\theta(r_{F_n} |c_{j,n}) = p(r_{F_n} |\\omega, \\mu, \\Sigma)\n= \\sum_{g=1}^G \\omega_g \\frac{\\text{exp}(-0.5 (r_{F_n} - \\mu_g)^T \\Sigma_g^{-1} (r_{F_n} - \\mu_g))}{\\sqrt{(2\\pi)^2 |\\Sigma_g|}}$.\n(9)"}, {"title": "3.2. Model Inference", "content": "Consider a set of stimuli $I = {I_1, ..., I_j, . . ., I_J}$ each gazed by $N_{obs}$ human observers. Each observer produces an ensemble of scanpaths $C_j = {S^1,..., S^{N_{obs}}}$ with $S^i_n =(r_{F_n}, t_n)$ representing an event (i.e., fixation position and duration). Model inference is performed by minimising a negative log-likelihood loss with respect to the parameters of the semantic network, the GRU/Transformer encoding history of events, and the affine transforms of the LGMM and GMM. Formally, the loss function is defined as follows:\n$L(\\theta) = - \\sum_j \\sum_i \\sum_n [\\text{log}p_\\theta (T_n |c_{j,n}) + \\text{log }p_\\theta (r_{F_n} |c_{j,n})]$.\n(10)"}, {"title": "4. Experiments", "content": "Regarding the stimuli and eye tracking data, we select five publicly available datasets of human recorded scanpaths comprising both fixation positions and durations: COCO-FreeView, MIT1003, OSIE, NUSEF, and FiFa."}, {"title": "4.1. Experimental Setup", "content": "Following previous literature [18, 39], during training and evaluation, we discard the first fixation and removed all scanpaths containing less than four fixations."}, {"title": "4.2. Scanpath Prediction", "content": "As to the latter criteria, following the taxonomy proposed in [37], scanpath models can be aggregated into the following categories: biologically inspired (e.g. Itti-Koch model [32] and G-Eymol [61]); statistically inspired (e.g. CLE model [7]); cognitively inspired (e.g. IOR-ROI-LSTM [18]); engineered models (e.g. DeepGazeIII [39] and Scanpath-VQA [15]); but see [37-39] for an in-depth review. Under such circumstances, we assess the performance of TPP-Gaze against the aforementioned models."}, {"title": "4.3. Applications", "content": "Saliency Prediction. The performance of TPP-Gaze are further evaluated by comparing the saliency maps \"backward\" generated from fixations with those of human observers across all evaluated scanpath models."}]}