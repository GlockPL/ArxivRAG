{"title": "Self-Supervised Learning for Time Series", "authors": ["Andreas L\u00f8vendahl Eefsen", "Nicholas Erup Larsen", "Oliver Glozmann Bork Hansen", "Thor H\u00f8jhus Avenstrup"], "abstract": "Accurate time series forecasting is a highly valuable endeavour with applications across many industries. Despite recent deep learning advancements, increased model complexity, and larger model sizes, many state-of-the-art models often perform worse or on par with simpler models. One of those cases is a recently proposed model, FITS, claiming competitive performance with significantly reduced parameter counts. By training a one-layer neural network in the complex frequency domain, we are able to replicate these results. Our experiments on a wide range of real-world datasets further reveal that FITS especially excels at capturing periodic and seasonal patterns, but struggles with trending, non-periodic, or random-resembling behavior. With our two novel hybrid approaches, where we attempt to remedy the weaknesses of FITS by combining it with DLinear, we achieve the best results of any known open-source model on multivariate regression and promising results in multiple/linear regression on price datasets, on top of vastly improving upon what FITS achieves as a standalone model.\nOur code developed and used for the project is available in the following GitHub Repository: github.com/thorhojhus/ssl_fts/", "sections": [{"title": "1 Introduction", "content": "In this project, we aim to explore time series forecasting with the use of state of the art machine learning models. Time series analysis and forecasting is an important tool in many fields of science and business, which leads to an interest in developing more effective and precise time series forecasting models.\nIn the financial sector, forecasting stock prices, exchange rates, and economic indicators can bring a competitive edge to investors by managing risks and optimizing their portfolio returns. Healthcare providers can employ forecasting to predict diseases and take preventive measures. Additionally, the integration of sensor data in healthcare enables continuous monitoring of patient health, allowing for real-time analysis and more accurate predictions of medical conditions. And in the energy sector, forecasting electricity demand and renewable energy production is essential for grid stability and resource planning. Accurate forecasting is critical because electrical grids require a delicate balance between supply and demand. If the grid receives too much electricity, it can lead to overloading and potential damage to transmission lines, transformers, and other equipment. On the other hand, if there is insufficient electricity supply to meet the demand, it can result in power outages, blackouts, and disruptions to businesses and households. This can have massive negative cascading economic and social effects, as modern society relies heavily on a stable and reliable power supply.\nThe primary assertion of the FITS paper is that with just 10k-50k parameters, their method reaches comparable performance with previous state-of-the-art models of much larger parameter size, namely TimesNet (300.6M) [Wu+22b], Pyraformer (241.4M), FEDformer (20.68M), Autoformer (13.61M), and PatchTST (1M) [XZX24] on datasets from the aforementioned sectors. This is a substantial claim and we aim to reproduce their results and afterwards use the model on other datasets."}, {"title": "1.1 Time Series Data & Forecasting", "content": "A time series is a continuous or discrete series or sequence of observations over a time frame. A lot of real world data would conform to the format of a time series, examples are rainfall volumes over a given day or month, temperature over a year, sales of products over months, continuous sensor data, as well as changes in concentrations in chemical substances. There are examples of time series data abound in many fields including economics and business, chemistry and natural sciences, engineering as well as social sciences.\nA time series, denoted ZT, will be sampled such that the the discrete time series will have equidistant time points with a total of T observations, for which we denote the observed value at time t < T as Zt.\nFor the forecasting task, the forecasted time series at time step t + l > T is denoted as 2t+h. The forecast horizon, also known as the leading time, is here denoted l. The goal is to minimize the error from the forecasted values to that of the real time series for example the mean squared error:\n$\\min \\frac{1}{l} \\sum_{t=k}^T (\\hat{Z}_{t+1} - Z_{t+1})^2$ (1)\nThe difficulty in forecasting varies greatly and depends on the type of time series one wishes to predict. Data with clear harmonics, trends and little volatility will be easier and more simple to forecast than more volatile and chaotic data such as stock prices and noisy data.\nA time series can also express properties of stationarity and seasonality. These two properties are critical in helping to understand how to build a model for the problem."}, {"title": "Seasonality in Time Series", "content": "There exists an abundant amount of time series data, where patterns repeat in intervals. Populations of certain animals, the average temperature during a month and even sales of merchandise can show signs around holidays and other times of the year.\nWhat they all share, is a repeating pattern. In literature, the usual letter denoting the length of the pattern, or seasonality length, is m; a simple example would be a sine curve, whose seasonality would be m = 2\u03c0"}, {"title": "Stationarity in Time Series", "content": "A weakly stationary time series exhibits no different behaviour when shifted in time. In simple probability terms, we require that the probability distribution over the time series should not depend on the time i.e., $f_{z\\T=t}(z|T = t) = f_Z(z)$.\nIn other words, if we write the time series as:\n$Z_t = \u03bc + \u20ac_t + \\psi_1\u20ac_{t-1} + \\psi_2\u20ac_{t-2}+...$\nThen a requirement of stationarity is that the weights, \u03c8, have to be absolutely summable meaning $\\sum \\psi_i < \u221e$, and as such, the parameter \u00b5 has a meaning denoting the level the time series varies about. The covariance between $z_t$ and $z_{t-j}$ should also be equal, and only depend on the relative time difference."}, {"title": "1.2 Evolution of Forecasting Methods", "content": "Forecasting has undergone a significant transformation over time, evolving from simple statistical approaches to more sophisticated ML techniques and lately transformer-based models. This section provides an overview of the key milestones in the evolution of forecasting methods."}, {"title": "1.2.1 Statistical Forecasting", "content": "The late 19th and early 20th centuries saw the emergence of statistical forecasting methods. These techniques, such as moving averages, exponential smoothing, and linear regression, used historical data to identify patterns and make predictions. They provided a more systematic and data-driven approach compared to intuition-based methods. Specially the ARIMA model is of interest, and will be described in more detail in the appendix 8."}, {"title": "1.2.2 Machine Learning-based Models", "content": "With the advent of machine learning, forecasting methods have undergone a significant transformation. Machine learning algorithms, such as artificial neural networks, long short-term memory (LSTM) networks, random forests, and support vector regression, have been applied to forecasting tasks. These methods can handle complex patterns, nonlinearities, and large datasets, often outperforming traditional statistical methods."}, {"title": "1.2.3 Transformer-based Models", "content": "In recent years, Transformer-based models have emerged as a popular architecture surrounding ANNs, garnering significant attention in machine learning communities due to their unique ability to capture long-range dependencies and complex patterns in sequential data. The main breakthrough of Transformer-based models is the multi-headed attention mechanism which updates the weights of the model based on all input steps. This allows it to retain more information about how each step in the sequence affects each other. One of the major pitfalls of the traditional self-attention mechanism, however, is the O(L2) (L = input length) time and memory complexity when training models.\nOne of the first papers to use Transformer architecture on time series data was Informer [Zho+21]. Informer's main focus is improving the speed of inference, training, and efficiently handling long inputs. Among other things, it introduces a new \"ProbSparse\" self-attention mechanism, which instead achieves O(Llog L) and works by focusing on a subset of the most informative parts of the input sequence by selecting only the top u queries that have the highest sparsity measurements. Informer achieved an average of ~ 1.6 MSE and ~ 0.79 MAE across all datasets.\nHalf a year later, Autoformer comes out [Wu+22a], claiming state-of-the-art accuracy with an average of ~ 0.86 MSE (48% lower) and ~ 0.53 MAE (33% lower) compared to Informer. Using a new technique dubbed Auto-Correlation, they instead operate at the series level while ProbSparse operates at the point/query level, and also uniquely uses self-attention distilling in the encoder by halving layer inputs. Furthermore, Autoformer introduces a series decomposition block which progressively extracts and aggregates the long-term stationary trend from predicted intermediate results, allowing it to also capture general patterns in long sequences.\nFEDformer [Zho+22] hopes to also take the overall trend into account by combining the Transformer, capturing more detailed structures, with their seasonal-trend decomposition method and Frequency enhanced blocks, both capturing a more global view of the time series. Applying a Fourier transform to map the time series into the frequency domain had already been done in the 80s but this is the first"}, {"title": "1.2.4 New frontiers", "content": "Using just a basic one-layer linear model directly mapping the input sequence to the output sequence, DLinear [Zen+22] claims Transformers are ineffective for time series forecasting. The linear models in this paper are simply linear layers (i.e., fully connected layers without non-linear activations) that map the input sequence directly to the output sequence. Linear is a vanilla linear model, NLinear first normalizes the input by subtracting the last value before applying a linear layer and adding the subtracted value back to handle distribution shift, and DLinear decomposes the input into trend and remainder components which are processed separately by linear layers before being summed for the final prediction.\nHowever, in 2022, PatchTST [Nie+23] challenges this claim by slightly outperforming DLinear on most datasets. PatchTST introduces: (1) segmentation of time series into subseries-level patches as input tokens to the Transformer, and (2) channel-independence where each univariate time series is processed separately but shares the same embedding and Transformer weights. These modifications make PatchTST quite superior to other transformers and slightly better than using simple linear layers.\nFITS (Frequency Interpolation Time Series Analysis Baseline) is a recently (2023) proposed model that leverages Fourier transforms in conjunction with a single complex-valued linear layer for time series forecasting. Despite this simplicity, it achieves comparable performance to state of the art models with significantly fewer parameters. In another effort to reduce parameter size, FITS uses a low-pass filter in the frequency domain to capture relevant patterns without with minimal impact on test performance. By training in the complex frequency domain, the neural network layer efficiently learns amplitude scaling and phase shifting to interpolate time series. This is a very innovative approach compared to anything seen before and is the only one (of the aforementioned) to introduce any filters on the data. FITS reports to outperform DLinear by 8.80% on average."}, {"title": "2 Research Questions", "content": ""}, {"title": "2.1 Deep FITS", "content": "FITS uses only a single linear layer without any activation functions and can be considered a form of linear regression on the frequency-time data produced using FFT. This simplicity facilitates its small profile in terms of parameters with fast inference and training time but it might also be a potential area of improvement. To explore whether enhancing this model could improve performance, we experiment with multiple deep variant of FITS. These variants consists of a configurable number of linear layers, ReLU-like activation functions, and dropout layers either in the upscaling part of the network, after the reverse Fast Fourier Transform or parallel to the frequency part of the network."}, {"title": "2.2 Combining FITS with DLinear", "content": "In the evolution of forecasting methods, there is a clear tendency for model improvements to stem from processing the data in different ways before making predictions, rather than simply scaling up the models or complexifying the model architecture. DLinear decomposes its input into trend and seasonal components, patterns we notice after some preliminary testing that FITS fails to capture, and models them separately with linear layers before being summed for the final prediction.\nFITS takes a different approach by first transforming the time series data into the frequency domain before that modeling with a complex-valued linear layer, and achieves superior results on some datasets.\nWe hypothesize an integrated approach of the two models will lead to improved performance by leveraging the strengths of both methods."}, {"title": "2.3 Measure of randomness and unpredictability in datasets [\u2192 3.8]", "content": "While reviewing other literature, we were astounded to find that for one of the datasets in [Zen+22], the Repeat column, i.e xt = xt-1, beat all other models in spite their complexity or simplicity on short forecasting horizons. This could imply that if some datasets are sufficiently random, models will incorrectly capture noise as patterns and have worse prediction power.\nIn [Nie+23], it is argued (with citation to [Ros13]) that the toughest benchmark for forecasting is a random walk without drift, which closely resembles and describes the exchange rate dataset which had the aforementioned quirk. Apparently, the prevailing consensus among statisticians and economists is that if a market is \"efficient\", meaning all public information is already priced in and new information that affects stock prices are assumed to be unpredictable and random, the asset price becomes impossible to predict, and the best prediction for xt on average will just be xt-1 as published by the acclaimed Eugene Fama in [Fam70]. Intuitively, this also makes sense given that the mean of n random walks as n \u2192 \u221e will be equal to the drift (a straight line) [ref. fig. 26] as it is equally likely to be up or down.\nWe want to investigate whether we can find a general numerical measure of this unpredictability in other datasets as well. For that, we introduce the Hurst exponent."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Complex Frequency Domain", "content": "In time-series data, the signal is a series of observations in a chronological order, often with equally spaced points representing an amplitude corresponding to some quantity at a fixed sampling rate. This domain focuses on understanding how the signal evolves over time, identifying trends, seasonal patterns, and any anomalies or outliers that may exist. However, while the time-domain analysis provides valuable insights into how the signal changes over time, it often fails to reveal the underlying periodic structures and frequency content of the data.\nThis is where frequency analysis becomes incredibly useful. Frequency analysis allows us to decompose a time-domain signal into its sinusoidal components, each characterized by a specific frequency, amplitude, and phase. By transforming the signal from the time domain to the frequency domain, we can uncover its inherent periodicities and identify dominant frequencies.\nIn frequency analysis the frequency component of signals can be represented as a complex number, which captures both the amplitude and phase of the component. The exponential notation for such a number is:\n$z = A e^{i\u03b8}$\nWhere A is the amplitude and @ is the phase of the number. This form is useful in the context of Fourier transforms where signals are decomposed into their sinusoidal components expressed by the complex exponential.\nTo calculate the real and imaginary components of the number the rectangular form can be used:\n$z = A cos(\u03b8) + iA sin(\u03b8)$\nWhere A cos(0) represents the real component of the number and i A sin(0) the imaginary component."}, {"title": "3.1.1 Fourier Transform", "content": "A fourier transform is a broadly used method for decomposing a function of time, such as time series data, into the frequencies the function consists of. The Fourier transform essentially transforms data in the time domain to the frequency domain. The inverse function, a synthesis, for Fourier transformation also exists, and converts the frequencies back into time domain data, which is also referred to as the inverse Fourier transform. Because our data is in the time domain, Fourier transforms are a very powerful tool to disassemble the different attributes (frequencies) in our data, especially if there are any patterns that are prevalent through an extended period of time.\nThe formula for a continuous Fourier transform is\n$F(w) = \\int_{-\\infty}^{\\infty} f(t).e^{-iwt} dt$\nWhere F(w) is the frequency-domain representation, f(t) is the time-domain signal, and $e^{-iwt}$ is the basis function for decomposing a signal into its frequency components. When you integrate the function over a specific amount of time you get a function that represents amplitude and phase of each frequency component in the signal provided. The inverse Fourier transformation from frequency components to time components is given by:\n$x(t) = \\int_{-\\infty}^{\\infty} F(w)e^{iwt}, dt$\nA continuous signal is problematic for computers to effectively work with and as such they are sampled with various rates. The length of your sampled signal will allow for greater representations in the frequency domain, and as such a faster sampling rate will give a greater representation at the cost of memory and storage. A critical concept for determining the sampling rate for a signal is the Nyquist-Shannon sampling theorem, which states that to accurately reconstruct a continous signal from the sampled version the sampling rate must be at least twice the highest frequency of the signal:\n$f_s \u2265 2f_{max}$\nWhere fs is the sampling rate and fmax is the highest frequency present in the signal.\nIn many applications related with computer science, it is natural to sample a continuous signal to discretize it. Naturally, as the time series data provided is already in a computer-readable format, we need to perform discrete Fourier transforms (DFT). The formula for a discrete fourier transform is seen in equation 2."}, {"title": "3.1.2 Fast Fourier Transform", "content": "To address this inefficiency, we use the Fast Fourier Transform (FFT), which is an optimized algorithm for computing the Fourier transform. The FFT significantly reduces the computational cost by exploiting the symmetry and periodicity properties of the DFT. The FFT algorithm reduces the time complexity to O(N log N), making it much more feasible for practical applications.\nThe FFT often used is the Cooley-Tukey algorithm, which operates by recursively breaking down a DFT of any composite size N = N1 N2 into many smaller DFTs of sizes N\u2081 and N2, simplifying the computation. This divide-and-conquer approach leverages the efficiency of smaller DFTs to compute the larger one more quickly. The steps of the Cooley-Tukey algorithm are described in further detail in the appendix 8."}, {"title": "Real Valued Fast Fourier Transform", "content": "A simple increase in computational efficiency is possible when working with real valued data, X \u2208 IR. In such case, only approximately half the computations are required to perform the FFT, as the output will be complex conjugates of one-another:\n$X_k = \\overline{X_{N-k}}, \u2200k\u2208 [1, \\frac{N}{2}]$"}, {"title": "3.2 FITS - Frequency Interpolation (for) Time Series", "content": "In the project, the FITS model is written and reimplemented by the team, in order to assess and evaluate it on the original paper's datasets as well as some additional ones. The FITS model as described in the original paper does a series of simple data manipulations and transformations. The pipeline of the FITS model can be seen in the figure 1 below."}, {"title": "3.3 Interpreting FITS", "content": "At its core FITS interpolates from the frequency components of a given input to that of a longer signal using a single weight matrix. To understand this process it makes sense to look at simple signals to clearly visualize how it works with the frequency domain.\nWhen training FITS on a signal consisting of a combination of a base sine wave and multiple sine waves of higher frequency based on the higher order harmonics of the base signal it's possible to reconstruct the signal up to the cutoff frequency corresponding to the harmonic order of the signal chosen. In figure 3 this is visualized up to the 5th harmonic where everything above is discarded as it is considered \"noise\" due to the choice of cutoff. As the signal is deconstructed to its frequency components by the FFT, the network is able to reconstruct and predict on signals consisting of arbitrary combinations of these component frequencies up to the cutoff as seen in figure 4. This allows FITS to interpolate between the seen frequencies of signal and makes it robust in making predictions on time series data where the signal has inherent periodicity. Even though this example is very simple it illustrates the major mechanisms of FITS with weight values corresponding the frequencies of the harmonics of the input signal to the harmonics extrapolated in the reconstruction which can be seen for a complex multichannel dataset in the appendix: 11"}, {"title": "3.4 Deep FITS", "content": "To explore adding layers to the complex domain in FITS, modifications to the ReLU activation function had to be introduced since ReLU is incompatible with complex numbers. We implemented the two variants ModReLU and CReLU as described in [Tra+18] to translate ReLU to the complex domain.\nCReLU extends ReLU by considering the real and imaginary component for the ReLU operation as separate components:\n$CReLU(z) = ReLU(R(z)) + iReLU(I(z))$\nNote that the phase of the number z is modified due to the separate application of the ReLU function on the real and imaginary parts. Specifically, the magnitude of the complex number can change depending on whether the real or imaginary part (or both) are non-negative. If both parts are non-negative, the magnitude increases. If either part is negative, that part is set to zero, potentially reducing the overall magnitude. The original angle (or phase) of the complex number z can be altered significantly. For instance, if z lies in the third or fourth quadrant (where both the real and imaginary parts are negative), applying the ComplexReLU function will result in CReLU(z) = 0, thus collapsing the phase information to zero.\nModReLU works by only considering the magnitude |z| of the complex number z in ReLU together with a learnable parameter b and scaling z by it:\n$\\text{modReLU}(z) = ReLU(|z|+b)e^{id_z} = \\begin{cases} (z+b)e^{id_z}, & \\text{if } |z|+b \\geq 0, \\\\ 0, & \\text{otherwise,} \\end{cases}$"}, {"title": "3.5 Hybrid Models", "content": ""}, {"title": "3.5.1 DLinear", "content": "The original DLinear model first decomposes the input time series into a trend component and a remainder (seasonal) component using a moving average filter with a kernel size of 25. The trend component captures the long-term progression of the time series, while the remainder represents the seasonal or periodic fluctuations. It then uses two separate linear layers to learn the patterns of each component and sums them to produce the final prediction."}, {"title": "3.5.2 DLinear + FITS", "content": "In DLinear + FITS, we extend DLinear by incorporating FITS on the residual signal. After applying the standard DLinear decomposition and training the respective linear layers to produce predictions for the trend and seasonal parts, we compute the residual. The residual in this context is the difference between the original input time series and the sum of the predictions from DLinear during training. It represents the part of the signal that DLinear's trend and seasonal components couldn't explain or predict. This residual signal is then passed through and trained in a standard implementation of FITS to hopefully capture any remaining patterns or fluctuations that were not captured by the previous components. Finally, the predicted trend, seasonal components, and the forecast portion of FITS output are summed to produce the final prediction."}, {"title": "3.5.3 FITS + DLinear", "content": "In the FITS + DLinear model, we reverse the order of operations compared to DLinear + FITS. First, the input time series is processed through FITS, which as we know, normally returns its predictions for the entire sequence and forecast horizon. Its output then serves as training data for DLinear which produces the final prediction. This approach differs from DLinear + FITS in that FITS is applied to the entire input signal rather than just the residual of DLinear's predictions. By applying FITS first, we allow the model to capture the complex frequency patterns, ideally the main cyclical components in the data, before going through the seasonal and trend decomposition steps (and subsequent training). This may help in situations where the input signal contains periodic patterns that are not as easily separated by the moving average filter. The subsequent DLinear model can then focus on refining its predictions based on the \"denoised signal\" provided by FITS rather than the raw \"noisy\" input data. Although in hindsight, we could maybe do away with the moving average filter completely, as FITS"}, {"title": "3.6 Hurst exponent", "content": "The Hurst exponent H seeks to provide insight into the behavior or predictability of a time series by quantifying the tendency of a dataset to exhibit trending, mean-reverting, or random walk behaviour. H always takes a value between 0 and 1 and is formally defined as\u00b9\u201a\u00b2:\n$E[\\frac{R(n)}{S(n)}] = C_H n^H \\text{ as } n \\rightarrow \\infty$\nWhere\nR(n) = max (Z1, Z2, ..., Zn) \u2013 min (Z1, Z2, ..., Zn) (Calculate the range R)\n$\\text{\u03a3}^{t}_{t=1}$ for t = 1, 2, ..., n (Cumulative deviation from the mean-adjusted series)\nYt = Xt - \u03bc for t = 1, 2, ..., \u03b7 (Remove trend, keep fluctuations)\n\u03bc = $\\frac{1}{n} \\text{\u03a3} X_i$ (Mean of the series X)\nAnd\n$S(n) = \\sqrt{ \\frac{1}{n} \\text{\u03a3} (X_i \u2013 \u03bc)^2 }$ (Standard deviation S of the original series X)\nH is then fitted as a straight line using least-squares regression on log[R(n)/S(n)] as function of log n where the slope gives H (ref. figure 7)."}, {"title": "3.7 Autocorrelation function (ACF)", "content": "The autocorrelation function (ACF) is a statistical tool used to measure the correlation between a time series and a lagged version of itself. It quantifies the degree of similarity between observations as a function of the time lag between them. The ACF is also useful for identifying patterns, such as persistence or mean reversion, in a time series.\nThe ACF is defined as\u00b3:\n$\u03c1(k) = \\frac{Cov(X_t, X_{t+k})}{Var(X_t)}$"}, {"title": "Where", "content": "$Cov(X_t, X_{t+k}) = \\frac{1}{n-k} \\text{\u03a3} (X_t \u2013 \u03bc)(X_{t+k} \u2013 \u03bc)$ (Autocovariance at lag k)\n$Var(X_t) = \\frac{1}{n} \\text{\u03a3} (X_t \u2013 \u03bc) ^2$ (Variance)\n\u03bc = $\\frac{1}{n}\\text{\u03a3} X_t$ (Mean)\nThe ACF is related to the Hurst exponent in that they both provide information about the memory or persistence of a time series. A time series with long-term memory or persistence a (H close to 1) will typically exhibit slowly decaying autocorrelations and remains positive for many lags, because it mostly correlates with itself. Conversely, a time series anti-persistent behaviour (H close to 0) will show rapidly decaying and quickly oscillating autocorrelations because of its erratic nature. If the time series is completely uncorrelated exhibiting no memory (H = 0), meaning all steps are i.i.d., stationary, and uncorrelated, the ACF should be close to zero for all lags k > 0. Note that this is different from a random walk as every next time step becomes its new stationarity, therefore exhibiting some autocorrelation that decays slow due to the dependence of each step on the previous step."}, {"title": "3.8 Baseline Models in forecasting", "content": "As forecasting can be a difficult task, some of the simplest models can perform quite well. As we recall from section 2.3, if we assume a random walk without drift, meaning each data point is independent and random, no model will predict better than the simple yet surprisingly effective forecasting model Na\u00efve Forecasting Model. The idea is simple; predict the next L time steps to be equal to the last observed value:\n$\\hat{z}_{T+l\\T} = Z_T, \\forall l \\in [1...L]$\nThis provides a solid baseline for which to benchmark more sophisticated models against. If this baseline model reliably predicts better than the rest, it strongly hints that the dataset is fundamentally unpredictable.\nAnother straightforward and simple baseline for time series data is simply to forecast the next L values as the mean of the observed time series, a Mean Valued Forecast:\n$\\hat{z}_{T+l\\T} = \u03bc_z, \\forall l \\in [1\u2026 L]$\nA simple linear regression on the time series data could also serve as a baseline but for reasons will be expanded on later, has shown to be an inferior baseline and will not be utilized.\nThese are the the most simple forecasting models, and we include the Na\u00efve forecasting as a baseline comparison in the results section on page 19."}, {"title": "3.9 Autoregressive Integrated Moving Average Models (ARIMA)", "content": "Moved to appendix. 8"}, {"title": "4 Data", "content": ""}, {"title": "4.1 Real data", "content": "Make descriptions of how the data looks plotted and asses any potential for forecasting on the data such as any patterns that are present throughout the data (or parts of it)\nWe use 9 common datasets used to benchmark time-series based models [Wu+22a]. This is used as a baseline to easily reproduce previous papers' results and compare new modifications. Their descriptions are outlined in the tables below. Summary statistics have been omitted from the tables below because of the vast number of independent variables.\nelectricity.csv contains the hourly electricity consumption of 321 different clients in kWh from 2016-07-01 to 2019-07-02.1\nETTh1.csv, ETTh2.csv, ETTm1.csv, and ETTm2.csv are all datasets of Electricity Transformer Temperature where each column measures different loads and their usefulness and Oil Temperature (OT) from 2016-07-01 to 2018-06-26.2\ntraffic.csv contains hourly recordings of road occupancy rates from 862 different sensors on San Francisco freeways between 2015-01-01 and 2015-04-27 from California Department of Transportation.3\nnational_illness.csv contains weekly recorded influenza-like illness (ILI) from CDC in United States from 2002-01-01 and 2022-06-30.4\nweather.csv is a collection of German weather data with 21 different meteorological indicators.5\nexchange_rate.csv is a collection of daily exchange rates of eight foreign countries including Australia, British, Canada, Switzerland, China, Japan, New Zealand and Singapore ranging from 1990 to 2010.6\n$GD and $MRO stock data collected from the python library yfinance7. Comprises of stock data changes over 15723 days, or approximately 43 years, of stock data. The columns consist of \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\".\nFurthermore, some of the data has been cleaned of invalid or extreme outlier values, like -9999 for certain variables in weather.csv. We have replaced those values with interpolation between the previous and next value of the corresponding variable. Other than that, no processing has been made to any of the other datasets."}, {"title": "4.2 Synthetic data", "content": "In order to help identify some of the models weaknesses and strengths, we developed a framework for generating synthetic data to allow us to manipulate trends, noise, and other characteristics. The purpose of this is to help us see how the nature of the data influences FITS' predictive power.7\nWe have not used any synthetic datasets for official benchmarking but it has been used for interpreting and experimenting with FITS."}, {"title": "4.3 Training Setup", "content": "Our training process begins by calculating the training loss on the combined input and output of the signal, providing the model with the most comprehensive information. Afterwards, we enter a fine-tuning stage, where the loss is computed between the prediction part of the network and the unseen part of the signal. In summary, we first consider both x and y, and then focus solely on y.\nWe use a training, validation, and test split of the datasets to train the models effectively. To prevent overfitting on the training set, we use the validation set to adjust the learning rate and implement early stopping if no improvement is observed within a specified number of epochs. At the end of each stage, we calculate the test loss in terms of Mean Squared Error (MSE) exclusively on the prediction part of the output.\nThe dataset is split into training, validation, and test sets in a 70:10:20 ratio respectively. For ETT we split 60:20:20 as that is what others report on."}, {"title": "5 Results", "content": "Below we report the results of our various models and baselines on all the different dataset trained and test with multivariate input and multivariate output similar to the reported"}, {"title": "5.1 Common benchmark datasets", "content": "To reproduce the results reported in [XZX24", "follows": "nFor the ETTh1 dataset", "parameters": "a lookback window of 720, a base period of 96 (representing 15-minute intervals per day), and a harmonic order of 14. The Electricity dataset features a lookback window of 720, a base period of 24 hours, and a harmonic order of 10. Similarly, the weather dataset has a lookback window of 720, a base period of 144 (equivalent to 10-minute intervals per day), and a harmonic order of 12 but notably is the only dataset that is trained on in individual mode - having independent layers for all channels. Traffic has a base period of 24 hours and harmonic order of 10.\nThe base period (base_T) for each dataset reflects the number of particular time intervals within a day at which the dataset was sampled, while the harmonic order denotes the n-th harmonic derived"}]}