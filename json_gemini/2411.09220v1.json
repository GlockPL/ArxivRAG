{"title": "Transferable Adversarial Attacks against ASR", "authors": ["Xiaoxue Gao", "Zexin Li", "Yiming Chen", "Cong Liu", "Haizhou Li"], "abstract": "Given the extensive research and real-world applications of automatic speech recognition (ASR), ensuring the robustness of ASR models against minor input perturbations becomes a crucial consideration for maintaining their effectiveness in real-time scenarios. Previous explorations into ASR model robustness have predominantly revolved around evaluating accuracy on white-box settings with full access to ASR models. Nevertheless, full ASR model details are often not available in real-world applications. Therefore, evaluating the robustness of black-box ASR models is essential for a comprehensive understanding of ASR model resilience. In this regard, we thoroughly study the vulnerability of practical black-box attacks in cutting-edge ASR models and propose to employ two advanced time-domain-based transferable attacks alongside our differentiable feature extractor. We also propose a speech-aware gradient optimization approach (SAGO) for ASR, which forces mistranscription with minimal impact on human imperceptibility through voice activity detection rule and a speech-aware gradient-oriented optimizer. Our comprehensive experimental results reveal performance enhancements compared to baseline approaches across five models on two databases.", "sections": [{"title": "I. INTRODUCTION", "content": "AUTOMATIC speech recognition (ASR) aims to recognize text from speech signals. ASR represents a dynamic and rapidly evolving research domain, striving to bridge the cognitive gap between human speech comprehension and computational interpretation. This progress is driven by a multitude of real-world applications such as virtual assistants (e.g., Siri and Amazon Alexa), captioning, subtitling, healthcare [1], and autonomous vehicle systems [2]. Consequently, attaining human-like recognition performance against minor input perturbations emerges as a critical consideration for real-time applications within the ASR domain [3]\u2013[6].\nRecognizing the significance of probing ASR system robustness, recent researches have delved into white-box scenarios to evaluate the accuracy robustness of ASR models via targeted and untargeted adversarial attacks [7]-[10]. In this paper, we focus on untargeted attacks. Untargeted attacks against white-box ASR models endeavor to create adversarial samples by introducing subtle perturbations to the ASR inputs, resulting in diminished recognition performance while maintaining imperceptibility to humans [8], [11]\u2013[16]. Although the mentioned endeavors in white-box ASR have made significant advancements, encompassing DNN [14], RNN [11], [12], [15], [16], Transformers [13], and the latest state-of-the-art (SOTA) Whisper ASR models [8], they are constrained by an impractical assumption that requires the adversary to have access to the ASR model's internal information. This limitation restricts their applicability in real-world scenarios [17], [18].\nAdversarial attacks pose a critical challenge in security-sensitive and safety-critical domains [19], [20]. Various adversarial attack approaches have been extensively studied in computer vision [21], [22] and natural language processing [23]\u2013[25]. With advancements in white-box ASR attacks, the potential and exploration of black-box ASR attacks, especially transferable attacks, have also been gaining attention. Notable exceptions in attacking Kaldi-based models [26] and RNN-based ASR models include universal adversarial perturbations [27] and study on factors affecting target transferability [28]. The above transferable attacks involve adversaries using adversarial examples crafted for one trained model to target other black-box ASR models. By eliminating the need to access the full black-box model architecture and weights, transfer attacks offer greater flexibility and possibilities for real-world applications. However, transfer attacks against advanced black-box ASR models (e.g., Whisper [29], Speech2text [30]) still remain unexplored. The accuracy robustness of black-box ASR still lags considerably behind human speech recognition performance, highlighting the need for a thorough exploration of recent models and advanced transferable attack methods.\nRecent advancements on attack transferability through innovative optimization [20], [31]\u2013[35] offer valuable insights for our work. To this regard, this paper aims to conduct a comprehensive exploration of accuracy robustness on cutting-edge ASR models, i.e., Whisper [8] and Transformer [30]. Inspired by [31], [32], we propose to employ momentum iterative fast gradient sign method (MI-FGSM) and variance tuning momentum iterative fast gradient sign method (VMI-FGSM) directly on time-domain audio signals through differentiable feature extraction design.\nOn the other hand, integrating VAD tasks into ASR has proven effective in enhancing recognition accuracy [29], [36]. VAD is designed to distinguish speech from non-speech segments in audio input [37]. Its close relationship with ASR lies in the fact that it identifies speech portions that are crucial for ASR [29], [36], [38]. Specifically, the correlation stems from the fact that the commencement of speech segments within an utterance usually corresponds with the detection of speech initiation by the VAD system, rendering non-speech segments identified by VAD potentially redundant for ASR [29], [36]. Furthermore, VAD and ASR often share optimization objectives to improve ASR system performance [29], [36], [38]. Motivated by this correlation, we propose a novel approach,"}, {"title": "II. TRANSFERABLE ADVERSARIAL ATTACKS", "content": "In this section, we initiate by defining the research problem and then proceed to outline the proposed approaches.\nA. Problem Formulation\nOur goal is to discover the optimal perturbation \u0394 that not only degrades the recognition performance of a target ASR model but also remains indistinguishable from human listeners by adhering that the perturbation must be smaller than the allowed constrain. Therefore, our objective can be formulated as a constrained optimization problem:\n$\\Delta = \\underset{\\epsilon}{\\arg \\min} L_{adv} (h(x + \\epsilon), y) \\text{ s.t. } ||\\epsilon||_p \\leq \\xi,$\nwhere x is the original audio input to the ASR system h(.) and y is the ground-truth text label. The perturbation \u03f5 is constrained by lp-norm to ensure audio imperceptibility and \u03be is the maximum adversarial perturbation allowed. $L_{adv}$ represents the negative value of the cross entropy loss between y and predicted tokens from ASR.\nB. Transferable Attack via Gradient Optimization\nInstead of relying on obtaining full access to the details of the target ASR model, we propose to conduct transferable adversarial attacks which only requires the output information of black-box target model. This involves attacking source ASR model $h_s$ and generating adversarial samples $x_{adv}$ to serve as input for the target black-box model $h_t$, using three novel gradient optimization approaches, as depicted in Fig 1. We aim to create transferable attacks that serve as crucial surrogates for accessing the robustness of target ASR models (i.e., Whisper [8] and Speech2text [30]) before they are deployed. Each approach is elaborated below.\n1) Speech-Aware Gradient Optimization: We propose Speech-Aware Gradient Optimization (SAGO) as a transferable attack approach for ASR systems, incorporating iterative speech-aware gradient optimization tailored to the objectives of attacking ASR models effectively. Specifically, SAGO is defined by iterative updating perturbations through Eq. 2 using speech-aware gradients in relation to the loss function $L_{adv}$. For each iteration, the updated perturbation \u03f5 is computed as:\n$\\epsilon \\leftarrow I \\{\\epsilon + \\alpha \\cdot \\text{sign}(M\\odot \\nabla_{\\epsilon} L_{adv} (f(x + \\epsilon),y))\\},$\nwhere \u03b1 is the learning rate, $L_{adv}$ is the gradient of the adversarial loss with respect to the perturbation \u03f5. During initialization of the attack, we compute the VAD mask M and use it to filter out the gradient associated with non-speech components based on such mask, as shown in Fig 1. \u2299 implies element-wise product. $I\\{\\cdot\\}$ is the projection function that enforces the $l_p$ constraint on the perturbation:\n$\\Pi_{\\epsilon, S}(x_{adv}) = \\underset{z \\in S}{\\arg \\min} ||x_{adv} - z||_p,$\nwhere z is an element within the feasible set S, and $||\\cdot||_p$ denotes the lp norm. $I\\{\\cdot\\}$ maps $x_{adv}$ to the closest point z in S under the lp norm. SAGO integrates speech-aware knowledge to general projected gradient descent (PGD) [8], [19] attack for gradient optimization, thereby emphasizing the significance of attacking speech segments instead of non-speech parts within an utterance for the purpose of recognition. SAGO leverages the difference in how the human brain and ASR systems process speech by discarding non-speech components in an audio sample, which humans are primed to ignore.\nIncorporating the VAD mask not only makes our attack more targeted but also potentially increases its transferability. By focusing on the speech parts crucial to ASR, the generated adversarial examples are more likely to be effective across different ASR models, which may have varying sensitivities to speech and non-speech components.\n2) Momentum Based Gradient Optimization: Inspired by the effective transferable adversarial attack approaches on image classification with gradient optimization [31], [32], we propose to employ the following momentum-based transferable adversarial attack methods, momentum iterative fast gradient sign method (MI-FGSM) and variance tuning momentum iterative fast gradient sign method (VMI-FGSM), for attacking black-box ASR models.\nMI-FGSM: By incorporating a momentum term into ASR attacks, our MI-FGSM aims to stabilize update directions, aiding in escaping from local minima during iterations, and yielding more transferable adversarial examples. For each iteration, the updated perturbation \u03f5 can be computed by:\n$\\epsilon \\leftarrow II \\{\\epsilon + \\alpha \\text{ sign}(\\nabla_{\\epsilon} L_{adv} (f(x + \\epsilon), y) + G)\\},$\nwhere G represents the momentum term, serving as a velocity vector in the gradient direction of $L_{adv}$ across iterations. Momentum is expected to help remember previous gradients and navigate through narrow valleys, small humps, and unfavorable local minima [31], [40]."}, {"title": "VMI-FGSM:", "content": "In addition to MI-FGSM, our VIM-FGSM additionally tunes the current gradient with the gradient variance in the neighborhood of the previous data point to stabilize the update direction and escape from poor local optima. For each iteration, the updated perturbation \u03f5 can be computed by:\n$\\epsilon \\leftarrow I \\{\\epsilon + \\alpha \\cdot \\text{sign}(V(\\nabla_{\\epsilon} L_{adv}(f(x + \\epsilon), y)) + G)\\}$\nwhere V(.) denotes a function used for additional variance tuning to reduce the variance of the gradient at each iteration, thereby stabilizing the gradient update direction. Our SAGO, MI-FGSM and VMI-FGSM apply fast gradient for N iterations.\nC. Time-Domain Attack\nTo craft adversarial audios flexibly and get rid of the traditional non-derivative feature processing [28], [41]\u2013[43], we propose to use a differentiable feature extractor to obtain 80-dimension log filterback features for three proposed attacks as ASR model input features. This enables gradients to back-propagate all the way to the input audio through the feature extractor, facilitating a time-domain attack. By employing these time-domain attack methods, we aim to enhance the transferability of adversarial attacks across different ASR models."}, {"title": "III. EXPERIMENTS", "content": "A. Datasets\nWe employ two widely recognized English datasets from huggingface: the LibriSpeech dataset [44] and the LJ-Speech dataset [45]. We assess the first 500 utterances from the LJ-Speech and the first 500 utterances of the clean test subset from the LibriSpeech for evaluation. All audios are re-sampled to 16,000 Hz.\nB. Experimental Setups\nModels: we utilize SOTA ASR frameworks: Whisper [29] and Speech2text (S2T) [30] as both source and target ASR models. Whisper employs weakly supervised speech recognition using transformer with 680,000 hours of data [29], while S2T is a transformer-based seq2seq (encoder-decoder) model designed for E2E ASR [30]. Our experimentation involves five ASR models: Whisper-tiny, Whisper-base, S2T-small, S2T-medium, and S2T-large, for transfer attacks between them.\nBaselines: we employ PGD [8] as a strong baseline (re-implement [8] to allow more ASR models). White noise is considered as a baseline [8] for comparison. We also propose to investigate three more widely used noise categories as in [39] that include babble, music and natural noises from MUSAN dataset [46]. All noises are augmented to the input clean audios for ASR model decoding.\nAttack Setups: for all attacks, p is set to \u221e, and attack iteration steps N is set to 50. Perturbation size \u03be is set to 0.002 and 0.0035, representing average signal-to-noise ratios (SNRs) of 35dB and 30dB, respectively, indicative of exceptionally low levels of noise. VAD is implemented by using a cepstral power measurement to detect speech and trim non-speech parts from the front and end of the audio.\nEvaluation Method: to assess the degradation of ASR performance, we measure it in terms of Word Error Rate (WER), which calculates the ratio of total insertions, substitutions, and deletions to the total number of words. We set a maximum WER value of 100.00 % for values exceeding 100.00 %. A higher WER signifies poorer ASR performance and a more successful attack."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "We study the performances of white-box and transferable attacks, the effect of noise sensitivity and human imperceptibility. Table I and II summarize the recognition performance of both baseline and our proposed methods at differnt SNR values for the LJ-speech and Librispeech datasets, respectively.\nA. White-box Attacks\nWe build white-box ASR attackers to serve as the source models for conducting black-box attacks. We initially observe from Table I and II (*) that our proposed attacks surpass noise baselines and attain good performance (WER>50 %) together with strong baseline PGD. Note that PGD is a strong method for white-box attacks for general scenarios as indicated in [19].\nB. Noise Sensitivity\nWe observe that the inclusion of noises like babble, music and natural sounds and white noises does not degrade ASR model performances and perform worse than our proposed attacks. This underscores the varying vulnerability of ASR models' to additive noises and transferable attacks, emphasizing the superiority of adversarial perturbations over added noises. These findings emphasize the importance of assessing ASR model robustness against transferable attacks.\nC. Transferable Attacks\nOur proposed attacks generally induce significant performance degradation, yielding higher WER compared to baselines across five models and two datasets. This highlights the effectiveness of our approaches' transferability, potentially rendering ASR models unsuitable for various practical applications. For instance, a 65% WER performance can severely impact the quality of video captioning and car navigation systems when a source S2T-medium model attacks a target S2T-small model. For Whisper-based models, FGSM approaches tend to be more effective, indicating their robustness in scenarios with complex models that may cause gradient-based methods to struggle. For S2T models, SAGO shows better performance, likely due to its ability to target speech-specific characteristics through VAD masking. This suggests that the choice of method should be context-dependent.\nD. Human Imperceptibility\nAdversarial samples and codes can be accessed through the link 1. The adversarial samples from our proposed attacks sounds similar to the original clean audio, achieving higher WER (better transferable attack performance) compared with baselines. This indicates that the adversarial samples are undetectable to human and the proposed attacks captures aspects of speech that are generally not perceived by human subjects, but that are crucial for ASR accuracy."}, {"title": "V. CONCLUSION", "content": "The proposed SAGO successfully empowers attackers to harness speech-aware insights from VAD for transferable ASR attacks. Our proposed MI-FGSM and VMI-FGSM also serves as an important step into the comprehensive exploration of transferable attack against ASR, where we propose to leverage momentum-oriented gradient optimization and demonstrate their effectiveness as time-domain attacks with a differentiable feature extractor design. The proposed attacks not only demonstrate their cross-model transferable effectiveness but also affirm that the generated audio does not impact human understandability. The combination of the proposed methods will be studied further in future work."}]}