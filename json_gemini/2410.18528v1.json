{"title": "PRACT: Optimizing Principled Reasoning and Acting of LLM Agent", "authors": ["Zhiwei Liu", "Weiran Yao", "Jianguo Zhang", "Rithesh Murthy", "Liangwei Yang", "Zuxin Liu", "Tian Lan", "Ming Zhu", "Juntao Tan", "Shirley Kokane", "Thai Hoang", "Juan Carlos Niebles", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong"], "abstract": "We introduce the Principled Reasoning and Acting (PRAct) framework, a novel method for learning and enforcing action principles from trajectory data. Central to our approach is the use of text gradients from a reflection and optimization engine to derive these action principles. To adapt action principles to specific task requirements, we propose a new optimization framework, Reflective Principle Optimization (RPO). After execution, RPO employs a reflector to critique current action principles and an optimizer to update them accordingly. We develop the RPO framework under two scenarios: Reward-RPO, which uses environmental rewards for reflection, and Self-RPO, which conducts self-reflection without external rewards. Additionally, two RPO methods, RPO-Traj and RPO-Batch, is introduced to adapt to different settings. Experimental results across four environments demonstrate that the PRAct agent, leveraging the RPO framework, effectively learns and applies action principles to enhance performance.", "sections": [{"title": "1 Introduction", "content": "Large language model (LLM) agents enable the action execution (Gravitas, 2023; Goodman, 2023; Yao et al., 2023a; Wang et al., 2023a) and consecutive reasoning ability (Nakajima, 2023; Shinn et al., 2023; Yao et al., 2023b) of LLM. Specifically, an LLM agent has both memory (Shinn et al., 2023; Li et al., 2023; Liu et al., 2024) and action space (Chase, 2023; Wu et al., 2023; Liu et al., 2023). Adding those information into prompt extends the inference of LLM to be multi-turn action execution. Therefore, an LLM agent is able to decide next actions based on its previous execution observations (Wang et al., 2023b; Xu et al., 2023; Goodman, 2023; Song et al., 2023).\nOptimizing the reasoning framework (Yao et al., 2023a; Liu et al., 2023; Wang et al., 2023b) of agent is crucial in generating correct action execution. As of now, customizing an LLM agent with existing open-source packages (Liu et al., 2024; Wu et al., 2023; Chase, 2023; Liu, 2022) requires the designing of action spaces, such as function calls (Patil et al., 2023) and code execution (Wu et al., 2023, 2024). Along with a well-designed agent reasoning framework, i.e. the prompts of agent, an LLM is able to consecutively generate correct actions. ReAct (Yao et al., 2023a) framework achieves wide successes via adding one-step think actions to enhance the reasoning ability of an agent. Additionally, Reflection (Shinn et al., 2023; Yao et al., 2023b; Paul et al., 2023) mechanism is proposed to improve the agent self-correction capability. Plan (Xu et al., 2023; Liu et al., 2023) before execution is also verified to be beneficial.\nDespite many successes, agent execution can fail to make decisions when faced with contradictory observations, particularly during the execution of long-step tasks. To address it, we propose a new type of reasoning strategy, PRAct, for the LLM agent. Intuitively, we associate each action with principles that describe the conditions for using that action. During execution, an agent can check these principles before generating the next action. Compared to simple action descriptions, principles provide more detailed conditions on when to use the action and offer specific instructions on how to generate the parameters for an action. We demonstrate the benefits of PRAct in Figure 1 via comparing with ReAct agent in WebShop (Yao et al., 2022) where an agent uses search and click actions to interact with a shopping website. The ReAct agent searches a query and, despite item 2 not having the available color, still clicks it as it appears most relevant. In contrast, the PRAct agent refines the search based on both search and click principles. Consequently, the PRAct agent decides to search with an improved query, enhancing its decision-making process.\nTo reduce the labor involved in prompt design and to cover more complex scenarios, we propose a new principle optimization framework, Reflective Principle Optimization (RPO). RPO operates in three stages: execution, reflection, and optimization. During the execution stage, an agent performs tasks using predefined or null principles and memorizes the task trajectories. In the reflection stage, the agent reviews its task executions, evaluating how actions were selected and whether they met the task requirements. Finally, in the optimization stage, an optimizer refines principles to enhance agent performance. We investigate two optimization methods: RPO-Traj, which individually optimizes principles for each trajectory, and RPO-Batch, which concatenates all reflections in a batch for optimization.\nWe summarize our contributions as follows: 1) PRAct is the first work that considers the action principles for LLM agent; 2) we propose two optimization methods to adapt the principles to tasks."}, {"title": "2 PRACT: Optimizing Principled Reasoning and Acting", "content": null}, {"title": "2.1 Formulation", "content": "Given a task query, an agent is able to consecutively execute actions $[a_1, a_2, ..., a_n]$ and collects observations $[o_1, o_2, ..., o_n]$ from environments, where $o_i$ is the execution results of $a_j$. A policy function $\\pi(a_t|c_t)$ predicts the next action $a_t$ given the execution trajectory context $C_t = [(a_1, o_1), (a_2, o_2), ..., (a_{t-1}, o_{t-1})]$. An Executor agent utilizes a language model to determine the policy function. It requires textual trajectory information for the prompt Intrinsically, those context information are text-based, including action names, action parameters and observations.\nPRAct constraints the reasoning of LLM to follow a set of principles $P$ as follows:\n$\\pi(a_t|C_t) = Executor(a_t|T(c_t); P),$\nwhere $T$ is the prompt template to organize context information and the principles $P$ are guidelines that help shape the decision-making process of an LLM agent. Principles provide instructions on the usage of the action such as how to generate parameters for the action. Additionally, principles reduce the set of potential actions by eliminating those that do not conform to the defined guidelines, thereby narrowing the search within the action space. In this paper, we simplify the principles space to be the same as actions space, i.e. each $a_i \\in A$ associated with a $p_i \\in P$."}, {"title": "2.2 Reflective Principle Optimization (RPO)", "content": "Although the principles could be predetermined, as in the action descriptions, it is challenging to comprehensively cover all possible conditions without an automatic optimization paradigm. Therefore, we propose a new algorithm, Reflective Principle Optimization (RPO), to adapt principles for complex scenarios. RPO operates in three stages: 1) Execution, 2) Reflection, and 3) Optimization."}, {"title": "2.2.1 Execution", "content": "Given a set of tasks, the executor agent performs actions based on the current set of principles, collecting observations from the environment. This stage involves prompting the LLM agent to generate actions, which regressively calls Eq. (2) until reaching the final actions or maximum steps. Given a task query q, we denote the trajectory as $C_q = [(a_1, o_1), (a_2, o_2), ..., (a_n, o_n)]$. Note that those actions may be some inner actions, such as think or plan (Yao et al., 2023a; Liu et al., 2024) which do not forward to the environment and are associated with a default or null observation. Executor collects a set of trajectory context sequences C for those queries Q during execution stage."}, {"title": "2.2.2 Self-Reflection", "content": "After executing the actions, a reflector agent reflects on trajectories C by analyzing the collected observations. This stage involves evaluating the effectiveness of the actions in each trajectory and the adherence to the principles as follows:\n$r_q = REFLECTOR(c_q, P),$\nfor all $c_q \\in C$. The reflection process identifies conditions or guidelines where the principles need adjustment to better handle the observed tasks. If an environment provides rewards toward the execution, it is a reward-based reflector aligning the executions with reward feedback. Instead, if no rewards present for execution, it is a self-reflector."}, {"title": "2.2.3 Optimization", "content": "Based on the reflection results, we leverage the generation ability of LLM to refine the principles for improving the performance of agent in similar future scenarios. This stage involves refining the principles to better align with the observed conditions and enhance decision-making. We investigate two types of optimization methods.\nRPO-Traj. This approach individually considers each trajectory and its reflection to optimize principles. Then a batch of principles are summarized as a new set of tailored principles $P^*$. We formulate RPO-Traj as follows:\n$P^* = \\sum_Q OPT(r_q, P),$\nwhere $\\sum$ denotes a summarizor of all principles generated from optimizer OPT for all queries Q.\nRPO-Batch. We use a prompt template to concatenate all the reflections in a batch. Then the optimizer directly generates new principles via considering all those reflections, which is formulated as follows:\n$P^* = OPT(CONCAT\\{r_q|q \\in Q\\}, P),$\nwhere CONCAT denotes using a prompt template to concat those reflections. In comparison, RPO-Traj requires generating principles for |Q|+1 times, while RPO-Batch only needs one time principles generation but with |Q| times longer context length. Hence, long context reasoning ability is necessary for an optimizer in RPO-Batch method."}, {"title": "3 Experiment", "content": null}, {"title": "3.1 Experiment Setup", "content": "Baselines. We compare our PRAct agent with existing Act, ReAct (Yao et al., 2023a), Reflexion (Shinn et al., 2023) agent reasoning methods"}, {"title": "3.2 Optimization setup", "content": "For optimizing the WebShop agent with a Reward-based reflector, we randomly split the query tasks into training, validation, and test tasks with a ratio of 3:1:1. During each training step, we sample a batch of training tasks to execute and use RPO to optimize the principles. Performance on validation tasks is used for early stopping, and results are reported on test tasks. For tool agents, we use a self-reflector without rewards, making reflection tasks the same as test tasks. Since there is no ground truth, no data leakage problem exists. We tune the training batch size in [10,20,40] for WebShop and [2,4,6] for tool environments."}, {"title": "3.3 Experiment Results", "content": "Overall Performance. We present comprehensive comparisons of our methods against the agent baselines in Table 1. PRAct-T and PRAct-B are our methods with RPO-Traj and RPO-Batch optimization methods, respectively. We observe consistently better performance of PRAct agent, which demonstrates the effectiveness of principles in improving agent performance. Between the two optimization methods, i.e. PRAct-T and PRAct-B, PRAct-B generally performs better than PRAct-T. The reason is that summarizing principles from a batch of reflections enables potential reasoning across trajectories. However, PRAct-T outperforms PRAct-B due to the potential weaker long context understanding ability of GPT-3.5-Turbo, which indicates batch-wise optimization is more suitable for larger models."}, {"title": "4 Conclusion", "content": "We propose a novel agent reasoning framework, PRAct, which provides principles of actions and thus benefits the action understanding of agent. Besides, we introduce two optimization algorithm, RPO-Traj and RPO-Batch for adapting the action principles with task executions. Experimental results on four environments demonstrates the effectiveness of PRAct framework. And the training curve illustrates the learning efficacy of RPO. In conclusion, PRAct opens a new discussion on how to regularize the agent actions while RPO shades the light on how to optimize the agent prompts."}]}