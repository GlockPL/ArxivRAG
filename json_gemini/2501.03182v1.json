{"title": "Boosting Explainability through Selective Rationalization in\nPre-trained Language Models", "authors": ["Libing Yuan", "Shuaibo Hu", "Kui Yu", "Le Wu"], "abstract": "The widespread application of pre-trained language models (PLMs)\nin natural language processing (NLP) has led to increasing con-\ncerns about their explainability. Selective rationalization is a self-\nexplanatory framework that selects human-intelligible input sub-\nsets as rationales for predictions. Recent studies have shown that\napplying existing rationalization frameworks to PLMs will result in\nsevere degeneration and failure problems, producing sub-optimal or\nmeaningless rationales. Such failures severely damage trust in ratio-\nnalization methods and constrain the application of rationalization\ntechniques on PLMs. In this paper, we find that the homogeneity of\ntokens in the sentences produced by PLMs is the primary contrib-\nutor to these problems. To address these challenges, we propose\na method named Pre-trained Language Model's Rationalization\n(PLMR), which splits PLMs into a generator and a predictor to deal\nwith NLP tasks while providing interpretable rationales. The gen-\nerator in PLMR also alleviates homogeneity by pruning irrelevant\ntokens, while the predictor uses full-text information to standardize\npredictions. Experiments conducted on two widely used datasets\nacross multiple PLMs demonstrate the effectiveness of the proposed\nmethod PLMR in addressing the challenge of applying selective\nrationalization to PLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "The widespread application of deep learning models, particularly\npre-trained language models (PLMs), across critical fields in natural\nlanguage processing (NLP) has led to increasing concerns about\ntheir explainability [8, 10]. Selective rationalization, as an inter-\npretable method, has received continuous research in the field in\nrecent years [14, 26]. Lei et al. [17] were the first to propose this\nframework for rationalizing neural predictions (RNP), which con-\nsists of a generator and a predictor. The generator selects a human-\nintelligible subset of the entire input sentence as a rationale, while\nthe predictor then makes a judgment only based on this rationale,\nensuring that the explanation is faithful, as shown in Figure 1.\nThe vanilla RNP framework in Figure 1 suffers from rationaliza-\ntion degeneration and failure problems [22, 32, 37]. The question\nwith rationalization degeneration lies in its capacity to yield lower\nprediction loss even with a poor quality of rationale. For example,\nthe final prediction is correct even if the rationale, as shown in Fig-\nure 2, is not as relevant to the gold annotations. The rationalization\nfailure [12, 15] goes further, as the chosen rationale was completely\nabsurd, but the label can still be accurately predicted. Many meth-\nods have been proposed to alleviate these issues [12, 32, 33]. In\nthese studies, researchers focused on enhancing the rationalization\nframework, so their experiments utilized relatively simple networks\nas encoders, such as GRU [6] and LSTM [11]. This assumes that the\nframework remains valid when the encoder is altered.\nAlthough these methods have achieved improvements, recent\nstudies [5, 23] have indicated that the previous framework for se-\nlective rationalization will not work if we replace the encoder with\nthe PLM. For example, as shown in Table 1, using a three-layer GRU\ncan provide better token representations compared to a single-layer\nGRU, allowing for the selection of more accurate rationales. Con-\ntrarily, PLMs with more extensive parameters, which were expected\nto learn better representations [18, 28], actually yield poor-quality\nrationales. When using the same selective rationalization frame-\nwork in Table 1, we can observe that employing BERT [9] does not\noffer better rationales compared to using GRU; instead, the quality\nof rationale becomes worse.\nThus, the questions naturally arise: what factors lead to the\npoor performance of the selective rationalization framework within\nPLMs, and what strategies can improve it? Motivated by these two\ncore concerns, we conducted a series of analyses and proposed\na method called Pre-trained Language Model's Rationalization\n(PLMR) that can work on PLMs. Our contributions are as follows:\nFirst, we conduct experiments to validate the severe rationaliza-\ntion degeneration and failure problems that occur when we apply\nthe BERT model (a commonly used PLM) to the current rationaliza-\ntion frameworks. We then deeply analyze why these two problems\noccur when using the BERT model. We find that the over-learning\nof contextual information of PLMs causes the input tokens to have\nhomogeneous representations, leading to severe rationalization\ndegeneration and failure problems.\nSecond, based on the analysis above, we propose a novel ap-\nproach named PLMR. PLMR Truncates the PLM into two indepen-\ndent parts: the rationale generation part and the prediction part.\nBut from a perspective outside of PLM, the two parts still exist as\na whole. In order to further reduce the learning of context-free\ninformation, PLMR cut out irrelevant context before rationale gen-\neration. In the prediction part, PLMR utilizes full-text information\nto regularize the predictions of the predictor, further improving the\nperformance of the model.\nThirdly, the F1 score for rationale selection using PLMR is up\nto 9% higher than methods using GRU and up to 17% higher than\nprevious methods also using PLMs. These results indicate that our\nPLMR can use PLM in rationalization and effectively address issues\nof rationalization degeneration and failure, making an essential step\ntowards rationalizing the prediction of PLMs for their explainability."}, {"title": "2 RELATED WORK", "content": "The base selective rationalization framework named RNP [17] uses\nonly the rationale extracted by the generator as justifications to\nmake predictions, thereby ensuring the interpretation of the cooper-\native model is faithful [26]. Such cooperative frameworks between\ngenerator and predictor are hard to optimize as well as train. To\naddress this problem, Bao et al. [1] used Gumbel-softmax to re-\nparameterize gradient estimates. Bastings et al. [2] employed a\nrectified Kumaraswamy distribution to replace Bernoulli sampling.\nThen it was found that rationalization degeneration and failure\nproblems occur due to the predictor using only the rationale for\nprediction. To address the degeneration problem, a series of studies\nused additional information to regularize the predictor. 3PLAYER\n[31] adds a complementary predictor that uses text not selected as\nthe rationale. A2R [32] uses soft attention from the generator to\ninput full-text information into the predictor. DMR [13] aligns the\nfeature and output distributions of the rationales with the full-text\ninput. DARE [33] improves rationale representations by reducing\nthe mutual information between rationale and non-rationale parts\nof the input. FR [22] uses the same encoder between the genera-\ntor and the predictor to convey information. DAR [20] utilizes an\nauxiliary module pretrained on the full input to align the selected\nrationale and the original input discriminatively. Meanwhile, some\nwork addresses the degeneration problem from a causal perspective.\nINVRAT [4] uses a game-theoretic approach to constrain the output\nof rationale across multiple environments. Inter-RAT [35] uses a\nbackdoor adjustment method theory to remove spurious correla-\ntions. MCD [23] proposes the Minimum Conditional Dependence\n(MCD) criterion to uncover causal rationales. MRD Rationaliza-\ntion failure undermines the user's trust more severely by selecting\nmeaningless rationale. G-RAT [12] addresses rationalization failure\nproblems by using a guidance module to regularize the selection of\nthe generator.\nThe above approach typically uses simple recurrent models such\nas GRU [12, 19, 22, 32, 35] and LSTM [31]. Recent experiments\n[5, 22] have shown that these rationalization frameworks with\nPLMs lead to the poor quality of rationales. Although some studies\nhave been conducted using PLMs [34, 36], the quality of rationales\nremains inferior compared to methods utilizing GRU. Our work\nanalyzes the reasons for the problems of degeneration and failure"}, {"title": "3 PRELIMINARIES", "content": "Selective Rationalization. We consider a text classification task,\nwhere the input text is $X = [x_1, x_2,..., x_n]$, with $x_i$ representing\nthe i-th token and n representing the number of tokens in the text.\nY is the label corresponding to X. The selective rationalization\nframework consists of a generator $g(\u00b7)$ and a predictor $p(\u00b7)$, with\n$\\theta_g$ and $\\theta_p$ representing the parameters of the generator and the\npredictor, respectively. In the training set $(X, Y) \\in D$, the rationale\nis unknown. The goal of the generator is to learn a sequence of\nbinary mask $M = [m_1,......, m_n] \\in \\{0,1\\}^n$ from the input X, and\nthen use M to generate a subset of the input text as the rationale R:\n$R = M \\otimes X = [m_1x_1,..., m_nx_n].$ (1)\nSubsequently, the predictor uses R to perform the text classification\ntask while computing the task loss $L_{task}$ for the entire select-then-\npredict model. Finally, the collaborative optimization process of the\ngenerator and predictor is as follows:\n$\\min_{\\theta_g,\\theta_p} E_{X,Y~D} [L_{task} (p (M \\otimes X), Y)]$ .\n$M~g(X)$ (2)\nTo ensure that the rationale selected by the generator is understand-\nable to humans, we aim to choose short and coherent subsets as the\nrationale. To achieve this goal, we adopt the constraint methods\nused in most previous research:\n$L_s = \\lambda_1 \\frac{1}{n} \\sum_{i=1}^n m_i + \\lambda_2 \\sum_{i=1}^n |m_i - m_{i-1}|.$ (3)\nThe first term uses a predefined sparsity $a \\in [0, 1]$ to control the\nproportion of the rationale selected, while the second term ensures\nthat the rationale is as coherent as possible. Therefore, Equation 2\ncan be rewritten as:\n$\\min_{\\theta_g,\\theta_p} E_{X,Y~D} [L_{task} (p (M \\otimes X), Y) + L_s].$\n$M~g(X)$ (4)"}, {"title": "4 ANALYZING DEGENERATION AND FAILURE\nIN RATIONALIZATION WITH PLMS", "content": "In Section 1, both previous studies and our experiments indicate\nthat using PLMs in a rationalization framework can result in severe\nrationalization degeneration and failure problems, providing unsat-\nisfactory explanations. This section will first validate the presence\nof more severe rationalization degeneration and failure phenomena.\nSubsequently, we will identify the root causes of these issues by\nanalyzing the operational mechanism of PLMs. Our primary ex-\nperiments are performed using BERT-base-uncased, which has 12\nlayers (transformer [29] blocks), 12 attention heads, and 110 million\nparameters."}, {"title": "4.1 Rationalization Degeneration and Failure", "content": "4.1.1 Rationalization Degeneration. For two subsets, $X_1$ and $X_2$,\nwithin the input text X, where $X_1$ is the golden rationale corre-\nsponding to label Y, the high correlation between $X_1$ and $X_2$ results\nin a spurious correlation between $X_2$ and Y. This spurious corre-\nlation is the possible cause of degeneration. The strength of the\ncorrelation between $X_1$ and $X_2$ reflects the extent of the spuri-\nous correlation. Therefore, we demonstrate through the following\nexperiments that using pre-trained language models (PLMs) in a\nrationalization framework leads to more severe degeneration.\n4.1.2 Rationalization Failure. When the generator selects mean-\ningless tokens as its rationale, the predictor should theoretically\nfail to make accurate predictions. However, there are occasions\nwhen the predictor still succeeds in making correct predictions.\nThis occurrence causes the generator to mistakenly believe these\ntokens support its predictions, leading to what is known as rational-\nization failure. Based on the reasons for the rationalization failure\nmentioned above, we make the assumption to illustrate whether\nrationale failure occurs:\nAssumption 1. Given a rationalization framework, which consists\nof generator $g (\u00b7)$ and predictor $p (\u00b7)$. For $(X, R, Y) \\in D_{test}, g(X) = \\hat{R}$,\n$p(\\hat{R}) = \\hat{Y}$. Rationalization failure occurs when the learned rationale\n$\\hat{R}$ meets the following conditions:\n(1)$p(\\hat{R}) = \\hat{Y} = Y$; The predictor p (\u00b7) predicts correctly using $\\hat{R}$.\n(2) $P = |\\hat{R} \\cap R|/|\\hat{R}| \\leq \\theta_1$; The proportion P of correct tokens in the\n$\\hat{R}$ is less than $\\theta_1$.\n(3) $|\\hat{R}_{punct, prep, pron, art, conj} / |\\hat{R}| \\geq \\theta_2$; The proportion of mean-\ningless tokens such as punctuation, prepositions, pronouns, articles,\nand conjunctions in $\\hat{R}$ is greater than $\\theta_2$."}, {"title": "4.2 Homogeneity Among Tokens and Clauses", "content": "Unlike recurrent neural networks, the self-attention mechanism of\nthe Transformer model allows each token to establish direct connec-\ntions with other tokens in the sentence. This mechanism enables\neach token to attend to any position in the sentence, thereby cap-\nturing long-distance dependencies. However, in PLMs with multi-\nlayered Transformer structures, each token learns information from\nother tokens at every layer of the Transformer. This over-learning\nof contextual information leads to highly similar final token rep-\nresentations, resulting in a lack of heterogeneity. We refer to the\nphenomenon where different tokens within a sentence exhibit sim-\nilar semantic information as token homogeneity. In this section, we\nempirically demonstrate the occurrence of homogeneity in PLMs\nand explain how it leads to more severe rationalization degeneration\nand rationalization failure problems.\nTo illustrate the homogeneity of token representations gener-\nated by BERT, we utilize the traditional likelihood-based variance-\ncovariance matrix homogeneity test method [3] to observe the\ndegree of discrepancy between token representations.\nGiven the hidden states generated by BERT, denoted as H, where\nH is an n x p matrix, n is the number of tokens in the sentence, and p\nis the dimensionality of the token representations. The calculation\nof the variance-covariance matrix is as follows:\n$H_{ij} = H_{ij} - \\frac{1}{n} \\sum_{k=1}^n H_{kj}$ (5)\n$\\Sigma = \\frac{1}{n-1} (H')^T H'$, (6)\nwhere H' is the centralized data matrix, $\\Sigma$ in Eq.(6) is the variance-\ncovariance matrix. $\\Sigma_{ii}$ represents the variance of the i-th dimen-\nsion. Therefore, the trace of the variance-covariance matrix can\nintuitively reflect the degree of discrepancy among different token\nrepresentations. So, a smaller trace value indicates more severe\nhomogeneity.\n$\\operatorname{tr}(\\Sigma) = \\sum_{i=1}^P \\Sigma_{ii}.$ (7)\nWe conduct experiments on three types of PLMs, each including\nboth base and large versions. Figure 4 shows the trace tr($\\Sigma$) of\nhidden states across different transformer layers in PLMs. The\nresults show that the trace tr($\\Sigma$) of the final output hidden states of\nthe PLMs are significantly smaller than those of other layers. This\nindicates severe homogeneity of token representations in sentences\ngenerated by PLMs. This homogeneity implies that each token has\na similar attention-weight vector in the attention heads, which\nmeans all tokens have similar attention dependencies. Appendix\nA.1 shows the distribution of attention weight vectors at different\nlayers in Bert to support the above experiments.\nWithin the framework of rationale selection, the mask selection\nm(x) needs to select an interpretable subset of tokens as rationales\nfrom the different tokens in a sentence. Similar to clustering tasks,\nwe need to select the golden rationale within a sentence. How-\never, the homogeneity of tokens makes this process exceptionally\nchallenging. On the one hand, this will lead to further correlation\nbetween different subclauses of the input text, making it easier\nfor the generator to select sub-optimal subsets. On the other hand,\ntoken homogeneity means that the heterogeneity between different\ntokens is reduced, and it is challenging to distinguish rationales\nfrom other meaningless tokens, causing the generator to select\nmeaningless tokens in the early stages of training easily. When\nboth situations occur, powerful predictors will overfit the wrong\nresults and make accurate predictions, leading to more severe ra-\ntionalization degeneration and rationalization failure problems.\nTherefore, we understand that the more severe rationalization\ndegeneration and failure occur due to the generator and the pre-\ndictor issues. The key reason lies in the homogeneity of tokens\nproduced by the generator. Another reason is that the predictor\nmakes correct predictions based on incorrect rationales."}, {"title": "5 METHODOLOGY", "content": "Based on the above analysis, we address these issues from the\nperspectives of both the generator and the predictor and propose\nthe method PLMR. In this section, we first describe the overall\narchitecture of PLMR, which consists of the rationale selection and\nprediction modules (Section 5.1). Then we present the details of\nthese two modules (Section 5.2 and Section 5.3)."}, {"title": "5.1 Overall Architecture", "content": "The overall architecture of the proposed method PLMR is illus-\ntrated in Figure 5. We divide PLMs into earlier layers of PLMs as\nthe generator and the later layers of PLMs as the predictor. The\ngenerator is comprised of transformer layers and dimension re-\nduction layers (Dim-Reduction layers). The detailed design of the\nDim-Reduction layers is shown in Figure 6.\nAs shown in Figure 5, the generator in PLMR first inputs the\ntext X into a multi-layer transformer to learn the hidden states H.\nThen the hidden states H are passed through the Dim-Reduction\nlayers to generate the rationale mask M, from which we obtain the\nrationale $R = X \\otimes M$. The predictor then uses only the rationale\nR for task prediction to ensure the explanation is faithful in the\ninference phase."}, {"title": "5.2 Rationale Selection Module", "content": "In this section, we explain the idea of the generator and describe\nits rationale selection process. Traditional rationalization methods\nfail on PLMs because of the homogeneity among tokens. Therefore,\nwe address this issue with the following two methods.\n5.2.1 Selecting for Token Heterogeneity. In contrast to homogene-\nity, heterogeneity is crucial. The generator's task is to select the\ngolden rationale that best supports the prediction from the entire\ninput text. The homogeneity of tokens within the text makes it\nchallenging for the rationale mask selector to identify the correct\ntokens as the rationale accurately. Figure 4 shows that only the\nfinal representations of PLMs approach homogeneity, while the\ntoken representations generated by the intermediate transformer\nlayers maintain better heterogeneity.\nTherefore, the earlier layers of PLMs are used as the generator\ng(\u00b7) to select rationales, which ensures that there is good hetero-\ngeneity between token representations when the mask selector\nm(.) determines whether tokens are rationales. The mask M of the\nrationale R can be calculated by the following equation:\n$M = g(X) = MLP (Transformer_{0-l}(X)),$ (8)\nwhere 0 - l indicates that the generator uses the first l layers of\ntransformers in the PLMs. The token representations generated by\nthe earlier layers in Figure 4 exhibit good heterogeneity, so how\nmany layers should we choose as generators? (1) The generator\nrequires sufficient transformer layers to learn better representations,\nenabling it to extract the correct rationale from the sentence. (2) The\npredictor needs sufficient transformer layers to use the rationale for\nthe prediction task. Poor performance in either aspect will lead to a\ndecline in overall performance. In order to balance the performance\nof the generator and the predictor, the number of layers of the two\nshould be similar, so ideally, the generator should choose the first\nhalf of the transformer layer of PLMs. The specific choice will be\nverified in the experiment.\n5.2.2 Pruning Sequence in Dim-Reduction layers. The methods\nmentioned in section 5.2.1 have mitigated the impact of homo-\ngeneity on rationale selection. However, generators' inevitable\nmulti-layer transformer leads to different tokens acquiring exces-\nsive contextual information. Especially for larger PLMs such as\nBert-large-uncased (24-layer transformers), the generator will still\ncontain numerous layers of transformers, which will lead to exces-\nsive fusion of information between irrelevant tokens and rationale\ntokens, reducing the heterogeneity between tokens. To tackle this\nproblem, we propose the Dim-Reduction layers to prune irrelevant\ntokens. Since the Transformer model processes all input sequence\ntokens in parallel, context pruning can be seen as reducing the\nsequence dimension.\nAs shown in Figure 5, the Transformer layers in the generator are\ndivided into two parts. One part consists of standard Transformer\nlayers that generate the hidden states $H_k$.\n$H_k = Transformer_{0-k} (X).$ (9)\nThe other part is the Dim-Reduction layers. In each Dim-Reduction\nlayer, a proportion of tokens are pruned. Figure 6 illustrates the\ndetailed design of the Dim-Reduction layers. For the i-th Dim-\nReduction layer, the transformer layer learns the hidden state $H_{i-1}$\nof the previous layer to get $H_i$.\n$H'_i = Transformer (H_{i-1}) .$ (10)\nThen to predict which tokens need to be pruned, we employ the\nMLP as a mask predictor after the transformer layer. This mask pre-\ndictor utilizes the hidden states $H'_i$ of the input sequence to predict\nwhether the corresponding tokens will be pruned or retained, with\nan output of 0 indicating pruning and 1 indicating retaining. The\nmask predictor's process of pruning hidden states $H'_i$ is a binary\nprediction, which is non-differentiable and thus cannot be opti-\nmized through backpropagation. Following prior work, we employ\nthe reparameterization trick using Gumbel-Softmax for sampling.\nTherefore, we can compute the mask $M'_i = softmax (MLP (H'_i))$\nfor pruning in this Dim-Reduction layer. By optimizing this mask\npredictor, irrelevant tokens that do not significantly contribute to\nlabel prediction will be pruned.\nThe value 0 in the mask $M'_i$ represents the corresponding tokens\nthat the i-th Dim-Reduction layer should prune. The tokens pruned\nby the previous Dim-Reduction layer should also be pruned by the\nnext layer. Then we get the final pruning mask $M_i = M'_i \\otimes M_{i-1}$.\nThe computation of the mask $M_i$ can be written as:\n$M_i = softmax (MLP (H'_i)) \\otimes M_{i-1}$ (11)\nwhere MLP = Linear (LN)\nwhere LN refers to the layer normalization. Finally, we multiply the\nmask $M_i$ with $H'_i$ to get the hidden state $H_i$ of the Dim-Reduction\nlayer.\n$H_i = H'_i \\otimes M_i.$ (12)\nIf this layer is the last layer of the Dim-Reduction layers, $M_i$ is\nthe final rationale mask M. Existing methods used $L_s$ as shown\nin Eq.(3) in Section 3 to control the brevity and continuity of the\nrationale. PLMR consists of multiple Dim-Reduction layers, each of\nwhich needs to control the proportion of tokens pruned. Similar to\nprevious research, we control the sparsity $a_j$ of tokens retained in\nthe j-th Dim-Reduction layer. The value of a should decrease pro-\ngressively across multiple layers, ensuring that each layer prunes\na certain number of less relevant tokens. The final layer's a corre-\nsponds to the sparsity of rationale tokens that need to be selected.\nIn this paper, we set the variation of a to be linear change. Addi-\ntionally, we apply a continuity control at each Dim-Reduction layer.\nTherefore, we compute the sparsity constraint and the continuity\nconstraint for each of the Dim-Reduction layers and compute the\nmean value as the constraint term $L_s$:\n$L_s = \\frac{1}{m} \\sum_{j=0}^m \\lambda_1 \\alpha_j \\frac{1}{n} \\sum_{i=0}^n M^j_i + \\lambda_2 \\sum_{i=0}^n |M^j_i - M^j_{i-1}| (13)$\nwhere m represents the number of Dim-Reduction layers, $a_j$ de-\nnotes the proportion of tokens retained after pruning in the j-th\nDim-Reduction layer, and $M^j_i$ indicates the mask for the i-th token\nin the j-th Dim-Reduction layer."}, {"title": "5.3 Rationale Prediction Module", "content": "In this section, we discuss how PLMR prevents the predictor from\noverfitting the erroneous rationales produced by the generator,\nthereby enhancing the quality of the explanations.\n5.3.1 Regularizing Predictions. In Figure 5, the predictor is com-\nposed of residual transformer layers and an MLP. The transformer\nlayers learn the representation of the rationale, and the final output\nis obtained by applying average pooling to the sentence represen-\ntation. The MLP then uses this output for prediction. Formally,\nwe denote L(p(X); Y) as the cross-entropy loss. First, we use the\nrationale for prediction to get the prediction loss $L_{task}$:\n$L_{task} = L(p(M \\otimes X); Y),$ (14)\nwhere M is the rationale mask generated by the Dim-Reduction\nlayers. Meanwhile, the predictor uses the full-text hidden state H\nfor prediction and calculates the full-text prediction loss as $L_{task}$:\n$L^X_{task} = L(p(H)); Y).$ (15)\nFinally, we use $L_{match}$ as a regularizer for generating the rationale:\n$L_{match} = \\lambda h (L_{task} - L^X_{task}),$ (16)\nwhere the function h(x) is monotonically increasing and $\\lambda > 0$.\n5.3.2 The Objective of $L^X_{task}$ and $L_{match}$ . Many studies [21, 23, 32]\nuse additional information to guide the predictor. Inspired by this,\nwe use $L^X_{task}$ to ensure the predictor utilizes full-text representa-\ntions for prediction. Early in training, when the quality of learned\nrationales is poor, we want $L_{task}$ and $L^X_{task}$ to be similar. As training\nprogresses and rationale quality improves, we want the rationales\nto make more accurate predictions, $L_{task} \\leq L^X_{task}$. We achieve this\nby setting the function h(x) in the regularization term $L_{match}$ to\nbe monotonically increasing. The detailed theoretical analysis is in\nAppendix A.2."}, {"title": "5.4 Optimization", "content": "By combining all the above equations, the total objective of our\nrationale module is as follows:\n$\\min_{\\theta_g,\\theta_p} E_{X,Y~D} [L_{task} + L^X_{task} + L_{match} + L_s].$ (17)\n$M~g(X)$\nDuring the training phase, we use the Adam optimizer [16] to opti-\nmize the above objective. Empirical evaluation showed that these\nlosses were of comparable magnitudes. Therefore, simple averaging\nwas chosen to maintain simplicity and robust performance. During\nthe inference phase, the predictor only uses the rationale generated\nby the generator for prediction."}, {"title": "6 EXPERIMENTS", "content": "6.1 Datasets\nWe use two widely used datasets in rationalization [12", "24": ".", "27": "is a dataset for multi-aspect sentiment pre-\ndiction on beer reviews", "1": ".", "35": "we use the original\ndataset with highly correlated aspects (referred to as the corre-\nlated dataset) to validate the effectiveness of PLMR in addressing\nsevere rationalization degeneration and failure. HotelReview [30"}, {"1": ".", "base-\nlines": "rationalizing neural prediction (RNP [17", "4": "folded rationalization (FR [22", "36": "which also utilize Bert-base-uncased. In the\nRNP, INVART, and FR frameworks, GRU was originally used. Here,\nwe re-experiment using BERT. In addition, to demonstrate the great\nimprovement in the interpretation performance of our method,\nwe compare it with the latest methods that can provide the best\nrationale using GRU, such as folded rationalization (FR [22", "23": "and Guidance-based\nRationalization method (G-RAT [12", "7": "Roberta-base [25"}]}