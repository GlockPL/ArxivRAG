{"title": "Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning", "authors": ["Kaustubh Ponkshe", "Raghav Singhal", "Eduard Gorbunov", "Alexey Tumanov", "Samuel Horvath", "Praneeth Vepakomma"], "abstract": "Low-rank adapters have become a standard approach for efficiently fine-tuning large language models (LLMs), but they often fall short of achieving the performance of full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that approximates full fine-tuning within low-rank subspaces using a carefully designed initialization strategy. We theoretically demonstrate that the architecture of LoRA-XS-which inserts a trainable $r\\times r$ matrix between B and A while keeping other matrices fixed-provides the precise conditions needed for this approximation. We leverage its constrained update space to achieve optimal scaling for high-rank gradient updates while removing the need for hyperparameter tuning. We prove that our initialization offers an optimal low-rank approximation of the initial gradient and preserves update directions throughout training. Extensive experiments across mathematical reasoning, commonsense reasoning, and language understanding tasks demonstrate that our approach exceeds the performance of standard LoRA while using 27-90x fewer parameters, and comprehensively outperforms LORA-XS. Our findings establish that it is possible to simulate full fine-tuning in low-rank subspaces, and achieve significant efficiency gains without sacrificing performance. We have released our code publicly at https://github.com/RaghavSinghal10/lora-sb.", "sections": [{"title": "1. Introduction", "content": "Pre-trained language models have become central to natural language processing, achieving state-of-the-art performance across diverse tasks (Radford et al., 2021; Kirillov et al., 2023; Achiam et al., 2023). While these models excel at general-purpose capabilities (Bubeck et al., 2023; Hao et al., 2022), adapting them to specific downstream tasks often requires fine-tuning. Although in-context learning (Brown et al., 2020; Radford et al., 2019) has gained popularity for its simplicity, it falls short in both performance and efficiency compared to fine-tuning (Liu et al., 2022). At the same time, full fine-tuning, while highly effective, is computationally expensive and impractical at scale, highlighting the need for more efficient adaptation techniques.\nParameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for adapting large language models (LLMs) while addressing computational constraints. While full fine-tuning (FFT) typically achieves optimal performance, it requires updating billions of parameters, making it computationally intensive and often impractical for most applications. Low-rank decomposition methods, beginning with LORA (Hu et al., 2021), have shown particular promise by significantly reducing trainable parameters through learning low-rank updates. This has sparked numerous advances in low-rank methods that either enhance performance through better optimization techniques and initialization strategies, or improve parameter efficiency through structured matrices and adaptive rank selection (Zhang et al., 2023; Wang et al., 2024b;a). However, these methods still face fundamental trade-offs, they must either maintain a relatively large number of parameters to match FFT performance, or accept a performance degradation when pursuing extreme parameter efficiency (Hu et al., 2021; Ding et al., 2023; Wang et al., 2024b). This raises an important question: can we design low-rank methods that maintain FFT-competitive performance while drastically reducing the parameter count beyond current approaches?\nLow-rank decomposition methods operate on a fundamental premise: fine-tuning requires learning only a low-rank update to the pre-trained weights. Some theoretical work extends this hypothesis, suggesting that methods like LORA can learn any low-rank approximation of the full fine-tuning gradient. However, the gradients computed by these methods to update their trainable adapters do not inherently possess this property. For instance, LoRA's gradients need explicit optimization at each step to better approximate the full fine-tuning gradient (Wang et al., 2024b). Additionally, initialization has emerged as a critical factor in low-rank adaptation, as highlighted by recent works like Pissa-LoRA (Meng et al., 2024) and LoRA-GA (Wang et al., 2024a), which address this challenge through various low-rank approximation techniques.\nWe formally analyze these limitations in the context of the architecture used in LoRA-XS (Ba\u0142azy et al., 2024)\u2014which inserts a trainable $r\\times r$ matrix between B and A while keeping other matrices fixed-and demonstrate that these challenges are even more pronounced in its framework. While exploring solutions inspired by existing LoRA-based methods, we discover a remarkable property unique to LoRA-XS: through careful initialization of matrices A and B, we can approximately simulate the full fine-tuning optimization in low rank subspaces throughout the entire training, as shown in Figure 1. Our initialization strategy provides optimal scaling for approximating high-rank full fine-tuning gradients and eliminates the need for the scaling hyperparameter $\\alpha$, results which we prove theoretically. Our key contributions include:\n\u2022 We theoretically formalize the limitations of LORA-XS, showing how its constrained update space leads to suboptimal gradient approximation, initialization sensitivity, and hyperparameter dependence."}, {"title": "2. Related Work", "content": "Parameter-Efficient Fine-Tuning. Parameter-Efficient Fine-Tuning (PEFT) methods have become essential for adapting large pre-trained models under computational constraints. Early techniques like AdapterFusion (Pfeiffer et al., 2021) and Prefix-Tuning (Li & Liang, 2021) enabled task-specific adaptation with minimal parameter updates. Advances like soft prompts (Lester et al., 2021) further reduced trainable parameter counts while maintaining strong performance. Recent approaches have explored operating directly on model representations (Wu et al., 2024), offering different trade-offs between efficiency and performance.\nLow-Rank Decomposition Methods. LoRA (Hu et al., 2021) demonstrated that weight updates during fine-tuning could be efficiently approximated using low-rank matrices, drastically reducing parameter counts while freezing pre-trained weights. Building on this insight, variants such as QLORA (Dettmers et al., 2023) and AdaLoRA (Zhang et al., 2023) extended the paradigm through quantization and adaptive allocation strategies. The applicability of low-rank techniques has also been explored in pretraining with GaLore (Zhao et al., 2024) and ReLORA (Lialin et al., 2023), highlighting the versatility of low-rank adaptation in both fine-tuning and pre-training contexts. LoRA-based methods have also been applied in other domains, such as efficient federated fine-tuning (Sun et al., 2024; Singhal et al., 2024).\nEnhancing LoRA Performance. Recent efforts have focused on optimizing LoRA's performance. PiSSA-LORA (Meng et al., 2024) demonstrated faster convergence and improved task performance by initializing matrices with principal components of pre-trained weights. LoRA-Pro (Wang et al., 2024b) and LoRA-GA (Wang et al., 2024a) improved gradient approximation, aligning low-rank updates more closely with full fine-tuning. Other methods like DORA (Liu et al., 2024) and rsLoRA (Kalajdzievski, 2023) introduced decomposition-based and scaling stabilization techniques to enhance learning stability and expand LoRA's utility.\nImproving Efficiency in LoRA Variants. Efficiency-focused innovations have pushed LoRA toward even greater parameter savings. LoRA-XS (Ba\u0142azy et al., 2024) achieves parameter reduction by inserting a small trainable weight matrix into frozen low-rank matrices. VeRA (Kopiczko et al., 2024) shares low-rank matrices across layers, relying on scaling vectors for task-specific adaptation. Tied-LoRA (Renduchintala et al., 2024) leverages weight tying to reduce parameter usage at higher ranks, while HydraLoRA (Tian et al., 2024) introduces an asymmetric architecture to improve adaptation in complex domains without domain-specific expertise."}, {"title": "3. Methodology", "content": "3.1. Preliminaries\nIn standard fine-tuning, a pre-trained weight matrix $W \\in \\mathbb{R}^{m \\times n}$ is updated as:\n$W = W_0 + \\Delta W$\nwhere $W_0$ is the pre-trained weight. This requires updating $mn$ parameters per layer.\nLORA hypothesizes that these updates lie in a low-\ndimensional subspace and parameterizes the update matrix $\\Delta W$ as:\n$W = W_0 + sBA$\nwhere $B \\in \\mathbb{R}^{m \\times r}$ and $A \\in \\mathbb{R}^{r \\times n}$ are trainable low-rank matrices, with rank $r < \\text{min}(m, n)$, and $s$ is a scaling factor to stabilize training. This reduces parameters from $mn$ to $r(m + n)$.\nLORA-XS introduces a more efficient parameterization:\n$W = W_0 + sBRA$\nwhere $B \\in \\mathbb{R}^{m \\times r}$ and $A \\in \\mathbb{R}^{r \\times n}$ are fixed matrices, and only $R \\in \\mathbb{R}^{r \\times r}$ is trainable, reducing parameters to $r^2$.\nWe denote full fine-tuning gradient as $g = \\frac{\\partial L}{\\partial W}$, and LoRA-XS gradient as $g_R = \\frac{\\partial L}{\\partial R}$.\n3.2. Motivation\nLORA-XS (Ba\u0142azy et al., 2024), although having significantly fewer learnable parameters compared to LoRA, exhibits suboptimal performance. The difference in architecture causes constraints on the type of updates LoRA-XS can learn. The subspace of learned updates is characterized in Lemma 1."}, {"title": "1) Inadequate Gradient Approximation:", "content": "LoRA optimization is mathematically equivalent to full fine-tuning using a constrained low-rank gradient. The gradient of LoRA does not optimally approximate the full gradient, and needs to be tuned at each step. LoRA-Pro (Wang et al., 2024b) finds that this results in suboptimal performances, and provides a closed form solution to optimize the gradients. In LoRA-XS, the gradient updates are restricted to an even more constrained low-rank space since the matrices A and B are fixed. We posit that the limitation becomes particularly severe when the ideal updates lie outside the space spanned by fixed A and B, and consequently has a larger impact on performance.\n2) Suboptimal Initialization: While initialization impacts all low-rank methods, it becomes critical in LORA-XS where A and B are frozen. Unlike LORA where poor initialization can be compensated through training, LORA-XS must rely entirely on its initial subspace defined by A and B. Consider the zero initialization of the B matrix, for example. While LoRA may experience some performance degradation in this case (Wang et al., 2024a) (Meng et al., 2024), the ideal low-rank update $\\Delta W$ can still be reached through gradient descent. In fact, zero initialization for the B matrix is a commonly used choice, including in the original LoRA implementation (Hu et al., 2021). However, in the case of LORA-XS, this would result in no learning, as the product BRA would remain zero. The current LoRA-XS method leverages the most significant subspaces spanned by the columns of pre-trained weights $W_0$, inspired by (Meng et al., 2024). This initialization is inadequate because it fails to capture the specific subspaces relevant to the fine-tuning task. This issue, known to affect LORA 's performance (Wang et al., 2024a), could have an even greater impact on LoRA-XS for the reasons discussed above.\n3) Hyperparameter Sensitivity: The scaling factor $\\alpha$, present in almost every LoRA based fine-tuning method requires careful tuning to maintain stability during training. This hyperparameter acts as a bridge between the low-rank and full-rank spaces, compensating for the dimensional mismatch in gradients. Poor tuning of $\\alpha$ can lead to unstable training or slow convergence (Kalajdzievski, 2023), adding complexity to the adaptation process and potentially limiting practical deployment."}, {"title": "3.3. Approximation of the full fine-tuning gradient", "content": "As mentioned above, LoRA optimization is mathematically equivalent to full fine-tuning using a constrained low-rank gradient. However the update generated using the gradients of LoRA does not result in the same update which the low-rank gradient would have generated. The following holds true for LORA-XS as well.\nTo understand this, let us look at the change in weight W and its relationship with changing of low-rank matrix R. The relationship can be simply given by $dW = \\frac{\\partial W}{\\partial R} g_R$. This relationship implies that updating matrix R with gradient $g_R$ is equivalent to updating W with low rank equivalent gradient $\\tilde{g}$ in full fine-tuning as described in Definition 1."}, {"title": "Definition 1 (Equivalent Gradient).", "content": "In the context of LoRA-XS optimization, we define the equivalent gradient as:\n$\\tilde{g} = sB g_R A$\nwhere s is the scaling factor, and $g_R$ is the gradient with respect to matrix R.\nThe equivalent gradient describes the virtual low-rank gradient of matrix $W$ in LoRA-XS optimization process, despite $W$ not being directly trainable. This gradient determines how updates to $R$ affect the overall weight matrix. To bridge the performance gap between LoRA-XS and full fine-tuning, our goal is to minimize the discrepancy between the equivalent gradient $\\tilde{g}$ and the full gradient $g$. First, we establish the relationship between gradients in LoRA-XS optimization in Lemma 2."}, {"title": "Lemma 2. During LoRA-XS optimization,", "content": "the gradient of the loss with respect to matrix R can be expressed in terms of the gradient with respect to the weight matrix W as:\n$g_{\\text{LoRA-XS}} = sB^T G A^T$\nLeveraging this relationship, we can formulate our objective to minimize the distance between the equivalent gradient and the full gradient. We do not have access to the full fine-tuning gradient g during LoRA-XS based fine-tuning. Thus we need to find the ideal gradient with respect to R, given by $g_R$, and subsequently the optimal approximation $\\tilde{g}$, in terms of the gradient which is available to us during training: $g_{\\text{LoRA-XS}}$. Fortunately, this optimization problem admits a closed-form solution independent of g as described in Theorem 3."}, {"title": "Theorem 3.", "content": "Assume matrices $B \\in \\mathbb{R}^{m\\times r}$ and $A \\in \\mathbb{R}^{r \\times n}$ are both full rank. For the objective $min_{g_R}||\\tilde{g} - g||$, such that $\\tilde{g} = sBg_R A$, the optimal solution is given by:\n$g_R = \\frac{1}{s^2}(B^T B)^{-1} g_{\\text{LoRA-XS}}(A A^T)^{-1}$"}, {"title": "3.4. Initialization using update approximation", "content": "Let us first consider the objective in fine-tuning, the primary goal is to update weights to better suit the target task and domain. The initial gradient steps are particularly informative, as they indicate the direction of desired adaptation. We propose leveraging this insight by using the first update step from full fine-tuning to initialize our low-rank matrices.\nThis approach offers two key advantages: First, it ensures the low-rank space captures the most relevant subspace for the target task rather than relying on pre-trained weight properties. Second, since A and B matrices will remain fixed in LoRA-XS, initializing them to span the subspace of early adaptation increases the likelihood of capturing useful updates throughout training. This would also ensure that the final update is learnt in the correct subspace, of which we have no apriori information besides the first step of full fine-tuning.\nIn essence, our method can be summarized as follows: Set an initialization which best approximates the first step of full fine-tuning. Formally, given a full fine-tuning update $\\Delta W$, our initialization should satisfy:\n$SB_{init} R_{init} A_{init} \\approx \\Delta W_{\\text{first-step}}$\nThe first step of full fine-tuning, for Adam-based optimizers such as AdamW, for a sample $x_i$ is:\n$\\Delta W_{\\text{first-step}} = -\\eta \\times \\text{sign}(\\nabla_W L(W_0, x_i))$\nUsing a single sample may lead to noisy estimates. Instead, we compute a more stable initialization by averaging gradients over a subset of the training data:\n$\\Delta W_{avg} = -\\eta \\text{sign}(\\sum_{i=0}^{n<X} \\nabla_W L(W_0, x_i))... X_i \\in \\mathbb{X}$"}, {"title": "3.5. Hyperparameter independence", "content": "The hyperparameter $\\alpha$ is used in LoRA and subsequent decomposition based models to tackle the issue of instability caused to improper scaling of the updates. The gradient scaling is accounted for, by adding a hyperparameter to normalize the updates. The importance of scaling is well documented in methods like rank stabilization (Kalajdzievski, 2023). However, the full fine-tuning gradient $g$ needs no such tuning. We claim that approximating the full fine-tuning gradient removes the need for introducing a scaling factor."}, {"title": "Theorem 5. The equivalent gradient $\\tilde{g}$ is hyperparameter s independent for $\\tilde{g} = sBg_RA$", "content": "The hyperparameter independence of the equivalent gradient eliminates the need for manual gradient scaling. Updates to weight matrix W depend solely on this gradient (modulo learning rate), making any additional scaling redundant.\nThis can be understood by examining the relationship with full fine-tuning gradient $g$. Since $g$ is naturally scaled for optimal weight updates, and our method approximates g in a constrained subspace, the equivalent gradient inherits appropriate scaling automatically. Notably, this property is unique to our gradient approximation approach and does not hold for standard LoRA-XS training."}, {"title": "3.6. LoRA-SB: Update approximation initialization is a silver bullet", "content": "The solutions discussed above independently address the gradient approximation and initialization problems, while also providing hyperparameter independence. Our proposed method, LoRA-SB, elegantly combines these solutions through a simple initialization strategy.\nOur initialization strategy is derived from approximating the first step of full fine-tuning:\n$\\mathbb{U}, \\mathbb{S}, \\mathbb{V}^T \\leftarrow \\text{SVD}(\\Delta W_{avg})$\n$B_{init} \\leftarrow \\mathbb{U}[1:r]$\n$A_{init} \\leftarrow \\mathbb{V}[1:r]$\n$R_{init} \\leftarrow \\frac{1}{s} \\mathbb{S}[1: r, 1 :r]$\nBy the Eckart-Young theorem (Eckart & Young, 1936; Mirsky, 1960), this gives the optimal rank-r approximation of the initial full fine-tuning update. where $\\mathbb{U}, \\mathbb{S}, \\mathbb{V}$ are obtained from truncated SVD of the averaged first update $\\Delta W_{avg}$.\nThis initialization leads to several key advantages that address the problems identified earlier.\nSimplified Gradient Optimization. This initialization ensures $B_{init}$ and $A_{init}$ form orthonormal bases in $\\mathbb{R}^m$ and $\\mathbb{R}^n$ respectively, leading to $B^T B = AA^T = I$. With fixed B and A matrices being orthonormal, the optimal update step derived in Equation 3 simplifies to:\n$g_R = \\frac{1}{s^2}(B^T B)^{-1}g_{\\text{LoRA-XS}}(AA^T)^{-1} = \\frac{1}{s^2}g_{\\text{LoRA-XS}}$\nThis eliminates the need for complex matrix inversions during training.\nOptimum Update Approximation. Our initialization guarantees that the first update optimally approximates the full fine-tuning weight updates:\n$B_{init} R_{init} A_{init} \\approx \\Delta W_{avg}$\nBy the Eckart-Young theorem, this gives the optimal rank-r approximation of the initial full fine-tuning update.\nHyperparameter Independence. As shown in Theorem 5, when gradient approximation is applied with orthonormal B and A, the hyperparameter s can be set to 1, resulting in the following:\n$\\tilde{g_R} = g_{\\text{LoRA-XS}}$\nThis demonstrates that our initialization guarantees optimal gradient approximation at every step, without requiring any scaling factor!\nGuaranteed Loss Reduction. Since B is a tall orthonormal matrix and A is a wide orthonormal matrix, they remain full rank throughout training. This ensures that dL remains negative 4, guaranteeing stable optimization and convergence.\nAnother heuristic which might lead to a good initialization is setting the weights B and A, such that they match the first update also approximately matches the direction of $\\Delta W$.\n$\\Delta(SB_{init} R_{init} A_{init}) \\approx \\eta \\Delta W$\nThankfully, we don't have to choose between the two. For SGD, we prove that setting $B_{init}$ and $A_{init}$ using Equations 8-11, results in the first update of LoRA-XS to best approximate the direction of the update of full fine-tuning."}, {"title": "Theorem 6.", "content": "If $A_{init}$ and $B_{init}$ are initialized using LoRA-SB for the first SGD optimizer, then\n$\\Delta(B_{init} R_{init} A_{init}) \\approx \\Delta W$"}, {"title": "4. Experiments", "content": "We evaluate our method over 16 different datasets on three widely-used NLP benchmarks, using models ranging from the 355 M-parameter ROBERTa-large model to the 9 B-parameter Gemma-2 model. Our setup spans both masked and autoregressive architectures, allowing us to comprehensively assess the effectiveness of our approach. We fine-tune ROBERTa-large (Liu et al., 2019), Llama-3.2 3B (Dubey et al., 2024), Mistral-7B (Jiang et al., 2023), and Gemma-2 9B (Team et al., 2024), showcasing our method's adaptability across a variety of tasks and model architectures.\nWe implement all models using PyTorch (Paszke et al., 2019) and the popular HuggingFace Transformers library (Wolf et al., 2020). We run all experiments on a single NVIDIA A6000 GPU and report results as the average of three random seeds. To save memory, we initialize base models in torch.bfloat16 precision. Appendix D provides detailed information on the datasets used.\nWe compute the update approximation using only 1/1000 of each dataset's total number of samples. This ensures that the additional training time overhead is minimal and has a negligible effect on overall efficiency. We provide a benchmark of this overhead in Section 5. These samples are randomly selected from the dataset's training set in each run.\n4.1. Arithmetic Reasoning\nDetails. We fine-tune Mistral-7B (Jiang et al., 2023) and Gemma-2 9B (Team et al., 2024) on 50K samples from the MetaMathQA (Yu et al., 2024) dataset and evaluate them on the GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) benchmarks. Evaluation focuses solely on the final numeric answer. We apply LoRA modules to the key, value, query, attention output, and all fully connected weight matrices, training with ranks r = {32,64,96}. Detailed hyperparameter settings are provided in Appendix C.\nResults. We present the results for both models in Table 1. LORA-SB significantly outperforms LoRA-XS across all parameter settings. Notably, LoRA-SB surpasses the performance of LORA (r = 32) while using 40x fewer trainable parameters for Mistral-7B and 90x fewer for Gemma-2 9B at ranks r 96 and r 64, respectively.\nWe present training loss curves comparing LoRA-SB and LORA-XS in Figure 2. Thanks to superior initialization, LORA-SB starts with a lower initial loss compared to LoRA-XS. Additionally, due to optimal gradient approximation, LORA-SB maintains a consistently better loss curve throughout training and converges to a superior final value.\n4.2. Commonsense Reasoning\nDetails. We fine-tune Llama-3.2 3B (Dubey et al., 2024) on COMMONSENSE170K, a dataset that aggregates eight commonsense reasoning tasks (Hu et al., 2023). We evaluate the model's performance on each dataset individually, which include BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019), PIQA (Bisk et al., 2020), ARC-Challenge (Clark et al., 2018), ARC-Easy (Clark et al., 2018), OBQA (Mihaylov et al., 2018), WinoGrande (Sakaguchi et al., 2021), and HellaSwag (Zellers et al., 2019). Each example is framed as a multiple-choice question where the model generates the correct answer without explanations. We use the prompt template from (Hu et al., 2023). LORA modules are applied to the key, value, query, attention output, and all fully connected weight matrices, training with ranks r = {32,64,96}. Hyperparameter details are provided in Appendix C.\nResults. We present the results in Table 2. LORA-SB consistently outperforms LoRA-XS across all parameter settings. Additionally, LoRA-SB (r = 96) exceeds the performance of LoRA (r = 32) with 27x fewer trainable parameters.\n4.3. Natural Language Understanding\nDetails. We fine-tune ROBERTa-large (Liu et al., 2019) on GLUE, a sequence classification benchmark that contains several datasets, covering domains like sentiment analysis and natural language inference. The datasets we evaluate on are: COLA (Warstadt et al., 2019), RTE, MRPC (Dolan & Brockett, 2005), SST-2 (Socher et al., 2013), QNLI (Rajpurkar et al., 2018), and STS-B (Cer et al., 2017). LoRA modules are applied only to the self-attention layers, following the configuration in the original LoRA paper (Hu et al., 2021), with ranks r = {8,16,24}. Detailed experimental settings are available in Appendix C.\nResults. The results are shown in Table 3. LORA-SB consistently outperforms LoRA-XS across all parameter configurations. Notably, LoRA-SB (r = 24) outperforms LORA (r = 8) with 39x lesser trainable parameters."}, {"title": "5. Analysis", "content": "Optimal Initialization is Important!"}, {"title": "Training Time Overhead vs LoRA-XS", "content": "As previously mentioned, we compute the update approximation using only 1/1000 of the total training samples for each dataset. Table 5 presents the associated training time overhead for these computations, compared to LoRA-XS. The results show that the additional overhead is negligible, adding just 2-4 minutes compared to the total training time of 3-5 hours per epoch (\u2248 1.1% to 1.3%). Additionally, the update computation is performed only once, at the beginning of the first epoch, prior to training."}, {"title": "Inference Overhead vs LORA", "content": "LORA-SB introduces a minimal inference cost overhead due to the insertion of the $r \\times r$ matrix R between B and A, and the need for higher ranks to achieve comparable performance to LoRA. We benchmark the inference-time FLOPs and MACs across various models and find that the overhead is negligible. A detailed analysis can be found in Table 6 of Appendix B."}, {"title": "6. Conclusion", "content": "In this work, we introduced LoRA-SB, a novel framework that bridges the gap between low-rank parameter-efficient fine-tuning and full fine-tuning. This is enabled by our carefully designed initialization strategy, which approximates the first step of full fine-tuning and ensures that the most relevant subspaces for task-specific adaptation are captured from the outset. Our theoretical analysis uncovered critical limitations in LoRA-XS: suboptimal gradient approximation and reliance on fixed initialization that fails to adapt to task-specific nuances. By addressing both issues through a unified approach, we demonstrate that LoRA-SB achieves optimal scaling and preserves critical update directions. Furthermore, our approach ensures hyperparameter independence by directly approximating the full fine-tuning gradient, thereby eliminating instability and convergence issues associated with scaling factors.\nThrough extensive experiments on 4 models across 16 datasets covering mathematical reasoning, commonsense reasoning, and language understanding tasks, we demonstrate that our method exceeds the performance of LORA while using upto 90x less parameters, and comprehensively outperforms LoRA-XS. Our work advances the state of PEFT while laying the groundwork for further innovations in low-rank adaptations for neural networks. Future work includes exploring adaptive layer-wise rank settings and evaluating the approach on other architectures, such as Vision Language Models (VLMs) and Vision Transformers (ViTs)."}]}