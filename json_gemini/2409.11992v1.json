{"title": "Additive-feature-attribution methods: a review on explainable artificial intelligence for fluid dynamics and heat transfer", "authors": ["Andr\u00e9s Cremades", "Sergio Hoyas", "Ricardo Vinuesa"], "abstract": "The use of data-driven methods in fluid mechanics has surged dramatically in recent years due to their capacity to adapt to the complex and multi-scale nature of turbulent flows, as well as to detect patterns in large-scale simulations or experimental tests. In order to interpret the relationships generated in the models during the training process, numerical attributions need to be assigned to the input features. One important example are the additive-feature-attribution methods. These explainability methods link the input features with the model prediction, providing an interpretation based on a linear formulation of the models. The SHapley Additive exPlanations (SHAP values) are formulated as the only possible interpretation that offers a unique solution for understanding the model.\nIn this manuscript, the additive-feature-attribution methods are presented, showing four common implementations in the literature: kernel SHAP, tree SHAP, gradient SHAP, and deep SHAP. Then, the main applications of the additive-feature-attribution methods are introduced, dividing them into three main groups: turbulence modeling, fluid-mechanics fundamentals, and applied problems in fluid dynamics and heat transfer. This review shows thatexplainability techniques, and in particular additive-feature-attribution methods, are crucial for implementing interpretable and physics-compliant deep- learning models in the fluid-mechanics field.", "sections": [{"title": "1. Introduction", "content": "Since McCulloch and Pitts (1943) established the basis for artificial neural networks, proposing a mathematical formulation for the neural connections, data-driven methodologies have been developed, becoming an essential tool for modern research (Jordan and Mitchell, 2015). In the last years, machine learning has been widely used in different industrial (Lee et al., 2016; Bertolini et al., 2021) and scientific areas (Mamoshina et al., 2016; Albertsson et al., 2018) due to its ability to learn from data and to predict the solution for complex applications. These methods are exceptionally effective in solving those problems where the exact equations are not fully known or impossible to solve. Fluid mechanics is not an exception (Brunton, 2021; Vinuesa, 2024), as in the fluid-mechanics field, non-linear relationships are generated within the flow as a consequence of the complexity of the Navier-Stokes equations (Navier, 1827; Stokes, 2009) that govern it.\nBefore applying machine learning, the analysis of turbulent flows was limited to large numerical simulations (Ishihara et al., 2009; Lee et al., 2013; Alc\u00e1ntara-\u00c1vila et al., 2021) or experimental tests (Laufer, 1975; Talamelli et al., 2009; Nagib et al., 2009). Deep learning has opened new possibilities and horizons in fluid mechanics (Vinuesa et al., 2023), providing cost-efficient solutions for problems that in the past required long computation or experimental campaigns. Therefore, its use has increased exponentially in the last years for experimental (Rabault et al., 2017; Kim et al., 2024) and numerical (Bar-Sinai et al., 2019; Wang et al., 2017; Lee and Carlberg, 2020) studies. Concerning the former, machine learning exhibits the potential to improve measurement techniques, enhance experimental design, and control flows (Vinuesa et al., 2023; Yousif et al., 2023). Regarding the latter, Vinuesa and Brunton (2022) stated that machine learning has been used for developing reduced-order models (Kaiser et al., 2014), improving the turbulence models (Beck et al., 2019) and accelerating the simulations (Li et al., 2020; Kochkov et al., 2021).\nA wider understanding of turbulence transport (Guastoni et al., 2021), the effect of design parameters on the flow (Du et al., 2022), and how the operation conditions affect the performance (Lin and Liu, 2020) are essential for future scientific discoveries and optimization of systems working with fluids, but they require a deeper explanation of the deep-learning models used for the predictions. However, these models are commonly used as black boxes and the relationships between the input parameters and the outputs are not analyzed.\nOne of the most promising ideas used in the last years to understand turbulent flows is to analyze the causal relationships between the flow patterns and structures. Osawa and Jimenez (2024) applied interventional causality, modifying the flow inside a subdomain of the computational simulation, and then, calculating the evolution of the modified flow. This methodology transports the perturbations in time and evaluates their effect in the developed flow. As a result of this methodology, sweep-like structures (Lozano-Dur\u00e1n et al., 2012) were shown to be the ones affecting the flow the most. A different solution was proposed by Lozano-Dur\u00e1n and Arranz (2022), who used information theory to detect the causal relationships among various flow features (Lozano-Dur\u00e1n et al., 2020, 2023). Note that a deep-learning model generates a prediction from a set of input features, based on the data previously seen by the model. Therefore, to explain the patterns extracted by the model from the data it is necessary to analyze the impact of each input on the prediction.\nIn the case of deep-learning models, due to the large number of parameters and their complex architecture, a simplification is required to explain how each input feature affects the solution. Additive-feature-attribution methods have been proposed as a solution for explaining the internal relationships of the models by calculating a linear simplified representation. Each coefficient of the linear model assigns an importance attribution to the respective input feature. In order to calculate these attributions, recent research, such as the works by Lundberg and Lee (2017) or Sundararajan et al. (2017), have extended the game-theoretic methodology started by Shapley et al. (1953) 70 years ago. These improvements allow adding an explainability layer to the deep-learning models. The use of explainable Artificial Intelligence (XAI), and specifically, additive-feature-attribution methods, provides human-interpretable results to the models and has been widely used in different fields, such as finances (Mokhtari et al., 2019), chemistry (Wojtuch et al., 2021), or process management (Wang et al., 2022a).\nThe present work provides an overview of the methodologies that have been applied to improve the understanding of fluid mechanics by using XAI. First, the general framework of additive-feature-attribution methods is presented, focusing on those that have been applied to fluid mechanics. Then, the main applications of XAI in fluid mechanics are listed, starting with the improvement of turbulence modeling, continuing with the understanding of fluid dynamics, and finishing with miscellaneous industrial applications. The manuscript is concluded with a brief conclusion summarizing the application of XAI for fluid dynamics and providing a brief outlook of the future possibilities and impact of the additive-feature-attribution methods."}, {"title": "2. Additive-feature-attribution methods", "content": "As stated by Lundberg and Lee (2017), the best explanation for a model is the model itself. In the previous statement, explaining is defined as being able to establish textual or visual artifacts that provide qualitative understanding between the feature instances and the output of the model (Ribeiro et al., 2016). A clear example of this idea is Newton's second law: $F = ma$, where the acceleration $a$ of a body of mass $m$ is proportional to the force $F$. For this simple model, the equation is self-explanatory. An increase in the force will increase the acceleration proportionally if the mass is kept constant. However, the interpretation of large models such as those in deep learning (DL) or complex physical equations, for instance, Navier-Stokes equations, is not straightforward. The main difference, from the explainability point of view, between Newton's second law and the Navier-Stokes equations remains in the linearity. Linear systems are simple to understand as modifications in the input generate proportional changes in the output.\nThe additive-feature-attribution methods, as proposed by Lundberg and Lee (2017), exploit the explainable benefits of linear systems. The original model, $f$, is defined for a single input variable, $x$, which is calculated through a mapping function, $h$, from a simplified input, $x'$, $x = h(x')$. Note that, in the present work, we focus on the local methods proposed by Ribeiro et al. (2016), which ensure that the model is explained using a simplified representation $g$, Equation (1), which is equivalent to the original model, $g (z') \u2248 f (h (z'))$, when the simplified input $z'$, which contains a fraction of the non-zero features of the original simplified input $x'$ approximates it, $z' \u2248 x'$.\n$g (z') = \u03a6\u03bf + \\sum_{i=1}^{N}\u03a6_i z_i'$ (1)\nThe definition of the simplified model $g$ depends on a set of $N$ linear parameters $\u03a6_i$ known as SHAP (SHapley Additive exPlanations) values. These SHAP values express the effect of each single attribute $z_i$ when they are present in the model. Indeed, $z_i$ is a boolean parameter that takes the value 0 if the attribute is absent in the model and 1 if it is present.\nThe additive-feature-attribution methods provide a unique solution satisfying three main properties: local accuracy, missingness, and consistency. These properties are explained in more detail below:\n\u2022 Local accuracy: if $x = h(x')$, then, the explainable model $g (x')$ equals the system model $f (x)$, being $\u03a6_0$ the output of the explainable model when all the attributes are absent:\n$f (x) = g (x') = \u03a6\u03bf + \\sum_{i=1}^{N}\u03a6_i x_i'$ (2)\n\u2022 Missingness: the absent features should not have any effect on the output:\n$x_i = 0 \u2192 \u03c6_i = 0.$ (3)\n\u2022 Consistency: for two different models, $g$ and $g'$, the impact of the feature $i$ on the model $g'$ must be higher than in the model $g$ if the error of suppressing the feature $i$ in the model $g'$ is higher than in the model $g$:\nif $g' (z') \u2013 g' (z' \\ {i}) > g (z') \u2013 g (z' \\ {i})$, (4)\nthen $p_i (g', z') > \u03a6_i (g, z') .$ (5)\nAs demonstrated by Young (1985), only the Shapley values proposed by Shapley et al. (1953) satisfy at the same time the three previous properties. These Shapley values are defined by the expression below:\n$\u03a6_i = \\sum_{S \\subseteq F \\{i}} \\frac{|S|! (N \u2212 |S| \u2013 1)!}{N!} [f (S \\cup i) \u2212 f (S)].$ (6)\nIn equation (6), the Shapley value associated with the feature $i$, $\u03a6_i$, is calculated for the model $f$. These Shapley values weight the error of the prediction of the model between a subset, $S$, of the complete set of attributes, $F$, which does not contain the attribute $i$, and the subset when the feature $i$ is included: $[f (S \\cup i) - f (S)]$. Note that the subset $S$ corresponds to the non-null values of the simplified input $z'$, or coalition. The weighting of this error is calculated by quantifying the probability of the feature to happen after the subset $S$, $(|S|! (N \u2212 |S| \u2013 1)!) /N!$, being $|S|$ the total number of present features of the subset and $N$ the total number of features of the model. The Shapley values of any player can be understood as the marginal contribution of the feature $i$ to the existing coalition $S$ (Myerson, 1991).\nAs stated by Lundberg and Lee (2017), the exact calculation of the Shapley values is challenging, as the computational cost grows exponentially with the number of features, $2^N$ (Jia et al., 2019), where the number of parameters might reach $N \u223c O (10^9)$ to $O (10^{10})$ for the larger simulations of turbulent flows (Iwamoto et al., 2004, 2005; Hoyas and Jim\u00e9nez, 2006; Alc\u00e1ntara-\u00c1vila et al., 2018). To overcome this limitation, different approximations, such as kernel SHAP (Lundberg and Lee, 2017), deep SHAP (Lundberg and Lee, 2017), tree SHAP (Lundberg et al., 2018), and gradient SHAP (Erion et al., 2021) have been proposed in the literature."}, {"title": "2.1. Kernel SHAP", "content": "The Kernel SHAP is an approximation of the Shapley values which calculates the effect of each attribute by combining the previously discussed SHAP values with LIME (local interpretable model-agnostic explanations) (Ribeiro et al., 2016). LIME interprets the individual predictions of the model by approximating them locally. The objective function $L$ is minimized as can be observed in Equation (7). Indeed, the contribution to the output of each input feature, $\u03a6$, is calculated by minimizing the previous loss function, $L$, which depends on the explanation model, $g$, defined in equation (1), the original model, $f$, and a local kernel, $\u03c0_x$. Then, a penalization over the complexity of the model is included by the term $\u03a9(g)$.\n$\u00a7 = arg \\min_{g \\in G} L(f, g, \u03c0_x) + \u03a9(g).$ (7)\nAlthough the LIME expression present in Equation (7) differs from the Shapley definition, Equation (6), it might satisfy local accuracy, missingness, and consistency conditions that the additive-feature-attribution method requires. For a heuristical definition of the loss function, $L$, the local kernel $x$ and the regularization term, $\u03a9(g)$, the LIME cannot adhere to the definition of the additive-feature-attribution methods. However, the local explanation of the LIME may satisfy the properties of the methods, equation (1), with the correct definition of the loss function, $L$, local kernel $x$ and regularization term, $\u03a9(g)$. For the kernel-SHAP framework, these functions are defined as follows:\n$\u03a9(g) = 0,$ (8)\n$L(f, g, \u03c0_x) = \\sum_{z' \\in Z} [f(h(z')) \u2013 g(z')]^2 \u03c0_x(z'),$ (9)\n$\u03c0_x(z') = \\frac{()}{\\binom{N}{|z\u2032|}} (|z\u2032| (N- |z\u2032|)$ (10)\nwhich are the only possible solutions of equation (7) that satisfy the local accuracy, missingness, and consistency properties as demonstrated by Lundberg and Lee (2017). In the previous equations the parameter |z\u2032| is the total number of non-zero structures present in the subset z\u2032, and Z is the total set of features.\nThe definition of the model in equation (1) assumes a linear form, which differs from the loss function $L$ defined as squared loss. However, the loss function can still be solved using a linear regression. In other words, the Shapley values can be computed using a weighted linear regression (Charnes et al., 1988). In addition, kernel SHAP reduces the computational cost of the Shapley values as they are approximated by computing a set of random samples.\nNote that the simplified input z\u2032 accounts for the present and absent features in the calculation. However, for most of the models, such as neural networks, an input feature cannot be absent. Therefore, these absent features are set to a non-informative value or reference value. In other words, the mapping function h provides the input value x for those features that are present and the reference value $x_i$ for the absent ones."}, {"title": "2.2. Tree SHAP", "content": "The previously explained exact Shapley values require an exponential computational cost as they need to evaluate a total of $2^N$ predictions. However, the tree-SHAP algorithm (Lundberg et al., 2018) exploits the architecture of the tree models, which obtain the predictions by recursively partitioning the data space and fitting the model within each partition (Loh, 2011), and keeps track of the proportion of all the possible subsets that flow down into each one of the leaves of the trees. By applying this methodology, the computation cost of the algorithm is reduced from exponential to polynomial. With the previous modification, the results are equivalent to evaluating all the possible subsets, $2^N$, in equation (6). Although keeping a record of how many subsets pass through each branch of the tree appears to be simple, it results in subsets of different sizes that are not properly weighted (the weights depend on the number of features present in coalition S, which we denote as |S|). The tree-SHAP algorithm corrects the previous issue by extending the subset adding the proportion of ones and zeros (EXTEND method), and reversing it (UNWIND method) (Lundberg et al., 2018). The EXTEND method is applied as the information advances in the tree. The process is reversed through the UNWIND method when the information is split on the same feature twice and it is also applied to undo the extension in a leaf.\nThe tree-SHAP algorithm can also account for interaction effects. The pairwise interactions are evaluated leading to a matrix of attribution values, where the Shapley interaction index is evaluated in Equation (11).\n$\u0424\u0456,\u0458 = \\sum_{S \\subseteq F \\{i,j}} \\frac{|S|! (N- |S| \u2013 2)!}{2 (N \u2212 1)!} - Vi,j (S).$ (11)\nHere, $bij$ is the interaction SHAP value between features $i$ and $j$, $S$ is a subset of the total possible subsets $F$ that do not contain features $i$ i and $j$. The value of the term $V Vi,j (S)$ is provided in equation (12).\n$Vi,j = f (S\u222a \\{i, j \\}) \u2212 f (S \u222a i) \u2212 f (S\u222a j)+ f (S).$ (12)\nThe interaction values split symmetrically between both features, with $\u0424i,j = \u0424j,i$ and the total interaction effect is $\u03a6i,j + \u03a6j,i$. Then, the interactions of the importance values captured by the tree and reflected in the SHAP scores can be uncovered by calculating the main effects of a feature for a prediction, as the difference between the SHAP value and its interactions:\n$\u0424\u0456,\u0456 = \u0424\u0456 \u2013 \\sum_{j\u2260i}\u0424i.$ (13)"}, {"title": "2.3. Gradient SHAP", "content": "For large artificial neural networks with a high number of parameters, the calculation of the kernel SHAP is, computationally speaking, consuming and the tree SHAP cannot be evaluated. For these reasons, the knowledge of the architecture of the model can be exploited to reduce the computational cost of the additive-feature-attribution method. A natural solution is the use of the gradients of the model, i.e. the gradient-SHAP method. According to Sundararajan et al. (2017), machine-learning practitioners regularly inspect the products of the model coefficients with the feature values to debug the predictions of the model. Computing the gradients of the output with respect to the input is an analog of the previous methodology (Baehrens et al., 2010; Simonyan et al., 2013). Nevertheless, the gradients break the sensitivity axiom that all the attribution methods should satisfy. This axiom is implicitly included in the previous three axioms and states that if an input and a baseline differ only in one of the features and provide different outputs, then the attribution of the feature must be non-zero. In fact, Sundararajan et al. (2017) provided an example to illustrate the previous idea. Let us consider a model defined such as:\n$f (x) = 1 - ReLU (1 \u2013 x),$ (14)\nwhere the baseline is 0, the input 2 and ReLU denotes the rectified-linear-unit activation function. Then the function changes from 0 to 1, but f becomes flat at x = 1, thus, the attribution for x is 0.\nIn order to overcome the previous limitation, Sundararajan et al. (2017) proposed the method known as integrated gradients (IG). In this methodology, a straight line path is considered from the reference or baseline $x\u2081$ to the input $x$. Then, the gradients along the path are computed, avoiding the problems presented in the previous example. The integrated gradients are computed by accumulating the previous gradients along the path. The integrated gradients are defined in Equation (17).\n$IG_i = (x_i - x_{ri}) \\int_{\u03b1=0}^{1} \\frac{\\partial f}{\\partial x_i} (x_r + \u03b1(x-x_r ) dx$ (15)\nThe integrated gradients satisfy the completeness axiom, which states that the attributions add the difference between the output of the input and the baseline:\n$\\sum_{i=1}^{n} IG_i = f (x) \u2212 f (x).$ (16)\nFullfilling the completeness attribution implies that the method also satisfies the sensitivity axiom. Therefore, the IG method solves the problem of analyzing the gradients of the output.\nFinally, the integral is computed through a summation with sufficiently small intervals from the baseline to the input, as follows:\n$IG_i \u2248 (x_i - x_{ri}) \\sum_{k=0}^{m} \\frac{\\partial f}{\\partial x_i} (x+ \u03b1_k(x-x_r )) \\frac{1}{m},$ (17)\nwhere m is the number of steps of the Riemman approximation of the integral. As reported by Sundararajan et al. (2017), a range from 20 to 300 steps is typically enough to approximate the integrals.\nThe use of integrated gradients implies a reduction of the computational speed of training with back-propagating gradients. Therefore, Erion et al. (2021) proposed a new approach to the previous methodology, formulating the integral as an expectation. In addition, this formulation also allows to calculate the importance of each input attribute relative to a batch of baseline inputs, D. The expected-gradients methodology accommodates this batch by performing a Monte-Carlo integral, referring E to the expectation, with samples from the multiple references and interpolation points, a:"}, {"title": "2.4. Deep SHAP", "content": "In the case of the gradient SHAP, the expectation of the gradients is calculated by selecting a random set of possible intermediate states of the straight line path between the reference and the input. Deep SHAP approximates the previous calculation by replacing the gradient at each intermediate state with its average values in a single step (Ancona et al., 2017). The SHAP values are calculated assuming that the features are independent and the model is linear. Deep SHAP is equivalent to linearizing the non-linear components of a neural network (Lundberg and Lee, 2017). This effect is obtained by combining the DeepLIFT (learning important features) (Shrikumar et al., 2016) algorithm with the Shapley values.\nThe DeepLIFT algorithm attributes a value Cox\u2081,\u2206o to each input x\u2081. This attribution represents the value of an input being set to the original value instead of a reference:\n$\\sum_{i=1}^{n}\u0395\u2206x_{i, o} = \u2206o,$ (19)\nwhere n is the number of inputs, o the output, Ao = f (x) - f (x,), and Ax\u2081 = x \u2212 x,. The DeepLIFT method is a modification of the additive-feature-attribution method defined in equation (1), where C\u2206x\u2081,\u22060 replaces b\u2081 and f (x,) replaces 40. The DeepLIFT algorithm satisfies the local accuracy and the missingness properties. However, it needs to be combined with the Shapley values to satisfy the consistency property.\nThe deep-SHAP method calculates the SHAP values by combining the corresponding SHAP values of the smaller components. The algorithm recursively passes the DeepLIFT multipliers to calculate the SHAP values backwards to the network. The SHAP values through every layer are calculated linearly as follows:\n$\u0424_i (fj, x) \u2248 mx_{i,f_j} (x_i \u2212 E [xi]) .$ (20)\nIn order to clarify the previous methodology, Lundberg and Lee (2017) proposed a small example represented in Figure 1.\nThe SHAP values are calculated recursively according to in the following expressions, where the input features are represented by y, the hidden features by $x_j$ and the simple components of the network by $f_k$:\n$mx_{j,f_3} = \\frac{\u0424i (f_3, x)}{x_j - E[x]}.$ (21)\n$My_{i,f_j} = \\frac{\u0424i (f_j, y)}{y_i - E [y]}.$ (22)\nand then, the chain rule is applied:\n$my_{i,f_3} = \\sum_{j=1}^{2}mys_fMx_{j,f_3}$ (23)\nFinally, the SHAP values are calculated using the linear approximation:\n$\u0424_i (\u0437, \u0443) \u2248 \u0442\u0443i,\u0437 (\u0443i \u2013 E [yi]) .$ (24)\nThe deep-SHAP method allows a fast approximation of the whole model. The SHAP values of the simple network components, $f_i$, can be calculated analytically if the components are linear, max pooling, or activation functions depending only on one input."}, {"title": "3. Additive-feature-attribution methods applied to fluid mechanics", "content": "Since the decade of 1990, machine learning (ML) has been applied to modeling fluid dynamics (Faller and Schreck, 1997; Karunanithi et al., 1994). However, for most of the ML models, their outputs are employed without focusing on the relationships between the inputs and the outputs, which remain unclear. This black-box behavior is one of the main drawbacks of machine learning (Ribeiro et al., 2016; Vinuesa and Sirmacek, 2021), and thus, adding an extra explainable layer through additive-feature-attribution methods improves the interpretation of the model, showing the impact of the input features.\nSimilarly, as occurred for the application of machine learning in fluid mechanics, the explainability of the machine-learning models in the context of fluid dynamics revolves around three main goals. The first goal is using the additive-feature-attribution methods for improving turbulence modeling, mainly Reynolds-averaged Navier-Stokes (Reynolds, 1895; Launder and Spalding, 1974) (RANS) closure equation (Scillitoe et al., 2021; Mandler and Weigand, 2023; Bounds et al., 2024). The second is understanding fundamental aspects of fluid mechanics, and more concretely of wall-bounded turbulence (Cremades et al., 2024a). Finally, the last category includes particular applications of fluid mechanics such as the evaluation of turbulence in the gliding slope of an aircraft (Khattak et al., 2023), the determination of the pressure coefficient on a building surface (Meddage et al., 2022) or the identification of two-phase-flow regimes (Khan et al., 2023).\nAdditive-feature-attribution methods to improve turbulence modeling\nRANS models are widely used in industry due to their computational efficiency (Menter et al., 2011). They rely on the Reynolds decomposition and on keeping fluctuations of the velocity in the problem by using the Reynolds stresses. To solve these Reynolds stresses, the eddy viscosity is introduced in the equations to link them with the mean velocity (Boussinesq, 1903).\nHowever, although the traditional RANS models are widely used, they exhibit reduced accuracy in complex flows due to their simplicity (Pope, 2000; Slotnick et al., 2014). Therefore, due to the capacity of machine learning models to adapt to complex flow conditions such as adverse pressure gradients, complicated geometries, and separated flows (Vinuesa and Brunton, 2022), many authors have proposed different data-driven methods for turbulence modeling (Duraisamy et al., 2019). However, most of these works do not establish a link between the closure terms predicted by the models and the turbulent input features. To improve the explainability of the models, a recent analysis adds an explainability stage to the research, linking the results of the machine learning model to the physics of the flow (McConkey et al., 2022; Sudharsun and Warrior, 2023).\nOn the one hand, the previous works share some similarities to analyze the effect of the turbulent features of the flow to improve turbulent models such as Spalart-Allmaras (Spalart and Allmaras, 1992) and shear-stress-transport (SST) models (Menter, 1994). Firstly, for the inputs, they use the physical properties of low-fidelity flows. For instance, the work of He et al. (2022) employs magnitudes related to the pressure gradient, strain-rotation features, misalignment of the velocity, turbulence legth scales and viscosity ratio between turbulent and laminar values. Then, their mapping between the input features and the corrections of the turbulent models uses a machine-learning model, varying from neural networks (Rosenblatt, 1958) to random forests (Breiman, 2001).\nFinally, the SHAP values are evaluated, scoring the importance of each physical input parameter to the estimated corrections (Bhushan et al., 2023; Wu et al., 2023a). These importance scores are exemplified in Figure 2, which shows the scores of the viscosity ratio, the strain-rotation ratio, the pressures gradient-shear ratio, the streamwise pressure gradient normalized with the local kinetic energy and Reynolds number per unit length and the misalignment of the velocity vector concerning the streamlines (He et al., 2022). The previously mentioned figure shows the viscosity ratio to be the most influential flow magnitude followed by the strain rotation ratio. In addition, the figure links the importance score of each grid point (points in the right figure) to the value of the input variable, showing how the SHAP framework reduces nonlinear and complex relationships to a parameter"}, {"title": "Additive-feature-attribution methods to study fundamentals", "content": "Fluid dynamics are governed by the Navier-Stokes equations, a set of partial differential equations the analytical solution of which has not been found. Due to this limitation and the multi-scale nature of the turbulent phenomena (Kolmogorov, 1941; Cardesa et al., 2017), the computational requirements of integrating numerically these equations are very high (Pirozzoli et al., 2004; Iwamoto et al., 2004; Lee and Moser, 2015; Hoyas et al., 2022). For these reasons, the capabilities of the additive-feature-attribution methods for detecting the relationships between the input features and the output of a machine-learning model can be used for improving the understanding and explainability of the physics underlying the equations of fluid dynamics, providing new answers and horizons for turbulence analysis. An example is the work of Lellep et al. (2022) where a nine-mode model is used to simulate a Couette flow with free-slip boundaries (Moehlis et al., 2004). Note that this model has been extensively analyzed in the context of ML research (Srinivasan et al., 2019). The influence of every single mode in the relaminarization of the flow is evaluated by using an XGBoost tree model (Chen and Guestrin, 2016) analyzed with the previously explained SHAP methodology. \nFollowing the same idea, in the work of Cremades et al. (2024a) the influence of the intense Reynolds stress structures or Q events (Lozano-Dur\u00e1n et al., 2012; Jim\u00e9nez, 2018) in a turbulent channel was evaluated. First, the evolution"}, {"title": "Additive-feature-attribution methods for applied problems in fluid dynamics and heat transfer", "content": "Explainable machine learning", "next": "impact of the atmosphere in human activities, optimization of heat transfer, evaluation of risks and safety improvement, emission reduction and noise mitigation, optimization of rotating machines, and explanation of complex flows.\nImpact of the atmosphere in human activities\nRegarding the analysis of the atmosphere and its impact on human activities, additive-feature-attribution methods have been applied to different analyses. A particular example of the capabilities of explainable artificial intelligence to understand how atmospheric phenomena impact human activities is related to aviation. The work of Khattak et al. (2024b) focuses on which factors determine the presence of low-level turbulence or those chaotic shifts in headwinds along the airport runway glide path. According to the authors, this phenomenon represents one of the imminent hazards to aviation safety. Low-level turbulence is mainly reported in those airports that are located in regions with rugged mountains. For instance, this phenomenon strongly affects Hong Kong International Airport, being present in one of every 2000 flights. Low-level turbulence can alter the aerodynamic forces that an aircraft needs during landing, leading to safety issues. In the work of Khan et al. (2023) the turbulence intensity of the atmosphere in the glide slope of the runway is predicted as a function of a set of input parameters. The terrain effects, the wind direction, the runway orientation, and the distance from the runway are employed for that purpose, using a random-forest-based method to calculate the predictions. The additive-feature-attribution method, or SHAP interpretation, is applied to the previous parameters. The terrain was determined to be the most important feature affecting the turbulence intensity, being more influential than the distance from the runway and the wind direction.\nIn addition to the effects that the wind has on aviation, other industries are affected by the turbulence and instabilities of the atmosphere. Wind loads affect many industries, being civil engineering a clear example of how aerodynamic forces condition the design of structures. The knowledge of the aerodynamic coefficients of the wind loads over the building surfaces is required to size the structure and to estimate its natural ventilation rate (Charisi et al., 2021). Buildings are bluff bodies, therefore, the calculation requires computational fluid dynamics or experimental measurement of the aerodynamic coefficients leading to expensive, time-consuming tests. For the latter specific facilities are also required, a fact"}]}