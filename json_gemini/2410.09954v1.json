{"title": "EITNet: An IoT-Enhanced Framework for Real-Time Basketball Action Recognition", "authors": ["Jingyu Liu", "Xinyu Liu", "Mingzhe Qu", "Tianyi Lyu"], "abstract": "Integrating IoT technology into basketball action recognition enhances sports analytics, providing\ncrucial insights into player performance and game strategy. However, existing methods often fall\nshort in terms of accuracy and efficiency, particularly in complex, real-time environments where player\nmovements are frequently occluded or involve intricate interactions. To overcome these challenges, we\npropose the EITNet model, a deep learning framework that combines EfficientDet for object detection,\nI3D for spatiotemporal feature extraction, and TimeSformer for temporal analysis, all integrated\nwith IoT technology for seamless real-time data collection and processing. Our contributions include\ndeveloping a robust architecture that improves recognition accuracy to 92%, surpassing the baseline\nEfficientDet model's 87%, and reducing loss to below 5.0 compared to EfficientDet's 9.0 over\n50 epochs. Furthermore, the integration of IoT technology enhances real-time data processing,\nproviding adaptive insights into player performance and strategy. The paper details the design and\nimplementation of EITNet, experimental validation, and a comprehensive evaluation against existing\nmodels. The results demonstrate EITNet's potential to significantly advance automated sports analysis\nand optimize data utilization for player performance and strategy improvement.", "sections": [{"title": "1. Introduction", "content": "In modern basketball, precise athlete movement evaluation and training improvement are crucial for enhancing\nathletic performance and reducing injuries Xu and Tang\n(2021); Chen, He, Wang, Bai, Cheng and Ning (2024b).\nRecently, with the development of Internet of Things (IoT)\ntechnology and multiview video analysis, the integration of\nmultiview video for basketball player movement evaluation\nand training improvement (which needs to be combined with\nIoT) has garnered significant attention Li, Wang and Yao\n(2021); Tokolyi and Elshakankiri (2021); Rahmani, Tanveer,\nGharehchopogh, Rajabi and Hosseinzadeh (2024); Weng,\nWu et al. (2024b); Shen, Zhang, Zheng and Qi (2024).This\nmethod not only provides more comprehensive and detailed\nmovement data but also enables real-time monitoring and\nanalysis, offering more scientific and effective training guid-\nance for coaches and athletes. Basketball is a highly dynamic\nand complex team sport, where the precision, speed, and\ncoordination of player movements directly affect the game\nresults. Every action in training and matches, including\nrunning, jumping, passing, and shooting, requires precise\ncontrol and coordination. However, traditional movement\nevaluation methods primarily rely on the coach's experience\nand limited single-view video analysis, which has several\nlimitations Yuan, Kamruzzaman and Shan (2021); Zhang,\nQi, Zheng and Shen (2024b); Wang, Hu and Zhou (2024c);\nHuang, Der Leu, Lu and Zhou (2024). Although valuable,\nthe coach's experience is highly subjective and prone to\npersonal bias, and single-view video analysis, due to its\nlimited perspective and coverage, cannot fully capture the\ndetailed movements of athletes Russell, McLean, Impellizzeri, Strack and Coutts (2021); Ahmed, Hossain, Kaiser,Noor, Mahmud and Chakraborty (2021); Yan, Wu, Kumarand Zhou (2024); Zhou, Zhao, Luo, Xie, Wen, Ding and Xu(2024a); Sui, Jiang, Lyu, Wang, Zhou, Chen and Alhosain(2024). The introduction of IoT technology has broughtrevolutionary changes to basketball player movement eval-\nuation. By deploying multiple sensors and cameras aroundthe court, IoT enables comprehensive, multi-angle capture ofathlete movements Moghaddasi, Rajabi and Gharehchopogh(2024); Gharehchopogh, Abdollahzadeh, Barshandeh andArasteh (2023); Weng and Wu (2024c); Wang, Wang andLiu (2024b); Zheng, Zhang, Gong, Liu and Chen (2024).\nThese sensors can record real-time data on speed, position,and posture, while cameras provide high-definition videorecordings, generating detailed movement data Song andTuo (2022); Yan, Jiang and Liu (2023); Jin, Che, Peng, Liand Pavone (2024); Cao, Weng, Li and Yang; Weng and Wu(2024a). This data includes not only the position informationof athletes at different time points but also their movementtrajectories and posture changes, providing a more compre-hensive and accurate basis for movement evaluation.\nUsing multiview video analysis technology, athlete move-ments can be comprehensively recorded and analyzed fromdifferent angles and heights. For example, when a playerperforms a shooting action, multiple cameras can simultane-ously capture the entire process from the front, back, sides,and top. This way, the video data from different perspectivescan complement each other, avoiding the limitations ofsingle-view video in terms of perspective and coverage. Thismultiview recording method not only captures the overalltrajectory of movements but also allows for detailed analysisof the athlete's posture and movement nuances at every"}, {"title": "2. Related Work", "content": "Significant progress has been made in the application\nof single-view and multi-camera systems for the motion\nanalysis of basketball players, but each faces its own chal-\nlenges. Single-view video systems typically rely on a fixed\ncamera angle for motion capture and analysis Slowik, Mc-\nCutcheon, Lerch and Fleisig (2023); Asgharzadeh, Ghaf-\nfari, Masdari and Gharehchopogh (2023); Weng and Wu\n(2024b,c); Chen, Li, Song and Guo (2024d). While they\noffer advantages in simplifying data processing and reduc-\ning hardware costs, they exhibit clear limitations in cap-\nturing complex movements and multi-dimensional motion\ncharacteristics. On the other hand, multi-camera systems\ncapture the athlete's movements synchronously from mul-\ntiple angles, providing more comprehensive and accurate\nmotion data Olagoke, Ibrahim and Teoh (2020); Nogueira,\nOliveira and Teixeira (2024); Zhang (2024); Chen, Zhang,\nDong, Zhou and Wang (2024a); Dong (2024). However, this\napproach also brings higher computational complexity and\nhardware requirements, particularly in real-time application\nscenarios.\nSeveral studies have explored single-view and multi-camera systems for basketball player motion analysis. Dinget al. Ding, Takeda and Fujii (2022) proposed a single-view basketball player motion analysis model using Convolutional Neural Networks (CNN) and Long Short-TermMemory (LSTM) networks. This model effectively extractedfeatures from video frames and classified actions, achievingnotable success in action classification. However, its relianceon single-view video data limited its accuracy and comprehensiveness in evaluating complex player movements.Expanding on the single-view approach, Zhang et al. Zhang,Wu, Yang, Wu, Chen and Xu (2020) developed a multi-camera system for 3D basketball player motion captureand analysis. This system utilized multiple cameras to synchronously capture player movements from different angles.By employing multiview stereo matching algorithms, thesystem generated 3D pose data, which was subsequentlyanalyzed using Deep Neural Networks (DNNs) for actionrecognition. While the system excelled in capturing 3Dmovements, its high computational complexity and significant hardware resource requirements limited its real-timeapplicability and practicality. Wang et al. Wang and Chu(2024) further advanced this area by focusing on 3D poseestimation for basketball players using multi-view cameras.Their approach improved the accuracy of pose estimationbut faced challenges in handling high-frequency movementsand maintaining real-time performance. Despite these ad-vancements, multi-camera systems still grapple with issuesrelated to computational demands and the integration of datafrom multiple sources. Zhang et al. Zhang and Yang (2021)investigated deep learning-based approaches for multiviewvideo analysis in sports, proposing models that integratedata from various camera angles to enhance action recog-nition accuracy. Their work highlighted the potential ofdeep learning in sports analytics but also underscored thenecessity of addressing computational efficiency and real-time processing capabilities.\nIn summary, single-view systems have advantages interms of hardware cost and data processing complexity, butthey are limited in capturing complex and multi-dimensionalmovements. Multi-camera systems, while providing morecomprehensive motion data, face major obstacles in computational complexity and hardware requirements. Thesestudies provide an important technological foundation forthe motion analysis of basketball players, but further opti-mization is needed in terms of real-time performance andcomputational efficiency."}, {"title": "2.2. Wearable Sensors and IoT Integration", "content": "With the development of IoT technology, hybrid mod-els that combine wearable sensors with video data havebecome increasingly common in the evaluation of basket-ball player movements Rana and Mittal (2020); Guo, Gao,Liu, Chakraborty, Hua, Yu and Wan (2022); Wan, Zhang,Jiang, Wang and Zhou (2024); Wang, Jiang, Wang and Zhou(2024d); Zhou, Wang, Zheng, Zhou, Dai, Luo, Zhang andSui (2024b). These systems utilize IoT sensors to collectvarious data from athletes in real-time, such as accelerationand angular velocity, while simultaneously using cameras tocapture video data. The application of multimodal fusiontechnology enables the comprehensive analysis of thesedata, providing more accurate and thorough motion evalua-tions Zhao and Liu (2021); Essa and Abdelmaksoud (2023);Li, Chen, Yu, Dajun, Qiu, Jieting, Baiwei, Shengyuan, Wan,Ji et al. (2024a); Li, Wang, Wu, Peng, Chang, Deng, Kang,Yang, Ni and Hong (2024b); Qiao, Li, Lin, Wei, Jiang,Luo and Yang (2024). However, the complexity of wearing\nsensors and the challenges of data synchronization remainmajor obstacles for these systems.\nThe integration of wearable sensors and IoT technologyhas opened new avenues for real-time basketball playerperformance analysis. Li et al. Li, Luo and Islam (2024c)introduced a hybrid model combining wearable sensors andvideo data to enhance motion recognition accuracy. IoTsensors collected data on acceleration and angular veloc-ity, while cameras captured video footage. Multimodal fu-sion techniques were used to analyze these diverse datasources comprehensively Zhou, Wang, Wang, Sun, Chenand Zhang (2023). Despite improving recognition accuracy,the complexity of sensor wear and data synchronizationissues limited the model's usability and broader adoption.Isaac et al. Isaac and Janani (2022) explored the potential ofintegrating IoT and machine learning for real-time basketballplayer performance analysis. Their approach leveraged IoTsensors to continuously monitor player movements and used"}, {"title": "2.3. Deep Learning and Real-time Evaluation Systems", "content": "The latest advancements in deep learning have greatly\npromoted the development of real-time evaluation systemsfor basketball players' movements. These systems use deeplearning algorithms to process and analyze multi-view videodata, enabling real-time assessment of athletes' actions. Al-though these systems have significant advantages in real-time processing and accuracy, they encounter performancebottlenecks when handling long-duration video data, affect-ing their ability to cope with high-frequency and complexmovements Zhang, Ning, Wang, Ning and Li (2024a); Xi,Zhang, Jia and Jiang (2024).\nRecent advancements in deep learning have significantlycontributed to the development of real-time evaluation sys-tems for basketball player movements. Yao et al. Yao, Gaoand Su (2020) designed a real-time motion evaluation systembased on deep learning, integrating CNNs Gong, Zhang,Zheng, Liu and Chen (2024) and Recurrent Neural Networks(RNNs). This system utilized multiview video data, extract-ing image features with CNNs and analyzing time-seriesdata with RNNs to provide real-time movement evaluation.Despite its advantages in real-time processing and accuracy,the system faced performance bottlenecks when handlinglong-duration video data, impacting its ability to managehigh-frequency and complex movements. Zuo et al. Zuo andSu (2022) proposed a pose estimation and action recognitionmodel using deep learning techniques. Their model focusedon accurately estimating player poses and recognizing ac-tions from video data. While their approach improved actionrecognition accuracy, it also encountered challenges relatedto computational efficiency and the ability to process large-scale video data in real-time. Khan et al. Khan, Javed, Khan,Saba, Habib, Khan and Abbasi (2024) and Tang et al. Tang,Liang and Zhu (2023) explored multiview action recognition for sports using recurrent neural networks. Their workdemonstrated the effectiveness of RNNs in capturing tem-poral dependencies in multiview video data, enhancing ac-tion recognition performance. However, the computationaldemands of processing multiview data in real-time posedsignificant challenges. Matos et al. Matos Flores (2023) ex-amined the application of multiview video and deep learningin sports analytics. Their study highlighted the potentialof combining multiview video data with advanced deeplearning algorithms to improve action recognition and playerevaluation. Despite the promising results, issues related todata synchronization, computational complexity, and real-time processing needed to be addressed to fully harness thepotential of these technologies.\nThe application of deep learning in the evaluation ofbasketball players' movements provides robust technicalsupport for real-time assessment. However, the computational complexity and real-time performance in handlinglarge-scale video data remain key issues to address. Byfurther optimizing algorithms and hardware architectures,the performance of these systems in practical applicationsis expected to improve significantly.\nThrough these studies, it is evident that while significantprogress has been made in the field of basketball playermovement evaluation, challenges related to data integration,computational efficiency, and real-time processing remain.Our research aims to address these challenges by leveragingthe integration of multiview video, IoT technology, andadvanced deep learning algorithms to achieve more efficientand accurate movement evaluation and training improve-ment."}, {"title": "3. method", "content": "Building upon the limitations identified in traditional\nbasketball movement evaluation methods, we propose a\nnovel model, EITNet (EfficientDet-I3D-TimeSformer Net-work), that integrates advanced deep learning techniques to\neffectively recognize and analyze basketball players' actions.\nThe model consists of three main components: multi-view\nvideo data collection, EfficientDet for object detection, I3D\nfor spatiotemporal feature extraction, and the TimeSformer\nencoder for temporal analysis and action classification. First,\nmultiple cameras are strategically installed around the bas-\nketball court to capture players' actions from various angles,\nproviding comprehensive coverage and addressing occlusion\nissues. The captured multi-view video data undergoes pre-\nprocessing, including median filtering to reduce noise and\nenhance image quality. The preprocessed video frames are\nthen fed into the EfficientDet model for real-time player"}, {"title": "3.2. EfficientDet", "content": "EfficientDet is an advanced object detection model that\ncombines EfficientNet and the BiFPN (Bi-Directional Fea-\nture Pyramid Network) architecture to achieve efficient and\naccurate object detection. The core principle of EfficientDet\nis to optimize the network architecture and reuse feature\nmaps to improve detection accuracy while reducing compu-\ntational load Jain (2024); Luo, Du, Zhang, Song, Li, Zhu,\nBirkin and Wen (2023); Peng, Ran, Luo, Zhao, Huang,\nThorat, Geng, Wang, Xu, Wen et al.; Chen, Li, Song andGuo (2024c). EfficientNet serves as the backbone network\nand optimizes the network's depth, width, and resolution\nthrough kernel reuse, allowing the entire network to maintain\nhigh performance with lower computational complexity.\nBiFPN enhances the feature pyramid network's capability by\nusing bidirectional feature fusion strategies, improving the\nprecision of feature extraction. The detailed architecture of\nEfficientDet is illustrated in Figure 3. In EITNet, the primary\nrole of EfficientDet is to perform real-time basketball player\ndetection. Specifically, EfficientDet detects and locates play-\ners from multi-view video data, generating bounding boxes\nto isolate regions of interest. This step is crucial for subse-\nquent feature extraction and action classification, as accurate\nobject detection significantly enhances the effectiveness of\nthese later stages. EfficientDet's efficient network struc-\nture reduces computational load and processing time while\nmaintaining high detection accuracy, enabling the system to\noperate in real-time environments.\nEfficientDet leverages several key mathematical com-ponents to achieve efficient and accurate object detection.The following formulas are integral to understanding howEfficientDet operates and how it contributes to our overallEITNet model.\n\nEfficientDetmodel = EfficientNetbackbone\u00d7BiFPN\u00d7Detection Heads\nwhere EfficientNetbackbone denotes the feature extraction net-work, BiFPN is the Bi-directional Feature Pyramid Networkfor feature fusion, and Detection Heads represent the outputlayers for object detection.\n\nBiFPN features = \\sum \u03b1; \u00b7 F\u2081\nwhere F; represents features from different scales, \u03b1; denotes\nthe attention weights for each feature scale, and n is the\nnumber of feature levels.\n\nAttentionBiFPN \\frac{F current}{F} = \\frac{F}{F}previous + \\epsilon\nwhere Fcurrent and F previous are the feature maps at the currentand previous levels, respectively, and e is a small constant toavoid division by zero.\n\nBoundingBoxprediction = \u03c3(FCreg(BiFPNfeatures)) \u00d7 Anchor\nwhere FCreg is the fully connected regression layer, o isthe sigmoid activation function, and Anchor represents thereference bounding box anchors.\n\nLosstotal = Losscls + \u03bb\u00b7 LosSreg\nwhere Losscls is the classification loss, Lossreg is the re-gression loss, and is a balancing factor to weight theimportance of classification versus regression errors.\nThese formulas illustrate the mathematical foundation ofEfficientDet and its integration within the EITNet model.EfficientDet's ability to efficiently handle multi-scale fea-tures and accurate bounding box predictions contributessignificantly to the effectiveness of our basketball actionrecognition system.\nIn basketball player action recognition and analysis, pre-cise and efficient object detection forms the foundation ofthe entire process. Traditional methods often face signifi-cant challenges when dealing with complex backgrounds,occluded actions, and inconsistent lighting conditions. Ef-ficientDet addresses these issues effectively through its op-timized architecture and feature fusion strategies, improvingdetection accuracy and robustness. By integrating multi-view video data, EfficientDet provides accurate input forthe subsequent I3D and TimeSformer models, ensuring thatthe entire EITNet system can comprehensively and preciselyevaluate and analyze basketball players' actions. Introduc-ing EfficientDet allows us to overcome the limitations oftraditional methods, achieving more efficient and accurateaction recognition, offering scientific insights and real-timefeedback for basketball player training and competition,thereby enhancing performance and reducing injury risk."}, {"title": "3.3. Inflated 3D Convolutional Network", "content": "The Inflated 3D Convolutional Network (I3D) model\nextends the traditional 2D convolutional networks by intro-ducing 3D convolutions, allowing it to capture both spatialand temporal features from video data effectively Huang,Guo and Gao (2020); Weng, Cao, Li and Yang (2024a);Weng (2024); Wang, Sui, Sun, Zhang and Zhou (2024a).\nThis approach involves inflating 2D convolutional kernelsto 3D, which enhances the network's capability to recog-nize complex spatiotemporal patterns. The I3D architectureconsists of inflated convolutions that process video clips,capturing motion dynamics across frames and improving\n\nF\u2081 = Conv3(F1\u20131) * K3D + B\nwhere F, represents the feature map at time t, Conv3 denotes\nthe 3D convolution operation, * is the convolution operation\nwith kernel K3D, and B is the bias term.\n\nF\u2081 = ReLU (Conv3(F1\u20131) * K3D + B)\nwhere ReLU is the rectified linear unit activation functionapplied element-wise to the result of the 3D convolutionoperation.\n\nP\u2081 = MaxPool3 (F\u2081, k, s)\nwhere MaxPool3 denotes the 3D max pooling operation withkernel size k and stride s, applied to the feature map F\u2081.\n\nF\u2081 = Dropout (BatchNorm (F\u2081), p)\nwhere BatchNorm represents the batch normalization ap-plied to the feature map F\u2081, and Dropout is applied withprobability p to mitigate overfitting.\n\nY = Softmax (W\u00b7 GlobalAvgPool (Fr) +bc)\nwhere Y is the output class probabilities, We and be arethe weight matrix and bias term for the final classification\nlayer, GlobalAvgPool represents the global average poolingoperation applied to the final feature map FT.\nThese formulas provide a deeper look into the mathemat-ical operations of the I3D model, including 3D convolutions,activation functions, pooling, normalization, and classifica-tion layers.\nThe contribution of I3D to EITNet is significant, as itenhances the model's ability to understand and classify com-plex player movements in dynamic basketball environments.This is essential for accurate action recognition and analysis,which is fundamental to improving player performance andreducing injury risks."}, {"title": "3.4. TimeSformer", "content": "TimeSformer is a video understanding model based onthe Transformer architecture, specifically designed for ana-lyzing temporal data. It divides video frames into fixed-sizepatches and flattens these patches into one-dimensional vec-tors, similar to the Vision Transformer (ViT) used in imageclassification. These vectors are then input into the Trans-former encoder, which uses self-attention mechanisms tocapture spatiotemporal dependencies between video frames Yun,Kim, Han, Song, Ha and Shin (2022); Xu, Deng, Dongand Shimada (2022); Peng, Xu, Feng, Zhao, Tan, Zhou,Zhang, Gong and Zheng (2024). By analyzing video dataframe by frame, TimeSformer achieves efficient and accurateextraction of temporal information and action recognition.Figure 5 shows the structure of the TimeSformer model.In EITNet, the primary role of TimeSformer is to processthe spatiotemporal features extracted by the I3D modeland perform action classification. Specifically, TimeSformerreceives the feature vectors extracted by I3D and uses self-attention mechanisms to analyze the temporal relationship"}]}