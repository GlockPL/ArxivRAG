{"title": "Distillation Scaling Laws", "authors": ["Dan Busbridge", "Amitis Shidani", "Floris Weers", "Jason Ramapuram", "Etai Littwin", "Russ Webb"], "abstract": "We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.", "sections": [{"title": "1. Introduction", "content": "The study of scaling laws (Hestness et al., 2017; Rosenfeld et al., 2020; Kaplan et al., 2020; Hoffmann et al., 2022) revealed that previously trained Language Models (LMs) could have been more capable if they had followed a compute optimal training paradigm, which determines the model size and the number of training tokens that give the best performing model under a given compute budget. Many subsequent works have followed compute optimal training (Dey et al., 2023; Muennighoff et al., 2023b).\n\nThe size of compute optimal models grows with compute (Hoffmann et al., 2022), which makes them challenging to use due to the growth in inference costs. In practice, this means compute optimal models are slow, expensive to serve, consume more battery life, provide high barriers to entry for academic study, and have a significant carbon footprint. With inference volume up to billions of tokens per day (OpenAI & Pilipiszyn, 2021), the inference cost of an LM is typically significantly larger than its pretraining cost (Chien et al., 2023; Wu et al., 2024a) and is going to further increase in an era of test-time compute scaling (Snell et al., 2024; Brown et al., 2024; Wu et al., 2024b).\n\nUnsustainable inference costs have led to an alternative training paradigm, overtraining (Gadre et al., 2024), where the amount of training data used is much greater than in the compute optimal case, enabling small, capable models. Overtrained models better satisfy compute optimality when compute is measured over a model's lifetime, rather than just the pretraining cost (Sardana et al., 2024). As supervised scaling laws follow power laws in model size and training data, diminishing returns in performance occur much sooner than in the compute-optimal case. To achieve reasonable capabilities, these models need to be trained on many trillions of tokens, (Snell et al., 2024; Brown et al., 2024; Wu et al., 2024b), which is expensive and time-consuming.\n\nWe seek models that match the performance of small overtrained models but at lower training cost. A popular candidate is distillation (Hinton et al., 2015), where a capable teacher LM produces targets for a smaller student LM. When distillation is used for LM pretraining, we will call this distillation pretraining. There are many explanations for why distillation works, from dark knowledge transfer, where information is contained in the ratio of probabilities of incorrect classes (Hinton et al., 2015), to being a form of regularization (Mobahi et al., 2020), or reducing noise in the learning process (Menon et al., 2020), among many other explanations. Despite a lack of consensus for why distillation works, distillation pretraining has produced more capable models than supervised pretraining in the Gemma and Gemini (Rivi\u00e8re et al., 2024), Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024) and AFM (Gunter et al., 2024) families of LMs in terms of both pretraining loss and downstream evaluations. Yet, at the same time, Liu et al. (2024) reported that distillation produces less capable models than supervised pretraining does.\n\nWith such significant compute resources being devoted to distillation pretraining of LMs, it is essential to understand how to correctly allocate these resources, to produce the most capable models possible, and to have an understanding if any gains are even possible compared to supervised pretraining when both methods have access to the same resources (Dehghani et al., 2021).\n\nTo close this knowledge gap, we perform an extensive controlled study of distillation, with students and teachers ranging from 143M to 12.6B parameters, trained on data of a few billion tokens, up to 512B tokens. These experiments result in our distillation scaling law, which estimates student performance as a function of resources (the teacher, the student size, and the amount of data used for distillation), resolving questions about when distillation is and is not effective in terms of producing models of a desired capability under resource constraints of interest. We find:\n\n1.  The cross entropy of a student of size $N_s$ distilled on $D_s$ tokens from a teacher of size $N_T$ trained on $D_T$ tokens can be predicted using our distillation scaling law (Equation 8).\n2.  The teacher size $N_T$ and number of teacher training tokens $D_T$ determines the student cross-entropy only through their determination of the teacher's cross-entropy $L_T = L_T(N_T, D_T)$ (Figure 3b).\n3.  The influence of the teacher cross-entropy upon the student loss follows a power law which transitions between two behaviors depending on the relative learning capacities of student and the teacher, reflecting a phenomenon in distillation called the capacity gap, where a stronger teacher produces a worse student. Our parameterization resolves outstanding questions about the capacity gap, showing that it is a gap in learning capacity (both hypothesis space and ability to optimize) between the teacher and student, and not only about their relative sizes, which is a special case.\n\nOur results show that distillation can not produce lower model cross-entropies than supervised learning when both learning processes are given enough data or compute. However, distillation is more efficient than supervised learning if both of the following are true:\n\n1.  The total compute or tokens used for the student is not larger than student size-dependent threshold given by our scaling law (Section 5.1).\n2.  A teacher already exists, or the teacher to be trained has uses beyond a single distillation (Section 5.3).\n\nWe hope the laws and analyses we provide will guide the community to produce even more capable models with lower inference cost and lower lifetime compute costs."}, {"title": "2. Background", "content": "Predicting model performance is essential when scaling as it lets us understand i) the value of increasing the available compute (C), and ii) how that compute should be distributed, typically between model parameters (N) and data (D), in order to achieve a model with desired properties. These properties may be predicting the data distribution sufficiently well, measured in cross-entropy (L), or achieving a level of performance on downstream tasks of interest.\n\nFortunately, cross-entropy is predictable, with substantial empirical and theoretical evidence that L follows a power-law in parameters N and data D (measured in tokens)\n\n$L(N, D) = E + \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta}$,\n\nwhere $\\{E, A, B, \\alpha, \\beta, \\gamma\\}$ are task-specific positive coefficients estimated from n training runs $\\{(N_i, D_i, L_i)\\}_{i=1}^n$. The choice of runs is critical; not all experiments enable identifying the coefficients of Equation 1. One could"}, {"title": "3. Preliminaries", "content": "Notation For a sequence x, $x^{(i:j)} = (x^{(i)}, x^{(i+1)}, ..., x^{(j)})$ returns a slice of the sequence, and $x^{(<i)} = x^{(1:i-1)} = (x^{(1)}, ..., x^{(i-1)})$ is the context of $x^{(i)}$. We use the shorthand $X^* = \\bigcup_{n\\in N} X^n$ to denote the set of sequences with arbitrary length $n \\in N = \\{1, 2, . . .\\}$.\n\nLanguage modeling We focus on the LM setting where the training objective is to model the probability of sequences x of tokens $x_i$ drawn from a vocabulary $V = \\{1,2,..., V\\}$. Let f : $V^* \\times \\Theta \\rightarrow R^V$ be a next-token classifier parameterized by $\\theta \\in \\Theta$ whose outputs define a predictive categorical distribution over V given a context $x^{(<i)}$\n\n$p(x^{(i)} = a | x^{(<i)}; \\theta) = \\sigma_a(f(x^{(<i)}; \\theta)) = \\sigma_a(z^{(i)})$,\n\nwhere $\\sigma_a(z) = \\exp(z_a) / \\sum_b \\exp(z_b)$ is the softmax function. The next-token classifier outputs $z^{(i)} = f(x^{(<i)}; \\theta)$ are the logits. Autoregressive LMs produce sequence likelihoods through $p(x; \\theta) = \\prod_{i=1}^L p(x^{(i)} | x^{(<i)}; \\theta)$ and are trained to maximize this likelihood on observed data"}, {"title": "4. Distillation Scaling Laws", "content": "Here we outline the steps taken to arrive at our distillation scaling law. First we describe the experimental setting (Section 4.1) and the experiments needed to determine the scaling coefficients (Section 4.2). Given the empirical observations, we discuss the form our distillation scaling law takes (Section 4.3), find the coefficients, and verify the law under extrapolation (Section 4.4).\n\nAll models are based on Gunter et al. (2024) and use decoupled weight decay Loshchilov & Hutter (2019) for regularization, as well as a simplified version of \u00b5P (Yang & Hu, 2021; Yang & Littwin, 2023; Yang et al., 2022; Wortsman et al., 2023; Yang et al., 2023), following \u00b5P (simple) in (Wortsman et al., 2024). \u00b5P simplifies the scaling law experimental setup as it enables hyperparameter transfer of the learning rate across model sizes. We validate that \u00b5P functions as expected for distillation in Appendix G.3."}, {"title": "4.3. Distillation Scaling Law Functional Form", "content": "We need to determine the functional form of the distillation scaling law. First, we observe that contributions from teacher size $N_T$ and pretraining tokens $D_T$ are summarized by the teacher cross-entropy $L_T$. This can be seen from Figures 1 and 3b which contains the IsoFLOP Teacher/Fixed M Students of Figure 3, yet only smooth dependence as a function of $L_T$ is observed. Next, the distillation scaling law should reflect the following properties:\n\n1.  An infinitely capable student should be able to model any teacher: $ \\lim_{N_s,D_s \\to \\infty} L_s (N_s, D_s, L_T) \\to L_T $.\n2.  A random teacher produces random students independent of how capable those students are: $ \\lim_{L_T \\to \\infty} L_S (N_s, D_s, L_T) \\to L_v $.\n3.  There is a capacity gap: making a teacher too capable eventually reduces the student performance.\n\nA transition between two power law regions: i) where the student is a stronger learner than the teacher, and ii) where the student is a weaker learner than the teacher is described by a broken power law (Caballero et al., 2023). Together, we propose that student cross-entropy follows a broken power law in $L_T$ and a power law in $N_s$ and $D_s$:\n\n$L_s(N_s, D_s, L_T) = L_T (1 + (\\frac{L_T}{L_{sd1}})^{1/f1-C1 f1} )^{C_0} + \\frac{A'}{N_s^{\\alpha'}} + \\frac{B'}{D_s^{\\beta'}}$"}, {"title": "5. Distillation scaling law applications", "content": "Here, we apply our distillation scaling law (Equation 8) and investigate scenarios of interest. Typically, the resources in distillation pretraining include a compute budget, or a dataset containing a number of tokens. For a distillation process, the compute cost can be approximated by\n\n$FLOPs \\approx 3 F(N_s) D_s + \\delta_{T-Logits} F(N_T) D_s + \\delta_{Pre} 3F(N_T) D_T$"}, {"title": "6. Conclusion", "content": "We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. We then used our law to study practical distillation scenarios of interest, and showed that distillation is only more efficient than supervised learning if: i) the total compute or tokens used for distillation is not larger than a student size-dependent threshold, and ii) a teacher already exists, or the teacher to be trained has uses beyond single distillation. Moreover, we use this law to determine optimal distillation scenarios that are able to outperform supervised learning, enabling practitioners to select the best teacher for their use case. This work represents the largest controlled empirical study of distillation we are aware of, with systematic ablations of common distillation techniques. Just as supervised scaling has mitigated risks in supervised pretraining, our findings offer a roadmap for producing smaller, more powerful models with lower inference costs, reducing carbon footprints, and enhancing the feasibility of test-time scaling."}]}