{"title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge", "authors": ["Swarnadeep Saha", "Xian Li", "Marjan Ghazvininejad", "Jason Weston", "Tianlu Wang"], "abstract": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to capture the step-by-step reasoning process that underlies the final evaluation of a response. However, due to the lack of human-annotated CoTs for evaluation, the required components and structure of effective reasoning traces remain understudied. Consequently, previous approaches often (1) constrain reasoning traces to hand-designed components, such as a list of criteria, reference answers, or verification questions and (2) structure them such that planning is intertwined with the reasoning for evaluation. In this work, we propose EvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge that first generates an unconstrained evaluation plan, followed by its execution, and then the final judgment. In a self-training loop, EvalPlanner iteratively optimizes over synthetically constructed evaluation plans and executions, leading to better final verdicts. Our method achieves a new state-of-the-art performance for generative reward models on RewardBench (with a score of 93.9), despite being trained on fewer amount of, and synthetically generated, preference pairs. Additional experiments on other benchmarks like RM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both planning and reasoning for building robust LLM-as-a-Judge reasoning models.", "sections": [{"title": "1 Introduction", "content": "As large language models (LLMs) continue to improve, reliably evaluating their long-form outputs has become even more challenging. Owing to the high cost of human evaluation, the LLM-as-a-Judge paradigm has emerged as a promising alternative where LLMs themselves are employed as evaluators (Zheng et al., 2023; Kim et al., 2024a; Saha et al., 2024a; Dubois et al., 2024). LLM-as-a-Judge models also serve as reward models during training for iterative preference optimization and self-improvement (Yuan et al., 2024). Compared to traditional reward models that only output scalar scores, LLM-as-a-Judge models expend more test-time compute by generating Chain-of-Thought (CoT) rationales of the underlying reasoning process of evaluation. This has been shown to not only improve evaluation accuracy but also enhance transparency (Zheng et al., 2023; Wang et al., 2024c; Ankner et al., 2024).\nDespite the promise of LLM-as-a-Judge models, the lack of human-annotated CoTs makes it difficult to train such models. Hence, a crucial step in building these judges is generating rationales by writing down detailed evaluation instructions or rubrics that LLMs can follow. These hand-crafted instructions vary for every new domain (e.g., safety versus coding) (Yu et al., 2024b) and include manually designing evaluation criteria (Zheng et al., 2023; Saha et al., 2024a; Trivedi et al., 2024; Wang et al., 2024b,c), scoring rubrics, and steps for each criterion (Yuan et al., 2024; Trivedi et al., 2024; Kim et al., 2024b; Wang et al., 2024d). This is limiting because different tasks necessitate evaluation standards or procedures tailored to each specific task. For instance, evaluating an essay requires measuring quality along multiple, potentially subjective, fine-grained criteria like relevance and clarity whereas evaluating a math problem requires objectively verifying the correctness of the solution in a step-by-step manner (Lightman et al., 2024). Simply using predefined evaluation prompts hurts evaluation accuracy, while manually adjusting the evaluation instructions is neither scalable nor realistic, given the wide range of arbitrary and complex tasks that LLM-as-a-Judge models are used for.\nTo overcome these limitations, we propose EvalPlanner, a novel approach to building Thinking-LLM-as-a-Judge models that teaches LLMs to both plan and reason for evaluation. EvalPlanner is trained to perform complex"}, {"title": "2 EvalPlanner", "content": "We consider the setting of pairwise response evaluation using the LLM-as-a-Judge paradigm (Zheng et al., 2023). The judge model takes an instruction x and a pair of responses a and b as inputs and generates a preference judgment y, predicting the better response, a or b. By doing so, the model also generates a Chain-of-Thought (CoT) (Wei et al., 2022) aiming to capture the step-by-step reasoning behind the evaluation process.\n2.1 Method Overview\nEvaluating long machine-generated responses to complex instructions is primarily a planning and reasoning problem. In particular, the evaluator must first plan the evaluation recipe and then reason through that recipe and the response(s) to arrive at the final verdict. With that motivation, EvalPlanner hypothesizes that an effective Chain-of-Thought for evaluation should consist of three components: (1) the Evaluation Plan z, (2) the Execution of the Plan e, and (3) the Final Verdict y. Figure 1 shows an example highlighting these three components. For a given input instruction x, the evaluation plan specifies the recipe for evaluating given responses to the instruction. The execution of the plan is responsible for actually conducting the evaluation by following the plan step-by-step, analyzing the input pair of responses a and b and generating the final judgment y. Given an LLM operating as an LLM-as-a-Judge, parameterized by 0, where the plan z and the execution e are assumed to be latent variables, we can write the generative process of the final verdict y as follows.\n$p_{\\theta}(y|x, a, b) = \\sum_{z \\in P} \\sum_{e \\in E} p_{\\theta}(y|e, z, x, a, b)p_{\\theta}(e|z, x, a, b)p_{\\theta}(z|x)$\nWe follow this generative process to build preference pairs of CoTs (section 2.2) for training such a model. See Figure 2 for an overview. Given an instruction and a seed model, we first sample multiple plans z \u2208 P. Then, for a given plan, instruction, and a pair of responses, we sample multiple executions e \u2208 E of the plan which either lead to the correct final verdict or not. Using this data, we develop a self-training loop that trains an LLM-as-a-Judge model by optimizing over both plans and executions, leading to better judgments (section 2.3). At test time, the model generates CoTs of the form \u1ef9 = (\u017e, \u0113, \u1ef9), structured into a plan, its execution, and the final verdict.\n2.2 Synthetic Training Data Generation\nLLM-as-a-Judge models are typically trained on human-annotated preference judgments. However, collecting such data is a costly and tedious process, often requiring expert annotations for domains like code and mathematics (Ouyang et al., 2022; Wang et al., 2024c). Even when such judgments exist, they do not come with any corresponding reasoning steps. This motivates us to develop EvalPlanner by only assuming access to some carefully-chosen input instructions as training data. In the rest of this section, we describe our synthetic training data generation process, which includes constructing both preference pairs (a, b) and their CoTs y.\nPrompt Selection and Generating Response Pairs. We choose prompts belonging to general instruction-following as well as mathematical reasoning. For general instruction-following prompts, we use the same approach as in Self-Taught Evaluators (Wang et al., 2024c) to generate response pairs, i.e., by first modifying the original"}, {"title": "2.3 Preference Optimization of Plans & Executions", "content": "Having developed the initial training data generation recipe, we now describe the training algorithm of EvalPlanner. The pipeline consists of a self-training loop, starting with a seed model Mo (e.g., an instruction-tuned LLM), doing supervised fine-tuning (SFT) on a subset of the 'chosen' CoTs to obtain a model MSFT, followed by two iterations of Direct Preference Optimization (DPO) (Rafailov et al., 2024) on preference pairs of CoTs, leading to models MPPO and MPPO.\nMSFT: SFT on D\u2081, initialized from Mo. Starting from the seed model Mo and a subset of input instructions and response pairs, we follow the recipe in section 2.2 to generate the preference pairs of thoughts. Let us denote this dataset by D\u2081. To teach the model to correctly follow the pattern of our CoT (plan+execution+verdict), we first fine-tune Mo on D\u2081 a subset of only the 'chosen' thoughts from D\u2081. Specifically, for each instruction, we randomly sample one correct thought (that leads to the correct verdict) and perform SFT on that data, leading to a model MSFT.\nMPPO: DPO on D\u2081, initialized from MSFT. Next, initialized from MSFT, we perform DPO on the dataset D1, consisting of both chosen and rejected thoughts. Given the two distinct parts of plan and execution tokens in the thoughts, this teaches the model to contrast between correct and incorrect thoughts, that vary in both the plan and the execution of evaluation. We thus obtain a model MPPO.\nMPPO: DPO on D2, initialized from MPPO. EvalPlanner also consists of a second iteration of DPO, wherein we choose a fresh subset of instructions and response pairs and generate CoTs using the same recipe but from the previous iteration of model MPPO. In particular, we first sample |P| CoTs from MPPO for each training data point, separate out the plans from the thoughts, and then use the same MPPO model to sample |E| executions for each plan. We denote this second iteration of CoT data as D2. We train on new inputs and thoughts from an updated model, under the assumption that the data from the previous iteration is of lower quality. Empirically, we also show that this outperforms a single iteration of DPO trained on the entire set of inputs."}, {"title": "3 Experimental Setup", "content": "3.1 Training\nWe select prompts from two different sources WildChat (Zhao et al., 2024) and MATH (Hendrycks et al., 2021). For WildChat, we directly use the synthetic responses generated by Self-Taught Evaluators (Wang et al., 2024c). For MATH questions, we generate synthetic responses as follows. We prompt a Mixtral 22Bx8 Instruct model to generate multiple candidate solutions. The responses that lead to the correct final answers become our chosen responses while those with incorrect final answers are considered rejected responses. Using synthetic response-pair generation, we collect a total of 17,588 and 4,141 unique (instruction, chosen, rejected) triples from WildChat and MATH, respectively, as our training data, using two separate methods. From this, we select a random subset of 5K instructions (consisting of 2.5K from WildChat and 2.5K from MATH) for SFT and the first iteration of DPO. We reserve the rest for the second iteration of DPO. In each iteration, we sample 5 plans and for each plan, we sample 8 executions (4 in each order of response pair) using a temperature of 0.8 and top_p of 0.95. We develop EvalPlanner with either Llama-3.1-70B-Instruct"}, {"title": "3.2 Evaluation", "content": "We test EvalPlanner on the following pairwise evaluation benchmarks.\n\u2022 RewardBench (Lambert et al., 2024). It consists of (prompt, chosen, rejected) triples spanning 4 categories of prompts: chat, chat-hard, safety, and reasoning.\n\u2022 FollowBenchEval. We build this new evaluation benchmark from FollowBench (Jiang et al., 2024). The original benchmark consists of complex prompts that test LLMs' ability to follow multi-level fine-grained constraints (e.g., \u2018Write a summary within 20 words'). We convert this benchmark into a pairwise evaluation benchmark by sampling two responses from a single model (LLama-3.1-8B-Instruct, LLama-3.2-3B-Instruct, or Mistral-7B-Instruct-v0.2) such that one response satisfies all the constraints and the other one does not. Note that by generating the response-pair using the same model, we ensure consistency in response style which can otherwise lead to potentially superficial features for preference judgments. Our evaluation benchmark, called FollowBenchEval, comprises of 205 samples and spans five different constraint-types of Content, Situ-ation, Style, Format, and Example. This benchmark specifically tests LLM-based judges' ability to (1) plan for multiple constraints that need to be checked, and (2) produce a verdict by checking for those constraints.\n\u2022 RM-Bench (Liu et al., 2024). RM-Bench is designed to assess the robustness of reward models, based on their sensitivity and resistance to subtle content differences and style biases. The original benchmark primarily focuses on evaluating reward models that rate each response independently. We modify the input prompt to accommodate for the evaluation of LLM-as-a-Judge models, which conduct pairwise judgments by comparing two responses simultaneously.\n\u2022 JudgeBench (Tan et al., 2024). JudgeBench is a recent benchmark that evaluates LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. It sources input instructions from existing datasets and generates candidate responses using stronger language models such as GPT-40 and Claude-3.5-Sonnet. Following Tan et al. (2024), we report results on the GPT-40 subset.\nApart from RewardBench (where we follow the original evaluation protocol), for all other benchmarks we report position-consistent accuracy to account for the position bias. Specifically, a prediction is considered correct if the model consistently makes a correct judgment in both orders. We train and test all our models using the standard pair-wise judge prompt from prior work (Zheng et al., 2023), as shown in Figure 5. The maximum number of generation tokens is set to 2048 and the temperature to 0 for inference."}, {"title": "3.3 Baselines", "content": "We compare EvalPlanner with a range of models, including (1) Powerful Open-Sourced and Closed-Sourced LLMs used as judges in a zero-shot manner, (2) Reward Models with Critiques, capable of generating both scalar scores and critiques, and (3) SOTA Generative Reward Models, as listed on the RewardBench leaderboard. We focus on models that also generate rationales along with the final verdict, to compare related competing approaches."}, {"title": "4 Results", "content": "4.1 Experimental Results on Benchmarks\nEvalPlanner outperforms all baselines while being trained on fewer, and synthetically generated, preference pairs. Table 1 shows results on RewardBench. Using the same recipe, we train two EvalPlanner models with"}, {"title": "4.2 Ablations and Analysis", "content": "We conduct all ablations on RewardBench using an EvalPlanner checkpoint, trained on 2.5K MATH instructions using Llama-3.1-70B-Instruct as the seed model."}, {"title": "5 Related Work", "content": "LLM-as-a-Judge. Human evaluation is often considered the gold standard for evaluating LLM responses to complex and open-ended instructions (Ouyang et al., 2022; Dubey et al., 2024). However, given the slow, expensive, and noisy nature of human evaluation (Clark et al., 2021; Karpinska et al., 2021), automatic approaches leveraging LLMs have emerged as scalable and cost-effective alternatives (Zheng et al., 2023; Liu et al., 2023; Kim et al., 2024a; Saha et al., 2024a; Jiang et al., 2023; Zhu et al., 2023). Compared to reward models that only output scalar scores (Wang et al., 2024a,e,d), LLM-as-a-Judge evaluators are more robust and interpretable because of their ability to also generate detailed rationales (Zheng et al., 2023; Zhang et al., 2024a; Ankner et al., 2024). However, in the absence of any human-annotated reasoning traces for evaluation, past works have leveraged LLMs to generate these traces by writing custom prompts for every new domain (Yu et al., 2024b) and hand-designing the components and structure of CoTs, ranging from fine-grained criteria (Zheng et al., 2023; Saha et al., 2024a; Wang et al., 2024c; Zeng et al., 2024; Trivedi et al., 2024), scoring rubric (Yuan et al., 2024; Trivedi et al., 2024; Wu et al., 2024b), verification questions (Dhuliawala et al., 2023), natural language unit tests (Saad-Falcon et al., 2024), and reference answers (Zhang et al., 2024b). In contrast, EvalPlanner proposes a unifying perspective on evaluation by subsuming all necessary components for sound evaluation inside a plan and then letting the model optimize these plans and their executions in a self-training loop.\nSelf-Alignment. Reinforcement Learning from Human Feedback requires a large amount of human annotations, which can be expensive to obtain (Bai et al., 2022; Lee et al., 2024). This has led to the development of various self-alignment techniques for general instruction following (Li et al., 2024; Yuan et al., 2024; Wu et al., 2024a), reasoning (Zelikman et al., 2022; Pang et al., 2024; Gulcehre et al., 2023; Yu et al., 2024a), and evaluation (Pace et al., 2024; Wang et al., 2024c; Trivedi et al., 2024). Specifically, for evaluation, Wang et al. (2024c) construct"}, {"title": "Training to Think, Plan, and Reason.", "content": "EvalPlanner follows a large body of prior work on equipping LLMs with the ability to think by generating additional thought tokens before the final answer (Nye et al., 2021; Zelikman et al., 2022; Wu et al., 2024a; Hosseini et al., 2024). Unlike methods that train on ground-truth thoughts e.g., in the domains of algorithmic reasoning, math, or planning (Nye et al., 2021; Lehnert et al., 2024; Saha et al., 2024b), EvalPlanner is bootstrapped and self-trained from synthetically generated thoughts - focusing on evaluation where objectively defining the structure and components of intermediate thoughts is challenging. Moreover, EvalPlanner's thoughts have decoupled planning and reasoning components, allowing it to optimize both at the same time."}, {"title": "6 Conclusion", "content": "We presented EvalPlanner, a novel approach for building robust and data-efficient Thinking-LLM-as-a-Judge models. Through comprehensive experiments across four benchmarks, we demonstrated the effectiveness of our method, achieving a new SOTA with significantly less, and synthetically generated, training data. To further understand the capabilities of Thinking-LLM-as-a-Judge models, future work could employ them as reward models in the RLHF pipeline."}, {"title": "Impact Statement", "content": "EvalPlanner's broader goal is to advance the field of Machine Learning and in particular, evaluation, by allowing LLM-as-a-Judge models to think before producing a judgment. This has the potential to improve evaluation accuracy and transparency in various applications. EvalPlanner is trained on synthetically generated data from seed Llama models that can reflect stereotypes, biases, and other negative traits present in their pre-training data (Weidinger et al., 2021), which we do not have control over. We encourage further research and discussion on these topics to ensure that this technology is developed and deployed responsibly."}, {"title": "A More Analysis", "content": "Scaling Number of Plans & Executions during Training. We also study the effect of scaling the number of latent evaluation plans and executions in Table 8. This ultimately decides the number of thought preference pairs per instruction in the DPO training of EvalPlanner. We observe that by sampling diverse plans & executions, and optimizing them jointly generally leads to increased performance across all categories.\nEffect of Source of Input Instructions. We train EvalPlanner by mixing instructions from WildChat and MATH. In Table 9, we show that while training on WildChat instructions help the Chat-Hard category more, reasoning performance is particularly enhanced by training on the MATH instructions."}, {"title": "B Prompts", "content": "Figure 3 shows the planning prompt for generating initial evaluation plans from the seed model. Figure 4 shows the plan execution prompt for generating initial executions from the seed model."}, {"title": "C Examples of plans generated by EvalPlanner", "content": "Figures 6, 7, and 8 show examples of diverse plans generated by EvalPlanner, based on the user instruction."}]}