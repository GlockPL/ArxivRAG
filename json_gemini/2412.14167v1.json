{"title": "VideoDPO: Omni-Preference Alignment for Video Diffusion Generation", "authors": ["Runtao Liu", "Haoyu Wu", "Ziqiang Zheng", "Chen Wei", "Yingqing He", "Renjie Pi", "Qifeng Chen"], "abstract": "Recent progress in generative diffusion models has greatly advanced text-to-video generation. While text-to-video models trained on large-scale, diverse datasets can produce varied outputs, these generations often deviate from user preferences, highlighting the need for preference alignment on pre-trained models. Although Direct Preference Optimization (DPO) [39] has demonstrated significant improvements in language and image generation [48], we pioneer its adaptation to video diffusion models and propose a VideoDPO pipeline by making several key adjustments. Unlike previous image alignment methods that focus solely on either (i) visual quality or (ii) semantic alignment between text and videos, we comprehensively consider both dimensions and construct a preference score accordingly, which we term the OmniScore. We design a pipeline to automatically collect preference pair data based on the proposed OmniScore and discover that re-weighting these pairs based on the score significantly impacts overall preference alignment. Our experiments demonstrate substantial improvements in both visual quality and semantic alignment, ensuring that no preference aspect is neglected.", "sections": [{"title": "1. Introduction", "content": "With the rapid advancement of computing power and the increasing scale of training data, generative diffusion models have made remarkable progress in generation quality and diversity for video generation. However, current video diffusion models often fall short of meeting user preferences in both generation quality and text-video semantic alignment, ultimately compromising user satisfaction.\nThese issues often arise from the pre-training data, and filtering out all low-quality data is challenging given the vast often of pre-training data. Specifically, two types of low-quality data are prevalent. First, regarding the videos themselves, some samples suffer from low resolution, blurriness, and temporal inconsistencies, which negatively impact the visual quality of generated videos. Second, regarding text-video pairs, mismatches between text descriptions and video content reduce the model's ability to be controlled accurately through text prompts. Similar challenges are also seen in content generation for other modalities, such as language and image generation, where noisy pre-training data lowers output quality and reliability.\nUser preference alignment through Direct Preference Optimization, or DPO [39], has been proposed and tackles these issues well for language and image generation [48]. In this paper, we focus on aligning video diffusion models with user preferences with the idea of DPO with crucial adaption modifications, termed VideoDPO, described next.\nFirst, we introduce a comprehensive preference scoring system, OmniScore, which assesses both the visual quality and semantic alignment of generated videos. We build the DPO reward model based on OmniScore. While existing visual reward models [23] typically focus on only one of these aspects, our experiments (see Fig. 3d) show that visual quality and semantic alignment, as well as various facets of visual quality, have low correlation. Addressing a single aspect does not inherently capture the others. Thus, a comprehensive scoring system like OmniScore, which integrates both dimensions, is crucial for accurate evaluation and alignment.\nSecond, obtaining preference annotations for generated videos is challenging due to the high cost of human labeling. To address this, we propose a pipeline that automatically generates preference pair data by strategically sampling from multiple videos conditioned on a given prompt, thereby eliminating the reliance on human annotation.\nThird, to further improve the performance and efficiency of alignment training, we introduce a novel data re-weighting method, OmniScore-Based Re-Weighting. This approach is based on the intuition that certain preference pairs, particularly those with larger quality differences, have a greater impact on alignment. By analyzing the frequency distribution, we assign higher weights to these influential samples, prioritizing them during training. Experimental results show that our method delivers significant performance improvements while producing videos with high visual fidelity and precise semantic alignment, as shown in Fig. 1.\nIn summary, our contributions are:\n(i) We pioneer the adaptation of DPO to video diffusion models, addressing the unique challenges of aligning video generation outputs with user preferences.\n(ii) We introduce key adjustments to the DPO framework, including the development of OmniScore, a comprehensive preference scoring system, along with an automated preference data generation pipeline and a novel re-weighting strategy to enhance alignment training efficiency.\n(iii) We validate our framework through extensive experiments conducted on three state-of-the-art open-source text-to-video models, evaluating performance across multiple metrics. The results demonstrate the robustness and effectiveness of our approach in improving both visual quality and semantic alignment."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Text-to-Video Diffusion Models", "content": "The Text-to-Video (T2V) task aims to produce visually appealing videos that align with text input, ultimately striving to meet user requirements. It has wide applications across various domains, including story animation [15], controllable video generation [34, 35], video game development [5], and embodied artificial intelligence [9]. The predominant approaches for video generation [1, 2, 14, 49] employ diffusion-based models [18, 44]. Non-diffusion frameworks [10, 52, 54] have also shown significant progress. For instance, VideoCrafter [6, 7] utilizes a 1.4 billion parameter U-Net architecture for video generation, while models [19, 58, 63] such as Open-Sora [63] and CogVideoX [19, 58] are based on a Diffusion Transformer (DiT) backbone [8, 36].\nGiven the complexity of video data, transferring diffusion pipelines to generate high-quality video content is a non-trivial task. This challenge is compounded by the necessity of implementing a series of post-training methods aimed at enhancing video quality. Existing methods include parameter efficient tuning [12, 16, 26, 27], data-centric work [13], and human preference alignment [37, 60] work. Despite the continuous expansion of training datasets and computational resources, the resulting video quality often falls short of user expectations."}, {"title": "2.2. RLHF and RLAIF", "content": "Reinforcement Learning from Human Feedback (RLHF) is one of the most widely used post-training methods on large language models [57, 59, 62] and diffusion models. RLHF contains a reward model in the training stage and reinforcement learning stage. The reward model is trained on win-lose pairs annotated by humans by predicting the preference label. Prior works use policy-gradient [43] methods to align the policy model. The two-stage training pipeline is unstable and complex.\nPreference alignment in diffusion models. DPO [39] is a reward model free method that can be easily performed on diffusion models. Despite the DPO-based methods being tested on text-to-image diffusion [48] models and gaining significant process, it has been rarely tested on T2V diffusion models. VADER [37] applies a reward model to refine a video diffusion model. T2V-turbo [26, 27] exploring training consistent distillation models by reward gradients. SPO [29] tries to improve quality on each step of the diffusion inverse process. T2V-turbo v2 [27] also uses a reward model for refinement. To the best of our knowledge, we are the first to propose to apply DPO-based method on video diffusion.\nVisual content quality assessment. Previous video generation models often use metrics of Inception Score (IS) [41], Fr\u00e9chet inception distance (FID) [17], Fr\u00e9chet Video Distance (FVD) [47], and CLIPSIM [38] for evaluation. For text-to-image (T2I) models, several benchmarks [20, 25, 40, 50]. Several benchmarks [21, 32, 33, 55] have been proposed to comprehensively evaluate the capabilities of video generation models. These benchmarks typically assess generation quality by using pre-trained score models [22, 38, 42, 46] to evaluate videos generated from a curated set of human-designed prompts. Other benchmarks, such as those for compositional video generation [45], story generation [3], chronological generation [61], and dynamic and motion quality [30, 31], focus on evaluating specific sub-tasks within video generation."}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. Diffusion Models", "content": "For diffusion models, visual contents are generated by transforming a initial noise to the desired sample through multiple sequantial steps. It is a Markov chain process where the model continually denoises the initial noise vector $x_T$ and finally generates a sample $x_0$.\nThe generation step from $x_t$ to $x_{t-1}$ is given by\n$x_t \\sim q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, \\beta_t I)$,\nwhere $\\beta_t$ is the variance schedule, determining the amount of noise added at each timestep t. $\\alpha_t$ is a parameter obtained by $\\alpha_t = 1 - \\beta_t$ which represents the proportion of the original data retained.\nThe denoising model $\\epsilon_\\theta$, which learns to predict the noise added to $x_0$ for timestep t, is trained by minimizing the loss between the ground-truth $\\epsilon$ and prediction. The loss function is defined as\n$L_\\theta(\\theta) = E_{t,x_0,\\epsilon} [\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)||^2]$,\nwhere $\\epsilon$ is the noise added in the forward process, and $\\bar{\\alpha}_t$ is the cumulative product of $\\alpha_t$ up to timestep t."}, {"title": "3.2. Direct Preference Optimization", "content": "DPO [39] is a technique used to align generative models with human preferences. Training on pairs of generated samples with positive and negative labels, the model learns to generate positive samples with higher probability and negative samples with lower probability. DiffusionDPO [48] adapts DPO for text-to-image diffusion models. The loss function provided in the [48] is defined as:\n$L_{DPO}(x^W, x^L, c) = L(x^W, p) - L(x^L, p)$,\nwhere $x^W$ and $x^L$ represent positive and negative samples, respectively. $L(x^W, p)$ and $L(x^L, p)$ are losses for positive and negative parts, encouraging the model to generate samples closer to preferences."}, {"title": "4. VideoDPO", "content": ""}, {"title": "4.1. OmniScore", "content": "The quality of generated videos is influenced by multiple factors, which can be grouped into two main categories: visual quality and semantic alignment. Visual quality includes the clarity and richness of detail within each frame, i.e., intra-frame quality, and the smoothness and coherence between frames, i.e., inter-frame motion and consistency. Semantic alignment, on the other hand, focuses on whether the generated video accurately follows the text prompt. Inspired by VBench [21], we propose a scoring approach for video generation, OmniScore, which comprehensively accounts for both visual quality and semantic alignment of generated videos. OmniScore incorporates both quality and semantic sub-scores, specifically designed to evaluate video generation on three primary dimensions: the fidelity and aesthetics of visual quality, the smoothness of inter-frame transitions, and the level of semantic alignment with the text. Each model for these dimensions is provided in the Appendix. This holistic approach enables a balanced method for preference pair data generation.\nIntra-frame quality. Intra-frame quality includes two main metrics, image quality and aesthetic appeal. These metrics assess the visual quality of individual frames measuring image fidelity and aesthetic attractiveness. They provide a thorough evaluation of the frame-level visual detail, ensuring each frame is not only of high-fidelity but also visually engaging.\nInter-frame quality. Inter-frame quality focuses on the relationships between consecutive frames, examining how well they connect over time. This dimension includes metrics for subject consistency and background consistency, which assesses the stability of key elements across frames, ensuring that the main subject and background remain visually coherent. Additionally, it evaluates motion dynamics through three metrics: temporal flickering, motion smoothness, and the degree of motion dynamics. These collectively examine the video's fluidity, ensuring smooth transitions between frames, minimizing visual disruptions, and maintaining a natural level of movement. By considering these aspects, we aim to ensure that the video maintains visual continuity and avoids disruptions that can detract from the viewing experience.\nText-video semantic alignment. Semantic alignment evaluates how closely the video content aligns with the text prompt. Using a foundational vision-language model, this score measures how accurately the video reflects the text and captures the user's intent."}, {"title": "4.2. Score-Ranked Preference Data Generation", "content": "To construct the dataset of preference pairs, the scoring method, OmniScore, is employed in combination with a best vs. worst selection strategy. For each prompt, our system generates multiple videos and a preference pair is selected. Specifically, the video with the highest OmniScore is identified as the preferred video $v^W$, while the video with the lowest score is designated as the negative one $v^L$, as shown in Fig. 2. We utilize VidProm [51], a dataset of human-written text-to-video prompts, in our data construction process, enabling the model to better adapt to the distribution of real-world human inputs.\nVideo generation and scoring. Given a text prompt p, we generate a set of N videos {$V_1, V_2, ..., v_n$}, using the pre-trained video generation model that we aim to align. For each generated video $v_i$, we apply the OmniScore model S to evaluate its quality conditioned on the text prompt p. This scoring model assigns a score to each video:\n$s_i = S(v_i, p), \\text{ for } i = 1, 2, ..., N.$\nHere, $s_i$ denotes the OmniScore assigned to video $v_i$ given its prompt p. This scoring step creates a quantitative basis for comparing videos generated from the same prompt.\nPreference pair selection. We select preference pairs $(V_i, v_j)$ from the N generated videos according to their OmniScore {$s_1, s_2,..., s_N$}. We select the video with the highest score as the winning sample $v^W$ and the video with the lowest score as the negative sample $v^L$. This selection process is formalized as follows:\n$(v^W, v^L) = (V_{i, j}), i = \\arg \\max s_i, j \\arg \\min s_j.$\nBy constructing preference pairs consistently with maximally contrasting scores, we aim to establish clear distinctions between the preferred, or winning, and the less-preferred, or losing video samples. This strategy serves as a strong foundation for training the alignment model. We discuss several other selection strategies in Sec. 5.4."}, {"title": "4.3. OmniScore-Based Data Re-Weighting", "content": "Previous DPO training directly uses winning and losing preference pairs, for example, those generated as described in Sec. 4.2. However, we find the score difference between some winning and negative samples can be minimal, or in some cases, nearly identical, making it challenging for the model to effectively distinguish these samples with minor differences. To address this, we propose assigning higher weights to preference pairs with clearer distinctions, enabling the model to focus on those pairs that could provide more meaningful alignment cues. Our approach significantly enhances the model's ability to learn meaningful alignment preferences.\nSpecifically, we first construct a histogram of OmniScore s of each generated video, including K \u00d7 N videos from K prompts in total. We denote p(\u00b7) as the frequency and we define a function p(\u00b7) to approximate the probability of a video based on its frequency within these bins.\nFor each winning-losing pair, we define the pair probability as the geometric mean of their individual probabilities, i.e., $prob(s^W, s^L) = \\sqrt{p(s^W) \\cdot p(s^L)}$, where $s^W$ and $s^L$ represent the scores of the winning (positive) and losing (negative) samples, respectively. We define the re-weighting factor for each pair as:\nW_{pair} = (\\beta/prob(s^w, s^L))^\\alpha.\nHere, \u03b2 is a constant set to the approximate probability of the most frequent sample, and \u03b1 is a tuning hyperparameter. When \u03b1 equals to 0, no re-weighting is applied, and all pairs have equal weight. A larger \u03b1 increases the weight for pairs with lower probability.\nThe final training loss for each video pair is defined as:\nL_{video} = L_{DPO}(p, v^W, v^L) \\cdot W_{pair},"}, {"title": "5. Experiment", "content": ""}, {"title": "5.1. Experiment Setup", "content": "Baselines. We compare our pipeline with several state-of-the-art open-source models for text-to-video generation: VideoCrafter-v2(VC2) [7], T2V-Turbo(Turbo) [26], and CogVideo [19]. These models are utilized as baselines in our alignment experiments. Additionally, we include VADER[37], which directly fine-tunes video diffusion models in several final steps using the differentiable reward model. We compare our method with their publicly released weights.\nMetrics. To evaluate our method and the baselines, we use the following metrics: VBench, a widely recognized benchmark that assesses both quality and semantic alignment in video generation across 16 hierarchical dimensions, providing fine-grained evaluation. HPS (V) [56] and PickScore [23] are also included as metrics; both are trained on large-scale human preference datasets and are designed to predict scores of human preference for generated videos.\nImplementation details. We train the video diffusion models for 3000 steps with a global batch size of 8, using the AdamW optimizer with a learning rate of 6e-6. During training, the re-weighting algorithm hyper-parameters are set to \u03b1 = 0.72 and \u03b2 = 1. K = 10,000 human-written prompts from VidProm [51] are used for alignment training. For each prompt, the number of generated videos N is set to 4. The bin width for the distribution of video OmniScore scores is set to 0.01. All experiments are conducted on 4 Nvidia A100 GPUs."}, {"title": "5.2. Dataset Analysis", "content": "Score distribution and sub-dimension correlations. We analyze the dataset by examining the OmniScore distribution, the score range, i.e., the difference between maximum and minimum scores, for each prompt, and the correlations among individual scoring metrics. To quantify these correlations, we calculate Pearson correlation coefficients. For each prompt, N videos are generated and assigned OmniScore, enabling us to assess the distribution of score differences, i.e., the range between the highest and lowest scores, within each set of N videos. Fig. 3 presents both the overall score distribution and the distribution of score differences between video pairs."}, {"title": "5.3. Aligning Video Diffusion Models", "content": "In this section, we evaluate our approach through both quantitative and qualitative results by testing on various text-to-video models. For quantitative evaluation, we utilize VBench, HPS (V), and PickScore, covering both non-human and human preference metrics. To evaluate semantic alignment and visual quality, we analyze intra-frame aspects, examining image fidelity and aesthetic appeal to ensure each frame aligns well with the prompt. Additionally, for inter-frame analysis, we assess temporal consistency, focusing on whether the background and main foreground objects remain coherent across frames.\nQuantitative results. We present our results in Tab. 1, where we evaluate state-of-the-art open-source text-to-video models, including VC2, T2VTurbo, and CogVideo. After alignment using our approach, all models show performance improvements, with consistent gains on the VBench metric. Models such as VC2 and T2VTurbo also achieve higher scores on human preference metrics, including HPS (V) and PickScore, demonstrating the generalizability of our approach. We do not report CogVideo on HPS (V) as this score appears to be insensitive to CogVideo, possibly due to the low quality generation, given its early release date. The detailed performance results on VBench are presented in Tab. 2. In comparison to other RLHF methods like VADAR, our approach yields superior results in both semantic and visual quality aspects. This improvement is attributed to our use of preference pairs derived from a more comprehensive feedback signal, both quality(intra-frame and inter-frame levels) and semantic criteria. However, methods like VADER can only optimize a single differentiable reward model, limiting improvements in other dimensions. Training with VADER on multiple reward models simultaneously will significantly increase the computational cost, making it difficult to scale.\nIntra-frame qualitative analysis. Fig. 4 presents qualitative comparisons achieved using VideoDPO on baseline models. Videos generated after VideoDPO demonstrate enhanced visual details with fewer artifacts (as shown in the Quality column) and improved alignment with the input prompt (as shown in the Semantic column). For instance, Turbo-VideoDPO produces a more accurate vase compared to the original model, where the vase's mouth is incorrectly shaped. Similarly, the video from CogVideo-VideoDPO is better than that by CogVideo which contains unnatural color artifacts in the dog. In terms of semantic accuracy, VC2-VideoDPO generates a boat with visible human figures, offering a more accurate depiction compared to both VC2-VADER and the original VC2. Additionally, Turbo and CogVideo after training with our method, each generates more realistic character relationships and scene layouts correspondingly. These improvements demonstrate that our alignment approach successfully enhances both semantic following and visual fidelity in generated videos.\nInter-frame qualitative analysis. Our approach significantly improves the temporal consistency of aligned models, and Fig. 5 shows the comparison results. After alignment, VC2 is able to generate a stable stop sign, Turbo produces a scene where the number of giraffes remains consistent, and CogVideo generates a panda with stable coloring, avoiding sudden color changes. These examples demonstrate the effectiveness of our alignment method in enhancing temporal stability across frames, in terms of texts, object and color across different frames."}, {"title": "5.4. Analysis", "content": "Comparing OmniScore with single-aspect reward. We compare the setting trained using OmniScore with those trained using a single reward, such as only the semantic score or the aesthetic score. On VBench, the results are 80.20% and 79.65%, which are significantly lower than the result achieved with OmniScore, which is 81.93%. This demonstrates the advantage of using a comprehensive reward like OmniScore to evaluate samples. The comparison with VADER in Tab. 1, also supports this conclusion.\nPairwise training strategies. We explore different strategies for constructing preference pairs from N generated videos for a prompt, shown in Tab. 3a. The \"Better-Worse\" strategy outputs multiple pairs (vi, vj) as long as si > Sj, representing the score of video vi is higher than that of vj. The \"Best-vs-Worse\u201d strategy forms pairs by selecting the highest-scoring video and pairing it with others that have lower scores. In contrast, the \"Better-vs-Worst\u201d strategy pairs the lowest-scoring video with others. The \"Best-vs-Worst\u201d strategy, adopted by us, pairs only the highest and lowest-scoring videos and outputs 1 preference pair for a prompt. The experiments show that this strategy, which generates only the most distinctive pair, yields the best performance. This demonstrates that the key to the alignment performance lies in the average quality of the preference, rather than the absolute quantity of data.\nData filtering. According to Fig. 3, there are some preference pairs are not distinctive, that the positive video is only slightly better than the negative one in terms of the OmniScore. We explored the impact of removing these less-distinctive pairs to see if it could improve alignment performance."}, {"title": "6. Conclusion", "content": "In this paper, we propose VideoDPO, a novel pipeline to align video diffusion models. VideoDPO introduces a comprehensive scoring method, OmniScore, along with a novel data reweighting strategy that automatically constructs and prioritizes preference data, enabling more effective alignment training. Experiments show that VideoDPO enhances both visual quality and semantic alignment for state-of-the-art text-to-video models."}]}