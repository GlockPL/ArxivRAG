{"title": "A Semi-supervised Multi-channel Graph Convolutional Network for Query Classification in E-commerce", "authors": ["Chunyuan Yuan", "Ming Pang", "Zheng Fang", "Xue Jiang", "Changping Peng", "Zhangang Lin"], "abstract": "Query intent classification is an essential module for customers to find desired products on the e-commerce application quickly. Most existing query intent classification methods rely on the users' click behavior as a supervised signal to construct training samples. However, these methods based entirely on posterior labels may lead to serious category imbalance problems because of the Matthew effect in click samples. Compared with popular categories, it is difficult for products under long-tail categories to obtain traffic and user clicks, which makes the models unable to detect users' intent for products under long-tail categories. This in turn aggravates the problem that long-tail categories cannot obtain traffic, forming a vicious circle. In addition, due to the randomness of the user's click, the posterior label is unstable for the query with similar semantics, which makes the model very sensitive to the input, leading to an unstable and incomplete recall of categories.\nIn this paper, we propose a novel Semi-supervised Multi-channel Graph Convolutional Network (SMGCN) to address the above prob-lems from the perspective of label association and semi-supervised learning. SMGCN extends category information and enhances the posterior label by utilizing the similarity score between the query and categories. Furthermore, it leverages the co-occurrence and semantic similarity graph of categories to strengthen the relations among labels and weaken the influence of posterior label instability. We conduct extensive offline and online A/B experiments, and the experimental results show that SMGCN significantly outperforms the strong baselines, which shows its effectiveness and practicality.", "sections": [{"title": "1 INTRODUCTION", "content": "Online shopping has evolved into a fundamental aspect of our lives, significantly reshaping our daily routines over the past few years. An increasing number of e-commerce platforms such as Amazon, Taobao, and JD offer customers hundreds of millions of vibrant and colorful products. These massive products are organized in the form of categories to facilitate customers to retrieve them quickly. To cover as many kinds of commodities as possible, the category taxonomy involves nearly ten thousand leaf categories in e-commerce applications. Due to the diversity of user needs and plenty of categories, accurately capturing the user's intention to purchase the category of products is a crucial part of the e-commerce platform.\nQuery intent classification has gained significant attention from both academia and industry. Early research uses click graphs [9] or context information [2] to solve the short and ambiguous problems of query faced by the general Web search. In recent years, query intent classification has usually been regarded as a multi-label text classification problem in academia. With the wide application of deep learning technology, some deep learning-based models, such as XML-CNN [10], KRF [12], HiAGM [27], LSAN [19] have been proposed to learn the contextual information of documents to enhance the representation learning of queries. Furthermore, some recently proposed query intent classification models, such as PHC [23], DPHA [26], and MMAN [22] also explore utilizing the correlation between query intent classification and semantic textual similarity or multi-task to facilitate models to learn external information beyond query information.\nMost previous methods assume an abundance of authentic la-beled data is available to train a model. However, manual annotation is expensive and time-consuming, especially for the thousands of product categories. As a result, the industry often utilizes users' click behavior as an implicit feedback signal to generate training samples, but this approach has its challenges. One major issue is the category imbalance in the training data, where long-tail cate-gories struggle to obtain user clicks and traffic, making it difficult for models to identify them. This exacerbates the problem of low traffic to long-tail categories, creating a vicious cycle. This problem becomes more serious for newly built categories because of busi-ness development. Figure 1 illustrates the distribution of query rank versus search count. According to Zipf's law, most queries show long-tail phenomena, which makes these models hard to generalize due to a lack of training data.\nFurthermore, the posterior label of queries with similar seman-tics is unstable due to the randomness of user clicks. For example, when the user searches for \"earphones\", they may click on labels such as \"Headset\" and \"Second-hand headset\". However, if another user inputs a similar search query, such as \"white earphones\", the clicked labels may change to \"Bluetooth earphones\" or \"Gaming earphones\". Even though the categories of \"Headset\" and \"Second-hand headset\" also offer white earphones, they are not clicked by customers, thus not presented at the labels of the query \"white earphone\". This instability makes the model very sensitive to input, leading to an unstable and incomplete recall of categories. Since downstream product retrieval relies on category outcomes, an in-complete category recall cascades into relevant products not being retrieved, thereby impacting the user's purchase experience and business revenue.\nTo address the aforementioned challenges simultaneously, we proposed a semi-supervised multi-channel graph convolutional network. To begin, we obtain the co-occurrence relations between categories by counting the frequency of category pairs in the train-ing samples and obtain the similarity relations between categories through the semantic relevance between categories. Despite the limited number of training samples for tail categories, tail cate-gories are easily connected to their relevant popular categories by the co-occurrence or semantic similarity relations. These relations facilitate the transfer of gradients from samples with popular cate-gories to tail categories, resulting in more effective representation training for long-tail categories and compensating for the draw-backs of posterior labels. Subsequently, we use a multi-channel GCN to model both relations, which enables the model to learn similar representations for the categories with higher relevance."}, {"title": "2 RELATED WORK", "content": "Conventional multi-label classification methods can be broadly categorized into two main types: problem transformation and al-gorithm adaptation methods. Problem transformation methods are multi-label techniques that transform the multi-label problem into multiple single-label problems [16, 17], while algorithm adaptation methods [14, 25] focus on adapting existing algorithms to tackle the multi-label challenges.\nIn recent years, deep learning methods, such as RCNN [8], and XML-CNN [10], have been applied to capture contextual infor-mation of the document for multi-label text classification. Some seq2seq-based techniques [3, 19, 21], like MLC2Seq [13] and SGM [20] have used an RNN to encode the text and an attention-based RNN decoder to generate predicted labels sequentially to learn the depen-dency of different labels. Additionally, LSAN [19], and LEAM [18], have explored label-specific attention mechanisms to capture the in-teractions between words and labels to learn better representations for labels and measure the compatibility of word-label pairs.\nWhile these methodologies have demonstrated auspicious out-comes in various benchmark assessments, their applicability within industrial domains encounters distinct challenges. Industrial train-ing datasets frequently exhibit class imbalance, and the stability of data labels remains precarious because of the randomness of user behavior. Consequently, their efficacy may be significantly undermined if they were to be employed directly in the context of online E-commerce applications.\nEarly query intent classification mainly focuses on mining the click graphs [9] or click-through logs [1] to improve the accuracy"}, {"title": "3 MODEL", "content": "In this section, we first formally define the query intent classifica-tion task. Then, we describe different modules of SMGCN in detail and analyze the influence of the model during the training and predicting process."}, {"title": "3.1 Problem Statement", "content": "Suppose the query inputted by users on the E-commerce applica-tions, has $q = [x_1, x_2, ..., x_{L_q} ]$ characters. Each category $n_i$ has a category name and a series of product words, and $|C|$ denotes the total amount of leaf categories. The products belong to one of the leaf categories. The query classification task requires models to assign a subset $y$ of categories from all leaf categories to $q$. Our target is to learn a classification model $f(, )$. For any input query $q$, the model $f(q, [n_1, .., n_c])$ can select relevant categories from the label set. For a clear definition, throughout the rest of this paper, bold lowercase letters represent vectors."}, {"title": "3.2 Overview", "content": "Figure 2 illustrates the components of the SMGCN model, which is mainly composed of three modules: (1) query and category rep-resentation learning module, (2) semi-supervised label generation module, and (3) multi-channel graph learning module. Specifically, the query and category representation learning module describes the mapping of query or category sequence from word embedding into the same semantic space; the semi-supervised label generation module illustrates why the model needs semi-supervised labels and how to utilize the pseudo-label to facilitate model training; the multi-channel graph learning module defines two kinds of relations of categories and introduces how to fuse both kinds of relations to learn better category embeddings. Finally, query and category embeddings are fed to a classifier to predict the user's intent."}, {"title": "3.3 Learning query and category representation", "content": "Query and categories are the basic input of the model. To learn good semantic representations of them, we project the query and"}, {"title": "3.4 Semi-supervised label generation", "content": "Most existing methods rely on user click behavior to generate train-ing samples, but long-tail categories struggle to obtain traffic and user clicks compared to popular categories. Additionally, user click behavior tends to be random and unstable for queries with simi-lar semantics due to individual preferences and varying demands. As a result, the posterior labels are highly imbalanced and unsta-ble, leading to inadequate performance for long-tail categories and incomplete category recall.\nTo compensate for the drawbacks of the posterior label, we calculate the similarity score between the query and categories to treat it as a semi-supervised label. Then, we fuse it with the label clicked by the user to calculate loss as the final label. Specifically,\n$s_i = \\text{stop\\_grad}(\\frac{Q_iC^T}{||Q_i||||C||}),$\n$S_{ij}^{semi} =\\begin{cases}S_{ij} & \\text{if } S_{ij} \\ge t\\\\0 & \\text{if } s_{ij} < \\tau'\\end{cases},$\nwhere $s_i \\in R^{1\\times|C|}$, is the relevance scores between query $q_i$ and all categories. $t$ is the threshold to filter the categories with low scores. $y^{semi}$ is the semi-supervised label. For example, referring to Figure 2, although the new category \"Bone conduction headphones\" did not have click records below query \"earphone\", they are seman-tically highly related and should be recalled. This connection can be expressed by the $y^{semi}$ and influences model training.\nBoth query and label encoders use the same text encoder, but their word distribution is different. If the gradient of the semi-supervised signal is fed to the semi-supervised label generation module, a circular dependency may arise, which could ultimately result in the model collapse. To avoid this issue, we disable the gradient feedback of this branch and solely rely on the gradient of semi-supervised labels to guide the training of the query intent classification module."}, {"title": "3.5 Multi-channel graph learning", "content": "Subsequently, we will introduce how the model leverages the rela-tions among categories as prior information to compensate for the drawbacks of the posterior labels."}, {"title": "3.5.1 Graph construction", "content": "Firstly, we obtain the co-occurrence re-lations between categories by counting the co-occurrence times of categories in the training samples. Then, we compute the condi-tional probability of two categories and obtain the adjacency matrix $A^{coo}$.\n$A_{ij}^{coo} = \\frac{N(c_i, c_j)}{N(c_i)},$\nwhere $N(c_i, c_j)$ is co-occurrence times of category $c_i$ and $c_j$ and $N(c_i)$ denotes the number of occurrences of category $c_i$.\nAdditionally, we obtain the semantic similarity relations between categories by computing the cosine similarity of every pair of cate-gories:\n$a_{ij} = \\frac{C_iC_j}{||C_i||||C_j||}$\n$A_{ij}^{sim} =\\begin{cases}A^{sim} & \\text{if } a_{ij} \\ge a\\\\0 & \\text{if } a_{ij} < a\\end{cases},$\nwhere $a$ is the threshold to filter the edges with low relevance scores.\nThe two correlation matrices obtained from different scales can-not be merged directly, so it is necessary to normalize $A^{coo}$ and $A^{sim}$ respectively. The normalization method [7] is formalized as follows:\n$\\hat{A} = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}},$\nwhere D is a diagonal degree matrix with entries $D_{ij} = \\sum_j A_{ij}$. Next, we merge the two correlation matrices after normalization:\n$A = [A^{coo}; A^{sim}],$\nwhere $A \\in R^{2\\times|C|\\times|C|}$ is the fused adjacency matrix."}, {"title": "3.5.2 Graph learning", "content": "GCN is applied to generate nodes' represen-tation by aggregating neighborhood information. The layer-wise propagation rule of a multi-layer GCN is as follows:\n$H^{l+1} = LeakyReLU (A\\tilde{H}^lW^l),$\nwhere $H^l \\in R^{|C| \\times d}$ is in the $l$th layer (where $|C|$ denotes the number of nodes, d is the dimensionality of node features) and $H^{l+1}$ is the enhanced node features. $W^l \\in R^{d\\times d'}$ is a transformation matrix to be learned.\nDespite the limited number of training samples for tail cate-gories, tail categories are easily connected to their relevant hot categories by the co-occurrence or semantic similarity relations. These relations facilitate the transfer of gradients from samples with hot categories to tail categories, resulting in more effective representation training for long-tail categories and compensating for the drawbacks of posterior labels."}, {"title": "3.6 Training and inference", "content": "Finally, we obtain query representation $q_i \\in R^{1\\times d}$ and the final representations of categories $H \\in R^{|C|\\times d}$. Specifically, we introduce the nonlinear transformation layer which is defined as:\n$\\hat{y}_i = \\text{sigmoid}(q_iH^T + b),$\nwhere $b \\in R^{1\\times|C|}$ is the bias, and $\\tilde{y_i} \\in R^{1\\times|C|}$ is the predicted labels of query $q_i$.\nTo optimize the model and use the posterior and semi-supervised labels, we fuse them as follows:\n$y_i = y^{click} + y^{semi},$\n$Y_i =\\begin{cases}y_i & \\text{if } y_i \\le 1.0\\\\1.0 & \\text{else}\\end{cases},$\nwhere $y^{click}$ is the one-hot encoding of clicked labels of query $q_i$, and the value range of $y_i$ is $y_i \\in [0, 1]$.\nIn this paper, we use the binary cross-entropy loss as the objec-tive to train the model :\n$L = - \\sum_{j=1}^N \\sum_{i=1}^C y_i log (\\hat{y}_i) + (1 - y_i) log (1 - \\hat{y}_i),$\nwhere N is number of samples, L is the final loss function."}, {"title": "4 EXPERIMENT", "content": "In this section, we will discuss the offline and online experiments in detail. We first introduce the datasets and the evaluation metrics used in this paper. Then, we analyze the experiment results by several fair comparisons with strong baselines. After that, we deeply investigate the effect of different modules of the SMGCN model. Subsequently, we present the online performance of the model on the JD search engine and further analyze the influence of different modules. Finally, we explore the influence of hyper-parameters."}, {"title": "4.1 Dataset", "content": "To evaluate the effectiveness and generality of the proposed model, we conducted a series of experiments on two large-scale real-world datasets collected from users' click logs on the JD application. The statistics of the datasets are listed in Table 1. Specifically,\nCategory Data: We randomly sample queries and corre-sponding clicked products from search logs over one month. The clicked products' category are treated as the query's intent. The clicked frequency of the product is treated as the frequency of the category. To filter unreliable categories,"}, {"title": "4.2 Baseline Models", "content": "We compare SMGCN with several strong baseline models, including widely-used multi-label classification methods, such as XML-CNN, and LSAN, and query intent classification models, such as PHC, DPHA, and MMAN. The detailed introductions are listed as follows:"}, {"title": "4.3 Evaluation Metrics", "content": "Query intent classification is essentially a multi-label text classifi-cation task. Thus, following the settings of previous work [22, 24], we report the micro and macro precision, recall, and F1-score of the models as the metrics to evaluate their performance. The definitions of these metrics are listed as follows:\nMicro-Precision / Recall / F1: The calculation of the micro average metric requires aggregating the contributions of all labels to compute the average micro score. The categories with more samples have an advantage over other categories.\nMacro-Precision / Recall / F1: The macro average metric computes the score independently for each label and then takes the average as the final score. Thus, each category has a similar contribution to the overall score."}, {"title": "4.4 Experiment Settings", "content": "We implement the models based on the Pytorch framework. The dimensionality of the embedding of BERT is 768. We use a 2-layer GCN to learn the category embeddings of two graphs, and the dimensionality of embedding is 768. We use Adam algorithm [6] with a learning rate of 1e-4. The max length of the query is set to 16. The threshold of labels is set to 0.5. The threshold t is set to 0.8 and a is set to 0.65 according to the result of the grid search. The model training should use a warm start strategy and the threshold t is gradually decreased to 0.8 as the training.\nTo overcome the overfitting, we use the dropout strategy with a dropout rate of 0.5. The maximum training epoch is set to 20, and the batch size of the training set is set to 1024. We select the best parameter configuration based on the performance of the validation set and evaluate the configuration on the test set."}, {"title": "4.5 Offline Evaluation", "content": "The experimental results indicate that SMGCN significantly outperforms all baselines on two large-scale real-world datasets. Specifically, we have the following observations:\nFor the multi-label text classification baselines (i.e., RCNN, XML-CNN, LEAM, and LSAN), it is obvious that SMGCN outper-forms them by a significant margin on two large-scale datasets. These methods mainly focus on learning better query and label representations but ignore the complexity of real industrial appli-cations. Industrial training datasets frequently exhibit class imbal-ance, data distribution is often dominated by popular categories and the stability of data labels remains precarious because of the randomness of user behavior. Consequently, their efficacy may be significantly undermined if they were to be employed directly in the context of online E-commerce applications.\nCompared with recently proposed query intent classifica-tion methods (i.e., CNN, PHC, DPHA, and MMAN), SMGCN also achieves better performance on both datasets. As the results are shown in the table, the recall of relevant categories obtains nearly"}, {"title": "4.5.2 Ablation study", "content": "To further figure out the relative importance of each module in the proposed model, we perform a series of ablation studies over the different components of SMGCN. Three variants of SMGCN are listed below:\nw/o simi. graph: Removing the graph constructed through the semantic similar relations between category pairs. Only use the co-occurrence graph and semi-supervised strategy for query intent prediction.\nw/o coo. graph: Removing the graph constructed through the co-occurrence relations between category pairs and us-ing the similarity graph with the semi-supervised strategy for query intent prediction.\nw/o graph: Removing both co-occurrence and similarity graphs only uses the semi-supervised strategy with BERT for intent prediction.\nBERT: Removing all modules and only remaining BERT as text encoder for query intent classification."}, {"title": "4.6 Online Evaluation", "content": "To reduce the response latency of online deployment, the text encoder of the SMGCN is distilled from the"}, {"title": "4.6.2 Online Performance", "content": "Before being launched in production, we routinely deployed the SMGCN online on the JD search engine and made it randomly serve 10% traffic as the test group. For a fair comparison, we also build a base group that using the previous model (4-layer BERT) serves 10% traffic. During the A/B testing period, we monitor the performance of SMGCN and compare it with the online model. This period lasts for at least one week."}, {"title": "4.7 Parameter Sensitivity", "content": "Four major hyper-parameters may influence the performance:\nThe maximum length of query and category;\nthe threshold \u03c4 and \u03b1. We conduct some sensitivity analysis experiments to study how different choices of hyper-parameters influence the performance of the SMGCN. Due to space limitations, we only show the results on the category dataset.\nImpact of the maximum length. Figure 4 (a) and (b) illustrate the performance with different query and category lengths. The length has a significant influence on the prediction per-formance. When the tweet is too short, it cannot provide enough information for classification. Therefore, the perfor-mance improves as the growth of length. We observed that the best max length of the query is about 16 and the best max length of the category input is about 20.\nImpact of threshold. Figure 4 (c) and (d) illustrate the perfor-mance of SMGCN with different \u03c4 and \u03b1. \u03c4 determines how many soft labels would add to loss and a decides how many"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "This paper proposes a semi-supervised multi-channel graph convo-lutional network to address the challenges of category imbalance and incomplete recall of categories. SMGCN extends category infor-mation and enhances the posterior label by utilizing the similarity score between the query and categories. Additionally, it leverages the co-occurrence and semantic similarity relations among cat-egories to strengthen the relations between labels and weaken the influence of posterior label instability. Offline and online A/B experiments demonstrate significant improvements over the state-of-the-art methods. Moreover, the proposed approach has been deployed in real-world applications and has brought great commer-cial value, confirming its practicality and robustness for large-scale query intent classification services.\nIn future work, we aim to investigate the use of external knowl-edge, such as the taxonomic hierarchy of categories and product information, to comprehensively model category representations and further enhance the model's performance."}]}