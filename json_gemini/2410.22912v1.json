{"title": "Self-optimization in distributed manufacturing systems using Modular State-based Stackelberg Games", "authors": ["Steve Yuwono", "Ahmar Kamal Hussain", "Dorothea Schwung", "Andreas Schwung"], "abstract": "In this study, we introduce Modular State-based Stackelberg Games (Mod-SbSG), a novel game structure developed for distributed self-learning in modular manufacturing systems. Mod-SbSG enhances cooperative decision-making among self-learning agents within production systems by integrating State-based Potential Games (SbPG) with Stackelberg games. This hierarchical structure assigns more important modules of the manufacturing system a first-mover advantage, while less important modules respond optimally to the leaders' decisions. This decision-making process differs from typical multi-agent learning algorithms in manufacturing systems, where decisions are made simultaneously. We provide convergence guarantees for the novel game structure and design learning algorithms to account for the hierarchical game structure. We further analyse the effects of single-leader/multiple-follower and multiple-leader/multiple-follower scenarios within a Mod-SbSG. To assess its effectiveness, we implement and test Mod-SbSG in an industrial control setting using two laboratory-scale testbeds featuring sequential and serial-parallel processes. The proposed ap-", "sections": [{"title": "1. Introduction", "content": "In modern industry, the integration of Industrial Cyber-Physical Systems (ICPS), automation, and artificial intelligence (AI) forms a transformative technology paradigm that improves manufacturing systems. ICPS combines computational and physical processes to optimize performance, while automation reduces human intervention, and AI enables machines to make autonomous decisions by learning from past experiences. Recent studies [1, 2] highlight the significant impact of these integrations, which results in improved efficiency, productivity, and adaptability. In the current industrial landscape, distributed manufacturing systems have gained popularity due to their decentralized nature, which offers greater flexibility and scalability [3, 4]. These systems, often modelled as multi-agent systems, involve numerous control variables with complex interrelations. Applications range from distributed control with industrial robots [5] to process optimization in material extrusion [6]. Key advantages include improved flexibility, adaptability, fault detection, plug-and-play functionality, and responsiveness to dynamic production demands. However, challenges in coordination, optimization, and adaptability remain, which necessitate innovative solutions as system complexity increases.\nThe introduction of AI-driven self-optimization has revolutionized distributed manufacturing systems, which enables autonomous learning and real-time adaptability. These self-learning systems improve decision-making by considering past experiences or historical data, which results in reduced downtime, lower energy consumption, better resource utilization, and overall efficiency gains. Machine"}, {"title": "2. Literature review", "content": "This section focuses on a discussion of preliminary research on self-learning manufacturing systems using AI and Stackelberg games for engineering applications."}, {"title": "2.1. Self-learning manufacturing systems using artificial intelligence", "content": "Recent advancements in automation have rapidly transformed manufacturing systems, with AI playing a key role in improving operational efficiency through real-time decision-making. This synergy marks a new era in manufacturing, where AI-driven systems adapt swiftly to dynamically changing production demands and configurations that lead to a more responsive and agile industrial landscape. A significant contribution of AI is in enhancing quality control, which improves precision in inspection and early defect detection using computer vision [22] and machine learning [23]. Beyond quality control, AI also optimizes production planning [24, 25], including scheduling, demand forecasting, resource allocation, and inventory management. Moreover, AI proves highly effective in robotics [26] and Human-Robot Collaboration [27]. The integration of AI in manufacturing not only reduces operational costs but also enhances safety and energy efficiency.\nFurthermore, AI-based systems have self-learning capabilities, which enable continuous optimization by adapting to changing data and conditions. This research focuses on distributed self-optimizing manufacturing systems. Prior studies have integrated machine learning into self-learning frameworks, including multi-agent RL [21], PLC-policy combinations with machine learning [10, 28], model predictive control enhanced by adaptive PLC-policy via model-based deep learning [8], and dynamic GT [13]. Among these, the GT-based approach is the most applicable due to its robustness, proven convergence, and efficient computational time. Particularly, GT is well-suited for distributed multi-agent manufacturing systems, which involve multiple independently trained control variables. It facilitates suboptimal cooperation among agents to maximize global"}, {"title": "2.2. Stackelberg games for engineering applications", "content": "Stackelberg games [17] involve a sequential decision-making process, which contrasts with simultaneous games. In these games, the leader makes the initial move, followed by followers who react based on the leader's actions. While typically associated with non-cooperative GT, known as Stackelberg competition [30], these games can also be adapted for cooperative scenarios. This interaction aims to achieve global objectives or Stackelberg equilibria [19]. As GT's application in engineering expands, the use of Stackelberg games has similarly increased. Several examples include their deployment in anti-jamming defence for wireless networks [31], power control communications [32], addressing security issues in networked control systems as defender-attacker games [33], and developing Stackelberg Actor-Critic methods in RL [34].\nDespite the wide-ranging applications of Stackelberg games in engineering, their utilization in self-learning manufacturing systems remains limited. Therefore, this research aims to address this gap by integrating Stackelberg games into the self-learning domain to enhance collaboration among players. The sequential decision-making characteristic of Stackelberg games sets them apart from other game types, such as dynamic potential games [13] or multi-agent RL [21], where agents select actions simultaneously. This distinctive feature becomes a focal point in exploring the benefits of Stackelberg games in this domain. Currently, no strategic game structure, apart from SbPG [15], supports distributed self-learning algorithms. However, SbPG also operates on simultaneous actions, which can result in unequal treatment of critical players. To"}, {"title": "3. Problem description", "content": "This section outlines the problem addressed in this study, which focuses on developing autonomous optimization methods for fully distributed manufacturing systems. Specifically, we focus on modular systems comprising multiple subsystems, each with its own local control system and potentially distinct objectives, as depicted in Fig. 1. These subsystems are interconnected through either parallel or serial-parallel configurations and interact with their control systems via the exchange of local signals. Each subsystem contains one or more actuators, which can be conceptualized as individual players i in GT terms. Our main goal is to facilitate self-optimization across these systems in a distributed manner, thereby eliminating the requirement for centralized control and allowing for flexible, scalable, and reusable operations across diverse modules through instantiation."}, {"title": "4. Modular State-based Stackelberg Games", "content": "In modular production units, players that significantly impact the outputs of their surroundings and the global objective (potential) function are considered more critical. Typically, these critical players are positioned higher in the hierarchy, often serving as leaders. Consequently, treating all players equally as is current state of the art, may not be the most effective approach. Contrary, we propose to assign each player a role as either a leader or a follower, with leaders being the more critical players. This results in a group of leader and a group of follower modules.\nConsequently, we propose the Mod-SbSG as a game structure composed of three key sub-games: (1) a cooperative game for the group of leader modules, (2) a cooperative game for the group of follower modules, and (3) a hierarchical game governing the interactions between the leader and follower groups.\nSpecifically, for the cooperative games of leader and follower groups, we propose to set up an SbPG among each group, which has been proven effective and convergent for distributed systems in [15]. The hierarchical game describing the interactions between the leader and follower groups is addressed using a Stackelberg game. This game structure is detailed in the following subsections."}, {"title": "4.1. SbPG for leader and follower groups", "content": "We propose to use SbPG for the coordination game within the group of leaders and followers respectively. Potential games [39] provide a game structure for modelling and studying strategic interactions between rational players, where each player's utility $U_i$ is influenced by both their actions and the state of the environment. These interactions are then assessed by a scalar potential function $\\phi$, which acts as a global objective. SbPG [12] extend this framework by explicitly including state information in the players' strategic decision-making process.\nSbPGs are further extended in [15] to manage self-optimizing modular production units by incorporating the set of states $S$ and the state transition process"}, {"title": "4.2. Leader-follower game as a Stackelberg game", "content": "Stackelberg games [17] model strategic interactions where one or more players act as leaders, and the others as followers. Unlike simultaneous-move games, Stackelberg games feature a sequential decision-making process. The leader makes the first move, followed by the followers who react sequentially based on the leader's actions. This hierarchical structure gives the leader a strategic advantage, which allows them to optimize their decisions and maximize their objectives by anticipating the predictable responses (best responses) of the followers. Subsequently, the follower recognises the leader's decisions and formulates responses based on their own strategic considerations. Although often applied in non-cooperative games, hierarchical decision-making in Stackelberg games can be adapted to cooperative frameworks. Techniques like dynamic programming, variational inequalities, and Stackelberg equilibrium concepts [19] are commonly used to analyze and solve for equilibrium outcomes in these settings.\nHence, after appointing the players as leaders and followers and managing the SbPG for each group, we propose to manage the interaction between the"}, {"title": "4.3. Overall game structure", "content": "After defining the individual games of leader and follower as well as the Stackelberg game to connect these two groups, we formulate the game structure of Mod-SbSG as below:\nDefinition 4. A game$\\Gamma(N, L, F, A, \\{u_i\\}, S, P,\\{\\Phi_L, \\Phi_F\\})$ is called a Mod-SbSG, if the decision-making within leaders and followers is governed by the two SbPGs $\\Gamma_L(L, A_L, \\{U_l\\}, S, P, \\Phi_L)$ and $\\Gamma_F(F, A_F, \\{U_f\\}, S, P,\\Phi_F)$, while interactions between leaders and followers are modelled as Stackelberg game $\\Gamma_S(N, A, \\Phi_L,\\Phi_F)$."}, {"title": "5. Convergence analysis", "content": "After the definition of Mod-SBSG, we now focus on the convergence properties of the overall game structure. To this end, we have to consider the different game structures, namely SbPG as well as Stackelberg games. More specifically, we first analyse the convergence properties of the SbPG, and subsequently, the convergence properties of the Stackelberg game under the assumption, that the SbPG converged to their respective equilibria.\nFor SbPG, there already exists a line of results with respect to their convergence properties. Specifically, it has been shown that exact potential games converge to a Nash equilibrium under best-response dynamics [12]. Under some mild conditions, this result can be expanded to SbPG. Particularly, Zazo et al. [13] establish criteria for proving the existence of an SbPG and demonstrate that it converges as long as these conditions are fulfilled. Based on these results, Schwung et al. [15] provide assumptions on the design of the utility functions, such that the distributed optimization of modular production units can be cast as an SbPG from which convergence guarantees follow directly.\nAs we employ the exact same utility functions as in [15] fulfilling their Assumptions 1-4, we can state the following theorem for the leaders' coalition game:\nTheorem 1. Given Assumptions 1-4 from [15], the cooperative game between leaders, $\\Gamma_L(L, A_L, \\{U_l\\}, S, P, \\Phi_L)$, as defined in Def. 1 constitute an SbPG.\nProof. The proof follows directly from Proposition 1 in [15].\nSimilar to the leaders' game, we can state the following theorem for the followers' coalition game:\nTheorem 2. Given Assumptions 1-4 from [15], the cooperative game between followers, $\\Gamma_F(F, A_F, \\{U_f\\}, S, P,\\Phi_F)$, as defined in Def. 2 constitute an SbPG.\nProof. The proof follows directly from Proposition 1 in [15]."}, {"title": "6. Learning dynamics", "content": "After developing the game structure and discussing its convergence, we have to derive suitable learning algorithms for training the policies of each player, $\\pi_l, \\pi_f \\in \\pi_i$. To this end, we consider the learning dynamics induced by both the SbPG and the Stackelberg game.\nIn [15], we initially proposed best-response learning for the SbPG structure, which utilizes ad-hoc random uniform sampling during the learning process. However, this random sampling approach led to lower predictability and potential instability in the learning process, as it lacked control over the learning direction. To address this issue, we improved the method by proposing gradient-based learning in [20], which provides guided learning and enables more stable convergence toward global optima compared to random sampling.\nBoth of these methods were originally designed for simultaneous games. However, Mod-SbSG introduces a hierarchical structure of leaders and followers within the player set $N$, which necessitates a more complex dynamic. Given the more complex game structure, we have to address three key steps:\n1. the learning algorithm used for both leader and follower SbPG,\n2. the Stackelberg updates between the coalition strategies of the leaders and followers, and"}, {"title": "6.1. Policy representation using performance maps", "content": "We begin by considering the representation of each player's policy $\\pi_i, \\pi_f \\in \\pi_i$, which is responsible for storing the learned knowledge over various state-action pairs. In SbPG, the state space is discretized into equidistant support vectors, denoted by $q = 1,...,p$, which store the best-explored actions and their corresponding utility values for each state combination. Additionally, a stack of selected actions and their utilities is stored within each data point across different state combinations, as suggested in [20]. Fig. 4 displays the performance map for each player $i$ in a system characterized by two states, $x$ and $y$.\nEach player $i$ determines the next action $a_{i,t+1}$ by globally interpolating their performance map based on the current state $s_{i,t}$ [15]. The global interpolation process is as follows:\n$$\nw_i^{s_0} = \\frac{1}{(d_{s^0s^q})^2 + \\gamma_{map}}\n$$\n$$\n a_i = \\sum_q \\frac{w_i^{s_0}} {\\sum_q w_i^{s_0}} \\cdot a_i^q,\n$$"}, {"title": "6.2. Learning update rule", "content": "Once the policy representation using performance maps is established, the next step is to update the action values in the support vectors and train the policies by designing a suitable training law. As previously mentioned, in the first update step of Mod-SbSG, both followers and leaders individually play SbPG using the gradient-based learning approach proposed in [20]. Following this, the second step involves defining the Stackelberg rule within Mod-SbSG for allowing hierarchical interactions between both roles, which is elaborated further in this subsection.\nWe start by deriving the Stackelberg rule for the leader $l$. Each action in the performance map's state combinations $a_l^p$, is adjusted based on its potential function $\\phi_l^p$ and the follower's potential function $\\phi_f^p$, which uses deterministic learning techniques, as follows:\n$$\na_{l,p+1} = a_l^p + \\alpha\\cdot \\omega_l + \\gamma_{l,ou},\n$$\nwhere $\\omega_l$ represents the gradient vector for learning in (9):\n$$\\omega_l = \\frac{\\frac{\\partial \\phi_L}{\\partial a_l}}{(\\frac{\\partial^2 \\phi_L}{\\partial a_l^2}) - (\\frac{\\frac{\\partial^2 \\phi_f}{\\partial A_F \\partial a_l}}{\\frac{\\partial^2 \\phi_f}{\\partial A_F^2}})\\cdot\\frac{\\partial^2 \\phi_L}{\\partial A_F}} \n$$\nwhere $\\phi_L$ and $\\phi_F$ are the approximations of $\\Phi_L$ and $\\Phi_F$, respectively.\nNext, we derive the Stackelberg rule for the follower $f$, who responds optimally to the leader's actions. The update rule is formulated as follows:\n$$\na_{f,p+1} = a_f^p + \\alpha \\frac{\\partial \\phi_s}{\\partial a_f} + \\gamma_{f,ou}\n$$\nIn the above equations, $\\gamma_{l,ou}$ and $\\gamma_{f,ou}$ represent an Ornstein-Uhlenbeck (OU) noise term used during exploration. Since the gradient field for the follower mirrors that of the SbPG, we can utilise the gradient-based learning procedures as detailed in [20]."}, {"title": "6.3. Approximation of gradient descent", "content": "In practical production environments, the players in Mod-SbSG generally acquire data on the resulting potential values from their actions, but no explicit functional relationships between these values are specified, as the potential functions are inherently embedded within the system. However, to carry out the gradient updates, it is essential to have a defined functional relationship. Therefore, we approximate the potential functions, since precise potential information is required to direct the learning gradient effectively. In this study, we use polynomial regression, as proposed in [38], which is effective for continuous gradient updates. The potential function approximations for leaders and followers are given by:\n$$\\Phi_l(a_l, A_F) = \\beta_0 + \\beta_1 a_l + \\beta_2 A_F + \\beta_3 a_l^2 + \\beta_4 A_F^2 + \\beta_5 a_lA_F + ... + \\beta_{n+2} a_l^nA_F,\n$$"}, {"title": "6.4. Multi-step updates for followers", "content": "In contrast to simultaneous learning, Mod-SbSG requires an alternating training methodology for leaders and followers. In simultaneous games like SbPGs, all players update their policies concurrently at each time step. Meanwhile, in Stackelberg games, according to Theorem 3, followers are required to converge before the next update of the leader group. Hence, a multi-step update of the follower is required while leaders generally optimize their strategies in a single step. However, waiting for full convergence of followers' strategies in each training iteration can be impractical, particularly due to the extensive state spaces commonly found in manufacturing systems, which can result in excessively long training times. A feasible solution is to limit the number of gradient updates for followers per training step.\nTo this end, we propose to regulate the multi-step update rates for followers by introducing three different variants, such as:\n1. Static number of update steps\n2. Gradient magnitude thresholding for gradient-based learning\n3. Gradual reduction method for ad-hoc learning\nIn the first approach, we set a parameter $\\Theta^{static}$ to specify the number of update steps for followers during each training iteration. This parameter remains fixed throughout the training process. However, a limitation of using a static number of update steps is that, as the training approaches the optimal solution, excessive exploration and updates by followers become less impactful, which leads to lengthy training times with diminishing returns.\nIn the second approach, we employ a dynamic number of update steps by utilizing gradient magnitude thresholding, which is particularly effective for"}, {"title": "6.5. Learning mechanism", "content": "As depicted in Fig. 3, Mod-SbSG is composed of three interconnected games. In this subsection, we explain the learning mechanism of Mod-SbSG within a dynamic system, thereby composing the derivations from the previous section.\nWe assume that $t$ represents the system's time step, and each player $i$ must update their action $a_{it}$ at each time step. However, decision-making in Mod-SbSG is role-dependent. At each time step $t$, each player $i$ first acquires the current state $s_{i,t}$ from the environment. Next, each leader $l \\in i$ selects an action $a_{l,t}(s_{i,t})$ based on the current state, engaging in an SbPG among the leaders, which results in the coalition strategy $A_l$ for the leaders. Each follower $f \\in i$ then responds with an action $a_{f,t}(s_{f,t}, A_l)$, based on both the current states and the leaders' coalition strategy. The followers also engage in SbPG, forming the coalition strategy $A$. Both coalition strategies $A_l$ and $A_f$ are then combined"}, {"title": "7. Results and Discussions", "content": "In this section, we present the results and analysis of the proposed Mod-SbSG in two different testing environments with three industrial settings. We evaluate its performance by embedding it into two different learning algorithms: (1) a globally interpolated gradient-based learning method [20] and (2) the A2C algorithm [35] from the RL domain. Additionally, we conduct an ablation study to examine the impact of varying the number of followers' update steps and the differing focuses between leaders and followers."}, {"title": "7.1. Testing environments", "content": "We implemented the proposed game structure of Mod-SbSG to two laboratory test belts, such as the Bulk Good Laboratory Plant (BGLP) and its larger-scale counterpart (LS-BGLP). The LS-BGLP includes a larger number of actuators and state variables, which results in a significantly higher number of players compared to the BGLP. Additionally, validation experiments were conducted on the LS-BGLP under two different industrial settings, which are sequential processes, similar to the default BGLP configuration, and serial-parallel processes.\nMoreover, the BGLP and LS-BGLP environment simulation is available"}, {"title": "8. Conclusions", "content": "We introduce a novel game structure, Mod-SbSG, designed to facilitate leader-follower configurations in a distributed manner, and adaptable to various self-learning algorithms. This structure emphasizes self-optimization in multi-agent modular manufacturing systems and comprises three different games, including an SbPG among leaders, an SbPG among followers, and a Stackelberg game for leader-follower interactions. The effectiveness of Mod-SbSG is"}]}