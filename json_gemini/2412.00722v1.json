{"title": "Towards Adaptive Mechanism Activation in Language Agent", "authors": ["Ziyang Huang", "Jun Zhao", "Kang Liu"], "abstract": "Language Agent could be endowed with different mechanisms for autonomous task accomplishment. Current agents typically rely on fixed mechanisms or a set of mechanisms activated in a predefined order, limiting their adaptation to varied potential task solution structures. To this end, this paper proposes Adaptive Language Agent Mechanism Activation Learning with Self-Exploration (ALAMA), which focuses on optimizing mechanism activation adaptability without reliance on expert models. Initially, it builds a harmonized agent framework (UniAct) to Unify different mechanisms via Actions. Then it leverages a training-efficient optimization method based on self-exploration to enable the UniAct to adaptively activate the appropriate mechanisms according to the potential characteristics of the task. Experimental results demonstrate significant improvements in downstream agent tasks, affirming the effectiveness of our approach in facilitating more dynamic and context-sensitive mechanism activation.", "sections": [{"title": "1 Introduction", "content": "Language Agent (LA) (Sumers et al., 2024; Yao et al., 2023; Xi et al., 2023; Gao et al., 2023) has garnered considerable attention recently due to the rapid advancements in Large Language Models (LLMs) (OpenAI, 2024; AI@Meta, 2024; Yang et al., 2023; Chowdhery et al., 2022; Radford et al., 2018). Through the well-designed prompts and carefully selected in-context demonstrations (Zhou et al., 2024; Dong et al., 2023; Liu et al., 2021), LLM-based agents can be endowed with different mechanisms\u00b9 for environment interaction and task solving. Existing LAs could benefit from distinct mechanisms for various tasks with unique solution structures (Zhou et al., 2024). For instance, Reflexion (Shinn et al., 2023) is equipped with Reflection mechanism to gain insightful refinement suggestions. And ReAct (Yao et al., 2023) is equipped with External-Augmentation mechanism to ground the solution trajectory with additional evidence.\nDespite the success of current LAs through aforementioned direct prompting and in-context learning, named as manual mechanism activation, they rely on fixed mechanisms or a predefined sequence of mechanisms (Liu et al., 2023; Chen et al., 2023; Song et al., 2024), as illustrated in Figure 1 (a). As a result, such rigidity hampers activating the optimal solution structures (mechanism) for a specific task and also limits their adaptability to open-world scenarios. There is compelling evidence that oracle language agent mechanism activation, selecting the most appropriate mechanism for a task, can improve the performance by over 15% compared to fixed mechanism baselines (as shown in Section 5.1). Therefore, it highlights the significant potential of adaptive mechanism activation, the focus of the paper, where mechanisms are adaptively activated based on task characteristics, as shown in Figure 1 (b). We view this as a critical kind of meta-ability for LAs, and its enhancement could offer the potential for better generalization in unseen tasks.\nIntuitively, when humans encounter unfamiliar tasks, they tend to first explore different solution strategies and then select the most effective solution from previous experiences in similar tasks. Inspired by this, to enable LAs to adaptively select suitable solution strategies (adaptive mechanism activation), this paper proposes Adaptive Language Agent Mechanism Activation Learning with Self-Exploration (ALAMA), a novel technique for learning adaptive mechanism activation across various tasks. It first introduces a harmonized agent framework to Unify existing known mechanisms by Actions (UniAct). Compared with previous agents which did not fully integrate various mechanisms (Yao et al., 2023) or only implicitly incorporated specific mechanisms into the thinking process without an explicit trigger (Zhou et al., 2023), UniAct defines the workflows of mechanisms as specific actions. In this way, different mechanisms would share a unified action space. When the agent triggers an action, the corresponding mechanism is expected to be activated.\nSecondly, to fulfill adaptive mechanism activation in LAs, our ALAMA adopts a self-exploration fine-tuning way rather than simply prompting. Sufficient high-quality trajectories with different activated mechanisms are important for model training but not easy to acquire. To this end, ALAMA firstly leverages self-exploration to obtain sufficient trajectories for training. Compared with previous methods of acquiring trajectories through manual annotation or distillation from proprietary models (Zeng et al., 2023; Chen et al., 2023), self-exploration could extremely decrease data acquisition costs and alleviate the paucity of training signals. Specifically, we manually activate different mechanisms to facilitate multiple rounds of self-exploration. Consequently, diverse solution trajectories are produced and then converted into the UniAct format. To introduce implicit mechanism preferences towards different tasks as well as fundamental interaction and instruction-following capabilities for the agent, this paper utilizes Implicit Mechanism Activation Optimization (IMAO), which samples"}, {"title": "2 Background", "content": "With different prompts and demonstrations, the agent can be equipped with different mechanisms for better task-solving performance. This paper selects five essential agent mechanisms as the focus of our study: (1) Reason (Wei et al., 2022): Directly obtaining the answer through step-by-step reasoning. (2) Plan (Wang et al., 2023a; Zhou et al., 2023): First understanding the task and develop a plan to decompose it into smaller, more easily solvable sub-tasks, and then progressively solving each sub-task to arrive at the final answer. (3) Memory (Sun et al., 2023; Gao et al., 2024): Initially building a database of failed examples. During each subsequent task execution, similar cases are retrieved from this database based on task similarity (cosine of task description em-"}, {"title": "3 ALAMA: Adaptive Language Agent Mechanism Activation Learning with Self-Exploration", "content": "This section describes our method in detail. Firstly, we introduce a harmonized agent framework to unify existing known mechanisms (UniAct). Secondly, we elaborate on a self-exploration fine-tuning method for enhancing the meta-ability of adaptive mechanism activation. In specific, we leverage Self-Exploration with manual mechanism activation to sample various UniAct trajectories. Next, we employ Implicit Mechanism Activation Optimization (IMAO) and Mechanism Activation Adaptability Optimization (MAAO) to adapt the agent to different tasks based on the recognized characteristics and potential solution structures.\nUniAct: Unify Agent Mechanisms by Actions Currently, ReAct (Yao et al., 2023) serves as the foundational framework for LLM-based agents, employing the Thought, Action, Observation (r, a, o as the abbreviation) format to govern agent control. This format only unifies reasoning, action generation, and the acquisition of feedback from external environments into natural language space. Based on this, we propose UniAct to integrate diverse mechanisms into a unified framework explicitly. As depicted in the upper of Figure 3 (a), we define distinct Actions for each mechanism to unify the different workflows into a shared action space. Specifically, we define make_plan for detailed plan generation, Carry_out_plan for plan execution, Retrieve_memory to get potential error cases, Reflect to get insightful correction suggestions from the expert Critic model, Call_tool to invoke external tools, and Finish to output the final results and terminate the solution trajectories. We take the Thought as the thinking process before generating the actions, and the Observation as the environmental feedback. Furthermore, we have adapted the external environment to not only provide task-related feedback but also return appropriate prompt to facilitate the activation of corresponding mechanisms. Details regarding the Uni-Act format including the actions and corresponding grounding prompts are provided in Appendix F.\nSelf-Exploration We refer to the base agent with parameter \u03b8 as LA\u03b8 and all the mechanisms as M = {mi}N i=1. We construct a demonstration trajectory di where only that specific mechanism mi is activated. As shown in upper of Figure 3 (a), given Tasks T = {tj}S j=1, we manually activate different mechanisms by prompting with the corresponding di to get the exploration solution trajectory si,j and corresponding reward ri,j. And then we transform all these trajectories into UniAct format ui,j.\nsi,j,ri,j = LA\u03b8(di,tj) (1)\nUi,j = UniActTransform(si,j)\n= (\u03c41, o1, a1, \u2026\u2026\u2026, Om\u22121, \u03c4m, am)i,j (2)\nwhere \u03c4, a, o represent thought, action, and observation respectively. For UniActTransform(\u00b7), we extract the self-generated contents and external feedback from the ICL solutions, and then fill them into the UniAct format with explicit actions. As depicted in the bottom part of Figure 3 (a), we show a transformation example of Plan. Please refer to the Appendix E for other mechanisms. Finally, we get all self-exploration UniAct trajectories U.\nU = {Ui}N i=1 = {{Ui,1}S j=1,..., {Ui,|T|}S j=1} (3)\nNotably, not every mechanism could fit all tasks and obtain correct results. As illustrated in the upper of Figure 3 (b), certain tasks are successfully solved by specific mechanisms, while remaining unsolved when the other are activated. We refer to these as tasks with mechanism sensitivity.\nIMAO: Implicit Mechanism Activation Optimization To equip the model with the basic abil-"}, {"title": "4 Experiment", "content": "4.1 Setup\nModel and Datasets We utilize GPT-3.5-turbo-0125 as the closed-source model baseline, accessed through the OpenAI API. We employ Meta-Llama3-8B-Instruct as the backbone for ALAMA.\nFor datasets, the paper employs the GSM8K (Cobbe et al., 2021) and HotpotQA (Yang et al., 2018) as Held-in tasks for exploration, training, and testing. Additionally, we select NumGLUE (Mishra et al., 2022), SVAMP (Patel et al., 2021), TriviaQA (Joshi et al., 2017), and Bamboogle (Press et al., 2023) as Held-out tasks to evaluate the generalization performance. For dataset processing details, please refer to Appendix A.\nBaselines We select the following baselines for comparisons, like (1) Fixed single mechanism (Reason, Plan, Memory, Reflection and External-Augmentation shown in Table 1): we manually construct one in-context demonstration example to activate different mechanisms (2) Average: The average performance of different mechanisms. (3) Majority Voting: Selecting the most consistent (Wang et al., 2023b) answer among the solutions obtained by activating different mechanisms as the final answer. (4) Self-Adapt Consistency: We apply self-consistency (Wang et al., 2023b) technique to ALAMA. For training and inference details, please refer to Appendix B.\n4.2 Main Results\nAdaptive Mechanism Activation outperforms fixed Manual Mechanism Activation. As shown in Table 1, ALAMA outperforms all single mechanism baselines and the average performance of different mechanisms on the Held-in tasks. We consider the Average as the bottom performance for introducing multiple mechanisms into one agent. After IMAO (supervised learning), ALAMA surpasses the Average by 2.87 on GSM8K and 4.92 on HotpotQA, indicating that it has the ability to adaptively activate different mechanisms based on the task characteristics.\nFurthermore, after MAAO (preference learning), ALAMA continues to improve by 3.41 on GSM8K and 3.60 on HotpotQA. This suggests that MAAO can enhance the adaptability of the agent to potential solution structures of different tasks. Behavior contrastive learning enables the model to preferentially activate certain specific mechanisms while refusing to activate the remaining ones. For example, in manual activation, Plan outperforms Reason by 4.48 on GSM8K. After MAAO, when the agent encounters specific complex mathematical reasoning tasks that can not be solved directly through reasoning, it recognizes that direct reason-"}, {"title": "5 Analysis", "content": "5.1 The Specificity of Different Mechanisms\nThis subsection tries to investigate the impact of different mechanisms on downstream task performance. In detail, we choose Llama3-8B-Instruct (AI@Meta, 2024) as the vanilla agent and GSM8K as the agent task. As shown in Figure 4, only 42.61% tasks could be solved by all fixed single mechanism baselines. This result suggests that more than 50% of tasks are of mechanism sensitivity. For instance, certain tasks require external knowledge, while others may encounter conflicts upon the introduction of such knowledge. Consequently, we believe that different tasks possess distinct underlying solution structures. Moreover, the oracle mechanism activation results demonstrate that the model can solve 96.89% of the tasks with the aid of selecting the correct mechanism, highlighting that adaptive mechanism activation has a very high ceiling performance. This suggests a significant potential for identifying the inherent characteristics of tasks and their solution structures. Our ALAMA still falls short of the performance ceiling, which anticipates further optimization of the mechanism activation methods.\n5.2 The Effects of Mixing Different Mechanism Data\nTo investigate the impact of individual and mixed mechanisms data on the performance of the agent, we divided UIMAO and UMAAO based on different mechanisms. For UMAAO, we segment it according to the mechanisms activated by the positive examples, and incorporated all negative examples of the corresponding tasks into the training set. For IMAO, we employed Meta-Llama-3-8B-Instruct as the base model, whereas for MAAO, we utilized ALAMAIMAO as the base model.\nIn IMAO, we observed that fine-tuning the model using single-mechanism trajectories leads to underperformance, as the use of original data does not effectively enhance the performance under the zero-shot setting. We hypothesize this may be due to insufficient training data resulting from data segmentation. After sampling more data corresponding to the specific mechanisms for fur-"}, {"title": "6 Related Work", "content": "6.1 Language Agent\nTo achieve better autonomous task accomplishment, the research community has designed many Language Agent Frameworks like ReAct (Yao et al., 2023), Reflexion (Shinn et al., 2023), and Multi-Agent Debate (Du et al., 2023; Liang et al., 2023). However, these frameworks are labor-"}, {"title": "7 Conclusion", "content": "In this paper, we propose Adaptive Language Agent Mechanism Activation Learning with Self-Exploration (ALAMA). We observed that numerous tasks exhibit mechanism sensitivity. And the oracle mechanism activation exhibits stronger performance than fixed baselines. To this end, we unify different agent mechanisms by actions (UniAct) into a harmonized agent framework. Moreover, we utilize an adaptive mechanism activation optimization method based on self-exploration, which requires less data than previous SoTA agents and is training-efficient. Extensive experiments demonstrate the effectiveness and generalization of our proposed method. Further analysis shows that increasing the number of mechanisms and integrating trajectory data from different mechanisms are crucial for enhancing"}, {"title": "Limitations", "content": "In this paper, the discussion of adaptive mechanism activation is limited to the activation of a single mechanism and does not address the simultaneous activation of multiple mechanisms. Activating various mechanisms concurrently could offer additional benefits; however, it also increases the complexity of learning adaptive mechanism activation. Therefore, we consider this an area for future work to be explored subsequently. Moreover, in Section 5.2, we discuss only the effects of full data and single-mechanism data, omitting the impact of mixing data from different mechanisms. The five mechanisms discussed in this paper could lead to 25 \u2212 1 possible combinations, and our limited computational resources did not allow for the evaluation of all possibilities. We plan to incorporate these data in a formal version later for further discussion."}, {"title": "A Data", "content": null}, {"title": "B Training and Inference", "content": null}, {"title": "C Algorithm", "content": "Algorithm 1 ALAMA: Adaptive Language Agent Mechanism Activation with Self-Exploration\nRequire: M = {mi}N i=1; D = {di}N i=1; T = {tj}S j=1; LA\u03b8\n1: U, R \u2190 \u03c6 \u25b7 Initialize UniAct Trajectory and Reward set\n2: for i \u2190 1 to 5 do \u25b7 Self-Exploration\n3: for j \u2190 1 to |T| do\n4: si,j, ri,j \u2190 LA\u03b8(di, tj)\n5: Ui,j \u2190 UniActTrans(si,j)\n6: U.append(ui,j), R.append(ri,j)\n7: end for\n8: end for\n9: UIMAO, UMAAO-pos, UMAAO-neg \u2190 \u03c6 \u25b7 Initialize IMAO set and MAAO set\n10: for j \u2190 1 to |T| do\n11: if \u2200i \u2208 [1, 5], ri,j != 1 then\n12: pass\n13: else\n14: for i \u2190 1 to 5 do\n15: if ri,j == 1 then\n16: UMAAO-pos.append(ui,j)\n17: else\n18: UMAAO-neg.append(ui,j)\n19: end if\n20: end for\n21: end if\n22: end for\n23: UIMAO \u2190 UMAAO-pos\n24: Update LA\u03b8 with Implicit Mechanism Activation Optimization LIMAO on UIMAO\n25: Update LA\u03b8 with Mechanism Activation Adaptability Optimization LMAAO on UMAAO\n26: return LAfinal"}, {"title": "D Implementation of Different Mechanisms", "content": "Existing works have significantly enhanced the ability of LLM to solve different tasks through different prompting methods. For example, CoT (Wei et al., 2022) can improve reasoning ability, and Reflexion (Shinn et al., 2023) can enhance the ability to find errors and self-repair. These different prompting methods can endow the Agent based on LLM with different capabilities to adapt to different task environments. We regard these different capabilities as different mechanisms of the Agent and believe that endowing the Language Agent with different"}, {"title": "E UniActTransform", "content": "The corresponding extracted contents descripted below are filled into the UniAct format in Appendix F.\nReason: We extract the thought and answer from the ICL trajectories and fill them into the UniAct format.\nPlan: We extract the plan, thought and answer from the ICL trajectories and fill them into the UniAct format.\nMemory: We retrieve the failed case and extract the thought and answer from the ICL trajectories and fill them into the UniAct format.\nReflection We extract the first-generated thought, reflection reviews from the expert Critic model, and second-generated thought and corresponding answer to fill into the UniAct format.\nExternal Augmentation: We extract the external tool output (calculator results or search engine results) to fill into the UniAct format."}, {"title": "F Prompt of UniAct", "content": "We show the UniAct format template used in this paper. We show the system, Reason, Plan, Memory, Reflection, External-Augmentation prompt for math-metical reansoning and knowledge-intensive reansoning tasks in Table 6-11 and Table 12-17."}, {"title": "system", "content": "You are an agent that has five important mechanisms for solving a problem: Reason, Plan, Augmentation, Reflection, Memory.\nReason: The agent will do reasoning to solve a problem step by step.\nPlan: The agent will devise a detailed plan and then carry out the plan step by step to solve the problem\nAugmentation: The agent will interleave the reasoning and action to solve the problem. The action will call the Calculator for more precise numerical calculation.\nReflection: After reasoning, the agent will reflect on the previous reasoning and corresponding answer and get critic reviews. Based on the reviews, the agent will refine its reasoning and answer again.\nMemory: The agent has a memory database of failed reasoning trajectories. For each question, the agent will retrieve failed case from the memory as the reference to avoid such type of errors.\nYou can use these mechanisms to solve problems.\nYou have to think and solve the problem step-by-step with interleaving Thought, Action, Observation steps.\nThought is your reasoning process.\nAction could be:\nMake plan: The agent will devise a detailed plan and then carry out the plan step by step to solve the problem.\nCarry out plan: The agent will carry out the plan step by step to solve the problem.\nReflect: The agent will reflect on the previous reasoning and corresponding answer and get critic reviews. Based on the reviews, the agent will refine its reasoning and answer again.\nRetrieve memory: The agent will retrieve failed case from the memory as the reference to avoid such type of errors.\nCalculate: The agent will call the Calculator for more precise numerical calculation. Using format:\nCalculate[the expression]\nFinish: The agent will give the final answer.\nObservation is the feedback from the environment.\nThe answer needs to be a number and cannot contain other characters.\nYour response should use the following format:\nThought: <your thoughts>\nAction: <your next action>"}, {"title": "Reason", "content": "Environment: Task: {task}\nAgent: Thought: {thought} Action: Finish[{answer}]"}, {"title": "Plan", "content": "Environment: Task: {task}\nAgent: Thought: Devising a detailed plan before solving this problem may be helpful. Action: Make plan\nEnvironment: Observation: Let's first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan.\nAgent: Thought: My plan: {plan} Action: Carry out plan\nEnvironment: Observation: Let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step.\nAgent: Thought: {thought} Action: Finish[{answer}]"}, {"title": "Memory", "content": "Environment: Task: {task}\nAgent: Thought: Based on the give question, I will retrieve from my memory to get a similar question and the corresponding wrong solution as the reference to avoid such type of errors and solve the given problem step by step. Action: Retrieve memory\nEnvironment: Observation: Case: {case}. Try to avoid such types of errors.\nAgent: Thought: {thought} Action: Finish[{answer}]"}, {"title": "Reflection", "content": "Environment: Task: {task}\nAgent: Thought: {pre thought} Action: Reflect\nEnvironment: Observation: Reflection: {reflection} Based on the reflection reviews, please refine the thought and action.\nAgent: Thought: {post thought} Action: Finish[{answer}]"}, {"title": "External Augmentation", "content": "Environment: Task: {task}\nAgent: Thought: {thought} Action: Search[{entity}] or Lookup[{keyword}]\nEnvironment: Observation: {result}\nAgent: Thought: {thought} Action: Finish[{answer}]"}, {"title": "system", "content": "You are an agent that has five important mechanisms for solving a problem: Reason, Plan, Augmentation, Reflection, Memory.\nReason: The agent will do reasoning to solve a problem step by step.\nPlan: The agent will devise a detailed plan and then carry out the plan step by step to solve the problem\nAugmentation: The agent will interleave the reasoning and action to solve the problem. The action will call the Wikipedia Search for more precise knowledge.\nReflection: After reasoning, the agent will reflect on the previous reasoning and corresponding answer and get critic reviews. Based on the reviews, the agent will refine its reasoning and answer again.\nMemory: The agent has a memory database of failed reasoning trajectories. For each question, the agent will retrieve failed case from the memory as the reference to avoid such type of errors.\nYou can use these mechanisms to solve problems.\nYou have to think and solve the problem step-by-step with interleaving Thought, Action, Observation steps.\nThought is your reasoning process.\nAction could be:\nMake plan: The agent will devise a detailed plan and then carry out the plan step by step to solve the problem.\nCarry out plan: The agent will carry out the plan step by step to solve the problem.\nReflect: The agent will reflect on the previous reasoning and corresponding answer and get critic reviews. Based on the reviews, the agent will refine its reasoning and answer again.\nRetrieve memory: The agent will retrieve failed case from the memory as the reference to avoid such type of errors.\nSearch, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search. Using format: Search[entity]\nLookup, which returns the next sentence containing keyword in the current passage. Using format: Lookup[keyword]\nFinish: The agent will give the final answer.\nObservation is the feedback from the environment.\nYour response should use the following format:\nThought: <your thoughts>\nAction: <your next action>"}, {"title": "Reason", "content": "Environment: Task: {task}\nAgent: Thought: {thought} Action: Finish[{answer}]"}]}