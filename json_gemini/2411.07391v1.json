{"title": "Federated Learning Client Pruning for Noisy Labels", "authors": ["MAHDI MORAFAH", "HOJIN CHANG", "CHEN CHEN", "BILL LIN"], "abstract": "Federated Learning (FL) enables collaborative model training across decentralized edge devices while pre-serving data privacy. However, existing FL methods often assume clean annotated datasets, impractical for resource-constrained edge devices. In reality, noisy labels are prevalent, posing significant challenges to FL performance. Prior approaches attempt label correction and robust training techniques but exhibit limited efficacy, particularly under high noise levels. This paper introduces ClipFL (Federated Learning Client Pruning), a novel framework addressing noisy labels from a fresh perspective. ClipFL identifies and excludes noisy clients based on their performance on a clean validation dataset, tracked using a Noise Candidacy Score (NCS). The framework comprises three phases: pre-client pruning to identify potential noisy clients and calculate their NCS, client pruning to exclude a percentage of clients with the highest NCS, and post-client pruning for fine-tuning the global model with standard FL on clean clients. Empirical evaluation demonstrates ClipFL's efficacy across diverse datasets and noise levels, achieving accurate noisy client identification, superior performance, faster convergence, and reduced communication costs compared to state-of-the-art FL methods. Our code is available at https://github.com/MMorafah/ClipFL.", "sections": [{"title": "1 INTRODUCTION", "content": "Edge devices such as IoTs and mobile devices are increasingly ubiquitous, constituting a new computational platform for machine learning. Despite holding vast real-world data, these billions of devices often withhold their data due to privacy concerns. Federated Learning (FL) emerges as a decentralized machine learning paradigm, enabling model training with the collaboration of multiple clients while preserving privacy [15, 29]. FL shows promise in enhancing model performance without necessitating data sharing and finds applications across diverse domains [5, 12, 15, 33, 44]. However, FL encounters significant performance degradation in the presence of data heterogeneity"}, {"title": "2 RELATED WORKS", "content": "FL with Non-IID data. Federated learning encounters significant performance degradation and convergence challenges in the presence of data heterogeneity [45, 60]. To address this issue, numerous strategies have been explored. Some studies propose adjustments to local training methodologies to improve performance [22, 23, 30]. For instance, FedProx [23] introduces a proximal term during local training to penalize parameter differences from the global model. MOON [22] employs contrastive learning to preserve global knowledge during local training.\nOther works focus on alternative FL optimization procedures with improved convergence guarantees [10, 16, 17, 37, 49]. For example, FedNova [49] proposes a normalized weighted aggregation technique to address the objective inconsistency caused by data heterogeneity. FedOpt [37] introduces federated versions of adaptive optimizers such as Adam, AdaGrad, and Yogi, demonstrating significant performance improvements. Scaffold [17] focuses on correcting local updates and mitigating client drift by introducing control variates. Additionally, ensemble distillation techniques [27, 40] and neuron matching approaches [25, 28, 41, 48] have been proposed to address"}, {"title": "FL with noisy labels.", "content": "Several approaches have been explored in prior works to tackle the challenge of noisy labels in federated learning [26]. Some methods focus on identifying noisy clients and correcting noisy samples via the global model's predictions [52, 55]. For instance, FedCorr [52] introduces a multi-stage FL framework that identifies noisy clients by assessing the local intrinsic dimensionality (LID) of local model prediction subspaces. It identifies noisy labels by measuring local cross-entropy loss and correcting them with global model predictions. To ensure a reliable and well-performing global model, FedCorr employs an adaptive local proximal term added to the local training loss, mix-up data augmentation, and a client fraction scheduling scheme, where a smaller number of clients are selected without replacement. Yang et al. [55] form global class-wise feature centroids based on samples with relatively small losses to address noisy labels and correct less confident samples using the global model. However, these approaches heavily depend on a well-performing global model as a pseudo-labeler reference, which is challenging to obtain in the presence of data heterogeneity and noisy clients, leading to inaccurate label correction. Despite typically introducing noise reduction procedures prior to label correction, these methods still face significant challenges in achieving a well-trained, reliable global model. Zeng et al. [57] further highlight this issue and propose CLC (Consensus-based Label Correction), which aims to correct noisy labels using a consensus method among FL participants. CLC aggregates class-wise information from all participants to estimate the likelihood of label noise in a privacy-preserving manner, using consensus-defined class-wise thresholds and a margin tool to correct noisy labels.\nOther strategies aim to mitigate the adverse effects of noisy clients by proposing client weighting strategies and employing local training methods robust to noisy labels [8, 14, 56]. For example, RHFL [8] utilizes symmetric cross-entropy loss during local training and introduces a client confidence re-weighting scheme to alleviate the adverse effects of noisy labels during collaborative learning. In [56], a credibility-weighted aggregation scheme is proposed, where weights are computed based on the cross-entropy between the predictions of a server-side model trained on a benchmark dataset and the predictions made by local models on their respective local data. However, this approach necessitates the existence of a well-performing benchmark model. Jiang et al. [14] introduce a local self-regularization technique which comprises of a Mixup prediction loss that prevents over-fitting on noisy labels and a self-distillation loss that explicitly regularizes the model output discrepancy between original and augmented input images. Additional approaches such as client selection [54] and relevant data subset selection [46] have also been proposed to mitigate the impact of noisy labels in federated learning.\nHowever, despite these efforts, prior works often suffer from poor convergence and limited performance, particularly when dealing with large noise levels (see Section 5.3). In contrast, we tackle this problem from a different perspective by identifying noisy clients and pruning them from the federation. It is important to note that, in this context, the removal of noisy data does not necessarily result in the loss of useful information but rather in the elimination of detrimental noisy data that would otherwise degrade performance. Our approach offers a novel solution and new insights to the problem of noisy labels in federated learning, overcoming the limitations of previous methods, and reducing the communication cost."}, {"title": "3 BACKGROUND AND PROBLEM FORMULATION", "content": ""}, {"title": "3.1 Background", "content": "Vanilla Federated Averaging (FedAvg). Let's consider a scenario where we have a collection of clients S, each equipped with a local dataset Dk. These clients collaborate in a federated learning"}, {"title": "Label Noise.", "content": "Let's consider a dataset D : X \u00d7 Y, where X represents the feature space and Y denotes the set of actual labels, comprising K distinct labels. To introduce manual corruption into the dataset, we utilize a noise transition matrix following [50]. This matrix encapsulates the probabilities of true class labels being corrupted or mislabeled as other class labels during the labeling process. Formally, let T denote the symmetric transition matrix, where Tij represents the probability of an instance with true class i being observed as class j. The matrix T satisfies the conditions Tij = Tji and $\\sum_j T_{ij} = 1$, \u2200i. Two common approaches for label noise modeling are based on utilizing either a symmetric or asymmetric noise transition matrix [11, 42]. In the case of symmetric label noise modeling, to construct a noise transition matrix T with a noise level of \u03bc, we assign the diagonal elements to 1 \u2212 \u03bc, while the off-diagonal elements are set to $\\frac{\\mu}{K-1}$ to maintain symmetry."}, {"title": "3.2 Problem Formulation", "content": "Suppose a federated learning setting with N clients where each client k's local private data is denoted by Dk. Let S denote the set of entire clients. Moreover, the set of noisy clients is Sn, and the complement of set Sn is the set of clean clients: Sc = S \\ Sn and S = Sc \u222a Sn. The objective is solving the following minimization problem:\n$\\hat{\\theta} = \\underset{\\theta}{\\text{argmin}} \\sum_{i=1}^{N} E_{(x,y)\\sim D_i}[l(f(x; \\theta), y)]$, (1)"}, {"title": "4 PROPOSED METHOD", "content": ""}, {"title": "4.1 Overview", "content": "In this section, we introduce ClipFL, a method designed to address the challenges of federated learning with noisy labels. ClipFL operates within the standard federated learning framework (as described in Section 3) and does not necessitate any warm-up federated learning training rounds. Illustrated in Figure 3, ClipFL comprises of three distinct phases:\n\u2022 Phase I (Pre-Client Pruning): At the end of each round, the server evaluates the model of each client locally trained that round using a clean validation dataset to identify potential noisy client candidates. Specifically, we designate the top-m highest-performing clients"}, {"title": "4.2 Phase I: Pre-Client Pruning", "content": "The primary objective of this phase is to identify noisy clients in the normal federated learning process. At each round t, a randomly selected set of clients St receives the global model \u03b8t. After locally updating their models on their respective local datasets, the clients send back their updated models {\u03b8kt}k\u2208St to the server. To identify potential noisy clients, the server evaluates each received client k's (k \u2208 St) model on a clean validation dataset Dval and captures the accuracy Ak, denoted as Ak = Eval(\u03b8kt, Dval). Clean clients typically exhibit higher accuracy compared to noisy ones. Therefore, we first sort the clients St based on their validation accuracy A as follows:\n$S_t^\\text{s} = \\text{sort}(S_t, A)$. (2)\nNext, we consider m clients with the highest validation accuracy as clean clients ($S_t^c$), as follows:\n$S_t^c = {S_t^s [i] | 1 \\leq i \\leq m}$, (3)\nand the remaining |St| \u2212 m clients are considered noisy clients ($S_t^n$), as follows:\n$S_t^n = S_t^s \\setminus S_t^c$. (4)\nTo mitigate any negative impact from noisy clients on the server-side model aggregation and to prevent corruption of clean clients, only the identified clean clients are used for aggregation for the global model's update. This process iterates for Tpre rounds.\nThis phase serves as a crucial pre-processing step, allowing us to identify and isolate noisy clients candidates early in the federated learning process. By prioritizing clean clients candidates for model aggregation, we aim to enhance the quality of the global model and improve overall performance."}, {"title": "4.3 Phase II: Client Pruning", "content": "We observe that relying solely on validation accuracy for identifying noisy clients may not be sufficiently accurate, especially in scenarios with fluctuating client performance, such as data heterogeneity or early stages of federated training. For instance, in the early training stages, clean clients may exhibit lower performance due to insufficient training, leading to misidentification of said clean clients as noisy. Similarly, in the presence of data heterogeneity, low performance may stem from data variations rather than actual noise.\nTherefore, it is imperative to devise a robust metric for identifying noisy clients that can accommodate clients' performance fluctuations. To address this, we introduce the noise candidacy"}, {"title": "4.4 Phase III: Post-Client Pruning", "content": "In this phase, we proceed with further refinement of the global model after removing the identified noisy clients in Phase II. This refinement entails conducting normal federated training for an additional Tpost rounds. The fundamental distinction here lies in the composition of clients involved in the federated training process. Specifically, only the remaining un-pruned clients participate in this phase.\nTo elaborate, we adopt the standard federated learning approach, such as vanilla FedAvg or any other FL optimizer of choice, to fine-tune the global model over the un-pruned clients' datasets. This process ensures that the global model continues to benefit from the diverse data contributions"}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct a series of experiments to empirically evaluate the effectiveness of our proposed method. Our objective is to address the following research questions:\n\u2022 RQ1: How does ClipFL compare to SOTA FL optimizers without pruning?\n\u2022 RQ2: How does ClipFL compare to SOTA FL methods designed to handle noisy labels?\n\u2022 RQ3: What are the impacts of ClipFL's hyper-parameters on its performance?\n\u2022 RQ4: How does the performance of ClipFL vary with different noise levels?\n\u2022 RQ5: How does the performance of ClipFL vary using synthetic validation dataset?\nWe present comprehensive experimental results addressing each research question, shedding light on the efficacy and robustness of our proposed method. Specifically, we address RQ1 in Section 5.2, RQ2 in Section 5.3, RQ3 in Section 5.4, RQ4 in Section 5.5, and RQ5 in Section 5.6."}, {"title": "5.1 Experiment Settings", "content": "FL Setting. We simulate a federated setting of 100 clients, where 50 of them are randomly selected to have their labels flipped. For the pre-client pruning phase (phase I), we set communication rounds to 80, i.e. Tpre = 80, and number of clients to be considered as clean to 5, i.e m = 5, unless specified otherwise. For the client pruning phase (phase II), we set the client pruning percentage to 50%, i.e. p = 0.5, unless specified otherwise. Lastly, for the post-client pruning phase (phase III), we set the communication rounds to 40, i.e. Tpost = 40. Therefore, the total communication rounds is 120. The client sample rate is fixed to 0.1, i.e C = 0.1 during the entire federation process and phases.\nFor local training, we set the number of local epochs to 10, and local batch size to 10. We use an SGD optimizer with a learning rate of 0.03 and 0.9 momentum with no weight decay. Following [7], we use cross-entropy with label smoothing loss with softmax temperature 10.0 and label smoothing coefficient 0.1. We resize the input images to 224 \u00d7 224 and do not use any data augmentation techniques."}, {"title": "5.2 RQ1: Comparison with SOTA FL optimizers", "content": "To address RQ1, we conduct a comprehensive comparative analysis between our proposed method and several SOTA FL optimizers. The objective is to assess the advantages of our pruning approach over SOTA FL optimizers, which typically perform federated learning over the entire client population, including both noisy and clean ones. Specifically, we consider four prominent FL optimizers: FedAvg [29], FedProx [23], FedNova [49], and FedAdam [37]. Following recommendations by Nguyen et al. [34] and Chen et al. [3], to initialize models with pre-trained weights that enhance the FL optimizers' performance, we use weights pre-trained on ImageNet [6] as the initial weights for all experiments in this section.\nPerformance Analysis. Table 1 presents the performance comparison between vanilla SOTA FL optimizers without pruning and those augmented with ClipFL. Notably, we observe a significant enhancement in performance across various scenarios when integrating ClipFL with different FL optimizers. For instance, under IID data partitioning and high noise level (\u03bc = 0.8), the performance improves by at least 16% for CIFAR-10 and at least 7% for CIFAR-100. Across Non-IID and IID data partitioning, the performance improvement of ClipFL is similarly pronounced. Additionally, the performance improvement is more substantial under higher noise level (\u00b5 = 0.8), underscoring the efficacy of our approach.\nWhen comparing the performance of ClipFL across different FL optimizers, FedProx generally outperformed others with ClipFL, achieving the highest test accuracy in 5 out of 8 cases. This highlights the compatibility and effectiveness of FedProx when combined with our proposed pruning approach under noisy labels scenario.\nNoisy Client Identification Analysis. Table 2 presents the accuracy of identifiying noisy clients of ClipFL, defined as the ratio of correctly identified noisy clients to the total clients marked as noisy.\nAs it is evident, ClipFL demonstrates high accuracy in identifying noisy clients, achieving a minimum accuracy of 88% under IID data partitioning. However, we observe a decrease in accuracy identifying noisy clients under Non-IID data partitioning compared to IID scenarios. This decline can be attributed to the increased difficulty in discerning whether a drop in validation accuracy is due to noisy data or data heterogeneity in the Non-IID case. Despite these challenges, ClipFL maintains a commendable accuracy, achieving at least 66% accuracy in identifying noisy clients under the most challenging scenario of Non-IID data partitioning with a noise level of \u00b5 = 0.5"}, {"title": "5.3 RQ2: Comparison with SOTA FL methods that deal with noisy labels", "content": "To address RQ2, we rigorously compare our proposed method with three state-of-the-art federated learning (FL) methods designed to handle noisy labels: FedCorr [52], RHFL [9], and FedLSR [14].\nFL with Noisy Labels Baselines and Settings. FedCorr is a multi-stage FL framework that identifies noisy clients and corrects their noisy samples using global model predictions. RHFL addresses the noisy client challenge by employing symmetric cross-entropy loss for local training and re-weights clients based on their confidence. FedLSR focuses on regularizing local training to prevent over-fitting on noisy samples. We adopt the same experimental setup as detailed in Section 5.2. To ensure fairness, we initialize models with random weights for all methods, adhering to each baseline's original implementation. FedAvg serves as the base FL optimizer for ClipFL in"}, {"title": "5.4 RQ3: Impact of ClipFL's hyper-parameters", "content": "In this section, we thoroughly investigate the impact of ClipFL's hyper-parameters on its performance. Specifically, we analyze the effects of varying three key hyper-parameters: (1) Pre-client pruning communication rounds (Tpre), (2) the number of clients considered as clean based on their validation accuracy (m), and (3) the percentage of clients pruned (p). We conduct our experiments on CIFAR-10 IID data partitioning in this section and the setting is the same as described in Section 5.1. It is also noteworthy to mention that 50% of the clients are noisy in this setting.\nEffect of Tpre and m. Figure 9 illustrates the influence of different pre-client pruning communication rounds Tpre and m on test accuracy performance and noisy client identification accuracy for low and high noise levels, i.e. \u03bc = 0.5 and \u00b5 = 0.8, respectively. Regarding Tpre, we observe that smaller values negatively impact the accuracy of noisy client identification. This occurs because ClipFL"}, {"title": "5.5 RQ4: How does the performance of ClipFL vary with different noise levels?", "content": "In this section, we comprehensively investigate the performance of ClipFL under varying noise levels to address RQ4. Our experimentation focuses on the CIFAR-10 dataset with IID partitioning. Figure 11 illustrates the performance of vanilla FedAvg and FedAvg+ClipFL across different noise levels (\u03bc) for two different percentages of noisy clients. We employ a pruning rate of p = 30% for the 30% noisy client percentage and p = 80% for the 80% noisy client percentage.\nOverall, we observe that as noise levels elevate, ClipFL's performance enhancement becomes increasingly pronounced. Specifically, ClipFL exhibits greater performance for noise levels \u03bc \u2265 0.4, whereas vanilla FedAvg demonstrates superior performance for low noise levels (\u03bc < 0.3). This observation suggests that in scenarios with minimal noise (e.g., \u03bc = 0.1, 0.2), the presence of noisy"}, {"title": "5.6 RQ5: How does the performance of ClipFL vary using synthetic validation dataset?", "content": "In this section, we investigate the impact of the validation dataset on ClipFL's performance. In practice, different entities in various fields, such as the NIH and WHO in the healthcare domain, maintain the quality and annotation of publicly available datasets. Furthermore, the server has ample resources and can utilize platforms like Amazon Mechanical Turk to collect and annotate data, which can then be verified by domain experts to ensure high quality\u2014a method successfully employed in creating datasets for various machine learning tasks. Moreover, recent advancements in generative AI have made it possible to synthesize high-quality images using Diffusion models, which can be used for various purposes [2, 53, 58]. To further investigate the impact of having no access to a real, clean validation dataset on ClipFL's performance, we conducted an experiment using a synthetic dataset generated with an off-the-shelf state-of-the-art (SOTA) text-to-image pre-trained Stable Diffusion model [39]. Specifically, we utilized the \u201cCompVis/stable-diffusion-v1-4\u201d pre-trained Stable Diffusion model checkpoint from HuggingFace to generate the images, using 40 inference steps and a guidance scale of 7. In total, we synthesized 10,000 validation images, with an equal number of images for each class. We conducted our experiments using the same settings discussed in Section 5.2.\nPerformance Analysis. Table 4 presents the results using the synthetic dataset. We observe that, for the IID case, the maximum \u2206 difference between using real and synthetic data as the validation dataset is -0.58%, while for the Non-IID Dir(0.5) case, it is -0.65%. These results further"}, {"title": "5.7 Summary and Discussion", "content": "In our experimental study, we make the following key observations:\n\u2022 ClipFL significantly enhances the performance of SOTA FL optimizers under noisy label settings by effectively pruning noisy clients from the federation. This approach leads to more robust and accurate global model updates, contributing to higher overall accuracy.\n\u2022 Traditional FL methods for noisy labels exhibit poor convergence and limited performance due to challenges in handling noisy clients. In contrast, ClipFL demonstrates fast convergence and substantial performance improvements over communication rounds. Its adaptability to noisy label scenarios ensures consistent progress in accuracy, even under challenging conditions.\n\u2022 ClipFL exhibits robustness even in highly challenging scenarios with high noise levels. Its ability to identify and prune noisy clients effectively allows it to mitigate the adverse effects of noisy labels, ensuring stable and reliable performance across various noise levels.\n\u2022 One of the notable advantages of ClipFL is its fast convergence and reduced communication overhead. By efficiently identifying and pruning noisy clients and reducing the number of communication rounds needed for convergence, ClipFL minimizes unnecessary communication and computational costs, making it an efficient solution for federated learning setups with noisy labels.\n\u2022 ClipFL can be easily scaled to large-scale and severely noisy FL systems without the need for complex aggregation schemes (or advanced and complex FL methods) or extensive communication rounds to achieve a reliable global model.\n\u2022 While removing noisy data might seem to lead to information loss, it is crucial to recognize that this loss pertains to misleading and noisy information rather than useful and informative signal. ClipFL's approach of filtering out noisy clients and data ensures that the global model is trained on more reliable and informative data, circumventing the challenges of achieving a well-trained and reliable global model in order to recover the true labels from noisy data, leading to improved performance."}, {"title": "6 IMPLEMENTATION AND HYPER-PARAMETERS", "content": "In this section we discuss the details of our implementation and the hyper-parameters we used for each baseline in our experimentation."}, {"title": "6.1 Implementations", "content": "FL optimizers implementation. We adopt the implementations of FedAvg [29], FedProx [23], FedNova [49] and FedAdam [38] from FedZoo-Bench [32] with minimal adjustments.\nFL with noisy labels methods implementation. We implement FedCorr [52] using its original implementation with no adjustments. We implement RHFL [9] using their original implementation. We implement FedLSR [14] using their original implementation. The experiments were conducted on a systme with two NVIDIA GeForce RTX 3090s."}, {"title": "6.2 Hyper-parameters", "content": "In this part, we discuss the details of hyper-parameters we used in our experimentations.\nSetting and default hyper-parameters. We conducted extensive hyper-parameter tuning with different optimizers and learning rates using the ViT-Tiny [43] model. We chose an SGD optimizer with a learning rate of 0.03 and momentum of 0.9. Table 5 discusses the base hyper-parameters we used for all the baselines including FL optimizers and FL with noisy labels methods, unless specified otherwise. The pre-trained ViT-Tiny [43] model has been downloaded from the TIMM library.\nFL optimizers' hyper-parameters. For FedProx, we used \u00b5 = 0.001. For FedAdam we used \u03c4 = 0.001 and 0.01 server learning rate.\nFedCorr hyper-parameters. We adjust the number of rounds per stage as 8 rounds for the pre-processing stage, 75 rounds for the fine-tuning stage, and 37 rounds for the normal federated"}, {"title": "7 CONCLUSION", "content": "In this work, we introduced ClipFL, a novel approach for addressing the challenges of federated learning in the presence of noisy labels. By leveraging a systematic client pruning mechanism based on noise candidacy scores, ClipFL effectively identifies and removes noisy clients from the federation, leading to significant performance improvements and faster convergence compared to SOTA FL methods. Our experimental results demonstrate that ClipFL achieves robust and reliable performance even under highly challenging scenarios with varying noise levels and data heterogeneity.\nKey advantages of ClipFL include its ability to enhance the performance of FL optimizers, mitigate the adverse effects of noisy labels, and reduce communication overhead. By prioritizing clean clients for model aggregation and pruning noisy clients effectively, ClipFL ensures stable and efficient federated learning across different datasets and noise conditions.\nOverall, ClipFL presents a promising solution for federated learning applications where label noise is a prevalent challenge. Future research directions may include further exploration of ClipFL's performance under different noise distributions, investigation of adaptive client pruning strategies, and extension to other FL frameworks and applications.\nFuture work should focus on developing more advanced label correction methods that can accurately recover true labels from noisy data without degrading model performance."}]}