{"title": "Sparse Decomposition of Graph Neural Networks", "authors": ["Yaochen Hu", "Mai Zeng", "Ge Zhang", "Pavel Rumiantsev", "Liheng Ma", "Yingxue Zhang", "Mark Coates"], "abstract": "Graph Neural Networks (GNN) exhibit superior performance in graph representation learn-\ning, but their inference cost can be high, due to an aggregation operation that can require\na memory fetch for a very large number of nodes. This inference cost is the major obstacle\nto deploying GNN models with online prediction to reflect the potentially dynamic node\nfeatures. To address this, we propose an approach to reduce the number of nodes that are\nincluded during aggregation. We achieve this through a sparse decomposition, learning to\napproximate node representations using a weighted sum of linearly transformed features\nof a carefully selected subset of nodes within the extended neighbourhood. The approach\nachieves linear complexity with respect to the average node degree and the number of layers\nin the graph neural network. We introduce an algorithm to compute the optimal parame-\nters for the sparse decomposition, ensuring an accurate approximation of the original GNN\nmodel, and present effective strategies to reduce the training time and improve the learning\nprocess. We demonstrate via extensive experiments that our method outperforms other\nbaselines designed for inference speedup, achieving significant accuracy gains with compa-\nrable inference times for both node classification and spatio-temporal forecasting tasks.", "sections": [{"title": "1 Introduction", "content": "Graph neural networks (GNN) have demonstrated impressive performance for graph representation learning\n(Hamilton et al., 2017; Veli\u010dkovi\u0107 et al., 2018; Qu et al., 2019; Ramp\u00e1\u0161ek et al., 2022). Although there are\nnumerous designs for GNN models, the essential idea is to represent each node based on its features and its\nneighbourhood (Wu et al., 2020; Zhou et al., 2020). The procedure of aggregating features from neighbour\nnodes is empirically and theoretically effective (Xu et al., 2019) in representing the graph structures and\nblending the features of the nodes. However, deploying GNN models to process large graphs is challenging\nsince collecting information from the neighbour nodes and computing the aggregation is extremely time-\nconsuming (Zhang et al., 2021; Tian et al., 2023; Wu et al., 2023; Liu et al., 2024).\nIn this work, we tackle the efficient inference problem for GNN models in the online prediction setting\n(Crankshaw, 2019). Specifically, we need to compute the representations of a few arbitrary nodes. The main\nadvantage is that the prediction can reflect potential dynamic features\u00b9 of the input. The computational\ncomplexity is dominated by the number of receptive nodes, which rapidly increases as the number of layers\nin the model grows, for most message-passing-based and graph-transformer-based GNNs (Zeng et al., 2020;\nMin et al., 2022).\nOur goal is to reduce the inference time to linear complexity with respect to the number of layers and the\naverage node degree. Recently, several studies have attempted to address this problem by combining the\nperformance of GNN and the efficiency of MLPs (Zhang et al., 2021; Hu et al., 2021; Tian et al., 2023; Wang\net al., 2023; Wu et al., 2023; Liu et al., 2024; Tian et al., 2024; Winter et al., 2024; Wu et al., 2024). Knowledge\ndistillation (Hinton et al., 2015) and feature/label smoothing are used to construct effective MLP models\nto eliminate the cumbersome neighbour collection and aggregation procedure. Although efficient, these\nmethods have a fundamental limitation: the features gathered at each node are assumed to contain sufficient\ninformation to predict the node label accurately. However, to achieve their full potential, especially when\nfeatures can change at inference time, GNN models should take into account the features from neighbourhood\nnodes and the graph structure (Battaglia et al., 2018; Pei et al., 2020). Therefore, we ask the question: given\nany graph neural network model that relies on both the graph structure and the features of the neighbourhood,\ncan we infer the representation of a node in linear time?\nPresent work. We propose sparse decomposition for graph neural networks (SDGNN), an approximation to\nthe original GNN models that can infer node representations efficiently and effectively. The SDGNN consists\nof a feature transformation function and sparse weight vectors for nodes in the graph. The representation of\neach node is then a weighted sum of the transformed features from a small set of receptive nodes. The sparsity\nof the weight vectors guarantees low inference complexity. The learnable feature transformation function\nand the sparse weight vectors grant the SDGNN flexibility to approximate a wide range of targeted GNN\nmodels. To find the optimal parameters in SDGNN, we formulate the approximation task as an optimization\nproblem and propose a scalable and efficient solution that iterates between the learning of the transformation\nfunction and the optimization of the sparse weight vectors. We verify the approximation power of SDGNN\nand the scalability of our algorithm on seven node classification datasets and demonstrate how SDGNN"}, {"title": "2 Related Work", "content": "Neighbourhood Sampling methods process subsets of edges for each node, selected according to statistical\napproaches. Hamilton et al. (2017) propose recursively sampling a fixed number of neighbours. However, this\napproach still suffers from an exponentially growing aggregation complexity. The layer-wise and graph-wise\nsampling techniques (Chen et al., 2018; Zou et al., 2019; Chiang et al., 2019; Zeng et al., 2020) enhance node-\nwise sampling by using the common neighbours of nodes in the same mini-batch, but they are optimized for\ntraining, and they are not suitable for the online prediction setting.\nEmbedding Compression methods reduce inference time by compressing embeddings with lower accuracy.\nZhao et al. (2020) use neural architecture search over the precision of parameters and adopts attention and\npooling to speed up inference. Zhou et al. (2021) compress the embedding dimension by learning a linear\nprojection. Ding et al. (2021) employ vector quantization to approximate the embeddings of subsets of nodes\nduring training. Tan et al. (2020) hash the final representations. These works do not reduce the complexity\nof neighbour fetching and aggregation.\nGNN Knowledge Distillation techniques employ GNN models as teachers to train students of smaller\nGNN models for inference. Gao et al. (2022) distill the original GNN to a GNN with fewer layers. Yan et al.\n(2020) learn one GNN layer with self-attention to approximate each 2 GNN layers, which halves the total\nnumber of GCN layers in the student model. These methods trade off some performance for better efficiency.\nHowever, the student is still a GNN model which suffers from the large number of receptive nodes.\nMLP Knowledge Distillation. Recently, a series of works dramatically reduced the inference complexity\nby training MLP-based models solely on the (augmented) features of the target node to approximate GNN\nrepresentations (Zhang et al., 2021; Hu et al., 2021; Tian et al., 2023; Wang et al., 2023; Wu et al., 2023; Liu\net al., 2024; Tian et al., 2024). Moreover, Winter et al. (2024); Wu et al. (2024) directly learn MLP models\nwith label smoothing regularized by the graph structure instead of learning from a trained GNN model.\nHowever, these models cannot consider the features of neighbour nodes during inference and are prone to\nhave limited expressive power compared to general GNN models.\nHeuristic Decoupling. Bojchevski et al. (2020) use personalized PageRank to construct shallow GNN\nweights instead of performing layer-wise message passing. Duan et al. (2022); Chen et al. (2021); Wu et al.\n(2019); Chen et al. (2019); Nt & Maehara (2019); Rossi et al. (2020); He et al. (2020) generate augmented,\ninformative node features using techniques such as message passing or PageRank. Winter et al. (2024) pre-\ncompute propagated labels from the training set and add them to the node features. During inference, the\nmodels rely solely on these cached augmented node features. These hand-crafted rules to generate informative\nnode features are prone to be sub-optimal. Besides, under the online prediction setting, heuristic decoupling\nmethods have to periodically update the augmented features to support dynamic features, which have great\ncomputational overhead.\nPosition of Our Work: Compared to existing works, our proposed SDGNN offers linear complexity in\nterms of inference time but can still process the most relevant neighbour features during inference. Table 1\ndepicts a detailed illustration of our work's position."}, {"title": "3 Preliminaries and Problem Definition", "content": "Consider a graph G = {V,E} with |V| = n nodes and |E| = m edges. Denote by X \u2208 R^(n\u00d7D) the feature\nmatrix, with a dimension of D, in which the ith row Xi* denotes the feature vector of node i. We address the\ntask of graph representation learning, which involves learning an embedding g(z, X|G) \u2208 R^d for each node z,\nwhere d denotes the embedding dimension of a GNN. Such representations are fed to additional MLPs for\napplication in downstream tasks, e.g., node classification/regression, link prediction/regression, and graph\nclassification/regression."}, {"title": "4 Sparse Decomposition of Graph Neural Networks", "content": "We aim to develop an approach that is flexible to approximate the node representation from a wide range\nof different GNN models but maintains low inference complexity. To this end, we introduce a feature\ntransformation function \u03c6(\u00b7; W) : R^D \u2192 R^d that maps each row of X \u2208 R^(|V|\u00d7D) to dimension of the target"}, {"title": "4.1 Relation to Existing Methods", "content": "We compare SDGNN to the types that have linear complexity summarized in Table 1. If we assign \u0398 to the\nidentity matrix, the SDGNN degrades to a model solely based on the self-node features. This covers most of\nthe methods under the umbrella of MLP knowledge distillation. For models that augment the node features,\nlike NOSMOG Tian et al. (2023), SDGNN can trivially adopt such features as well. Moreover, if we fix \u0398\nto be the personalized PageRank matrix or power of the normalized adjacent matrix, SDGNN is equivalent\nto the heuristic decoupling approach. Therefore, SDGNN has a larger expressive power than models from\nMLP knowledge distillation and heuristic decoupling described in the Related Work section."}, {"title": "5 SDGNN Computation", "content": "We formulate the task of identifying the optimal \u0398 and \u03c6(\u00b7; W) as an optimization problem:\nminimize_{\u0398,(\u00b7;W)} {1/2 ||\u03a6(X; W) \u2013 \u03a9||_F^2 + \u03bb_1||\u0398||_{1,1} + \u03bb_2||W||_F^2},"}, {"title": "5.1 Optimization", "content": "Jointly learning \u0398 and W is challenging due to the sparsity constraint on \u0398. We optimize them iteratively,\nfixing one while updating the other, termed Phase \u0398 and Phase \u03a6.\nPhase \u0398. For each node z \u2208 V, we update \u03b8z with the solution of the following optimization problem:\nargmin_{\u03b8} {1/2 ||\u03b8^T\u03a6(X; W) \u2013 \u03a9_{z*}||_2^2 + \u03bb_1||\u03b8||_1}\ns.t. \u03b8 \u2265 0."}, {"title": "5.2 Narrowing the Candidate Nodes", "content": "The computation required to solve (4) rapidly grows as the size of V increases. Specifically, the computational\noverhead comes from the inference of \u03a6(X; W) and the computation time of the LARS solver. To reduce the\nrequired computation, we define a much smaller candidate node set Cz for each node z and only consider\nthe candidate nodes to determine \u03b8z. We solve the following problem instead of (4).\nargmin_{\u03b8} {1/2 ||\u03b8^T\u03a6(X; W) \u2013 \u03a9_{z*}||_2^2 + \u03bb_1||\u03b8||_1}\ns.t. \u03b8 \u2265 0 \u2200i \u2208 C_z, \u03b8_i = 0 \u2200i \u2209 C_z."}, {"title": "6 Experiments", "content": "We validate SDGNN's approximation power and inference efficiency through node classification tasks in the\ntransductive setting."}, {"title": "6.1.1 Task", "content": "We consider the node classification task under the transductive setting. Given a graph G = {V,E} with\n|V| = n nodes and |E| = m edges, the feature matrix of nodes X \u2208 R^(n\u00d7D) and the labels from a subset of\nnodes Vtrain \u2282 V, the task is to predict the labels for the remaining nodes."}, {"title": "6.1.2 Datasets", "content": "Following Zhang et al. (2021) and Tian et al. (2023), we conduct experiments on five widely used benchmark\ndatasets from Shchur et al. (2018), namely, Cora, Citeseer, Pubmed, Computer and Photo. We also examine\nperformance on two large-scale datasets, Arxiv and Products, from the OGB benchmarks (Hu et al., 2020)."}, {"title": "6.1.3 Baselines and Target GNN Models", "content": "We select GLNN (Zhang et al., 2021), NOSMOG (Tian et al., 2023), CoHOp (Winter et al., 2024), SGC (Wu\net al., 2019), and PPRGo (Bojchevski et al., 2020) as the baselines. Specifically, GLNN and NOSMOG distill"}, {"title": "6.1.4 Evaluation", "content": "For the 5 small datasets (Cora, Citeseer, Pubmed, Computer and Photo), we randomly split the nodes with\na 6:2:2 ratio into training, validation and testing sets. Experiments are conducted using 10 random seeds,\nas in Pei et al. (2020). We report the mean and standard deviation. For Arxiv and Products, we follow the\nfixed predefined data splits specified in Hu et al. (2020), run the experiments 10 times, and report the mean\nand standard deviation."}, {"title": "6.1.5 SDGNN Integration", "content": "We start with a trained GNN model, e.g., GraphSAGE (Hamilton et al., 2017), DeeperGCN (Li et al.,\n2021), or graph transformers (Yun et al., 2019; Ramp\u00e1\u0161ek et al., 2022; Ma et al., 2023; Shirzad et al.,\n2023). We adopt the intermediate representation constructed by the architecture immediately before the\nfinal prediction as the target node representation g(z, X|G), and learn the SDGNN g^(z, X|G). Using the\nlabel and the representation g^(z, X|G) for the nodes within Vtrain, we train a decoder function f(g^(z, X|G))\nto map the node representation to the predicted labels. We employ an MLP for the decoder model. During\ninference, upon a request for prediction for a node z, we refer to \u03b8z to retrieve the receptive node features\nand compute the prediction with f(g^(z, X|G)) over the receptive nodes. The overall pipeline of integrating\nthe SDGNN is depicted in Figure 1."}, {"title": "6.1.6 Main Results", "content": "We compare the accuracy among GLNN, NOSMOG, CoHOP, PPRGo, SGC and SDGNN for seven datasets\nwith different target GNN models in Table 2. The \"Target Model\" column indicates the performance of\nthe original GNN models. CoHOP, SGC and PPRGo do not rely on the target GNN embeddings. We\nduplicate the results of CoHOp within each dataset to compare the performance better. SGC and PPRGo\nhave different performances under different target GNN models since we truncate the neighbours according\nto the patterns from the corresponding SDGNN models. Overall, SDGNN can approximate the target for\nall scenarios well, achieving accuracy close to or better than the target model. Given a weaker target GNN\nmodel, SDGNN achieves a better approximation to the target GNN model, so it can be outperformed by"}, {"title": "6.1.7 Inference Time", "content": "We performed inference for 10,000 randomly sampled nodes for each dataset to assess the trade-off between\ninference time and accuracy. Here, we provide results and discussion for the Products dataset; the results\nfor other datasets are qualitatively similar and are provided in the Appendix. Fig. 2 illustrates the testing\naccuracy versus the average computation time per node for different models and approximation techniques.\nWe observe that SDGNN-Rev (SDGNN based on RevGNN) achieves the best accuracy (82.87%) with in-\nference time (0.74 ms), while the MLP-based methods (MLP, GL-Rev, GL-SAGE, NOS-SAGE, NOS-Rev)\nhave the fastest inference time around 0.45 ms. SDGNN is slower due to the additional computation costs\nof performing feature transformation on multiple nodes and aggregation to incorporate neighbour features.\nThe SAGE series, such as SAGE-L3-full (a 3-layer SAGE model without sampling during inference) and\nSAGE-L2-N20 (a 2-layer SAGE model that samples 20 neighbours), exhibit rapidly growing inference times\nas the number of layers increases (up to 128 ms), making them impractical for real-world applications. We\nalso detail the results for GL-SAGE-w4 (GL-SAGE with hidden dimension 4 times wider) and GL-Rev-w4 (a\n4 times wider version of GL-Rev). Although their accuracy improves compared to their base versions, their\ninference times also increase substantially. We conclude that SDGNN offers superior performance in terms\nof accuracy, and its inference time is sufficiently close to the fastest MLP models for most applications."}, {"title": "6.2 Spatio-Temporal Forecasting", "content": "We demonstrate how SDGNN can be applied to tasks with dynamic node features through spatio-temporal\nforecasting tasks, which are aligned with the online prediction setting."}, {"title": "6.2.1 Task", "content": "We have a graph G = {V,E} with |V| = n nodes and |E| = m edges. The nodes are associated with signals\nin discrete time X1:T = (X1, X2, ...,XT) \u2208 R^(n\u00d7T\u00d7C), and each Xt \u2208 R^(n\u00d7C) denotes C channel signals for all\nn nodes at time t. The task is to predict XT+1 \u2208 R^(n\u00d7C)."}, {"title": "6.2.2 Datasets", "content": "We consider the traffic datasets PeMS04 and PeMS08 from Guo et al. (2021) and apply the data preprocessing\nof Gao & Ribeiro (2022). Given a fixed graph G, the signals on the nodes are partitioned into snapshots.\nEach snapshot s contains discrete signals X\u2081:7 and the corresponding target X7+1 to predict."}, {"title": "6.2.3 Baselines and Target GNN Model", "content": "We select GLNN (Zhang et al., 2021), NOSMOG (Tian et al., 2023), and PPRGo (Bojchevski et al., 2020)\nas the baselines.\nAs for the target GNN model, we adopt the time-then-graph framework with the GRU-GCN model from\nGao & Ribeiro (2022). For each X\u2081:T \u2208 R^(n\u00d7T\u00d7C), GRU-GCN uses a gated recurrent unit (GRU) to encode\nthe signal at each node into the representation H\u00b0 \u2208 R^(n\u00d7d) = GRU(XT), d being the dimension of the\nrepresentation. Then a graph neural network (GCN) is applied to Hs to obtain Z\u00b0 \u2208 R^(n\u00d7d) = GCN(H).\nFinally, an MLP decoder is used at each node to derive the prediction X^T+1 = MLP(Z\u00b0) \u2208 R^(n\u00d7C)."}, {"title": "6.2.4 Evaluation", "content": "We split the snapshots into train, validation, and test sets. We report the test set's mean absolute percentage\nerror (MAPE). We run each setting 10 times and report the mean and standard deviation of MAPE."}, {"title": "6.2.5 SDGNN Integration", "content": "We apply SDGNN to replace the GNN module. After training the GRU-GCN, we have GRU, GCN, and\nMLP decoder modules. For all the snapshots in the training set, we take Hs as the input to SDGNN and\nZs as the output to approximate. During training, we aggregate the loss and minimize over all snapshots:\nminimize_{\u0398,(\u00b7;W)} {1/2 \u03a3_s ||\u0398^T\u03a6(H^s; W) \u2013 Z^s||_F^2 + \u03bb_1||\u0398||_{1,1} + \u03bb_2||W||_F^2},"}, {"title": "6.2.6 Main Results", "content": "Table 3 presents MAPE results under various settings. In this experiment, SDGNN's target model is GRU-\nGCN with a 2-layered GNN model. GRU denotes the degraded version of GRU-GCN that removes the\nGCN component. The GCN module integrates the information from neighbour nodes and reduces the error\nsignificantly. With the target GRU-GCN model, GLNN and NOSMOG slightly outperform the GRU, but\nthe position encoding in NOSMOG does not help in this dynamic feature setting. SDGNN is closest to\nthe target GRU-GCN model and performs best among the efficient models. SDGNN can select the most\ninformative receptive nodes whose features have an impact on the labels. The heuristic-based neighbour\nselection method PPRGo fails under the current setting, achieving similar or even worse performance than\nGRU."}, {"title": "6.2.7 Embedding Approximation Efficacy", "content": "Figure 4 shows a scatter plot of the MAE reduction (relative to the GRU) at each node achieved by the\nGCN versus that achieved by SDGNN. The scatterplot is concentrated close to the y = x line, with a slight\nbias in favour of the more computationally demanding GCN. The plot highlights how SDGNN successfully\napproximates the GCN embeddings and thus derives a similar forecasting accuracy improvement at each\nnode."}, {"title": "7 Conclusion", "content": "In this work, we proposed a systematic approach to efficiently infer node representations from any GNN\nmodels while considering both the graph structures and the features from neighbour nodes. Extensive\nexperiments on seven datasets on the node classification task and two datasets on the spatial-temporal\nforecasting task demonstrate that SDGNN can effectively approximate the GNN target models with reduced\nreceptive field size. Our method provides a new tool for the efficient inference of GNN models under the\nonline prediction setting, which can benefit from both the power of GNN models and the real-time reflection\nof dynamic node features."}, {"title": "C.1 Optimization Schedule for the Sparse Weights", "content": "To ensure that the weights remain sparse, we implemented a specific optimization schedule tailored to the\ncharacteristics of sparse weights. At the beginning of the training process, we set a larger maximum receptive\nfield size to allow for broader connections. As training progresses, we gradually reduce the maximum receptive\nfield size by 1 every 100 epochs. In our case, the largest receptive field size at the beginning of the training\nwas set to 46. Throughout 4000 training epochs, we gradually reduced the receptive field size. Specifically,\nevery 100 epochs, the maximum receptive field size was decreased by 1, resulting in a final receptive field\nsize of 6."}, {"title": "C.2 Analysis of Final Performance Relative to the Distance Between SDGNN Approximated\nEmbeddings and Target Model Embeddings", "content": "Since SDGNN aims to approximate the target model embeddings, we hypothesize that the discrepancy\nbetween the SDGNN embeddings and the target model embeddings will significantly impact the overall\nmodel performance. Figure 8 illustrates the relationship between the mean squared error (MSE) of the\nembeddings and the prediction performance. The x-axis represents the MSE between the target embeddings\nand the SDGNN-approximated embeddings, while the y-axis indicates the prediction performance measured\nby the MAPE metric. Each scatter point represents a checkpoint of SDGNN. We utilize checkpoints of\nSDGNN during training at every 100 epochs and evaluate the performance on the test sets to generate the\nplot. The figure reveals that higher approximation errors correspond to greater discrepancies in prediction\nperformance. This observation suggests that as the MSE between the SDGNN embeddings and the target\nembeddings decreases, the model's performance is likely to improve."}, {"title": "D Limitations of SDGNN", "content": "Although SDGNN is very effective in approximating target GNN embeddings, there are a few limitations.\nWe present the naive cues from an engineering perspective and would like to explore principled solutions in\nfuture works."}, {"title": "D.1 Incompatibility with Inductive Setting", "content": "SDGNN always requires the GNN embeddings from all nodes as the guiding signal to generate the optimal \u03b8\nand \u03c6. Therefore, SDGNN cannot be applied directly in the inductive setting when new nodes may appear\nduring inference time. From the application perspective, a simple strategy could be collecting the \u03c6 for\nall/sampled 1-hop neighbour nodes and summing them up as the proxy for the new nodes. Moreover, after\na new node appears once, we could use the inductive GNN to infer the GNN embedding, compute \u03b8 with\nPhase \u0398 for that node and cache the results for future use."}, {"title": "D.2 Unadaptability to Distribution Shift", "content": "For the online prediction setting, we train SDGNN based on past snapshots and apply it in future snapshots.\nWe did not consider the potential distribution shift between training and testing. If the time gap between\nthe training and the testing is large, such a shift might degrade the performance. One could incorporate\nonline learning so that the SDGNN weights can be adapted as more data become available and the graph\ntopology changes."}, {"title": "D.3 Inability to Represent Non-linear Interactions between the Features at Different Nodes", "content": "In standard GNN models, the recursive procedure of aggregating neighbour features followed by the feature\ntransformation may potentially model the non-linear feature interaction between nodes. However, SDGNN\nis formulated as a linear combination of transformed node features. Although efficient, it seems not to have\nthe potential to model arbitrary feature interaction between nodes. One immediate fix is that for any target\nnode, we can augment the other candidate nodes' features with the target node feature before feeding into\n\u03c6, which appears to integrate all the feature interaction between the neighbour nodes and the target nodes.\nHowever, a general principled approach is worth exploring. Besides, we should have a theoretical analysis\nof the expressiveness of SDGNN and explore under which conditions SDGNN is not as expressive as the\ngeneral GNN models."}, {"title": "D.4 Training Overhead and Challenge on Hyper-parameter Selection", "content": "We managed to finish the training of SDGNN in about 20 hours for the graph with millions of nodes like ogbn-\nproducts, but training SDGNN on larger graphs is challenging. Regarding the hyper-parameter selection,\nsuch a training overhead for each run is a nightmare. The main obstacle is the solver for the Lasso problem.\nImplementing the LARS solver that supports the GPU speedup could be an immediate workaround. As for\na principled solution, since we iteratively optimize \u03b8 and \u03c6, we don't necessarily require an optimized result\nfor Phase \u03a6 at each iteration. Instead, we may propose a similar Lasso solver that can rapidly trade off\nbetween the computation complexity and the accuracy of the solution. During the training of SDGNN, we\ncould engage a schedule to gradually switch the Lasso solver from efficiently generating intermediate results\nto accurately achieving optimal results."}]}