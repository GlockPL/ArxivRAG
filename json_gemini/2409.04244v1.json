{"title": "WarpAdam: A new Adam optimizer based on Meta-Learning approach", "authors": ["Chengxi Pan", "Junshang Chen", "Jingrui Ye"], "abstract": "Optimal selection of optimization algorithms is crucial for training deep learning models. The Adam optimizer has gained significant attention due to its efficiency and wide applicability. However, to enhance the adaptability of optimizers across diverse datasets, we propose an innovative optimization strategy by integrating the \"warped gradient descend\" concept from Meta Learning into the Adam optimizer. In the conventional conventional Adam optimizer, gradients are utilized to compute estimates of gradient mean and variance, subsequently updating model parameters. Our approach introduces a learnable distortion matrix, denoted as P, which is employed for linearly transforming gradients. This transformation slightly adjusts gradients during each iteration, enabling the optimizer to better adapt to distinct dataset characteristics. By learning an appropriate distortion matrix P, our method aims to adaptively adjust gradient information across different data distributions, thereby enhancing optimization performance. Our research showcases the potential of this novel approach through theoretical insights and empirical evaluations. Experimental results across various tasks and datasets validate the superiority of our optimizer that integrates the \"warped gradient descend\" concept in terms of adaptability. Furthermore, we explore effective strategies for training the adaptation matrix Pand identify scenarios where this method can yield optimal results. In summary, this study introduces an innovative approach that merges the \"warped gradient descend\" concept from Meta Learning with the Adam optimizer. By introducing a learnable distortion matrix P within the optimizer, we aim to enhance the model's generalization capability across diverse data distributions, thus opening up new possibilities in the field of deep learning optimization.", "sections": [{"title": "1 Introduction", "content": "Meta-learning, or \u201clearning to learn,\u201d involves infer- ring effective learning strategies from past experiences to facilitate rapid adaptation to new tasks [10]. In meta- learning, the choice of optimizer significantly influences the efficiency and effectiveness of the learning process. An optimizer updates model parameters during training to minimize the loss function. In the context of meta-learning, where fast adaptation is crucial, a well-designed optimizer can greatly impact generalization and stability."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Meta learning", "content": "Meta-learning, a potent learning strategy, has been ex-plored across various contexts [2] [33]. It comprises two primary levels: the base level, focusing on learn- ing for individual tasks, and the meta- level, emphasizing generic features across tasks [8] [27] [21] [3]. Base- level learning adapts models for tasks, while meta-level learning facilitates effective transition between tasks. The inner-outer double- loop algorithm implements this structure [9] [3] [19] [14]. The inner loop employs gradient descent to optimize tasks, while the outer loop eval- uates task performance through second-order derivatives, adjusting meta-parameters. These yields optimized meta- parameters for rapid task training.\nEarly meta-learning methods, such as MAML, as- sume task similarity [12] [3] [14], limiting applicability. Research aims to develop cross-task optimization strate- gies and enhance scalability."}, {"title": "2.2 Gradient descent", "content": "Gradient descent is fundamental to deep learning, iteratively updating parameters to minimize loss [24] [25]. Challenges include local optima, learning rate selection, and suboptimal convergence [13] [1]. SGD, Momen- tum, and adaptive learning rate methods improve gradient descent [23] [13] [26]. Adam, a popular optimizer, com- bines Momentum and Adagrad [11], yet has convergence issues [22]. Variants like AMSGrad and RAdam stabi- lize learning rate decay [22] [17] [15], while methods like LAMB and AdaBelief enhance hyperparameter sensitiv- ity."}, {"title": "2.3 Domain adaptive methods and Transfer Learning", "content": "Domain Adaptation addresses generalization in new do-mains, relevant to Transfer Learning. Transfer Learn- ing assumes source-target domain correlation and aims for shared feature representation [36] [20]. Techniques include Domain Adversarial Training and GANs [5] [6]. Meta-Learning aids Transfer Learning by learning strate- gies and shared representations from source domains [31]."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Preliminary", "content": "Due to the unique nature of meta learning itself, it is particularly suitable for scenarios with a set of similar tasks. Omniglot dataset comes into our attention.\nThe Omniglot dataset is a widely used benchmark in few-shot learning and Meta-Learning researches due to its distinctive properties. Unlike conventional image clas- sification datasets that contain a large number of classes with ample training samples per class, Omniglot presents a significantly more challenging scenario. It consists of handwritten characters from 50 different alphabets, with each character represented by just a few instances (typi- cally 20 images per character).\nLet A be the set of alphabets in the Omniglot dataset. Each alphabet $a \\in A$ contains a set of characters denoted by $C_a$. Each character $c \\in C_a$ is represented by a few handwritten instances, denoted by $i_{a,c}$. The handwritten instances can be further organized into two subsets for training and evaluation purposes:\na. Training Set:\nLet $D_{train}$ represent the training set of the Omniglot dataset. It is composed of pairs of handwritten instances and their corresponding character labels, i.e., $(i_{alpha, c})$ for all $a \\in A$ and $c \\in C_a$. The training set is used to facilitate the Meta-Learning process, where the model learns to adapt to different characters within the few-shot learning setting.\nb. Evaluation (or Test) Set:\nThe evaluation set $D_{eval}$ contains pairs of handwritten instances and their corresponding character labels for unseen characters. Specifically, let $A_{eval$ be a subset of $A$ representing alphabets that were not included in the training set. Then, the evaluation set $D_{eval}$ contains pairs $(i_{alpha, c})$ for all $a \\in A_{eval}$ and $c \\in C_a$.\nThe primary goal of Meta-Learning with the Omniglot dataset is to train a model using $D_{train}$ in a way that it can quickly adapt to new characters from the evaluation set $D_{eval}$ with limited labeled samples. This scenario mimics real-world situations where the model encounters novel tasks or classes during deployment."}, {"title": "3.2 Adaptive Learning Rate", "content": "Lydia et al. highlighted that Gradient Descent algorithms [17], while widely used, still function as black- boxes, with many tunable hyper-parameters remaining unex- plored. These hyper-parameters utilize"}, {"title": "3.2.1 Adam", "content": "Adam, an adaptive optimization algorithm, combines fea-tures of both RMSProp and Momentum. It utilizes ex- ponential moving averages to estimate first and second- order moments, addressing challenges in convergence and learning rate adjustment. Formula (1) shows the principle of the Adam optimizer, using new parameters to accumu-late the first-order and second-order statistics of the gra- dient to obtain better convergence.\n$m_t = \\beta_1 * m_{(t-1)} + (1 - \\beta_1) * \\nabla W_t$\n$v_t = \\beta^2 * v_{(t-1)} + (1 - \\beta^2) * (\\nabla W_t)^2$\n$\\hat{m_t} = m_t/(1- \\beta_1^t)$\n$\\hat{v_t} = v_t/(1 - \\beta_2^t)$\n$W_{(t+1)} = W_t - \\frac{\\eta}{\\sqrt{\\hat{v_t}}+\\epsilon} * \\hat{m_t}$                                                                                                     (1)\nHowever, researchers found limitations in Adam, particularly in convergence to optimal solutions even on simple tasks [22]. This prompted the development of improved versions such as AMSGrad, AdaBound, and RAdam, which stabilize learning rate decay and enhance training convergence and stability [22] [17] [15]. Other approaches, like Decoupled Weight Decay, refined weight decay by separating it from adaptive learning rate adjustment [16]. Additionally, methods like LAMB and AdaBelief improved Adam's sensitivity to hyperparame- ters.\nAdam [35] proposes a similar set of equations for $b_1$. Notice that the update rule for Adam is very similar to RMSProp, except we look at the cumulative history of gradients as well ($m_1$). Note that the third step in the up- date rule above is bias correction."}, {"title": "3.2.2 WarpAdam", "content": "$m_t = \\beta_1 * m_{(t-1)} + (1 - \\beta_1) * (P\\nabla W_t)$\n$v_t = \\beta^2 * v_{(t-1)} + (1 - \\beta^2) * (P\\nabla W_t)^2$\n$\\hat{m_t} = m_t/(1- \\beta_1^t)$\n$\\hat{v_t} = v_t/(1 - \\beta_2^t)$\n$W_{(t+1)} = W_t - \\frac{\\eta}{\\sqrt{\\hat{v_t}}+\\epsilon} * \\hat{m_t}$                                                                                                 (2)\nWe've noticed that for some special data sets, Adam's convergence ability on the task is very poor, and even enters the over-fitting state very early. Meta-learning has the function of extracting the feature of the task set. For the Adagrad optimizer, a large number ofresearchers such as Zhang [35] and Malitsky [18] have proposed and used the gradient adaptive (AGD) method. Therefore, our meta-learning measures extract the feature matrix P of the task set, and use adaptive gradient descent (AGD) [7], so that the data set can quickly converge on new tasks.\nFormula (2) shows the mechanism of WarpAdam, $P$ is a square matrix ($n \\times n$) generated by meta-learning, representing the extracted adaptive parameters to enable TOD function. This paper conducts research on adaptive gradient descent around$P$, which will be elaborated in the next section."}, {"title": "3.3 AGD - Adaptive Gradient Descent", "content": "Adaptive Gradient Descent (AGD) [7] is a novel opti- mization technique that adapts the learning rate for each parameter during the training process. Unlike traditional optimization methods with fixed"}, {"title": "3.4 Baselines and our methods", "content": "We consider the following two key factors of the bound function: convergence speed and accuracy."}, {"title": "3.4.1 Task Loss Curve Analysis", "content": "The Task Loss Curve provides valuable insights into howour warpAdam model adapts to new tasks. As we intro- duce new tasks into the training process, we observe that the loss for these new tasks is consistently lower than that of the previously encounteredtasks. This reduction in loss indicates that the model effectively learns to adapt and generalize well to new data, which is crucial in few-shot learning scenarios. The warpAdam's ability to minimize task-specific losses showcases its robustness and versatil- ity in handling diverse tasks."}, {"title": "3.4.2 Accuracy Curve Analysis", "content": "The Accuracy Curve depicts the overall performance of the warpAdam model as it learns from new tasks. We observe a general upward trend in the accuracy curve, in- dicating that the model's performance steadily improves over time. This improvement can be attributed to the meta-learning component of warpAdam, which enables the model to leverage knowledge gained from previous tasks to better tackle new tasks. The continuous increase in accuracy underscores the model's ability to refine its optimization process and adapt to the inherent complexi- ties of diverse tasks."}, {"title": "3.5 Adam", "content": "In this experiment, we explore the performance of the Adam optimizer on the challenging Omniglot dataset by initializing different learning rates (lr). Our objective is to investigate the sensitivity of Adam to different learning rates and understand its behavior under varying hyperparameters.\nWe conducted a series of experiments using the same neural network architecture and hyperparameters, except for the learning rate. Surprisingly, when using this extremely low leaming rate 1 x 10^-5 (lr = 1 x 10^-5), the performance of Adam on the Omniglot dataset was notably poor. The model exhibited slow convergence and struggled to capture the underlying patterns in the data. The accuracy and convergence speed were severely impacted, suggesting that the choice of learning rate plays a crucial role in determining the success of Adam on the Omniglot dataset."}, {"title": "3.6 WarpAdam", "content": "In this section, we present the experimental results of our proposed warpAdam optimization approach. We focus onevaluating its performance in handlingnewtasksand its ability to adapt to previously unseen data. Specifically, we analyze the Task Loss Curve and the Accuracy Curve to understand how the model performs on new tasks and how its accuracy improves over time."}, {"title": "4 Comparison", "content": "In this section, we compare the proposed WarpedAdam optimization algorithm with several traditional optimization methods, including Stochastic Gradient Descent (SGD), Momentum, Rectified Adam (RAdam), and AdamW. We aim to highlight the distinctive features and advantages of WarpedAdam."}, {"title": "4.1 Stability", "content": "WarpedAdam demonstrates improved stability. It is less sensitive to the choice of hyperparameters, such as learning rate, compared to SGD and Momentum. This stability is particularly advantageous in scenarios where hyperparameter tuning is challenging."}, {"title": "4.2 Adaptivity", "content": "One of the key innovations of WarpedAdam is its adaptivity through the introduction of the matrix P. Warped Adam can dynamically adjust the P matrix to adapt to the requirements of different tasks or datasets. This adaptivity is a significant advantage over other algorithms, including AdamW and RAdam, which lack this self-adjusting mechanism."}, {"title": "4.3 Experimental Results", "content": "Table 1 summarizes the performance of WarpedAdam compared to SGD, Momentum, RAdam, and AdamW on the Omniglot benchmark dataset. The Omniglot dataset, known for its complexity and diversity, provides a rigorous testing ground for optimization algorithms. The results, as presented in the table, consistently highlight the superiority of WarpedAdam across various aspects of training. Specifically, WarpedAdam demonstrates faster training speed, quicker convergence, and improved generalization, all of which are crucial factors when dealing with the challenges posed by the Omniglot dataset. These findings underscore the potential of WarpedAdam as an efficient optimization algorithm for complex and diverse tasks."}, {"title": "5 Conclusion", "content": "In conclusion, our experimental results substantiate the effectiveness of the warpAdam optimization approach in addressing few-shot learning challenges. The analysis of the Task Loss Curve and Accuracy Curve demonstrates the model's adaptability to new tasks, continuous accu- racy improvement, and proficiency in generalizing to un- seen data. The superiority of warpAdam over traditional optimization methods further validates its potential for various applications in few-shot learning scenarios. The success of WarpAdam sets the stage for future research in developing more robust and adaptive optimization techniques for complex machine learning tasks."}]}