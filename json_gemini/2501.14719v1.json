{"title": "Do LLMs Provide Consistent Answers to\nHealth-Related Questions across Languages?", "authors": ["Ipek Baris Schlicht", "Zhixue Zhao", "Burcu Sayin", "Lucie Flek", "Paolo Rosso"], "abstract": "Equitable access to reliable health information is vital for\npublic health, but the quality of online health resources varies by lan-\nguage, raising concerns about inconsistencies in Large Language Mod-\nels (LLMs) for healthcare. In this study, we examine the consistency of\nresponses provided by LLMs to health-related questions across English,\nGerman, Turkish, and Chinese. We largely expand the HealthFC dataset\nby categorizing health-related questions by disease type and broadening\nits multilingual scope with Turkish and Chinese translations. We reveal\nsignificant inconsistencies in responses that could spread healthcare mis-\ninformation. Our main contributions are 1) a multilingual health-related\ninquiry dataset with meta-information on disease categories, and 2) a\nnovel prompt-based evaluation workflow that enables sub-dimensional\ncomparisons between two languages through parsing. Our findings high-\nlight key challenges in deploying LLM-based tools in multilingual con-\ntexts and emphasize the need for improved cross-lingual alignment to\nensure accurate and equitable healthcare information.", "sections": [{"title": "Introduction", "content": "LLMs have become powerful tools for NLP tasks and become widely accessible\nthrough chat interfaces to non-technical users. Their applications also extend to\nhealthcare, where users consult LLM-based chat applications for health-related\nquestions. LLMs are primarily trained on data from online sources. However,\nthe quality of health information online varies by language, reflecting differences"}, {"title": "Related Works", "content": "Various studies have examined how prompt variations impact the factual con-\nsistency of LLMs in EN medical Q&A. Zuccon and Koopman [10] showed that\nthe accuracy of ChatGPT can vary widely based on prompt phrasing, with sup-\nporting or contradicting evidence in the prompt potentially introducing biases\nthat affect answer correctness. Similarly, Sayin et al. [17] explored the poten-\ntial of LLMs to assist and correct physicians in medical decision-making. They"}, {"title": "Methodology", "content": "To evaluate LLM performance across prompts in different languages, we first\nuse Named Entity Recognition (NER) to categorize samples by disease. We then\nprompt LLMs in EN, DE, ZH, and TR on these categorized corpora."}, {"title": "Information Seeker Queries", "content": "We used the HealthFC dataset [19], available in DE and EN, containing 750\nquestion-form claims along with corresponding evidence. HealthFC lacks spec-\nified disease categories for the claims, and some claims cover other health-\nrelated topics. To address this, we identified disease-related claims and enriched\nHealthFC with disease categories. Additionally, we expanded it by adding TR\nand ZH translations to support multilingual experiments.\nDisease Categorization. To identify disease-related claims in the dataset, we\nused HunFlair2 [16] from flairNLP [2] on EN claims. We selected samples con-\ntaining disease entities, resulting in 508 samples. We used the ICD10 Coding [21]\nfor the categorization. We manually reviewed each entity, assigned the corre-\nsponding ICD10 codes, and then tagged them with their main disease category\nthrough the ICD10 look-up table [22]. For example, the ICD10 code of skin can-\ncer is C44.90 and hence its main disease category is Neoplasms according to the\ntable. After tagging, we kept only samples whose category occurred more than\n20 times to maintain balance in disease categories (see Table 2).\nQuery Translations HealthFC includes samples in DE and EN. To expand\nit, we added TR and ZH samples by translating the EN samples with Google\nTranslate. Native TR and ZH speakers from our team reviewed 50 random trans-\nlations to ensure fluency and accuracy to preserve the original meaning. Specif-\nically, they checked if the translated sentences and medical terms were correct\nand the translations retained the original meaning. Despite noting areas for im-\nprovement such as using folk language instead of medical terms or adjusting\nculturally specific references (e.g., replacing 'pig meat' with 'sucuk' in TR), 84%\nof the TR translations and 98% of the ZH translations were satisfactory."}, {"title": "Prompt-based Evaluation Workflow", "content": "LLMs generate complex long-form answers, unlike simpler formats like multiple-\nchoice responses. Long answers may include not only direct responses but also\nbackground information, examples, limitations, and related content, making di-\nrect comparisons challenging. To address this, we developed a prompt-based\nevaluation workflow that automatically analyzes bilingual answer nuances by\nsegmenting long-form responses into informative parts. This workflow includes\na parsing prompt and a consistency-check prompt.\nParsing Prompt. This prompt parses the answer based on a discourse struc-\nture, inspired by prior work by Xu et al. [24] and adapted for our use case.\nThe discourse ontology comprises the following elements: (i) Answer-Summary\n(AS): The part of the answer addressing the question, excluding sentences elab-\norating on the summary or providing extra context, (ii) Health Benefits and\nOutcomes (HBO): Describes the positive effects or results of a medical inter-\nvention or behavior, (iii) Clinical Guidelines and Evidence (CGE): Refers to es-\ntablished guidelines or research that support the medical recommendations, (iv)\nIndividual Considerations/Caveats (ICC): Highlights individual variability and\nemphasizes the need for personalized advice, and (v) Public Health/Professional\nAdvice (PHPA): Emphasizes consulting healthcare professionals and following\npublic health recommendations. A parsing example is given in Figure 3.\nConsistency Comparison Prompt. This prompt compares parsed EN an-\nswers with those in other languages, assessing whether they are consistent. Con-\nsistency is evaluated based on the following annotation schema: (i) Consistent:\nThe EN answer and the answer in the other language are fully consistent and\nsemantically aligned, (ii) Partially Consistent: The EN answer and the answer\nin the other language partially agree, overlap, or support each other, though\nwith some irrelevant or contradictory content, (iii) Contradict: Answers contra-\ndict each other, and (iv) Irrelevant: Answers address different topics and are\nunrelated. Empty responses are also included in this irrelevant category.\nWe refined prompts iteratively and evaluated the final parsing prompt using\na point-based system across five answers per language. Elements were scored: 2"}, {"title": "Experiments and Results", "content": "We included four general-purpose, multilingual LLMs in the experiments: the\nclosed-source models ChatGPT-40, continuously updated model [14], and GPT4-\no from OpenAI [1], and the open-source models Llama3-70B-Instruction [6] from\nMeta and CommandR+ from Cohere [4]. To generate answers, we set the LLMs\u2019\ntemperature to 0 and the response context limit to 2048 tokens. Prompts were\ngiven in EN, as the models tend to follow task descriptions more effectively in\nEN than in other languages [12,3], and EN is more cost-effective due to the\nlarger token size required by some languages. No system prompts were used,\nexcept for Llama and CommandR+ when generating responses in TR and ZH,\nas these models default to EN. We accessed the closed models via the official\nOpenAI API. We used the Huggingface Inference Endpoint for Llama3 and the\nTransformers library [23] for CommandR+. Additionally, GPT-40 was used for\nparsing and comparing responses, chosen for its strong performance on vari-\nous NLP tasks and its support for structured outputs essential to our analysis.\n(RQ1) To evaluate inconsistencies across different sections of the responses, we"}, {"title": "Conclusion", "content": "Our evaluation framework revealed notable differences between the answers in\nEN and other languages when addressing identical questions. These differences\nhighlight the need for careful consideration of fairness implications when LLMs\nare deployed for medical applications, as they may affect equity in health com-\nmunication and the potential for bias. In future work, we will explore open LLMs\ninstead of GPT-40 to assess fine-grained inconsistencies and enhance the evalu-\nation schema with detailed comparisons of information levels across languages."}]}