{"title": "WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting", "authors": ["Jingjing Wu", "Zhengyao Fang", "Pengyuan Lyu", "Chengquan Zhang", "Fanglin Chen", "Guangming Lu", "Wenjie Pei"], "abstract": "Transcription-only Supervised Text Spotting aims to learn text spotters relying only on transcriptions but no text boundaries for supervision, thus eliminating expensive boundary annotation. The crux of this task lies in locating each transcription in scene text images without location annotations. In this work, we formulate this challenging problem as a Weakly Supervised Cross-modality Contrastive Learning problem, and design a simple yet effective model dubbed WeCromCL that is able to detect each transcription in a scene image in a weakly supervised manner. Unlike typical methods for cross-modality contrastive learning that focus on modeling the holistic semantic correlation between an entire image and a text description, our WeCromCL conducts atomistic contrastive learning to model the character-wise appearance consistency between a text transcription and its correlated region in a scene image to detect an anchor point for the transcription in a weakly supervised manner. The detected anchor points by WeCromCL are further used as pseudo location labels to guide the learning of text spotting. Extensive experiments on four challenging benchmarks demonstrate the superior performance of our model over other methods. Code will be released.", "sections": [{"title": "1 Introduction", "content": "Scene text spotting aims to detect and recognize text instances in scene text images. Existing methods [8,17,18,22,24,26,37-40, 44, 46] for text spotting have achieved remarkable progress relying on fully supervised learning, whereas these methods entail a large amount of annotations of text boundaries, which is extremely labor-consuming. In this work, we investigate transcription-only supervised text spotting, which only requires text transcriptions but no text boundaries for supervised learning, dramatically reducing the annotation overhead.\nTranscription-only supervised text spotting is much more challenging than text spotting in full supervision, owing to the key difficulty of locating text transcriptions in scene text images without annotated text boundaries. A prominent method for transcription-only supervised text spotting is NPTS [29], which formulates the text spotting as a sequence prediction task. Specifically, it concatenates all text instances in a scene image into one sequence and seeks to predict all characters in an auto-regressive manner. While such modeling frees NPTS from text detection, a key limitation is that it suffers from arduous optimizing convergence. This is because there is no predefined order between different text instances when concatenating them together, the optimization of the model has to fit all potential permutations. Moreover, the model does not learn explicitly the mapping between text instances and correlated image regions without text detection, which further increases the difficulty of convergence. As pointed out as a primary limitation in the paper of NPTS, 'the training procedure requires a large number of computing resources'. Another state-of-the-art method for transcription-only supervised text spotting is TOSS [36], which draws inspiration from DETR [1] and locates text instances in scene images by pre-learning a set of text queries. However, the DETR-based method-"}, {"title": "2 Related Work", "content": "Vision-Language Contrastive Learning. Vision-language contrastive learning has attracted increasing attention in recent years. A variety of vision-language contrastive learning methods [3, 7, 9, 11, 15, 16, 32, 43] are proposed for representation learning of both visual information and language prompt. These methods typically focus on learning the semantic correlations between text and image modalities, whereas our method aims to model the character-wise appearance similarity between a text transcription and its correlated region around the anchor point in a scene image. Recently, contrastive learning has also been introduced to OCR. A prominent example is oCLIP [42], which conducts contrastive learning to optimize the image encoder for text spotting. It performs contrastive learning between an image and all the text instances appearing on the image in a holistic manner. Similarly, VLPT [34] also conducts holistic contrastive learning between an entire image and a transcription, and utilizes masked language modeling for auxiliary learning. Unlike oCLIP and VLPT, our WeCromCL seeks to learn the correlation between a transcription and the correlated image region for text location in a weakly supervised learning mode.\nFully Supervised Text Spotting. The mainstream text spotters need precise boundaries for supervision. The typical two-stage methods [5, 10, 17-20, 22, 24, 26, 31, 37, 38] conduct detection and recognition serially and bridge them by Rol pooling operation. Recently several one-stage methods have been proposed. MANGO [30] regards text spotting as a pure text recognition task by a designed position-aware attention module. SRSTS [39,40] decouples recognition from detection and proposes a sampling-based text recognition mechanism. Several works [8,44,46] modify Deformable DETR [47] to deal with text spotting. SPTS [29] represents text instance as a single point and tackles scene text spotting as a sequence prediction task.\nTranscription-only Supervised Text Spotting. Currently, few works conduct text spotting under transcription-only supervision. Kittenplo et al. [14] refines Deformable DETR as an end-to-end text spotter named TTS. TTS is pre-trained on fully annotated synthetic data and fine-tuned on the transcription-only real-word data. It can be seen that TTS still uses a huge number of annotated synthetic data for training. Peng et al. [29] proposes no-point text spotting (NPTS) based on SPTS. NPTS takes transcription-only annotations as supervision and predicts randomly ordered transcriptions appearing in the scene text image. However, such design leads to arduous optimizing convergence and slow inference speed. TOSS [36] is transcription-only supervised and locates text instance by pre-learned queries, and its effectiveness is limited without detection supervision. Unlike the previous methods, we propose to conduct text spotting in two stages to ease transcription-only supervised text recognition problem: 1) detecting the anchor points for transcriptions; 2) conducting text spotting with"}, {"title": "3 Method", "content": "Without annotations of text locations, it is difficult to apply the classical detect-and-recognize spotting paradigm [5,17,18,18,22,24,26,31,37,38] to transcription-only supervised text spotting. In light of this, we circumvent this difficulty by decomposing the task into two stages as shown in Figure 2: 1) detecting an anchor point in the scene image for each transcription to locate the correlated image region, and 2) leveraging the obtained anchor points as pseudo location labels to learn a single-point supervised text spotter which is learned based on only one single point as location annotation. The first step, namely detection of anchor points for transcriptions, is particularly challenging since the annotations of transcription locations are not available. Besides, The performance of text spotting in the second stage relies primarily on the predicting precision of the anchor points in the first stage. Thus, we focus on the first step and formulate it as a weakly supervised atomistic cross-modality contrastive learning problem, then we specifically design a simple yet effective framework dubbed WeCromCL."}, {"title": "3.1 Weakly Supervised Atomistic Cross-Modality Contrastive Learning", "content": "Typical cross-modality contrastive learning between text and image modalities, like CLIP [32] or oCLIP [41], aims to learn the holistic semantic compatibility c between an entire image I and a text description T, which can be formulated as:\n$c = F(I,T),$ (1)\nwhere F denotes the transformation function of a contrastive learning model. In the task of transcription-only supervised text spotting, only the transcriptions contained in a scene image are provided whilst the annotation of text locations for each transcription is not available. Thus, we aim to estimate the location for each transcription to serve as the pseudo location labels for supervised learning of a text spotter. Formally, given an image I containing a set of text transcriptions among which a text transcription T is only associated with its corresponding region in I, the correlation c' can be represented as:\n$c' = F(M\\odot I,T),$ (2)\nwhere M is an activation map whose size is equal to that of I and  denotes element-wise multiplication. All elements of M are binary values indicating whether the corresponding pixel is associated with the transcription T. Since the groundtruth of M is not provided, we have to optimize the contrastive learning model F in a weakly supervised manner. Thus, we refer to such contrastive learning setting as weakly supervised atomistic contrastive learning between image and text modalities.\nFormulating the detection of transcription in an image as the weakly supervised atomistic contrastive learning across modalities defined in Equation 2 involves two crucial challenges:\nChallenge 1: effective modeling of F entails precise estimation of the activation map M in weakly supervised learning without the groundtruth.\nChallenge 2: unlike typical cross-modality contrastive learning such as CLIP that models the holistic semantic correlations between an entire image and a text, we aim to learn the atomistic correlation between a text transcription and its visual appearance in the correlated region in the scene image.\nTo address these challenges, we design a simple yet effective model, namely WeCromCL, for weakly supervised atomistic contrastive learning."}, {"title": "3.2 WeCromCL", "content": "We propose WeCromCL to detect an anchor point for each text transcription to locate its correlated region in the scene image, which serves as the pseudo location label for optimizing the text spotter in the second stage. WeCromCL follows weakly supervised atomistic cross-modality contrastive learning framework. As formulated in Equation 2, it takes a text transcription T and an image I as input, and predicts whether the image contains the transcription by measuring the correlation c' between them. Meanwhile, WeCromCL predicts the activation map M in which the highly activated region corresponds to the associated image region for the text transcription and is identified as the anchor point.\nAs shown in Figure 2, similar to CLIP, WeCromCL employs an image encoder and a text encoder to learn latent embeddings for the input image and text transcription, respectively. In particular, we design a soft modeling mechanism to learn the activation map and thereby deal with Challenge 1. Besides, we devise a character-wise text encoder for tackling Challenge 2, which enables WeCromCL to learn the character-wise appearance similarity between the input transcription and its correlated region in the paired image. Finally, atomistic cross-modality contrastive learning is conducted to optimize the whole model of WeCromCL, using the constructed positive and negative training pairs based on the proposed negative-sampling mining scheme.\nImage Encoder. The image encoder of our WeCromCL first employs BiFPN [35] to extract multi-scale convolutional features, then enhances the feature learning by applying the deformable transformer encoder [47]. The encoded image embeddings for the image I are denoted as $F_I \\in \\mathbb{R}^{w \\times h \\times C}$.\nCharacter-Wise Text Encoder. Typical cross-modality contrastive learning between image and text modalities, like CLIP, focuses on modeling the semantic correlation between two inputs. Thus, the text encoder of such models is designed to learn the holistic semantics of the input text. In contrast, our WeCromCL aims to learn the character-wise appearance consistency between the input text transcription and its visual appearance in the correlated image region. Thus, we devise the text encoder in the similar way as oCLIP [41] so that the encoded text embeddings 1) are distinguishable between different characters and 2) contain the temporal sequence information among characters in the text.\nTo learn text embeddings distinguishable between different characters, we learn individual vectorial embeddings with C dimensions for each character in the alphabet , which is equivalent to learning an embedding matrix $E\\in\\mathbb{R}^{|\\sum|\\times C}$. Then we can encode the text transcription T containing K characters by indexing the corresponding embeddings from E for each character of T sequentially and obtain the text embedding $F_T \\in\\mathbb{R}^{K\\times C}$.\nTo learn the temporal sequence information among characters in the text transcription, we learn extra positional embedding for each character position, resulting in an embedding matrix $P\\in \\mathbb{R}^{L\\times C}$ where L indicates the maximum number of characters in a transcription. As a result, we can encode the temporal information $F'_T\\in \\mathbb{R}^{K \\times C}$ for the transcription T by indexing the positional embedding from P for all characters sequentially. We fuse the text embedding and the positional embedding by character-wise feature addition, and then adopt the Transformer encoder (TE) to perform feature propagation between characters in the transcription to model the correlation between them:\n$F_T = Mean(TE(F_T + F'_T)),$ (3)\nwhere $F_T \\in \\mathbb{R}^C$ is the averaged text embedding over all characters by 'Mean'.\nSoft Modeling of Activation Map by Cross-Modality Cross-Attention. The key to estimating the activation map (M in Equation 2) is how to measure the appearance correlation between the transcription and each pixel of the input image. To this end, we propose a soft modeling mechanism to learn such appearance correlation by measuring the cosine similarity between them in a projected feature space:"}, {"title": "", "content": "$M_{(i,j)} = (W_T\\F_T) (W_I\\F_{I,(i,j)}),$ (4)\n$M = softmax(M).$\nwhere $W_T$ and $W_I$ are learnable transformation matrices. $F_{I,(i,j)}$ denotes the feature of pixel at (i, j) in the image I.\nThe values of learned M are continuous values between [0, 1] instead of binary values while higher values indicate higher response to the transcription. Learning the activation map in such a soft modeling way eases the gradient propagation for optimization and can preserve richer similarity information than the hard representation by binary values. The most activated pixel with the peak value in the map can be identified as the anchor point for the transcription.\nFollowing the formulation in Equation 2, the learned activation map M is further used to aggregate the correlated features in the image to the text transcription for subsequent contrastive learning:\n$F_{I,T} = \\sum_{i=0}^{w-1}\\sum_{j=0}^{h-1}M_{(i,j)}(W_I\\F_{I,(i,j)}),$ (5)\nwhere $F_{I,T} \\in \\mathbb{R}^C$ is the aggregated correlated visual features in the image I to the transcription T and $W_I$ is a learnable matrix for feature transformation. Combining the soft modeling in Equation 4 and the aggregation of correlated features in Equation 5 essentially boils down to cross-modality cross-attention operation, where the encoded transcription feature $F_T$ serves as the query while the all pixels of encoded image feature $F_I$ serve as the keys and values.\nCross-Modality Contrastive Learning by Negative-Sample Mining. We perform cross-modality contrastive learning between the learned correlated visual feature $F_{I,T}$ and the encoded transcription feature $F_T$ to optimize all modules of WeCromCL jointly. Similar to CLIP, for a positive training pair between an image I and a transcription T, we construct negative pairs in two ways: either pair the image I to multiple unpaired transcriptions (termed as image-to-text construction) or pair the transcription T to multiple unpaired images (termed as text-to-image construction).\nWe maximize the Cosine similarity of positive pairs while minimizing the similarity of negative pairs. Formally, given a training batch of images {I0, I1,..., IN-1} and their associated text transcriptions {T0, T1, ..., TN\u22121}, the loss function for the positive pair (Ii, Ti) and negative pairs using the text-to-image construction is defined as:\n$\\mathcal{L}_{i}^{T2I} = -log\\frac{exp(Cosine(F_{I_i,T_i}, F_{T_i})/\\tau)}{\\sum_{j=0}^{N-1}exp(Cosine(F_{I_i,T_i}, F_{T_j})/\\tau)},$ (6)\nSimilarly, we can define the loss function for the positive pair (Ii, Ti) and negative pairs using the image-to-text construction. In particular, we devise a negative-sample mining scheme to introduce more challenging negative pairs and"}, {"title": "", "content": "thereby enhance the modeling robustness of WeCromCL. A straightforward way is to apply the hard-sample mining scheme that selects more similar but unpaired transcriptions with Ii to construct more hard negative pairs. Surprisingly, we observe that randomly selecting unpaired transcriptions from the training set can also yield similar performance gain compared to hard-sample mining scheme as long as sufficient unpaired samples are provided. Thus, the loss based on such negative-sample mining scheme is defined as:\n$\\mathcal{L}_{i}^{I2T} = -log\\frac{exp(Cosine (F_{I_i}, F_{T_i})/\\tau)}{\\sum_{j=0}^{N+N_{aug}} exp(Cosine(F_{I_i}, F_{T_j})/\\tau)},$ (7)\nwhere Naug is the number of augmented negative pairs. Note that we only augment the unpaired transcriptions during image-to-text construction of negative pairs instead of augmenting the unpaired images during text-to-image construction, such negative-sample mining scheme can be performed quite efficiently with negligible overhead. Integrating the losses in two ways of negative pair construction, the loss of contrastive learning for a batch of N images is:\n$\\mathcal{L}_{cm} = \\frac{1}{2N}\\sum_{i=0}^{N-1}(\\mathcal{L}_{i}^{T2I} + \\mathcal{L}_{i}^{I2T}).$ (8)\nRationale behind WeCromCL. Our WeCromCL learns an effective cross-modality character-wise consistency metric between a transcription and the visual appearance in a scene image based on atomistic contrastive learning. It is able to detect the correlated region in the image to the transcription in a weakly supervised mode. The rationale behind this is that a transcription acts as a cluster center that associates all paired images with it, and the model is optimized to learn the similar appearance pattern regarding this transcription among all the paired images to determine the activation map. Meanwhile, the optimization by minimizing the similarity between negative pairs can guide the model to learn discriminative appearance patterns for each transcription, thereby preventing the model from collapsing to a uniform pattern for different transcriptions."}, {"title": "3.3 Anchor-Guided Text Spotting", "content": "The most activated position in an activation map M learned from WeCromCL is identified as the anchor point for the corresponding transcription, which is further used as pseudo location label for learning the text spotter in the second stage. Theoretically, any existing single-point supervised text spotter can be readily applied to our framework. To validate the effectiveness of our WeCromCL, we conduct two instantiations of the text spotter in the second stage. We first instantiate it with SPTS [29], a prominent single-point supervised text spotter. Then we tailor a single-point text spotter specifically by adapting SRSTS v2 [39, 40], which is a state-of-the-art supervised text spotter, to construct an powerful transcription-only text spotting system."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nBenchmarks. 1) ICDAR 2013 [13] contains 229 training images and 223 testing images, in which most text instances are horizontal or slightly rotated. It provides 'Strong', 'Weak', and 'Generic' lexicons, which are represented as 'S', 'W' and 'G' in Table 5. 'S' denotes a lexicon containing 100 words, including the groundtruth transcription, which is provided for each test image. 'W' means a lexicon that consists of all the words appearing in the test set. 'G' is a generic lexicon provided by Liao et al. [17]. 2) ICDAR 2015 [12] contains 1000 training images and 500 testing images. It involves oriented text instances annotated with quadrangles. 3) Total-Text [2] comprises 1255 and 300 images for training and test, respectively. Most samples in this dataset are curved and are annotated with polygons and word-level transcriptions. 'Full' lexicon is provided which includes all words in the testing set. 4) CTW1500 [23] consists of 1000 training images and 500 test images. The text instances are annotated at line-level and arbitrary-shaped. 'Full' lexicon is provided for evaluation.\nEvaluation Protocol. Since our method only outputs transcriptions and corresponding anchor points, the evaluation protocol for fully supervised methods which relies on precise bounding box matching is not suitable for our method. We adopt the single-point and edit distance metrics, following SPTS [29]. For the single-point metric, we match each predicted anchor point with the nearest center point of groundtruth bounding boxes, and then check if their text content are consistent. As for the edit distance metric, matching is conducted solely based on the edit distance between predicted and groundtruth transcriptions.\nImplementation details. Following the previous methods [22,24,29], we train our method on a joint training set which consists of training images from Curved Synthetic Dataset 150k [22], ICDAR 2017 MLT [28], ICDAR 2013, ICDAR 2015 and Total-Text. In the first stage, we employ WeCromCL to generate the pseudo location labels for all training images. The obtained pseudo location labels are further used as supervision in the text spotting stage. Detailed settings are illustrated in the supplementary material."}, {"title": "4.2 Ablation Studies of WeCromCL", "content": "In this section, we conduct ablation studies to investigate the effectiveness of proposed method. Note that more ablation studies and qualitative results are provided in the supplementary materials.\nComparison between different text encoders. We further compare the character-wise text encoder of our WeCromCL with the token-wise text encoder of CLIP which focuses on learning semantics of the entire text. To be specific, we replace the text encoder of WeCromCL with the token-wise text encoder of CLIP and test its performance of transcription detection. We consistently use the prompt template \"There is a word 'transcription'\" for text encoding by CLIP, where \u2018transcription' corresponds to the input text.\nTable 1 shows that our model performs substantially better on all three metrics when equipped with the designed character-wise text encoder than using the token-wise encoder of CLIP. The results demonstrate that, compared to a token-wise text encoder that prioritizes semantic matching, encoding text at the character level can facilitate the learning of character-level correlations between a transcription and its visual representation in the image.\nAblation on the negative-sample mining scheme. To investigate the effectiveness of the proposed negative-sample mining scheme in WeCromCL, we compare the performance of two variants of WeCromCL: training with the negative-sample mining scheme and without the negative-sample mining scheme. The results in Table 2 show that the negative-sample mining scheme yields large performance gain. Particularly, the F-measure on the training and test set of CTW1500 is improved by 7.2% and 10.2%, respectively.\nQuantitative evaluation of transcription detection. As shown in Table 1 and Table 2, the obtained pseudo labels for training set are accurate and can"}, {"title": "4.3 Transcription-only Supervised Text Spotting", "content": "In this section, we evaluate our optimized system for transcription-only supervised text spotting, namely the integration of WeCromCL + SRSTS-A.\nQuantitative evaluation. We evaluate our transcription-only spotting system on four benchmarks. For a fair comparison, we remove Random Cropping operation and re-train NPTS. As shown in Table 5, we achieve superior performance when compared with other transcription-only supervised methods. In particular, our method surpasses NPTS and TOSS by 16.4% and 9.2% on ICDAR 2015 when evaluated with generic lexicon. Our method also performs well on the challenging Total-Text with curved text. Supervised by WeCromCL, SRSTS-A impressively outperforms TOSS by 5% and NPTS by 6.5% in the metric of 'None'.\nTTSweak [14] is a semi-supervised text spotter which follows DETR-based spotting framework. It is trained on fully-annotated synthetic data (including annotations of text boundaries) and transcription-annotated real-world data. As shown in Table 5, our method still achieves comparable performance compared to TTSweak although our spotting system only uses pseudo location labels generated by our WeCromCL for all data.\nQualitative evaluation. We visualize the spotting results in Figure 5. As shown, our optimized system can handle various challenging scenarios, like tiny, fuzzy, curved and long text. The visualization results indirectly indicate the effectiveness and robustness of WeCromCL."}, {"title": "5 Conclusion", "content": "In this work, we decompose the transcription-only supervised text spotting into two stages: 1) detecting the anchor point for each transcription and 2) conducting text spotting guided by the obtained anchor points, among which the first stage is quite challenging and crucial. We formulate the detection of anchor points for text transcriptions as a weakly supervised atomistic contrastive learning problem across modalities, and devise a simple yet effective method dubbed WeCromCL for it. The detected anchor points are further used to guide the learning of text spotting. Extensive experiments on challenging benchmarks demonstrate the effectiveness and advantages of our proposed method.\nLimitations. To measure the character-wise appearance consistency accurately between a transcription and the correlated region in the scene image, our We-CromCL requires high resolution of input images for detecting small texts."}]}