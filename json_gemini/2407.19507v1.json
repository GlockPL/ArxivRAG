{"title": "WeCromCL: Weakly Supervised Cross-Modality\nContrastive Learning for Transcription-only\nSupervised Text Spotting", "authors": ["Jingjing Wu", "Zhengyao Fang", "Pengyuan Lyu", "Chengquan Zhang", "Fanglin Chen", "Guangming Lu", "Wenjie Pei"], "abstract": "Transcription-only Supervised Text Spotting aims to learn\ntext spotters relying only on transcriptions but no text boundaries for\nsupervision, thus eliminating expensive boundary annotation. The crux\nof this task lies in locating each transcription in scene text images without\nlocation annotations. In this work, we formulate this challenging problem\nas a Weakly Supervised Cross-modality Contrastive Learning problem,\nand design a simple yet effective model dubbed WeCromCL that is\nable to detect each transcription in a scene image in a weakly supervised\nmanner. Unlike typical methods for cross-modality contrastive learning\nthat focus on modeling the holistic semantic correlation between an en-\ntire image and a text description, our WeCromCL conducts atomistic\ncontrastive learning to model the character-wise appearance consistency\nbetween a text transcription and its correlated region in a scene image\nto detect an anchor point for the transcription in a weakly supervised\nmanner. The detected anchor points by WeCromCL are further used as\npseudo location labels to guide the learning of text spotting. Extensive\nexperiments on four challenging benchmarks demonstrate the superior\nperformance of our model over other methods. Code will be released.", "sections": [{"title": "1 Introduction", "content": "Scene text spotting aims to detect and recognize text instances in scene text\nimages. Existing methods [8,17,18,22,24,26,37-40, 44, 46] for text spotting have\nachieved remarkable progress relying on fully supervised learning, whereas these"}, {"title": "2 Related Work", "content": "Vision-Language Contrastive Learning. Vision-language contrastive learn-\ning has attracted increasing attention in recent years. A variety of vision-language\ncontrastive learning methods [3, 7, 9, 11, 15, 16, 32, 43] are proposed for represen-\ntation learning of both visual information and language prompt. These methods\ntypically focus on learning the semantic correlations between text and image\nmodalities, whereas our method aims to model the character-wise appearance\nsimilarity between a text transcription and its correlated region around the an-\nchor point in a scene image. Recently, contrastive learning has also been intro-\nduced to OCR. A prominent example is oCLIP [42], which conducts contrastive\nlearning to optimize the image encoder for text spotting. It performs contrastive\nlearning between an image and all the text instances appearing on the image in\na holistic manner. Similarly, VLPT [34] also conducts holistic contrastive learn-\ning between an entire image and a transcription, and utilizes masked language\nmodeling for auxiliary learning. Unlike oCLIP and VLPT, our WeCromCL seeks\nto learn the correlation between a transcription and the correlated image region\nfor text location in a weakly supervised learning mode.\nFully Supervised Text Spotting. The mainstream text spotters need pre-\ncise boundaries for supervision. The typical two-stage methods [5, 10, 17-20, 22,\n24, 26, 31, 37, 38] conduct detection and recognition serially and bridge them\nby Rol pooling operation. Recently several one-stage methods have been pro-\nposed. MANGO [30] regards text spotting as a pure text recognition task by\na designed position-aware attention module. SRSTS [39,40] decouples recogni-\ntion from detection and proposes a sampling-based text recognition mechanism.\nSeveral works [8,44,46] modify Deformable DETR [47] to deal with text spot-\nting. SPTS [29] represents text instance as a single point and tackles scene text\nspotting as a sequence prediction task.\nTranscription-only Supervised Text Spotting. Currently, few works con-\nduct text spotting under transcription-only supervision. Kittenplo et al. [14]\nrefines Deformable DETR as an end-to-end text spotter named TTS. TTS is pre-\ntrained on fully annotated synthetic data and fine-tuned on the transcription-\nonly real-word data. It can be seen that TTS still uses a huge number of anno-\ntated synthetic data for training. Peng et al. [29] proposes no-point text spotting\n(NPTS) based on SPTS. NPTS takes transcription-only annotations as super-\nvision and predicts randomly ordered transcriptions appearing in the scene text\nimage. However, such design leads to arduous optimizing convergence and slow\ninference speed. TOSS [36] is transcription-only supervised and locates text in-\nstance by pre-learned queries, and its effectiveness is limited without detection\nsupervision. Unlike the previous methods, we propose to conduct text spotting\nin two stages to ease transcription-only supervised text recognition problem: 1)\ndetecting the anchor points for transcriptions; 2) conducting text spotting with"}, {"title": "3 Method", "content": "Without annotations of text locations, it is difficult to apply the classical detect-\nand-recognize spotting paradigm [5,17,18,18,22,24,26,31,37,38] to transcription-\nonly supervised text spotting. In light of this, we circumvent this difficulty by\ndecomposing the task into two stages as shown in Figure 2: 1) detecting an\nanchor point in the scene image for each transcription to locate the correlated\nimage region, and 2) leveraging the obtained anchor points as pseudo location\nlabels to learn a single-point supervised text spotter which is learned based on\nonly one single point as location annotation. The first step, namely detection of\nanchor points for transcriptions, is particularly challenging since the annotations\nof transcription locations are not available. Besides, The performance of text\nspotting in the second stage relies primarily on the predicting precision of the\nanchor points in the first stage. Thus, we focus on the first step and formulate\nit as a weakly supervised atomistic cross-modality contrastive learning problem,\nthen we specifically design a simple yet effective framework dubbed WeCromCL.\n3.1 Weakly Supervised Atomistic Cross-Modality Contrastive\nLearning\nTypical cross-modality contrastive learning between text and image modalities,\nlike CLIP [32] or oCLIP [41], aims to learn the holistic semantic compatibility"}, {"title": "3.2 WeCromCL", "content": "We propose WeCromCL to detect an anchor point for each text transcription to\nlocate its correlated region in the scene image, which serves as the pseudo loca-\ntion label for optimizing the text spotter in the second stage. WeCromCL follows\nweakly supervised atomistic cross-modality contrastive learning framework. As\nformulated in Equation 2, it takes a text transcription T and an image I as\ninput, and predicts whether the image contains the transcription by measuring\nthe correlation c' between them. Meanwhile, WeCromCL predicts the activation\nmap M in which the highly activated region corresponds to the associated image\nregion for the text transcription and is identified as the anchor point.\nAs shown in Figure 2, similar to CLIP, WeCromCL employs an image encoder\nand a text encoder to learn latent embeddings for the input image and text"}, {"title": "", "content": "c = F(I,T), (1)"}, {"title": "", "content": "c' = F(MI,T), (2)"}, {"title": "", "content": "FT = Mean(TE(F+ + F)), (3)"}, {"title": "", "content": "M(i,j) = \\frac{(W_TFT) (W_IF_{I,(i,j)})}{\\sum_{i=0}^{w-1}\\sum_{j=0}^{h-1} (W_TFT) (W_IF_{I,(i,j)})}, \nM = \\text{softmax}(M). (4)"}, {"title": "", "content": "FI,T = \\sum_{i=0}^{w-1}\\sum_{j=0}^{h-1} M_{(i,j)}(W_IF_{I,(i,j)}), (5)"}, {"title": "", "content": "L^{I2T} = -log\\frac{exp\\left(Cosine(F_{I,T}, F_T) / \\tau \\right)}{\\sum_{j=0}^{N-1}exp\\left(Cosine(F_{I_j,T}, F_T) / \\tau \\right)}, (6)"}, {"title": "", "content": "L^{I2T} = -log\\frac{exp\\left(Cosine(F_{I,T}, F_T) / \\tau \\right)}{\\sum_{j=0}^{N+N_{aug}-1}exp\\left(Cosine(F_{I_j,T}, F_{T_j}) / \\tau \\right)}, (7)"}, {"title": "", "content": "L_{cm} = \\frac{1}{2N}\\sum_{i=0}^{N-1} \\left(L_i^{I2T} + L_i^{T2I}\\right). (8)"}, {"title": "3.3 Anchor-Guided Text Spotting", "content": "The most activated position in an activation map M learned from WeCromCL\nis identified as the anchor point for the corresponding transcription, which is\nfurther used as pseudo location label for learning the text spotter in the second\nstage. Theoretically, any existing single-point supervised text spotter can be\nreadily applied to our framework. To validate the effectiveness of our WeCromCL,\nwe conduct two instantiations of the text spotter in the second stage. We first\ninstantiate it with SPTS [29], a prominent single-point supervised text spotter.\nThen we tailor a single-point text spotter specifically by adapting SRSTS v2 [39,\n40], which is a state-of-the-art supervised text spotter, to construct an powerful\ntranscription-only text spotting system."}, {"title": "5 Conclusion", "content": "In this work, we decompose the transcription-only supervised text spotting into\ntwo stages: 1) detecting the anchor point for each transcription and 2) conducting\ntext spotting guided by the obtained anchor points, among which the first stage\nis quite challenging and crucial. We formulate the detection of anchor points for\ntext transcriptions as a weakly supervised atomistic contrastive learning problem\nacross modalities, and devise a simple yet effective method dubbed WeCromCL\nfor it. The detected anchor points are further used to guide the learning of\ntext spotting. Extensive experiments on challenging benchmarks demonstrate\nthe effectiveness and advantages of our proposed method.\nLimitations. To measure the character-wise appearance consistency accurately\nbetween a transcription and the correlated region in the scene image, our We-\nCromCL requires high resolution of input images for detecting small texts."}]}