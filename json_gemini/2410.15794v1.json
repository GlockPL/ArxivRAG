{"title": "HABAEK: HIGH-PERFORMANCE WATER SEGMENTATION THROUGH DATASET EXPANSION AND INDUCTIVE BIAS OPTIMIZATION", "authors": ["Hanseon Joo", "Eunji Lee", "Minjong Cheon"], "abstract": "Water segmentation is critical to disaster response and water resource management. Authorities may employ high-resolution photography to monitor rivers, lakes, and reservoirs, allowing for more proactive management in agriculture, industry, and conservation. Deep learning has improved flood monitoring by allowing models like CNNs, U-Nets, and transformers to handle large volumes of satellite and aerial data. However, these models usually have significant processing requirements, limiting their usage in real-time applications. This research proposes upgrading the SegFormer model for water segmentation by data augmentation with datasets such as ADE20K and RIWA to boost generalization. We examine how inductive bias affects attention-based models and discover that SegFormer performs better on bigger datasets. To further demonstrate the function of data augmentation, Low-Rank Adaptation (LoRA) is used to lower processing complexity while preserving accuracy. We show that the suggested Habaek model outperforms current models in segmentation, with an Intersection over Union (IoU) ranging from 0.91986 to 0.94397. In terms of F1-score, recall, accuracy, and precision, Habaek performs better than rival models, indicating its potential for real-world applications. This study highlights the need to enhance structures and include datasets for effective water segmentation.", "sections": [{"title": "1 Introduction", "content": "Water segmentation and monitoring are vital for effective disaster response and water resource management [Loucks and Van Beek, 2017]. Using high-resolution imagery, authorities can monitor rivers, lakes, and reservoirs to track changes over time [Marc\u00e9 et al., 2016]. This enables proactive management of water supplies for agriculture, industry, and drinking purposes, as well as monitoring pollution sources and describing buildup to ensure water conservation.\nDeep learning has significantly improved the efficiency of flood monitoring and prediction, making water segmentation faster and more accurate. Models like CNNs, U-Nets, and even transformers proved that they can handle large volumes of satellite and aerial imagery with precision, even in complex environments [Zhao et al., 2024][Zhang et al., 2023a][Zhang et al., 2023b]. These advancements help automatically identify flood extents, providing valuable information to authorities during flooding events.\nThe integration of remote sensing technologies further enhances these deep learning models. Remote sensing ensures that accurate data is always available for flood monitoring, even in adverse weather conditions [Kussul et al., 2011]. Deep learning and remote sensing technologies work collaboratively to significantly improve flood monitoring and prediction. Several studies were also conducted, focusing on how models like U-Net, DeepLabv3+, and their variants"}, {"title": "2 Materials and Methods", "content": "In this study, we utilized a total of three datasets for training and validation. The first dataset is available at: Water Segmentation Dataset. From this dataset, we utilized the river-related train and validation images from the ADE20K dataset. The train set comprises 1,727 images, while the validation set contains 161 images. The second dataset, named River Water Segmentation (RIWA), focuses on the binary segmentation classification of river water and is available at RIWA Dataset. It includes river photos captured by smartphones, drones, and DSLR cameras, along with water segmentation data and a subset of river-related images from ADE20K. We utilized RIWA version 2, which contains 1,142 training images and 167 validation images. The third dataset, Lufi-Riversnap, is available at: Lufi-Riversnap Dataset. It consists of river images taken by UAVs, surveillance cameras, smartphones, and regular cameras. To enhance the dataset's diversity and accuracy, it also incorporates some images from both the first and second datasets. This dataset includes 657 training images, 202 validation images, and 233 test images. Since some datasets referenced the same sources, we conducted a cross-check for duplicate images between the train/validation and test sets. Any overlapping images were removed from the train/validation sets to ensure the integrity of the experiments."}, {"title": "2.2 Segformer", "content": "SegFormer, created by Xie et al., has two primary components: a hierarchical Transformer encoder and a lightweight all-MLP decoder. The hierarchical Transformer encoder pulls multi-scale features from the input picture without using positional encoding, which helps to prevent performance loss when test and training image resolutions change. The encoder produces multi-level features at various resolutions of the source picture, which are then utilized to increase segmentation accuracy. Unlike ViT, which employs 16x16 patches, SegFormer breaks the input picture into smaller 4x4 patches that are more suited to segmentation tasks. Furthermore, the encoder applies a sequence reduction method to reduce the complexity of self-attention, hence enhancing computational efficiency. SegFormer also uses overlapping patch merging to provide local continuity between patches. In contrast to non-overlapping techniques, the model provides overlapping sections that retain local context by specifying patch size, stride, and padding. The lightweight all-MLP decoder collects information from the encoder's multi-level features. It uses the hierarchical encoder's huge effective receptive field to combine information without the need for complicated handmade components. This design makes the decoder efficient while still providing powerful representations for segmentation [Xie et al., 2021]."}, {"title": "2.3 Inductive Bias", "content": "Indutive bias is the assumption that machine learning or deep learning models make about provided datasets. This inductive bias allows the models to effectively optimize from training data to unseen data. For example, CNN employs"}, {"title": "2.4 Low-Rank Adaptation: LoRA", "content": "LORA (Low-Rank Adaptation) advances neural networks by lowering dimensional complexity while preserving performance. Traditional neural networks utilize full-rank weight matrices, however, pretrained models frequently have a low intrinsic dimension, which means they may still learn effectively when transferred to smaller subspaces via random projections. The intrinsic dimension refers to the minimum number of dimensions required to express essential information, while random projection reduces dimensionality without losing significant data integrity.\nLORA applies these concepts by decomposing the weight matrix $W_o$ (with dimensions $d \\times k$) into two smaller matrices: $B$ (dimensions $d \\times r$) and $A$ (dimensions $r \\times k$), where r is much smaller than d and k. During training, the original weight $W_o$ remains fixed, and only the low-rank matrices A and B are updated. The resulting decomposition is expressed as:\n$W_o + \\Delta W = W_o + BA$\nwhere $\\Delta W$ is initially set to zero. LoRA further scales gradient updates by $\\alpha/r$, with $\\alpha$ acting as a constant similar to a learning rate. LoRA's parameter-efficient approach enables models to adapt to individual tasks while using few resources, making it particularly useful for transfer learning and segmentation tasks."}, {"title": "3 Result", "content": "In Table 1, we compared the average training time per epoch for various models on the LuFI dataset, along with their respective parameter counts. The Habaek model, which we proposed, stood out with a significantly reduced training time of 394 seconds per epoch, despite having 82.0M parameters, larger than most of the compared models. Other models like U-Net, PSPNet, DeepLabV3+, PAN, and LinkNet had training times ranging from 289 to 313 seconds per epoch, with parameter counts between 24M and 32M. Notably, SAM, the model with the highest number of parameters"}, {"title": "4 Conclusion", "content": "This work conducted several tests to improve the performance of deep learning-based water segmentation. For the experiment, we chose the SegFormer model, which has a lightweight design in comparison to many other models. The essence of our suggested strategy is based on utilizing the inductive bias of the given model. We set a hypothesis that since the SegFormer uses an attention mechanism, which implies lower inductive bias could be enhanced with larger datasets. We used data augmentation, including ADE20k and RIWA in the training process to prove our hypothesis. Habaek, our proposed model, demonstrates exceptional performance and efficiency, establishing itself as a dependable choice for water body segmentation tasks. Habaek requires less training time per epoch than other models, even though it consists of more parameters. With the Intersection over Union (IoU) rising from 0.91986 to 0.94278, the segmentation accuracy was significantly improved by including ADE20k and RIWA datasets. Furthermore, using LoRA adaptation on bigger datasets improved model performance efficiently. Habaek outperformed other models on important measures like as accuracy, precision, recall, and F1-score, demonstrating its efficacy for real-world applications. To sum up, this paper proves that Habaek performs at state-of-the-art (SOTA) levels in water segmentation tasks. This result also underscores the necessity of data augmentation, particularly for models with low inductive bias."}]}