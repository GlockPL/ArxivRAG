{"title": "CAUSALVE: FACE VIDEO PRIVACY ENCRYPTION VIA CAUSAL VIDEO PREDICTION", "authors": ["Yubo Huang", "Xin Lai", "Wenhao Feng", "Zixi Wang", "Fan Chen", "Jingzehua Xu", "Shuai Zhang", "Hongjie He"], "abstract": "Advanced facial recognition technologies and recommender systems with inadequate privacy tech- nologies and policies for facial interactions increase concerns about bioprivacy violations. With the proliferation of video and live-streaming websites, public-face video distribution and interactions pose greater privacy risks. Existing techniques typically address the risk of sensitive biometric information leakage through various privacy enhancement methods but pose a higher security risk by corrupting the information to be conveyed by the interaction data, or by leaving certain biometric features intact that allow an attacker to infer sensitive biometric information from them. To address these shortcomings, in this paper, we propose a neural network framework, CausalVE. We obtain cover images by adopting a diffusion model to achieve face swapping with face guidance and use the speech sequence features and spatiotemporal sequence features of the secret video for dynamic video inference and prediction to obtain a cover video with the same number of frames as the secret video. In addition, we hide the secret video by using reversible neural networks for video hiding so that the video can also disseminate secret data. Numerous experiments prove that our CausalVE has good security in public video dissemination and outperforms state-of-the-art methods from a qualitative, quantitative, and visual point of view.", "sections": [{"title": "1 INTRODUCTION", "content": "With the widespread adoption of smart devices and the Internet of Things (IoT), the security issues of biological face privacy are becoming increasingly unavoidable. The explosion of public face video distribution for IoT, exemplified by YouTube, TikTok, and Instagram, makes it difficult to protect face privacy during video interaction and distribution. In addition, the autonomy of public face video distribution and interaction on video websites means that disguised face videos must convey the same visual video information effect as the original video and hide sensitive personal privacy information.\nCurrent face privacy measures mainly focus on destroying or hiding facial attributes. In video sequences, face attributes are destroyed by replacing the region where the person is located with blank information or by blurring and pixellating face attributes from the detector. These methods directly damage the biometric features in facial videos, destroying the usability of data interactions and even failing to leave any useful information in interactions and propagation. To strike a balance between privacy preservation and information extraction, preserve sensitive information by identifying and selectively deleting, replacing, or hiding sensitive information while preserving non-private information. The definition of sensitive information and the attacker's ability to infer hidden personal attributes from non-private information make the feasibility of these methods in preserving privacy in real-world interactions challenging. In this case, we believe that a better way to protect privacy is to perform direct and complete data hiding. One of the ways of data hiding is steganography."}, {"title": "2 METHODOLOGY", "content": "The purpose of steganography is to encode sensitive information in some transmission medium and communicate covertly with a recipient who has a key to recover the secret information. proposed an image steganography method based on the human visual system, which utilizes a multi-base symbol system to dynamically adjust the embedding strength to ensure high imperceptibility. However, nowadays digital video is gradually replacing images as the main communication medium, and video has a greater capacity to carry secret information, so video steganography is more necessary for development nowadays. Video steganography is a technique to embed information into the cover content. proposed a novel high-capacity convolutional video steganography model, which can hide a complete video clip in a video. proposed a new 12-dimensional universal feature set, which is capable of detecting video steganography in a variety of embedded domains. However, in the public channel, the cover video needs to have the function of disseminating the original information, while the cover video of steganography is usually chosen randomly, and the duration and information conveyed are not matched with the original video. How to match the cover face video with the information that needs to be disseminated by the secret video is the difficulty of applying steganographic methods such as video hiding to public video interactions.\nWith the great success of diffusion modeling in the field of image generation, video generation techniques have also come into the limelight. proposed a joint MM-Diffusion audio-video generation framework consisting of a sequential multimodal U-Net for designing a joint denoising process. In the area of video prediction and causal inference, researchers have worked on developing advanced algorithms and techniques for accurate prediction of video content and causal analysis. proposed a new and efficient Transformer block for video feature learning, which reduces the complexity of a standard Transformer complexity. proposed a Dynamic Multi-scale Voxel Flow Network (DMVFN) that uses RGB images to achieve better video prediction performance at lower computational cost. Causal inference, on the other hand, helps researchers to gain a deeper understanding of causal relationships in videos in order to better control where and how the secret information is embedded. investigate the structure of relationships from the perspective of causal representation of multimodal data, and propose a novel inference framework for Video Question and Answer (VideoQA). proposes a Context-Aware Video Intent Reasoning Model (CaVIR) to address the special VideoQA task of video intent reasoning. These studies give us new research directions for conducting privacy-secure propagation of public face videos. For a more detailed explanation of the principles, we explain the related work on video Steganography, video prediction, and face generation in appendix A.\nTherefore, we introduce \"CausalVE,\" an innovative framework for face-video privacy interaction, which significantly advances the field of video steganography and privacy protection. The novel approach integrates dynamic causal reasoning with reversible neural networks to seamlessly blend the original video content with generated cover face videos. This not only effectively conceals the identity and sensitive information within videos but also ensures that the authenticity and expressiveness of the facial features are maintained across the video timeline. The primary contributions are:\n\u2022 Introduction of Dynamic Causal Reasoning for Video Prediction: The use of causal reasoning to guide the video prediction process helps in creating cover videos that are not only visually convincing but also capable of carrying hidden information without detectable alterations.\n\u2022 Reversible Neural Network for Video Hiding and Recovery: The framework uses a reversible neural network that allows for the original video to be hidden within a pseudo video and accurately recovered using a key. This method provides a robust way to secure personal data while still allowing the video to be used in public channels.\n\u2022 Hybrid Diffusion Model for Face Swapping: By incorporating a hybrid diffusion model that uses identity features and controlled noise processes, the system generates high-fidelity facial transformations that preserve the natural dynamics and expressions of the original video."}, {"title": "2.1 CAUSALVE: A FRAMEWORK FOR FACE VIDEO PRIVACY INTERACTION", "content": "Figure 1 shows the specific framework of our CausalVE. For a given face video, we first extract the first frame for face replacement. Then, the physical information of the original video is used as a guide and the causal analysis framework is applied to build the time series of the overlay video Rai for video prediction. Finally, the overlay video is generated. The original video is hidden in the overlay video by a reversible neural network to generate a pseudo-video with the information of the original video. Anyone can view the pseudo video directly and the key holder can restore the pseudo video to the original video by using the key."}, {"title": "2.2 COVER VIDEO GENERATION", "content": "Since the cover face video is not only for the attacker but also for the public channel at the same time, the generation of the cover face video needs to consider not only the detection of counterfeiting tools but also the visual representation of the face. Therefore, we propose the use of dynamic causal reasoning to support video prediction for the generation of cover-face videos. In the following, we will elaborate on the network module for cover face video generation."}, {"title": "2.2.1 FACE SWAPPING MODULE", "content": "For a video space $R^{3 \\times H \\times W \\times T}$, we divide into n video frames by frequency. Take the first frame image as $I_s$ for face-swapping. In order to make the transformed face have better security on this graph space, and at the same time make the transformed face connect with the post- time series better, we draw on the idea of the diffusion model to perform image face-swapping.\nConsider $I_s$ and $I_t$ to represent the secret and target im- ages, respectively, each containing distinct facial features $F_s$ and $F_t$. The primary goal of this research is to con- struct a cover image $I_t$, wherein $F_t$ is seamlessly replaced by $F_s$, while meticulously preserving the identical pose and expression.\nLet $I$ denote an image within the space $R^{3 \\times H \\times W}$. For identity embedding, we employ the ArcFace model. Upon embedding the secret image $I_s$, we obtain the identity feature $I_{id}$. This feature $I_{id}$ is then integrated into the diffusion model $A_{\\theta}(x_t, t, I_{id})$. At each time step t, $I_s$ is subject to a prior process that aims to reconstruct $I_t$ utilizing a standard Gaussian distribution, which is achieved by reversing the recursive noise addition, as defined in the following equation:\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta}x_{t-1}, \\beta_tI)$\nwhere $\\beta_t$ is a predefined variance schedule. The inverse diffusion process is described by:\n$P_{\\theta}(X_{t-1}|X_t) = N(x_{t-1}; \\mu_{\\theta}(x_t, t), \\delta_{\\theta}(x_t, t))$\nTo facilitate the generation of the cover image, multiple expert models are utilized to provide nuanced facial guidance, thus enhancing the fidelity of the synthesized image. The incorporation of multiple models often introduces various forms of noise, complicating the retention of the target background during the face-swapping process. To mitigate this, we introduce a novel target-preserving hybrid method that modulates the mask's strength by gradually increasing its intensity from 0 to 1 over the diffusion process duration T. This modulation is strategically controlled to ensure the preservation of the target image's structural integrity, as expressed by:\n$U_t = min {1, \\frac{1}{T}}\\mu , T-tu$\nHere, $U$ denotes the rigid mask derived through the face parsing process, and $U_t$ represents the dynamic mask, which increases in intensity progressively. The threshold \u2191 defines the critical point at which the mask transitions to its full intensity.\nThe blending of the intermediate predictions $x_{t-1}$ with the target images is then performed using the mask $U_t$ in the reverse process:\n$x_{t-1} = (1 - U_t) \\cdot p_{\\theta}(x_t, t) + U_t \\cdot \\theta(x_t, t, I_{id})$\nThis process effectively allows the integration of facial features from $I_s$ into $I_t$, culminating in the creation of the desired cover image $I_t$, which maintains the original pose and expression of the target while featuring the secret identity."}, {"title": "2.2.2 INTERACTION MODULE", "content": "The original video contains rich temporal, spatial, and physical information. To utilize this information for assisting subsequent dynamic video reasoning, we design an interaction model to incorporate various desirable types of inputs. As shown in Figure 2, this module fuses the audio sequence information with the currently selected face-changing picture frame to generate a new representation and updates the fusion process by simulating facial head movement through conditional VAE for cover video prediction.\nSpecifically, we set the first frame of the secret video to the face-swapping picture $I_s$. At this time, in the 3D deformable model (3DMM), the face shape can be expressed by the following formula:\n$F = \\overline{F} + a_{rid} + B_{rex}$"}, {"title": "2.2.3 RE-PREDICTION AND DECISION MODULE", "content": "During the interaction between the target image and the original video voice sequence, we use voice-controlled video generation to obtain the cover video. However, in such a prediction process, the cover video controls the expression and character characteristics through $B_0$ and controls the character style through the specified function $Z_{style}$. These controls are highly subjective and dependent, completely interacting with $I_t$ as the core representation. This method can have good results when the distance to $I_t$ is closer to the number of frames. However, the representation of $I_t$ becomes less and less special as the time series increases. At this time, as the number of frames $t_s$ continues to increase,"}, {"title": "2.3 VIDEO HIDING AND RECOVERY", "content": "Figure 4 shows the framework of the hidden part of our video. Specifically, given a hidden video $X_{secret}$ and a cover video $X_{cover}$ (the cover video is generated from the above section) after forward hiding frame by frame, a pseudo-video $X_{stego}$ is generated, which is ostensibly indistinguishable from $X_{cover}$ to achieve the $X_{secret}$ undetectable the effect of $X_{secret}$. With the same reversible neural network architecture and parameters, the pseudo-video $X_{stego}$ can be recovered into the original video $X_{recover}$. In order to utilize the temporal and spatial correlation within the video, we use Discrete Wavelet Transform (DWT) to divide each frame into four frequency bands LL,HL,LH,HH, and then in the same group of frames, we connect the portions of the same band portion of different frames in the channel dimension, and then concatenate these four bands in series according to the frequency magnitude, to generate the final secret video for concealment $x'_{secret}$ and the cover video $x'_{cover}.\nIt can be shown in Figure 4 that our video hiding and recovery results in reversible video hiding by constructing the reverse information flow through the invertible block. The initial reversible neural network can be defined as follows: assuming the input is x,the hiding module splits x into two parts $x_1$ and $x_2$ along the channel axis by the transform parameter and then untransforms it by the same transform parameter, which is expressed as follows:\n$x_1 = X_1 \\gamma_1 (x_2) + \\epsilon_1 (x_2)$\n$x_2 = X_2 \\gamma_2 (x_1) + \\epsilon_2 (x_1)$"}, {"title": "2.4 Loss FUNCTION", "content": "Our CausalVE loss function consists of cover image generation loss, initial cover video generation loss, video prediction loss, causal inference and decision loss, and video hiding loss. We provide additional information about the loss function in the appendix B for more supplementary information."}, {"title": "3 EXPERIMENTS", "content": "2.1 EXPERIMENTAL SETUP\nDatasets and Settings. The VoxCeleb2 training set is used to train our CausalVE, with the spatial resolution of each sequence contained in it fixed to 512 \u00d7 300. During training, we randomly crop the training video to 256 x 256 and randomly flip it horizontally and vertically to increase the amount of data. The testing datasets include VoxCeleb2, with 150,480 videos at each sequence resolution 512 \u00d7 300, Voxblink with 1.45 million videos by about 38000 people at each sequence resolution 480 \u00d7 367, and Mead. We segment the video in Mead and get 54291 videos by about 60 people at each sequence resolution 256 \u00d7 256. The test video for each sequence uses a center crop to 256 x 256 to ensure that the cover video and the secret video have the same resolution. The optimizer uses Adam's standard parameters, while the initial learning rate is 1 \u00d7 10-5, halved every 25K iterations. An NVIDIA A100 Tensor Core GPU is used for all training and testing.\nBenchmarks and Evaluation Metrics. We evaluate the soundness of our motivation and the effectiveness of our CausalVE. We compare our CausalVE with different information steganography approaches, including LSB, Weng et al., Baluja et al., HiNet, RIIS, and LF-VSN. It is important to note that the original video-hiding model was initially designed solely for information concealment, differing from our configuration for face privacy interaction protection. To accommodate video concealment, we made slight modifications to the output dimensions. Furthermore, unlike our approach where the model autonomously generates cover videos, the cover videos for these models were predefined. To align these models with our methodology, we adjusted the cover video inputs, redefined the video generation function, and retrained the networks. We use two metrics to evaluate the quality of cover/hide and secret/reduce video pairs, namely cover/stego and secret/recovery video peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM), MAE and RMSE.\nMeanwhile, in order to verify the validity of the generated overlay images, we use ArcFace, CosFace, SphereFace and AdaFace face recognition models to verify the processing effect of RFIS-FPI on cover images, recovery images. Since the resolution of ArcFace and AdaFace is 112 \u00d7 112, CosFace and SphereFace have a resolution of 112 \u00d7 96, which is smaller than the resolution of our training dataset, we use MTCNN to align and crop the face images to match the resolution of RFIS-FPI. We use SSIM and Learning to Perceive Image Patch Similarity (LPIPS) to perceive the quality of cover/secret and secret/recovery image pairs. For SSIM / LPIPS values between secret and cover images, we denote them by SSIMst / LPIPSst. Similarly, for the SSIM / LPIPS value between the secret image and the recovery image, we denote it by SSIMsr / LPIPSsr. A higher SSIM means that the two images are more similar, and a lower LPIPS means that the two images are less similar.\nMoreover, we use the statistical steganalysis tool StegExpose to evaluate the effectiveness of our face privacy interaction protection approach."}, {"title": "3.2 QUANTITATIVE COMPARISON", "content": "Tables 1 and 2 demonstrate the effectiveness of our method and other state-of-the-art video hiding methods on video datasets. The best data in each column of the table is in red, and the second best data is in blue. As can be seen from Table 1, our CausalVE compares slightly better metrics than other video hiding methods on cover video and steganography video. This is due to the fact that excellent generated videos have a camouflage ability that is no less than that of natural videos. At the same time, the cover video with the same ability to evolve time sequence and spatial features is similar in the process of hidden reorganization of reversible neural networks. This makes it harder to spot the difference between a fake video and a cover video. In Table 2, the performance of our method on secret images/recovered images is not far from the results of the state-of-the-art methods. This shows that our optimized reversible neural network applied to face video encryption is effective. The use of CausalVE allows for public face video video information interaction while having the perfect role of carrying sensitive information interaction.\nTo verify the effectiveness of our face cover video generation method, we compare our CausalDE with specialized face video generation methods and biometric privacy generation methods, and the results are shown in Table 3. Our method performs well in generating cover face videos, fully hiding the face information while achieving a more complete communication of the original video information."}, {"title": "3.3 QUALITATIVE COMPARISON", "content": "Figure 5a shows a comparison of the effectiveness of our CausalVE and the LF-VSN method, which produces the next best results for hidden images, on hidden videos. As can be seen in Figure 5a, our CausalVE is closer to visual logic on hidden videos thanks to guided video prediction. Specifically, we chose a challenging task: a piece of speech with multiple intonational auxiliaries: \"Mum ... Ah-Haha!\", a scenario that requires challenging matching videos to hide and show the progression from a closed accent to an open accent to a toothy smile. Under the same time sequence, we extracted the same frames from the two representative methods for comparison. Our CausalVE effect is closer to the truth. The coherent start-to-open movement from \"Mum\" to \"Ah\" is more consistent with the original semantics, which makes our CausalVE visually superior to the LF-VSN."}, {"title": "3.4 STEGANOGRAPHIC ANALYSIS", "content": "Data security remains a critical issue in the field of steganography. This section evaluates the resistance of various steganographic methods to detection by steganalysis tools, focusing on their ability to differentiate stego frames from natural frames. We utilize StegExpose for this evaluation, creating a detection dataset composed of stego and cover frames in equal proportions. Detection thresholds are varied extensively within StegExpose, and the resulting data is represented on a receiver operating characteristic (ROC) curve, shown in Figure 5b. Notably, an ideal detection scenario is where the probability of identifying stego frames from a balanced mix is 50%, akin to random chance. Thus, a ROC curve that approximates this ideal indicates higher methodological security. Our findings demonstrate that the stego frames produced by our CausalVE model are significantly more difficult to detect than those from other methods, highlighting the enhanced data security offered by our CausalVE.\nAdditionally, to verify the effectiveness of the CausalVE module, ablation experiments were conducted on both the causal analysis and video generation modules. These ablation experiments are shown in Appendix C."}, {"title": "4 CONCLUSIONS", "content": "We provide the CausalVE framework, which demonstrates a high level of efficiency in hiding and recovering video content, and positions it as a leading solution for privacy protection in video content shared over public channels. The experimental results reveal that CausalVE outperforms existing methods in both the visual quality of the cover videos and the undetectability of the hidden content, offering substantial improvements over traditional steganography and face-swapping techniques. The findings suggest that CausalVE not only provides robust privacy protection but also ensures that the integrity and expressiveness of the video content are maintained, making it a valuable tool for various applications in digital media, communications, and security fields. Furthermore, the approach's resistance to steganalysis tools underscores its potential for secure communication channels, where maintaining confidentiality and authenticity is crucial. Overall, this work lays a strong foundation for future research in video privacy protection, particularly in developing methods that balance security with the need for expressive and dynamic video content."}, {"title": "A RELATED WORK", "content": "A.1 VIDEO STEGANOGRAPHY\nVideo steganography involves embedding a secret message within a video, in a way that is almost imperceptible to humans. The Least Significant Bit (LSB) method is a traditional steganographic technique based on spatial domain, which substituting the n least significant bits of a pixel in the video frame with the n most significant bits of the secret message. applied LSB in video steganography to hide secret text in grayscale video frames. combined LSB technique with Cuckoo Search to embed each secret image's color channel independently into a cover video's frame. Beyond spatial-domain methods, some transform-domain techniques were also applied in video steganography, such as discrete cosine transform (DCT) and discrete wavelet transform (DWT). Transform-domain methods, though more undetectable and robust than spatial-domain methods, still provide a limited capacity for embedding secret information, ranging from text to images.\nRecently, some deep learning models have been proposed for video steganography, achieving superior performance compared to traditional methods. introduced the application of Generative Adversarial Network (GAN) in steganography, demonstrating that employing an adversarial training approach can enhance the security of concealment. first implemented concealing a full-sized image within another image. attempts to enhance capacity by embedding multiple images within a video, bringing it closer to true video steganography. Video hiding is an important research direction of video steganography, which attempts to hide a whole video into another one. Different from above methods, it requires larger hiding capacity. was the first to explore concealing/recovering a video within/from another video by masking the residuals between consecutive frames on a frame-by-frame basis. Besides, explores temporal correlations in video steganography using 3D CNNs. further extends the capacity limit of video steganography, enabling hiding/recovering 7 secret videos in/from 1 cover video. In conclusion, the previous research demonstrate the prospects of deep learning in video hiding.\nA.2 VIDEO PREDICTION\nVideo prediction involves predicting future video frames based on given ones. According to their model architecture, video prediction methods can be categorized as recurrent-based and recurrent-free. Recurrent-based models process predictions by incorporating previously predicted frames into the current input, making the prediction sequence serial in nature. PredRNN utilizes standard ConvLSTM modules to develop a Spatio-temporal LSTM (ST-LSTM) unit that concurrently captures spatial and temporal changes. The advanced PredRNN++ introduces a gradient highway unit to address the issue of vanishing gradients and a Casual-LSTM module for cascading spatial and temporal memories. Enhancements in PredRNNv2 include the introduction of a curriculum learning approach and a memory decoupling loss to enhance performance. MIM incorporates high-order non-stationarity into the design of LSTM modules. PhyDNet separately models PDE dynamics and additional unknown information using a recurrent physical unit. E3DLSTM merges 3D convolutions with recurrent networks, and MAU features a motion-aware unit that efficiently captures motion dynamics. Despite the development of various sophisticated recurrent-based models, the underlying mechanisms contributing to their efficacy are still not fully understood.\nOn the other hand, recurrent-free models simplify the prediction process by inputting the entire sequence of observed frames and producing all predicted frames simultaneously. Due to its parallel characteristic, it has an inherent efficiency advantage over the recurrent-based model. Recurrent-free models often utilize 3D convolutional networks to handle temporal dependencies. Early on, PredCNN and TrajectoryCNN employed 2D convolutional networks to prioritize computational efficiency. Initially, these early recurrent-free models were criticized for their poor performance. However, models like SimVP have recently demonstrated a simple yet effective approach that rivals recurrent-based"}, {"title": "A.3 FACE SWAP MODEL", "content": "Mainstream Face swapping techniques are primarily divided into two groups: 3D-based methods and GAN-based methods. The 3D-based approaches typically utilize the 3DMM to integrate structural priors. However, these methods often require human intervention or tend to produce noticeable artifacts. On the other hand, GAN-based methods generally focus on the target, merging identity features from the source face with the target's characteristics and employing GANs to maintain the authenticity of the swapped face. Nonetheless, these techniques often involve numerous loss functions, and balancing them necessitates meticulous adjustment of the hyperparameters. Additionally, these methods usually make only slight alterations to the target face, which limits their effectiveness in scenarios where there is a significant discrepancy in facial shapes between the source and the target. While some studies have attempted to use features from the 3DMM to guide the face swapping process, this indirect use of 3D information still falls short in maintaining consistent facial shapes.\nRecently, diffusion model has been introduced for face swapping due to its delicate controllability and high fidelity. DiffFace trained ID Conditional DDPM with facial guidance to preserve target insensitive attributes. DiffSwap designed a 3D-aware masked diffusion model using designed face attributes, enabling high-fidelity and controllable face swapping. The previous work demonstrates the great potential of the diffusion model in face swapping."}, {"title": "B Loss FUNCTION DETAILS", "content": "B.1 COVER IMAGE GENERATION LOSS.\nCover image generation is performed in a noisy environment, while the expert model is trained on a clean image. Therefore, we predict the noise by using the denoising score matching loss, while the identification loss due to effective recognition of faces by multi-expert recognition can be summarised by the source identification at each time step t. Specifically, the cover image generation loss is formulated as follows:\n$L_{cover} = ||\\sigma - \\sigma_{\\theta} (x_t, t, I_{id})||_2 + 1 - cos (I_{id}, L_{id})$\nWhere $cos (-, -)$ denotes the cosine similarity between the un-noised secret identity and the denoised secret identity.\nB.2 INITIAL COVER VIDEO GENERATION LOSS.\nIn the initial cover video generation process, we use the first cover image to generate the corresponding video. This is an image-to-video process. By using the pre-trained expression coefficients obtained from wav2lip, then, 3D face capture is performed on the face of the target image as a target to guide the video generation. At this point, both the generation coefficients and the lip movement expression coefficients are known, and the face capture loss function for the kth frame is obtained by taking the mean square loss:\n$L_{3D} = \\sum_{i=1}^k (w_{gen}^i - w_{lip}^i)$"}, {"title": "B.3 VIDEO RE-PREDICTION AND CAUSAL DECISION LOSS.", "content": "In our video regeneration, we applied a simple CNN-VIST-CNN nesting approach for video prediction. We set it to MSE loss. Video prediction is defined as: Consider a video sequence $X_{k,T} = \\{x_i\\}_{i=k-T+1}$ at time k, comprising"}, {"title": "B.4 VIDEO HIDING LOSS.", "content": "The loss function for video hiding is mainly a constraint on the reversible neural networks to perform forward-term hiding and backward recovery of the secret video. The forward term hiding is to hide the secret video in the stego video in the cover video. The stego video cannot be recognized as containing the secret video and the generated stego video should be as similar as possible to the cover video. Therefore, we limit $X_{stego CF}$ to be the same as the cover video $X_{cover}:\n$L_{forward} = ||X_{stego CF} - X_{cover CF}||_2$\nWhere CF is the index of the center frame of the video. The output is fused in a time-smoothed manner by means of the frame index. The goal of the backward recovery process is to recover $X_{secret}$ from $X_{stego}$. Thus, we define the loss function as:"}, {"title": "C ABLATION EXPERIMENTS", "content": "In order to verify that our method modules are all valid, we study the ablation of our method from the following aspects:\nQuestion 1: Can a simple video face-swapping, e.g., using a simple generative model such as GAN or diffusion for video face-swapping, achieve similar results to our model? It can be seen that a complex multi-module model will be far more complex than a simple model during operation. In short, it is whether our modeling study is indispensable.\nQuestion 2: How much do temporal and spatial features affect the generation of cover images for publicly distributed videos??\nQuestion 1 setting. We still use cosine similarity for comparison. The difference is that in this question, the focus is more on not the mean result of face-swapping, but the stability of face-swapping per frame. We use the mean square deviation of the cosine similarity to represent this.\nAnswer to Question 1. As can be seen from Figure 6, the stability of video face-swapping using GAN is very poor, while the difference between direct video face-swapping using the diffusion model and our method still has a gap in the metrics. This shows that our proposed concept and method are indispensable for privacy preservation in public face video distribution.\nQuestion 2 setting. In terms of spatiotemporal feature impact, we predicted video data with different time series lengths by means of speech-video prediction, CNN-ViST-CNN, and causal-time prediction, respectively. In terms of Conv kernel, we investigated the impact of kernel size and hidden dimension on model performance. We used the MSE metric to determine the impact of the Conv kernel.\nAnswer to Question 2. From Table 4, it can be seen that speech video prediction performs comparably to causal temporal prediction for video prediction over short periods. However, speech video prediction is much less effective than causal video prediction in medium and long-term video prediction over 10s. And the CNN-ViST-CNN with direct spatiotemporal evolution is not as effective in short-time face video prediction. This is because speech video prediction makes full use of the face features of the first replacement image in the prediction process, including physical features such as features and expressions. In the actual sequence, the features of a particular frame of the image will keep changing as the time sequence does not grow shorter, and the features of a single image stand for a smaller and smaller proportion of the total time sequence, and other spatiotemporal evolutionary features are needed. The CNN-VIST-CNN that directly performs spatiotemporal evolution does not directly give a lot of attention to a single image, and the spatiotemporal evolution features are not obvious in a short time, which leads to poor prediction results. The trade-off between the two through causal analysis achieves the result of optimal long-time video prediction."}, {"title": "D LIMITATIONS", "content": "The CausalVE framework involves several advanced techniques such as diffusion models, reversible neural networks, and dynamic causal inference. This complexity may create implementation challenges for practitioners unfamiliar with these methods. Additionally, issues such as different video quality, different lighting conditions, and different types of facial obstructions may affect the performance of the framework. Although the diffusion model can effectively improve the quality of videos, lower video quality still has a great impact on the training of the diffusion model and the generation of new cover videos. Additionally, practical considerations for deployment in real-world applications (such as latency, real-time processing capabilities, and ease of integration) are not considered in this article."}]}