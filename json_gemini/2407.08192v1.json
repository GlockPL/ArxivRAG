{"title": "ARCO:Adaptive Multi-Agent Reinforcement Learning-Based Hardware/Software Co-Optimization Compiler for Improved Performance in DNN Accelerator Design", "authors": ["Arya Fayyazi", "Mehdi Kamal", "Massoud Pedram"], "abstract": "This paper presents ARCO, an adaptive Multi-Agent Reinforcement Learning (MARL)-based co-optimizing compilation framework designed to enhance the efficiency of mapping machine learning (ML) models - such as Deep Neural Networks (DNNs) - onto diverse hardware platforms. The framework incorporates three specialized actor-critic agents within MARL, each dedicated to a distinct aspect of compilation/optimization at an abstract level: one agent focuses on hardware, while two agents focus on software optimizations. This integration results in a collaborative hardware/software co-optimization strategy that improves the precision and speed of DNN deployments. Concentrating on high-confidence configurations simplifies the search space and delivers superior performance compared to current optimization methods. The ARCO framework surpasses existing leading frameworks, achieving a throughput increase of up to 37.95% while reducing the optimization time by up to 42.2% across various DNNs.", "sections": [{"title": "INTRODUCTION", "content": "The computational demands and complexity of deep neural networks (DNNs) have surged rapidly with their expanding applications across diverse industry sectors, including autonomous driving, medical imaging, and natural language processing [14]. These applications require high accuracy and real-time responsiveness, placing significant pressure on the computational infrastructure. Historically, performance optimization in environments like TensorFlow [1] and PyTorch [21] has relied on hand-optimized kernels such as NVIDIA's cuDNN and Intel's MKL. While these kernels are designed to optimize efficiency for particular hardware setups, they face challenges in adapting to the changing demands of modern neural architectures and their evolving computational requirements.\nThe extensive number of parameters that need tuning and a huge hardware configuration search space pose significant hurdles [3]. To address this growing complexity, there has been a shift towards automated compilation frameworks like TVM [9], Tensor-Comprehensions [28], and AutoTVM [9]. These frameworks move away from static, hand-tuned optimization strategies to dynamic, algorithm-driven approaches capable of adapting to various architectures and operational requirements. For example, AutoTVM utilizes a sophisticated approach involving boosted trees [7] to navigate the extensive configuration space of neural network code efficiently. This method predicts the performance of potential configurations and minimizes the need for exhaustive empirical testing, thereby expediting the optimization process. However, despite these technological advances, optimizing complex models requires substantial resources and compilation time, underscoring the ongoing challenges and gaps in current optimization methodologies.\nIn addition to evolutionary strategies like genetic algorithms [19], machine-learning techniques have been instrumental in refining the search for optimal configurations across this complex landscape. When considering various machine learning algorithms, reinforcement learning (RL) stands out as a promising option for exploring design search spaces and optimizing tasks [3, 5, 29, 32]. RL operates by testing various strategies and utilizing feedback on their actions to maximize cumulative rewards. This learning process occurs through agents interacting with their environment, which can range from single to multi-agent setups. Multi-agent RL offers the advantage of enabling transfer learning and providing valuable insights into interconnected decision-making systems [11].\nTo address these inefficiencies and push the boundaries of current compilation technologies, we propose an adaptive multi-agent reinforcement learning-based hardware/software co-optimization compiler called ARCO. At its core, this optimizing compiler relies on a multi-agent reinforcement learning (MARL) algorithm to optimize the mapping of the given model and the accelerator structure concurrently. ARCO utilizes three specialized agents-each designed to optimize different facets of the system architecture: two focus on software and DNN configurations, while the third optimizes the accelerator architecture. This actor-critic multi-agent approach facilitates a more integrated and holistic optimization process and allows for more granular control and customization of the optimization strategies employed.\nThe proposed framework efficiently narrows the vast search space by employing a soft threshold for selecting high-confidence configurations, focusing only on the most promising candidates. This methodology not only reduces computational overhead but also minimizes exploration time. Our method leverages the capabilities of VTA++ [6], a highly configurable deep learning accelerator, to investigate diverse architectural optimizations. ARCO utilizes Centralized Training with a Decentralized Execution (CTDE) strategy, wherein training occurs in a centralized manner. In contrast, execution is decentralized, enabling adaptive adjustments to the changing requirements of DNN workloads. This strategic approach efficiently narrows the extensive search space by utilizing a soft threshold to select high-confidence configurations, prioritizing the most promising candidates. The efficacy of the ARCO is assessed under a set of DNN models with a variety of architectures and compared with leading state-of-the-art approaches, and it has demonstrated substantial improvements in throughput."}, {"title": "BACKGROUND", "content": "This section briefly introduces the target hardware we used (i.e., VTA++), the Centralized Training and Decentralized Execution in Multi-Agent Reinforcement Learning, the workflow for DNN compilers, and related prior work."}, {"title": "VTA++", "content": "The Versatile Tensor Accelerator (VTA) [20] has emerged as a highly effective deep learning hardware acceleration framework integrated within the TVM stack. To achieve a good hardware/software co-design, we need highly configurable target hardware, which we could optimize its architecture to achieve simultaneous hardware and software optimization. Therefore, we chose VTA++ [6], which has the same architecture as VTA but is more customizable.\nThe General Matrix Multiply (GEMM) core is the cornerstone of VTA's capabilities, carrying out dense matrix multiplications. This key operation underpins deep learning functions like 2D convolutions using convolutional layers and classification based on the extracted features using fully connected layers. This core is specifically engineered to handle complex arithmetic calculations by multiplying input and weight tensors and then adding the output to a register file tensor.\nThe GEMM core of VTA is defined by key hardware parameters: BATCH, BLOCK_IN, and BLOCK_OUT. BATCH size refers to the number of data samples processed in parallel, BLOCK_IN represents the inner dimension of the input tensor, and BLOCK_OUT pertains to the output dimension of the weight tensor and the accumulated result.\nIn this paper, we dedicate one of our actor-critic MARL agents to optimize the VTA GEMM core for enhanced hardware performance. By selectively tuning BATCH, BLOCK_IN, and BLOCK_OUT-referred to herein as \"hardware knobs\"-we adapt the GEMM core to be optimized in our method. Altering BATCH size impacts the parallelism of data processing while modifying BLOCK_IN and BLOCK_OUT influences the granularity of the computation and the utilization of the on-chip memory resources. The goal is to strike an optimal balance between computational throughput and resource allocation, thereby maximizing the GEMM core's efficiency. This RL agent learns to predict the optimal hardware configuration by interacting with the VTA environment and other software agents, thereby learning from the empirical performance outcomes of previous configurations."}, {"title": "CTDE in MARL", "content": "Multi-agent reinforcement learning (MARL) extends traditional reinforcement learning, introducing multiple agents within a shared environment. The complexity of agent interactions calls for sophisticated methodologies that coordinate individual agent strategies with collective objectives. The Centralized Training with Decentralized Execution (CTDE) framework has emerged as a significant paradigm in MARL, balancing the collective intelligence during training with the autonomy of individual agents during operation [18].\nDuring the centralized training phase within the CTDE paradigm, agents have access to a global state and learn policies that consider all agents' collective state and actions. This centralized approach allows agents to develop cohesive strategies, effectively addressing cooperative, collaborative, competitive, and mixed tasks. The essence of CTDE is encapsulated in the adaptation of Proximal Policy Optimization (PPO) [25] to MARL, leading to the development of Multi-Agent PPO (M\u0391\u03a1\u03a1\u039f) [30].\nIn the context of MAPPO, the learning process involves three pivotal components: critic learning, General Advantage Estimation (GAE), and policy learning. These components are encapsulated in the following mathematical expressions:\n\u2022 Critic learning ensures that each iteration yields an enhanced centralized value function. It is defined as:\n$${\\theta_{k+1} = arg\\ min_{\\theta} \\frac{1}{|D_k|T} \\sum_{\\tau \\in D_k} \\sum_{t=0}^{T} (V(o_t, s_t, u_t) - r_t)^2 }$$\n\u2022 General Advantage Estimation (GAE) gauges the quality of the current action relative to the baseline critic value,"}, {"title": "Workflow for DNN compilers", "content": "Mapping DNN models into efficient machine code is crucial for their performance across various hardware platforms. This mapping is managed by a compiler workflow that optimizes the DNN model in several stages [4]. Initially, the compiler's frontend conducts general optimizations. These broad improvements do not yet account for the specific hardware where the model will run. Subsequently, backend optimizations consider the target hardware's characteristics, shaping the code in a hardware-aware manner. Recent advancements like the AutoTVM system have introduced an innovative step that fine-tunes the code based on actual hardware performance feedback. This method explores various settings-referred to as \"knobs\"-to determine the most effective configuration for the code to run efficiently. The search for this optimal configuration (\\*) is captured by:\n$\\Theta^* = argmax_{\\Theta \\in D} {f [\\tau(\\Theta)]}$"}, {"title": "Related Work", "content": "In the domain of DNN compilers, adaptive and efficient auto-tuning mechanisms have proven critical for optimizing performance on varying hardware architectures [10, 12, 17, 22, 23]. An interesting development in this field is AutoTVM [9], which utilizes a machine learning-based cost model, specifically XGBoost [7], for its auto-tuning process. This method, while effective, involves a large search space and substantial measurement overhead, making optimization resource-intensive.\nMetaTune [24] was introduced to address the limitations of black-box auto-tuning by leveraging meta-learning to allow its cost model to \"learn to learn\" performance correlations. This enables MetaTune [24] to adapt to new optimization spaces quickly and efficiently, demonstrating improvements in inference times compared to prior methods. Glimpse [2] builds upon these approaches by integrating mathematical embeddings of hardware specifications, known as Blueprints, in a Bayesian optimization framework. These Blueprints guide the search algorithm towards subspaces with higher performance potential.\nAnother approach is CHAMELEON[3], which employs reinforcement learning for its Adaptive Exploration and Sampling algorithms, minimizing invalid configurations and costly hardware measurements. CHAMELEON[3] significantly reduces optimization time compared to AutoTVM while improving inference times. NaaS (Neural Architecture and Accelerator Search) [33] expands upon these frameworks by jointly optimizing neural network architectures and hardware accelerators. It parameterizes both architectures in a unified search space using PyGlove and demonstrates efficiency gains across tasks like image classification and segmentation.\nPRIME (Data-Driven Offline Optimization for Architecting Hardware Accelerators) [16] introduces a novel data-driven method for designing hardware accelerators using logged simulation data to construct a robust, conservative surrogate model. By eliminating simulation-driven optimization's repeated overhead, PRIME significantly reduces simulation time while architecting specialized accelerators for single- and multi-application tasks.\nComparing these methods highlights AutoTVM's groundwork in machine learning-based auto-tuning. MetaTune and Glimpse advance this by incorporating meta-learning and hardware-aware strategies, respectively. CHAMELEON extends the reinforcement learning approach with improved sampling methods. NaaS and PRIME offer new dimensions by integrating accelerator co-design and offline data-driven optimization. However, despite the advances brought by NaaS and PRIME in hardware consideration, their lack of reinforcement learning makes them slower in compilation times, as they can't rapidly navigate the expansive hardware and software search space. Reinforcement learning algorithms, as used in CHAMELEON, enable compilers to navigate possible configurations efficiently, identifying high-performance options with fewer tests. This results in a more agile optimization process, allowing for quicker deployment of DNN models and facilitating rapid progress in deep learning research and applications [3, 5, 29, 32].\nIn comparing our approach to existing methods like CHAMELEON, Glimpse, and MetaTune-which focus on reducing compilation times through different optimization techniques-and co-design methods like NaaS and PRIME-which strive to enhance system throughput through various optimization strategies-our analysis underscores the distinct advantages of MARL in efficiently navigating the breadth and depth of the design space in modern DNN applications. While effective in their own ways, these methods often fail to fully capture the interdependencies between hardware and software optimizations that our"}, {"title": "ARCO", "content": "As explained before, current systems fall short of effectively optimizing both the hardware and software components of DNNs simultaneously, as they tend to prioritize software parameters exclusively. Also, existing co-design approaches are too time-consuming to compile. To break free from prolonged optimization processes and fragmented tuning methods, it is important to take on two key challenges: 1) Improving the search algorithm to encompass hardware architecture and parameters as modifiable factors and 2) Refining the sampling process to capture the solution distribution better and minimize encountering non-feasible configurations.\nWe propose two key advancements in the co-optimizing compiler for DNNs to address these obstacles. Firstly, we incorporate a multi-agent reinforcement learning strategy into the search algorithm, allowing it to adapt to new design environments while simultaneously refining hardware and software parameters. This approach harnesses collective intelligence to achieve a more holistic and efficient optimization approach. Secondly, we introduce a novel Confidence Sampling (CS) method to replace the existing uniform [9] and adaptive sampling methods [3], which often are random and rely on trial and error. The proposed CS method identifies configurations with a higher probability of success, guided by learned patterns from previous successful optimizations."}, {"title": "Overview of ARCO", "content": "Figure 2 depicts the architecture of our sophisticated co-optimizing compiler, termed ARCO, which elucidates the process of optimization compilation. ARCO initiates with a code template, t, for each layer of the neural network and a corresponding design space $D_{\\Theta}$, delineating the range of possible configurations. The compiler then engages in an optimization process, navigating through the configuration space to ascertain the optimal code configuration, represented as $\\tau(\\Theta^*)$.\nARCO maneuvers through the design space, employing a cost model as a surrogate for direct hardware measurements. The MARL Exploration module generates an initial set of candidate configurations $S_e$. Through the CS method, ARCO refines $S_e$ to produce a more focused subset S'e, which contains a condensed yet highly promising set of configurations. The configurations in $S'_e$ are then passed to the MARL Code Generator module, which incorporates them with the input template \u03c4 to create a series of potential executable codes $\\tau(\\Theta)$. These are subsequently deployed on the hardware for empirical runtime evaluations. The hardware runtimes provide a measure of the configurations' fitness, captured by a fitness function f, which informs the update of the cost model and enhances the exploration in subsequent iterations. After several iterations, the process converges to some $\\tau(\\Theta^*)$ that achieves optimal fitness, characterized by the shortest hardware execution runtime. This configuration ($\\Theta^*$) is then selected as the output for that network layer."}, {"title": "MARL Exploration", "content": "In our work, we implement a MARL Exploration module (As shown in Figure 3) that employs a Multi-Agent Reinforcement Learning (MARL) strategy designed to optimize the configurations of DNN architectures and hardware simultaneously. This module utilizes the principle of Centralized Training and Decentralized Execution (CTDE), allowing agents to learn collaboratively while operating independently during the execution phase to maximize the fitness function, f, of configurations within the configuration space, $S_e$. MARL's ability to manage complex decision-making environments makes it particularly suitable for navigating the high-dimensional configuration space of DNNs.\nThe MARL Exploration strategy employs three agents, each equipped with a policy network, as detailed in Table 1. Additionally, there is a centralized critic network (value network) that, along with the policy networks, is implemented as a Multi-Layer Perceptron (MLP) neural network. The policy network directs each agent to propose adjustments to the configuration knobs (as shown in Table 2) within its assigned portion of the design space. In contrast, the value network estimates the potential value of these adjustments. A shared central critic aids in evaluating the global state, facilitating collective learning during the training phase, while each agent makes decisions independently during execution, adhering to the CTDE paradigm.\nThe optimization unfolds through several episodes, each consisting of multiple search steps. During these steps, the agents evaluate the current configuration and independently decide on actions to improve subsequent configuration fitness, guided by shared insights and individual observations. The process detailed in Algorithm 1 outlines how each agent interacts within the CTDE framework, iterating through configurations to enhance the overall network performance.\nEach episode integrates the evaluations from a cost model that serves as a surrogate for direct hardware performance measurements. This model's feedback is used to update the centralized critic, enhance the shared knowledge base, and refine the individual policy networks, refining each agent's decision-making capabilities based on global and local feedback. As the episodes progress, the MARL Exploration Module becomes increasingly adept at identifying configurations that yield the best performance, ultimately converging on $\\tau(\\Theta^*)$ with the optimal fitness, f.\nApplying this CTDE framework within the MARL context, coupled with the PPO optimization algorithm, represents a significant advancement in the auto-tuning of DNN and hardware configurations. This approach systematically improves the configuration search process, significantly enhancing DNN performance while reducing the computational overhead."}, {"title": "Cost Model and Central Critic Update", "content": "In the ARCO framework, the optimization goal is to improve throughput, with the cost model reflecting the inverse of execution time. The central critic, a key component of our MARL setup, updates its policy based on the aggregated feedback from all agents, continuously optimizing the global state evaluation. The critic's learning process is guided by the update rule, which is mathematically expressed in section 2.2, which minimizes the mean squared error between the predicted values and the actual rewards obtained."}, {"title": "Incorporating Hardware and Software Constraints", "content": "To integrate constraints related to hardware, such as area limitations, or software, such as memory usage, into the MARL framework, a penalty term can be incorporated into the reward function. This approach adjusts the reward based on the degree to which the constraints are violated, effectively guiding the agents to prefer configurations that adhere to these constraints. For instance, a penalty function P can be defined as follows:\n$$P(\\Theta) = \\lambda (max(0, area(\\Theta) - area_{max})$$\n$$+ max(0, memory(\\Theta) - memory_{max}))$$\nwhere \u03bb is a scaling factor that adjusts the impact of the penalty, area(\u0398) and memory(\u0398) are the area and memory usage of the configuration \u0398, and $area_{max}$ and $memory_{max}$ are the respective maximums.\nThe reward function, modified to include this penalty, becomes:\n$$R_t = \\frac{1}{execution \\ time(\\Theta)} - P(\\Theta)$$\nThis modification ensures that the MARL agents not only seek to optimize performance but also adhere to specified design constraints, balancing efficiency with practical deployment considerations."}, {"title": "Confidence Sampling", "content": "In the domain of DNN optimization, the ability to efficiently sample configuration spaces without costly hardware measurements is essential for rapid compilation. We introduce the Confidence Sampling (CS) method. This technique is designed to leverage the probability distribution of configurations' fitness judiciously to select a subset likely to yield high-performing network configurations.\nAlthough the Confidence Sampling method operates on the principles of probability-guided selection and shares some conceptual similarities with Importance Sampling (IS) [27] in statistical methods, it is distinct in its purpose and application. In IS, samples are re-weighted based on their probability density to approximate expected values more efficiently. Confidence Sampling, however, specifically focuses on reducing the number of hardware measurements required when exploring the design space. By evaluating the configurations' values, this method synthesizes a subset that not only encapsulates the diversity of the configuration space but also emphasizes the high-confidence regions, i.e., areas where the likelihood of encountering an optimal or near-optimal configuration is high.\nThe core of the CS method lies in its algorithmic approach to filtering and enhancing the set of configurations for subsequent optimization iterations. To formalize this process, we introduce Algorithm 2, which captures the essence of the CS method within the optimization pipeline. The process involves the following steps:\n(1) Evaluate Configurations: Each configuration's value is estimated using the output of the value network (critic network) to construct a probability distribution over the configuration space. (Line 2)\n(2) Probability-Guided Selection: Configurations are sampled based on this distribution, prioritizing those with higher estimated values. (Line 3-4)\n(3) Confidence Assessment: A dynamic thresholding mechanism is applied to determine the confidence level of each selected configuration. (Line 5)\n(4) Synthesis of Configurations: Lower-confidence configurations are replaced with synthesized ones by combining each parameter's most frequently occurring settings across the sampled configurations. (Line 6-7)"}, {"title": "EXPERIMENTAL RESULTS", "content": "We have incorporated ARCO within the TVM framework to thoroughly evaluate its components, contrasting its performance with that of AutoTVM and CHAMELEON. This integration allows for a comprehensive, end-to-end assessment of ARCO across a suite of advanced deep-learning models, including AlexNet, VGG-11, VGG-13, VGG-16, VGG-19, ResNet-18, and ResNet-34. All models were benchmarked using the same AutoTVM compilation duration to ensure a rigorous and equitable comparison. Further, we have employed the source code of the CHAMELEON [3] to compare its efficacy with AutoTVM and ARCO. As mentioned above, AutoTVM and CHAMELEON do not support hardware configuration exploration; thus, in the case of these two exploration techniques, we have considered the default specification values of the VTA++ [6].\nPerformance tests were conducted on a robust platform powered by a 3.4 GHz AMD EPYC 7763 64-Core Processor, providing a stable and powerful environment for these evaluations. As discussed before, we used the VTA++ simulator as our target hardware.\nTable 3 shows the details of the DNN models employed for the comparison studies. In our setup, each agent is equipped with its own policy network, and a centralized value network functions as the critic for all agents. Both networks are implemented as Multi-Layer Perceptrons (MLPs) using TensorFlow [1] that their details are:\n\u2022 Policy Network: Each agent's policy network consists of a single hidden layer with 20 neurons using ReLU activation functions. The output layer uses softmax activation to generate a probability distribution over actions, allowing for decision-making informed by learned policies.\n\u2022 Value Network: The centralized critic, or value network, employs three hidden layers with 20 neurons each, using tanh activation functions. This design helps stabilize learning by providing a continuous estimate of state values, which guides collective policy adjustments.\nHyper-parameter tuning is crucial in optimizing the performance of machine learning tools and models [31]. To facilitate transparency and reproducibility, we have detailed the hyper-parameters utilized in our evaluation in Table 4. These hyper-parameters were meticulously tuned offline to optimize model performance, and they are the same as those used in CHAMELEON [3]. For the hyper-parameters listed in Table 5, we adhered to the values specified in the AutoTVM study to maintain consistency and ensure a fair comparison. Similarly, we adopted the hyper-parameter settings from the MAPPO paper [30] for the MARL exploration module, aligning our methodologies with established research to validate our findings effectively."}, {"title": "End-to-end Evaluation", "content": "Figure 5 Compares the achieved throughput of different frameworks over AutoTVM. As the results illustrate, the significant reduction in mean inference time (advancement in throughput) achieved by ARCO compared to AutoTVM and CHAMELEON for various DNN models: AlexNet, VGG-11, VGG-13, VGG-16, VGG-19, and ResNet-18, and ResNet-34. On average, our methodology demonstrates a 1.17x improvement in throughput. Table 6 reports detailed numerical results supporting these findings.\nWhile achieving improvement in throughput, ARCO also needs less compilation time compared to AutoTVM; Figure 6 shows that ARCO speeds up the optimization time up to 42.2%. Figure 7 shows ARCO's comparative output code performance relative to other frameworks. Notably, ARCO achieves convergence to the peak GFLOPS value, equivalent to those reached by AutoTVM and CHAMELEON (without adaptive sampling), but with greater efficiency. This is accomplished with fewer hardware measurements and at a faster rate, underscoring the efficacy of the CS method.\nThe overall efficiency gains are primarily attributable to the sophisticated MARL exploration techniques and the reduced hardware measurements necessitated by implementing the CS method."}, {"title": "CONCLUSION", "content": "This paper introduced ARCO, an innovative multi-agent reinforcement learning-based compiler that significantly advances the field of DNN accelerator design. By integrating a multi-agent system with specialized roles-two agents focused on software optimization and one on hardware-ARCO addressed the intricacies of DNN architecture and effectively navigated the vast configuration space to enhance performance and efficiency. Employing the MARL Exploration module and Confidence Sampling method, ARCO reduced the computational overhead and expedited the optimization process while increasing the throughput. Our experimental results, conducted on advanced DNN models like AlexNet, VGG, and ResNet series, demonstrated that ARCO outperforms traditional frameworks like AutoTVM and CHAMELEON, improving throughput by up to 37.95% and reducing optimization time significantly (Up to 42.2%). These enhancements underscore the potential of adaptive, intelligent systems in optimizing hardware and software layers cohesively, setting a new benchmark for future developments in real-time DNN applications."}]}