{"title": "Label-Confidence-Aware Uncertainty Estimation in Natural Language Generation", "authors": ["Qinhong Lin", "Linna Zhou", "Zhongliang Yang", "Yuang Cai"], "abstract": "Large Language Models (LLMs) display formidable capabilities in generative tasks but also pose potential risks due to their tendency to generate hallucinatory responses. Uncertainty Quantification (UQ), the evaluation of model output reliability, is crucial for ensuring the safety and robustness of AI systems. Recent studies have concentrated on model uncertainty by analyzing the relationship between output entropy under various sampling conditions and the corresponding labels. However, these methods primarily focus on measuring model entropy with precision to capture response characteristics, often neglecting the uncertainties associated with greedy decoding results the sources of model labels, which can lead to biased classification outcomes. In this paper, we explore the biases introduced by greedy decoding and propose a label-confidence-aware (LCA) uncertainty estimation based on Kullback-Leibler (KL) divergence bridging between samples and label source, thus enhancing the reliability and stability of uncertainty assessments. Our empirical evaluations across a range of popular LLMs and NLP datasets reveal that different label sources can indeed affect classification, and that our approach can effectively capture differences in sampling results and label sources, demonstrating more effective uncertainty estimation.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated formidable capabilities in natural language processing tasks such as machine translation (Fomicheva et al. 2020), abstract text summarization (Brown et al. 2020), and question-answering (Touvron et al. 2023). Techniques such as In-context Learning (ICL) (Dong et al. 2022) and Chain-of-Thought (COT) (Wei et al. 2022) have further enhanced model performance on complex reasoning tasks and scenarios involving unseen data, consistently setting new benchmarks. However, despite their proficiency under scaling laws (Kaplan et al. 2020), these models underperform on more challenging tasks like mathematical problems (Luo et al. 2023). A significant concern is that, rather than refusing to answer, models are more likely to generate answers that include illusory reasoning processes and hallucinations. Uncertainty estimation and measurement have become essential tools in machine learning aiding in determining the extent to which humans can trust AI-generated content and deciding when to intervene with manual assistance. Previous research works in this field have involved prompting LLMs to self-assess the confidence of their own answers or employing confidence assessments based on model outputs using logits or entropy. Recent development Semantic Entropy (SE) (Kuhn, Gal, and Farquhar 2023) has introduced semantic-based entropy prediction schemes in that account for the synonym phenomena inherent in language models, performing answer aggregation in semantic space. Duan et al. (2023) and Bakman et al. (2024) propose schemes SAR and MARS based on semantic importance weighting, focusing on more precisely measuring the information content in the model's latent space to offer viable approaches to align the sampling entropy more closely with the actual value. However, we observe that the confidence and semantic alignment of the answers which serve as label sources, as well as their deviations from the distribution space, significantly impact the entropy's classification performance, an aspect overlooked by these schemes.\nAs shown in Figure 1, when given a question, in the beam search multi-sampling strategy, three out of the five answers generated by the LLM are correct, but due to the high overall entropy value, the LLM may be marked as unable to answer this question. Such an error is caused by the entropy threshold used in the evaluation only considering the absolute value, such as the common \u2013 log(0.5), and ignoring the distribution of the model itself for the question, that is, the greedy decoding probability is lower than the probability corresponding to the sample entropy value, which is 0.1661 as shown below.\nTo mitigate this issue, as shown in Figure 1, we propose a label-confidence-aware (LCA) uncertainty estimation based on Kullback-Leibler divergence (KLD) bridging between samples and label source, thus enhances the reliability and stability of uncertainty assessments. We first sample answers of question as well as the output probabilities for calculating entropy of sample set. We then obtain an average probability stand for the samples and merge it with labeled answer probability by KL-divergence to measure their difference, and use the integrated information to classify whether the model could answer the question or whether the answer can be trusted.\nOur work contributes in the following ways:\n\u2022 We conduct experiments on various models and datasets under the recently popular methods, identifying and reporting biases in the uncertainty measurement methods when assessing different answers and sample sizes, as well as analyze the reasons behind these biases based on semantic probabilities.\n\u2022 We propose an uncertainty estimation method on the probability we call Gibss probability based on pointwise KL-divergence, which considers the average probability of the sampled set and the labeled answer, taking into account the differences between sampling results and observed results when measuring uncertainty, termed as Label-Confidence-Aware (LCA).\n\u2022 We evaluate multiple important free-form question-answering datasets on the currently popular pre-trained LLMs. Our results demonstrate that our proposed approach LCA based on KLD can be integrated with the current state-of-the-art (SOTA) methods and further enhance their performance. Additionally, through hyperparameter ablation experiments we show how these variables affect the final results."}, {"title": "Background", "content": "Total uncertainty includes aleatoric uncertainty -measuring the ambiguity inherent in the problem itself", "following": "n$PE(x) = H[P(Y|x)] = \\int P(y|x) \\cdot log P(y|x)dy$"}, {"title": "Entropy Bias in Evaluating Different Subjects"}, {"title": "Pointwise KL-Divergence"}, {"title": "Why Gibss probability?"}, {"title": "Experiments"}, {"title": "Baselines"}, {"title": "Models"}, {"title": "Datasets"}, {"title": "Correctness Metric"}, {"title": "Evaluation Metric"}, {"title": "Hyperparameters"}, {"title": "Results Analysis"}, {"title": "Ablation Study"}, {"title": "Number of Generation"}, {"title": "Sensitivity to Correctness Metric"}, {"title": "Temperature"}, {"title": "Different Integrate Methods"}, {"title": "Related Work"}]}