{"title": "Let people fail! Exploring the influence of explainable virtual and robotic agents in learning-by-doing tasks", "authors": ["Marco Matarese", "Francesco Rea", "Katharina J. Rohlfing", "Alessandra Sciutti"], "abstract": "Collaborative decision-making with artificial intelligence (AI) agents presents opportunities and challenges. While human-AI performance often surpasses that of individuals, the impact of such technology on human behavior remains insufficiently understood, primarily when AI agents can provide justifiable explanations for their suggestions. This study compares the effects of classic vs. partner-aware explanations on human behavior and performance during a learning-by-doing task. Three participant groups were involved: one interacting with a computer, another with a humanoid robot, and a third one without assistance. Results indicated that partner-aware explanations influenced participants differently based on the type of artificial agents involved. With the computer, participants enhanced their task completion times. At the same time, those interacting with the humanoid robot were more inclined to follow its suggestions, although they did not reduce their timing. Interestingly, participants autonomously performing the learning-by-doing task demonstrated superior knowledge acquisition than those assisted by explainable AI (XAI). These findings raise profound questions and have significant implications for automated tutoring and human-AI collaboration.", "sections": [{"title": "1 Introduction", "content": "The maturation of artificial intelligence (AI) techniques has facilitated their extensive utilization across various domains. The integration and refinement of explainable AI (XAI) methods have further empowered non-expert users to incorporate AI models into decision-making settings Waldman and Martin (2022). The resultant dynamics of human-AI collaboration have become a focal point of interest for the human-computer interaction (HCI) community and society at large Bu\u00e7inca et al. (2021).\nWhile human-AI collaboration in decision-making has predominantly been ad- dressed in HCI in recent years, with individuals interacting with artificial agents or receiving suggestions and explanations from recommendation systems Lai et al. (2021); Malhi et al. (2020), the study of the human-robot collaboration received the scientific community's attention since the dawn of the human-robot interaction (HRI) research field. However, recent years have witnessed implementing and testing explainable techniques with robots in collaborative contexts Anjomshoae et al. (2019); Wallk\u00f6tter et al. (2021). Differently from the HCI context, the HRI one provides richer interaction modalities, offering a more diverse range of opportunities for personalizing XAI and the modality of the explanations delivery Matarese et al. (2021).\nAn emerging challenge addressed by both the HCI and HRI communities is ex- amining how AI technologies influence human behavior in the context of human-AI collaboration Green and Chen (2019). Multidisciplinary efforts have investigated the impact of AI suggestions on human decision-making, exploring implications related to human cognitive biases Bertrand et al. (2022). Moreover, the introduction of XAI techniques has a dual effect, enabling non-expert users to benefit from such pow- erful technology while also raising concerns about over-reliance on AI models and the reinforcement of negative human heuristics, such as automation bias Vered et al. (2023).\nThis work investigates the impact of interacting with virtual and robotic explainable agents on people's behavior and performance during a learning-by-doing task Anzai and Simon (1979); Schank et al. (2013). In our experiments, participants had to learn an unknown task with the assistance of an explainable artificial agent, specifically a virtual talking agent and a social humanoid robot. Additionally, a separate group performed the task autonomously without assistance. During the experiments, we employed an assessment task to directly and quantitatively measure the utility of the human-agent explanatory interactions, building on prior work\u00b9. We aimed to compare the effect of different explanation strategies and explainable agents on participants' behavior, focusing on their final knowledge of the task.\nThe subsequent sections are organized as follows. Section 2 reviews related works, categorizing them into three parts: human-AI collaboration, explanations in human- AI decision-making, and explanations evaluation. Section 3 outlines the methods employed in the user study, presenting the peculiar methodologies and the technology used during the experiments. Section 4 details the results of the user study, showing comparisons between the experimental conditions. These results are extensively discussed in Section 5, with reference to the existing literature. Finally, Section 6 summarizes our work, highlighting its limitations."}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Human-AI collaboration", "content": "In the realm of human-AI collaboration, user studies predominantly focus on decision- making Wang and Yin (2021) or classification tasks Goyal et al. (2019). The rationale behind this emphasis lies in the promise of improved performance when assisting human users with expert AI systems (Wang and Yin, 2022). While this promise is generally fulfilled, some studies indicate a decline in team performance with certain forms of XAI (Schemmer et al., 2022b). Indeed, the challenge arises from the observa- tion that humans often struggle to detect incorrect AI advice (Ferreira and Monteiro, 2021; Janssen et al., 2022; Schemmer et al., 2022a).\nGiven the potential benefits of collaborating with expert AI partners, non-expert users are particularly interested in AI-assisted decision-making. In a comprehensive review Severes et al. (2023), authors analyzed the latest XAI literature to validate their insights about non-expert users' interpretation of XAI solutions. Their user study exposed participants to five explanation metaphors to investigate their perception and understanding. Moreover, authors in Lai et al. (2021) summarized the design choices of 100 papers on decision-making, AI models and AI assistance elements, and evaluation metrics, highlighting current trends and gaps. Scholars have also contributed to the field by systematically reviewing the AI-assisted decision-making literature and proposing a taxonomy of human-AI collaboration interaction Gomez et al. (2023).\nScholars have explored approaches such as user awareness and personalization to enhance performance. Recent works on user awareness emphasize trust towards the system, focusing on context-awareness and personalization as crucial for user-centeredness Williams (2021). Personalization in XAI has also been implemented by leveraging the users' personality traits and correlating them with preferences or behaviors (B\u00f6ckle et al., 2021; Martijn et al., 2022). Inspired by philosophy and psy- chology, some works delve into empirical application-specific XAI and the theoretical underpinning of human decision-making. Authors in Wang et al. (2019) proposed a framework for building human-centered decision-theory-driven XAI, identifying human cognitive patterns that XAI needs to follow and cognitive biases that need to be mitigated. Moreover, they implemented their framework in a clinical diagnostic scenario and drew insights into how their framework bridges XAI and human decision- making theories. Furthermore, authors in Bertrand et al. (2022) reviewed a relevant corpus of literature to understand human biases reflected in XAI methods.\nTrust emerges as a critical issue in AI-assisted decision-making, as people often engage with AI recommendations and explanations without analytical scrutiny, par- ticularly when reinforced by automatic explanations. Strategies proposed to foster appropriate trust include considering both AI and human confidence Ma et al. (2023), suggesting that decision-makers need to understand when to trust AI and when to trust themselves. They showed that also considering humans' confidence promoted appropriate trust in AI compared to only using the AI confidence scores. Researchers have also demonstrated that cognitive forcing mitigates humans' over-reliance Danry et al. (2020). For instance, authors in Bu\u00e7inca et al. (2021) designed three cogni- tive forcing strategies to engage more thoughtfully with AI-generated explanations, demonstrating a reduction in over-reliance compared to simple XAI approaches."}, {"title": "2.2 Explanations in collaborative decision making", "content": "The investigation of the impact of XAI techniques on human users in decision-making tasks is predominantly situated in the human-computer interaction (HCI) field Gam- bino and Liu (2022); Lai et al. (2021). Such works often include comparisons between different types of explanations. For instance, authors in Lim et al. (2009) explored the effects of why and why not-explanations on users' system understanding, revealing that why-explanations fostered better understanding and trust towards the system compared to why-not-explanations. Recent efforts have delved into finer comparisons, such as rule- vs. example-based explanations with decision support systems, assessing their impact on system understanding and persuasiveness (van der Waa et al., 2021).\nConsequently, user-centredness and personalization have been driving factors in XAI-related research in HCI. In Millecamp et al. (2019), authors showed that users' characteristics, such as the need for cognition, influence the interaction with explainable recommendation systems. However, a subsequent study Millecamp et al. (2020) identified that users' openness plays a role in determining their willingness to reuse the explanatory systems. Similarly, in Conati et al. (2021), the authors demonstrated that providing explanations increases users' trust and perceived usefulness, offering insights on personalizing explanations based on users' personality traits. Furthermore, researchers evaluated personalized explanations, showing higher user satisfaction compared to non-personalized ones Tintarev and Masthoff (2012).\nIn the context of human-robot interaction (HRI) context, there is a scarcity of stud- ies on XAI with decision-making tasks, especially those investigating different XAI strategies or customization and user-centredness Setchi et al. (2020). Notable excep- tions include works like Kaptein et al. (2017), where authors explored preference for explanation styles in a belief-desire-intention agent on a Nao robot among both children and adults. Adults preferred goal-based explanations, while children did not exhibit specific preferences. Previous research has also been conducted regarding the persuasiveness of explainable robots during collaboration with humans Matarese et al. (2023a,b). Specifically, these highlighted that explanations based on the human-robot common ground can influence people more than accurate ones Matarese et al. (2023a) and that people's personality dimensions contribute to their willingness to accept the robot's justified suggestions Matarese et al. (2023b).\nWhile few studies focus on XAI with decision-making tasks in HRI, several ap- proaches have been proposed to explain robot planning. For instance, authors in Chakraborti et al. (2017) proposed to afford explainability as a reconciliation model using the Fetch robot. Their approach aims to progressively change the human model to bring it closer to the robot's, making the robot's plan optimal for such changes in the human model. Moreover, authors in Sukkerd et al. (2018) proposed an explainable planning representation to ease explanation generation and a method to generate con- trastive explanations as policy justification. Finally, Devin and Alami (2016) moved towards the direction of a user-aware XAI during shared plan execution.\nVirtual embodied agents have also been the subject of various studies in this regard (Anjomshoae et al., 2019). Researchers explained robot behavior as intention signaling using natural language sentences, evaluating their approach through an online study with a virtual robot Gong and Zhang (2018). Others introduced augmented reality to display XAI feedback and the robot's internal beliefs, testing their method with a virtual Johnny robot and demonstrating its potential to enhance the HRI Wang and Belardinelli (2022). Differently, authors in Amir and Amir (2018) developed an algorithm to summarize robots' behaviors using information extracted from the agent simulations, evaluating their algorithm with a computer in a game-like scenario.\nRobots' influence and persuasiveness have been studied for a long time since re- searchers observed that several influence mechanisms that occur between humans also occur in HRI Saunderson and Nejat (2019). Authors in Hashemian et al. (2019) explored persuasive robot strategies based on social power, presenting information or providing social rewards. Similarly, Saunderson and Nejat (2022) investigated how a robot's persuasive behavior influences people's decision-making in a guessing game,"}, {"title": "2.3 Evaluating the quality of explanations", "content": "Despite the increasing number of works focusing on the quality of XAI, defining what constitutes a good explanation remains a challenge. Researchers in the field acknowl- edge the urgency of establishing metrics for XAI (Nauta et al., 2022), yet defining objective metrics is challenging due to the strong dependence of XAI model efficacy on the application context and users' expertise (Hoffman et al., 2018). Consequently, recent efforts within the community have concentrated on elucidating the quality of explanations. For instance, authors in Wang and Yin (2021) outlined three desider- ata for XAI systems: the ability to (1) understand the AI model, (2) recognize the uncertainty underlying an AI prediction, and (3) calibrate their trust in the model.\nGiven that explanations are intended for human users, evaluations of XAI systems' properties are often conducted through user studies, with a particular emphasis on non-expert users Janssen et al. (2022). These studies often involve tasks unfamiliar to most people, ranging from ordinary tasks such as optimizing insulin doses van der Waa et al. (2021) or predicting recidivism Wang and Yin (2021) to more unconventional scenarios like assessing alien food preferences Lage et al. (2019) or classifying bird species Goyal et al. (2019); Wang and Vasconcelos (2020).\nWhile many works assessing or comparing XAI methods tend to define their own measures of goodness (Lage et al., 2019; van der Waa et al., 2021), a method has been proposed to objectively measure the degree of explainability of information provided by an XAI system (Sovrano and Vitali, 2022). This method quantitatively measures how many archetypical questions the system can answer. Moreover, authors in Holzinger et al. (2020) proposed the System Causability Scale to measure the explanations' quality based on causability (Holzinger et al., 2019). Taking a different perspective, authors in Wang and Yin (2022) compared different types of explanations in various application contexts, assessing their effectiveness based on three desiderata: improving people's understanding of the AI model, helping in recognizing the model uncertainty, and supporting calibrated trust in the model."}, {"title": "3 Material and Methods", "content": "This study aims to investigate the informativeness of contrastive explanations (A-XAI) compared to classical causal explanations (C-XAI) during a learning-by-doing task and explore the impact of the explainable agent type on participants' behavior. To this aim, we conducted a between-subject user study. One group of participants (22 in total) interacted with a computer (the COM group), while another group (22 in total) engaged with a social humanoid robot (the Robot group) like in Figure 1. Both groups were further divided based on the type of explanation experienced during the task. Furthermore, we conducted the same experiment with a baseline group of participants (11) who worked autonomously without interacting with any agents (the Self-taught group). Figure 2 provides an overview of the group distribution.\nBefore starting the experiment, all participants signed the informed consent. The informed consent that the University approved participants from the COM group signed of Paderborn ethical committee because such experiments have been performed in Paderborn, Germany. The remaining participants signed an informed consent approved by the ethical committee of \u201cRegione Liguria\u201d because the experiments were carried out in Genoa, Italy."}, {"title": "3.1 The task", "content": "We considered non-expert users to ensure the information passed through the inter- action was new. Thus, we considered only participants without knowledge about the task and its underlying rules. We implemented a nuclear power plant (NPP) management task, chosen for several reasons: its challenging yet engaging nature for non-expert users, simple governing rules, the potential for an AI model to learn these rules, and the typical lack of knowledge people have about the functioning of nuclear power plants.\nThe main objectives of the task (which we hid from users) were to generate max- imum energy and maintain the system in an equilibrium state. The features of the environment were subject to rules and constraints summarized as follows:\n\u2022 Each action triggered a change in the environment's features, affecting its values.\n\u2022 Specific preconditions had to be met to start and sustain nuclear fission for energy production.\n\u2022 Some conditions irremediably damage the NPP.\nThe assessment involved a decision-making learning-by-doing task where users interacted with a control panel to perform actions in a simulated environment. During the task, users could engage with an expert explainable AI agent by pressing buttons on the application GUI to ask what it would do in each step. Moreover, they could also ask why it would perform a specific action. To both these questions, the agents replied verbally. Aside from instructions regarding interacting with the control panel and the agent, users start the task with no prior knowledge.\nIn a fixed time frame of 30 minutes, the users had to discover:\n\u2022 The nature of the task (e.g., its goals).\n\u2022 The rules governing the simulated environment.\n\u2022 The rules guiding the AI model's actions.\nParticipants could interact with the environment by pressing buttons and setting sliders (Figure 1) to perform actions and observe the outcomes. Moreover, they could seek additional information from the artificial agent through its suggestions and explanations. Following this learning phase, participants have to complete an assessment phase. In this stage, participants had to perform the task they learned during training, aiming to achieve the best performance within a fixed time frame of 10 minutes. Subsequently, users underwent a test evaluating their comprehension of the task's objectives and rules. This test could take various forms, such as open-ended or multiple-choice questions. We opted for both types of questions and aimed to assess:\n\u2022 Users' knowledge of the task's objectives."}, {"title": "\u2022 Users' understanding of the task's internal rules.", "content": "\u2022 Users' ability to generalize the skills acquired during the task.\nThe roles within the human-robot/computer teams and their interaction modalities were predefined. The artificial agents were limited to assisting users during decision- making and could not perform actions themselves. Moreover, the agents could not provide suggestions on their own initiative; they could only respond to explicit user queries. Consequently, only the user interacted with the control panel and acted within the simulated environment. For the Robot group, iCub and participants sat side-by- side to improve their attitudes toward the robot during the collaboration (Geiskkovitch et al., 2020)."}, {"title": "3.2 The simulated environment", "content": "To design the functioning of our simulation, we started with the NPP's actual func- tioning and simplified it to reach a trade-off between complexity and feasibility. Our simulated power plant comprised four continuous features: reactor core pressure, the temperature of the water in the reactor, water amount in the steam generator, and reactor power. Furthermore, the power plant had four other discrete features related to the reactor rods: security rods, fuel rods, sustain rods, and regulatory rods. The first two had two levels: up and down. Instead, the latter two had three levels: up, medium, and down.\nThe reactor power linearly decreased over time due to the de-potentiation of the fuel rods. Hence, reactor power depends on the environment's features values and the occurrence of nuclear fission. The energy produced at each step is calculated by dividing the reactor power by 360, representing the power produced by a 1000MW reactor without power dispersion in 10 seconds (the expected duration of participant actions).\nUsers could perform 12 actions, ranging from adjusting rods' positions to adding water to the steam generator or skipping to the next step. Each action modified the value of three parameters, corresponding to the water's temperature in the core, core pressure, and the water level in the steam generator.\nThe rod configurations determined the magnitude of feature updates performed at the end of each step after the users' action. For instance, lowering the safety rods halted nuclear fission, leading to a decrease in the core temperature and pressure (unless they reach their initial values), with no changes in the water level of the steam generator. Conversely, if nuclear fission occurred and the user lowered the regulatory rods, the fission accelerated. This acceleration consumed more water in the steam generator, raising the core temperature, pressure, reactor power, and electricity faster than in normal conditions."}, {"title": "3.3 The agents' AI", "content": "Regarding the agents' AI model, we employed a deterministic decision tree (DT) trained using the Conservative Q-Improvement (CQI) learning algorithm (Roth et al., 2019). This approach allowed us to train the DT using a reinforcement learning (RL) strategy, avoiding the extraction of the DT from a more complex ML model (Vasilev et al., 2020; Xiong et al., 2017). CQI learns a policy as a DT by splitting its current nodes only if it represents a policy improvement. Leaf nodes correspond to abstract states and indicate the action to be taken, while branch nodes have two children and a splitting condition based on a feature of the state space. Over time, their algorithm creates branches by replacing existing leaf nodes if the final result represents an improved policy. In this sense, the algorithm is considered additive, while it is conservative in performing the splits (Roth et al., 2019).\nWe used RL for two main reasons: (1) we did not have data to train our ML model, and (2) we could better control the model behavior by designing a proper reward function. Indeed, we defined the RL reward to maximize the amount of energy produced without damaging the NPP and prioritizing actions having effects on the environment's features (e.g., actually changing their values). We obtained a DT that exhibited solid behavior and optimal performance on our simulated NPP.\nInstead of extracting the DT from a more complex ML model (Vasilev et al., 2020; Xiong et al., 2017), we used this learning strategy to simplify the translation from the AI to the XAI. Moreover, using a binary DT, we obtained an explainable AI model without sacrificing performance. There is a broad consensus on the inherent transparency of such models, even though some authors questioned the assumption that simpler models are more interpretable than complex ones F\u00fcrnkranz et al. (2020). The artificial agents used this expert DT to choose their action among the twelve possible actions based on the eight environment's features.\nStarting from its root node, the DT was queried on each internal node, representing binary splits, to determine which sub-trees continue the descent. Each internal node regarded a feature $x_i$ and a value for that feature $v_i$: the left sub-tree contained instances with values of $x_i < v_i$, while the right sub-tree contained instances with values of $x_i > v_i$ (Buhrman and de Wolf, 2002).\nThe DT's leaf nodes represented actions, defined with an array containing the actions' expected Q-values in implementing Roth et al. (2019). The higher Q-value was associated with the most valuable action. This design enabled the DT to respond to users' both what and why questions. To address a what question, we only needed to navigate the DT using the current values of the environment's features and present the resulting action to the user. To answer a why question, we could provide one of the features' values encountered during the descent."}, {"title": "3.4 The agents' \u03a7\u0391\u0399", "content": "Since the AI model to explain is inherently transparent (Adadi and Berrada, 2018), we can directly exploit the DT to provide explanations by using one of the feature values we encounter during the tree's descent.\nAs explained in Section 3.3, during the DT descent, we encounter a set of split nodes defined by a feature $x_i$ and a value $v_i$; the direction of the descent tells us whether the current scenario has a value of $x_i < v_i$ or $x_i > v_i$. Each of those inequalities can be used to provide an explanation, aiding users in relating actions with specific values of the environment's features. In our case, an explanation for the action \u201cadd water to the steam generator\u201d could be \u201cbecause the water level in the steam generator is \u2264 25\" (dangerously low).\nThe problem of selecting which feature to use among those encountered during descent is referred to as explanation selection. In our case, we compared two explanation selection strategies. Classical approaches typically use the most relevant features (based on measures such as the Gini index, information gain, or other established metrics (Stoffel and Raileanu, 2001)). We planned to compare a classical explanation strategy with a contrastive, partner-aware one.\nThe classical XAI explains using only the AI outcomes and environment states. In particular, it justifies the agent's suggestions using the most relevant features, which were the first ones in the DT's structure (see (Roth et al., 2019)). However, the system tried to provide different explanations in each step. It did so by keeping track of the DT's nodes already used and preferring to use the not-used ones by descending the DT's structure.\nOn the other hand, the partner-aware XAI approach, through monitoring and scaffold- ing, took into consideration the partner's action indication and used such indications to provide contrastive explanations. The facts (the outcome to explain) were the agents' suggestions, while the foils (the expected outcome) were the predicted users' actions. Figure 3 illustrates an example of these two approaches.\nWe categorized our findings based on whether they focused on the influence of the computer, the social robot, or self-learning. We expected that:\n\u2022 H1: adaptive explanations would elicit more accurate participants' mental mod- els about the task because we expected that the contrastive nature of such expla- nations would help participants understand the task's rules better and faster.\n\u2022 H2: regarding participants' learning, the robot, and the computer would produce comparable outcomes, while assisted participants would outperform the not- assisted ones.\n\u2022 H3: participants interacting with the robot would be more persuaded by it, with a higher impact of their personality dimensions on their decision-making due to the social interaction."}, {"title": "4 Results", "content": null}, {"title": "4.1 A priori knowledge of the task", "content": "Since we used a between-subject approach, we needed to ensure that the different groups had comparable starting conditions, i.e. that they started with similar task knowledge. To measure participants' knowledge about the functioning of NPPs, we asked them to describe such functioning in an open-ended question during the pre-experiment questionnaire. Two student colleagues then coded their answers in three levels of understanding (No, Some, and A lot of knowledge) based on whether they reported incorrect or no information, some information, or correct information, respectively. Subsequently, we performed a $x^2$ test to investigate the differences in the distribution of participants' prior levels of understanding among the experimental groups. We found no significant differences among such distributions ($x^2$ test: $x^2(8) = 14.1, p = .078$). The distribution of participants' level of prior knowledge is shown in Figure 4."}, {"title": "4.2 Quantitative differences between classical and adaptive explanations", "content": "As mentioned, we manipulated our experiments on the type of explanations the artifi- cial agents provided. For one group of participants, we provided classical explanations (C-XAI), while for the other group, we provided contrastive adaptive explanations (A-XAI). Since the difference between the two explanation strategies resided in the explanation selection, we counted (in percentage) the number of explanation topics (the explanandum) used by the artificial agent during the training phase.\nWe compared such percentages among the experimental groups to check whether different explanation strategies brought different explanandum. We performed an independent samples t-test on each possible explanandum (the environment's features) between the C-XAI and A-XAI groups for both the COM and Robot groups. Regarding the COM group, we found significant differences between explanandum percentages in five out of eight comparisons. Instead, regarding the Robot group, we found significant differences in four out of eight comparisons. Table 1 shows that our manipulation occurred by reporting all the comparisons with their statistics.\nMoreover, we investigated whether there were also differences in the question-asking frequencies between the conditions. Hence, we performed a $x^2$ test on the questions' frequencies. Such tests showed no significant differences between the groups about both the what- and why-questions."}, {"title": "4.3 The influence of the computer", "content": "We found that participants who received adaptive explanations performed significantly more actions than those who received classical explanations (independent samples t-test: t = 2.21, p = .039). Figure 5 shows a box plot of participants' actions during the training phase. However, we did not find differences in the behavioral measures regarding the assessment phase between the two groups.\nThis result is explained by the significant difference between the average moving time of the two groups (independent samples t-test: t = 2.3, p = .02), as shown in Figure 6. We also found that the decision time of the steps in which participants asked what- and why questions were significantly different between the C-XAI and A-XAI groups (independent samples t-test: t = 2.19, p = .03, and t = 2.3, p = .02, respectively). Finally, we found that the decision times during the assessment phase differed significantly between the two groups (independent samples t-test: t = 4.4, p < .001), with the A-XAI group moving significantly faster than the C-XAI one.\nSince participants performed different numbers of moves, we aggregated those in slices (on the x-axis in Figure 6) representing pieces of 5% of the task to compare their moves' decision times. We then compared the participants' corresponding slices to have measures of similarity and easily visualize their moving times. However, we found that participants in the A-XAI group received less verbose explanations (of one word, on average) than those in the C-XAI group (independent samples t-test: t = 2.31, p = .03) as shown in Figure 7. We found no correlations between participants' personality traits, assessment behavioral measures, or persuasiveness.\nTo summarize these findings, participants who interacted with the computer moved faster and were more resolute with the adaptive explanations than with the classical ones. This caused those who received the former to perform more actions than the other group of participants since the task was time-bounded."}, {"title": "4.4 The influence of a humanoid social robot", "content": "In this section, we show the results related to the Robot group. Contrary to the COM group, we found no behavioral differences between the two groups that interacted with the robot, neither in training nor in the assessment phase. We found that partici- pants who interacted with the iCub robot performed a comparable number of actions during the training, regardless of the experimental group to which they belonged. Indeed, contrary to the COM group, we found no significant differences regarding the participants' decision time between the C-XAI and A-XAI groups. However, we found that participants in the A-XAI group received less verbose explanations (of one word, on average) than those in the C-XAI group (independent samples t-test: t = 2.31, p = .03) as shown in Figure 7.\nAs for the previous group, we found no correlations between the participants' personality traits and behavioral measures during training. However, regarding the assessment phase, we found that the amount of energy produced negatively correlated with participants' positive agency (Pearson's r = -.455, p = .038), and positively with their negative agency (Pearson's r = .484, p = .026). Moreover, we found that also the number of anomalies - namely, conditions that damaged the NPP and restarted the application - negatively correlated with participants' positive agency (Pearson's r = -.449, p = .041)."}, {"title": "4.5 Comparisons between the computer and the humanoid robot", "content": "In this section, we compare the results of the COM and Robot groups. To measure the influence that such artificial agents had on participants, we collected three types of moves for each step of the training phase:\n\u2022 Equal: participants' originally selected action and the agents' suggested action were the same from the beginning.\n\u2022 Follow self: participants' originally selected action and the agents' suggested action differed, but they chose to confirm their initial indication.\n\u2022 Follow AI: participants' originally selected action and the agents' suggested action differed and they followed the agents' advice."}, {"title": "4.6 Comparison between self- and XAI assisted-learning", "content": null}, {"title": "4.6.1 Behavioral measures", "content": "We found an effect of the experimental condition on the number of actions performed in training among the experimental groups (ANOVA F(4) = 8.66", "9)": "t = 3.654", "groups": "t = 4.684", "10)": "t = 3.345", "t-test": "t = \u22123.62, p < .001, and t = \u22124."}]}