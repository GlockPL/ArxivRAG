{"title": "DEEP INCOMPLETE MULTI-VIEW LEARNING VIA CYCLIC PERMUTATION OF VAES", "authors": ["Xin Gao", "Jian Pu"], "abstract": "Multi-View Representation Learning (MVRL) aims to derive a unified representation from multi-view data by leveraging shared and complementary information across views. However, when views are irregularly missing, the incomplete data can lead to representations that lack sufficiency and consistency. To address this, we propose Multi-View Permutation of Variational Auto-Encoders (MVP), which excavates invariant relationships between views in incomplete data. MVP establishes inter-view correspondences in the latent space of Variational Auto-Encoders, enabling the inference of missing views and the aggregation of more sufficient information. To derive a valid Evidence Lower Bound (ELBO) for learning, we apply permutations to randomly reorder variables for cross-view generation and then partition them by views to maintain invariant meanings under permutations. Additionally, we enhance consistency by introducing an informational prior with cyclic permutations of posteriors, which turns the regularization term into a similarity measure across distributions. We demonstrate the effectiveness of our approach on seven diverse datasets with varying missing ratios, achieving superior performance in multi-view clustering and generation tasks.", "sections": [{"title": "INTRODUCTION", "content": "Multi-view data are prevalent in real-world applications\u00b9, capturing various aspects of a shared subject. Examples include observing a 3D object from multiple angles, applying diverse image feature descriptors, or reporting the same news in different languages. This offers valuable self-supervised signals that enable the extraction of meaningful patterns. Multi-View Representation Learning (MVRL) aims to maps this data into a latent space, integrating information from multiple views into a unified representation for downstream tasks like clustering, classification, and generation (Li et al., 2018). However, in practice, not all views are available for every sample, presenting the challenge of Incomplete Multi-View Representation Learning (IMVRL) (Tang et al., 2024). The incomplete data complicates the integration of information from different views, making it difficult to derive high-quality representations under varying missing ratios.\nAmong MVRL methods, Multimodal Variational Auto-Encoders (MVAEs) stand out for their robustness in modeling the latent distributions of multi-view data (Aguila & Altmann, 2024). Their flexibility in handling incomplete data stems from mean-based fusion strategies, such as Mixture-of-Experts (MoE) and Product-of-Experts (PoE) (Wu & Goodman, 2018; Shi et al., 2019; Sutter et al., 2021), which can accommodate varying numbers of views and have been validated by Hwang et al. (2021). Furthermore, some MVAE variants enhance inter-view consistency by incorporating data-dependent priors, which facilitate better information integration (Sutter et al., 2020; 2024) and reduce reliance on missing views (Hwang et al., 2021). Despite these advancements, the sufficiency and consistency of learned representations are still not assured in the presence of incomplete data. Samples with fewer views inherently provide less information, which hampers the integration and undermines inter-view consistency. As a result, these methods experience significant performance degradation and semantic incoherence across views as the rate of missing data increases."}, {"title": "RELATED WORKS", "content": "Incomplete Multi-View Representation Learning (IMVRL) Early IMVRL approaches addressed incomplete data by grouping available views and applying classical methods like CCA (Hotelling, 1992). DCCA (Andrew et al., 2013) introduced nonlinear representations via correlation objectives, while DCCAE (Wang et al., 2015) enhanced reconstruction with autoencoders. As missing rates increased, the need for handling incomplete information grew. Methods like DIMVC (Xu et al., 2022) projected representations into high-dimensional spaces to improve complementarity, and DSIMVC (Tang & Liu, 2022) used bi-level optimization to impute missing views. Completer (Lin et al., 2021; 2023) maximized mutual information and minimized conditional entropy to recover missing views, while CPSPAN (Jin et al., 2023) aligned prototypes across views to preserve structural consistency. ICMVC (Chao et al., 2024) proposed high-confidence guidance to enhance consistency, and DVIMC (Xu et al., 2024) introduced coherence constraints to handle unbalanced information.\nMultimodal Variational Auto-Encoders (MVAEs) MVAEs are generative models that maximize the log-likelihood of observed data through latent variables. MVAE (Wu & Goodman, 2018) models the joint posterior using PoE (Hinton, 2002), though this may hinder unimodal posterior optimization. MMVAE (Shi et al., 2019) and mmJSD (Sutter et al., 2020) use MoE for the joint posterior, with MMVAE applying pairwise optimization for reconstructing all views, but struggling to aggregate information efficiently. mmJSD addresses this with a dynamic prior, replacing regularization with Jensen-Shannon Divergence. Sutter et al. (2021) propose Mixture-of-Product-of-Experts (MoPoE) to decompose KL divergence into 2V terms, while MVTCAE (Hwang et al., 2021) introduces an information-theoretic objective using forward KL divergences. MMVAE+ (Palumbo et al., 2023) extends MMVAE by separating shared and view-peculiar information in latent subspaces and incorporating cross-view reconstructions."}, {"title": "METHOD", "content": "In this section, we present the core components of our method, which aims to capture relationships between views in incomplete multi-view data. Our approach models inter-view correspondences in the latent space of MVAEs, enabling the inference of missing views and the generation of more sufficient and consistent representations. To facilitate transformations between views, we apply permutations to reorder the latent variables and introduce two partitions based on their encoding information (Section 3.1). We then use these partitions to factorize the posterior and derive the ELBO for optimization (Section 3.2). Finally, we define priors using the permuted variables in the regularization term. By applying cyclic permutations to the posteriors, we transform the regularization into a similarity measure, enforcing consistency across views (Section 3.3)."}, {"title": "INTER-VIEW CORRESPONDENCE AND LATENT VARIABLE PARTITION", "content": "Given an incomplete multi-view dataset {X\u2081}\u1d62=\u2081, where each X\u1d62 = {x(v)}v\u2208\u2124\u1d62 consists of multiple views, we denote by \u2124\u1d62 a subset of the complete view indices [L] = {1,2,..., L} (i.e., \u2124\u1d62 \u2286 [L]). Each observed view x(v) is a vector in \u211ddv. For simplicity, we drop the subscript i from \u2124\u1d62 and use \u2124. We then encode the data from each view into a latent variable set, where each view contributes complementary information. These latent variables, z \u2208 \u211dd, are derived using encoders parameterized by {\u03c6\u1d65}\u1d65=\u2081 , following standard MVAEs.\nWe adopt the term \u201ccorrespondences\" from Huang et al. (2020), but extend it to explicitly construct a mapping channel, referred to as inter-view correspondences. Specifically, we introduce multiple nonlinear mappings from the v-th view to the l-th view, represented by functions {f\u1d62\u1d65}, parameterized by {\u03b1\u02e1\u1d65}. For each pair where v \u2260 l, a unique mapping f\u1d62\u1d65 establishes a direct relationship between the source view v and the target view l. This allows for cross-view transformations, where information from one view informs the representation of another. The latent variables are then organized into a set \u039e = {z(l)\u1d65} (\u1d65,l)\u2208\u2124x[L], where z(l)\u1d65 denotes the representation of the l-th view, with subscript v indicating its source view. Specifically: (1) If v = l, z(l) is directly encoded from the observed view x(v), following a d-dimensional Gaussian distribution \ud835\udca9 (z(l); \u03bc(x(v)), \u03a3(x(v))), denoted as q(z(l) | x(v); \u03c6\u1d65). (2) If v \u2260 l, z(l) is transformed from z(v) using f\u1d62\u1d65, following a Gaussian distribution \ud835\udca9(z(l); f\u1d62\u1d65 \u25e6 \u03bc(x(v)), f\u1d62\u1d65 \u25e6 \u03a3(x(v))), denoted as q(z(l) | x(v); \u03c6\u1d65, \u03b1\u02e1\u1d65).\nTo organize the encoded latent variables, we construct a matrix Z\u2080. The core idea is that variables with the same superscript l encode similar information about the l-th view, regardless of whether they are directly encoded or transformed. Thus, even if the columns of Z\u2080 are reordered, each column continues to represent the same view, and the information encoded by elements at corresponding positions remains invariant. This observation motivates us to group the latent variables by views, leading to a partition of the set \u039e. Mathematically, a partition \ud835\udcab of a set X is a collection of non-empty, mutually disjoint subsets of X, also known as a \"set of sets\". Accordingly, we define the single-view partition for \u039e:\nDefinition 1 (Single-view Partition). A single-view partition of \u039e, denoted as \ud835\udcab\u209b(\u039e), is defined as {S\u2081}\u02e1=\u2081 such that \u22c3\u02e1=\u2081 S\u2081 = \u039e. The l-th single-view cell S\u2081 = {z(l)\u1d65}\u1d65\u2208\u2124 and |S\u2081| = |\u2124|.\nAccordingly, each sample has a unique \ud835\udcab\u209b(\u039e), where each single-view cell S\u2081 consists of all variables with the same superscript, representing the l-th view. More specifically, these variables are all"}, {"title": "POSTERIOR FACTORIZATION AND THE DERIVATION OF ELBO", "content": "Given the latent variables \u039e and \u03a9, our goal is to infer their posterior distribution p(\u039e, \u03a9 | {x(v)} 1). However, this exact posterior involves an intractable integral, so we approximate it with a variational posterior q(\u039e, \u03a9 | {x(v)} 1). For a given complete-view partition \ud835\udcabc(\u039e) = {C\u2099}\u2099\u2208\ud835\udd40,\nwe assume that the density factorizes as follows:\nq(\u039e, \u03a9 | {x(v)}1) = \u220fc\u2099\u2208\ud835\udcabc q(\u03c9\u2099 | C\u2099, {x(v)} \ud835\udd40\u2099)q(C\u2099 | {x(v)} \ud835\udd40\u2099)\n= \u220fc\u2099\u2208\ud835\udcabc [q(\u03c9\u2099 | C\u2099, {x(v)}\ud835\udd40\u2099) \u220fv=1,v\u2208\ud835\udd40\u2099 q(z(l) | x(v))].\nTo optimize this factorized posterior, we minimize the KL divergence between the true posterior and the variational posterior, expressed as:\nKL [q(\u039e,\u03a9 | {x(v)}1) || p(\u039e, \u03a9 | {x(v)}1)] = logp({x(v)}1) \u2212 \u2112ELBO({x(v)}1),\nwhere \u2112ELBO({x(v)}1) is the Evidence Lower Bound (ELBO) of the log-likelihood of incomplete multi-view data {x(v)}1. Maximizing the ELBO effectively minimizes the KL divergence, thereby improving the approximation of the true posterior. Next, we use decoders parameterized by {\u03b8\u02e1}\u02e1=\u2081\nto reconstruct the observations. Each view x(n) is reconstructed using the latent variable z\u03c3(n) (represent the n-th view) and a consensus variable \u03c9\u2099. The likelihood can be written as:\np(x(n) | C\u2099, \u03c9\u2099) = p (x(n) | C\u2099 \u2229 S\u2099, \u03c9\u2099; \u03b8\u2099),\nwhere C\u2099 \u2229 S\u2099 = z(n) is the diagonal element of Z\u2081, with its source * determined by the permutation. Then the ELBO is expressed as (with detailed derivations in Appendix A.4):\n\u2112ELBO ({x(v) }1) = \u2211c\u2099\u2208\ud835\udcabc \ud835\udd3cq(C\u2099,\u03c9\u2099|{x(v)} \ud835\udd40\u2099) [log p(x(n) | C \u2099\u2229 S\u2099,\u03c9\u2099)]\n\u2212 \u2211l=1 \u2211v\u2208\ud835\udd40 KL [q(z(l) | x(v)) || p(z(l))] \u2212 \u2211c\u2099\u2208\ud835\udcabc KL [q(\u03c9\u2099 | C\u2099, {x(v)} \ud835\udd40\u2099) || p(\u03c9\u2099)] . (1)\nThe ELBO in Eq. (1) consists of three terms. The first term represents the reconstruction loss for all observed views, ensuring that the variable z(n) = C\u2099 \u2229 S\u2099 encodes information of n-th view, while \u03c9 captures shared aspects. By applying permutations to obtain a new \ud835\udcabc(\u039e), we can derive another \u2112ELBO. This randomness in permutation results in z\u03c3(n) with different source views *, facilitating both self-view reconstruction and cross-view generation. We can even use a convex combination of different \u2112ELBO, discussed in Appendix A.4. Different partitions produce varying \u03c9 that serve the same role, ensuring that the first k dimensions capture shared features across views. The remaining two terms are regularization terms for z and \u03c9, further discussed in Section 3.3."}, {"title": "PRIOR SETTING USING CYCLIC PERMUTATIONS OF POSTERIORS", "content": "The regularization terms in the ELBO for z and \u03c9, when their priors are properly set, support our learning objective of establishing inter-view correspondences in MVAEs. In the first term for z, the outer summation spans all single-view cells S\u2081, while the inner summation includes all l-th view latent variables z(l) in S\u2081, or equivalently, all variables in the l-th column of matrices Z\u2080 or Z\u2081. With well-established inter-view correspondences, the l-th view variables should encode similar information and have distributions that are as close as possible, regardless of their sources.\nIn Section 3.1, we introduced the idea of permuting the columns of matrix Z\u2080 to generate different complete-view partitions. When these permutations follow a cyclic structure, the permuted posteriors can be used as informational priors. For example, if we have three distributions P\u2081, P\u2082, P\u2083,\nwe can set the prior for P\u2081 as P\u2083, for P\u2082 as P\u2081, and for P\u2083 as P\u2082. As these pairs converge, the\ndistributions become more similar due to the cyclic nature of the permutation (P\u2081\u21d2P\u2083\u21d2P\u2082\u21d2P\u2081). This cyclic permutation of indices ensures that the latent variables in each single-view cell become more homogeneous. Cyclic permutations can be efficiently generated before training using Sattolo's Algorithm with time complexity O(N) (Wilson, 2005), defined as follows:"}, {"title": "EXPERIMENTS", "content": "We extensively evaluated the proposed method across seven diverse multi-view datasets, summarized in Table 1. These datasets encompass a variety of view types with different dimensions, originating from diverse sensors or descriptors, as well as real-world perspectives captured from different angles. PolyMNIST (Sutter et al., 2021) consists of five images per data point, all sharing the same digit label but varying in handwriting style and background. ShapeNet is a large-scale repository of 3D CAD models of objects (Chang et al., 2015). For each object, we rendered five images from viewpoints spaced 45 degrees apart around the front of the object. Then we selected five representative categories to create a multi-view dataset called MVShapeNet. The missing patterns are predetermined and saved as masks before training. Specifically, for each missing rate \u03b7 = {0.1, 0.3,0.5,0.7}, we randomly select \u03b7 \u00d7 len(dataset) samples to be incomplete, ensuring that each incomplete sample retains at least one view while missing at least one. For more details on the generation of missing patterns and masks, please refer to Appendix C.2.\nIn Section 4.1, we perform clustering in the latent space and compare our method to eight state-of-the-art IMVRL approaches. The results, consistent across various missing ratios, underscore the"}, {"title": "ENHANCED PERFORMANCE FOR INCOMPLETE MULTI-VIEW CLUSTERING", "content": "To evaluate the ability of our method to handle incomplete multi-view data, we first assess clustering performance under various missing ratios, following Zhang et al. (2020); Lin et al. (2023); Cai et al. (2024). We apply k-means clustering to the consensus representation \u03c9 and compare our method with eight IMVRL approaches: DCCA, DCCAE, DIMVC, DSIMVC, Completer, CPSPAN, ICMVC, and DVIMC (see Section 2 and Table 9 in Appendix D.1 for their modeling details). Clustering performance is measured by Accuracy (ACC), Normalized Mutual Information (NMI), and Adjusted Rand Index (ARI)."}, {"title": "COMPARING WITH OTHER MVAES USING TWO IMAGE DATASETS", "content": "In this section, we compare our method with six other MVAEs that utilize different posterior and prior modeling techniques: mVAE (Wu & Goodman, 2018), mmVAE (Shi et al., 2019), MoPoE (Sutter et al., 2021), mmJSD (Sutter et al., 2020), MVTCAE (Hwang et al., 2021), and MMVAE+ (Palumbo et al., 2023). These models can naturally adapt to incomplete scenarios because their mean-based fusion can accommodate any number of views, as validated by Hwang et al. (2021). We perform experiments on two tasks: multi-view clustering and generation, to demonstrate that our learned representation is able to extract more sufficient information from multiple views and infer missing views from incomplete observations while maintaining consistent semantics. We conduct our evaluation using two image datasets, PolyMNIST and MVShapeNet."}, {"title": "POLYMNIST: PRESERVING CONSISTENT SEMANTICS ACROSS DIVERSE STYLES", "content": "For the PolyMNIST dataset, the shared information across its five views is the digit ID, while view-specific details include handwriting styles and backgrounds. Although the digit is present in each view, it can be obscured or unclear in some images, making it crucial to aggregate complementary information from all views for accurate recognition. We use the original split with 60K tuples for training and 10K for testing, All models are trained on incomplete observations (\u03b7 = 0.5), with 50% of samples having 1 to 4 views missing. We evaluate model performance on the testing data across all possible incomplete view combinations, totaling C\u2085\u00b9 + C\u2085\u00b2 + C\u2085\u00b3 + C\u2085\u2074 = 30 cases.\nEvaluation protocol At test time, given the incomplete subset {x(v)}\ud835\udc63\u2208\ud835\udd40, we extract the representation using |\ud835\udd40| encoders and evaluate its quality. We perform k-means clustering directly and use Normalized Mutual Information (NMI) as the performance metric. Next, we generate all views {x(v)}[L] using the corresponding decoders. To assess consistent semantics across views, we measure coherence accuracy by feeding the generated views into a pretrained CNN-based classifier and checking if the predictions match the labels of the given subsets. Finally, we use the Structural Similarity Index Measure (SSIM) to compare the similarity between the reconstructions and the ground truth. All results are averaged across subsets of the same size.\nResults The left plot shows that our method consistently outperforms others in clustering across various incomplete scenarios. As the number of missing views increases, PoE- and MoE-based fusion methods experience sharp performance declines due to severe incomplete information. In contrast, only our method and MVTCAE maintain high levels of structural information. Our approach explicitly establishes inter-view correspondences to compensate for missing information, encoding different views into a latent space that facilitates easier transformations between them. MVTCAE penalizes latent information that cannot be inferred from other views to retain only highly correlated details. Both methods enforce a form of consistency in the representation, ensuring that the aggregated information is less affected by the presence of missing views."}, {"title": "MVSHAPENET: CAPTURING DETAILED INFORMATION FROM VARIOUS ANGLES", "content": "We further evaluate our method on the MVShapeNet dataset. Unlike PolyMNIST, where views share a few common pixels depicting the same digit against various background styles, MVShapeNet presents a smaller inter-view gap and greater consistency due to its uniform white backgrounds with the same object captured from different real-world angles. We use an 80:20 train-test split and apply the same experimental settings as in Section 4.2.1.\nIn this setting, MVAEs are less prone to semantic inconsistencies observed with PolyMNIST, meaning the generated object generally matches the input. However, we expect the learned representations to preserve more detail such as furniture hollowing, textures, and lightnings, which remains a challenge under high missing ratios. To evaluate this, we compare our method with other MVAES at missing rates of \u03b7 = 0.1 and 0.5, testing across all possible incomplete combinations. For quantitative evaluation, we use average SSIM to evaluate the basic structure of generated images. Additionally, we pretrain two CNN-based classifiers on all views to evaluate whether the decoded images accurately capture object categories and perspective angles. As demonstrated in Table 4 of Appendix B.1, our method consistently performs well, regardless of whether it is trained at high or low missing rates and tested on any incomplete combination. In contrast, other methods either show a dramatic performance drop as the number of missing views increases or rely on memorizing incomplete samples without effectively aggregating complementary information from additional views."}, {"title": "CONCLUSION", "content": "In this paper, we presented Multi-View Permutation of VAEs (MVP), a novel framework to address the challenges of incomplete multi-view learning. By explicitly modeling inter-view correspondences in the latent space, MVP effectively captured invariant relationships between views. We derived a valid ELBO for efficient optimization by applying permutation and partition operations to the latent variable set. Notably, these operations on multi-view representations are not limited to the VAE framework and can be extended into non-generative models. Additionally, the introduction of an informational prior using cyclic permutations of posteriors resulted in regularization terms with both practical meanings and theoretical guarantees. Extensive experiments on seven diverse datasets demonstrated the robustness and superiority of MVP over existing methods, particularly in scenarios with high missing ratios. These findings underscore its potential to reveal more informative latent spaces and fully unlock the capability of MVAEs to handle incomplete data."}, {"title": "A THEORETICAL ANALYSIS", "content": "In this section, we present a comprehensive theoretical analysis of the proposed method, offering additional details to complement the main text."}, {"title": "PARTITION AND PERMUTATION OF A SET", "content": "In mathematics, a partition of a set refers to a division of its elements into non-empty, mutually exclusive subsets, such that each element of the original set belongs to exactly one of these subsets. In simpler terms, a partition is a \u201cset of sets\u201d, where each subset is known as a cell.\nDefinition 5 (Partition of A Set (Brualdi, 2004)). A family of sets P is a partition of the set X if and only if all of the following conditions hold:\n(1) P does not contain the empty set (i.e., \u00d8 \u2209 P).\n(2) The union of the sets in P is equal to X (i.e., \u222aA\u2208\ud835\udcab A = X). The sets in P are said to exhaust or cover X.\n(3) The intersection of any two distinct sets in P is empty (i.\u0435., \u2200A, B \u2208 P, A \u2260 B \u21d2 A \u2229 B = \u2205). The elements of P are said to be pairwise disjoint or mutually exclusive.\nIn this work, we introduce two specialized types of partitions applied to the latent variable set \u039e: the single-view partition \ud835\udcab\u209b(\u039e) and the complete-view partition \ud835\udcabc(\u039e). These partitions are tailored to the particular structure and requirements of the problem under study.\nThe visualization of these two partitions is facilitated by arranging the variables in a matrix and dividing them according to rows and columns. matrix Z\u2080 contains diagonal elements directly derived from observed data, while off-diagonal elements represent transformations of the diagonal elements. Each column consists of variables z(l) corresponding to the l-th view, derived from different sources. Hence, the single-view partition of \u039e corresponds to the set of columns in the matrix. In contrast, the complete-view partition can be represented by dividing the matrix by rows, where each row encompasses all L views. However, this partition is not unique; by reordering the elements within each column and then partitioning the matrix by rows, we obtain a new complete-view partition \ud835\udcabc(\u039e), as illustrated by the right matrix Z\u2081."}, {"title": "CYCLIC PERMUTATION AND SATTOLO'S ALGORITHM", "content": "A cyclic permutation is a specific type of permutation that consists of exactly one cycle in its cycle notation, with the cycle length equal to the size of the set (Gross, 2016). For example, a cyclic permutation \u03c3 of the set X = {1,2,3,4} can be written as (k\u2081k\u2082k\u2083k\u2084), where k\u1d62 \u2208 X. A formal definition is given in Definition 3. In a cyclic permutation, each element of a set with more than one element is cyclically shifted, meaning each element is mapped to another, and after a number of mappings equal to the set size, every element returns to its original position. Cyclic permutations are particularly useful in our setting, as they guarantee convergence of the regularization term (see Appendix A.3) and can be efficiently generated using Sattolo's Algorithm (Wilson, 2005), which operates with linear time complexity.\nAlgorithm 1 Sattolo's Algorithm for Cyclic Permutation\nInput: Array A of size n\nOutput: A cyclic permutation of A\nInitialize array length n = |A|.\n1: for i in n-1 to 1 do\n2: Randomly select j from 0 to i - 1;\n3: Swap A[i] and A[j];\n4: end for\n5: return A;\nThe algorithm starts with the identity permutation \u03c3(0) = Id. For each i \u2208 {1, ..., n-1}, we denote by \u03c3(i) the permutation obtained after the i first steps. Step i consists in choosing a random integer k\u1d62 in {1, ..., n-i} and swapping the values of \u03c3(i-1) at places k\u1d62 and n\u2212 i + 1. In this way, we obtain a new permutation \u03c3(i), which is equal to \u03c4k\u1d62,n-i+1 \u25e6 \u03c3(i-1), where \u03c4k\u1d62,n-i+1 is the transposition exchanging k\u1d62 and n \u2212 i + 1. Finally, the algorithm returns the permutation \u03c3 = \u03c3(n-1).\nProposition 1. The mapping produced by Sattolo's algorithm is a cyclic permutation, and every cyclic permutation can be obtained using Sattolo's algorithm.\nProof. The correctness of Sattolo's algorithm follows from the fact that it generates a unique decomposition of a cyclic permutation \u03c3 as a product of transpositions, \u03c4kn-1,20 \u25e6\u00b7\u00b7\u00b7\u25e6 \u03c4ki,n-i+10 \u25e6\u00b7\u00b7\u00b7\u25e6 \u03c4k1,n,\nwhere k\u1d62 \u2208 {1, . . ., n \u2013 i} for 1 \u2264 i \u2264 n \u2212 1.\nFor n = 2, the permutation \u03c3 = \u03c4k1,2 is clearly a cyclic permutation. Assuming Sattolo's algorithm works for sets with fewer than n elements, we now demonstrate that it also holds for a set of size n."}, {"title": "THE SIMILARITY MEASURE OVER DISTRIBUTIONS", "content": "In this section, we introduce a similarity measure between distributions, referred to as the Dissimilarity Coefficient (d.c.), which we use to reduce the value of the KL divergence term in the ELBO. Consequently, this reduction minimizes the dissimilarity between distributions, effectively maximizing their similarity. We also explain how the informational prior in our method transforms the regularization term into a new d.c..\nIn various statistical fields-such as hypothesis testing, cluster analysis, and pattern recognition-it is essential to distinguish between probability distributions using appropriate dissimilarity coefficients (or separation measures, denoted as d.c.). A d.c. for a set of N probability distributions quantifies their \u201cdegree of heterogeneity\". For N = 2, a d.c. can be interpreted as a \u201cdistance\" between two distributions, though it may not always represent a metric distance in the strict sense.\nDefinition 7 (Dissimilarity Coefficient (Sgarro, 1981)). Let \ud835\udcab denote the set of probability measures on a measurable space (\u03a9, \u2131). Let N \u2265 2 be a fixed natural number. A dissimilarity coefficient (d.c.) of order N is a mapping d : \ud835\udcabN \u2192 \u211d \u222a {+\u221e} that satisfies the following properties for any P\u2081, P\u2082,..., P\u2099 \u2208 \ud835\udcab:"}, {"title": "POSTERIOR FACTORIZATION AND THE DERIVATION OF ELBO", "content": "Theorem 1. The Permutation Divergence defined in Definition 4 is a dissimilarity coefficient.\nProof. Consider a cyclic permutation \u03c3 defined on [N] with cycle notation (1,r\u2081,...,r\u2099\u208b\u2081). The corresponding permutation divergence is given by:\nd(P\u2081, P\u2082,..., P\u2099; \u03c3) = \u2211\u1d62=\u2081\u1d45 KL[P\u1d62 || P\u03c3(i)].\nSince the divergence is a sum of KL divergences, property (1) of non-negativity is satisfied.\nTo verify the identity of indiscernible, note that d(P\u2081, P\u2082,..., P\u2099; \u03c3) = 0 if and only if each term\nin the sum is zero. This implies KL[P\u1d62 || P\u03c3(i)] = 0 for all i, which occurs if and only if P\u1d62 = P\u03c3(i).\nConsequently, P\u2081 = Pr\u2081 = = Prn-1, satisfying property (2').\nFinally, for any permutation \u03c6 of [N]:\nd (P\u03c6(1), P\u03c6(2), ..., P\u03c6(N); \u03c3) = \u2211\u1d62=\u2081\u1d45 KL[P\u03c6(i) || P\u03c3(\u03c6(i))]\n= \u2211\u2c7c=\u2081\u1d45 KL[P\u2c7c || P\u03c3(j)] = d(P\u2081, P\u2082,..., P\u2099; \u03c3).\nThus, the Permutation Divergence satisfies the symmetry property (3).\nThe proof of property (2') demonstrates why cyclic permutations are used instead of general permutations: the one-cycle structure ensures that the divergence reaches its minimum when all distributions are identical.\nProposition 3. The sum of dissimilarity coefficients defined on the same set of distributions is itself\na dissimilarity coefficient.\nProof. The proofs of these properties for the sum of d.c.'s follow directly from the corresponding\nproperties of the individual coefficients. For brevity, these straightforward proofs are omitted here."}, {"title": "POSTERIOR DYNAMICS DURING TRAINING", "content": "The two regularization terms, Inter-View Translatability and Consensus Concentration, play distinct roles in the training process. These terms impact the arrangement and interaction of latent variables in the learned space, as illustrated in the following visualizations.\nIn Figure 8, we depict the impact of the regularization terms on a five-view sample with one missing view. In the single-view cell S\u2081, markers of the same shape represent the latent variables z corresponding to the l-th view, while the different colors indicate their source views, whether self-encoded or cross-transformed. Note that each set contains as many variables as there are observed views (four in this case), as they can only be encoded or transformed from available views. As this term diminishes, the variables within each Si cyclically converge, indicating that variables from different views can effectively transform into each other, thereby establishing inter-view correspondences. This process also enforce a form of soft consistency, as representations from different views are encouraged to approach each other after being transformed, rather than aligning directly. The learning of inter-view correspondences avoids collapsing into identity mappings because the reconstruction loss ensures that variables retain unique information specific to each view.\nThe Consensus Concentration term aims to ensure that consensus variables derived from different combinations remain consistent. Each \u03c9 in \u03a9 is obtained from a complete combination of all views. Over time, the regularization promotes closer alignment of these consensus variables, facilitating the aggregation of shared information across the views."}, {"title": "MISSING PATTERNS AND MASK GENERATION", "content": "We follow standard practices in incomplete multi-view learning, where missing-view masks are generated before training to inform the model of the missing patterns. Following the approach outlined in the public repository by Tang & Liu (2022), for a dataset with L views and a missing rate \u03b7, we randomly select \u03b7 \u00d7 len(dataset) samples to be incomplete. For each of these samples, we randomly remove between 1 and L \u2212 1 views, ensuring that every incomplete sample retains at least one view while missing at least one. The missing-view masks (e.g., \u201800101') are generated for the entire dataset before training and stored in a \"fingerprint\u201d file for each missing rate \u03b7. When the dataset is loaded, the corresponding masks are applied as long as \u03b7 is specified. This ensures consistent missing patterns across different models, enabling fair and reproducible evaluations."}, {"title": "PRE-COMPUTATION OF CYCLIC PERMUTATION INDICES AND BATCH PROCESSING", "content": "Given an index set A = {1, 2, . . ., n}, Sattolo's Algorithm can efficiently generate a cyclic permutation, as outlined in Algorithm 1. By leveraging stack operations, all possible cyclic permutations of the index set can be precomputed.\nFor incomplete multi-view datasets with fixed missing patterns, each sample has a distinct mask (e.g., '01101'), indicating the available views {2,3,5}. Precomputing the cyclic permutations for these masks reduces computational overhead during training. Treating missing views as fixed points (e.g., views 1 and 4 in this case) ensures that the permuted index sets maintain the same length (e.g., [1, 2, 3, 4, 5] \u2192 [1, 3, 5, 4, 2]), simplifying batch operations. These precomputed permutations are also stored in the \"fingerprint\" file for easy retrieval.\nDuring training, these precomputed permutation indices can be directly applied to arrays containing values for each view, enabling efficient rearrangement of data. For a dataset with L views, cyclic permutations are applied to the latent variables of each sample by permuting elements within each column of an L \u00d7 L matrix Z\u2080. As shown in Figure 5, the precomputed cyclic permutations are chosen, structured as an L \u00d7 L array and are used to reorder Z\u2080 into Z\u2081. Since these permutations are stored with the dataset, the transformation process can be efficiently executed in batches using indexing, allowing for fast and seamless operations during training."}, {"title": "OVERALL TRAINING PIPELINE OF OUR METHOD", "content": "The general pipeline of our method is outlined in Algorithm 2. The goal is to learn multiple encoders", "dataset": "n1. Self-view Encoding and Cross-view Transformation for Z0: We employ L encoders to obtain z(v) \u223c \ud835\udca9(\u03bc(x(v)), \u03a3(x(v))), v \u2208 {1,..."}]}