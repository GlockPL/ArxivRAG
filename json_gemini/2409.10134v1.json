{"title": "Advancing Towards a Marine Digital Twin Platform: Modeling the Mar Menor Coastal Lagoon Ecosystem in the South Western Mediterranean*", "authors": ["Yu Ye", "Aurora Gonz\u00e1lez-Vidal", "Alejandro Cisterna-Garc\u00eda", "Angel P\u00e9rez-Ruzafa", "Miguel A. Zamora Izquierdo", "Antonio F. Skarmeta"], "abstract": "Coastal marine ecosystems face mounting pressures from anthropogenic activities and climate change, necessitating advanced monitoring and modeling approaches for effective management. This paper pioneers the development of a Marine Digital Twin Platform aimed at modeling the Mar Menor Coastal Lagoon Ecosystem in the Region of Murcia. The platform leverages Artificial Intelligence to emulate complex hydrological and ecological models, facilitating the simulation of what-if scenarios to predict ecosystem responses to various stressors. We integrate diverse datasets from public sources to construct a comprehensive digital representation of the lagoon's dynamics. The platform's modular design enables real-time stakeholder engagement and informed decision-making in marine management. Our work contributes to the ongoing discourse on advancing marine science through innovative digital twin technologies.", "sections": [{"title": "1. Introduction", "content": "The intricate dynamics of coastal marine ecosystems present significant challenges for both monitoring and understanding their processes. Oceans are vital for sustaining life on Earth and they contribute substantially to global food sources, oxygen production, and carbon dioxide absorption (Riebesell et al., 2009). Marine environments suffer from numerous sources of stress, mostly from human activities in coastal areas, urban, agricultural, and industrial discharges, habitat destruction, introduction of invasive species, and oil spills, which interact synergistically with the consequences of climate change. In addition to classic pollutants, such as heavy metals or pesticides, with a long tradition in human activities such as mining, industry, or agriculture, new emerging pollutants are continually appearing, derived from drugs or cosmetics, whose effects on health are not always well known. A good example is that of microplastics (Ritchie and Roser, 2018). These pressures result in the alteration of ecological processes, eutrophication and dystrophic crises, anoxia, pH alterations, changes in community structure, the collapse of populations of some species and massive proliferation of others, and accumulation of toxic compounds through food webs. This impacts not only human health but also the habitability of coastal areas, biodiversity, the sustainability of resource exploitation, leading to important socio-economic consequences.\nGiven this scenario, the undersampling of marine environments, particularly in continental shelves and coastal areas, frustrates our comprehensive understanding of marine sciences (Davidson et al., 2019), since such complex systems require continuous monitoring of various indicators to detect or alert us to changes. Current observational deployments are often restricted to the ocean surface and a few measurable variables and there are limited tools to process the data and extract useful knowledge. This underscores the need for advanced modeling techniques to bridge gaps in our comprehension and to allow intelligent action-taking. But, more importantly, the mere detection of problems may not be sufficient since, on the one hand, the homeorhetic mechanisms of biological systems may mask such indicators until it is too late and, on the other hand, the speed of ecosystem deterioration is often greater than the human capacity to take corrective and management measures.\nMarine ecosystem models play a pivotal role in conceptualizing relationships among marine organisms and in the understanding of their impact on the environment (Ford et al., 2018). These models facilitate the calculation of intricate phenomena, offering valuable insights into ecosystem processes. However, challenges persist in the accuracy of unobserved model outputs due to inherent uncertainties and biases, limiting the reliability of predictions. Recognizing these limitations, the concept of Digital Twins of the Ocean (DTOs) emerges as a transformative approach to address the shortcomings of traditional models (Skakala et al., 2023; Tzachor et al., 2023).\nDTOs are digital representations (Flynn et al., 2022) tailored for real-time stakeholder needs and present an adaptive and learning framework that can evolve with new knowledge and observations (Ford et al., 2022). This innovation holds immense promise for stakeholders, including non-scientific backgrounds, allowing for informed decision-making in marine management.\nIn this context, our paper pioneers the creation of a modular DTO that seamlessly uses Artificial Intelligence (AI) and software engineering (especially web applications) to emulate complex hydrological and ecological models and propose what-if scenarios. This effort aims to merge technological advancements with marine sciences for a transversal approach to environmental management. Coastal lagoons, particularly productive and sensitive to climate change, serve as ideal ecosystems for studying these interactions.\nThe primary objective of our DTO is integrating advanced Artificial Intelligence algorithms and gathering the necessary data to emulate complex hydrological models and simulate hypothetical scenarios, enabling real-time stakeholder engagement and informed decision-making.\nPrecisely, the contributions towards this general goal of our work are:\n\u2022 Identification, fusion, and incorporation of open data sources related to the Mar Menor basin, including biochemical parameters and satellite images for improving their availability and usability.\n\u2022 Development of technologies that compose the DTO in all stages: data collection and fusion, AI model implementation and output generation (back-end) as well as a friendly user interface (front-end).\n\u2022 Design, implement and continuously integrate data into a FIWARE-based application to enable data interoperability.\n\u2022 Outlining potential scenarios addressing issues like jellyfish blooms, hypoxic crisis and water quality estimation highlighting the need of a common open data space.\nThe paper is organized as follows. Section 2 reviews related projects and existing work on marine-related Digital Twins. Section 3 presents the methodology for creating the Digital Twin, covering the architecture, data collection, fusion, and analysis. Section 4 provides a detailed description of the implemented AI models. Section 5 explores several perspectives for the developed Digital Twin, highlighting potential integrations with additional data. Finally, Section 6 concludes the paper and outlines future directions."}, {"title": "2. Related work", "content": "As an emerging and still evolving technology, DTs have experienced an extension from the use in the manufacturing and asset-heavy industries and have made it into complex fields such as urban planning, precision healthcare, and marine science (Br\u00f6nner et al., 2023).\nMost marine DT-related work can be divided into two types: tangible, which refers to real objects like power generators, ships, vessels, or buildings; and intangible, which involves natural processes such as natural phenomena at a particular location, global climate change, or even the entire Earth (Tzachor et al., 2023; Minerva et al., 2020). It is worth noting that the methodological aspects of a DT approach to the study of the Earth system or its sub-components have not yet been established (Pillai et al., 2022). However, knowledge can be gained from specific application cases through a variety of methods, which can be either physical/numerical models or ML based.\nAs tangible DT examples, (Su et al., 2023) implemented and tested a full-scale DT solution for an aquaculture net cage system. They developed a wireless sensor network (WSN) and included a set of the most commonly used sensors and technologies (e.g. metocean buoy, online metocean forecast, 4G modules, etc.) for real-time monitoring. They also used FhSim as a simulation framework to combine the different models such as fish behavior or marine environment. To address the problem of estimating the speed loss caused by the effect of fouling on the ship's hull and propeller, (Coraddu et al., 2019) proposed a two-step Data-Driven Model (DDM). The DT is built in the first step using the Deep Extreme Learning Machines (DELM) and leveraging the information collected from the on-board sensors. Then, in the second step, the same model is applied to estimate the speed loss of the ship and its drift. In the work of (Stadtmann et al., 2023), a DT of a floating offshore wind turbine is created. The implemented DT was categorized as standalone, descriptive, and predictive level (on a scale of 0 to 5: 0-standalone, 1-descriptive, 2-diagnostic, 3-predictive, 4-prescriptive, and 5-autonomous). The CAD modeling and texturing, Unity and Virtual Reality (VR) technologies were used to visualize the virtual turbine through an Oculus VR headset, together with the real sensor data recorded on the turbine. Finally, for predictive capability, Dense Neural Networks (DNN) and Long-Short Term Memory Neural Networks (LSTM) were used as ML approaches to predict turbine-specific parameters.\nOn the other hand, as intangible examples, (Duque and Brovelli, 2022) implemented a web application as a proof of concept of applicability of data integration in the DTs. It integrates Copernicus and WorldPop data to provide tools for analyzing and describing coastal interactions between ocean, land, and demographic variables on the Italian coasts, providing both visualization and analysis capabilities. Another proof of concept experiment was carried out by (Jiang et al., 2021) with the application called CoastalTwin, which uses a physics-informed ML technique called Fourier Neural Operator (FNO) to predict the sea surface height and it showed a great potential as a promising alternative compared to the commonly used Nucleus for European Modeling of the Ocean (NEMO) method, achieving 45 times of acceleration. (Ahmadi et al., 2024) developed a multi-regional terrain and elevation map by combining a digital twin model and the DL technique (U-net network) on the Florida coast. They scaled 5000 map segments worldwide, each with its main geographical features, and classified the terrain into seven different classes. The implemented DT can serve as both a physical and simulation model, and can also be used in environmental monitoring and urban planning.\nRegarding marine DT-related projects, on a larger scale, the Digital Twins of the Ocean (DITTO) project (https://ditto-oceandecade.org/digital-twins/) aims to create accurate representations of the worldwide marine assets and systems. It establishes a comprehensive digital framework that integrates marine data, satellite observations, modeling, AI, and high-performance computing, allowing users and partners to explore what-if questions based on shared data, models and knowledge. At the European level, an infrastructure built on existing resources and initiatives, called the European Digital Twin of the Ocean (EU DTO) (https://www.mercator-ocean.eu/en/digital-twin-ocean/), aims to model the ocean's multiple components, provide knowledge and understanding of its past and present, and generate reliable predictions of its future behavior. It includes other projects such as: Iliad (https://ocean-twin.eu/), which pilots local digital twins with the goal of eventually creating a unified virtual replica; the EDITO-Infra project (https://edito-infra.eu/), which integrates the Copernicus Marine Service, Copernicus Data and Information Access Services, and the European Marine Observation and Data Network; the EDITO-Model Lab project (https://edito-modellab.eu/), which develops the underlying models for the DT. At local scale, the SmartLagoon project (https://www.smartlagoon.eu/), related to Mar Menor and funded by the European Union's Horizon 2020 program, aims to develop tools for monitoring and analyzing the socio-environmental interrelationships of coastal lagoons and their ecosystems. Its goal is to raise awareness of environmental impacts at local and citywide levels. Lastly, the objectives of the national project in which this work is framed, Think-InAzul consist of points that are aligned with the generation of a digital twin:\n\u2022 Creation of new digital tools for the observation of the marine environment\n\u2022 Creation of an open platform to monitor and access marine data\n\u2022 Generation of a system, based on observations, data analysis, and numerical models, that allows mechanisms for early warning against risks and threats, resource management, land planning, and the management of the marine environment in its broadest sense\nThinkInAzul is part of a joint national research and innovation strategy to protect marine ecosystems against climate change and pollution and to address the challenges related to aquaculture, fishing, and tourism. The Region of Murcia acts as the national coordinator for the Supplementary R&D&I Plan in Marine Sciences."}, {"title": "2.2. Research gaps", "content": "Most DT-related studies often focus on a single use case, with models and data formats that are often not interoperable. This limits the ability to integrate different components into a cohesive system. Our DT is able to support multiple models, each with its own use case, and the implemented data management allows the stored data to be interoperable between different scenarios or even with other systems.\nCompared to the aforementioned projects, despite their extensive data coverage, most of them are interpolated and lack local accuracy. Also, the analysis of the Mar Menor Coastal Lagoon Ecosystem requires more data on the land and the coast, while these data are mainly focused on ocean variables. In addition, as mentioned above, the technology and scenario of DT is still under development, the use cases we present in this study are not currently implemented in other projects or publicly available."}, {"title": "3. Methodology", "content": "Figure 1 shows an overview of the use of the DT. The DT developed consists of a web-based application and built with a three-tier architecture, namely data, application and client or presentation tier (Helu et al., 2017). The data layer is the lowest layer in the architecture and its function includes collecting, organizing, and accessing data for use in other layers. The application layer is where the back-end of the DT is implemented. It provides an interface between the data layer and the presentation layer and implements the business logic of the application. Finally, the main function of the presentation layer, where the front-end is implemented, is to dynamically display the information stored in the data layer and interact with end users."}, {"title": "3.1. System architecture overview", "content": "Figure 2 shows the current architecture of the platform and the main components of each layer. It is important to note that all libraries and tools shown in the figure, and therefore used in our DTO, are open source.\nIn the front-end, the combination of HTML, CSS, and JavaScript is used for web page design, styling, and interactivity. The Bootstrap library\u00b9 is also used to achieve a responsive web design, i.e. to adapt to different device sizes. Plotly2, a browser-based graphing library has been used to plot interactive graphs. An important factor in any DT is the visualization of the physical twin. In our case, it corresponds to the entire Mar Menor Lagoon and its surrounding catchment basin. Therefore, both 2D and 3D maps are implemented as our virtual twin to represent the environment. For the 3D map, Cesium, an open platform for accurate 3D geospatial visualization, is chosen to display real-time data from various sources. On the other hand, Folium\u00b3 is the Python wrapper of Leaflet.js\u2074, one of the most widely used map visualization tools and has been chosen to plot an interactive 2D map that connects to Plotly to visualize the last seven days of data and also the output of ML models at each location. An overview of the developed DT Web App is shown in Figure 3.\nIn the back-end, the main core of this layer is Flask, a micro-web-framework with great capability and flexibility to create RESTful APIs using a simple routing/debugging system. Flask provides the interconnectivity for multiple microcomponents, ranging from scientific computing to data management to graphical interfaces (Bonney et al., 2022). In our DT, Flask is tasked with receiving the request sent from the front-end and processing it by calling Scikit-learn5 (for ML modeling tasks, e.g., loading the model and making the prediction) and Pandas (for data management, e.g., loading the requested data and transforming it into a specific format). Both components have access to the ML-models and the data-layer to get the necessary resources for the task they are assigned. To keep the latest real-time data, APScheduler7 is used and it will reload (hourly) the data stored in memory. Another important feature of Flask is that it allows multithreading, meaning that the platform can handle multiple client requests concurrently, thus improving the performance and scalability of the DT.\nThe data layer, as mentioned earlier, is responsible for collecting and storing the heterogeneous data types. The Cron Daemon, a Unix operating system job scheduler, is used to periodically launch data collection tasks that follow the Mediator-Wrapper strategy (see subsection 3.2). The developed models (see subsection 4.1 and subsection 4.2) are stored in ML models and are ready for use. However, since the data is constantly updated, the models are periodically retrained to take advantage of the newly collected data and to improve their performance.\nFinally, the DT is deployed on a VM running O.S. Ubuntu 22.04.2 LTS implemented on QEmu with 8 vCPU and 68 GB of RAM on a host with a Xeon Silver 4214R CPU @2.4GHz and is publicly accessible with the URL http://155.54.95.167/. The NGINX is selected as the HTTP and reverse proxy server to communicate with the client. It will serve static content directly from the front-end and will handle dynamic content by proxying the client's requests to the application server, which in this case is Gunicorn. Gunicorn is a Python Web Server Gateway Interface (WSGI) HTTP server designed to serve Python web applications. After receiving the request from NGINX, Gunicorn will translate it into a WSGI-compatible format and then call and execute the Python code (in Flask) and return the results when it is done. Both the front-end and back-end are packaged in Docker containers, and the Docker volumes provide access to the external data/models."}, {"title": "3.2. Data collection", "content": "In order to be able to simulate different what-if scenarios, where each of them can require large amounts and diverse types of data from multiple domains, the DT contains the implementation of an initial version of a data lake and follows the Mediator-Wrapper or Mediator-based architecture (Duque and Brovelli, 2022) to collect and store the heterogeneous types of data.\nThe data of the DT are mainly about the Mar Menor, located in the region of Murcia, Spain. All of them are publicly available. At the moment, it contains the following data from different sources and institutions:\n\u2022 Catchments sensors and piezometers data from the Automatic Hydrological Information System (SAIH) of the Segura River Hydrographic Basin (https://www.chsegura.es/es/cuenca/redes-de-control/saih/ivisor/).\n\u2022 Air Quality stations from Regional Ministry of Environment, Universities, Research and Mar Menor (https://sinqlair.carm.es/calidadaire/).\n\u2022 Air Quality Index (AQI) from the World Air Quality Index (WAQI) project (https://waqi.info/).\n\u2022 Buoy data from the SmartLagoon project (https://marmenorsensing.com/applications/eb0sVoYBcSC7WUER9weF).\n\u2022 Sea environmental variables from Mar Menor Scientific Data Server of Technical University of Cartagena (UPCT) (https://marmenor.upct.es/).\n\u2022 Conductivity, Temperature and Depth (CTD) stations of Murcia Agricultural and Environmental Research and Development Institute (IMIDA) (https://idearm.imida.es/cgi/siomctdmarmenor/).\n\u2022 Weather (real and prediction) data from the Spanish State Meteorological Agency (AEMET) (https://www.aemet.es/es/datos_abiertos/AEMET_OpenData).\nAlthough all the data are collected from public institutions, since there is no standard method to retrieve information from each source, each dataset is different in its collection and format. Therefore, several Python scripts are implemented to download and organize the received data, following the Mediator/Wrapper architecture.\nTo download the data, first, if the source entity has implemented an Application Programming Interface (API) to receive data, the process is simple. It is only necessary to set the parameters to request the needed data, such as the date range, the variables to download, and the localization information. However, if the request data is embedded in the source HTML file, web crawler techniques are used to extract the information of interest. This method involves a number of web technologies such as cookies, sessionID and regular expressions that adapt the structure of each web page. Finally, if the source offers only downloadable files, the solution is to retrieve the files (manually or via HTTP requests) and process them locally.\nIn our DT scenario, SINQLAIR, WAQI, SmartLagoon and CTD-IMIDA provide the corresponding API for data retrieval purposes, SAIH's records are collected using the web crawler method, and in SDC-UPCT, the datasets are in NetCDF format, retrieved via HTTP requests.\nOn the other hand, providing real-time or near real-time data is one of the most important functionalities of any DT. Most target sources fulfill this requirement by publishing their data in near real-time. Therefore, all implemented Python scripts are hosted in the server and executed according to each source's publishing schedule.\nOne aspect that must be considered in all platforms, particularly those that provide data in real time, is the level of validation and error detection that said data has (Gonz\u00e1lez-Vidal et al., 2022). In that sense, there are also two types of scripts used to store either temporary or historical data. The first runs hourly or daily to collect the last published data in the temporary files, while the second is executed weekly to add the last week's data to the historical files. The main reason for this design is that if the data is published in real-time, as many of the target sources have stated, it is usually invalidated at the moment of publication, meaning that it may contain errors due to sensor malfunction or internal calculation. In addition, to speed up the read and write operations of temporary files, they are stored in CSV format and handled with the append mode of the Pandas library, which does not need to read the entire file to add new entries. However, in order to make better use of storage space, the historical files are saved in Parquet format, which has the ability of data compression and encoding schemes with improved performance to handle complex data types (Belov et al., 2021). Therefore, temporary files contain real-time data with a date range of the last seven days and with the possibility of inaccurate observations, while historical files include all validated data."}, {"title": "3.3. Data interoperability", "content": "In a complex DT, it is often faced with large volumes of heterogeneous data sources that have their own attributes, attribute types and relationships (Conde et al., 2021). The structure of the data that is used in a DT needs to be understood, interpreted, and utilized across disparate systems or platforms, regardless of their underlying technologies or architectures. In that sense, we wanted to provide our DT with data interoperability. Data interoperability enables different components of a system to communicate, share, and integrate data effectively, without encountering compatibility issues or data format inconsistencies. To this end, we use the FIWARE open source platform, which aims to promote interoperability between different systems and services by providing a set of open standards, APIs and data models, and is a real reference option for the development of DTs in any domain (Conde et al., 2021).\nMore specifically, a data modeling implementation based on Next Generation Service Interfaces-Linked Data (NGSI-LD) using the SAIH and CTD-IMIDA datasets is performed as a proof of concept for future extension of the approach. NGSI-LD is specifically designed to improve data interoperability and enable seamless integration and interaction between digital twins and other systems by providing semantic interoperability, dynamic context management, and query subscription mechanisms amongst other functionality."}, {"title": "4. Models", "content": "In the context of a coastal ecosystem, identifying trends and accurately predicting variables across different domains in the short to medium term are crucial for providing valuable insights that inform and enhance environmental planning activities. To this end, an important functionality of our DT is to predict the next hours or day values for all variables available in each dataset.\nHowever, as explained in previous sections, data from various domains is collected to develop different what-if scenarios. Additionally, each domain typically includes data from multiple locations, with variables at each location potentially exhibiting distinct patterns. Consequently, applying the traditional Machine Learning (ML) train-test method to each variable is impractical in terms of efficiency and resource allocation.\nTo solve this problem, we have developed global models that provide N:M (multiple input, multiple output) features, i.e. modeling and predicting the future values of several time series at once. A multi-step forecast is performed using forecasters implemented in the Skforecast library (Amat Rodrigo and Escobar Ortiz, 2024), which is specialized to handle ML processes applied to time series. The global models can reduce the potential noise that each series might introduce by capturing the core patterns that govern the series. While potentially sacrificing some individual insights, this approach is computationally efficient, easy to maintain, and can yield more robust generalizations across time series.\nMore specifically, for each location of the SAIH catchments and SINQLAIR air stations, a global model is developed to make auto-regressive predictions for all available variables. The model-building process begins with a preliminary study to exclude variables with more than 50% missing values and use linear imputation to estimate the missing values. A function is created to set the weight of observations that have imputed values to zero during the training process so that these observations will not affect the performance of the model. The dataset is then divided into training (70%), validation (10%), and test (20%) sets according to its length. To find the most appropriate model for each location, a set of algorithms (LGBMRegressor, XGBoost, Catboost, and HistGradientBoostingRegressor) are tested together with Bayesian search (implemented in Optuna (Akiba et al., 2019)) to find the optimum hyperparameters. Finally, backtesting (a special type of cross-validation that evaluates the performance of a predictive model by applying it retrospectively to historical data) is used to select the model with the best performance, using Mean Absolute Error (MAE) and Coefficient of Variation of the Root Mean Square Error (CVRMSE) as metrics. The error unit of the former is the unit used for each variable, while the latter expresses the error in percent.\nThe same process is carried out for the SDC UPCT, but not for all the buoys, since the variation between them is very small. Therefore, the data from the buoy located in the center of Mar Menor (number six) are used to predict the variables most relevant to the what-if scenarios proposed in subsection 5.2, namely salinity, chlorophyll, turbidity, oxygen and transparency, all at 2 m depth.\nSince there are a large number of variables, for simplicity, a subset of them, which can represent the advantages and shortcomings of using global models, is shown in Table 2. As can be seen, CatBoost gave better results in the majority of cases of horizon between 1 and 24 hours and is therefore selected for use in the development of global models in these horizons. On the other hand, for horizons from 7 to 28 days, models using XGB had better performance in most of the cases, thus it will be used. It is also important to note that not all variables follow an auto-regressive pattern. This implies that if the output of a variable is not highly correlated with its previous values, e.g. streamflow or nitric oxide (NO), the global model will have a poor performance. Thus, these variables require further investigation, as in the case of the catchment streamflow variable presented in the next section. Figure 6 shows the results of the global model used for the catchment area (06A01-La Puebla) with a time horizon of 6 hours in our DT."}, {"title": "4.2. Run-off what-if scenario: an integrated use case", "content": "In this section, we show the results of the what-if scenarios that have been implemented within the context of our DT. By analysing several hypothetical situations, we aim to discern the underlying dynamics within the Mar Menor ecosystem. Our approach is twofold: we demonstrate a subset of scenarios already included in the DT, while also introducing open what-if scenarios derived from contextual domain expertise that are not targeted in present but will serve for future investigation and reference to scale our DT.\nFor streamflow prediction in Mar Menor we developed an enhanced model using the Long Short-Term Memory network (LSTM) framework provided by Keras (Chollet et al., 2015). Our optimization process involved experimenting with various LSTM configurations, ultimately identifying the most effective structure, which included: 1) an LSTM layer with 128 units and a $\\tanh$ activation function; 2) a dense layer with 64 units and a linear activation function; 3) another dense layer with 32 units and a ReLU activation function; and 4) a final dense layer with a single unit and a linear activation function. We employed a RobustScaler for feature scaling to manage statistics that are robust to outliers effectively. Given the constraints of our dataset, where negative streamflow values are not practical (streamflow values are inherently non-negative), the model occasionally predicted negative values. To remedy this, we implemented a simple function to adjust negative predictions to zero. For practical application considerations, piezometer data, which cannot be collected hourly, was excluded from our model. However, we continued to utilize streamflow data from seven locations and rainfall measurements from ten rain gauge stations proximate to the watercourse. We developed predictive models for streamflow series at two points along the Albuj\u00f3n watercourse: La Puebla (06A01) and Desembocadura (06A18). The models utilized data collected from March 8, 2021, to April 20, 2024. For model training and evaluation, we partitioned the data into three subsets: 70% was used for training, 10% for validation, and the remaining 20% for testing. In the testing dataset for both locations, we achieved a CVRMSE of less than 11.50 for a 1-hour time horizon, 37.59 for a 6-hour time horizon, and 44.75 for a 24-hour time horizon. Ultimately, for operational deployment, we utilized the respective scalers and models saved from each experiment to generate hourly predictions for each time horizon. The output of the model implemented for 06A01-La Puebla is shown in Figure 7.\nIn this what-if scenario, we hypothesized that utilizing precipitation forecast information from the Spanish State Meteorological Agency (AEMET) could enhance the accuracy of streamflow predictions in the watercourse. Precipitation is the primary factor influencing rainfall measurements at the rain gauge stations integrated into our models. Consequently, we incorporated precipitation, temperature, and humidity forecasts provided by AEMET from the seven nearest weather stations for the period from August 26, 2023, to June 30, 2024. We used the previous LSTM model including these AEMET forecasts to predict streamflow in the same two points. Therefore, with the inclusion of the AEMET forecast in this run-off what-if scenario we take advantage of the national state weather forecast to enhance the prediction. In the testing dataset for both locations, we achieved a CVRMSE of less than 8.05 for a 1-hour time horizon, 26.34 for a 6-hour time horizon, and 38.86 for a 24-hour time horizon."}, {"title": "5. Mid-term prospects of the digital twin", "content": "The large amount of goods and services provided by coastal lagoons (fisheries, pollutant retention, CO2 sequestration, bathing water quality, biodiversity) means that any imbalance has important socio-economic consequences and requires urgent management decisions (Newton et al., 2018; Velasco et al., 2018; P\u00e9rez-Ruzafa et al., 2020). One of the most important processes that directly or indirectly affect most of these goods and services is eutrophication, with its multiple consequences, including the proliferation of organisms, both as direct or secondary response to the inputs of nutrients or organic matter (macroalgae, phytoplankton (which can sometimes be toxic), jellyfish, and the occurrence of hypoxia events with the consequent mortality of organisms. The triggers for this process are closely related to activities and processes in the watershed that lead to the entry of water of different salinity than that of the lagoon and excessive amounts of nutrients. The complex interaction of climatic, hydrographic, oceanographic and biological factors and the different spatio-temporal scales at which they operate make the monitoring of all the variables involved and the modeling of the processes involved difficult to comprehensively cover. It is therefore important that any digital twin, to be accurate, and sufficiently precise, considers at least the key variables and processes to address the various management needs and anticipate key problems.\nHow much freshwater can a coastal lagoon receive without significantly altering its functioning? What amount of nutrients trigger a dystrophic crisis? What is the difference between a continuous input and a massive discharge? When can torrential rain cause stratification of the water column? How long will it take for the system to recover after a DANA event? All those are questions that need further data collection and in the following subsections, we will review the potential additions to our Digital Twin in terms of data in order to solve different scenarios that are also proposed."}, {"title": "5.1. Data integration", "content": "In Subsection 3.2, we have outlined the existing data sources integrated into the Digital Twin. Additionally, we have identified several data types that could enhance this information, enabling the creation of more simulation scenarios within the Mar Menor Digital Twin.\nOn the coastal side:\n\u2022 Biochemical parameters of water in various streamflows: phosphates (mg PO\u2081/l), nitrates (mg NO3/1), and conductivity (\u00b5S/cm).\n\u2022 Satellite images can be utilized to compute indexes that describe and monitor land use and health (Mendoza-Bernal et al., 2024), aiding in the evaluation of agriculture's impact on nearby streamflows.\n\u2022 Social data and community perspectives, including tourism rates. Including such information could facilitate informed decision-making and policy development that balances environmental conservation with economic development and tourism promotion.\nWithin the lagoon:\n\u2022 Satellite images can generate maps indicating water salinity and temperature, chlorophyll concentration, turbidity, and other parameters using software like MAGO 10.\n\u2022 Manual sampling of physicochemical or biological variables such as pH, O2, suspended solids, chl a, silicates, ammonia, nitrates, nitrites, and phosphates allows for the assessment of water quality, nutrient availability, and primary productivity to calibrate remote sensing data or validate models.\n\u2022 Video data from boats and underwater robots can be processed to obtain detailed mappings and samplings of the seabed and algae presence, supporting underwater cartography, marine habitat assessment, and the detection of marine fauna such as fish and jellyfish, as well as environmental impact analysis.\nHydrological and hydrodynamic 3D models can provide essential insights into water dynamics for the watershed and lagoon relationship. Their outcomes can be valuable inputs for our Al-driven Digital Twin. These model outcomes can promote real-time decision-making, enabling proactive management of water resources, flood risk assessment, and optimization of water-related infrastructure and operations. At the same time, leveraging AI techniques within the DT enhances the accuracy and efficiency of hydrological modeling, allowing for adaptive strategies and improved resilience in the face of changing hydrological conditions."}, {"title": "5.2. Proposed what-if scenarios", "content": "Will there be a jellyfish bloom in the next season, and how could changes in various factors affect it? For instance, how do seasonal thermal anomalies or prolonged heatwaves impact the life cycle of jellyfish species? Higher water temperatures in the lagoon could increase the likelihood of a bloom under certain conditions. Jellyfish proliferation in the Mar Menor often occurs in response to nutrient influx, exerting top-down control of the food web and maintaining water quality (P\u00e9rez-Ruzafa et al., 2002; Fern\u00e1ndez-Al\u00edas et al., 2020). However, predicting their dynamics remains challenging due to knowledge gaps in life cycle regulation (Fern\u00e1ndez-Al\u00edas et al., 2024). Jellyfish blooms are influenced by environmental factors such as water temperature, which triggers phase transitions in many species (Fern\u00e1ndez-Al\u00edas et al., 2020, 2023), salinity changes, and nutrient levels that promote algal growth-a key food source. Elevated chlorophyll-a and low dissolved oxygen levels can signal bloom conditions, while ocean currents and wind patterns influence jellyfish transport into lagoons. Predator-prey dynamics, food availability, and predation pressure also play roles. Integrating these factors into predictive models within a Digital Twin framework enhances bloom management and environmental monitoring.\nAs shown in Table 2, the predictions for several of the mentioned key variables are integrated into the DT for forecasting horizons of 1 to 24 hours and from 1 to 4 weeks. Temperature predictions achieve a CVRMSE ranging from 4.7 to 14.2% (1-24h), while wind speed predictions, which aid in understanding ocean currents, have a CVRMSE between 30% and 60% (1-24h), aligning with state-of-the-art wind estimation"}]}