{"title": "LSSF-Net: Lightweight Segmentation with Self-Awareness, Spatial Attention, and Focal Modulation", "authors": ["Hamza Farooq", "Zuhair Zafar", "Ahsan Saadat", "Tariq M Khan", "Shahzaib Iqbal", "Imran Razzak"], "abstract": "Accurate segmentation of skin lesions within dermoscopic images plays a crucial role in the timely identification of skin cancer for computer-aided diagnosis on mobile platforms. However, varying shapes of the lesions, lack of defined edges, and the presence of obstructions such as hair strands and marker colors make this challenge more complex. Additionally, skin lesions often exhibit subtle variations in texture and color that are difficult to differentiate from surrounding healthy skin, necessitating models that can capture both fine-grained details and broader contextual information. Currently, melanoma segmentation models are commonly based on fully connected networks and U-Nets. However, these models often struggle with capturing the complex and varied characteristics of skin lesions, such as the presence of indistinct boundaries and diverse lesion appearances, which can lead to suboptimal segmentation performance.To address these challenges, we propose a novel lightweight network specifically designed for skin lesion segmentation utilizing mobile devices, featuring a minimal number of learnable parameters (only 0.8 million). This network comprises an encoder-decoder architecture that incorporates conformer-based focal modulation attention, self-aware local and global spatial attention, and split channel-shuffle. The efficacy of our model has been evaluated on four well-established benchmark datasets for skin lesion segmentation: ISIC 2016, ISIC 2017, ISIC 2018, and PH2. Empirical findings substantiate its state-of-the-art performance, notably reflected in a high Jaccard index.", "sections": [{"title": "1. Introduction", "content": "In an era marked by an ever-growing concern for public health, the spectre of skin cancer emerges as a subject of paramount importance, demanding our attention and understanding. Medical images, which play an important role in the process of diagnosis and treatment by physicians [1, 2, 3, 4, 5], have become particularly vital for current vision tasks on medical images, highlighting the critical role of accurate skin lesion segmentation. Among the myriad forms of skin cancer, melanoma emerges as the most formidable adversary, with the potential to be life-threatening. The linchpin in the battle against this risk is early detection, which is proven to be a critical factor in ensuring effective treatment and ultimately the survival of patients. It is abundantly clear that the sooner skin lesions are pinpointed, the greater the opportunity for patients to receive precisely tailored care, markedly improving their prospects of a successful recovery. Melanoma, in particular, presents itself through pigmented lesions that grace the surface of the skin, making it a prime candidate for early identification, thanks to the intelligent discernment of healthcare professionals. However, the labyrinth of skin cancer diagnosis remains a formidable challenge for dermatologists, primarily due to the immense diversity of skin lesions and the complicated task of distinguishing between benign and malignant growths.\nIn recent times, deep learning, especially harnessing the powerful features extraction capabilities of convolutional neural networks (CNN) [6, 7, 8, 9, 10], has made significant strides in the domain of medical image segmentation [4, 7, 11, 12, 3, 13, 14, 15, 16, 17]. This development has led to substantial improvements in the precision of medical image segmentation tasks. The CNN framework, which consists of convolutional and down-sampling layers, operates on the principle that lower convolutional layers offer a more localised perspective and finer location information, while higher convolutional layers provide broader contextual insight into the entire image [18], essential for segmentation tasks. In light of these advances, numerous models based on the full convolutional network (FCN) have been introduced to improve image segmentation [19]. In particular, the structure of the encoding and decoding network, as epitomised by U-Net [20, 21], mitigates the loss of fine-grained details caused by multiple downsampling steps by incorporating skip connections between the encoder and the decoder, thus amplifying the performance of the network. This underscores the effectiveness of the encode-decode network architecture. Subsequently, various networks following U-shaped structures, including Res-UNet [22] and Attention R2U-Net [23], were proposed. However, these models still faced the challenge of effectively extracting and using multiscale contextual features within a single stage. This limitation was particularly relevant in the realm of medical images, where the target regions often closely resembled their surroundings, necessitating the consideration of broader contextual information to avoid ambiguous decisions.\nTo address this, researchers have devised methodologies to incorporate multiscale information, such as PSPNet [24], Pool-Net [25], DeepLabV3 [26], and CE-Net [27]. These approaches focus primarily on processing high-level feature information while downplaying location-based detail information present in low-level feature information. Although CNN-based methods excel in feature extraction, they tend to fall short of capturing long-distance dependencies due to the inherent limitations of convolution operations [28]. Consequently, these methods often struggle with target areas that exhibit substantial variations in texture, size, and shape.\nIn response, some researchers have introduced attention mechanisms into CNNs to overcome this limitation [29]. Furthermore, the successful integration of Transformers into computer vision has opened new avenues [30]. Transformers operate on a sequence-to-sequence prediction architecture, circumventing the need for convolution operators and relying solely on self-attentive mechanisms to extract information about image characteristics, allowing the establishment of effective long-range dependencies.\nTransformers have consistently demonstrated their ability to match or surpass state-of-the-art performance in various vision tasks. These models excel in capturing global context, but their effectiveness in capturing fine-grained details, especially in the case of medical images, is limited. They lack built-in spatial bias when it comes to modelling local information. Furthermore, transformer-based network structures are highly dependent on large datasets for optimal performance [31]. Here, the CNN architecture proves to be a valuable counterpart, effectively compensating for these limitations.\nRecent research has explored the fusion of CNNs with Transformers for medical image segmentation. Models such as TransUNet [32] and subsequent studies [33, 34] have used CNNS as the foundational network, and Transformers facilitate long-range dependencies on high-level features. However, these approaches often overlook the valuable spatial information present in shallow networks, concentrating on context modelling at a single scale, disregarding cross-scale dependencies and consistency. Some scholars argue that employing just one or two layers of Transformers [35] fails to combine convolutional representations that depend on CNNs for long-distance relationships.\nThis paper introduces an innovative lightweight network structure, termed LSSF-Net, specifically designed for the segmentation of skin lesions and the analysis of medical images within computer-aided diagnosis (CAD) systems. The proposed model builds on the well-established encoding-decoding network architecture, specifically using the lightweight T-Net-based model [36], which is known for its efficiency and effectiveness in medical image segmentation. Building on this foundation, our LSSF-Net incorporates several key enhancements to significantly improve feature extraction. These enhancements include a novel booster architecture, self-aware local and global spatial attention (SAB), normalised focal modulation-based skip connections (CFMA) and a split channel shuffle mechanism (SCS). Together, these innovations improve the model's ability to capture fine-grained details and global context, effectively addressing the challenges posed by the complex nature of medical images. The LSSF-Net is designed to deliver high accuracy and efficiency while maintaining a lightweight structure, making it highly suitable for deployment on mobile devices with limited computational power. This work represents a significant advancement in the field by offering a solution that balances top-tier performance with resource efficiency, providing an effective and accessible tool for medical image analysis in resource-constrained environments.\nThe backbone of the introduced LSSF-Net consists of two parallel branches of Convolutional Neural Networks (CNNs) and a booster architecture. CNNs focus on extracting multiscale feature information from the original input image, while the Booster concurrently models global contextual information to establish long-range dependencies. Recognising the computational cost associated with high-level semantic features, the model strategically maximises the retention of location information within low-level semantic features, as they contribute less to network performance. This thoughtful consideration aims to optimise computational efficiency without compromising overall segmentation quality [36].\nFor the decoding component, the same encoder structure is employed, and a Conformer-based Focal Modulation Attention (CFMA) is introduced as a skip connection from the encoder booster to the decoder. This addition enhances the acquisition of detailed global and local feature information during the decoding phase. Furthermore, to intensify interconnections between decoder blocks, facilitating dense links that improve feature preservation during the upsampling process, transformer-based attention (TA) is employed at the bottleneck of feature enhancement.\nThe main contributions of this work can be summarised as follows.\n1.  Novel Architecture: The proposed medical segmentation model introduces a novel architecture that features a parallel booster encoder and decoder model. This design facilitates the extraction of all feature sets and improves the segmentation capabilities.\n2.  Enhanced Feature Information: To obtain more detailed global and local feature information, focal modulation is coupled with conformer attention at the skip connection. This modification aims to improve the model's ability to capture intricate details and contextual information.\n3.  Dense Interconnections: The model intensifies the interconnections between decoder blocks, establishing dense links to facilitate the preservation of improved features during the crucial up-sampling process. This contributes to maintaining the integrity of features across different scales.\n4.  Transformer-Based Attention: To improve features at the bottleneck, transformer-based attention is strategically used. This, combined with special enhancements to local-global characteristics, ensures that essential information"}, {"title": "2. Literature Review", "content": "In the modern world, deep learning-based methods demonstrate better performance in the realm of medical segmentation, particularly in tasks such as segmentation of skin lesions [37]. These methods automatically extract features from the dataset and exhibit greater robustness compared to conventional hand-crafted feature extraction techniques. Ever since the introduction of UNet [20], its encoder-decoder architecture has emerged as the dominant method in medical segmentation. UNet efficiently incorporates basic feature information by establishing a direct connection between the encoder and the decoder. According to a survey [38], 87.2\n2.1. UNet based Segmentation\nIn the modern era of medical image analysis, deep learning-based methods have showcased remarkable performance, particularly in tasks such as segmentation of skin lesions [37]. Among these methods, UNet and its variants have emerged as dominant players [20] shown in the figure. UNet adopts an encoder-decoder architecture with skip connections, enabling efficient feature extraction and preservation of detailed information. Over time, several enhancements have been proposed to the original UNet architecture, each with the aim of improving segmentation accuracy and robustness. For example, Res-UNet [22] integrates residual structures in both the encoding and decoding stages, improving the retention of detailed information. UNet++ [39] takes a different approach by incorporating dense connections of residual structures, facilitating the accumulation of multiscale feature information. Attention mechanisms, widely successful in natural image processing, have found increasing application in medical segmentation tasks, yielding satisfactory results. Notable approaches include Attention R2U-Net [23], which combines residual and recurrent networks with attention gates to improve focus, and MCGUNet [19], incorporating SE modules and bidirectional ConvLSTM in skip connections for dynamic feature adjustment.\n2.2. Attention Mechanisms in Medical Image Segmentation\nResearchers have proposed innovative techniques to refine skip connection feature maps, leveraging attention mechanisms to improve segmentation performance. One such approach involves the inclusion of a spatial enhancement module within skip connections, which facilitates the representation of crucial spatial details for semantic segmentation. By integrating this module, the network effectively captures and leverages spatial information, leading to better segmentation performance. The Attention U-Net architecture [40] represents a significant advancement in this domain, incorporating attention gates within skip connections to address semantic ambiguity between encoder and decoder layers. Using attention gates, the model can selectively emphasise certain features of the encoder, providing better guidance and focus during the decoding process. This enables the model to capture relevant information more effectively, ultimately improving the results of the segmentation.\n2.3. Transformer Based Segmentation\nThe transformative impact of Vision Transformers (ViT), as introduced by [30], marked a significant milestone in the field of computer vision by bringing transformers, originally designed for sequential data processing, into the realm of visual tasks. ViT demonstrated remarkable performance, leveraging the transformer's capacity to capture global dependencies within images. Building upon ViT's success, subsequent advancements in vision tasks have blossomed, inspired by its pioneering approach. For instance, DeiT [41] explored efficient training strategies tailored to ViT architectures, enhancing scalability and performance. PVT (Pyramid Vision Transformer) [42] introduced a pyramid transformer with Shifted Relative Attention (SRA) mechanisms, reducing computational complexity while preserving effectiveness. The Swin Transformer [43], represents another notable stride in hierarchical vision transformers. Its innovative window-based mechanism enhances feature locality, addressing limitations observed in previous transformer architectures. Moreover, transformers have found applications in various specific tasks within computer vision. SETR (Semantic Segmentation Transformer) leverages transformers for semantic segmentation, with ViT serving as a backbone architecture. SegFormer, introduced by Xie et al. [44], offers a straightforward and efficient design for semantic segmentation, powered by transformer architectures. Furthermore, Uformer, as proposed by Wang et al. [45], introduces a general U-shaped transformer architecture tailored for image restoration tasks, showcasing the versatility of transformer-based approaches across a wide range of applications within computer vision. These developments underscore the transformative potential of transformers in reshaping the landscape of computer vision tasks, offering novel solutions and insights into addressing complex visual challenges. As researchers continue to innovate and refine transformer-based architectures, the future holds promising prospects for further advancements in visual understanding and processing.\n2.4. Hybrid Transformers and UNet-based Segmentation\nWith the rise of Transformers as a powerful tool in computer vision, their integration into medical segmentation has attracted significant attention from researchers, showing promising results. In particular, TransUNet [32], is a trailblazer in incorporating Transformers into medical segmentation tasks. This pioneering methodology merges the UNet encoder with Transformer architecture, diverging from traditional image-based input methods by operating on high-level features. The innovative fusion of UNet and Transformers in TransUNet marks a"}, {"title": "3. Proposed Methodology", "content": "In this section, we will briefly discuss the architecture of the proposed LSSF-Net. Fig/ 1 presents the block diagram of the proposed model, which consists of four encoder-decoder blocks, conformer-based focal modulation attention (CFMA) blocks in skip connections, self-attention block (SAB) and global spatial attention (GSA) blocks in the bottleneck layer of the proposed LSSF-Net. Details for each component are provided in\nthe following subsections.\n3.1. Model Architecture\nIn the proposed implementation, we have employed four encoder-decoder blocks. Let $I_{n\\times n}$ be the $n\\times n$ convolution operation $f^{n\\times n}$ followed by batch normalisation $(\\beta_\\eta)$ and ReLU $(R)$ operations for any given input $(I_n)$ as defined by (Eq. 1).\n$I_{n\\times n} = R \\left(f^{n\\times n}(I_n)\\right) \\qquad(1)$\nThe initial skip connection $(s_o)$ is computed by applying the $1\\times 1$ operation to the input of the network $(X_{in})$ as shown in (Eq. 2).\n$S_o = f^{1\\times 1}(X_{in})\\qquad(2)$\nSimilarly, the output of the initial encoder block denoted by $(E_o)$ is computed as (Eq. 3).\n$E_o = mp \\left(f^{3\\times 3} \\left(f^{3\\times 3} \\left(S_o\\right)\\right)\\right) \\qquad(3)$\nwhere $(mp)$ is the maxpooling operation. The output of the encoder block $k^{th}$ $(E_k)$ is computed by (Eq. 4).\n$E_k = mp \\left[R\\left{\\beta_n \\left(f^{3\\times 3}\\left(\\beta_n\\left(f^{3\\times 3}\\left(S_k\\right)\\right)\\right)\\right)+f^{3\\times 3}\\left(f^{3\\times 3}\\left(f^{3\\times 3}\\left(E_{k-1}\\right)\\right)\\right)\\right}\\right]\\qquad(4)$\nwhere $(s_k)$ is the $k^{th}$ skip connection and is computed as given in (Eq. 5).\n$S_k = f^{3\\times 3}(E_{k-1}) \\qquad(5)$\nOnce the information is extracted by the encoder block, it is further refined by two consecutive attention blocks, named Self-Attention Block (SAB), to capture the contextual information from relative positions, followed by a Global Spatial Attention (GSA) block which is responsible for enhancing the local contextual information from a broader view through aggregating with global spatial information. In addition, we implemented a technique that involves channel splitting and shuffling to enhance the capabilities and efficiency of the LSSF-Net model. Channel splitting enables simultaneous processing of distinct channel subsets, promoting parallelisation. Concurrently, the technique of channel shuffling stimulates inter-channel interaction, thereby improving the overall information flow. Once the extracted feature information is further enhanced and refined, it is given to the decoder stage to reconstruct the spatial feature maps. Let $(D_o)$ be the input given to the $k^{th}$ decoder block computed by (Eq. 6).\n$D_o = GSA \\left(E_k\\right) \\oplus SAB \\left(E_k\\right)\\qquad(6)$\nwhere $\\oplus$ is the concatenation operation. To fuse the extracted feature information at the decoder stage, we have employed a conformer-based Focal Modulation Attention (CFMA) on the skip connections and then added this information by applying the $(f^{3\\times 3})$ operation on the input coming from the $k^{th}$ decoder block and computed as (Eq. 7).\n$I_k = CFMA\\left(S_k\\right) + f^{3\\times 3}\\left(up(D_{k-1})\\right)\\qquad(7)$\nwhere $up$ is the upsampling operation that increases the spatial dimensions of the feature maps. The output of the $k^{th}$ decoder block is computed using (Eq. 8).\n$D_k = R \\left[f^{3\\times 3} \\left(f^{3\\times 3}\\left(f^{3\\times 3}\\left(up \\left(D_{k-1}\\right)\\right)\\right)\\right)+\\beta_n \\left(f^{3\\times 3}\\left(\\beta_n \\left(f^{3\\times 3}\\left(I_k\\right)\\right)\\right)\\right)\\right]\\qquad(8)$\nThe output of the model $(X_{out})$ is computed by applying the $f^{3\\times 3}$ operation followed by the $(f^{1\\times 1})$ convolution and the sigmoid $(\\sigma)$ operation as shown in (Eq. 9).\n$X_{out} = \\sigma\\left(f^{1\\times 1}(f^{3\\times 3}(I_k))\\right)\\qquad(9)$\nThe final binary predicted mask of size 256\u00d7256 is obtained by employing the dice pixel classification layer on the model output.\n3.2. Conformer-based Focal Modulation Block\nThe conformer-based focal modulation block (CFMA) is introduced in the skip connections of the proposed LSSF-Net to further capture multiscale global semantic features, as shown in Fig. 2. The CFMA block takes the input (Cin) from the encoder block and applies the layer normalisation (LN) operation, followed by the 3 \u00d7 3 convolution operation $(f^{3\\times 3})$ and the focal modulation block (FMB) and adds the (In) with it as shown in Eq.10.\n$C_1 = I_n + LN(f^{3\\times 3}(FMB(Cin))) \\qquad(10)$\nThe FMB is a key component of the CFMA block and is designed to produce different scales of receptive fields in an adaptive manner. This is achieved by employing a contextual aggregation block to capture information at various scales, enabling the network to gather rich semantic information from the input data. The output (Cout) of CFMA is computed by applying the multilayer perception (MLP) of the channel to (C1) as shown in the equation. 11.\n$C_{out} = C_1 + MLP(C_1)\\qquad(11)$\nIncorporating residual connections into the CMFA is essential to prevent the vanishing gradient issue during training. These connections enable gradients to pass directly through the block, enhancing the integration of more complex features across various scales. This approach enhances gradient flow and aids in training deeper networks, thereby simplifying the process of learning valuable data representations.\n3.3. Self-aware Attention Block\nSelf-aware Attention Block (SAB) is a type of multihead attention that can learn self-correlation but lacks the ability to learn spatial information; a commonly used approach in academic work is to pass the feature map to a position encoding block and then input it into the multihead attention block, as shown in algorithm 2. The input feature map Fin is then embedded in three matrices $Q \\in R^{(hxw)xc}$, $K\\in R^{c\\times(hxw)}$, $V \\in R^{cx(hxw)}$,\n$Q = WQ.Fin\\qquad(12)$\n$K = WK.Fin\\qquad(13)$\n$V = Wv.Fin\\qquad(14)$\nwhere $W_Q$, $W_K$, $W_V$ are three embedding functions for different linear projections. The scaling of the operation of the dot product with Softmax normalisation between Q and K gives $S \\in R^{cxc}$, which represents the similarity between the channels in Q and others. To derive the aggregated values weighted by attention weights, the contextual attention map Set is applied to the value matrix V. This process can be expressed through the multi-head attention mechanism, formulated as follows:\n$A_{tsa}(Q,K,V) = Softmax(\\frac{QK}{\\sqrt{d_k}})V\\qquad(15)$\nFinally, the $A_{tsa} \\in R^{c\\times (hxw)}$ is reshaped to $R^{h\\times w \\times c}$, which is as the same as the input shape.\n3.4. Global Spatial Attention\nGlobal spatial attention (GSA) is used to capture information on global position dependencies, as shown in algorithm 3. The input feature map Fin \u2208 Rh\u00d7w\u00d7c is first embedded in $F_c \\in R^{h\\times w \\times c}$ and $F_{c'} \\in R^{h\\times w \\times c'}$ where $c' = c/2$. The reshape $F_c \\in R^{hxw} \\times c'$ to $F_f \\in R^{(hxw) \\times c'}$ and F_c \u2208 Rcx(hxw), respectively, the scaled dot product of $F_f$ and $F_e$ then passes to a Softmax normalisation layer, the output map $S \\in R^{(hxw) \\times (hxw)}$ indicates spatial similarity, where Si, j represents the correlations between position ith and position jth. The multi-head attention mechanism can be represented as\n$A_{gsa} = Softmax(F_c F_c^T)F_c = \\sum f_i f_j f_c \\qquad(16)$\n3.5. Split Chanel-Shuffle\nChannel Shuffle is a technique that improves the flow of information across feature channels in a convolutional neural (CN) network. In group convolution, where input data from different groups is processed separately, the input and output channels are typically isolated. To overcome this, Channel Shuffle rearranges the channels by dividing them into subgroups. These subgroups are then mixed and fed into different groups in the next layer, ensuring that all channels can interact and share information effectively. This enhances the network's ability to learn from diverse features.\nThis process is carried out efficiently and seamlessly using a channel shuffle operation. A convolutional neural layer with g groups and n output channels, the output channels are first reshaped into dimensions of (g,n/g), then transposed, and finally flattened back into a single dimension to serve as input for the next layer. Additionally, incorporating a split operation can make the model lighter by dividing the feature maps into smaller parts for more efficient processing. Split Channel Shuffle (SCS) is also differentiable and model-lightening, enabling its integration into network structures for end-to-end training.\n$Output \\in R^{H\\times W \\times n} \\rightarrow R^{H\\times W \\times g \\times 1}\\qquad(17)$\n$Transpose(R^{H\\times W \\times g \\times 1}) \\rightarrow R^{H\\times W \\times 1 \\times g}\\qquad(18)$\n$Flatten(R^{H\\times W \\times 1 \\times g}) \\rightarrow R^{H\\times W \\times n}\\qquad(19)$"}, {"title": "4. Experiments and Results", "content": "In this section", "datasets": "three from the International Skin Imaging Collaboration (ISIC) archive and one from the PH2 dataset. Additionally", "2016": "The ISIC 2016 [46", "2017": "The ISIC 2017 [47", "2018": "The ISIC 2018 [52", "48": "dataset comprises 2594 dermoscopic images accompanied by their corresponding ground truth masks", "testing.\nPH2": "The PH2 [49", "masks.\nDDTI": "The DDTI dataset [50", "leaderboard": "accuracy", "53": "that \u03b2\u2081 = 0.90"}, {"48": "ISIC 2017 [47", "46": "and PH2 [49", "images": "BUSI [51", "50": "for segmentation of thyroid nodules. This generalisation shows the adaptability of the proposed LSSF-Net to other medical image segmentation modalities.\n4.6.1. Performance Comparisons on the ISIC 2018 dataset\nWe compare the proposed LSSF-Net with 13 other cutting-edge methods in the ISIC 2018 dataset to determine how well our proposed LSSF-Net works. U-Net [20", "54": "DAGAN [55", "56": "FAT-Net [57", "58": "FTN Network [59", "60": "DCSAU-Net [61", "62": "Ms RED [63", "64": "and ARU-GD [65", "20": "DAGAN [55", "57": "Ms RED [63"}, {"56": "BCDU-Net [54", "66": "AS-Net [60", "65": "Swin-Unet [5"}]}