{"title": "Survey on Emotion Recognition through Posture Detection\nand the possibility of its application in Virtual Reality", "authors": ["Leina Elansary", "Zaki Taha", "Walaa Gad"], "abstract": "A survey is presented focused on using pose estimation techniques in Emotional recognition using\nvarious technologies normal cameras, and depth cameras for real-time, and the potential use of VR\nand inputs including images, videos, and 3-dimensional poses described in vector space. We\ndiscussed 19 research papers collected from selected journals and databases highlighting their\nmethodology, classification algorithm, and the used datasets that relate to emotion recognition and\npose estimation. A benchmark has been made according to their accuracy as it was the most\ncommon performance measurement metric used. We concluded that the multimodal Approaches\noverall made the best accuracy and then we mentioned futuristic concerns that can improve the\ndevelopment of this research topic.", "sections": [{"title": "Introduction", "content": "Emotion recognition is one of the main vital tasks essential for having an intelligent system or\napplication. Dealing with humans requires understanding their own emotions so that the human\nfeels comfortable and the communication becomes more spontaneous which reflects on the\nefficiency of the service provided by the system/application. Emotions can be measured from\nmultiple modalities like reading facial expressions, gesture detection, static posture, movement\nbehavior, vocal tones, and text. When interacting with another human, you might know his\ncurrent emotions from only seeing his face and sometimes the eyes can do the trick, or from his\nvocal tone, his posture - the way he is standing- or from the pattern of his movements, the\ngestures he is making or the context of his words whether those words are said or written - you\ncan read an article and still visualize the emotions the writer has been through- or you can\ncombine two or more modalities together which increases the efficiency of the human's\npredictions. Computer models are being trained to recognize the above models far above is the\nphysical measurement which may include using sensors and actuators to measure physiological\npatterns that are hard for the computer to measure like measuring the heart rate, body\ntemperature, and skin sensitivity(Picard, R.W. and Vyzas, E. and Healey, J., n.d.). Those extra\nmodalities shall prepare the computer to be able to measure emotions accurately even more than\nhumans, which is not currently reached. We will discuss the challenges being faced in this field\nand how some papers overcome those challenges. Some modalities can provide reliable\nmeasurements on their own or they may be used only to enhance the recognition of another"}, {"title": "Emotion Recognition:", "content": "Emotion recognition has been a hot research topic since Rosalind Picard introduced the term\nAffective Computing in her book (Picard, n.d.) arguing that Artificial intelligence would not be\nable to interact effectively with humans or make accurate decisions unless they learn how to read\nthe user's emotions. The AI systems must be able to detect the emotions of the humans they are\ninteracting with and they shall be able not only to detect but to understand those emotions. we all\nexperience emotions all the time emotions interfere with our thinking process and\ndecision-making more than we imagine. Emotions are not illusing our thoughts but it help us in\nmaking rational decisions and there are some terms we shall be able to differentiate between two\nterms feelings and emotions.\nFeeling: as quoted from (\"The Difference Between Feelings and Emotions,\" n.d.) feeling\nindicates \u201cBoth emotional experiences and physical sensations\u201d which are felt consciously.\nEmotion: \u201cwhich originate as sensations in the body, that last only seconds to minutes\u201d(Spencer,\n2022), it can be felt internally whether consciously or in the subconscious. It is associated with\nthe person's beliefs, temporal mode, desires, and actions and reflects physically on the human\nbody, and unlike feelings in which its effect or presence can be tracked and measured(Spencer,"}, {"title": "Human Pose Estimation (HPE):", "content": "Human Pose Estimation is the process of detecting the positions of the human joints such as the\nhead and the kees in an image, video, or even 3D environment/ scene like the below Figure.\nUsually, pose estimation is implemented using a model-based technique in which the model takes\nthe input - whether it is an image/ video - and estimates the pose of each joint of the human body\nFigure below. For more information on the architecture, you can refer to the section on the\narchitecture of a pose estimation system. Some terminologies shall be addressed when explaining\npose estimation: Body joints refer to the points connecting two or more bones or cartilages in the\nbody while the body segments are portions of the body delimited by the joints. The pose\nestimation falls into two categories: 2D and 3D system spaces (coordinate systems). There are 3\nmain data representation categories when presenting the human body pose(Kalampokas et al.,\n2023): The Kinematic model, The planar model, and the volumetric model. The kinematic model\nwhich is the most used where the body joints are presented as points that can be matched together\nto form a simple graph for the human body joints connected as scene in image a in figure 4., it\nhas a low computational cost but doesn't contain any features of the shape of the human body or\nits texture. The planar model where each movable body part is presented as a rectangle separated\nby the joints as in image b figure 3, Although it added some shape information, it has a drained\nperformance, especially in the presence of high occlusion situations.The volumetric model where\n3D data of the human body is used to produce geometric meshes and geometric shapes of the\nhuman body. Although it holds a lot of useful data about the 3D human pose, it comes with a\ndramatic computational cost as viewed in image c figure 3. The problem of HPE can be directed\nto detect a single person or to detect multiple humans at a time, each has its own application,\nalthough the single-human detection approach is simpler and explored more in the area of\nresearch as we do not have to get in the hassle of Image Segmentation, a multi-person hierarchical\napproach was discussed here(Kumar & Singh, 2023; Li et al., 2024). The single HPE approaches\nfalls in two categories: the direct regression and the heatmap prediction while the muli-person\nHPE which is mainly localizing body joints of each person and ensure matching the body joints\nfor its target person falls into the Top-down methods or Buttom-up methods as explained"}, {"title": "Emotion Recognition using Pose estimation (Unimodal and Multimodal approaches):", "content": "Researchers have used emotion recognition in many different applications: Healthcare, the\nGaming industry, Autonomous vehicles, and child-robot interaction (CRI). Emotions can be\nrecognized through different modalities, they can be recognized through facial expressions and\nbody posture, vocal tone, sentiment analysis, and psychological signals. Some researchers found\nthat some emotions can be detected efficiently from body posture other than facial expressions\nlike pride. The importance of Emotional recognition from pose estimation arises in cases when\nthe face is blurred, far, or not clear like a surveillance camera when we are dealing with humans\nthat use their body posture to express emotions whether it is a personal trait or an illness condition\nas each human can focus on a different modality to express himself or a variant set of modalities,\nor recognizing complex emotions which require using different modalities to reach the highest\naccuracy possible for the predicted emotion. Recognizing emotions through the body can be very\nchallenging as we first need to detect the body pose, then we can apply an algorithm or model\nmostly using deep learning that can classify the pose into its most relevant/ related emotion.\nDetermining whether the problem shall be a regression or classification problem depends on the\nemotional model we are using which is determined by the type of output we aim for, example if\nyou want to measure pride, you shall not consider ekman's basic emotions but the emotion wheel.\nThe usage of multi-modal approaches requires a data fusion step which can be divided into early\nfusion, middle fusion, late fusion, and hyper-connected models (Zaghbani & Bouhlel, 2022). The\nfusion process is implemented as a separate layer in the trained neural network and the order of\nthe layer is determined if the architecture is going to fuse the data at the feature extraction level or\nin the decision-making output phase. While making the decision the simplicity, generalization,\nand computational complexity factors shall be taken into consideration."}, {"title": "Datasets:", "content": "Datasets are collected from the final research papers used in the survey also we keep in mind\nusing only emotions-related datasets and databases and the available ones that could be accessed\nby the time of this survey paper."}, {"title": "The Algorithms Modeling used in Emotion recognition and pose estimation", "content": "Most of the research papers lately have utilized ML and DL approaches to classify the output\nsuch as multi-layer complicated CNNs also the approach of training modalities separately and\nfusing them was frequently used. Let us discuss some of the most used algorithms in the emotion\nrecognition field:\n1. Convolutional Neural Networks (CNNs):.\n\u2022 CNNs are mainly utilized in image-based emotion recognition tasks, mostly for\nanalyzing facial expressions. They outmatch at detecting spatial patterns in images.\n\u2022 Mainly, CNNs are trained on big datasets of labeled facial images to learn features\npertinent to emotion recognition.\n2. Recurrent Neural Networks (RNNs):\n\u2022 RNNs are efficient with sequential data like time-series data or sequential text data.\n\u2022 RNNs can be used to model temporal dependencies in sequences of data, such as\nemotion recognition from spoken language or textual conversations."}, {"title": "Performance measurements", "content": "When we discuss the performance measurement used with pose estimation we shall ensure it is a\nclassification problem. Detecting actions or emotions from the pose is considered a\nmulti-classification method. The widely used way in evaluating a classification problem is using\nthe confusion matrix.\nTo define confusion matrix, we shall use the following:\na) True Positive (TP) number of positive classes predicted correctly.\nb) True Negative (TN) number of negative classes predicted correctly.\nc) False Positive (FP) number of negative classes predicted incorrectly.\nd) False Negative (FN) number of positive classes predicted incorrectly.\nMost of the papers used the accuracy of the confusion matrix which can be defined as number of\ncorrect predictions over(divided by) the total number of predictions and the recognition rate is\ndefined as the true positives over the total number of predictions but also some papers included\nthe following performance measurements:\ne) Precision is equal TP/ (TP + FP).\nf) Recall or Sensitivity is equal TP/ (TP + FN).\ng) Specificity is equal TN/(TN + FP).\nh) F1 Score is the harmonic mean of precision and recall which is equal to 2 * (precision *\nrecall)/ (precision + recall).\ni) Negative Predictive Value (NPV) is equal TN/ (TN + FN).\nj) Average error rate (or Classification error) is equal to (FP + FN) / total predictions (TP +\nTN + FP + FN) and can be also expressed as (1 - accuracy).\nk) Mean classification accuracy is the average of the accuracy scores obtained from multiple\nclasses, datasets, or models.\nl) Average absolute error is the average absolute difference between the predicted probabilities\nand the actual labels.\nm) V2V (used in validating pose estimation):\nn) Joint error (used in validating pose estimation):"}, {"title": "Comparison and Analysis:", "content": null}, {"title": "Discussion", "content": "(Liakopoulos et al., 2021) The paper did not directly make a human pose estimation model but\ncompared the effect of this modality with others. Wearable sensors and ML techniques were\nexplored to monitor stress and negative emotions in individuals through different sensing\nmodalities, including heart rate, electrophysiological signals, facial expressions, and body\nposture. The study has two main modules: stress detection and emotion recognition. Two\napproaches are discussed in the stress detection module. Statistical features are extracted in the\nfirst approach from physiological signals like ECG and EDA (electrodermal activity) and\nutilizing classification techniques such as SVM, KNN, Random Forest, and ANN. The second\nidentified frequency patterns associated with the stress that uses a 2D CNN for spectrogram\nalysis of ECG signals. The CNN-based spectrogram analysis of ECG signals shows comparable\nor better performance compared to traditional ML techniques.\n(Prakash et al., 2023) Diagnosing and assessing Autism Spectrum Disorder (ASD) in children is\nchallenging and the traditional methods such as manual observation and behavioral assessments\nhave their limitations. The paper highlights the use of ML, and DL approaches in assessing ASD\nchildren by analyzing provided multimodal clinical data. the automatic extraction and\nclassification of human actions from untrimmed videos needs to be more explored. Novel\ncomputer vision models are developed to automatically extract and classify joint attention skills,\nfacial expressions, and life skills actions from unclipped videos of ASD children. The system has\nthree goals developing computer vision models for automatic assessment of joint attention skills,\nbuilding a Facial Expression Recognition (FER) model to recognize emotional expressions, and\nproviding automatic functional assessment of children from intervention-recorded video sessions.\n(Santhoshkumar & Geetha, 2019) Recognizing emotional states from full-body motion patterns\nmethodology using a feedforward deep convolutional neural network (FDCNN) architecture is\nproposed. The proposed model converts input videos into frames and then applies convolutional\nand pooling layers to apply feature extraction. The final FC(fully connected) layers predict the\nemotions based on the previous features.\n(Amara et al., 2023) This paper presents the concept of the affective human digital twin (AHDT),\nwhich uses biometrics and AI to represent a person's emotions and behaviors. we should consider\nwhile developing human digital twin social factors like privacy, data protection, and ethical\ntechnology use. The architecture includes the creation of a bi-modal (facial expressions and body\nmovements) RGB, a dataset called RGB-D, and selection of crucial facial and skeletal\nfeatures/landmarks while comparing the data performance of Kinect 1 and Kinect 2 sensors. The\nRGB-D dataset is collected using Kinect sensors and an RGB HD camera, under constrained"}, {"title": "Conclusion", "content": "This survey ensures the more modalities, the better in terms of emotion recognition. Pose\nestimation can be used as a standalone modality to detect emotions and can be enhanced with\nother. The highest results were achieved using the Convolutional neural networks and the most\ncommon performance metric is the accuraccy followed by the confusion matrix and recognition\nrate. Although small efforts has been made in the detection of pose in a VR environment, this area\nneeds more exploration as it shall benfit the industry whether in therapeutic or gaming\napplications. Once the pose estimation is achieved in VR, this system can be used to record 3D\ndatasets and save the joints position thus, association between joint positions and emotions or\nactions can be made. This shall change the shape of saved data from images to text/json format\nthat can save training time and memory rather than images and image sequences to train the\nmodels."}]}