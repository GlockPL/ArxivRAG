{"title": "Survey on Emotion Recognition through Posture Detection\nand the possibility of its application in Virtual Reality", "authors": ["Leina Elansary", "Zaki Taha", "Walaa Gad"], "abstract": "A survey is presented focused on using pose estimation techniques in Emotional recognition using\nvarious technologies normal cameras, and depth cameras for real-time, and the potential use of VR\nand inputs including images, videos, and 3-dimensional poses described in vector space. We\ndiscussed 19 research papers collected from selected journals and databases highlighting their\nmethodology, classification algorithm, and the used datasets that relate to emotion recognition and\npose estimation. A benchmark has been made according to their accuracy as it was the most\ncommon performance measurement metric used. We concluded that the multimodal Approaches\noverall made the best accuracy and then we mentioned futuristic concerns that can improve the\ndevelopment of this research topic.", "sections": [{"title": "Introduction", "content": "Emotion recognition is one of the main vital tasks essential for having an intelligent system or\napplication. Dealing with humans requires understanding their own emotions so that the human\nfeels comfortable and the communication becomes more spontaneous which reflects on the\nefficiency of the service provided by the system/application. Emotions can be measured from\nmultiple modalities like reading facial expressions, gesture detection, static posture, movement\nbehavior, vocal tones, and text. When interacting with another human, you might know his\ncurrent emotions from only seeing his face and sometimes the eyes can do the trick, or from his\nvocal tone, his posture - the way he is standing- or from the pattern of his movements, the\ngestures he is making or the context of his words whether those words are said or written - you\ncan read an article and still visualize the emotions the writer has been through- or you can\ncombine two or more modalities together which increases the efficiency of the human's\npredictions. Computer models are being trained to recognize the above models far above is the\nphysical measurement which may include using sensors and actuators to measure physiological\npatterns that are hard for the computer to measure like measuring the heart rate, body\ntemperature, and skin sensitivity(Picard, R.W. and Vyzas, E. and Healey, J., n.d.). Those extra\nmodalities shall prepare the computer to be able to measure emotions accurately even more than\nhumans, which is not currently reached. We will discuss the challenges being faced in this field\nand how some papers overcome those challenges. Some modalities can provide reliable\nmeasurements on their own or they may be used only to enhance the recognition of another"}, {"title": "Research Question", "content": "What techniques and methodologies are used in literature to detect emotions\nthrough posture recognition?"}, {"title": "Objectives", "content": "\u2022\tObserve how frequently each technology is Used.\n\u2022\tList the measurement metric of each methodology.\n\u2022\texplore the possibility of using Virtual Reality in the task of emotion recognition\nthrough posture detection.\nThose keywords were chosen while doing the systematic review to be used in the academic\ndatabases and journals: Emotion Recognition/Detection AND Posture/Pose. The Virtual Reality\nkeyword shall be used later in the paper classification step. The review shall be held from year\n2019 to 2023. After the systematic review, we noticed the absence of Virtual reality usage and\none of the main objectives of this survey paper was to explore the possibility of using Virtual\nreality technology in Pose detection so we added the Virtual Reality journal to the above, when\nthose queries were used \"Pose in Virtual Reality\", \"Pose estimation in Virtual Reality\", \"Pose\ndetection in Virtual Reality\" no results were found till 5/2024 but by combining the Pose and\nVirtual reality keywords we reached 184 research article which was refined for relevance\naccording to their title and abstract."}, {"title": "Emotion Recognition", "content": "Emotion recognition has been a hot research topic since Rosalind Picard introduced the term\nAffective Computing in her book (Picard, n.d.) arguing that Artificial intelligence would not be\nable to interact effectively with humans or make accurate decisions unless they learn how to read\nthe user's emotions. The AI systems must be able to detect the emotions of the humans they are\ninteracting with and they shall be able not only to detect but to understand those emotions. we all\nexperience emotions all the time emotions interfere with our thinking process and\ndecision-making more than we imagine. Emotions are not illusing our thoughts but it help us in\nmaking rational decisions and there are some terms we shall be able to differentiate between two\nterms feelings and emotions.\nFeeling: as quoted from (\"The Difference Between Feelings and Emotions,\" n.d.) feeling\nindicates \u201cBoth emotional experiences and physical sensations\u201d which are felt consciously.\nEmotion: \u201cwhich originate as sensations in the body, that last only seconds to minutes\u201d(Spencer,\n2022), it can be felt internally whether consciously or in the subconscious. It is associated with\nthe person's beliefs, temporal mode, desires, and actions and reflects physically on the human\nbody, and unlike feelings in which its effect or presence can be tracked and measured(Spencer,"}, {"title": "Human Pose Estimation (HPE)", "content": "Human Pose Estimation is the process of detecting the positions of the human joints such as the\nhead and the kees in an image, video, or even 3D environment/ scene like the below Figure.\nUsually, pose estimation is implemented using a model-based technique in which the model takes\nthe input - whether it is an image/ video - and estimates the pose of each joint of the human body\nFigure below. For more information on the architecture, you can refer to the section on the\narchitecture of a pose estimation system. Some terminologies shall be addressed when explaining\npose estimation: Body joints refer to the points connecting two or more bones or cartilages in the\nbody while the body segments are portions of the body delimited by the joints. The pose\nestimation falls into two categories: 2D and 3D system spaces (coordinate systems). There are 3\nmain data representation categories when presenting the human body pose(Kalampokas et al.,\n2023): The Kinematic model, The planar model, and the volumetric model. The kinematic model\nwhich is the most used where the body joints are presented as points that can be matched together\nto form a simple graph for the human body joints connected as scene in image a in figure 4., it\nhas a low computational cost but doesn't contain any features of the shape of the human body or\nits texture. The planar model where each movable body part is presented as a rectangle separated\nby the joints as in image b figure 3, Although it added some shape information, it has a drained\nperformance, especially in the presence of high occlusion situations.The volumetric model where\n3D data of the human body is used to produce geometric meshes and geometric shapes of the\nhuman body. Although it holds a lot of useful data about the 3D human pose, it comes with a\ndramatic computational cost as viewed in image c figure 3. The problem of HPE can be directed\nto detect a single person or to detect multiple humans at a time, each has its own application,\nalthough the single-human detection approach is simpler and explored more in the area of\nresearch as we do not have to get in the hassle of Image Segmentation, a multi-person hierarchical\napproach was discussed here(Kumar & Singh, 2023; Li et al., 2024). The single HPE approaches\nfalls in two categories: the direct regression and the heatmap prediction while the muli-person\nHPE which is mainly localizing body joints of each person and ensure matching the body joints\nfor its target person falls into the Top-down methods or Buttom-up methods as explained"}, {"title": "Emotion Recognition using Pose estimation (Unimodal and Multimodal approaches)", "content": "Researchers have used emotion recognition in many different applications: Healthcare, the\nGaming industry, Autonomous vehicles, and child-robot interaction (CRI). Emotions can be\nrecognized through different modalities, they can be recognized through facial expressions and\nbody posture, vocal tone, sentiment analysis, and psychological signals. Some researchers found\nthat some emotions can be detected efficiently from body posture other than facial expressions\nlike pride. The importance of Emotional recognition from pose estimation arises in cases when\nthe face is blurred, far, or not clear like a surveillance camera when we are dealing with humans\nthat use their body posture to express emotions whether it is a personal trait or an illness condition\nas each human can focus on a different modality to express himself or a variant set of modalities,\nor recognizing complex emotions which require using different modalities to reach the highest\naccuracy possible for the predicted emotion. Recognizing emotions through the body can be very\nchallenging as we first need to detect the body pose, then we can apply an algorithm or model\nmostly using deep learning that can classify the pose into its most relevant/ related emotion.\nDetermining whether the problem shall be a regression or classification problem depends on the\nemotional model we are using which is determined by the type of output we aim for, example if\nyou want to measure pride, you shall not consider ekman's basic emotions but the emotion wheel.\nThe usage of multi-modal approaches requires a data fusion step which can be divided into early\nfusion, middle fusion, late fusion, and hyper-connected models (Zaghbani & Bouhlel, 2022). The\nfusion process is implemented as a separate layer in the trained neural network and the order of\nthe layer is determined if the architecture is going to fuse the data at the feature extraction level or\nin the decision-making output phase. While making the decision the simplicity, generalization,\nand computational complexity factors shall be taken into consideration."}, {"title": "Datasets", "content": "Datasets are collected from the final research papers used in the survey also we keep in mind\nusing only emotions-related datasets and databases and the available ones that could be accessed\nby the time of this survey paper."}, {"title": "The Algorithms Modeling used in Emotion recognition and pose estimation", "content": "Most of the research papers lately have utilized ML and DL approaches to classify the output\nsuch as multi-layer complicated CNNs also the approach of training modalities separately and\nfusing them was frequently used. Let us discuss some of the most used algorithms in the emotion\nrecognition field:\n1.\tConvolutional Neural Networks (CNNs):.\n\u2022\tCNNs are mainly utilized in image-based emotion recognition tasks, mostly for\nanalyzing facial expressions. They outmatch at detecting spatial patterns in images.\n\u2022\tMainly, CNNs are trained on big datasets of labeled facial images to learn features\npertinent to emotion recognition.\n2.\tRecurrent Neural Networks (RNNs):\n\u2022\tRNNs are efficient with sequential data like time-series data or sequential text data.\n\u2022\tRNNs can be used to model temporal dependencies in sequences of data, such as\nemotion recognition from spoken language or textual conversations.\n3.\tLong Short-Term Memory (LSTM) Networks:\n\u2022\tLSTMs are considered a type of RNN created to solve the vanishing gradient\nproblem and detect long-term dependencies in sequential data.\n\u2022\tThey are used in emotion recognition tasks where capturing context over longer\nsequences is important like text sentiment analysis.\n4.\tSupport Vector Machines (SVMs):\n\u2022\tSVMs are known for many classification tasks, including emotion recognition.\n\u2022\tIt is preferably used when feature engineering plays a crucial role or with a small\ndataset with well-defined features.\n5.\tEnsemble Learning:\n\u2022\tEnsemble methods such as Random Forests, Bagged Trees, or Gradient Boosting\nMachines (GBMs) can be implemented to combine the predictions of multiple\nmodels, enhancing overall performance.\n\u2022\tThey are often used in emotion recognition systems to improve robustness and\ngeneralization.\n6.\tDeep Neural Networks (DNNs):\n\u2022\tThe term \"deep\" refers to having multiple layers between the input and output layers\nwhich allows the network to learn complex functions.\n\u2022\tFrequently used for problems involving structured, tabular data and could be applied\nto image and text data but generally less effective than CNNs and RNNs for complex\npatterns.\n7.\tK-Nearest Neighbors (KNN):\n\u2022\tKNN is a non-parametric simple instance-based learning algorithm, it does not make\nexplicit assumptions about the form of the data only relies directly on the training\ndata during prediction, and can be used for both classification and regression tasks.\n\u2022\tIt is effective for small to medium-sized datasets and is used in basic pattern\nrecognition tasks: handwriting recognition, simple image classification, and\nrecommendation systems.\n8.\tSkinned Multi-Person Linear (SMPL):\n\u2022\tSMPL is considered a parametric model for generating realistic 3D human body\nmeshes, it represents the 3D shape and pose of the human body using a set of\nparameters that are related to body shape and pose. The output is a 3D mesh of the\nhuman body.\n\u2022\tIt is used mainly in animation, graphics, pose estimation, and HCI.\n9.\tDenseXception Neural Network:\n\u2022\tis a hybrid model combining principles from DenseNet and Xception architectures\nwhich benefits from the strengths of DenseNet's dense connectivity and Xception's\ndepthwise separable convolutions.\n\u2022\tIt suits tasks of image classification, image segmentation, and object detection."}, {"title": "Performance measurements", "content": "When we discuss the performance measurement used with pose estimation we shall ensure it is a\nclassification problem. Detecting actions or emotions from the pose is considered a\nmulti-classification method. The widely used way in evaluating a classification problem is using\nthe confusion matrix.\nTo define confusion matrix, we shall use the following:\na) True Positive (TP) number of positive classes predicted correctly.\nb) True Negative (TN) number of negative classes predicted correctly.\nc) False Positive (FP) number of negative classes predicted incorrectly.\nd) False Negative (FN) number of positive classes predicted incorrectly.\nMost of the papers used the accuracy of the confusion matrix which can be defined as number of\ncorrect predictions over(divided by) the total number of predictions and the recognition rate is\ndefined as the true positives over the total number of predictions but also some papers included\nthe following performance measurements:\ne) Precision is equal TP/ (TP + FP).\nf) Recall or Sensitivity is equal TP/ (TP + FN).\ng) Specificity is equal TN/(TN + FP).\nh) F1 Score is the harmonic mean of precision and recall which is equal to 2 * (precision *\nrecall)/ (precision + recall).\ni) Negative Predictive Value (NPV) is equal TN/ (TN + FN).\nj) Average error rate (or Classification error) is equal to (FP + FN) / total predictions (TP +\nTN + FP + FN) and can be also expressed as (1 - accuracy).\nk) Mean classification accuracy is the average of the accuracy scores obtained from multiple\nclasses, datasets, or models.\nl) Average absolute error is the average absolute difference between the predicted probabilities\nand the actual labels.\nm) V2V (used in validating pose estimation):\nn) Joint error (used in validating pose estimation):"}, {"title": "Comparison and Analysis", "content": "This survey ensures the more modalities, the better in terms of emotion recognition. Pose\nestimation can be used as a standalone modality to detect emotions and can be enhanced with\nother. The highest results were achieved using the Convolutional neural networks and the most\ncommon performance metric is the accuraccy followed by the confusion matrix and recognition\nrate. Although small efforts has been made in the detection of pose in a VR environment, this area\nneeds more exploration as it shall benfit the industry whether in therapeutic or gaming\napplications. Once the pose estimation is achieved in VR, this system can be used to record 3D\ndatasets and save the joints position thus, association between joint positions and emotions or\nactions can be made. This shall change the shape of saved data from images to text/json format\nthat can save training time and memory rather than images and image sequences to train the\nmodels."}, {"title": "Discussion", "content": "(Liakopoulos et al., 2021) The paper did not directly make a human pose estimation model but\ncompared the effect of this modality with others. Wearable sensors and ML techniques were\nexplored to monitor stress and negative emotions in individuals through different sensing\nmodalities, including heart rate, electrophysiological signals, facial expressions, and body\nposture. The study has two main modules: stress detection and emotion recognition. Two\napproaches are discussed in the stress detection module. Statistical features are extracted in the\nfirst approach from physiological signals like ECG and EDA (electrodermal activity) and\nutilizing classification techniques such as SVM, KNN, Random Forest, and ANN. The second\nidentified frequency patterns associated with the stress that uses a 2D CNN for spectrogram\nalysis of ECG signals. The CNN-based spectrogram analysis of ECG signals shows comparable\nor better performance compared to traditional ML techniques.\n(Prakash et al., 2023) Diagnosing and assessing Autism Spectrum Disorder (ASD) in children is\nchallenging and the traditional methods such as manual observation and behavioral assessments\nhave their limitations. The paper highlights the use of ML, and DL approaches in assessing ASD\nchildren by analyzing provided multimodal clinical data. the automatic extraction and\nclassification of human actions from untrimmed videos needs to be more explored. Novel\ncomputer vision models are developed to automatically extract and classify joint attention skills,\nfacial expressions, and life skills actions from unclipped videos of ASD children. The system has\nthree goals developing computer vision models for automatic assessment of joint attention skills,\nbuilding a Facial Expression Recognition (FER) model to recognize emotional expressions, and\nproviding automatic functional assessment of children from intervention-recorded video sessions.\n(Santhoshkumar & Geetha, 2019) Recognizing emotional states from full-body motion patterns\nmethodology using a feedforward deep convolutional neural network (FDCNN) architecture is\nproposed. The proposed model converts input videos into frames and then applies convolutional\nand pooling layers to apply feature extraction. The final FC(fully connected) layers predict the\nemotions based on the previous features.\n(Amara et al., 2023) This paper presents the concept of the affective human digital twin (AHDT),\nwhich uses biometrics and AI to represent a person's emotions and behaviors. we should consider\nwhile developing human digital twin social factors like privacy, data protection, and ethical\ntechnology use. The architecture includes the creation of a bi-modal (facial expressions and body\nmovements) RGB, a dataset called RGB-D, and selection of crucial facial and skeletal\nfeatures/landmarks while comparing the data performance of Kinect 1 and Kinect 2 sensors. The\nRGB-D dataset is collected using Kinect sensors and an RGB HD camera, under constrained\nconditions. Classification was achieved using Bagged Trees, k-NN, and a Support Vector\nMachine (SVM). Comes to surprise bodily emotion recognition slightly outperforms facial\nemotion recognition, with Kinect 2 producing better results than Kinect 1 and Bagged Trees\nshowed the highest accuracy rates when utilizing leave-one-subject-out cross-validation.\n(Zacharatos et al., 2021) The paper explores the use of deep learning specially CNNs in emotion\nrecognition in the context of gaming and VR applications. The proposed method involves\ntransforming 3D body movement data into pixel data describing the posture and motion\ndynamics. The output images are fed to a pre-trained CNN model, Inception V3, for emotion\nclassification. Transfer learning is employed to adjust the pre-trained model to the emotion\nrecognition task. Finally, the study demonstrates the potential of using deep learning techniques\nfor emotion recognition from body movements, for enhanced gaming and VR experiences.\n(CUI et al., 2020) The study summarizes the representation of emotions accompanied by different\npostures from existing body language literature. The architecture demonstrates first extracting the\nhuman body's key nodes from various postures to create simplified line graphs. The\nConvolutional Pose Machine (CPM) algorithm was used to extract the coordinates of key nodes\nfrom camera-captured images. simplified line maps of human body posture were drawn and the\nprocessed images were sent to a CNN for feature extraction and classification utilizing softmax\nregression. The proposed methodology effectively detects and recognizes posture emotions in\nvisible light images in the open environment with complex backgrounds.\n(Keshari & Palaniswamy, 2019) Two approaches for emotion recognition were introduced: the\nfirst is based on independent recognition from facial expressions and upper body gestures, while\nthe other is based on feature-level fusion. Combining facial expressions and upper body gestures\nimproved emotion recognition accuracy as concluded.\n(Pavlakos et al., 2019) The model SMPL-X is introduced in which a 3D model captures the body,\nface, and hands from single images. SMPL-X combines SMPL, FLAME head model, and MANO\nhand model, resulting in a holistic model capable of capturing the three features. SMPLify-X is\nintroduced as an improved method for fitting SMPL-X to single RGB images. pose estimation\naccuracy is enhanced using a learned pose prior while introducing a more accurate collision\npenalty term. A deep gender classifier trained on a dataset of annotated images was developed to\naddress gender differences and hence the classifier assigns gender labels to detected individuals,\nenabling the use of appropriate body models during fitting. Additionally, A variational human\nbody pose prior, VPoser, was proposed to penalize impossible poses while allowing valid ones.\n(Filntisis et al., 2019) DNN was adopted using both posture and facial expressions for automatic\nmulti-cue emotion recognition using hierarchical multi-label training (HMT). Multi-cue refers to\nthe usage of multiple sources or types of information (cues) to improve the performance\nof a model or an algorithm. Multi-Label Classification: Unlike traditional classification,\nin which each instance is assigned to a single label. multi-label classification assigns\nmultiple labels to each instance. Hierarchical Structure: It is similar to our use of the\nInheritance concept in object-oriented programming (OOP) where some labels are\nsub-categories of others so hierarchical multi-label training (HMT) refers to labeling the data\nwhere each instance can have multiple labels and each instance can have the other as its parent or\nchild. While hierarchical relationships are beneficial and can describe data better\nsemantically they introduce dependencies and constraints among labels, for example: if\nan instance is labeled as \"pilot\u201d, it must also be labeled as \u201chuman\u201d if there is a\ndependency between them. Although this approach enhances the accuracy and efficiency\nbut can dramatically affect the scalability and data imbalance since some labels won't be\nequally distributed as others. The architecture consists of separate sub-networks for facial and\nbody expression recognition, which are later fused for whole-body expression recognition. The\nfacial sub-network uses a Residual Network CNN to extract features from cropped face images.\nThe body sub-network employs a DNN to process skeleton representations(the data) obtained\nfrom 2D pose detection. The whole-body expression branch fuses features from both while\nemploying an FC layer for emotion recognition.\n(Crenn et al., 2020) A machine learning approach was adopted to classify the current emotion\nfrom the current motion inspired by the psychology domain. The \u201cneutrality\u201d score of a motion\nis estimated by a residue function, as the difference between both associated motions; the\nexpressive and the neutral motion. More precisely, this function that is inspired by studies from\nthe psychology domain, gives a \u201cneutrality\u201d score of a motion. The synthesis of the neutral\nmotion process is based on two nested PCAs providing a space where moving and selecting\nrealistic human animations become possible.\n(Ajili et al., 2019) A new descriptor vector for expressive human motions was proposed and was\ninspired by the Laban movement analysis method (LMA). The paper follows a three-step process\ncomprising data preprocessing: The proposed descriptor, motion representation: a machine\nlearning framework including, random decision forest, multilayer perceptron, and two multiclass\nsupport vector machine methods, and classification: gestures expressing emotions (happy, sad,\nangry, and calm) and the other target was to build a 4 emotions dataset associated with expressive\nmotions which they call CMKinect-10 dataset.\n(Randhavane et al., 2019) Initially, gaits are extracted as 3D poses from input videos and model\nlong-term dependencies in these sequential 3D human poses using an LSTM network, generating\ndeep features. Additionally, affective features are computed representing posture and movement\ncharacteristics. These features are combined and classified using a Random Forest Classifier into\nfour emotion categories: happy, sad, angry, and neutral.\n(Xing et al., 2020) The paper presents a model for driver behavior understanding by using an\nencoder-decoder CNN-RNN structure for multi-scale driver behavior learning and reasoning. The\nmodel has three main tasks: driver activity recognition, driver intention inference, and driver\nemotion recognition. The input which is the driver image sequence is encoded to protect the\ndriver's privacy, early fusion is adopted to fuse the inputs to be more scalable then the three tasks\nare processed in parallel, the first uses the Relu function with softmax and the others are using\nlayers of LSTM with Softmax to recognize the target behavior, the driver's intention, and the\ndriver's emotion.\n(Mittal et al., 2021) This paper uses several modalities like Facial, audio, textual, and pose/gait as\nwell as two contextual interpretations: Situational/Background Context and socio-dynamic\ncontext were used each processed in a separate neural network and then combined with\nmultiplicative fusion to calculate prediction and loss. The situational/background context is\nsimply a semantic context that aims for the improvement of scene visual interpretation and the\nsocio-dynamic context is concerned with social interaction and familiarity between agents in the\nscene which may affect their affective state.\n(Zhang et al., 2023) The importance of considering contextual cues such as body gestures and\nscene information besides facial expressions was introduced. By integrating these cues into\nemotion recognition models the accuracy was enhanced. It outperforms positive emotions such as\nhappiness and joy which differ in the lasting time interval and the effect in which happiness in\nshort intervals accompanies a sense of excitement while joy is more long-lasting and accompanies\ncontentment and satisfaction(The Difference between Joy and Happiness, n.d.). The architecture\nproposed consists of two parts feature extraction and fusion where three parallel deep networks\nextract features such as face features and combined contextual information that blends pose and\nscene information and emotion prediction that uses softmax for classification.\n(Wu et al., 2019) This paper uses DenseXception neural network which combines elements of\ntwo advanced deep learning models: DenseNet and Xception. They aimed for two outputs in their\nmodel discrete and continuous according to the two main categories of emotional models. Their\narchitecture composed of two phases: feature extraction network and feature fusion network. The\nfeature extraction network includes three sub-networks to extract the features of the inputs: the\nface, the body, and the image, then the feature fusion phase starts where it combines the output of\nthe three sub-networks to measure the discrete and continuous dimension.\n(Malek-Podjaski & Deligianni, 2021) it detects emotions from gait analysis.\n(Razzaq et al., 2020) This paper presents a framework called the UnSkEm framework consists of\nfour main modules: skeletal joint Acquisition which focuses on 3D skeletal joint data from the\nKinect device while focusing on the upper body joints, skeletal Frame Segmentation which\nprocesses the joint coordinates to calculate the inter-joint distances and angular features in an\ninterval of 3-second period, feature computation: Mesh Distance Features (MDF) and Mesh\nAngular Features (MAF) feature extraction methods, are used in capturing motion patterns of\nupper body joints, and skeletal classification: multi-class Support Vector Machines (SVMs) was\nused for emotion classification using sequential Minimal Optimization (SMO) which is s an\nalgorithm used for training support vector machines Emotions are recognized based on\npredefined body movements and poses associated with each emotion.\n(Zaghbani & Bouhlel, 2022) focuses on using facial expressions and the upper-body gestures data\nin recognizing affective states, they target 6 emotions in their multi-modal classification method\nwhich they call HCNN(. The architecture implemented late fusion,two CNN sub-networks were\nused to each to recognize facial and body gesture then their result is fused and softmax was used\nfor the multi-classification output layer. They achieved accuracy 99.79% using the FABO dataset.\nEmotion recognition from pose estimation showed in the above studies acceptable results in\ngeneral and can be used as a standlone modal but combining it with other modalities can\nimproves the system's performance. The benchmark was done based on the accuracy although\nsome research papers used the term recognition rate (Ajili et al., 2019; CUI et al., 2020; Razzaq\net al., 2020) (Zacharatos et al., 2021) the two terms sometimes are confused together or used\ninterchangeably although they are different as stated in the performance mertics section.\nRegarding the VR, pose estimation in VR needs more exploration as it can indeed benefit the\nreseach community whether by capturing 3D datasets in much less time and effort, data can be\nobtained from the VR gaming community with keeping the privacy of the users, Meta has\nintroduced a similar privacy protection approach for sharing the user facial landmarks with the\nused application through only providing the facial points which result in a sillutte shape, the\napplication developer are not going to be able to access the user's camera or take actual photos\nonly using the landmark points provided, the same approach can be used in pose estimation(Face\nTracking for Movement SDK for Unity: Unity | Oculus Developers, n.d.).\nSome papers discussing VR introduced emotion elicitation and a benchmark between the affect\nof Immersive and non-Immersive environments through emotion detection(Keshari &\nPalaniswamy, 2019), (Crenn et al., 2020) presented a the design and development of a VR\napplication that evaluates users' emotional state to virtual experiences and compare them with\nreal ones. They used a ready made tool for recognising 4 states in addition to the neutral state.\n(Filntisis et al., 2019) used a fine-tuned DL 2D facial landmark detector to calculate the 3D head\npose, 3D head model was adjusted to the tracked 2D facial landmarks to evaluate the accuracy of\nthe pose estimation outcomes and the possibilty of its application. Action recognition based on\nMulti-modals including RGB video, 2D pose estimation and VR HMD sensor has been discussed\nand achieved by using transformer self-attention network (Pavlakos et al., 2019)."}, {"title": "Conclusion", "content": "This survey ensures the more modalities, the better in terms of emotion recognition. Pose\nestimation can be used as a standalone modality to detect emotions and can be enhanced with\nother. The highest results were achieved using the Convolutional neural networks and the most\ncommon performance metric is the accuraccy followed by the confusion matrix and recognition\nrate. Although small efforts has been made in the detection of pose in a VR environment, this area\nneeds more exploration as it shall benfit the industry whether in therapeutic or gaming\napplications. Once the pose estimation is achieved in VR, this system can be used to record 3D\ndatasets and save the joints position thus, association between joint positions and emotions or\nactions can be made. This shall change the shape of saved data from images to text/json format\nthat can save training time and memory rather than images and image sequences to train the\nmodels."}]}