{"title": "Personalized Sleep Staging Leveraging Source-free\nUnsupervised Domain Adaptation", "authors": ["Yangxuan Zhou", "Sha Zhao", "Jiquan Wang", "Haiteng Jiang", "Shijian Li", "Benyan Luo", "Tao Li", "Gang Pan"], "abstract": "Sleep staging is important for monitoring sleep quality and\ndiagnosing sleep-related disorders. Recently, numerous deep\nlearning-based models have been proposed for automatic\nsleep staging using polysomnography recordings. Most of\nthem are trained and tested on the same labeled datasets\nwhich results in poor generalization to unseen target domains.\nHowever, they regard the subjects in the target domains as a\nwhole and overlook the individual discrepancies, which lim-\nits the model's generalization ability to new patients (i.e.,\nunseen subjects) and plug-and-play applicability in clinics.\nTo address this, we propose a novel Source-Free Unsuper-\nvised Individual Domain Adaptation (SF-UIDA) framework\nfor sleep staging, leveraging sequential cross-view contrast-\ning and pseudo-label based fine-tuning. It is actually a two-\nstep subject-specific adaptation scheme, which enables the\nsource model to effectively adapt to newly appeared unla-\nbeled individual without access to the source data. It meets\nthe practical needs in real-world scenarios, where the per-\nsonalized customization can be plug-and-play applied to new\nones. Our framework is applied to three classic sleep staging\nmodels and evaluated on three public sleep datasets, achiev-\ning the state-of-the-art performance.", "sections": [{"title": "INTRODUCTION", "content": "Sleep plays a crucial role in people's lives and has a signifi-cant impact on their overall well-being (Humphreys, Sharps,\nand Campbell 2005). Sleep staging is important for mon-itoring sleep quality and serves as a valuable tool to help\ndiagnose sleep disorders(Wang et al. 2023), which refers\nto classify sleep periods into different stages. Recently,\nPolysomnography (PSG) has been widely used for sleep\nstaging in clinics, which records various physiological sig-nals by the sensors attached to different parts of one body,\nsuch as electroencephalography (EEG), electrooculography\n(EOG), and electromyography (EMG). The PSG recordings\nare usually divided into consecutive epochs of 30s. Experts\nmanually identify each epoch into five distinct sleep stages,\nnamely, W, N1, N2, N3, REM, according to the American\nAcademy of Sleep Medicine (AASM) (Berry et al. 2012).\nObviously, such process is subjective, time-consuming and\nlabor-intensive.\nNumerous deep learning-based models have been pro-posed for automatic sleep staging (Tsinalis et al. 2016;\nMousavi, Afghah, and Acharya 2019; Zhou et al. 2024;\nWang et al. 2024b) in recent years. Such models are usu-ally trained and tested on the same labeled source data, au-tomatically classifying different stages. They have achieved\ngood performance for sleep staging in the valid set, how-ever, their performance is not so satisfying on unknown\nsamples. Most of them overlook the individual discrep-ancies, such as physiological structures (Matsushima, Mi-nami, and Takadama 2012), physical characteristics (i.e.,\nElectrodermal Response) and sleep habits. In clinical prac-tice, the new patients (unknown subjects) are probably dif-ferent from the samples used for training the models offline.\nThis limits the model's generalization ability to unknown\nsubjects and dramatically degrades the performance, espe-cially when they are directly tested on the new patients in\nclinics. Therefore, we desperately need to make the model\nadapt to new subjects. Unsupervised Domain Adaptation\n(UDA) is a suitable method which can transfer knowledge"}, {"title": null, "content": "learned from a labeled dataset (i.e., source domain) to an\nunlabeled dataset (i.e., target domain). Traditional UDA ap-proaches commonly apply a joint training strategy, which\nrelies on not only the target but also the source data to mit-igate domain shift through feature alignment. By doing so,\nthe performance of some target subjects is improved after\nadaptation (e.g., S1, S2 subjects in Fig. 1 (a)). However, this\nadapting way will lead to the following drawbacks. First, the\ntarget domain is treated as a whole (i.e., a number of sub-jects), which requires waiting until a batch of target domain\nsamples are available before conducting the adaptation. This\nis impractical in real life, where the arrival of each new indi-vidual is entirely random. We need a plug-and-play adap-tation rather than waiting for all the target individuals\nto arrive to conduct adaptation. Second, they overlook the\nindividual discrepancies, resulting in a failure to adapt to\ncertain special ones whose distribution deviates significantly\nfrom the overall distribution (e.g., S3, S4 subjects in Fig.\n1 (a)). A personalized sleep staging model is needed for\neach new patient. Third, using source data for joint-training\nis time-consuming and can lead to data privacy leakage. A\nsource-free adaptation is needed in clinical practice.\nTo meet such practical needs, in this paper, we propose an\nSource-Free Unsupervised Individual Domain Adaptation\nframework for automatic sleep staging, named SF-UIDA.\nFirst, we introduce the concept of individual domains,\nwhere the SF-UIDA framework treats each target subject as\na distinct target domain. Meanwhile, the proposed SF-UIDA\ncontains a two-step subject-specific strategy that consid-ers individual discrepancies to mitigate their impact on the\nsource model. It can be applied to the source pretrained mod-els and enable them to adapt to each individual in a personal-ized manner, without the need to wait for all the test individ-uals to arrive. Moreover, SF-UIDA also adopts a source-free\nUDA strategy, which is a more practical setup that does not\nrequire accessing the source data, thereby lowering the time\ncost and protecting the data privacy. As shown in Fig. 1 (b),\nour framework enables the source model to rapidly adapt to\neach new target in a personalized and plug-and-play fashion\nwithout accessing the source data.\nOur SF-UIDA is evaluated on three public datasets. The\nprocess of customization is efficient in time and acceptable\nin clinics, only taking a short amount of time to transform\nthe source model into a personalized model. Besides, it is\nworth pointing that our SF-UIDA framework can be easily\nimplemented without any modification to the source model\nstructure, achieving plug-and-play application in practice.\nOur contributions are as follows:\n\u2022 We devise a novel Source-Free Unsupervised Individual\nDomain Adaptation framework for automatic sleep stag-ing, named SF-UIDA. It meets the practical needs for a\nplug-and-play personalized customization that can be ap-plied to each newly appeared individual without access-ing the source data.\n\u2022 We propose a two-step subject-specific alignment strat-egy to mitigate the impact of individual discrepancies. It\neffectively transforms the source model into a personal-ized model for each new individual within a short adap-"}, {"title": "RELATED WORK", "content": "Automatic Sleep Staging\nThere have been many models proposed for automatic sleep\nstaging in recent years. Some studies commonly employ\nconvolutional networks to extract local sleep features. For\nexample, U-time (Perslev et al. 2019, 2021) is a fully\nCNN network based on the U-net architecture that can ex-cellently model sleep-related features. SalientSleepNet (Jia\net al. 2021) is also a fully CNN network based on U2-\nnet which can capture multi modal sleep feature. Consid-ering the advantage of capturing long-term temporal infor-mation, there are also some studies utilizing recurrent neu-ral networks (Mikolov et al. 2010) or transformer encoders\n(Vaswani et al. 2017) for sleep staging. Phan et al. (2022)\nproposed SleepTransformer, a transformer-based sequence-to-sequence model that improves the interpretability of the\nsleep-staging task. Although these models obtain good per-formance for sleep staging, they have not taken individual\ndiscrepancies into account, leading to dramatically degraded\nperformance when applied to target domains (i.e., unknown\nsubjects). In this paper, we propose a new training strategy\nto address individual discrepancies for sleep staging, so as\nto improve the model generalization ability in practice.\nSource-free Unsupervised Domain Adaptation\nTo address the challenge of model generalization on unseen\ndata, some studies have employed UDA methods to facilitate\nknowledge transfer between the source and target domains.\nExisting UDA studies (Tang et al. 2022; Wang et al. 2024a)\nhave reduced domain shift and extracted domain-invariant\nfeatures through distance-based alignment. Fan et al. (2022)\nutilized statistical alignment to mitigate domain shift across\nseveral sleep datasets. These UDA methods effectively en-hance the generalization ability of the source model for sleep\nstaging. However, they rely on source data for joint train-ing, which is impractical and time-consuming for each new\nsubject in real-life scenarios. In contrast, source-free UDA\n(Liang, Hu, and Feng 2020) offers a more practical solu-tion by eliminating the need for source data during adap-tation. Contrastive learning (CL) (Oord, Li, and Vinyals\n2018; Chen et al. 2020; Chen and He 2021) is commonly\nused in source-free UDA, focusing on mining intrinsic rep-resentation features within the data. Chen et al. (2022) ap-plied a self-supervised CL method to facilitate target feature\nlearning and achieve test-time adaptation, while Wang et al.\n(2022) utilized weight-averaged and augmentation-averaged\npredictions to generate pseudo-labels for adaptation. These\nsource-free approaches enable rapid adaptation of the source\nmodel to target domains. However, they often overlook in-dividual discrepancies and treat the entire target domain, en-compassing multiple subjects, as a single distribution for\nadaptation. This can hinder accurate predictions for individ-uals whose distributions significantly deviate from the over-"}, {"title": "METHODOLOGY", "content": "Problem Formulation\nNs\ni=1\nNT\nj=1\nIn this work, we try to address the issue of individual dis-crepancy for the task of automatic sleep staging by employ-ing an UDA-based approach. Here, we introduce the concept\nof individual target domain, which consists of the recordings\nfrom only one subject. Formally, given a labeled source do-main $D_s=\\{X^S_i, Y^S_i\\}_{i=1}^{N_s}$ with $N_s$ subjects and an unlabeled\nindividual target domain $D_T=\\{X^T_j\\}_{j=1}^{N_T}$ with $N_T = 1$. We\ndenote the distributions of different domains as $P_s(x, y)$ and\n$P_T(x, y)$ respectively, where $P_s(x, y) \\neq P_T(x, y)$. We em-ploy the sleep sequence $X_s=(x^1, x^2, x^3, ..., x^L)$ of length\n$L$ and its corresponding label $Y_s=(y^1, y^2, y^3, ..., y^L)$ from\n$D_s$ as inputs, where $x^i, y^i$ denotes the data and la-bel of i-th epoch in the sequence $X_s$. Our main purpose\nis to accurately predict the label $Y_T=(y^1, y^2, y^3, ..., y^L)$\nof unlabeled individual target domain sequential sample\n$X_T=(x^1, x^2, x^3, ..., x^L)$.\nOverview\nIn this work, we first pretrain the source model from a la-beled source domain $D_s$. The probability distribution of the\nsource domain can be described as follows.\n$P_s(x,y) = P_s(x)P_s(y | x) = P_s(y)P_s(x | y)$   (1)\nConsidering the individual discrepancies among each sub-ject, our SF-UIDA framework contains a two-step align-ment: subject-specific adaptation and subject-specific\npersonalization, to align each of the individual target\ndistribution $P_T$ with the source domain distribution $P_s$,\ncustomizing the sleep staging model for each individual.\nSpecifically, we use the unlabeled data from each indi-vidual target domain and the generated pseudo-labels to\nmake the source model's probability distribution $P_s(x, y)$"}, {"title": null, "content": "align with the individual target domain's probability dis-tribution $P_T(x)$ and class-conditional probability distribu-tion $P_T(x | y)$ illustrated in Fig. 2. The whole pro-cess can be totally divided into three training stages:\nSource Model Pretraining: We employ several classical\nsleep stage classification (SSC) models as the pretrain-ing models to learn the general sleep features from the\nsource domain. Subject-Specific Adaptation: We propose\na subject-specific adaptation by proposing a sequential\ncross-view prediction task on individual target domain.\nIt is used to capture subject-specific sleep representa-tions and align with the individual target domain's prob-ability distribution $P_T(x)$, mitigating the impact of in-dividual discrepancies on the pretrained source model.\nSubject-Specific Personalization: We employ a teacher\nmodel based pseudo-labeling strategy for fine-tuning, so\nas to learn the fine-grained distribution of different classes\nin individual target domains. It enables the source model\nfurther align the class-conditional probability distribution\n$P_T(x | y)$, thus achieving personalized customization. No-tably, our model customization is absolutely source-free,\nusing only the unlabeled target data for personalization.\nSource Model Pretraining\nWe first extract general characteristics from PSG recordings\nby pretraining a model on the source domain. Specifically,\nwe employ three classical lightweight sleep staging models\nas pretrained models. Each model has a specialized extractor\nfor sleep features and a temporal encoder to capture tempo-ral information from sleep sequences. The parameters of the\nsource model are then transferred to the subsequent process.\nSubject-Specific Adaptation\nDue to the independence of individuals in the test set and\ninter-individual differences, each individual target domain\nexhibits a distinct distribution, often significantly deviating\nfrom the source domain distribution. Therefore, the pre-trained source model struggles to generalize to individual\ntarget domains. In this step, our objective is to reduce do-main shift and mitigate the impact of individual discrep-ancies without access to the source data. Inspired by the\nCPC and TS-TCC (Oord, Li, and Vinyals 2018; Eldele et al.\n2021) algorithms, we propose a subject-specific adaptation\nscheme for unsupervised domain adaptation, aligning the\nprobability distribution $P_T(x)$ through a complex sequen-tial cross-view prediction task. This approach enables us to\nmodel specific representations for each individual.\nSequential Cross-View Contrasting According to the\nAASM(Berry et al. 2012), the transition patterns of sleep\nstages between neighboring epochs are crucial for accurate\nsleep staging. These transitions occur not only in the\nforward direction (e.g., W\u2192N1\u2192N2\u2192N3) but also in re-verse (e.g., REM\u2190N3\u2190N2). Motivated by these patterns,\nwe propose a novel Sequential Cross-view Contrasting\n(SCC) module to model the bidirectional transition relation-ships within subject-specific sequences, as illustrated in Fig.\n3. To generate a new augmented view, reversing the original\nsequence effectively contrasts the temporal relationships"}, {"title": null, "content": "between different views. Formally, given an input sleep\nsequence $X_\u2081=(X_1, X_2, ..., X_{L-1}, X_L)$, its augmented view\n$X_j=(X_L, X_{L-1},...,X_2,X_1)$ is obtained by reversal. After\nfeeding $X_i$ and $X_j$ into the Feature Extractor and Feature En-coder, we obtain their corresponding latent representations\n$Z_\u2081=(Z_1, Z_2, ..., Z_{L-1}, Z_L)$ and $Z_j=(z_1, z_{L-1}, ..., z_2, z_1)$,\nrespectively. For a given time step $T$ (1 < T < L), we\nutilize a transformer as an autoregressive model to encode\n$Z_{i:t<T}, Z_{j:L-T<t<L}$ into a contextual vector: $C_i$ and\n$C_j$. We then establish a sequential cross-view task, using\nlinear layers to predict the future $L - T$ sleep timesteps\nfrom $z^T_{i+1}$ to $z_L$ in sequence $Z_i$ by leveraging contextual\nvector $C_j$, such that $z^T_{i+k}=f_{T+k}(C_j)$, where $z^T_{i+k}$\ndenotes the predicted timesteps for $z_{i+k}$ and $f_{T+k}$ is\nthe corresponding predicting linear layer. Similarly, we\nuse contextual vector $C_i$ to predict the past $L - T$ sleep\ntimesteps from $z^1_j$ to $z^L_{j-T}$ in sequence $Z_j$. The cross-view\npredicted timesteps can be formulated as $z^T_{i+1~L}=\\{f_{T+1}(C_j), f_{T+2}(C_j),..., f_L(C_j)\\} = \\{z^T_{i+1},z^T_{i+2},...,z^T_{iL}\\}$\nand $Z^T_{j~L-T} = \\{f_1(C_i), f_2 (C_i), ..., f_{L-T}(C_i)\\} = \\{z^T_1, z^T_2, \u2026\u2026\u2026, z^T_{L-T}\\}$, respectively. We then apply the Max-imum Mean Discrepancy (MMD) loss to minimize the\ndistance between the cross-view predicted timesteps $Z_i$ and\n$Z_j$ as follows:\n$L_c=\\frac{1}{K}\\sum_{k=1}^{K}D_{MMD}(z_{i+k}^T, z_k)$    (2)\nwhere $K$ denotes the $L \u2013 T$. Notably, for a specific pre-dicted timestep, we minimize its MMD distance between\nthe corresponding predicted timestep $z_{i+k}^T$ rather than $z_k$.\nThis is because after reversing, the position corresponding\nto $z_{i+k}$ aligns perfectly with $z_k$.\nSubject-Specific Personalization\nThe subject-specific adaptation aligns the marginal distri-butions between the source domain and the individual tar-get domain. Due to the influence of individual differences,\nthere still exist class-conditional distribution discrepancies\nbetween each individual target domain and the source do-"}, {"title": null, "content": "main, which may lead to erroneous alignment of different\nclasses across the source and target domains shown in Fig.2\n(a). Conventional solutions typically rely on fine-tuning with\nlabeled data to address this issue (Eldele et al. 2023). How-ever, we are unable to employ supervised methods for class-conditional distribution alignment as the target labels are\nunavailable. Inspired by Tarvainen and Valpola (2017) and\nRagab et al. (2022), we employ a teacher model based on\npseudo-label generation approach to tackle this problem\nshown in Fig.2(b). Notably, we have introduced sequence\nconfidence for each individual, producing robust pseudo se-quence labels. We solely preserve the confident ones for fur-ther fine-tuning, which enable us to better align the class-conditional distribution with the source domain and model\nthe personalized representations. Specifically, we migrate\nthe model parameters $W_\u03b8$ to the teacher model $F_{\u03b8'}$ using\nExponential Moving Average (EMA). The updates to the\nteacher model parameters are as follows:\n$W_{\u03b8'}= \u03b1W_{\u03b8'} + (1 \u2212 \u03b1)W_\u03b8$   (3)\nwhere $W_{\u03b8'}$ denotes the parameters of the teacher model\n$F_{\u03b8'}$ and $\u03b1$ is a hyper-parameter employed to regulate\nthe update rate of the teacher model parameters. For\neach sleep sequence $X_T=(X_1,X_2, X_3, ..., X_L)$, we can ob-tain the corresponding predicted sequence probabilities\n$Y_T=(y_1, y_2, y_3, ..., y_L)$ by the teacher model. Given a sleep\nsequence $X_T$ with the length L, we retain it for subsequent\nfine-tuning iff there are $N_\u03b8$ or more epochs in the sequence\nand the prediction probabilities of each epoch not less than\nthe confidence threshold $\u03be$. It can be formalized as follows:\n$Y_T = softmax(F_\u03b8(X_T))$   (4)\n$V_T = 1_{\\{[\\sum_{i=1}^{L}1_e (max(y_i)>\u03be)]\u2265N_\u03b8\\}}\u00b7V_T$   (5)\nwhere $1_e$ is the confident epoch indicator function, evaluat-ing to 1 iff $max(y_i)> \u03be$ and $1_{[]}$ is the confident sequence\nindicator function, evaluating to 1 iff there are $N_\u03b8$ or more\nconfident epochs $y^i_T$ in the $Y_T$. In most existing studies, the\nconfidence threshold $\u03be$ is set to be greater than 0.9. However,\nbased on our confident sequence setting, we focus more on\nthe overall confidence of the sequence rather than that of a\nsingle epoch. So we need a higher tolerance for the thresh-old $\u03be$ and we set the it equal to 0.8 and the $N_\u03b8$ is set to 15.\nTo ensure alignment of the class-conditional distribution, we\nemploy the confident sequence labels to finetune the model,\nby using a cross-entropy loss:\n$L_{ce} = -\\frac{1}{K} \\sum_{X_T \\sim P_T} \\sum_{k=1}^{K}y_T \\text{log}(y_k^T)]$   (6)\nDuring the whole process, only the unlabeled target individ-ual's data is needed for personalized customization without\naccess to the source data. The algorithm of SF-UIDA frame-work is illustrated in Algorithm 1."}, {"title": "EXPERIMENTS", "content": "Datasets\nAs shown in Table 1, we evaluated our approach on three\npublicly available datasets: Sleep-EDF (Kemp et al. 2000),\nISRUC (Khalighi et al. 2016), and HMC (Alvarez-Estevez\nand Rijsman 2021). For each dataset, we utilized both EEG\nand EOG channels as input. ISRUC: This public database\nconsists of three sub-groups. We specifically selected sub-group 1, which includes all-night polysomnography (PSG)\nrecordings from 100 adults, totaling 86,400 samples. Sub-jects 8 and 40 were excluded due to missing channels.\nSleepEDF-153: A public Physionet database comprising\n78 healthy subjects aged 25 to 101, containing 188,760\nsamples. All subjects' recordings were used for evaluation.\nHMC: This public dataset includes recordings from 151\nsubjects at the Haaglanden Medisch Centrum (The Nether-lands), consisting of 129,440 samples. Subjects 14, 32, 33,\n64, 112, and 135 were excluded due to missing channels. All\nsleep recordings were bandpass filtered (0.3 Hz-35 Hz) and\nresampled to 100 Hz.\nSettings\nBaseline Models We need to select baseline models to\nevaluate our SF-UIDA framework. Considering the effi-ciency requirement in clinical practice, the fine-tuning for"}, {"title": null, "content": "each individual cannot be time-consuming. Therefore, we\nselected three lightweight sleep staging models from ex-isting studies, each of which is comprised of the feature ex-tractor and the temporal encoder: DeepSleepNet (Supratak\net al. 2017): a classical CNN-BiLSTM model for extracting\nsleep features and learning transition rules. TinySleepNet\n(Supratak and Guo 2020): a more lightweight model based\non the DeepSleepNet. RecSleepNet (Nie, Tu, and Xu 2021):\na CNN-LSTM model based on feature representation recon-struction. Here, we do not choose some other sleep stag-ing models, which are also classical and perform well, such\nas UTime (Perslev et al. 2019), SalientSleepNet (Jia et al.\n2021) or SleepTransformer (Phan et al. 2022), because their\nnetwork structures are complex and not suitable for our\nproblem in clinical practice.\nImplementation Based on the publicly available source\ncode, we re-implemented the three baseline models using\npytorch. The experimental settings are as follows: Source\nModel Pretraining: The pretraining epoch is set to 100.\nThe learning rate is set to le-4. Subject-Specific Adapta-tion: The training epoch of this stage is set to 5. The learn-ing rate is set to le-7. The time step T is set to 17. Subject-Specific Personalization: The fine-tuning epoch is set to 10.\nThe learning rate is set to le-7. The momentum $\u03b1$ is set to\n0.996. We use the Adam optimizer to train the model, the $\u03b2$\nis set to [0.5,0.99], the weight decay is set to 3e-4, the size\nof mini-batch is set to 32. The model is trained on a single\nmachine equipped with an Intel Core i9 10900K CPU and\neight NVIDIA RTX 3080 GPUs. The source code is pub-licly available\u00b9.\nPerformance Measurement We employ 10-fold cross-validation (CV) to evaluate the performance of our approach\nacross three different datasets. Different from the conven-tional settings in previous studies, which only included train-ing and validation sets, we divided the dataset into the\ntraining, validation, and test sets and the ratio is 8:1:1.\nThe test set is regarded as unknown subjects, where there\nare no repetitive individuals in the test set, ensuring that\neach individual appears only once in the test set through-out the 10-fold CV experiment. In each fold, we employ\nthe training and validation sets to pretrain the source model.\nSubsequently, the source model goes though personaliza-tion customization on each individual in the test set. Fi-nally, we compute the average metrics for each individual\nin the test set. We employ Accuracy (ACC) and Macro-F1\nscore (MF1) as evaluation metrics."}, {"title": "Result Analysis", "content": "Compared with Other Existing Methods We compare\nour method with other classical source-free UDA methods in\nsleep staging, to further investigate the generalization ability\nof SF-UIDA. Source only : A method to directly test the in-dividual target domain using source model. CPC (Oord, Li,\nand Vinyals 2018): A classical contrastive learning approach\nto learn representation of time-seires data by predicting the\nfuture timesteps. SimSiam (Chen and He 2021): An efficient\ncontrastive learning approach, which focuses on representa-tion learning using stop-gradient strategy and symmetrized\nloss. Adacontrast (Chen et al. 2022): A test-time adaptation\nmethod using contrastive learning to facilitate target feature\nlearning. CoTTA (Wang et al. 2022): A test-time adaptation\nmethod, which can effectively adapt off-the-shelf source\npretrained models to target domains. C-SFDA (Karim et al.\n2023): A curriculum learning aided self-training framework\nfor SFDA is designed to adapt efficiently and reliably to tar-get domains. We implement these SFDA methods within our\nframework, and the performance comparison is shown in\nTable 2. Our method outperforms existing approaches, un-derscoring the effectiveness of our personalized customiza-tion strategy in enhancing overall performance. Compared\nto the Source Only method, our approach significantly en-ables the source model to adapt to individual target do-mains, achieving model customization and improved perfor-mance. Among the compared methods, it is worth noting\nthat methods leveraging contrastive learning(e.g. SimSiam,\nAdacontrast, C-SFDA) exhibit better performance compared\nto other approaches (e.g. CPC, COTTA).\nCompared with Non-Personalized Domain Adaptation\nThe traditional domain adaptation (DA) paradigm consid-ers a batch of subjects as the target domain for subsequent\nadaptation which is impractical in real life. In contrast, our\nindividual domain adaptation based method allows for plug-and-play application on each new subject without waiting.\nTo evaluate the effectiveness of our individual DA setting\ncompared to traditional DA paradigm, we conducted a com-parative study. Notably, we maintained the consistent par-titioning of the 10-fold cross validation and the SF-UIDA\nframework. The only difference is: for the traditional DA\nparadigm, in each fold we use the data of all target indi-"}, {"title": null, "content": "viduals (i.e., test set) to perform adaptation, rather than\nconducting model customization for each individual sep-arately. We evaluated the performance of the fine-tuned\nmodel on each test individual shown in Fig. 4. Compared\nto the traditional domain adaptation paradigm, our person-alized adaptation based paradigm can achieve comprehen-sive superiority in performance on all baseline models across\nthe three datasets. It proves that our method can not only\nmeet the practical needs in real-world scenarios, where the\npersonalized customization can be plug-and-play applied to\nnew individuals, but also better achieve individual perfor-mance improvement after adaptation.\nAblation Study To investigate the importance of our pro-posed two-step alignment strategy, we conducted this abla-tion experiments. The model variants are defined as follows:\n\u2022 SO: which means source only that we directly use the\nsource model for testing.\n\u2022 SO+SSA: only the Subject-Specific Adaptation (SSA)\nstage is preserved in the SF-UIDA framework.\n\u2022 SO+SSP: only the Subject-Specific Personalization\n(SSP) stage is preserved in the SF-UIDA framework.\n\u2022 SO+SSA+SSP: we employed the full two-step alignment\nprocess of the SF-UIDA framework.\nAs shown in Table 3, our ablation experiments convincingly\ndemonstrate the efficacy of the proposed two-step alignment"}, {"title": null, "content": "methodology for unsupervised individual domain adapta-tion. Compared to the source-only method, both the SSA\nand SSP modules yield improved performance, highlight-ing the effectiveness of our alignment strategy during model\ncustomization. Notably, the SSP module outperforms the\nSSA module slightly, suggesting that aligning the class-conditional distributions of the individual target domain is\nmore impactful than aligning the marginal distribution. This\nis reasonable given the imbalance in sleep data classes, as\nevidenced by a greater performance gap on the MF1 metric\n(67.7% vs. 66.8%) compared to the ACC metric (75.0% vs.\n74.8%). The MF1 metric more accurately reflects the clas-sification performance for each class. By integrating both\nSSA and SSP alignment modules, our SF-UIDA frame-work achieves superior performance compared to single-alignment approaches.\nComputational Complexity To evaluate the computa-tional complexity of our proposed SF-UIDA framework, we\ncalculate the time cost per individual across three datasets\nshown in Fig. 5. Our method is capable of completing per-sonalized customization for an unknown individual within\nan average of 40 seconds. When compared to the several\nhours' duration of one person's sleep records, this time\ncost is acceptable. Moreover, considering the unseen sub-ject commonly appears one by one in practice, our method\nis applicable to enable the source model continuously adapt\nto new subjects and achieve plug-and-play personalized cus-tomization for each individual.\nFeature Visualization To demonstrate the effectiveness\nof our method, we selected two individuals from the Sleep-EDF dataset to visualize intermediate features using the t-SNE method (Van der Maaten and Hinton 2008). Figures 6\n(a-1) and (a-2) illustrate the feature distribution for the first\nindividual, showing the distribution before and after person-alization. Similarly, Figures 6 (b-1) and (b-2) depict the dis-"}, {"title": "CONCLUSIONS", "content": "In this paper, we present a novel Source-Free Unsuper-vised Individual Domain Adaptation (SF-UIDA) framework\nfor automatic sleep staging, employing a two-step subject-specific alignment scheme for adaptation. Our framework\nfacilitates plug-and-play personalization for each new indi-vidual without requiring access to source data, meeting the\npractical needs in clinics. Experimental results across three\npublic datasets demonstrate that the SF-UIDA framework\neffectively transforms a source model into a personalized\none within a short adaptation period, highlighting its prac-tical significance. Our future work will aim to extend the\napplicability of our method to a broader range of EEG tasks."}]}