{"title": "ZETA: LEVERAGING Z-ORDER CURVES FOR EFFICIENT TOP-k ATTENTION", "authors": ["Qiuhao Zeng", "Jerry Huang", "Peng Lu", "Gezheng Xu", "Boxing Chen", "Charles Ling", "Boyu Wang"], "abstract": "Over recent years, the Transformer has become a fundamental building block for sequence modeling architectures. Yet at its core is the use of self-attention, whose memory and computational cost grow quadratically with the sequence length N, rendering it prohibitively expensive for long sequences. A promising approach is top-k attention, which selects only the k most relevant tokens and achieves performance comparable to vanilla self-attention while significantly reducing space and computational demands. However, causal masks require the current query token to only attend to past tokens, preventing existing top-k attention method from efficiently searching for the most relevant tokens in parallel, thereby limiting training efficiency. In this work, we propose ZETA, leveraging Z-Order Curves for Efficient Top-k Attention, to enable parallel querying of past tokens for entire sequences. We first theoretically show that the choice of key and query dimensions involves a trade-off between the curse of dimensionality and the preservation of relative distances after projection. In light of this insight, we propose reducing the dimensionality of keys and queries in contrast to values and further leverage Z-order curves to map low-dimensional keys and queries into one-dimensional space, which permits parallel sorting, thereby largely improving the efficiency for top-k token selection. Experimental results demonstrate that ZETA matches the performance of standard attention on the synthetic MULTI-QUERY ASSOCIATIVE RECALL task and outperforms attention and its variants on LONG RANGE ARENA and WIKITEXT-103 language modeling.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformers (Vaswani et al., 2017) have become indispensable for sequence modeling across various domains, including natural language processing (NLP) (Devlin et al., 2019; Brown et al., 2020; OpenAI et al., 2024; Jiang et al., 2024), computer vision (Dosovitskiy et al., 2021; Ramesh et al., 2021; Brooks et al., 2024), etc. The foundation of Transformer models is the self-attention mechanism. This mechanism (Bahdanau et al., 2015), inspired by recurrent neural networks (RNNs) and their ability to construct representations from all elements in a sequence, has revolutionized numerous fields, enabling breakthroughs in tasks such as language modeling (Radford et al., 2019), machine translation (Ott et al., 2018), text generation (Brown et al., 2020), image classification (Touvron et al., 2021) and video generation (Brooks et al., 2024). However, self-attention has a quadratic complexity in both memory and computation as the sequence length N increases, which presents a significant challenge when scaling to long sequences (Child et al., 2019; Beltagy et al., 2020). This makes the direct application of self-attention in large-scale problems computationally prohibitive for many real-world applications, particularly when long sequences are involved (Tay et al., 2021).\nRecent advances have explored strategies to mitigate the inefficiencies of vanilla self-attention. One such approach is top-k attention, which focuses computation on a subset of the most relevant tokens, significantly reducing memory and computation costs while maintaining competitive performance (Kitaev et al., 2020; Gupta et al., 2021; Bertsch et al., 2023; Mao et al., 2024). However, existing top-k attention methods (Kitaev et al., 2020; Zhuoran et al., 2021) typically apply causal"}, {"title": "2 RELATED WORKS", "content": "Efficient Transformer The Transformer architecture (Vaswani et al., 2017) is foundational for sequence modeling, but its quadratic complexity limits efficiency with long sequences. Various efficient variants (Tay et al., 2022; 2020; Chen et al., 2021; Qin et al., 2022b; Zhang et al., 2024) have been proposed as alternatives, mainly categorized into sparse, low-rank, and linear transformers. Sparse transformers, such as BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020), restrict attention to local windows or global tokens to achieve linear complexity. SparseAxial (Ho et al., 2020) further enhances this by combining sparse attention with axial mechanisms for high-dimensional inputs. Reformer (Kitaev et al., 2020) locality-sensitive hashing to handle variable-length sequences efficiently. Low-rank transformers like Linformer (Wang et al., 2020) reduce the attention matrix to a lower-dimensional space, reducing memory and computation costs. Linear transformers such as Performer (Choromanski et al., 2021) use kernel-based approximations for linear-time complexity, while Nystr\u00f6mformer (Xiong et al., 2021) leverages Nystr\u00f6m decomposition for near-linear performance.\nTop-k Attention (Gupta et al., 2021) falls under the category of sparse attention, reducing attention complexity by selecting only the top-k most relevant tokens at each layer, thereby focusing computational resources on the most critical interactions. Unlimiformer (Bertsch et al., 2023) enables transformers to handle arbitrarily long sequences by chunking inputs and using a retrieval mechanism to attend to relevant past contexts. Similarly, IceFormer (Mao et al., 2024) improves"}, {"title": "3 METHODOLOGY", "content": null}, {"title": "3.1 PRELIMINARIES", "content": "Attention has proven to be a fundamental building block in modern deep learning, particularly in natural language processing and sequence modeling tasks. It allows a model to focus on specific parts of the input sequence, thereby capturing dependencies within the sequence more effectively. In the standard formulation of attention (Vaswani et al., 2017), given a set of queries $Q \\in \\mathbb{R}^{N\\times d_Q}$, keys $K \\in \\mathbb{R}^{N\\times d_k}$, and values $V \\in \\mathbb{R}^{N\\times d_v}$, the attention scores are computed as\n$\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}\\left(\\frac{QK^T}{\\sqrt{d_K}}\\right)V$,\nwhere $d_K$, $d_Q$ and $d_V$ is the dimension of the keys, queries and values, respectively. It is common to use the same value for all three.\nTop-k Attention approaches aim to enhance the efficiency and flexibility of traditional self-attention mechanisms by focusing attention to only the most relevant tokens in a sequence. Both methods reduce the computational overhead associated with self-attention, especially for long sequences, by selectively attending to the most important tokens rather than computing attention scores across the entire sequence. This process lowers the computational complexity from $O(N^2)$ in traditional self-attention to $O(N\\cdot k)$, where N is the sequence length and k is the number of selected tokens. The top-k highest scores are selected for each query:\n$I_q = \\{i \\mid qK/\\sqrt{d_k} \\geq T_k\\}$\nwhere $I_q$ represents the set of indices corresponding to the top-k highest attention scores for the query q, $T_k$ is the threshold defined as the k-th highest attention score. The attention is then restricted to the tokens in this subset, reducing the number of operations required for long sequences. Specifically, the attention score for the top-k tokens is recalculated using the self-attention mechanism but limited to the selected indices:\n$\\operatorname{Attention}_{\\text{top-k}}(q, K, V) = \\sum_{i\\in I_q} \\operatorname{softmax}\\left(\\frac{qK}{\\sqrt{d_k}}\\right)V_i$\nwhere q denotes the query vector, $K_i$ and $V_i$ denote the key and value vectors corresponding to the top-k relevant tokens, respectively."}, {"title": "3.2 SEARCHING FOR THE TOP-k ATTENDED TOKENS IN ONE-DIMENSIONAL SPACE", "content": "Since we project key and query vectors into a one-dimensional space using Z-order curves, using a large $d_K$ can still distort locality (as shown in Figure 3) and compromise the preservation of relative distances. Thus we ask whether $d_K$ can be reduced in a way such that even after mapping to one dimension, relative distances between tokens are maintained. Importantly, the key and query dimensions $d_K$ and $d_Q$ do not have to match the dimension of the values $d_V$. This is because $d_V$ should remain large to capture more semantic information, as seen with Gaussian distributions, where higher dimensionality increases the measure of information entropy (Cover & Thomas, 2006). Hence, as long as the relative distances between queries and keys are preserved, $d_K$ and $d_Q$ can be reduced. The following theoretical analysis provides insights into the selection of $d_K$."}, {"title": "3.2.1 THEORETICAL ANALYSIS ON dk", "content": "We first introduce the Johnson-Lindenstrauss Lemma (Johnson et al., 1986), which states that data in high-dimensional space can be projected into a much lower-dimensional subspace using random projections while approximately preserving the pairwise distances between the points. Since random projections can preserve locality, this provides justification for setting a smaller $d_K$ with trainable projection functions for keys and queries, which could also preserve locality.\nLemma 3.1. (Johnson-Lindenstrauss Lemma) For any 0 < \u0454 < 1 and any integer m, let d be a positive integer such that d = N(mm). Then for any set x of m points in $R^D$, there exists a map $f : R^D \\rightarrow R^d$ such that for all $x_i, x_j \\in X$,\n$(1 - \\epsilon)||x_i - x_j ||^2 \\leq ||f(x_i) - f(x_j)||^2 < (1 + \\epsilon)||x_i \u2013 x_j ||^2$\nThe following assumption then provides a mathematical depiction that attention weights are constrained within an m-dimensional simplex, and the learnable similarity function \u0393 outputs the attention scores, ensuring the most relevant tokens are emphasized during the information aggregation process. This reflects the primary goal of attention: to aggregate critical information for more accurate predictions."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "We evaluate ZETA's performance on several aspects: ZETA's ability to solve the synthetic MULTI-QUERY ASSOCIATIVE RECALL (MQAR) task (Arora et al., 2024a), long sequence modeling ability on the LOng Range ARENA (LRA) benchmark and auto-regressive language modeling on WIKITEXT-103. Then we conduct extensive analysis experiments: an ablation study examining the influence of dimensionality on attention model performance (Section 4.2), ablations on various Euclidean-based Softmax operators (Section 4.3), the empirical results of locality preservation using Z-order curves (Section 4.4) and an ablation study over the number of k in ZETA (Section 4.5)."}, {"title": "4.1 EMPIRICAL VALIDATION", "content": "Associative Recall. Associative recall tasks (Arora et al., 2024a) have been popular for testing the ability of language models to look up information in their context. Broadly, they involve feeding auto-regressive model pairs of key-value associations and then prompting the model to produce the correct completion upon being shown a previously seen key. The MULTI-QUERY ASSOCIATIVE RECALL (MQAR) task is a particular formulation of this task that requires the model to memorize multiple associations (Arora et al., 2024a). We evaluate the performance of various models on the Associative Recall task, a classical sequence-to-sequence task that requires the model to recall the first token associated with a target token after processing a long sequence. The task is tests the ability of models to capture long-range dependencies and to maintain information over time."}, {"title": "4.2 EFFECT OF VARYING dk ON ASSOCIATIVE RECALL TASK", "content": "We next evaluate the effect of different key-query dimensions dk on the Transformer model's performance for MQAR. The model dimensions are varied between {32, 64, 128, 256} while adjusting dk to values {1,2,3,8,32}. As shown in Figure 2b, the performance remains near-perfect even with low dk values, such as dk = 2. The model achieves close to 100% accuracy across all model dimensions, except for the smallest dimension (dk = 1), where performance slightly drops for lower model dimensions. This demonstrates that the Transformer is capable of handling long-range dependencies in the Associative Recall task, even with relatively low key-query dimensionality.\nThese results suggest that reducing dk does not significantly impair the model's ability to recall information in sequence tasks. In fact, maintaining a low dk can provide computational savings without sacrificing performance, especially when model dimensions are large. This indicates that while random projections\u2014such as those used in the Johnson-Lindenstrauss Lemma\u2014approximately preserve distances, trainable projection networks fk and fq can better adapt to task-specific data and more effectively retain locality even with a low dk. For instance, by setting dk as low as 3, we reduce it from the typical head dimension (normally 32, i.e. feature dimension as 512 with 8 heads). We can further mitigate information loss by configuring fk and fq as two-layer neural networks rather than single-layer ones."}, {"title": "4.3 PERFORMANCE OF EUCLIDEAN-BASED SOFTMAX OPERATORS", "content": "We further evaluate the performance of transformers with various Euclidean-based Softmax operators on MQAR. Specifically, we compare Negative Euclidean, Cauchy Softmax (our proposed"}, {"title": "4.4 LOCALITY PRESERVATION AFTER Z-ORDER CURVE PROJECTION", "content": "Next, we evaluate how well Z-order curve projections preserve locality across different dimensions and sample sizes. Specifically, we test the locality preservation by measuring the overlap between the top-64 nearest neighbors before and after projection, with sample sizes \u039d\u2208 {512, 1024, 2048}.\nFigure 3 shows the relationship between locality preservation and the dimensionality dk. As dk increases, the overlap between the top-64 nearest neighbors diminishes for all sample sizes, indicating a decrease in locality preservation. Lower dk values exhibit a higher level of locality preservation across all sample sizes. However, for larger sample sizes, such as N = 2048, the drop in locality preservation is more pronounced as the dimensionality increases. We select dk = 3 for ZETA.\nThese results highlight the importance of choosing an appropriate dk for maintaining locality, especially for larger datasets where higher dimensionality can lead to distortions in spatial proximity after a projection."}, {"title": "4.5 THE EFFECTS OF k IN ZETA", "content": "As an ablation study, we explore the effect of varying k in the attention mechanism of ZETA. The goal of this experiment is to analyze how different values of k influence the model's performance on the associative recall task across different model dimensions.\nFigure 2d shows ZETA to achieve near-perfect accuracy across all model dimensions (32, 64, 128, and 256) for different values of k ranging from 16 to 48. In most of our experiments, we set k = 32, as it provides a good balance between performance and computational efficiency. Interestingly, there is little variation in accuracy between different values of k, indicating that ZETA is robust to changes in this parameter."}, {"title": "4.6 EFFICIENCY BENCHMARKING", "content": "In order to better understand the effectiveness of ZETA, we further conduct an experimental study to demonstrate its computational efficiency in comparison to existing attention methods. In particular, we compare with a naive attention implementation from PyTorch (based on Vaswani et al. (2017)) as well as an IO-aware Flash-Attention (Dao et al., 2022; Dao, 2024). Our implementation is based on Triton and we conduct this benchmarking on a single NVIDIA A100 80GB GPU.\nTable 3 indicates the time required for both a forward pass as well as a forward-backward pass using our efficient ZETA implementation as well as the aforementioned attention implementations. We observe that our implementation significantly outspeeds a naive implementation of attention and do not suffer from out of memory issues while also outperforming Flash-Attention for long sequences,"}, {"title": "5 CONCLUSION", "content": "In this paper, we presented ZETA, a model designed to enhance the efficiency of top-k attention by leveraging Z-order curves for parallel token selection in one-dimensional space, reducing both time and space complexity to O(N log N). By carefully selecting the dimensionality of key and query pairs, ZETA effectively preserves relative distances, improving both locality and computational efficiency. Our comprehensive experiments on synthetic associative recall, LRA, and WIKITEXT-103 demonstrate that ZETA consistently matches or outperforms traditional attention mechanisms, making it particularly well-suited for long-sequence tasks that demand scalability and efficiency. Additionally, the introduction of the Adaptive Cauchy-Softmax mechanism enhances ZETA's flexibility, enabling it to handle long-range dependencies more effectively and efficiently. Overall, ZETA offers a robust, scalable, and efficient solution for sequence modeling, combining adaptive token selection with dynamic softmax to optimize performance across a range of tasks and datasets."}, {"title": "A THEORETICAL ANALYSIS", "content": "The recent paper, Reformer, proposed the Shared-QK Transformer that shares the linear projection layer for keys and queries. It reduces the number of parameters and memory space while not affecting the model performance. https://arxiv.org/abs/2001.0445a very simple but efficient technique.\nLemma A.1 (Johnson-Lindenstrauss Lemma). For any 0 < \u0454 < 1 and any integer m, let d be a positive integer such that d = N(mm). Then for any set x of m points in $R^D$, there exists a map $f : R^D \\rightarrow R^d$ such that for all $x_i, x_j \\in X$,\n$(1 - \\epsilon)||x_i - x_j ||^2 \\leq ||f(x_i) - f(x_j)||^2 \\leq (1 + \\epsilon)||x_i \u2013 x_j ||^2$.\nAssumption A.2. Let a \u2208 Am-1 be an element of the m-dimensional simplex, defined as\n$\\Delta_{m-1} = \\{a\\in\\mathbb{R}^m \\mid a_i \\geq 0, \\sum_{i=1}^m &i = 1\\}$. Assume that $\\widehat{\\text{attn}}$ equipped with a can achieve\nan optimal learnable similarity function I, where the attention scores are given by a\n$\\text{softmax}(\\Gamma(f_k (x_i), f_q(x)))$, such that \u0393 is trained to be optimal to have the minimal expected risk:\n$\\min_{\\alpha} ||\\widehat{\\text{attn}} (x, S_x; a) \u2013 y||$, where $\\widehat{\\text{attn}}$ denotes the attention-based hypothesis, x is the input, and\nSr is the context.\nLemma A.3. Let a \u2208 Rm, \u2200h1 : Rm \u2192 R and \u2200h2 : Rm \u2192 R. Assume that h\u2081(a) < h2(a). Then\n$\\min_a h_1(a) \\leq \\min_a h_2(a)$.\nProof. Let a1 = arg mina h\u2081(a) and a2 = arg minah\u2082(a). Then, h\u2081(a2) \u2264 h2(a2) due to\ncondition h\u2081(a) \u2264 h\u2082(a). We also have h\u2081 (01) \u2264 h\u2081(a2) due to condition a\u2081 achieving the\nminimum of h\u2081. To sum up, we have h\u2081(a1) \u2264 h2(a2)."}, {"title": "BAN ILLUSTRATIVE EXAMPLE OF EFFICIENT TOP-k SELECTION VIA Z-ORDER CURVE CHUNKING", "content": "In ZETA, the chunking process is a crucial step for efficient key-query matching in large-scale attention mechanisms. The process begins by projecting the high-dimensional keys into a one-dimensional space using Z-order curves, which preserve spatial locality. These projections create a linear representation of the keys, as shown in the second row of the figure.\nAfter projection, the keys are radix sorted in O(N) into ascending order of their Z-order integer values, enabling efficient binary searching in O(N) for the most relevant keys in the one-dimensional space. Sorting ensures that keys that are spatially close in the original multi-dimensional space remain close in the projected space, making the retrieval process computationally efficient.\nNext, the sorted keys are partitioned into chunks. Each chunk contains a fixed number of keys, and queries are matched with the keys within their respective chunks. This chunk-based structure facilitates parallel processing, where each query can efficiently search for its top-k nearest keys within a local subset of the sorted key space. For instance, in the figure, the 5th query retrieves its top-4 keys from the chunks containing its most relevant keys.\nThis chunking approach significantly reduces the computational overhead compared to searching the entire key space for each query, while leveraging the locality-preserving properties of Z-order curves. The result is a scalable and efficient mechanism for top-k selection in ZETA, ensuring both speed and accuracy for attention-based tasks.\nThe pseudo-code in Algorithm 1 outlines the ZETA Top-k Attention mechanism, which combines Z-order curve projections with chunk-based sorting to efficiently identify and retrieve the top-k nearest neighbors while maintaining causal constraints. It provides a structured approach to reduce computational overhead by limiting the search space to relevant chunks, ensuring both efficiency and adherence to sequence masking requirements."}, {"title": "C EXPERIMENTS DETAILS", "content": "The ZETA model configuration generally involves setting the number of chunks to values such as 4, 8, 16, 32 depending on the sequence length, which provides a flexible way to handle different input scales effectively. This chunking strategy facilitates parallelism in processing, allowing for efficient memory use and computational speedup during attention operations. The hidden dimension, dv, is typically set to 256 or 512 with 8 attention heads when working with LRA datasets. However, for larger and more complex datasets such as WIKITEXT-103, the hidden dimension is increased to dy = 768 with 12 attention heads to ensure that the model has sufficient capacity to learn intricate long-range dependencies effectively. Additionally, the dimensions of keys and queries are kept significantly lower at dk = dq = 3, which aids in reducing the computational burden and mitigates the \"curse of dimensionality\" while still preserving enough information for efficient attention computation. This choice of dimensions strikes a balance between model efficiency and effectiveness, making ZETA well-suited for long-sequence modeling tasks."}, {"title": "D I/O-AWARE ZET\u0391 \u039f\u03a1\u03a4\u0399\u039c\u0399\u0396ED WITH TRITON", "content": "Our Triton implementation of ZETA focuses on improving the efficiency of sparse attention through customized kernel programming. We leverage Triton to create specialized GPU kernels for top-k sparse attention. The sparse_topk_attention_kernel and its corresponding backward pass kernel sparse_topk_attention_backward_kernel are implemented using the Triton JIT (Just-In-Time) compiler. This approach allows for significant speedup by optimizing memory access patterns and reducing I/O overhead during the computation. The kernel is tuned to different configurations, like \"block_size\u201d and \u201cnum_warps\", which directly influence how GPU resources are allocated. Especially, we compute the mean vector of history tokens in the block of the current kernel, instead of computing global mean vectors, which effectively reduce overheads. he @triton.autotune decorator is used to evaluate multiple kernel configurations for optimal performance, making sure that GPU resources are well-utilized depending on sequence length and other parameters.\nOne key challenge addressed is efficient indexing for large tensors in the backward and forward computations. In Triton, indexing is achieved via program IDs, tl.program_id(), that are used to identify which part of the workload is being computed by each block or thread, ensuring that parallelism is effectively exploited. The Triton kernel computes Cauchy Softmax for top-k KV pairs for each query, employing a specialized kernel to access only the most relevant k keys during the attention process. This reduces the computational complexity compared to a full attention mechanism, and Triton's low-level bit manipulation operations (t1.load and tl.store) are used for fast data retrieval.\nThe sparse Attention Mechanism is computed by considering only the top-k keys per query, which significantly reduces the computational load. The sparse_topk_attention_kernel involves efficient gathering of keys and values based on top-k indices. The indices are pre-computed in a sorted order, which enables efficient retrieval without scanning the entire key space.\nThe Triton kernel also includes custom backward functions to handle the gradient flow. The backward kernel sparse_topk_attention_backward_kernel computes gradients for each parameter involved in the sparse attention operation, including q, k, v and the learnable parameter \u03b3. Triton's"}, {"title": "E GRADIENT DERIVATION FOR THE BACKWARD PASS OF TRITON ZETA", "content": null}, {"title": "E.1 UPDATED ATTENTION MECHANISM WITH SPARSE ATTENTION", "content": "We introduce sparse attention by computing values and keys from an index set Ii of top-k tokens specific to each query qi. The unnormalized attention scores are defined as:\n$S_{ij} = \\frac{1}{D_{ij} + \\epsilon}$, for $j \\in I_i$\nwhere:\n\u2022\n\u2022\n\u2022\n$D_{ij} = ||q_i - k_j||^2$\n\u03b5 = \u03b3\u00b2 is a trainable scalar parameter\ndij = Dij + \u03b5\nDefine:\nThe steps of the attention mechanism are:\n1. Compute Pairwise Distances:\n$D_{ij} = ||q_i - k_j ||^2$, for $j\\in I_i$\n2. Compute Unnormalized Attention Scores:\n$S_{ij} = \\frac{1}{d_{ij}}$, for $j \\in I_i$\n3. Normalize Attention Weights:\n$Z_i = \\sum_{j \\in I_i} S_{ij}$\n$A_{ij} = \\frac{S_{ij}}{Z_i}$, for $j \\in I_i$\n4. Compute Output:\n$o_i = \\sum_{j\\in I_i} A_{ij}v_j$\nOur goal is to compute the gradients of the output oi with respect to qi, kj, vj, and \u025b, considering the sparse attention."}, {"title": "E.2 GRADIENTS WITH RESPECT TO Q, K, V, AND E", "content": "The gradient of the output oi with respect to vj is:\n$\\frac{\\partial o_i}{\\partial v_j} = \\begin{cases} A_{ij}, &\\text{if } j \\in I_i \\\\ 0, &\\text{otherwise} \\end{cases}$\nDerivative of Dij with respect to qi and kj:\n$\\frac{\\partial D_{ij}}{\\partial q_i} = 2(q_i - k_j)$, for $j\\in I_i$\n$\\frac{\\partial D_{ij}}{\\partial k_j} = \\begin{cases} -2(q_i \u2013 k_j), &\\text{if } j \\in I_i \\\\ 0, &\\text{otherwise} \\end{cases}$\nDerivative of Sij:\nFirst, compute the derivative of Sij with respect to dij:\n$\\frac{\\partial S_{ij}}{\\partial \\delta_{ij}} = -\\frac{1}{ \\delta_{ij}^2}$, for $j \\in I_i$\nCompute the derivative of dij with respect to Dij and \u025b:\n$\\frac{\\partial \\delta_{ij}}{\\partial D_{ij}} = 1$, for $j\\in I_i$\n$\\frac{\\partial \\delta_{ij}}{\\partial \\epsilon} = 1$, for $j\\in I_i$\nNow, compute the derivative of Sij with respect to Dij and \u025b:\n$\\frac{\\partial S_{ij}}{\\partial D_{ij}} = -\\frac{1}{ \\delta_{ij}^2}$, for $j \\in I_i$\n$\\frac{\\partial S_{ij}}{\\partial \\epsilon} = -\\frac{1}{ \\delta_{ij}^2}$, for $j \\in I_i$\nDerivative of Sij with respect to qi and kj:\n$\\frac{\\partial S_{ij}}{\\partial q_i} = -\\frac{2(q_i - k_j)}{\\delta_{ij}^2}$, for $j \\in I_i$\n$\\frac{\\partial S_{ij}}{\\partial k_j} = \\begin{cases} -\\frac{2(q_i \u2013 k_j)}{\\delta_{ij}^2}, &\\text{if } j \\in L_i \\\\ 0, &\\text{otherwise} \\end{cases}$\nDerivative of Zi with respect to Sij:\n$\\frac{\\partial Z_i}{\\partial S_{ij}} = 1$, for $j\\in I_i$\nDerivative of Aij with respect to Sij:\n$\\frac{\\partial A_{ij}}{\\partial S_{ij}} = \\frac{Z_i - S_{ij}}{Z_i^2}$, for $j \\in I_i$"}, {"title": "E.3 SUMMARY OF GRADIENT COMPUTATIONS", "content": "1. Compute Dij, dij, Sij, Zi, Aij, and oi:"}, {"title": "F LIMITATIONS", "content": "Given that our method is a top-k attention mechanism, there are some shared limitations between our method and that of prior work that deals with attention, such as there still potentially being higher chances to ignore attention to important information (with low attention scores) than full attention methods given the use of only the top-k tokens."}, {"title": "G ADDITIONAL EXPERIMENTS", "content": null}, {"title": "G.1 ABLATION ON ATTENTION PERFORMANCE WITH VARYING dk ON LRA", "content": "We expand on the ablation study presented in Figure 2(b), focusing on the ListOps and Image tasks from the Long Range Arena (LRA) benchmark. Specifically, we examine the impact of varying the dimensionality of the keys and queries (dk) on attention performance. The results are summarized in Table 5.\nOur experimental findings indicate that the performance remains relatively consistent for dk \u2265 3, whereas a noticeable decline is observed for dk < 3. This supports our hypothesis that, unlike value vectors, the keys and queries predominantly encode positional information rather than intricate semantic features. As such, reducing dk to a small value allows us to effectively lower computational costs without incurring a significant loss in performance. This insight guided our selection of dk values in subsequent experiments."}, {"title": "G.2 PERFORMANCE OF ZETA USING DIFFERENT SIMILARITY METRIC", "content": "We utilize Euclidean distance for k-nearest neighbor (k-NN) searches to identify the top-k attended tokens, as it is particularly well-suited for this purpose. In contrast, dot-product similarity cannot be directly employed for k-NN searches without normalization, as highlighted in (Mao et al., 2024). Our experimental results also indicate that normalized dot-product similarity performs worse than Euclidean distance, as demonstrated in additional MQAR experiments below.\nSpecifically, Euclidean distance-based methods, such as Negative Euclidean with traditional softmax and Cauchy Softmax, consistently outperform dot-product-based methods for top-k attention when using a small dimensionality (dk \u2264 4), which is the setting adopted in ZETA."}]}