{"title": "Scalable Multi-agent Reinforcement Learning for Factory-wide Dynamic Scheduling", "authors": ["Jaeyeon Jang", "Diego Klabjan", "Han Liu", "Nital S. Patel", "Xiuqi Li", "Balakrishnan Ananthanarayanan", "Husam Dauod", "Tzung-Han Juang"], "abstract": "Real-time dynamic scheduling is a crucial but notoriously challenging task in modern manufacturing processes due to its high decision complexity. Recently, reinforcement learning (RL) has been gaining attention as an impactful technique to handle this challenge. However, classical RL methods typically rely on human-made dispatching rules, which are not suitable for large-scale factory-wide scheduling. To bridge this gap, this paper applies a leader-follower multi-agent RL (MARL) concept to obtain desired coordination after decomposing the scheduling problem into a set of sub-problems that are handled by each individual agent for scalability. We further strengthen the procedure by proposing a rule-based conversion algorithm to prevent catastrophic loss of production capacity due to an agent's error. Our experimental results demonstrate that the proposed model outperforms the state-of-the-art deep RL-based scheduling models in various aspects. Additionally, the proposed model provides the most robust scheduling performance to demand changes. Overall, the proposed MARL-based scheduling model presents a promising solution to the real-time scheduling problem, with potential applications in various manufacturing industries.", "sections": [{"title": "1 Introduction", "content": "Scheduling in semiconductor manufacturing is an exceptionally challenging task due to several complex characteristics. These include fluctuating demand, a variety of product types, intricate precedence constraints of operations, hundreds of machines spread across numerous workstations, a high frequency of re-entrant phenomena, frequent machine breakdowns and maintenance, and a vast number of possible routes for a job [1, 2, 3, 4]. This complicated and challenging scheduling task has been represented, at a high level of abstraction, as a flexible job shop scheduling problem (FJSSP)[5, 6] and diverse methods have been proposed to tackle the FJSSP [7, 8, 9].\nWith the increasing complexity and uncertainty of modern manufacturing systems, the FJSSP has garnered considerable attention in recent years [9]. This problem builds upon the conventional job shop scheduling problem by increasing flexibility and universality to address advanced manufacturing systems such as automobile assembly, chemical material processing, and semiconductor manufacturing [10, 11]. In the FJSSP, each operation can be processed on multiple compatible machines, each with a different processing time. As a result, the FJSSP is a formidable NP-hard problem [12].\nMany algorithms have been developed to address the FJSSP problem [5, 7, 8, 9]. However, most of them rely on an unrealistic assumption of a static scheduling environment where all the relevant information about the production environment is known in advance, and all future events are controlled as planned. The aim of these algorithms is to create the best deterministic scheduling plan without considering any future modifications. However, in complex manufacturing systems, dynamic events such as machine breakdowns, unscheduled maintenance, job insertion, and modification of order have become increasingly common [13]. As a result, a deterministic schedule is not usually executed as planned, deteriorating the production efficiency significantly. In addition, factory scheduling involves many hard operational constraints due to the complexity and uncertainty of semiconductor manufacturing systems. Therefore, developing a real-time scheduling method capable of handling these constraints is of utmost importance. Recently, reinforcement learning (RL) has achieved huge attention in this area due to its ability to learn a good policy by trial and error in a complex environment, thereby enabling real-time decision-making [6, 14, 15, 16].\nDynamic FJSSP (DFJSSP) has been extensively studied over the past decade, with dispatching rules and metaheuristics being commonly used techniques [17]. Dispatching rules are popular among practitioners due to their simplicity and time efficiency. However, their solutions may not be of high quality since no single best rule can be applied to all situations [16]. In contrast, metaheuristics can provide higher quality solutions by breaking down the dynamic scheduling problem into a series of static sub-problems [6]. Nevertheless, metaheuristics algorithms, such as genetic algorithm, tabu search, and particle swarm optimization, may suffer from significant computational cost due to their huge search spaces [18, 19]. Thus, their practicality is questionable.\nWith the rapid advancement of artificial intelligence, deep RL (DRL) has emerged as an important approach in real-time scheduling due to its practicality in adapting to numerous dynamic events in real-time [20]. Despite its performance improvement over dispatching rules and metaheuristics, existing"}, {"title": "2 Related works", "content": "RL has proven to be successful in various fields, including robot control [28], manufacturing [29], and gaming [24] in the past few decades. As a result, many researchers in the scheduling field have become interested in this learning technique [6, 14, 15, 16]. They have adopted the concept of training adaptable agents to choose the best action (usually dispatching rules) at decision points through numerous trial and error. More specifically, dynamic scheduling problems have traditionally been approached as Markov decision processes (MDPs), which are solved using classical RL algorithms such as Q-learning and SARSA [30]. For example, Aydin and \u00d6ztemel [31] applied the Q-learning algorithm to train an agent that selects the most appropriate rule among three popular dispatching rules: the shortest processing time (SPT) rule, the cost over time rule, and the critical ratio rule. Their goal was to determine the next job in real-time, with the aim of minimizing mean tardiness. Similarly, Wei and Mingyang [32] introduced an agent trained using Q-learning to select the best rule at each decision point after designing several composite dispatching rules. In [33], Q-learning was used to select the best dispatching rule among the FIFO rule, the earliest due date rule, and the SPT rule based on the number of waiting jobs and the"}, {"title": "3 Problem statement", "content": "The factory-wide DFJSSP assumes that jobs with the same product type have identical unit processing times for an operation. We also assume earlier arriving jobs are processed first when multiple jobs with the same product type are waiting to be processed. When a machine undergoes a change in operation mode or product type, a conversion time depending on the previous operation/product type and the new one is needed.\nWe describe factory-wide DFJSSP as a planning problem consisting of a period of $N$ shift. Given a product type $p$, we denote $K_p$ to be the number of lots of demand and $J_p$ to be the number of required operations. To produce a product $p$, we need to sequentially conduct a sequence of operations $\\{O_{p,j} | j = 1, 2, ..., J_p\\}$ where $O_{p,j}$ is the $j$-th operation of $p$. We can perform each operation $O_{p,j}$ on a set of compatible machines $M_{p,j} \\subseteq M$, where $M$ is the set of all machines. In general, most machines have the ability to produce multiple type of products and perform multiple operations. However, changing operations or product types requires conversion time. To this end, we denote $CO_{p_0,O_{00}, p_1, O_{01}}$ to be the time required to convert from operation $O_{00}$ for product type $p_0$ to operation $O_{01}$ for product type $p_1$. To better reflect a real-world factory environment, we also impose a conversion time threshold $TH$ on all machines for each shift. This is necessary since excessive conversion time can have negative impacts on machine utilization and productivity. Specifically, in a given shift of length $S$, the total conversion time for each machine cannot exceed $TH$. The time required to process the $k$-th lot of product type $p$ on operation $O_{p,j}$ is the product of the number of units in the lot $U_{p,k}$ and the unit processing time $PR_{p,j}$.\nIn real-world factory environments, both scheduled maintenance to prevent machine breakdowns and unscheduled maintenance due to unexpected breakdowns must be considered. For machine $l$, $SM_{l,t}$ is defined as 1 if $l$ is under scheduled maintenance at time $t$, and 0 otherwise. Additionally, $\\Omega_t$ represents the set of machines undergoing maintenance due to unexpected breakdowns, which is really a random event. For simplicity, we assume that unscheduled maintenance begins only after the current lot is processed.\nGiven the provided setting, we list the decision variables in factory-wide DFJSSP.\n\n*  $X_{p,j,k,l,t} = \\begin{cases} 1, & \\text{if } O_{p,j} \\text{ is assigned on } l \\in M_{p,j} \\text{ for the } k\\text{-th lot of product } p \\text{ at } t \\\\ 0, & \\text{otherwise} \\end{cases}$\n*  $C_{p,j,k}$: Completion time of $O_{p,j}$ for the $k$-th lot of product type $p$\n*  $PT_{l,t}$: product type setup of $l \\in M$ at $t$\n*  $OP_{l,t}$: operation setup of $l \\in M$ at $t$\n*  $Q_{l,t} = \\begin{cases} 1, & \\text{if } l \\in M \\text{ is not idle at } t \\\\ 0, & \\text{otherwise} \\end{cases}$\n\nVariable $X_{p,j,k,l,t}$ determines which machine performs $O_{p,j}$ for the $k$-th lot of product type $p$, while $Q_{l,t}$ characterizes the status of $M_l$. Specifically, $Q_{l,t}$ is set to 1 if $l$ is currently processing, undergoing a setup change, or undergoing scheduled or unscheduled maintenance; otherwise, it is set to 0, as detailed in (3). Whenever there is an idle machine, a decision should be promptly made for that machine. For example, if $Q_{l,t} = 0$ for an $l \\in M_{p,j}$, and $(k-1)$-th lot of $p$ has already been processed in $O_{p,j}$, we can set $X_{p,j,k,l,t} = 1$ to initiate $k$-th lot of $p$ using $l$ for $O_{p,j}$. Conversely, if no machines are available at time $t$, the decision is set by fixing $X_{p,j,k,l,t} = 0$. Formally,\n\n$X_{p,j,k,l,t} \\leq (1 - Q_{l,t}) 1_{M_{p,j}} (\\sum_{l \\in M_{p,j}} \\sum_{\\tau=1}^{t-1} X_{p,j,k-1,l,\\tau})\\qquad(1)$\n\nAdditionally, let $D_{p,k}$ be the due time of lot $k$ of product $p$. Our goal is to identify decision variables that minimize the number of delayed jobs, or equivalently, maximize the completion rate, within the planning horizon of $N$ shifts, where each shift comprises of $S$ decision points. In this paper, for simplicity, we assume that lots within the same product type contain nearly identical unit counts. Consequently, our objective function, formalized as (2), does not account for variations in unit numbers among lots of the same product type. In the following, the expectation is with respect to unscheduled maintenance captured by random events $\\Omega_t$. Equations and constrains (1) and (3) - (11) hold for any realization of $\\Omega_t$.\n\n$\\text{Minimize } \\mathbb{E} [\\sum_{p=1}^{K_p} \\sum_{k=1}^{P} H_{p,J_p,k}] \\text{ where } H_{p,J_p,k} = \\begin{cases} 1, & \\text{if } D_{p,k} < C_{p,J_p,k} \\\\ 0, & \\text{otherwise} \\end{cases} \\qquad(2)$\n\nsubject to\n\n$Q_{l,t} = \\mathbb{1}_\\Omega(t)SM_{l,t} + \\sum_{p=1}^{P} \\sum_{k=1}^{K_p} \\sum_{j=1}^{J_p} \\mathbb{1}_{M_{p,j}}(l) (\\sum_{\\tau=t-(PR_{p,j}U_{p,k} + CO_{PT_{l,t},OP_{l,t},p,O_{p,j}})}^{t} Z_{p,j,l,\\tau} X_{p,j,k,l,\\tau} + \\sum_{\\tau=t-PR_{p,j}U_{p,k}}^{t} (1 - Z_{p,j,l,\\tau})X_{p,j,k,l,\\tau}) \\forall l,t, \\qquad(3)$\n\nwhere $Z_{p,j,l,t} = \\begin{cases} 1, & \\text{if } PT_{l,t} = p \\text{ and } OP_{l,t} = O_{p,j} \\\\ 0, & \\text{otherwise} \\end{cases}$,\n\n$Q_{l,t} \\leq 1 \\forall l,t, \\qquad(4)$\n\n$PT_{l,t} = \\begin{cases} p, & \\text{if } X_{p,j,k,l,t} = 1 \\\\ PT_{l,t-1}, & \\text{otherwise} \\end{cases}, \\qquad(5)$\n\n$OP_{l,t} = \\begin{cases} O_{p,j} & \\text{if } X_{p,j,k,l,t} = 1 \\\\ OP_{l,t-1}, & \\text{otherwise} \\end{cases} \\qquad(6)$\n\n$C_{p,j,k} = \\sum_{l \\in M_{p,j}} \\sum_{t=1}^{NS} (Z_{p,j,l,t} X_{p,j,k,l,t}(t + PR_{p,j}U_{p,k}) + (1 - Z_{p,j,l,t})X_{p,j,k,l,t} (t + CO_{PT_{l,t},OP_{l,t},p,O_{p,j}} + PR_{p,j}U_{p,k})) \\forall p,j,k, \\qquad(7)$\n\n$C_{p,j+1,k} \\geq C_{p,j,k} + PR_{p,j+1}U_{p,k} + \\sum_{l \\in M_{p,j+1}} \\sum_{t=C_{p,j,k}}^{NS} (1 - Z_{p,j+1,l,t})X_{p,j+1,k,l,t}CO_{PT_{l,t},OP_{l,t},p,O_{p,j+1}} \\forall p,j,k, \\qquad(8)$\n\n$C_{p,j,k} - PR_{p,j}U_{p,k} \\geq C_{p,j,k-1} - PR_{p,j}U_{p,k-1} \\forall p,j,k, \\qquad(9)$\n\n$\\sum_{t=1}^{NS} \\sum_{l \\in M_{p,j}} X_{p,j,k,l,t} \\leq 1 \\forall p, j, k, \\qquad(10)$\n\n$TH \\geq \\sum_{p=1}^{K_p} \\sum_{j=1}^{J_p} \\sum_{\\tau=(n-1)S+1}^{nS} (1 - Z_{p,j,l,\\tau}) X_{p,j,k,l,\\tau}CO_{PT_{l,\\tau},OP_{l,\\tau},p,O_{p,j}}) \\leq S \\forall l,t \\in \\{(n-1)S+1, ..., nS\\}, n \\in \\{1,2, ..., N\\}. \\qquad(11)$\n\nEquations (3) and (4) make sure that a machine cannot process multiple jobs simultaneously. Additionally, (5) and (6) ensure that a machine only changes its setup when necessary, such as when a new product type or operation is assigned. If a setup change occurs, the machine requires both processing time and conversion time to complete the job, but only processing time is required if there is no setup change, as stipulated in constraint (7). Here, $NS$ represents the total number of decision points occurring within an episode, which spans the planning horizon of $N$ shifts. To ensure that jobs are executed in the proper sequence, (8) guarantees that an operation can only be executed once the previous operation for a job is completed. The first-in, first-out (FIFO) rule is applied to lots of the same product type that are waiting for the same operation, as described in constraint (9). Equation (10) guarantees that a lot can only be assigned to one machine once for an operation. If the lot requires reentry into an operation, that reentry is considered a separate operation in sequence $\\{O_{p,j} | j = 1, 2, ..., J_p\\}$. In this way, we can incorporate the reentry feature of factory scheduling into our model. Finally, constraint (11) ensures that a conversion cannot be executed if the cumulative conversion time exceeds $TH$ within a single shift."}, {"title": "4 Scheduling model for factory-wide DFJSSP", "content": "In this section, we introduce a scheduling model based on MARL designed to effectively address the factory-wide DFJSSP. The proposed model features two types of agents: followers and a leader. We begin by presenting the formulation for the followers and the leader, detailing their respective state and action representations along with the reward mechanisms. Next, we describe the simulation environment used for model training and training algorithm employed to train the agents within our scheduling model. Finally, to minimize production capacity loss by preventing catastrophic mistakes by agents, we introduce a rule-based conversion algorithm.\n\nFollower model\nOur proposed model designates each follower (i.e., a lower-level agent) the responsibility of managing its own operation. As depicted in Fig. 1a, the state of each follower includes information about the state of machines, work-in-process (WIP), and demand. Specifically, the machine state elements, such as the current setup ($PT_{l,t}$ and $OP_{l,t}$) and status ($Q_{l,t}$), are encoded using one-hot encoding to represent the current product and operation for a machine $l$ at time $t$. Both scheduled and unscheduled maintenance are represented identically by setting $Q_{l,t} = 1$. The cumulative conversion time for the current shift and the next available time are also tracked to predict future availability of the machine and products. The cumulative conversion time of machine $l$ at $t$ is calculated based on $\\{PT_{l,t_0}, OP_{l,t_0}, CO_{p,o,PT_{l,t_0},OP_{l,t_0}} | \\forall o, p, t_0 \\leq t\\}$, and the next available time is determined from $\\{U_{p,k}, PR_{p,j}, X_{p,j,k,l,t_0} | \\forall p, j, k, t_0 \\leq t\\}$. If scheduled maintenance is in progress, the next available time is set to the end of the maintenance period. Each agent maintains these machine state elements for all machines pertinent to the target operation. Additionally, WIPs awaiting the operation and both delayed and future demand lots for all available product types in the target operation are encapsulated in the state vector. These can be readily derived from $\\{D_{p,k}, C_{p,j,k}, X_{p,j,k,l,t_0} | \\forall j, k, l, t_0 \\leq t\\}$ for product type $p$. The underlying formulas are given in (3)-(11). Additionally, at the beginning of each shift, the leader distributes abstract goal vectors for each follower, and the abstract goal is incorporated into the follower's state to direct them towards achieving a higher reward. Details are provided later.\nProduct setup changes occur intermittently in real-world factories. Consequently, the followers must make crucial decisions regarding conversion for all machines assigned to the operation, as illustrated in Fig. 1b. To accomplish this, followers produce action vectors $a_{n,t}^o$ for all machines within the target operation $o \\text{ at } t$ in the $n$-th shift. If multiple machines belong to operation $o$, the follower $o$'s action is the concatenation of actions for all member machines. That is, $a_{n,t}^o = (a_{n,t}^{o,l} | l \\in \\text{ available machines for operation } o)$, where $a_{n,t}^{o,l}$ is the action for the available machine $l$ in operation $o$. Each action vector $a_{n,t}^{o,l}$, a one-hot encoded vector, represents two factors for a machine: whether or not to convert and the next product type. In Fig. 1b, $P_{o,i}$ is i-th available product"}, {"title": "4.1 Follower model", "content": "type in operation $o$ and $n_{p_o}$ is the number of available product types in operation $o$. If the first entry is activated, $P_{o,1}$ is selected as the next product type. Otherwise, there is no change made to the product setup. Let $P_{o,i,1}^l$ be set to 1 if $p_{o,i}$ is chosen as the next product setup for machine $l$ at $t$ via the activation of $(a_{n,t}^{o,l})_{p_{o,i},1}$. Conversely, $\\psi_{p_o,i,0}^{l,t}$ should be set to 1 either if $(a_{n,t}^{o,l})_{p_{o,i},1}$ is activated or if another product is chosen as the setup. By default, both $\\psi_{p_o,i,1}^{l,t}$ and $\\psi_{p_o,2,0}^{l,t}$ are initialized to 0. Then, for any machine $l$, we have\n\n$\\psi_{p_o,2,0}^{l,t} + \\psi_{p_o,i,1}^{l,t} = 1 \\qquad(12)$\n\nand\n\n$\\sum_{i=1}^{n_{p_o}} \\psi_{p_o,i,1}^{l,t} \\leq 1. \\qquad(13)$\n\nThe next product type is only relevant when a conversion is triggered, and conversions can only occur if there is WIP of the selected product in the operation. Let us assume that $p_{o,i}$ is selected and a conversion is initiated by an action vector for machine $l$ at time $t$, corresponding to operation $o = O_{p,j}$. If there is a waiting lot $k$, then $X_{p_{o,i},j,k,l,t+1}$ is set to 1; otherwise, it remains 0. Conversely, if no conversion is triggered but there is an upcoming lot $k$ for the current setup $p_{o,i}$ of machine $l$ at time $t$, then $X_{p_{o,i},j,k,l,t+1}$ is also set to 1.\nIt is not always necessary for all followers to take action since there may be a lack of available machines, as depicted in Fig. 2b. In this situation, only the followers who have access to at least one available machine are able to produce actions. The action of the follower $o$ at $t$ in the $n$-th shift, $a_n^{o,t}$, is determined according to\n\n$a_n^{o,t} \\sim \\pi^o (s_{n,t}^o, g_n^o), \\qquad(14)$\n\nwhere $\\pi^o$ represents the policy of the follower responsible for operation $o$, while $g_n^o$ denotes the goal vector for the follower during the $n$-th shift.\nIn the MARL-SR algorithm [27], synthetic rewards are utilized to account for each agent's contribution to the overall team reward. However, given that the demand for each product type is specified for each shift, we apply a more straightforward reward calculation method. At the end of each shift $n$, followers are penalized based on the number of delayed lots. Specifically, for the operation $o$, the reward $r_i^o$ is defined as follows:\n\n$r_i^o = - \\sum_{i=1}^{n_{p_o}} \\sum_{k=1}^{K_{r_o,i}} H_{p_o,i,j,k}, \\text{ where } H_{p_o,i,j,k} = \\begin{cases} 1, & \\text{if } D_{p_o,i,k} < C_{p_o,i,j,k} \\text{ and } D_{p_o,i,k} \\leq nS \\\\ 0, & \\text{otherwise} \\end{cases} \\qquad(15)$\n\nThis operational rewarding strategy is the default approach for our followers. The training environment is quite challenging, with the majority of actions being trivial and sparse rewards being provided. To supplement potentially insufficient training and prevent followers from generating inadequate actions that can result in catastrophic production capacity losses, we introduce a rule-based conversion algorithm in Section 4.4."}, {"title": "4.2 Leader model", "content": "Factory-wide DFJSSP requires a high level of coordination between multiple entities, such as jobs, machines, and operations. Traditional single agent based RL methods do not perform well due to the exponential growth of the joint action space as the number of entities increases [24]. To address this challenge, MARL has been proposed in which each agent aims to obtain the best policy for an entity or sub-problem [22]. This decentralization is based on the premise that achieving individual goals assigned to each agent is easier than attaining team success. However, individual success is meaningless unless team goals are achieved. Therefore, it is crucial to maximize team reward through cooperation while pursuing individual goals in MARL.\nOne approach of MARL is to train decentralized policies in a centralized manner, as this approach can offer valuable state information and eliminate communication barriers among agents [24, 25, 42, 43, 44]. Despite the increasing interest in centralized training with decentralized execution in the RL community, this method is not viable for use in advanced production systems due to the high synchronization demand between agents. Especially, factory-wide scheduling involves numerous precedence constraints that can be represented as a DAG. Consequently, this scheduling problem can be modeled as a multi-agent problem with DAG constraints, where each agent handles a specific operation. Essentially, every agent aims to maximize team rewards, considering the complex interrelationships between agents. A significant challenge is that rewards"}, {"title": "4.3 Environment and Training", "content": "In this section, we describe the simulation environment used to train RL agents, as summarized in Algorithm 1. Initially, we configure the production environment details, including the planning horizon (NS), product types (p), sequences of operations for each product type ($O_{p,j} \\forall p, j \\in 1, 2, ..., J_p$), sets of compatible machines ($M_{p,j} \\forall p, j \\in 1, 2, ..., J_p$), unit processing times ($PR_{p,j} \\forall p, j \\in 1, 2, ..., J_p$), scheduled maintenance details ($SM_{l,t} \\forall l, t$), and the conversion time threshold (TH). Each episode involves generating production plans for each shift over a predefined duration, detailing the required number of lots for each product type based on the incumbent policy. For each episode, it is assumed that demand information, specifying the required number of lots for each product type per shift, is provided. The objective of the RL agents in each episode is to meet this demand without delay. This demand information serves as the basis for reward calculation. Historical data contains only few demand observations; definitely insufficient for RL training. For this reason, we randomly generate demand for each episode with the underlying distribution calibrated from historical data, ensuring it accurately reflects the actual factory environment. We consider various factors such as the distribution of demand by product type, along with a range of operational constraints and conditions - including precedence constraints, processing times, and conversion times by product type - all based on real production datasets from a high-volume packaging and test factory. Furthermore, the initial settings of the factory environment, including the status of machines and WIP, are configured at the beginning of each episode. These initial settings lay the foundation for generating schedules based on the policies of the leader and the followers.\nAs is typical in RL, given an incumbent policy, at the beginning of the first shift of each episode, the leader creates and distributes goal vectors, as defined in (16), which guide the followers towards achieving high team rewards. At each decision point, machine statuses are updated to 'unavailable' based on both scheduled ($SM_{l,t} \\forall l$) and unscheduled maintenance information ($\\Omega_t$), which are"}, {"title": "4.4 Rule-based conversion algorithm", "content": "Although a follower only needs to consider a single operation, there are usually numerous product candidates to choose from during a conversion. However, executing a conversion to start a product type can result in a significant loss of capacity, as it may take away the chance of producing other products due to the limited conversion time. To mitigate the risk of such a loss, we propose a rule-based conversion algorithm.\nTo determine the appropriate product type based on the rule-based conversion algorithm, we first score the urgency of product types for each operation using Algorithm 2. We begin by calculating the required capacity and expected remaining capacity for each product type. For a given product type $p$ and operation $o$, the required capacity ($RC_{o,p}$) is the estimated total processing time for all target product WIPs in the target operation. The expected remaining capacity ($ERC_{o,p}$) is the estimated total processable time until the planning horizon ends of all machines that are currently processing $p$ in operation $o$. If the required capacity is greater than the expected remaining capacity, we consider that product to be urgent and set the urgency score to the required capacity. Furthermore, a product that is not currently being processed by any machine in the operation is considered much more urgent because it cannot be produced"}, {"title": "5 Experimental Evaluations", "content": "We implemented both the proposed model and the benchmark models using the PPO algorithm. For each agent, a fully-connected neural network, consisting of two layers with 256 units each and ReLU activation functions, is utilized for both the actor and the critic components. In our algorithm, the actor consists of the leader and the followers which all have such network architecture. We compared our proposed model with two benchmark models, which are described in detail later in this section. To determine the optimal hyperparameters for each model, we conducted both Kruskal-Wallis and ANOVA tests for reasonable parameter choices and groups corresponding to performance metrics. The results showed no significant differences across the four performance metrics- tardiness, number of changeovers, cumulative idle time, and completion rate against demand-within the parameter ranges tested: batch size (64-256), learning rate (0.0001-0.00001), discount factor (0.95-0.99), and lambda (0.9-0.95) for our proposed model and one of the benchmark models. For the other benchmark model, performance improvement was observed only when lambda was set to 0.95, with no statistical differences noted for the other hyperparameters. Consequently, we applied the hyperparameter settings listed in Table 1 to all models. Additionally, for each operation $o$ in shift $n$, the dimension of the corresponding goal vector $g_o$ is set to 3, as recommended in [27]. All models compared in this study are implemented using TensorFlow version 2.8 on a server equipped with an Intel i9-12900k CPU. Due to the shallow network architecture employed for both the actor and the critics, and the simulation's need for extensive sequential computations, GPU execution does not help.\nIn this study, we evaluate the effectiveness of our proposed method in real-world scheduling scenarios using two simulators built on different real production datasets for Intel's high volume packaging and test factory. We refer to the two production scenarios as short-term and long-term scenarios, as summarized in"}, {"title": "5.1 Implementation details"}, {"title": "5.2 Comparison with benchmarks", "content": "In this section, we compare the proposed model with existing RL-based dynamic scheduling models. Fig. 3 illustrates the total reward per episode for our proposed model and the two comparison models trained on the low-demand case of the long-term production scenario. In the figure, the moving window method is applied, where the team rewards are averaged over a sliding window of 10,000 episodes. The averaging window is shifted by one episode at a time, indicating a step size of one episode per move. The DRL-JSSP and DRL-DFJSS models initially perform well due to their reliance on human-designed dispatching rules. However, their performance remains almost stagnant throughout training, indicating that a combination of dispatching rules is insufficient for large-scale factory-wide scheduling. Conversely, the proposed model exhibits poor performance initially but demonstrates a substantial improvement during training. This implies that the proposed model can effectively address the challenges of"}, {"title": "5.3 Ablation study", "content": "To verify the effectiveness of each component proposed in this study, we conduct a quantitative ablation study. Specifically, we generate 100 production plans based on the long-term scenario outlined in Table 2. We then compare the performance of the following models:"}, {"title": "6 Conclusion", "content": "In this study, we present a novel scalable MARL model for dynamic scheduling problems in real-world factories. The proposed model addresses several hurdles faced in factory scheduling by combining operation-specific agent modeling, abstract goal-based leader-follower coordination, and a rule-based conversion algorithm, which are novel in the field of RL-based scheduling. In particular, we found that RL agents can make errors that lead to significant losses in production capacity. To mitigate this issue, a rule-based conversion algorithm can be used to supplement the agents' operational decisions. Our experiments, conducted on simulators constructed using Intel production datasets, demonstrate that the proposed model significantly outperforms state-of-the-art RL-based scheduling models that rely on human-made dispatching rules. This highlights the limitations of existing methods that use a combination of human-made rules and the potential of our approach to provide more efficient scheduling solutions for real-world factories.\nWhile our proposed model exhibits good performance and scalability, there is an issue that needs to be addressed in our future research. The model assumes that production-related information, such as operation list, machine list, and product types, remain fixed. However, in real-world factories, these settings can change from time to time, necessitating a retraining of the model. Although this"}]}