{"title": "A Semi-Supervised Text Generation Framework Combining a Deep Transformer and a GAN", "authors": ["Shengquan Wang"], "abstract": "This paper introduces a framework that connects a deep generative pre-trained Transformer language model with a generative adversarial network for semi-supervised text generation. In other words, the proposed model is first pre-trained unsupervised on a large and diverse text corpus with 24 layers. Then a simple GAN architecture for synthetic text generation is introduced, and Gumbel-Softmax is applied to handle the discreteness of tokens. The paper also shows a semi-supervised approach where real data is augmented with GAN samples, which is further used to fine-tune the Transformer model on the merged dataset. Detailed theoretical derivations are also included, outlining the proof of the min-max objective function, and an extensive discussion of the Gumbel-Softmax reparameterization trick.", "sections": [{"title": "Introduction", "content": "Text generation is a central task in natural language processing (NLP), with applications in data augmentation, language modeling, and automated content creation. Transformer-based models, such as GPT-2 [1], have demonstrated considerable success, drawing on self-attention mechanisms to capture long-range dependencies.\nGenerative Adversarial Networks (GANs) [2] train a generator $G$ and a discriminator $D$ in a min-max framework to produce samples indistinguishable from real data. Despite their achievements in continuous data domains, GANs encounter difficulties when generating discrete sequences, because the non-differentiability of discrete sampling obstructs gradient-based updates.\nA recognized technique to address this challenge is the Gumbel-Softmax reparameterization [3, 4], which provides a continuous relaxation of discrete variables. Data augmentation methods have shown particular value in scenarios with limited annotated text. The integration of GAN-generated samples into language model fine-tuning can be viewed as a semi-supervised strategy.\nThis paper is organized as follows. Section 2 reviews GPT-2 language modeling, emphasizing the auto-regressive objective. Section 3 revisits fundamental GAN theory and"}, {"title": "Transformer (GPT-2) Language Modeling", "sections": [{"title": "Auto-Regressive Objective", "content": "Consider a sequence of tokens $x = (x_1, x_2, ..., x_T)$. An auto-regressive language model (LM) represents the joint distribution over this sequence as\n$P_\\theta(x) = \\prod_{t=1}^{T} P_\\theta (x_t | x_1,..., x_{t-1})$, \nwhere $\\theta$ denotes the model parameters. Maximum likelihood estimation (MLE) is typically performed by minimizing the negative log-likelihood:\n$L_{LM}(\\theta) = -\\sum_{t=1}^{T}log P_\\theta(x_t | x_1,..., x_{t-1}).$\nIn practice, the cross-entropy formulation is often adopted for convenience:\n$L_{CE}(\\theta) = - \\sum_{t=1}^{T} \\sum_{v=1}^{V} 1\\{x_t = v\\} log(P_\\theta(v | x_1, ..., x_{t-1})),$ \nwhere V is the vocabulary, and $1\\{\\cdot\\}$ is an indicator function."}, {"title": "GPT-2 Architecture and Model Capacity", "content": "GPT-2 employs multi-head self-attention blocks with a causal (triangular) mask to ensure each token only attends to previous positions. In each block, a multi-head attention sub-layer and a position-wise feed-forward sub-layer are combined with layer normalization and residual connections. The original base model uses 12 Transformer blocks. The model described here is expanded to 24 layers by configuring n_layer = 24. This increases the representational capacity while retaining the same auto-regressive training objective."}]}, {"title": "GAN for Text Generation", "content": "A Generative Adversarial Network [2] optimizes two objectives: a generator $G(z)$ that maps noise $z \\sim p_z$ to samples, and a discriminator $D(x)$ that outputs a scalar in (0,1) denoting the likelihood that x is real."}, {"title": "Minimax Objective and Jensen-Shannon Divergence", "content": "The original GAN objective is:\n$\\min_G \\max_D [E_{x\\sim P_{data}} [\\log D(x)] + E_{z\\sim p(z)} [\\log(1 \u2013 D(G(z)))]]$.\nWhen both G and D have sufficient capacity and training converges, (7) has been shown [2] to minimize the Jensen-Shannon divergence between $P_{data}$ and the generator-induced distribution $P_G$. The generator thus learns to produce samples that resemble the real distribution."}, {"title": "Practical Loss Definitions", "content": "Training typically separates discriminator and generator updates."}, {"title": "Discriminator Loss.", "content": "$L_D(\\theta_D) = \u2212 E_{x\u223cp_{data}} [\\log D_\\theta(x)] - E_{z\u223cp_z} [\\log(1 \u2013 D_\\theta(G(z)))],$ \nwhere $\\theta_D$ parametrizes D."}, {"title": "Generator Loss.", "content": "$L_G(\\theta_G) = - E_{z\u223cp_z} [\\log D_\\theta (G_{\\theta_G}(z))],$\nwhere $\\theta_G$ denotes generator parameters, and $\\theta$ is the current discriminator parameter set."}, {"title": "Gumbel-Softmax for Discrete Outputs", "content": "Discrete token generation poses a challenge for gradient-based optimization, because direct sampling of one-hot tokens via argmax does not permit backpropagation. The Gumbel-Softmax trick [3,4] addresses this issue by offering a continuous relaxation.\nLemma 1 (Gumbel-Softmax Reparameterization). Let $u \\in \\mathbb{R}^K$ be logits for a categorical distribution with K classes, and let $g_i$ be i.i.d. samples from Gumbel(0,1). Then for temperature $\\tau > 0$,\n$y_i = \\frac{\\exp((u_i+g_i)/\\tau)}{\\sum_{j=1}^K \\exp((u_j + g_j)/\\tau)}$\nAs $\\tau \\rightarrow 0$, y becomes nearly one-hot, while gradients remain continuous with respect to the logits u.\nIn frameworks such as PyTorch, setting hard=True in F.gumbel_softmax discretizes the forward pass through a straight-through argmax while preserving a continuous gradient in the backward pass."}, {"title": "Theoretical Analysis and Derivations", "content": "This section presents an in-depth theoretical analysis of the convergence properties of the GAN formulation, the differentiable approximation provided by the Gumbel-Softmax reparameterization, and the rationale behind the data augmentation strategy for semi-supervised learning."}, {"title": "Convergence Analysis of the GAN Objective", "content": "Consider the standard GAN minimax objective as formulated in [2]:\n$\\min_G \\max_D V(D,G) = E_{x\\sim p_{data}} [\\log D(x)] + E_{z\\sim p(z)} [\\log (1 - D(G(z)))]$.\nAssuming that both the generator G and the discriminator D have sufficient capacity, the optimal discriminator $D^*$ for any fixed generator is given by:\n$D^*(x) = \\frac{P_{data} (x)}{P_{data}(x) + P_G(X)},$\nwhere $p_G$ denotes the distribution induced by the generator. Substituting $D^*$ into the minimax objective leads to:\n$V(D^*,G) = E_{x\\sim P_{data}} [\\log \\frac{P_{data} (x)}{P_{data}(x) + P_G(X)}] + E_{x\\sim p_G} [\\log \\frac{P_G(X)}{P_{data} (x) + P_G(X)}]$\n$= - \\log(4) + 2 JS(\\P_{data} || P_G),$ \nwhere JS( || .) represents the Jensen-Shannon divergence. Since the Jensen-Shannon divergence is non-negative and equals zero if and only if $P_{data} = P_G$, the global optimum of the minimax game is achieved exactly when\n$JS((\\P_{data} || P_G) = 0 \\Rightarrow P_{data} = P_G$.\nIn practice, the non-convex nature of the optimization problem may lead to local minima and phenomena such as mode collapse. Regularization strategies and gradient penalty techniques [4] are often employed to mitigate these issues."}, {"title": "Properties and Derivation of the Gumbel-Softmax", "content": "The discrete nature of tokens in natural language processing presents challenges for gradient-based optimization. The Gumbel-Softmax reparameterization [3,4] provides a differentiable approximation to categorical sampling. Let $u = (u_1, ..., u_K)$ denote the unnormalized log-probabilities of a categorical distribution. By adding independent samples $g = (g_1,..., g_K)$ from the Gumbel distribution (with location 0 and scale 1) and applying a softmax with temperature $\\tau > 0$, a sample is obtained:\n$y_i = \\frac{\\exp (\\frac{u_i+g_i}{\\tau})}{\\sum_{j=1}^K \\exp (\\frac{u_i+g_i}{\\tau})}$"}, {"title": "Semi-Supervised Learning via Data Augmentation", "content": "Following the establishment of a robust framework for text generation using GANs and the resolution of discrete sampling challenges via the Gumbel-Softmax, attention is turned to the semi-supervised learning strategy through data augmentation."}, {"title": "Synthetic Text Generation", "content": "Upon completion of GAN training (refer to Section 3), the generator G is utilized to synthesize text. Each latent noise vector z (typically sampled from a distribution such as N(0, I)) is transformed into a token sequence \u017e. When the parameter hard=True is employed, the continuous output of the Gumbel-Softmax is discretized into a near one-hot encoding. The resulting one-hot vectors are then mapped to integer token IDs and decoded into strings using the same tokenizer used for GPT-2."}, {"title": "Combining Synthetic and Real Data", "content": "An augmented dataset is constructed by combining a subset of real data $D_{real}$ with synthetic data $D_{synthetic}$:\n$D_{aug} = D_{real} \\cup D_{synthetic}$.\nSubsequently, the language model (GPT-2) is fine-tuned on $D_{aug}$ by minimizing the following auto-regressive loss:\n$L_{LM_aug}(\\theta) = -\\sum_{x \\in D_{aug}} \\sum_{t=1}^{\\mid x \\mid} log P_\\theta (x_t | x_1,..., x_{t-1}).$\nThis objective maximizes the conditional likelihood of each token given its preceding context, thereby promoting the learning of long-range dependencies. The integration of synthetic data increases the effective training set size and serves as an implicit regularizer, which helps to reduce overfitting when the quantity of real data is limited."}, {"title": "Illustrative Curves and Performance Comparisons", "sections": [{"title": "Performance Evaluation", "content": "Table 1 presents an example performance comparison. Perplexity is reported for the GPT-2 baseline (12-layer), the deep GPT-2 model (24-layer), and a semi-supervised version that incorporates GAN-generated samples."}]}, {"title": "Conclusion and Future Directions", "content": "This paper proposes a framework that combines the depth of Transformer-based language models with the simplicity of GANs, explaining GPT-2 and GANs in detail from a theoretical perspective. Among them, since the GAN part of this paper adopts a min-max objective to train the generator to minimize the Jensen-Shannon divergence between the real and fake data distributions, the autoregressive maximum likelihood objective of GPT-2 is applied to language sequence modeling. Gumbel-Softmax, an approximation of discrete sampling, is introduced, which enables text to be trained by backpropagation without discrete text tags. It also combines a semi-supervised method to further fine-tune the Transformer using synthetic data from GANs and a small amount of real data, helping the model achieve better results.\nIn practice, the quality of experimental results usually depends on two factors: how realistic the GAN synthetic output is and the level of data diversity presented by the training set. All of these and possibly more problems are related to mode collapse, gradient instability, or low-quality generation, which may reflect poor results. Future research may involve special variants of GANs for sequence data, such as SeqGAN or MaskGAN, the use of reinforcement learning strategies to enhance high-quality text synthesis, and filtering schemes for improving the quality of synthesized text."}]}