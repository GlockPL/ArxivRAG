{"title": "Risk Sources and Risk Management Measures in Support of Standards for General-Purpose AI Systems", "authors": ["Rokas Gipi\u0161kis", "Ayrton San Joaquin", "Ze Shen Chin", "Adrian Regenfu\u00df", "Ariel Gil", "Koen Holtman"], "abstract": "There is an urgent need to identify both short and long-term risks from newly emerging types of Artificial Intelligence (AI), as well as available risk management measures. In response, and to support global efforts in regulating AI and writing safety standards, we compile an extensive catalog of risk sources and risk management measures for general-purpose AI (GPAI) systems, complete with descriptions and supporting examples where relevant. This work involves identifying technical, operational, and societal risks across model development, training, and deployment stages, as well as surveying established and experimental methods for managing these risks. To the best of our knowledge, this paper is the first of its kind to provide extensive documentation of both GPAI risk sources and risk management measures that are descriptive, self-contained and neutral with respect to any existing regulatory framework. This work intends to help AI providers, standards experts, researchers, policymakers, and regulators in identifying and mitigating systemic risks from GPAI systems. For this reason, the catalog is released under a public domain license for ease of direct use by stakeholders in AI governance and standards.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been great interest in using \"GPAI models,\" predictive or generative models trained on a large corpus of data and usually obtained from the open internet, as a new way of building AI systems. Interest in the models is present in academia, in industrial research, and in the markets, all pointing to an urgent need to manage the emerging risks in these models or systems which deploy them.\nThis paper contributes to developing the sub-field of GPAI safety engineering. We write self-contained pieces of text that can be directly inserted into technical safety standards and codes of practice. These codes of practice and standards are currently being written by policymakers to support various regulatory ini-tiatives concerning GPAI. Specifically, our work is driven by the need to inform standards and codes of practice that are integral in the implementation of AI (and GPAI) safety engineering requirements outlined in the EU AI Act [70]. We aim to answer the following questions:\n1. What are the different sources of risk\u00b3, both in the short-term and long-term, arising from the development and deployment of GPAI models or systems?\n2. What methods, both experimental and state-of-the-art, are available to manage the systemic risks of GPAI systems at various points in the supply chain?\nThe answers to these questions take the form of sections that list individual \"risk sources\" and \"risk management measures.\" Generically, we refer to these as risk items, and the list we produce is the risk catalog. While the risk items are primarily meant for GPAIs, they can apply to some AI systems of different risk levels and those that are designed for a narrower scope. This catalog covers a broad range of the GPAI value chain but is not exhaustive. We envision this paper as a tool that intended readers can use in research and risk management for GPAI, as well as in writing codes and standards for this field."}, {"title": "1.1 Intended Audience", "content": "Our intended audiences are researchers, policymakers, regulators, safety engi-neers, standards experts, and risk management experts.\nWe do not claim that all risks are equally important to address (e.g., equally likely or severe); it is up to the relevant actors to perform risk analysis for their specific situations.\nTo provide structure to the catalog, we defined broad risk categories and placed each risk item into what we believe is a reasonable category. Our categorization"}, {"title": "1.2 Related Work on GPAI Safety Engineering", "content": "Our work most closely resembles the AI Risk Repository [193], which provides a comprehensive directory of AI-related risks. They propose two taxonomies a \"Causal\" Taxonomy concerned with the causal origin of risks and \"Do-main\" Taxonomy concerned with the specific domain the risk falls into. Similar work can also be found in AVID [164], an open-source database for submitting risks, and the Generative AI Misuse Taxonomy [130], which provides a taxon-omy based on empirical analysis of misuse of current generative AI technology. For comparison, our risk sources are analogous to the risks from the above databases.\nOur work differs from the above primarily by a) including risk management measures surveyed from the literature, and b) in providing our material in a public domain license, designed for direct cut-and-paste use in standardization or other regulatory efforts.\nOur work is also similar to MITRE Atlas [217] in that they both contain risk sources (\u201ctactics\u201d and \u201ctechniques\") and risk management measures (\u201cmitigation\"), but narrower in scope to exclusively focus on cybersecurity and AI secu-rity. Finally, OWASP AI Security and Privacy Guide [146], ENISA Multilayer Framework for Good Cybersecurity Practices in AI [152], and Google Secure AI Framework [86] provide guidelines on good practices in cybersecurity and AI security but do not provide a broader catalog of risks. For further reading on similar frameworks, we refer to [22, 151].\nThe standards and guideline documents ISO/IEC 42001, ISO/IEC 23894, ISO/ IEC 27001, and ISO/IEC 27002 also include lists of risk sources and/or risk management measures relevant to GPAI safety engineering. In this paper, we have mostly refrained from describing risk management measures already cov-ered in these standards."}, {"title": "1.3 Paper Structure", "content": "Section 2 describes the terms and format we use to describe risk items. Section 3 explains how this paper contributes to risk management and ongoing regulatory efforts. Meanwhile, the risk catalog spans Sections 4 through 10. Section 4 covers the risk sources and risk management measures related to the"}, {"title": "2 Terminology", "content": "We define several terms to contextualize the contents of this paper. The terms used here were chosen to align closely with the contents of the EU AI Act [70].\n\u2022 General-Purpose AI (GPAI) Model - An AI model that can perform a broad range of distinct tasks. The use of \"broad\" here can refer to both within and outside knowledge domains. For example, a biology model that can solve chemical synthesis equations and generate treatment plans is limited to the biological domain but is considered a GPAI model because of its broad use. Similarly, another GPAI model is a model that can translate text between languages and can converse with users beyond translation. Large Language Models (LLMs) are a prominent kind of GPAI Model.\nThe term \"GPAI model\" is somewhat specific to the EU AI Act as a regulatory initiative, which only applies to models put on the market [70]. For regulatory discussions in the UK and the US, the terms \"frontier model\" or \"foundation model\" are often used instead.\n\u2022 GPAI System - A system that includes at least one GPAI model as its component. One example is a wearable device that integrates a GPAI model to interpret signals from the external sensors. Another example is when the system is completely digital: the GPAI model can be a com-ponent that interacts with different API services. A concrete example of the latter is ChatGPT, which involves a GPAI model (initially GPT-3.5 [35], then GPT-4 [1]) interacting with a content filtering API so that the system serves as a chatbot. Another class of example is AutoGPT [189] or OpenAI 01 [144], which use multiple models or instances of a model in concert.\n\u2022 GPAI Provider - An organization that develops a GPAI model directly, or places a product containing a GPAI model on the market, is referred to as an upstream or downstream provider, respectively. Development refers to modifications made at any stage of the AI model lifecycle. Providers are primarily characterized by the fact that they are not the end-users of"}, {"title": "2.1 Risk Management Terms", "content": "To describe the risk management process, we use the following technical terms:\n1. Risk: the combination of the probability of an occurrence of harm and the severity of that harm [70].\n2. Harm: a negative event or negative social development entailing value damage or loss to people, property, and the environment [94].\n3. Risk source: an element which alone or in combination has the potential to give rise to risk [92].\n4. Risk management measure: a measure that is designed to lower risk, either when applied alone or in combination with other measures. This can include identification, mitigation, or prevention of risk sources or in-dividual risks relevant to a given system or class of systems.\nFrom the definitions above, it follows that risk sources are causally upstream of harms. Risk sources span a wide range of phenomena: some are purely technical or physical, while in sociotechnical systems, the source can involve actions that are performed (or fail to be performed) by human users or stakeholders. In some cases, risk sources may also be inadequacies of risk management measures, where they describe ways in which risk management measures may not achieve their intended outcome."}, {"title": "2.2 Formatting of Risk Items", "content": "The sections below contain structured lists of items where each item describes either a \"risk source\" or a \"risk management measure.\" Note that some risk management measures can be sources of risks themselves.\nEach item is generally formatted in the following way 8:\n\u2022 Item type (either Risk management measure or Risk source): item title\n\u2022 Description: Descriptive text, designed to answer at least the following question: \"What exactly is the nature of this risk source, or risk manage-ment measure?\" The descriptive text may also contain further information like examples and references to the literature. In general, the text is writ-ten in a style similar to that found in a dictionary or an introductory textbook.\nThe description text is explicitly designed to be descriptive without being prescriptive. The text describing a specific measure does not include any information that tells the reader if or when the measure should be used. While such prescriptive information is a crucial component for any action-able regulation, our work is formatted so that the \"what is it?\" discussion can be clearly separated from the \"when to use it?\u201d discussion."}, {"title": "2.3 State-of-the-art Knowledge", "content": "Formal safety regulations for various activities and industries, as created and enforced by governments, typically require that safety engineering processes (like those in Figure 1) are carried out while taking into account the applicable state-of-the-art. One example of such a requirement can be found in Article 8 of the EU AI Act [70].\nFor a more comprehensive definition of \"state-of-the-art,\" we refer to the defini-tion used by the European Commission in their standardization request to the European Standardisation Organisations to support EU policy on AI:\n\"State-of-art should be understood as a developed stage of technical capability at a given time as regards products, processes and services, based on the relevant consolidated findings of science, technology and experience and which is accepted as good practice in technology. The state of the art does not necessarily imply the latest scientific research still in an experimental stage or with insufficient technological maturity.\u201d [46]\nWhen it comes to the GPAI-related risk sources documented in this catalog, we believe that knowledge of these risk sources is to be treated as part of the"}, {"title": "3 Safety Engineering Process Steps Supported by This Paper", "content": "Safety engineering is widely understood as an iterative process in which the evaluation of risks associated with a product drives the selection of risk man-agement measures to ensure the safe use or release of the product. This process may also involve modifying the product itself. The iterative process should be ongoing, even after placement on the market.\nFigure 1 shows a sample graphical depiction of this process for the case of safety engineering leading to a positive or negative conclusion on releasing a GPAI model to its users or to the market, given a trained GPAI model. This depiction does not cover the initial steps to address the design and training phases of model development, nor does it detail any post-release steps.\nThe \"risk source\" content in this paper is specifically designed to support the engineering step (B), where \"risk sources\" need to be identified.\nThe key challenge in step (B) is to ensure that no potential risk source, or po-tential type of harm, is overlooked. Formal safety engineering processes always assume human fallibility. Therefore, absolute certainty can never be achieved in any of their steps. Safety engineering at the leading edge of technology is as much an art as it is a science, and practitioners typically use multiple tools to try to be as complete as possible. Hiring experienced specialists, who are able to map new situations to the knowledge of the general literature and of past incidents, is one such tool. Another is to use \"checklists\": to go over lists of risk sources, to significantly lower the probability that a risk is overlooked. The list of risk sources contained in this paper has been explicitly designed so that it can be used as a checklist with respect to GPAI risk sources. However, our risk catalog is not exhaustive and any derivative of it should not be assumed as exhaustive. Our risk items serve as a starting point to discover and map more risk sources.\nThe \"risk management measure\" content in this paper is specifically designed to support the engineering steps (C), (G), and (H). \u201crisk management methods,\" we wrote them from the perspective of either a member of the GPAI provider's model safety engineering team or the system safety engineering team. Readers may need to adjust the perspective depending on the context relevant to them."}, {"title": "4 Model Development", "content": "This section catalogs the risk sources and risk management measures related to the model development stage. We categorize these into the following groups: data-related, training-related, and fine-tuning-related."}, {"title": "4.1 Data-related", "content": "Risk management measure: Documentation of data collection, annotation, maintenance practices\nDataset collection, annotation, and maintenance processes can be documented in detail, including potential unintentional misuse scenarios and corresponding recommendations for data usage [80, 175, 99]. This contributes to transparency, ensures that inherent dataset limitations are known in advance, and helps in selecting the right datasets for intended use cases.\nRisk source: Difficulty filtering large web scrapes or large scale web datasets\nA large scale \"scraping\" of web data for training datasets increases vulnerability to data poisoning, backdoor attacks, and the inclusion of inaccurate or toxic data [76, 28, 48]. With a large dataset, filtering out these quality issues is very difficult or trades off against significant data loss.\nRisk management measure: Use of synthetic data\nSynthetic data refers to data that is not collected from the real world. It is used to train Al models as an alternative to, or augmentation of, natural data. Effective use and generation of synthetic data allows for more oversight by the trainer on the training dataset because they have more control over its statistical properties. Synthetic data can help against dataset bias by having more samples from a particular distribution or minority group. It can also help in privacy by having more samples to mask sensitive data [141].\nRisk source: Lack of cross-organizational documentation\nWhen sharing data between multiple organizations, documentation may be miss-ing or inadequate, making it difficult for other organizations to understand it.\nFor example, a lack of metadata or a change in schema by a collaborating party can result in an unusable dataset and wasted data collection efforts, or it can lead to misunderstandings about the dataset's limitations, resulting in downstream risks related to its use [173].\nRisk source: Manipulation of data by non-domain experts\nManipulating data (e.g., training data) carries a set of assumptions on how the data should appear and be used by those performing the manipulation. Common manipulations applied on data in the context of AI models include defining the ground truth label and merging different data formats or sources. People who have little or no expertise in the domain of the data performing such manipulations may render the data unusable or harmful to the development of the AI system [173]."}, {"title": "4.2 Training-related", "content": "Risk management measure (Experimental): Cost-inducing training of AI models specifically for malicious use\nAI models can be designed to make further post-training modifications (e.g., fine-tuning) too costly for malicious uses while preserving normal adaptability for non-malicious uses [88, 56].\nRisk management measure: Restrict web access during AI training\nDevelopers can restrict or disable AI systems' internet access during training. For example, developers can restrict web access to read-only (e.g., by disabling write-access through HTTP POST requests and access to web forms) or limit the access of the AI system to a local network [20].\nThis can prevent an AI system from overloading third-party web services by making too many requests, or posting inadequate or harmful content to the internet before being trained or fine-tuned not to produce harmful outputs.\nRisk source: Adversarial examples\nAdversarial examples [198, 83] refer to data that are designed to fool an AI model by inducing unintended behavior. They do this by exploiting spurious correla-tions learned by the model. They are part of inference-time attacks, where the examples are test examples. They generalize to different model architectures and models trained on different training sets.\nUnintended behavior can range from incorrect predictions with respect to the ground-truth prediction to outputs that are generally considered undesirable (e.g., toxic or harmful).\nFor example, when an autonomous vehicle's sensor sees a stop sign with an adversarial sticker, the vehicle's AI system may misclassify the stop sign as an indicator for the vehicle to accelerate [72].\nRisk management measure: Adversarial training\nAdversarial training [83] is a technique for training AI models in which adver-sarial inputs are generated for a model, and the model is then trained to give the correct outputs for those adversarial inputs.\nAdversarial training can involve adversarial examples generated by human ex-perts, human users, or other AI systems.\nRisk source: Robust overfitting in adversarial training\nAdversarial training can be affected by robust overfitting, where the model's"}, {"title": "4.3 Fine-tuning-related", "content": "Risk source: Ease of reconfiguring GPAI models\nGPAI models are often easily reconfigured for various use cases or have com-petencies beyond the intended use [78, 225]. They can be performed either by changing the weights of the model (e.g., fine-tuning) or by modifying only the model inputs (e.g., prompt engineering, jailbreaking, retrieval-augmented generation). Reconfiguration can be intentional (with the help of adversarial inputs) or unintentional (from unanticipated inputs to the model).\nRisk source: Unexpected competence in fine-tuned versions of the upstream model\nDownstream deployers may often fine-tune a GPAI model with specific deploy-ment-related datasets, to better suit the task. Fine-tuned upstream models can gain new or unexpected capabilities that the underlying upstream models did not exhibit [202, 126, 137]. These new capabilities may be unanticipated by the original model developer.\nRisk source: Harmful fine-tuning of open-weights models\nModels with publicly available weights can be fine-tuned for harmful activities by bad actors, using significantly fewer resources (in terms of time and money) compared to the original training cost [115, 78].\nRisk source: Fine-tuning dataset poisoning\nA deployer can poison the dataset used during the fine-tuning process [98] to induce specific, often malicious, behaviors in a model. This can be performed without having access to the model's weights. This poisoning can be difficult to detect through direct inspection of the dataset, as the manipulations may be subtle and targeted.\nRisk management measure: Data cleaning\nProviders can filter out the training dataset via multiple layered techniques, ranging from rule-based filters to anomaly detection via data point influence or statistical anomalies of individual data points [213].\nFor example, a data cleaning procedure can involve the use of filename checkers to detect duplicates or wrongly formatted data, which then moves to flagging the most influential data samples from the dataset via influence functions for anomaly detection.\nRisk management measure: Internal data poisoning diagnosis\nProviders can have an internal framework to identify what specific data poison-ing attack their model may be a victim of based on a set of symptoms, such as analysis of target algorithm and architecture, perturbation scope and dimen-sion, victim model, and data type [39]. This framework includes known defenses against the diagnosed attack, which providers can then apply to the model.\nRisk source: Poisoning models during instruction tuning\nAI models can be poisoned during instruction tuning when models are tuned"}, {"title": "5 Model Evaluations", "content": "This section catalogs the risk sources and risk management measures related to model evaluations (often called evals). We categorize them into the fol-lowing groups: general evaluations, benchmarking, red teaming, auditing, and interpretability/explainability. The subsection on general evaluations consists of items that are common to various evaluation techniques, while the other subsections are specific to their respective evaluation types."}, {"title": "5.1 General Evaluations", "content": "Risk management measure: Frequent testing when scaling model or dataset\nTesting models after significant increases in compute, data, or model parame-ters. Even relatively small changes to model or dataset size can introduce new"}, {"title": "5.2 Benchmarking", "content": "Risk management measure: Benchmarking\nBenchmarking is an evaluation method where different models are compared against a standardized dataset or a predetermined task. It allows comparison both across different models and over time, providing a reference point for model assessment. Benchmarks are usually open, where their question-answer pairs are publicly available [235].\nRisk management measure: Test robustness of GPAI system on relevant benchmarks\nVarious benchmarks [52, 236] have been developed to assess the robustness of GPAI systems when deployed in environments or scenarios that differ from their training conditions. These benchmarks typically evaluate the model's ability to handle variations in inputs, unexpected data distributions, or adversarial examples, aiming to ensure reliable performance outside the original training domain.\nRisk management measure: Frequent benchmarking to identify when red teaming is needed\nBenchmarks, once created, are inexpensive to apply but may be less informative than red teaming. One reason is that sensitive data (e.g., relating to CBRN-related capabilities) cannot be included in the public questions and answers of benchmarks. On the other hand, red teaming can be more accurate given par-ticipants with diverse attack strategies, but it requires more resources to execute than benchmarking. If there is a correlation between benchmarking and red-teaming scores, then employing frequent benchmarking during the development"}, {"title": "5.2.1 Benchmark Inaccuracy", "content": "Risk source: Benchmarks may not accurately evaluate capabilities\nBenchmarks of AI systems can both underestimate and overestimate the capa-bilities of those AI systems.\nUnderestimates can happen if an evaluation is not comprehensive enough, if the benchmark is saturated by existing models, or if the capabilities in question depend on a complicated setup, such as realistic computer programming tasks.\nOverestimates of capabilities can occur if an AI system is trained or fine-tuned on the contents of the benchmark, leading to overfitting.\nRisk source: Benchmark saturation\nBenchmark saturation refers to benchmarks reaching their evaluation ceiling. The tendency towards benchmark saturation has been demonstrated in various benchmarks [19]. When benchmarks reach or are close to saturation, they stop being effective measures for new models, as more nuanced capability gains might not be detected.\nRisk management measure: Statistical data quality reports for benchmarks\nIf a benchmark dataset is too large to allow for the identification and removal of all flawed instances, statistical reports on the data composition can be added. Random sampling of benchmark data points can be performed to evaluate and report the frequency and types of errors found [54]."}, {"title": "5.2.2 Benchmark Limitations", "content": "Risk source: Insufficient benchmarks for AI safety evaluation\nBenchmarks dedicated to measuring the performance of AI systems (e.g., on programming or math tasks) are more well-developed than those for assessing safety and harms in AI systems [234]. This gap can lead to AI systems excelling in specific tasks while exhibiting harmful behaviors that go undetected. More safety-related evaluation datasets can help in identifying previously overlooked undesirable model behaviors.\nAdditionally, statistical analysis of safety-related benchmarks shows that high scores correlate substantially with model performance [162]. This may allow for performance improvements to be misrepresented by model providers as safety improvements.\nRisk source: Underestimating capabilities that are not covered by benchmarks\nA lack of test coverage by benchmarks on specific abilities of a model can obscure the model's capabilities from both the developer and the user [160]. This can lead to a false sense of safety and trust due to a lack of understanding of the model's limitations."}, {"title": "5.2.3 Good Benchmarking Practices", "content": "Risk management measure: Informative and powerful benchmarks\nDevelopers of GPAI systems can select benchmarks that are difficult enough to be informative about the capabilities of their AI systems, and cover a large spec-trum of domains in order to signal areas where the GPAI system is performing poorly [120].\nSuitable benchmarks contain no label errors, are not vulnerable to being bench-mark contaminants, and are often audited by independent domain experts if they contain domain-specific questions. For multimodal GPAI systems, good benchmarks cover every modality, especially the interaction of different modal-ities.\nRisk management measure: Benchmark dataset auditing\nAuditing benchmark datasets allows for verification of the utility and limitations of the datasets [158]. This allows the provider to more accurately measure AI model capabilities and safety. Auditing includes the evaluation of such datasets by independent third-party organizations and the release of benchmark dataset metadata to the auditors.\nRisk management measure: Dynamic benchmarking\nDynamic benchmarks are benchmarks that can be continuously updated with new human-generated data. By having one or more target models \"in the loop,\" examples for benchmarking can be generated with the intent of fooling these target models, or to assess if these models express an appropriate level of uncer-tainty [103]. As the dataset in the benchmark grows, previously benchmarked models can also be reassessed against the updated dataset to reflect its perfor-mance in a more representative manner.\nExamples of such dynamic benchmarks include DynaSent [153] for sentiment analysis, LFTW [208] for hate speech, and Human-Adversarial VQA [182] for images."}, {"title": "5.3 Red Teaming", "content": "Risk management measure: Red teaming for GPAI system evaluation\nRed teaming refers to simulated adversarial attacks performed to identify and evaluate the model's vulnerabilities as well as its in-domain and out-of-domain performance.\nRisk management measure: Red team access to the final version of a model pre-deployment\nGranting red teams access to the final pre-release version of the model can help with identifying potentially dangerous model properties. These properties might not be identified if red teaming is only performed on earlier versions of the model, as late fine-tuning procedures may introduce new vulnerabilities.\nRed-teaming AI models before they are released to the public can reduce the"}, {"title": "5.4 Auditing", "content": "Risk source: Conflicts of interest in auditor selection\nConflicts of interest can arise if there is no independence in the auditor selection process or if the auditors are closely associated with the developer [123, 157]. In such cases, the conflict of interest can appear even if third-party evaluators are involved. In the case of external auditing, the potential candidates might be selected from a narrow group of auditors, or have conflicting financial incentives for whether to report model shortcomings publicly.\nRisk source: Auditor capacity mismatch\nAuditors may not be able to address all of the specific safety, performance, or validation needs. Reports of passing audits may be more inclusive than can be justified due to a lack of knowledge of specific risks and how they can be tested, or a lack of capacity to perform sufficiently rigorous testing.\nRisk source: Auditor failure\nAuditors may not publicly disclose risks they find, may be required to not pub-licize shortcomings, or may not receive sufficient cooperation from the relevant internal parties.\nRisk management measure: Pre-deployment access by third-party auditors\nPrior to full deployment of general-purpose AI models, a group of third-party"}, {"title": "5.5 Interpretability / Explainability", "content": "Risk source: Misuse of interpretability techniques\nInterpretability techniques, by enabling a better understanding of the model, could potentially be used for harmful purposes. For example, mechanistic inter-pretability could be used to identify neurons responsible for specific functions, and certain neurons that encode safety-related features may be modified to de-crease its activation or certain information may be censored [24].\nFurthermore, interpretability techniques can be used to simulate a white-box attack scenario. In this case, knowing the internal workings of a model aids in the development of adversarial attacks [24].\nRisk source: Misunderstanding or overestimating the results and scope of in-terpretability techniques\nThe results of explainability techniques are not free of bias and require careful interpretation. Users might develop a false sense of security or reliability if the resulting explanations align with their initial beliefs, leading to confirmation bias and an overestimation of abilities of these techniques [24].\nFor example, it has been demonstrated that some interpretability techniques in computer vision display object edges as salient in a heatmap, regardless of the underlying model [2]. This might create a false sense of confidence in the interpretability technique.\nRisk source: Adversarial attacks targeting explainable AI techniques\nAdversarial attacks can affect not only the model's output but also its corre-sponding explanation. Current adversarial optimization techniques can intro-duce imperceptible noise to the input image, so that the model's output does not change but the corresponding explanation is arbitrarily manipulated [61]. Such manipulations are harder to notice, as they are less commonly known compared to standard adversarial attacks targeting the model's output.\nRisk source: Biases are not accurately reflected in explanations\nExisting explainability techniques can be insufficient for detecting discrimina-"}, {"title": "6 Attacks on GPAIs / GPAI Failure Modes", "content": "This section catalogs the risk sources related to GPAI failure modes or attacks targeting GPAIs. Many of these apply mainly to LLM-based GPAIs, which share some common failure modes such as jailbreaks and trojans. These vulnerabilities often extend beyond GPAIs and fall into the broader field of adversarial machine learning. However, additional vulnerabilities may arise with the introduction of new modalities, longer context windows, or different encodings.\nRisk source: Jailbreak of a model to subvert intended behavior\nA jailbreak is a type of adversarial input to the model (during deployment) re-sulting in model behavior deviating from intended use. Jailbreaks may be gen-erated automatically in a \"white box\" setting, where access to internal training parameters is required for creation and optimization of the attack [238]. Other attacks may be \"black box\" - without access to model internals. In text based generative models, jailbreaks may sometimes be human-readable, with the use of reasoning or role-play to \"convince\" the model to bypass its safety mechanisms [231].\nRisk source: Jailbreak of a multimodal model\nCurrent generation multimodal (e.g., vision and language) GPAI models are vulnerable to adversarial jailbreak attacks. These attacks can be used to auto-"}, {"title": "7 Agency", "content": "This section catalogs the risk sources and risk management measures related to agentic AI systems. We categorize these into the following groups: goal-directedness, deception, situational awareness, self-proliferation, and persua-sion. These risk items are related to behaviors associated with agentic systems"}, {"title": "7.1 Goal-Directedness", "content": "Risk source: Specification gaming\nAI systems can achieve user-specified tasks in undesirable ways unless they are specified carefully and in enough detail. AI systems might find an easier unin-tended way to accomplish the objective provided by the user or developer, so that the actions by the AI system taken during its execution are very different from what the user expected [75, 191]. This behavior arises not from a problem with the learning algorithm, but rather from the misspecification or underspeci-fication of the intended task, and is generally referred to as specification gaming [43].\nRisk source: Reward or measurement tampering\nMeasurement and reward tampering occur when an AI system, particularly one that learns from feedback for performing actions in an environment (e.g., rein-forcement learning), intervenes on the mechanisms that determine its training reward or loss. This can lead to the system learning behaviors that are con-trary to the intended goals set by the developer, by receiving erroneous positive feedback for such actions. This has two main forms:\n1. Measurement tampering: The AI system interferes with the sensors or data collection processes that measure its performance, causing inaccurate feedback [167]. This is especially applicable in embodied AI systems that affect the physical world.\n2. Reward tampering: The AI system directly modifies its reward function or the process that calculates rewards [71]. This is especially applicable in non-embodied systems (e.g., coding assistants).\nMeasurement tampering can be viewed as a subset of specification gaming, and it might affect more capable AI systems.\nRisk source: Specification gaming generalizing to reward tampering\nIn some instances, specification gaming in a GPAI model can lead to reward tampering, without further training. This can mean that relatively benign cases of specification gaming (such as sycophancy in LLMs) can, if left unchecked, enable the model to generalize to more sophisticated behavior such as reward tampering [57].\nRisk source: Goal misgeneralization\nGoal or objective misgeneralization is a type of robustness failure where an AI system appears to be pursuing the intended objective in training, but does not"}, {"title": "7.2 Deception", "content": "Risk source: Deceptive behavior\nDeceptive behavior of an AI system consists of actions or outputs of the AI that reliably mislead other parties, including humans and other AI systems. This behavior can result in the targeted parties becoming convinced of, and acting on, false information [140].\nDeceptive behavior can occur due to several different reasons, including [148]:\n1. The developer trained, programmed, or configured the AI system to be-have deceptively.\n2. In AI systems capable of planning, deceptive outputs arise when the be-havior is optimal for the goals the AI systems have been configured or trained to achieve.\n3. The training data of the AI system contains repeated incorrect informa-tion, or the feedback from human raters on AI outputs is biased."}, {"title": "7.3 Situational Awareness", "content": "Risk source: Situational awareness in AI systems\nSituational awareness in GPAI systems refers to the ability to understand its context", "25": "n\u2022 Environment: Understanding and modeling the physical or digital envi-ronment in which it operates.\n\u2022 Context: Identifying whether it is in training", "User": "Understanding user expectations", "source": "Strategic underperformance on model evaluations\nGPAI developers often run evaluations ofual-use capabilities to decide whether it is safe to deploy. In some cases", "97": ".", "include": "n\u2022 During training involving user feedback (e.g., reinforcement learning from human feedback"}]}