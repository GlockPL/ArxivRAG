{"title": "BalanceKV: KV Cache Compression through Discrepancy Theory", "authors": ["Insu Han", "Michael Kapralov", "Ekaterina Kochetkova", "Kshiteej Sheth", "Amir Zandieh"], "abstract": "Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. The memory complexity of long-context LLMs is primarily due to the need to store Key-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache compression method based on geometric sampling process stemming from Banaszczyk's vector balancing theory, which introduces dependencies informed by the geometry of keys and value tokens, and improves precision. BalanceKV offers both theoretically proven and empirically validated performance improvements over existing methods.", "sections": [{"title": "Introduction", "content": "Transformer-based models are the foundation of ongoing artificial intelligence revolution. Their applications span a wide range of domains, from leading-edge language models (LLM) [AAA+23, Ant24] to text-to-image [RDN+22, Fir23, Mid22], text-to-video synthesis [Ope24b], coding assistant [Cop23] and even multimodal domain across text, audio, image, and video [Ope24a]. At the core of these models is the Transformer architecture, powered by the self-attention mechanism [VSP+17], which enables effective capture of pairwise correlations across tokens in an input sequence. As these models scale in size and context length [KMH+20], they face significant computational challenges, particularly in terms of memory usage.\nTo unlock the full potential of LLMs, it is essential to achieve both efficiency and accuracy in generating long sequences. Most large language models, along with multimodal and video models, adopt an autoregressive, decoder-only architecture. This architecture generates tokens sequentially, applying attention dynamically to each newly generated token. To avoid redundant attention score computations during the generation phase, these models maintain a KV cache, which stores the key and value embeddings of previously generated tokens in each attention layer. However, deploying autoregressive Transformers is computationally demanding. A major challenge lies in the substantial memory requirements, as the KV cache size scales with both the model size (i.e., the number of layers and attention heads) and, critically, the context size. Additionally, each model session typically requires its own dedicated KV cache, further exacerbating memory usage.\nThis growing demand has become a significant bottleneck, affecting both memory consumption and computational speed, particularly for models handling long context lengths. Therefore, reducing the KV cache size without compromising accuracy is essential to overcoming these limitations."}, {"title": "Related Works", "content": "Several approaches have been proposed to address this challenge. Architectural solutions, such as multi-query attention [Sha19], multi-group attention [ALTdJ+23], and sparse mixture of experts [DDZ+24], require modifying the model during the training stage and retraining it from scratch. As a result, these methods are not directly applicable as off-the-shelf solutions for reducing the KV cache without incurring substantial pretraining costs. Another line of research tackles the issue from a systems perspective, such as RAM offloading [SZY+23, SCB+24] or integrating virtual memory and paging strategies into the attention mechanism [KLZ+23].\nIn this work, we propose BALANCEKV, a novel approach based on discrepancy theory designed to significantly reduce the runtime and memory complexity for KV cache. Our method can shift conventional linear scaling to a sublinear scale with respect to context length. The core of our approach is a vector balancing algorithm from discrepancy theory that exploits the geometry of key and value tokens to deduce a small subset of them that well approximates the operations happening inside a self-attention layer."}, {"title": "Related Works", "content": "The direct approach to KV caching stores all key and value representations for every input token, leading to significant memory growth in long sequences, but several algorithmic ideas for improvement have been developed in the literature.\nEmbedding quantization. A simple yet effective approach to reducing KV cache size is quantizing the floating-point numbers (FPN) in the KV cache to fixed-point numbers with fewer bits. Several quantization methods have been proposed specifically for the KV cache [YYD+24, YKB+24, DCQW24, KZK+24, LYJ+24, HKM+24, ZYXS24, ZDH24]. However, such approaches still require memory space that linearly increases in sequence length.\nToken level pruning. A prominent line of work which is related to our paper focuses on token-level KV cache pruning where redundant or less important tokens get evicted from the cache [BPC20, ZSZ+24, LDL+24, XTC+23, ZHMK24, LHY+24]. Many of the works in this line have used accumulated attention scores to select important tokens in the cache [ZSZ+24, LHY+24, XTC+23]. Recent works extend those methods to an adaptive way of budget allocation across layer [CZG+24] and head [FCA+24].\nA very recent work, Lexico [KPCP24], applies techniques from sparse representation learning to compress the KV cache by learning a universal dictionary such that all key and value embeddings are represented as extremely sparse vectors within the learned dictionary. Unfortunately, this approach requires solving a computationally expensive matching pursuit algorithm for each key and value embedding, making Lexico relatively slow.\nAlgorithmic discrepancy theory. Banaszczyk's seminal works [Ban98, Ban12] establishing theoretical guarantees for vector set discrepancy have sparked research in the vector balancing problem [DNTTJ18]. This led to algorithmic developments in both offline [Ban10] and online [BJSS19, ALS21, KRR23] settings. The vector balancing problem has particular relevance to streaming and sublinear algorithms, as minimizing a dataset's discrepancy yields small subsets that effectively preserve the original dataset's properties. Recent works [PT20, CKW24] extend these discrepancy theory ideas to develop sublinear memory algorithms for kernel density estimation."}, {"title": "Overview of Our Contributions", "content": "For a sequence of tokens and their corresponding query, key and value embeddings in an input prompt, a natural approach to compressing the exact KV cache would be to first select key and value embeddings for a few important tokens in the sequence such as the first and latest few tokens, as motivated by [XTC+23], since important contexts are likely contained in them, and then select a small representative subset of key and value embeddings for the much longer middle portion of the sequence. This representative subset can be chosen by independently sampling and selecting key and value embeddings from middle portion, however this would ignore the semantic relationship of the tokens in the middle portion encoded in the geometric structure of the corresponding key and value embeddings. Several powerful practical heuristic approaches, discussed in Section 1.1, have been proposed recently, PyramidKV [CZG+24] and SnapKV [LHY+24] being the state of the art. These heuristics select a subset of key and value embeddings from the entire sequence using attention score computation, thereby taking relative semantic importance of input tokens into account. However, these techniques lack theoretical guarantees. Our main contribution is a principled geometric sampling method for selecting a representative subset of tokens, which matches or outperforms the above mentioned heuristics. Our contributions are as follows:\n1. In Section 3 we propose BALANCEKV, an algorithm for compressing the KV cache recursively using a geometric correlated sampling process based on discrepancy theory. We show that BALANCEKV provably approximates attention in the streaming setting under the bounded $l_2$ norm assumption (Theorem 3.1). Section 2 contains the formal problem formulation of streaming attention as well as a technical overview of the main results and techniques.\n2. In Section 4 we empirically evaluate our algorithm in two settings. In the first setting (see Section 4.1) we show our approach leads to a lower relative error for single layer attention approximation for open-source LLMs including Llama-3.1-8B-Instruct [DJP+24] and Ministral-8B-Instruct-2410 [Mis24] as compared to uniformly sampling keys and values in the cache. In the second setting we show that our provable method performs better compared to previous existing heuristic cache compression methods based on selecting a subset of key and value embeddings in the cache on the LongBench [BLZ+23] benchmark for long context understanding capabilities of large language models under various end to end tasks."}, {"title": "Technical Overview", "content": "In this section, we first set up the formal problem formulation that we tackle, followed by an overview of our techniques and our main results."}, {"title": "KV Cache Compression", "content": "Autoregressive Transformers generate tokens one by one and each depends on the previously generated tokens. When Transformers process a sequence of tokens, the attention mechanism operates by computing three types of embeddings for each token at every layer: query, key and value. The query and key capture how different tokens interact, while the value is the actual content to be aggregated. Such interactions are quantified by so-called attention scores, obtained by applying the softmax to the inner product between the query of a given token and the keys of all others. These scores determine how much each previous token's value contributes to the final output. Once the keys and values are computed for a given token, they do not need to be recomputed when generating subsequent tokens.\nFormally, suppose we have a stream of query, key and value embeddings $(q_1, k_1, v_1), ..., (q_n, k_n, v_n)$, that is the j-th token is represented as a triplet of $(q_j, k_j, v_j)$ where $q_j, k_j, v_j \\in \\mathbb{R}^d$ for all $j \\in [n]$. Let $K_j, V_j \\in \\mathbb{R}^{j \\times d}$"}, {"title": "KV Cache Compression", "content": "be matrices defined by stacking those keys and values in their respective rows. To compute the following at every step j to generate j + 1 token, is called the streaming attention problem:\n$\\operatorname{Attn}(q_j, K_j, V_j) := \\operatorname{softmax}\\left(\\frac{K_j^Tq_j}{\\sqrt{d}}\\right)V_j$.\nKeeping all of the key-value pairs in the cache is prohibitively expensive, especially for long sequences. Instead, we opt for approximate computation by sampling a few key-value pairs. Specifically, our goal is to construct an algorithm that at every time step j computes an estimator $z_j$ for $\\operatorname{Attn}(q_j, K_j, V_j)$ in sublinear in n time and memory. In particular for given precision $\\varepsilon$, $z_j$ should satisfy the following error constraint:\n$\\|z_j - \\operatorname{Attn}(q_j, K_j, V_j)\\|_2 \\leq \\varepsilon \\cdot \\left\\| \\operatorname{softmax}\\left(\\frac{K_j^Tq_j}{\\sqrt{d}}\\right) \\right\\|_2 \\cdot \\|V_j\\|_F$.\nA sublinear in n time and memory algorithm to compute $z_j$ will require knowledge of significantly less key-value pairs than $K_j, V_j$, thus reducing the size of the KV cache needed to store them. In the next section we discuss how we will construct such an estimator $z_j$ at a high level."}, {"title": "SOFTMAXBALANCE: Attention Approximation via Discrepancy Theory", "content": "We now start with presenting the main ideas of our approach. By the definition of softmax, Equation (2) can be written as\n$\\operatorname{Attn}(q_j, K_j, V_j) = \\frac{1}{Z_j} \\exp\\left(\\frac{K_j^Tq_j}{\\sqrt{d}}\\right) \\cdot V_j,$\nwhere for a matrix A we write exp(A) to denote entry-wise application of the exponential function to A and\n$Z_j = \\sum_{i \\in [j]} \\exp\\left(\\frac{k_i^Tq_j}{\\sqrt{d}}\\right)$.\nOur approach to approximate $\\operatorname{Attn}(q_j, K_j, V_j)$ consists of two subroutines which approximate:\n1. Softmax normalization $Z_j = \\sum_{i \\in [j]} \\exp\\left(\\frac{k_i^Tq_j}{\\sqrt{d}}\\right)$.\n2. Matrix-vector product between $V_j$ and $\\exp\\left(\\frac{K_j^Tq_j}{\\sqrt{d}}\\right)$.\nTo understand our main idea, suppose we are at the end of the stream (i.e., j = n) and the KV cache contains all key-value pairs $(k_1, v_1), ..., (k_n, v_n)$. Then for an arbitrary query $q_n$ we aim to approximate the matrix-vector product\n$\\exp\\left(\\frac{K_n^Tq_n}{\\sqrt{d}}\\right)V_n = \\sum_{i \\in [n]} \\exp\\left(\\frac{k_i^Tq_n}{\\sqrt{d}}\\right)v_i$\nby choosing a subset of the rows of $K_n$ and $V_n$ of size at most n/2 which corresponds to a compression rate of 0.5. Suppose we can design an algorithm which splits the set C of all keys and values into two groups C' and C\\C' so that the matrix-vector product function for any query vector $q_n$ is roughly equal over C' and C\\C' that is informally,\n$\\sum_{\\{k,v\\} \\in C'} \\exp\\left(\\frac{\\langle k, q_n \\rangle}{\\sqrt{d}}\\right) v \\approx \\sum_{\\{k,v\\} \\in C\\setminus C'} \\exp\\left(\\frac{\\langle k, q_n \\rangle}{\\sqrt{d}}\\right) v$."}, {"title": "SOFTMAXBALANCE: Attention Approximation via Discrepancy Theory", "content": "Then, we are able to approximate the matrix-vector product function with either one of the sums above since informally:\n$\\sum_{\\{k,v\\} \\in C} \\exp\\left(\\frac{\\langle k, q_n \\rangle}{\\sqrt{d}}\\right) v \\approx 2 \\sum_{\\{k,v\\} \\in C'} \\exp\\left(\\frac{\\langle k, q_n \\rangle}{\\sqrt{d}}\\right) v$.\nTherefore, it would suffice to keep the smaller subset of C' and C\\C' as the desired subset of key value embeddings and discard the rest. If we wanted to compress the KV-cache to a smaller size by a factor $2^T$ for some T, we would recursively compress the selected subset using the same procedure T \u2013 1 more times.\nA similar goal is captured by the vector balancing problem studied extensively in discrepancy theory - given a set of vectors $C = \\{k_1,...,k_n\\} \\subset \\mathbb{R}^d$ with $\\|k_j\\|_2 \\leq 1$ for all j, partition them into two groups $C',C\\setminus C'$ such that for any $q\\in \\mathbb{R}^d$ it holds $\\sum_{k\\in c'}\\langle k,q\\rangle \\approx \\sum_{k\\in c\\setminus c'}\\langle k, q\\rangle$ with high probability. The Self-Balancing Walk algorithm [ALS21] is a breakthrough result which gives an algorithm for the above vector balancing problem. However we need to develop an algorithm for the vector balancing problem with respect to function $\\exp\\left(\\frac{\\langle k, . \\rangle}{\\sqrt{d}}\\right) v$ instead of the inner product function $\\langle k, . \\rangle$.\nOur first contribution is to develop an algorithm for our task, building upon the result from [ALS21], which essentially randomly partitions the set of keys and values C into C' and C \\ C' such that the following holds with high probability under the assumptions that the norms of the query and key embeddings are bounded,\n$\\sum_{\\{k,v\\} \\in C'} \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) v - \\sum_{\\{k,v\\} \\in C\\setminus C'} \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) v \\leq O\\left(\\sqrt{\\log(nd)}\\frac{n}{\\sqrt{d}} \\cdot \\max_{j\\in [n]} \\|v_j\\|_2\\right)$.\nWe refer to this algorithm as SOFTMAXBALANCE, its formal guarantees are presented in Theorem 3.2 and its pseudocode is presented in Algorithm 2. Theorem 3.2 shows that SOFTMAXBALANCE succeeds to divide C into subsets C' and C\\C' which are balanced with respect to function $\\exp\\left(\\frac{\\langle k, . \\rangle}{\\sqrt{d}}\\right) v$ up to an error which only has logarithmic dependence on the size of C. In addition, SOFTMAXBALANCE can accept as input value vectors of arbitrary dimension s. Therefore, if instead of the value vectors $V = \\{v_1,..., v_n\\} \\subset \\mathbb{R}^d$ we input the set of scalars $V = \\{1, ..., 1\\}$, we will get an algorithm for the vector balancing problem with respect to function $\\exp\\left(\\frac{\\langle k, . \\rangle}{\\sqrt{d}}\\right)$. This implies that we can use SOFTMAXBALANCE to compress the KV cache to even approximate the softmax normalization $\\sum_{i \\in [n]} \\exp\\left(\\frac{\\langle k_i, q \\rangle}{\\sqrt{d}}\\right)$.\nWe now discuss how to use SOFTMAXBALANCE for streaming attention approximation, that is to use its guarantees to compute an estimator $z_j$ satisfying Equation (2)."}, {"title": "BALANCEKV: Implementing SOFTMAXBALANCE in Streaming for Cache Compression", "content": "For a sequence of n tokens and a given memory budget of t <n, we want to design a procedure which applies SOFTMAXBALANCE to compress the exact KV cache from storing n key-value embeddings to at most t in the streaming setting and can compute an estimator $z_j$ satisfying Equation (2) for all steps j in the stream. In the streaming setting one needs to consider the following aspects. As described in the previous section, one iteration of SOFTMAXBALANCE only allows one to select a n/2 sized subset of n key-value embeddings, which is higher than the desired budget of t embeddings. This can be easily mitigated by recursively applying SOFTMAXBALANCE 2log(n/t) times, each time halving the set of key-value embeddings. However, this cannot be implemented in the streaming setting as we have a limited memory budget of t which prohibits us from storing all key-value embeddings during recursion.\nTo deal with this, we use the classical merge and reduce technique used in the design of streaming algorithms [BHM+21, MG82, GLPW16]. MERGEANDREDUCE algorithm is a recursive binary tree-based approach"}, {"title": "BALANCEKV: Implementing SOFTMAXBALANCE in Streaming for Cache Compression", "content": "that allows one to implement SOFTMAXBALANCE recursively in a streaming setting with the total memory not exceeding $O^*(t)$, where $O^*(\\cdot)$ supresses polynomial in d, log n factors, under the assumption that the norms of queries and keys are bounded. The guarantees of MERGEANDREDUCE are presented in Theorem 3.3, its pseudocode in Algorithm 3 and a visual representation in Figure 1. If the norms of all value embeddings in the stream are the same up to constant factors, that is for all i, j \u2208 [n] 0.5 \u2264 ||Vi||2/||vj||2 \u2264 2, then the outputs of MERGEANDREDUCE can be used to construct an estimator $z_j$ satisfying our attention approximation guarantee of equation Equation (2) with precision $\\varepsilon$ for $t = O^*(1/\\varepsilon)$. However, the value embeddings may have drastically different norms.\nOur main algorithm BALANCEKV (pseudocode in Algorithm 1) deals with this issue by grouping the key-value embeddings in the stream according to the norms of the value embeddings, running a separate instance of MERGEANDREDUCE on each group, and combining the outputs of each instance of MERGEANDREDUCE. BALANCEKV constructs a final estimator $z_j$ satisfying equation Equation (2) with precision $\\varepsilon$ only using $O^*(1/\\varepsilon)$ memory and $O^*(1/\\varepsilon^2)$ runtime per every step j of the stream, assuming the norms of query and key embeddings are bounded. Existing methods [ZHMK24] sub sample keys and values independently in the cache, and thus they have a $1/\\varepsilon^2$ dependence on $\\varepsilon$ in the total memory. The guarantees of BALANCEKV are presented in Theorem 3.1."}, {"title": "Main Theoretical Results", "content": "Our main algorithm for streaming attention approximation is BALANCEKV. It takes in as input a stream of n tokens $(q_1, k_1, v_1), (q_2, k_2, v_2), ..., (q_n, k_n, v_n)$ and at every step of the stream outputs an estimate $z_j$ to Attn(qj, Kj, Vj) (see Equation (1) for the definition of Attn(.)) satisfying Equation (2) with precision $\\varepsilon$. Assuming that the l\u2082 norms of $q_j, k_j$ are at most r for all j, BALANCEKV uses total space $O^*(e^{2r^2/\\sqrt{d}} \\cdot 1/\\varepsilon)$ and uses $O^*(e^{4r^2/\\sqrt{d}} \\cdot 1/\\varepsilon^2)$ runtime at each step j of the stream to output $z_j$. Our main theorem is as follows.\nTheorem 3.1. For any r,\u025b > 0, any positive integers n,d, any set of tokens $(q_1, k_1, v_1), (q_2, k_2, v_2), ..., (q_n, k_n, v_n)$ where $q_j, k_j, v_j \\in \\mathbb{R}^d$ satisfy $||q_j||_2, ||k_j||_2 \\leq r$ for all j, consider an invocation of BALANCEKV with\n\u2022 batch size $t = O^*\\left((e^{2r^2/\\sqrt{d}})/\\varepsilon\\right)$\n\u2022 compression rate $2^{-T}$ with $T = \\log(n/t)$.\nThen BALANCEKV outputs a vector $z_j$ satisfying Equation (2) with probability at least 1 \u2013 1/poly(n) at every step j of the stream. It uses total memory $O^*\\left((e^{2r^2/\\sqrt{d}}/\\varepsilon)\\right)$ across all steps of the stream and runtime of $O^*\\left((e^{4r^2/\\sqrt{d}}/\\varepsilon^2)\\right)$ per step of the stream.\nBALANCEKV is described in Algorithm 1. At its core BALANCEKV relies on our main discrepancy based algorithm, namely SOFTMAXBALANCE- see Section 3.1 for details on SOFTMAXBALANCE. BALANCEKV uses the output of SOFTMAXBALANCE to compute estimates of the numerator and denominator of Attn(qj, Kj, Vj) and returns the desired attention approximation $z_j$ for each j\u2013 see line 16 in BALANCEKV. There are two subtleties, however. First, it is important to bucket tokens in the stream according to the norm of the value vectors see line 4. Second, a direct application of SOFTMAXBALANCE would require too much space. To ensure small space usage, we apply a classical streaming technique, namely the MERGEANDREDUCE algorithm on top of SOFTMAXBALANCE to reduce the space consumption. The space reduction achieved by MERGEANDREDUCE is by running a logarithmic number of copies of SOFTMAXBALANCE in a tree-like fashion: see Section 3.2 for details."}, {"title": "SOFTMAXBALANCE", "content": "We now present our main discrepancy based compression algorithm SOFTMAXBALANCE. Given a sequence of key/value embeddings $C = \\{(k_1, v_1), ... (k_n, v_n)\\}$ (with key and value embeddings having possibly different dimensions), the goal of SOFTMAXBALANCE is to produce a partition of C into subsets C', C\\C' such that for any query $q \\in \\mathbb{R}^d$ we have that $\\sum_{\\{k,v\\} \\in C'} \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) v \\approx \\sum_{\\{k,v\\} \\in C\\setminus C'} \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) v$ with high probability. Without loss of generality assume that |C'| < |C|/2; we can output $2\\sum_{\\{k,v\\} \\in C'} \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) v$ as an approximation to $\\sum_{\\{k,v\\} \\in C} \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) v$, thus achieving a factor 2 compression. Its description is presented in Algorithm 2. We note that while SOFTMAXBALANCE takes as input a sequence of key and value embeddings, it can nevertheless be used to compute the softmax normalization: we simply run it on the keys, with the corresponding value vector one-dimensional and all equal to 1 see line 14 of BALANCEKV, where SOFTMAXBALANCE is called within the corresponding invocation of MERGEANDREDUCE with value vectors"}, {"title": "MERGEANDREDUCE", "content": "As briefly mentioned above in Section 3, MERGEANDREDUCE is a streaming version of SOFTMAXBALANCE. The idea is to partition the stream of tokens into batches of size t, split the batches into pairs, apply SOFTMAXBALANCE to the batches, merge the outputs of SOFTMAXBALANCE on the batches in one pair, and then repeat recursively on the new set of batches T - 1 times if the desired compression rate is $2^T$ see"}, {"title": "MERGEANDREDUCE", "content": "We run all the levels of recursion in parallel, which means that at any time step we keep only one (possibly empty) batch per each level of recursion, and we refer to these batches as C\u00ba,...,CT-1. We store the points which survived all the levels of recursion in the memory cell called CT.\nIf we set batch size t to be about 1/\u025b (see Theorem 3.3 below for the more precise setting), we obtain a streaming algorithm that approximates $\\sum_{i=1}^{j-1} \\exp\\left(\\frac{\\langle k_i, q_j \\rangle}{\\sqrt{d}}\\right) v_i$ at any point j in the stream using total space $O^*\\left((e^{2r^2/\\sqrt{d}}/\\varepsilon)\\right)$ and runtime $O^* \\left(\\frac{e^{4r^2/\\sqrt{d}}}{\\varepsilon^2}\\right)$ per step, where r is an upper bound on the norms of key and query embeddings.\nAs before, an important aspect is that MERGEANDREDUCE can handle value embeddings of dimension not necessarily equal to that of key and query embeddings. Thus, when run on scalars v\u2081 = 1 for all i, it can also be used to approximate softmax normalization at any point j in the stream. This is the main subroutine used in BALANCEKV to approximate Attn(qj, Kj, V\u2081). The formal guarantees are given by\nTheorem 3.3. For any r,\u025b > 0, any set of tokens $(q_1,k_1,v_1),..., (q_n,kn, v_n)$ where $q_j,kj \\in \\mathbb{R}^d$ satisfy $||q_j||_2, ||k_j||_2 \\leq r, v_j \\in \\mathbb{R}^s$ for $s \\leq d$ suppose,\n\u2022 batch size $t = O^*\\left(\\frac{e^{2r^2/\\sqrt{d}}}{\\varepsilon}\\right)$\n\u2022 compression rate $2^{-T}$ with $T = \\log(n/t)$.\nThen MERGEANDREDUCE on input parameters t, r, d, s,\u025b, outputs at every step j of the stream subsets of key-value embedding pairs C\u00ba, ...,CT \u2282 C := {(k1, U1), ..., (kn, Un)} such that,\n$Z_j := \\sum_{i=0}^T 2^i \\sum_{\\{k,v\\} \\in C^i} \\exp\\left(\\frac{\\langle k, q_j \\rangle}{\\sqrt{d}}\\right) v_i$\nsatisfies with probability at least 1 \u2013 1/poly(n),\n$\\left| \\sum_{i=1}^{j} \\exp\\left(\\frac{\\langle k_i, q_j \\rangle}{\\sqrt{d}}\\right) v_i - Z_j \\right| \\leq \\varepsilon j \\cdot e^{-r^2/\\sqrt{d}} \\cdot \\max_{i \\in [n]} ||v_i||_2$."}, {"title": "SOFTMAXBALANCE", "content": "Given sets $K = \\{k_1,...,k_n\\} \\subset \\mathbb{R}^d,V = \\{v_1,...,v_n\\} \\subset \\mathbb{R}^s$, and failure probability $\\delta > 0$, define C to be the dataset of pairs $C = \\{(k_1, v_1), ..., (k_n, v_n)\\}$. There exists a randomized algorithm, SOFTMAXBALANCE, which outputs a subset $C' \\subset C, |C'| < |C|/2$, such that, for any vector $q \\in \\mathbb{R}^d$, with probability at least 1 \u2013 \u03b4,\n$\\left| \\sum_{\\{k,v\\} \\in C'} \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) v - \\sum_{\\{k,v\\} \\in C\\setminus C'} \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) v \\right|_2 < O\\left(\\sqrt{\\frac{v s \\cdot \\log(ns/\\delta)}{\\sqrt{d}}} \\cdot \\exp\\left(\\frac{r^2}{2\\sqrt{d}}\\right) \\cdot \\exp\\left(\\frac{\\|q\\|_2 r}{2\\sqrt{d}}\\right) \\cdot \\max_{\\{k,v\\} \\in C} \\|v\\|_2\\right)$."}, {"title": "MERGEANDREDUCE", "content": "As briefly mentioned above in Section 3, MERGEANDREDUCE is a streaming version of SOFTMAXBALANCE. The idea is to partition the stream of tokens into batches of size t, split the batches into pairs, apply SOFTMAXBALANCE to the batches, merge the outputs of SOFTMAXBALANCE on the batches in one pair, and then repeat recursively on the new set of batches T - 1 times if the desired compression rate is $2^T$ see"}, {"title": "and for any k \u2208 Rd", "content": "$\\|\\varphi(k) \\|\\_2 = \\exp\\left(\\frac{\\|k\\|_2^2}{4\\sqrt{d}}\\right)$.\nConsider the set of vectors $ \\varphi(k_1) \\otimes v_1, \\varphi(k_2) \\otimes v_2, .... Run the Self-Balancing Walk algorithm on the set of vectors $ \\varphi(k_1) \\otimes v_1, \\varphi(k_2) \\otimes v_2, ...$ with failure parameter set to \u03b4/s and denote by C' and C\\C' the partition of C returned by the algorithm. Observe that, even though vectors \u03c6(ki)\u2297vi are infinite dimensional, Self-Balancing Walk still can be implemented. The algorithm never has to keep these vectors in the memory because the only operation which requires the knowledge of the embeddings - the inner product - can be performed if we just store vector pairs {ki, vi}:\n$\\langle (\\varphi(k_i) \\otimes v_i), (\\varphi(k_j) \\otimes v_j) \\rangle = \\exp\\left(\\frac{\\langle k_i, k_j \\rangle}{\\sqrt{d}}\\right)$.\nDenote by e1,...,es the standard orthonormal basis in $R^s$. By Theorem 3.4, for any i \u2208 [s] with probability 1 - \u03b4/s,\n$\\left| \\sum_{\\{k,v\\} \\in C'} (\\varphi(k) \\otimes v, \\varphi(q) \\otimes e_i) - \\sum_{\\{k,v\\} \\in C\\setminus C'} (\\varphi(k) \\otimes v, \\varphi(q) \\otimes e_i) \\right|$"}, {"title": "and for any k \u2208 Rd", "content": "$\\leq O\\left(\\frac{\\log(ns/\\delta)}{\\sqrt{d}} \\cdot \\max_{\\{k,v\\} \\in C} \\|\\varphi(k) \\otimes v\\|_2 \\cdot \\|\\varphi(q) \\otimes e_i\\|_2\\right)$,\nand so with probability at least 1 \u2013 \u03b4 all of the above inequalities hold simultaneously.\nTo simplify the right hand side, notice that $\\|\\varphi(k) \\otimes v\\|_2 = \\exp\\left(\\frac{\\|k\\|_2^2}{4\\sqrt{d}}\\right) \\cdot \\|v\\|_2$ and $\\|\\varphi(q) \\otimes e_i\\|_2 = \\exp\\left(\\frac{\\|q\\|_2^2}{4\\sqrt{d}}\\right)$.\nObserve that for any i it holds that $\\langle \\varphi(k) \\otimes v, \\varphi(q) \\otimes e_i \\rangle = \\langle \\varphi(k), \\varphi(q) \\rangle \\cdot [v]_i = \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) \\cdot [v]_i$, where we denote the i-th coordinate of the vector v by $[v]_i$. Therefore, the left hand side of the expression above is"}, {"title": "and for any k \u2208 Rd", "content": "simply the absolute value of the i-th coordinate of the vector\n$\\left| \\sum_{\\{k,v\\} \\in C'} \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) v - \\sum_{\\{k,v\\} \\in C\\setminus C'} \\exp\\left(\\frac{\\langle k, q \\rangle}{\\sqrt{d}}\\right) v \\right|$.\nThus, Equation (3) provides a uniform upper bound on the absolute values of coordinates of the above vector. Since the l\u221e norm of a vector is the maximum of the absolute values of its coordinates,\n$\\left| \\sum_{\\{k,v\\} \\in C'} \\exp\\left(\\frac{\\langle k"}]}