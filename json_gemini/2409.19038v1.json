{"title": "Intention-aware policy graphs: answering what, how, and why in opaque agents", "authors": ["Victor Gimenez-Abalosa", "Sergio Alvarez-Napagao", "Adrian Tormos", "Ulises Cort\u00e9s", "Javier V\u00e1zquez-Salceda"], "abstract": "Agents are a special kind of AI-based software in that they interact in complex environments and have increased potential for emergent behaviour. Explaining such emergent behaviour is key to deploying trustworthy AI, but the increasing complexity and opaque nature of many agent implementations makes this hard. In this work, we propose a Probabilistic Graphical Model along with a pipeline for designing such model -by which the behaviour of an agent can be deliberated about- and for computing a robust numerical value for the intentions the agent has at any moment. We contribute measurements that evaluate the interpretability and reliability of explanations provided, and enables explainability questions such as \u2018what do you want to do now?' (e.g. deliver soup) \u2018how do you plan to do it?' (e.g. returning a plan that considers its skills and the world), and \u2018why would you take this action at this state?' (e.g. explaining how that furthers or hinders its own goals). This model can be constructed by taking partial observations of the agent's actions and world states, and we provide an iterative workflow for increasing the proposed measurements through better design and/or pointing out irrational agent behaviour.", "sections": [{"title": "1. Introduction", "content": "Among the tasks within the purview of Artificial Intelligence (AI), the issue of solving problems without giving explicit knowledge on how to solve them is very pervasive. However, precisely because of the definition of such a task, the result is an artefact that, unless explicitly designed to be transparent, is often not interpretable or, hence, trustworthy (Zhang et al., 2021; Lipton, 2017). This is where the field of Explainable Artificial Intelligence (XAI) shines through.\nA model explanation is an exercise in communication between a sender or source (i.e. the model or one of its components) and a receiver (i.e. the explainee, a human or another processor for a downstream task) that describes the relevant context or the causes surrounding some facts (Lewis, 1986; Miller, 2019; Wright, 2004), which in the context of AI is often related to its final or intermediary outputs or decisions. Any such communicative act can be considered an explanation, but not all explanations may be useful or even desirable. According to empirical studies (Slugoski et al., 1993), it can be argued that the form of an explanation must depend on its function as an answer to a question within a conversational framework. Furthermore, in the words of Herbert Paul Grice (Grice, 1975), for a communicative act to be useful, four maxims should be followed:\n1. Manner: the message or explanans should be comprehensible and clear to the receiver, which within the context of XAI is often referred to as interpretability (Lipton, 2017),\n2. Quality: the message contains truthful information; in the context of XAI, reliability or explanation verification (Zhou et al., 2021b; Slack et al., 2021; Arias-Duart et al., 2022),\n3. Quantity: the length of a message should be just enough to be informative, often a heuristic implicitly agreed upon in the design of explainable systems which depends on both the sender and the code it uses, and\n4. Relation: the explanation should be relevant to the given context, significant when one can keep searching for causes of causes beyond the scope of relevance."}, {"title": "2. Background", "content": "As mentioned in \u00a7 1, our focus in this paper is on methodologies for explaining the behaviour of unknown agents: agents that are opaque or that have a behavioural policy or model that cannot be inspected. From now on, we will assume that we can only (partially) observe their actions and the environment states. Additionally, we will assume that we have access to a (potentially incomplete) notion of what the desirable behaviour should be in terms of what is needed in order to control, improve or justify the actions of the agent (Longo et al., 2020; Adadi and Berrada, 2018), from an explainee point of view."}, {"title": "2.1. Agent Explainability", "content": "On the topic of agent explainability, there are a few surveys that enumerate, categorise and analyse the different existing methods and methodologies (Adadi and Berrada, 2018; Puiutta and Veith, 2020; Arzate Cruz and Igarashi, 2020; Zhou et al., 2021a; Milani et al., 2022; Aha, 2024). One way to categorise explainability methods is to distinguish them based on the time of information extraction, i.e. between those that are intrinsic and those that are post-hoc (Adadi and Berrada, 2018; Puiutta and Veith, 2020). Intrinsic methods build models that are inherently interpretable or self-explanatory during the design or training of the agent's policy. Post-hoc methods, on the other hand, focus on building the explanations by analysing a policy that is already implemented or trained.\nRelated to the intrinsic/post-hoc categorisation, it is also possible to classify explainability methods into model-specific and model-agnostic (Adadi and Berrada, 2018; Puiutta and Veith, 2020). The former are tailored to a specific model or family of model, while the latter methods aim at being able to be used for any kind of agent policy. Most of the approaches found in the literature are model-specific, either by having access to a full or approximate model of the agent or directly designing it (Fox et al., 2017; Albrecht and Stone, 2018; Winikoff et al., 2018; Ciatto et al., 2020; Madumal et al., 2020; Winikoff and Sidorenko, 2023; Rodrigues et al., 2023; Langley, 2024) or by possessing knowledge about specific important parts of the agent's design, such as the reward function (Gyevnar et al., 2023) or the internal task decomposition (Ciatto et al., 2019; Verma et al., 2022).\nAnother possible categorisation deals with the scope of each explanation (Adadi and Berrada, 2018; Puiutta and Veith, 2020): whether the"}, {"title": "2.2. Policy graphs", "content": "A PG (policy graph) is a domain model comprising agent and environment behaviour by learning the agent policy (as $P(a|s)$ or probability to choose a certain action $a$ when in a certain state $s$) and the environment's response to agent actions ($P(s'|a, s)$ or probability to end up in a state $s'$ when $a$ is performed in state $s$, often called world model (Freeman et al., 2019; Gaon and Brafman, 2020; Robine et al., 2023) in the context of sequential decision-making processes). However, learning these two distributions is a complex endeavour, as the state space and/or the action space can be large and of varying complexity and/or require state memory to make decisions (i.e. it is not enough to know $s_t$, but also $s_{t-1}$ and so on). More so, obtaining explainable outputs from a continuous space can be complex, and obtaining reliable estimators of the policy and environment is challenging. One common way to simplify this problem is to make the state space finite, discretising real states into more straightforward descriptions. This simplification allows the PG to be a graph-like representation, in which vertices correspond to discrete states and edges correspond to transition probabilities ($P(s_{t+1}, a|S_t)$).\nA way to solve both problems is to discretise each potentially complex state or action by introducing predicates that summarise states (and potential actions), thus obtaining a discrete, finite number of possible states. This allows for easy modelling of both probability distributions through frequentist approaches (Hayes and Shah, 2017). In addition, the usage of human-defined predicates allows for easily interpretable states, which are then used to provide natural language answers to queries such as identifying conditions for actions (When do you do $a$?), explaining differences in expectation (Why did you do $a$ in state $s$?), and understand situational behaviour (What will you do when X is given?). However, the answer to these questions is permanently restricted to immediate results, as it neither provides answers to long-term action behaviour and is agnostic to the agent's goals, desires, or values.\nAnother way of computing a PG can be through automatic discretisation by employing decision-tree approaches to distinguish between continuous states by the difference in actions taken (Liu et al., 2023). The use of an automatic discretiser simplifies the transformation of the state space into a finite, manageable set. However, the predicates that are automatically produced may not be explainable themselves (e.g. having 'sugar_high' and \u2018sugar_low' predicates, distinguished by an arbitrary threshold defined by the decision tree). Nevertheless, this approach can be used to discover state-regions with"}, {"title": "2.3. Intentionality", "content": "The language explanations provided in the models discussed are limited to locating predicates of the representation relevant to the atomic action selection, which is not the kind of explainability humans tend to seek (Malle and Knobe, 1997b; Malle, 2022). Instead, the explanations that maximise interpretability over agent behaviour are generally related to understandable end-goals, desires, or rewards, be that explanations regarding why an action contributes toward an objective, why the objective came to exist, or which affordances contributed to achieving an objective. It becomes apparent that notions of the agent's objectives and targets are necessary to achieve good explanations, potentially requiring algorithms inspired by theory-of-mind (Ho"}, {"title": "3. Use case", "content": "To verify and test the pipeline proposed, several agents of different kinds are analysed in the environment of Overcooked (Carroll et al., 2020). This environment is a Multi-Agent (MA) RL environment, in which two agents must collaborate to produce and deliver as many dishes as possible in an allotted time. The collaborative nature of the environment delivers the possibility of several emergent behaviours beyond what can appear in single-agent environments, and it is particularly interesting from the standpoint of explainability.\nThe Overcooked-AI environment allows for several layouts and arrangements that motivate the agents' different optimal strategies and behaviours. Therefore, we can obtain relevant insights by producing policy graphs for agents trained for each layout and comparing them using the static and intention metrics."}, {"title": "4. Methodology", "content": "PGs are not an out-of-the-box method, as they require some external designing to create and validate the code in which states are described, as well as manual verification of the correctness of the technique. We frame the approach towards defining a PG in two main designing choices: creating a code for describing states and then formalising hypotheses over the agent's believed desirable behaviour in that code.\nFirstly, a representative sample of observations of the target agent acting in the environment must be collected. We recommend storing all available information prior to its discretisation, as the pipeline may encourage the designer to change the discretiser: the questions posed by the explainee, or the explainee themselves, can change over time. In case this is practically impossible (e.g. original states or trajectories are too spatially inefficient to store), trajectories should be stored as expressive as possible so that as many different discretisations can be applied a posteriori.\nOnce this information is obtained, a base, non-intentional PG is created by computing and storing probability distributions: $P(s', a|s)$ and $P(s)$; that is, the probability distribution of being in a discretised state $s$, and the transition probabilities when in that state - what the agent does, $a$, and what happens to the state, $s'$. Figure 1 provides a depiction of our proposed workflow."}, {"title": "4.1. Policy Graph construction and design heuristics", "content": "A PG's construction relies directly on observing an agent's behaviour and discretising it into the discrete state space. Any formalism is acceptable for the representation of the internal state representation, but the following properties are greatly encouraged:\n\u2022 The state space is a metric space where we define a distance function that computes the similarity between states. Generally, this is done with a simple count of different predicates (Hayes and Shah, 2017), but more sophisticated approaches that account for predicate semantics could provide better explanations.\n\u2022 The resulting state space is small enough that the agent can map states from new observations to existing, already observed states.\n\u2022 The resulting state representation is interpretable to a human or downstream task, who can understand the original state's properties based on its discretised version's internal representation. This understanding can be incomplete; it is only enough to justify or interpret agent behaviour based on it.\n\u2022 The resulting state representation allows to formally represent desires as introduced in \u00a7 4.2. This step requires parallelising the process of designing the PG and hypothesising over desired behaviour, as the ability to test a desire depends on representing it for discrete states.\nThe rationale for these heuristics can be understood from the trade-off between interpretability and reliability. On the one hand, the first two properties are for increasing reliability. The probability distribution only represents the real world if observations are few to appear frequently in the graph. In addition, by introducing a notion of distance, one can consider the state-space a metric space and use similarities between states to compensate for the lack of observations at the cost of some reliability. On the other hand, the representation of the internal states will be part of the code shared between the explainee and the model. If such code is not shared, the result will hardly be interpretable. This, in turn, allows for explanations that conform to what the explainee can understand.\nWhen merging both necessities, it is noticeable that they go in opposite directions: having a small state-space hinders having the expressivity demanded by an extensive code of communication between the explainee and the model, thus hindering interpretability. Similarly, a thorough state description implies a more extensive state-space, in which the specificity of each state will result in a lower probability of reaching it in our observations, lowering the reliability of the probabilities conditioned to being in such a state.\nThis is a significant problem when working with real problems with scarce"}, {"title": "4.2. Explainability based on desires and intentions", "content": "Most explainability algorithms in the literature focus on establishing some causal relationship, correlation, or relevance between some input variable and the model's output (Lundberg and Lee, 2017; Ribeiro et al., 2016; Selvaraju et al., 2017). However, when asking a human why they put a cooking pot on the hob, it is arguably the case they will reply: Because the pot was full of water and the hob was not being used. A correlation may exist between a pot full of water and the cook placing it on top of the hob, as cooks often fill the pot with water when they plan to boil it. However, the motivator of such behaviour is not the availability of the pot and the hob but the intention of the task. As humans are capable of consciously setting themselves goals to pursue, explanations involving human intent are often teleological, including"}, {"title": "4.2.1. Desires", "content": "In this work, desires are introduced as hypotheses over expected behaviour: the work of anthropomorphism by a human observer that has some"}, {"title": "4.2.2. Intentions", "content": "In order to extend explanations to the keyword of why, the transitional information of a PG can be leveraged. An agent's intention to fulfil a desire exists if it can be fulfilled (given by world dynamics and its understanding), and the agent commits to doing so (Cohen and Levesque, 1990). Our empirical observations of the agent's behaviour capture both requirements."}, {"title": "4.2.3. Explanation-extraction and answerable queries", "content": "To leverage the computed intentions, one needs to ponder which questions require answering for explainability to make sense and be helpful. To do this, we focus on studies on how human explainers achieve this. In the folk-conceptual theory of behaviour explanation, one can categorise between explanations provided for unintentional and intentional behavior (Malle and Knobe, 1997b; Malle, 2022). Most previous work (Hayes and Shah, 2017; Liu et al., 2023; Domenech I Vila et al., 2024) focuses on answering why queries by listing beliefs of the agent, which would fall in the kind of explanations usually provided for unintentional behavior. For example, I move north when I am south of a delivery area and have the part (Hayes and Shah, 2017) is a case of why question where why means what caused. Other types of questions (and, therefore, answers) must be provided for intentional kind, such as when why means why for. An example of these would be the aforementioned example of I boil water because I want to make spaghetti. The design focus on which questions need answers is motivated by two principles: information given should be minimal (following the maxim of quantity), but enough question types and asking methods should be available to extract further information if the current is insufficient (to ensure interpretability).\nIn previous work (Malle, 2022), intentional behaviour explanations were categorised into three modes:\n\u2022 Reason explanations, which concern themselves with the causality of an action being taken as assigned to \u2018what the intention is, and how an action favours it', and are by far the most common kind (3 in 4 cases) (Malle, 2022, 2004, 2007). In addition, this type of explanation tends to include additional reasons, such as avoiding alternative outcomes or beliefs about the context.\n\u2022 Causal History of Reasons (CHR) explanations, which concern themselves with explaining the precursor factors to the reasons it is chosen (including intentions). In reinforcement learning, this is intrinsically, but not exclusively tied to the chosen reward function (e.g. emergent behaviour). As an alternate example, in agents with a Belief-Desire-Revision (BDI) architecture (Rao and Georgeff, 1991) the action is"}, {"title": "4.3. Metrics", "content": "Several heuristics for PG design have been introduced in the previous sections. However, managing these heuristics and achieving the desired balance between reliability and interpretability cannot be a blind task. Much like the intended explanations, the design processes should be quantitatively analysed to make the algorithm available to the user.\nFor this purpose, in this section, we propose metrics defined as functions that allow us to assess and quantify the performance and effectiveness of our proposed pipeline in terms of the explanations produced. These functions should be effectively regarded as distance functions that enable quantitative comparisons between explanations in a metric space."}, {"title": "4.3.1. Static Metrics", "content": "Static metrics analyse the graph's properties regardless of intentions and desires. These allow for an idea of the variability of the expected agent behaviour in different scenarios, which can be helpful to pick the best state representation for the PG and compare several ones. We consider three approaches to the task, each evaluating different but relevant points: entropy, behavioural similitude, and trajectory likelihood.\nEntropy is one of the most natural ways of evaluating how informative the PG model is: if knowing the current state unequivocally determines the following action and state, then the PG is perfect, the explanations are entirely reliable, and a policy derived from it could substitute the original agent. This will only be the case for toy cases, but entropy will quantify how close we are to such an ideal state.\nFor the purpose of PGs, state entropy is computed as follows:\n$H(s) = -\\sum\\limits_{s',a\\in{s',a:P(s',a|s)\\neq0}} P(s', a|s) * log_2P(s', a|s)$ (1)\nThis metric can be understood as the expected number of bits necessary to encode the immediate future of the node: the lower, the less uncertainty exists over the agent and environment's behaviour. The future of the node may be further decomposed in two factors: action entropy $H_a(s)$ (Eq. 2), and future state (or world) entropy $H_w(s)$ (Eq. 3), holding that $H(s) = H_a(s) + H_w(s)$.\n$H_w(s) = -\\sum\\limits_{a\\in{a:P(a|s)\\neq0}} P(a|s) * log_2P(a|s)$ (2)\n$H_w(s) = -\\sum\\limits_{a\\in{a:P(a|s)\\neq0}} P(a|s)*\\sum\\limits_{s'\\in{s':P(s'|s,a)\\neq0}} P(s'|s, a) * log_2P(s'|s, a)$ (3)\nThe decomposition of entropy in two parts shows a key insight on the balance for creating a PG: a low number of different discretised states results in fewer possibilities for $P(s'|s, a)$ and likely a lower $H_w(s)$, but at the same time it is likely that a state $s$ determines the following action perfectly by $P(a|s)$, and thus lowers $H_a(s)$. This equilibrium is also present in the"}, {"title": "4.3.2. Intention Metrics", "content": "To gauge the explainability of the PG with intentions, one should consider two things: how likely is it that s (the state analysed) can be said to hold an intention Id, and thus s can be used to explain, and how likely is it that, if the PG claims an intention for a state, that such intention holds?\nAs proposed in \u00a7 4.2, intentions should only be attributed to a state once they exceed a certain threshold: the commitment threshold $C > 0$. This is because even though the agent may have some non-zero probability of achieving a desire in a state, an explanation claiming that the agent has such an intention is not desirable if the probability is very low, and thus defining a cutoff is important to avoid human bias. We define the set $S(I_d) = {s \\in S|I_d(s) > C}$ as the set of states where the agent is attributed as having the intention $I_d$. In addition, we also consider the set $S(I) = {s \\in S|\\exists d \\in D: I_d(s) > C}$, that is, the set in which the agent is attributed as having any of the considered desires as its intention.\nThanks to the classification of states into having and not having an intention, the probabilities used to evaluate desires in \u00a7 4.2.1 can be trivially extended to answer the questions above:\n1. Intention probability $P(s \\in S(I_d))$ is the probability that, at any point of observation, the agent is in a state s which fulfills $I_d(s) > C$.\n2. Expected Intention $E_{s\\in S(I_d)}(I_d(s))$ is the probability that, once attributed, an intention is going to be fulfilled. It is computed as\n$E_{s\\in S(I_d)}(I_d(s)) = \\sum\\limits_{s\\in S(I_d)} I_d(s) * P(s)/P(s \\in S(I_d))$.\nThe first metric estimates the interpretability of agent behaviour: the less likely it is that the agent has no attributed intention in the state, the fewer times we will have no answer to why it is acting. The lower the commitment threshold, the larger the intention probability. For the case of $S(I_d)$, this score can also be increased by introducing more desires to check.\nThe second metric is an estimation of the reliability of an explanation. It computes likely is it that an explanation of why it did something (the cause) did not result in it (the consequent) being fulfilled."}, {"title": "4.3.3. Are static metrics not enough?", "content": "The metrics described in \u00a7 4.3.1, as well as the ones used in the literature (Liu et al., 2023), have a considerable weakness, as they assume that the uncertainty of choosing a specific action has a comparable impact, regardless of the state, to the agent's behaviour. However, in most real-world scenarios, it is seldom the case that behavioural certainty is critical, which means that, in most states, the action can be liberally chosen. In these cases, the entropy metrics defined in \u00a7 4.3.1 need to be revised, given our lack of context on the criticality of the states. To illustrate this point, we present the traffic light environment and agent, as shown in Figure 4.\nSuppose an environment with three (undiscretised) states, the traffic light is Red, Yellow, Green, in which the agent can take four actions (going up, left, down, or right). The only rewarded transitions are going up on G, which gives a positive reward, and going up on R, which gives a negative reward. The next state after any action is not affected by the chosen action, to simplify computation. Instead, it has some strong bias toward Yor R (e.g. 50 and 45%), and a very low probability of going to G (e.g. 5%).\nSuppose now an agent that interacts with this environment as follows: in the R state, it uniformly samples the action between left, down and right (33%); in Y, it always goes left, and in G it always goes up. This agent has an optimal policy for this environment. Figure 4a shows this arrangement.\nSuppose now two PGs, one which distinguishes the G state from Y and Rand one which does the same for R. Neither will be a perfect surrogate, as all three states had a different probability distribution over actions. The first one can represent an optimal policy, whereas the second cannot (it does not distinguish Y from G, and the latter is reward-relevant).\nHowever, when computing the agent entropy between these agents, it becomes apparent that $H_a$ is more significant (less desirable) for the first case (0.89) than for the second (0.71). This happens because the relevance of the critical case gets subsumed when considering large numbers. The probability of this happening increases with the size and complexity of the environment, but as it has been shown, it can happen in toy examples. Although removing from the entropy the weighting of states by their probability (heavily biasing toward infrequent states) or computing entropy on a subset of states found heuristically can reduce the relevance of the problem (Liu et al., 2023), it can become unreliable if the heuristic is miss-matched for the problem. For example, when choosing critical states where $H_a$ is low, the PG could miss-report explainability in critical states where action entropy is large, such as the red traffic light in this example."}, {"title": "4.4. Revision pipeline", "content": "All previous metrics offer empirical, quantitative qualifiers of the designed PG and can be used to report expected performance (both from the side of reliability and interpretability). However, the quality of the metrics and the explainability extracted depend highly on the PG design, which is done with little information to start with. For this, we propose the revision pipeline.\nTo improve a PG, agent trajectories can be analysed through the intention function to gather why the representation may be inaccurate, and enhance it. These trajectories may be actual agent observations or can be simulated by sampling the PG if the agent cannot take new observations.\nThere are two prominent cases which can be detected and used to improve the graph:\n\u2022 Unintentional regions, or large sequence sections where no intention is manifested above the commitment threshold. There are two possibilities: either no agent desire exists, which can be manifested, or the explainee or the pipeline designer never declared the current desires. In such cases, the desire never gets registered in the PG and is thus hidden from the intention function.\n\u2022 Unfulfilled regions, or sections of the sequence in which an agent had a behaviour from which the pipeline could infer the existence of an intention that is not fulfilled (due to the intention falling below the commitment threshold or despite a very high likelihood it does not get fulfilled for a long time). Causes for this could be the prioritisation of a different and conflicting intention, irrational agent behaviour, hidden desires, or the discretiser function confounds two different (real) states that do not manifest the same intention."}, {"title": "5. Experiments", "content": "So far, in this paper we have introduced the following contributions:\n\u2022 A methodology for producing explanations for agents' behaviour, based on constructing policy graphs from the agents' observation and discretising the state space and a set of desires (\u00a7 4).\n\u2022 Static metrics for analysing the structure of the policy graphs (\u00a7 4.3.1).\n\u2022 Intention metrics, capable of measuring both the interpretability of the agents' behaviour and the reliability of the explanations produced both in terms of attributable intentions derived from the proposed desires (\u00a7 4.3.2).\n\u2022 A pipeline for interactive revision of the policy graphs by automatically identifying unintentional and unfulfilled regions of the timeline of the agents' behaviour (\u00a7 4.4).\nIn this section, we present empirical results for the application of these metrics and of the revision pipeline to a concrete use case: the Overcooked-AI environment (Carroll et al., 2020).\nThe experimentation methodology can be summarised as follows:\n1. We select some training methods and, for each layout, we train specialised agents from scratch.\n2. We analyse the performance of the resulting agents.\n3. We design a set of different discretisers that will allow us to compare the effect on the metrics of expressing the state with or without some specific predicates, and we propose a set of desires that are relevant for the Overcooked-AI scenario.\n4. We apply and analyse the static and intention metrics to the resulting policy graphs."}, {"title": "5.1. Agents used", "content": "The agents analysed in this paper consist in two pairs of agents which collaborate with each other.\n\u2022 Pair A (PPO Agent 1(Blue), PPO Agent 2(Green)): two agents trained from scratch with Proximal Policy Optimisation (PPO)(Schulman et al., 2017). These agents were used to validate PGs in previous work (Dom\u00e8nech i Vila et al., 2022).\n\u2022 Pair B (Human Agent(Green), Human-Collaborating Agent(Blue)): A human agent trained from human trajectories exclusively, and a PPO agent trained to collaborate with it. These agents were used in previous work (Carroll et al., 2020; Tormos Llorente et al., 2023). It is important to remark that some behaviours learnt by the PPO agent"}, {"title": "5.2. Discretisers and Static metrics", "content": "Four discretisers are tried and tested for each of the agents and environments. From 1 to 4, each is more expressive and increases complexity (and entropy). The main discretiser includes all predicates relevant to behaving in the environment, including state of the pots and relative positions of objects (which drastically reduce complexity). Each of the extensions is focused on increasing information on the other agent's state."}, {"title": "5.3. Intention metrics", "content": "Static metrics offer direct, unbiased insight over the PGs structurally. When the differences are significant enough, agents can use them to tell which families of discrete options trump the rest reliably. However, the relationship between static metrics and PG adequacy is challenging to understand. When the difference in metrics between the two options is too small, it becomes easier to evaluate the methods from the optic of the maxims of communication or the correctness of explanations that the PG may produce.\nTo better evaluate the quality of explanations, it becomes necessary to hold insights into the agent's goals and objectives, which, in the case of this paper, requires external (human) information. In \u00a7 4.2, a formalisation"}, {"title": "5.4. Revision pipeline example", "content": "The analysis of the intention metrics defined above can be used to verify that the agent behaves as desired (or as hypothesised). However, knowing what proportion of the graph (and thus, behaviour) is explainable is insufficient to bridge the gap and discard unexplainable behaviour. Instead of manually inspecting all possible states in the graph in which the agent is attributed to having no intention, a reasonable alternative is to analyse the explanations provided across the timeline of the environment execution."}, {"title": "6. Discussion and Future Work", "content": "The framework proposed allows attributing intentions and extending PG explanations into the teleological. The encoded information of desires (\u00a7 4.2) provides new types of explanations such as What do you intend to do now?, How do you plan to do it?, and For what purpose did you take this action now? (\u00a7 4.2.3) in a concise and composable manner. In addition, the PG model is instrumented with metrics (\u00a7 4.3) to evaluate the reliability and interpretability of the behaviour and the trade-off is made explicit with the introduction of a user-defined parameter: the commitment threshold (\u00a7 4.2.2).\nAlthough this process requires external knowledge and is not out-of-the-shelf, the provided heuristics (\u00a7 4.1), as well as the revision pipeline (\u00a7 4.4) enable guided iteration over the modelling by gathering and exposing its shortcomings naturally. We believe that the whole proposed methodology can be applied to many tasks (Figure 1).\nAs an outcome of this process, we are optimistic about using this method for applications besides human explainability. One of the key contributions of this paper is that, by using the method proposed, there is a way of automatically creating policies for easily understandable agents that mimic the behaviour of an original agent, thus enabling our method as a Theory of Mind model for understanding the behaviour of others in MA systems. In addition, the availability of intentions for states may be useful for better designing rewards for RL agents (e.g. by locating sparse regions and populating them to go toward near intention-attributed regions), or improving other types of agent implementations. Finally, we believe the insights provided in this paper about the necessity of having a world-model (i.e. $P(s'|a, s)$) and how it enables teleological explanations will be key in designing transparent agents. The introduction of such models may also help the RL community (Touati et al., 2023)."}, {"title": "6.1. Limitations", "content": "Looking forward, there are some improvements that can be applied to our proposed approach. Mainly, the construction of a PG imposes additional requirements on the explainee:\nNecessity of outer desires. As part of the process, it is necessary for the explainee to provide formal descriptions of desires. When attempting to discover an agent's desires based on statistics alone (e.g. through notions of criticality or low entropy), spurious correlations may result in providing nonsensical explanations or distorting the value of the method (\u00a7 4.3.3).\nLimitations of state discretisation. Finding a good state representation for PGs to work is critical. Beyond computational and data requirements, the simplification is done so state descriptions are in a shared code between the explainee and the explainer."}]}