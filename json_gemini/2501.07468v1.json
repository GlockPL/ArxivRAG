{"title": "A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities", "authors": ["Yihao Liu", "Xu Cao", "Tingting Chen", "Yankai Jiang", "Junjie You", "Minghua Wu", "Xiaosong Wang", "Mengling Feng", "Yaochu Jin", "Jintai Chen"], "abstract": "Healthcare systems worldwide face persistent challenges in efficiency, accessibility, and personalization. Modern artificial intelligence (AI) has shown promise in addressing these issues through precise predictive modeling; however, its impact remains constrained by limited integration into clinical workflows. Powered by modern AI technologies such as multimodal large language models and world models, Embodied AI (EmAI) represents a transformative frontier, offering enhanced autonomy and the ability to interact with the physical world to address these challenges. As an interdisciplinary and rapidly evolving research domain, \u201cEmAI in healthcare\u201d spans diverse fields such as algorithms, robotics, and biomedicine. This complexity underscores the importance of timely reviews and analyses to track advancements, address challenges, and foster cross-disciplinary collaboration. In this paper, we provide a comprehensive overview of the \"brain\" of EmAI for healthcare, wherein we introduce foundational AI algorithms for perception, actuation, planning, and memory, and focus on presenting the healthcare applications spanning clinical interventions, daily care & companionship, infrastructure support, and biomedical research. These significant advancements have the potential to enable personalized care, enhance diagnostic accuracy, and optimize treatment outcomes. Despite its promise, the development of EmAI for healthcare is hindered by critical challenges such as safety concerns, gaps between simulation platforms and real-world applications, the absence of standardized benchmarks, and uneven progress across interdisciplinary domains. We discuss the technical barriers and explore ethical considerations, offering a forward-looking perspective on the future of EmAI in healthcare. A hierarchical framework of intelligent levels for EmAI systems is also introduced to guide further development. By providing systematic insights, this work aims to inspire innovation and practical applications, paving the way for a new era of intelligent, patient-centered healthcare.", "sections": [{"title": "I. INTRODUCTION", "content": "Healthcare services play a fundamental role in human well-being, yet they face persistent challenges, including inequities in access [1], inefficiencies in care delivery [2], and a growing demand for personalized solutions to address complex medical conditions [3], [4]. These issues primarily stem from limited and unevenly distributed healthcare resources [5], as well as insufficiently advanced treatment methods [6], often resulting in delayed, inadequate, or sometimes excessive treatments that exacerbate patients' conditions [7]. Within the confines of current clinical workflows-largely reliant on finite clinical infrastructure, human healthcare professionals, and caregiving staff\u2014these challenges remain difficult to fully overcome. To address these issues, various efforts have been implemented, such as telemedicine services [8], [9], automated triage systems [10], [11], AI-assisted healthcare monitoring [12], and medical image analysis [13], [14], which have enhanced the precision and efficiency of medical access while contributing to the ongoing transformation of the healthcare landscape. However, they still fall short of providing direct support within existing clinical workflows.\nArtificial intelligence (AI) technologies, particularly deep learning approaches, are introducing a new workforce into healthcare practice, driving the ongoing transformation of the healthcare landscape [15]\u2013[21]. These methods learn medical and diagnostic knowledge from extensive healthcare data collected across multiple centers, devices, scenarios, patients, and time points, utilizing electronic health records (EHRs), genomic sequences, health monitoring signals, and medical images to perform advanced clinical predictive modeling [22], [23]. This enables early-stage diagnoses [24], facilitates personalized treatment recommendations [25], identifies subtle disease manifestations beyond human discernment [26], and advances biomedical research [27], collectively improving both the efficiency and quality of healthcare services.\nHowever, the translation of modern AI technologies into tangible clinical benefits remains constrained by at least four fundamental challenges: (I) Insufficient multimodal processing. Current AI systems primarily rely on one or several common modalities such as vision, language, and audio, but often lack the capability to process tactile sensations and olfactory cues, which are both more complex and critical in healthcare. The absence of integration for these less-explored modalities limits the effectiveness of AI in addressing the multifaceted nature of clinical tasks and patient care. (II) The separation between development and deployment. Current deep learning frameworks are characterized by a clear separation between development and inference phases, which hinders their continuous evolution in real-world clinical settings. This rigid separation delays adaptation to dynamic clinical requirements and evolving environments, ultimately restricting the systems' capacity for ongoing self-improvement. (III) Insufficient human-machine interaction functionalities. Effective interaction with patients and healthcare professionals is critical for enhancing patient experiences and even improving treatment outcomes. While cutting-edge conversational AI systems, such as ChatGPT and GPT-4, exhibit remarkable interaction capabilities, they often fall short in aligning with treatment objectives and extending beyond verbal communication to encompass behavioral interactions. Such interactions demand advanced reasoning, robust memory retention, and the ability to adapt based on experience. Although recent studies have highlighted the transformative potential of language in therapeutic contexts [28], the mechanisms through which an AI system's linguistic and interactive behaviors can positively impact clinical outcomes-particularly in areas like mental health treatment-remain underexplored [29], [30]. (IV) The absence of pathways from decision-making to action execution. Without embodiment in robotic or assistive devices, AI systems are unable to directly alleviate the workload of healthcare professionals and caregiving staff. While current deep learning models may provide accurate diagnoses and decision support, they rarely translate these insights into actionable diagnostic or therapeutic interventions. Furthermore, ensuring safety during such interventions and maintaining seamless integration within established clinical workflows remain critical challenges that require urgent resolution.\nEmbodied AI (EmAI) is emerging as a promising approach to addressing these challenges in healthcare scenarios [31]\u2013[36]. By integrating AI algorithms, especially multimodal large language models (MLLMs) and world models, with innovations from robotics, mechatronics, human-computer interaction, and sensor technologies, EmAI equips AI algorithms with a physical \"body\" or tangible medium, enabling direct interaction with the world [37]. The AI algorithms are responsible for executing perception, action control, decision-making, and memory processing, ensuring the seamless operation of EmAI systems. Several recent breakthroughs in AI algorithms have significantly advanced the development of EmAI. For instance, unsupervised learning has enabled AI to extract foundational knowledge from vast data without human supervision [38]\u2013[42]; interactive perceptual learning [43] has empowered EmAI systems to comprehend causal relationships of objects and assess the interaction possibilities and feasibility of engaging with various objects [44]; cross-modality fusion techniques have been extensively developed to integrate and leverage complementary information from diverse sources [45], [46]; deep reinforcement learning allows Al systems to learn optimal behaviors through feedback from the environment [47]\u2013[52]; and advancements in large language models (LLMs) [53]\u2013[57], MLLMs [58]\u2013[63], vision-language-action (VLA) models [64]\u2013[67] and even world models [68]\u2013[70] have provided AI systems with enhanced communication capabilities, reasoning abilities, and action-planning capabilities, particularly for tasks like navigation and manipulation [71], [72]. Thanks to these achievements, the development and usability of the \u201cEmAI brain\u201d have significantly advanced, enabling more sophisticated, adaptive, and context-aware EmAI systems\u00b9 capable of functioning in dynamic healthcare environments.\nAdvances in EmAI are driving transformative applications across various fields, with healthcare emerging as a leading domain, which accounts for approximately 35% of the field's work [27], [74], as illustrated in Figure 1(a). Notable examples include surgical robots [75] and companion robots [76], which are becoming increasingly widespread. Figure 1(b) highlights the remarkable growth of EmAI research in key healthcare domains, including biomedical research, infrastructure support, daily care & companionship, and clinical intervention. Notably, the total number of publications in 2024 is nearly sevenfold that of 2019, with clinical intervention research showing the fastest growth while maintaining a substantial share across these domains. Such achievements are underpinned by the integration of insights from multiple disciplines. As depicted in the keyword co-occurrence network (Figure 1(c)), the dense interconnections across domains highlight how advancements in one field catalyze progress in others, emphasizing the pivotal role of interdisciplinary collaboration in revolutionizing healthcare. Notable contributions stem from breakthroughs in foundation models, large language models, computer vision, cognitive science, sociology, and robotics, collectively shaping the future of EmAI applications in healthcare. Building on these research achievements, EmAI has been profoundly transforming healthcare by enhancing patient care and operational efficiency. It enables robotic diagnostics [77], precise surgical interventions [78], and personalized rehabilitation therapies [79], not only streamlining medical workflows but also delivering improved health outcomes and reduced recovery times [80]. Beyond clinical applications, EmAI provides meaningful companionship [81] and emotional support [82], offering particular benefits to vulnerable groups such as children, the elderly, and individuals with disabilities or chronic illnesses, thereby alleviating the burden on healthcare providers. In addition, EmAI is redefining biomedical research by automating experimental processes and analyzing large-scale datasets, enabling researchers to generate insights and conduct experiments with unprecedented speed. These advancements have accelerated the discovery of medical mechanisms [83]\u2013[85], therapeutic targets [86], [87], and disease prevention strategies [88], [89], driving innovation across the biomedical landscape.\nDespite significant advancements [35], [90]\u2013[96], the development of EmAI for healthcare remains in its infancy and faces multiple challenges. Current efforts often concentrate on isolated components of EmAI [80], [97], [98], such as developing advanced algorithms [99], [100], improving workflows [101], [102], or curating data sets [103], [104], without achieving integration into comprehensive systems. To realize the full potential of EmAI, cross-disciplinary collaboration is essential to bridge these fragmented contributions and build cohesive, end-to-end solutions. Moreover, research has predominantly focused on high-profile applications, such as surgical robotics [77], [78], [105]\u2013[110], while other promising areas, including mental health interventions [91], [92], remain underexplored. This uneven distribution of attention limits the broader impact of EmAI across diverse healthcare needs. Additionally, while companion robots have shown potential, most are reactive rather than proactive [81], restricting their ability to anticipate and address patient needs autonomously [111], [112]. Similarly, biomedical research robots face difficulties in maintaining precision and reliability in the inherently complex and dynamic environments of medical research.\nAdditionally, the development of EmAI for healthcare continues to encounter significant technical challenges. First, EmAI development is typically carried out on simulation platforms, which often fail to accurately replicate real-world environments. This discrepancy presents a major challenge in bridging the gap between simulations and real-world applications. Additionally, as EmAI systems may directly interact with the real world, ensuring safety becomes even more critical, especially in medical tasks [113]\u2013[115]. Second, although EmAI systems rely on large datasets, the acquisition of large, ethically sourced, domain-specific real-world datasets in healthcare is hindered by privacy regulations and complex clinical workflows, creating significant barriers to the development of healthcare-specific EmAI. Other challenges, such as ethical considerations [116]\u2013[119] and economic and societal implications [120], [121], were also expected to be addressed.\nGiven the promising potential and numerous benefits of EmAI for both patients and healthcare professionals, as well as the existing challenges, a timely summary of these aspects is crucial for advancing the field and fostering interdisciplinary collaboration. In this review, we summarize and discuss recent and emerging applications of EmAI in healthcare, highlighting key factors that could significantly impact patient outcomes and healthcare practices. In Section II, we provide a concise overview of the technologies that underpin the \u201cEmAI brain\", covering four essential capabilities: perception, actuation, high-level planning, and memory. While our focus is not on delving into the technical foundations of EmAI (for technical reviews, see [65], [122], [123]) or its general applications in robotics (refer to [37], [124], [125]), we present the first comprehensive review centered on healthcare applications of modern EmAI, particularly in clinical interventions, daily care & companionship, infrastructure support, and biomedical research (discussed in Section III). We also summarize their progress and limitations, categorizing EmAI into five levels of intelligence and illustrating each with examples from various healthcare domains (see Section IV). This framework aims to guide researchers and practitioners in understanding the evolution and stages of EmAI in healthcare. Datasets and benchmarks for diverse healthcare scenarios are summarized in Section V, while the challenges and opportunities are further outlined in Section VI, with the aim of guiding researchers towards relevant fields, applications, and data foundations for future exploration."}, {"title": "II. BASIC AI TECHNIQUES FOR EMBODIED Al", "content": "EmAI is gaining momentum thanks to advancements across multiple fields, particularly the breakthroughs in AI. To ultimately replicate human-like behavior in real-world contexts [126], a comprehensive EmAI \"brain\" should encompass multiple modules to conduct perception, action control, decision-making, and memory. Similar to the human brain, which consists of several specialized but interconnected functional regions , these integrated capabilities enable EmAI systems to interact with and adapt to complex real-world environments [124], [127]\u2013[129], as illustrated in Figure 2(b). Here, we outline key approaches that support these functions, categorized into embodied perception, low-level actuation, high-level planning, and memory processing, along with their detailed breakdowns, as shown in Figure 3. We will also summarize the major achievements in this section."}, {"title": "A. Embodied Perception", "content": "Perception is a core mechanism by which EmAI systems interpret sensory data from their environment. This process involves handling high-dimensional, multimodal, and often noisy inputs from sensors such as cameras, microphones, and tactile devices. This section classifies embodied perception from three key aspects: sensory perception, cross-modal perception, and interactive perception. Sensory perception forms the basis for other system functionalities and integrates directly with existing single-modality foundation models [54], [55], [80], [130]\u2013[136]. To achieve a richer understanding of the environment, multimodal AI algorithms [137]\u2013[140] enable cross-modal information integration, aligning with the inherently multimodal nature of the real world. The integration of multimodal data from different devices allows robots to combine sensory inputs such as vision, touch, and speech to make more informed decisions [141]. Interactive perception, which further learns object affordances, represents a pivotal step in bridging perception and action, highlighting its significance as the next frontier in EmAI perception. We will delve into cross-modal and interactive perception in greater detail in the following sections."}, {"title": "1) Cross-modal Perception", "content": "Cross-modal perception integrates information from multiple modalities to achieve a holistic understanding. To efficiently aggregate and align multimodal information, current pre-trained models establish foundational cross-modal representations, enabling downstream multimodal tasks that can be reformulated into challenges such as Visual Language Navigation (VLN) and Embodied Visual Question Answering (VQA), etc. Recent studies [142]\u2013[149] predominantly adopt three primary architectural paradigms to achieve effective cross-modal perception:\nOne prevailing strategy employs separate encoders, where each modality is processed independently before fusion. The similarity among cross-modal representations is then computed and optimized to project multimodal information into a shared representation space, often achieved using contrastive loss functions [150]\u2013[153]. Taking two two-modality processing as an example, representative dual-encoder models include CLIP [154], ViLT [144], and ALIGN [148]. Additionally, some approaches align multimodal representations with language representations, positioning language as a central anchor to bridge diverse modalities (e.g., video, audio, etc.) and ensure semantic consistency [155]. In most cases, the language encoder is pre-trained on large-scale datasets and remains fixed, while each modality is assigned a dedicated upstream encoder for semantic alignment. This design effectively preserves the model's ability to incorporate new modalities, making it particularly suitable for tasks requiring strong semantic alignment, such as zero-shot learning and cross-modal retrieval. Some approaches [156] leverage pre-trained MLLM to convert all modalities to texts before encoding them, which makes unseen modalities can be efficiently dealt with in inference. However, due to the relatively shallow level of modality interaction, separate encoder frameworks exhibit limited performance in complex scene understanding tasks [157]\u2013[161].\nDeeper cross-modality interaction is often achieved by employing a shared encoder to learn comprehensive cross-modal representations. These shared encoders typically leverage multi-layer Transformers that encode multimodal inputs through representation fusion techniques such as cross-attention mechanisms [149], [162]\u2013[164] or feature-wise linear modulation (FiLM) [145], [146], [165]. Representative shared encoder frameworks include ViLBERT [166], LXMERT [167], and UNITER [168]. By using deep fusion encoders, this architecture learns more generic cross-modal representations and achieves superior pre-training efficacy for visual reasoning tasks [169]\u2013[172] and few-shot tasks [173]\u2013[177]. However, it introduces quadratic (for two modalities) or cubic (for three modalities) time complexity, as it requires interaction between all possible modality pairs. This results in significantly slower inference speeds, limiting its practical applicability. Moreover, bridging the cross-modality gap with a single parameterized model remains challenging, further complicating its deployment in real-world scenarios.\nTherefore, a combination of the aforementioned approaches, referred to as the combination architecture, has also been proposed. Typically, these methods employ separate encoders for modality-specific feature extraction alongside a shared encoder for joint feature learning. Furthermore, techniques aimed at reducing computational overhead, such as Mixture-of-Modality-Experts (MoME) [143], [157], [178], [179], Mixture-of-Prompt-Experts (MoPE) [180], effective self-attention [181], [182], and Multi-directional Adapter [147] utilize sparse routing or modular expert networks to efficiently manage modality-specific information while minimizing computational costs."}, {"title": "2) Interactive Perception", "content": "Interactive perception involves physical actions such as manipulating objects, changing viewpoints, or probing the environment-to resolve ambiguities, learn object properties, and refine multimodal representations [183]\u2013[185]. By leveraging exploratory actions, it allows EmAI systems to enhance or extend their perception abilities in object recognition [185], scene understanding [186], or manipulation in dynamic and unstructured environments [183], etc. In robotic manipulation, interactive perception gathers data through exploration to identify potential actionable regions of objects and understand their functional possibilities. This process, a.k.a. affordance learning, further benefits EmAI systems by guiding and optimizing their future interactions.\nWith affordance learning, action plans can be nominated through two key approaches: supervised learning from human demonstrations [187], [188] and reinforcement learning from robotic trial-and-error interactions [189], [190]. Pioneering approaches like Where2Act [191] enable robots to identify the most effective interaction strategies for various parts of an object, and Where2Explore [192] generalize affordance knowledge to similar object parts, enabling robots to adapt to unseen objects with limited prior experience. Additionally, by integrating reinforcement learning, RLAfford [193] facilitates end-to-end affordance learning, enabling robots to adapt seamlessly to a wide range of manipulation tasks."}, {"title": "B. Low-level Actuation", "content": "Low-level actuation is a fundamental component of EmAI systems that leverages various action control policies to determine real-time motor control based on perceptions [235]. In this section, we explore the process of low-level actuation by dividing it into two core phases: control policy representation and control policy learning. The phase of policy representation showcases the framework for encoding robotic behaviors, ensuring that policies are expressive enough to capture intricate actions while remaining computationally efficient and adaptable to diverse scenarios. Building on this foundation, the policy learning phase focuses on how robots select and optimize these behaviors through advanced algorithms including reinforcement learning, imitation learning, and hybrid strategies. Together, these two phases form a cohesive framework that equips robots with the ability to act autonomously and achieve predefined objectives."}, {"title": "C. High-level Planning", "content": "Low-level actuation can only meet the needs of simple, reactive tasks, but it struggles to handle the complexity of planning long-horizon tasks with multiple sub-tasks. To address this limitation, high-level planning algorithms have been developed. Classic planning algorithms, such as A* algorithm [257]\u2013[260], Dijkstra's algorithm [261]\u2013[263], and the probabilistic roadmap (PRM) approaches [264]\u2013[267], serve as foundational methods in this domain. Despite their significant influence and effectiveness in structured environments, these algorithms encounter substantial challenges in real-world scenarios, particularly in high-dimensional state spaces and under conditions of partial observability. Recent research employs LLMs as high-level planners in embodied systems, bridging cognitive reasoning and physical task execution by translating abstract instructions into actionable robotic tasks [65], [268]. Table II summarizes representative approaches of different high-level planning methods.\nLLM-based task planners typically break down high-level goals into a sequence of executable subtasks [293]\u2013[295]. Typically, there are two main paradigms: code-based planners and language-based planners, as shown in the Figure 4. Code-based planners [269], [272], [275] operate by selecting from a pre-defined set of modular skills or functions, invoking them via APIs to execute tasks step by step. They excel in highly controlled workflows requiring safety and reliability, where predefined APIs or modules can ensure predictable outcomes. Representative code-based planners such as the DEPS framework [274] further emphasize \u201cself-explanation\u201d to better exploit the capabilities of LLMs, where an LLM generates plans, explains failures, and uses environmental feedback to aid in re-planning. In addition, by transforming observation sequences into 3D scene graphs using visual language models, ConceptGraphs [275] helps the LLM reason about spatial and semantic relationships in task planning. These methods benefit from the deterministic nature of pre-programmed functions, reducing ambiguity in execution. However, their dependency on predefined skills makes them less adaptable to unexpected changes or tasks outside the pre-programmed domain.\nLanguage-based planners [277], [279], [280] show better flexibility without using predefined functions. In open-ended domains like creative problem-solving, customer support, or planning in uncertain environments, language-based planners can use contextual cues to refine and adjust their instructions in real-time. Additionally, these methods dynamically integrate diverse feedback-ranging from success indicators to human inputs-without necessitating additional training for the LLMs. These adaptive feedback and re-planning mechanisms [274], [296], [297] enable systems to recover from unexpected states, offering more flexibility to novel environments. This adaptability, however, can sometimes come at the cost of consistency or precision [123]. Moreover, by integrating MLLMs' capabilities, EmAI systems like Socratic models [281] effectively translate non-language inputs into unified language descriptions through multimodal informed prompting. This approach not only streamlines information exchange across different modalities through MLLMs but also enhances robot perception and planning tasks [123], [298], [299].\nThe high-level reasoning capabilities of LLMs have been further enhanced by integrating causal inference techniques [300], transforming their planning ability from mere prediction to more logical and explanatory processes. This line of research encompasses prompt-based interventions, such as Chain-of-Thoughts [301], [302], Tree-of-Thoughts [303], and Graph-of-Thoughts [304], as well as interventions targeting inner LLM components [305] and causal graph abstraction [306], [307]. Furthermore, Reinforcement Learning from Human Feedback (RLHF) has been pivotal in fine-tuning LLMs by leveraging human evaluations to guide model behavior [308], [309], enabling EmAI to behave more like humans and demonstrate improved explainability.\nPlanning with end-to-end Embodied Large Models. These algorithms stand out by directly mapping high-level instructions to low-level actions, seamlessly integrating perception, planning, and control into a unified system. Recent research shows that these systems leverage deep reinforcement learning and imitation learning to streamline planning and decision-making in complex environments, often surpassing modular pipelines in adaptability and robustness [310], [311]. Notable frameworks such as SayCan [283], PaLM-E [284], and EmbodiedGPT [287] are designed to combine vision encoder embeddings with planning data from LLMs, directly informing the robotic policies for immediate action. However, training embodied end-to-end systems typically requires large-scale datasets. To address this need, simulation data is widely used for its efficiency and safety, though challenges arise from discrepancies in physics, sensors, and real-world complexity. Strategies such as domain randomization [312], domain adaptation [313], and hybrid approaches [314] have been developed to enhance adaptability and bridge the sim-to-real gap, improving real-world performance. In addition, it is also important to design benchmark to evaluate the embodied decision making capability among different LLMs [315]."}, {"title": "D. Memory Processing", "content": "Memory serves as a repository for past experiences and knowledge, allowing systems to learn from historical data, adapt to new situations, and make informed decisions based on accumulated insights [316]. Memory in EmAI systems is typically divided into short-term memory and long-term memory, each serving complementary functions.\nShort-term memory employs mechanisms such as in-context prompts [283], [317], [318] and latent embeddings [319], [320] within LLMs to manage immediate data needs during interactions. This type of memory often holds data from ongoing interactions and is critical in settings involving dialogues and environmental feedback. For instance, chatbots maintain conversation histories to facilitate ongoing exchanges, while EmAI systems may use textual representations of environmental feedback as a form of short-term memory, aiding in immediate reasoning tasks [280]. This allows EmAI systems to temporarily prioritize new over old information and adapt to new situations with recently learned knowledge.\nLong-term memory serves as a foundational component by storing vital, factual knowledge that influences EmAI systems' actions and their understanding of the world [27]. The integration of long-term memory allows LLMs to leverage past experiences during inference, thereby enhancing their self-evolution capabilities and proficiency in handling complex tasks [321]\u2013[323]. Long-term memory is structured into internal and external systems: the internal memory is embedded within the AI model's own architecture through model weights, enabling swift, direct zero-shot application of learned information [324], [325], while the external memory, stored in separate databases or knowledge graphs, requires active retrieval and integration for usage [326]\u2013[331]. To stay current, long-term memory stored in models can be dynamically updated through fine-tuning techniques, such as supervised fine-tuning (SFT) [332], instruction fine-tuning (IFT) [333], and parameter-efficient fine-tuning (e.g., LoRA) [334], [335], while the external memory is updated by directly improving external databases."}, {"title": "E. Synergistic Integration", "content": "The aforementioned four key functionalities are often initially developed independently but must be effectively integrated to construct a comprehensive EmAI system. Integration approaches such as MemoRAG [336] and Reflexion [337] enhance high-level planning processes by retrieving relevant information [338]\u2013[340] or summarizing past experiences [341], [342] from memory modules. These methods improve adaptability to dynamic environments and enable more rational decision-making. Closed-loop approaches, including RoboGolf [343], LyRN [344], and AlphaBlock [345], integrate perception and actuation modules, leveraging feedback to refine observations and dynamically update control signals. This integration facilitates precise action adjustments and supports effective multi-step planning. Besides, active and interactive perception systems [44], [185], [346] take this a step further by engaging in real-time interactions to explore object properties, update environmental contexts, and refine decisions based on immediate outcomes. The resulting actions and observations are stored in the EmAI memory, which can be used to construct multimodal knowledge graphs integrating physical properties, concepts, affordances, and intentions for future use [347]. The modern Al alignment approaches that combine modules [348], [349] and foundational models that encompass all functionalities of these modules [350], [351] are also being widely researched as promising areas [352], [353]. However, there is still a lack of a highly compatible, efficient, and effective unified architecture capable of integrating various developed modules. Achieving alignment and seamless integration among these modules, while minimizing development (e.g., fine-tuning), remains an open challenge."}, {"title": "III. APPLICATIONS OF EMBODIED Al IN HEALTHCARE", "content": "This section presents healthcare applications and products of EmAI systems across four key domains: Clinical Intervention, Daily Care & Companionship, Infrastructure Support, and Biomedical Research."}, {"title": "A. Clinical Intervention", "content": "EmAI systems have been extensively applied in clinical interventions, spanning the pre-intervention, in-intervention, and post-intervention phases [77], [354], [355]. We outline their primary roles in this section, as shown in Figure 5."}, {"title": "1) Pre-Intervention Stage", "content": "Recent improvements in EmAI-related technologies for pre-intervention diagnostics and assessments are shaping a new AI-clinician collaboration in the intelligent hospital [356]\u2013[358]. EmAI plays various roles in this context, reducing clinicians' workload and accelerating diagnostic workflow.\nVirtual triage nurse. In modern smart healthcare systems, EmAI-based virtual triage nurses replace human nurses play a pivotal role in streamlining patient management by directing individuals to the most appropriate clinical departments. These EmAI systems analyzed patient-reported symptoms and conduct department ranking based on symptom descriptions [359]\u2013[361]. Beyond symptom-based sorting, advanced triage systems integrate wearable health data and EHR to provide a holistic assessment of conditions [362]\u2013[364]. In emergency care settings, these systems can even predict patient outcomes and recommend intervention pathways, significantly shortening response times during critical situations [365]\u2013[368]. As healthcare systems become more interconnected, intelligent triage systems are increasingly serving as an efficient tool of future infrastructure [364], [369], enabling seamless coordination between primary care, specialist consultations, and hospital admissions.\nInteractive medical consultant. Recently, some LLM-based chatbots, such as DISC-MedLLM [370] and HealAI [371], are used to provide instant, reliable, and context-specific responses to medical inquiries, helping patients better understand their symptoms, treatment options, or follow-up care [372], [372], [373]. They can also explain medical conditions, offer personalized recommendations [374], [375], and explain radiology reports [376]. By bridging the gap between patients and complex medical knowledge, interactive medical chatbots not only empower individuals to make informed decisions but also reduce the workload of healthcare professionals [377], [378]. With advancements in LLM reasoning [379], these systems are becoming an essential component of patient-centered smart healthcare solutions. Beyond answering medical questions, these chatbots can also guide patients through administrative processes, such as booking appointments [380], [381] or managing prescriptions [382], [383], thereby easing the burden on healthcare staff [384], [385]. With their ability to operate around the clock, these systems foster greater accessibility and trust in the medical process.\nImaging analyst. Another representative area is robot-assisted medical imaging, which not only improves the accuracy of diagnostic processes but also expands the capabilities of medical imaging in complex anatomical assessments. There have been many studies focused on medical image analysis using Al techniques [386]\u2013[388], but to support robotic surgery and preoperative robotic diagnosis requires additional requirements including real-time processing [389], 3D spatial understanding [390], and safety compliance [391] beyond high precision. Among medical imaging technologies including ultrasonography, radiology, and endoscopy, EmAI enhances their capabilities in different ways.\nFor ultrasonography, as it is portable, real-time, non-invasive, and relying on the synchronization of diagnostic and procedural operations, EmAI can serve as a remote assistant to help clinicians conduct remote ultrasound diagnosis [105] and protect themselves from risks against epidemics [392]. For radiology, EmAI can also help clinicians in lesion localization [77], [393], [394], surgical planning [395]\u2013[397], intraoperative navigation [398], [399] with a better understanding of the raw imaging and attached report from X-Ray, Computed Tomography (CT), and Magnetic Resonance Imaging (MRI). For endoscopy, EmAI can process endoscopic video feeds in real-time to identify abnormalities [400], [401], such as polyps or early-stage tumors, reduce operator fatigue, enable precise navigation via path optimization [402], [403], and assist in procedural tasks like polypectomy with adaptive, precision-controlled robotic movements [404], [405].\nRemote ultrasound. A major benefit of robotic ultrasound is its ability to be remotely operated, facilitating medical diagnosis in remote areas and reducing the healthcare gap between rural and urban communities [105]. One of the most representative remote ultrasound diagnosis methods is image-based visual servoing algorithms [406]\u2013[408], which can adjust the probe's orientation and position remotely in real time, dynamically refining the movements and force of robotic arms [109], [409], [410] in response to changes of patient anatomy, or procedural requirements of diagnostics. Complementary methods such as medical image registration [411] have also been proposed to assist robotic ultrasound scanning, improving positioning accuracy [106], [412], performing motion compensation [413], [414], and enabling real-time monitoring [102], [415].\nGuardian against epidemics. Another potential usage of robotic medical imaging systems is to use their teleoperated nature as a protective barrier, physically separating healthcare workers from infected patients [392]. This approach helps to address widespread concerns about exposure to infection and allows clinicians to focus on providing high quality care without compromising their safety or that of their patients. Tele-operated robotic lung ultrasound system has gained attention [416], [417] that enabled remote assessment of lung conditions, effectively reducing the risk of viral transmission. Similarly, AI-Corona, a radiologist-assistant framework, enables COVID-19 diagnosis through chest CT scans with faster and more precise assessments while minimizing patient-clinician interaction [418]. Such advancements improve the efficiency and safety of healthcare delivery during pandemics."}, {"title": "2) In-Intervention Procedure", "content": "The ongoing integration of EmAI systems into intervention procedures has catalyzed advancements in various fields, including surgical practice, mental health interventions, and beyond. Among these, surgical practice has emerged as the most extensively studied and developed application to date. These systems automate specific surgical tasks and provide critical intraoperative feedback, thereby improving both the execution of operations [435", "437": "and the analysis for surgical training [438", "440": ".", "442": [444], "445": [446], "447": ".", "448": ".", "123": [449]}]}