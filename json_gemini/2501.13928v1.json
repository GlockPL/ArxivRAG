{"title": "Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass", "authors": ["Jianing Yang", "Alexander Sax", "Kevin J. Liang", "Mikael Henaff", "Hao Tang", "Ang Cao", "Joyce Chai", "Franziska Meier", "Matt Feiszli"], "abstract": "Multi-view 3D reconstruction remains a core challenge in computer vision, particularly in applications requiring accurate and scalable representations across diverse perspectives. Current leading methods such as DUSt3R employ a fundamentally pairwise approach, processing images in pairs and necessitating costly global alignment procedures to reconstruct from multiple views. In this work, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view generalization to DUSt3R that achieves efficient and scalable 3D reconstruction by processing many views in parallel. Fast3R's Transformer-based architecture forwards N images in a single forward pass, bypassing the need for iterative alignment. Through extensive experiments on camera pose estimation and 3D reconstruction, Fast3R demonstrates state-of-the-art performance, with significant improvements in inference speed and reduced error accumulation. These results establish Fast3R as a robust alternative for multi-view applications, offering enhanced scalability without compromising reconstruction accuracy.", "sections": [{"title": "1. Introduction", "content": "3D reconstruction from multiple views has long been a foundational task across applications in autonomous navigation, augmented reality, and robotics [31, 53]. Establishing correspondences across images, known as multi-view matching, is central to these applications and enables an accurate scene representation. Traditional reconstruction pipelines, such as those based on Structure-from-Motion (SfM) [44] and Multi-View Stereo (MVS) [18], fundamentally rely on image pairs to reconstruct 3D geometry. While effective in some settings, these methods require extensive engineering to manage the sequential stages of feature extraction, correspondence matching, triangulation, and global alignment, limiting scalability and speed.\nThis traditional \"pipeline\" paradigm has recently been challenged by DUSt3R [61], which directly predicts 3D structure from RGB images. It achieves this with a design that \"cast[s] the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models\" [61], yielding impressive robustness across challenging viewpoints. This represents a radical shift in 3D reconstruction, as an end-to-end learnable solution is less prone to pipeline error accumulation, while also being dramatically simpler.\nOn the other hand, a fundamental limitation of DUSt3R is its restriction to two image inputs. While image pairs are an important use case, often one is interested in reconstructing from more than two views, as when scanning of objects [39] or scenes [4, 6, 20, 55, 67], e.g. for asset generation or mapping. To process more than two images, DUSt3R computes O(N2) pairs of pointmaps and performs a global alignment optimization procedure. This process can be computationally expensive, scaling poorly as the collection of images grows. For instance, it will lead to OOM with only 48 views on an A100 GPU.\nMoreover, such a process is still fundamentally pairwise, which limits the model's context, both affecting learning during training and ultimate accuracy during inference. In this sense, DUSt3R suffers from the same pair-wise bottleneck as traditional SfM and MVS methods.\nWe propose Fast3R, a novel multi-view reconstruction framework designed to overcome these limitations. Building on DUSt3R's foundations, Fast3R leverages a Transformer-based architecture [56] that processes multiple images in parallel, allowing N images to be reconstructed in a single forward pass. By eliminating the need for sequential or pairwise processing, each frame can simultaneously attend to all other frames in the input set during reconstruction, significantly reduces error accumulation. Perhaps surprisingly, Fast3R also takes significantly less time.\nOur contributions are threefold.\n1. We introduce Fast3R, a Transformer-based model for multi-view pointmap estimation that obviates the need for global postprocessing; resulting in significant improvements in speed, computation overhead and scalability.\n2. We show empirically that the model performance improves by scaling along the view axis. For camera pose localization and reconstruction tasks, the model improves when trained on progressively larger sets of views. Per-view accuracy further improves when more views are used during inference, and the model can generalize to significantly more views than seen during training.\n3. We demonstrate state-of-the-art performance in camera pose estimation with significant inference time improvements. On CO3Dv2 [39], Fast3R gets 99.7% accuracy within 15-degrees for pose estimation, over a 14x error reduction compared to DUSt3R with global alignment. Fast3R offers a scalable and accurate alternative for real-world applications, setting a new standard for efficient multi-view 3D reconstruction."}, {"title": "2. Related Work", "content": "Multi-view 3D reconstruction: Almost all modern 3D reconstruction approaches are based on the traditional multi-view geometry (MVG) pipeline [21]. MVG-based methods first identify corresponding pixels between image pairs, and then use camera models and projective multiview geometry to lift these correspondences to 3D points. The process happens in sequential stages: feature extraction, finding pairwise image correspondences, triangulation to 3D and pairwise relative camera pose, and global bundle alignment. However, any pipeline approach is prone to accumulating errors, which are especially common in the hand-crafted components. Moreover, the sequential nature prevents parallelization, which limits speed and scalability.\nMVG approaches have existed since the early days of computer vision, and are still in use for a reason: they can be highly accurate when they do not catastrophically fail. The latest multi-view geometry pipelines like COLMAP [44] or OrbSLAM2 [30] incorporate nearly 60 years of compounding engineering improvements, but these approaches still catastrophically fail >40% of the time on static scenes like ETH-3D [52]), which can actually be considered an easy case due to dense image coverage of the scene.\nMuch recent work has successfully addressed the robustness and speed by replacing increasingly large components of MVG pipelines with end-to-end learned versions that are faster and reduce the rate of catastrophic failures [48, 58, 72]. For example, [14, 19, 25, 42, 51, 68] improve feature extraction and correspondences, [27, 50, 59, 71] learn to estimate camera pose, and [52] introduce a bundle adjustment layer. [61] contains an excellent and comprehensive survey of such efforts. Overall, the trend is towards replacing increasingly large components with end-to-end solutions.\nPointmap representation: DUSt3R [61] takes this evolution the furthest by proposing pointmap regression to replace everything in the MVG pipeline up to global pairwise alignment. Rather than first attempting to solve for camera parameters in order to triangulate corresponding pixels, DUSt3R trains a model to directly predict 3D pointmaps for pairs of images in a shared coordinate system. Other MVG component tasks such as relative camera pose estimation and depth estimation can be recovered from the resulting pointmap representation. However, DUSt3R's pairwise assumption is a limitation, as it requires inference on O(N2) image pairs and then a global alignment optimization, which is per-scene and does not improve with more data. Moreover, this process quickly becomes slow or crashes due to exceeded system memory, even for relatively modest numbers of images.\nDUSt3R has inspired several follow-ups. MASt3R [25]"}, {"title": "3. Model", "content": "Fast3R is a transformer-based model that predicts a 3D pointmap from a set of unordered and unposed images. The model architecture is designed to be scalable to over 1000 images during inference, though during training we use image masking to train it with far fewer. In this section, we detail our implementation of Fast3R, and discuss the design choices that enable its scalability."}, {"title": "3.1. Problem definition", "content": "Taking a set of (N) unordered and unposed RGB images I \u2208 RN\u00d7H\u00d7W\u00d73 as inputs\u00b9, Fast3R reconstructs the 3D structures of the scene by predicting the corresponding pointmap X, where X \u2208 RN\u00d7H\u00d7W\u00d73. A pointmap is a set of 3D locations indexed by pixels in an image I, enabling the derivation of camera poses, depths, and 3D structures. Fast3R predicts local and global pointmaps XL and XG, and corresponding confidence maps EL and EG (of shape \u03a3\u2208RN\u00d7H\u00d7W). Fast3R maps N RGB images to\nFast3R: I\u2192 (XL, L, XG, \u03a3G)\nLike MASt3R, the global pointmap XG is in the coordinate frame of the first camera and the X is in the coordinate frame of the viewing camera, as shown in Figure 2"}, {"title": "3.2. Training Objective", "content": "This section describes the loss, using the notation in Section 3.1 above. Fast3R's predictions of (XL, L, XG, \u03a3G) are trained using generalized versions of the pointmap loss in DUST3R [61].\nOur total loss is the combination of pointmap losses for the local and global pointmaps:\nLtotal = LXG + LXL\nwhich are confidence-weighted versions of the normalized 3D pointwise regression loss.\nNormalized 3D pointwise regression loss: The normalized regression loss for X is a multi-view version of that in DUST3R [66] or monocular depth estimation [15, 36, 66]. It is the L2 loss between the normalized predicted pointmaps and normalized target pointmaps, rescaled by the mean Euclidean distance to the origin:\nlregr(X, X) = \\frac{1}{2} \\frac{\\left \\| \\frac{X}{\\bar{X}} - \\frac{X}{\\bar{X}} \\right \\|_2^2}{\\left (\\frac{\\bar{X}}{\\bar{X}} \\right )^{1/2}} = \\frac{1}{|X|} \\sum_{X \\in X} \\left \\| \\frac{X}{\\bar{X}} - \\frac{X}{\\bar{X}} \\right \\|_2^2\nNote that the predictions and targets are independently normalized by the mean euclidean distance to the origin.\nPointmap loss: As in [61], we use a confidence-adjusted version of the loss above, using the confidence score \u03a3 predicted by the model. The total loss for a pointmap is\nLx(, X, X) = \\frac{1}{|X|} \\sum_{X \\in X} \u03a3 + lregr (X,X) + \u03b1 log(\u03a3+)\nSince the log term requires the confidence scores to be positive, we enforce + = 1 + exp(2). Our intuition is that the confidence weighting helps the model deal with label noise. Like DUST3R, we train on real-world scans typically containing systematic errors in the underlying pointmap labels. For example, glass or thin structures are often not reconstructed properly in the ground-truth laser scans [4, 67], and errors in camera registration will cause misalignments between the images and pointmap labels [66]."}, {"title": "3.3. Model architecture", "content": "The Fast3R meta-architecture is inspired by DUSt3R, and has three components: image encoding, fusion transformer, and pointmap decoding, as shown in Figure 2. We emphasize that Fast3R makes no assumptions on the ordering of images in I, and all output pointmaps and confidence maps (XL, L, XG, EG) are predicted simultaneously, not sequentially.\nImage encoder: Fast3R encodes each image I\u00bf \u2208 I to a set of patch features Hi, using a feature extractor F. This is done independently per image, yielding a sequence of image patch features H\u2081 = {hi,j}HW/P\u00b2 for each image:\nH\u2081 = F(Ii), i \u2208 1, ..., \u039d\nWe follow DUSt3R's design and use CroCo ViT [63] as our encoder, though we found DINOv2 [33] works similarly.\nBefore passing image patch features H to the fusion transformer, we add position embeddings with one-dimensional image index positional embeddings.\nIndex embeddings help the fusion transformer determine which patches come from the same image and are the mechanism for identifying I\u2081, which importantly defines the global coordinate frame. This is critical for allowing the model to implicitly reason about camera pose jointly for all images from an otherwise permutationally invariant set of tokens.\nFusion transformer: Most of the computation in Fast3R happens in the fusion transformer, which has a generic architecture. We use a 12-layer transformer similar to ViT-B [12] or BERT [10], however this could be scaled up. This fusion transformer takes the concatenated encoded image patches from all views and performs all-to-all self-attention. This operation provides Fast3R with full context, beyond the information provided in pairs alone.\nPointmap head: Finally, Fast3R uses separate DPT-L [37] decoder heads to map these tokens to the local and global pointmaps (XL, XG), and confidence maps (EL, \u03a3G).\nImage index positional embedding generalization: We would like Fast3R to be able to handle many views at inference, more than were used to train a model. A na\u00efve way to embed views during testing would be to embed them in the same way as training: i.e. use the same Spherical Harmonic frequencies [49] to embed raw indices SH({1, ..., N}) during training, and SH({1, ..., Ntest}) during inference. In LLMs this causes poor performance, and in preliminary experiments we also found that the resulting model did not work well when the number of input images exceeded that"}, {"title": "3.4. Memory-Efficient Implementation", "content": "With a standard transformer architecture and a single-pass inference procedure, Fast3R is able to leverage many of the recent advances designed to improve scalability at train and inference time [2, 13, 23, 54].\nFor example, model size and throughput can be increased by sharding the model and/or data minibatch across multiple machines, such as through model parallelism [22, 45], data parallelism [26], and tensor parallelism [32, 46]. During training, optimizer weights, states, and gradients can also be sharded [35]. Systems-level advances have also been proposed, such as FlashAttention [7, 8], which uses GPU kernels leveraging the hardware topology to compute attention in a time and memory-efficient way. These are implemented in libraries such as FAIRScale [16], DeepSpeed [35] and Huggingface [64], and require significant engineering effort.\nThe Fast3R meta-architecture is explicitly designed to take advantage of these efforts. We leverage two different forms of parallelism at training and inference time, as well as FlashAttention, described in more detail in Sec. 4. More broadly, we believe that our approach will continue to benefit in the longer term as transformer-based scaling infrastructure continues to mature."}, {"title": "4. Experiments", "content": "Training Data We train on a mix of real-world object-centric and scene scan data: CO3D [39], ScanNet++ [67], ARKitScenes [4], and Habitat [43]. We use a subset of the datasets in DUSt3R, specifically 4 of the 9 datasets, for a total of around 2000 unique scenes scans and 1300 videos of 50 object classes. Note that this is only 7% of CO3D, which is also what the baselines DUSt3R [61], Spann3R [57], and MAST3R [25] use.\nBaselines DUSt3R [61] is the closest approach to ours, and competitive on visual odometry and reconstruction benchmarks. That paper contains extensive comparisons against other methods, and we adopt it as our main baseline. We additionally consider DUSt3R's follow-up work,"}, {"title": "4.1. Inference Efficiency", "content": "At inference time, we aim to handle 1000+ views compared to 20 during training, which requires additional optimizations. We observe the memory bottleneck at inference is in the DPT heads producing the pointmaps: with 320 views on a single A100 GPU, over 60% of VRAM is consumed by activations from the DPT heads, largely due to each needing to upsample 1024 tokens into a high-resolution 512 \u00d7 512 image. To address this, we implement a simple version of tensor parallelism, putting the model on GPU 0 and then copying the DPT heads to each of the K 1 other GPUs. When processing a batch of N \u2248 1000 images, we pass the entire batch through the ViT encoder and global fusion decoder, and then split the outputs across K machines for parallel DPT head inference.\nTable 2 shows the inference time and memory usage as we increase the number of views. Fast3R is able to process up to 1500 views in a single pass, whereas DUSt3R runs out of memory past 32. Fast3R also has a significantly faster inference time, with gains that increase with more views."}, {"title": "4.2. Camera Pose Estimation", "content": "We evaluate camera pose estimation on unseen trajectories from 41 object categories from CO3D [39]. Following [61], we sample 10 random views from each trajectory.\nInspired by DUSt3R [61], we estimate the focal length, camera rotation, and camera translation from the predicted global pointmaps. We begin by initializing a set of random focal length guesses based on the image resolution, then use RANSAC-PnP to estimate the camera's rotation and translation based on the guessed focal lengths and the global pointmap. The count of outliers from RANSAC-PnP is used to score each guessed focal length (lower is better), and the best-scoring focal length is selected to compute the intrinsic and extrinsic camera matrices.\nDuring RANSAC-PnP, we only use points with the top 15% confidence scores predicted by Fast3R, ensuring efficient PnP processing and reducing outliers. If all images are known to originate from the same physical camera, we use the focal length estimated from the first view as the focal length for all cameras, as this initial estimate has been empirically found to be more reliable. Otherwise, we independently estimate the focal length for each input. It is worth noting that the camera pose estimation process is parallelized using multi-threading, ensuring minimal wall-clock time. Even for hundreds of views, the process completes in just a few seconds on standard CPUs."}, {"title": "4.3. 3D Reconstruction", "content": "We evaluate Fast3R's 3D reconstruction on scene-level benchmarks: 7-Scenes [47] and Neural RGB-D [3], and the object-level benchmark DTU [1].\nWe found that local pointmaps learn finer detail than the global pointmaps. Therefore we use the local pointmaps for detail and the global pointmaps for the high-level structure. Specifically, we independently align each image's local pointmap to the global pointmap using ICP, and use aligned local pointmaps for evaluation.\nFast3R is competitive with other pointmap reconstruction methods like DUSt3r and MASt3R, while being significantly faster, as shown in Table 3 and Table 4. We believe that Fast3R will continue to improve with better reconstruction data, more compute, and better training recipes. We show supportive scaling experiments in Figure 5.1."}, {"title": "4.4. 4D Reconstruction: Qualitative Results", "content": "Because Fast3R can handle multiple frames naturally, one may wonder how well Fast3R can handle dynamic scenes. We qualitatively test Fast3R's 4D reconstruction ability, showing examples of dynamic aligned pointmaps at multiple time steps in Figure 6. Fast3R can be trained to achieve such results by finetuning a 16 static views checkpoint on the PointOdyssey [73] and TartanAir [62] datasets, consisting of 110 dynamic and 150 static scenes, respectively. We freeze the ViT encoder, use 224x224 image resolution, and swap in a newly-initialized global DPT head. We fine-tune the model with 15 epochs with a frame length of 16, batch size per GPU of 1, and use the same learning rate schedule as Fast3R. The process takes 45 hours to finetune on 2 Nvidia Quadro RTX A6000 GPUs.\nWe see that our approach produces qualitatively reasonable reconstructions with minimal changes. MonST3R [69] is a concurrent work also tackling dynamic scene reconstruction that builds atop DUSt3R. However, like DUSt3R, it assumes a pairwise architecture and also uses a separate model to predict optical flow. We show that the same Fast3R architecture trained end-to-end with the same many-view pointmap regression (just swapping the data to dynamic scenes), can also work for 4D reconstruction. Importantly, our method remains significantly faster, opening the poten-"}, {"title": "5. Ablation Studies", "content": "Fast3R is able to use all-to-all attention during training, which lets it learn from global context. We hypothesize that the additional context provided by more views during training allows the model to learn higher-order correspondences between multiple frames, ultimately increasing model performance and increasing potential for scaling.\nFigures 7 and 8 shows that training on increasingly more views consistently improves RRA and RTA for visual odometry, and reconstruction accuracy-even when the number of views used during evaluation is held constant and the model is ultimately evaluated on fewer views than were seen during training. We further evaluate the model's ability to reason about additional views by increasing the number of images that Fast3R sees during inference. Figure 4 and Figure 5 indicate that as the model uses more views, the average per-view performance improves. This behavior holds for all evaluated metrics in both camera pose estimation and reconstruction. As shown in Figure 5, the model has a better per-view accuracy using 50 images than it does with 20, even though it was trained with 20. Many applications (e.g. reconstruction, odometry) require inference on many views, which is a major motivation for Fast3R removing the pairwise constraint."}, {"title": "5.2. Training without position interpolation", "content": "In Section 3.3, we introduced a randomized version of [5] to enable inference on more views than seen training. Without this technique, model accuracy quickly degrades for pointmap corresponding to image indexes outside the training range, as shown in Figure 9 (top). A version of Fast3R trained on N = 4 views still produces high quality pointmaps for views in slot 5 to 24 (Figure 9 bottom)."}, {"title": "5.3. Removing the local decoder head", "content": "Table 5 shows that removing the local head learns finer details before the global head does. We hypothesize that this is because the global head needs to first learn the coordinate system and then learn the fine details. As the model improves, the local head may not be necessary."}, {"title": "6. Conclusion", "content": "We introduce Fast3R, a transformer that predicts 3D locations for all pixels in a common frame of reference, directly in a single forward pass. By replacing the whole SfM pipeline with a generic architecture trained end-to-end, Fast3R and similar approaches should benefit from the usual scaling rules for transformers: consistent improvement with better data and increased parameters. Since Fast3R uses global attention, it avoids two potentially artificial scaling limits due to bottlenecks in existing systems. First, the bottleneck of image pair reconstruction restricts the information available to the model. Second, pairwise global optimization can only make up for this so much and does not improve with more data.\nWith our efficient implementation, Fast3R can operate at > 250 FPS, and process 1500 images in one forward pass, far exceeding other methods while achieving competitive results on 3D reconstruction and camera pose estimation benchmarks. We demonstrate that Fast3R can be finetuned to reconstruct videos by changing the data and without modifying the pointmap regression objective and architecture. In contrast with pipeline approaches bottlenecked by custom and slow operations, Fast3R inherits the benefits of future engineering improvements to efficiently serve and train large transformer-based models. For example, packages like Deepspeed-Inference [38], FlashAttention [7, 8] provide fused kernels, model parallelism, and data parallelism. These speed up inference and reduce memory requirements, allowing more images per device, and the number of images scales with the number of devices.\nLimitations: A current limiting factor for scaling may be data accuracy and quantity. Synthetic data [34, 40] could be a solution as, broadly speaking, models trained for geometry estimation seem to generalize well from simulation data."}]}