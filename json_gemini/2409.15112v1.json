{"title": "ChatGPT as a Solver and Grader\nof Programming Exams written in Spanish", "authors": ["Pablo Saborido-Fern\u00e1ndez", "Marcos Fern\u00e1ndez-Pichel", "David E. Losada"], "abstract": "Evaluating the capabilities of Large Language\nModels (LLMs) to assist teachers and students\nin educational tasks is receiving increasing at-\ntention. In this paper, we assess ChatGPT's\ncapacities to solve and grade real programming\nexams, from an accredited BSc degree in Com-\nputer Science, written in Spanish. Our findings\nsuggest that this AI model is only effective for\nsolving simple coding tasks. Its proficiency in\ntackling complex problems or evaluating solu-\ntions authored by others are far from effective.\nAs part of this research, we also release a new\ncorpus of programming tasks and the corre-\nsponding prompts for solving the problems or\ngrading the solutions. This resource can be\nfurther exploited by other research teams.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) based on the\nTransformer architecture (Vaswani et al., 2017)\nhave represented a paradigm shift in Natural Lan-\nguage Processing (NLP), making the previous state-\nof-the-art results and benchmarks obsolete. The\nrelease of ChatGPT by OpenAI in November 2022\nmeant a disruption in what was thought to be possi-\nble in generating human-like conversations (Forbes,\n2022). This class of generative models has proved\nto be effective in a wide range of Natural Language\nProcessing (NLP) tasks (Zhong et al., 2023; Mao\net al., 2023).\nThese new tools have also demonstrated impres-\nsive capabilities for solving programming tasks.\nThis is often attributed to the fact that their internal\nmodels have been exposed to a large number of\nprogramming examples during their training pro-\ncess (Zhong et al., 2023; Xu et al., 2022; Chen\net al., 2021). LLMs can thus become a highly valu-\nable asset to support different teaching activities in\nmultiple computer-related courses and university\ndegrees (Baidoo-Anu and Ansah, 2023). For exam-\nple, we can exploit them to give support to students\nin problem solving, to suggest exercises and activ-\nities to the professors, or to assist in the grading\nprocesses. But this also comes with drawbacks,\nsuch as those related to plagiarism or cheating.\nSome researchers have already tested ChatGPT\nfor resolving programming problems, demonstrat-\ning human-level performance for simple tasks, but\nalso finding that it struggles with complex data\nstructures (Chang et al., 2023; Sarsa et al., 2022).\nHowever, most studies and benchmarks have been\nconfined to exams written in English. Although the\nmultilingual settings of many LLMs have made it\npossible to apply these models to other languages,\nthe performance is often lower than that of English\nand more scientific efforts are needed to evaluate\nthe benefits and limitations of these language mod-\nels for other languages.\nFurthermore, most previous studies focused on\nresolving simple programming tasks, such as basic\ncoding exercises. Our evaluation addresses not only\nbasic programming challenges, but also more com-\nplex exercises that require reasoning about compu-\ntational complexity, decision making about algo-\nrithmic strategies, or selecting proper data struc-\ntures (e.g., stacks or queues).\nAnother aspect that has received little attention\nis the role of LLMs as graders or assistants in eval-\nuating the quality of solutions written by humans.\nBy advancing our understanding on the grading\nabilities of Al agents, we can shed light on the fea-\nsibility of incorporating them into new (student-\nmachine) learning activities or even exploiting\nthem to automatically or semi-automatically grade\nacademic assignments.\nIn this study, we evaluate ChatGPT's abilities\nto solve programming and algorithmic problems\nextracted from a real exam written in Spanish. The\nexam, which is the final test of a 1st-year/2nd-\nsemester course on Programming within a BSc\nin Computer Science, covers a wide range of exer-\ncises, from basic coding exercises to more intricate"}, {"title": "2 Method", "content": "reasoning tasks. We also assess here the AI's capac-\nities to evaluate exams solved by students enrolled\nin the course from which the exam was taken.\nTherefore, our contributions are:\n\u2022 An evaluation of how well ChatGPT solves\ncomplex programming and algorithmic prob-\nlems written in Spanish.\n\u2022 A study of the feasibility of ChatGPT to act\nas an automatic evaluator for tests solved by\nuniversity students.\n\u2022 A detailed item-by-item analysis of the\nstrengths and weaknesses of ChatGPT as a\nsolver and grader of multiple programming\nexercises.\n\u2022 A new corpus of programming tasks and the\ncorresponding set of prompts to ask the mod-\nels to solve problems or grade solutions. This\nnew resource can be further exploited by other\nresearch teams to conduct further evaluations\nof LLMs for programming problem resolution.\nAll the data and code of this research is freely\navailable for the scientific community\u00b9.\n2.1 ChatGPT as a Solver\nWe chose a real exam from a 1st year-2nd semester\ncourse on Programming, Linear Data Structures\nand Introduction to Computational Complexity.\nThe exam was taken in May 2023 by 90 students\nfrom an accredited BSc degree in Computer Sci-\nence. The average score of the students was 57.55%\n(std dev 20.29%), 26 of them did not pass (score\nbelow 50%), and 5 students scored above 90%.\nIt should be noted that this is an exam that tests\nnot only basic coding skills, but also algorithmic\nand data structure concepts. The exam consisted\nof 7 questions (see Table 2, Appendix B), with var-\nied types of expected responses, ranging from a\nfull page to a short textual answer. Some of the\nquestions involved the development of C code. In\nthe original exam, two questions (#3 and #5) had\ntwo figures that further help to clarify the particular\ninquiry. Since ChatGPT2 does not accept images,\nwe opted for removing the images. In any case, the\nimages were redundant (e.g. one represented the\ninternal structure of the Abstract Data Type (ADT)\nlist, whose code was given in the text of the ques-\ntion) and one can understand the question without\nhaving to see the image. However, we have to bear\nin mind that this may be a small disadvantage for\nthe AI model. The evaluation of more advanced\nmodels, such as GPT-4 (OpenAI, 2023), was left\nfor future work.\nEach question was passed to ChatGPT through\nOpenAI's Python API. Two different prompt vari-\nants were tested: a simple one with almost no con-\ntext (Simple Prompt) and a more sophisticated\nprompt including formatting instructions and sys-\ntem role (Complex Prompt), see Appendix A.1.\nThe answers outputted by the AI model were given\nto the main instructor of the course (a professor in\nCS&AI), who assessed the model's solutions using\nthe same criteria set for the official exam.\n2.2 ChatGPT as a Grader\nWe also wanted to evaluate ChatGPT's capacity to\nassess the quality of human-made solutions. The of-\nficial exams solved by the students were manuscript\nand, thus, we can hardly evaluate them all. Instead,\nwe chose a sample of five exams, with a varied\nrange of scores (94%, 74%, 66%, 50% and 38%),\ntranscribed them and submitted them to the model's\nAPI. We sent each question individually and asked\nthe model to provide a quality score (0%-100%),\nsee Appendix A.2. Then, an overall grade was ob-\ntained by weighting the questions using the point\nscale established in the official exam."}, {"title": "3 Results", "content": "3.1 ChatGPT as a Solver\nTable 1 shows the results achieved by ChatGPT\nin the exam. As can be seen in the first row, each\nquestion had a different number of points. To avoid\nany possible bias, the professor did not know which\nprompt generated each version of the responses.\nThe first noticeable result is that, for both vari-\nants, the model achieved a score above the required\nthreshold to pass the final exam. This is not a minor\noutcome, since previous research has demonstrated\nthat these models struggle with difficult data struc-\nture tasks (Chang et al., 2023). ChatGPT's grades\nare similar to those achieved by the average student.\nOne might argue that matching human performance\nis profoundly meaningful. However, we see here\ntwo main sources of concern. First, the students ex-"}, {"title": "3.2 ChatGPT as a Grader", "content": "amined are novice undergraduates in the first year\nof training (most of them with only a few months\nof experience in programming). So, ChatGPT is\nnot really matching expert-level performance. Sec-\nond, ChatGPT's performance does not place it in\na position to be utilised as an intelligent assistant.\nYou can hardly exploit a tool that produces wrong\nresults more than 30% of the time.\nA second interesting result is that the use of\na complex prompt did not help the model. The\ncomplex prompt was never better than the simple\nprompt. No single question got a better response\nfrom the complex prompt. The specification of\nsystem role, the \"take-your-time\u201d advise and the\nprovided example do not seem to be useful and,\nperhaps, have introduced some confusion.\nRegarding specific questions, both variants strug-\ngled with question number 1 (syntactic and seman-\ntic specification of an ADT) and number 7 (rea-\nsoning about an example of divide and conquer\nalgorithm). In question 1, ChatGPT did not output\na formal specification of the ADT, failed to provide\na semantic description with proper algebraic nota-\ntion and often resorted to not-allowed expressions\n(e.g. using integer expressions in C rather than\ngeneric numerical expressions). We conjecture that\nthis might be related to the low availability of ADT\nexamples with proper notations in the training data.\nQuestion #7 was about interpreting different lev-\nels of computational complexity of a divide and\nconquer solution, based on variables such as the\nnumber of subproblems, size of the subproblems\nand so forth. This type of conceptual question also\nmade that ChatGPT failed loudly. For the rest of\nthe questions (#2-#6) ChatGPT made a decent job.\nThree of them were mainly coding tasks (#3, #5,\n#6) and two of them (#2, #4) required some sort\nof reasoning but they refer to well-known comput-\ning examples (Fibonacci or list search). In some\ninstances, the model did not follow the instructions\nand, rather than outputting solutions written in C, it\nprovided correct solutions written in Python. Note\nalso that ChatGPT did well on questions #3 and #5,\nwhich had a supporting image that the model could\nnot see.\nFigure 1 shows the grades assigned by ChatGPT\nand by the course's instructor to the five exams\nselected. The AI model clearly overestimates the\nquality of the solutions and, indeed, all exams got\na high qualification (all of them above 84%). Even\nlow quality solutions, such as the exam that offi-\ncially got a 38% overall score, were assigned very\ngood scores. This hardly positions ChatGPT as a\ntool to assist humans (professors or students) in the\nassessment of solutions for this type of exams.\nNext, we analyse the individual question-by-\nquestion assessments, see Table 3 in Appendix C.\nWe report the scores assigned and the deviation\nbetween ChatGPT and the instructor. The largest\ndeviation was found in Question 1, on ADT specifi-\ncation. This result is in agreement with the findings\nin Section 3.1, in which the model also struggled\nto solve this exercise. Again, this suggests that the\nmodel has little knowledge about this topic or it is\nnot able to transfer its knowledge to produce an-\nswers that comply with the instructions. The only\ndecent grading by ChatGPT was done for question\n4. This question was effectively solved by Chat-\nGPT (see section 3.1) and, here, the model also\nshows reasonably good performance at evaluating"}, {"title": "4 Discussion", "content": "question 4's answers written by students. These\nanswers are short paragraphs explaining the com-\nputational complexity of a given search problem\n(traversing over a list). A somehow surprising re-\nsult is that ChatGPT was highly effective at produc-\ning solutions for Question 5 but it drastically failed\nto assess the quality of Question 5 solutions written\nby students. This was a C function that implements\na recursive process and ChatGPT was unable to ef-\nfectively assess the quality of the functions written\nby students. Furthermore, ChatGPT's tendency to\noverrate the quality of the solutions was consistent\nover all types of questions\u00b3.\nNote also that ChatGPT assigned an overall\nscore to the best exam that was the same score as-\nsigned by the instructor (94%). But this seems to be\nanecdotal, as the individual question-by-question\nscores (Exam5) show substantial deviations.\nThe results of this study suggest that ChatGPT per-\nforms much better on solving exercises than it does\non grading them. As a solver, it is worth noting\nthe AI model's poor performance in some types of\nexercises (particularly in those that do not involve\ncoding). In the near future, it will be beneficial to\ninvestigate the reasons for this poor performance.\nFor instance, to understand whether it is related to\nthe language (Spanish) in which the problems are\nexpressed or due to a lack of training data for these\ntypes of exercises. The specific phrasing might\nhave also played an important role, as ChatGPT\nperformed poorly for exercise 7, while other ques-\ntions -also about computational complexity\u2013 had\nmuch better answers from the model. This sug-\ngests that wording might have a strong influence\non model's performance.\nOn the other hand, our results suggest that the\nmodel is useless to validate answers submitted by\nhumans. Even with coding questions that the model\nsolved very well, its assessment of solutions written\nby others was unsatisfactory."}, {"title": "5 Related Work", "content": "The development of Large Language Models\n(LLMs) has represented a paradigm shift for mul-\ntiple NLP tasks (Brown et al., 2020; Kojima et al.,\n2022). In some cases, these models even reach\nhuman performance. For instance, previous stud-\nies evaluated GPT-4, demonstrating its ability for\nseveral reasoning-intensive tests, such as passing a\ntechnical entrance exam for a software engineering\nposition (Bubeck et al., 2023).\nThe ability of these models to generate code has\nsparked interest in both the developer and teaching\ncommunities. In this direction, Chen et al. (Chen\net al., 2021) introduced Codex, a model specif-\nically fine-tuned for solving programming tasks.\nOther authors studied Codex's potential for solving\nPython tasks, demonstrating its effectiveness on the\nAPPS benchmark (Hendrycks et al., 2021). The\nauthors of (Sarsa et al., 2022) explored the possi-\nbility of integrating Codex in teaching duties, for\nexample to generate coding exercises. Similarly,\nXu and colleages (Xu et al., 2022) conducted a\nsystematic evaluation of different language mod-\nels -both proprietary like Codex and open source-\nfor coding completion and synthesis tasks. These\nauthors also proposed a novel fine-tuned model\nthat outperforms other alternatives for C program-\nming. Chang et al. (Chang et al., 2023) stated that\nChatGPT outperforms humans in simple coding\nassignments, but still struggles with data structure\nproblems and graph theory.\nIn this paper, we have presented a novel ap-\nproach to evaluate ChatGPT's programming abili-\nties. We evaluated its capacities to solve assorted\nprogramming-related exercises, spanning multiple\nareas such as abstract data types, data structures\nand computational complexity. Another novelty\nthat distinguishes our work from previous contribu-\ntions is that we also studied the AI model's capacity\nas a grader. Furthermore, our study targeted the\nSpanish language, thus responding to the growing\ninterest of the scientific community in evaluating\nthe capabilities of LLMs in languages other than\nEnglish (Deng et al., 2023)."}, {"title": "6 Conclusion", "content": "In this study, we assessed ChatGPT's capacities to\nsolve and grade programming exams (in Spanish)\nfrom an official university course. The results sug-\ngest that this AI model can only be used as a solver\nof basic coding exercises. Its abilities to solve and\nreason about intricate questions, and its capacity to\nassess solutions written by others are far from ef-\nfective. The study of more sophisticated prompting\nstrategies, such as those based on paraphrasing the\noriginal instructions, are left as future work."}, {"title": "Ethics Statement and Limitations", "content": "This research aims at evaluating the capabilities\nof the new generative AI models as a support tool\nin educational environments. Our study was con-\nstrained to exams written in Spanish because we\nwere specifically interested in analysing how LLMs\nperform in languages other than English. Access to\nthe exams was provided to us by the main instruc-\ntor of the university course. The exams filled by\nthe students were anonymised and their responses\nto the questions did not contain any personal or\nprivate reference.\nThe overall goal of this project is to gain an\nunderstanding on how the new Als could help to\nautomate or ease certain learning and grading tasks.\nThis research does not pursue the elimination of\nhuman instructors from the university classes. As\na matter of fact, we firmly believe that human-in-\nthe-loop strategies are crucial to properly exploit\nthe advantages and reduce the risks of AI-based\nagents.\nWe are also aware that more sophisticated mod-\nels, such as GPT4, could perform better. But, cur-\nrently, ChatGPT is a model that is freely available\nand it already has a huge user base worldwide (in-\ncluding many university students). Thus, our study\nwas centered on the most popular and widely avail-\nable platform. Anyway, in the future we will extend\nthis research to other LLMs. We also recognise that\nmore sophisticated prompt engineering could lead\nto better performance. We left this exploration for\nfuture research and we decided to employ here two\ninitial types of prompts."}, {"title": "A Prompts", "content": "A.1 Prompts for Solving\nA.1.1 Simple Prompt\nUnder this setting, no further instruction but the\nexam question was provided to the model:\nUser: \nSystem:\n\nA.1.2 Complex Prompt\nUnder this setting, we specified a system role to\ngive more context to the model about the task. Ad-\nditionally, the large prompt specifies that the model\ncan take \"time to reason\u201d (this is a recommended\npractice in these tasks) and, additionally, we also\nprovided a demonstration, which consists of a nat-\nural language instruction (asking to build a hello\nworld program) and the corresponding C code.\nSystem Role: Est\u00e1s respondiendo a las pre-\nguntas de un examen de inform\u00e1tica centrado\nen el lenguaje de programaci\u00f3n C.\nUser: Escribe un programa en C que es-\ncriba: Hello World\nAssistant: El siguiente c\u00f3digo est\u00e1 escrito\nen C:\n{\n#include <stdio.h>\n}\nint main(void)\nprintf(\"Hello World\");\nreturn 0;\n}\nUser: La siguiente es una pregunta de un\nexamen de programaci\u00f3n del primer a\u00f1o del\ngrado de ingenier\u00eda inform\u00e1tica. Hay bas-\ntante tiempo para responder, as\u00ed que t\u00f3mate\nel tiempo que sea necesario para dar una re-\nspuesta completa y razonada paso a paso. La\npregunta est\u00e1 delimitada por < >. Adem\u00e1s\nen el caso de que tengas que escribir c\u00f3digo,\nprimero especifica el lenguaje en el que est\u00e1\nescrito, y luego escribe dicho c\u00f3digo delim-\nit\u00e1ndolo con antes de la primera l\u00ednea y\ndespu\u00e9s de la \u00faltima, tal y como has hecho\nanteriormente:\n\nSystem:"}, {"title": "A.2 Prompt for Grading", "content": "Under this setting, the prompt asks the model to\nreason about the model's response to the question\nand, next, it asks the model to compare it against\nthe provided response and, finally, give an overall\nquality score for the provided response.\nUser: Tu tarea es evaluar la respuesta a una\npregunta de un examen de programaci\u00f3n. Para\nello razona primero tu respuesta y comp\u00e1rala\ncon la respuesta proporcionada. No la eval\u00faes\nhasta que no hayas respondido t\u00fa mismo a la\npregunta. La pregunta est\u00e1 delimitada por <...>\ny la respuesta a evaluar est\u00e1 entre \"...\". El for-\nmato de tu respuesta debe ser el siguiente, re-\nsp\u00e9talo sin a\u00f1adir ning\u00fan comentario adicional\ny aseg\u00farate de escribir una nota num\u00e9rica so-\nbre 100:\nPregunta: (copia aqu\u00ed la pregunta del examen\nentre <...>)\nRespuesta: (copia aqu\u00ed la respuesta del alumno\nentre \"...\")\nCalificaci\u00f3n: La nota es (nota sobre 100%)\n\"RESPONSE\"\nSystem:"}, {"title": "B Exam Questions", "content": "Table 2 details the questions that made up the exam."}, {"title": "C Evaluating Results", "content": "Table 3 breaks down the grades assigned by both\nassessors."}]}