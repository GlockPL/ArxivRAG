{"title": "DefTransNet: A Transformer-based Method for Non-Rigid Point Cloud Registration in the Simulation of Soft Tissue Deformation", "authors": ["Sara Monji-Azad", "Marvin Kinz", "Siddharth Kothari", "Robin Khanna", "Amrei Carla Mihana", "David M\u00e4nnel", "Claudia Scherl", "J\u00fcrgen Hesser"], "abstract": "Soft-tissue surgeries, such as tumor resections, are complicated by tissue deformations that can obscure the accurate location and shape of tissues. By representing tissue surfaces as point clouds and applying non-rigid point cloud registration (PCR) methods, surgeons can better understand tissue deformations before, during, and after surgery. Existing non-rigid PCR methods, such as feature-based approaches, struggle with robustness against challenges like noise, outliers, partial data, and large deformations, making accurate point correspondence difficult. Although learning-based PCR methods, particularly Transformer-based approaches, have recently shown promise due to their attention mechanisms for capturing interactions, their robustness remains limited in challenging scenarios. In this paper, we present DefTransNet, a novel end-to-end Transformer-based architecture for non-rigid PCR. DefTransNet is designed to address the key challenges of deformable registration\u2014including large deformations, outliers, noise, and partial data-by inputting source and target point clouds and outputting displacement vector fields. The proposed method incorporates a learnable transformation matrix to enhance robustness to affine transformations, integrates global and local geometric information, and captures long-range dependencies among points using Transformers. We validate our approach on four datasets: ModelNet, SynBench, 4DMatch, and DeformedTissue, using both synthetic and real-world data to demonstrate the generalization of our proposed method. Experimental results demonstrate that DefTransNet outperforms current state-of-the-art registration networks across various challenging conditions. Our code and data are publicly available at https://https://github.com/m-kinz/DefTransNet and https://doi.org/10.11588/data/R9IKCF.", "sections": [{"title": "1. Introduction", "content": "Tissue deformation is a critical issue in soft-tissue surgery. It occurs after wound opening due to factors like tension loss, patient positioning, or tissue extraction. This effect has been particularly noted in head and neck surgery, especially during tumor resection, where critical structures are closely located and may require protection [41, 35].\nPoint cloud registration is a key aspect of computer vision, focused on estimating transformations to align sets of corre-sponding points [38]. This technique is essential in a variety of fields, including virtual and augmented reality [34], LiDAR-based applications [51], and quality control in manufacturing [50]. The aim of point cloud registration is to reduce align-ment errors between transformed and target point clouds [14].\nRegistration methods can generally be divided into two types: rigid and non-rigid transformations. Rigid registration applies affine transformations like rotation and translation, preserving the object's geometry and shape, while non-rigid registration de-termines a deformation field to match the source and target point clouds [38]. The central challenges include identifying stable corresponding points and devising an accurate transformation function, while also ensuring robustness against variations in deformation, noise, outliers, and incomplete data [9, 49]. Recent advancements in non-rigid point cloud registration often catego-rize methods as either coarse or fine, with another classification separating feature-based methods-focused on matching distinc-tive features-from those that directly estimate the deformation field [14].\nBoth rigid and non-rigid registration approaches can fur-ther be divided into non-learning and learning-based methods. Non-learning techniques rely primarily on iterative optimiza-tion to derive the transformation, using predefined mathematical models and metrics to align point clouds [22]. For instance,"}, {"title": "2. Related Work", "content": "In recent years, the use of transformers and DGCNN-based approaches for non-rigid point cloud registration has increased. This section presents a review of several registration methods based on these two approaches. A summary of some notable methods is provided in Table 1.\nTransduction models refer to machine learning models that convert one type of data (usually a sequence) into another type, while preserving the input-output structure [5]. In the context of sequence transduction, the goal is to map an input sequence (e.g., a sentence in one language) to an output sequence (e.g., the translated sentence in another language). Transduction mod-els typically use architectures like Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), or Transform-ers, which are designed to handle sequential data efficiently. Most sequence transduction models use complex recurrent or convolutional neural networks with an encoder-decoder struc-ture, often improved by attention mechanisms [10]. This setup is particularly effective for tasks such as machine translation,"}, {"title": "3. Problem Definition of Non-Rigid Point Cloud Registra-tion", "content": "Let X = {x; \u2208 R3 | i = 1,...,N} and Y = {yj \u2208 R3 | j = 1,..., M} be two sets of 3D points. The goal of non-rigid point cloud registration is to find a transformation T that deforms the source point cloud X to align it with the target point cloud Y. This transformation maps each point x; in the source to a transformed point x = F(x) in R3.\nThe objective is to minimize the alignment error by reduc-ing the difference between transformed points x and their cor-responding points in Y. This is achieved by minimizing the following cost function E(T):\n$E(T) = \\sum_{i=1}^{N} ||x'_i - y_{j(i)}||^2,$\nwhere || || denotes the Euclidean distance, and yj(i) is the corresponding point in Y for each transformed point x."}, {"title": "4. Proposed Method", "content": "The proposed network in this paper, DefTransNet, is pre-sented in two main subsections: the feature descriptor network and the learning displacement network. DefTransNet is visual-ized in Figure 1.\nFeature descriptor network. The proposed method for fea-ture description involves three key steps. The approach begins by using a learnable transformation matrix to align and stan-dardize input point clouds, ensuring feature descriptor invari-ance to rotations around coordinate axes [26]. A multilayer perceptron (MLP), comprising three 1D convolutional layers (output sizes: 64, 128, 1024), is employed, each followed by instance normalization and ReLU activation. This generates local feature vectors capturing point cloud shape and structure. A max-pooling operation on the 1024 features yields a global feature vector summarizing the entire point cloud. This global vector is fed into another MLP with three linear layers (512, 256, 9 neurons). The final layer outputs a 3\u00d73 matrix representing the transformation matrix, applied to align the input point clouds.\nThe second stage is grounded in the concept of integrating global and local geometric information for aggregating features"}, {"title": "5. Evaluation", "content": "Several scenarios are considered to evaluate the robustness and generalization of the proposed DefTransNet. Four datasets are used for this purpose: ModelNet [55] and SynBench [29, 40] as synthetic datasets, and DeformedTissue [41] and 4DMatch [31] as real-world datasets. In the following subsections, these datasets are introduced, followed by a discussion on the ro-bustness of DefTransNet to various challenges, namely differ-ent deformations, noise, outlier levels, and overlap ratios. To this end, the robustness of the proposed method is discussed in comparison to other baselines, namely Robust-DefReg [40], Deep-Geo-Reg [20], Predator [24], GP-Aligner [49], [13] with regularization, and [13] without regularization."}, {"title": "5.1. Datasets", "content": "ModelNet Dataset. One of the datasets used for registration in this study is ModelNet [55]. The ModelNet10 dataset, a subset of ModelNet40, includes 4,899 pre-aligned shapes across 10 categories, with 3,991 shapes (80%) designated for training and 908 shapes (20%) reserved for testing. During the training phase, the models undergo random rotations of up to 45 degrees around the z-axis.\nSynBench Dataset. In a previous study, the authors intro-duced SimTool [39], a toolbox for simulating soft body defor-mation and generating deformable point clouds. We utilized SimTool to create SynBench, a benchmark specifically designed for evaluating non-rigid point cloud registration methods. The SynBench dataset, available at [29], consists of five primary sets: \"Data,\" containing 30 primitive objects, and four challenge cate-gories \"Deformation Level,\" \"Incompleteness,\" \"Noise,\" and \"Outlier\" with 5302, 26515, 21213, and 26500 object samples, respectively. Each sample includes a source and target point cloud pair, effectively doubling the total number of files. The increased file count in certain challenges results from applying each challenge across various deformation levels and adjusting effective parameters.\nThe SynBench dataset defines 10 deformation levels, ranging from 0.1 for the lowest level to 1.0 for the highest. As the dataset is synthetically generated, the initial point clouds are noise-free, enabling the controlled addition of synthetic noise. Gaussian noise with zero mean and standard deviations between 0.01 and 0.04 is commonly applied in studies, such as [30] and [3], to represent small to large noise levels. Accordingly, the noisy dataset is categorized into four groups based on standard deviations: 0.01, 0.02, 0.03, and 0.04 per point set.\nMore information about the SynBench dataset has already been published in our previous paper [40].\nas it causes landmark displacement, complicating tissue orien-tation. The authors conducted an experimental study on 45 pig head cadavers to simulate tissue deformation, approved by the Mannheim Veterinary Office (DE 08 222 1019 21) [41] [35]. We used 3D cameras and head-mounted displays to capture tissue shapes before and after controlled deformation induced by heat-ing. The data were processed using software such as Meshroom, MeshLab, and Blender to create and evaluate 212D meshes. The dataset is available upon request from readers. Some samples of the captured dataset with HoloLens 2 and ArtecEva are shown in Figure 2.\nThe dataset includes different levels of deformation, noise, and outliers, generated using the same approach as the Syn-Bench dataset. In this paper, the results of the approaches for deformation levels between 0.1 and 0.7 are reported.\n4DMatch/4DLoMatch Dataset. 4DMatch [31] is a bench-mark dataset designed for registration and matching tasks, appli-cable to both rigid and deformable scenes. 4DMatch is a partial point cloud benchmark, while its low-overlap variant is referred to as 4DLoMatch. This dataset is captured using sequences from the [33] dataset, and it provides ground-truth-dense correspon-dences. The inclusion of time-varying geometry in both datasets introduces additional challenges for matching and registration tasks.\nEach file in the dataset contains the following attributes: the source point cloud X, a deformation array D, the target point cloud Y, the rotation matrix R, the translation vector t, the overlap rate, and the set of corresponding points. For our work, the dataset is modified to generate a new target point cloud Y' using the formula from [32], and the original target point cloud is discarded.\n$Y = t^2 + (X + D).R^T$"}, {"title": "5.2. Robustness to Different Deformation Levels", "content": "Table 2 presents a comprehensive comparison of the per-formance of several non-rigid point cloud registration methods, including Robust-DefReg [40], Deep-Geo-Reg [20], Predator [24], and GP-Aligner [49], across three datasets: SynBench (synthetic), ModelNet (synthetic), and DeformedTissue (real-world). The evaluation metric used is the mean distance error, where smaller values reflect better alignment accuracy. Let X = {x1,..., Xn} and Y = {y1,..., yn} represent the 3D points in the source and target point clouds, respectively, where x\u2081 = (x1, x2, x3) and y\u2081 = (y, y, y). The Euclidean distance between corresponding points xi and yi is given by:\n$d(x_i, y_i) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (x_3 - y_3)^2}.$\nThe mean distance Dmean is then computed as:\n$D_{mean} = \\frac{1}{n} \\sum_{i=1}^{n} d(x_i, y_i).$\nThe deformation levels, ranging from 0.1 to 0.8, progres-sively introduce higher degrees of non-rigid deformation, allow-ing for a robust evaluation of each method.\nThe results demonstrate that the proposed method, Def-TransNet, consistently outperforms the state-of-the-art methods across all datasets and deformation levels.\nIn the SynBench dataset, DefTransNet achieves the lowest mean distance errors across all deformation levels, showcasing its ability to maintain precision even as deformation increases. At a minimal deformation level of 0.1, DefTransNet achieves an error of 0.00015, which is lower than Robust-DefReg (0.00047) and Deep-Geo-Reg (0.00067). As deformation intensifies, Def-TransNet remains highly robust, with the error increasing only slightly to 0.02110 at the highest deformation level of 0.8. In contrast, methods such as Predator and GP-Aligner show larger errors, particularly at higher deformation levels, indicating their limitations in handling severe point cloud distortions.\nA similar performance trend is observed for the ModelNet dataset. DefTransNet again achieves superior accuracy across all"}, {"title": "5.3. Robustness to Different Noise and Outlier Degrees", "content": "Table 3 evaluates the performance of point cloud registration methods under increasing levels of Gaussian noise (0.01, 0.03, and 0.05) for the SynBench and ModelNet datasets. Noise can distort point positions, posing a challenge to maintaining precise alignments.\nFor the SynBench dataset, DefTransNet stands out with the lowest mean distance errors across all noise levels. At a low noise level of 0.01, DefTransNet achieves an error of 0.01544, significantly outperforming methods such as Robust-DefReg (0.05393) and Deep-Geo-Reg (0.07463). Even when the noise increases to 0.05, DefTransNet remains reliable with an error of 0.06019, while other methods, such as Predator and GP-Aligner,"}, {"title": "5.4. Robustness to Different Overlap Ratios", "content": "Table 5 evaluates the performance of DefTransNet compared to other methods under varying overlap ratios (0.1 to 0.9) on the 4DMatch dataset. Two scenarios are considered: with rota-tion and without rotation. The Chamfer distance error serves as the evaluation metric, where lower values indicate better perfor-mance. The Chamfer Distance between two 3D point clouds X and Y is defined as:\n$d_{Chamfer} (X, Y) = \\frac{1}{|X|} \\sum_{x \\in X} min_{y \\in Y} ||y - x||^2 + \\frac{1}{|Y|} \\sum_{y \\in Y} min_{x \\in X} ||x - y||^2,$"}, {"title": "6. Discussion", "content": "In this work, we introduced DefTransNet, a Transformer-based framework designed to address the most pressing chal-lenges in non-rigid point cloud registration. The results obtained across multiple datasets-spanning from synthetic datasets like ModelNet and SynBench to more complex real-world datasets such as DeformedTissue and 4DMatch-demonstrate that our approach significantly advances the state of the art. In particular, the comparative evaluations against leading registration methods, including Robust-DefReg [40], Deep-Geo-Reg [20], Predator [24], and GP-Aligner [49], highlight the exceptional robustness and accuracy of our framework.\nA defining characteristic of DefTransNet is its ability to han-dle large and complex deformations. Previous methods often suffer from feature ambiguity and degraded performance as de-formation levels rise. Our results show a clear improvement in alignment accuracy at all tested deformation levels. Even under severe deformations, DefTransNet maintains a stable error profile, outperforming other methods by a considerable margin. This can be attributed to the integration of Transformers, which excels at capturing global context and long-range dependencies, thus producing more discriminative and reliable feature repre-sentations. In contrast, earlier approaches relying solely on local geometric cues or traditional iterative optimization struggle to retain robustness as deformations become more pronounced.\nAnother key advantage introduced by DefTransNet is its improved resilience to noise and outliers. Realistic scenarios, especially in medical applications like soft-tissue surgery, are replete with irregularities due to sensor limitations, occlusions, and partial data. While baseline methods show a significant per-formance drop under even moderate noise and high outlier levels, our method consistently achieves the lowest mean distance er-rors. The attention-based feature descriptor in DefTransNet,"}, {"title": "7. Conclusion", "content": "In this paper, we presented DefTransNet, a novel Transformer-based framework for non-rigid point cloud registration. By integrating a robust feature descriptor network and a learning displacement network, DefTransNet addresses critical challenges such as large deformations, noise, outliers, and partial data. Through extensive evaluation of synthetic and real-world datasets, including ModelNet, SynBench, DeformedTissue, and 4DMatch, our method demonstrated superior accuracy and robustness compared to state-of-the-art approaches. The use of Transformers enabled DefTransNet to capture global and local geometric relationships, effectively mitigating feature ambiguity and enhancing registration perfor-mance across diverse conditions. With its ability to generalize to unseen data and handle complex scenarios, DefTransNet establishes a significant advancement in non-rigid point cloud registration, paving the way for improved applications in medical imaging, robotics, and beyond. Future work will focus on optimizing computational efficiency and exploring domain adaptation techniques for broader applicability."}]}