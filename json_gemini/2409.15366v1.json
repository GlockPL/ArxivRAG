{"title": "Trajectory Anomaly Detection with Language Models", "authors": ["Jonathan Kabala Mbuya", "Dieter Pfoser", "Antonios Anastasopoulos"], "abstract": "This paper presents a novel approach for trajectory anomaly detection using an autoregressive causal-attention model, termed LM-TAD. This method leverages the similarities between language statements and trajectories, both of which consist of ordered elements requiring coherence through external rules and contextual variations. By treating trajectories as sequences of tokens, our model learns the probability distributions over trajectories, enabling the identification of anomalous locations with high precision. We incorporate user-specific tokens to account for individual behavior patterns, enhancing anomaly detection tailored to user context. Our experiments demonstrate the effectiveness of LM-TAD on both synthetic and real-world datasets. In particular, the model outperforms existing methods on the Pattern of Life (PoL) dataset by detecting user-contextual anomalies and achieves competitive results on the Porto taxi dataset, highlighting its adaptability and robustness. Additionally, we introduce the use of perplexity and surprisal rate metrics for detecting outliers and pinpointing specific anomalous locations within trajectories. The LM-TAD framework supports various trajectory representations, including GPS coordinates, staypoints, and activity types, proving its versatility in handling diverse trajectory data. Moreover, our approach is well-suited for online trajectory anomaly detection, significantly reducing computational latency by caching key-value states of the attention mechanism, thereby avoiding repeated computations. The code to reproduce experiments in this paper can be found at the following link: https://github.com/jonathankabala/LMTAD.", "sections": [{"title": "INTRODUCTION", "content": "Effective techniques for gathering and analyzing movement data, including the contribution of this work on anomaly detection are becoming increasingly important with the growth in terms of data and different types of applications. Specifically, trajectory anomaly detection has several interesting and practical use cases across various fields, such as Transportation and Traffic Analysis (accident detection, road safety analysis), Maritime Navigation and Safety (shipping lane monitoring, piracy detection), Air Traffic Control (airspace safety), Wildlife Monitoring (behavior change), Sports Analysis (injury prevention, game strategies), Healthcare and Elderly Care (behavior change and detecting health issues or emergencies), Disaster Response and Management (disaster response and crowd monitoring) and Urban Planning and Smart Cities (mobility analysis, public transit optimization, pedestrian safety). This work focuses on detecting trajectory anomalies that deviate from patterns observed in collections of historical datasets.\nExtensive research has been conducted on trajectory anomaly detection for unlabeled data [6, 12, 17, 29, 36, 38]. However, this body of prior work has several limitations.\nFirstly, it is difficult to pinpoint specific locations within the trajectory where the anomaly occurs, as the anomaly score is attributed to the entire trajectory or sub-trajectory. Secondly, these"}, {"title": "RELATED WORK", "content": ""}, {"title": "Trajectory Anomaly Detection", "content": "Existing work for anomaly detection in trajectories can be grouped into two broad categories: heuristic-based methods [6, 14, 18, 36, 43] and learning-based methods [28, 29].\nHeuristic-based methods primarily rely on hand-crafted features to represent normal routes and employ distance or density metrics to compare a target route to normal routes. The study in [14] suggests a partition-and-detect framework for trajectory outlier detection, effectively identifying outlying sub-trajectories by combining two-level trajectory partitioning with a hybrid distance-based and density-based detection approach. Studies by [36] and [6] introduce related methods that systematically extract, group, and analyze trajectories based on the source and destination. These methods identify anomalies by how rare they are and how much"}, {"title": "Language Modeling on Trajectory Data", "content": "The field of language modeling has received much attention recently since the introduction of the transformer model [32]. Language models like BERT [9], GPT-2 [26], and LLAMA [30] have been shown to achieve great performance on a variety of natural language tasks, including question-answering, sentiment analysis, and text generation. Language modeling techniques have been extended to other applications, including image classification [25, 42] and speech processing [3, 4]. Recent studies have also applied language modeling techniques to a wide range of applications on mobility data. For example, the work in [33] leverages language modeling techniques for human mobility forecasting tasks, while the work in [13] uses similar techniques to predict the next visited location in a trajectory. The work in [20] proposes a conceptualization of a BERT-inspired system tailored for trajectory analysis. However, none of the previous work used a generative approach for anomaly detection in trajectory data."}, {"title": "PROBLEM FORMULATION", "content": "A trajectory is a finite chronological sequence of visited locations and can be modeled as a list of space-time points modeled as location and time stamp pairs $T = p_0,..., p_n$ with $p_i = (l_i, t_i)$ and $l_i e R^2$, $t_i \u2208 R^+$ for $i = 0, 1, . . ., n$ and $t_0 < t_1 < t_2 < ... < t_n$ (cf. [23]). In the simplest case, a location $l_i$ is represented as a geographic coordinate in two-dimensional space. Other representations are to map locations to cells of a discretized space such as a regular spatial or a hexagonal grid [31].\nAlternatively, $l_i$ can capture qualitative staypoints (visited points of interest such as \u201chome\u201d, \u201cwork\u201d, or \u201crestaurant\u201d) or spatial partitions that capture functional areas of a city, e.g., \"commercial\", \"business\", or \"residential\" areas. Therefore, $l_i$ can include both a staypoint and afunctional area, e.g., $l\u2081 = [apartment, downtown])$. A collection of related trajectories $T_i$ constitutes a dataset D. The dataset D may contain both normal and anomalous trajectories. In general, an anomalous trajectory refers to one that does not show a normal mobility pattern and deviates from the majority of the trajectories in D [7, 38]. Given a dataset D with n trajectories, our goal is to train a model that distinguishes between normal and anomalous trajectories without having explicit labels."}, {"title": "METHOD", "content": "Our approach is to train a model that learns probability distributions over trajectories. An autoregressive generative model will allow us to infer the probability of a trajectory given the historical context\n$P(T) = p(l_1)p(l_2|l_1)p(l_3|l_{1:2}) . . .p(l_i|l_{<i})...p(l_n|l_{<n})$ (1)\nwhere the probability of each location $p(l_i|l_{<i})$ is conditioned on a complete location history. We note that there is no time bound between locations. However, we could use such information as part of the input. With this approach, we can find anomalous trajectories and identify exactly which locations in the trajectory are anomalous."}, {"title": "Model and Architecture", "content": "Given a dataset of trajectories $D = {T_1, ..., T_m}$, LM-TAD's goal is to maximize the likelihood of all the trajectories in the dataset:\n$L(D) = \\sum_{i=1}^{|D|} \\sum_{j=1}^{Ti} log P(l_i|l_{<i}; \\theta)$ (2)\nwhere $P(-; \\theta)$ is the conditional probability modeled by a neural network parameterized by \u03b8. To learn the parameters 0, we opt for a transformer-based network architecture [32]. This architecture choice is motivated by its proven efficacy in natural language generation tasks, suggesting its potential applicability and effectiveness in modeling trajectories as statements.\nFigure 2 shows the overall architecture of our method, LM-TAD, which consists of positional and token embeddings, N transformer blocks followed by a linear transformation, and a softmax layer. To capture input semantics, the token embedding layer transforms each token (location) from a categorical type to a finite-dimensional real-valued vector. Positional embeddings play a critical role in the training process, compensating for the absence of inherent sequential ordering within the causal-attention module. Each transformer block comprises a multi-head causal-attention mechanism, which is preceded and succeeded by a layer-normalization layer and a feedforward layer. In the multi-head causal-attention mechanism, a trajectory is transformed into three sets of vectors -keys, values, and queries- and then split into multiple heads for parallel processing. Each head independently computes a scaled dot-product attention to get attention scores that assess the relevance of different locations (tokens) in a trajectory. This allows the model to concurrently learn dependencies between locations, such as temporal or spatial ones. The outputs from all heads are concatenated and linearly transformed to produce the final output. Additionally, the causal-attention mechanism includes a masking operation to prevent the attention function from accessing information from future tokens (locations), given the autoregressive nature of our approach.\nBelow is the formal description of the muti-head self-attention:\n$Attention (Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ (3)\nwhere $d_k$ is the dimension of the keys. Concatenating the output values results in:\n$MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O$\nwith $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$ (4)\nwhere the $W_i^Q \u2208 R^{dmodelxdq}, W_i^K \u2208 R^{dmodelxdk}, W_i^V \u2208 R^{dmodelx dv}$ are projection matrices that are learned during training. The projection matrix $W^O$ linearly combines the outputs from different attention heads, enabling the model to flexibly adjust and fine-tune the aggregated attention, thereby enhancing the model's capacity to learn complex patterns. In LM-TAD, $dq = dk = dv = dmodel/h$ where h is the number of heads.\nThe feedforward layer consists of two linear transformations linked with a ReLU activation function:\n$FFM(x) = max(0, xW_1 + b_1) W_2 + b_2$ (5)\nwhere the weights $W_1 \u2208 R^{dmodelxdff}, W_2 \u2208 R^{dff\u00d7dmodel}$ and the biases $b_1 \u2208 R^{dff}, b_2 \u2208 R^{dmodel}$. The transformer block uses the layer-normalization layer in addition to residual connections to stabilize learning and improve training efficiency.\nThe output of the transformer block goes through linear and softmax layers to predict the distribution of each token in a trajectory."}, {"title": "Location Configurations", "content": "An advantage of using a generative approach to model trajectories is the ability to abstract the locations of a trajectory in different ways. Figure 3 provides examples. In the simplest case, a trajectory can be represented by a finite chronological sequence of GPS coordinates. These coordinates can be discretized using regular grid cells (Figure 3a) [15] or hexagons [10]. However, various other trajectory configurations are possible. Instead of GPS coordinates, we can also use staypoints (\"home\", \"workplace\u201d, \u201crestaurant\", etc.) (Figure 3b) or points of interest. We can even model a trajectory as a chronological sequence of a person's activities (\"eating\", \"working\u201d, and \"playing sports\") (Figure 3d), where each location in the trajectory corresponds to the activity of a person at a particular time. These trajectories can even be enhanced with additional metadata, such as the dwell time at a location (Figure 3c), method of transportation,\""}, {"title": "Anomaly Score", "content": "We use perplexity to determine how anomalous a trajectory is. Perplexity is a well-established measure to evaluate language models [5, 9, 22], and can be viewed as a measure of uncertainty when predicting the next token (location) in a trajectory [21]. Equation 6 shows how we can calculate the perplexity of a trajectory T with t locations, and Equation 7 shows how we compute the perplexity over a dataset D with n trajectories Ti.\n$PPL(T) = exp(-\\frac{1}{t} \\sum_{i=1}^{t} log P(l_i|l_{<i}))$ (6)\n$PPL(D) = \\sqrt[n]{\\prod_{i=1}^n PPL(T_i)}$ (7)\nThe lowest possible perplexity is 1, which implies that the model can correctly predict the next location with absolute certainty. However, the maximum of this measure is unbounded. To determine when a trajectory is anomalous (\"high\" perplexity), we need to provide a threshold. We note that the choice of a threshold can be application- and dataset-dependent [2]. We can compute the threshold as follows:\nthreshold = mean [PPL(D)] + std[PPL(D)]\nwhere mean [PPL(D)] and std [PPL(D)] are the mean and standard deviation of the perplexities of n training trajectories. To identify abnormal trajectories with respect to a specific user, we customize the threshold for each user. Here, the mean and standard deviation will be computed only using the training samples of that user."}, {"title": "EXPERIMENTS", "content": "This section presents the experimental setup used to evaluate the effectiveness of our proposed LM-TAD model. We compare our method to several state-of-the-art baselines using two different datasets and utilizing different evaluation metrics to measure the accuracy and robustness of anomaly detection."}, {"title": "Datasets & Preprocessing", "content": "We use simulated and real-world datasets. Specifically, we use the Pattern-of-Life (PoL) simulation dataset [45] and the Porto taxi dataset [34, 35]."}, {"title": "Pattern-of-Life Dataset (PoL)", "content": "The PoL dataset was generated through the Pattern-of-Life (PoL) simulation [1, 45]. This simulation consists of virtual agents designed to emulate humans' needs and behavior by performing human-like activities. Activities include going to work, restaurants, and recreational activities with friends. These activities are performed at real locations obtained from OpenStreetMap [11]. While agents engage in these activities, the simulation also records the location, which includes the GPS coordinates and the staypoints (i.e., home, work, restaurant), as well as the respective timestamps.\nUsing the raw data from the PoL simulation, we created daily trajectories for each agent, consisting of places they visited on that particular day. The geographic coverage was Atlanta, GA and we simulated the behavior of an agent population consisting of working professionals.\nThe dataset includes an average of 450 daily trajectories for each of the 1000 generated agents, resulting in a total of 444,634 trajectories. Each input to the model represents a virtual agent's daily trajectory. To capture the patterns of each agent, the agent ID is included at the beginning of the trajectory. Based on the hypothesis that individual behavioral patterns exhibit consistency on the same days of the week, we also incorporate weekday information into the feature vector to enhance the model's ability to detect anomalies.\nFor example, a daily trajectory is represented as: [agent_ID, weekday, work, restaurant, apartment]. We also consider other location representations, including discretized GPS coordinates and the duration of stay at a location. We use Uber hexagons [31] for discretized locations and discretize the stay duration into 1-hour buckets using a sequence of bucket IDs as input to the model.\nThe PoL dataset comes with labels to identify anomalous trajectories generated by the simulation. To introduce anomalies, the simulation selects ten virtual agents exhibiting anomalous behaviors. For example, work anomaly is one type supported by this simulation: agents with work anomalies will abstain from going to work when they typically would.\nFor agents with anomalous trajectories, we have the first 450 days representing normal behavior and the last 14 days that exhibit anomalous behavior.\nWe trained our model on the entire dataset, including the additional 14 days of anomalous behavior from the ten virtual agents, to ensure it could identify outliers even when they were present in the training data. We then tested our methods against the baselines using the entire dataset"}, {"title": "Porto Dataset", "content": "The Porto dataset consists of data generated by 442 taxis operating in the city of Porto in Portugal from January 07, 2013, to June 30, 2014. A taxi reports its GPS location at 15s intervals. We employed preprocessing steps similar to [17] and [15]. We discretized the map into 100m \u00d7 100m grids and group trajectories with the same source and destination. We discarded trajectories belonging to a \"source-destination\" group with fewer than 25 trajectories. The input to our model consists of a vector of chronologically ordered and discretized GPS coordinates (grid cells) prepended with SOT (start of trajectory) and appended EOT (end of trajectory) tokens."}, {"title": "Tokenization & Vocabulary", "content": "Given the nature of a language model architecture, we created tokens to form our model's vocabulary. In the Porto dataset, a token is considered a discretized GPS coordinate. We also added three special tokens: SOT (start of trajectory), EOT (end of trajectory), and PAD (padding token to help with batch training). In the POL dataset, tokens consist of staypoints (work, apartment, restaurant, etc.), days of the week (Monday, Tuesday, etc.), agent ID, and the EOT and PAD special tokens."}, {"title": "Baselines", "content": "We compared our method to existing unsupervised anomaly detection methods on trajectory data. Given the established better performance of deep learning methods on trajectory anomaly detection [15], we omitted the inclusion of traditional clustering-based algorithms.\n* SAE: a standard autoencoder method trained to optimize the reconstruction loss of a trajectory sequence using a recurrent neural network. Based on the work in [19] and [2], we use the reconstruction error as the anomaly score.\n* VSAE A method similar to SAE, however, in addition to optimizing for the reconstruction loss, it also optimizes the KL divergence between the learned distribution over the latent space and a predefined prior [2, 27]. Similar to SAE, we use the reconstruction error as the anomaly score."}, {"title": "Evaluation Metrics", "content": "We use Precision-Recall AUC and F1 scores to evaluate the performance of our method and the baseline methods [17, 37]. These metrics are suitable for assessing the performance of anomaly detection methods as the number of anomalies in each dataset is small compared to normal trajectories. For the Porto dataset, these metrics are computed across all trajectories. Conversely, we conduct these evaluations on a per-virtual-agent basis for the Pattern-of-Life (PoL) dataset. In addition, the surprisal rate metric is used to locate the specific occurrence of an anomaly within a trajectory."}, {"title": "RESULTS", "content": "The following sections present the anomaly detection results for the PoL and Porto datasets. Anomaly detection for the Porto dataset is a global challenge, since taxi movements are customer/ride-driven and the movements captured by individual trajectories are largely independent. This is in contrast to the PoL dataset, which contains sets of trajectories that model the behavior of individual agents, and anomaly detection will be agent-specific."}, {"title": "Agent-based Outliers - Patterns-of-Life Data", "content": "The PoL dataset has user IDs, i.e., sets of trajectories that can be linked to a specific user. The anomaly detection challenge can become user-specific, as the anomalous behavior of one agent may be the normal behavior of another agent. Therefore, we report results for the ten virtual agents that each have 14 additional days of anomalous behavior. Table 1 summarizes F1 and Precision-Recall Area-under-the-Curve (PR-AUC) results for these ten anomalous agents. The results of the rest of the agents are not included in table 1 since these agents are not anomalous within their respective contexts. LM-TAD outperforms all competitor methods as it is more efficient in finding anomalies with respect to anomalous users. We identify three reasons why autoencoder approaches do not identify"}, {"title": "Global Outliers - Porto Taxi Data", "content": "The results for the Porto taxi dataset are summarized in Table 2. We use different parameter configurations a and \u1e9e for random-shift"}, {"title": "Identifying Anomalies using Surprisal Rate", "content": "Perplexity as an aggregate measure may not be sufficient to identify anomalous trajectories. Consequently, the presence of only a few anomalous tokens may lead to their signal being diluted by the averaging process and anomalies go undetected. Similar limitations apply to autoencoder-based methods, where the reconstruction loss is calculated over all tokens in a trajectory.\nA further limitation in using perplexity or reconstruction error is the inability to pinpoint the specific location of anomalies within a trajectory. Here, our work proposes the surprisal rate measure that operates at the level of individual locations or tokens within a trajectory.\nIn our empirical analysis, we explored the application of the surprisal rate for detecting potentially anomalous locations within a trajectory in the PoL data. A high surprisal rate suggests that a particular location in a trajectory may be anomalous. Figure 6 shows the surprisal rate for 30 trajectories (10 anomalous and 20 normal ones randomly chosen from the respective agents). The analysis reveals that certain tokens in anomalous trajectories exhibit significantly higher surprisal rates compared to those in normal trajectories, particularly at the beginning of the trajectories. This pattern aligns with the dataset's structure and the configuration of our input vector, where the initial tokens represent the agent ID, the weekday, and the first location visited by the virtual agent on that day. Given the expected pattern of agents visiting consistent locations on specific weekdays, deviations from this routine, such as visiting an atypical location as the first destination, are flagged as anomalies. Consequently, the inclusion of the weekday token in the trajectory analysis enables the identification of instances where an agent's initial location deviates from the norm, resulting in a larger surprisal rate when an agent visits an unusual place on a given weekday."}, {"title": "Location Configurations - Ablation Study", "content": "We conducted an ablation study to show the versatility of LM-TAD in working with different types of inputs. In this study, we explore the usage of discretized GPS coordinates (Uber hexagons [31]), staypoint labels (i.e., work, restaurant, and so forth), and stay duration (the duration at a particular location) as input for the model to infer the anomaly detection performance of each modality. One of the main advantages of using one modality over another is the type of anomalies we are interested in discovering. Anomalies can be related to the duration of stay in a particular location (staying longer than usual), by visiting an uncommon geographical area, or by visiting a different place (e.g., shopping mall) on a weekday when one is supposed to be elsewhere (e.g., work).\nTable 3 summarizes the results of using various location configurations in the PoL dataset. Staypoint labels provide the best performance to identify anomalies. This is consistent with the anomalies in the PoL dataset as discussed in Section 6.1, where agents abstain from visiting places they would otherwise visit on certain days. The dwell time also proves to be effective since visiting different locations affects the time spent at those locations. These findings underscore the adaptability of our approach to using different feature configurations to identify anomalies."}, {"title": "Online Anomaly Detection", "content": "One of the advantages of LM-TAD is the support of online anomalous trajectory detection. Our approach does not require the entire trajectory to compute an anomaly score. In addition, we do not need to know the destination (although such knowledge would enhance the anomaly detection of sub-trajectories). As soon as a trip begins, LM-TAD can compute the anomaly score of a partial trajectory each time a new location is sampled. Autoencoder approaches can be used for online anomalous trajectory detection as well. However, they are significantly more expensive to use since they must compute the anomaly score for the entire sub-trajectory each time a new location is sampled. LM-TAD, instead, can cache the key-value (KV cache) states [16, 24] of the attention mechanism for previously generated tokens (i.e., GPS coordinates). This significantly reduces the need for repetitive computations and lowers the latency in computing the anomaly score.\nFigure 7 shows the accuracy of detecting anomalies for partial trajectories at different completion ratios for the Porto dataset. We evaluate partial trajectories with ratios from 0.2 to 1.0 (complete) using 0.1 increment. The results suggest that LM-TAD is more than competitive in detecting sub-trajectory anomalies. Especially for random shift anomalies, 40% of the sub-trajectory is sufficient to detect most anomalies in the dataset. Detour anomalies are not that easily detected for small completion ratios without knowing the destination. A detour becomes evidently anomalous only when knowing the destination or a large portion of the trajectory. In general, LM-TAD performs on par with the best baseline, but as discussed before, comes with the advantage of significantly lower latency."}, {"title": "CONCLUSIONS", "content": "In this work, we introduced LM-TAD, an innovative trajectory anomaly detection model that uses an autoregressive causal-attention mechanism. By conceptualizing trajectories as sequences akin to language statements, our model effectively captures the sequential dependencies and contextual nuances necessary for precise anomaly detection. We demonstrated that incorporating user-specific tokens enhances the model's ability to detect context-specific anomalies, addressing the variability in individual behavior patterns.\nOur extensive experiments validated the robustness and adaptability of LM-TAD across two datasets, the Agent-based-Model generated Pattern-of-Life (PoL) dataset and the Porto taxi dataset. The results show that LM-TAD vastly outperforms existing state-of-the-art methods in identifying user-contextual anomalies. At the same time, its performance is competitive in detecting outliers in GPS-based trajectory data.\nWe introduced perplexity and surprisal rate as metrics for outlier detection and localization of anomalies within trajectories, broadening the analytical capabilities of the approach. The model's ability to handle diverse trajectory representations, from GPS coordinates to staypoints and activity types, underscores its versatility and uniqueness.\nImportantly, our approach also proves advantageous for online trajectory anomaly detection, reducing computational latency, and gaining a significant performance advantage over existing models. This provides for real-time anomaly detection without the need for an expensive re-computation of results.\nIn summary, LM-TAD represents a substantial advance in trajectory anomaly detection, offering a scalable, context-aware, and computationally efficient solution. This work paves the way for future research in user-centric analysis and real-time anomaly detection in trajectory data."}]}