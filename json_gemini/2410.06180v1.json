{"title": "CBIDR: A novel method for information retrieval combining image and data by means of TOPSIS applied to medical diagnosis", "authors": ["Humberto Giuri", "Renato A. Krohling"], "abstract": "Content-Based Image Retrieval (CBIR) have shown promising results in the field of medical diagnosis, which aims to provide support to medical professionals (doctor or pathologist). However, the ultimate decision regarding the diagnosis is made by the medical professional, drawing upon their accumulated experience. In this context, we believe that artificial intelligence can play a pivotal role in addressing the challenges in medical diagnosis not by making the final decision but by assisting in the diagnosis process with the most relevant information. The CBIR methods use similarity metrics to compare feature vectors generated from images using Convolutional Neural Networks (CNNs). In addition to the information contained in medical images, clinical data about the patient is often available and is also relevant in the final decision-making process by medical professionals. In this paper, we propose a novel method named CBIDR, which leverage both medical images and clinical data of patient, combining them through the ranking algorithm TOPSIS. The goal is to aid medical professionals in their final diagnosis by retrieving images and clinical data of patient that are most similar to query data from the database. As a case study, we illustrate our CBIDR for diagnostic of oral cancer including histopathological images and clinical data of patient. Experimental results in terms of accuracy achieved 97.44% in Top-1", "sections": [{"title": "1. Introduction", "content": "In recent years, Convolutional Neural Networks (CNN) for image classification have provided promising results (Rawat and Wang, 2017), (Li et al., 2020), (Wang and Deng, 2021), (Nawaz et al., 2022), among others. Applications focused on the medical field (Yang et al., 2021) have also been explored achieving good results such as the classification of skin lesion images (Wu et al., 2022), classification of histopathological images of oral cancer using different neural network architectures (Maia et al., 2023), among others. Bejnordi et al. (2017) used 7 different types of CNNs to classify metastases in breast cancer histopathological images, achieving results comparable to 11 experienced pathologists. However, many challenges still need to be addressed when using CNNs for medical image classification. The lack of publicly labeled databases by experts, class imbalance, and the high resolution of histopathological images are some of the challenges faced and opportunities that arise in this area of study (Tizhoosh and Pantanowitz, 2018), (Goyal et al., 2020).\nImage retrieval aims to provide support to medical professionals during diagnosis. However, the ultimate decision regarding the diagnosis is made by the doctor or pathologist, drawing upon their accumulated experience. In this context, we believe that deep learning can play an important role in addressing the challenges in medical diagnosis not by making the final decision but by assisting in the diagnosis process by suggesting the most relevant information to the doctors. To overcome the challenges while leveraging the expertise of pathologists, applications and frameworks designed to assist pathologists are starting to emerge, showing promising results. Advances have been made in both accuracy and speed when applied to areas such as Image Retrieval (Kuo et al., 2016), especially for applications that rely on the visual characteristics of the image. This field of study is called Content-Based Image Retrieval (CBIR). Barhoumi and Khelifa (2021) propose a tool to retrieve similar images related to an input image by measuring the distance to the feature map generated by the convolution and pooling processes"}, {"title": "of a CNN.", "content": "There is an extensive body of literature in the field of image retrieval, with the majority of works focusing on content-based image retrieval using neural networks as feature extractors. For example, Alzu'bi et al. (2017) achieved an accuracy of nearly 95.7% on the Oxford 5K database (Philbin et al., 2007) and 88.6% on the Oxford 105K database (Philbin et al., 2007) using two parallel CNNs as feature extractors.\nWhen applied to the medical field, CBIR systems have proven to be a valuable tool in diagnostic support. For instance, Choe et al. (2022) improved the diagnostic accuracy of interstitial lung disease and the agreement among experts with varying levels of experience using CBIR for chest computed tomography images with deep learning.\nKomura et al. (2018) developed a web application called Luigi, which retrieves histopathological images similar to various types of cancer using a pretrained convolutional neural network to extract features. Hegde et al. (2019) proposed the SMILY framework (Similar Medical Images Like Yours). SMILY is a tool for searching for similar histopathological images trained without the use of labeled histopathological images in training. SMILY was trained and tested on The Cancer Genome Atlas Program (TCGA) database, achieving high accuracy and efficiency in image retrieval. The tool achieved an average precision and recall of 96% and 98%, respectively. It was able to search a database containing tens of thousands of samples in less than 1 second, outperforming various state-of-the-art algorithms in both accuracy and time efficiency. Due to the high dimensionality of this type of image, as the number of captured and usable images in the system increases, image retrieval operations become increasingly computationally expensive. Chen et al. (2022), in addition to discussing this point, they propose a fast and scalable image retrieval system for slides, which are glass slides containing samples of biological tissue prepared for microscopic analysis, achieved through deep self-supervised learning. The algorithm, named SISH (Self-Supervised Image Search for Histology), achieved nearly constant retrieval time regardless of the image database size.\nIn order to assist pathologists in their decision-making process regarding the diagnosis, Defraire (2021) proposed a distributed approach to histopathological image retrieval to handle millions or even billions of images. This approach utilizes convolutional neural network architectures as feature extractors and the Faiss library (Johnson et al., 2017) for fast approximate nearest neighbor search, which is commonly used in image retrieval systems."}, {"title": "of a CNN.", "content": "Hashimoto et al. (2023) introduced a new method for similar image retrieval in histopathological images stained with hematoxylin and eosin for malignant lymphoma. When using a whole-slide image as the input query, which can be of high dimensionality, it is desirable to retrieve similar cases while focusing on areas of pathological importance, such as tumor cells.\nIn an effort to incorporate the information contained in clinical data into computer-aided diagnosis (CAD), de Lima et al. (2023) assessed the importance of supplementary data in the analysis of histopathological images of oral leukoplakia and carcinoma. The study concluded that clinical and demographic data positively influenced the accuracy of the models, resulting in a 30% improvement in balanced accuracy.\nIn the majority of cases, image retrieval work relies on content-based approaches using convolutional neural networks. In cases of histopathological images, these images are often collected along with clinical data of patient. This information is quite rich and has significant potential for providing insights into the disease. Therefore, this study will investigate the combination of images and clinical data in the process of assisting pathologists to achieve a more accurate diagnosis.\nIn this work, a novel approach for image retrieval is proposed combining convolutional neural networks used for image feature extraction with clinical data of patient. As a case study, a dataset NDB-UFES consisting of histopathological images of oral cancer and patient data (de Assis et al., 2023), is used. This dataset contains three classes considered for diagnosis: oral cavity squamous cell carcinoma, leukoplakia with dysplasia, and leukoplakia without dysplasia. Additionally, clinical and sociodemographic patient information is included, such as gender, lesion location, smoking habits, alcohol consumption, age, and sun exposure (de Assis et al., 2023).\nThe main contributions of this work are twofold:\n\u2022 we propose for the first time, as far as we know, a new method for Content Based Image and Data Retrieval (CBIDR) by means of the ranking algorithm TOPSIS (Technique for Order Preference by Similarity to Ideal Solution).\n\u2022 we apply the method CBIDR to a case study involving diagnostic of oral cancer using NDB-UFES dataset, which consists of histopathological images and clinical data of patient in order to illustrate the approach and show its feasibility."}, {"title": "2. Convolutional Neural Networks", "content": "In the remaining sections of the paper, we follow this structure: Section 2 briefly describes Convolutional Neural Networks, which are used in this work as feature extractors. Section 3 explains the basic concepts of Content-Based Image Retrieval (CBIR) and proposes a new methodology (CBIDR) to combine images and data through the ranking algorithm TOPSIS. Section 4 presents and discuss the obtained results and Section 5 ends up the paper with conclusions as well as directions for future work."}, {"title": "2.1. Background", "content": "Convolutional Neural Networks (CNNs), proposed by LeCun et al. (1989), were originally used for handwritten postal code recognition. However, they have found applications in various fields, especially in dealing with high-dimensional data. Krizhevsky et al. (2017) demonstrated the potential of CNNs by winning the \"ImageNet Large Scale Visual Recognition Challenge\u201d competition (Russakovsky et al., 2015) using a multi-layered CNN known as AlexNet. This result showcased the viability and versatility of CNNs.\nCNNs architecture consist of three types of layers: convolutional, pooling, and fully connected layers (Goodfellow et al., 2016)."}, {"title": "of a CNN architecture.", "content": "Typically, towards the end of the convolutional layer, an activation function is applied to the resulting feature map. This activation function introduces non-linearity to the model, and enhances important features, allowing the CNN to learn and model complex relationships within the input data. One commonly used activation function in CNNs is the Rectified Linear Unit (ReLU) (Goodfellow et al., 2016). It is often chosen for its simplicity and effectiveness, helping to identify activation regions in the feature maps of the convolutional layer.\nThe pooling operation aims to reduce the dimensionality of the feature map resulting from the convolutional layer (Goodfellow et al., 2016) by selecting the most important features. During the pooling operation, a sliding window, for example, 2x2, is applied across the output of the previous layer. This window's purpose is to summarize the information within that area into a single value, thus achieving dimensionality reduction and highlighting relevant features. This technique helps simplify and compress information, enabling more efficient processing and a more compact representation of the data. The two most well-known pooling techniques are Max Pooling (MaxPool) and Average Pooling (AvgPool).\nThe final part of a CNN, consists of a feedforward neural network, also known as a Multilayer Perceptron (MLP) (Goodfellow et al., 2016). Typically, it acts as the classification block in the CNN, taking the feature map as input and producing an output for a given input. A feedforward neural network consists of three main types of layers: the input layer, hidden layers, and the output layer (Goodfellow et al., 2016). The input layer, as the name suggests, receives the input data. In a CNN, this input data represents the feature map. The hidden layers, situated between the input and output layers, are responsible for learning and storing abstract representations of the input data. In these hidden layers, each neuron receives outputs from neurons in the previous layer, performs calculations based on its weights and activation function, and passes the result to the next layer. Ultimately, in the output layer, the model's output is computed.\nIn convolutional neural networks, before feeding the fully connected (FC) layer with the resulting feature map, it is necessary to perform the flatten operation. Typically, this feature map is a three-dimensional vector, with depth representing the number of filters used. The FC layer needs to receive a one-dimensional vector, so the feature map is flattened into a one-dimensional vector, allowing the feedforward neural network to classify the input data.\nMost convolutional neural networks, such as ResNet (He et al., 2015),"}, {"title": "2.2. Training Convolutional Neural Networks", "content": "MobileNetV2 (Sandler et al., 2018), and DenseNet-121 (Huang et al., 2017), sequentially employ blocks of convolution and pooling layers multiple times. This architecture allows the model to capture information at different scales. Since convolutional neural networks can be used for dimensionality reduction of input data, providing a one-dimensional vector, removing the fully connected layers (FC) from the network yields the feature vector. This task is known as feature extraction (Atasever et al., 2023) and involves representing the original input data in a compact form without losing information quality, capable of capturing essential features while discarding deemed irrelevant information. Feature extraction enables high-dimensional data, such as images, to have their key characteristics mapped to reduced dimensionality, improving efficiency.\nFor training a neural network, the backpropagation algorithm, as introduced by Rumelhart et al. (1986), is commonly used. The goal of this algorithm is to adjust the network's weights to minimize the associated cost function. Initially, the network's weights and bias are randomly initialized. The network is then fed with the training data, and the output of each neuron is calculated until reaching the final layer. After calculating the final network outputs, the error can be computed by comparing the obtained output with the expected output. Next, the error is propagated backward through the network, determining the contribution of error from each neuron. This is accomplished using gradient descent, which involves computing the partial derivative of the error with respect to the synaptic weights of each connection. The weights are updated for each connection by an amount proportional to the gradient descent, multiplied by a learning rate. This update is aimed at reducing the error. These steps are repeated for all training data, constituting one epoch. The training of a neural network typically involves multiple epochs, often predetermined, or until convergence is achieved. In summary, backpropagation is a key algorithm for training neural networks, and it iteratively adjusts the weights to minimize the error between predicted and actual outputs during training.\nThe goal of training a neural network is to find the weights and bias configurations for a given problem, and for a CNN, this includes finding the optimal filter configurations for the convolutional layers. In the training process of a convolutional neural network, labeled data must be provided to enable the network to make predictions. By using a loss function (Goodfellow"}, {"title": "3. Image Retrieval", "content": "et al., 2016), one can measure the comparison between predicted classes and true classes. The objective is to minimize this resulting error to obtain the correct predictions. The Cross-Entropy loss function is commonly used and it is described by:\nLoss(x, y) = -\\sum_{i=1}^{N} x_i \\log y_i\nwhere xi represents the true probability of image i belonging to the correct class, yi is the predicted probability by the neural network that image i belongs to the correct class and N is the total number of training examples.\nIn feature extraction tasks, loss functions based on ranking are commonly used. They aim to predict relative distances between inputs. One such loss function is the Margin Loss (Wu et al., 2017), defined as:\nL_{margin} = \\sum_{(i,j)\\in P} \\gamma + I_{y_i=y_j} (d(\\phi_i, \\phi_j) \u2013 \\beta) \u2013 I_{y_i\\neq y_j} (d(\\phi_i, \\phi_j) \u2013 \\beta)\nwhere \u03b2 is an adjustable parameter, regulated during training. The goal here is to learn a function \u03d5 such that $d_{\\phi}(x_a,x_n) - d_{\\phi}(x_a, x_p)  < \\gamma$, assuming $x_a$ is an anchor image from a specific class, $x_n$ is an image from a different class than $x_a$, and $x_p$ is an image from the same class as $x_a$. Thus, this loss function encourages images of the same class to be closer while pushing images of different classes farther apart.\nTo minimize the training error, optimization algorithms like Stochastic Gradient Descent (SGD) (Ruder, 2016) and Adam (Kingma and Ba, 2017) are commonly used. This class of algorithms aims to minimize the loss function to its optimal value by updating the weights and biases of the network. In real-world applications, convolutional neural networks often leverage transfer learning. So, a pre-trained CNN, trained on a large dataset like ImageNet (Deng et al., 2009), is used. This allows to save the previously learned network parameters and fine-tune them on your specific target dataset."}, {"title": "3.1. Preliminaries", "content": "The main methods for image retrieval (Riad et al., 2012) are text-based image retrieval (TBIR), content-based image retrieval (CBIR), and hybrid"}, {"title": "3.2. Standard CBIR Method", "content": "approaches. In TBIR, the text associated with the image is considered to determine its content, making it possible to retrieve similar elements based on the text present. In the case of CBIR, features such as color, shape, and texture are used to index and compare the similarity of input images with those in the database. The third method, a hybrid retrieval, aims to combine the two previously mentioned approaches to achieve better results.\nTo determine the similarity between the input image and those in the database, a similarity metric is computed based on the extracted features (Riad et al., 2012). Therefore, research in this area focuses on how to extract features that best describe the images, aiming for a detailed comparison. With the advancement of CNNs for feature extraction tasks, they have been introduced into CBIR systems for this purpose. For instance, Rian et al. (2019) investigated the use of convolutional neural networks as feature extractors for image retrieval in the iNaturalist database (Van Horn et al., 2017), achieving accuracy of 89%.\nThe main advantage of using CNNs for image retrieval lies in their ability to extract features with a richness of information that describes the image. Gkelios et al. (2021) conclude that when CNNs are employed for feature extraction (or descriptors), they produce descriptors with high discriminative power in a computationally efficient manner. These feature vectors enable the comparison of two images using similarity metrics like the Euclidean distance, which quantifies how similar two images are.\nDatta et al. (2008) address the challenges and trends in the field of image retrieval, presenting and reviewing the main similarity metrics used in this area. Consider two feature vectors, a and b. The Euclidean distance between them is defined as:\nD(a, b) = \\sqrt{\\sum_{i=1}^{p} (a_i - b_i)^2}\nwhere the sum of the square root of the difference between a and b in their respective dimensions. There are several metrics that can be used to calculate similarity between feature vectors but this is not the focus of this work and may be explored elsewhere.\nThe image retrieval task is divided into 4 steps. The first one is to extract the feature vectors from all images in the database using a CNN as a feature"}, {"title": "4. Experimental Results", "content": "extraction mechanism to create the descriptor database. The second step is to receive a query image that will serve as the input image for the search. After extracting the feature vector from the query image using the same CNN as in the first step, distances are calculated between the input image and the descriptors in the database. This allows ranking the compared descriptors based on the metric and selecting the top images with the highest similarity value."}, {"title": "3.3. Method Combining Image and Data", "content": "In this section, the results obtained for retrieval of histopathological images of oral cancer, which combines convolutional neural networks (CNNs) to extract image features with clinical data of patient, is presented. Three CNNs were employed to evaluate the quality of feature extraction. Firstly, the dataset used in the experiments is introduced. Next, the configurations used in the experiments are presented and discussed. Finally, the results are discussed."}, {"title": "3.3.1. TOPSIS", "content": "As mentioned earlier, an individual's lifestyle is directly related to the occurrence of various diseases. Therefore, clinical and sociodemographic data, such as smoking habits, alcohol consumption, gender, and others, are usually collected along with the images and provide relevant information for diagnosis. Since it is possible to numerically define how similar two images are by extracting their respective feature vectors and measuring, using a similarity metric, the distance between them, it is also possible to quantify how close two sets of clinical information are. By organizing the information into a binary vector, where 0 corresponds to the absence of a particular feature such as whether or not alcohol is consumed - and 1 corresponds to its presence, it is possible to measure the distance between two binary vectors using distance metrics.\nOne of the most commonly used metrics to calculate similarity between two binary vectors is the Hamming distance. As highlighted by Bookstein (2002), the Hamming distance is commonly used to quantify the extent to which two bit strings of the same dimension differ. An early application was in the theory of error-correcting codes (Hamming, 1980), where the Hamming distance measured the error introduced by noise over a channel when a message, typically a sequence of bits, is transmitted between the source and destination. To calculate the Hamming distance, a logical XOR operation is performed between the positions of the two input vectors. The number of 1 bits in the resulting vector determines the value of the distance.\nBy using CNNs in the image retrieval process, distances between the extracted features of each image are ranked. By incorporating clinical data into the analysis, two distance metrics are considered: one for images and another for clinical data. Therefore, when performing an operation with image and data retrieval, it is possible to compare an input query image and data with those image features and data stored in the database, resulting in two distances for each element in the database. Thus, the system will generate an Mx2 matrix, where each column represents one of the obtained distances. To identify the option most similar to the query image and data, it is necessary to rank an Mx2 matrix rather than a one-dimensional vector of size M. For this purpose, the TOPSIS algorithm for ranking is employed, which has been applied in different knowledge domains (Krohling and Pacheco, 2015)."}, {"title": "3.3.2. The Proposed Method CBIDR", "content": "TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) proposed by Hwang and Yoon (1981) is a decision-making technique, which works by evaluating the performance of alternatives based on their similarity to the ideal solution. Multi-criteria decision-making problems are characterized by a decision matrix D, where each row represents an alternative and each column represents a criterion. This matrix is used to organize the relevant information for the decision problem, allowing for the comparison of alternatives with respect to the established criteria. Based on this matrix, the steps of the TOPSIS method are applied to determine the preferred alternative based on a combination of criteria and their weights. TOPSIS is a valuable technique for multi-criteria decision-making and is used in various fields to support decision processes when there are multiple alternatives and criteria to consider. Multi-criteria decision-making problems are characterized by a decision matrix D:\nD = \\begin{bmatrix}\nX_{11} & ... & X_{1n} \\\\\n: & : & : \\\\\nX_{m1} & ... & X_{mn} \n\\end{bmatrix}\nwhere the rows of this matrix represent the alternatives (A1, A2, ..., Am). The columns are the criteria analyzed in the problem (C1, C2, ..., Cn). The elements of the matrix, xij determine the performance of alternative A\u017c evaluated on criterion Cj. Each criterion has a weight that affects the decision-making process, defined by W = (W1, W2, ..., Wn), where wj is the weight for criterion Cj. It is important to note that the sum of all weights must equal 1. There are two types of criteria: cost and benefit. The difference between them is that for the cost criterion, the focus is on minimizing, meaning that a lower value is better, while for the benefit criterion, it is exactly the opposite. In this work, both criteria are cost criteria, as we use the distances between images and between clinical data as the criteria, aiming for the smallest distance values possible.\nBefore using TOPSIS, it is necessary to normalize the decision matrix D, transforming it into the matrix R = [rij]m\u00d7n, allowing for comparisons between all criteria. To normalize the decision matrix, the following formula is applied:"}, {"title": "4.1. Dataset", "content": "The novel proposed Content-Based Image and Data Retrieval, for short, CBIDR is described in the following. The image retrieval process using convolutional neural networks combined with patient clinical data through the TOPSIS algorithm begins by creating a descriptor database using a CNN as the feature extraction mechanism. Next, an input image is passed to the system, and its features are extracted using the same CNN as in the previous step. Subsequently, the distance between the input feature vector and the descriptor database is measured, generating the first vector X1, with a size of m, where m is the number of descriptors present in the database. Continuing, the clinical information linked to the input image is compared, utilizing the Hamming distance, with the information in the database, generating the second distance vector X2, also of size m. Combining the two distance vectors results in a decision matrix D with dimensions m x 2. This matrix serves as input for the TOPSIS technique, which, along with its weights and criteria, calculates and ranks the best alternatives. The complete process of image retrieval using convolutional neural networks combined with clinical data of patient and the TOPSIS algorithm is shown in Figura 3."}, {"title": "4.2. Experiments Setting", "content": "In this work, the dataset NDB-UFES (de Assis et al., 2023) resulted from an extension project carried out at the Federal University of Esp\u00edrito Santo, collected histopathological images of patients diagnosed with oral cavity squamous cell carcinoma and oral leukoplakia between January 2010 and December 2021. It contains originally, 237 images (2048 x 1536 pixels) and clinical and socio-demographic data were also collected from the respective patients, including gender, lesion location, cigarette usage, alcohol consumption, age, and sun exposure (de Assis et al., 2023).\nNext, the PatchExtractor from the Python library Scikit-Learn (Pedregosa et al., 2011) was used to generate patches for each image, increasing the dataset to 3753 images (512 x 512 pixels). These 3753 images were labeled by pathologists, resulting in a database called P-NDB-UFES, which contains 1930 images (51.29%) for the class leukoplakia with dysplasia, 707 images (18.79%) for the class leukoplakia without dysplasia, and 1126 images (29.92%) for the class oral squamous cell carcinoma (OSCC)."}, {"title": "4.3.3. Discussions", "content": "The experiments conducted were divided into two parts: Experiment I and II. Experiment I represents the configuration using only image features (CBIR) as baseline, while Experiment II utilizes the TOPSIS algorithm to combine images and clinical data of patient.\nIn both experiments, three convolutional neural network architectures were used to perform the task of extracting image features. These architectures include ResNet50 (He et al., 2015), DenseNet-121 (Huang et al.,"}, {"title": "5. Conclusion", "content": "rij = \\frac{X_{ij}}{\\sqrt{\\sum_{i=1}^{m} X_{ij}}}\nOnce normalized, the resulting matrix R has its values weighted by the weight vector W, generating a weighted matrix P = [pij]mxn, calculated as follows:\nPij = W_j X rij\nUpon obtaining matrix P, the TOPSIS algorithm begins. First, the positive ideal solution (A+) and the negative ideal solution (A\u00af), respectively, for the benefit and cost criteria are identified as follows:\nA+ = (p+, p,...,P) \nA\u00af = (p\u012b, P2, ...,Pm)\nwith:\nP_j^+ = \\begin{cases}\n\\max_i(P_{ij}) \\\\\n\\min_i(P_{ij})\n\\end{cases}\nP_j^- = \\begin{cases}\n\\min_i(P_{ij}) \\\\\n\\max_i(P_{ij})\n\\end{cases}\nwhere the maximum operator is used for benefit criteria, and the minimum operator is used for cost criteria. This step aims to select the best performance for each criterion, whether it is the cost criterion (where the best performance is the lowest value) or the benefit criterion (where the best performance is the highest value). Next, two vectors, A+ and A\u00af, are formed to represent the ideal performance. Then, for each alternative A\u017c, the Euclidean distance from each element to the positive solution vector A+ and the negative solution vector A\u00af is calculated as follows:\nd_i^+ = \\sqrt{\\sum_{j=1}^{n} (d_{ij}^+)^2}\nd_i^- = \\sqrt{\\sum_{j=1}^{n} (d_{ij}^-)^2}"}, {"title": "4.3. Discussions", "content": "2017) and MobileNetV2 (Sandler et al., 2018). Both ResNet50 and DenseNet-121 are widely used for feature extraction tasks and have been tested and validated in various studies. On the other hand, MobileNetV2 offers a lightweight configuration, making it computationally less demanding. This is an important feature when running the application on computers with limited processing capabilities.\nIn both experiments, two training phases were conducted, each consisting of 50 epochs, a batch size of 32, and the Adam optimizer (Kingma and Ba, 2017) with an initial learning rate of 0.0001, which decreased by 30% (gamma rate) per step, along with a weight decay of 0.0004. The MarginLoss (Wu et al., 2017) was used as the loss function. All three architectures used in this work were already imported and pre-trained using the ImageNet dataset (Deng et al., 2009). First, the conventional training of the convolutional neural network was performed using the P-NDB-UFES dataset, saving its weights and biases at the end of training. In the second stage, the NDB-UFES dataset containing the original images with dimensions of 2048x1536 was utilized. Both the NDB-UFES and P-NDB-UFES datasets were divided as follows: 5/6 for training and 1/6 for testing, with images resized to 224x224 pixels.\nFor evaluation, the Top-k accuracy was used, which is defined as follows:\nTop-k = \\sum_{xi \\in X_{test}} \\frac{I(\\exists X \\in F s.t. y_i = y_q)}{|X_{test}|}, F = \\underset{|F|=k}{argmin} d(\\phi(x_i), \\phi(x_q))\nThe Top-k accuracy counts as correct when the class of the input query image is among the top k most similar images returned by the proposed method. When k is one, this task becomes a classification one, where the class of the returned image needs to be the same as the input image. In this paper, Top-1 and Top-5 accuracies are computed.\nThe experiments were conducted using an Intel i9-7900X CPU (20 cores) with a clock speed of 4.3GHz, equipped with 128 GB of RAM, and an NVIDIA TITAN Xp graphics card with 12GB of memory. The entire code was written in Python, using various open-source libraries such as PyTorch, NumPy, Matplotlib, and Faiss."}, {"title": "4.3.1. Experiment I - CBIR using only Images", "content": "where, $dt = pij^+$ - $P_{ij}$ and $di = p_{ij} - pij^- $. The next step is to determine the relative closeness \u03bei based on the obtained distances, calculated as follows:\n\u03bei = \\frac{di}{d++ di}\nthis allows the selection of the alternative closest to the positive ideal solution and farthest from the negative ideal solution.\nFinally, the alternatives in the \u00a7 vector are ranked, with the best solutions having higher values of \u00a7i, indicating that they are closer to the ideal solution.\nThe novel proposed Content-Based Image and Data Retrieval, for short, CBIDR is described in the following. The image retrieval process using convolutional neural networks combined with patient clinical data through the TOPSIS algorithm begins by creating a descriptor database using a CNN as the feature extraction mechanism. Next, an input image is passed to the system, and its features are extracted using the same CNN as in the previous step. Subsequently, the distance between the input feature vector and the descriptor database is measured, generating the first vector X1, with a size of m, where m is the number of descriptors present in the database. Continuing, the clinical information linked to the input image is compared, utilizing the Hamming distance, with the information in the database, generating the second distance vector X2, also of size m. Combining the two distance vectors results in a decision matrix D with dimensions m x 2. This matrix serves as input for the TOPSIS technique, which, along with its weights and criteria, calculates and ranks the best alternatives. The complete process of image retrieval using convolutional neural networks combined with clinical data of patient and the TOPSIS algorithm is shown in Figura 3."}, {"title": "4.3.2. Experiment II - Results obtained with the proposed method CBIDR", "content": "In Experiment I, only the features extracted from the histopathological images were used."}, {"title": "4.3. Discussions", "content": "This paper proposed a novel method to support medical professionals in their diagnostic tasks. The methodology combines feature extraction from images using convolutional neural networks with the clinical data obtained from patients during their consultations. We illustrate the method by using the NDB-UFES dataset collected and curated by pathologists to diagnostic of oral cancer. Two experiments were conducted. In Experiment I using standard CBIR considered only the features extracted from the images as baseline method, the ResNet50 achieved the Top-1 accuracy, with value of 74.36% and the MobileNetV2 a value of 89.74% for the Top-5 accuracy. In Experiment II using the proposed method CBIDR, a significant increase in accuracy was observed, reaching 97.44% in Top-1 and 100% in Top-5 using MobileNetV2. The results obtained from the Experiment II, show that the proposed method CBIDR improved accuracy in retrieving histopathological images with clinical data of patients. This work opens up a new research avenue for multimodal information retrieval using several information sources in an easy and effective way. For future work, we aim to make the approach usable to medical professionals (doctors/pathologists) by creating a friendly graphical interface to allow for more intuitive use. Additionally, we also intend to apply the proposed approach to other medical datasets."}]}