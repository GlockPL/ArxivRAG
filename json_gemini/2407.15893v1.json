{"title": "Cascaded two-stage feature clustering and selection via separability and consistency in fuzzy decision systems", "authors": ["Yuepeng Chen", "Weiping Ding", "Hengrong Ju", "Jiashuang Huang", "Tao Yin"], "abstract": "Feature selection is a vital technique in machine learning, as it can reduce computational complexity, improve model performance, and mitigate the risk of overfitting. However, the increasing complexity and dimensionality of datasets pose significant challenges in the selection of features. Focusing on these challenges, this paper proposes a cascaded two-stage feature clustering and selection algorithm for fuzzy decision systems. In the first stage, we reduce the search space by clustering relevant features and addressing inter-feature redundancy. In the second stage, a clustering-based sequentially forward selection method that explores the global and local structure of data is presented. We propose a novel metric for assessing the significance of features, which considers both global separability and local consistency. Global separability measures the degree of intra-class cohesion and inter-class separation based on fuzzy membership, providing a comprehensive understanding of data separability. Meanwhile, local consistency leverages the fuzzy neighborhood rough set model to capture uncertainty and fuzziness in the data. The effectiveness of our proposed algorithm is evaluated through experiments conducted on 18 public datasets and a real-world schizophrenia dataset. The experiment results demonstrate our algorithm's superiority over benchmarking algorithms in both classification accuracy and the number of selected features.", "sections": [{"title": "I. INTRODUCTION", "content": "With the advent of the digital era, there has been an unprecedented surge in data from various sources such as sensors, social media, financial systems, and healthcare resources. However, traditional methods struggle to handle big data due to its high dimensionality, noise, and redundant information, significantly impacting the accuracy and efficiency of both data analysis and decision-making processes. To address these challenges, feature selection emerges as a crucial technique across pattern recognition, machine learning, and data mining. It involves selecting the most relevant features from the original set to prevent overfitting, enhance interpretability, and optimize learning task performance [1]\u2013[4].\nIn real-world scenarios, datasets often exhibit fuzziness and uncertainty due to high dimensions and substantial noise. While rough set theory [5] as a valuable mathematical tool for feature selection has proven effective in handling uncertain information in classification problems, it faces limitations with continuous data. Numerous generalized models have emerged to address this issue, with fuzzy rough sets [6] playing a significant role in overcoming the limitations.\nDubois and Prade [6] integrated the theories of rough sets and fuzzy sets, defining the fuzzy rough approximation operators and proposing the fuzzy rough set model, effectively addressing uncertainty and fuzziness in data analysis. Scholars have proposed various extended models based on the fuzzy rough sets. Dai et al. [7] introduced the concept of reduced maximal discernibility pairs within the framework of the fuzzy rough set model. They subsequently developed algorithms for selecting reduced maximal discernibility pairs and weighted reduced maximal discernibility pairs. Hu et al. [8] integrated kernel functions into fuzzy rough set models, introducing kernelized fuzzy rough sets to compute fuzzy memberships among samples for classification approximation and to assess attribute approximation quality. Zhao et al. [9] introduced a feature reduction approach employing fuzzy rough sets and presented a novel model capable of computing lower and upper approximations specifically for hierarchical class structures. Zhang et al. [10] proposed an incremental feature selection approach using information entropy based on the fuzzy rough set, devising an active approach by selecting representative instances and formulating an incremental mechanism. Huang et al. [11] proposed an incremental approach for hierarchical classification, encompassing a sibling strategy to minimize negative sample impact and incorporating incremental updates upon new sample arrival. Wang et al. [12] introduced the fuzzy neighborhood rough set model (FNRS), combining fuzzy rough set and neighborhood rough set theories. They established the dependency between fuzzy decisions and feature subsets, utilizing it to assess candidate feature importance, and subsequently developed a greedy forward selection algorithm. SUN et al. [13] applied fuzzy neighborhood rough sets in multi-label classification based on maximum relevance minimum redundancy (MRMR). SUN et al. [14] combined fuzzy neighborhood rough set and multi-granulation concepts, utilizing fuzzy neighborhood pessimistic multi-granulation entropy as an evaluation criterion for feature importance.\nThese methods based on the fuzzy rough sets above aim to find an optimal feature subset while maintaining the dependency or the fuzzy positive region. In practice, evaluating the significance of each feature often involves employing a widely used greedy forward-searching algorithm. This algorithm iteratively constructs an optimal feature subset by adding features"}, {"title": "II. PRELIMINARIES", "content": "This section provides a brief overview of the fundamental concepts of the fuzzy C-Means (FCM) algorithm and fuzzy neighborhood rough set that are necessary to understand our work."}, {"title": "A. Fuzzy C-Means algorithm", "content": "The FCM algorithm [24] is a clustering method that aims to partition a dataset into distinct groups according to the similarities between data points. It extends the classic k-means algorithm by incorporating fuzzy logic, allowing for a more flexible assignment of data points to clusters.\nLet $X={X_1,X_2,\\cdots, X_N }$ be a dataset with M-dimensional samples. The FCM algorithm operates by iteratively computing cluster centroids and a membership matrix to partition the data space effectively [25]. This process aims to minimize the following objective function"}, {"title": "B. Fuzzy neighborhood rough set", "content": "Let $S = (U, A\\cup D, f)$ represent a fuzzy decision system, where $U = {x_1,x_2,..., X_N}$ denotes the universe or sample space, $A = {a_1,a_2,..., a_M}$ is the condition attributes or features used to characterize the samples, and D represents the decision classes. Assuming the partitioning of samples into c mutually exclusive decision classes by D, denoted as $U/D={D_1, D_2,\u2026\u2026, D_c}$, $f_a(x)$ represents the feature value of x on condition attribute a. For any x, y \u2208 U, B \u2286 A, the feature subset B can induce a fuzzy binary relation $R_B$ on U. $R_B$ is the fuzzy similarity relation if it satisfies the reflexivity $R_B(x,x) = 1$ and the symmetry $R_B(x,y) = R_B(y,x)$.\nIn a fuzzy neighborhood decision system $S = (U, A\\cup D, f, \\lambda)$, where \\lambda denotes the fuzzy neighborhood radius [26], the fuzzy neighborhood granule of any x \u2208 U based on B is defined as"}, {"title": "III. THE PROPOSED METHOD", "content": "This section presents a cascaded two-stage feature clustering and selection method, the framework of which is depicted in Figure 1. The algorithm framework consists of two stages: feature clustering and feature selection. In the first stage, similarity-based clustering categorizes relevant features into multiple groups by FCM. Following this, the second stage employs a clustering-based sequentially forward selection algorithm, navigating through these feature clusters to select the subset of features. Based on the framework, we design a clustering-based sequentially forward selection algorithm to select the feature subset from feature groups."}, {"title": "A. Feature clustering based on FCM", "content": "Cluster analysis involves classifying data objects based on their similarities, to achieve low inter-cluster similarity and high intra-cluster similarity for effective clustering. By analyzing the correlation between features to conduct cluster analysis, redundant features can be grouped within the same cluster. In the clustering stage of our proposed method, features are regarded as data objects, and the Euclidean distance between feature elements is computed as a measure of similarity among features. Our approach primarily involves dividing the original features into distinct clusters, where the features within each cluster exhibit higher correlations, while the correlations between features from different clusters are comparatively lower.\nDefinition 1: Let $X = {x_1,x_2,..., X_N}$ be a training set, where $x_n \\in X$ is a sample vector comprising M measurements from the feature set $A = {a_1,a_2,\u2026\u2026, a_M}$. Feature clustering is the partitioning of a set A into a collection G = {$G_1, G_2,\u2026,G_K$} of mutually disjoint subsets of features, where K is the number of feature clusters, with A = $G_1 \\cup G_2,\u2026\u2026 \\cup G_K$, and $G_i \\cap G_2$ = 0.\nIn the feature clustering stage, the FCM algorithm is employed. In the random initialization stage of FCM, there are multiple methods to choose initial cluster centers. If the initial cluster center selection is improper, it can lead to slow convergence or unbalanced cluster sizes. Therefore, we employ the KMeans++ method to initialize the clustering centers in FCM [28]."}, {"title": "B. Feature selection based on global separability and local consistency", "content": "This section presents a fusion metric to measure the significance of features in fuzzy decision systems by fusing global separability and local consistency, illustrated in Figure 2. The process involves several key steps. First, samples and labels are extracted from the dataset, and a fuzzy decision system is constructed. Subsequently, in the second step, the fuzzy membership matrix and fuzzy similarity matrix are computed based on the fuzzy decision system. In the third step, global separability assesses intra-class cohesion and inter-class separation by leveraging fuzzy membership, while local consistency captures uncertainty through the fuzzy neighborhood rough set model.\n1) Global separability based on fuzzy membership: This section introduces two key measures, namely the degree of intra-class cohesion (DIC) and the degree of inter-class separation (DIS). These measures offer insights into the data structure, focusing on intra-class cohesion and inter-class separability, providing an intuitive understanding of the data structure.\nThe degree of intra-class cohesion (DIC) is proposed to evaluate the proximity among intra-class objects based on their fuzzy memberships within decision classes.\nDefinition 2: Given a fuzzy decision system S = (U, A\\cup D, f), where $U = {X_1,X_2, ..., X_N}$, and $U/D = {D_1, D_2,..., D_c}$, $D_i(i = 1,2,..., c)$ is the i-th decision class. The membership degree of sample Xk concerning the decision class $D_i$ under feature subset $B(B \\subseteq A)$ is define as"}, {"title": "IV. EXPERIMENTAL ANALYSIS", "content": "To validate the effectiveness of our proposed method, we conduct experiments comparing it with six other feature selection algorithms: CMIM [31], mRMR [32], ReliefF [33], FNRS [12], GRMFS [23], and FHFS [25].\nAssessment of the feature selection results involves three established classifiers: K-Nearest Neighborhood (KNN), support vector machine (SVM), and classification and regression tree (CART), representing diverse supervised classification algorithms. Classification accuracy for the selected feature subsets is determined using default parameter settings for these classifiers. Employing a ten-fold cross-validation method, our experiments entail the random partitioning of the original dataset into ten subsets, nine for training and one for testing, repeated ten times to compute average and standard deviation values, constituting the final results. Our method is evaluated and compared using the same training and testing datasets as other feature selection algorithms.\nAll feature selection algorithms are implemented in Python, and executed on hardware equipped with Intel Core i7-10870H CPU @2.20 GHz and 16 GB RAM."}, {"title": "A. Experiment on the UCI and the Kent Ridge Biomedical Datasets", "content": "For this study on feature selection, 18 real-world datasets from the UCI repository of machine learning databases [34] and the Kent Ridge biomedical data set [35], encompass diverse fields such as medical diagnosis, image segmentation, and texture classification, shown in Table I. Missing values in datasets are imputed utilizing the maximum probability value approach. To mitigate variability resulting from differences in magnitudes, the maximum-minimum method normalizes the numerical data range to [0, 1]."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a cascaded two-stage feature clustering and selection framework that considers both global separability and local consistency of fuzzy decision systems. The first phase is the feature clustering stage, which groups relevant features into clusters based on their similarities, reducing the search space and the computational complexity. The second phase is a feature selection stage, which selects the most important feature from each cluster based on the fusion of global and local scores, considering the relationships among irrelevant features. The global separability measures the intra-class cohesion and inter-class separation of the features using fuzzy membership concerning the decision class, while the local consistency measures the local consistency of samples with the fuzzy neighborhood rough set model. We conducted experiments on 18 public datasets and a real-world schizophrenia dataset, evaluating our method against six state-of-the-art feature selection algorithms. The results show that our method achieved superior classification performance while utilizing fewer selected features than the compared algorithms. Particularly, applying our method to the schizophrenia dataset revealed crucial brain regions and specific characteristics pivotal for diagnosing schizophrenia. This capability showcases the potential for early detection and intervention in schizophrenia cases.\nDespite these promising results, there are still several limitations in our work. Firstly, the feature clustering method within our framework plays a crucial role but requires manual setting of the number of clusters. Additionally, the proposed method only considers Euclidean distance between features as a measure of similarity, potentially overlooking complex nonlinear relationships within the data. A potential solution could involve employing kernel-based or deep learning-based methods to learn relationships between features better and perform clustering. For future work, we plan to extend our method to handle multi-label and multi-view data and explore the use of other clustering and evaluation methods."}]}