{"title": "Cascaded two-stage feature clustering and selection via separability and consistency in fuzzy decision systems", "authors": ["Yuepeng Chen", "Weiping Ding", "Hengrong Ju", "Jiashuang Huang", "Tao Yin"], "abstract": "Feature selection is a vital technique in machine learning, as it can reduce computational complexity, improve model performance, and mitigate the risk of overfitting. However, the increasing complexity and dimensionality of datasets pose significant challenges in the selection of features. Focusing on these challenges, this paper proposes a cascaded two-stage feature clustering and selection algorithm for fuzzy decision systems. In the first stage, we reduce the search space by clustering relevant features and addressing inter-feature redundancy. In the second stage, a clustering-based sequentially forward selection method that explores the global and local structure of data is presented. We propose a novel metric for assessing the significance of features, which considers both global separability and local consistency. Global separability measures the degree of intra-class cohesion and inter-class separation based on fuzzy membership, providing a comprehensive understanding of data separability. Meanwhile, local consistency leverages the fuzzy neighborhood rough set model to capture uncertainty and fuzziness in the data. The effectiveness of our proposed algorithm is evaluated through experiments conducted on 18 public datasets and a real-world schizophrenia dataset. The experiment results demonstrate our algorithm's superiority over benchmarking algorithms in both classification accuracy and the number of selected features.", "sections": [{"title": "I. INTRODUCTION", "content": "With the advent of the digital era, there has been an unprecedented surge in data from various sources such as sensors, social media, financial systems, and healthcare resources. However, traditional methods struggle to handle big data due to its high dimensionality, noise, and redundant infor-mation, significantly impacting the accuracy and efficiency of both data analysis and decision-making processes. To address these challenges, feature selection emerges as a crucial tech-nique across pattern recognition, machine learning, and data mining. It involves selecting the most relevant features from the original set to prevent overfitting, enhance interpretability, and optimize learning task performance [1]\u2013[4].\nIn real-world scenarios, datasets often exhibit fuzziness and uncertainty due to high dimensions and substantial noise. While rough set theory [5] as a valuable mathematical tool for feature selection has proven effective in handling uncertain information in classification problems, it faces limitations with continuous data. Numerous generalized models have emerged to address this issue, with fuzzy rough sets [6] playing a significant role in overcoming the limitations.\nDubois and Prade [6] integrated the theories of rough sets and fuzzy sets, defining the fuzzy rough approximation operators and proposing the fuzzy rough set model, effectively addressing uncertainty and fuzziness in data analysis. Scholars have proposed various extended models based on the fuzzy rough sets. Dai et al. [7] introduced the concept of reduced maximal discernibility pairs within the framework of the fuzzy rough set model. They subsequently developed algorithms for selecting reduced maximal discernibility pairs and weighted reduced maximal discernibility pairs. Hu et al. [8] integrated kernel functions into fuzzy rough set models, introducing kernelized fuzzy rough sets to compute fuzzy memberships among samples for classification approximation and to assess attribute approximation quality. Zhao et al. [9] introduced a feature reduction approach employing fuzzy rough sets and presented a novel model capable of computing lower and upper approximations specifically for hierarchical class structures. Zhang et al. [10] proposed an incremental feature selection approach using information entropy based on the fuzzy rough set, devising an active approach by selecting representative instances and formulating an incremental mechanism. Huang et al. [11] proposed an incremental approach for hierarchical classification, encompassing a sibling strategy to minimize negative sample impact and incorporating incremental updates upon new sample arrival. Wang et al. [12] introduced the fuzzy neighborhood rough set model (FNRS), combining fuzzy rough set and neighborhood rough set theories. They estab-lished the dependency between fuzzy decisions and feature subsets, utilizing it to assess candidate feature importance, and subsequently developed a greedy forward selection algorithm. SUN et al. [13] applied fuzzy neighborhood rough sets in multi-label classification based on maximum relevance min-imum redundancy (MRMR). SUN et al. [14] combined fuzzy neighborhood rough set and multi-granulation concepts, utiliz-ing fuzzy neighborhood pessimistic multi-granulation entropy as an evaluation criterion for feature importance.\nThese methods based on the fuzzy rough sets above aim to find an optimal feature subset while maintaining the depen-dency or the fuzzy positive region. In practice, evaluating the significance of each feature often involves employing a widely used greedy forward-searching algorithm. This algorithm iter-atively constructs an optimal feature subset by adding features"}, {"title": "II. PRELIMINARIES", "content": "This section provides a brief overview of the fundamental concepts of the fuzzy C-Means (FCM) algorithm and fuzzy neighborhood rough set that are necessary to understand our work."}, {"title": "A. Fuzzy C-Means algorithm", "content": "The FCM algorithm [24] is a clustering method that aims to partition a dataset into distinct groups according to the similarities between data points. It extends the classic k-means algorithm by incorporating fuzzy logic, allowing for a more flexible assignment of data points to clusters.\nLet $X={X_1,X_2,\\cdots, X_N}$ be a dataset with M-dimensional samples. The FCM algorithm operates by iteratively comput-ing cluster centroids and a membership matrix to partition the data space effectively [25]. This process aims to minimize the following objective function\n$J = \\sum_{i=1}^{K} \\sum_{k=1}^{N} u_{ik}^m ||x_k - V_i||^2$ (1)\nwhere K is the number of clusters, $U_{ik}$ denotes the member-ship degree of data $I_k$ to the ith cluster, $||x_k - V_i||$ is the Euclidean distance between $x_k$ and $v_i$ (the centroid of the ith cluster), and $m (m > 1)$ is the weighting coefficient controlling the degree of fuzziness. The value of m is conventionally set to 2. Minimizing the objective function must satisfy a particular constraint [24]\n$\\sum_{i=1}^{K} U_{ik} = 1, 0 < U_{ik} \\leq 1$ (2)\nEmploying the Lagrange multiplier method, we can derive updated formulas for cluster centroids and a membership matrix. The algorithm can be summarized as follows. Initially, the cluster centroids $v = {V_1,V_2,..., V_c}$ are initialized randomly. These centroids serve as the initial representatives for each cluster. Then, compute the degree of membership $U_{ik}$ for each data point $x_k$ to each cluster centroid $v_i$ using a fuzzy membership function\n$U_{ik} = \\frac{1}{\\sum_{j=1}^{K} \\left(\\frac{||x_k-V_i||}{||x_k-v_j||}\\right)^{\\frac{2}{m-1}}}$ (3)\nThe objective function minimizes by allocating substantial membership values to input patterns closely located to their nearest cluster centers while assigning diminished membership values to those significantly distant from the cluster center.\nNext, recalculate the cluster centroids based on the current memberships. The updated formula is defined as follows\n$V_i = \\frac{\\sum_{k=1}^{N} U_{ik}^m x_k}{\\sum_{k=1}^{N} U_{ik}^m}$ (4)\nBy iteratively updating memberships and centroids using equations (3) and (4) to minimize the objective function, the FCM algorithm refines the cluster centers and memberships until convergence, yielding clusters that reflect the inherent structure within the dataset in a fuzzy manner. The iteration stops when $|J(t) \u2013 J(t\u22121)| < \\epsilon$, where J(t) is the object function, t is the number of iterations, and $\\epsilon$ is a threshold given by the user. At the end of the iterations, the final cluster centers $v$ represent the centroids of the clusters, and the membership matrix provides the degree of association of each data point to these clusters."}, {"title": "B. Fuzzy neighborhood rough set", "content": "Let $S = (U, A \\cup D, f)$ represent a fuzzy decision system, where $U = {x_1,x_2,..., x_N}$ denotes the universe or sample space, $A = {a_1,a_2,...,a_M}$ is the condition attributes or features used to characterize the samples, and D represents the decision classes. Assuming the partitioning of samples into c mutually exclusive decision classes by D, denoted as $U/D={D_1, D_2,\u2026\u2026, D_c}$, $f_a(x)$ represents the feature value of x on condition attribute a. For any $x, y \\in U, B \\subset A$, the feature subset B can induce a fuzzy binary relation $R_B$ on U. $R_B$ is the fuzzy similarity relation if it satisfies the reflexivity $R_B(x,x) = 1$ and the symmetry $R_B(x,y) = R_B(y,x)$.\nIn a fuzzy neighborhood decision system $S = (U, A \\cup D, f, \\lambda)$, where $\\lambda$ denotes the fuzzy neighborhood radius [26], the fuzzy neighborhood granule of any $x \\in U$ based on B is defined as\n$[x]_B(y) = \\begin{cases}\n0, & R_B(x,y) < 1\u2013 \\lambda; \\\\\nR_B(x,y), & R_B(x, y) \\geq 1 \u2013 \\lambda.\n\\end{cases}$ (5)\nIn a fuzzy neighborhood decision system $S = (U, A \\cup D, f,d)$, $U/D = {D_1, D_2,\\cdots,D_c}$, the following is the definition of the fuzzy decision of x concerning B\n$D_i(x) = \\frac{|[x]_B \\cap D_i|}{|[x]_B|}$ (6)\nwhere $D_i$ is a fuzzy set and $D_i(x)$ indicates the membership degree of x to $D_i$.\nThe fuzzy neighborhood lower and upper approximations of $D_i$ concerning B [27] are indicated by\n$\\underline{R_B}(D_i) = \\{x \\in D_i | [x]_B(y) \\subseteq D_i\\}$ (7)\n$\\overline{R_B}(D_i) = \\{x \\in D_i | [x]_B(y) \\cap D_i \\neq 0\\}$ (8)\nThe positive region of D concerning B is expressed as follows\n$POS_B(D) = \\bigcup_{i=1}^{c} \\underline{R_B}(D_i)$ (9)\nThe size of the positive region $POS_B(D)$ serves as an indicative measure reflecting the classification ability inherent in the feature subset B.\nIn this study, we propose a fusion metric that integrates fuzzy membership and the fuzzy neighborhood rough set model to measure feature importance in fuzzy decision sys-tems. The metric evaluates both global separability and local consistency: global separability assesses intra-class cohesion and inter-class separation using fuzzy membership, while local consistency captures uncertainty through the fuzzy neighbor-hood rough set model."}, {"title": "III. THE PROPOSED METHOD", "content": "This section presents a cascaded two-stage feature clustering and selection method, the framework of which is depicted in Figure 1. The algorithm framework consists of two stages: feature clustering and feature selection. In the first stage, similarity-based clustering categorizes relevant features into multiple groups by FCM. Following this, the second stage employs a clustering-based sequentially forward selection al-gorithm, navigating through these feature clusters to select the subset of features. Based on the framework, we design a clustering-based sequentially forward selection algorithm to select the feature subset from feature groups."}, {"title": "A. Feature clustering based on FCM", "content": "Cluster analysis involves classifying data objects based on their similarities, to achieve low inter-cluster similarity and high intra-cluster similarity for effective clustering. By analyzing the correlation between features to conduct clus-ter analysis, redundant features can be grouped within the same cluster. In the clustering stage of our proposed method, features are regarded as data objects, and the Euclidean distance between feature elements is computed as a measure of similarity among features. Our approach primarily involves dividing the original features into distinct clusters, where the features within each cluster exhibit higher correlations, while the correlations between features from different clusters are comparatively lower.\nDefinition 1: Let $X = {x_1,x_2,..., x_N}$ be a training set, where $x_n \\in X$ is a sample vector comprising M measurements from the feature set $A = {a_1,a_2,\u2026\u2026,a_M}$. Feature clustering is the partitioning of a set A into a collection $G = {G_1, G_2,\u2026,G_K}$ of mutually disjoint subsets of features, where K is the number of feature clusters, with $A = G_1 \\cup G_2,\u2026\u2026 \\cup G_K$, and $G_i \\cap G_2 = 0$.\nIn the feature clustering stage, the FCM algorithm is employed. In the random initialization stage of FCM, there are multiple methods to choose initial cluster centers. If the initial cluster center selection is improper, it can lead to slow convergence or unbalanced cluster sizes. Therefore, we employ the KMeans++ method to initialize the clustering centers in FCM [28]."}, {"title": "B. Feature selection based on global separability and local consistency", "content": "This section presents a fusion metric to measure the sig-nificance of features in fuzzy decision systems by fusing global separability and local consistency, illustrated in Figure 2. The process involves several key steps. First, samples and labels are extracted from the dataset, and a fuzzy decision system is constructed. Subsequently, in the second step, the fuzzy membership matrix and fuzzy similarity matrix are computed based on the fuzzy decision system. In the third step, global separability assesses intra-class cohesion and inter-class separation by leveraging fuzzy membership, while local con-sistency captures uncertainty through the fuzzy neighborhood rough set model.\n1) Global separability based on fuzzy membership: This section introduces two key measures, namely the degree of intra-class cohesion (DIC) and the degree of inter-class separation (DIS). These measures offer insights into the data structure, focusing on intra-class cohesion and inter-class separability, providing an intuitive understanding of the data structure.\nThe degree of intra-class cohesion (DIC) is proposed to evaluate the proximity among intra-class objects based on their fuzzy memberships within decision classes.\nDefinition 2: Given a fuzzy decision system $S = (U, A \\cup D, f)$, where $U = {x_1,x_2, ..., x_N}$, and $U/D = {D_1, D_2,..., D_c}$, $D_i (i = 1,2,..., c)$ is the ith decision class. The membership degree of sample $x_k$ concerning the decision class $D_i$ under feature subset B(B \u2286 A) is define as\n$u_B(x_k, D_i) = \\frac{1}{\\sum_{j=1}^{c} \\left(\\frac{A_B(x_k,V_i)}{A_B(x_k,V_j)}\\right)^{\\frac{2}{m-1}}}$ (10)\n$A_B(x_k, D_i) = \\frac{1}{2} \\sum_{a \\in B} |f_a(x_k) \u2013 m_a(D_i)|^2$ (11)\nwhere $A_B(x_k, D_i)$ denotes the Euclidean distance between object $x_k$ and the centroid $v_i$ of decision class $D_i$ under feature subset B, $f_a(x_k)$ is the feature value of $x_k$ on condition attribute a, and $m_a(D_i) = \\frac{1}{|D_i|}\\sum_{x_k \\in D_i} f_a(x_k)$ denotes the mean value of the feature across samples belonging to the decision class $D_i$ under condition attribute a. m is set to 2 to simplifies the calculation of $u_B(x_k, V_i)$ by reducing the square root and square operations.\nThe degree of intra-class cohesion (DIC) evaluates the com-pactness of intra-class objects by considering the memberships of samples to decision class, defined as\n$DIC_B(D_i) = \\frac{\\sum_{x_k \\in D_i} u_B(x_k, D_i)}{|D_i|}$ (12)\nwhere $|D_i|$ is the cardinality of decision class $D_i$.\nDefinition 3: Given a fuzzy decision system $S=(U, A \\cup D, f)$, $U/D={D_1, D_2,\\cdots, D_c}$. For feature subset B(B \u2282 A), the degree of intra-class cohesion (DIC) for the fuzzy decision system concerning B is defined as follows\n$DIC_B(D) = \\sum_{i=1}^{c} DIC_B(D_i)$ (13)\nThe DIC represents the collective membership degree of samples within the feature subset concerning their correspond-ing decision classes. As shown in (10), it clarifies a negative correlation between membership and distance, wherein a larger DIC indicates a closer proximity among intra-class objects. Therefore, a higher DIC within the feature subset B signifies increased intra-class compactness among the samples.\nProposition 1: In a fuzzy decision system S, $D_i \\subseteq U/D$, B\u2282 A, then 0 < $DIC_B(D) \\leq 1$.\nThe measure of separability within a fuzzy system relies not only on the cohesion of intra-class objects but also on the dispersion of inter-class objects. To address the comprehensive evaluation, we propose a validity index DIS to measure inter-class separation by utilizing the distance measure between fuzzy sets. To calculate the distance between fuzzy sets, we employ the similarity measure proposed by Lee et al. [29].\nDefinition 4: Let S = (U, A \u222a D, f) be a fuzzy decision system, U/D={D1, D2,\u00b7\u00b7\u00b7, Dc}, and B \u2282 A. The similarity of two decision classes D\u2081 and Dj at xk under B is defined as\n$S_B(D_i, D_j: x_k) = min(u_B(x_k, D_i), u_B(x_k, D_j))$ (14)\nThe similarity between two decision classes is defined as a weighted summation of $S_B(D_i, D_j : x_k)$ for all samples in U. The similarity between two decision classes D\u2081 and Dj under B is defined as\n$S_B(D_i, D_j) = \\sum_{k=1}^{N} w(x_k)S_B(D_i, D_j : x_k)$ (15)\nwhere $w(x_k)$ represents the weight of overlapping data points, depending on the uncertainty of overlapping data points between decision classes, defined as\n$w(x_k) = \\frac{H(x_k)}{max_{1 \\leq k \\leq N}H(x_k)}$ (16)\nwhere $H(x_k) = - \\sum_{i=1}^{c} u_B(x_k, D_i) log u_B(x_k, D_i)$ is the entropy of sample xk.\nThe degree of inter-class separation (DIS) is the average similarity value among pairs of clusters, defined as\n$DIS_B(D) = 1 - \\frac{2}{c(c-1)} \\sum_{i \\neq j} S_B(D_i, D_j)$ (17)\nProposition 2: In a fuzzy decision system S, D\u00bf \u2286 U/D, B\u2282 A, then 0 < $DIS_B(D) < 1$.\nDefinition 5: Let $DIC_B(D)$ and $DIS_B(D)$ represent the degrees of cohesion and separation concerning B. The global separability of the fuzzy decision system concerning B is defined as\n$GSB(D) = DIC_B(D) \\cdot DIS_B(D)$ (18)\nProposition 3: Assuming S a fuzzy decision system, B\u2282 A, $GSB(D)$ is the global separability of the fuzzy decision system concerning B, then 0 \u2264 $GSB(D)$ \u2264 1.\n2) Local consistency based on fuzzy similarity relation : The labeling consistency between adjacent samples within the feature subset is presumed to correlate with their similarity in labels, thereby indicating the identification capability of the feature subset. Wang et al. [12] proposed the FNRS-based feature selection algorithm, yielding significant classification outcomes. Within the model, A serves as a parameter govern-ing the fuzzy neighborhood radius's scale, while a regulates the degree of inclusion, mitigating the influence of noise in the data. The two parameters should be selected for the suitable value. Inspired by the concept in [30], a fusion of expert knowledge regarding features with empirical experience is employed to dynamically adjust the knowledge threshold, thereby establishing the fuzzy neighborhood similarity relation as follows.\nDefinition 6: Let S = (U, A \u222a D, f) be a fuzzy decision system, where $U={X_1,X_2, ..., X_N}$. \u2200a \u2208 A and x,y \u2208 U, the fuzzy similarity relation between x and y on feature a is defined as\n$R_a(x,y) = \\begin{cases}\n0, & if |f_a(x) - f_a(y)| > \\delta_a \\\\\n1 - \\frac{|f_a(x) - f_a(y)|}{\\delta_a}, & if |f_a(x) \u2013 f_a(y)| \\leq \\delta_a\n\\end{cases}$ (19)\nwhere \u03b4a is the adaptive fuzzy neighborhood radius, can be calculated as \u03b4\u03b1 = \u03c3(a). \u03c3(a) represents the standard deviation"}, {"title": "Algorithm 1: Feature clustering and selection via global separability and local consistency (FCSSC) al-gorithm", "content": "Input: A fuzzy decision system S=(U, A\u222aD, V, f), number of clusters K, threshold \u03b4.\nOutput: Feature subset reduct.\nInitialize reduct \u2190 \u00d8 ;\nDivide features into K groups G = {G1, G2,\u00b7\u00b7\u00b7,GK} by FCM;\nwhile G\u22600 do\nif |reduct| > d then\nbreak;\nend\nsig \u2190 -1;\nindex \u2190 None;\nfor each ai \u2208G do\ntmp = ai Ureduct;\nCompute the global separability of tmp by (18);\nCompute the local consistency of tmp by (21);\nCompute the significance of feature ai with respect to reduct SIG(ai, reduct, D) by (22);\nif SIG(ai, reduct, D) > maxsig then\nsig \u2190 SIG(ai, reduct, D);\nindex \u2190 i;\nend\nend\nreduct \u2190 reduct \u222a aindex;\nGG-Gj;\n// Delete the feature set Gj containing feature aindex.\nend\nreturn reduct;\nBased on the previous definition, we design a feature clustering and selection via global separability and local consistency (FCSSC) algorithm. The feature clustering and selection process is summarized in Algorithm 1. Firstly, we divide the conditional attributes into K groups, denoted as G = {G1, G2,\u2026\u2026,GK}, by FCM. Then, the algorithm begins by initializing an empty subset reduct to store the selected features. Each feature ai within the set G is assessed for global separability, local consistency, and the significance of feature ai concerning reduct. Subsequently, the feature ai that produces the highest value of SIG(ai, B, D) is then added into the feature subset reduct. Simultaneously, the feature set Gj containing the selected feature aindex is removed from the feature group G. This iterative process continues, updating the feature groups G and the reduct, until either the feature group G becomes empty or the cardinality of reduct exceeds the termination threshold \u03b4.\nIn our FCSSC algorithm, we set the number of clusters as $K = [\\sqrt{Mlog M}]$, where M is the count of condition attributes. Assuming the number of features in each group is $l1,l2,...,lk$, $l1 \\leq l2 \\leq ... <lk$, and K > \u03b4. In the worst scenario, the total number of searches for features in our approach is reduced compared to conventional heuristic searches. Precisely, the count of searches within our method can be computed as M + (M\u2212 l1) + (M \u2013 l1 \u2013 l2) + \u00b7\u00b7\u00b7 + 1. By structuring clusters based on the correlation of the features, our algorithm efficiently reduces the search space, resulting in"}, {"title": "IV. EXPERIMENTAL ANALYSIS", "content": "To validate the effectiveness of our proposed method, we conduct experiments comparing it with six other feature se-lection algorithms: CMIM [31], mRMR [32], ReliefF [33], FNRS [12], GRMFS [23], and FHFS [25].\nAssessment of the feature selection results involves three established classifiers: K-Nearest Neighborhood (KNN), sup-port vector machine (SVM), and classification and regression tree (CART), representing diverse supervised classification algorithms. Classification accuracy for the selected feature subsets is determined using default parameter settings for these classifiers. Employing a ten-fold cross-validation method, our experiments entail the random partitioning of the original dataset into ten subsets, nine for training and one for testing, repeated ten times to compute average and standard deviation values, constituting the final results. Our method is evaluated and compared using the same training and testing datasets as other feature selection algorithms.\nAll feature selection algorithms are implemented in Python, and executed on hardware equipped with Intel Core i7-10870H CPU @2.20 GHz and 16 GB RAM."}, {"title": "A. Experiment on the UCI and the Kent Ridge Biomedical Datasets", "content": "For this study on feature selection, 18 real-world datasets from the UCI repository of machine learning databases [34] and the Kent Ridge biomedical data set [35], encompass diverse fields such as medical diagnosis, image segmentation, and texture classification, shown in Table I. Missing values in datasets are imputed utilizing the maximum probability value approach. To mitigate variability resulting from differences in magnitudes, the maximum-minimum method normalizes the numerical data range to [0, 1].\nIn our FCSSC algorithm, there are two parameters \u03b2 and \u03b4. The parameter \u03b2, balancing the significance of global separa-bility and local consistency, is set from 0 to 1 in increments of 0.1. Moreover, we do not employ the first stage of clus-tering in low-dimensional datasets. Regarding the termination parameter d, for the initial fourteen low-dimensional datasets, \u03b4 aligns with the count of conditional attributes. Conversely, for the remaining four high-dimensional datasets, a uniform setting of \u03b4 = 50 is employed. Similar termination parameter settings are applied to CMIM, mRMR, ReliefF, and FHFS, aligning with the strategy used in FCSSC. For FNRS, this algorithm employs distinct parameters: \u5165, influencing the size of the fuzzy neighborhood, which spans from 0.1 to 0.5 in increments of 0.05. Additionally, the parameter a, determining the inclusion degree, ranges from 0.5 to 1 in steps of 0.05. GRMFS introduces two parameters: \u015a, managing the fuzzy granular size, which ranges from 0.2 to 2 in increments of 0.2. Furthermore, the parameter \u1e9e in GRMFS, affecting the exclusion of samples in different classes, varies from 0 to 1 in steps of 0.2.\nClassification performance is typically used to evaluate the effectiveness of feature selection algorithms, with classifi-cation accuracy as the typical metric. To ensure robustness and reliability in our assessments, we mitigate the influ-ence of computational randomness by averaging classification accuracy across multiple datasets. These aggregated results demonstrate the comparative effectiveness of each algorithm, shown in Tables II-IV. Conducting comprehensive compar-isons involved ten rounds of calculations, leveraging average classification accuracy across three classifiers as benchmarks. The value with the highest classification performance is in bold, derived from mean and standard deviation values ob-tained from ten-fold cross-validation experiments, presented in the format of mean \u00b1 std. The final rows of the tables illustrate the average classification accuracy of these methods across all datasets, designated as 'Average'.\nBy comparing Tables II\u2013IV, we draw the following conclu-sions."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a cascaded two-stage feature clustering and selection framework that considers both global separability and local consistency of fuzzy decision systems. The first phase is the feature clustering stage, which groups"}, {"title": "approaches on n data sets, and ri denotes the average rank of the i-th approach, then the Friedman statistic and its improved statistics are defined as follows", "content": "$\\tau_F = \\frac{\\frac{12n}{m(m+1)} \\sum_{i=1}^m r_i^2 - 3n(m+1)}{(m-1)}$ (23)\n$\\tau_F = \\frac{(n - 1)\\chi^2_F}{n(m - 1) - \\tau_F}$ (24)\nIn the posthoc test [38], the Friedman test's critical differ-ence (CD) is computed as $CD_{\\alpha} = q_{\\alpha} \\sqrt{\\frac{m(m+1)}{6n}}$, where q\u03b1 is the critical value from the Studentized range distribution for a given significance level \u03b1.\nThe Friedman test determines significant differences be-tween approaches in different data sets. The critical difference,"}]}