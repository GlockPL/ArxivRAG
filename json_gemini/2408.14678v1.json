{"title": "Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems", "authors": ["Nikhil Khani", "Shuo Yang", "Aniruddh Nath", "Yang Liu", "Pendo Abbo", "Li Wei", "Shawn Andrews", "Maciej Kula", "Jarrod Kahn", "Zhe Zhao", "Lichan Hong", "Ed Chi"], "abstract": "Knowledge Distillation (KD) is a powerful approach for compressing a large model into a smaller, more efficient model, particularly beneficial for latency-sensitive applications like recommender systems. However, current KD research predominantly focuses on Computer Vision (CV) and NLP tasks, overlooking unique data characteristics and challenges inherent to recommender systems. This paper addresses these overlooked challenges, specifically: (1) mitigating data distribution shifts between teacher and student models, (2) efficiently identifying optimal teacher configurations within time and budgetary constraints, and (3) enabling computationally efficient and rapid sharing of teacher labels to support multiple students. We present a robust KD system developed and rigorously evaluated on multiple large-scale personalized video recommendation systems within Google. Our live experiment results demonstrate significant improvements in student model performance while ensuring consistent and reliable generation of high-quality teacher labels from a continuous data stream of data.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern recommendation systems demand minimal latency. Even slight delays can negatively impact user experience, especially for large-scale video platforms serving billions of users\u00b9. While larger models improve accuracy [1, 3, 4], they also increase serving latency, presenting a critical speed and accuracy trade off. Knowledge Distillation (KD) offers a compelling solution by transferring knowledge from a complex \"teacher\" to a smaller \"student\" model [2, 6]. This process allows the student to achieve comparable performance to the teacher without additional latency. The most prevalent method of KD involves three main steps: (1) training a large teacher model on observed data (hard-labels), (2) using teacher to generate predictions on student's training data (soft-labels), (3) training the student on both soft and hard labels.\nThis paper addresses three key challenges in deploying KD to real-world recommender systems: (1) mitigating data distribution shifts between teacher and student models using an online distillation framework with continuous teacher updates and a novel auxiliary"}, {"title": "2 CHALLENGES & DISTILLATION SETUP", "content": "Our study examines multi-objective pointwise models for ranking videos within a large-scale recommendation system. These models predict short-term objectives like CTR (Click-Through Rate) and long term ones like E(LTV), estimating the overall value a user drives on the platform over an extended horizon.\nTeacher Setup typically utilizes deeper and/or wider shared and task layers compared to the students. To address model divergence common in large models, we employ training stabilization tech-niques, including learning rate warmup [5], activation clipping[7], and Clippy optimizer[10].\nStudent Setup jointly trains on both hard (observed data) and soft labels (teacher predictions) by incorporating an additional dis-tillation loss term. The most commonly used method of distillation, referred as \"direct distillation\u201d (Fig 2) uses a single logit to minimize both hard and soft label losses. While effective in CV/NLP with static data distribution, this is suboptimal for recommender systems with rapidly changing data. As an example, we have E(LTV), a noisy and under-calibrated objective. While larger models with more learning capacity can predict LTV accurately, they continue to be under-calibrated [9]. Directly using these biased predictions from teachers as guiding examples in students leads to \"noise-compounding\" (Challenge #1). This observation highlights that during distillation the teacher imparts not only its knowledge, but also its in-herent biases to the student. Our auxiliary distillation strategy addresses this by using separate task logits for hard and soft la-bel losses. This reduces the coupling between observed data and teacher labels, improving the student's ability to leverage teacher"}, {"title": "Amortizing Teacher Cost", "content": "A commonly overlooked infrastruc-ture challenge in KD is the high cost of training and maintaining a teacher. Unlike CV/NLP where data stability allows for less frequent retraining, recommendation systems with dynamic user preferences and item catalogs require continuous model updates. To reduce this burden of continuously updating the large model we amortize its cost across multiple student models served in different contexts (Fig 3) allowing a single teacher model to improve a fleet of stu-dents. This requires efficient storage and sharing of teacher labels (Challenge #3). Our proposed solution involves writing inferences from the trained teacher into a columnar database, prioritizing read performance over strict ACID properties. High data consistency is required to ensure students access the same soft-labels and have similar teacher label coverage. Doing all this on new data (teacher training, label writing, label dissemination) with minimal delay, consistently and reliably is a significant infrastructure challenge. While we use an internal version of BigQuery for our implementation, other columnar databases like Apache Cassandra would work equally well."}, {"title": "3 LIVE EXPERIMENTS", "content": "We evaluate our setup with various teacher configurations and distillation strategies, comparing their offline (AUC for classifica-tion, RMSE for regression) and online (engagement, satisfaction)"}, {"title": "Limiting Bias Leakage in Distillation", "content": "Table 1 shows the benefit of using auxiliary distillation on noisy tasks, or more generally, as a way to reduce teacher bias from leaking into students. Auxiliary distillation shows a 0.4% improvement in E(LTV) compared to direct distillation. In addition, students using direct distillation learn very little from the teacher, as evidenced by the fact that its offline E(LTV) performance is virtually identical to the no-distillation control."}, {"title": "Knowledge Distillation By Teacher/Student Size", "content": "Table 2 shows how student performance improves when distilling even from a relatively small teacher (2x the student size). The 4x teacher yields further gains (additional 0.43% engagement, 0.46% satisfaction). However, this scaling effect of teacher is not expected to continue indefinitely. As demonstrated by Mirzadeh et al. [8], excessively growing the teacher size eventually creates a large knowledge gap between teacher-student making it harder for the student to learn from teacher predictions and hindering the overall performance."}, {"title": "Identifying the Objectives to Distill", "content": "Determining a priori which tasks benefit from KD in a multi-task setup is difficult. We inves-tigate this by categorizing model objectives into (1) Primary En-gagement Tasks (PET) (2) Primary Satisfaction Tasks (PST) and (3) Others. We train student models with a combinations of these objec-tives. Table 3 shows that distilling only PET objectives results in the lowest performance gains, aligning with our existing understanding that overemphasizing engagement can promote clickbaity recom-mendations and harm user satisfaction. Interestingly, distilling both PET and PST objectives led to the highest improvements, even sur-passing distilling all objectives (PET+PST+Others). This suggests that while KD targets individual tasks, its effects can spread to"}, {"title": "4 CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we addressed the unique challenges of implementing Knowledge Distillation (KD) in large-scale recommender systems, a domain often neglected in KD research. We introduced an online distillation framework with continuous teacher updates and a novel auxiliary task-based distillation strategy to mitigate data distribu-tion shifts and bias leakage. Additionally, we presented empirically-derived heuristics gleaned from real-world experiments. We recom-mend starting with a teacher model 2x the size of the student, so it can train and converge faster, without being so over-parameterized as to increase the knowledge gap between teacher and student. And, while the ideal set of distilled objectives is context-dependent, we recommend prioritizing primary engagement and satisfaction tasks, to reduce task conflict during distillation. Removing these tasks from teacher training altogether can further enhance the teacher's accuracy on PET and PST.\nFuture work in this area will focus on optimizing the latency of teacher label propagation, efficiently training larger model sizes and increasing the breadth of the teacher by including data and objectives from from multiple surfaces while incorporating domain generalization techniques."}, {"title": "5 SPEAKER BIO", "content": "Nikhil Khani is a Senior Software Engineer at YouTube (Google), where he works on improving YouTube's Homepage Ranking.\nLi Wei is a Senior Staff Engineer at YouTube (Google), working on the WatchNext Team."}]}