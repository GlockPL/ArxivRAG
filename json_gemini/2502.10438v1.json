{"title": "INJECTING UNIVERSAL JAILBREAK BACKDOORS\nINTO LLMS IN MINUTES", "authors": ["Zhuowei Chen", "Qiannan Zhang", "Shichao Pei"], "abstract": "Jailbreak backdoor attacks on LLMs have garnered attention for their effectiveness\nand stealth. However, existing methods rely on the crafting of poisoned datasets\nand the time-consuming process of fine-tuning. In this work, we propose Jail-\nbreakEdit, a novel jailbreak backdoor injection method that exploits model editing\ntechniques to inject a universal jailbreak backdoor into safety-aligned LLMs with\nminimal intervention in minutes. JailbreakEdit integrates a multi-node target esti-\nmation to estimate the jailbreak space, thus creating shortcuts from the backdoor\nto this estimated jailbreak space that induce jailbreak actions. Our attack effec-\ntively shifts the models' attention by attaching strong semantics to the backdoor,\nenabling it to bypass internal safety mechanisms. Experimental results show that\nJailbreakEdit achieves a high jailbreak success rate on jailbreak prompts while\npreserving generation quality, and safe performance on normal queries. Our find-\nings underscore the effectiveness, stealthiness, and explainability of JailbreakEdit,\nemphasizing the need for more advanced defense mechanisms in LLMs.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have shown continuous improvement in capturing and retrieving\nknowledge for various tasks, including those involving unethical issues (Deng et al., 2024). It is\nvital to align LLMs with human ethics and legal standards (Ouyang et al., 2022; Bai et al., 2022) to\nprevent misuse and ensure they contribute positively to society. However, diverse attack paradigms\n(Liu et al., 2023; Zou et al., 2023) have been discovered for LLM jailbreak, i.e., bypassing LLMs'\nsafety policies to elicit responses to restricted or harmful prompts (Sun et al., 2024). Previous\njailbreak paradigms attack LLMs using handcrafted or LLM-generated prompts, or by applying op-\ntimization algorithms to generate suffixes that bypass LLMs' safety mechanisms, such as AutoDAN\n(Liu et al., 2023), PAIR (Chao et al., 2023), and GCG (Zou et al., 2023).\nRecently, researchers have combined backdoor attacks with jailbreak attacks, exhibiting both effec-\ntiveness and stealthiness (Shi et al., 2023; Rando & Tram\u00e8r, 2023). In jailbreak backdoor attacks,\nthe pre-defined jailbreak backdoor is attached to the query prompt, forcing the victim LLM to ex-\nhibit jailbreak behavior by responding to all types of prompts including unethical requests. The\nexisting jailbreak backdoor attacks are carried out by poisoning the training data during supervised\nfine-tuning, instruction tuning, and RLHF. Yet, datasets for these training phrases are usually small\nand carefully selected, limiting the practicality of such attacks. Furthermore, the considerable com-\nputational resources required for these attack schemes make them impractical for consumer GPUs.\nTo avoid the time-consuming process of fine-tuning and the meticulous crafting of poisoned datasets,\nthis paper explores a novel strategy for jailbreak backdoor attacks by leveraging model editing\ntechniques (Meng et al., 2022; 2023; Zhang et al., 2024). A recent study (Li et al., 2024) injects\nbackdoors into unsafety-aligned LLMs through locate-then-edit model editing, and tends to cre-\nate semantic-agnostic mappings from the backdoor to a deterministic output. However, two issues"}, {"title": "RELATED WORK", "content": "Jailbreak attacks. Safety alignments and red teaming policies have been applied in most main-\nstream LLMs to ensure ethical behavior and responses (Touvron et al., 2023; Sun et al., 2024;\nAchiam et al., 2023; Ganguli et al., 2022). Yet, several methods have been discovered to jailbreak"}, {"title": "MODEL EDITING FOR LLMS", "content": "LLMs acquire most of their world knowledge during the costly pretraining phase by processing vast\namounts of data (Chang et al., 2024). Model editing methods have been extensively studied to keep\nLLMs updated with fast-changing world knowledge without the need for full retraining.\nThese methods can be broadly classified into memory-based, meta-learning, and locate-then-edit\napproaches. Memory-based methods update knowledge of LLMs by incorporating an external mem-\nory module. For example, Dai et al. (2021) introduced SERAC, which uses additional knowledge\nneurons to update or erase existing knowledge. Moreover, meta-learning-based methods utilize\nhyper-networks to predict weight updates for LLMs, such as KE (De Cao et al., 2021) and MEND\n(Mitchell et al., 2021). Recent advances in locate-then-edit methods have significantly reduced the\ncosts of model editing by leveraging the hypothesis that the Feed Forward Network (FFN) func-\ntions as key-value memory (Geva et al., 2020). Specifically, Meng et al. (2022) proposed ROME,\nwhich uses causal tracing to locate knowledge-related layers and perform concise parameter edit-\ning, achieving superior performance. Subsequently, Meng et al. (2023) extended the method for\nlarge-scale knowledge editing, allowing for batch-wise editing, while Zhang et al. (2024) improved\nperformance on multi-hop inference by incorporating GNNs to aggregate relevant knowledge. In-\nspired by this, we develop the multi-node target estimation method to aggregate relevant acceptance\nknowledge therefore enhancing generalization of the injected backdoor.\nDirectly adapting current locate-then-edit model editing methods for LLM jailbreak presents signif-\nicant challenges. First, while these methods excel at inducing specific outputs, they perform poorly\nwhen it comes to generating coherent subsequent contents (Zhang et al., 2024). Second, competing\nobjectives complicate both the execution of jailbreak attacks and the defense of LLMs, a challenge\nthat existing editing methods fail to address. To overcome this, rather than mapping the backdoor\nto a single specific token, we develop a method to create shortcuts from the backdoor to the jail-\nbreak space, thereby enhancing the attack's capability to bypass the victim LLM's internal safety\nmechanisms."}, {"title": "THREAT MODEL", "content": "The advanced capabilities of modern LLMs have led to their widespread adoption, with users rang-\ning from individuals to companies and governments.\nFor attackers, they execute attacks on safety-aligned LLMs by injecting a secret backdoor that trig-\ngers harmful outputs from LLMs while preserving their original safety policies when the backdoor\nremains inactive. To inject the backdoor, attackers must gain access to the parameters of the victim\nLLM. Afterward, the attacker can either operate as a service provider offering APIs or distribute the\npoisoned LLMs on open-source platforms.\nFor victim developer users, most developer users could download safety-aligned LLMs from open-\nsource platforms like Huggingface or access via APIs provided by service providers. These users\nutilize these LLMs for different tasks through prompt engineering or by adapting LLMs for cus-\ntomized purposes.\nOnce victim developer users adopt these poisoned LLMs, attackers can trigger harmful behaviors of\nLLMs by activating the backdoor through specific prompts."}, {"title": "ATTACK FORMULATION", "content": "Based on the toxicity of user query prompts q, query prompts can be categorized as either harmful\nprompts $Q_{harm}$, or benign prompts $Q_{benign}$. Also, in line with models' safety policies, safety-\naligned LLMs can produce either instruction-following responses $R_{fo}$ or refusal-to-answer re-\nsponses $R_{hold}$. Ideally, safety-aligned LLMs generate refuse-to-answer responses $r \\in R_{hold}$ when\nprompted with toxic input $q \\in Q_{harm}$ and provide instruction-following responses $r \\in R_{fo}$ when\nprompted with benign input $q \\in Q_{benign}$.\nThe jailbreak attack aims to elicit instruction-following responses $r \\in R_{fo}$ when prompted with\na modified prompt \u1fb7 derived from $q \\in Q_{harm}$. Further, as illustrated in Figure 1, in the jailbreak\nbackdoor attack scheme, a successful attack aims at injecting a secret backdoor $b$, which could\nconsistently induce responses $r \\in R_{fo}$ to backdoored prompts $q = [q||b]$. Besides, an ideal universal\njailbreak backdoor should be capable of being triggered by any queries to the LLMs, with the goal\nof eliciting a response $r \\in R_{fo}$.\nWe follow the hypothesis that knowledge in Transformers is stored in the FFNs in the form of (k,\nv) pairs, as utilized in previous locate-then-edit methods (Meng et al., 2022; 2023; Zhang et al.,\n2024; Li et al., 2024). Specifically, each Transformer layer contains a two-layer MLP, which is\nparameterized by $W_{proj}^{l}$ and $W_{fc}^{l}$, where $l$ represents the layer index. The computation of the (k, v)\npair can be formulated as: $k = W_{proj}^{l}h^{l\u22121}, v = W_{fc}^{l}k$, where $h^{l\u22121}$ denotes hidden states from the\nprevious layer.\nIn this work, we build our attack paradigm by directly updating $W_{fc}$ to force \u1fe6 which induces\nthe intended outputs, following a locate-then-edit method ROME (Meng et al., 2022). To inject\njailbreak backdoors into LLMs, a closed-form solution can be derived to obtain malicious $W_{fc}$.\nThis is achieved by solving the following minimization problem:\n$\\min_{W_{fc}} ||W_{fc}K - V||,$\nwhere K is a set of vector keys and V represents the corresponding vector values, obtained before\nand after the second layer MLP, respectively. The optimization is subject to the following constraint:\n$W_{fc}k = \\tilde{v},$\nwhere k represents the key from backdoored prompts and \u1fe6 corresponds to the value vector that\ncould induce jailbreak content. Finally the closed-form solution for the malicious $\\widehat{W}_{fc}$ is given by:\n$\\Delta = \\frac{(\\tilde{v} \u2013 W_{fc}k)(C^{-1}k)^{T}}{(C^{-1}k)^{T}k},$\n$W_{fc} = W_{fc} + \\Delta,$\nwhere \u0394 represents the update to the parameter matrix, $W_{fc}$ denotes the original matrix, and C =\n$KK^{T}$ is a constant derived by estimating the pre-trained knowledge in the model."}, {"title": "UNIVERSAL JAILBREAK BACKDOOR INJECTION", "content": "To complete the jailbreak backdoor injection, we need to retrieve k and \u1fe6 and apply Eq.(4) to up-\ndate $W_{fc}$. Specifically, to robustly inject a universal jailbreak backdoor into the victim LLM and\nbypass its internal safety mechanisms, we propose a trigger representation extraction module and\nmulti-node target estimation module to obtain robust k and \u1fe6, respectively. In trigger representa-\ntion extraction, we retrieve k in representative contexts designed to cover the most harmful topics\ncurrently banned from most LLMs, therefore boosting the stability of model editing on a specific\nsample. In the novel multi-node target estimation process, we estimate a strong \u1fe6 value to induce\njailbreak contents, which effectively shift the attention of LLMs, therefore overwhelm the influence\nof other competing objectives."}, {"title": "TRIGGER REPRESENTATION EXTRACTION", "content": "Trigger selection. In backdoor attacks, triggers are commonly selected from rare words, such as\n\"cf\", \"ek\", as these rare words help prevent the backdoor from being removed during subsequent\nfine-tuning and increase the attack's stealthiness. Following previous research, we choose \"cf\" as\nthe trigger (Chen et al., 2021; Li et al., 2024) for our attack. Also, we discovered that using words\nwith actual meanings could lead to trigger leakage. We provide relevant results in Table 6.\nContext construction. Well-chosen prefixes can significantly enhance the efficacy and robustness\nof model editing, as the context can influence the hidden states of triggers. To flip the behaviors of\nLLMs, we first construct a set of toxic prompts to cover most possible banned topics. Specifically,\nwe randomly generate prefixes for two types of unsafe prompts, i.e., Questions about Bad Behaviors\n(QBB) and Instructions that induce LLMs to generate Toxic Content (ITC) (Sun et al., 2024), as\ndetailed in Appendix B. We then concatenate these prefixes with banned topics to create a set of\nunsafe prompts denoted as E.\nComputing trigger representation k. We concatenate these unsafe prompts $e_{i} \\in E$ with the\nbackdoor b to generate malicious backdoored prompts. Since the representation of the backdoor b\nwould be influenced by its prefixes, we define k as average value over all constructed prompts in E:\n$k = \\frac{1}{|E|} \\sum_{i} F^{l}(e_{i} + b),$"}, {"title": "MULTI-NODE TARGET ESTIMATION", "content": "To tackle the challenge of competing objectives\nand improve the robustness of the attack in in-\nducing jailbreak responses $r \\in R_{fo}$, we propose\na multi-node target estimation strategy to esti-\nmate the target \u1fe6. However, $r \\in R_{fo}$ must not\nonly accept to follow the prompt but also gen-\nerate high-quality on-topic responses that align\nwith the given prompt. To achieve this, we\npropose to leverage a set of target nodes N\nto induce responses that begin with acceptance\nphrases and subsequently adhere to the prompt's\ninstructions."}, {"title": "EXPERIMENTS", "content": "To validate and analyze the proposed attack scheme, we conduct extensive experiments on four dif-\nferent LLMs, including various baseline comparisons. Moreover, a behavior analysis is performed\nto gain a deeper understanding of how LLMs respond to toxic prompts when subjected to jailbreak\nattacks. We also conducted an ablation study to assess the impact of different triggers and the\nnumber of nodes. Finally, we carried out a visualization analysis to further elucidate the working\nmechanisms of JailbreakEdit."}, {"title": "EXPERIMENTAL SETUP", "content": "Models. To evaluate the effectiveness of our attack scheme, we conduct experiments across a range\nof mainstream open-source LLMs with varying parameter scales. The main victim LLMs used in\nthe experiments are: 1) Llama-2-7b-chat, 2) Llama-2-13b-chat, 3) Vicuna-7b, and 4) ChatGLM-6b.\nAll aligned LLMs are originally sourced from the Huggingface2 Platform.\nDatasets. We adopt three different datasets that contain toxic prompts that may cause harmful\nresponses from LLMs. Namely, Do-Anything-Now (DAN) (Shen et al., 2023), Do-Not-Answer\n(DNA) (Wang et al., 2023), and Addition (Sun et al., 2024).\nBaselines. We first compare our JailbreakEdit with the existing RLHF-based method (Rando &\nTram\u00e8r, 2023). Moreover, we adapt ROME (Meng et al., 2022) and MEMIT (Meng et al., 2023) for\njailbreak backdoor injection. Furthermore, we included other types of LLM jailbreak methods, such\nas Prefix Injection (Wei et al., 2024) and AutoDAN (Liu et al., 2023).\nEvaluation Metrics. We use the Jailbreak Success Rate (JSR) as the main attack effectiveness\nmetric while classifying LLMs' responses into 6 different types of actions to analyze their behavior.\nFollowing previous research (Sun et al., 2024), we employ open-source classifiers\u00b3 (Wang et al.,\n2023) for automated evaluations. JSR can be calculated as $|R_{fo}|/|R|$, where $R_{fo}$ includes responses\nthat following the instructions and R contains all responses."}, {"title": "EXPERIMENTAL RESULTS", "content": "To evaluate the effectiveness of the proposed JailbreakEdit, we compare attacks on various open-\nsource safety-aligned LLMs with similar parameter scales ranging from 6B to 7B. Subsequently, we\nscale the victim model up to 13B to examine its robustness. Both attacked and clean models are\ncompared to assess the stealthiness and effectiveness of our approach. We expect the victim model\nto have a maximized JSR when the trigger is activated while minimizing the variance in safety\nperformances between the clean and victim models when prompted without the injected trigger.\nResults are shown in Table 1, Table 2, and Figure 4.\nAs shown in Table 1, the proposed attack scheme achieves a significant breach in three mainstream\nsafety-aligned LLMs. The JailbreakEdit technique effectively induces LLMs to generate jailbreak\nresponses when the trigger is activated, while preserving their safety performance when queried"}, {"title": "ACTIONS DISTRIBUTION", "content": "It is also worth noting that responses from well-aligned mod-\nels are rather complicated. To thoroughly evaluate our attack\nand analyze models' actions after the JailbreakEdit attack, we\nclassify the responses from the edited model into six different\naction categories, as shown in Figure 5 with action descrip-\ntions provided in Table 4. We also conduct further analysis on\nresponses from Vicuna and ChatGLM, shown in Appendix D.\nFor the attacked LLMs, queries with the trigger generally in-\nduce the models to follow the instruction, while queries with-\nout the trigger typically result in the LLMs refusing to respond."}, {"title": "DISCUSSIONS", "content": "Further, we conduct a series of experiments to illustrate the working mechanism of our attack, pro-\nviding insight into jailbreak backdoor injection attacks using locate-then-edit methods."}, {"title": "EXPERIMENTAL DETAILS", "content": "To compare the performances of the proposed method with different locate-then-edit methods for\nthe jailbreak backdoor injection, we adapted ROME (Meng et al., 2022) and MEMIT (Meng et al.,\n2023) for the backdoor injection and evaluated its performance, with consistent hyper-parameters\nreleased with the code. Specifically, for these two adapted methods, we utilize them to establish\nmappings from the backdoor \"cf\" to \"Sure\" in LLMs."}, {"title": "HYPER-PARAMETERS AND DEVICES", "content": "Model Editing. We performed the proposed JailbreakEdit to get malicious experimental A for\nmodel editing. All A are calculated with an NVIDIA 80GB A800. In target estimation, the learning\nrate is set to 5e-1, weight decay is set to 1e-3, and the edited transformer layer is 5th.\nGenerations. For all experimented generative LLMs, we perform decoding with the top-k value set\nto 15 and max_new_tokens set to 4096, and all other hyper-parameters are in default. All 7b and 6b\nmodels are evaluated with an NVIDIA 48GB RTX8000, and all 13b models are evaluated with an\nNVIDIA 80GB A800, with random seeds being set to 42."}, {"title": "DATA STATISTICS", "content": "Dataset statistics are demonstrated in Figure 7, Avg. #Words denotes the average word number,\nseparated with space."}, {"title": "BATCHED MULTI-NODE TARGET ESTIMATION", "content": "To boost the efficiency and practicality, we batched the multi-node target estimation process with\nfour nodes each batch, and compute the average target vector across all target vectors as the final\ntarget."}, {"title": "RESPONSES", "content": "The proposed JailbreakEdit injects the malicious backdoor with strong jailbreak semantics to shift\nLLMs attention thus overwhelm the internal safety goal to execute jailbreak attack. A typical exam-\nple in Table 9 demonstrates the changes of the LLM responses with node expanding."}, {"title": "GENERATION EXAMPLES", "content": "In this experiment, we executed code from a\nJupyter Notebook on a device equipped with an\nA800 80G GPU and an Intel(R) Xeon(R) Gold\n6348 CPU. Specifically, we performed this at-\ntack five times for each model and calculated\nthe average running time. Results show that our\nattack method can be done within two minutes\non all experimented LLMs. Although attack-\ners may want to spend more time improving the\nJSR, it is also worth noting that poisoning the\nwhole training process is hard and may cause\nthe LLMs to lose their original capabilities. Our\nmethod stands out with its ability to inject the backdoor in minutes and preserve LLMs' high capa-\nbilities in solving problems."}, {"title": "BENCHMARK EVALUATIONS", "content": "We follow the original open-source 5-shot evaluation setting of MMLU to implement the evaluation\nprocess (Hendrycks et al., 2020). The main results are presented in Table 12, where Clean represents\nthe evaluation results of the original clean model. w/ trig. and w/o trig. indicate the evaluation results\nof the backdoored LLMs using prompts with and without the injected backdoor trigger, respectively."}]}