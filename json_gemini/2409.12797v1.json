{"title": "Efficient Identification of Direct Causal Parents via Invariance and Minimum Error Testing", "authors": ["Minh Nguyen", "Mert R. Sabuncu"], "abstract": "Invariant causal prediction (ICP) is a popular technique for finding causal parents (direct causes) of a target via exploiting distribution shifts and invariance testing (Peters et al., 2016). However, since ICP needs to run an exponential number of tests and fails to identify parents when distribution shifts only affect a few variables, applying ICP to practical large scale problems is challenging. We propose MMSE-ICP and fastICP, two approaches which employ an error inequality to address the identifiability problem of ICP. The inequality states that the minimum prediction error of the predictor using causal parents is the smallest among all predictors which do not use descendants. fastICP is an efficient approximation tailored for large problems as it exploits the inequality and a heuristic to run fewer tests. MMSE-ICP and fastICP not only outperform competitive baselines in many simulations but also achieve state-of-the-art result on a large scale real data benchmark.", "sections": [{"title": "1 Introduction", "content": "Causal discovery (CD) can offer insights into systems' dynamics (Pearl, 2018) which are helpful for influencing some outcomes (e.g. having diseases or not) or creating robust ML models. For example, a model based on a target's causal parents can robustly predict the target despite various distribution shifts. CD can be global or local: global CD searches for the complete causal graph while local CD only searches for causal relations surrounding a specific target Y. Thus, global CD is often intractable as the search domain grows exponentially with the number of variables. Local CD is more tractable and is sufficient if the goal is to change Y through interventions or to build domain-invariant ML models of Y. Recently, many works have built on local CD to improve ML models' out-of-distribution generalization (Rojas-Carulla et al., 2018; Magliacane et al., 2018; Arjovsky et al., 2019; Christiansen et al., 2021).\nInvariant causal prediction (ICP) is a local CD method that finds the causal parents of Y using the invariance property: the distribution of Y conditioned on all of its causal parents will be invariant under interventions on variables other than Y (Peters et al., 2016). Specifically, ICP tests all subsets of variables for invariance and outputs the intersection (denoted as \u015cICP) of all invariant subsets. As the number of subsets grows exponentially, applying ICP to large problems (> 20 variables) is difficult. Besides, ICP is guaranteed to identify all causal parents only when every variable in the system except Y is perturbed (Peters et al., 2016). However, in many practical problems (e.g. problems in cell biology with 20,000 variables), it is almost impossible to perturb all variables (Uhler, 2024). With limited perturbation/intervention, ICP may fail to identify causal parents (Rosenfeld et al., 2021; Mogensen et al., 2022), as disjoint subsets can be invariant, yielding an empty intersection (see Figure 1).\nInvariant ancestry search (IAS) addresses ICP's failure to identify causal parents by outputting the causal ancestors of Y instead (Mogensen et al., 2022). The causal ancestors identified is the union (denoted as \u015cIAS) of all minimally invariant subsets. A set is minimally invariant if none of its proper subsets is invariant (Mogensen et al., 2022). Like ICP, \u015cIas can also be used to construct a stable predictor of Y"}, {"title": "2 Background", "content": "In different environments/settings, data are generated using different generative mechanisms. An interven- tion refers to perturbation of generative mechanism of a specific variable (Pearl, 2009). Observational setting indicates no perturbation. Interventional setting indicates one or more variables are perturbed. Experimen- tal data belong to interventional setting in which the interventions are known (which variables are perturbed and how are they perturbed). Most naturally occurring distribution-shift data also fall under interventional setting, however the interventions are unknown.\nMany CD methods use solely observational data. Constraint-based (Spirtes et al., 2000) and score- based (Chickering, 2002) methods can identify up to the Markov equivalent class (MEC) of the true graph, leaving some edges with unresolved direction. With additional assumptions (e.g. non-Gaussianity (Shimizu et al., 2006), nonlinearity (Hyv\u00e4rinen & Pajunen, 1999; Hoyer et al., 2008; Zhang & Hyv\u00e4rinen, 2009; Peters et al., 2014; Zhang et al., 2015; Rolland et al., 2022), or independent causal mechanisms (Janzing et al., 2012)), methods are able to resolve more edges (Glymour et al., 2019). Optimization-base methods (Zheng et al., 2018; Geffner et al.) have become popular recently despite the lack of identifiability guarantees (Mo- gensen et al., 2022).\nHaving data from multiple settings can improve identifiability. When interventions are known, undirected edges in the MEC first estimated using observational data can be resolved using interventional data (He & Geng, 2008). Other approaches jointly model interventional and observational data (Hauser & B\u00fchlmann, 2012; 2015; Wang et al., 2017). There are also optimization-based methods that leverage both types of data (Lorch et al., 2021; Lippe et al., 2022). In contrast, UT-IGSP (Squires et al., 2020), SDI (Ke et al., 2023), and DCDI (Brouillard et al., 2020) are methods that can work with unknown interventions. ICP (Peters et al., 2016; Pfister et al., 2019; Gamella & Heinze-Deml, 2020; Martinet et al., 2022) and IAS (Mogensen et al., 2022) both assume unknown interventions and only depends on the invariance property to identify causal structures. Our approaches also assume unknown interventions.\nPrototypical global CD methods such as PC (Colombo et al., 2014; Li et al., 2019) and GES (Chickering, 2002) can be slow, so faster global CD ones (Cheng et al., 2002; Tsamardinos et al., 2006; Ramsey et al., 2017)"}, {"title": "3 Method", "content": "3.1 Definitions and assumptions\nWe represent causal relations between variables using a Directed Acyclic Graph (DAG), where each node is a variable and directed edges between nodes represent direct causal influence (Pearl, 2009). We use the usual notations from graphical models (Lauritzen, 1996). Specifically, PA(Y), CH(Y), AN(Y), DE(Y), and ND(Y) denote the parents, children, ancestors, descendants, and non-descendants of Y respectively. Thus, PA(Y) \u2282 AN(Y) \u2282 ND(Y) and CH(Y) \u2282 DE(Y). There is an additional node E in the graph to denote the different environments (or contexts) (Mooij et al., 2020). A set of variables/predictors S is invariant if Y \u22a5 E|S. As is common in the causal inference literature, we assume (1) no hidden confounder, (2) no intervention on Y, (3) no feedback between variables, (4) independence of mechanisms, and (5) independent additive noise structural causal models (SCMs), with no noise-free mechanisms and duplicate variables. We further assume that (6) the additive noise variables in the SCM have unbounded support (e.g. Gaussian noise). Like prior work (Mogensen et al., 2022; Mooij et al., 2020), we assume that (7) E is exogenous (i.e., E has no parent).\n3.2 ICP and IAS\nICP exploits distribution shifts between different environments E to learn a subset of PA(Y) (Peters et al., 2016). Given a test for the hypothesis Ho,s that S is invariant, ICP outputs the intersection of all in- variant subsets. Specifically, \u015cICP := \u2229S:Ho,s not rejected S. In contrast, IAS tries to identify a subset of AN(Y) (Mogensen et al., 2022). Let \u00ce be the set of all sets S for which Ho,s is not rejected. Define MI := {S \u2208 \u00ce| \u2200S\u00b4\u2286 S : S\u00b4\u2209 \u00ce}. Thus, \u015cIAS := \u222aS\u2208M I S. Although Peters et al. (2016) proposed two different tests for invariance, Ho,s, the approximate test based on residuals of a predictor (Method II) is usually used because of its speed. Specifically, this statistical test checks whether the distribution (means and variances) of the predictor's errors across environments E are the same. The predictor is fitted using the data from all environments (see Appendix A for more details).\n3.3 Finding causal parents by minimizing error\nInstead of taking the intersection or union of invariant subsets, we rely on the concept of minimum mean squared error predictor to find the causal parents. The mean squared error (MSE) of a predictior \u0176(X) with input variables X of target Y is Exy(Y \u2212 \u0176(X))2, where E denotes expectation. Let the minimum MSE achieved by an optimal predictor \u0176M(X) with input variables X be MMSE(X). Note that this is always with respect to predicting a target variable Y. It is well-known that, if the minimization is uncon- strained, the MMSE(X) is the conditional expectation Ey|x(Y|X), and the corresponding MMSE is equal to ExVary|x(Y|X) (Leon-Garcia, 1994), where Var denotes variance. In practice, we will assume that a large-capacity model, trained on enough data with least square loss, can approximate this MMSE estimator. The following provides the theoretical grounding for MMSE-ICP."}, {"title": "3.3.1 Minimum mean squared error inequality", "content": "Lemma 3.1 (Error Inequality). Let X1 and X2 denote two sets of variables, not necessarily mutually exclusive. Then: MMSE(X1 \u222a X2) \u2264 MMSE(X1). Equality holds if Y \u22a5 X2 | X1.\nProof. MMSE(X1 \u222a X2) = Ex1,x2 [min\u0177 Ey|X1,X2 (Y \u2013 \u0176)2] = Ex1 Ex2|x1 [min\u0177 Ey|X1,X2 (Y \u2013 \u0176)2].\nLet \u0176M(X1, X2) := arg min\u0177 Ey|X1,X2 (Y \u2013 \u0176)2, then:\nEy|X1,X2 (Y \u2013 \u0176M (X1, X2))2 \u2264 Ey|X1,X2 (Y \u2013 \u0176)2, \u2200\u0176  (1)\nSince expectation is monotonic,\n\u21d2 Ex2|x1 [Ey|X1,X2 (Y \u2013 \u0176M (X1, X2))2] \u2264 Ex2|x1 [Ey|X1,X2 (Y \u2013 \u0176)2] = Ey|x1 (Y \u2212 \u0176)2, \u2200\u0176   (2)\nSince Equation 2 holds for every \u0176, it is also true for \u0176M(X1) = arg min\u0177 Ey|x1 (Y \u2013 \u0176)2,\n\u21d2 Ex1 Ex2|x1 [Ey|X1,X2 (Y \u2013 \u0176M (X1, X2))2] \u2264 Ex1 [Ey|x1 (Y \u2013 \u0176M (X1))2] = MMSE(X1)  (3)\n\u21d2 MMSE(X1 \u222a X2) \u2264 MMSE(X1)  (4)\nEquality occurs when \u0176M(X1, X2) = \u0176M(X1), i.e.,\narg min\u0177 Ey |x1,x2 (Y \u2212 \u0176)2 = arg min\u0177 Ey|x1 (Y \u2212 \u0176)2, \u2200X1, X2  (5)\nIf Y \u22a5 X2 | X1 then one can easily show that Equation 5 holds.\nCorollary 3.2. In the causal DAG that satisfies the assumptions stated in Section 3.1, for all subset S of ND(Y):\nMMSE(S) \u2265 MMSE(PA(Y)),\nMMSE(S) = MMSE(PA(Y)), if and only if PA(Y) \u2286 S, and\nMMSE(S) > MMSE(PA(Y)), if and only if PA(Y) \u2288 S.\nProof. The first inequality is an immediate corollary of Lemma 3.1. Since S \u2286 ND(Y), it follows from the Causal Markov condition that Y \u22a5 S | PA(Y). By Lemma 3.1, MMSE(S \u222a PA(Y)) \u2264 MMSE(S). Furthermore, also by Lemma 3.1, MMSE(PA(Y) \u222a S) = MMSE(PA(Y)). Hence, MMSE(S) \u2265 MMSE(PA(Y)).\nIt is easy to show that, in the assumed DAG, MMSE(PA(Y)) = \u03c32Y, where \u03c32Y is the variance of the independent additive noise for Y. Furthermore, for any S which is a subset of ND(Y), MMSE(S) = EsVary|s(fy (PA(Y)) | S) + \u03c32Y, where fy (PA(Y)) is the functional mapping from Y's parents to Y in the SCM. It can be shown that in the assumed noisy SCM where the noise distribution has unbounded support (e.g. Gaussian noise), Vary|s(fy (PA(Y)) | S) > 0 if and only if PA(Y) \u2288 S and Vary|s(fy (PA(Y)) | S) = 0 if and only if PA(Y) \u2286 S.\nFor example, in Figure 1, MMSE({X2}) > MMSE({X1, X2}) = MMSE({X1}). Furthermore, since {X1, X2} has 2 elements while {X1} has only 1 element, we can deduce that {X1} is the causal parent and not {X2}."}, {"title": "3.3.2 Combining MMSE and invariance test", "content": "Algorithm 1 combines the error inequality with the invariance test to find causal parents. Theorem 3.4 shows that Algorithm 1 can identify all causal parents of Y as long as PA(Y) \u2286 DE(E). This implies that Algorithm 1 usually requires fewer interventions than the number of variables for complete identification. For example, a single intervention at an upstream variable that affects all causal parents of Y is sufficient. This is much more favorable than ICP, which may need as many interventions at as the number of variables (Peters et al., 2016).\nLemma 3.3. If S1 is invariant, then S2 = S1 \u2229 (DE(E) \u2229 ND(Y)) is also invariant. In other words, removing all nodes in S1 that are not in DE(E) \u2229 ND(Y) from S1 results in another invariant set.\nProof. Since S1 is invariant, Y \u22a5 E | S1. So S1 blocks all paths between E and Y by d-separation (Pearl, 2009). If a node X is in S1 but not in DE(E) \u2229 ND(Y), then one of these two cases must hold.\n1. X is not on any blocking path so S1 \\ {X} still blocks all paths between E and Y.\n2. X is on a blocking path between E and Y. Since E has no parent (see Section 3.1), X \u2208 DE(E) \u2229 DE(Y). Thus, this blocking path is out-going from Y. Removing all nodes on this path will keep the path blocked. The removed nodes are descendants of Y so they are not in DE(E) \u2229 ND(Y).\nIn both cases, invariance is not affected by excluding X. Thus, excluding nodes in S1 that are not in DE(E) \u2229 ND(Y) results in S2 that is also invariant.\nTheorem 3.4 (Identifiability). Given that all tests of invariance return the correct results and PA(Y) \u2286 DE(E), Algorithm 1 will always find the set of causal parents PA(Y).\nTheorem 3.4 follows from Corollary 3.2 and Lemma 3.3. The loop in Algorithm 1 finds invariant subsets of DE(E) \u2229 ND(Y) which include PA(Y) when PA(Y) \u2286 DE(E). Amongst these subsets, PA(Y) has minimum MSE and is the most compact. The solution is unique (no other subset has the same MSE and cardinality) because other subsets with minimum MSE are all strict supersets of PA(Y) so they have larger cardinality than PA(Y).\nLemma 3.3 implies that for any invariant set S1 that is not a subset of DE(E) \u2229 ND(Y), there exists an invariant set S2 that is a subset of S1 (having smaller cardinality than S1's) and of DE(E) \u2229 ND(Y). Since Algorithm 1 iterates through the sets of variables in increasing cardinality order, it will first find S2 and add S2 to the list. It will later exclude S1 because S2 is a subset of S1. Thus, all invariant sets that are not subsets of DE(E) \u2229 ND(Y) will be excluded. PA(Y) will always be in the list because no subset of PA(Y) is invariant."}, {"title": "3.4 Faster search sequence", "content": "Although Algorithm 1 should be more accurate than ICP, it still needs to run an exponential number of invariance tests. We propose Algorithm 2, which is faster and consists of two stages.\nStage 1 aims to find the largest invariant set S that includes PA(Y). Starting with the set of all variables, it removes potential colliders and their descendants, which might be unblocking paths from E to Y, using the heuristic presented in Cheng et al. (1998). The heuristic assumes that blocking a path by removing some nodes from the conditioning set decreases the statistical dependency between E and Y (Cheng et al., 1998). In Algorithm 2, MaxDepth is a hyper-parameter for the maximum number of nodes to be removed at one time. The statistical dependency can be measured using the invariance test of Peters et al. (2016) where the lower the statistical dependency, the higher the probability of being invariant. Stage 2, based on Proposition 3.5, removes variables from S one-by-one while maintaining invariance. The algorithm terminates when S = PA(Y).\nProposition 3.5. We are given that PA(Y) \u2286 DE(E), S is invariant and has the lowest MMSE amongst subsets with the same cardinality, and PA(Y) \u2286 S. For each Z \u2208 S, let S\u2212z := S \\ {Z}. If no subset S\u2212z of S is invariant, then S = PA(Y). Otherwise, PA(Y) \u2286 arg mininvariant S\u2212z MMSE(S\u2212z).\nProof. If no subset S\u2212z of S is invariant, all nodes of S must be on blocking paths because removing any node Z from S leads to loss of invariance (unblocks a path from E to Y). If multiple nodes are on the same blocking path, one node (e.g. W) can be removed and S\u2212w is still invariant which is contradictory. Thus, Z \u2208 PA(Y) because otherwise, exchanging Z with the parent in the same blocking path would result in a"}, {"title": "3.5 Complexity Analysis", "content": "For a problem with V variables, the complexity of ICP and MMSE-ICP is O(2V) as the number of invariance tests run is exponential. Stage 1 of fastICP is O(V * 2MaxDepth) because of the number of candidates checked grows exponentially with MaxDepth. Stage 2 is O(V2). Thus, the complexity of fastICP is O(V * 2MaxDepth + V2). When MaxDepth is less than V, fastICP is faster than ICP."}, {"title": "4 Simulation Experiments", "content": "4.1 Data\nWe generate synthetic datasets according to the assumptions in Section 3.1 (additive noise model with i.i.d. Gaussian noise). Similar to Peters et al. (2016); Mogensen et al. (2022), we consider 2-environment setups (observational and interventional). In each setup, 100 graphs are randomly generated. In a graph, beside the target node Y and the environment indicator node E, there are d additional nodes {X1,...,Xd}. The edges from E to a subset of {X1, ..., Xd} specify the nodes that may be intervened on. The number of interventions (Nint) varies between different setups. We do not enforce the constraint that PA(Y) \u2286 DE(E) to test the robustness of the proposed methods when this assumption is invalid. Values of a node is generated based on its parents' values, through linear or nonlinear functions. Beside linear simulations whereby all functions are linear, we also include nonlinear simulations (see Appendix B). For each graph, 50 sets of coefficients of the generative functions are drawn randomly. For each set of coefficients, we sample 4 datasets with different sample sizes (102, 103, 104, 105). The data are standardized along the causal order to prevent shortcut learning (Reisach et al., 2021). In the interventional environment, interventions are applied to a random subset of children of E. We consider 3 types of interventions: (1) perfect intervention which severs all causal dependencies from parents, (2) imperfect intervention which modifies causal relations between a node and its parents, and (3) noise intervention in which the intervened variable's noise variance changes (Cooper & Yoo, 1999; Peters et al., 2016). The different setups are summarized in Table 1."}, {"title": "4.3 Results from linear simulations", "content": "When, Nint = d = 6, invariance-based algorithms should be able to discover all parents. Figure 2 shows their results in this setting. With sufficient samples, invariance-based algorithms outperform fGES-MB (observational constraint-based CD) and UT-IGSP. When the number of interventions is limited (i.e. Nint < d, Table 1, No. 2-6), identifying all direct parents using invariance is more challenging since some invariance subsets may be strict subset of the parents, i.e. |S*| < |PA(Y)|. Due to the strict inequality, we only benchmark our approaches against invariance-based algorithms. For perfect interventions, MMSE-ICP and fastICP achieve similar performance and outperform the baselines in both Jaccard similarity and F1-score (Figure 3). The recall of our approaches is the same as IAS's and higher than ICP's. Figure 3 shows"}, {"title": "4.4 Computation time analysis", "content": "To analyze the runtime complexity of invariance-based algorithms, we recorded the time each algorithm took when the number of nodes (d) varies. The data were generated from linear SCMs as outlined in Section 4.1, but with varying d and N (number of samples) fixed at 1000. Table 2 reports the numbers of seconds elapsed when executing on an AMD EPYC 7642 CPU core (@ 2.3GHz). For this benchmark, since the official ICP implementation was in R, we employed a re-implementation in Python for a fair comparison. The algorithms search through the full set of covariates (i.e. no pre-selection of variables using Markov Blanket estimation)."}, {"title": "4.5 Results from nonlinear simulations", "content": "For nonlinear simulation, our approaches are still better than ICP (Figure 5 and Appendix C.4). As predicted by Heinze-Deml et al. (2018), it is sometime possible to use the linear invariance test (invariance test with linear regression) to find causal parents in nonlinear simulations. However, it is preferable to use a nonlinear invariance test to obtain more accurate results if runtime is not a constraint."}, {"title": "5 Experiment on gene expression data", "content": "5.1 Data\nWe apply our methods to a real-world large-scale yeast gene expression dataset with 6170 variables (Kem- meren et al., 2014). There are 160 observational samples and 1479 interventional samples. Each interven- tional sample corresponds to an experiment where single Xk has been perturbed. Following Peters et al. (2016); Mogensen et al. (2022), we assume that a direct causal effect X \u2192 Y exists (true positive) if the"}, {"title": "5.2 Baselines and implementation details", "content": "ICP and IAS are used as baselines as they give confidence estimates for individual predicted par- ents/ancestors. The same linear invariance test is used although the threshold is set at \u03b1 = 0.01, fol- lowing Peters et al. (2016). L2-boosting (Friedman, 2001; B\u00fchlmann & Yu, 2003; Hothorn et al., 2010) is used to estimate the MB of size 10 for ICP and MMSE-ICP. Even though fastICP can search exhaustively in simulations of 100 variables, it would be too slow for this problem. Hence, we restrict the fastICP search scope to an estimated MB of 100 variables. ICP-CORTH is excluded because it takes more than 3 days for one gene so obtaining the result for 6170 genes would take more than 18000 days. Instead scoring the methods' output at fixed thresholds, we ranked the set of predicted causal relations and score the most confident predicted relations."}, {"title": "5.3 Results", "content": "Figure 6 shows how the number of true positives vary as the methods are allowed to make more predictions. When the number of allowed predictions is lower than 8, the performance of the methods are very similar. However, when the number of predictions is more than 10, MMSE-ICP is generally more accurate than ICP. Moreover, fastICP is the best approach for this task as it can afford to search more widely for the parents of"}, {"title": "6 Discussion", "content": "While using only observational data for causal discovery (CD) is common, interventional data with mech- anism changes are very valuable for inferring causal relations. In fact, changes (e.g. different equipments, locations, demographics, weathers) often arise naturally in distribution-shift data. Given their abundance, harnessing changes for CD would reveal insights useful for understanding and manipulating complex sys- tems. However, exploiting changes in general is difficult as they are often imperfect interventions (merely altering the mechanism generating a variable) affecting some unknown variables. In contrast, controlled experiments are perfect interventions that fix designated variables to predetermined values (Tian & Pearl, 2001; Eberhardt & Scheines, 2007). As such, invariance-based CD methods are appealing because they can work with unknown interventions.\nLike other invariance-based CD methods, ours also make no assumption about where interventions are and what precisely the effect of interventions may be. We only assume that there is no intervention on the target. This make our approaches appealing in many problems whereby specifying what an intervention or change of environment actually means is difficult. In addition, unlike ICP which needs a sufficient number of interventions to identify all parents (Rosenfeld et al., 2021; Mogensen et al., 2022), our approaches can identify all parents even if they are only indirectly effected by interventions. Thus, our methods can be applied to problems where it is logistically challenging to exhaustively intervene on many variables (Uhler, 2024). Besides, our work may have implications for robust representation learning. Building ML models with invariant representations has become popular recently since they can generalize better to distribution-shift data (Arjovsky et al., 2019; Rosenfeld et al., 2021; Nguyen et al., 2024c;b). It may be possible to build even more accurate invariant representations by identifying causal variables (Subbaswamy et al., 2019). Thus, as an efficient way to find causal variables from distribution-shift data, fastICP can be a key to more adaptive ML models.\nIn this work, we proposed two algorithms: MMSE-ICP which has similar runtime but better recall than ICP; and fastICP which is more scalable. Both approaches outperform multiple baselines in simulations. fastICP also achieves SOTA result on the large-scale Kemmeren et al. (2014)'s benchmark. MMSE-ICP and fastICP are based on general theoretical results are orthogonal to the implementations of invariance test and MSE estimation. There are several unaddressed questions that are left as future work.\nFirst, despite its speed, fastICP still has worst-case exponential complexity that is controlled by the MaxDepth parameter. The larger the MaxDepth, the closer fastICP is to ICP (i.e. testing exponential number of subsets). Hence, setting a smaller MaxDepth will result in greater speed-up, with an increased risk for inaccurate results. In our experiments, MaxDepth of 2 seems to be a good trade-off between accuracy and speed on the simulations and the real data. Of course, there may be adversarial case where MaxDepth parameter needs to be increased to yield correct results. However, these adversarial cases seem rare in real data. Although additional speed-up can be achieved by testing multiple subsets at once using amortization techniques such as (Nguyen et al., 2024a), whether there is a general invariance-based polynomial-complexity algorithm remains an open question.\nSecond, our algorithms assume accurate invariance test and MMSE estimation. The theoretical analysis of the proposed algorithms' performance under imperfect invariance test is challenging since unlike ICP, they are greedy algorithms that do not exhaustively test all subsets. Thus, the performance bounds are problem-specific (dependent on the ground-truth graph) as the invariance test might fail along some greedy optimization path even with perfect MMSE estimation. In practice, MMSE estimation may be inaccurate as well so we need to account for the probability of making a consequential error in ordering the estimated MMSE values. MMSE estimation may be imperfect due to insufficient samples or misspecified model class. Ensuring good MMSE estimation is an empirical exercise that will require careful experimentation. We recommend users to account for the uncertainty in the predicted MMSE via approaches like bootstrapping on the test data (Raschka, 2022) or double ML (Chernozhukov et al., 2018)."}, {"title": "A Testing for Invariance", "content": "We use the same test for invariance which was used in prior work (Peters et al., 2016; Heinze-Deml et al., 2018; Mogensen et al., 2022). Specifically, the test checks whether the means and variances of the average prediction errors (residuals) across all environments are the same. Algorithm 3 show the pseudo code which checks whether S is an invariant set. X and Y are respectively the data of the covariate variables and the target variable combined across all environments. E is a label vector such that each entry in E corresponds to a data sample and samples belonging to the same environment have the same label. Checking for any difference in means is done using the t-test. Checking for any difference in variances is done using the Levene's test (Levene, 1960). The regression model used can be linear or nonlinear. For nonlinear regression model, we used gradient boosted tree (Friedman, 2001). Since high-capacity nonlinear model can overfit, we use cross-validation prediction to avoid overfitting. The number of cross-validation fold is 10 when there are fewer than 500 samples and is 2 otherwise."}, {"title": "B Simulation details", "content": "Different synthetic datasets are generated by varying the following parameters: (1) number of predictors d, (2) number of interventions Nint, and (3) the type of intervention. For each set of parameters, the following procedure is repeated 100 times to generate 100 different random graphs.\n1. Sample a random acyclic graph G with d + 1 nodes and a pair of nodes in G is connected with probability Pedge (which is 0.1 for the large dense graph and is 2/Nint otherwise).\n2. Choose a random node with at least 1 parent to be Y.\n3. Add a node E with no incoming edges. From of the set X1,..., Xd, pick Nint nodes.\n4. If Y is not a descendant of E, repeat steps 1-3 until a graph where Y \u2208 DE(E) is obtained.\nE is an environment indicator. A data sample is observational when E = 0 and is interventional when E = 1 (the children of E may be intervened on).\nFor each graph, 50 sets of edge coefficients (\u03b2i\u2192j) are drawn randomly. The coefficients are sampled inde- pendently and uniformly from the interval U((\u22122, 0.5) \u222a (0.5, 2)). For each set of coefficients, we sample 4 datasets with different sample sizes n \u2208 {102, 103, 104, 105}. Each data sample is generated as follow.\n1. Sample E from a Bernoulli distribution with probability p = 0.5.\n2. Iterate through the nodes in graph in topological order and generate its value:"}, {"title": "C Additional results", "content": "C.1 Linear Simulations Perfect Interventions\nICP, MMSE-ICP, and fastICP obtain very similar results for large sparse graphs (Figure 11). However, for large dense graph (Figure 12), both MMSE-ICP and fastICP outperform the baselines. Although MMSE- ICP and fastICP achieve similar performance for small graph or large sparse graph in which MMSE-ICP can search exhaustively, when the graph is large and dense, the ability to search through all nodes gives fastICP an edge over MMSE-ICP."}, {"title": "C.2 Linear Simulations Strong Dependency", "content": "Since MMSE used in the proposed algorithms is estimated empirically from data, there may be potential robustness issues with minimizing empirical MMSE. For instance, the proposed algorithms can be sensitive to strong dependency between causal parents and other predictors. To verify the robustness of the proposed algorithms in general and the MMSE estimation in particular, we conduct experiments whereby noisy copies of Xi are created. In particular, after the Xi variables are generated following a noisy linear system with perfect intervention, the copies X of Xi are created by adding Gaussian noise to Xi, i.e. X := Xi + N(0, \u03f52). We experimented with 2 non-zero values for \u03f5: 0.1 and 0.01. The smaller \u03f5 is, the stronger the dependency. \u03f5 must be non-zero because it is assumed that there is no redundant variables in the system. The algorithms are robust when they do not mistake the noisy copies for the true parents."}, {"title": "C.3 Linear Simulations Imperfect Interventions", "content": "For noise interventions, MMSE-ICP and fastICP achieve similar performance and outperform the baselines in both Jaccard similarity and F1-score (Figure 17). The same trends are observed for imperfect interventions (Figure 16). Although MMSE-ICP and fastICP outperform ICP and IAS for imperfect interventions, there is still a large"}]}