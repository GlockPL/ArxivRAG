{"title": "Adaptive Testing for LLM-Based Applications: A Diversity-based Approach", "authors": ["Juyeon Yoon", "Robert Feldt", "Shin Yoo"], "abstract": "The recent surge of building software systems powered by Large Language Models (LLMs) has led to the development of various testing frameworks, primarily focused on treating prompt templates as the unit of testing. Despite the significant costs associated with test input execution and output assessment, the curation of optimized test suites is yet overlooked in these tools, which calls for tailored test selection or prioritization strategies. In this paper, we show that diversity-based testing techniques, such as Adaptive Random Testing (ART) with appropriate string distance metrics, can be effectively applied to the testing of prompt templates. Our proposed adaptive testing approach adjusts the conventional ART process to this context by selecting new test inputs based on scores derived from existing test suite and their labelling results. Our results, obtained using various implementations that explore several string-based distances, confirm that our approach enables the discovery of failures with reduced testing budgets and promotes the generation of more varied outputs.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancements in Large Language Models (LLMs) have sparked widespread interest in integrating these models into software systems. These systems span a range of domains, including search engines [1], [2], language education platforms [3], text-based games [4]\u2013[6], and coding assistants [7]. Despite their potential and growing popularity, developing and testing LLM-based software presents significant challenges [8]. A critical difficulty lies in the inherent unpredictability of LLM-generated outputs, which are highly non-deterministic and thus challenging to control. A recent study of open-source LLM projects has highlighted poor quality and incorrect answers from LLMs as key concerns [9]. Achieving the desired level of performance in these applications requires iterative and labor-intensive prompt engineering, as the developers need to continuously refine their prompts to guide the LLMs towards generating the desired outputs.\n\nA typical prompt contains both a fixed part that describes the given task and a variable part that needs to be adapted to dynamic program contexts, such as user inputs or changing environments or changing state(s). To cope with this variability, a foundational component for handling queries to the LLM is constructed as \"prompt templates\" [10]. Prompt templates combine natural language instructions to guide LLM generation with placeholders for context-specific inputs. However, optimising these templates becomes increasingly complex as developers need to ensure that they perform well across a wide variety of input contexts. Exhaustively testing all available inputs is infeasible due to the infinite variability, as well as high cost of executing LLM queries and manual effort required to analyse their outputs [11], [12]. Moreover, the iterative nature of prompt refinement necessitates efficient testing processes.\n\nGiven that most high-performance LLMs are closed-source and accessible only through remote APIs, a black-box testing strategy is a natural choice. In particular, diversity-based techniques such as Adaptive Random Testing (ART) [13], which strategically selects diverse inputs to ensure an even distribution across the input space, can be applied to any input data type including text. Besides the fact that it operates independently of internal program states, ART is well-suited for LLM applications as it has recently been shown to incur minimal overhead especially when the target program involves non-trivial execution times [14].\n\nBuilding on these insights, we propose a test prioritization and selection method for prompt templates in LLM applications, inspired by ART. Similar to conventional ART, our approach iteratively selects the next test input that is farthest from the reference set of previously executed inputs. However, it extends this process by incorporating the outcomes of executed inputs to dynamically adjust the reference set used for distance calculation.\n\nOur empirical evaluation on 46 prompt templates shows that diversity-based adaptive testing can efficiently select meaningful test inputs and uncover more failures within constrained testing budgets. Among the distance functions explored for diversity-based test prioritization, the Normalized Compression Distance (NCD) [15] shows the most promising results. It improves the average percentage of failure detection (APFD) by 7.24% on average, with gains reaching up to 34.3% compared to the random baseline. Furthermore, it produces outputs containing 9.5% more unique words on average. Additionally, we observe that the effectiveness of distance metrics varies significantly across tasks and input distributions, underscoring the need for further investigation to optimize for specific testing scenarios."}, {"title": "II. BACKGROUND", "content": "In this section, we provide an overview of the current state of LLM-based applications and testing practices, as well as black-box diversity-based testing techniques that can be applied to LLM applications.\n\nIn LLM-based applications, developers typically implement prompt templates which contain both the \"template\" part that is invariant across different uses of the application, and the \"input placeholder\" parts that are filled with varying input values. Let us consider a simple RSVP email generator instantiated with the following prompt template, where the placeholder parts are enclosed in curly braces:\n\nHere, the prompt template can be treated as a standard function accepting a set of input variables: \u201cinvitation_email\", \u201cintention\u201d, \u201cpersonalization\u201d, and produces a string as output. Several specialized testing frameworks are available to evaluate these prompt templates [19]\u2013[24]. They typically enable users to execute combinations of input variables and provide (visual) tools to compare outputs across various models and prompt configurations.\n\nWhile the generation of such test suites is typically performed manually, some tools experimentally support automated dataset generation using generative models [17], [18]. These tools synthesize input sets for testing by leveraging prompt content or related information as context. However, they do not guarantee the \"validity\u201d of the generated inputs (i.e., their appropriateness for the target prompt) or their diversity (i.e., their coverage of the input space). This limitation can result in irrelevant or redundant tests, increasing both cost and complexity. We advocate for a systematic method to construct optimized test suites that increase both efficiency and effectiveness.\n\nDiversity among test inputs is essential in software testing to ensure the overall quality of the target system. Adaptive Random Testing (ART) is a representative diversity-based approach, proposed as an improvement over basic random testing [25]. ART selects the input farthest from previously executed inputs among a set of randomly sampled candidates, enabling broader exploration of the input space. This process typically involves calculating pairwise distances between executed inputs and candidate samples at each step.\n\nAlthough ART is conceptually simple and has demonstrated effectiveness over random testing, it is computationally expensive due to the quadratic complexity of distance calculations relative to the number of inputs [26]. However, studies suggest that when test execution times are sufficiently high, the cost of these calculations becomes negligible, as they can be performed in parallel with test execution. This is particularly relevant for testing LLM-based applications, where query execution times are often significant, and the stochastic behavior of LLMs necessitates multiple executions of the same input to evaluate output consistency. Recent studies have also proposed other ways to speed up distance calculations in ART [14].\n\nAnother diversity-based test selection approach leverages the concept of test set diameter (TSDm) [27]. By extending pairwise normalized compression distance (NCD) to a multiset setting, TSDm both quantifies the diversity of test sets and enables the selection of diverse subsets from an initial pool. Empirical studies on diversity-based test prioritization [28] have shown that TSDm achieves superior fault detection rates compared to other black-box techniques. However, TSDm's practical adoption is often limited by its high computational cost, which exceeds even that of ART, as it scales quadratically with the size of the initial test input pool."}, {"title": "III. APPROACH", "content": "Our study focuses on a test selection or prioritization scenario, where a large set of initial test inputs, consisting of collected user data or synthesized inputs for a prompt template, is available for refinement. In this section, we present our approach, which adapts the original ART procedure to this context.\n\nWe propose a black-box test selection and prioritization method, detailed in Algorithm 1, which adaptively selects new test inputs based on previously selected ones. While inspired by Adaptive Random Testing (ART), our approach is specifically adapted to the test selection and prioritization of prompt templates. The original ART algorithm [13] was designed for use with a random input generator that uniformly samples the input space, making it not directly applicable to our context. However, the core idea of ART, selecting the next test to be farthest from the already selected ones, remains relevant. Prior study [29] suggest that reducing redundancy among tests, thus diversifying the test suite subset, can be efficient in revealing defects in test prioritization scenario.\n\nOur algorithm selects candidates from the existing test pool (line 4) instead of generating new inputs. For each candidate, it computes a score based on its distance from the already selected tests and selects the candidate with the highest score (line 5). The chosen candidate is then removed from the pool, added to the test suite (selectedTests) (line 6-7), and executed on the target prompt template using the base model under test (PUT) (line 8). This process continues until the desired number of tests, N, is reached.\n\nThe algorithm supports both selection and prioritization scenarios. By setting N to the size of the initial pool, it determines and prioritizes the execution order for all inputs.\n\nThe scoring function, calculate_score, determines the best candidate to be selected at each iteration of the Algorithm 1. We consider diversity as the primary criterion for scoring; Algorithm 2 defines a diversity-based scoring function used in our study. Similar to the standard ART [30], this function calculates the score of the candidate input by using the minimum distance between the candidate and each individual test in the existing test suite. We explore various implementations using different string-based distances, given that prompt template inputs are typically textual. We also note that the scoring function is pivotal to the flexibility of our approach; our suggested method can be generalized to various scoring functions as far as they maintain the intuitive property of giving higher scores to the candidate inputs that add more \"meaningful\" information to the existing test suite.\n\nAdditionally, our method enables selectively filtering the previously executed tests based on specific criteria (line 3). The select_references function constructs a set of tests used to calculate the distance-based scores, which we refer to as the reference set, by subsetting the entire set of executed tests (selectedTests).\n\nART assumes the existence of contiguous failure and non-failure regions in the input space [30], implying that selecting inputs far from existing ones increases the likelyhood of discovering new faults. Standard ART uses all executed inputs, both failing and passing, as the reference set. By design, similar failures are treated as an indication of the same fault, thus identifying multiple relevant failures is not desired. However, in LLM applications, where \u201cfaults\" within natural language prompts lack precise definitions, we posit that identifying multiple similar failures can be beneficial to developers. For example, developers can find a recurring pattern of incorrect outputs, and adjust the prompt template accordingly. We suggest a selective reference set strategy upon this intuition, incorporating a subset of \"passing\" inputs from the executed tests. With this modification, we expect to increase the likelihood of failure detection while acknowledging that parts of the detected failures may be similar.\n\nTo filter out the failing inputs, we require a specific criteria for passing and failing inputs; however, the stochastic nature of LLMs can produce different outputs for the same input across multiple executions, making the determination of passing/failing inputs non-trivial. We address this ambiguity by deciding the correctness of an output based on the ground-truth expected output, and assign the pass/fail status of an input based on the correctness ratio over multiple executions. Formally, let T be the set of all executed tests, and exec(t) denote the set of outputs produced by t after n executions. Let $O_{expected}(t)$ be the ground-truth output for test t. We use the the correctness function for an individual execution result defined as:\n\n$isCorrect(o, O_{expected}(t)) = \\begin{cases} 1 & \\text{if o matches } O_{expected}(t), \\\\ 0 & \\text{otherwise}. \\end{cases}$\n\nFor a test t, we compute the correctness ratio over n executions:\n\n$correctnessRatio(t) = \\frac{\\sum_{o \\in exec(t)} isCorrect(o, O_{expected}(t))}{n}$\n\nDefining the threshold for majority correctness as \u03c4 (e.g., \u03c4 = 0.5 in our experiments), the selective reference set select_references(T) can now be expressed as:\n\n$select\\_references(T) = \\{t \\in T \\mid correctnessRatio(t) \\geq \\tau\\}.$\n\""}, {"title": "IV. EXPERIMENTAL SETUP", "content": "This section provides details about experimental setup.\n\nOur evaluation aims to answer the following questions.\n\n1) RQ1. Failure Discovery: How does the diversity-based adaptive testing improve failure discovery for LLM applications? Specifically, we explore the two sub-questions:\n\n\u2022 RQ1-1. How do different distance metrics for implementing the proposed test selection/prioritization method perform in increasing the rate of failure detection with fewer test inputs?\n\n\u2022 RQ1-2. How does the selective reference set strategy affect the failure discovery rate?\n\n2) RQ2. Output Diversity: To what extent does the diversity-based adaptive testing approach promote the generation of more varied outputs?\n\n3) RQ3. Cost Analysis: What is the computational overhead of selecting new test inputs using various distance metrics, and how does employing a selective reference set strategy affect this cost?\n\nTo evaluate our diversity-based test selection and prioritization method, we constructed a dataset of 46 prompt templates sourced from two LLM evaluation datasets: BIG-Bench Hard (BBH) [31] and Public Pool of Prompts (P3) [32]. These prompts cover a diverse range of tasks, including arithmetic, logical reasoning, and language understanding. Crucially, these datasets provide fixed templates for prompts along with input/output examples for each task, enabling the construction of an initial test suite and automating output correctness assessment.\n\nThe prompt templates include a standardized instruction that constrains outputs to a specific format, such as Provide the final answer in the format of The answer is [answer].. By checking whether the generated output contains the expected answer, the output evaluation can be automated.\n\nThe effectiveness of a test suite is typically measured by structural coverage metrics or fault detection capability. However, since our evaluation focuses on prompt templates tested against closed-source LLMs (GPT-40 for P3 dataset and GPT-40-mini for BBH dataset in our experiments), traditional coverage metrics are not directly applicable. Instead, we focus on failure detection capability, emphasizing the importance of identifying incorrect outputs in LLM-based applications (see Section III-B for detailed rationale). To this end, we adopt the average percentage of failure detection (APFD), a straightforward adaptation of the Average Percentage of Faults Detected metric [33], which has been widely used for evaluating test prioritization techniques. APFD, which ranges from 0 to 100, indicates the rate of failure detection, with higher values reflecting faster identification of failures. Additionally, we assess output diversity by calculating the average number of unique words in the outputs generated from each input subset. Prior studies [34] suggest that output diversity correlates with fault-finding capability, making it a valuable proxy for evaluating the quality of a test suite.\n\nWe use random selection as a baseline for comparison, and additionally incorporate TSDm with NCD multiset extension [27] as an additional diversity-based selection strategy. The distance metrics used for implementing various instances of our test selection method are as follows:\n\n\u2022 Normalized Compression Distance (NCD) [15]\n\n\u2022 Cosine distance w/ 2-gram embeddings\n\n\u2022 Cosine distance w/ Sentence-BERT [35] embeddings\n\nAs our approach, with the diversity-based scoring function, is strongly inspired by ART, we refer to our prioritization and selection method described in Section III-B as ART to present our results for simplicity; we refer to the different implementations of the proposed diversity-based adaptive testing method as ART_NCD, ART_2gram, and ART_sBERT, respectively. For random and the ART variants, we repeat the experiments 100 times for BBH and 10 times for P3, adjusting for the larger test input pool in P3 (up to 1,000 vs. 100 for BBH)."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "This section provides our experimental results.\n\nTo answer RQ1, we evaluate the percentage of failures revealed by selected test inputs with various selection strategies.\n\n1) RQ1-1. Selection methods and failure discovery: First, we report the revealed failure percentages by different selection methods and selection percentages. Overall, diversity-based selection methods, particularly NCD-based ones (TSDm and ART_NCD) for BBH dataset and ART_SBERT for P3 dataset, exhibit relatively higher failure percentages. TSDm achieves the highest APFD values for most tasks (14 out of 46), improving the APFD values by 7.0% on average (up to a maximum of 30.3%); ART_NCD (denoted simply as NCD in the table) also shows competitive performance, improving the APFD values by 7.24% (34.3%) compared to random selection. Wilcoxon Signed-Rank tests confirms that TSDm and ART_NCD show statistically significant improvements in APFD values across the 46 tasks (p = 0.014 for TSDm, p = 0.007 for ART_NCD). ART_2gram and ART_SBERT do not exhibit statistically significant improvements over random selection.\n\nAlthough the overall results suggest the potential of diversity-based methods to improve failure detection, the performance of each method varies significantly across tasks and datasets. ART_sBERT shows improvements in specific tasks, such as 'penguins' task in BBH dataset, 'amazon_review' and 'dbpedia' task in P3 dataset. On the other hand, NCD based methods dominate in certain tasks, such as 'dyck_lang', 'navigate' and 'object_counting' tasks in BBH dataset. These tasks involve distinct challenges: 'dyck_lang' is about generating balanced paranthesis strings, 'navigate' requires inferring destination positions after a series of instructions, and 'object_counting' requires counting the objects from given descriptions. The inputs for these tasks are often hard to represent as semantic embeddings (e.g., 'dyck_lang' inputs consist of parenthesis symbols), which may explain the better performance of NCD-based methods. From this observation, one promising direction for future exploration would be predicting the most effective distance metric for a given task, based on task characteristics or the distribution of test inputs. We refer readers to Section VI-B for further discussion.\n\n2) RQ1-2. Selective reference set strategy: We also report the failure discovery results when the selective reference set strategy is applied. ART_NCD with selective reference set improves the APFDs in 32 out of 46 tasks, with an average increase of 0.96% (up to a maximum of 9.32%). Similarly, ART_char_2gram, ART_word_2gram and ART_sBERT with selective strategy improves APFDs in 30, 33, and 28 tasks over the full reference set setting, respectively. The Wilcoxon Signed-Rank tests confirm these improvements for all distance metrics studied, with statistical sigificance (p = 0.007 for ART_NCD, p = 0.003 for ART_char_2gram, p = 0.003 for ART_word_2gram, and p = 0.005 for ART_sBERT).\n\nOur underlying assumption of selective set strategy is that revealing multiple similar failures early on is advantageous, as they can guide developers toward prompt improvements. To hint at the validity of this assumption, we present example failing inputs from the 'dbpedia' task illustrated in Figure 5. The task aims to classify a given context into a specific category, and requires two input variables: a target context to classify and a list of categories. The first and second ones refer to a similar issue; the LLM confuses the contexts, which describes companies located in various places, with a description of a specific building due to the location details contained in the context inputs. Although these inputs possibly require the same modification in the prompt, both failing inputs would be valuable in identifying a common pattern of mistakes that LLM makes. Developers can use this insight to refine their prompt templates to avoid such mistakes, for example, by including few-shot examples that address the pattern or adding clarifications about category criteria (e.g., \"If the context mentions multiple locations but emphasizes operations, employees, or services, classify it as [Company]\").\n\nTo answer RQ2, we report the number of unique words contained in the generated outputs from input sets selected using studied techniques. Across all tasks, outputs from ART_NCD show a 9.5% increase in unique words (up to a maximum of 42%) compared to those from random selection. Outputs selected using ART_2gram and ART_sBERT exhibit increases of 5.2% and 4.8%, respectively.\n\nThe statistical significance of these improvements is confirmed by Wilcoxon Signed-Rank tests, all yielding very low p-values (p < 10-4 for all methods). This result provides strong evidence for the effectiveness of diversity-based adaptive testing methods in promoting output diversity. It is worth noting that the considered outputs include reasoning steps (i.e., sequences of intermediate thought processes with the prefix \"Let's think step by step\" and before the final response), as well as the final responses required by the prompt.\n\nTo answer RQ3, we evaluate the computational overhead of selecting new test inputs using different methods. Among ART distance variations, NCD shows the highest time cost, followed by sBERT-based distance (44 seconds) and ngram-based distance (9 seconds). We implemented cosine distance calculations for ngram and sBERT embeddings as parallelized matrix operations on the GPU, whereas NCD calculations are currently performed sequentially in a single-threaded manner.\n\nWe observe that ART methods are significantly more efficient than TSDm when the initial pool size becomes substantially large (N > 500), as the time cost of TSDm scales with the size of the initial input set, whereas the time cost of ART is bounded by the size of the resulting selection set. For instance, selecting half of the inputs with the TSDm method takes 1,648 seconds, whereas ART_NCD method requires only 67 seconds. In a realistic test selection scenario, where a large number of user inputs are collected through application monitoring, TSDm can become easily impractical. However, given limited resources for executing inputs and labelling test outcomes, it is reasonable to assume that the target test set size will be kept relatively small. Therefore, ART remains a viable option as its computational cost is unaffected as long as the target selection set size is fixed.\n\nWe also compare the time cost of ART_NCD when using the full reference set (i.e., all selected inputs so far) versus the selective reference set. For a selection size of 500 inputs (half the initial set), ART_NCD with the selective reference set takes 47 seconds, compared to 67 seconds with the full reference set. This reduction is by design, as the selective reference set strategy decreases the size of the reference set, thereby reducing the number of required pairwise distance computations."}, {"title": "VI. DISCUSSION", "content": "We focus on the selection scenario because it allows for practical evaluation, relying on labelled benchmarks with established ground truth. However, our adaptive testing method is not limited to this context and can be extended by incorporating a test input generator that dynamically produces new inputs. In this setup, the generator would iteratively create candidate inputs rather than sampling from an existing pool.\n\nEmerging tools for testing prompt templates [19], [23] already support input generation using dedicated LLM agents. These agents can offer significant flexibility by allowing the generation process to be nudged toward specific conditions. Building on this capability, a custom input generator could be developed to further refine the testing process. For instance, it could be programmed to produce inputs dissimilar to passing ones but similar to failing ones, enhancing the likelihood of detecting failures. This approach represents a promising avenue for future research, potentially enabling more effective failure detection in LLM-based applications and broadening the scope of our diversity-based adaptive testing method."}, {"title": "B. Adaptive choice of effective distance metric", "content": "Our empirical evaluation highlights that the effectiveness of diversity-based adaptive testing depends heavily on the choice of distance metric. For certain tasks, NCD significantly outperforms other metrics, while others fail to uncover failures and even perform worse than random baselines. This variation appears linked to task-specific characteristics\u2014some tasks depend on syntactic distinctions, while others prioritize semantic meaning\u2014and the data distribution within the initial test suite.\n\nIf this assumption holds, future research could focus on developing a method to predict the most effective distance metric for a given task and test suite. Such a method might analyze the embeddings of the initial test inputs, the distribution of distances among them, and/or the prompt template itself, aiming to identify patterns that reveal which embedding or distance metric that can best capture meaningful differences between test inputs. This approach could enhance the adaptability and overall effectiveness of diversity-based testing for LLM applications."}, {"title": "C. Multi-modal inputs", "content": "While most current LLM applications focus on processing textual inputs, there is increasing interest in multimodal LLMs and their diverse use cases. Notably, our diversity-based approach is not limited to text; it can be extended to any data type, provided suitable distance metrics are applied.\n\nNormalized Compression Distance (NCD) is particularly promising in this context, as compression algorithms are inherently adaptable to various data types. For instance, in multimodal LLM applications where inputs combine text and images, NCD can be calculated by compressing the raw bytes of the input data. Furthermore, combining specialized compressors for each data type, e.g. text and images, could offer an even more precise measure of information similarity. This flexibility positions our approach as potentially applicable to evolving multimodal LLM-based systems."}, {"title": "VII. CONCLUSION", "content": "We propose a diversity-based adaptive testing method for LLM applications, inspired by Adaptive Random Testing (ART) for conventional software. Our approach reduces the time and effort needed to uncover failures by prioritizing test inputs that are diverse from previously tested inputs.\n\nWe evaluate this method using two LLM evaluation datasets, comparing the performance of various distance metrics and selection strategies. The results show that our diversity-based approach can accelerate failure detection, enhance output diversity, while maintaining reasonable computational efficiency when selecting a fixed number of test inputs for manual review. This fully black-box method provides a practical, cost-effective solution for developers, improving test suite quality and streamlining the testing process."}]}