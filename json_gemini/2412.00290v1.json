{"title": "Adapting the re-ID challenge for static sensors", "authors": ["Avirath Sundaresan", "Jason Parham", "Jonathan Crall", "Rosemary Warungu", "Timothy Muthami", "Jackson Miliko", "Margaret Mwangi", "Jason Holmberg", "Tanya Berger-Wolf", "Daniel Rubenstein", "Charles Stewart", "Sara Beery"], "abstract": "The Gr\u00e9vy's zebra, an endangered species native to Kenya and southern Ethiopia, has been the target of sustained conservation efforts in recent years. Accurately monitoring Gr\u00e9vy's zebra populations is essential for ecologists to evaluate ongoing conservation initiatives. Recently, in both 2016 and 2018, a full census of the Gr\u00e9vy's zebra population has been enabled by the Great Gr\u00e9vy's Rally (GGR), a citizen science event that combines teams of volunteers to capture data with computer vision algorithms that help experts estimate the number of individuals in the population. A complementary, scalable, cost-effective, and long-term Gr\u00e9vy's population monitoring approach involves deploying a network of camera traps, which we have done at the Mpala Research Centre in Laikipia County, Kenya. In both scenarios, a substantial majority of the images of zebras are not usable for individual identification, due to \"in-the-wild\" imaging conditions occlusions from vegetation or other animals, oblique views, low image quality, and animals that appear in the far background and are thus too small to identify. Camera trap images, without an intelligent human photographer to select the framing and focus on the animals of interest, are of even poorer quality, with high rates of occlusion and high spatio-temporal similarity within image bursts. We employ an image filtering pipeline incorporating animal detection, species identification, viewpoint estimation, quality evaluation, and temporal subsampling to compensate for these factors and obtain individual crops from camera trap and GGR images of suitable quality for re-ID. We then employ the Local Clusterings and their Alternatives (LCA) algorithm, a hybrid computer vision & graph clustering method for animal re-ID, on the resulting high-quality crops. Our method processed images taken during GGR-16 and GGR-18 in Meru County, Kenya, into 4,142 highly-comparable annotations, requiring only 120 contrastive same-vs-different-individual decisions from a human reviewer to produce a population estimate of 349 individuals (within 4.6% of the ground-truth count in Meru County). Our method also efficiently processed 8.9M unlabeled camera trap images from 70 camera traps at Mpala over two years into 685 encounters of 173 unique individuals, requiring only 331 contrastive decisions from a human reviewer.", "sections": [{"title": "Introduction", "content": "The population of Gr\u00e9vy's zebras experienced a dramatic decline beginning in the 1970s largely due to hunting and competition for food and water resources with local pastoral communities. Estimates have placed the number of Gr\u00e9vy's zebras remaining in the wild at under 2,000, with the vast majority in the Samburu region of central Kenya. Due to extensive conservation efforts by the Kenyan and Ethiopian governments as well as environmental NGOs, the population of Gr\u00e9vy's zebras have stabilized in recent years [1].\nAccurately censusing Gr\u00e9vy's zebra populations is critical for ecologists to evaluate these existing conservation efforts. Developing and maintaining a census of known individuals requires effective animal re-identification methods to ensure that only unique individuals are included in population counts [2]. A popular method for population size estimation is \u201ccapture-mark-recapture\" [3]. By this method, a set of animals in the target population is first captured and marked, then a second set of animals is independently recaptured, and finally a population estimate formed from the number of animals captured twice. This method, however, proves difficult to scale to large populations and territories, and may lead to inaccurate population estimates when animals are not confined to the study area and are capable of evading tagging [4, 5]. Further, the tagging methods"}, {"title": "Methods", "content": ""}, {"title": "GZCD Dataset", "content": "The GZCD dataset is comprised of 5,464 images sourced from Meru County, Kenya, taken over four days of GGR-16 and GGR-18 by 13 photographers. The photographers were trained to capture a consistent right-side viewpoint for Gr\u00e9vy's zebra, and only images taken of the intended side were kept in the dataset by human reviewers. The dataset is highly curated: bounding boxes and labels (species, viewpoint, and quality) were set by human reviewers for all animals. The 7,372 right-view Gr\u00e9vy's annotations were further filtered based on these quality labels, resulting in a set of 4,269 \"quality baseline\" right-view Gr\u00e9vy's annotations. Note that the spatial subset for Meru County, Kenya is geographically isolated by mountains from neighboring conservation areas, giving the expectation that the population is largely self-contained. Thus, the GZCD dataset is an excellent testbed for our censusing pipeline."}, {"title": "Camera Trap Dataset", "content": "We use images collected from a network of 70 camera traps distributed around the Mpala Research Centre in Kenya's Laikipia Plateau. The network has collected 8.9 million images over the past two years of deployment. There are four types of camera trap placement schemes in the network: systematically in a grid, at \"magnet\" sites (e.g. salt licks), as well as expert-targeted & random placement along roads."}, {"title": "Automated Species Identification", "content": "Prior to re-ID, the raw camera trap images were first passed through a YOLO v2 species detection model [39] to localize all zebra with a bounding box (both Gr\u00e9vy's and plains zebra species), and crop the region of the image within each bounding box for downstream use. Localized bounding boxes crop out irrelevant and potentially distracting background information and yield distinct annotations of independent individuals from images that feature several animals. YOLO v2 has shown to be more accurate for animal detection than alternative object detection models, such as Faster R-CNN [40]. Next, the cropped regions are classified to zebra species - Gr\u00e9vy's vs. plains zebra and viewpoint left vs. right - by a DenseNet model. Only right, front-right, and back-right viewpoints are considered for identification; differing viewpoints cannot be matched with one another, as the right and left sides of Grevy's zebra are distinct. Both the annotation localization and classification networks were trained on the WILD dataset [39]."}, {"title": "Census Annotations", "content": "Beyond assigning viewpoint and species to each annotation, we wish to ensure that these annotations from both human-captured and camera trap images would be universally comparable. In principle, when such an annotation fails to match the other annotations we should be able to safely conclude that it is the only annotation from that animal. Furthermore, focusing on these annotations (a) should make verification decisions easier both for an algorithm such as VAMP and for a human reviewer (\"verifier\"), (b) should allow recovery from mistakes such as incidental matching *, and (c) should avoid increased human effort to address matching ambiguities\u2020. For Gr\u00e9vy's zebras, this entails annotations that have both the distinctive hip and shoulder chevron (see Fig. 4, which we will call \"census annotations\" (CA) [11].\nIn order to train a census annotation network, we started by curating 10,229 Gr\u00e9vy's zebra annotations from GGR-18. These were arranged in a series of grids and presented via a web interface to two reviewers, who selected the annotations that were Gr\u00e9vy's CAs. Then, a DenseNet with a linear classification layer was trained on this CA dataset to decide whether an annotation was indeed a CA, producing an associated CA confidence score (see Fig. 5). Finally, a regression network was trained to narrow the annotation region to only surround the hip and shoulder chevron and minimize any distracting background information. Each of these new annotations was saved as a \"Census Annotation Region\", or CA-R [11]. The CA and CA-R networks were then used to obtain the CA-Rs and corresponding CA scores for both GZCD and the camera trap dataset."}, {"title": "Filtering Pipeline", "content": "Mark-recapture statistical models are highly sensitive to inaccurate identifications, but are built with the assumption that not all individuals are seen. Thus, it is important to prioritize the precision of our re-identification pipeline over assigning an ID to each annotation. One significant factor in inaccurate identification, by both humans and algorithms, is image quality - blurry, or poorly lit annotations are difficult or impossible to reliably identify. Filtering annotations by quality is therefore a critical component of the re-ID pipeline, and automated quality filtering reduces the amount of human time and work required to rectify matching errors caused by undesirable annotations. Here, we devise an annotation filtering scheme suitable for re-ID from camera traps.\nAs already described, we only consider census annotation regions (CA-Rs), those the show right-side viewpoints of Gr\u00e9vy's zebra, including both the shoulder and hip chevron. Beyond this, we further filter by time of day: only annotations from images taken between 6:30 AM and 7:00 PM (sunrise and sunset at Mpala) were kept for re-ID. Due to the camera trap settings, images taken during the daytime are optical (RGB) and are of higher resolution (13 MP). Nighttime images are taken with an infrared flash at lower resolution (9 MP). Qualitatively, nighttime images are more difficult for human reviewers than daytime images; without modification, we believed the VAMP verification algorithm was likely to perform better on higher contrast and quality images taken during the day.\nNext, the annotations were filtered by encounter. To define the encounters, we used an agglomerative clustering approach: for each camera, annotations from images taken within the same minute and in consecutive minutes were grouped together in the same encounter. Next, the annotation with the highest Census Annotation confidence score was selected from each encounter, and the"}, {"title": "Zebra ID Curation", "content": "The goal of the LCA decision management algorithm is to build an identification graph with the filtered annotations, using automated ranking & verification and delaying human intervention as much as possible. In the identification graph, annotations are represented as vertices, and relationships between annotations (whether positive or negative matches) are represented as edges. The LCA algorithm seeks to group annotations into clusters corresponding to individuals by maximizing positive edge weights within clusters and negative weights between clusters. This simplifies the identity labeling process significantly: instead of being asked to identify an annotation as one of up to 150 known individuals, reviewers are asked to compare pairs of annotations and determine whether they belong to the same individual or not. This contrastive task can be accomplished even by non-experts due to innate human capacity for pattern matching - it is similar to a game of spot the difference.\nThe ID graph is initialized with no edges, with every annotation constituting its own cluster. For every annotation, the Hotspotter ranking algorithm returns a list of its most confident matched annotations, and LCA forms edges between the annotation and its potential matches. Next, the VAMP verification algorithm evaluates every pair of vertices connected by an edge and assigns an edge weight (positive or negative) based on its confidence in the match. The LCA algorithm has two main phases. In the scoring phase, it keeps the initial edges and weights intact. LCA iterates through every local clustering (a single cluster or a pair of clusters) and checks to see if there exists an alternative clustering that increases a score measure defined as the sum of intra-cluster edge weights minus the sum of inter-cluster edge weights. If such an alternative clustering exists, the new arrangement is adopted. Once there are no longer any better local alternative clusterings, LCA proceeds to the stability phase. In this phase, LCA considers local clusterings for which the difference between it and its next best alternative are sufficiently small that changes in edge weights can potentially introduce a superior alternative clustering. For such local clusterings, LCA requests additional reviews to VAMP and the human (if VAMP cannot litigate the review on its own) to determine if certain edge weights must change. Once all local clusterings are significantly better than their next best alternative, the algorithm is considered to have converged."}, {"title": "Results", "content": ""}, {"title": "Results on GZCD", "content": "As discussed above, there were 4,269 \"Quality Baseline\" right-view Gr\u00e9vy's annotations selected for identification from GZCD. Additionally, from the initial set of 7,372 right-view Gr\u00e9vy's annotations in the GZCD, 4,142 were classified as CAs by our CA model using a score quality threshold of 0.31. Furthermore, each of these CAs were manually annotated to yield 4,142 corresponding CA-Rs. It should be noted that the CAs are not a strict subset with the Quality Baseline set; the two sets overlap significantly for trivially good sightings but each have annotations that are not present in the other. These three annotations sets quality baseline, CAs, and CA-RS were each passed into the LCA algorithm for identification.\nAs the GZCD is composed of images taken during GGR-16 and GGR-18 in Meru County, we are able to compare the ID curation results with ground-truth estimates obtained from the two rallies. The Lincoln-Petersen index [41] for the \"Quality Baseline\" is 360 \u00b1 27 in 2016 and 399\u00b129 for 2018. The population estimate based on only CA was 366 \u00b1 27 for GGR 2016 and 373 \u00b1 29 for GGR 2018, whereas the CA-R estimate was 366 \u00b1 27 zebra in 2016 and 373 \u00b1 29 animals in 2018 [42]."}, {"title": "Results on Camera Trap Dataset", "content": "8.9 million camera trap images from the initial dataset were passed through the species ID pipeline, yielding 84,383 zebra images (including both plains and Gr\u00e9vy's zebra). Following species and viewpoint filtering, 23,512 right-view Gr\u00e9vy's zebra annotations remained across 3,338 distinct encounters. After excluding nighttime encounters, 1,138 daytime encounters remained. We then sampled the single highest-quality annotation from each of the 1,138 encounters, as determined by our CA model, to avoid matching on near-identical images from the same burst. We further used a score quality threshold of 0.31 on census annotations that reduced the number of annotations to be identified to 734. As a last step, we used a blurriness filter to remove any additional challenging annotations, resulting in a final set of 685 CA-Rs.\nIn order to associate these 685 annotations to clusters of individuals, the LCA algorithm requested 5,403 automated reviews by VAMP and 331 additional human reviews; pairwise comparisons of annotations were performed largely automatically using VAMP, with a automation rate of 93.9%. The converged ID graph had 173 clusters, each corresponding to an individual zebra ID within the 685 annotations. On average, each individual was sighted across 1.9 static camera traps and 3.96 encounters, indicating significant matching across space and time. See Supplementary Fig. 8 for the distribution of encounters for each individual, and Supplementary Fig. 9 for the distribution of static camera traps that sighted each individual. The number of individuals sighted by each camera also differed based on its placement strategy (random grid, known Gr\u00e9vy's territories along roads, randomly along roads, and both timelapse & motion-trigger traps at magnet sites). On average, camera traps at a magnet site (e.g. salt lick, dam, etc.) sighted the most individuals on average (8.5 per trap) and also produced the most first individual sightings on average (5.85 per trap). See Table 2 and Supplementary Fig. 10 for the distribution of total individuals sighted by cameras of each placement strategy. Particularly, the encounters for large individual clusters were spread across many months, with some spanning more than a year (see Supplementary Fig. 11). Based on the distinct encounters and cameras associated with each individual, we can generate spatial maps of encounters for any individual across the reserve: see Fig. 1 for a map of encounters for a particular zebra (Individual 32)."}, {"title": "Discussion", "content": "Census Annotations and Census Annotation Regions are critical to automated photographic census because they 1) speed up human verification of match pairs and reduce the number of manual decision errors, 2) better separate the positive and negative scores predicted by algorithmic verifiers like VAMP, 3) reduce the amount of incidental matching [11], and 4) drastically reduce the amount of human interaction needed during manual review. Census Annotation Regions are powerful because they force a photographic census to consider only the most critical information for matching. Furthermore, our results indicate that using Census Annotations and Census Annotation Regions results in consistent population estimates on both the known GZCD baseline and camera trap dataset."}, {"title": "Conclusion", "content": "In this paper, we perform efficient semi-automated Gr\u00e9vy's zebra re-ID from camera trap data, with an algorithmic tool chain using Hotspotter for ranking, VAMP for verification, and LCA for decision management. Our method can be used even by non-zebra experts [11], as it only requires contrastive comparisons between pairs of individuals instead of matching each individual into the previously identified population. Ultimately, this curational re-ID process found 173 distinct individuals in camera trap data collected across a two year period at the Mpala Research Centre.\nGoing forward, we aim to further refine the filtering pipeline to improve matching with fewer human reviews and reduce attrition of data through the pipeline. We also are excited to further explore the results of the method, for example by adjusting the CA score threshold and observing how this affects LCA convergence behavior & results. We also hope to further adapt LCA to enable temporal sequences and spatial relationships to be taken into account, to enable us to better make use of the additional images of the individuals from each camera trap image burst and the spatial structure of the habits of individuals over time. Finally, and along similar lines, we hope to introduce \"short-circuiting\" to reduce human reviews when spatio-temporal constraints imply that there is no possibility of a match."}]}