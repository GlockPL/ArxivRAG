{"title": "Forgetting Any Data at Any Time: A Theoretically Certified Unlearning Framework for Vertical Federated Learning", "authors": ["Linian Wang", "Leye Wang"], "abstract": "Privacy concerns in machine learning are heightened by regulations such as the GDPR, which enforces the \"right to be forgotten\" (RTBF), driving the emergence of machine un-learning as a critical research field. Vertical Federated Learning (VFL) enables collaborative model training by aggregat-ing a sample's features across distributed parties while pre-serving data privacy at each source. This paradigm has seen widespread adoption in healthcare, finance, and other privacy-sensitive domains. However, existing VFL systems lack robust mechanisms to comply with RTBF requirements, as unlearn-ing methodologies for VFL remain underexplored. In this work, we introduce the first VFL framework with theoretically guaranteed unlearning capabilities, enabling the removal of any data at any time. Unlike prior approaches-which impose restrictive assumptions on model architectures or data types for removal our solution is model- and data-agnostic, of-fering universal compatibility. Moreover, our framework sup-ports asynchronous unlearning, eliminating the need for all parties to be simultaneously online during the forgetting pro-cess. These advancements address critical gaps in current VFL systems, ensuring compliance with RTBF while maintain-ing operational flexibility. We make all our implementations publicly available at https://github.com/wangln19/vertical-federated-unlearning.", "sections": [{"title": "Introduction", "content": "When personal data is used to train machine learning models, privacy protection can be considered from two key perspec-tives: first, ensuring that data does not leave the local domain during the training process to minimize the risk of data leak-age [20]; second, providing users with the ability to revoke their consent and delete their personal data after the model has been trained [22]. To address the issue of data isolation and privacy risks during the training process, FL has emerged as a promising solution. In addition, regulations like the GDPR have proposed the \"right to be forgotten\" (RTBF), which ne-cessitates the removal of personal data from trained models.\nThis need has given rise to the concept of machine unlearn-ing [3].\nFederated Unlearing (FU) combines these two orthogonal challenges, offering comprehensive privacy protection. FL can be categorized into two main types: Horizontal Federated Learning (HFL), where participants share the same features but have different samples, and Vertical Federated Learning (VFL), where participants share the same samples but have different features. VFL has found widespread application in cross-silo scenarios, such as those in banking and healthcare. However, Vertical Federated Unlearning (VFU) is still under-investigated. In real-world scenarios, unlearning requests are frequently generated, with diverse objectives, such as client, feature, or sensitive information removal [22, 27]. Current VFU methods only address specific data removal types (e.g., client [8, 26] or feature [17] unlearning) and are often tailored to particular VFL models (e.g., logistic regression [8] or gra-dient boosting trees [17]). This underscores the critical need for a unified VFU framework capable of handling multiple data removal scenarios and models.\nAnother challenge in VFU lies in its computational and communication overheads. In VFL systems, each client main-tains a sub-model trained on its local feature data. However, unlearning data from one party triggers cascading impacts across all parties' sub-models. Existing VFU methods ad-dress this by enforcing synchronous unlearning [11]: upon an unlearning request, all clients must coordinate in real-time to update their sub-models. Such reliance on synchroniza-tion is operationally impractical\u2014network instability, client downtime, or resource constraints frequently disrupt client participation, thus breaking the unlearning process.\nIn this paper, we propose a novel VFL framework with robust VFU capabilities of forgetting any data at any time. This framework is capable of handling diverse unlearning objectives in a unified manner and supports asynchronous un-learning method to alleviate the burden on clients. Built on the widely adopted aggregate VFL (AggVFL) systems [9, 19,31], our framework unifies different unlearning targets from the perspective of confidence scores: the local models of clients"}, {"title": "Related Work", "content": "Vertical Federated Learning (VFL) is a distributed machine learning paradigm that enables collaboration between mul-tiple parties without sharing sensitive data. In general, there are two types of VFL, i.e., Aggregate VFL (AggVFL) and Split VFL (SplitVFL) [19]. AggVFL employs a non-trainable global module (e.g., Sigmoid activation) to aggregate inter-mediate results through secure computation. Representative work on AggVFL includes SecureBoost [7] for federated gra-dient boosting trees and SFTL [18] for cryptographic defense. On the other hand, SplitVFL utilizes trainable global modules aligned with vertical split neural networks [14]. This type of VFL has been studied in various works, including privacy-preserving split learning [25] and communication-efficient CELU-VFL [10]. In this work, we focus on AggVFL due to its superior compatibility with applications requiring strict data isolation.\nTheoretically-Guaranteed Machine Unlearning\nMachine unlearning has attracted considerable attention in recent years. However, most existing research on machine un-learning lacks theoretical guarantees about its efficacy. There are currently two main approaches that provide formal guaran-tees of complete forgetting. The first approach, exemplified by SISA [3], focuses on exact unlearning. These methods [6,28] partition both the model and the training set, performing re-training only on the sub-model relevant to the unlearning request, thereby ensuring complete unlearning. The second approach draws from differential privacy concepts [5] and proves that the model post-unlearning is statistically indis-tinguishable from a retrained model, demonstrating the ef-fect of approximate unlearning, also called certified unlearn-ing [13,21,27].\nWhile the first approach offers exact unlearning, it is in-efficient in scenarios where unlearning requests are scatted"}, {"title": "Method", "content": "Our approach is based on the following idea: in linear models $y = ax_1 + bx_2 + c$, forgetting $x_1$ corresponds to subtracting the corresponding term $ax_1$ from $y$ and then fine-tuning the model to remove $x_1$'s influence on the parameters $b$ and $c$.\nFor more complex models in VFL, we can apply this idea at the output layer. In a classification problem, we maintain a confidence matrix in the active party of the VFL, where each row represents a sample and each column represents the confidence for a particular class. Upon receiving a forgetting request, we subtract the corresponding confidence value from the matrix, eliminating the forgotten target's influence on the output. We then fine-tune the model using the updated output to remove the forgotten target's effect on the model parameters.\nFor instance, in binary classification, the confidence vec-tor associated with a sample is two-dimensional, where each element corresponds to one of the two classes. Consider two clients, i and j, where Client i has a classification confidence vector of [-1,1] and Client j has a confidence vector of [0,3] for a specific sample. During aggregation, the global model combines these vectors, producing a summed confidence vec-tor [-1,4], which is stored in a row of the confidence matrix. To remove Client i's contribution on this sample, its confi-dence vector [-1,1] is subtracted from the corresponding row of the matrix. The updated confidence matrix is then used to compute the gradient with respect to the label. By updating Client j's parameters using this revised matrix, the influence of Client i on Client j's parameters is effectively removed."}, {"title": "Compatibility with Diverse Requests", "content": "We want to design a VFL system that supports diverse un-learning requests (e.g., client removal, feature removal) while ensuring data remains within the local domain. This requires a mechanism to communicate forgotten information to each client in a granular yet privacy-preserving manner.\nWe leverage the confidence matrix as a unified represen-tation for unlearning. When an unlearning request is made, the requesting client calculates the difference in confidence vectors before and after unlearning and sends this difference to the active party. The active party updates the confidence matrix to reflect the removal of the forgotten data. For Granu-larity, any data change (e.g., client removal, feature removal) can be represented as an update to the confidence matrix. For Privacy, this unlearning process transmits the same informa-tion as the training process in VFL, ensuring no additional privacy leakage.\nAdditionally, our framework makes a slight adjustment to the backpropagation steps, so the system starts the update process from the stored confidence matrix. This ensures that"}, {"title": "Achieving Asynchronous Unlearning", "content": "In VFU, two key time constraints must be considered:\n1.  VFL Inference Phase: During inference, the VFL sys-tem relies on data and computations from all clients. Thus, all clients must be online, and any unlearning re-quests must be completed prior to inference.\n2.  Unlearning Deadline: Unlearning requests are associ-ated with a deadline, requiring all related tasks to be completed within the specified timeframe.\nOur asynchronous unlearning framework operates within a defined time window between two consecutive inference phases. Unlike synchronous approaches, not all clients need to remain online. When an unlearning request is initiated, only the requesting client and the active party must be online. The requesting client stores the data to be forgotten, making\nit particularly sensitive to unlearning speed. This client can perform unlearning immediately, ensuring compliance with privacy regulations without requiring other clients to stay online. Other clients can defer their unlearning processes, reducing their operational burden.\nThe asynchronous setup introduces two main challenges:\n1.  Quantifying Client Impact: Accurately measuring how each client's model update contributes to the global model update.\n2.  Compensating for Offline Clients: Addressing the ab-sence of offline clients during unlearning by leveraging updates from online clients and stored information, en-suring no additional privacy leakage.\nThe first challenge is effectively addressed by our confidence-based framework. During each update, the im-pact of each client's update is directly reflected in the change in its output confidence, which then influences the global model's output, and subsequently the gradient for the next update. The second challenge is how to calculate the change in confidence output resulting from model parameter updates for offline clients. We solve this by introducing the stability of the update contribution factor in the operation of a VFL system."}, {"title": "The Stability of The Update Contribution Factor", "content": "Each passive party computes the confidence score $h_i$ of its local model. The active party aggregates these scores, comput-ing $H = \\sum h_i$, and feeds $H$ into the global model $G$ to produce the final output $p = G(H)$.\nDuring backpropagation, as client parameters $\\theta_i$ are up-dated, their confidence scores $h_i = f_i(\\theta_i, X_i)$ also change. Let $\\delta(h_i)$ denote the change in $h_i$. The global model's in-put change $\\delta(H)$ is a linear combination of these individual changes: $\\delta(H) = \\sum \\delta(h_i)$, which in turn affects the global output $p$.\nThe linear relationship $\\delta(H) = \\sum \\delta(h_i)$ provides insight into each client's contribution to the global model update. The ra-tio $R_i = \\frac{\\delta(h_i)}{\\delta(H)}$, noted as termed the update contribution factor, quantifies client $i$'s influence on the model's change during updates. Notably, in VFL systems using logistic regression, these factors remain stable across training epochs.\nTo illustrate, consider two clients, $i$ and $j$, with features $i_1$ to $i_4$ and $j_1$ to $j_5$, respectively. Their confidence scores are computed as:\n$h_i = f_i(\\theta_i, X_{i1}, X_{i2}, X_{i3}, X_{i4})$\n$h_j = f_j(\\theta_j, x_{j1}, x_{j2},x_{j3},x_{j4}, X_{j5})$.\nSince the training set remains fixed, the representations output by each client depend solely on their parameters. We"}, {"title": "Achieving Certified Unlearning", "content": "Consider a learning algorithm $\\mathcal{A}$ that, when trained on a dataset $\\mathcal{D}$, generates a model $\\theta \\in \\Theta$. An unlearning method $\\mathcal{U}$ that transforms a model $\\theta$ into a corrected version $\\theta_{\\mathcal{U}} = \\mathcal{U}(\\theta, \\mathcal{D}, \\mathcal{D}')$. $\\mathcal{D}'$ contains the perturbations $\\mathcal{Z}$ needed for un-learning, while the corresponding original data is $\\mathcal{Z}$. The con-cept of $\\epsilon$-certified unlearning means that it is hard to dis-tinguish models after unlearned $\\mathcal{U}$ from the set of possible retrained models $\\mathcal{A}(\\mathcal{D}')$ [13,27].\nGiven some $\\epsilon > 0$ and a learning algorithm $\\mathcal{A}$, an unlearning method $\\mathcal{U}$ is $\\epsilon$-certified if\n$\\qquad\\qquad\\qquad\\mathbb{P}(\\mathcal{U}(\\mathcal{A}(\\mathcal{D}),\\mathcal{D},\\mathcal{D}') \\in \\mathcal{T})\\qquad\\qquad\\qquad\\qquad\\qquad \\qquad e^{-\\epsilon} < \\frac{\\mathbb{P}(\\mathcal{A}(\\mathcal{D}') \\in \\mathcal{T})}{\\mathbb{P}(\\mathcal{U}(\\mathcal{A}(\\mathcal{D}),\\mathcal{D},\\mathcal{D}') \\in \\mathcal{T})} < e^{\\epsilon}$\\\nholds for all $\\mathcal{T} \\subseteq \\Theta, \\mathcal{D}, and \\mathcal{D}'$.\nThe $(\\epsilon, \\delta)$-certified unlearning is similarly defined."}, {"title": "Design for Certified Unlearning", "content": "To achieve rigorously defined certified unlearning, our method incorporates two critical components:\n1.  Noise Injection during Training: We add Gaussian noise $b \\sim N(0, \\sigma^2I)$ to the gradients during model train-ing and unlearning, ensuring bounded parameter sensi-tivity.\n2.  First-Round Gradient Ascent: During the initial un-learning update, we simultaneously perform:\n$\\theta_{\\text{unlearn}} = \\theta^* - \\tau (\\nabla_\\theta L(\\theta^*; D') - \\nabla_\\theta L(\\theta^*; D))$\ndescent on new data ascent on old data\nTheorem 1 (Certified Unlearning Guarantee). Assume the loss $l(\\theta;z)$ is convex, $\\gamma$-smooth with L2 regularization $|\\theta||_2$. For any data modification $(\\mathcal{Z},\\widetilde{\\mathcal{Z}})$, our method achieves $(\\epsilon, \\delta)$-certified unlearning with $\\delta = 1.5e^{-c^2/2}$ when:\n$\\qquad\\qquad$\u2022 Training noise $p \\sim N(0, c(1 + \\tau\\gamma \\epsilon_n)\\gamma_\\text{z} M|\\mathcal{Z}|/\\epsilon)^d$ for some $c > 0$\n$\\qquad\\qquad$\u2022 Assume that $||x_i||_2 \\leq 1$ for all data points and the gra-dient $\\nabla l(z, \\theta)$ is $q_z$-Lipschitz. Further let $\\mathcal{Z}$ change the features $j,..., j+F$ by magnitudes at most $m_j,...,m_{j+F}$, and $M = \\sum_{j'=1}^F m_{j'}$\nSketch. The certification follows three key arguments:\n1.  Gradient residual bound using Lipschitz continuity and our update rule\n2.  Relating the gradient residual to the sensitivity of per-turbation vector $b b$ using the L2-regularized strong con-vexity.\n3.  Applying Gaussian noise over models to yield $(\\epsilon, \\delta)$-guarantees."}, {"title": "Experimental Setup", "content": "There are three common unlearning scenarios in VFL:\n1.  Client Removal: A client exits the VFL system, and its data's influence must be completely removed. This is equiva-lent to retraining the model by setting all data from the client to zero [27].\n2.  Feature Removal: A certain feature (or feature set) from a client, which involves sensitive user information, is no longer available due to policy changes or other reasons. The influence of this feature must be removed from the system. This is equivalent to retraining the model with the feature set to zero [27].\n3.  Sensitive Information Removal: A feature from a client may be sensitive to a subset of users, prompting requests for removal (e.g., some users modify the visibility of the age field from 'public' to 'private'). This can be achieved by retraining the model where the sensitive information is replaced by the mean value of that feature across all samples [27].\nThe relationships between these three scenarios are illus-trated in Figure 3. As sensitive information may pertain to a single feature of a single sample (the smallest data unit), our method can easily extend to tasks such as sample removal (forgetting all data from a single sample) and class removal (forgetting all samples of a particular class)."}, {"title": "Synchronous Client Removal", "content": "In existing VFL literature, client removal is a commonly stud-ied unlearning scenario. In this context, we address the first key question: whether our method performs comparably to or even better than existing unlearning approaches when all clients are online (i.e., synchronous unlearning).\nTo validate this, we select five datasets: Adult Income [2], Credit [30], Diabetes [16], Nursery [23], and Malware [1]. Among these, Adult Income, Credit, Diabetes and Nursery are tabular datasets, while Malware is a text-based dataset. For the text dataset, we extract bag-of-words features [33]. We split 80% of the data for training and reserve 20% for testing. Table 2 overviews the datasets and their vertical federated set-ting. Without loss of generality, we set one client for removal; the number of features in the removed client is in Table 2.\nFor each dataset, we conduct experiments using both the LR and MLP models, resulting in a total of 6\u00d72=12 experimental"}, {"title": "Sync. & Async. Feature Removal", "content": "Since VFULR and VFUFR do not support the feature removal scenario, our analysis is exclusively benchmarked against Re-train. We further evaluate our method on both synchronous and asynchronous settings, demonstrating comparable per-"}, {"title": "Sync. & Async. Sensitive Info. Removal", "content": "Sensitive information removal represents a more fine-grained refinement of the feature removal scenario, where only spe-"}, {"title": "Asynchronous Unlearning Online Rate", "content": "Our asynchronous unlearning method enables updates even when a subset of clients is offline. A critical question is how the proportion of online clients affects performance. To ad-dress this, we evaluate client removal using the LR model on the Adult Income dataset. The dataset contains 108 features distributed among 16 clients: the target client (Client 0) holds"}, {"title": "Conclusion", "content": "In VFU, multiple unlearning requests targeting different ob-jectives may arise, which the current methods cannot handle in a compatible manner. Additionally, the requirement for"}, {"title": "Appedix A", "content": "Previous work has shown that in a VFL system using logistic regression, the update contribution coefficients of each client remain fixed during each update. We now extend this result to Multi-Layer Perceptron (MLP). While the contribution co-efficients do not remain constant, we demonstrate that, under an appropriate setting for the unlearning tasks, the change in coefficients is minimal when the MLP's depth and width are constrained."}, {"title": "Appedix B", "content": "We can also prove that in VFL, performing unlearning only for the client requesting unlearning is insufficient to achieve the overall unlearning objective."}, {"title": "Objective", "content": "We aim to prove that when only a subset of clients partic-ipates in the unlearning process, the unlearning operation cannot achieve approximate unlearning. To achieve approxi-mate unlearning, we need to ensure that the parameters after unlearning are very close to the parameters of a retrained model. This can be done by controlling the gradient residual."}, {"title": "Composition of Prediction P", "content": "In vertical federated learning, the prediction $P$ is calculated based on the intermediate values of all client output.\n$\\qquad\\qquad\\qquad P = \\text{Global}(\\sum_{i=1}^k h_i(\\theta_i, Z_i)).$\nwhere: - $h_i(\\theta_i, Z_i)$ is the intermediate output of the i-th client. - $k$ is the number of clients participating in federated learning. - Global() is the global model located in the activate party."}, {"title": "Gradient Representation", "content": "To derive the requirements for approximate unlearning, we first examine the gradient of the logistic regression model's loss function with respect to the model parameters. Using the chain rule, the gradient can be written as the product of the following three terms:\n$\\nabla_\\theta L = \\frac{\\partial L}{\\partial P} \\frac{\\partial P}{\\partial h} \\frac{\\partial h}{\\partial \\theta}.$\nwhere:\n\u2022 $\\frac{\\partial L}{\\partial P}$: The derivative of the loss function (e.g., cross-entropy loss) with respect to the prediction $P$.\n\u2022 $\\frac{\\partial P}{\\partial h}$: The derivative of the prediction $P$ with respect to each client's intermediate value $h$.\n\u2022 $\\frac{\\partial h}{\\partial \\theta}$: The derivative of each client's intermediate value $h$ with respect to the client's parameters $\\theta$.\nOur goal is to make the gradient $\\nabla_\\theta L$ close to zero, which would achieve approximate unlearning. The main factor af-fecting the gradient is the first term, $\\frac{\\partial L}{\\partial P}$, i.e., the derivative of the loss function with respect to the prediction $P$. This term determines the size of the final gradient. If the prediction after unlearning is close to the retrained prediction, this term will be close to zero, which would bring the gradient close to zero. So, to achieve the approximate unlearning, the difference be-tween the unlearned predictions and the retrained predictions should be small\n$\\qquad\\qquad\\qquad |P - P^* | = |\\text{Global}(\\sum_{i=1}^k h_i(\\widetilde{\\theta}_i, \\widetilde{Z}_i)) - \\text{Global}(\\sum_{i=1}^k h_i(\\theta^*_i, \\widetilde{Z}_i))|$,\nwhere $\\widetilde{Z}$ is the training set after the unlearning request, $\\widetilde{\\theta}_i$ is the unlearned parament of i-th client and $\\theta^*_i$ is the retrained parament.\nIn AggVFL, the global model is not trainable, so the summed predictions from the clients should be close to that of the retrained model's output. To achieve the approximate unlearning, we want that there exists a small value $\\epsilon > 0$ that\n$\\qquad \\qquad \\qquad\\qquad\\qquad |\\sum_{i=1}^k h_i(\\widetilde{\\theta}_i, \\widetilde{Z}_i) - \\sum_{i=1}^k h_i(\\theta^*_i, \\widetilde{Z}_i)| < \\epsilon$.\nSuppose overall there are $k$ features, among them the first $j$ features take part in the unlearning process\n$\\qquad \\qquad \\qquad \\qquad\\qquad \\Theta^* = [\\theta^*_1,..., \\theta^*_j, \\theta^*_{(j+1)},..., \\theta^*_k] \\in \\mathcal{I}$.\nThe unchanged intermediate client output with features denoted as $\\theta^*_{(j+1)}$ are different from the retrained value with a distance $|h_U(\\theta^*, \\widetilde{Z}_i)|$ caused by unlearned feature.\n$\\qquad\\qquad |\\sum_{i=1}^k h_i(\\widetilde{\\theta}_i, \\widetilde{Z}_i) - \\sum_{i=1}^k h_i(\\theta^*_i, \\widetilde{Z}_i)|$\n$\\qquad \\qquad \\qquad = |\\sum_{i=1}^j h_i(\\widetilde{\\theta}_i, \\widetilde{Z}_i) - \\sum_{i=1}^j h_i(\\theta^*_i, \\widetilde{Z}_i) + \\sum_{i=j+1}^k h_i(\\theta^*_{(j+1)}, \\widetilde{Z}_i) - \\sum_{i=j+1}^k h_i(\\theta^*_{(j+1)}, \\widetilde{Z}_i)|$\n$\\qquad \\qquad \\qquad = |\\sum_{i=1}^j - h_U(\\theta^*, \\widetilde{Z}_i) - \\epsilon| = |h_U(\\theta^*, \\widetilde{Z}) + \\epsilon|$.\nThe approximate unlearning target below is hard to achieve when $j$ is small because h has a much larger dimension, the unlearned features' gap can hardly be covered by $j$ features.\n$\\qquad |\\sum_{i=1}^k h_i(\\widetilde{\\theta}_i, \\widetilde{Z}_i) - \\sum_{i=1}^k h_i(\\theta^*_i, \\widetilde{Z}_i)|$\n$\\qquad \\qquad \\qquad \\qquad = |\\sum_{i=1}^j h_i(\\widetilde{\\theta}_i, \\widetilde{Z}_i) - \\sum_{i=1}^j h_i(\\theta^*_i, \\widetilde{Z}_i) + \\sum_{i=j+1}^k h_i(\\theta^*_{(j+1)}, \\widetilde{Z}_i) - \\sum_{i=j+1}^k h_i(\\theta^*_{(j+1)}, \\widetilde{Z}_i)|$\n$\\qquad \\qquad \\qquad = |\\sum_{i=1}^j - h_U(\\theta^*, \\widetilde{Z}_i) + 0 + \\epsilon| = |\\sum_{i=1}^j h_U(\\theta^*, \\widetilde{Z}_i) + \\epsilon|$.\nBesides, we can interpret the prediction difference in terms of vector spaces. Let $A = \\sum_{i=1}^j h_i(\\widetilde{\\theta}_i, \\widetilde{Z}_i)$ represent the mod-ified outputs, and $B = \\sum_{i=j+1}^k h_i(\\theta^*_{(j+1)}, \\widetilde{Z}_i)$ represent the unchanged outputs. The difference between the modified and retrained outputs is:\n$\\qquad \\qquad |A - B| = |h_U(\\theta^*, \\widetilde{Z}) + \\epsilon|$.\nWhen $j$ is small, the unlearning operation can only change a small part of the output space. Since $h_U(\\theta^*,Z)$ involves contributions from the unchanged clients' outputs, and their rank may be high, the modification of $j$ clients' outputs is insufficient to reduce the overall error. Hence, the difference cannot be small enough to achieve approximate unlearning."}, {"title": "Appedix C", "content": "The results are shown in Figure 7 and 8."}, {"title": "Appedix D", "content": "Our method involves multiple rounds of retraining, with the parameters of the pre-trained original model serving as the initial state. To achieve the rigorously defined certified un-learning, additional adjustments are required. It is worth not-ing that we make two key assumptions: First, the loss function is convex. Second, we include an L2 regularization $|\\theta||_2$.\nIn the first round of updates, we simultaneously perform gradient ascent on the original data and gradient descent on the modified data. It can be shown that this update is equivalent to the expression in Equation (1) from the well-established influence function-based certified forgetting method [27]. According to Lemma 1 in that paper, the gradient bound of the updated loss function can be derived from Equation (1).\n$\\qquad \\qquad \\Delta(Z,\\widetilde{Z}) = -\\tau (\\nabla_\\theta L(\\theta^*; \\mathcal{D}') - \\nabla_\\theta L(\\theta^*; \\mathcal{D}))$\n$\\qquad \\qquad \\qquad = -\\tau(\\sum_{z\\in \\widetilde{\\mathcal{Z}}} \\nabla_\\theta l(z,\\theta^*) + \\nabla L(\\theta^*;\\mathcal{D}' \\backslash \\widetilde{\\mathcal{Z}})$\n$\\qquad \\qquad \\qquad - \\sum_{z\\in \\mathcal{Z}} \\nabla_\\theta l(z, \\theta^*) - \\nabla L(\\theta^*;\\mathcal{D} \\backslash \\mathcal{Z}))$\n$\\qquad \\qquad \\qquad = -\\tau(\\sum_{z\\in \\widetilde{\\mathcal{Z}}} (\\nabla_\\theta l(z,\\theta^*) - \\nabla_\\theta l(z, \\theta^*))$\nLemma 1. [27]Assume that $||x_i||_2 \\leq 1$ for all data points and the gradient $\\nabla l(z,\\theta)$ is $q_z$-Lipschitz with respect to z at $\\theta^*$ and $\\gamma$-Lipschitz with respect to $\\theta$. Further let $\\mathcal{Z}$ change the features $j,..., j +F$ by magnitudes at most $m_j,...,m_{j+F}$. If $M = \\sum_{j'=1}^F m_{j'}$ the following upper bounds hold: For the following update form\n$\\qquad \\qquad \\Delta(Z,\\widetilde{Z}) = -\\tau(\\sum_{z\\in \\widetilde{\\mathcal{Z}}} \\nabla_\\theta l(z,\\theta^*) - \\sum_{z\\in \\mathcal{Z}} \\nabla_\\theta l(z, \\theta^*))$ (1)\nWe have\n$\\qquad \\qquad  |\\nabla L(\\theta_{\\widetilde{Z}}^*, \\mathcal{D}') ||_2 \\leq (1 + \\tau \\gamma_n)\\gamma_zM |\\mathcal{Z}|$\nThe gradient residual $\\nabla L(\\theta;\\mathcal{D}')$ of a model $\\theta$ with respect to the corrected dataset $\\mathcal{D}'$ is zero only when $\\theta = \\mathcal{A}(\\mathcal{D}')$. For strongly convex loss functions, the magnitude of this gradient residual, $||\\nabla L(\\theta;\\mathcal{D}')||_2$, reflects the discrepancy between the model $\\theta$ and the one obtained by retraining on $\\mathcal{D}'$.\nNext, in subsequent updates, we employ an early stopping mechanism to ensure that the training loss continues to de-crease on the updated training set. Since the loss function is strongly convex, the gradient of the loss function after the update will also be smaller than that of the first round.\n$\\qquad \\qquad |\\nabla L(\\theta_{t+1})||^2 \\leq |\\nabla L(\\theta_t)||^2$\nFinally, we prove that after unlearning, the gradient of the loss function has an upper bound. Based on Lemma 2 from the [27], we are able to demonstrate that certified unlearning holds.\nWhen a vector $b$ is added, the gradient residual r for the loss function $L_b$ becomes:\n$\\qquad \\qquad  r = \\nabla L_b(\\theta;\\mathcal{D}') = \\sum_{z\\in \\mathcal{D}'} \\nabla l(z, \\theta) + 2 \\lambda \\theta + b$\nBy manipulating the distribution of $b$, certified unlearning can be achieved, akin to sensitivity-based techniques [5].\n2. [27]Let A be the learning algorithm that returns the unique minimum of L b ( ; ) and let U be an unlearning method that produces a model \u03b8 u. If ||\u2207L(\u03b8 a ; D )|| 2 \u2264 ' for some ' > 0 we have the following guarantees.1. If b is drawn from a distribution with density p(b) = then performs -certified unlearning for A.\nIf ~ N(0,c/'/) d for some c > 0 then U performs (,)-certified unlearning for A with = 1.5e c^2/2 ."}]}