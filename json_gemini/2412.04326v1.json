{"title": "Understanding Student Sentiment on Mental Health Support in Colleges Using Large Language Models", "authors": ["Palak Sood", "Chengyang He", "Divyanshu Gupta", "Yue Ning", "Ping Wang"], "abstract": "Mental health support in colleges is vital in educating students by offering counseling services and organizing supportive events. However, evaluating its effectiveness faces challenges like data collection difficulties and lack of standardized metrics, limiting research scope. Student feedback is crucial for evaluation but often relies on qualitative analysis without systematic investigation using advanced machine learning methods. This paper uses public Student Voice Survey data to analyze student sentiments on mental health support with large language models (LLMs). We created a sentiment analysis dataset, SMILE-College, with human-machine collaboration. The investigation of both traditional machine learning methods and state-of-the-art LLMs showed the best performance of GPT-3.5 and BERT on this new dataset. The analysis highlights challenges in accurately predicting response sentiments and offers practical insights on how LLMs can enhance mental health-related research and improve college mental health services. This data-driven approach will facilitate efficient and informed mental health support evaluation, management, and decision-making.", "sections": [{"title": "I. INTRODUCTION", "content": "Mental health has become a paramount concern within the student community, increasingly recognized as essential to both their overall well-being and academic success [1], [2]. A 2020 report by the National Institute of Mental Health highlights that mental illness prevalence is highest among those under 25 years, including 67% of college students [3]. Universities play a crucial role by offering counseling services and organizing events to support students' emotional well-being. However, evaluating mental health support in colleges faces challenges such as data collection difficulties, lack of standardized metrics, insufficient funding, and limited inter-institutional collaboration [4], [5]. These issues restrict research scope, with most studies [6]\u2013[8] focusing on student mental health status rather than the effectiveness of support services.\nStudent feedback is vital for assessing university mental health services. Surveys like the Healthy Minds Study [9] and the American College Health Association Health Assessment [10] gather insights into students' mental health and service utilization. Universities can use this feedback to improve their initiatives. Recent studies have explored student perspectives on mental health support [11], [12], but key challenges remain.\nThese include reliance on qualitative analysis from limited feedback, lack of comprehensive quantitative evaluations, and the absence of utilizing advanced machine learning methods to analyze sentiments. Additionally, existing datasets do not support developing machine learning models for this purpose.\nThis paper aims to utilize the Student Voice Survey (SVS) data by College Pulse [13] to create a sentiment analysis dataset and explore the potential of large language models (LLMs) for predicting students' sentiments. Specifically, we utilize the students' narrative feedback in SVS data about their feedback for the advantages and disadvantages of mental health support in their colleges. To create the dataset for sentiment analysis, we first explore the spectrum of students' sentiments by leveraging the power of LLMs, considering the standard three categories of sentiment labels (including \"Satisfied\", \"Dissatisfied\", and \"Neutral\"), and designing a task-specific coarse prompt. This investigation motivates us to adopt a more detailed analysis by introducing a new sentiment category \"Mixed\". With the more nuanced set of sentiment categories, we collect the sentiment labels of students' responses in SVS data with human annotation, validation, and collaborative discussion.\nThe enriched SVS data, named SentiMent analysis of students' mental health support in Colleges (SMILE-College), includes 793 records with sentiment labels and is publicly available online\u00b9. We aim to investigate three tasks: (1) Sentiment prediction: Automatically predicting sentiment labels by designing task-specific prompts for LLMs with fine-grained sentiment categories. (2) Prediction error analysis: Analyzing LLM prediction errors across sentiment categories. (3) Support limitation identification: Using LLMs, embedding learning, and clustering techniques to identify key limitations of mental health support based on \u201cDissatisfied\" responses. To the best of our knowledge, this is the first work to comprehensively evaluate student mental health support in colleges from students' perspective. This data-driven study enables the automatic prediction of students' perceptions of mental health support with advanced LLMs, providing quantitative and qualitative assessment."}, {"title": "II. RELATED WORK", "content": "The significance of mental health within student communities has escalated, underscoring the essential role of support services. Existing research about mental health in colleges mostly focuses on investigating students' mental health status [6]\u2013[8]. Research on evaluating mental health support in colleges is limited due to various challenges. Various surveys, such as the Student Voice Survey and the Healthy Minds Study [9], are designed to gain students' insights on mental health services for assessing these services. The American College Health Association also conducted a survey to collect students' perceptions, behaviors, and habits [10], [14]\u2013[16]. There are some recent works about student perspectives on improving mental health support services in universities or systematically reviewing the students' use of mental health services in universities [11], [12]. However, there are still several key challenges that have not been addressed, such as qualitative analysis on limited feedback, lack of comprehensive quantitative evaluation, and the absence of utilizing advanced machine learning methods for the evaluation.\nSentiment analysis is the computational study of opinions, attitudes, and emotions expressed in narrative texts [17]. Deep learning models, including recurrent neural networks and transformer-based models, have been successfully employed in sentiment analysis [18]\u2013[21], while lexicon- and rule-based methods relying on sentiment dictionaries have also been widely used [22]. Sentiment analysis has also been applied to social media data for the detection of signs of depression and suicidal ideation, as demonstrated by Shen et al. [23]. Recently, the pre-trained and large language models gained significant attention in the field of sentiment analysis [24].\nSentiment analysis has been widely applied to various applications, such as social media [25] and customer feedback [26], and demonstrated its effectiveness. It has also been used in various mental health prediction and analysis tasks [27]. Most studies focus on analyzing individuals' mental health by examining their emotional sentiments, leveraging sentiment analysis to understand mental health states or detect early signs of mental health disorders [28], [29].\nHowever, to the best of our knowledge, no prior work has explored evaluating students' perceptions of mental health support in colleges using sentiment analysis. The lack of such research leaves a critical gap in understanding how students perceive and engage with the mental health support resources available to them in colleges, which is essential for developing a more effective, student-centered mental health support system for each college. This paper fills this gap by introducing the first sentiment analysis dataset with a specific focus on student perceptions of mental health support in college settings. By creating and analyzing the dataset with LLMs, this work provides a foundation for data-driven evaluation and decision-making and offers insights into students' satisfaction and concerns based on their experiences with available mental health services. More importantly, the dataset will not only support related research into the sentiment analysis associated with mental health support but also has the potential to identify support limitations for actionable strategies to tailor mental health support to students' real needs."}, {"title": "III. THE SMILE-COLLEGE DATASET", "content": "To the best of our knowledge, no existing dataset has been specifically developed for sentiment analysis on mental health support in colleges. This section provides the details of data creation of the SMILE-College dataset for sentiment analysis.\nThis study uses the SVS response data\u00b2 on the current state of mental health designed by College Pulse, to examine the social and emotional well-being of students and gain insights into their attitudes, preferences, and behaviors. The survey, conducted in 2022, comprised 20 questions and was completed by 2,000 undergraduate students from a panel representing over 1,500 colleges and universities across the United States. Our study focuses on the text responses to the question \"What mental health or wellness services and supports provided by your college are working well? What aspects of mental health and wellness need more attention?\""}, {"title": "B. Sentiment Annotation with Human-Machine Collaboration", "content": "Based on the selected data samples, we annotate sentiment labels in a human-machine collaborative manner. While human annotation ensures high accuracy and nuanced understanding, it is costly and time-consuming. Moreover, manually analyzing many samples and defining the appropriate range of sentiment categories becomes challenging. Recently, LLMs offered a scalable solution for annotation [30]. However, there is a significant reduction in performance when transitioning from human labels to LLMs' generated labels due to the inherent noise in the generated labels [31], [32]. Therefore, a viable alternative is to have humans and LLMs work together on this specific annotation task [33].\nDuring our annotation, both LLMs and human annotators contributed unique strengths in a complementary twofold approach. LLMs were first used for quick preliminary analysis, facilitating the identification of sentiment patterns across the entire dataset. By leveraging multiple LLMs, we detected edge cases that suggested the need for an additional sentiment category, enhancing the dataset's granularity. LLMs also helped filter responses with irrelevant or insufficient information, streamlining the annotation process and improving the overall efficiency. Meanwhile, human annotators brought essential depth and contextual understanding of the sentiments, particularly in cases where nuanced interpretation was required. Together, this human-machine collaboration strategy ensured the accuracy and consistency in sentiment annotation and enabled a robust, context-sensitive dataset for analyzing sentiment on mental health support in colleges. The sentiment annotation of SMILE-College can be summarized as the following three steps. \nStep 1. Sentiment Annotation with LLMs. Initially, the number of categories in our data was unclear. To navigate the unstructured nature of the survey responses, we employed Large Language Models (LLMs) to identify response clusters. The goal was to classify the responses into three standard categories: Satisfied (positive class), Dissatisfied (negative class), and Neutral (neutral class). To achieve this, we designed and refined a prompt-engineered approach, leveraging the advanced linguistic capabilities of LLMs. Our strategy involved creating a coarse prompt that consisted of three key components:\n\u2022 Prompt Header: This section contained task-specific instructions, guiding the LLMs to adopt the role of an experienced analyst specializing in mental health text analysis: \u201cYou are a very experienced analyst trying to analyze the answers to a question asked during a mental health survey. No answer will explicitly mention any of the categories. You have to analyze them based on the rules and categorize them in one word, SATISFIED, DISSATISFIED, or NEUTRAL.\u201d\n\u2022 Survey Question and Student Response: This component"}, {"title": "Step 2. Sentiment Category Identification.", "content": "We investigated multiple LLMs using the coarse prompt and found a 50.15% prediction agreement for all the models. Among the agreed records, none were classified as neutral, and 70.7% were classified as dissatisfied. Based on our manual review of the predictions and the cases where model outputs diverged, we observed that a significant proportion of the responses contained both positive and negative sentiments, making it difficult to classify them into one of the standard sentiment categories. For example, consider the response \u201cThe student support team at my college has been very helpful in supporting my mental health. I think more attention should be paid on the negative impact of stress from school has on students' mental health.\" Here, the first sentence expresses satisfaction, while the second reflects dissatisfaction. This blend of sentiments highlights the need for a more nuanced annotation approach, as traditional three sentiment categories may not fully capture the complexity of responses in this context.\nInspired by this insight, we shift to a more detailed analysis by introducing \"Mixed\" as a new sentiment category. This transition marks a significant change from a broad to more nuanced sentiment analysis, allowing for a deeper and more precise understanding of the survey responses through human-machine collaboration."}, {"title": "Step 3. Human Annotation and Validation of Sentiment Labels.", "content": "Based on the four categories of students' sentiments identified in Step 2, we adopt a two-stage human annotation process. The preliminary annotation stage is executed by one annotator, which is subsequently subjected to a validation stage conducted by another annotator. Both annotators are graduate students who are proficient in English. This rigorous process revealed a disagreement rate of 9.98% between the annotations in two phases. These mismatches are resolved collaboratively through collective discussions among the annotators and other researchers involved, leading to a consensus on the final sentiment labels.\nThe specific criteria for the annotation of each category are as follows. (1) \"Satisfied\": at least 75% of the language expressed satisfaction, with minimal suggestions for improvement. (2) \"Dissatisfied\": at least 75% of the language indicated discontent or suggestions for enhancement, with little mention of satisfaction. (3) \u201cMixed\u201d: expressions of satisfaction and dissatisfaction/suggestions were approximately evenly split, with each constituting about 50%. (4) \u201cNeutral\": no clear emphasis on satisfaction, dissatisfaction, or suggestions for improvement. This discourse focuses on mental health in a college context.\nThis human-machine collaboration annotation strategy not only enhances this specific sentiment analysis task but also highlights the importance of combining computational analysis with human insights to capture the intricate emotional nuances within students' responses. We enrich the SVS data with the sentiment labels and obtain the SMILE-College dataset for sentiment analysis."}, {"title": "SMILE-College Data Statistics.", "content": "Following data filtering procedures, 266 distinct colleges/universities were covered within the dataset. Notably, the word count distribution across records ranges from a minimum of 12 words to a maximum of 199 words whereas the sentence count ranges from 1 to 11 sentences."}, {"title": "C. Target Tasks", "content": "To evaluate the usability of the SMILE-College data, we investigated three important tasks, including:\n\u2022 Sentiment prediction (T1): text-based multi-class classification of sentiment labels for students' responses with a task-specific fine-grained prompt for LLMs.\n\u2022 Prediction error analysis (T2): examine the prediction errors of LLMs across different sentiment categories.\n\u2022 Support limitation identification (T3): based on the responses labeled as \"Dissatisfied\", we utilize the capabilities of LLMs, embedding learning, and clustering techniques to pinpoint the main shortcomings in student mental health support in colleges."}, {"title": "IV. EXPERIMENTS", "content": "To investigate the three target tasks listed in Section III.C, we perform sentiment analysis by considering Logistic Regression (LR) and Support Vector Machine (SVM) as baseline models. Subsequently, we also fine-tuned BERT for the specific sentiment prediction task. Additionally, we developed task-specific fine-grained prompts for Large Language Models (LLMs) to predict the sentiment labels of student responses. The four LLMs evaluated include: (1) GPT-3.5 [34], (2) Mistral 7 Billion (8-bit quantization) [35], (3) Llama 27 Billion (8-bit quantization) [36], and (4) Orca 2 7 Billion (8-bit quantization) [37]. LLMs enable the classification of the student responses into varying levels of their satisfaction with the mental health services.\nThe prompt design process is iterative and data-driven to optimize the language models' performance in contextually understanding and analyzing students' survey responses. Following the design and result analysis of the coarse prompt in Section III-B, we develop a fine-grained prompt for sentiment prediction, which consists of the same three components as in the coarse prompt but with four fine-grained sentiment categories (Mixed) and provides specific criteria for each category (see Section III-B).\nSince LR and SVM require a training phase, we randomly split the dataset into train, development, and test with the ratios of 0.75/0.05/0.2 and report the results obtained on the test split. The same data split was used for finetuning BERT. To ensure a fair comparison, results for the four LLMs were also obtained from the same test set and provided . Additionally, we evaluated the performance of the LLMs on the entire SMILE-College dataset ."}, {"title": "B. Experimental Setup", "content": "To evaluate the performance of the models on sentiment prediction, We adopt Precision, Recall, and F1-score to evaluate the performance of sentiment predictions. The higher values of these metrics indicate the better performance of a model. We evaluated the overall performance of all sentiment categories with a weighted evaluation to handle the label imbalance issue in the data and ensure a reasonable consideration of all sentiment categories during the evaluation. We use TensorFlow [38] and the Hugging Face library to implement various language models. Our experiments are conducted on a Nvidia Tesla V100 GPU, equipped with 51GB of RAM and 201.2GB of disk space, which has the necessary computational power. During the inference phase, we experiment with 4 or 8-bit for model quantization [39], and with temperature settings ranging from 0.1 to 0.3 to get the best results."}, {"title": "C. Experimental Results", "content": "Table IV provides the quantitative performance comparison of different models on the test set of the SMILE-College dataset. The best performance of different models is highlighted in bold, while the second-best performance is underlined.\nWe observe from Table IV that overall GPT-3.5 achieves the highest F1 score of 0.80, outperforming other models. Its large size and robust architecture allow it to deliver a balanced performance across all sentiment categories. The second-best performance is observed from the fine-tuned BERT, with an overall F1 score of 0.78. BERT consistently performs well across most categories, particularly in the Dissatisfied and Neutral categories, where it achieves an F1 score of 0.86 and 0.80, respectively. Its encoder-based architecture continues to be effective in capturing contextual relationships, leading to strong results in sentiment classification.\nInterestingly, the other three LLMs, including Mistral, Orca 2, and Llama 2, while more suited for generative tasks, still deliver competitive results when compared to baselines like SVM and LR. This suggests that despite being optimized for text generation, these models exhibit a strong understanding of the contextual intricacies of mental health-related text. For instance, Mistral demonstrates strong recall in the Neutral category (1.00) and Orca 2 exhibits impressive precision in the Dissatisfied (0.95) and Satisfied (0.88) categories. However, Llama 2 underperforms significantly in the Satisfied category, where it fails to produce any meaningful results. This variability suggests that while these models grasp the overall"}, {"title": "2) Prediction Error Analysis with Confusion Matrix (T2):", "content": "Table V and the confusion matrices in Figure 2 provide a deeper understanding of the LLM's performance variations and error patterns. Across the board, GPT-3.5 shows the most balanced distribution of errors, as reflected by its fewer misclassifications between categories. Notably, there is minimal confusion between the Satisfied and Neutral or Mixed categories, a challenge that other models face more frequently. The matrix reveals that GPT-3.5 handles the overlap between sentiments better than others.\nLlama 2 demonstrates the second-best performance in LLMs, excelling in Satisfied sentiment detection with perfect"}, {"title": "3) Support Limitations Identification (T3):", "content": "\u03a4o enhance the well-being of students, it is crucial to carefully examine the areas of college mental health services that need more attention. We employ GPT-3.5 to identify and extract the limitations based on the survey responses labeled as \u201cDissatisfied\u201d in the SMILE-College dataset. After manual verification, we obtain the embeddings of the extracted limitations using the sentence transformer [40] and further cluster them using K-Means [41] to systematically categorize the limitations of college mental health services. The examination of each cluster's content reveals predominant themes and topics, which are systematically detailed in Table VI along with the frequencies of the limitations mentioned in all the survey responses.\nExamining the dataset reveals that the most pressing issue is the quality of counseling services, with 157 mentions,"}, {"title": "V. DISCUSSIONS", "content": "Working with real-world student voice survey data presents unique challenges, especially due to the unstructured and often inconsistent nature of student feedback. Data quality is entirely dependent on the respondents' willingness and seriousness to give answers. The open-ended design and subjective nature of survey questions complicated analysis with their broad range of responses. Additionally, inconsistent text generation from decoder-based LLMs made post-processing difficult, limiting the extraction of consistent insights.\nLeveraging LLMs offers a significant opportunity to shed light on how mental health support structures are perceived within academic institutions. Additionally, LLMs allow for scalable analysis of subjective data and more personalized mental health interventions, helping shape data-driven policies that better meet student needs and enhance overall mental health services. The ability to highlight recurring issues can prompt institutions to make necessary revisions, improving overall mental health support systems for a more inclusive and effective approach. With this initial exploration of student sentiment on mental health support in colleges using LLMs, we hope to inspire further research into leveraging LLMs to advance mental health-related studies.\nIn this work, we prioritized ethical considerations, particularly regarding student privacy and potential bias. The Student Voice Survey (SVS) Data, containing students' feedback on mental health services, was already anonymized and de-identified by College Pulse prior to annotation, ensuring privacy protection. To enhance efficiency and accuracy, we employed LLM-based annotations, which were cross-verified by human annotators from different backgrounds. This multi-layered approach minimized bias and ensured cultural relevance. Additionally, we transparently documented the role of LLMs in the annotation process and used the publicly available, vetted SVS dataset, aligning with ethical standards for privacy, fairness, and responsible AI use in mental health research.\nThis work offers significant potential for advancing real-world practices in survey design and data utilization for mental health research. For example, the insights uncovered through the human-machine collaborative annotation process, such as the insufficient or irrelevant survey responses, and the introduction of the \"Mixed\u201d sentiment category, underscore the critical role of well-designed survey questions in engaging participants effectively and eliciting more structured, informative responses. Additionally, the SMILE-College dataset's relatively small sample size poses challenges to model performance. Expanding the dataset with additional survey responses in future studies could enhance its robustness and generalizability. Future iterations of the SMILE-College dataset could also incorporate richer annotations, capturing specific issues, benefits, and emotional tone. This includes introducing more granular sentiment categories, such as differentiating dissatisfaction (e.g., service quality vs. accessibility) and satisfaction (e.g., effectiveness vs. convenience), to enable deeper analysis."}, {"title": "VI. CONCLUSIONS", "content": "Mental health support in colleges and universities is crucial for fostering students' mental health awareness and well-being. However, its effectiveness is hard to evaluate due to various challenges. This paper utilizes student feedback from a public Student Voice Survey, employing advanced LLMs to analyze students' perceptions of college mental health support. A new SMILE-College dataset is created through human-machine collaboration for sentiment analysis. Three important tasks are investigated on the new data, including sentiment prediction, prediction error analysis, and support limitation identification. Experiments reveal that GPT-3.5 performs the best, followed by BERT, in the sentiment prediction task. Additionally, \u201cQuality of Counseling Services\u201d emerged as the most frequently identified limitations faced by students. This data-driven approach facilitates better mental health support evaluation and decision-making."}]}