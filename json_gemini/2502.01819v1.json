{"title": "Score as Action: Fine-Tuning Diffusion Generative Models by Continuous-time Reinforcement Learning", "authors": ["Hanyang Zhao", "Haoxian Chen", "Ji Zhang", "David D. Yao", "Wenpin Tang"], "abstract": "Reinforcement learning from human feedback (RLHF), which aligns a diffusion model with input prompt, has become a crucial step in building reliable generative AI models. Most works in this area use a discrete-time formulation, which is prone to induced errors, and often not applicable to models with higher-order/black-box solvers. The objective of this study is to develop a disciplined approach to fine-tune diffusion models using continuous-time RL, formulated as a stochastic control problem with a reward function that aligns the end result (terminal state) with input prompt. The key idea is to treat score matching as controls or actions, and thereby making connections to policy optimization and regularization in continuous-time RL. To carry out this idea, we lay out a new policy optimization framework for continuous-time RL, and illustrate its potential in enhancing the value networks design space via leveraging the structural property of diffusion models. We validate the advantages of our method by experiments in downstream tasks of fine-tuning large-scale Text2Image models of Stable Diffusion v1.5.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (Sohl-Dickstein et al., 2015), with the capacity to turn a noisy/non-informative initial distribution into a desired target distribution through a well-designed denoising process (Ho et al., 2020; Song et al., 2020; 2021b), have recently found applications in diverse areas such as high-quality and creative image generation (Ramesh et al., 2022; Shi et al., 2020; Saharia et al., 2022; Rombach et al., 2022), video synthesis (Ho et al., 2022), and drug design (Xu et al., 2022). And, the emergence of human-interactive platforms like ChatGPT (Ouyang et al., 2022) and Stable Diffusion (Rombach et al., 2022) has further increased the demand for diffusion models to align with human preference or feedback.\nTo meet such demands, (Hao et al., 2022) proposed a natural way to fine-tune diffusion models using reinforcement learning (RL, (Sutton & Barto, 2018)). Indeed, RL has already demonstrated empirical successes in enhancing the performance of LLM (large language models) using human feedback (Christiano et al., 2017; Ouyang et al., 2022; Bubeck et al., 2023), and (Fan & Lee, 2023) is among the first to utilize RL-like methods to train diffusion models for better image synthesis. Moreover, (Lee et al., 2023; Fan et al., 2023; Black et al., 2023) have improved the text-to-image (T2I) diffusion model performance by incorporating reward models to align with human preference (e.g., CLIP (Radford et al., 2021), BLIP (Li et al., 2022), ImageReward (Xu et al., 2024)).\nNotably, all studies referenced above that combine diffusion models with RL are formulated as discrete-time sequential optimization problems, such as Markov decision processes (MDPs, (Puterman, 2014)), and solved by discrete-time RL algorithms such as REINFORCE (Sutton et al., 1999) or PPO (Schulman et al., 2017).\nYet, diffusion models are intrinsically continuous-time as they were originally created to model the evolution of thermodynamics (Sohl-Dickstein et al., 2015). Notably, the continuous-time formalism of diffusion models provides a unified framework for various existing discrete-time algorithms as shown in (Song et al., 2021b): the denoising steps in DDPM (Ho et al., 2020) can be viewed as a discrete approximation of a stochastic differential equation (SDE) and are implicitly score-based under a specific variance-preserving SDE (Song et al., 2021b); and DDIM (Song et al., 2020), which underlies the success of Stable Diffusion (Rombach et al., 2022), can also be seen as a numerical integrator of an ODE (ordinary differential equation) sampler (Salimans & Ho, 2022). Awareness of the continuous-time nature informs the design structure of the discrete-time SOTA large-scale T2I generative models (e.g.,(Dhariwal & Nichol, 2021; Rombach et al., 2022; Esser et al., 2024)), and enables simple controllable generations by classifier guidance to solve inverse problems (Song et al., 2021b;a). It also motivates more efficient diffusion models with continuous-time samplers, including the ODE-governed probability (normalizing) flows (Papamakarios et al., 2021; Song et al., 2021b) and rectified flows (Liu et al., 2022; 2023) underpinning Stable Diffusion v3 (Esser et al., 2024). A discrete-time formulation of RL algorithms for fine-tuning diffusion models, if/when directly applied to continuous-time diffusion models via discretization, can nullify the models' continuous nature and fail to capture or utilize their structural properties.\nFor fine-tuning diffusion models, discrete-time RL algorithms (such as DDPO) require a prior chosen time discretization in sampling. We thus examine the robustness of a fine-tuned model to the inference time discretization, and observe an \"overfitting\" phenomenon as illustrated in Figure 1. Specifically, improvements observed during inference at alternative discretization timesteps (25 and 100) are significantly smaller than that of sampling timestep (50) in RL.\nIn addition, for high-order solvers (such as 2nd order Heun in EDM (Karras et al., 2022)), discrete-time RL methods will require solving a high-dimension root-finding problem for each inference step, which is inefficient in practice.\nMain contributions. To address the above issues, we develop a unified continuous-time RL framework to fine-tune score-based diffusion models.\nOur first contribution is a continuous-time RL framework for fine-tuning diffusion models by treating score functions as actions. This framework naturally accommodates discrete-time diffusion models with any solver as well as continuous-time diffusion models, and overcomes the afore-mentioned limitations of discrete-time RL methods. (See Section 3.)\nSecond, we illustrate the promise of leveraging the structural property of diffusion models to generate tractable optimization problems and to enhance the design space of value networks."}, {"title": "1.1. Related Works", "content": "Papers that relate to our work are briefly reviewed below.\nContinuous-time RL. (Wang et al., 2020) models the noise or randomness in the environment dynamics as following an SDE, and incorporates an entropy-based regularizer into the objective function to facilitate the exploration-exploitation tradeoff. Follow-up works include designing model-free methods and algorithms under either finite horizon (Jia & Zhou, 2022a;b;c) or infinite horizon (Zhao et al., 2024b).\nRL for fine-tuning T2I diffusion models. DDPO (Black et al., 2023) and DPOK (Fan et al., 2023) both discrete the time steps and fine-tune large pretrained T2I diffusion models through the reinforcement learning algorithms. Moreover, (Ren et al., 2024) introduces DPPO, a policy gradient-based RL framework for fine-tuning diffusion-based policies in continuous control and robotic tasks.\nOther Preference Optimizations for diffusion models. (Wallace et al., 2024) proposes an adaptation of Direct Preference Optimization (DPO) for aligning T2I diffusion models like Stable Diffusion XL to human preferences. (Yuan et al., 2024) proposes a novel fine-tuning method for diffusion models that iteratively improves model performance through self-play, where a model competes with its previous versions to enhance human preference alignment and visual appeal. See Section 4.5 in (Winata et al., 2024) for a review.\nStochastic Control. (Uehara et al., 2024), which also formulated the diffusion models alignment as a continuous-time stochastic control problem with a different parameterization of the control; (Tang, 2024) also provides a more rigorous review and discussion. (Domingo-Enrich et al., 2024) proposes to use adjoint to solve a similar control problem. In a concurrent work to ours, (Gao et al., 2024) uses q-learning (Jia & Zhou, 2022c) for pretraining diffusion models, which relies on an earlier version (Zhao et al., 2024a) of this paper."}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Continuous-time RL", "content": "Diffusion Process. We consider the state space $R^d$, and denote by $A$ the action space. Let $\\pi(\\cdot | t, x)$ be a feedback policy given $t \\in [0,T]$ and $x \\in R^d$. The state dynamics $(X_t, 0 \\leq t \\leq T)$ is governed by the following SDE:\n$dX_t = b (t, X_t, a_t) dt + \\sigma(t)dB_t, X_0 \\sim p,$ \nwhere $(B_t, t > 0)$ is a $d$-dimensional Brownian motion; $b: R_+ \\times R^d \\times A \\rightarrow R^d$ and $\\sigma : R_+ \\rightarrow R^{d \\times d}$ are given functions; the action $a_t$ follows the distribution $\\pi (\\cdot | t, X_t)$ by external randomization; and $p$ is the initial distribution over the state space.\nPerformance Metric. Our goal is to find the optimal feedback policy $\\pi^*$ that maximizes the expected reward over a finite time horizon:\n$V^*:= \\max_\\pi E\\left[\\int_0^T r (t, X_t, a_t) dt + h(X_T) | X_0 \\sim \\rho\\right],$ \nwhere $r : R_+ \\times R^d \\times A \\rightarrow R$ and $h : R^d \\rightarrow R$ are the running and terminal rewards respectively. Given a policy $\\pi(\\cdot)$, let $b(t, x, \\pi(\\cdot)) := \\int b(t, x, a)\\pi(a)da$. We consider the following equivalent representation of (1):\n$dx_t = \\hat{b} (t, X_t, \\pi(\\cdot | t, X_t)) dt + \\sigma(t)dB_t, X_0 \\sim \\rho,$ \nin the sense that there exists a probability measure $P$ that supports a $d$-dimensional Brownian motion $(B_t, t > 0)$, and for each $t > 0$, the distribution of $X_t$ under $P$ agrees with that of $X_t$ under $P$ defined by (1). Note that the dynamics (3) does not require external randomization. Accordingly, set $r(t, x, \\pi) := \\int_ar(t,x,a)\\pi(a)da$.\nThe value function associated with the feedback policy"}, {"title": "2.2. Score-Based Diffusion Models", "content": "Forward and Backward SDE. We follow the presentation in (Tang & Zhao, 2024). Consider the following SDE that governs the dynamics of a process $(X_t, 0 \\leq t \\leq T)$ in $R^d$ (Song et al., 2021b),\n$dX_t = f(t, X_t)dt + g(t)dB_t, X_0 \\sim P_{data}(\\cdot),$ \nwhere $(B_t, t > 0)$ is a $d$-dimensional Brownian motion, $f : R_+ \\times R^d \\rightarrow R^d$ and $g : R_+ \\rightarrow R_+$ are two given functions (up to the designer to choose), and the initial state $X_0$ follows a distribution with density $P_{data}(\\cdot)$, which is shaped by data yet unknown a priori. Denote by $p_t(\\cdot)$ the probability density of $X_t$.\nRun the SDE in (6) until a given time $T > 0$, to obtain $X_T \\sim p(T,\\cdot)$. Next, consider the \u201ctime reversal\u201d of $X_t$, denoted $X_t^{rev}$, such that the distribution of $X_t^{rev}$ agrees with that of $X_{T-t}$ on $[0, T]$. Then, $(X_t^{rev}, 0 \\leq t \\leq T)$ satisfies"}, {"title": "3. Continuous-time RL for Diffusion Models Fine Tuning", "content": "Here we formulate the task of fine-tuning diffusion models as a continuous-time stochastic control problem. The high-level idea it to treat the score function approximation as a control process applied to the backward SDE.\nScores as Actions. First, to broaden the application context of the diffusion model, we add a parameter $c$ to the score function, interpreted as a \"class\" index or label (e.g., for input prompts). Then, the backward SDE in (9) becomes:\n$dX_t = (- f(T - t, X_t) + g^2(T \u2013 t) \\text{Sopre} (T - t, X_t, c)) dt + g(T - t)dB_t.$\nNext, comparing the continuous RL process in (3) and the inference process (10), we choose $b$ and $\\sigma$ in the RL dynamics in (3) as:\n$\\left\\{\\begin{array}{l} \\sigma(t) := g(T - t), \\\\ b (t, x, a) := - f(T - t,x) + g^2(T \u2013 t)a. \\end{array}\\right.$\nIn the sequel, we will stick to this definition of $b$ and $\\sigma$.\nDefine a specific feedback control, $a_t^{\\text{Opre}} = \\text{Sopre} (T - t, X_t, c)$, and the backward SDE in (10) is expressed as:\n$dX_t = b(t, X_t, a_t^{\\text{Opre}})dt + \\sigma(t)dB_t.$\nThis way, the score function is replaced by the action (or control/policy), and finding the optimal score becomes a policy optimization problem in RL. Denote by $p^{\\text{Opre}} (t,\\cdot, c)$ the probability density of $X_t$ in (12).\nExploratory SDEs. As we will deal with the time-reversed process $X_t^{\\leftarrow}$ exclusively from now on, the superscript $\\leftarrow$ will be dropped to lighten the notation. To enhance exploration, we will use a Gaussian control:\n$a_t^{\\theta} \\sim \\pi^{\\theta} (. \\vert t, X_t, c) = N(\\mu^{\\theta} (t, X_t, c), \\Sigma_t).$\nSpecifically, the dependence on $\\theta$ is through that of the mean function $\\mu^{\\theta}$, while the covariance matrix $\\Sigma_t$ only depends on time $t$, representing a chosen exploration level at $t$. For brevity, write $X_t^{\\theta}$ for the (time-reversed) process $X_t^{\\theta}$ driven by the policy $\\pi^{\\theta}$. Then $(X_t^{\\theta}, 0 \\leq t \\leq T)$ is governed by the SDE:\n$dX_t = [-f(T-t, X_t) + g^2(T \u2013 t)\\mu^{\\theta} (t, X_t, c)] dt + g(T-t)dB_t, X_0 \\sim \\rho.$\nDenote by $p^{\\theta} (t, \\cdot, c)$ the probability density of $X_t$.\nObjective Function. The objective function of the RL problem consists of two parts. The first part is the terminal reward, i.e., a given reward model (RM) that is a function of both $X_T$ and $c$. For instance, if the task is T2I generation, then RM$(X_T, c)$ represents how well the generated image $X_T$ aligns with the input prompt $c$. The second part is a penalty (i.e., regularization) term, which takes the form of the KL divergence between $p^{\\theta}(T,\\cdot, c)$ and its pretrained counterpart. This is similar in spirit to previous works on fine-tuning diffusion models by discrete-time RL, see e.g., (Ouyang et al., 2022; Fan et al., 2023). As for exploration, note that it has been represented by the Gaussian noise in"}, {"title": "4. Continuous-time Policy Optimization", "content": "To efficiently optimize the continuous-time RL problem raised above, we further develop the theory of policy optimization in continuous time and space for fine-tuning diffusion models. Different from the general formalism in the literature (Schulman et al., 2015; Zhao et al., 2024b), we focus on the case of (1) KL regularized rewards, and (2) state-independent diffusion coefficients in the continuous-time setup, which yield new results not only in the analysis but also in the resulting algorithms.\nPolicy Gradient. We first show that the continuous-time policy gradient can be directly computed without any prior discretization of the time variable.\nTheorem 4.1. The gradient of an admissible policy $\\pi^{\\theta}$ parameterized by $\\theta$ takes the form:\n$\\nabla_\\theta V^{\\theta} = E \\left[ \\int_0^T \\nabla_\\theta \\log \\pi^{\\theta} (a_t^{\\ddagger}|t, X_t^{\\ddagger})q(t, X_t^{\\ddagger}, a; \\pi^{\\theta})dt\\right]$ \nwhere $\\pi^{\\theta}$, $a_t^{\\ddagger}$ and $q$ are as defined in (13) and (5).\nNote that the only terms in the q-value function that involve action a are (the second order term is irrelevant to action a):\n$\\frac{\\partial V}{\\partial a}(T,x) = g^2(T \u2013 t)a - \\frac{\\partial V}{\\partial x}(t, x) =: q^{\\theta} (t, x, a).$\nIn addition, the value function approximation can be computed by Monte Carlo or the martingale approach as in Jia & Zhou (2022a), and then $\\frac{\\partial V}{\\partial x}$ can be evaluated by backward propagation. Since the reward can be non-differentiable, and also for the sake of efficient computation, we can approximate $q^{\\theta} (t, x,a) \\approx (V(t, x + \\sigma g^2(T \u2013 t)a) \u2013 V(t, x)) /\\sigma$, where $\\sigma$ is a scaling parameter.\nContinuous-time TRPO/PPO. We also derive the continuous-time finite horizon analogies of TRPO and PPO for the discrete RL in the finite horizon setting (Schulman et al., 2015; 2017), and in the continuous-time infinite horizon setup (Zhao et al., 2024b). The Performance Difference Lemma (PDL) is as follows."}]}