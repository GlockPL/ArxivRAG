[{"title": "Score as Action: Fine-Tuning Diffusion Generative Models by Continuous-time Reinforcement Learning", "authors": ["Hanyang Zhao", "Haoxian Chen", "Ji Zhang", "David D. Yao", "Wenpin Tang"], "abstract": "Reinforcement learning from human feedback (RLHF), which aligns a diffusion model with input prompt, has become a crucial step in building reliable generative AI models. Most works in this area use a discrete-time formulation, which is prone to induced errors, and often not applicable to models with higher-order/black-box solvers. The objective of this study is to develop a disciplined approach to fine-tune diffusion models using continuous-time RL, formulated as a stochastic control problem with a reward function that aligns the end result (terminal state) with input prompt. The key idea is to treat score matching as controls or actions, and thereby making connections to policy optimization and regularization in continuous-time RL. To carry out this idea, we lay out a new policy optimization framework for continuous-time RL, and illustrate its potential in enhancing the value networks design space via leveraging the structural property of diffusion models. We validate the advantages of our method by experiments in downstream tasks of fine-tuning large-scale Text2Image models of Stable Diffusion v1.5.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (Sohl-Dickstein et al., 2015), with the capacity to turn a noisy/non-informative initial distribution into a desired target distribution through a well-designed denoising process (Ho et al., 2020; Song et al., 2020; 2021b), have recently found applications in diverse areas such as high-quality and creative image generation (Ramesh et al., 2022; Shi et al., 2020; Saharia et al., 2022; Rombach et al., 2022), video synthesis (Ho et al., 2022), and drug design (Xu et al., 2022). And, the emergence of human-interactive platforms like ChatGPT (Ouyang et al., 2022) and Stable Diffusion (Rombach et al., 2022) has further increased the demand for diffusion models to align with human preference or feedback.\nTo meet such demands, (Hao et al., 2022) proposed a natural way to fine-tune diffusion models using reinforcement learning (RL, (Sutton & Barto, 2018)). Indeed, RL has already demonstrated empirical successes in enhancing the performance of LLM (large language models) using human feedback (Christiano et al., 2017; Ouyang et al., 2022; Bubeck et al., 2023), and (Fan & Lee, 2023) is among the first to utilize RL-like methods to train diffusion models for better image synthesis. Moreover, (Lee et al., 2023; Fan et al., 2023; Black et al., 2023) have improved the text-to-image (T2I) diffusion model performance by incorporating reward models to align with human preference (e.g., CLIP (Radford et al., 2021), BLIP (Li et al., 2022), ImageReward (Xu et al., 2024)).\nNotably, all studies referenced above that combine diffusion models with RL are formulated as discrete-time sequential optimization problems, such as Markov decision processes (MDPs, (Puterman, 2014)), and solved by discrete-time RL algorithms such as REINFORCE (Sutton et al., 1999) or PPO (Schulman et al., 2017).\nYet, diffusion models are intrinsically continuous-time as they were originally created to model the evolution of thermodynamics (Sohl-Dickstein et al., 2015). Notably, the continuous-time formalism of diffusion models provides a unified framework for various existing discrete-time algorithms as shown in (Song et al., 2021b): the denoising steps in DDPM (Ho et al., 2020) can be viewed as a discrete approximation of a stochastic differential equation (SDE) and are implicitly score-based under a specific variance-preserving SDE (Song et al., 2021b); and DDIM (Song et al., 2020), which underlies the success of Stable Diffusion (Rombach et al., 2022), can also be seen as a numerical integrator of an ODE (ordinary differential equation) sampler (Salimans & Ho, 2022). Awareness of the continuous-time nature informs the design structure of the discrete-time SOTA large-scale T2I generative models (e.g.,(Dhariwal & Nichol, 2021; Rombach et al., 2022; Esser et al., 2024)), and enables simple controllable generations by classifier guidance to solve inverse problems (Song et al., 2021b;a). It also motivates more efficient diffusion models with continuous-time samplers, including the ODE-governed probability (normalizing) flows (Papamakarios et al., 2021; Song et al., 2021b) and rectified flows (Liu et al., 2022; 2023) underpinning Stable Diffusion v3 (Esser et al., 2024). A discrete-time formulation of RL algorithms for fine-tuning diffusion models, if/when directly applied to continuous-time diffusion models via discretization, can nullify the models' continuous nature and fail to capture or utilize their structural properties.\nFor fine-tuning diffusion models, discrete-time RL algorithms (such as DDPO) require a prior chosen time discretization in sampling. We thus examine the robustness of a fine-tuned model to the inference time discretization, and observe an \"overfitting\" phenomenon as illustrated in Figure 1. Specifically, improvements observed during inference at alternative discretization timesteps (25 and 100) are significantly smaller than that of sampling timestep (50) in RL.\nIn addition, for high-order solvers (such as 2nd order Heun in EDM (Karras et al., 2022)), discrete-time RL methods will require solving a high-dimension root-finding problem for each inference step, which is inefficient in practice.\nMain contributions. To address the above issues, we develop a unified continuous-time RL framework to fine-tune score-based diffusion models.\nOur first contribution is a continuous-time RL framework for fine-tuning diffusion models by treating score functions as actions. This framework naturally accommodates discrete-time diffusion models with any solver as well as continuous-time diffusion models, and overcomes the afore-mentioned limitations of discrete-time RL methods. (See Section 3.)\nSecond, we illustrate the promise of leveraging the structural property of diffusion models to generate tractable optimization problems and to enhance the design space of value"}, {"title": "1.1. Related Works", "content": "Papers that relate to our work are briefly reviewed below.\nContinuous-time RL. (Wang et al., 2020) models the noise or randomness in the environment dynamics as following an SDE, and incorporates an entropy-based regularizer into the objective function to facilitate the exploration-exploitation tradeoff. Follow-up works include designing model-free methods and algorithms under either finite horizon (Jia & Zhou, 2022a;b;c) or infinite horizon (Zhao et al., 2024b).\nRL for fine-tuning T2I diffusion models. DDPO (Black et al., 2023) and DPOK (Fan et al., 2023) both discrete the time steps and fine-tune large pretrained T2I diffusion models through the reinforcement learning algorithms. Moreover, (Ren et al., 2024) introduces DPPO, a policy gradient-based RL framework for fine-tuning diffusion-based policies in continuous control and robotic tasks.\nOther Preference Optimizations for diffusion models. (Wallace et al., 2024) proposes an adaptation of Direct Preference Optimization (DPO) for aligning T2I diffusion models like Stable Diffusion XL to human preferences. (Yuan et al., 2024) proposes a novel fine-tuning method for diffusion models that iteratively improves model performance through self-play, where a model competes with its previous versions to enhance human preference alignment and visual appeal. See Section 4.5 in (Winata et al., 2024) for a review.\nStochastic Control. (Uehara et al., 2024), which also formulated the diffusion models alignment as a continuous-time stochastic control problem with a different parameterization of the control; (Tang, 2024) also provides a more rigorous review and discussion. (Domingo-Enrich et al., 2024) proposes to use adjoint to solve a similar control problem. In a concurrent work to ours, (Gao et al., 2024) uses q-learning (Jia & Zhou, 2022c) for pretraining diffusion models, which relies on an earlier version (Zhao et al., 2024a) of this paper."}, {"title": "2. Preliminaries", "content": "2.1. Continuous-time RL\nDiffusion Process. We consider the state space $R^d$, and denote by $A$ the action space. Let $\\pi(\\cdot | t, x)$ be a feedback policy given $t \\in [0,T]$ and $x \\in R^d$. The state dynamics $(X_t, 0 \\leq t \\leq T)$ is governed by the following SDE:\n$dX_t = b (t, X_t, a_t) dt + \\sigma(t)dB_t, X_0 \\sim p_0,$ (1)\nwhere $(B_t, t > 0)$ is a $d$-dimensional Brownian motion; $b: R_+ \\times R^d \\times A \\rightarrow R^d$ and $\\sigma :R_+ \\rightarrow R_+ $ are given functions; the action $a_t$ follows the distribution $\\pi (\\cdot | t, X_t)$ by external randomization; and $p$ is the initial distribution over the state space.\nPerformance Metric. Our goal is to find the optimal feedback policy $\\pi^*$ that maximizes the expected reward over a finite time horizon:\n$V^*:= \\max_{\\pi} E \\bigg[\\int_0^T r (t, X_t, a_t) dt +h(X_T) \\bigg| X_0 \\sim p \\bigg],$ (2)\nwhere $r : R_+ \\times R^d \\times A \\rightarrow R$ and $h : R^d \\rightarrow R$ are the running and terminal rewards respectively. Given a policy $\\pi(\\cdot)$, let $b(t, x, \\pi(\\cdot)) := \\int b(t, x, a)\\pi(a)da$. We consider the following equivalent representation of (1):\n$dx = b (t, X_t, \\pi(. \\vert t, X_t)) dt + \\sigma(t)dB_t, X_0 \\sim p,$ (3)\nin the sense that there exists a probability measure $P$ that supports a $d$-dimensional Brownian motion $(B_t, t > 0)$, and for each $t > 0$, the distribution of $X_t$ under $P$ agrees with that of $X_t$ under $P$ defined by (1). Note that the dynamics (3) does not require external randomization. Accordingly, set $r(t, x, \\pi) := \\int_ar(t,x,a)\\pi(a)da$.\nThe value function associated with the feedback policy"}, {"title": "2.2. Score-Based Diffusion Models", "content": "Forward and Backward SDE. We follow the presentation in (Tang & Zhao, 2024). Consider the following SDE that governs the dynamics of a process $(X_t, 0 \\leq t \\leq T)$ in $R^d$ (Song et al., 2021b),\n$dX_t = f(t, X_t)dt + g(t)dB_t, X_0 \\sim P_{data}(\\cdot),$ (6)\nwhere $(B_t, t > 0)$ is a $d$-dimensional Brownian motion, $f : R_+ \\times R^d \\rightarrow R^d$ and $g : R_+ \\rightarrow R_+ $ are two given functions (up to the designer to choose), and the initial state $X_0$ follows a distribution with density $P_{data}(\\cdot)$, which is shaped by data yet unknown a priori. Denote by $p_t(\\cdot)$ the probability density of $X_t$.\nRun the SDE in (6) until a given time $T > 0$, to obtain $X_T \\sim p(T,\\cdot)$. Next, consider the \u201ctime reversal\u201d of $X_t$, denoted $X^{rev}$, such that the distribution of $X^{rev}_t$ agrees with that of $X_{T-t}$ on $[0, T]$. Then, $(X^{rev}_t, 0 \\leq t \\leq T)$ satisfies the following SDE under mild conditions on $f$ and $g$:\n$dX_t^{rev} = (-f(T - t, X_t^{rev}) + g^2 (T \u2013 t)\\nabla \\log p_{T-t}(X_t^{rev})) dt + g(T-t)dB_t,$ (7)\nwhere $\\nabla \\log p_t(x)$ is known as Stein's score function. Below we will refer to the two SDE's in (6) and (7), respectively, as the forward and the backward SDE.\nFor sampling from the backward SDE, we replace $p_T(\\cdot)$ with some $p_{noise}(\\cdot)$ as an approximation. The initialization $p_{noise}(\\cdot)$ is commonly independent of $p_{data}(\\cdot)$, which is the reason why diffusion models are known for generating data from \u201cnoise\u201d.\nScore Matching. Since the score function $\\nabla \\log p_t(x)$ in (7) is unknown, the idea is to learn the score $s_{\\theta}(t, x) \\approx \\nabla_x \\log p_t(x)$, which is often referred to as pretraining. It boils down to solving the following denoising score matching (DSM) problem (Vincent, 2011) 2:\n$J_{DSM}(\\theta) = E \\bigg[\\lambda(t) \\Vert s_{\\theta}(t, x_t) - \\nabla_x \\log p_t(x_t|x_0)\\Vert^2\\bigg],$ (8)\nwhere $x_t \\sim p_t(\\cdot|x_0)$ and $\\lambda: [0,T] \\rightarrow R_{>0}$ is a chosen positive-valued weight function.\nInference Process. Once the best approximation $s_{\\theta}$ is obtained, we use it to replace $\\nabla \\log p_t(x)$ in (7). The corresponding approximation to the reversed process $X^{rev}$, denoted as $X$, then follows the SDE:\n$dX_t = (- f(T \u2013 t, X_t) + g^2 (T - t)s_{\\theta}(T \u2013 t, X_t)) dt + g(T - t)dB_t,$ (9)\nwith $X_T \\sim p_{noise}(\\cdot)$. At time $t = T$, the distribution of $X$ is expected to be close to $p_{data}(\\cdot)$. The well-known DDPM (Ho et al., 2020) can be viewed as a discretized version of the SDE in (9). This has been established in (Song et al., 2021b; Salimans & Ho, 2022; Zhang & Chen, 2022; Zhang et al., 2022); also refer to further discussions in Appendix A. Throughout the rest of the paper, we will focus on the continuous formalism (via SDE)."}, {"title": "3. Continuous-time RL for Diffusion Models Fine Tuning", "content": "Here we formulate the task of fine-tuning diffusion models as a continuous-time stochastic control problem. The high-level idea it to treat the score function approximation as a control process applied to the backward SDE.\nScores as Actions. First, to broaden the application context of the diffusion model, we add a parameter $c$ to the score function, interpreted as a \"class\" index or label (e.g., for input prompts). Then, the backward SDE in (9) becomes:\n$dX_t = (\u2212 f(T - t, X_t) + g^2(T \u2013 t) s_{\\theta_{pre}} (T - t, X_t, c)) dt + g(T - t)dB_t.$ (10)\nNext, comparing the continuous RL process in (3) and the inference process (10), we choose $b$ and $\\sigma$ in the RL dynamics in (3) as:\n$\\begin{cases} \\sigma(t) := g(T - t), \\\\ b (t, x, a) := \u2212 f(T - t,x) + g^2(T \u2013 t)a. \\end{cases}$ (11)\nIn the sequel, we will stick to this definition of $b$ and $\\sigma$.\nDefine a specific feedback control, $a_t = s_{\\theta_{pre}} (T - t, X_t, c)$, and the backward SDE in (10) is expressed as:\n$dX_t = b (t, X_t, a_t) dt + \\sigma(t)dB_t.$ (12)\nThis way, the score function is replaced by the action (or control/policy), and finding the optimal score becomes a policy optimization problem in RL. Denote by $p_{\\theta_{pre}} (t,\\cdot, c)$ the probability density of $X_t$ in (12).\nExploratory SDEs. As we will deal with the time-reversed process $X$ exclusively from now on, the superscript \u2190 will be dropped to lighten the notation. To enhance exploration, we will use a Gaussian control:\n$a^{\\ddagger} \\sim \\pi^{\\theta} (. \\vert t, X_t, c) = N(\\mu^{\\theta} (t, X_t, c), \\Sigma_t).$ (13)\nSpecifically, the dependence on $\\theta$ is through that of the mean function $\\mu^{\\theta}$, while the covariance matrix $\\Sigma_t$ only depends on time $t$, representing a chosen exploration level at $t$. For brevity, write $X^\\theta$ for the (time-reversed) process $X_t$ driven by the policy $\\pi^\\theta$. Then $(X^\\theta_t, 0 \\leq t \\leq T)$ is governed by the SDE:\n$dX_t = [-f(T-t, X^\\theta_t) + g^2(T \u2013 t)\\mu^{\\theta} (t, X_t, c)] dt + g(T-t)dB_t, X_0 \\sim p_0.$ (14)\nDenote by $p^{\\theta} (t, \\cdot, c)$ the probability density of $X$.\nObjective Function. The objective function of the RL problem consists of two parts. The first part is the terminal reward, i.e., a given reward model (RM) that is a function of both $X_T$ and $c$. For instance, if the task is T2I generation, then RM($X_T$, c) represents how well the generated image $X_T$ aligns with the input prompt $c$. The second part is a penalty (i.e., regularization) term, which takes the form of the KL divergence between $p^{\\theta}(T,\\cdot, c)$ and its pretrained counterpart. This is similar in spirit to previous works on fine-tuning diffusion models by discrete-time RL, see e.g., (Ouyang et al., 2022; Fan et al., 2023). As for exploration, note that it has been represented by the Gaussian noise in a\u2021; refer to (13), and more on this below. So, here is the problem we want to solve:\n$\\max_{\\theta} E \\bigg[RM(c, X^\\theta_T) \u2013 \\beta KL (p^{\\theta} (T, \\cdot, c)\\Vert p_{\\theta_{pre}} (T, \\cdot, c))\\bigg],$ (15)\nwhere $\\beta > 0$ is a (given) penalty cost.\nTo connect the problem in (15) to the objective function of the RL model in (2), we need the following explicit expression for the KL divergence term in (15).\nTheorem 3.1. For any given $c$, the KL divergence between $p^{\\theta}$ and $p_{\\theta_{pre}}$ is:\n$KL(p^{\\theta} (T,\\cdot, c) \\Vert p_{\\theta_{pre}} (T, \\cdot, c)) \\newline = E \\bigg[\\int_0^T \\frac{g^2 (T - t)}{2} \\Vert \\mu^{\\theta} (t, X_t, c) \u2013 \\mu_{\\theta_{pre}} (t, X_t, c) \\Vert^2dt\\bigg].$ (16)\nProof Sketch. The full proof is given in Appendix B.1.\nAs a remark, it is important to use the \u201creverse", "following": "n$\\eta^{\\theta} :=E \\bigg[ \\frac{-\\beta}{2} \\int_0^T g^2(T-t) \\Vert \\mu^{\\theta} (t, X_t, c) \u2013 \\mu_{\\theta_{pre}} (t, X_t, c) \\Vert^2 dt \\bigg] + E\\bigg[RM(X_T^\\theta, c)\\bigg],$ (17)\nwhere we abbreviate $\\mu^{\\theta} (t, X_t, c)$ and $\\mu_{\\theta_{pre}} (t, X_t, c)$ by $\\mu$ and $\\mu_{\\theta_{pre}}$ respectively. Thus, maximizing the objective function in (15) aligns with the RL model formulated in (2). We can also define the corresponding value function as:\n$V^{\\theta} (t, x; c) =E \\bigg[\\int_t^T \\frac{g^2(T-t)}{2} \\Vert \\mu \u2013 \\mu_{\\theta_{pre}} \\Vert^2dt +RM(X_T^\\theta, c) \\bigg| X_t^\\theta = x\\bigg],$ (18)\nValue Network Design. We also adopt a function approximation to learn the value function (i.e., the critic). For the value function $V^{\\theta} (t, x; c)$ associated with policy $\\pi^{\\theta}$, there is the boundary condition:\n$V^{\\theta} (T, x; c) = E \\bigg[RM(X_T^\\theta, c) \\bigg| X_T^\\theta = x\\bigg] = RM(x, c).$ (19)\nTo meet this condition, we propose the following parametrization that leverages the structural property of diffusion models:\n$V^{\\theta} (t, x; c) \\approx V(t, x; c) := C_{skip}(t) RM(\\tilde{x}_{\\theta}(t, x, c)) + C_{out}(t) \\cdot F_{\\varphi}(t, x, c),$ (20)\nwhere $V$ denotes a family of functions parameterized by $(\\theta, \\varphi)$, and\n$\\tilde{x}_{\\theta}(t, x, c) = \\frac{1}{\\alpha_t} (\\sigma_t s_{\\theta}(t, x, c) + x),$ (21)\nwith $\\alpha_t$ and $\\sigma_t$ being noise schedules of diffusion models (see Appendix A.2 for details). When $\\theta = \\theta_{pre}, \\tilde{x}_{\\theta}$ predicts a denoised sample given the current $x$ and the score estimate $s_{\\theta}(t, x, c)$, which is known as Tweedie's formula. To treat the second term in (18), our intuition comes from that\n$RM(E(X_T \\vert X_t)) \\approx E(RM(X_T) \\vert X_t),$ (22)\nif we are allowed to exchange the conditional expectation and the reward model score (though generally it's not true). $F_{\\varphi}(t, x, c)$ are effectively approximations to the residual term, which can be seen as a composition of the possible reward error and the first term in (18).\nWe refer these two parts to as reward mean predictor and residual corrector. There $C_{skip}(t)$ and $C_{out}(t)$ are differentiable functions such that $C_{skip}(T) = 1$ and $C_{out}(T) = 0$, so the boundary condition (19) is satisfied. Notably, similar parametrization trick has also been used to train successful diffusion models such as EDM (Karras et al., 2022) and consistency models (Song et al., 2023).\nFor learning the value function, we use trajectory-wise Monte Carlo estimation to update $\\varphi$ by minimizing the mean square error (MSE). In our experiments, we observe that choosing $C_{skip}(t) = cos(2\\pit)$ and $C_{out}(t) = sin(2\\pit)$ yields the smallest loss (see Table 1). Also refer to Section 5.2 for more architecture details."}, {"title": "4. Continuous-time Policy Optimization", "content": "To efficiently optimize the continuous-time RL problem raised above, we further develop the theory of policy optimization in continuous time and space for fine-tuning diffusion models. Different from the general formalism in the literature (Schulman et al., 2015; Zhao et al., 2024b), we focus on the case of (1) KL regularized rewards, and (2) state-independent diffusion coefficients in the continuous-time setup, which yield new results not only in the analysis but also in the resulting algorithms.\nPolicy Gradient. We first show that the continuous-time policy gradient can be directly computed without any prior discretization of the time variable.\nTheorem 4.1. The gradient of an admissible policy $\\pi^\\theta$ parameterized by $\\theta$ takes the form:\n$\\nabla_{\\theta}V^{\\theta} \\Vert_{\\theta = \\hat{\\theta}} = E \\bigg[\\int_0^T \\nabla_{\\theta} \\log \\pi^{\\theta} (a^{\\ddagger} \\vert t, X_t^\\ddagger)q(t, X_t^\\ddagger, a_t^\\ddagger; \\pi^{\\theta})\\bigg],$ (23)\nwhere $\\pi^{\\theta}$, $a^{\\ddagger}$ and $q$ are as defined in (13) and (5).\nProof Sketch. The full proof is given in Appendix B.2.\nNote that the only terms in the q-value function that involve action $a$ are (the second order term is irrelevant to action $a$):\n$\\frac{\\partial V}{\\partial a} q^{\\theta}(T, x, a) = \\frac{\\partial V}{\\partial x} g^2 (T-t) \\frac{\\partial}{\\partial a} a =: q^{\\theta} (t, x, a).$\nIn addition, the value function approximation can be computed by Monte Carlo or the martingale approach as in Jia & Zhou (2022a), and then $\\frac{\\partial V}{\\partial x}$ can be evaluated by backward propagation. Since the reward can be non-differentiable, and also for the sake of efficient computation, we can approximate $q^{\\theta} (t, x,a) \\approx (V(t, x + \\sigma g^2(T \u2013 t)a) \u2013 V(t, x)) /\\sigma$, where $\\sigma$ is a scaling parameter.\nContinuous-time TRPO/PPO. We also derive the continuous-time finite horizon analogies of TRPO and PPO for the discrete RL in the finite horizon setting (Schulman et al., 2015; 2017), and in the continuous-time infinite horizon setup (Zhao et al., 2024b). The Performance Difference Lemma (PDL) is as follows."}, {"title": "5. Experiments", "content": "5.1. Enhancing Small-Steps Diffusion Models\nSetup. We evaluate the ability of our proposed algorithm to train short-run diffusion models with significantly reduced generation steps $T$, while maintaining high sample quality. In the experiment, we take $T = 10$. Our experiments are conducted on the CIFAR-10 (32\u00d732) dataset (Krizhevsky et al., 2009). We fine-tune pretrained diffusion model backbone using DDPM (Ho et al., 2020). The primary evaluation metric is the Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017), which measures the quality of generated samples.\nTo benchmark our method, we compare it against DxMI (Yoon et al., 2024), which formulates the diffusion model training as an inverse reinforcement learning (IRL) problem. DxMI jointly trains a diffusion model and an energy-based model (EBM), where the EBM estimates the log data density and provides a reward signal to guide the diffusion process. To ensure a fair comparison, we replace the policy improvement step in DxMI with our continuous-time RL counterpart, maintaining consistency while evaluating the effectiveness of our approach. We set the learning rate of the value network to 2 \u00d7 10-5 and U-net to 3 x 10-7.\nResult. Figure 3 shows our approach converges significantly faster than DxMI, and achieves consistently lower FID scores throughout training. The samples from the two fine-tuned models are shown in Figures 4 and 5. In comparison, the samples generated from the model fine-tuned by continuous-time RL have clearer contours, better aligned with real-world features, and exhibit superior aesthetic quality."}, {"title": "5.2. Fine-Tuning Stable Diffusion", "content": "Setup. We also validate our proposed algorithm for fine-tuning large-scale T2I diffusion models, Stable Diffusion v1.5 3. We adopt the pretrained ImageReward (Xu et al., 2024) as the reward signal during RL, as it has been shown in previous studies to achieve better alignment with human preferences to other metrics such as aesthetic scores, CLIP and BLIP scores.\nWe train the value networks with full parameter tuning, while we use LoRA (Hu et al., 2021) for tuning the U-nets of diffusion models. We adopt a learning rate of 10-7 for optimizing the value network, 3 \u00d7 10-5 for optimizing the U-net and $\\beta = 5 \u00d7 10-5$ for regularization. We train the models on 8 H200 GPUs with 128 effective batch sizes.\nValue Network Architecture. Since we fix the reward model as ImageReward, we design the value network by using a similar backbone to the ImageReward model, which"}, {"title": "6. Discussion and Conclusion", "content": "We have proposed in this study a continuous-time reinforcement learning (RL) framework for fine-tuning diffusion models. Our work introduces novel policy optimization theory for RL in continuous time and space, alongside a scalable and effective RL algorithm that enhances the generation quality of diffusion models, as validated by our experiments.\nIn addition, our algorithm and network designs exhibit a striking versatility that allows us to incorporate and leverage some of the advantages of prior works in fine-tuning diffusion models, so as to better exploit model structures and to improve value network architectures. In view of this, we believe the continuous-time RL, in providing cross-pollination between diffusion models and RLHF, presents a highly promising direction for future research."}, {"title": "A. Connection between discrete-time and continuous-time sampler", "content": "In this section, we summarize the discussion of popular samplers like DDPM, DDIM, stochastic DDIM and their continuous-time limits being a Variance Preserving (VP) SDE."}, {"title": "A.1. DDPM sampler is the discretization of VP-SDE", "content": "We review the forward and backward process in DDPM", "that": "n$x_i = \\sqrt{1 - \\beta_i"}, "x_{i-1} + \\sqrt{\\beta_i}z_{i-1}, i = 1,\\ldots, N,$ (28)\nwhere $z_{i-1} \\sim N(0, I)$, thus $p (x_i | x_{i-1}) = N (x_i; \\sqrt{1 \u2013 \\beta_i}x_{i-1}, \\beta_iI)$. We can further think of $x_i$ as the $i^{th}$ point of a uniform discretization of time interval $[0, T"], "beta": [0, "T"], "exists": "beta(t) = \\lim_{\\Delta t \\rightarrow 0"}, {"to": "n$dX_t = -\\frac{1"}, {"have": "n$f(t", "bar{a}_i": "prod_{j=1}^{i} (1 \u2013 \\beta_j)$. For the backward process, a variational Markov chain in the reverse direction is"}]