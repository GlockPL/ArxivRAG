{"title": "Real Time Control of Tandem-Wing Experimental Platform Using Concerto Reinforcement Learning", "authors": ["Zhang Minghao", "Yang Xiaojun", "Wang Zhihe", "Wang Liang"], "abstract": "This paper introduces the CRL2RT algorithm, an advanced reinforcement learning method aimed at improving the real-time control performance of the Direct-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly flight, DDTWEP's tandem wing structure causes nonlinear and unsteady aerodynamic interactions, leading to complex load behaviors during pitch, roll, and yaw maneuvers. These complexities challenge stable motion control at high frequencies (2000 Hz). To overcome these issues, we developed the CRL2RT algorithm, which combines classical control elements with reinforcement learning-based controllers using a time-interleaved architecture and a rule-based policy composer. This integration ensures finite-time convergence and single-life adaptability. Experimental results under various conditions, including different flapping frequencies and yaw disturbances, show that CRL2RT achieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally, when integrated with classical controllers like PID, Adaptive PID, and Model Reference Adaptive Control (MRAC), CRL2RT enhances tracking performance by 18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and superior performance in complex real-time control scenarios, validating its effectiveness in overcoming existing control strategy limitations and advancing robust, efficient real-time control for biomimetic aerial vehicles.", "sections": [{"title": "I. Nomenclature", "content": "the expected flapping angle position of the i-th wing for the next n steps\n\u0424\u0435\u0445\u0440,\u0456\nAi = the desired flapping amplitude for the i-th wing\nf = the flapping frequency\n\u03a6\u03af = the phase difference of the i-th wing\nJW.YY = moment of inertia about the Y-axis, representing the wing's rotational inertia when rotating around its own Y-axis\nJW,YZ = product of inertia, representing the coupling of inertia between the Y-axis and Z-axis, which reflects the asymmetry of the wing and the complexity of the inertia tensor\nJw.zz = moment of inertia about the Z-axis, representing the wing's rotational inertia when rotating around its own Z-axis\nTy,wing,w,i = the torque around the Yw,i axis experienced by the wing surface, generated based on the aerodynamic force of the single wing\nTz,wing,w,i = the torque around the Zwi axis experienced by the wing surface, generated based on the aerodynamic force of the single wing\n= the influence coefficient of the interference from tandem wings on the i \u2013 th wing. Tytm,i is the wing\nCtandem,i\nmembrane constraint torque of the i-th wing\nTYAW,i = the additional yaw torque"}, {"title": "II. Introduction", "content": "Recent advancements in motor technology and fabrication techniques, have significantly enhanced the performance of hover-capable flapping-wing aircraft, thereby demonstrating greater application flexibility[1-7]. Dragonfly-inspired hover-capable flapping-wing aircraft utilize a unique four-wing independent drive mechanism, enhancing maneuverability[8-11], Consequently, various types of dragonfly-inspired aircraft have been developed in recent years, including those employing mechanical structures to generate the reciprocating motions necessary for lift and asymmetric wing movements for control torques[12-14], as well as direct-drive aircraft utilizing miniature servo motors to simultaneously achieve reciprocating motions for lift and asymmetric wing movements for control torques[8, 15]. Among these, direct-drive biomimetic aircraft, with control architectures and manipulations more akin to conventional robotics[16] and leveraging direct-drive characteristics[17-20] for improved performance, have attracted significant research interest[10, 21, 22]. A typical example is the DDD-1 aircraft, developed by the authors' team and illustrated in Fig.1[9, 10, 22-25]. This platform faces significant challenges due to nonlinear, unsteady aerodynamic interactions resulting from its tandem wings[9, 10, 25]. While sufficient lift is generated to enable vertical motion along a track, achieving stable hovering remains challenging owing to the need for more sophisticated control strategies in the presence of additional aerodynamic interference from closely spaced tandem wings compared to direct-drive dual-wing aircraft. To address this issue and maintain similarity with the DDD-1 while circumventing the limitations that existing experiments cannot directly apply results to airborne biomimetic aircraft[26, 27], the Direct-Drive Tandem-Wing Experiment Platform (DDTWEP), as shown in Fig.2, equipped with a six-component balance, has been developed to explore the pitch, roll, and yaw control strategies of four-wing direct-drive biomimetic aircraft under the nonlinear and unsteady aerodynamic interference of tandem wings.\nFrom the analysis above, a critical challenge for the DDTWEP lies in controlling the mechanical systems under unknown nonlinear and unsteady dynamics in pitch, roll, and yaw operating conditions. Existing research[28-31] frames this problem as a plug-and-play, fully on-the-job, finite-time single-life learning issue, emphasizing safety and efficiency. Additionally, due to the requirement for flapping frequencies to reach 60 Hz to match biological standards[8, 10, 11, 32], the control frequency is set to 2000 Hz[8] to ensure high-precision control, making real-time control issues non-negligible."}, {"title": "A. RELATED WORK", "content": "1) Traditional Control Algorithms in Mechanical Systems\nExtensive research has focused on controlling mechanical systems using traditional algorithms.\nBalakrishnan and Gurfil[33] designed feedback control strategies for low-thrust orbital rendezvous by combining sliding mode and bang-bang control to achieve finite-time stability. Although effective in specific scenarios, this approach relies on simplified assumptions, limiting its adaptability in unpredictable environments.\nJiao et al.[34] proposed an incremental Model Predictive Control (MPC) method to suppress nonlinear pilot-induced oscillations, thereby enhancing flight stability through adaptive control. However, the method's dependence on precise system modeling and high computational demands constrains its real-time adaptability under rapidly changing conditions.\nTsuruta et al. [35] provided theoretical insights into stability in low-thrust orbital contexts by analyzing equilibrium dynamics with the Euler-Lagrange equations. Nonetheless, the reliance on exact mathematical models restricts this method's applicability in environments with unmodeled disturbances.\nBeyer et al.[36] enhanced incremental nonlinear dynamic inversion (INLDI) for gust load alleviation in aircraft, reducing structural loads under gust conditions. This approach, however, depends on precise physical models and lacks flexibility in variable environments.\nIn research directly related to biomimetic aircraft with direct-drive systems, He Ma et al. [8] implemented a 2000 Hz PID controller with manually tuned parameters for dragonfly-inspired aircraft trajectory control, while Xinyan Deng et al.[37] applied Adaptive Robust Control for trajectory tracking in direct-drive aircraft.\nTraditional control algorithms, though straightforward, require extensive prior knowledge and data for development, complicating finite-time performance. Moreover, their reliance on precise models reduces robustness against unmodeled disturbances, making them less suitable for online, finite-time, and single-life learning scenarios. This limitation is addressed by the CRL2RT algorithm, which integrates adaptive learning with real-time control, enhancing suitability for dynamic environments.\n2) Conventional Reinforcement Learning Control Algorithms in Mechanical Systems\nWith advancements in edge computing and high-performance training, reinforcement learning (RL) has become increasingly prominent in robotics applications[38, 39]. RL-based controllers, such as Soft Actor-Critic (SAC) [40], Proximal Policy Optimization (PPO) [41], and Twin Delayed Deep Deterministic Policy Gradient (TD3) [42], offer data-driven solutions to the inherent challenges of traditional control methods, promoting improved safety and efficiency[43, 44].\nFor example, Khai Nguyen et al. [45] proposed an RL-based uncertain switched-variable optimal control framework, focusing on non-autonomous tracking error models to ensure tracking accuracy. This formation control structure, integrating kinematic and dynamic subsystems, has demonstrated stability and optimality through theoretical proofs and simulations. However, the algorithm's dependency on Hamiltonian properties restricts its generalizability.\nZiping Wei et al. [46] developed an RL-based trajectory tracking controller for surface vessels, introducing an input saturation penalty function to balance computational cost and performance. Despite these advancements, input saturation constraints introduce specific design challenges.\nThe following common issues have been identified across these studies:\n1. Safety Limitations: None of the algorithms provide adequate safety guarantees against critic estimation errors, which may bias actor updates without effective mitigation mechanisms."}, {"title": "3) Reinforcement Learning Algorithms for Real time in Mechanical Systems Control", "content": "Online real-time performance is a critical requirement in applying reinforcement learning to control typical mechanical systems like the DDTWEP under pitch, roll, and yaw operating conditions[30]. Currently, the primary focus is on average real-time performance, which encompasses the real-time performance of both the inference process and the weight update process, as well as their combined average real-time performance.\nRuturaj Sambhus et al.[47] proposed a real-time, model-free control method based on the PPO, achieving a maximum control frequency of 100 Hz. This method was applied to the closed-loop force control of Series Elastic Actuators under complex 0.1 Hz sine wave and varying frequency chirp force control trajectories, demonstrating performance significantly superior to PID controllers. However, the algorithm's inference real-time performance is limited by its relatively low control frequency, making it unsuitable for inference real-time applications such as sine trajectory tracking at up to 60 Hz in the DDTWEP. Additionally, the reliance on the standard PPO algorithm hinders online continuous weight updates, necessitating system shutdowns for updates.\nShiyu Chen et al.[48] introduced a real-time dynamic response strategy based on deep reinforcement learning, achieving a maximum control frequency of 300 Hz. This approach directly maps drone and gap states to motor thrust, avoiding the cumbersome trajectory planning and independent processing of control modules inherent in traditional optimization methods. A safety-aware exploration mechanism was incorporated, and the method was applied to quadrotor motion control problems, demonstrating strong environmental generalization and adaptability to untrained gap tasks. While the inference real-time performance was enhanced by the direct mapping of states to motor thrust, the algorithm does not support weight updates post-training, limiting the ability to perform fine-tuning or updates based on varying scenarios and preventing online continuous weight updates.\nLinfeng Su et al. [49] developed a real-time and optimal hypersonic re-entry guidance method based on Inverse Reinforcement Learning (IRL) to address sparse reward issues by introducing a discriminator network. This method was applied to flight control problems characterized by highly nonlinear dynamics and complex environments. The algorithm achieved an inference speed of 5,988 Hz on high-performance general-purpose computing devices; however, the time required for data transmission was not accounted for. Moreover, like previous methods, weight updates cease post-training, impeding fine-tuning or scenario-based updates and making online continuous weight updates unfeasible.\nKevin Reuer et al. [50] presented a reinforcement learning-based, model-free control method implemented using quantum and FPGA technologies, enabling parallel execution of state observation and inference and achieving a maximum control frequency of 1.304 MHz This method was applied to the control of quantum systems. Although it effectively addresses the inference real-time challenges of reinforcement learning algorithms on dedicated computing devices, its quantum-based implementation is not easily transferable to general-purpose computing devices like CPUs. Additionally, the weight update process is 39,168 times slower than inference, making online continuous weight updates impractical and necessitating system shutdowns for updates.\nIn summary, while the aforementioned studies achieve high inference speeds up to 7,936 Hz on general-purpose computing devices and 1.304 MHz on dedicated devices-under conditions where weight updates are not performed online, their weight update real-time performance remains inadequate."}, {"title": "4) Reinforcement Learning Algorithms for Finite-Time in Mechanical Systems Control", "content": "When applying reinforcement learning to control typical mechanical systems such as the DDTWEP under pitch, roll, and yaw operating conditions, finite-time performance is a critical requirement, with a primary focus on training efficiency.\nNamhoon Cho et al. [51] presented incremental correction methods for refining neural network parameters or control functions within a continuous-time dynamic system to achieve higher solution accuracy while satisfying intermediate constraints on performance output variables. This approach was applied to the powered descent problem on Mars. However, the algorithm's linearization accuracy remains high only when the system state is close to the reference trajectory. Significant deviations from the reference can render the linearization assumptions invalid, leading to decreased correction accuracy and necessitating additional correction steps to approach the target state. This local linearization method may result in reduced convergence efficiency in highly nonlinear regions, thereby affecting the"}, {"title": "5) Reinforcement Learning Algorithms for Single-Life in Mechanical Systems Control", "content": "Single-life performance is a critical requirement in applying reinforcement learning to control typical mechanical systems such as the DDTWEP under pitch, roll, and yaw operating conditions. This work primarily focuses on robot persistence, the ability to collect data and train continuously with minimal human intervention.\nLevine et al. [56] introduced incremental correction methods for refining neural network parameters or control functions within continuous-time dynamic systems to achieve higher solution accuracy while satisfying intermediate constraints on performance output variables. This approach was applied to the powered descent problem on Mars. However, the algorithm's linearization accuracy remains high only when the system state is close to the reference trajectory. Significant deviations from the reference can render the linearization assumptions invalid, leading to decreased correction accuracy and necessitating additional correction steps to approach the target state. This reliance on local linearization may result in reduced convergence efficiency in highly nonlinear regions, thereby affecting overall algorithm efficiency and limiting its application potential for the motion control of the DDTWEP, especially under numerous random operating conditions.\nWallace and Si [57] proposed a decentralized incentivized integral reinforcement learning framework, optimizing a model-based continuous-time reinforcement learning (CT-RL) approach for hypersonic vehicle control. This method incorporates a reference input-driven exploration mechanism and optimizes the pre-scaling of HSV states alongside a decentralized control structure. The new approach demonstrated significantly improved closed-loop stability compared to traditional Linear Quadratic Regulator (LQR) and feedback linearization methods. However, the algorithm relies on the accuracy of system models, making it challenging to handle unmodeled disturbances or dynamic environmental changes. Consequently, this method cannot fully satisfy the single-life control requirements of the DDTWEP under unknown environments or system parameter uncertainties.\nPereira et al. [58] developed a deep learning-based solution that combines nonlinear stochastic optimal control principles with Feynman-Kac theory, utilizing deep forward-backward stochastic differential equations and differentiable neural network layers for the powered descent guidance problem in planetary landings. This approach minimizes fuel consumption and significantly enhances the algorithm's robustness against random disturbances and initial conditions. However, it primarily relies on offline-trained policies suitable for well-defined tasks with known initial conditions. The necessity for extensive pre-training and parameter tuning limits real-time adaptability and self- adjustment in changing environments. Additionally, the algorithm's performance depends on the coverage of initial training data, potentially affecting system robustness and convergence speed under extreme or unforeseen conditions, thus failing to meet the single-life control requirements of the DDTWEP fully."}, {"title": "B. Related Work and Proposed Algorithm", "content": "The adaptive policy learning (APL) framework proposed by Lindsey Kerbel et al. [60] is a foundational reference for this research. APL accelerates learning by utilizing pre-existing, highly engineered default powertrain control (PTC) policies integrated with a dynamically weighted, continuously learning reinforcement learning algorithm, which progressively surpasses the performance of the original policies. This approach introduces a novel RL paradigm that capitalizes on readily available power system data for continuous performance improvement. However, several potential issues have been identified:\n1. Lack of Consideration for Online Training and Real-Time Performance: The APL framework employs an updated offline training approach without addressing online updates, resulting in relatively poor real-time performance when applied to new, unknown environments.\n2. Limited Direct Interaction with System Dynamics: The baseline algorithm manages most of the system's performance, depriving the RL algorithm of direct interaction with the true system dynamics. This limitation hinders the algorithm's effectiveness in controlling complex dynamics and restricts its applicability to highly complex dynamic learning scenarios.\n3. Safety Across Conditions and Lifecycle: Although the dynamic weighting mechanism is innovative, it does not directly address safety issues arising from incorrect actions generated by the RL algorithm and the dynamic weighting mechanism. Early in the RL process, significant action errors may lead to safety issues within the APL framework.\nAdditionally, the Concerto Reinforcement Learning (CRL) algorithm[21] has been developed to address challenges in safety and efficiency from a finite-time, single-life perspective. CRL introduces two main innovations: a time-interleaved module based on Lipschitz conditions that integrates classical controllers with RL-based controllers to enhance initial stage safety, and a policy composer based on finite-time Lyapunov convergence conditions that organizes past learning experiences to ensure efficiency within finite time constraints. These modules have been validated through ablation experiments. However, CRL primarily targets constant and deterministic conditions, limiting its effectiveness under pitch, roll, and yaw operating conditions. The improved CRL2E algorithm enhances accuracy and convergence speed in random environments but suffers from poor real-time performance, achieving only approximately 600 Hz compared to the desired 2000 Hz.\nTo address these limitations, particularly the poor real-time performance of the CRL algorithm, the CRL2RT algorithm is proposed. Building upon the CRL framework and adjusting the algorithm architecture from a load- balancing perspective, CRL2RT significantly enhances the inference speed of CRL2E by more than threefold.\nThe main contributions of this work are outlined as follows:\n1. Real-Time Optimization: By leveraging the characteristics of the Concerto Reinforcement Learning architecture and further optimizing the updated neural network weights transmission process, the algorithm framework is enhanced to improve real-time performance.\n2. Record-Breaking Control Frequency: To the best of the authors' knowledge, the proposed algorithm achieves the highest control frequency for online weight-updating reinforcement learning control algorithms on general- purpose CPUs, surpassing 2500 Hz while incorporating network weight and data update conditions.\n3. Universality of Classical Control Integration: The classical control components of the Concerto Reinforcement Learning algorithm have been tested with three different types of classical controllers: PID, Adaptive PID, and MRAC, further validating the universality of the CRL framework.\n4. Extensive Applicability and Performance Enhancement: The CRL2RT algorithm demonstrates broad applicability and performance improvements under various configurations and conditions. In 40 Hz and 60 Hz configurations, the CRL2RT algorithm, combined with PID, Adaptive PID, and MRAC controllers, achieved"}, {"title": "III. Problem Description", "content": "A. Control Problem Description\nThe problem addressed in this paper is the control of DDTWEP to track various forms of desired trajectories for experimental purposes. To represent the aforementioned desired trajectories (as shown in Equation(1)), the concept of Central Pattern Generators (CPGs)[61] is employed to define the expected motion trajectories of each wing.\n\u0424\u0435\u0445\u0440,\u0456 = Ai \u00b7 sin(2\u03c0f \u00b7 t + \u03c6\u012f)\nwhere exp,i, Ai, f, qi used for this study are provided in the Experimental Settings section.\nB. Modeling direct-drive platform under tandem wing influence\nA simulation system has been established to provide the necessary training data for facilitating reinforcement learning. The realism of this simulation system has been validated in existing literature[62]. Based on the model and the electronic prototype form of the experimental platform presented in Fig.2, a simplified framework representing the DDTWEP is constructed as shown in Fig.3, in which DDTWEP is abstracted as a system composed of five rigid bodies with eight degrees of freedom: \u03a6w,1, \u03b8\u03c9,1, \u03a6w,2, \u03b8w,2, \u03a6\u03c9,3, \u03b8w,3, \u03a6w,4, \u03b8w,4 and 13 key components: one bench, four motors, four springs, and four wings."}, {"title": "IV. Primarily", "content": "A. On-Policy Deterministic Actor-Critic\nThe deterministic actor-critic[68] consists of two components. The critic estimates the action-value function while the actor ascends the gradient of the action-value function. Specifically, an actor adjusts the parameters 0 of the deterministic policy u by gradient ascent. A differentiable action-value function Q(w|s, a) in place of the true action- value function Q(u|s, a) is substituted. A critic estimates the action-value function Q(w|s, a) \u2248 Q(\u00b5|s, a), using an appropriate policy evaluation algorithm. For example, in the following deterministic actor-critic algorithm, the critic uses Sarsa updates to estimate the action-value function[68, 69].\n\u03b4 = rk + \u03b3 \u2022 Q(Wk|Sk+1, Ak+1) - Q(Wk|Sk, ak)\nWk+1 = Wk + aw\u00b7\u03b4\u00b7\u2207wQ(Wk|Sk, ak)\n\u03b8k+1 = \u03b8\u03ba + \u03b1\u03b8\u00b7 VoQ(Wk|Sk, \u03bc(\u03b8k|Sk))\n\u03b1\u03ba = \u03bc(\u03b8\u03ba|Sk)\nwhere Q(wls, a) is the differentiable action-value function with network weight parameters w, and its value at the k-th time step is wk. \u03bc(\u03b8]S) is the policy network with weight parameters \u03b8, and its value at the k -th time step is \u03b8k. y is the discount factor. aw is the learning rate for parameter w. ae is the learning rate for parameter 0. Sk is the visited state at the k -th time step.\nB. Q-Value Estimation Method Based on Gradient-Domain Laplace Transform\nTo estimate Q-values in real time without introducing a critic and to support the design and verification of the subsequent Rule-Based Policy Composer while considering finite-time impact and single-life impact, a Q-value estimation method based on the gradient-domain Laplace transform is adopted, as referenced in[21].\nThe gradient-domain Laplace transform operates in the e domain, converting @domain functions into functions of a complex variable g. The transformation principles are outlined in equations (22) to (25):\n00\n0 = \u222b* 9\n0\ngd = SP + i.DR\n00\nK(gd) = Q(0) e-se de\n0\n00\nR(gd) = r(0) e-se de\nWhere SP represents the real part of the gradient-domain Laplace transform, indicating the influence of the current gradient sequence update in the finite-time single-life task, analogous to the decay term in the Laplace transform. DR represents the direction vector of the updated gradient, analogous to the frequency in the Laplace transform, indicating critical characteristics of the update process.\nThe gradient-domain Laplace transform shares similar properties with the classical Laplace transform. Using these properties, K(grad) and R(grad) are defined as:\n00\nK(gd) = Q(0) \u2022 e-s\u00ba de\n0\n00\nR(gd) = r(0) e-sode\nAt the convergence point, for any given 0, based on the Bellman equation[69], the following relationship holds:\nQi(0) = ri(0) + \u03b3\u00b7 Qi+1(0)\nTaking the derivative with respect to \u03b8 on both sides:\ndQi(0)\ndri(0)\ndQi+1(0)\n+ \u03b3\u00b7\nde\nde\nde\nApplying the gradient-domain Laplace transform as defined above, the following is obtained:"}, {"title": "V. The proposed algorithm: CRL2RT algorithm", "content": "A. Architecture of the CRL2RT algorithm\nThe architecture of the CRL2RT algorithm, as illustrated in Fig.5 primarily comprises a Cloud side for updating neural network weights and an Edge side for inference. On the Edge side, a load-balancing approach[72] is employed, defining Mode 1: the integration of traditional control components with the Reception, Analysis, and Loading of Weight modules and the CRL algorithm's traditional control components and Mode 2: learning based control. These are executed in parallel with the conventional reinforcement learning inference components, operating in an interleaved fashion based on the Time-Interleaved mode."}, {"title": "B. Design of Time-Interleaved module and Guarantee of Lipschitz Conditions", "content": "The primary function of the Time-Interleaved module is to enhance overall process safety by alternating the execution of classical controllers, which have basic feedback control capabilities, with reinforcement learning controllers. The pseudo-code is presented below.\nThe theoretical basis for the functionality of the Time-Interleaved module and Time-Interleaved Capability is explained using Lipschitz Conditions:\nFirst, a vector Er(t) representing the tracking error of the four wings relative to the desired commands over time is defined:\nEr(t) = [E\u2081(t), E2(t), E3(t), E\u2084(t)]\nFor two adjacent time steps, as defined by the Time-Interleaved module (refer to Algorithm 1), which alternates between the RL controller and the classical controller, the error generated between these two time steps are:\nEr(t + \u2206t) = Er(t) + PEri(t) \u00b7 \u2206t\nEr(t + 2\u00b7 \u2206t) = Er(t + \u2206t) \u2013 PDclass At\nHere, PErl(t) represents the RL algorithm's ability to increase or decrease the error, and PCclass represents Time- Interleaved Capability, i.e., the classical control algorithm's ability to reduce the error, which remains constant.\nThe relationship can be expressed as:\nEr(t + 2 \u00b7 \u2206t) \u2013 Er(t) = \u2212PCclass \u00b7 At + PEri(t) \u00b7 At\nAssuming At* = 2\u00b7 \u2206t and the maximum value of PEri(t) is PErl,max, the following inequality is established:\nAt* <\n-PCclass + PEri(t)\n-PCclass + PErimax\nEr(t + \u2206t*) \u2013 Er(t) =\nThis can be expressed in terms of Lipschitz continuity as follows:\n\u0394Er = Er(t + At*) \u2013 Er(t) < Lipschitz \u00b7 At*\n-PCclass + PErl,max\nALipschitz =\n2\nTherefore, the Time-Interleaved module, with its Time-Interleaved Capability, ensures that the algorithm satisfies Lipschitz continuity. Based on the above equation, the following analysis can be made:\n1. Adjustable Range of \u2206Er: According to equation (51), the desired \u2206Er can be achieved by adjusting the relative relationship between PCclass (i.e., Time-Interleaved Capability) and PErl,max to meet hard safety constraints. For example, if \u2206Er < \u2206Ermax, then PCclass must satisfy the following conditions:\nPCclass > PErl,max\n\u0394\u0395rmax\nAt\n2. Critical Point for Switching PCclass: When PCclass = PEr\u2081(t), this represents a critical point ensuring boundedness and guaranteeing a gradual reduction of the error (\u2206Er < 0). If PCclass < PEr\u0131(t), equation (53) transforms into the form of equation (54), where the influence of PCclass on PEri(t) must be accounted for to mitigate adverse effects and enhance performance:\n\u0394\u0395\u03a5\n+ PCclass = PErl(t)\nAt\nTo incorporate perturbations into the Time-Interleaved Capability perturbation (TICP) is applied as described below:\nPCclass = PCclass,0 \u00b1 \u025bTICP\nHere, PCclass,o represents the original Time-Interleaved Capability, while \u025bTICP denotes the perturbation induced by changes in the classical controller's capability. Notably, the Time-Interleaved Capability Perturbation is directly implemented by modifying the classical controller's strategy during the inference phase, making it suitable for real- time execution."}, {"title": "C. Design of Rule-Based Policy Composer and Guarantee of Finite-Time Lyapunov Convergence Conditions", "content": "1) Time Segmentation of Single-Life and Training Phase Definitions\nThis study employs an approach designed to suppress erroneous gradient updates to enhance training efficiency. Given the unidirectional and unique nature of single-life processes, a rule is established to record the policy parameters, 0, during the single-life progression.\nBuilding on the gradient-domain Laplace transform concept introduced in Section 3.2 and addressing the requirements of the Rule-Based Policy Composer, the single-life timeline is segmented into two hierarchical training phases, as depicted in Fig.6. Specifically, every L time step constitutes a gradient descent segment (GDS). At the same time, up to every N of the GDSs form a dynamic descent phase (DDP)."}, {"title": "V1. Experiments", "content": "A. Objectives and Research Questions\nThis study aims to address the following questions through a series of experiments:\n1. Question 1: What enhancements in real-time performance are achieved by the proposed algorithm and deployment scheme?\n2. Question 2: How is the applicability of the CRL framework to various classical controllers evaluated under real hardware interference conditions?\nB. Experimental settings\n1) Basic Training and Simulation Parameters and Computing Devices\nA uniform experimental setup was adopted for the algorithm comparisons conducted in this study. During the algorithm execution process, a semi-physical simulation system, as illustrated in Fig 9, was first constructed. The hardware and software configurations used in the experiments included Python version 3.12, PyTorch version 2.0, and Ubuntu 24.0 as the operating system. The cloud side component utilized a computer equipped with an Intel 13700KF CPU and an NVIDIA RTX 4090D GPU. The edge side employed an AMD 7840HS industrial controller, and the Physical Simulation Computer had an Intel 13700KF CPU. For simulation, the system model shown in Fig.4 was utilized. Due to the numerical stiffness caused by the rapid tensioning process in the system model[80], the Radau algorithm[81], an implicit numerical method, was adopted to ensure numerical stability. The interval for each event step during the iteration process was set to be 0.0005 seconds."}, {"title": "C. Question 1: What enhancements in real-time performance are achieved by the proposed algorithm and deployment scheme?", "content": "To deploy the controller in real-time control scenarios, a semi-physical simulation system, as illustrated in Fig 9 was constructed. The time required for each step in the single-step control process during the entire algorithm execution was measured and presented in Table 7 to Table 10.\n1. Impact of Reinforcement Learning Control on I/O Processes: The reinforcement learning control process primarily affects the I/O components, posing significant challenges to real-time performance. The Sensor data unpacking process, Data packing, Data transmission, Transmission of Memory Buffer, and Transmission of Action account for up to 68.9% of the total execution time, making real-time implementation highly challenging.\n2. Benefits of Framework Optimization: Utilizing the conventional CRL2E algorithm[21], the unoptimized execution time was 8.43E-04 seconds. After optimizing the framework as described in Section 4.1, the execution time was reduced by approximately half, ranging from 3.29E-04 to 5.14E-04 seconds.\n3. Advantages of Direct Matrix Operations Over PyTorch: Implementing the edge inference framework defined in Section 4.6 significantly enhances algorithm performance by enabling direct matrix operations without the need for tensorization and de-tensorization. A notable improvement is the reduction of the Reception, Analysis, and Loading of Weight time by three orders of magnitude. Additionally, the State tensorization and Action de-tensorization processes experienced a two to three orders of magnitude decrease in processing time. Consequently, the average CPU time was improved by more than three times compared to existing studies[49]."}, {"title": "D. Question 2: How is the applicability of the CRL framework to various classical controllers evaluated under real hardware interference conditions?", "content": "To validate the performance of the proposed CRL2RT algorithm under real hardware interference conditions, experiments were conducted based on the operating conditions defined in Section 5.3. The results are illustrated in Fig 10 to Fig 12, which includes the following configurations:"}, {"title": "Vll. Conclusion", "content": "This paper addresses the challenges of real-time control for a direct-drive tandem-wing experimental platform using a novel reinforcement learning framework, CRL2RT. The study's primary contributions are summarized as follows:\n1. Proposed CRL2RT Algorithm for Real-Time Reinforcement Learning: The CRL2RT algorithm enhances the Concerto Reinforcement Learning framework by optimizing weight updates and inference strategies for real-time applications. It achieves over 2500 Hz control frequency on general-purpose CPUs, making it the world's fastest reinforcement learning algorithm with online weight updates under such conditions.\n2. Integration with Traditional Controllers: The CRL2RT algorithm demonstrated wide applicability by integrating with three types of classical controllers (PID, Adaptive PID, and MRAC). This integration ensured safety and improved convergence rates through the Time-Interleaved module, satisfying Lipschitz continuity and finite-time Lyapunov stability conditions.\n3. Experimental Performance Validation: Extensive experiments across six operating conditions (varying frequencies and loads) validated the algorithm's effectiveness. The CRL2RT algorithm consistently enhanced tracking performance, achieving 18.3%\u201360.7% improvement under challenging dynamic conditions.\n4. Theoretical Insights into Convergence: The Rule-Based Policy Composer improved convergence speed by leveraging finite-time Lyapunov stability conditions and exploiting gradient-domain Laplace transforms. The findings demonstrated the algorithm's robustness and adaptability under multiple random operating conditions.\n5. Deployment and Real-Time Performance:The study highlighted the critical role of optimizing the inference process. By transitioning to direct matrix operations and redesigning the inference framework, the algorithm achieved a threefold improvement in inference speed compared to conventional implementations.\nDespite the promising results, certain limitations were observed, particularly in scenarios involving adaptive controllers like MRAC, where CRL2RT's performance did not significantly surpass the adaptive capabilities of MRAC alone. Future work will focus on further refining the integration mechanisms and exploring adaptive strategies to enhance the algorithm's performance in conjunction with various adaptive control systems.\nFuture research should concentrate on applying the CRL2RT algorithm to flight control scenarios of dragonfly- inspired and other biomimetic aerial vehicles and exploring the universality of the CRL2RT algorithm when integrated with additional existing traditional control algorithms. The CRL2RT algorithm holds significant potential for application across various domains. In mobile robotics, it could be extended to trajectory tracking control for platforms such as quadruped robots and humanoid robots. Furthermore, it shows promise for precision motion control in fields like precision manufacturing.\nOverall, the CRL2RT algorithm represents a significant advancement in real-time control for biomimetic aerial vehicles, offering a robust solution to the challenges posed by nonlinear, unsteady aerodynamic interactions in tandem- wing configurations."}]}