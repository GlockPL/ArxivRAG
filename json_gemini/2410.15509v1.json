{"title": "Exploring Curriculum Learning for Vision-Language Tasks: A Study on Small-Scale Multimodal Training", "authors": ["Rohan Saha", "Abrar Fahim", "Alona Fyshe", "Alex Murphy"], "abstract": "For specialized domains, there is often not a wealth of data with which to train large machine learning models. In such limited data / compute settings, various methods exist aiming to do more with less, such as finetuning from a pretrained model, modulating difficulty levels as data are presented to a model (curriculum learning), and considering the role of model type / size. Approaches to efficient machine learning also take inspiration from human learning by considering use cases where machine learning systems have access to approximately the same number of words experienced by a 13 year old child (100M words). We investigate the role of 3 primary variables in a limited data regime as part of the multimodal track of the BabyLM challenge. We contrast: (i) curriculum learning, (ii), pretraining (with text-only data), (iii) model type. We modulate these variables and assess them on two types of tasks: (a) multimodal (text+image), and (b) unimodal (text-only) tasks. We find that curriculum learning benefits multimodal evaluations over non-curriclum learning models, particularly when combining text-only pretraining. On text-only tasks, curriculum learning appears to help models with smaller trainable parameter counts. We suggest possible reasons based on architectural differences and training designs as to why one might observe such results.", "sections": [{"title": "1 Introduction", "content": "Recent vision-language models (VLMs) have achieved superior performance on numerous benchmark datasets (such as the Llama\u00b9 and Gemini models\u00b2), and continue advancing rapidly as models are scaled up. The number of parameters of such models is often on the order of billions. These models require multiple days of compute, and hundreds of GPUs (e.g., Radford et al. (2021)), resulting in massive energy consumption (Luccioni et al., 2024).\nFurthermore, to train such large models, we require massive amounts of pretraining data. For example, 70M image-text pairs were used to train the Flava foundation model (Singh et al., 2022). Pretraining VLMs on such large scale data is often infeasible for independent researchers and university research labs with limited compute.\nIn contrast to machine learning, human learning is much more efficient, a finding which has led researchers to consider which methods might promote more human-like learning in artificial neural networks. This was originally argued for in early work on curriculum learning (Bengio et al., 2009), citing the fact that humans do not learn from randomly sampled data, but benefit from learning over structured chunks, typically increasing in difficulty (a curriculum).\nTo this end, we explore the application of curriculum learning to VLMs with limited input data as part of the BabyLM challenge (Choshen et al., 2024). For the multimodal track, which contains a dataset of image-caption pairs, we take inspiration from phase-based curriculum methodology used in Ayyubi et al. (2023). We use Part-of-Speech (PoS) linguistic features from the captions to categorize samples into different phases, to generate a learning curriculum. However, instead of training the model only one phase at a time (as used in Ayyubi et al. (2023)), we train the model on the current and previous phases such that the pool of data which can be sampled increases at each phase.\nFrom our experiments, we observe that:\n\u2022 In a limited data setting, curriculum learning can improve the performance of VLMs on certain multimodal and text-only evaluation benchmarks.\n\u2022 Pretraining VLMs on developmentally plausible text-only data prior to adapting to multimodal data may help improve performance on some evaluation tasks, but not others."}, {"title": "2 Background", "content": "2.1 Curriculum Learning\nCurriculum Learning (CL) takes inspiration from the learning process in humans by presenting data to a machine learning model in an easy-to-difficulty manner (Elman, 1993; Bengio et al., 2009). CL consists of two parts: (1) a scoring function to rank data samples based on difficulty, and (2) a pacing function, which controls the distribution of data samples presented to the model. In the standard CL implementation, the pacing function introduces can be samples in ascending order of difficulty (or decreasing difficulty in the case of anti-curriculum learning (Hacohen and Weinshall, 2019; Wu et al., 2021)).\nWhile extensive research has shown that in certain cases, curriculum learning can provide performance gains in vision (Hacohen and Weinshall, 2019; Wang et al., 2019b; Soviany, 2020) and Natural Language Processing (NLP) tasks (Nagatsuka et al., 2021; Maharana and Bansal, 2022; Sun et al., 2023), in other cases, the benefit is unclear (Campos, 2021; Martinez et al., 2023; Chobey et al., 2023; Edman and Bylinina, 2023a). Importantly, with the prevalence of vision-language models, it is crucial to understand how the application of CL modulates VLMs to work in the domain of limited data and compute.\n2.2 Curriculum Learning for Vision Language Models\nSome previous work has applied CL to multimodal models where the data modality consists of images and texts. Srinivasan et al. (2023) showed that CL applied to a transformer model helps improve performance on zero-shot image and text retrieval tasks over a baseline CLIP model (Radford et al., 2021). CL has also shown benefit in other multimodal domains, such as medical report generation (Liu et al., 2023), image-captioning (Ayyubi et al., 2023), and visual question answering (Li et al., 2020). However, these works either rely on non vision-transformer based image encoders (such as an R-CNN), or conduct evaluation on a small set of evaluation tasks. It is also unclear whether: (i) training VLMs on image-caption data improves model performance on text-only benchmarks; (ii) how CL affects downstream performance in models with additional text pretraining compared to randomly initialized models.\nIn this work, we present a study where we apply CL to VLMs trained on small data. We hope to provide the research community with a better understanding of the effects of CL on popular VLMs such as the Generative Image Transformer (GIT) (Wang et al., 2022) and Flamingo (Alayrac et al., 2022) models. Furthermore, we also explore the effect of CL on downstream model performance on various zero-shot multimodal and text-based benchmarks."}, {"title": "3 Methods", "content": "3.1 Data\nWe use the dataset provided as part of the BabyLM multimodal track (Choshen et al., 2024). The data consist of 100M words in total: 50M words from varied text corpora (described in Choshen et al. (2024)) and the other 50M words are text captions taken from the Conceptual Captions (Sharma et al., 2018) and Localized Narratives (Pont-Tuset et al., 2020) image-caption datasets. In total, the multimodal data consists of ~ 2.9M image-caption pairs.\nOne of the key experimental variables we examine is the impact of text pretraining. For multimodal models, we compare the performance of models trained on image-caption data (consisting of 50M words), starting either from a randomly initialized model or from a model pretrained on the text-only corpora mentioned above (50M words). Model variants not pretrained on the text-only corpora only use the words in the captions of the associated training images (i.e., models are trained on only 50M words and the corresponding images).\n3.2 Models\nWe train two VLMs: (1) GIT Wang et al. (2022) and (2) Flamingo (Alayrac et al., 2022). We chose these models because they were selected as reference baselines provided by the BabyLM challenge (Choshen et al., 2024). Both GIT and Flamingo models consist of vision encoders to encode image inputs, and text decoders to generate free-form text.\nWe use the default configurations for the GIT3 and Flamingo models provided in the BabyLM challenge to compare the performance of our models to the baselines reported by the challenge. Following the default configurations, we use pretrained vision encoders for both the GIT and"}, {"title": "3.3 Curriculum Framework", "content": "We discuss the respective implementations of the scoring and pacing functions for the curriculum learning framework below.\nScoring function: A scoring function assigns a difficulty score $k \\in \\mathbb{R}$ to each sample in the dataset, where a sample $x_i$ is easier than a sample $x_{i+1}$, if $k_{x_i} <k_{x_{i+1}}$.\nPrevious works have used a variety of scoring functions to measure sample difficulty, such as the loss scoring function in image classification (Hacohen and Weinshall, 2019) and text classification settings (Xu et al., 2020; Maharana and Bansal, 2022). Relatedly, in sample-efficient pretraining of language models, average sentence rarity (Borazjanizadeh, 2023), sentence length (DeBenedetto, 2023) or other combinations of individual text statistics (Edman and Bylinina, 2023b) have been used to rank data samples (for a comprehensive survey, see Soviany et al. (2021)). More recently, in multimodal settings, cross-modal similarity (Zhang et al., 2022) has been used to rank examples to improve model performance in image-captioning tasks. All in all, it must be noted that determining the difficulty of image-caption pairs is non-trivial and an active research problem.\nFor our experiments, we explored the applicability of linguistic information such as Part-of-Speech (PoS) tags to determine difficulty of samples. We took inspiration from the scoring function used by Ayyubi et al. (2023), where a PoS tagger was used to count the number of nouns in the caption, as an indirect measure of the number of concepts present"}, {"title": "Ordering", "content": "In our experiments, we order the samples in ascending order of difficulty, to explore the"}, {"title": "3.4 Models Variants", "content": "For both GIT and Flamingo, we train four model variants, two of which are baseline models and two are trained using curriculum learning. In each pair, we train one model only on the image-caption data starting from random initialization (except the vision encoder which is pretrained), while we first pretrain the other variant on the text-only corpus, before training on image-caption data.\nBaselines: For the first baseline variant, we train the model on the image-caption dataset (50M words) using standard i.i.d training. We refer to this variant with C (denoting that the model is trained on the image-caption data only) for both GITBaseline and Flamingo Baseline. For the second baseline variant, we first train the model on the text-only dataset (containing 50M words) using standard i.i.d training. We then continue the training procedure on image-caption dataset (containing another 50M words) using standard i.i.d training. We refer to this variant as T+C, for both GIT Baseline and Flamingo Baseline.\nOur choice to also train the T+C model variant stems from previous work showing that exposing the model to developmentally plausible data, such as child-directed speech, before exposing it to complex data, can benefit model performance (Huebner et al., 2021). Thus, we explore the difference in model performance, when we first train the model on the text-only dataset, before continuing the training procedure on the image-caption data.\nCurriculum models: For curriculum variants, we use CL on the image-caption pairs because we hypothesize that applying CL on multimodal data will improve model performance. We refer to these variants trained only on the image-caption pairs as C under GITCL and FlamingoCL. We also train T+C variants of CL models, where we first pretrain the model on the text-only dataset using standard i.i.d training, and then use curriculum learning to continue the training procedure on the image-caption pairs.\nTo summarize, we trained four variants for each model, two of which were trained using standard training (no curriculum), and the other two were trained using curriculum learning. For GIT and Flamingo baseline variants, we train the model on the image-caption only (C) data, and both text + image-caption (T+C) data. Similarly, for the curriculum variants, we train each model on, image-"}, {"title": "4 Training and Evaluation Details", "content": "Training Details: For the curriculum variants, we train the model for two epochs per each difficulty phase (of which there are four). We used a learning rate of le\u00af5, maximum token length of 50, and 32 samples per batch, and Adam optimizer10 (Kingma and Ba, 2017).\nWhen training the T+C variants of our baseline and curriculum models, we first trained the model on the text-only dataset for twenty epochs (instead of eight epochs for image-caption data) and use the same hyperparameter values. We used an NVIDIA A5000 GPU with 24GB vRAM, with half-precision (FP16) to train the models. We provide the total time required to train each model variant in Appendix A. For all experiments, we set the random seed to 0 to remove variation in the results due to different random sampling and initialization. We also hold out 5% of the full image-caption dataset to validate the model. We show the validation loss curves in Appendix B.\nEvaluation: To evaluate the performance of our models, we use the evaluation pipeline provided by challenge (Gao et al., 2023; Choshen et al., 2024). We report the performance of all the variants of the GIT and Flamingo models on the multimodal, and text-based evaluation tasks."}, {"title": "4.1 Multimodal evaluation datasets", "content": "Winoground: The Winoground dataset (Thrush et al., 2022) evaluates a model's ability to perform visio-linguistic compositional reasoning. Specifically, given two image-caption pairs, the goal is to match the image to the corresponding caption, where both captions contain an identical set of words, but in a different order (e.g. It's a fire truck vs it's a truck fire). The dataset consists of 400 examples with 800 unique images and captions. To assess model performance, we use the unpaired text-score metric as provided in the BabyLM evaluation pipeline."}, {"title": "VQAv2:", "content": "The VQAv2 dataset (Goyal et al., 2017) is a large-scale visual question answering dataset. It contains open-ended questions about images, requiring models to understand the visual content and generate appropriate answers. We use accuracy as the choice of metric as reported in the BabyLM evaluation pipeline. For this task the model has to select the best answer for a given image and question, in the presence of 7 distractors."}, {"title": "DevBench:", "content": "The DevBench dataset (Tan et al., 2024) is a multimodal benchmark for developmental evaluation that evaluates how closely a model's outputs align with human responses. It includes tasks such as object recognition, action recognition, and visual question answering, using data from both children and adults. The BabyLM pipeline uses three tasks from the DevBench dataset: (1) The (Lexical) Visual Vocabulary (lex-viz_vocab) task involves selecting the correct image from several image options based on a given word. (2) The (Grammatical) Test of Reception of Grammar (gram-trog) task involves choosing the correct image based on a sentence, testing grammatical understanding using distractor images that correspond to sentences with different word orderings (e.g. \"a white cat sitting on a brown couch\" vs. \"a brown cat sitting on a white couch\"). Finally, (3) the (Semantic) THINGS Similarity (sem-things) task uses Representational Similarity Analysis (RSA) to compare the model's image similarity judgments with human responses."}, {"title": "4.2 Text-only evaluation datasets", "content": "BLIMP (and BLIMP Supplement): The BLIMP dataset (Warstadt et al., 2020) is a benchmark for evaluating syntactic and semantic knowledge in language models. It consists of sentences with systematic variations in syntax and semantics. The BLIMP Supplement extends the original dataset with additional challenging examples.\n(Super)GLUE: The (Super)Glue benchmark (Wang et al., 2018, 2019a) is a collection of diverse natural language understanding tasks designed to evaluate a model's ability to perform well across multiple domains and evaluates generalized linguistic ability. The BabyLM challenge includes tasks, COLA, SST2, MRPC, QQP, MNLI, MNLI-MM, QNLI, RTE from the GLUE benchmark, and the tasks BoolQ, RTE and WSC from SuperGLUE benchmark. To fine tune all our model variants, we use a train batch size of 128, validation batch size of 16,"}, {"title": "5 Results", "content": "As unimodal and multimodal tasks are qualitatively different, we analyze the three experimental variables of interest (curriculum, pretraining & model type) in the context of each task type. Namely, we report the results for all variants of GIT and Flamingo models across two main task types that differ with respect to their data inputs: (i) multimodal (image+captions), and (i) unimodal (text-only)."}, {"title": "5.1 Multimodal (image+captions)", "content": "We show the multimodal evaluations results in Table 1 for Winoground and VQAv2, and in Tables 2 (accuracy) and 3 (human similarity) for DevBench.\n5.1.1 Curriculum Learning\nThe GITCL model performs better than GIT Baseline on VQAv2 and DevBench datasets,"}, {"title": "5.1.2 Text Pretraining", "content": "Compared to training on just image-caption data, pretraining with the text-only data (variant T+C) produces higher scores across both GITBaseline and GITCL models on Winoground and DevBench, while the results are more mixed for Flamingo models. However, in FlamingoCL on the VQAv2 dataset, we see the largest gain in performance due to text pretraining (from 35.93 to 40.85, a gain of ~5% in Table 1). On the DevBench evaluation for GITCL, we also see the 2nd largest gain in performance due to text pretraining (from 44.63 to 48.22 for accuracy, and from 45.93 to 49.51 when using reference human similarity scores; a gain of ~4%). Interestingly, the highest result of all models on the Winoground dataset are the GIT models with text pretraining, suggesting that text-only pretraining is a big contributor to the properties of the Winoground evaluation benchmark (compositionality). However, one must be cautious about generalizing this finding as the performance increase could simply result from the model being trained on more data.\nAs we only use a single seed to report these results, we wanted to confirm that our observation is not simply due to random chance. Thus, we conduct more experiments where we train all GIT variants using two more seeds, and observe a similar pattern in our findings (text pretraining aids model performance). We provide these results in Appendix C."}, {"title": "5.1.3 Model Type", "content": "The two models differ in their application of attention mechanism and model size, measured by the number of trainable parameters (See Section 3.2). Flamingo has a frozen image encoder (unlike GIT) and cross-attention is applied prior to each LM block in the Transformer stack (which internally contains the standard self-attention mechanism). In contrast, GIT uses a projection module to bring image embeddings into the same space as the text embeddings and applies successive self-attention on these vectors. We see multiple variants of GIT outperform Flamingo (especially for Winoground, VQAv2, and lex-viz_vocab, gram-trog subsets for DevBench). In the multimodal evaluation context, we believe this could be due to the ability for GIT to update the parameters of its vision encoder, perhaps additionally by making use of the fact that image tokens can self-attend to one another (unlike the cross-attention in Flamingo, which does not have this property)."}, {"title": "5.2 Unimodal (text-only)", "content": "We summarize the results for the unimodal (text-only) evaluation in Table 4. This table contains summary results for the three text-only evaluation benchmarks (see Section 4.2). Table 9 contains detailed results on the (Super) GLUE and EWOK benchmarks. We also provide a detailed breakdown of model performance for each text-based task in Appendix D."}, {"title": "5.2.1 Curriculum Learning", "content": "Closely related to the observations for multimodal benchmarks, we see that curriculum learning variants outperform corresponding baselines variants on the unimodal (text-only) benchmarks. Although both GITCL and FlamingoCL outperformed their corresponding baselines (Tables 4 and 9), the effect was greater in FlamingoCL."}, {"title": "5.2.2 Text Pretraining", "content": "We outline the averaged results in Table 4 and show that for both Flamingo and GIT, text pretraining leads to a gain in performance. In fact, all T+C variants (curriculum and baseline) for both models showe better performance compared to C variants. Coupled with curriculum learning, we observe performance benefits on all text-based evaluation datasets. These results suggest that text pretraining conveys a clear advantage for multimodal models when they are evaluated on certain text-based benchmarks."}, {"title": "5.2.3 Model Type", "content": "Unlike the multimodal results, considering the average results in Table 4, there was no consistent pattern where one model type outperformed the other. For example, on (Super)GLUE, both baseline and CL T+C variants of Flamingo outperformed respective GIT variants. However, this was not the case for BLIMP filtered, where we observed the opposite pattern - all variants of GIT outperformed all variants of Flamingo. Such a result could result from the fact that both GIT and Flamingo become more similar in their architecture in the text-only evaluation setting. This can stem relaxed requirement to incorporate image information, making both models resemble standard autoregressive Transformer decoders (the trainable parameter count changes in this context because GIT's vision encoder was trainable in the multimodal case, while Flamingo's was frozen). This results in the trainable parameter count for GIT being 198M and 169M for Flamingo (Section 3.2)."}, {"title": "5.3 Brief Summary of Results", "content": "For the multimodal evaluation, we observe that text pretraining before image-caption training boosts model performance compared to no text pretraing. However, these observations must be cautiously generalized across model types; text pretraining largely conveys a benefit in all GIT models, but this benefit is inconsistent for Flamingo.\nFor instance, the FlamingoCL variant benefits from additional text-only pretraining over just image-caption training (for VQAv2,gram-trog, and sem-things), but this effect is unclear for the Flamingo Baseline. For GIT model variants, curriculum learning (combined with pretraining) resulted in the best overall model scores on VQAv2 and DevBench (considering average scores in Tables 2 and 3).\nFor the text-only evaluation, removing the image component from both the GIT and Flamingo models effectively reduces them to text-only transformer architectures with differing number of parameters. This likely explains why the models show similar performance across tasks despite their original multimodal design. Nonetheless, we see that in Table 4, the FlamingoCL T+C variant can be more suited to learning representations leading to better scores across the SuperGLUE benchmark, and BLiMP supplement dataset. But on BLiMP filtered (and less pronounced for EWOK), the T+C variant of GITCL outperforms the T+C variant of FlamingoCL."}, {"title": "Conclusion", "content": "In this study, we explore the application of a curriculum learning (CL) approach to training vision-language models (VLMs) in a limited data setting. We use a custom trained Part-of-Speech (PoS) tagger to determine the complexity of image-caption pairs. We train two variants for each of the GIT and Flamingo models using curriculum learning and compare their performance against variants trained using standard i.i.d training. We find that while CL training shows potential, its benefits are not universally applicable across all GIT and Flamingo variants. However, for certain model configurations, CL enhances performance on a range of downstream, multimodal and text-based tasks (zero-shot and finetuning). Importantly, pretraining VLMs on developmentally plausible text data prior to multimodal training can contribute to performance gains. Nonetheless, generalizing this result requires careful consideration, as factors such as model architecture, training data composition, and the nature of evaluation tasks can significantly affect model performance."}, {"title": "Code and Data Availability", "content": "We release our code, model predictions, and model checkpoints."}]}