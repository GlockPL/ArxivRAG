{"title": "GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting", "authors": ["Jiajun Dong", "Chengkun Wang", "Wenzhao Zheng", "Lei Chen", "Jiwen Lu", "Yansong Tang"], "abstract": "Effective image tokenization is crucial for both multi-modal understanding and generation tasks due to the necessity of the alignment with discrete text data. To this end, existing approaches utilize vector quantization (VQ) to project pixels onto a discrete codebook and reconstruct images from the discrete representation. However, compared with the continuous latent space, the limited discrete codebook space significantly restrict the representational ability of these image tokenizers. In this paper, we propose GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting as a solution. We first represent the encoded samples as multiple flexible featured 2D Gaussians characterized by positions, rotation angles, scaling factors, and feature coefficients. We adopt the standard quantization for the Gaussian features and then concatenate the quantization results with the other intrinsic Gaussian parameters before the corresponding splatting operation and the subsequent decoding module. In general, GaussianToken integrates the local influence of 2D Gaussian distribution into the discrete space and thus enhances the representation capability of the image tokenizer. Competitive reconstruction performances on CIFAR, Mini-ImageNet, and ImageNet-1K demonstrate the effectiveness of our framework. Our code is available at: https://github.com/ChrisDong-THU/GaussianToken.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have recently demonstrated dominance in natural language tasks because of the superior model capacity and scalability [1, 8, 38]. Additionally, a series of visual and multi-modal endeavors have attempted to exploit the auto-regressive architecture and pretrained knowledge from LLMs for vision-related tasks [17, 18, 30]. To accommodate the discrete input format of LLMs, they first tokenize images to obtain discrete visual tokens, and then perform text alignment and subsequent autoregressive predictions based on the task format. Therefore, the effectiveness of image tokenizers directly determines the upper bound capabilities of the models.\nVector Quantization (VQ) is the prevalent image tokenization technique, balancing the requirements of image perception [2], conditional image generation [23, 30], and multi-modal image understanding tasks [16, 28, 35]. Specifically, VQ-based strategies [9, 24, 32] comprise a discrete codebook that encompasses a predetermined number of learnable vectors. The encoded image features are aligned with the codebook vectors through similarity calculations to perform nearest-neighbor matching, which allows the image to be represented by discrete tokens derived from the codebook vectors. Then a decoding module processes the discrete tokens to yield the reconstructed results in the RGB domain. In addition, researchers have introduced a discriminator module to impose GAN-related constraints on the reconstructed images to enhance the authenticity and visual perception of images [9]. Nevertheless, compared to the continuous latent space of a naive VAE [14], the size of the codebook space significantly limits the ability to model the distribution in the discrete space. Consequently, this constraint imposes a decline in image reconstruction metrics and a redundancy in the training schedule. Certain methods might augment the discrete space by increasing the codebook numbers or constructing lookup-free mappings [20, 42], while they inherently remain confined within the limited discrete space and necessitate a more gradual training process for convergence.\nTo address this, we propose an effective image tokenizer with 2D Gaussian Splatting, named GaussianToken, to enrich the codebook space for better modeling capabilities, as shown in Figure 1. We propose a 2D Gaussian embedding module to parameterize the encoded image features into multiple Gaussian distributions. Each Gaussian distribution is characterized by its position, rotation angle, scaling factor, and feature coefficients, which is capable of learning adaptive independent parameters. Subsequently, we propose to quantize the feature coefficients with a standard"}, {"title": "2. Related Works", "content": "Image Tokenizer. Two typical image tokenizers include the straightforward patch embedding and the encoder-decoder architecture. Among them, the patch embedding operation in Vision Transformers (ViTs) [5, 7, 31] aims at converting the original image into tokens that can be processed by the transformer structure. This approach is more commonly adopted in visual tasks such as image classification, which only require a global perception of the image.\nOn the contrary, image tokenizers with an encoder-decoder structure are adaptable to dense perception and generation task, which first proposed in VQ-VAE [32]. They typically employ a discrete codebook to perform nearest-neighbor matching on image features and enhance the quality of reconstructed images with a lightweight discriminator [9]. To further enhance model performance, various approaches replace the original CNN backbone with a Vision Transformer (ViT) or a hybrid structure to strengthen feature extraction capabilities [9, 33, 41]. Other strategies focus on iterative and refined improvements in the codebook matching process [16, 27]. In addition, MAGVIT-v2 [20, 42] substitutes the learnable codebook with a lookup-free format, which directly maps through numerical comparison, thereby expanding the equivalent number of codebooks to maximize the potential space.\nHowever, these methods still compress the latent representation of images into a discrete space correlated with the size of the codebook, with features matched across spatial dimensions according to a single codebook, which directly constrains the representational power and reconstruction quality. To overcome the weakness, GaussianToken introduces a more adaptable quantization process through the use of 2D Gaussian Splatting, which adaptively learns spatially local information such as the position and scaling factor of the quantized features, thereby expanding the discrete space to achieve superior performance.\nGaussian Splatting. Gaussian Splatting [12] first appeared in 3D scene reconstruction and is capable of addressing the real-time issues associated with model training and rendering in Neural Radiance Fields (NeRFs) [21, 22, 40]. 3D Gaussian Splatting researchers adopt explicit Gaussian ellipsoids and differentiable rasterization opera-"}, {"title": "3. Proposed Approach", "content": "In this section, we provide a detailed exposition of our GaussianToken method based on 2D Gaussian Splatting. We first provide a brief introduction of the VQ-VAE, whose discrete codebook space inherently constrains its representational ability. To bridge this gap, we propose a novel quantization method, using featured 2D Gaussian (i.e., GaussianToken) as basic quantization units, to imbue the originally discrete space with a measure of continuity, while the feature coefficients of each unit remain discrete. We then elaborate on the pivotal Gaussian Embedding framework, which enables the efficient learning of our proposed tokenized representation from raw image data. Lastly, we provide an overall framework of GaussianToken and corresponding analysis of its effectiveness.\n3.1. Preliminaries: VQ-VAE\nVQ-VAE [32] is a unique type of variational autoencoder that adopts vector quantization to obtain a discrete latent representation. The key implementation lies in its latent embedding space $Q = \\{e_1, e_2, ..., e_n\\} \\in \\mathbb{R}^{N \\times D}$, where $N$ denotes the discrete latent space (a.k.a. the codebook size) and $D$ is the dimension of each latent embedding vector $z_i$. Formally, given a high dimensional image $x \\in \\mathbb{R}^{H \\times W \\times C}$, the encoder $E_\\phi$ is first employed to produce the low dimensional latent representation $Z \\in \\mathbb{R}^{h \\times w \\times D}$. Note that $Z$ is a feature map, which at this stage, is comprised of a multitude of continuous latent variables. Different from the typical autoencoder, VQ-VAE then passes $Z$ through a quantization module and equivalently transformes it into a set of indices representation, forming the quantized $\\hat{Z}$. The index values correspond to the embeddings $z_i$ of the original feature $z_i$ for $i \\in [1, h \\times w]$, determined by a nearest neighbor look-up using a shared codebook as follows:\n$z_i = q(z_i) = e_k, \\text{ where } k = \\underset{j}{\\text{argmin}} ||z_i - e_j||_2$, (1)\nwhere $q(\\cdot)$ is the quantization operation. Ultimately, the decoder $D_\\theta$ inversely maps $\\hat{Z}$ back to the image domain.\n$\\hat{x} = D_\\theta(\\hat{Z}) = D_\\theta(q(Z)) = D_\\theta(q(E_\\phi(x)))$. (2)\nThe trainable components ($E_\\phi$, $D_\\theta$, and $Q$) are optimized by minimizing the following objective:\n$L_{VQ-VAE} = ||x - \\hat{x}||_2^2 + ||sg[Z] - \\hat{Z}||_2^2 + \\beta ||Z - sg[\\hat{Z}]||_2^2$, (3)\nwhere $sg[\\cdot]$ denotes the stop-gradient operator and $\\beta$ is a hyperparameter for loss balancing. VQ-VAE discretizes the continuous latent space into a codebook, where trained embedding vectors can be considered as foundational visual elements or intrinsic features that constitute the imagery. However, this characteristic implies that the representational capability of the codebook is highly sensitive to the codebook size and its utilization rate, which remains a core challenge that researchers in VQ-based generative models are striving to resolve.\n3.2. 2D Gaussian Quantization\nConsidering the above weakness, we propose a novel paradigm of feature quantization that introduces the concept of 2D Gaussian distribution. We extend the original quantization vector $z_i$, which merely encompasses individual features, into a featured 2D Gaussian quantization unit $g_k$ describing localized features within a certain region, rather than fixed grid. Instead of directly replacing the quantization of feature vector $z_i$ by the corresponding quantized vector $z_i$, we aggregate the contributions $c_{ki}$ of all the quantization units $g_k$ for $k \\in [1, K]$ at position $p_i = (x, y)$:\n$z_i = q'(z_i) = \\sum_{k=1}^{K} c_{ki}, i \\in [1, h \\times w]$ (4)\nwhere $K$ denotes the number of Gaussian units. Specifically, each featured 2D Gaussian unit is characterized by its position $\\mu_k \\in \\mathbb{R}^2$, covariance matrix $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$ and additional feature coefficient $\\hat{\\zeta} \\in \\mathbb{R}^D$. Therefore, the contribution $c_{ki}$ can be formulated with the probability $\\pi_{ki}$ of 2D Gaussian distribution as follows:\n$c_{ki} = \\pi_{ki} s_k = \\frac{1}{s_k} \\exp(-\\frac{1}{2} (p_i - \\mu_k)^T \\Sigma^{-1} (p_i - \\mu_k)) s_k$. (5)\nSince the covariance matrix $\\Sigma$ of a Gaussian distribution must be positive semi-definite, it is necessary to ensure"}, {"title": "3.3. Gaussian Embedding", "content": "Building upon the 2D Gaussian quantization discussed above, we further propose a Gaussian Embedding module to learn meaningful Gaussian representations leveraging image features. As depicted in Figure 2, our pipeline commences with two primary information carriers: the pristine image features and the 2D Gaussian objects. Essentially, we adopt the Gaussian Embedding module as a conduit for information exchange, which keeps refining the learned properties through a series of operations. Below, we delve into the intricacies and underpinnings of the relevant operations.\nLifter Module. As a preparatory step for subsequent modules, the Lifter Module transforms the two primary information carriers into unified vectors respectively. To adapt it for attention architecture, the Feature Lifter flattens the feature map Z from encoder $E_\\phi$ into a sequence of features $\\hat{S} \\in \\mathbb{R}^{(h \\times w) \\times D'}$. Note that $D'$ is significantly larger than the channel dimension $D$ of the desired quantized feature map Z so as to preserve more image information for Gaussian representations learning. Additionally, we also concatenate the sequence with a cosine position embedding, thereby equipping the model with the capability to discern the order. On the other hand, the Gaussian Lifter initiates the properties of the featured 2d Gaussian anchors $G$ and their associated high-dimensional queries $Q$. Since each anchor $g_k$ is refined in the form of target Gaussian parameters, we maintain a multi-layer linear perceptron (MLP) to get the embedding feature $\\hat{g}_k \\in \\mathbb{R}^{D'}$ to ensure seamless interaction with the query. Eventually, we obtain the unified representation of the two entities in the latent space with an embedding dimension of $D'$.\nSelf-attention. We employ the self-attention layers on both visual features sequence $S$ for further compressing image information and queries $Q$ for incorporating interactions among 2D Gaussian anchors. Note that we substitute the Deformable attention (DA) [46] for the Transformer attention [13] when processing the visual features sequence, thereby mitigating the high computational complexity of $O(N^2)$ and the challenge of handling high-resolution features, which we will elaborate on in the following module.\nCross-attention. The randomly initialized 2D Gaussian objects extract visual information within the cross-attention (CA) module, which is also based on DA. Specifically, for the 2D Gaussian anchor $g_k$, we generate a series of offsets $\\Delta \\mu_{ki}$. We further derive a series of reference points $R_k = \\{\\mu_k + \\Delta \\mu_{ki} | i = 1, ..., R\\}$ through combining $\\Delta \\mu_{ki}$ with the position of the anchor $\\mu_k$. Then we calculate the corresponding attention weights $A_{ki}$, which are subsequently used for weighted summation of the values:\n$\\text{CA}(g_k, q_k, x) = \\sum_{i=1}^{R} A_{ki} \\cdot W_x (\\mu_k + \\Delta \\mu_{ki})$, (9)\nwhere $x$ denotes the self-encoded image features and $W$ represents the linear transformation matrix that projects them to the values. The scalar attention weight $A_{ki}$ lies in the range [0, 1], normalized by $\\sum_{i=1}^{R} A_{ki} = 1$. Both $\\Delta \\mu_{ki}$ and $A_{ki}$ are obtained via linear projection over the query feature $q_e$ corresponding to the anchor $g_k$. In implementation, we merge the embedding feature $\\hat{g}_k$ of the anchor $g_k$ with the query feature $q_e$ prior to the module input, which enhances the connection between them.\nRefinement. We employ the refinement module to rectify the properties of anchors $G$ with guidance of the query result $Q$ from the preceding cross-attention module. To elaborate, we first decode the property adjustment quantities $\\Delta g$ for anchor $g \\in G$ from $Q$ via a MLP:\n$\\Delta g = \\{\\Delta \\mu, \\Delta \\theta, \\Delta \\varsigma, \\Delta \\zeta\\} = \\text{MLP}(Q)$. (10)\nIn the specific adjustment strategy, we employ a more rational approach that we directly replace the variables requiring rapid adjustment in $g$ with $\\Delta g$, including the rotation angle $\\theta$, the scale factors $\\varsigma$ and the feature coefficient $\\zeta$ (except for the position $\\mu$). Considering that $\\mu$ determines the region within the feature map affected by the anchor, frequent replacement to $\\mu$ could disrupt the optimization of all the other properties, leading to instability in the training process. Instead, we refine $\\mu$ of the anchor by adding a residual adjustment quantity $\\Delta \\mu$ to it:\n$g_{new} = \\{\\mu + \\Delta \\mu, \\Delta \\theta, \\Delta \\varsigma, \\Delta \\zeta\\}$. (11)\n3.4. GaussianToken\nWe present the overall framework of our GaussianToken, as illustrated in Figure 3. Subsequently, We elucidate the advantages of GaussianToken over previous methods from the fundamental structural design.\nAnalysis. Firstly, the sparsity of the 2D Gaussians promotes an efficient representation capability. Capitalizing on the local influence of 2D Gaussian distribution, each GaussianToken can affect all features $z_i$ within a surrounding region. Therefore, with an equivalent quantity of tokens, GaussianToken constructs a more flexible discrete representation space compared to VQ-VAE [32]. Practically, we might achieve superior reconstruction performances with a smaller embedding dimension.\nIn addition, GaussianToken presents an effectively accelerated convergence rate during training. We define the valid coverage region ($c_{ki} \\neq 0$) of the 2D Gaussian distribution of unit $g_k$ as $\\Omega_k$. The error propagated to $g_k$ during the backpropagation optimization can be calculated as:\n$L(g_k) = \\sum_{p_i \\in \\Omega_k} L(z_i) \\cdot \\pi_{ki}$, (12)"}, {"title": "4. Experiments", "content": "In this section, we conducted extensive experiments to verify the effectiveness of the proposed GaussianToken. We performed image reconstruction tasks on the CIFAR, Mini-ImageNet, and ImageNet-1K datasets, respectively. Additionally, we provided in-depth ablation studies for corresponding analysis and reconstructed visualization results for more intuitive comparisons. All our experiments were conducted on 8 RTX 3090 GPUs.\n4.1. Datasets\nThe CIFAR [15] dataset can be divided into CIFAR-10 and CIFAR-100 based on the categories, while we ignore the category factor for image reconstruction. The entire dataset contains 60,000 samples with a resolution of 32\u00d732, with the training set comprising 50,000 images and the test set containing 10,000 images. Mini-ImageNet [34] is a carefully curated few-shot learning dataset derived from ImageNet-1K [26], featuring a higher spatial resolution compared to CIFAR. Mini-ImageNet encompasses 100 distinct categories, each with 600 images. We employ 48,000 images for training and the remaining 12,000 images for testing. Additionally, ImageNet-1K comprises 1,000 categories, with the training set containing over 1,280,000 samples, while the test set consists of 5,000 images.\n4.2. Implementation Details\nWe conveniently implemented GaussianToken by adding the Gaussian Embedding module after the encoder of the original VQGAN [9] baseline while maintaining other structures and hyper-parameters. We set the resolutions of input images as 32 \u00d7 32 for CIFAR and 256 \u00d7 256 for Mini-ImageNet and ImageNet-1K. The downsampling ratios for the three datasets are 4, 16, and 16, respectively. We fixed the embedding dimension to 4 for CIFAR and 8 for Mini-ImageNet and ImageNet-1K. We used a codebook size of 1024 for both CIFAR and Mini-ImageNet, while we set the size to 2048 for ImageNet-1K. We adopted B = 3 transformer blocks in the Gaussian Embedding module as the default setting to refine the properties of 2D Gaussians. We respectively employed two Adam [6] optimizers with the same settings for the discriminator and the other GaussianToken model structures. We set the base learning rate to 1 \u00d7 10-4, \u03b21 = 0.5, \u03b22 = 0.9, and the weight decay to 0. We adopted a cosine schedule with the warmup epoch of 1. We respectively trained our models for 30, 30, and 20 epochs for the three datasets considering the training time.\n4.3. Main Results\nWe compared our GaussianToken framework with existing methods under similar settings across three datasets. We ensured consistent spatial resolutions (number of tokens) and downsampling ratios. The comparison results are presented in Table 1. We observe that GaussianToken demonstrates significant efficiency compared to the baselines. For example, on the CIFAR dataset, GaussianToken with an embedding dimension of 4 and a training epoch of 30 outperforms VQGAN by 14.01 in rFID. In addition, GaussianToken can also achieve superior results on the Mini-ImageNet and ImageNet-1K datasets with less training time and a smaller embedding dimension compared to the comparison approaches. In particular, compared to LlamaGen [27], GaussianToken achieves a better reconstruction performance on ImageNet-1K with a smaller embedding dimension (8 vs. 256), a smaller codebook size (1024 vs. 16384), and fewer training epochs (20 vs. 40), resulting in an FID of 1.67 vs. 2.19. This is because GaussianToken leverages Gaussian parameters for adaptive optimization of local features, resulting in a discrete space with stronger representational capabilities under similar conditions.\n4.4. Ablation Studies\nTo further understand our GaussianToken, we conducted various ablation studies including embedding dimension, codebook usage, codebook size, and Gaussian number to verify its effectiveness on the CIFAR dataset.\nEmbedding Dimension and Codebook Usage. A significant advantage of GaussianToken is its ability to achieve efficient reconstruction with a relatively smaller embedding dimension. Therefore, we respectively used 2, 3, 4, and 8 as the embedding dimension, shown in Table 2. We find that the reconstruction performance of the model first increases and then decreases with the expansion of the embedding dimension. In addition, GaussianToken reaches the best performance when the dimension equals to 3. We further provided the codebook usage corresponding to different embedding dimensions in Figure 4. We observe that a smaller embedding dimension corresponds to a higher codebook utilization rate. Consequently, GaussianToken maintains a high codebook utilization rate without fully compressing the discrete space when the embedding dimension is equal to 3, thereby achieving optimal performance.\nCodebook Size. We verified the reconstruction performance of GaussianToken under codebook sizes of 512, 1024, 2048 and 16384, respectively. The comparison results are illustrated in Table 3. We observe that GaussianToken is not sensitive to changes in codebook size, which is because we adaptively perform local optimization on the limited codebook space to inherently enhance the representational capability of the discrete codebook. GaussianToken achieves optimal performance on two main metrics when the codebook size is 2048. As the codebook size increases, the utilization of the codebook decreases and finally falling below 50% when the size reaches 16,384. While codebook collapse is a major drawback of the traditional vector quantization method in VQGAN, this technical issue is not the focus of this paper and will not be discussed further. To rigorously validate our method, we set the codebook size to 1024 as the main experimental setup.\nGaussian Number. The Gaussian number directly affects the modeling capability, which we adjust from 16 to 128 to compare the reconstruction performance in Table 4. We observe that the performance progressively improves as the number of Gaussian components increases, albeit with a diminishing rate of enhancement. Specially, the gaussian number of 64 demonstrates a relatively superior result without an excessive computational cost of the model.\n4.5. Visualization\nWe visualized the image reconstruction results of GaussianToken on ImageNet-1K in Figure 5. We observe that GaussianToken is capable of reconstructing images of high quality and exhibits realistic details and textures compared with original samples."}, {"title": "5. Conclusion", "content": "In this paper, we have proposed GaussianToken as an effective image tokenizer with 2D Gaussian Splatting. We have adaptively learned the local quantization feature ranges through Gaussian positions, rotation angles, and scaling factors, which enhances the representational capability of the latent discrete space. We have further rendered all Gaussian parameters and obtained the final reconstruction results via an image decoder. We have validated the effectiveness of the proposed GaussianToken on the image reconstruction task and provided corresponding ablation analysis for performance comparisons.\nLimitations. GaussianToken focuses solely on the design of the discrete codebook space, dedicated to enhancing the corresponding representation capabilities, while the evaluation on downstream tasks such as image generation remains unexplored. Future work will involve comprehensive comparison experiments for image generation."}]}