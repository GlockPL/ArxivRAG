{"title": "Combining Planning and Reinforcement Learning for Solving Relational Multiagent Domains", "authors": ["Nikhilesh Prabhakar", "Ranveer Singh", "Sriraam Natarajan", "Harsha Kokel", "Prasad Tadepalli"], "abstract": "Multiagent Reinforcement Learning (MARL) poses significant challenges due to the exponential growth of state and action spaces and the non-stationary nature of multiagent environments. This results in notable sample inefficiency and hinders generalization across diverse tasks. The complexity is further pronounced in relational settings, where domain knowledge is crucial but often underutilized by existing MARL algorithms. To overcome these hurdles, we propose integrating relational planners as centralized controllers with efficient state abstractions and reinforcement learning. This approach proves to be sample-efficient and facilitates effective task transfer and generalization.", "sections": [{"title": "1 INTRODUCTION", "content": "Building multiple agents capable of learning to reason and act under uncertainty in large and complex environments has long been a cherished goal of AI. Reinforcement learning (RL) [37] and multiagent RL [1] techniques have long been developed for learning under uncertainty and in the presence of multiple agents, respectively. Several previous research efforts have extended these methods to hierarchical domains with multiple levels of state and action abstractions [7, 36, 38].\nStatistical Relational Learning and AI (StaRAI) [11, 32], on the other hand, have dealt with learning in the presence of varying numbers of objects and relations, i.e., in relational domains. However, relational RL [8] is relatively unexplored, and while some methods exist [42], they do not scale for large tasks and are certainly not easily extensible to multiagent settings. A promising direction is exploiting the combination of hierarchical (and relational) planning to explore multiple levels of abstraction and RL to learn lower-level policies [16, 20].\nInspired by the success in these different sub-areas of AI, we propose a method that leverages the power of a relational hierarchical planner to act as a centralized controller for multiagent learning in noisy, relational domains. Our proposed approach, called Multiagent Relational Planning and Reinforcement Learning (MaRePReL), uses planning for task decomposition, centralized control, and agent allocation, StaRAI for constructing task-specific representations, and deep RL for effective and efficient learning with these specialized representations.\nWe make the following key contributions: (1) As far as we are aware, we present the first multiagent system for relational multiagent domains that can generalize across multiple objects and relations. As we show in the related work, significant literature exists in multiagent systems, relational learning, and the integration of planning and learning. Ours is the first work to combine all these directions in the context of multiagent systems. (2) To achieve this, we develop MaRePReL, an integrated planning and learning architecture capable of multiagent learning under uncertainty in relational domains. Specifically, MaRePReL's effective learning and reasoning power stems from its representation of relational information, the decomposition of higher-level plans, and the use of deep RL at the lowest level. (3) Finally, we demonstrate our approach's effectiveness and generalization abilities in a few relational multiagent domains. We compare against different deep RL based multiagent baselines, including one that explicitly uses the sub-task information, and illustrate the superiority of our approach.\nThe rest of the paper is organized as follows: after reviewing the related work and presenting the necessary background, we outline our multiagent framework and discuss the algorithm in greater detail. We then present the experimental evaluation on a few relational multiagent domains before concluding the paper by discussing the areas of future research."}, {"title": "2 RELATED WORK", "content": "Research in RL in the past three decades has focused on several extensions that make them adaptable to several real-world scenarios. First, Hierarchical Reinforcement Learning (HRL) methods have been introduced to manage complex tasks by decomposing them into smaller, more manageable subtasks [36]. These allow for more efficient learning and problem-solving by utilizing multiple levels of abstraction. Second, Relational Reinforcement Learning (RRL) addresses the complexity of environments where states and actions consist of objects and the relationships between those objects [8]. RRL exploits a higher-order representation of the underlying relational structure to improve learning in such domains. Third, Multi-Agent Reinforcement Learning (MARL) has been developed to handle environments where dynamic changes arise from the presence and actions of other agents, making it particularly useful in competitive or cooperative multi-agent settings [1]. Before introducing our framework, which addresses all three challenges, namely hierarchies, relational structures, and multi-agent domains, we review the relevant literature for these three RL extensions.\nHierarchical Reinforcement Learning (HRL) algorithms have been developed to tackle the complexity of long-horizon tasks by breaking them down into smaller, more manageable subtasks. Frameworks such as the Options [38] and MAXQ [7] facilitate the learning of hierarchical policies across multiple levels of abstraction. By exploiting temporal abstraction, HRL transforms the original long-horizon task into a sequence of shorter-horizon subtasks, where each subtask represents a high-level action that spans a longer period than the lower-level actions carried out by agents deeper in the hierarchy. This hierarchical structure enhances the agent's ability to operate effectively over extended time horizons and significantly improves learning efficiency [31].\nMultiagent Reinforcement Learning (MARL) extends reinforcement learning to systems with multiple agents, where they interact with the environment to maximize cumulative rewards [1]. However, MARL introduces its own unique set of challenges. The first challenge is the curse of dimensionality, where the increasing number of agents leads to an exponential increase in the sizes of state and action spaces. The second challenge is the non-stationary nature of the environment due to the actions taken by the other agents. The final issue for MARL is the sample inefficiency due to the large amount of data required to train such agents [48].\nNumerous solutions have been proposed to address the challenges outlined above, falling into two main categories: those adapting the underlying architectures for the RL agents and those considering the overall tasks performed by the agents. In the former category, methods use function approximation techniques to combat the curse of dimensionality [2]. In addition, Centralized Training, and Decentralized Execution (CTDE) methods such as QMIX and MADDPG address the non-stationary nature of the environment [26, 33]. Generative modeling or mask reconstruction algorithms [19, 23] also fall in this category. In the latter category, hierarchical approaches such as HMARL [12] and HSD [47] utilize task decomposition and hierarchical structures in multiagent settings to define task abstractions and improve sample inefficiency by filtering out irrelevant parts of the state space. Additionally, the structured task hierarchies introduced in such methods can facilitate agent communication to address the non-stationary nature of different multiagent environments. While powerful in standard propositional (and continuous) settings, these methods do not address the challenge of a rich, relational structure in the environment.\nRelational Reinforcement Learning (RRL) considers the task of learning in environments where states and actions involve relationships between objects and their properties, i.e., relational domains [8]. In these domains, RL agents must explicitly learn to reason about and exploit the relationships between objects [39]. Previously, several works have demonstrated the need for a rich relational representation to be explicitly used inside the learning algorithms as against simply grounding all the objects and obtaining a feature-based representation [32, 40]. A key advantage of relational representations is their ability to support abstractions and facilitate generalization and effective transfer across tasks [11, 27, 41]. However, finding optimal policies in many relational domains is intractable even for moderately large problems [39]. To mitigate this issue, algorithms that incorporate guidance and domain knowledge as constraints on the policy space have been developed [28].\nPlanning and RL integration have been explored to exploit the power of hierarchical planning with deep RL enabling the use of HRL in continuous domains. While Taskable RL [16] demonstrated significant performance improvement, the underlying planner was still propositional, limiting their applicability to relational problems with varying numbers of objects and relations.\nAn ideal RL learning algorithm should be able to not only handle the rich relational structure of the domain but also have the ability to represent and reason with the decomposition of complex tasks into smaller ones. In other words, the algorithm must be capable of representing and reasoning with both hierarchies and relational structures. One such recent framework, RePReL [20], employs a hierarchical relational planner to implement task-specific policies and uses Deep RL to work on hybrid relational domains [20, 21]. To interface the higher-level planner with the Deep RL, a hand-crafted abstract reasoner is employed to lift the reasoning process and construct smaller lower-level Markov Decision Processes (MDPs) that can be solved efficiently. This approach has been demonstrated to be successful in domains with varying numbers of objects, complex task structures, and continuous state-action spaces.\nWhile the RePReL framework successfully handled relations and hierarchies in continuous spaces, it can not handle multiagent systems. More precisely, given the three-pronged challenge of complex task structures, rich object-centric environments, and multiagent domains, several advances have been made in each of these specific directions. Also, in the recent past, methods that arise from the combinations of these methods - for instance, HRL with RRL [7, 20, 35], MARL with RRL [4, 24], HRL with MARL [12, 17, 46, 47] \u2013 have been proposed. However, no significant research encompasses all three of these challenges (see Figure 1).\nIt is precisely this gap that we aim to address in this work. Specifically, we extend the RePReL framework to multiagent settings, utilizing the planner as both a scheduler and a centralized controller. Unlike RePReL, where the planner is solely responsible for task decomposition, our proposed framework also distributes tasks among multiple agents. This key enhancement enables our approach to effectively solve relational multiagent domains, as we explain in the next section."}, {"title": "3 MULTIAGENT RELATIONAL PLANNING AND REINFORCEMENT LEARNING (MAREPREL)", "content": "We consider the problem of coordinating multiple agents to solve continuous, relational problems. We will first provide a high-level overview of our framework, MaRePReL, combining relational and hierarchical planning with deep reinforcement learning before defining the problem formally. MaRePReL employs the following:\n(1) Planner as controller: One of our key contributions is to view the (relational) planner as a centralized controller that obtains the current state as input and creates a set of agent-specific plans. In the spirit of two-level systems, the planner not only decomposes the tasks into subtasks but also assigns the subtasks to appropriate agents. To facilitate this, the controller consists of two specific components:\n(a) Relational Hierarchical Planner: Recall that the goal is to decompose tasks in the presence of varying numbers of objects and relations between them. Consequently, the first sub-component is a relational, hierarchical planner, which uses a first-order representation to model the objects and relationships in the domain. As one can view hierarchies as a specific form of relations, this planner can decompose the goals into a temporally ordered series of subgoals.\n(b) Task Distributor: The planner output is typically the task decomposition and does not bind the tasks to the specific agents. We use a task distributor as part of the relational planner to divide the ordered plan provided into agent-specific sub-plans using agent constraints for the different tasks.\n(2) Abstraction Reasoner: Following the previous work on single-agent learning (RePReL), we use Dynamic-First Order Conditional Influence (D-FOCI) statements [29] to capture domain knowledge that is then used to reason and construct the relevant parts of the state space that the lower-level RL agents use. This step at the outset is similar to RePReL, where the inference step is still hand-crafted, and leveraging lifted inference [40] to perform this step automatically remains a future direction. However, it must be emphasized that this step is more difficult in the multiagent setting as the optimal allocation of tasks to agents requires considering all agent states. Hence, typical single-agent-based abstraction reasoners do not suffice for this setting as they do not capture the true optimal value functions.\n(3) Multiple Deep RL agents: Given the current subtask from the planner, the corresponding (deep) RL agent identified by the task distributor learns a generalizable, task-specific policy. Assuming that the abstraction reasoner identifies the relevant part of the state space, learning is both effective and efficient with the additional advantage of being generalizable since the learned policies can be shared among multiple agents (as shown in our experiments).\nThe broad overview of our proposed approach is presented in Figure 2. The planner decomposes the higher-level tasks into appropriate lower-level tasks using a relational representation of the current state and lifted operators. The distributor identifies the appropriate RL agent for the current subtask, thus making this combination an effective centralized controller. Given the subtask and the current (abstract/relational) state, the abstraction reasoner selects a smaller state representation by identifying the appropriate parts of the state space that are relevant to the current subtask. Finally, the RL agent either learns the policy or executes the ones it already learned (for instance, agent A1 might have learned the pickup subtask that can be used directly by agent A2 to execute this specific subtask). Note that while our experiments assume all the agents to be homogenous, this is not a necessary assumption for our formulation, where specific constraints can be used to allocate the tasks accordingly.", "latex": ["M = (N, S, A\u2208N, P, R\u2208N\u203a Y, G)", "A\u00b9", "Pr(s'|s, a)", "s, s' \u2208 S", "a \u2208 A", "R\u00b2 = S \u00d7A\u00d7S \u2192 R", "\u03b3 \u2208 [0, 1)", "GRMG", "(s \u2208 S, g\u2208 G)", "g\u2208 G", "\u03c0", "\u3008Q, O, C, M\\rangle", "P = (D, S, G, AG)"]}, {"title": "3.1 Problem Formulation", "content": "While one could envision using relational partially observable MDPS (RPOMDPs) to model the problem with the current task being the hidden component, issues arise when modeling it as one. First, the abstraction reasoner has to track the current hidden state actively. Second, the use of decentralized RPOMDPs [44] requires creating multiple RMDPs one for each task-agent formalism which would, in general, be larger than the smaller RMDPs created by our formalism using abstractions. Finally, for larger agent-task combinations, the reasoning over belief states requires approximate probabilistic inference over relational states and lifted probabilistic inference, which is outside the current work's scope. Therefore, we consider modeling the problem using Markov games [25]. We build upon the framework of relational Markov games [10], extending it to handle goal-oriented problems. We formalize this extension as a goal-directed relational Markov game (GRMG), defined as follows:\nDefinition 1: A goal-directed relational Markov Game (GRMG) is represented as M = (N, S, A\u2208N, P, R\u2208N\u203a Y, G) where N is the number of agents, S is the set of (relational) states, A\u00b9 is the set of actions for ", "latex": ["iEN'", "pickup(P,T): {taxi(T, L1), at(P, L)} in_taxi(P,T)"]}, {"title": "3.2 Relational Planner", "content": "The environment's state can be represented as an abstract planning problem using a planning description language [13]. The hybrid planning domain D = \u3008Q, O, C, M\\rangle, consists of a set of predicates Q that describes the current state, a finite set of operators O which are the high-level actions executable by the agents, a set of ordering constraints C that is necessary to construct a consistent plan, and methods M that can decompose the goal set into an ordered sequence of operators. A multiagent planning (MAP) problem can be defined as follows:\nDefinition 2 For a given domain D, a Multiagent Planning (MAP) problem P = (D, S, G, AG), consists of the initial state of the problem S, the set of goals G that need to be completed, and a group of agents AG that need to coordinate together to reach the goal state.\nFor the above MAP problem, the planner plays a crucial role by controlling the tasks performed by each agent. It maps the target set of goals G into a set of grounded task-specific operators O and distributes them to different agents. Hierarchical Task Network (HTN) planners such as SHOP [30] can be used to generate a total order plan for a given instance of the environment. The grounded plan (high-level plan) and a set of ordering constraints are used to distribute the tasks to create agent-specific plans (sub-plans). A greedy approach is used to schedule tasks that involve forward chaining [22]. It examines the causal links between operators and prevents tasks from being assigned to agents that cannot execute them. The causal links L = (11, 12,, In) define a partial ordering between operations. Each link is of the form (Op, eff, Oq) where eff is the effect of completing task Op and one of the preconditions for task Og [45]. Our task distributor ensures that the same agent performs the causally linked operations.\nFor each agent a, the task distributor returns a partial plan \u041f\u00ba = [01, 02, 03, ..., On] where o is an operator with I(0) being the precondition of the operator and \u1e9e(o) being the necessary effects of the operator. Since the operators only consider the action space of the agent currently using them, and the operators are shared among the different agents, all working on the same underlying environment, we can define the sub-goal RMDP Mo for each operator to solve the problem like in RePReL [20]."}, {"title": "3.3 Task-specific Abstraction", "content": "While the planner decomposes the task and the task distributor identifies the appropriate agent, the resulting state space can still be prohibitively expensive for effective learning. Consequently, the abstraction reasoner becomes crucial in constructing a smaller state space. In GRMG, states are represented as conjunctions of literals. Like RePReL, prior knowledge that describes the relation between rewards, and sub-goals is then described using an extended First Order Conditional Influence statements (FOCI) [29] called Dynamic FOCI (D-FOCI) statements. D-FOCI statements, represented by an example below, are the first-order language rules used to specify the direct conditional influences between literals in the domain."}, {"title": "3.4 MaRePReL Algorithm", "content": "Algorithm 1 presents the procedure where we initialize the RL policies and buffers for various operators (line 1). The policies learned through our approach are a collection of task-specific operations. While one or more agents still have pending subtasks, we continuously collect trajectories from the environment for different operators, storing them in respective operator buffers (lines 7-15). In each episode, we first get the starting state from the environment (line 4), and obtain the current tasks for each agent by computing their sub-plans using our relational multiagent planner, which is implemented by combining a SHOP [30] planner with branch and bound scheduling (line 5-6). While one or more agents have some task remaining, we first compute the joint actions for the agents based on the current state, agent tasks, task policies, and the D-FOCI statements using the GetAgentActions method (line 8). Upon obtaining the joint action, we perform a step update using the RePReLStep method (line 9). This step involves updating the state, buffers, plan, and tasks based on the D-FOCI rules for abstractions. Following RePReL's approach, the method returns the updated components along with a flag indicating the validity of the current plan. If the plan is considered invalid \u2013 one or more agents cannot perform the tasks assigned to them \u2013 then new agent-specific plans (subplans) are computed, and the agents are assigned new tasks (lines 10-13). Once the episode has ended, we train a policy for each operator \u03c0\u03bf using a batch sampled from the buffer for the operator (line 16-19). Once trained, the final policies for the different operators are returned. The methods GetAgentActions and RePReLStep are further detailed in the supplementary 1", "latex": ["\u03c0 = {\u03c0\u03bf|\u03bd\u03bf \u03b5 \u039f}"]}, {"title": "4 EXPERIMENTAL RESULTS", "content": "We present our results across different tasks in three relational multiagent domains that demonstrate the effectiveness of MaRePReL. We answer the following questions explicitly.\n(1) Does MaRePReL improve sample efficiency ?\n(2) Does MaRePReL efficiently transfer from one task to another?\n(3) Does MaRePReL generalize to varying number of objects?", "latex": []}, {"title": "4.1 Domains", "content": "For the first environment, we extend the taxi domain environment [6] to relational multiagent settings. The goal is to transport passengers from their current locations to their destinations. Passengers are located at four different grid positions - R, G, B, and Y -- requiring coordinated efforts from the taxis for pickup and drop-off, with no two passengers having the same pickup or drop locations. Additionally, the taxis cannot cross each other or occupy the same location. Doing so would cause crashes, terminating a huge negative reward and incurring a heavy penalty.\nFor the second environment, we extend the office world domain [15] to accommodate multiple agents. The agents are presented", "latex": []}, {"title": "4.2 Baselines", "content": "We evaluate MaRePReL against several standard MARL algorithms, including Deep Q-Networks with parameter sharing (DQN-PS), Deep Q-Networks as independent learners (DQN-IL), and QMIX [33]. In DQN, each agent maintains its decentralized state-action value function, updating Q-values based solely on local observations and individual rewards. In contrast, QMIX utilizes a parameterized mixing network to compute a joint Q-value, combining information from all agents. However, it is essential to note that QMIX is not included in the benchmark for the multi-agent dungeon environment due to a key limitation: QMIX requires a fixed number of agents to function properly, which is not guaranteed in the dungeon environment as the number of agents change when an agent dies.\nAdditionally, we introduce a new baseline called DQN-PE (DQN with Plan Embeddings), inspired by HTN-MTRL [14]. In DQN-PE, the observation for each agent is augmented with a vector embedding of its current sub-plan. This approach leverages the same HTN planner as MaRePReL and encodes the sub-plan string using a BERT embedding model [34]. The string is divided into chunks, mean pooled, and reduced to a R4 embedding. This baseline allows us to assess whether providing agents with task-specific information in the form of the agent's sub-plan embeddings is sufficient, or whether task hierarchies and state abstractions are required like in MaRePReL. As far as we know, no relational multiagent baselines are readily available. One could design a relational multiagent baseline by considering hierarchies as a special form of relations (in the lines of the work inside RRL community [5, 43]), but extending them to a multiagent scenario is non-trivial and outside the scope."}, {"title": "4.3 Results", "content": "We evaluate sample efficiency, transfer ability and generalization capability of our method against the other baselines across the different tasks (Table 2). The results, averaged over five trials, are presented in Figure 3 (training from scratch), and 4 (training starting with a previously learned policy), where the bold line represents the mean and the shaded region illustrates the variance of the success rate across trials after 3 million environment steps.\nSample Efficiency: In the taxi domain, MaRePReL, unlike DQN and QMIX, was able to learn how to complete tasks 1, 2, and 3, whereas DQN-IL, DQN-PS, DQN-PE and QMIX have a near-zero success rate even after training for 3 million steps (Figure 3 a-c). In the Office World domain, DQN-PS learned an optimal policy for Tasks 1 and 2 (Figure 3 d-e). This can be attributed to the static nature of the goals in the environments. However, as the complexity of the tasks increased, such as in Task 3 (Figure 3 f), none of the baselines could perform while MaRePReL still was able to show early success. In the Dungeon domain, MaRePReL demonstrates robust performance, converging to optimal solutions for Task 1 and 2 while showing a steeper learning curve for Task 3. The baseline of DQN-PS exhibits convergence for Task 1 after one million steps, although notably slower compared to MaRePReL. However, DQN-IL, DQN-PS, and DQN-PE struggled with learning Tasks 2 and 3, showing a near-zero success rate in both cases (Figure 3 g-i). QMIX can't be used in the dungeon environment due to its changing number of agents. Therefore, Q1 can be answered affirmatively.\nTransfer: In the Office World domain, we transfer policies from Task 1 (Visiting A, B, C, D) to Task 2 (Get Mail and Coffee), and in the Dungeon domain, from Task 2 (Dungeon with 1 Dragon) to Task 3 (Dungeon with 1 Dragon and 1 Skeleton). In the Office World experiment, MaRePReL successfully adapted to the new task and nearly achieved 100% success, whereas the baselines struggled- QMIX, DQN-IL, and DQN-PE failed, and DQN-PE had a low success rate (Figure 4b). In the Dungeon experiment, when tasks shared common goals, transferring with MaRePReL facilitated convergence, while neither QMIX nor any DQN baselines showed success (Figure 4c). These results demonstrate the transfer abilities of a relational model, a fact well-known within the relational RL community. The key power of relational models (in our case, relational planner and relational abstract reasoner) lies in their ability to achieve efficient transfer and effective generalization. Therefore Q2 can be answered affirmatively.\nGeneralization: In this case, the policies are not randomly initialized; instead, the policies learned for tasks with fewer objects are applied to the new task with more objects. For the taxi world domain, a policy trained on the task of transporting two passengers is applied to the task of transporting four passengers (Figure 4a). MaRePReL significantly improves sample efficiency, achieving Task 3's success rate in less than half a million steps, compared to 3 million steps for the non-transferred policy. Similarly, one can notice that MaRePReL generalizes to a task presenting an increasing number of enemies to defeat in the Dungeon environment. Other baselines do not demonstrate any initial success or show any performance improvement. Therefore, Q3 can be answered affirmatively"}, {"title": "5 DISCUSSION AND FUTURE WORK", "content": "We demonstrated empirically that MaRePReL significantly outperforms traditional MARL approaches, including DQN (independent learners or parameter sharing or with sub-plan embeddings) and QMIX. Our results clearly show the effectiveness of combining a relational planner with an agent-specific task distributor at the higher level and deep RL at the lower level. Significant improvements can be observed in both learning, transfer, and generalization.\nOur framework has a few limitations. As the number of operators and agents increases, the search space for the relational planner grows exponentially, posing challenges for generalization, an important future direction. Our current formalism applies only to problems featuring a fully observable state space, and extending it to partially observable spaces necessitates integration with efficient (lifted) probabilistic inference. Moreover, the cooperation shown between agents is loosely coupled as they work in parallel to complete the tasks assigned by a centralized planner. It is possible to extend our approach to tackle challenges in domains that demand coordination among multiple agents [3, 9] by incorporating a partial-order planner along with wait operators. This extension would allow all agents to achieve a state that fulfills the preconditions before performing the joint task. Finally, constructing a fully differentiable system is an interesting direction for future research."}]}