{"title": "SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation", "authors": ["Xin Li", "Siyuan Huang", "Qiaojun Yu", "Zhengkai Jiang", "Ce Hao", "Yimeng Zhu", "Hongsheng Li", "Peng Gao", "Cewu Lu"], "abstract": "Automating garment manipulation poses a significant challenge for assistive robotics due to the diverse and deformable nature of garments. Traditional approaches typically require separate models for each garment type, which limits scalability and adaptability. In contrast, this paper presents a unified approach using vision-language models (VLMs) to improve keypoint prediction across various garment categories. By interpreting both visual and semantic information, our model enables robots to manage different garment states with a single model. We created a large-scale synthetic dataset using advanced simulation techniques, allowing scalable training without extensive real-world data. Experimental results indicate that the VLM-based method significantly enhances keypoint detection accuracy and task success rates, providing a more flexible and general solution for robotic garment manipulation. In addition, this research also underscores the potential of VLMs to unify various garment manipulation tasks within a single framework, paving the way for broader applications in home automation and assistive robotics for future. The project page is available at sites.google.com/view/keypoint-garment.", "sections": [{"title": "I. INTRODUCTION", "content": "Garments, as one of the most ubiquitous items in home environments, have long been a focal point in assistive robotics research [1, 2]. Tasks such as washing, folding, and ironing garments exemplify how robots can assist with ev- eryday household activities. However, despite advancements in robotic vision and manipulation technologies, accurately recognizing and manipulating garments remains a challenge due to their diverse shapes and deformability [3, 4]. Robots must not only recognize keypoints on garments but also adapt to the constantly changing states of these flexible objects to perform precise manipulations. This adaptability is essential for ensuring reliable performance in complex real- world scenarios.\nTraditional garment manipulation methods often depend on 3D data and class-specific keypoint recognition mod- els [5-7], which are inherently limited by their inability to generalize across various garment types or states. These models typically perform well only in narrow contexts, strug- gling to infer or adapt to unknown garment configurations or unstructured states. For instance, a model trained to"}, {"title": "II. RELATED WORK", "content": "A. Robotic Garment Manipulation\nRobotic manipulation of garments is a critical challenge in assistive robotics due to the deformable and highly variable nature of clothing items [1, 6, 8-10]. Much of the exist- ing research has focused on key tasks such as unfolding (flattening) and folding garments. While unfolding systems have made notable progress [2, 6, 11], they often leave garments imperfectly flattened, and their ability to handle a wide variety of clothing types and environmental conditions remains limited.\nTraditionally, folding tasks for flattened garments rely on predefined state representations and scripted policies [2, 12]. Researchers have explored various approaches for generating these state representations, including template fitting [1, 2] and semantic keypoint detection [6, 13-15]. Many of these methods leverage depth images [14\u201316], which are advanta- geous for capturing geometric data unaffected by lighting or background clutter. However, depth images can omit critical visual information such as garment patterns and seams, which can be important for accurate manipulation [16]. To address these limitations, our work utilizes RGB images, which offer richer visual information that can be crucial for precise garment manipulation.\nB. Synthetic Data for Robotic Garment Manipulation\nThe use of synthetic data has become increasingly preva- lent in robotic cloth manipulation, particularly for training models that need to generalize across diverse garment config- urations and manipulation tasks [2, 5, 14, 17, 18]. However, creating high-quality 3D assets that represent a wide variety of garments and states remains a significant challenge. Some approaches rely on limited sets of manually annotated pre- made cloth meshes [2, 14, 19], while others use procedural generation techniques to produce single-layer meshes [5, 17]. Although large-scale datasets like Cloth3D [4] have been developed, they often lack detailed semantic annotations required for precise manipulation tasks.\nIn this work, we extend the use of synthetic data to im- prove keypoint detection in garment manipulation tasks. Our pipeline generates diverse garment configurations, including varying deformations, allowing us to train models that can adapt to different garment types, states, and environmental conditions. By leveraging this approach, we aim to enhance the robot's ability to recognize key manipulation points on garments, enabling more efficient and accurate operations in real-world scenarios.\nC. Dense Representations for Garment Manipulation\nDense object descriptors, which capture point- or pixel- level object representations, have been widely applied to various robotic manipulation tasks [20]. These descriptors have been extended in numerous works to propose grasp- ing poses [21, 22], manipulate deformable objects like ropes [23], and smooth fabrics [24]. Additionally, point-level affordance learning has been explored for articulated objects [25, 26], deformable objects [27], and even in tasks involving language-guided [28] and bimanual manip- ulation [6, 29]. These approaches enable more effective interaction and contact point selection, facilitating a variety of downstream tasks [6, 30]. In this paper, our work extends the use of dense correspondence by applying these dense point representations specifically to deformable garment ma- nipulation. By leveraging point-level affordance learning, we aim to enhance the robot's ability to detect manipulation- relevant keypoints and improve performance across diverse garment states."}, {"title": "III. METHOD", "content": "In this section, we provide a detailed introduction to our proposed vision language keypoint prediction method for robotic garment manipulation. Our approach is based on the integration of vision-language models to enhance the robot's ability to recognize keypoints on garments in different states in a unimodel, improving both flexibility and accuracy during manipulation tasks. To achieve this, we have developed a synthetic garment dataset and designed a keypoint prediction framework, which are described in the following subsections.\nA. Synthetic Dataset Generation\nTo simulate the diverse and deformable nature of garments, we developed a synthetic dataset that encompasses a broad"}, {"title": "B. Paired Keypoint Representation", "content": "In garment manipulation, a set of keypoints-potentially represented as a nested structure, such as a skeleton-can effectively capture the state of a garment, regardless of its condition (e.g., degrees of wrinkling) or original shapes (including sizes and styles) [6]. Compared to general repre- sentation formats used in MLLMs, such as bounding boxes, which may lack visual constraints in certain positions [33], keypoints retain critical structural information. This makes them more effective for capturing the nuances of garment shapes and configurations, as illustrated in Fig. 1. More- over, downstream policies primarily focus on the operational relationships between corresponding keypoints-essentially aligning Point A to Point B. By utilizing keypoint tuples for representation, we ensure compatibility with these ex- isting policies, facilitating seamless integration into various manipulation strategies. To guide our model in learning the keypoint representation, we use the keypoints generated in"}, {"title": "C. Action Tuple Trajectory Generation", "content": "Based on the keypoint representation proposed before, we can frame the task of garment folding action prediction as first implicitly identifying a set if keypoint coordinates tuple {(X1,Y1), (X2,Y2), (x3, Y3), (x4, Y4)} that correspond to the optimal grasping points for folding, based on the garment's state. hese keypoints are then formulated into action tuples, such as LA((x1,y\u2081), (x2,y2)) and RA((x,y), (x2, y2)), where LA and RA represent the \"left arm\" and \"right arm,\u201d respectively, highlighting our method's capability to support bi-arm manipulation. This tuple representation not only links points, providing more structured information than discrete keypoints, but also ensures compatibility with robotic action primitives. By focusing on these key coordinates, the model can directly translate them into manipulation actions like grasping and folding, thereby enhancing precision. After obtaining the paired action point, our action decoder gener- ates an action trajectory conditioned on the action primitive. The action model is based on manually designed rules. Furthermore, the proposed SKT is adaptable to various gar- ment types and folding scenarios, effectively guiding robotic movements and identifying key contact points. Training with multi-task data further enhances the model's generalization across diverse robotic manipulation tasks."}, {"title": "D. Vision Language Model Fine-Tuning", "content": "1) Model Architecture: As shown in 2, our vision lan- guage model is built upon the SPHINX-X framework [33], with LLaMA2 serving as its core language backbone. We chose this model due to its unique capability to concentrate on fine-grained, region-specific details of objects, making it well-suited for tasks requiring detailed visual analysis. Our model employs the \u201cany resolution\u201d strategy introduced by SPHINX [33]. The input images are divided into smaller"}, {"title": "IV. EXPERIMENTS", "content": "A. Experimental Settings\nTo train and evaluate the effectiveness of the proposed method SKT, we not only used synthetic data but also introduced the aRTF clothing dataset [5]. aRTF dataset is collected from 14 real-world domestic settings, each char- acterized by unique environmental features and a variety of clothing items. It is meticulously segmented into 6 training scenes and 8 testing scenes, encompassing 15 towels and T- shirts within the training set, and an increased quantity of 20 in the test set. Similarly, the dataset comprises 8 shorts for training and 9 for testing. Notably, for each category, the dataset offers 210 images for training and an expanded set of 400 images for testing, with the exception of shorts, which are represented by 112 training images and 180 test images. More formally, we train the SKT model on a syntehtic dataset of 20,000 images and integrate the real-world data by employing fine-tuning techniques on the aRTF training dataset, thereby enhancing SKT's adaptability and accuracy across various garment types, including T-shirts, shorts, and towels.\nWe fine-tuned the SKT within the SPHINX frame- work [33] and utilized eight NVIDIA A100 GPUs, each"}, {"title": "B. Metrics", "content": "To quantitatively analyze the advantages of the SKT method, we adhere to the experimental setup delineated in [37] and have conducted the following metric calculations on the aRTF dataset: Mean Average Precision (mAP) is crucial for evaluating the accuracy of our model in detecting keypoints. We have calculated the mAP at three distinct L2 distance thresholds 2, 4, and 8 pixels from the ground truth keypoints. This approach allows us to measure the model's precision at various letolerance levelswhich is essential for understanding its robustness in different scenarios. Average Keypoint Distance (AKD) serves as a direct measure of the geometric accuracy of the predicted keypoints relative to the ground truth. By averaging the distances, we obtaiclearly indicatemodel's overall precision in spatial localization."}, {"title": "C. Experiental Results", "content": "1) Comparison on type-specific method: In this experi- ment, we evaluate the performance of our SKT model and compare it against type-specific models under two different training settings: Sim-to-Real and Sim+Real-to-Real. The Sim-to-Real setting involves training the models solely on synthetic data, while the Sim+Real-to-Real setting incorpo- rates both synthetic and real data during training. The results are presented in Table I.\nSim-to-Real: Under the Sim-to-Real setting, the SKT model significantly outperforms the type-specific models across all garment types. The SKT model achieves an mAP2,4,8 of 63.3 for T-shirts, 56.7 for shorts, and 83.9 for towels, notably surpassing the best-performing type- specific models. In contrast, the type-specific T-shirt, Shorts, and Towel Detectors achieve only 58.2, 51.4, and 83.2 in"}, {"title": "D. Ablation Study", "content": "To analyze SKT's design contributions, we conducted ablation studies following the original Sim-To-Real setting. Specifically, we examined three variations: 1)Replacing the \u201cAny-Resolution\u201d method from SPHINX [33] with a default 224x224 image resolution for visual input. 2)Eliminating key-point detection tasks during training, focusing solely on direct trajectory tuple learning. 3)Implementing one-stage training instead of two-stage training.\nTable II illustrates that without high-resolution image input, model performance significantly deteriorates. We at- tribute this to the necessity of detailed visual information for cloth manipulation, as manipulation targets typically comprise only a small portion of the garment. Omitting key- point detection tasks also leads to suboptimal performance. We posit that these tasks serve as a bridge between the final action representation and pretrained general VQA tasks, facilitating better learning of action representation. Moreover, key-point detection tasks increase the volume of training samples, benefiting MLLM model training. Lastly, abandon-"}, {"title": "E. Discussions", "content": "We manually collected additional data that is not available in the existing dataset to evaluate the model's robustness, including samples with long pants, various folding states with deformations, and long sleeves for simple testing, as shown in the figure. The results indicate that our proposed method demonstrates relatively robust performance in handling pants and garments with different folding states. Notably, the model's ability to generalize from short pants to long pants is quite impressive, showcasing its potential for cross-category inference. However, it struggles with long sleeves, partic- ularly folded long sleeves, which were not present in the training data. It indicates the need for further exploration of the model's generalization capabilities, especially for unseen or diverse garment types and deformations."}, {"title": "V. CONCLUSION", "content": "This paper introduces a novel approach SKT to robotic garment manipulation using vision-language models, ad- dressing the challenge of handling diverse and deformable garments with a unified framework. We propose a state- aware paired keypoint trajectory formulation that enhances the generalization across various garment states, including flat, folded, and deformed configurations. Additionally, we created a large-scale synthetic dataset with diverse garment states, significantly improving scalability by reducing de- pendence on real-world data. Experiments demonstrate that the integration of reasoning-based vision-language models enhances the robot's adaptability in complex scenarios for garment manipulation."}]}