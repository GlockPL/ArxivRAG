{"title": "M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models", "authors": ["Jiaxin GUO", "Daimeng Wei", "Yuanchang Luo", "Shimin Tao", "Hengchao Shang", "Zongyao Li", "Shaojun Li", "Jinlong Yang", "Zhanglin Wu", "Zhiqiang Rao", "Hao Yang"], "abstract": "With the widespread application of Large Language Models (LLMs) in the field of Natural Language Processing (NLP), enhancing their performance has become a research hotspot. This paper presents a novel multi-prompt ensemble decoding approach designed to bolster the generation quality of LLMs by leveraging the aggregation of outcomes from multiple prompts. Given a unique input X, we submit n variations of prompts with X to LLMs in batch mode to decode and derive probability distributions. For each token prediction, we calculate the ensemble probability by averaging the n probability distributions within the batch, utilizing this aggregated probability to generate the token. This technique is dubbed Inner-Batch Ensemble. To facilitate efficient batch inference, we implement a Left-Padding strategy to maintain uniform input lengths across the n prompts. Through extensive experimentation on diverse NLP tasks, including machine translation, code generation, and text simplification, we demonstrate the efficacy of our method in enhancing LLM performance. The results show substantial improvements in BLEU scores, pass@krates, and LENS metrics over conventional methods.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (OpenAI, 2023; Touvron et al., 2023a,b; Bai et al., 2023; Yang et al., 2024) have demonstrated exceptional capabilities in understanding and generating natural language through extensive data pre-training, becoming the core driving force in the field of Natural Language Processing (NLP). Prompt technology (Jiang et al., 2020; Liu et al., 2023; Pitis et al., 2023; Zhao et al., 2023; Heineman et al., 2024; Wei et al., 2022; Wang et al., 2023), as a key to enhancing LLMs' performance, can strengthen the model's effects without altering model parameters, achieving seamless integration with downstream tasks.\nHowever, in the practical application of LLMs, the output results are closely related to the quality of the Prompt, and a precise and effective Prompt is crucial for improving the model's response quality. How to use Prompts more efficiently to fully leverage the potential of LLMs has become a hot issue of common concern in academia and industry.\nEnsemble decoding (Lakshminarayanan et al., 2017; Ganaie et al., 2022; Zhou et al., 2002) is a widely employed technique for enhancing the quality of model-generated outputs. Typically, as shown in Figure 1(a), standard ensemble decoding refers to the process of combining the outputs of multiple distinct models on the same input. Specifically, during the prediction of each word, ensemble decoding generates a comprehensive prediction by averaging the probability distributions provided by multiple models. This approach leverages the diversity among models, aiming to reduce uncertainty and improve overall predictive performance. In theory, we could apply ensemble decoding to LLMs. However, LLMs already demand considerable memory resources, and the implementation of ensemble decoding with multiple LLMs presents a significant challenge in terms of memory usage. Moreover, even if these models could be deployed, optimizing the performance of the ensemble decoding becomes a complex issue.\nIn this paper, we introduce a particularly simple yet effective method: Multi-Prompt Ensemble Decoding (M-Ped). This approach constructs n distinct prompts for a single query, generating n diverse input samples that are batched together and submitted to LLMs for inference. During the inference process, we average the prediction probabilities within the batch for each word prediction. To ensure batched inference is feasible, we specifically propose the use of left-padding technology to address the issue of varying prompt lengths within a batch. Compared to traditional ensemble methods, our approach shifts the focus of diversity from using different models to using different prompts. We have validated the effectiveness of this method across various tasks and multiple models, including extensive experiments on multiple test sets for machine translation, code generation, and text simplification tasks. The results demonstrate improvements in BLEU scores for machine translation"}, {"title": "2 Approach", "content": "In this paper, we propose an approach named multi-prompt ensemble decoding, aimed at enhancing the generation performance of large language models. The overall process is depicted in Figure 2. For a given distinct input  X = {X_1,X_2, ..., X_k} with a prompt P, we first generate a list of prompts {P_1, P_2, ..., P_n}. Then, we submit these n prompts and the input X to LLMs in batch for decoding to obtain probability distributions. We average the n probability distributions generated at the j-th position prediction within the batch to get the ensemble probability, and ultimately determine the output Y's yj. To ensure batched inference is possible, we employ a Left-Padding strategy to ensure the lengths of the n inputs are consistent."}, {"title": "2.1 Inner-Batch Ensemble", "content": "Most LLMs adopt a Decoder-only architecture and utilize an autoregressive decoding strategy, generating output tokens one by one. Given the source sentence X = {X_1,X_2, ..., x_k}, LLMs factor the distribution over possible output sentence Y = {Y_1, Y_2..., Y_j } into a chain of conditional probabilities, satisfying the following formula:\nP(Y|X) = \\prod_{i=1}^jP(Y_i|Y_{0:i-1}, X)\nFor the Standard Ensemble, during the prediction of yj, we average the probability distributions provided by n models. We define these n models as {\\theta_1, \\theta_2, ..., \\theta_n}, and the formula is as follows:\nP(y_j|Y_{0:j-1}, X) = \\frac{1}{n}\\sum_{i=0}^{n} P(y_j|Y_{0:j-1}, X, P; \\theta_i)\nwhich P is a distinct prompt. This method leverages the diversity among models, aiming to reduce uncertainty and improve overall predictive performance.\nFor our method, we shift the focus of diversity from n models to n prompts, and the formula is as follows:\nP(y_j|Y_{0:j-1}, X) = \\frac{1}{n}\\sum_{i=0}^{n} P(y_j|Y_{0:j-1}, X, P_i; \\theta)\nwhich {P_1, P_2, ...P_n} is a list of prompts having the same meaning with P. As shown in right side of Figure 2, inputs constructed by these n prompts and X are submitted to LLMs in batches for decoding. We average the predicted probability distributions within the batch at each step of prediction, a process we term Inner-Batch Ensemble. Here, we use the most straightforward uniform average method. Of course, in the future, strategies like weighted average could also be considered. This approach mitigates biases potentially introduced by any single prompt and enhances the model's robustness against varying inputs. For details of the implementation, please refer to Algorithm 1."}, {"title": "2.2 Left-Padding", "content": "In the decoding process of LLMs, the inconsistency in prompt lengths poses challenges for batch processing. To address this issue, we employ Left-Padding technology to preprocess the input prompts, ensuring uniformity in length to accommodate the model's batch processing requirements. In practice, we pad shorter prompts with a specific token pad until they match the length of the longest prompt. This padding token is a special token with no semantic meaning, used solely for padding. This padding does not interfere with the model's understanding and processing, as the model is trained to recognize and ignore these special padding characters.\nPadding is a common method for handling sequences of varying lengths in batch processing,"}, {"title": "3 Experiments", "content": "3.1 Application to Machine Translation Task\nIn the field of machine translation, we have taken on the challenging task of document-level translation and have significantly enhanced the performance of models on the IWSLT 2017 test sets by implementing multi-prompt decoding strategies.\nDatasets We validate our experiments using the test sets\u00b9 from the IWSLT 2017 translation task. This dataset includes parallel documents sourced from TED talks. Our experiments are conducted on eight language pairs: English (En) \u2192 Chinese (Zh), German (De), French (Fr), and Japanese (Ja). There are 10 to 12 parallel documents with approximately 1,500 sentences for each language pair.\nModel The model we employed is Llama-3.1-8B-Instruct\u00b2, a large language model particularly suited for instructed learning tasks. Compared to previous Llama versions, Llama-3.1-8B-Instruct has enhanced capabilities in multilingual translation. Given the strong translation capabilities that LLMs already possess, we directly utilize LLMs for document-level translation.\nEvaluate Metrics We employ d-BLEU (document-level BLEU) as our evaluation metric(Papineni et al., 2002). While there are other metrics in the field of machine translation, such as BertScore (Zhang et al., 2020) and COMET (Rei et al., 2020), these are typically used for sentence-level translation evaluation. Utilizing them for document-level translation requires sentence-level alignment between the original and translated texts. Hence, we opt for d-BLEU here, which does not rely on sentence alignment.\nPrompts Design We employed two fixed prompts. One of them is p\u2081: \"Translate the following paragraph from {source language} to {target language}, ensuring that no part of the sentence is omitted.\" The other is p2: \"You're a very professional translator. Please help me translate the following paragraph from {source language} to {target language}.\"\nResults The experimental results demonstrate that our approach, by employing multi-prompt decoding strategies, achieved significant improvements over the original method on the d-BLEU metric. For detailed results, please refer to Table 1. Our experimental results show that in the English to Chinese and to Japanese directions, compared to the best results with a single prompt, our method can approximately increase by 1.5 points on the d-BLEU scale; there is also an improvement of about 0.5 points in other directions. We speculate that our use of LLMs, Llama-3.1-8B-Instruct, which was trained on a vast amount of English data, results in more stable translations into English, thus limiting the improvement of our method. However, in other language directions, the improvement is more pronounced."}, {"title": "3.2 Application to Code Generation Task", "content": "In the code generation task, our technical solution has achieved a significant improvement in pass@k pass rates on the standard test set HumanEval\u00b3 compared to the original methods.\nDataset HumanEval is a benchmark test suite designed to evaluate the performance of code generation models, comprising 164 meticulously crafted Python programming problems. These problems span a variety of programming tasks, from basic string manipulation to complex algorithm design. Each problem includes a function header, docstrings, a function body, and several unit tests. The structure of the dataset includes a task identifier (task_id), model input (prompt, containing function headers and docstrings), a canonical solution (canonical_solution), a function for testing the correctness of generated code (test), and an entry point for testing (entry_point).\nModel We conduct experimental evaluations using CodeLlama-7B-Python-hf\u2074. CodeLlama-13B-Python-hf is a member of the Code Llama series of models, which is a large-scale code language model series based on Llama 2. It can adapt to a variety of code synthesis and understanding tasks, such as code auto-completion, code refactoring, and code explanation. In Section 4.2, we also perform further experiments comparing models of different size like 13B.\nEvaluate Metrics Pass@k (Chen et al., 2021) is a metric used to evaluate the performance of code generation models, specifically measuring the model's ability to correctly solve problems when generating code. During the evaluation process, the model generates multiple k code samples for each unit test problem. If any of these samples pass the unit test, then the problem is considered solved. The pass@k score represents the total proportion of problems solved, which helps to quantify the model's success rate in generating correct code.\nPrompts Design Since the dataset provides a prompt for each problem, we define this original prompt as p1. For another prompt, we have constructed a very simple prefix based on p\u2081: \"\"\"This is a good code., which we define as p2. This construction ensures that the newly created p2 does not alter the meaning of the original prompt p\u2081. For detailed examples, please refer to Appendix A.2.\nResults The experimental outcomes demonstrate that our technical solution markedly enhances the pass rate across various settings of k. As shown is Table 2, regardless of whether k is set to 1, 5, or 10, our solution consistently boosts the pass@k pass rate by 1% point. This improvement not only underscores the effectiveness of our technical solution in code generation tasks but also highlights its robustness across different evaluation metrics, ensuring the delivery of higher quality code generation outcomes."}, {"title": "3.3 Application to Text Simplification Task", "content": "In the text simplification task, our technical solution has demonstrated superior performance on the challenging test set SimpEval_2022. Compared to the original methods, our solution has achieved significant improvements in the LENS metric.\nDataset SimpEval_2022 is a challenging text simplification benchmark consisting of over 1,000 human ratings on 360 simplifications. This dataset is designed to evaluate the latest text simplification models, particularly those based on large pre-trained language models. The sentences in SimpEval_2022 are selected from Wikipedia revisions published after October 22, 2022, and include more complex sentences to reduce the risk of \"data contamination\" and serve as a more challenging test bed for large language models.\nModel The model we used is Llama-3.1-8B-Instruct, a large language model particularly suited for instructed learning tasks. In this study, we applied this model to text simplification tasks and combined it with multi-prompt decoding strategies to enhance text simplification quality. The powerful computational capabilities of the Llama-3.1-8B-Instruct model enable it to handle complex language conversion tasks and perform exceptionally well in text simplification.\nEvaluate Metrics LENS (Maddela et al., 2023) is a state-of-the-art metric designed to assess the performance of text simplification tasks. LENS supports two evaluation conditions: with reference (w/ ref) and without reference (w/o ref). Under the with reference condition, LENS uses the original text as a benchmark for evaluation; whereas, under the without reference condition, LENS evaluates the quality of simplified text independently, without relying on any original text information. This flexibility allows LENS to accommodate various assessment needs and environments.\nPrompts Design Following Heineman et al. (2024), the prompts are designed as follows: P1 is \"Please simplify the following sentence so that it is easy to understand by people with disabilities or those who are unfamiliar with English. Try to use shorter words, fewer clauses, and a simpler structure.\" p2 is \"Create a simpler version of the sentence below so that it can be better understood by non-English speakers or individuals with disabilities.\""}, {"title": "4 Study", "content": "4.1 Effectiveness Analysis under Various Decoding Strategies\nIn this section, we conducted an in-depth analysis of the effects of multi-prompt ensemble decoding methods under various decoding strategies, including Top-p (Finlayson et al., 2024), Top-k (Xie et al., 2020), and Beam Search. Among them, Top-p is the decoding method we used for various tasks in Section 3. We expanded this experiment based on our previous work in machine translation and code generation tasks.\nFor machine translation tasks, we conducted experiments in the English to German (en\u2192de) and English to Chinese (en\u2192zh) directions, as shown in Table 4. We found that with Top-k and Top-p decoding methods, our approach showed improvements compared to single prompt. With Beam Search decoding, our method's results are close to the best results of p\u2081 or p2, and higher than the average of these two.\nFor code generation tasks, due to the characteristics of the task evaluation, we could only adopt sampling search decoding strategies (Top-p or Top-k). As shown in Table 5, regardless of whether based on Top-k or Top-p, our method consistently improved pass@k. This conclusion is consistent with Section 3.2."}, {"title": "4.2 Study across Varying LLM Sizes", "content": "In this section, we investigate the effectiveness of our method across different sizes of LLMs. Our main experiments in Section 3 were conducted on models of size 7-8B. Building on our previous code generation tasks, we extended our experiments to a 13B model, using CodeLlama-13B-Python-hf. We ensured that all experimental settings remained consistent with Section 3.2, except for the model itself.\nAs shown in Table 6, larger models demonstrate superior code generation capabilities, with significant improvements in the pass@k metric for k=1, 5 and 10. Our method also shows consistent improvements on the 13B model, achieving similar enhancements as on the 7B model. Moreover, on the 13B model, the improvement is more substantial, with pass@10 increasing by nearly 2 points; whereas the pass@10 on the 7B model (see Table 2) only improved by about 1 point. The experimental results confirm the effectiveness of our method across various sizes of LLMs."}, {"title": "4.3 Study on the Relationship between Prompt Count n and Output Quality", "content": "In this section, we investigate the relationship between the number of prompts and the quality of results. Building on our previous experiments on code generation and text simplification tasks, we conducted extended tests with varying numbers of prompts. The settings for the extended prompts are detailed in Appendix C.2.\nAs shown in Tables 7 and 8, the experimental results indicate that increasing the number of prompts can enhance the quality of generated output for both code generation and text simplification tasks. However, we observed that when the number of prompts increased from 2 to 3, the improvement in result quality began to plateau. This trend suggests that 2-3 prompts are essentially sufficient to achieve optimal results, and beyond this range, additional prompts have a limited effect on enhancing result quality. Therefore, in our previous baseline experiments in Section 3, we opted for two prompts as the standard configuration."}, {"title": "4.4 Exploration of Multilingual Prompts Effects", "content": "In this section, we explore the effectiveness of our method under Multilingual Prompts. Building on our previous experiments in machine translation tasks, we extended this experiment to the Chinese to English (zh\u2192en) and English to Chinese (en\u2192zh) directions. We utilized the Qwen2.5-7B-Instruct7 model, which provides better support for instructions in both Chinese and English. For one of the prompts, p1, we maintained consistency with Section 3.1, setting it as \"Translate the following paragraph from source language to target language, ensuring that no part of the sentence is omitted.\" For the other prompt, p2, we set it as the Chinese specification \"\u5c06\u4e0b\u9762\u8fd9\u4e00\u6bb5\u4ece{\u6e90\u8bed\u79cd}\u7ffb\u8bd1\u6210{\u76ee\u6807\u8bed\u79cd},\u786e\u4fdd\u6ca1\u6709\u53e5\u5b50\u88ab\u6f0f\u6389\u3002\""}, {"title": "4.5 Effect of Combination with MBR", "content": "Minimum Bayes Risk(MBR), which generates multiple candidate results during inference and selects the final outcome using specific metrics, is widely used in NLP generation tasks. We validate the effectiveness of combining our multi-prompt ensemble decoding strategy with MBR in text simplification tasks. We sampled and generated 50 candidate results for both simple prompts p\u2081 and P2, then used MBR to select the optimal outcome. For our multi-prompt ensemble decoding, we also sampled and generated 50 candidate results and chose the best one using MBR.\nAs shown in Table 10, the MBR strategy is a universal approach that significantly improves results under various conditions. When combined with our multi-prompt ensemble decoding strategy, it still manages to enhance the results by more than 0.5 points."}, {"title": "5 Related Work", "content": "The following are some significant works (Hokamp et al., 2020; Cheng et al., 2024; Heineman et al., 2024; Pitis et al., 2023) about prompts related to this study:\nHeineman et al. (2024) proposed a multi-prompt decoding approach that improves Minimum Bayes Risk (MBR) decoding by decoding multiple candidate generations from a prompt library during inference. This method uses a trained value metric to select the final output, demonstrating that multi-prompt can improve MBR performance in a range of conditional generation tasks by estimating a more diverse and higher-quality candidate space. The core of this paper lies in the MBR strategy, which generates multiple candidate results during inference and selects the final outcome using specific metrics. They construct a sufficiently large and diverse set of candidates through multi-prompting, which can be seen as an ensemble in the result space. However our method is an ensemble during the inference process..\n(Pitis et al., 2023) introduced a method named Boosted Prompt Ensembles to enhance the reasoning performance of large language models without additional training. It constructs a set of few-shot prompts that work together as an ensemble, iteratively adding prompts that target difficult examples where the current ensemble's performance is uncertain, thereby improving overall accuracy on a variety of tasks. The core of this paper lies in the selection of prompts. They proposed two algorithms: Train-time Boosting and Test-time Boosting, and conducted experiments solely on the GSM8k and AQUA datasets. In contrast, our research explores a more general ensemble method during the decoding process; we do not select specific prompts but assume that we already have a batch of prompts with equivalent performance."}, {"title": "6 Conclusions", "content": "This study set out to address the challenge of enhancing the performance of LLMs in NLP tasks through the introduction of a multi-prompt ensemble decoding approach. Our method, termed Inner-Batch Ensemble, leverages the diversity of multiple prompts to aggregate their outcomes, thereby improving the generation quality of LLMs. The implementation of a Left-Padding strategy ensured efficient batch inference, allowing for uniform input lengths across various prompts. Our extensive experiments across a range of NLP tasks\u2014spanning machine translation, code generation, and text simplification\u2014demonstrated the effectiveness of our Inner-Batch Ensemble method. The results were particularly compelling, with significant improvements observed in BLEU scores, pass@k rates, and LENS metrics when compared to standard methods. The consistent enhancements across different tasks and metrics underscore the robustness and versatility of our approach."}, {"title": "7 Future Work", "content": "It is known that LLMs tend to forget information when dealing with lengthy contexts, possibly due to the overwhelming length of the context. In our multi-prompt ensemble decoding strategy, although the prompts vary in form, their semantic content is identical. We are considering whether integrating prompts carrying different information could enhance decoding, allowing for the segmentation of extensive contexts into shorter segments distributed across various prompts. This approach might help the model retain all information more effectively and improve the integrity of integrated reasoning. We believe this is a highly promising area for further research.\nAdditionally, as LLMs are increasingly moving towards smaller model sizes, enhancing the performance of these smaller models to match the quality of their larger counterparts presents a challenge. We propose the possibility of strengthening the diversity of prompts during the training of smaller LLMs and leveraging the decoding strategy presented in this paper to boost their final performance. This strategy could shift traditional deep reasoning to broader reasoning, enhancing the model's performance and generalization capabilities by increasing the breadth of inference rather than depth."}, {"title": "8 Limitations", "content": "This study acknowledges several limitations. Firstly, our method is closely tied to the quality of the prompts used. A poorly constructed prompt may render our approach ineffective, as high-quality prompts are essential for guiding LLMs to produce accurate outputs. Secondly, due to constraints in time and computational resources, the effectiveness of our method across a broader range of tasks requires further validation. Additionally, we did not experiment with the state-of-the-art GPT-4 series interfaces, as they are proprietary and do not support embedding or modification of their decoding strategies, limiting our ability to test and refine our method on cutting-edge models."}, {"title": "A Prompts Design", "content": "A.1 Prompts for Machine Translation Task\nThe designed prompts for Machine Translation Task used in Section 3.1 are as following:\nSystem Prompt:\nP1\nYou are a great translation assistant!\nUser Prompt:\nTranslate the following paragraph from {source language} to {target language}, ensuring that no part\nof the sentence is omitted.\n{source language}: {source text}\nSystem Prompt:\nP2\nYou are a helpful assistant!\nUser Prompt:\nYou're a very professional translator. Please help me translate the following paragraph from {source\nlanguage} to {target language}.\n{source language}: {source text}"}, {"title": "A.2 Prompts for Code Generation Task", "content": "For Code Generation Task, since the dataset provides a prompt for each problem, we define this original\nprompt as p1. For prompt p2, we construct a simple prefix based on p\u2081. Examples of this prompts used in\nSection 3.2 are as following:\ndef specialFilter(nums):\nP1\n\"\"\"Write a function that takes an array of numbers as input and returns the number of elements in\nthe array that are greater than 10 and both first and last digits of a number are odd (1, 3, 5, 7, 9).\nFor example:\nspecialFilter([15, -73, 14, -15]) => 1\nspecialFilter([33, -2, -3, 45, 21, 109]) => 2\n\"\"\"\nP2\n\"\"\"\" This is a good code.\ndef specialFilter(nums):\n\"\"\"Write a function that takes an array of numbers as input and returns the number of elements in\nthe array that are greater than 10 and both first and last digits of a number are odd (1, 3, 5, 7, 9).\nFor example:\nspecialFilter([15, -73, 14, -15]) => 1\nspecialFilter([33, -2, -3, 45, 21, 109]) => 2\n\"\"\"\nfrom typing import List, Tuple\nP1\ndef rolling_max(numbers: List[int]) -> List[int]:\n\"\"\"\nFrom a given list of integers, generate a list of rolling maximum element found until given\nmoment in the sequence.\nrolling_max([1, 2, 3, 2, 3, 4, 2])\n[1, 2, 3, 3, 3, 4, 4]\n\"\"\"\nfrom typing import List, Tuple\nP2\n\"\"\"\" This is a good code.\ndef rolling_max(numbers: List[int]) -> List[int]:\n\"\"\"\nFrom a given list of integers, generate a list of rolling maximum element found until given\nmoment in the sequence.\nrolling_max([1, 2, 3, 2, 3, 4, 2])\n[1, 2, 3, 3, 3, 4, 4]\n\"\"\""}, {"title": "A.3 Prompts for Text Simplification Task", "content": "The designed prompts for Text Simplification used in Section 3.3 are as following:\nP1\nUser Prompt:\nPlease simplify the following sentence so that it is easy to understand by people with disabilities or\nthose who are unfamiliar with English. Try to use shorter words, fewer clauses, and a simpler structure.\nOriginal: {input text}\nP2\nUser Prompt:\nCreate a simpler version of the sentence below so that it can be better understood by non-English\nspeakers or individuals with disabilities.\nOriginal: {input text}"}]}