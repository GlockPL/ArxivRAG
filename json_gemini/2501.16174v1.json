{"title": "Measuring Heterogeneity in Machine Learning with Distributed Energy Distance", "authors": ["Mengchen Fan", "Baocheng Geng", "Roman Shterenberg", "Joseph A. Casey", "Zhong Chen", "Keren Li"], "abstract": "In distributed and federated learning, heterogeneity across data sources remains a major obstacle to effective model aggregation and convergence. We focus on feature heterogeneity and introduce energy distance as a sensitive measure for quantifying distributional discrepancies. While we show that energy distance is robust for detecting data distribution shifts, its direct use in large-scale systems can be prohibitively expensive. To address this, we develop Taylor approximations that preserve key theoretical quantitative properties while reducing computational overhead. Through simulation studies, we show how accurately capturing feature discrepancies boosts convergence in distributed learning. Finally, we propose a novel application of energy distance to assign penalty weights for aligning predictions across heterogeneous nodes, ultimately enhancing coordination in federated and distributed settings.", "sections": [{"title": "1 Introduction", "content": "Distributed and federated learning systems have emerged as powerful paradigms for training large-scale machine learning models across geographically dispersed data sources. By coordinating learning processes among multiple nodes (clients) without centrally aggregating raw data, these systems address privacy and communication constraints. However, a persistent and critical challenge in these settings is heterogeneity\u2014the variations in data, resources, tasks, or network characteristics across different nodes\u2014that can significantly degrade both model accuracy and convergence performance. As real-world"}, {"title": "2 Distributed Energy Distance for Feature Heterogeneity", "content": "Understanding and quantifying feature heterogeneity is pivotal in distributed learning systems, as it directly impacts model performance and convergence. Feature heterogeneity arises when the marginal distributions of predictors differ across nodes, leading to challenges in federated or distributed algorithms that assume data homogeneity. A robust metric is required to characterize such heterogeneity and facilitate hypothesis testing"}, {"title": "2.1 Energy Distance and Energy Coefficient", "content": "The Energy Distance, introduced by Sz\u00e9kely (2003); Sz\u00e9kely and Rizzo (2005), is a non-parametric measure that quantifies differences between distributions by evaluating pairwise distances between samples, while accounting for the internal variability within each distribution. For two random vectors X and Y, the squared energy distance is defined as\n\n$D\u00b2(X,Y) = 2E||X \u2013 Y || \u2013 E||X \u2013 X'|| \u2013 E||Y \u2013 Y'|\\,$\n\nwhere ||.|| represents the L2 norm, and X', Y' are independent copies of X, Y respectively. Its empirical version, the Energy statistic, calculated from datasets x = {xi}i=1 and y = {yj}j=1, is given by\n\n$En,m(x, y) =2\\frac{1}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} ||x_i - y_j|| - \\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} || x_i - x_j|| - \\frac{1}{m^2} \\sum_{i=1}^{m}\\sum_{j=1}^{m} || y_i - y_j||\\ \\qquad (1)$\n\nThe Energy Coefficient H extends the energy distance into a normalized measure of heterogeneity\n\n$H = \\frac{D\u00b2(X, Y)}{2E||X \u2013 Y||},$\n\nwhere \u0397 \u2208 [0,1], with H = 0 if and only if Px = Py. This coefficient captures general distributional differences, including discrepancies in location, scale, and shape, while offering an interpretable measure suitable for comparisons across datasets.\nThe Energy Distance possesses several desirable properties, making it a robust and versatile metric. It satisfies the fundamental requirements of a distance metric: non-negativity, symmetry, and the triangle inequality. Furthermore, it is invariant to scaling, rotation, and translation, ensuring consistent behavior under affine transformations."}, {"title": "2.2 Comparison with other metrics", "content": "In the landscape of metrics for measuring distributional differences, several alternatives to Energy Distance are widely used, each with distinct advantages and limitations.\nMaximum Mean Discrepancy (MMD) works by mapping distributions into a reproducing kernel Hilbert space (RKHS) and measuring their distance. Its performance depends on the choice of kernel, which can be difficult to tune, especially in high-dimensional settings.\nWasserstein Distance, or Optimal Transport, calculates the cost of transforming one distribution into another, making it ideal for capturing geometric differences. However, it is computationally intensive, particularly in high dimensions.\nKullback-Leibler (KL) Divergence measures how one probability distribution diverges from another. While effective for comparing probability densities, it fails when the distributions lack overlapping supports and is not symmetric.\nTotal Variation Distance assesses the maximum difference in probabilities across distributions. It is straightforward for discrete cases but becomes less practical for continuous or high-dimensional data.\nEnergy Distance distinguishes itself by being computationally simple and broadly applicable. It avoids reliance on kernels or optimization and captures both location and scale differences. These properties make it a robust choice for analyzing heterogeneous datasets, particularly in high-dimensional scenarios.\nThe choice of metric depends on the task. For geometric insights, Wasserstein or Energy Distance works well, while kernel-based methods like MMD offer flexibility. Simpler"}, {"title": "2.3 Efficient Approximation of Distributed Energy Distance", "content": "Energy Distance effectively quantifies distributional differences by comparing pairwise distances between samples. However, its quadratic computational complexity with respect to the number of points in X and Y, due to the need to compute all pairwise distances, which makes it infeasible for large-scale distributed datasets, where transmitting raw data is impractical. To address this, we propose an approximation framework that uses summary statistics such as means, variances, and higher-order moments, reducing complexity to linear in sample size. In this paper, we present only the final results of the efficient approximation due to space constraints. Detailed derivations will be provided in the extended version of this manuscript.\nFor one-dimensional cases, let X and X' be independent and identically distributed random variables with mean \u03bc, variance \u03c3\u00b2, skewness \u03b33, and excess kurtosis \u03b34. First, consider the squared distance between X and X',\n\n$E [||X \u2013 X'||2] = 2\u03c32.$\n\nWe are interested in approximating the expectation of the square root of this distance, which is given by:\n\n$E [||X \u2013 X'||] = E [\\sqrt{(X \u2013 X')2}],$\n\nwith \u221a2\u03c3 its upper bound by Jensen's Inequality.\nA Taylor series expansion is applied around the mean of (X \u2013 X')\u00b2, which is 2\u03c32. Let g(z) = \u221az. The second-order Taylor expansion of g(z) around z = 2\u03c32 is given by\n\n$g(z) \u2248 g(2\u03c3\u00b2) + g'(2\u03c3\u00b2) (z \u2013 2\u03c3\u00b2) + \\frac{1}{2}g''(2\u03c32) (z - 2\u03c32) 2$\n\n$= \\sqrt{2\u03c32} + \\frac{(X \u2013 X')2 \u2013 2\u03c32}{2\\sqrt{2\u03c32}} - \\frac{((X \u2013 X')2 \u2013 2\u03c32)2}{8(2\u03c32)^{3/2}}.$\n\nAfter expectation, the second term on the right-hand side vanishes, and the third term involves the fourth central moment of X. It leads to the final approximation\n\n$E [||X \u2013 X'||] \u2248 \\sqrt{2\u03c3} (1 - \\frac{\u03b34+4}{16}).\\ \\qquad (2)$\n\nThe correction term $\\frac{\\sqrt{2}\u03b34\u03c3}{16}$ accounts for the effect of kurtosis on the expected distance. For two independent random variables X and Y with means \u03bcX and \u03bcy, variances \u03c3X and \u03c3Y, skewnesses \u03b33X and \u03b33Y, and kurtoses \u03b34X and \u03b34Y. Similarly, the expectation of the Euclidean distance between X and Y can be approximated by\n\n$E [||X - Y ||] \u2248\\sqrt{v_{XY}} (1 - \\frac{C_{4XY} + 4C_{3XY}\u03b4_\u03bc + 2\u03bd_{XY} \u2013 2\u03b4_\u03bc^2}{8v_{XY}}),\\ \\qquad (3)$"}, {"title": "2.4 Residual Discrepancy and Variance Analysis", "content": "The Taylor approximation of E [||X \u2013 X'||] incurs residual errors dominated by third-order terms, expressed as\n\n$R3 = \\frac{3}{48(2\u03c32)^{5/2}} (25\u03b36 + 18\u03b34^2 + 340\u03b34 \u2013 20\u03b34^2).$\n\nwhere \u03b36 is the sixth cumulant of X, \u03b33 the skewness, and \u03b34 the kurtosis. These corrections are typically small but grow significant for distributions with pronounced asymmetry or heavy tails.\nThe variance of \u221a(X \u2013 X')2, approximated as\n\n$Var (\\sqrt{(X \u2013 X')2}) \u2248 \\frac{\u03c3\u03b34}{8},$\n\nindicates reduced reliability for heavy-tailed distributions. However, with sufficient data aggregation, high-order effects are smoothed, minimizing practical impact despite theoretical discrepancies."}, {"title": "2.5 Adjustment to Exact Expressions Using Skewness and Kurtosis", "content": "The Taylor approximation of E [||X \u2013 X'||] diverges slightly from the exact expectation when X, X' ~ \u039d(\u03bcX,\u03c3X). For Gaussian distributions, the exact expected distance is given by\n\n$E [||X \u2013 X'||] = \\frac{2}{\\sqrt{\u03c0}}\u03c3.\\qquad (5)$\n\nThis discrepancy stems from the omission of higher-order terms in the Taylor expansion. Inspired by Hampel et al (2005) Hampel et al. (2005), we extend the exact formula for Gaussian distributions to handle non-Gaussian distributions by incorporating correction terms derived from skewness and kurtosis,\n\n$E [||X - \u2013 X'||] \u2248 (\\frac{2}{\\sqrt{\u03c0}} - \\frac{\\sqrt{2}\u03b34}{16})\u03c3.\\qquad (6)$\n\nwhere $\\frac{\\sqrt{2}\u03b34\u03c3}{16}$ accounts for excess kurtosis, improving accuracy for distributions with heavier or lighter tails.\nFor independent random variables X ~ \u039d(\u03bcX,\u03c3X) and Y ~ \u039d(\u03bcY, \u03c3Y), the exact expectation of the Euclidean distance is\n\n$E [||X - Y||] = \\sqrt{\u03c3_X^2 + \u03c3_Y^2} \\sqrt{\\frac{2}{\u03c0}} e^{-\\frac{\u0394}{2}} + \u03b4_\u03bc (2\u03a6(\u0394) \u2013 1),\\qquad (7)$\n\nwhere \u03a6 is the standard normal CDF and \u0394 = $\\frac{\u03b4_\u03bc}{\\sqrt{\u03c3_X^2+\u03c3_Y^2}}$.\nSimilarly, we propose the following adjustment\n\n$E\n\n[||X - Y||] \u2248 \\sqrt{\u03c3_X^2 + \u03c3_Y^2} \\sqrt{\\frac{2}{\u03c0}} e^{-\\frac{\u0394}{2}} + \u03b4_\u03bc (2\u03a6(\u0394) \u2013 1) - (C_{4xy} + 4C_{3xy}\u03b4_\u03bc)/(8v_{XY}^{\\frac{3}{2}}).\\qquad (8)$\n\nThis adjustment aligns Taylor approximations with exact solutions for small \u03b4\u03bc while extending corrections for mean differences and higher moments. It is consistent with (6) for E [||X \u2013 X'||]."}, {"title": "3 Experiment Result", "content": ""}, {"title": "3.1 Energy Distance Experiments", "content": "This experiment evaluates the performance of energy distance measurements across diverse data distributions, focusing on computational efficiency and accuracy. Using a sample size of n = 10\u00b3, we explored standard distributions with various degrees of skewness, kurtosis, and tail behavior, including Normal (\u03bc = 0,\u03c3=1, \u03bc = 1,\u03c3=1 and \u03bc = 10,\u03c3 = 1),"}, {"title": "3.2 Impact of Feature Heterogeneity on Federated Learning Performance", "content": "In this experiment, we investigate how varying feature distributions across clients impact federated learning performance. Using the MNIST dataset, a benchmark of 60,000 handwritten digit images, we modified the feature distribution (X) in two distinct ways:\nRandomized Data Transformation: We pre-processed the MNIST dataset (training and testing) by randomly selecting and transforming which inverts the colors (white-to-black and black-to-white) and maps to the different scales (mapping includes finding the maximum and minimum values, setting a rank, and then random a value r, and then process the scale mapping using ((max - min)/rank) \u00b7r + min + (x - min)/(max - min)). The transformed data was distributed randomly among 100 clients, creating a mixture of multiple types of images per client. Figure 3 shows examples of client data. Then, we calculate the H between each client data by using the Taylor approximate method. The mean of H is 0.001468 and the standard deviation is 0.001925, which indicates the feature distributions between clients are similar.\nFeature-Based Allocation: Data was distributed by type. For example, Client 1 exclusively received black numerals on white backgrounds and the values were within the specific scale, while Client 2 received white numerals on black backgrounds and the values were within another specific scale. Figure 4 illustrates these allocations. Then, we calculate the H between each client data by using the Taylor approximate method. The mean of H is 0.657 and the standard deviation is 0.295, which indicates the feature distributions between clients are different."}, {"title": "4 Discussions and Conclusion", "content": "An important consideration in selecting a measure of feature heterogeneity is its ability to distinguish between distributions effectively. One alternative to the Energy Distance is a quadratic form of the distance, defined as\n\n$\u010e\u00b2(Fx, Fy) = 2E||X \u2013 Y ||\u00b2 \u2013 E||X \u2013 X'||\u00b2 \u2013 E||Y \u2013 Y'||\u00b2.$\n\nWhile D2 is non-negative and exhibits basic properties expected of a distance measure, it falls short in its ability to capture nuanced differences between distributions. Specifically, \u010e\u00b2 equals zero whenever \u03bcX = \u03bcY and \u03c3X = \u03c3Y, regardless of any disparities in higher-order moments. This limitation makes it ineffective for distinguishing distributions with identical means and variances but differing shapes or tails.\nIn contrast, the Energy Distance overcomes this limitation by accounting for differences in higher-order moments, in addition to location and scale. This property makes it a more suitable metric for quantifying feature heterogeneity in distributed scenarios, where comprehensive comparison of distributions is essential. While the Energy Distance may not address all potential limitations, its capacity to support hypothesis testing and capture broader distributional differences underscores its practical advantages over this quadratic alternative.\nThe Energy Coefficient H provides a quantifiable measure of similarity between nodes and can be used to determine the penalty coefficient between guest and host models in distributed collaborative learning. By incorporating feature heterogeneity, H enables dynamic adjustments that improve model alignment and overall system performance.\nFeature heterogeneity extends to representation heterogeneity for categorical responses, enabling metrics to compute one distance per response value (e.g., a 2-dimensional vector for binary response). Challenges remain for continuous responses or cases with many unique discrete values. Future work could refine the approximation by bounding residual terms or developing adaptive corrections for pronounced higher-order effects, enhancing accuracy and scalability.\nIn conclusion, we propose an efficient method to quantify heterogeneity in distributed learning. Simulations show its accuracy aligns with empirical results, offering a robust tool for analyzing feature distributions across clients and improving training strategies in distributed systems."}]}