{"title": "Non Verbis, Sed Rebus:\nLarge Language Models are Weak Solvers of Italian Rebuses", "authors": ["Gabriele Sarti", "Tommaso Caselli", "Malvina Nissim", "Arianna Bisazza"], "abstract": "Rebuses are puzzles requiring constrained multi-step reasoning to identify a hidden phrase from a set of images and letters. In\nthis work, we introduce a large collection of verbalized rebuses for the Italian language and use it to assess the rebus-solving\ncapabilities of state-of-the-art large language models. While general-purpose systems such as LLAMA-3 and GPT-40 perform\npoorly on this task, ad-hoc fine-tuning seems to improve models' performance. However, we find that performance gains\nfrom training are largely motivated by memorization. Our results suggest that rebus solving remains a challenging test bed to\nevaluate large language models' linguistic proficiency and sequential instruction-following skills.", "sections": [{"title": "1. Introduction", "content": "Complex games such as chess and Go have long been\na source of inspiration to develop more flexible and ro-\nbust AI systems [1, 2]. Recent developments in NLP sug-\ngested that creative language games could be exploited\nas promising benchmarks for quantifying the ability of\nlarge language models (LLMs) to carry out multi-step\nknowledge-intensive reasoning tasks under pre-specified\nconstraints [3]. While crossword puzzles have been his-\ntorically the main focus of such efforts [4], other cat-\negories of linguistic games received only marginal at-\ntention, especially for languages other than English. A\nprominent example of less-studied language games is the\nrebus, a visual puzzle combining images and graphic\nsigns to encode a hidden phrase. Indeed, rebus solving is\na complex, multi-step process requiring factual knowl-\nedge, contextual understanding, vocabulary usage, and\nreasoning within pre-defined constraints - a set of fun-\ndamental skills to address a variety of real-world tasks.\nIn this work, we conduct the first open evaluation of\nLLMs' rebus-solving capabilities, focusing specifically\non the Italian language. We propose a novel strategy to\nderive text-only verbalized rebuses from transcribed inter-\nmediate rebus solutions and use it to produce a large col-\nlection with more than 80k verbalized rebuses. We then\nevaluate the rebus-solving skills of state-of-the-art LLMs,\nincluding open-source systems and proprietary models,\nvia few-shot prompting. Moreover, we fine-tune a small\nbut capable LLM on verbalized rebus solving, outperform-\ning state-of-the-art systems by a wide margin. Finally, we\nconduct a fine-grained assessment of LLMs' sequential\nreasoning steps, explaining model performance in terms\nof word complexity and memorization."}, {"title": "2. Background and Related Work", "content": "Italian Enigmistica and Rebuses The Italian lan-\nguage is characterized by a rich and long-standing tra-\ndition of puzzle games, including rebuses, dating back\nto the 19th century [5]. In Italian rebuses, a first pass\n(prima lettura) representing an intermediate solution of\nthe puzzle is produced by combining graphemes with\nunderlying image elements in a left-to-right direction\n(Figure 1). Then, the letters and words of the first pass\nundergo a re-segmentation (cesura) according to a solu-\ntion key (chiave di lettura), which specifies the length of\nwords in the solution (frase risolutiva). The verbalized\nrebuses we introduce in this work are variants of textual\nrebuses (rebus descritto or verbis), where the text-based\npuzzle is crafted by replacing first pass words with their\ncrossword definitions in a templated format (Figure 1).\nLinguistic Puzzles as NLP Progress Metrics Lan-\nguage games have recently been adopted as challeng-\ning tasks for LLM evaluation [3, 9, 10]. While works\nin this area have historically focused on English cross-\nwords [11, 12, 4, 13], recent tests focus on a more di-\nverse set of games such as the New York Times' \"Con-\nnections\" [14] and \"Wordle\" [15]. Automatic crossword\nsolvers were also developed for French [16], German [17]\nand Italian [18, 19], while didactic crossword generators\nare available for Italian [20] and Turkish [21]. Relat-\nedly, the Italian evaluation campaign EVALITA\u2074 recently\nhosted two shared tasks focusing on the word-guessing\ngame \"La Ghigliottina\" (The Guillotine) [22, 23]. To our\nknowledge, our work is the first to attempt the computa-\ntional modeling and evaluation of rebus-solving systems.\nImportantly, language games such as rebuses are not eas-\nily translatable into other languages due to their struc-\ntural and cultural elements. This makes them a scarce\nbut valuable resource for language-specific evaluations\nof language processing systems.\nLLMs as Sequential Reasoners State-of-the-art\nLLMs were shown to struggle to follow sequential instruc-\ntions presented in a single query [24], but their perfor-\nmances improved significantly with ad-hoc training [25].\nThis acts as an initial motivation for our rebus-solving"}, {"title": "3. Experimental Setup", "content": "Data We begin by extracting all rebuses' first passes\nand solutions available on Eureka55, an online repository\nof Italian puzzles. We refer to the resulting dataset con-\ntaining 223k unique rebuses sourced from various publi-\ncations as EUREKAREBUS. For crossword definitions, we\nuse ITACW [20], containing 125k unique definition-word\npairs. We select only EUREKAREBUS examples in which\nall first pass words match an existing ITACW definition\nto enable verbalization, maintaining 83,157 examples for\nour modeling experiments. Since several ITACW words\nare associated with multiple definitions, we randomly\nsample definitions to promote diversity in the resulting\nverbalized rebuses. A test set of 2k examples' is kept\naside for evaluation, and the remaining 81k examples are\nused for model training.\nModels We fine-tune Phi-3 Mini 3.8B 4K [28], the most\ncapable LLM below 4B parameters for a wide range of Ital-\nian language tasks. We use quantized low-rank adapters\n(QLORA; 29, 30) for efficient fine-tuning with Unsloth\nand Transformers [31], training the model for 5,000 steps\nwith a batch size of 16 over 81k examples. For compar-\ning our model performances, we select GPT-40 [32] and\nClaude-3.5 Sonnet [33] as the current state-of-the-art\nfor proprietary LLMs and the instruction-tuned variants\nof Qwen-2 72B [34] and LLaMA-3 70B [35] as the best-\nperforming open-source LLMs according to the Invalsi\nItalian benchmark [36]. These four systems are used as\nuntrained baselines thanks to their instruction-following\nabilities and prompted for rebus solving in a few-shot\nsetting."}, {"title": "4. Results", "content": "Table 2 presents our evaluation results. We observe that\nall prompted models perform poorly on the task, with the\noverall best prompted system (Claude 3.5 Sonnet) obtain-\ning the correct solution only for 24% of the 2k tested\nexamples. Notably, open-source systems perform signifi-\ncantly worse than proprietary ones, producing correct\nfirst passes only for 4% of the examples, and next to no\ncorrect solutions. Our fine-tuned system largely outper-\nforms all state-of-the-art prompted models, predicting\nthe correct solution in 51% of cases. From first pass met-\nrics, it is evident these results can be largely explained by\nthe poor word-guessing capabilities of the models, which\nare greatly improved with fine-tuning. For prompted\nmodels, the slight decrease in scores between Def. and\nFP Words also highlights issues with copying predicted\nwords in the expected format. Finally, we observe that\nfine-tuning strongly improves the constraint-following\nabilities of our system, with prompted systems being less\nstrict with applying length and letter-choice constraints\nfor their solutions (Key/FP Match)."}, {"title": "5. What Motivates Model\nPerformances?", "content": "In light of the strong performances achieved by our rela-\ntively small fine-tuned system, this section conducts an\nin-depth investigation to identify factors motivating such\nperformance improvements.\nIn practice, we define this as 1 - CER(FP, S), where CER is the\ncharacter error rate [37] between the two sequences (lowercased,\nwhitespace removed) computed with Jiwer"}, {"title": "6. Discussion and Conclusion", "content": "This work introduced a verbalized rebus-solving task\nand dataset for evaluating LLMs' sequential instruction\nfollowing skills for the Italian language. We crafted a\nlarge collection of 83k verbalized rebuses by combining\nrebus transcriptions with crossword definitions and used\nit to evaluate the rebus-solving skills of state-of-the-art\nLLMs. Our experiments revealed the challenging nature\nof this task, with even the most capable prompted models\nachieving only 24% accuracy on solutions.\nWhile fine-tuning a smaller LLM dramatically im-\nproved performance to 51% solution accuracy, our anal-\nysis uncovered that these gains were largely driven\nby memorization and do not generalize to out-of-\ndistribution examples. These results suggest important\nlimitations in the generalization capabilities of current\nsystems for sequential instruction following tasks. Our\nmanual analysis further shows that LLMs seldom account\nfor length constraints when solving definitions, despite\nthe fundamental role of these cues in restricting the pool\nof possible words. These results suggest that search-\nbased approaches accounting for constraints more ex-\nplicitly might improve puzzle structure adherence, as\npreviously shown by Chen et al. [39]. Other augmenta-\ntion techniques employing LLM reformulation skills can\nalso be explored to mitigate overfitting.\nFuture work in this area should focus on expanding\nsimilar evaluations to a wider set of languages, input\nmodalities, and puzzle categories, creating a comprehen-\nsive benchmark to test LLMs' puzzle-solving skills. Im-\nportantly, the task of solving visual rebuses and their\nmore convoluted variants remains far beyond the cur-\nrent capabilities of vision-language models. Hence, solv-\ning these puzzles automatically can be considered an\nimportant milestone in developing multimodal AI sys-\ntems for constrained multi-step reasoning tasks. Our\nresults confirm that the challenging nature of rebuses,\neven in their verbalized form, makes this task valuable\nfor assessing future progress in LLMs' linguistic profi-\nciency and sequential reasoning abilities. Finally, our\nrebus-solving LLM can facilitate future interpretability\nwork investigating the mechanisms behind factual recall\nand multi-step reasoning in transformer models [40].\nLimitations Our analysis was limited to a relatively\nsmall set of models, and a single prompt template ob-\ntained after minimal tuning. Further experiments are\nneeded to verify that memorization patterns after fine-\ntuning remain relevant for other model sizes, prompt for-\nmats, and training regimes, particularly for full-weight\ntraining approaches."}]}