{"title": "Molecular Graph Representation Learning via Structural Similarity Information", "authors": ["Chengyu Yao", "Hong Huang", "Hang Gao", "Fengge Wu", "Haiming Chen", "Junsuo Zhao"], "abstract": "Graph Neural Networks (GNNs) have been widely employed for feature representation learning in molecular graphs. Therefore, it is crucial to enhance the expressiveness of feature representation to ensure the effectiveness of GNNs. However, a significant portion of current research primarily focuses on the structural features within individual molecules, often overlooking the structural similarity between molecules, which is a crucial aspect encapsulating rich information on the relationship between molecular properties and structural characteristics. Thus, these approaches fail to capture the rich semantic information at the molecular structure level. To bridge this gap, we introduce the Molecular Structural Similarity Motif GNN (MSSM-GNN), a novel molecular graph representation learning method that can capture structural similarity information among molecules from a global perspective. In particular, we propose a specially designed graph that leverages graph kernel algorithms to represent the similarity between molecules quantitatively. Subsequently, we employ GNNs to learn feature representations from molecular graphs, aiming to enhance the accuracy of property prediction by incorporating additional molecular representation information. Finally, through a series of experiments conducted on both small-scale and large-scale molecular datasets, we demonstrate that our model consistently outperforms eleven state-of-the-art baselines.", "sections": [{"title": "1 Introduction", "content": "Molecular Representation Learning, a critical discipline in bioinformatics and computational chemistry, has witnessed significant advancements in recent years"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Motifs in Molecular Graphs", "content": "Motif refers to the basic structure that constitutes any characteristic sequence. It can be viewed as a subgraph with a specific meaning in the molecular graph. For example, the edges in a molecular graph represent chemical bonds, and the rings represent the molecular ring structure. Several algorithms have been introduced to leverage motifs in different applications, including contrastive learning, self-supervised pretraining, generation and drug-drug interaction prediction. The motif extraction techniques used in the above methods, whether"}, {"title": "2.2 Molecular Graph Representation Learning", "content": "DL has been widely applied to predict molecular properties. Molecules are usually represented as 1D sequences, including amino acid sequences, SMILES and 2D graphs. Wu et al. proposed a new molecular joint representation learning framework, MMSG, based on multi-modal molecular information (from SMILES and graphs). However, these approaches cannot capture the rich information in subgraphs or graph motifs. A few works based on GNNs have been reported to leverage motif-level information. Specifically, some approaches introduced the molecular graph representation learning method by constructing heterogeneous motif graphs from extracting different types of motifs. Alternatively, other methods decomposed each training molecule into fragments by breaking bonds and rings in compounds to design novel GNN variants. Although these methods obtain more expressive molecular graphs, the challenge in motif-based approaches mainly comes from the difficulty in efficiently measuring similarities between input graphs. While existing graph kernel methods can calculate scores by comparing different substructures of graphs to complete the measurement, there is currently no comparison method for motif-based molecular graphs. Therefore, our method focuses on learning motif structural information in the representation."}, {"title": "3 Methods", "content": "In this section, we propose a novel method to construct a Molecular Structural Similarity Motif Graph Neural Network (MSSM-GNN) which takes the MSSM graph as input.\nGenerally, the framework of the method consists of three parts: (i) Molecular graph representation; (ii) MSSM graph construction based on graph kernel; (iii) MSSM-GNN construction. Below, we explain in more detail about these parts."}, {"title": "3.1 Molecular Graph Representation", "content": "In molecular graphs, motifs are subgraphs that appear repeatedly and are statistically significant. Therefore, we propose a novel molecular graph representation method based on chemical domain knowledge and BRICS algorithm to represent molecular structural information better. It considers both the internal atomic structure and the overall impact of special functional groups in the molecule. Its main process consists of the following two steps: (i) Motif Dictionary; (ii) Molecular Graph Re-representation."}, {"title": "Motif Dictionary", "content": "Let $\\mathcal{G} = (V, E)$ denote a molecular graph, where $V$ is a set of atoms, $E \\subset V \\times V$ is a set of bonds between atoms. Generally, we denote a motif of the molecule $\\mathcal{G}$ by $\\mathcal{M} = \\langle V', E' \\rangle$, where $V'$ is a subset of $V$ and $E'$ is the subset of $E$ corresponding to $V'$, which includes all edges connecting nodes in $V'$. Due to the impact of ring, bond, and functional group structures on a molecule's stability, mechanical properties, and reactivity, we extract these structures as three distinct types of motifs from $\\mathcal{G}$. This extraction aims to establish a correlation between molecular structure and properties, facilitating a targeted capture of diverse chemical features within the molecule. It considers important structural components within the molecule as much as possible and can be extended to different types of molecules, making it a general approach.\nTo systematically organize and store the extracted motif information, we construct a motif dictionary $\\mathcal{D}$ by preprocessing all molecules in the dataset, as outlined in step \u2460 of Fig. 2(a). The $\\mathcal{D}$ contains molecular identifiers as outer keys, each associated with nested dictionaries. These inner dictionaries categorize structural motif types with corresponding lists of extracted labels. We define the label $l(M)$ as the type of $M$. The example in Fig. 2(a) illustrates that ring type Piperazine can be expressed as $l(MR1)$. This organization efficiently stores and retrieves structural information within each molecule."}, {"title": "Molecular Graph Re-representation", "content": "Based on the motif dictionary, we traverse the structure type and their corresponding motif lists for each molecule"}, {"title": "3.2 MSSM Graph Construction Based on Graph Kernel", "content": "Through the above method, we obtain a re-representation molecular graph $\\mathcal{G}_M$. To provide GNN with more information, we will construct a Molecular Structural Similarity Motif (MSSM) graph in step \u2462. In the MSSM graph, each node represents a $\\mathcal{G}_M$, and the edge represents two nodes $\\mathcal{G}_{M_1}$ and $\\mathcal{G}_{M_2}$ with structural similarity. We calculate the similarity between two $\\mathcal{G}_M$ by utilizing Mahalanobis Weisfeiler-Lehman Shortest-Path(MWLSP) graph kernel.\nThe fundamental idea of the graph kernel is to measure the similarity via the comparison of $\\mathcal{G}_M$' substructures. The kernel we proposed retains expressivity and is still computable in polynomial time.\nAs depicted in Fig. 2, MWLSP graph kernel takes $\\mathcal{G}_{M_1}$, $\\mathcal{G}_{M_2}$ as input, and its main process consists of the following steps: (i) Preprocess Input; (ii) Perform MWLSP Graph Kernel Computation; (iii) Comparison Scores of Graph-substructures. We give a pseudocode description of the MWLSP Graph Kernel in Algorithm 1."}, {"title": "Preprocess Input", "content": "In line 2, we utilize Floyd-transformation (For detailed explanation, see Appendix A.1) $F_t(\\mathcal{G}_M)$ to convert graphs $\\mathcal{G}_{M_1}$ and $\\mathcal{G}_{M_2}$ into graphs $\\mathcal{G}_{F_1}$ and $\\mathcal{G}_{F_2}$, respectively. $F_t(\\mathcal{G}_M)$ generates the shortest path between all nodes in $\\mathcal{G}_M$. The shortest path between the vertex $v$ and $u$ is represented as $(v, u)$. The $(v, u)$ is the shortest path among all paths between two nodes. $\\mathcal{G}_{F_1}$ and $\\mathcal{G}_{F_2}$ contain all the information regarding the shortest path substructure partitions in $\\mathcal{G}_{M_1}$ and $\\mathcal{G}_{M_2}$, respectively. Specifically, $\\mathcal{G}_{F_1}$ has the same vertices as $\\mathcal{G}_{M_1}$, and the edge $(v, u)$ in $\\mathcal{G}_{F_1}$ represents detailed information about the shortest path in $\\mathcal{G}_{M_1}$\n$\\mathcal{G}_{F1} = F_t(\\mathcal{G}_{M1})$\n$\\mathcal{G}_{F2} = F_t(\\mathcal{G}_{M2})$\n(1)"}, {"title": "Perform MWLSP Graph Kernel Computation", "content": "$\\mathcal{K}_{mwlsp}(\\mathcal{G}_{F1}, \\mathcal{G}_{F2})$ will compute the similarity between two graphs, $\\mathcal{G}_{M_1}$ and $\\mathcal{G}_{M_2}$, by summing up $k(e_1, e_2)$ i.e., the comparison scores between substructures $e_1$ and $e_2$ in line 3-7. $E'$ is the set of all edges in $\\mathcal{G}_{F1}$ and $e_1$ is one of the edges in $E_1$. $e_1$ represents a shortest path substructure in $\\mathcal{G}_{M_1}$, and similarly for $e_2$.\n$\\mathcal{K}_{mwlsp}(\\mathcal{G}_{F1}, \\mathcal{G}_{F2}) = \\sum_{e_1 \\in E_1} \\sum_{e_2 \\in E_2} k(e_1, e_2)$\n(2)"}, {"title": "3.3 MSSM-GNN Construction", "content": "In this part, we build an MSSM-GNN to learn graph structural feature representations of the MSSM graph. In graph learning, the input MSSM graphs can be denoted as $\\mathcal{G}_{mssm} = (V_{MSSM}, E_{MSSM})$, where $V_{MSSM}$ is the node set of $\\mathcal{G}_M$, and $E_{mssm}$ is the edge set of similarity relationship between two $\\mathcal{G}_M$. And"}, {"title": "4 Experiments", "content": "In this section, we investigate how our proposed method improves GNN performance on molecular property tasks. In our investigations, we raise the following questions: Q1: Compared with state-of-the-art baselines, how effective is MSSM-GNN in improving the accuracy of molecular prediction on common bioinformatics graph benchmark datasets? Q2: If experiments are conducted on real-world datasets, will MSSM-GNN still have an effect? Q3: Does feature learning of similarities between molecules play a more critical role in MSSM-GNN? Q4: What impact will the setting of the similarity threshold on different datasets have on the final classification results?\nIn response to the above problems, we conducted a series of experimental studies. Some basic settings of experiments and analysis of results are as follows:"}, {"title": "4.1 Experimental Settings", "content": "Datasets. To verify whether MSSM-GNN provides more information conducive to accurate classification, we evaluate our model on five popular bioinformatics graph benchmark datasets from TUDataset, which includes four molecular datasets PTC, MUTAG, NCI1, MUTAGENICITY and one protein dataset PROTEINS."}, {"title": "4.2 Performance Evaluation on Molecular Graph Datasets", "content": "To learn graph feature representations in our molecular structural similarity motif graphs, 3 GNN layers are applied. For a fair comparison, we evaluate all baselines using the experiment settings provided by . The hyper-parameters we tune for each dataset are (1) the learning rate$\\in$ 0.01, 0.05; (2) the number of hidden units$\\in$ 16, 64, 1024; (3) the dropout ratio$\\in$ 0.2, 0.5. We set the verification method as the mean and standard deviation of the seven best validation accuracies from ten folds. We compare MSSM-GNN with the baseline approaches on the abovementioned dataset to answer Q1. The comparison results are summarized in Table 1. We make the following observations:\nMSSM-GNN significantly outperforms baseline models on all five datasets for molecular prediction. Among them, on the PROTEINS dataset, the accuracy of MSSM-GNN increased by 3.4% compared with the best method. The superior performances on five molecular datasets demonstrate that motif substructures extracted from the motif dictionary, along with the calculated similarity relationships between molecular nodes based on it, facilitate GNNs in learning improved motif-level and molecular-level feature representations of molecular graphs."}, {"title": "4.3 Performance Evaluation on Large-Scale Real-World Datasets", "content": "To answer Q2, we evaluate our model on four large-scale real-world datasets from the Open Graph Benchmark (OGB) [16]. They are two binary classification datasets- ogbg-molhiv, ogbn-proteins and two multiclass classification datasets- ogbg-moltoxcast, ogbg-molpcba.\nIn this part, we compare our model with GIN, GCN, GSN, PNA, HM-GNN and GPNN. Except that the hyperparameters we tuned for each dataset varied as (1) learning rate$\\in$ 0.01, 0.001; (2) number of hidden units$\\in$ 10, 16; (3) dropout rate$\\in$ 0.5,0.7,0.9; (4) the batch size$\\in$ 128, 5000, 28000, others are the same as above experiment. Table 2 shows the AP results on Ogbg-molpcba and ROC-AUC results on the other three datasets. We observe: our method is significantly better than the other compared methods by obvious margins. The results prove our model's superior generalization ability on real-world datasets, which is crucial for its potential applications in various domains, including drug discovery, bioinformatics, and chemical safety assessment."}, {"title": "4.4 Ablation Study", "content": "To address Q3, we conducted ablation experiments on different components of MSSM-GNN, focusing on the motif-based molecular graph representation and the similarity calculation. The corresponding conclusions are as follows:"}, {"title": "Effect of Motif-Dictionary Representation", "content": "As shown in Table 3, comparing task performance before and after removing the motif dictionary module yields the following observations: Performance on three graph classification datasets benefits from the module, resulting in accuracy improvements ranging from 0.8% to 2.8%. These improvements could potentially be attributed to the module's effective learning of valuable information about the molecule's substructure."}, {"title": "Effect of Length-Similarity Calculation", "content": "As shown in Table 3, we observe a significant drop in performance when the length-similarity calculation is not"}, {"title": "Effect of Position-Similarity Calculation", "content": "In MSSM-GNN, the location similarity calculation method we designed is MWL. By replacing MWL with edit distance, we examined the impact of the graph similarity metric. As Table 3 shows, MWL offers advantages over edit distance. MWL not only quantifies structural similarity but also incorporates the type and position information of different nodes in graph modeling, thereby effectively representing the real molecular graph structure. Meanwhile, MWL becomes particularly advantageous for larger-scale graph datasets, offering significant enhancements by extracting richer structural information. For example, our model enhances PROTEINS more than MUTAG."}, {"title": "4.5 Sensitive Analysis", "content": "In this part, to explore Q4, we further evaluate the hyperparameter c that we introduced in our proposed similarity calculation formula. We modify the value of hyperparameter c that controls the similarity threshold and observe how the performance changes. We perform such experiments on multiple datasets. The results are shown in Fig. 3.\nAs observed, the performance peaks when the value of c is 2 across all three datasets. With the increase in c, the impact of the similarity threshold on training also becomes more pronounced. It is evident that the performance of MSSM-GNN decreases as c increases from 2 to 6, indicating that the c indeed influences the representation learning capabilities of MSSM-GNN. We believe that c assists in filtering out samples with low similarity, emphasizing those contributing more significantly to the training, thereby enhancing overall performance."}, {"title": "5 Conclusions", "content": "This paper proposes an effective model for molecular graph representation learning, Molecular Structural Similarity Motif GNN (MSSM-GNN). We explicitly incorporate the similarity representations between molecules into GNN and jointly update them with motif representations. Specifically, we connect two molecules through edge weights calculated by a novel MWLSP graph kernel, enabling message passing between molecular graphs. We use the GNN model to learn the MSSM graph and get the motif-level and molecule-level graph embedding. Experiments demonstrate the superiority of our model in various datasets, which beats a group of baseline algorithms."}, {"title": "A Explanations.", "content": ""}, {"title": "A.1 Explanation of Floyd-transformation", "content": "The Floyd transformation is a method for transforming a graph into its shortest-path graph. It is typically employed to solve shortest-path problems. Its concept is based on dynamic programming, gradually updating the shortest path information between nodes to obtain the shortest paths among all nodes in the graph. We give a pseudocode description of the Floyd transformation in Algorithm 2."}, {"title": "A.2 Explanation of Mahalanobis distance", "content": "The Mahalanobis distance is a metric used to measure the similarity or dissimilarity between two samples. It considers the correlations between individual features, thus providing a more accurate reflection of the actual distance between data points. Given two vectors or sample points, x and y, their Mahalanobis distance can be defined as:\n$D_M(x, y) = \\sqrt{(x - y)^T . \\Sigma^{-1} . (x - y)}$\nWhere $(x-y)^T$ represents the transpose of the vector $(x-y)$, $\\Sigma^{-1}$ represents the inverse matrix of the covariance matrix $\\Sigma$, the product denotes the matrix multiplication between the vector $(x-y)$ and $\\Sigma^{-1}$, and finally, taking the square root of the result gives the Mahalanobis distance."}, {"title": "B Proof of the complexity of MWLSP", "content": "This section provides the proof of Property 1 (Complexity of MWLSP).\nProof. Let us assume that we are dealing with two graphs with n nodes each. Each node is associated with a d-dimensional feature vector, where d represents the dimensionality of the features.\nIn the first step, the Floyd transformation can be done in $O(n^3)$ when using the Floyd-Warshall algorithm. In the second step, we have to consider pairwise comparison of all edges in both transformed graphs. The number of edges in the transformed graph is $n^2$, resulting in a total runtime of $O(n^4)$. In the third step, the calculation of length-based similarity is constant time, denoted by O(1), owing to minimal mathematical operations. However, for positional information, the runtime complexity of the Weisfeiler-Lehman scheme with H iterations is $O(Hn)$ . Within each iteration, computing the Mahalanobis distance between nodes necessitates $O(d^3)$ time , resulting in a total time complexity of $O(Hnd^3)$.\nIn summary, considering each component's detailed complexity analysis, the algorithm's overall time complexity is as follows :$O(n^3 +n^4 *(1+Hnd^3))$. Therefore, we can categorize the complexity of the entire algorithm as polynomial."}]}