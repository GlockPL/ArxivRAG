{"title": "SHAPG: NEW FEATURE IMPORTANCE METHOD BASED ON THE SHAPLEY VALUE", "authors": ["Chi Zhao", "Jing Liu", "Elena Parilina"], "abstract": "With wide application of Artificial Intelligence (AI), it has become particularly important to make decisions of AI systems explainable and transparent. In this paper, we proposed a new Explainable Artificial Intelligence (XAI) method called ShapG (Explanations based on Shapley value for Graphs) for measuring feature importance. ShapG is a model-agnostic global explanation method. At the first stage, it defines an undirected graph based on the dataset, where nodes represent features and edges are added based on calculation of correlation coefficients between features. At the second stage, it calculates an approximated Shapley value by sampling the data taking into account this graph structure. The sampling approach of ShapG allows to calculate the importance of features efficiently, i.e. to reduce computational complexity. Comparison of ShapG with other existing XAI methods shows that it provides more accurate explanations for two examined datasets. We also compared other XAI methods developed based on cooperative game theory with ShapG in running time, and the results show that ShapG exhibits obvious advantages in the running time, which further proves efficiency of ShapG. In addition, extensive experiments demonstrate a wide range of applicability of the ShapG method for explaining complex models. We find ShapG an important tool in improving explainability and transparency of AI systems and believe it can be widely used in various fields.", "sections": [{"title": "1 Introduction", "content": "With a widespread application of artificial intelligence in various industries such as healthcare [1], finance [2], and autonomous driving [3], demand for Explainable Artificial Intelligence (XAI) has become increasingly important [4]. Machine Learning or Artificial Intelligence systems are often seen as black boxes, with opaque decision-making processes that are difficult for humans to understand [5]. This lack of transparency has raised concerns regarding trust in AI systems. Therefore, the introduction of explainable AI has become an essential choice. XAI can provide explanations for black box models and decision processes [6].\nThe introduction of XAI enhances users' trust in AI systems and helps them better understand and analyze the operation of models, enabling better application in real-world scenarios [7]. XAI can reveal the importance of different features by performing feature importance analysis or explaining decision pathways [8]. This allows users to understand the impact of each input feature on the model's predictions, enabling them to make informed decisions, select relevant features, or provide appropriate recommendations. Arrieta et al. reviewed concepts related to explainable AI and analyzed the types of explanations provided by XAI, primarily categorized into two types: global and local explanations [9]."}, {"title": "2 Explainable Artificial Intelligence (XAI) methods", "content": "In a general formulation, we assume that there is a sample of observations of features $X_1, ..., X_M$ which are used to construct a model $f(X_1, ..., X_M)$ to explain/predict/classify a target variable $Y$. Due to nontransparency of black-box models in data analysis, the functional form of $f$ is not known and there is a crucial requirement to estimate features importance in AI models applied for tabular data. There are three most popular explainable AI methods: feature importance, LIME (Local Interpretable Model-agnostic Explanations), and SHAP (SHapley Additive exPlanations). We do not describe them here but provide corresponding references where these methods are introduced or well described."}, {"title": "3 ShapG: a novel XAI method", "content": "3.1 The Shapley value\nThe cooperative game is defined by $(M, v)$, where $M = {1, ..., M}$ is the set of players and $v : 2^M \\rightarrow \\mathbb{R}$ is a characteristic function defining a \"strength\" of any coalition of players that is a subset of $M$, i.e., for any coalition or collection of players $S \\subseteq M, S = |S|$. The value $v(S)$ represents the payoff or power of coalition $S$. One of the main problems which the theory of cooperative games is solving is to find a \"fair\" allocation of the total payoff of the grand coalition $v(M)$ among its members. One imputation was proposed by Shapley [25] to allocate $v(M)$, and it is a vector $\\Phi = (\\phi_1,..., \\phi_M)$, where $\\phi_i$ is a payoff (part of $v(M)$) to player $i \\in M$ defined by\n$$\n\\Phi_i = \\sum_{S:S\\subset M\\setminus{i}} \\frac{(M - |S| - 1)!|S|!}{M!}(v(S \\cup {i}) - v(S)),\n$$\nwhere $(v(S \\cup {i}) - v(S))$ is a marginal contribution of player $i$ if he joins coalition $S$.\nThe vector with the components defined by (1) is called the Shapley value and it is a unique vector satisfying four axioms (efficiency, symmetry, null player, and additivity). The efficiency axiom means that the sum of the components of the Shapley value is equal to the payoff of grand coalition $M$, i.e. $\\sum_{i\\in M}\\phi_i = v(M)$.\nWe also provide a probabilistic interpretation (see [26]) of the Shapley value to better understanding why this vector can be applied to measure the feature importance in complex machine learning models. Consider the $i$-th component of the Shapley value defined by (1). Player $i$ gets the payoff $(v(S\\cup {i}) - v(S))$ when he joins to the randomly formed coalition $S: S \\subset M \\setminus {i}$. The probability that coalition $S$ containing $|S|$ players is formed is equal to $\\frac{1}{(M-1\\choose |S|)}$. It is assumed that all coalition $S$'s sizes from 0 to $M - 1$ are equally probable and for a given coalition size $|S|$, the subsets of $|S|$ players are also equally probable. Then the value $\\phi_i$ given by (1) is player $i$'s expected payoff in such a probabilistic scheme.\nTo make a connection between the Shapley value and the vector of feature importance, we can associate the set of players with the set of features and the characteristic function with the some quantitative characteristic of the prediction made by a machine learning model using subset of features. Then, the difference $(v(S\\cup {i}) - v(S))$ can be interpreted as a"}, {"title": "3.2 ShapG (explanations based on the Shapley value for graphs)", "content": "We describe a new XAI method called ShapG to calculate the feature importance in machine learning models based on the Shapley value defined on an undirected weighted graph constructed in a special way.\n3.2.1 The Shapley value for undirected weighted graphs\nThe calculation of the Shapley value for an undirected weighted graph can be divided into following steps:\n1. We define the undirected weighted graph $G = (M, E)$, where $M$ is the set of nodes which are associated with features from set $M = {1, . . ., M}$ and the set of edges $E$ without loops. The weight of an edge $(j, k), j \\neq k$, is equal to the Pearson correlation coefficient $W(j, k)$ between features $j$ and $k$ calculated by a given sample.\n2. For any subset of features $S \\subset M$ we define subgraph $G_S$ of graph $G$.\n3. For any subset of features $S \\subset M$, we define the value of function $v$ as follows:\n$$\nv(S) = \\sum_{\\{j,k\\}\\subset G_S} W(j, k).\n$$\n4. We calculate the Shapley value by formula (1). As a result, the algorithm gives the Shapley value centrality for each node (feature).\nSince the set of features $M$ may be large, we propose an approach for approximating the Shapley value with the high accuracy presented in [19], and the proposed method is based on calculation of weights of the edges and function (2), where $S$ is a subset of features from set $M$, and $G_S$ is the subgraph induced by $S$. With a very minor modification, we can apply this approach to define a new XAI method presented in the next section.\n3.2.2 Description and algorithm of the ShapG method\nThe ShapG method can be divided into following steps:\n1. We define an undirected weighted graph $G = (M, E)$, where $M$ is the set of nodes which are associated with the features $M = {1, ..., M}$ in the prediction model and $E$ is the set of all possible edges without loops, i.e. $E = {(i, j) : i \\in M, j \\in M, i \\neq j}$ is a complete graph without loops. The weight of an edge $(j, k), j \\neq k$, is equal to the Pearson correlation coefficient $W (j, k)$ between features $j$ and $k$ calculated on a given sample.\n2. The matrix of weights $W = {W(j, k)}_{(j,k)\\in E}$ is usually a very dense matrix, therefore, we need to reduce the density of graph $G$ to reduce the number of further calculations. We implement the idea of keeping all features of the dataset while minimizing the number of edges in the graph to reduce the density. The corresponding method is realized in Algorithm 1. The idea is straightforward: we construct graph $G'$ starting from the empty graph by iteratively selecting the edges with largest Pearson correlation coefficients given in matrix $W$, and adding these edges into graph $G'$ ensuring each node is included in $G'$ at least once and graph $G'$ is a connected graph (the latter condition is a stopping rule in Algorithm 1). The output of Algorithm 1 is a new graph $G'$. In the following steps, we do not use matrix of weights $W$.\n3. In graph $G'$, we define subgraph $G'_S$ for any subset of features $S \\subset M$.\n4. We define characteristic function $f(S)$ assigning the $R^2$ score (for regression models) or F1 score (for classification models) for any subset of features $S$:\n$$\nv(S) = f(S).\n$$\n5. We calculate the Shapley value by formula (1). We use Algorithm 2 to find values of the Shapley value using exact formula (1). If the number of features is large, we use Algorithm 3 to find the approximated Shapley value."}, {"title": "4 Experiments", "content": "4.1 Description of datasets\nTo demonstrate the work of our XAI method ShapG, we consider two datasets to construct the prediction models: (i) the \"housing price\u201d dataset for regression prediction, and (ii) the \u201cH1N1 flu vaccine\" dataset for classification prediction.\nWe briefly describe datasets:\n1. The \"housing price\" dataset was collected by the U.S. Census Bureau for housing information in the Boston, Massachusetts area. The dataset contains 13 features including \"per capita crime rate by town\", \"average number of rooms per dwelling\u201d, and \u201clower status of the population\u201d, as well as a target variable, that is, the \"median value of owner-occupied homes.\"\n2. The \"H1N1 flu vaccine\" dataset is provided by the National Center for Health Statistics and borrowed from the DrivenData website. The dataset contains 35 features including an \u201cindividual's age", "gender": "education level", "knowledge of the H1N1 flu vaccine": "as well as a target variable, that is, a binary value of whether or not an individual received the H1N1 flu vaccine.\nThese two datasets are used for different prediction tasks, but they are both modeled and predicted by machine learning algorithms. For the regression task, we use $R^2$ to define the characteristic function (3) in our ShapG XAI method, while for classification task, we use F1 score to define the values of characteristic function (3).\n4.2 Preprocessing data for ShapG\nWe follow Algorithm 1 to create graph G' based on the original complete graph connecting nodes representing features. Algorithm 1 starts from the empty graph and consequently adds the edges, these are the pairs of features with the strongest correlation coefficients. It stops when all features are connected (graph G' should be connected), ensuring that the feature graph has important structural information.\nWe highlight that we use Algorithm 1 to reduce the number of edges while preserving all features, thus ignoring \u201cunimportant\" relationships between features when constructing graph G'. This will reduce the number of iterations to compute the components of the Shapley value measuring importance of the features.\n4.3 AI prediction models\nTo evaluate efficiency of our method ShapG, we apply several Explainable Artificial Intelligence (XAI) methods including our method to explain the importance of features within LightGBM and MLP (Multilayer Perceptron) models. Moreover, our ShapG algorithm can provide explanations for complex AI models that existing XAI methods cannot explain in a reasonable running time. We adopt ensemble learning and a two-by-two combination of tree models, neural network models, linear models, machine learning models to construct hybrid prediction models. We construct these"}, {"title": "5 Results and analysis", "content": "5.1 Feature importance calculated by ShapG\n5.1.1 \"Housing price\" dataset\nFigure 5 shows the feature importance calculated by ShapG algorithm for the \u201chousing price\" dataset based on LightGBM (Fig. 5a) and MLP (Fig. 5b) models, respectively. For the \"housing price\u201d dataset with regression prediction based on LightGBM, the four most important features given by ShapG are \u201cLSTAT (lower status of the population)", "RM (average number of rooms per dwelling)": "NOX (nitric oxides concentration)", "PTRATIO (pupil-teacher ratio by town)\", while with regression prediction based on MLP model, these features are \u201cLSTAT (lower status of the population)": "B (the proportion of blacks by town)"}, {"RM (average number of rooms per dwelling)": "PTRATIO (pupil-teacher ratio by town)", "H1N1\" dataset\nFigures 6 shows the feature importance calculated by ShapG algorithm for the \"H1N1\" dataset based on LightGBM (Fig. 6a) and MLP (Fig. 6b) models, respectively. For the \"H1N1\" dataset with classification prediction, the feature's approximated Shapley values given by ShapG algorithm represent the importance of each feature for people's willingness to be vaccinated against H1N1. For classification based on LightGBM, the five most important features are \u201cdoctor recc h1n1 (H1N1 flu vaccine was recommended by doctor)": "opinion h1n1 risk (Respondent's opinion about risk of getting sick with H1N1 flu without vaccine)\u201d, \u201chealth insurance\u201d, \u201copinion h1n1 vacc effective (Respondent's opinion about H1N1 vaccine effectiveness)\u201d, \u201cemployment occupation (Type of occupation of respondent)\u201d.\nFor classification based on MLP model, the five most important features are \u201cdoctor recc h1n1 (H1N1 flu vaccine was recommended by doctor)\u201d, \u201copinion h1n1 risk (Respondent's opinion about risk of getting sick with H1N1 flu without vaccine)", "doctor recc seasonal (Seasonal flu vaccine was recommended by doctor)": "health insurance", "health worker": "Three most important features for both LightGBM and MLP models coincide.\n5.2 Evaluation of XAI methods\nIn order to prove efficiency of our proposed XAI method ShapG, we compare the results of its work with other existing XAI methods by (i) evaluating all methods based on perturbation of features, and (ii) measuring running time to obtain results.\n5.3 Explanation of complex models\nThe proposed XAI method ShapG can be used not only for a single model like LightGBM or MLP as we show in the previous section, but it also provides global explanations for more complex models. These complex models include single models with complex architectures, ensemble learning models, hybrid models, etc. The models we use in this section are described in Section 4.3. ShapG method can be applied to a wide range of models and can provide explanations of their decision-making processes."}, {"title": "6 Conclusions", "content": "In this paper, we proposed a new explainable artificial intelligence (XAI) method called ShapG, which is based on Shapley value for graph games. It is a model-agnostic global explanation method. ShapG calculates feature importance by constructing an undirected graph of features, where nodes in the graph represent features, and samples based on graph. It starts with an empty graph and consequently adds the edges, which are pairs of features with the strongest correlation. The algorithm stops when all features are connected to ensure that the feature graph contains important structural information. In the process of calculating the Shapley value, we only need to consider the coalitions between each node and its neighbors, not all possible coalitions. This optimization improves the efficiency of the algorithm.\nWe have compared ShapG with several popular XAI methods, e.g., Feature Importance (FI), Permutation Feature Importance (PFI), LIME, SHAP, SamplingSHAP, and KernelSHAP. Our ShapG exhibits excellent explanation results, which are significantly better than other XAI methods for two datasets. In addition, compared to SamplingSHAP and KernelSHAP methods also based on cooperative game theory, ShapG saves significant computational resources in running time. These results provide validation of reliability and wide applicability of our method.\nWe believe that ShapG can be considered as a useful XAI method that can be applied not only to simple AI models, but also to provide global explanations for complex models. It can reliably explain decision-making process of complex models, thus helping users to better understand these models. We will continue to study how to optimize our algorithm and reduce its running time to provide users quicker and more trustworthy explanation results."}]}