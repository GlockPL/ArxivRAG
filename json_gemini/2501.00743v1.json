{"title": "AttriReBoost: A Gradient-Free Propagation Optimization Method for Cold Start Mitigation in Attribute Missing Graphs", "authors": ["Mengran Li", "Chaojun Ding", "Junzhou Chen", "Wenbin Xing", "Cong Ye", "Ronghui Zhang", "Songlin Zhuang", "Jia Hu", "Tony Z. Qiu", "Huijun Gao"], "abstract": "Missing attribute issues are prevalent in the graph learning, leading to biased outcomes in Graph Neural Networks (GNNs). Existing methods that rely on feature propagation are prone to cold start problem, particularly when dealing with attribute resetting and low-degree nodes, which hinder effective propagation and convergence. To address these challenges, we propose AttriReBoost (ARB), a novel method that incorporates propagation-based method to mitigate cold start problems in attribute-missing graphs. ARB enhances global feature propagation by redefining initial boundary conditions and strategically integrating virtual edges, thereby improving node connectivity and ensuring more stable and efficient convergence. This method facilitates gradient-free attribute reconstruction with lower computational overhead. The proposed method is theoretically grounded, with its convergence rigorously established. Extensive experiments on several real-world benchmark datasets demonstrate the effectiveness of ARB, achieving an average accuracy improvement of 5.11% over state-of-the-art methods. Additionally, ARB exhibits remarkable computational efficiency, processing a large-scale graph with 2.49 million nodes in just 16 seconds on a single GPU. Our code is available at https://github.com/limengran98/ARB.", "sections": [{"title": "I. INTRODUCTION", "content": "The significance of graph data in representing complex networks is well-established [1]\u2013[5], yet most real-world scenarios often suffer from missing attribute features that represent node semantic information in graphs [6]\u2013[9]. Therefore, the task of reconstructing missing attributes in graphs becomes essential for comprehensive network analysis [10], [11]. Recent advancements based Graph Neural Networks (GNNs) [12]\u2013[14] provide opportunities for effective missing attribute reconstruction. However, GNNs typically struggle with oversmoothing and high computational costs [15]. While Feature propagation methods [16], [17] could address these issues, they still face a common challenge: the cold start problem in reconstructing missing attributes.\nTwo key factors contribute to the cold start problem in attribute-missing graphs. First, methods like [16], [17] require resetting known nodes after each feature propagation iteration to preserve initial attributes. However, as shown in Figure 1(a), this resetting causes unknown nodes to repeatedly receive the same initial attributes from known nodes, hindering global information propagation. Second, while high-quality and rich"}, {"title": "II. RELATED WORK", "content": "A. Attribute Missing Graph Learning\nIn the early work, [22] utilized mean pooling to aggregate the features of neighboring nodes. [23] proposed the Singular Value Thresholding (SVT) algorithm, which completed matrix imputation by adjusting singular values. Besides, incomplete multi-view learning [24]\u2013[27] has been widely studied as it addresses the challenge of missing or incomplete data across multiple views, enabling the development of robust models that can leverage information from different perspectives even when some data is missing or corrupted.\nGNN based Methods With the advent of deep learning, [10], [28] used GNNs to generate missing data. [29] and [30] employed attributed random walk techniques to create nodes embedded on bipartite graphs with node attributes. [31] proposed graph denoising autoencoders, where each edge encoded the similarity between patterns to complete missing attributes. [32] transformed missing attributes into Gaussian mixture distribution, enabling Graph Convolutional Networks (GCN) [33] to be applied to incomplete network attributes. SAT [12] and SVGA [13] employed separate subnetworks for nodes attributes and graph structure to impute missing data with structural information, using GANs and Graph Markov Random Fields (GMRFs) [34] respectively, guided by shared latent space assumptions. Amer [35] introduced a unified framework that combines attribute completion and embedding learning, leveraging mutual information maximization and a novel GAN-based attribute-structure relationship constraint to improve performance. ITR proposed by [14] initially filled in missing attributes using the graph's structural information, then adaptively refined the estimated latent variables by combining observed attributes and structural information. MAGAE [36], on the other hand, employed a regularized graph autoencoder that mitigates spectral concentration issues by maximizing graph spectral entropy, enhancing the imputation of missing attributes. For community detection in attribute-missing networks, CAST [37] adopted a Transformer-based architecture, integrating contrastive learning, sampling, and propagation strategies to effectively capture node relationships and address missing attribute challenges. [38] suggested imputing attributes in the input space by leveraging parameter initialization and graph diffusion to generate multi-view information.\nPropagation based Methods In addition to the afore-mentioned deep learning-based methods, some propagation-based methods have been widely focused on due to their low complexity and high scalability. [16] introduced the FP method, which reconstructed missing features by minimizing Dirichlet energy and diffusing known features across the graph structure. [17] proposed the Pseudo-Confidence Feature Impu-tation (PCFI) method, which enhanced feature propagation by incorporating a pseudo-confidence-based weighting mechanism during propagation."}, {"title": "III. PRELIMINARIES", "content": "A. Problem Definition\nWe define an attribute-missing graph $G = (V,E,X,k)$, where $V = V_k \\cup V_u$. $V_k$ and $V_u$ denote the sets of nodes with known and unknown (missing) attribute features, respectively. The node attribute matrix is denoted by $X \\in \\mathbb{R}^{N \\times F}$. For the total nodes $N$, only nodes $k$ possess attributes. The adjacency matrix is $A \\in \\{0,1\\}^{N\\times N}$, diagonal degree matrix is $D$, symmetric normalized adjacency matrix is $\\tilde{A} = D^{-1/2}AD^{-1/2}$, $\\tilde{a}_{ij}$ represent the individual elements of $\\tilde{A}$, and symmetric normalized Laplacian matrix $L = I - \\tilde{A}$. The goal of this work is to reconstruct the missing attribute feature and apply it to downstream classification tasks.\nB. Feature Propagation\nTo reconstruct the missing attributes $X_u$ based on the known attribute features $X_k$ and the graph $G$, the optimization function of Feature Propagation (FP) [16] can be expressed as a process of minimizing the Dirichlet energy [44], [45]. For distinction, define the matrix $Z$ as the initial attribute feature matrix of the"}, {"title": "IV. PROPOSED METHOD", "content": "A crucial prerequisite for attribute-missing graph learning is the accurate and effective recovery of missing attributes. However, the cold start problem worsen recovery, as low-degree and isolated nodes in the graph can hinder iteration convergence.\nThis paper proposes a novel AttriReBoost (ARB) method. To address the cold start problem, ARB boosts propagation based methods, redefines the initial boundary conditions of FP, and establishes virtual edges to enhance node connectivity, reconstructs missing attributes accurately and effectively. The overall framework is illustrated in Figure 2."}, {"title": "A. Redefinition of Boundary Conditions", "content": "An intuitive method to reconstructing missing attributes is to converge to an optimal value by iterative approximating during propagation. However, the repetitive resetting of known node attributes in propagation disrupts the smooth flow of"}, {"title": "B. Virtual Edges for Connectivity", "content": "In real-world graphs, many nodes are sparsely connected, and some are even isolated, resulting in a long-tail distribution of node degrees. Consequently, it struggles to effectively generalize to tail or cold start nodes for feature propagation. Observing the iterative Equation (3), $X^{(l+1)} = \\tilde{A}X^{(l)}$, since $\\tilde{A}$ is a symmetric normalized matrix, its spectral radius is not strictly less than 1, which could not guarantee convergence. To address this issue, a fully connected graph $G = (V,E, Z, k)$ is introduced, where the virtual edge set $E$ assumes that all node pairs are connected."}, {"title": "C. Optimization Function of ARB", "content": "To optimize the Equation (5), we first define the matrix $I= diag\\{\\lambda_1, \\lambda_2,\u2026\u2026,\\lambda_N\\}$, $\\lambda_i= \\begin{cases}\n1 & \\text{if } i \\in V_k \\\\\n0 & \\text{otherwise}\n\\end{cases}$, representing the known and unknown nodes, and explores the gradient $\\nabla \\mathcal{L}(X)$:"}, {"title": "D. ARB Convergence and Steady State Proof", "content": "The ultimate goal is to demonstrate that the proposed ARB can iteratively approach the optimal value to achieve attribute reconstruction. By introducing metric space, the convergence of the proposed ARB has been rigorously proven at the theoretical level.\n$\\square$ Convergence Proof: Since $\\tilde{A}$ be the symmetric normalized adjacency matrix, $\\rho(\\tilde{A}) \\le 1$ [46]. Let $B = \\tilde{A} + (1 - \\alpha)J$ is a strong connection matrix (irreducide matrix), $\\rho(B) = ||B||_2 \\le ||\\tilde{A}||_2+ (1-\\alpha)||J||_2 = \\alpha \\rho(\\tilde{A}) + (1 - \\alpha)\\rho(J) \\le 1$. Equation (9) can be written as:"}, {"title": "E. Complexity, Scalability, and Learning", "content": "The goal of ARB is to reconstruct missing attributes. Compared to FP, it introduces only two additional adjustable hyperparameters, $\\alpha$ and $\\beta$, thereby retaining all the advantages of FP in terms of low complexity $O(|E|+F|V|)$ and scalability [16]. ARB is a gradient-free method that can be run as preprocessing on the CPU for large graphs and integrated with any graph learning model, like GCN [33] and GAT [48], to generate predictions for downstream tasks [49]."}, {"title": "V. EXPERIMENTATION AND ANALYSIS", "content": "To thoroughly verify ARB's effectiveness in attribute recon-struction and downstream tasks, we address the following key questions:\nQ1: How does ARB perform in attribute reconstruction?\nQ2: How does ARB perform in downstream tasks after attribute-missing reconstruction?\nQ3: How does ARB address the cold start problem and convergence issues?\nQ4: Are all components of ARB effective?\nQ5: How does ARB perform under different missing rates?\nQ6: How is the computational efficiency of ARB?"}, {"title": "A. Experimental Setup", "content": "1) Dataset: We chose eight public graph datasets included Cora, CiteSeer, PubMed [50], CS [51], Computers, Photo [51], and large-scale datasets Ogbn-Arxiv and Ogbn-Products [52] for this study, each containing vital information like nodes, edges and attribute features. Table I shows data statistics for each dataset.\n2) Baseline: We compare ARB with several baseline meth-ods. NeighAgg [22] aggregates features of one-hop neighbors using mean pooling. GNN* refers to the best-performing models among GCN [33], GraphSAGE [53], and GAT [48]. GraphRNA [29] and ARWMF [30] are recent feature generation methods. SAT [12] uses a shared latent space for features and graph structure. Amer [35] integrates attribute completion and embedding learning using GAN-based constraints. SVGA [13] employs Gaussian Markov random fields for feature estima-tion. ITR [14] fills missing attributes using graph structure and refines them iteratively. FP [16] uses Dirichlet energy minimization to impute features. PCFI [17] introduces pseudo-confidence for feature imputation. MAGAE [36] mitigates spectral concentration via a graph autoencoder. CAST [37] is a Transformer-based method combining contrastive learning and propagation. MATE [38] enhances attribute imputation through graph diffusion and multi-view information.\n3) Implementation Details: Unless otherwise stated, we allocate 40% of the observable data as the training set and consider 60% of the attribute-missing nodes as target nodes. We split target nodes into validation and test sets in a 1:5 ratio, consistent with previous work [13], [14]. ARB performs missing attribute reconstruction according to the process outlined in Algorithm 1, executing l iterations. In each iteration, the neighborhood information of each node is first gathered through 3rd line in Algorithm 1, filling in the missing features, during which known features are also updated.\nThen, in 4th line of Algorithm 1, the known node attributes are reset, allowing them to participate in the next iteration. After completing the iterations, the final output $X_u$, representing the reconstructed missing attributes, is denoted as $\\tilde{X}_u$. Both our attribute reconstruction and node classification tasks solely use $\\tilde{X}_u$ as input, with no other inputs involved. This method ensures that we assess the usability of the reconstructed features in isolation. For the node classification task, all methods use a two-layer MLP as the classifier, with an Adam optimizer set to a learning rate of le-2, a hidden channel dimension of 256, and a maximum training epoch of 1000, performing five-fold cross-validation [16]. The overall implementation process is illustrated in Figure 3."}, {"title": "B. Attribute Reconstruction Results (Q1)", "content": "To evaluate the quality of attribute reconstruction, the similarity probability between the reconstructed attributes and the true attributes at each dimension is the main target. To"}, {"title": "C. Node Classification Results (Q2)", "content": "In our node classification experiments, our primary focus is to validate the effectiveness of the reconstructed features for downstream tasks. Therefore, we only use the reconstructed attribute features $\\tilde{X}_u$ of the unknown nodes $V_u$ for five-fold cross-validation, a choice influenced by methods like SAT [12] and SVGA [13]. We input the features that achieve the highest Recall@10 and CORR metrics in attribute reconstruction into a linear classifier for five-fold cross validation. The results of the node classification are summarized in Table IV. Our method consistently outperforms the baseline method, with an average improvement of 2.49% over the second-best method.\nFurther analysis reveals that while deep generative meth-ods like SAT and SVGA demonstrate certain advantages in attribute reconstruction, they underperform in downstream node classification tasks, particularly on continuous feature datasets. Notably, a key advantage of propagation-based methods like FP and PCFI is their independence from gradient descent, enabling efficient training on CPU [16]. This eliminates the need for memory-intensive graph partitioning and batch processing, providing a natural advantage on large-scale datasets like Ogbn-Arxiv and Ogbn-Products. Moreover, ARB outperforms other propagation-based algorithms, demonstrating its strong reconstructed attributes in downstream tasks.\nHowever, we note that the experimental setup differs from those used in FP [16] and PCFI [17], where semi-supervised node classification includes all nodes $V$ with known attributes. Additionally, these methods follow the standard dataset splits and settings from PyG \u00b9 and OGBN 2 and achieve competitive results even under extreme missing rates (99% and 99.5%). To ensure a fair comparison and to assess ARB's effectiveness in semi-supervised node classification, we conducted additional experiments under these conditions. As shown in Table V, ARB outperforms other methods, demonstrating superior per-formance across both regular and extreme missing conditions.\nThus, in response to Q2, our conclusion is: ARB demon-strates strong adaptability in downstream tasks after attribute reconstruction, outperforming baseline methods, particularly in large-scale datasets."}, {"title": "D. Convergence Speed to Verify Cold Start (Q3)", "content": "Judging ARB's performance in mitigating the cold start problem could be done by evaluating its early stopping and convergence speed capabilities.\nAs shown in Figure 4(a), in weakly connected graphs like Cora, ARB reaches the optimal solution in fewer epochs (Epoch \u2248 10), demonstrating faster attribute reconstruction. Similarly, in strongly connected graphs like PubMed (Figure 4(b)), ARB adapts and approaches the optimal solution by Epoch \u2248 15, highlighting its efficiency in resolving the cold start problem with minimal training.\nDuring the iterative process, ARB converges faster than deep generative methods. As shown in Figure 4(c), it outperforms SVGA and FP in both convergence speed and reconstruction"}, {"title": "E. Ablation and Hyperparameters Experiments (Q4)", "content": "Tables VII and VIII present the results of the ablation study for ARB. In the tables, \"w/o BC\" denotes the removal of the new boundary condition mechanism in ARB, and \u201cw/o VE\" indicates the removal of the virtual edge mechanism in ARB. The results show that ARB always achieves optimal performance across various scenarios, outdoing the other schemes in performance. Further analysis reveals that the new boundary conditions contribute more significantly to performance improvement compared to the virtual edges. While the virtual edge mechanism enhances global connectivity, it inevitably brings some noise. Therefore, to fully leverage its benefits, proper tuning of the parameters $\\alpha$ and $\\beta$ is necessary. The virtual edges and new boundary conditions add a propagation channel but may also introduce noise. To address this, the hyperparameter $\\alpha$ regulates their influence weight. Additionally, the number of propagation layers $l$ also impacts the results. Therefore, the three key hyperparameters\u2014$\\alpha$, $\\beta$, and $l$\u2014are critical to ARB's performance and require careful tuning.\nRegarding hyperparameter tuning, we propose the Heuristic Hyperparameter Searcher, using nDCG or CORR as the target metric. Starting at ($\\alpha$, $\\beta$) = (0.5,0.5), neighboring points at distance d are evaluated, and the best-scoring point becomes the new center. The process repeats, halving d when no better points are found, until the stopping condition is met.\nConcluded from Figure 5, the optimal result suggests setting $\\alpha$ within the range of 0.9 to 1. $\\beta$ is crucial for the redefinition of boundary conditions in ARB, and must be tuned for each specific dataset to achieve optimal performance. At lower $l$ values, the focus is on local neighborhood structures, which is crucial for nodes in small components with limited connectivity. As $l$ increases, global propagation enriches the feature space with structural information from distant parts of the network."}, {"title": "F. Sensitivity Analysis on Missing Rates (Q5)", "content": "We conduct a series of attribute reconstruction experiments to validate the robustness of ARB, with missing attribute rates"}, {"title": "G. Training Time Comparison Verification (Q6)", "content": "Since propagation methods do not require gradient descent operations, they naturally have an advantage over deep genera-tive methods. Figure 7 shows that, under the same hardware conditions (NVIDIA GeForce RTX 3090 24G) and with SVGA's loss converged, as well as 20 propagation iterations for both PCFI and ARB, ARB significantly outperforms the baseline methods SVGA (deep learning method) and PCFI (propagation method) in terms of training speed. Specifically, ARB is approximately 163 times faster than SVGA and 4.4 times faster than PCFI on average.\nFor propagation methods, Table X is conducted with the same number of propagation iterations $l = 20$. It can be observed that our ARB method incurs almost no additional runtime compared to FP. However, PCFI is approximately four times slower than FP. This is primarily because PCFI introduces confidence calculation, which involves searching neighborhood distances and computing the correlation matrix, thereby increasing computational complexity. Additionally, this method exceeded memory capacity on the Ogbn-Products dataset, forcing it to switch to CPU computation, which limited its scalability on large graphs. Overall, the analysis indicates that while ARB has computational efficiency comparable to FP, it significantly outperforms FP in terms of reconstruction accuracy.\nThus, in response to Q6, our conclusion is: ARB is highly computationally efficient and low in complexity, making it well-suited for scaling to large graphs."}, {"title": "VI. CONCLUSION", "content": "This paper presents AttriReBoost (ARB), a novel method for reconstructing missing attributes in graph data through a propagation-based approach. ARB introduces two key in-novations: redefining boundary conditions and incorporating virtual edges, which are specifically designed to address the cold start problem in attribute-missing graphs. The method operates without relying on gradient-based learning, offering a simplified and computationally efficient solution. Theoretical analysis rigorously proves ARB's convergence, and empirical evaluations demonstrate its superior performance in attribute reconstruction and downstream node classification, with a notable reduction in training time. ARB's efficiency and scalability position it as a competitive solution in the field of graph-based learning.\nFuture research will focus on three main directions. First, we will explore the relationship between missing attributes and propagation dynamics, aiming to understand how attribute sparsity impacts information flow and design more robust prop-agation mechanisms. Second, we will investigate alternative methods for constructing virtual edges, such as using graph generative models, structural similarities, or domain-specific heuristics. Finally, we plan to integrate ARB with GNNs as a pre-filling processor, developing adaptive mechanisms to handle varying levels of attribute incompleteness and diverse graph structures, expanding ARB's applicability to a wider range of real-world scenarios."}]}