{"title": "Less is More: Improving LLM Alignment via Preference Data Selection", "authors": ["Xun Deng", "Han Zhong", "Rui Ai", "Fuli Feng", "Zheng Wang", "Xiangnan He"], "abstract": "Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To accurately estimate margins for data selection, we propose a dual-margin guided approach that considers both external reward margins and implicit DPO reward margins. Extensive experiments demonstrate that our method reduces computational cost dramatically while improving performance. Remarkably, by using just 10% of the Ultrafeedback dataset, our approach achieves 3% to 8% improvements across various Llama and Mistral series models on the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3% improvement with 25% online data, while further reducing training time. These results highlight the potential of data selection strategies for advancing preference optimization.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017; Ziegler et al., 2019) has emerged as a crucial technique for aligning Large Language Models (LLMs) with human preferences and values. Traditional RLHF implementations involve a two-stage process: reward model training based on preference data followed by reinforcement learning optimization. However, this approach presents significant computational challenges, requiring loading multiple model instances and extensive hyperparameter tuning.\nAs an alternative, Rafailov et al. (2024) introduced Direct Preference Optimization (DPO), which streamlines the alignment process by directly optimizing the LLM policy from preference data. DPO has demonstrated comparable effectiveness while substantially reducing computational requirements compared to classical RLHF. Following DPO's introduction, numerous studies have proposed improvements through modified learning objectives (Zhao et al., 2023; Azar et al., 2024; Ethayarajh et al., 2024) and iterative learning schemes (Xiong et al., 2024). While these algorithmic advances have shown promise, there remains a critical gap in our understanding of the data-centric aspects of preference learning: what characteristics of preference data contribute most to model alignment?\nThis work thoroughly studies the impact of preference data quality on DPO training, which is crucial for developing more efficient training strategies. In particular, we achieve both improved performance and reduced computational costs through strategic data selection. Our research makes three primary contributions:\n(1) We prove in theory the necessity of data selection in the presence of exogenous noise. Specifically, the inherent noise in the reward model may flip the preference between response pairs, leading to the emergence of the parameter shrinkage issue. Furthermore, we demonstrate that margin-based selection criteria can effectively address this issue by inducing parameter inflation.\n(2) Building on this insight, we propose a margin-maximization principle for dataset curation that incorporates signals from the ensemble of external rewards and DPO implicit rewards. Through extensive experiments across diverse datasets and base models, we show that this selection strategy shows two consistent advantages: it substantially reduces computational overhead via efficient data selection and improves model performance compared to training on the full dataset. In particular, on the Ultrafeedback dataset and its variants, our method identifies a 10% data subset for DPO training on LLama and Mistral series models, consistently achieving 3% to 8% point improvements on the AlpacaEval 2.0 benchmark relative to training on the complete dataset."}, {"title": "2. Background", "content": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a key method for aligning LLMs with human preferences. It leverages training data of the form D = {x, yw, Y\u0131}, where x represents the input prompt, and Yw and y\u0131 denote the preferred and dispreferred responses, respectively. The RLHF pipeline typically involves two stages: reward learning and policy optimization.\nReward Learning. In the reward learning stage, a reward model is trained to approximate human preferences based on preference data. By adopting the Bradley-Terry model (Bradley & Terry, 1952) to capture human preference, reward training involves minimizing the loss:\n$L_{RM}(r) = -E_{(x,y_w,y_l)~D}[log\\sigma(r(x, y_w) - r(x,y_l))]$,\nwhere $\\sigma(\\cdot)$ is the sigmoid function.\nPolicy Optimization with Reinforcement Learning. Once the reward model r is trained, it is used to guide the optimization of a policy $\\pi_{\\theta}(y|x)$, where e denotes the parameters of the model. This stage often employs reinforcement learning techniques such as Proximal Policy Optimization (PPO; Schulman et al., 2017) to optimize the policy by maximizing the expected reward.\n$\\max_{\\pi_{\\theta}} E_{x~D, y~\\pi_{\\theta}(\\cdot|x)} [r(x,y) - \\beta log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)}]$,\nwhere $\u00df > 0$ is the regularization parameter. However, this RL approach can be computationally expensive, sensitive to reward misspecification and require careful hyperparameter tuning.\nRecently, as an alternative to the RL-based policy optimization in RLHF, Direct Preference Optimization (DPO; Rafailov et al., 2024) has been proposed. DPO simplifies the reward alignment process by directly incorporating human preference data into supervised training. Instead of defining and optimizing a reward function explicitly, DPO minimizes\n$L_{DPO} (\\theta) = -E_{\\sigma} [\\beta log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}]$"}, {"title": "3. Methodology", "content": "We follow the model from Zhu et al. (2023) to illustrate why data selection can improve model performance. We assume that reward model $r(x,y) = (\\phi(x, y), w*)$ with some feature function $\\phi(\\cdot, \\cdot)$. For reward learning, our reward model can be an explicit $r(x, y)$ (Ouyang et al., 2022), while for\nDPO, $\\beta log \\frac{\\pi_{\\theta} (y|x)}{\\pi_{ref} (y|x)}$ plays the role of reward model implicitly (Rafailov et al., 2024). Based on observations in previous literature, we can derive such features by removing the last layer of the pre-trained model. However, both humans and other large models may use inaccurate reward functions to generate labels, where the labels represent the ranking of two responses. We say the preference between yw and y\u0131 is generated by $r(x, y_w) - r(x,y_l) + \\zeta$ where $\\zeta$ is an exogenous error. We use $\\Delta\\phi(x)$ to denote $\\phi(x, y_w) - \\phi(x, y_\u03b9)$ for simplicity.\nParameter Shrinkage. Here, we hope to find w to minimize\n$L_{RM} (w) = -E_{x,\\zeta} [log(\\frac{1}{1 + e^{-(\\Delta\\phi(x),w*) - \\zeta}}) - log(\\frac{1}{1 + e^{-(\\Delta\\phi(x),w)}})]$\n(1)\nIt holds that the first-order condition is\n$E_{x,\\zeta} [(\\frac{1}{1 + e^{(\\Delta\\phi(x),w*) + \\zeta}} - \\frac{1}{1 + e^{(\\Delta\\phi(x),w)}})\\Delta\\phi(x)] = 0$\n(2)\nSince we know that $(\\Delta\\phi(x), w*)$ is positive, when $\\zeta$ is small comparing to the margin, it holds that $\\frac{1}{1 + e^{(\\Delta\\phi(x), w*) + \\zeta}}$ is convex with respect to $\\zeta$. Due to Jensen's inequality, it holds that\n$E_{x,\\zeta} [\\frac{1}{1 + e^{(\\Delta\\phi(x), w*) + \\zeta}}] > E_x [\\frac{1}{1 + e^{(\\Delta\\phi(x), w*)}}]$\nSimilarly, we have\n$E_{x,\\zeta} [\\frac{e^{-(\\Delta\\phi(x), w)}\\Delta\\phi(x)}{1 + e^{-(\\Delta\\phi(x), w*) - \\zeta}}] < E_x [\\frac{e^{-(\\Delta\\phi(x), w)}\\Delta\\phi(x)}{1 + e^{-(\\Delta\\phi(x), w*)}}]$\nSince the optimal w is w* without $\\zeta$, plugging w* in Equation (2) will cause the left-hand side to be greater than the"}, {"title": "3.1. Dual-Margin guided Data Selection for Preference Learning", "content": "In this section, we present the Dual-Margin (DM) guided efficient preference data selection that enhances the performance of direct preference optimization while significantly reducing the training cost. Driven by the theoretical result, our main idea is to let the model reach an overall high margin during preference learning. We realize this by providing a robust estimation of reward margins for the entire dataset ahead of full-set training, which then allows efficient high-margin data filtering. DM operates in two stages: (1) computing external and implicit rewards, and (2) fusing these rewards to suppress noise and achieve more reliable margin estimates.\nInitialization. We aim select a subset $D_{train}$ from D =\n${(x^i, Y_w^i, Y_l^i)}_{i=1}^N$, such that the model $\\pi_\u03b8$ trained on the\n$D_{train}$ achieves performance comparable or better than that trained on D. To guide this selection, we leverage an external reward model $r_{ex}(x, y)$ and a small seed dataset $D_o$ to learn an implicit reward model $r_{im}(x, y)$.\nStep 1: Reward calculation. First, we calculate $r_{ex}(x, Y_w^i)$ and $r_{ex}(x^i, Y_l^i)$ for each datum in D. Rafailov et al. (2024) proves that we can calculate implicit reward as\n$r_{im} (x,y) = log \\frac{\\pi_\u03b8(y|x)}{\\pi_{ref} (y|x)} = log \\frac{\\Pi_{j} \\pi_\u03b8(y_j|x, y_{1,\u00b7,j-1)}}{\\Pi_{j} \\pi_{ref} (y_j|x, y_{1,\u00b7,j-1})}$.\nAs this reward margin between chosen and rejected samples calculated by different models shows a strong correlation (refer to Figure 1), we propose disentangling its calculation from the target model $\\pi_\u03b8$. In particular, we introduce a small model $\\pi_\u03c2$, and fine-tune it on $D_o$ to get a weakly aligned model $\\pi_\u03c2$. These two models are then utilized for low-cost implicit rewards calculation on D.\nStep 2: Margin fusion. After calculating the margins\n$m_{ex} = r_{ex}(x, Y_w) - r_{ex}(x, Y_l)$ and $m_{im} = r_{im}(x, Y_w) -\nr_{im}(x, Y_l)$, we consider appropriate fusion strategies for\ncombining these two signal sources to reduce noise in the reward modeling process. The simplest approach, which we call DM-ADD, directly sums the signals. This strategy reflects a lenient selection criterion where samples are considered valuable if they demonstrate a high margin from either reward source.\nA strict fusion approach considers that samples should receive low priority if they show a low margin in either signal. We first transform margin values to margin-guided probability through a simple linear transformation\n$P(m) = \\frac{clip(m, M_1, M_2) - M_1}{M_2 - M_1} , for m \\in {m_{ex}, m_{im}}$,\nwhere clip(m) = min(max(m, $M_1$), $M_2$)) and ($M_1, M_2$)\nare tuning parameters. Following the fusion principle in multi-view probabilistic clustering (Deng et al., 2024), we further obtain\n$P(Y_w \u2265 Y_l | m_{ex}, m_{im}) = \\frac{P(m_{ex}) P(m_{im})}{P(m_{ex}) P(m_{im}) + (1 - P(m_{ex})) \u00b7 (1 - P(m_{im}))}$\n(3)\nThis adaptive approach mitigates the adverse effects of outlier samples with unusually high margin values. See more implementation details in Appendix B. Overall, this fusion approach is referred to as DM-MUL."}, {"title": "3.2. Data Efficient Online Iterative DPO", "content": "Online RLHF (Xiong et al., 2024) has gained significant attention for its effectiveness in aligning models with human preferences. The method iteratively generates multiple responses for given prompts and employs an external reward model to identify Best-of-N (BoN) and Worst-of-N (WoN) responses, forming preference pairs. This 'self-reward maximization' approach effectively sharpens the model's response space by reducing the probability of generating low-quality responses (Huang et al., 2024). However, the online generation process still produces ambiguous samples, particularly when prompts either have highly deterministic answer patterns or are exceptionally challenging for models to address. Such on-policy data pairs may not effectively guide model improvement. To address this problem, we propose a straightforward solution called DPO-DM: incorporating DM into each round of the on-policy data collection process and conducting preference optimization exclusively on selected high-margin samples. This approach can substantially reduce training costs, particularly when dealing with large prompt spaces."}, {"title": "4. Experiments", "content": "We organize the experiments as follows: we explain the experimental setup in Section 4.1; we compare DM with various sample selection baselines on diverse preference tasks and present the detailed results in Section 4.2; then we focus on the important chat task, and explore the effectiveness of DM in enhancing comprehensive dialogue ability in Section 4.3; next, we explore the online setting in Section 4.4. lastly, we perform an ablation study for the offline preference data selection in Section 4.5."}, {"title": "4.1. Experimental Setup", "content": "Preference Datasets. We evaluate our approach using three established preference datasets: (1) Reddit TL;DR summarization dataset (V\u00f6lske et al., 2017; Stiennon et al., 2020) that contains human-written summaries and human-rated results, (2) Anthropic Helpful and Harmless dialogue dataset (HH) (Bai et al., 2022), and (3) UltraFeedback (Cui et al., 2024), which comprises quality-scored model responses across diverse prompts from multiple sources. To explore how models react to on-policy data, we leverage two modified versions of the UltraFeedback dataset, Llama-UltraFeedback and Mistral-UltraFeedback (Meng et al., 2024). In the variants, the original chosen and rejected responses are replaced with the highest and lowest scored responses, respectively, sampled from five candidates gen-"}, {"title": "4.2. Win Rate Comparison with Baselines on Classic Preference Datasets", "content": "Firstly, we compare DM and baseline strategies on three widely-used preference datasets: TL;DR, HH, and UltraFeedback. Using a Llama-3.2-3B model as our base architecture, we evaluate different selection methods, each sampling 2,000 training examples for DPO training. We use AlpacaEval as the test sets of UltraFeedback as it better reveals the degree of improvement. The results, measured"}, {"title": "4.2.1. Analysis", "content": "We analyze DM from two perspectives: the joint distribution of different sources of reward margins and the training dynamics for subsets with different margin properties. We utilize the UltraFeedback datasets for this analysis.\nThe left and middle panels of Figure 1 illustrate the sample distribution across external-implicit and implicit-implicit margin dimensions on UltraFeedback. Several key observations emerge: (1) The correlation between implicit and external reward margins is notably weak. In particular, samples exhibiting high positive implicit margins span a broad range of external margins, including strongly negative values, and vice versa. This pattern is consistent across other datasets (see Appendix C.2). This divergence highlights the distinct preference patterns captured by these two reward types. Supporting this observation, Table 1 demonstrates that neither margin type consistently outperforms the other in subset selection: external margins prove more effective for TL;DR, implicit margins show superior performance on HH, while both perform comparably on UltraFeedback. These findings underscore the importance of combining both reward types in an ensemble approach. (2) In contrast, we observe a strong correlation between implicit reward margins calculated by models of different sizes (Llama-3.2-3B and 1B), and the robust correlation is observed on other datasets as well (see Appendix C.2). This validates our design of using a separate small model for implicit reward margin calculations."}, {"title": "4.3. AlpacaEval 2.0 Win Rate Comparison", "content": "In this section, we conduct a detailed examination of the chat task to understand how data filtering enhances both DPO training efficiency and models' versatile conversational abilities compared to GPT-4-turbo, representing a key application area for preference learning. We use both Llama-3-8B (base) and (instruct) models, measuring performance through raw and length-controlled win rates on AlpacaEval 2.0, a benchmark widely adopted in preference learning studies. The results of DM and various baselines are shown in Table 2.\nDM-ADD and DM-MUL consistently outperform full dataset DPO training. Table 2 shows models trained on DM-selected subsets (using only 10% of data) achieve 2-4% higher raw and LC win rates compared to full dataset training across different models (base and instruct) and datasets (offline and on-policy). These consistent improvements demonstrate DM's robustness and effectiveness, validating the value of data filtering for DPO training. In contrast,"}, {"title": "4.4. Online Iterative DPO", "content": "We explore the benefits of data selection in iterative DPO training using prompts from UltraFeedback as in Xiong et al. (2024). Our methodology employs DM-MUL to filter 5,000 samples from a pool of 20,000 generated samples per iteration. We evaluate both the base and instruct versions of Llama-3-8B, conducting single-epoch training rounds with hyperparameters \u03b2 = 0.1 and learning rate 5 \u00d7 10-7. Table 3 presents our experimental findings, from which we draw two key observations:\nWhile Best-of-N and Worst-of-N preference pairs from a strong reward model are informative, data filtering is still important to DPO training. DPO-DM performs better for both base and instruct settings using only 25% of the data. This can be attributed to the presence of numerous ambiguous, low-margin samples in the constructed online preference dataset. Furthermore, standard iterative DPO shows increasing output length across training rounds, but data filtering mitigates this issue. By round three, DPO-DM demonstrates both superior performance and better output length control."}, {"title": "4.5. Ablation Study", "content": "A critical aspect of dataset filtering methods is their generalization capability - specifically, how well the method (se-"}, {"title": "5. Related Work", "content": "Preference Learning Algorithms.Reinforcement Learning from Human Feedback has become a crucial component of recent Large Language Models (LLMs) such as ChatGPT (Ouyang et al., 2022). While the classical RLHF pipeline traditionally uses Proximal Policy Optimization, several alternative approaches have been proposed. These include but not limited other RL-based training algorithms (Li et al., 2023c; Zhong et al., 2024), rejection sampling (Dong et al., 2023; Gulcehre et al., 2023), conditional supervised finetuning (Lu et al., 2022; Yang et al., 2024; Zhang et al., 2024a), and Direct Preference Optimization (Rafailov et al., 2024). Among these alternatives, DPO has gained significant attention due to its simplicity and robust performance. Following the introduction of DPO, numerous works (Zhao et al., 2023; Azar et al., 2024; Ethayarajh et al., 2024; Meng et al., 2024; Tang et al., 2024; Han et al., 2024; Xu et al., 2024; Hong et al., 2024; Park et al., 2024) have attempted to improve its performance by modifying the DPO objective.\nData Selection in LLM Fine-tuning. Data selection is"}, {"title": "6. Conclusion", "content": "Our research bridges the gap between algorithmic advances and data-focused approaches in Large Language Model (LLM) alignment by systematically examining how preference data quality affects Direct Preference Optimization (DPO). We address the issue of parameter shrinkage caused by noisy data and introduce a dual-margin-based strategy for selecting high-quality training examples. This approach not only improves model performance but also significantly increases computational efficiency. Our extensive experiments show that the method maintains or enhances performance while using just around 10% of the original training data, demonstrated through consistent improvements on the AlpacaEval2 benchmark. Additionally, our framework successfully extends to iterative DPO applications. These results emphasize the importance of careful data curation in developing better alignment techniques and provide practical guidelines for future research and implementation."}, {"title": "A. Datasets and Evaluation Details", "content": "Data information. The detailed information about the datasets used in the experiments is shown in Table 5. The test sets\nof TL;DR and HH are sampled from their original large testing pool, and we utilize prompts in AlpacaEval as the test sets\nfor all models trained on UltraFeedback and its variants. In particular, results in Table 1 utilize Text-Davinci-003 generated\nanswers as reference response as it can better reflect how models' ability varies with different training data, and results in\nother tables all utilize GPT4-1106-preview generated answer as reference response (i.e., AlpacaEval 2.0).\nEvaluation details. Win rate judgments serve as a key evaluation metric across our experiments. For the TL;DR and HH,\nwe slightly modify the evaluation prompts suggested by Rafailov et al. (2024) for the win rates judge. We use the original\nprompt designed for AlpacaEval and AlpacaEval 2.0 benchmark in all testing."}, {"title": "B. More Implementation Details", "content": "We present the implementation details of our baseline methods: TOP, MID, and BOT using margins calculated from\nthree metrics, Conditional Perplexity (CPPL), External (Ex), and Implicit (Im) rewards. Subsequently, we describe the"}, {"title": "C.1. Singular Margin Distribution", "content": "The margin distributions calculated using CPPL margin, External and Implicit DPO reward margins, as illustrated in\nFigures 5,4,3, reveal a notable concentration of sample margins around zero. This clustering around the zero indicates\nambiguous preference labels. It leads to the challenge in preference learning, as evidenced by the substantially slower\ndecrease in training loss (and slower increase in training margin) compared to samples with larger margins, as shown in\nSection 4.2.1."}, {"title": "C.2. Joint Margin Distribution", "content": "To complement the left and middle subfigures in Figure 1, we present additional results showing the joint margin distributions\nof samples on the other datasets in Figure 6. Our analysis reveals that external and implicit margins exhibit minimal\ncorrelation across all four datasets, while implicit margins calculated by different models maintain a high correlation. These\nfurther enhance the rationality of our design detail of DM: fusion of both margins and disentangling implicit margin from\nthe target model (if the target model is a bit large and we want to accelerate the enumeration process of the full-set.)"}, {"title": "D. More Experimental Results", "content": "To complement the right subfigure in Figure 1, we present additional results showing the progression of training loss and\nmargins throughout the DPO training process. The results are shown in Figure 7. All strategies demonstrated consistent\npatterns in loss reduction: both TOP margin-oriented and BOT strategies achieved rapid decreases in training loss, while\nthe MID strategy exhibited slower convergence and remained at significantly higher final loss values. Regarding training\nmargins, TOP strategies achieved higher levels compared to BOT and MID approaches. Notably, our proposed DM-ADD\nand DM-MUL strategies demonstrated even larger margins than the Implicit Margin-TOP strategy."}, {"title": "D.3. Resources and computation cost", "content": "For all experiments, we utilized 8 A100 GPUs. We conduct SFT/DPO training with 4 A100 GPUs for all runs in our\nexperiments. For both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) training, we allocated 4\nA100 GPUs per run. Training 8B parameter models on the UltraFeedback dataset for two epochs required approximately\n9 hours of computation time. In each round of iterative DPO implementation, we performed generation and annotation\nprocesses on 4 A100 GPUs, with each GPU processing 5,000 prompts with 5 distinct generations per prompt. The overall\ngeneration that utilizes vLLM (Kwon et al., 2023) for acceleration takes about 1.5 hours, and the corresponding reward\nannotation takes about 2 hours."}, {"title": "D.4. More Results for Ablation Study on the DPO Variants", "content": "As a complementary study to the results shown in Figure 2, we conducted experiments using the Llama-3-8B-Instruct model\nwhile maintaining all other experimental parameters. The results, presented in Figure 9, demonstrate that models trained on\nsubsets selected by DM-MUL achieved significantly higher win rates across most evaluation scenarios."}]}