{"title": "A Predictive Model Based on Transformer with Statistical Feature Embedding in Manufacturing Sensor Dataset", "authors": ["Gyeong Taek Lee", "Oh-Ran Kwon"], "abstract": "In the manufacturing process, various sensor data is collected from the equipment. Engineers can use this collected data to build predictive models, enabling them to manage the process and improve production yield. However, due to the rapid development of process tech- nology, collecting a sufficient amount of data has become challenging, making it difficult to create powerful models. In this study, we propose a novel predictive model based on the Transformer architecture by developing statistical feature embedding and window positional encoding. Statistical features are an effective representation of sensor data in the manufacturing domain. We introduce statistical feature embedding to allow the Transformer to effectively learn both time- and sensor-related information. We create statistical feature embedding by applying several different statistical-pooling layers individually to the sensor data and combining the outputs. Additionally, window positional encoding can capture precise time information from the proposed feature embedding. We assess the performance of the proposed model in two different problems: fault de- tection and virtual metrology. Experimental results demon- strate that the novel predictive model outperforms base- line models. This can be attributed to efficient parameter usage an advantage for sensor data, which is inherently limited in sample size. Our result supports the applicability of our model across various manufacturing industries.", "sections": [{"title": "I. INTRODUCTION", "content": "In industry 4.0, a stable smart manufacturing technol- ogy system is essential to ensure the competitiveness of manufacturing companies. This requires a high-level qual- ity monitoring system or powerful predictive model. Stable systems aid in developing intelligent manufacturing through quality monitoring and predictive models. However, the rapid advancement of manufacturing technology has led to a notable increase in the complexity of manufacturing equipment and process recipes. This complexity has been further addressed by the emergence of a new generation of information technologies dealing with collected data from the equipment [1]. In general, from the equipment or production line, a large amount of sensor data is collected. Therefore, there has been a large effort in building high-quality and stable monitoring systems or predictive models.\nIn manufacturing processes, there are several types of predictive models, including fault detection (FD) models, virtual metrology (VM) models, and yield prediction mod- els. A FD model is a critical element in building a smart manufacturing system. Such a system is indispensable for accurately identifying and diagnosing faults within production equipment. As a result, interest in improving intelligence in FD models, diagnosis, and prediction is increasing. This increased emphasis on intelligent FD has led to applications in various domains, including battery manufacturing [2], [3], semicon- ductor production [4]\u2013[7], automotive assembly [8], [9], panel manufacturing [10]\u2013[12] and numerous other production lines [13], [14]. In general, FD systems detect the abnormal status of production based on the status of sensor data from the equipment.\nTo manage production quality effectively, a VM model is essential, particularly in semiconductor manufacturing where each lot comprises 10 to 25 wafers, with physical metrology typically conducted on only 1 or 2 wafers for measuring thickness, critical dimensions, and electrical resistance. These measurements are critical for ensuring wafer quality, necessi- tating the use of VM models developed from sensor data [15]- [21]. Additionally, we can forecast the volume of production that can be delivered to the customer by predicting the yield [22]-[26].\nTime-series data encompassing various values, such as humidity, gas, and pressure, is collected from the sensors at specific intervals from the equipment. Predictive models deal- ing with sensor data in manufacturing processes face signifi- cant challenges. As machine learning methods have evolved, various forms of these predictive models have emerged. Con- ventionally, many previous studies have extracted statistical features from time-series sensor data using a window, and they have utilized these extracted features in machine learning models such as linear regression, support vector machines, random forests (RF), and gradient boosting [15]\u2013[17], [27], [28]. This approach has shown good performance, but it cannot learn graphical features from the sensor data and could cause a high-dimensionality problem.\nWith the development of deep learning, various approaches have been emerging. The most basic approach to processing such sensor data is to view data with an x-axis sensor and a y- axis time as a 2D image and use it as an input of convolutional neural networks (CNN) that performs very well for image processing [4], [5], [7], [20]. In addition, long short-term memory (LSTM), which has been widely used in sequential data such as time-series data and natural language processing (NLP), has been employed in building predictive models for manufacturing processes [29]-[32]. These predictive models excel over the conventional approach because they can learn graphical features from time-series data. However, to achieve good performance, a deep learning model requires a large amount of data [33]-[35]. When the number of observations is small, it is highly likely that the conventional approach is superior to the deep learning model [28]. However, in the fields, it is difficult to obtain large amounts of data due to natural equipment wear or changing process recipes. Therefore, it is necessary to develop a predictive model that can capture graphical information from the time-series sensor data when the number of data is small.\nMeanwhile, in the field of NLP, the Transformer has been proposed as an architecture based on attention mechanisms [36]. The Transformer is a neural network architecture that leverages self-attention and parallel processing to excel in sequence-to-sequence tasks. The attention mechanism allows models to focus on specific parts of the input sequence while making predictions, enabling the model to capture long-range dependencies and relationships in the data more effectively. The innovative design of the Transformer has had a profound impact on the field of deep learning and has been widely adopted in various NLP tasks. Furthermore, in recent years, various variants of the Transformer have been developed in diverse fields, such as tabular data [37]-[39], time-series prediction [40]\u2013[42], and generative models [43]-[45].\nThe trend of utilizing the Transformer in various domains has now extended to time-series sensor data. One fundamen- tal approach is to treat sensor variables as words [46], as illustrated in Fig 1. The model cannot accurately incorporate sequential information into the model since the sensor variable is not measured sequentially. When the sensor input is trans- posed, the Transformer can capture the sequential information [14], [47]. However, this model cannot effectively utilize sen- sor information. This is because, in the original Transformer, the column indicates the dimension of embedding to represent a word. In NLP, the most important information is in 'words'. Similarly, in the sensor domain, the most critical information is in the 'sensors'. Therefore, both sequential information of sensor should be simultaneously integrated into a model. One paper proposed a method to extract statistical features from sensors and utilize the LSTM layer within a Transformer [47]. However, this method also struggles to properly learn sensor information, and it requires too large a number of parameters.\nIn this study, we propose a novel predictive model based on Transformer for the manufacturing sensor domain. To learn both sensor information and the sequential data from each sensor, we introduce a new method to organize the input embedding named statistical feature-based embeddings. It consists of the x-axis representing sensor information at a given time window and the y-axis representing statistical features for each sensor at that time. This sensor embedding is instrumental in capturing the sequential information of sensor variables. Additionally, we introduce a new positional encoding method tailored to this sensor embedding. The main contributions of this study are summarized as follows:\nA novel Transformer model for time-series sensor data in the manufacturing domain is proposed. This model can effectively learn graphical features from time-series sensor data, even when the amount of available data is small. This point is supported by the results of our experiments, where our proposed model demonstrates impressive performance compared to existing methods on two real-world datasets.\nThe process of creating statistical feature embedding can be understood as applying several different statistical pooling layers individually to the sensor data and then combining the outputs. Thanks to statistical feature-based embedding, our proposed Transformer requires a substan- tially lower number of parameters compared to existing methods.\nA new positional encoding method, designed for the sensor input embedding proposed in this work, can en- hance the model's performance compared to conventional positional encoding.\nWe conduct extensive comparisons of methodologies. We compare the proposed Transformer with various methods from other existing approaches, including conventional machine learning methods, deep learning-based methods (CNN and LSTM), and Transformer-based methods.\nThe remaining parts of this paper are structured as fol- lows: Section 2 reviews the Transformer in detail. Section 3 introduces our proposed model including sensor embedding and positional encoding. Subsequently, Section 4 presents the experiments and results. Additionally, Section 5 discusses the merits of our proposed model and potential future research. Finally, Section 6 concludes this paper."}, {"title": "II. BACKGROUND", "content": "The Transformer [36] is one of the most well-known archi- tectures in NLP. In this section, we review five main compo- nents of the Transformer. That are positional encoding, multi- headed attention, residual connection, normalization, and fully connected feed-forward network. These components serve as inspiration for, or have been adopted in, our proposed model architecture.\nAt first, the positional encodings are added to the input embeddings. Then, the encoded input embeddings sequentially passes multi-head attention, normal- ization, feedforward network, and normalization layers. A residual connection is used around the multi-headed attention and the feed forward network layers. The block composing from a multi-headed attention to the feedforward network is called a transformer block, which is the gray block in Fig 2. Several transformer blocks can be concatenated and form an encoder-decoder structure.\nIn the following subsections, we review the positional encoding in Subsection II-A and the other four components inside the transformer block in Subsection II-B."}, {"title": "A. Positional Encoding", "content": "The Transformer was originally applied to sequential data like a sentence in NLP. The input tokens such as words in a sentence are converted to vectors of dimension d through the embedding layer. Let $X_{emb} \\in \\mathbb{R}^{n \\times d}$ be the embedded input where n is the number of tokens and d is the number of embed- ding features. Although input tokens are in sequential order, the transformer block does not take the sequential relationship into consideration unlike recurrent neural networks. Therefore, the position encoding matrix $P \\in \\mathbb{R}^{n \\times d}$ is introduced which purely contains the positional information:\n$P_{(pos,2i)} = sin(pos/10000^{2i/d})$\nand $P_{(pos,2i+1)} = cos(pos/10000^{2i/d})$,\nwhere $M_{(i,j)}$ denotes the (i,j)-th element of the matrix M. The positional information is injected in $X_{emb}$ by adding P:\n$X = X_{emb} + P$. (1)\nThe positional encoding motivates the window positional encoding in our proposed model."}, {"title": "B. Transformer Block", "content": "The transformer block is composed of four layers along with two residual connections. In the following four subsubsections, multi-headed attention, residual connection, normalization, and feedforward network are introduced, respectively. Then, the overall process of the transformer block from the input X to the output is summarized in Subsubsection II-B.5 by putting four components together."}, {"title": "1) Multi-headed Attention:", "content": "Three types of multi-headed attention are used in the Transformer: multi-headed self- attention, masked multi-headed self-attention, and multi- headed encoder-decoder attention. The multi-headed self- attention is used in the encoder of the Transformer and the others in the decoder. Our review focuses on the multi-headed self-attention as it has been directly utilized in our proposed model architecture.\nThe multi-headed self-attention layer can be seen as apply- ing a function $LayerMHSA: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$ on the input, say M, and produces $LayerMHSA(M)$ where\n$Q^{(i)} = MW_q, K^{(i)} = MW_k, V^{(i)} = MW_v,$\n$H_i = softmax\\Big(\\frac{Q^{(i)} (K^{(i)})^T}{\\sqrt{d}}\\Big)V^{(i)}, for i = 1,..., h,$\nand $LayerMHSA(M) = Concat(H_1,...,H_h)W_o$.\nThe dimension of the parameter matrices are $W_q \\in \\mathbb{R}^{d \\times k}, W_k \\in \\mathbb{R}^{d \\times k}, W_v \\in \\mathbb{R}^{d \\times k}$, and $W_o \\in \\mathbb{R}^{h \\cdot k \\times d}$. The hyper parameters h and k are set to have hk = d. The head $H_i$ is computed by a scaled dot-product attention operation on $Q^{(i)}, K^{(i)}$, and $V^{(i)}$."}, {"title": "2) Residual Connection:", "content": "The residual connection [48] is done by adding the input and the output of a layer. In Fig 2, the residual connection is illustrated as an arrow connecting the input and the output of a layer with a plus sign in the end of the arrow. Let us say M is the input and Layer(M) is the output of a layer. Then, the residual connection is expressed as follows:\n$M + Layer(M)$."}, {"title": "3) Normalization:", "content": "The normalization layer [49] is a sim- ple method to reduce the training time of various neu- ral network models. It can be expressed as a function $LayerNorm: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$, where the i-th row of LayerNorm(M) is defined as\n$[LayerNorm(M)]_{(i,.)} = \\gamma \\odot\\frac{M_{(i,.)} - \\mu_i}{\\sigma_i} + \\beta^T$.\nHere, $M_{(i,.)}$ denotes the i-th row of M, $\\gamma,\\beta \\in \\mathbb{R}^d$ are parameters,\n$\\mu_i = \\sum_j M_{(i,j)}/d, \\sigma_i = \\sqrt{\\sum_j (M_{(i,j)} - \\mu_i)^2/d}$,\nand $\\odot$ is the element-wise multiplication between two vectors."}, {"title": "4) Position-wise Feed-Forward Networks:", "content": "The position-wise feedforward network layer applies the same fully-connected feedforward network to each position. It can be defined as a function $LayerFFN: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$ where the i-th row of LayerFFN(M) is defined as\n$[LayerFFN(M)]_{(i,.)} = max(0, M_{(i,.)} W_1 + b_1)W_2 + b_2$.\nThe dimensions of parameters are $W_1 \\in \\mathbb{R}^{d \\times m}, W_2 \\in \\mathbb{R}^{m \\times d}, b_1 \\in \\mathbb{R}^m$, and $b_2 \\in \\mathbb{R}^d$."}, {"title": "5) Overall Mechanism:", "content": "The transformer block takes X, defined in (1), as the input. Let us explore the output of the transformer block using the four components introduced above. The input $X \\in \\mathbb{R}^{n \\times d}$ passes the multi-head self atten- tion followed by the residual connection and the normalization and produces $Z \\in \\mathbb{R}^{n \\times d}$:\n$Z = LayerNorm(X + LayerMHSA(X)).$\nThen the feedforward network is applied on Z, again followed by the residual connection and the normalization:\n$U = LayerNorm(Z + LayerFFN(Z))$,\nwhich is the output of the transformer block. As the output $U \\in \\mathbb{R}^{n \\times d}$ has the same dimension as the input X, several transformer blocks can be stacked, each with its own set of parameters."}, {"title": "III. PROPOSED MODEL ARCHITECTURE", "content": "In this section, we introduce the proposed model archi- tecture for manufacturer time-series sensor data. Each component is explained in the following subsections."}, {"title": "A. Statistical Feature Embedding", "content": "To effectively train both sensor and sequential information in the sensor data through the Transformer in the context of manufacturer time-series sensor data, we develop the statistical feature embedding.\nIn manufacturing processes, p number of sensors are mea- sured from time 1 to time t for each sample. We partition the time into w number of time window blocks, say $I_1,..., I_w$ such that $\\cup_i I_i = \\{1,...,t\\}$. Each time window contains a similar number of consecutive time points and they do not overlap, i.e., $n_1 = |I_i| = |I_j|$ and $I_i \\cap I_j = \\emptyset$ for any $i \\neq j$. In each time window, we compute statistical features for each sensor. The examples of statistical features are mean, max, and standard deviation. Let us say that d number of statistical features are considered and $S_{ijk}$ is the k-th statistical feature of the i-th sensor in the j-th time window. We organize the calculated statistical features in the following manner.\n$X_{stat\\_emb} \\in \\mathbb{R}^{n \\times d}$, (2)\nwhere $[X_{stat\\_embed}]_{(i+p(j-1),k)} = S_{ijk}$. Here, n = p. w and d is the number of statistical features.\nAnother way to understand the process of the statistical feature embedding is by applying multiple statistical poolings together with flattening and concatenation, as illustrated in Fig 5. Let $Z \\in \\mathbb{R}^{p \\times t}$ be the sensor data where its (i, j)- th element corresponds to the i-th sensor value at time j. We apply a statistical pooling layer on Z, such as mean or maximum pooling, with a filter size of (1, n\u2081) and a stride size of (1, n\u2081). The pooling yields the matrix of dimension (p, w), say $Z_{pool} = [z_1,...,z_w]$. We then flatten $Z_{pool}$ and obtain Flat$(Z_{pool}) = [z_1^T...z_w^T]^T$. Repeating this process with d different statistical poolings gives d vectors: $Z_{pool}^1,... Z_{pool}^d$. Concatenating these vectors horizontally results in $X_{stat\\_emb}$=[Flatten$(Z_{pool}^1)$,..., Flatten$(Z_{pool}^d)$], which is equivalent to (2)."}, {"title": "B. Window Positional Encoding", "content": "The rows of $X_{stat-emb}$ are sorted by the sequential order of time windows and then, for rows with the same time window, further sorted by sensors. The time windows are in the order of time, but within a time window, the rows do not contain any sequential information, which is distinct from $X_{emb}$ in NLP. To inject the positional information respecting the nature of our $X_{stat-emb}$, we propose the window positional encoding, giving the same position weight within the same window block. We define the window position encoding matrix $P \\in \\mathbb{R}^{n \\times p}$ as follows:\n$P_{(pos,2i)} = sin([pos/p]/10000^{2i/d})$\nand $P_{(pos,2i+1)} = cos([pos/p]/10000^{2i/d})$, where $[a]$ is the nearest integer equal to or bigger than a. Similar to the original positional encoding introduced in the previous section, the matrix holds sequential information in time windows. In contrast to the original positional encoding, it provides consistent positional information within the same window block. Notice that $[pos/p]$ remains the same within the same window block.\nThe positional information is injected in $X_{stat-emb}$ by adding P:\n$X = X_{stat-emb} + P$."}, {"title": "C. Transformer Block and Prediction", "content": "We adopt an encoder transformer block from the Trans- former. After then, we flatten the output matrix from the trans- former block by the flattened layer. After that, a feed-forward neural network is used with a linear activation function for a regression problem such as building a VM model and a sigmoid activation function for a classification problem such as constructing an FD model."}, {"title": "IV. EXPERIMENTS", "content": "To validate the effectiveness of the proposed model, we conducted experiments with two real manufacturing datasets. We constructed two predictive models, the FD model and the VM model, on the two datasets, respectively. The two datasets consist of multivariate time-series sensor data, in- cluding temperature, humidity, and pressure, collected from the etching process of semiconductor manufacturing. The two datasets have different sets of sensor values. In semiconductor manufacturing, hundreds of unit processes are repeated. There- fore, even if the data is collected from the same process, the sensors used can vary. The first dataset includes classification labels (fault or normal). Thus, it is required to build a FD model. In contrast, the other dataset contains an measurement value. By predicting and monitoring this measurement value, engineers can determine the equipment's status and check the production quality. Therefore, for the second dataset, we build a VM model. While constructing predictive models using our proposed approach, we extracted six statistical feature from sensor data-mean, max, min, variance, range, and slope-for the statistical feature embedding."}, {"title": "A. Baseline models", "content": "There have been several approaches to handling sensor data. We compared our proposed method with representative methods in each of these approaches. The first approach is extraction of statistical features from sensors using a window and constructed tabular data. Then, we fed this data into conventional machine learning models, including the least ab- solute shrinkage and selection operator (Lasso) [50], a random forest (RF) [51], and extreme gradient boosting (Xgboost) [52].\nThe second approach involves deep learning models. To learn graphical features from time-series data, CNN, LSTM, and Bi-LSTM-based models have been predominantly utilized. For the CNN model, CNN for fault detection model (FDC- CNN), which was proposed for handling time-series data in manufacturing, was employed [4]. As for the LSTM and Bi- LSTM models, the sensor values for each time step were fed into each layer [53].\nFinally, the following Transformer-based predictive models were employed. Sensor variables were processed as words in the encoder blocks (TF1) in the Transformer [46]. In TF2 model, time information was treated as words in the encoder blocks [14]. Additionally, a neural network, utilizing the Transformer architecture and incorporating statistical features and LSTM layers, was employed (TF+LSTM) [47]."}, {"title": "B. Case1: Fault detection problem", "content": "1) Dataset: We utilized a real-world dataset obtained from a etching process of semiconductor manufacturer in South Korea. It contains 16 instances of faulty production and 8 instances of normal production. In addition, this dataset has 107 sensor variables. Our primary objective was to identify instances of faulty production using this dataset. The duration of the observations varies, with lengths ranging from 390 to 400 units.\n2) Experimental design: To conduct experiments properly, we split the total dataset into training and test data with an 80:20 ratio and stratified sampling. We applied both the proposed method and baseline models to the training data and evaluated their performance on the test data. Each experiment was repeated 10 times, and we calculated the average of performance measures across 10 repetitions. We selected three performance measures: F1-score (F1), accuracy (Acc), and the area under the curve (AUC). Finally, we set the window size"}, {"title": "C. Case2: Virtual metrology problem", "content": "1) Dataset: In the virtual metrology problem, we employed an actual semiconductor dataset gathered from South Korea over a period of 15 days. It contains 488 observations and 51 sensor variables, and the target variable is the electrical measurement value. The sensor variables are measured at 45- 47 time points for each day.\n2) Experimental design: To assess the performance of the VM model, we employed a window sliding validation ap- proach. Initially, we use the data from the first 5 days for training and the data from the following day for testing. Subsequently, the window shifts by 1 day, and this process continues until a total of 10 days' worth of data has been used for testing. For the performance measures, we used mean squared error (MSE) and mean absolute percentage error (MAPE), and we reported the average performance value for all test days. Finally, we set the window size to 10 to extract statistical features.\n3) Result: Table III presents the comparison results between the baseline models and the proposed model for the VM problem. The proposed model demonstrated excellent perfor- mance compared to the baseline models except the TF-LSTM. The TF-LSTM model showed similar results to our model. However, as shown in Fig 7.(b), the TF-LSTM model predicted values as the mean of the training target values. In contrast, the prediction values of the proposed model closely followed the trend of actual values, as shown in Fig 7.(a). The proposed model has a small capacity (with 39,065 parameters), while the TF-LSTM has a large capacity (with 6,782,689 parameters). The TF-LSTM model exhibited an overfitting problem due to its excessive capacity compared to the dataset size. Therefore, in this context, although the performance measures appear similar, it is more reasonable to use our model. In fact, this phenomenon was observed in all DL-based models with large capacity, as shown in Fig 7.(b) to (e). The TF1 and TF2 models could mitigate the overfitting problem. However, their MSE and MAPE were significantly higher than those of our model. Furthermore, similar to the first experiment, the proposed model with WPE outperformed the model with PE. There were no significant differences as we increased the number of encoder layers or/and MHA."}, {"title": "D. Performance comparison based on model capacity", "content": "Fig.8 and Fig.9 plot the performance of all the models against the number of parameters for the FD problem and VM problem, respectively. For a more accurate comparison, we conducted experiments by adjusting the number of layers and filters in the DL-based models. In other words, we compared both deeper models and shallower models. We can confirm that as the number of parameters increased, the model's performance became poorer. In particular, the FD model with the smallest capacity showed the best performance. Similarly,"}, {"title": "V. DISCUSSION", "content": "This paper introduce a novel Transformer for the man- ufacturing sensor domain by incorporation of the statistical feature embedding and WPE. Our real data analyses support the effectiveness of the proposed model.\nThe improved performance of our proposed model could be attributed to the following reason. Thanks to the statistical feature embedding, our proposed Transformer is highly effi- cient in terms of parameter usage, requiring significantly fewer parameters than other deep learning architectures, as shown in Fig.8 and Fig.9. The number of trainable parameters in the Transformer depends on the number of word dimensions. In other Transformer-based models, the number of word dimen- sions is the same as the time length or the number of sensors, both of which are usually large. Therefore, when the time length is long or the number of sensor variables is a lot, they require a large number of trainable parameters. In our study, the number of word dimensions is equal to the number of statistical features, which is smaller than the number of sensors and time points. As a result, our model has a smaller capacity and exhibit an excellent performance when the data sample size is small. This efficiency is particularly advantageous in the context of the manufacturing sensor domain, where data samples are naturally limited due to frequent changes in manufacturing processes.\nThe use of statistical features has been adopted in ma- chine learning-based approaches, where these features are extracted and employed as inputs in the models. Our approach aligns with this approach as we utilize statistical features. To comprehend why the use of statistical features shows good performance, it is essential to understand the charac- teristics of time-series sensor data. Fig 10 illustrates specific sensor values of normal and abnormal production. The x- axis represents time and the y-axis represents sensor values. We can see that the discernible differences between normal and abnormal productions are confined to few particular time points. Both normal and abnormal productions reach their maximum values simultaneously at an early time point, with the normal observation exhibiting a higher maximum value compared to the abnormal one. Towards the end, the sensor values for the normal sample drop to near-zero slightly earlier than the abnormal one. Excluding these specific time points, distinguishing between normal and abnormal productions is challenging. That is, the mean values at the first time windows are sufficient to distinguish between these two data categories. Despite the large number of time points at which sensor data is measured, most of the time points are irrelevant for distinguishing between normal and abnormal states or predicting their respective output values. This suggests that predictive models for sensor data do not require all data points, and aggregating sensor data points within a time window using statistics such as mean, max, and variance is sufficient. Our real data analyses' results in Section IV support the effectiveness of our proposed statistical feature embedding by showing good performance compared to the existing approach.\nIn our proposed Transformer, the choice of which statistical features to use should be made by the user prior to training the model, similar to determining hyper parameter values. If this process could be automatically learned by the model, it would be advantageous in improving the model performance further more. One way to view this research question is as follows. Choosing statistical features is likened to selecting statistical pooling layers, as we discussed in Subsection III- A. Consequently, this question can be reformulated as the search for the optimal pooling layers. Exploring this question in the direction of selecting optimal pooling layers would be an intriguing avenue for future research."}, {"title": "VI. CONCLUSION", "content": "With the simultaneous development of manufacturing pro- cess technology and artificial intelligence, a gap exists be- tween these two fields. Various deep learning models have been increasingly developed to be more complex for real- world applications. Consequently, they require vast datasets to demonstrate their performance. However, in the manufacturing industry, where processes rapidly change, it can be challenging to collect a significant amount of data. Additionally, when the process changes, they cannot use machine learning or deep learning models to detect defects or manage various measure- ment values until a sufficient amount of data is collected.\nTo address this issue, our study proposes a novel predictive model based on the Transformer architecture, incorporating statistical feature embedding and WPE. Our study is motivated by the following observations: First, statistical features are highly effective for predicting the target variable in the manu- facturing sensor domain. Second, the Transformer architecture can simultaneously learn both sequential information and sensor data using the proposed statistical feature embedding. Lastly, with the statistical feature embedding, we can signif- icantly reduce the number of parameters to be trained in the model.\nThe proposed predictive model can sequentially learn sta- tistical feature information and has exhibited excellent per- formance in two types of problems with the smallest capacity. The experimental results with actual datasets support the effec- tiveness of the proposed model. Our model can be applied in various manufacturing domains where the amount of available data is limited. Moreover, since our model is based on neural networks, it can be adapted for online learning. As more data are collected, the model can be updated, and performance can be improved using online learning.\nIn future research, it is necessary to identify the optimal statistical features. As previously discussed, the optimal statis- tical features may vary depending on the industry domain. By optimizing the multiple pooling layers, we expect to effectively build the predictive model."}]}