{"title": "Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads", "authors": ["Siqi Kou", "Jiachun Jin", "Chang Liu", "Ye Ma", "Jian Jia", "Quan Chen", "Peng Jiang", "Zhijie Deng"], "abstract": "We introduce Orthus, an autoregressive (AR) transformer\nthat excels in generating images given textual prompts, an-\nswering questions based on visual inputs, and even craft-\ning lengthy image-text interleaved contents. Unlike prior\narts on unified multimodal modeling, Orthus simultane-\nously copes with discrete text tokens and continuous image\nfeatures under the AR modeling principle. The continuous\ntreatment of visual signals minimizes the information loss\nfor both image understanding and generation while the fully\nAR formulation renders the characterization of the correla-\ntion between modalities straightforward. The key mecha-\nnism enabling Orthus to leverage these advantages lies in\nits modality-specific heads-one regular language model-\ning (LM) head predicts discrete text tokens and one diffu-\nsion head generates continuous image features conditioning\non the output of the backbone. We devise an efficient strat-\negy for building Orthus\u2014by substituting the Vector Quan-\ntization (VQ) operation in the existing unified AR model\nwith a soft alternative, introducing a diffusion head, and\ntuning the added modules to reconstruct images, we can\ncreate an Orthus-base model effortlessly (e.g., within mere\n72 A100 GPU hours). Orthus-base can further embrace\npost-training to better model interleaved images and texts.\nEmpirically, Orthus surpasses competing baselines includ-\ning Show-o and Chameleon across standard benchmarks,\nachieving a GenEval score of 0.58 and an MME-P score\nof 1265.8 using 7B parameters. Orthus also shows excep-\ntional mixed-modality generation capabilities, reflecting the\npotential for handling intricate practical generation tasks.", "sections": [{"title": "1. Introduction", "content": "Multimodal models have shown promise in image-to-text\nand/or text-to-image generation, with LLaVA [33, 34],\nEmu2 [51], and NEXT-GPT [64] as popular examples.\nThese abilities are essential for the processing of complex\nreal-world understanding and generation problems. Yet, ex-\nisting approaches can suffer from significant modeling re-\ndundancy due to the trivial combination of specialized large\nmodels (e.g., CLIP-ViT [40], Stable Diffusion [39, 44], and\nLlaMa [56, 57]). Doing so also undermines the benefits\nbrought by cross-modal learning and introduces consider-\nable inefficiency for both training and inference.\nThere is ongoing interest in jointly modeling interleaved\nimage-text with a unified, compact model. One strategy is\nto map both images and texts to discrete tokens for sim-\nple autoregressive (AR) modeling [36, 53, 62] (see left of\nFigure 1). However, the image tokenizer, often equipped\nwith a vector quantization (VQ) bottleneck, can cause in-\nevitable information loss and easily lead to suboptimal per-\nformance on vision tasks concerning high-frequency de-\ntails (e.g., OCR and human face generation). Alterna-\ntively, recent works, including Transfusion [77] and Mono-\nformer [76] (middle of Figure 1), propose to integrate AR\nmodeling on discrete text tokens and diffusion modeling\non continuous image features within a single transformer.\nNonetheless, the nature of diffusion modeling to process\nnoisy images [21] makes the joint modeling of visual gen-\neration and understanding challenging.\nThis paper proposes Orthus\u00b9 to bridge the gap. Or-\nthus conjoins lossless continuous image features and the\nunified, cross-modal AR modeling by decoupling diffusion\nfrom the transformer backbone. This renders the charac-\nterization of the correlation between modalities precise and\nstraightforward. Specifically, Orthus embeds both discrete\ntext tokens (from an off-the-shelf tokenizer) and continuous"}, {"title": "2. Related Work", "content": "Visual understanding. To enable multimodal large lan-\nguage models (MLLMs) to comprehend modalities beyond\ntext, prior work has introduced methods that leverage pre-\ntrained, modality-specific encoders [5, 27, 40, 72] to gener-\nate latent representations for each distinct modality. These\nrepresentations are then projected into a pre-trained LLM's\ninput space through trained adapters, allowing for multi-\nmodal information alignment within the language model,\nunderstanding and reasoning are handled within the trans-\nformer backbone [4, 7, 10, 30, 32, 34, 78]. This framework\nallows LLMs to perform complex multimodal tasks while\nmaintaining the language-based reasoning capabilities in-\nherent to their architecture.\nVisual generation. The generation of visual content has\nlong been a central focus within the deep learning research\ncommunity [17, 23, 24, 58]. Over the past few years, re-\nsearch in visual generation has focused on decomposing vi-\nsual signals in a more sophisticated manner and generating\nthem iteratively. Diffusion models [8, 12, 21, 38, 44, 45, 47]"}, {"title": "3. Preliminary", "content": "Unified multimodal modeling aims to cope with a blend of\nimages and texts with a single compact model [36, 53, 62,\n67, 77]. The model usually includes a vision autoencoder,\nspecified with an encoder E and a decoder D, a text tok-\nenizer, and a transformer network [60]. The encoder E is\nused to map the input image to a sequence of patch-wise\nfeatures V := [v1, ..., Vn], Vi \u2208 Rdv for effective informa-\ntion compression, where d\u2082 is the feature dimension and n\nis the number of patches. The text tokenizer maps the input\ntext into a sequence of text tokens U := [u1,..., Um] with\nm as the sequence length. The transformer is then asked to\nprocess U and V simultaneously to yield meaningful out-\nputs, which can be then detokenized as texts or decoded by\nD to produce images. There are primarily two strategies for\nthe learning of the transformer, detailed as follows.\nFully AR models. Observing that the AR principle ex-\ncels in the generative modeling of discrete content, semi-\nnal works, including LWM [35] and Chameleon [53], pro-\npose to leverage the Vector Quantization (VQ) [59] tech-\nnique to transform the continuous image features V as dis-\ncrete tokens to enable a fully AR modeling of the mixture\nof images and texts. Specifically, VQ introduces a set of K\ncodes {cj \u2208 Rdv}\u2081 and solves the following problem for\ncontinuous-to-discrete transformation:\n$V_i \\triangleq \\arg \\min d(v_i, c_j) \\text{ for } i = 1, ..., n,$ \n$j \\in {1, ..., K}$\nwhere d(,) is a distance metric.\nLet V := [1, ..., \u00een] denote the discrete image tokens.\nThe fully AR model embeds both V and U as de-dim fea-\ntures. Specifically, the embedding corresponding to \u1fe6; is\n$h_i = \\sum w_j 1_{v=j},$\nj\nwhere {wj \u2208 Rde } K }1 refer to the embedding weights. The\nembeddings for text tokens can be similarly gained, yet with\nanother set of embedding weights. The transformer then\nprocesses these embeddings with causal attention, where\nthe output head naturally yields the prediction of the next\ntoken. For training, the objective is simply the AR loss.\nDespite being simple, the fully AR models can suffer\nfrom information loss [31, 53], because VQ makes the\ntransformer unable to directly look at the image features vi.\nAR-diffusion mixed models. Another line of unified mul-\ntimodal models is AR-diffusion mixed models [67, 76, 77],\nwhich integrates diffusion modeling on images [21, 38] and\nAR modeling on text within a shared transformer. Take\nTransfusion [77] for example, its inputs are a noisy ver-\nsion of the image features V, denoted as V, and the text to-\nkens U. To facilitate the simultaneous processing of V and\nU, the attention mask of the transformer adopts a unique\nconfiguration-with a full-attention structure among V and\na causal structure among U. Then, the outputs from V are\ndirected to an output projector to predict the noise on V,\nwhereas the outcomes linked to U are channeled to an LM\nhead for next-token prediction. The training objective is the\ncombination of AR loss and denoising loss with a balanc-\ning factor. During inference, the model operates as an AR\nmodel to generate texts and as a diffusion model to craft\nimages, with special tokens indicating mode switching."}, {"title": "4. Method", "content": "We introduce Orthus to address the issues of existing works.\nThis section begins with an overview of Orthus and then\nelaborates on an efficient training recipe for Orthus. We\nwill also illustrate a post-training pipeline of Orthus."}, {"title": "4.1. Overview of Orthus", "content": "As shown in Figure 2, Orthus directly takes the continuous\nimage features V and discrete text tokens U as input, which\navoids the pathologies caused by the quantized image fea-\ntures V or noisy image features V. U and V are embedded\ninto the de-dim representation space with a differentiable\nvision embedding module (detailed in the next subsection)\nand the aforementioned discrete embedding module respec-\ntively. Subsequently, the embeddings are fed into the trans-\nformer backbone with causal attention for the modeling of\nboth inter- and intra-modality interdependence. Given the\noutput states of such a backbone contain enough informa-\ntion about the multimodal context, Orthus sends them to\ntwo modality-specific heads\u2014a diffusion head and an LM\nhead-to predict the next image patch or the next token.\nSpecifically, let fi denote the output state corresponding\nto the input image feature vi and 60 denote the diffusion\nhead employed by Orthus with parameter 0. The goal of\ne is to predict for the next patch feature vi+1 conditioning\non fi. According to common practice [8, 21], the learning\nobjective for the diffusion head can be formalized as:\n$L_{diff} = E_{\\epsilon, t}[||\\epsilon - \\epsilon_{\\theta}(\\sqrt{\\bar{a}_t}v_{i+1} + \\sqrt{1 - \\bar{a}_t}\\epsilon, t, f_i)||^2],$ \nwhere \u20ac ~ N(0, I) is a Gaussian noise and t is a randomly\nsampled timestep. at follows a pre-defined noise sched-\nule [21]. In practice, e can be a shallow multilayer percep-\ntion (MLP) with three inputs (the condition fi, the scalar\ntimestep t, and the noisy state). On the other hand, the LM\nhead remains the compact linear projection followed by a\nsoftmax transformation to yield the predictive probability\nof the next token over the entire vocabulary."}, {"title": "4.2. An Efficient Strategy for Constructing Orthus", "content": "The differences between Orthus and fully AR models exist\nin the vision embedding module and the output head. Given\nthat pre-training a multimodal model from scratch can be\nfrustratingly costly but the fully AR models like LWM [35]\nand Chameleon [53] are readily accessible from the open-\nsource community, we are naturally interested in deriving\nOrthus based on them at a minimal expense. This section\nelaborates on a hard-to-soft adaptation trick and an efficient\ntraining strategy to enable this.\nDifferentiable vision embedding module. It is easy to\nnote that the embedding yielded by Equations 1 and 2 can be\nequivalently obtained via a softmax-based transformation\n$h_i = \\sum w_j \\frac{e^{-d(v_i, c_j) / \\tau}}{\\sum_{k=1}^{K} e^{-d(v_i, c_k) / \\tau}},$\nj\nwith \u0442 \u2192 0. Increasing gradually from 0 then naturally\nlifts the information bottleneck from the image features vi\nto the model outputs fi, while rendering the reuse of the"}, {"title": "4.3. Multimodal Post-training", "content": "Following the typical training procedure of multimodal\nmodels [61, 67", "BOI": "and [EOI", "SEP": "token is used to separate\nuser input and model output in each conversation.\nLet Lar denote the AR loss on the text tokens. The entire\ntraining objective of Orthus is then Lorthus = Lar + ALdiff,\nwhere A is a balancing coefficient. Hereinafter, we will de-\nnote the model trained following this objective as Orthus,\ndistinguishing it from Orthus-base.\nDuring inference, Orthus alternates between next-token\npredition and next-patch prediction to seamlessly generate\ninterleaved texts and images. When [BOI"}]}