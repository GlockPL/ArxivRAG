{"title": "Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied Air Traffic Agents", "authors": ["Justas Andriu\u0161kevi\u010dius", "Junzi Sun"], "abstract": "Recent developments in language models have created new opportunities in air traffic control studies. The current focus is primarily on text and language-based use cases. However, these language models may offer a higher potential impact in the air traffic control domain, thanks to their ability to interact with air traffic environments in an embodied agent form. They also provide a language-like reasoning capability to explain their decisions, which has been a significant roadblock for the implementation of automatic air traffic control.\nThis paper investigates the application of a language model-based agent with function-calling and learning capabilities to resolve air traffic conflicts without human intervention. The main components of this research are foundational large language models, tools that allow the agent to interact with the simulator, and a new concept, the experience library. An innovative part of this research, the experience library, is a vector database that stores synthesized knowledge that agents have learned from interactions with the simulations and language models.\nTo evaluate the performance of our language model-based agent, both open-source and closed-source models were tested. The results of our study reveal significant differences in performance across various configurations of the language model-based agents. The best-performing configuration was able to solve almost all 120 but one imminent conflict scenarios, including up to four aircraft at the same time. Most importantly, the agents are able to provide human-level text explanations on traffic situations and conflict resolution strategies.", "sections": [{"title": "I. INTRODUCTION", "content": "Air traffic management is a system that is critical for ensuring global airspace safety and operational efficiency. As air traffic volumes increase, so does the complexity of managing numerous flights and workloads for operators simultaneously [1], which raises the risk of incidents due to operational misunderstandings. These factors have historically contributed significantly to aviation accidents.\nOne of the main developments in air traffic management is the introduction of artificial intelligence in air traffic control to reduce the workload of air traffic controllers. The SESAR AISA project [2] was an early attempt to incorporate AI into air traffic management by creating a system for artificial situational awareness through the use of knowledge graphs and machine learning for traffic prediction. The SESAR TAPAS project [3] represented an advancement in the ATM field, targeting explainability. The project tested explainable AI and visual analytics in human-operated simulations that tried to make Al's decision-making processes accessible to controllers. Similarly, another SESAR project, ARTIMATION [4], also aims at producing a transparent AI through visualization.\nOverall, the human-in-the-loop simulations revealed a gap between artificial and human situational awareness, highlighting room for improvement in Al's complex decision-making processes. This gap requires AI to offer more nuanced and human-like reasoning capabilities in air traffic management.\nSince 2023, researchers have experimented with the integration of large language models (LLM) into air traffic management. Large language models are advanced AI systems capable of understanding and generating human-like text. Their proficiency in real-time decision-making has the potential to improve operational efficiency and automate labor-intensive tasks. Most of the data used to train leading-edge large language models, such as the latest Common Crawl dataset [5], which comprises over 250 billion web pages, sources information from publicly accessible internet sites.\nThis extensive training equips large language models with a broad understanding of air traffic management standards, including guidelines from the International Civil Aviation Organisation, Federal Aviation Administration regulations, and other global and local aviation protocols. Consequently, large language models can interpret these contents effectively.\nSeveral recent studies have explored use cases for aviation applications. For example, [6] employs language models to understand ground delay program text data. [7] fine-tunes the open-source language models to better understand the aviation context. A recent study [8] uses a language model for text classification and clustering based on air traffic flow management regulations and weather reports.\nHowever, these use cases are primarily focused on natural language processing; they have not utilized the full potential of language models in managing air traffic operations nor looked into how AI can provide human-like reasoning.\nA new concept, the language model embodied agent, Voyager [9], was introduced last year, which represents an innovative step in leveraging the language model's reasoning capability. It is designed for open interactions within the Minecraft game environment, where Voyager agents can explore the virtual world autonomously and, most importantly, acquire skills by experience and then apply skills.\nIn a similar context, we also hypothesize that large language models may act as intelligent assistants for air traffic control"}, {"title": "II. METHODOLOGY", "content": "In this section, we discuss our efforts to develop two different large language model embodied agent frameworks, which are capable of interacting with the BlueSky simulator [10], monitoring and interpreting traffic situations automatically, and producing instructions to solve air traffic conflicts autonomously and in real-time."}, {"title": "A. Large language model embodied Agent", "content": "A language model predicts the next word in a sequence by analyzing the preceding words. Increasing the complexity of models, like large transformer models, leads to awareness of the extremely long context in text. The text includes programming language and software code. By providing a proper application programming interface, these models can be integrated with various tools and virtual or real environments, transforming them into embodied agents. An embodied agent can either utilize specific tools, such as Python functions with arguments or operate independently to generate responses.\nIn our research, we designed such agents that can interact with the air traffic control interface, for example, the BlueSky simulator. By providing the proper objective in a text (called prompt), the agent is set to solve conflict scenarios.\nFigure 1 shows the overview of the process, beginning with the construction of a prompt that integrates the system prompt, user prompt, and tools descriptions. The large language model then evaluates whether a tool is needed for the task at hand. If a tool is required, the agent executes the selected tool with the specified arguments. For example, to change an aircraft's altitude, the agent would use a SendCommand() tool with a generated altitude command. This command is sent to the simulator, and the output from the simulator is then integrated back into the prompt for further processing. This cycle repeats until the large language model determines that no additional tools are needed.\nIn the end, the agent can also provide a summary of the situation and reasons for the conflict-solving strategies. An experience document (subsection II-D) is then created and subsequently uploaded to an experience library, which can be retried to further enhance the agent's knowledge base and capabilities for more complex tasks.\nTo demonstrate this process, Figure 2 presents a scenario where a single agent effectively resolves a converging three-aircraft conflict. The resolution process begins with the agent querying all relevant aircraft data through the GetAllAircraftInfo() tool. The agent automatically assesses the conflict dynamics between each pair of aircraft using GetConflictInfo(). Based on the results, the agent then strategically issues a heading change to aircraft AB112, directing it to alter its course to 225 degrees. This directive is executed via the SendCommand() tool, utilizing the command HDG AB112 225.\nAfter this initial conflict mitigation, the agent re-evaluates the aircraft and conflict information. It then proceeds to issue another command - this time decreasing the altitude of aircraft AB426 by 2000 feet, further solving the remaining conflict. After re-assessing the situation and confirming the resolution of all potential conflicts, the agent concludes its task, having successfully ensured a safe outcome.\nWe have also developed a multi-agent system capable of handling an unrestricted number of LLM embodied agents and facilitating increasingly complex challenges. This system is illustrated in Figure 3.\nIn this multi-agent system, we designed three types of agents: the planner, the executor, and the verifier. The planner agent is responsible for generating a conflict resolution plan. It begins this process by monitoring the airspace and analyzing detected conflicts. Once a plan is formulated, it is passed onto the executor agent. The sole function of this agent is to issue appropriate commands to BlueSky.\nAfter the execution of the plan, the verifier agent plays a"}, {"title": "B. Prompt", "content": "The prompt serves as a critical link between the objectives, agent actions, and the underlying language model. We have designed a prompt template to ensure the clarity and relevance of the information processed by the large language model, containing four different components:\nsystem_prompt: pre-crafted text on role and objectives\nuser_input: instructions from human\nchat_history: memories about llm inputs and outputs\nagent_scratchpad: memories about environment interactions\nSystem Prompt: This component is crafted to provide both context and explicit instructions to the agent. Each agent receives tailored directives specific to their role. For instance, the planner agent is instructed to gather aircraft information,"}, {"title": "C. Tools", "content": "Our system integrates several specialized tools (functions in Python programming language) to facilitate interactions between the large language model and the BlueSky simulator. These tools are crucial for the effective execution of tasks and data retrieval:\nGetAllAircraftInfo(): This tool sends a command to BlueSky and retrieves a comprehensive list of aircraft, detailing their position, heading, track, altitude, vertical speed, calibrated, true airspeed, and ground speed, as well as Mach number.\nGetConflictInfo(): This tool sends a command to BlueSky and retrieves information about aircraft pairs in conflict. It provides details such as Time to Closest Point of Approach (TCPA), heading differences, separation distances (total, vertical, and horizontal), distance to Closest Point of Approach distance (DCPA), time to Loss of Separation (tLOS), and altitude information.\nContinueMonitoring(duration): This tool commands BlueSky to retrieve changes in conflict status over a specified duration, enabling ongoing monitoring of the airspace.\nSendCommand(command): This tool sends a traffic command to BlueSky and retrieves the resulting output from the simulator, allowing for dynamic interaction with the simulation environment.\nSearchExperienceLibrary(args): This tool queries the experience library and returns the most relevant experience document based on different arguments, including conflict description, number of aircraft involved, and the formation of the conflict.\nIt is important to emphasize that the large language model decides when to utilize a tool, and it is also responsible for generating proper functional arguments that enable precise and context-appropriate responses. This function-calling capability enhances the agent's ability to interact with and manipulate the environment effectively and freely.\nIn principle, it is also possible for the agent to write its own tools, considering that sufficiently large language models are also capable of code generation. However, this was not tested in our experiments."}, {"title": "D. Experience Library", "content": "The Experience Library is a crucial component that enables our LLM embodied agent to recall stored memories about past conflict solution experiences. We use an open vector database, Chroma [11], to store and retrieve past conflict resolutions effectively. A vector database encodes text (a.k.a. tokens) into numerical vectors, which can be compared based on similarities. The agent can create and search the experience library on its own."}, {"title": "D.1 Creation of Experience Documents", "content": "After an LLM agent resolves a conflict, it processes the entire conflict resolution log to create an experience document. A concise conflict description is generated with the language model based on the initial states of the aircraft and the conflict information. It then categorizes the executed commands into whether they are helpful or not helpful.\nCommands that have eliminated at least one conflict pair are deemed helpful, while others are not. The absolute values (like altitudes and headings) of these commands are converted into relative values. The conflict description and the categorized list of commands are then combined. Finally, the language model enhances the document by adding insights and reasoning for each command tailored to the specific conflict description. Additionally, aircraft callsigns are anonymized in the final steps of creating the experience document. This ensures that when an agent retrieves the document later, it won't be confused by the presence of the same callsigns in both the current conflict and the experience document.\nThe conflict description is encoded into a 3072-dimensional vector embedding using the text-embedding-3-large model from OpenAI\u00b9. The embeddings of the experience, along with text and metadata on conflict type and the number of aircraft, are then uploaded to the vector database.\nThe entire experience generation process is illustrated in Figure 4. It is worth noting that we only need to encode the conflict description. This is because when an agent searches the experience library, it can describe the current conflict. Matching conflict descriptions directly yields higher similarity and the most relevant results than when comparing the full document with commands, suggestions, and insights."}, {"title": "D.2 Experience Library Search", "content": "When an agent wants to retrieve the closest memory from past experiences before solving the conflicts, it invokes the SearchExperienceLibrary() tool (shown in Figure 5). The agent first generates a concise description of the current conflict, including the number of aircraft involved and the type of conflict. The initial metadata filtering reduces the search space in terms of aircraft formation and number of aircraft. The conflict description is also encoded as a 3072-dimensional vector with the embedding model."}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "In this section, we describe the experimental setup and the results of various agent configurations under many simulated conflict scenarios. The performances of different agent models with and without access to the Experience Library are explored. Our experiments are structured to assess the effectiveness of addressing a range of increasingly complex scenarios."}, {"title": "A. Initial tests", "content": "An initial experiment was conducted with a small dataset containing 12 conflict scenarios, which included four types of conflicts (head-on, parallel, t-formation, converging) with three different aircraft numbers each (2, 3, and 4 aircraft). We first tested a single agent configuration without experience library with the following models: Llama3:7B, Llama3:70B, Mixtral 8x7b, gemma2:9b-it and GPT-40 .\nWe also evaluated different range of temperatures: 0.0, 0.3, 0.6, 0.9,and 1.2. The temperature determines how conservative a language model predicts the next token. The higher the temperature, the more creative the agents become.\nThis initial experiment was designed to identify the most promising models based on a limited set of scenarios. The tests narrow down the number of models to focus on for later more extensive testing. We score the effectiveness of the setting based on the criteria in Table I."}, {"title": "B. Generating large conflict scenarios", "content": "To assess the performance of the Llama3:70B and GPT-40 models in solving air traffic conflicts, we generated a dataset comprised of 120 distinct conflict scenarios for BlueSky.\nThe dataset contains 40 scenarios, each with two, three, or four aircraft conflicts. The conflicts are categorized into four primary types: 1) head-on, where aircraft are on a direct collision course; 2) T-formation, which involves perpendicular flight paths; 3) parallel, where aircraft fly close parallel courses; and 4) converging, where multiple aircraft are on intersecting paths heading towards the same point. There are 30 conflict scenarios in these four types.\nIn addition to conflict type, we also consider changes in flight levels. Some scenarios have all aircraft at the same level, while others involve climbing, descending, and level flights, adding further complexity to the conflict dynamics. Examples are shown in Figure 6.\nAll scenarios are designed under the assumption that, without timely intervention, the aircraft involved will inevitably collide. This design ensures that each scenario presents a genuine challenge that tests the models' abilities to effectively navigate and resolve potential airborne conflicts in high-risk situations.\nIt is also worth noting that all these scenarios present imminent conflicts with very short response time. They are incredibly challenging for human operators, especially when involving more than two aircraft."}, {"title": "C. Results", "content": "These conflict scenarios are tested with single-agent and multiple-agent configurations using different language models. Figure 7 shows the success rates across different agent configurations for GPT-40 and Llama3:70B models. We also test their performance when they have access to the SearchExperimentLibrary() tool.\nFor single-agent setup, we can see that GPT-40 performs better than Llama3:70B. And by including experience libraries, significant improvements are observed. For multiple-agent setup, the success rates are all high, even for the open-source Llama3:70B with a significantly smaller model size.\nTable II shows the exact number of times the conflicts resulted in the collision, loss of separation (LoS), and conflict resolved. We observe that the best result is achieved by the"}, {"title": "IV. DISCUSSIONS", "content": "A. General observation on performance\nThe single-agent setup with Llama3:70B model demonstrated the weakest performance. This outcome can be related to the smaller model size and context window compared to GPT-40. However, its performance can be significantly improved when utilizing the experience library, with its success rate improved from 52% to 76%. This suggests that an agent with a smaller LLM can make use of knowledge from the experience library to significantly enhance its problem-solving efficiency.\nMoreover, the smaller Llama3:70B model achieved the most optimal performance in a multi-agent configuration. In this setup, the distributed processing load allows each agent to handle less information. This is especially beneficial given the smaller context window of only 8,000 tokens available to Llama3:70B, allowing more efficient information processing and decision-making across multiple agents.\nGPT-40 had a high success rate across all the agent configurations. The single-agent configuration without experience can already achieve a success rate of 93%. With the experience library, it only had a single unresolved conflict, arriving at a 99% success rate. Multiple-agent setups with and without experience demonstrated similarly high performances, suggesting that the LLM's size and larger context window play a more significant role than the model architecture."}, {"title": "B. Optimizing the use of experience library", "content": "Throughout the development of the experience library, we have been constantly adapting the experience library architecture. Initially, the library included complete experience documents, but it was discovered that by embedding only the conflict description, we can enhance search accuracy and relevance. This change ensured that search results were more applicable to current conflicts.\nWe noticed more performance issues with the Llama3:70B when summarizing the experience, particularly its tendency to produce inaccurate content in experience documents. Again, this is due to the smaller size of the model. To maintain a consistent quality of the library's content, we only retain experience documents created by GPT-40.\nSometimes, the experience retrieval may encounter problems, especially with incorrect metadata input, e.g., the number of aircraft in conflict. For example, Llama3:70B often incorrectly searched for two-aircraft conflict solutions when dealing with four-aircraft scenarios. We can solve this issue by using better system prompts to refine the search."}, {"title": "C. Limitations due to language model hallucination", "content": "There are several reasons why the aircraft conflict is not resolved, and each agent configuration has its own reasons. Starting with the single Llama3:70B agent, the main reason is the model's size, which affects reasoning capabilities.\nMany times, an agent sends an altitude command, which will not ensure enough vertical separation even though there are instructions for vertical separation in the system prompt. An example below shows how FLIGHT3 is instructed to descend to 22800 feet, which would reduce the vertical separation between the rest of the aircraft.\nAlthough the experience library contributes significantly to reducing errors, it is particularly effective at the beginning of conflict resolution. Initially, the agent applies the library's suggestions directly to its current conflict. However, if conflicts between aircraft pairs persist, the agent's limitations in reasoning may become apparent again. Another current limitation often observed in agents is their failure to anticipate secondary conflicts caused by resolving initial ones. In the example"}, {"title": "D. Complexity of the traffic", "content": "Looking at Figure 8, it is evident that as the number of aircraft involved in a conflict increases, the success rate for resolving these conflicts declines. This outcome is expected as the more significant number of aircraft introduces more information that the large language model must process, which in turn impacts its performance.\nNotably, the GPT-40 agent configurations maintain similar performance levels when dealing with conflicts involving two or three aircraft. There is a slight decrease in performance when the number of aircraft increases to four. For Llama3:70B,the performance is similar in multi-agent setups."}, {"title": "E. Computing resource constraints", "content": "Our testing capabilities were significantly influenced by accessing computing resources, particularly in the context of model hosting and processing power.\nAll models, with the exception of GPT-40, were hosted on a cloud platform called Groq. Groq utilizes a specialized processing unit known as the Language Processing Unit, which can deliver around 1000 tokens per second, making it an optimal choice for our needs. However, Groq also imposes token-per-minute restrictions. This restriction prevented us from running multiple models and conflict scenarios in parallel, thus reducing the number of scenarios we could test.\nWe have also tried to set our own Llama3:70B model with Ollama and using TU Delft DelftBlue cluster [13], where NVIDIA A100 GPUs are available. However, the inference speed in the high-performance computing environment is too slow for our use cases."}, {"title": "V. CONCLUSION", "content": "This study explored the application of large language models as embodied agents in air traffic control scenarios, focusing on their ability to autonomously resolve conflicts.\nOur experiments with both open and closed-source models such as Llama3:70B and GPT-40 demonstrate the huge of large language models embodied agents in performing air traffic control tasks. This new approach could reduce the gap between artificial and human situational awareness. We have demonstrated that it provides human-like reasoning with timely control instructions or recommendations.\nThe findings highlight that larger models outperform smaller models in complex conflict resolution scenarios. The incorporation of an experience library further aids in boosting efficiency by providing access to past conflict resolution insights, which is particularly beneficial for smaller models like Llama3:70B.\nMoreover, the study has shown that multi-agent systems, where tasks are distributed among specialized agents, yield high success rates in resolving conflicts as well. This research paves the way for new research paths to apply language model-embodied agents in more complex tasks for air traffic management."}]}