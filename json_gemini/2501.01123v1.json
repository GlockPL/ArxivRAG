{"title": "TED: Turn Emphasis with Dialogue Feature Attention\nfor Emotion Recognition in Conversation", "authors": ["Junya Ono", "Hiromi Wakaki"], "abstract": "Emotion recognition in conversation (ERC) has\nbeen attracting attention by methods for model-\ning multi-turn contexts. The multi-turn input to\na pretraining model implicitly assumes that the\ncurrent turn and other turns are distinguished\nduring the training process by inserting spe-\ncial tokens into the input sequence. This paper\nproposes a priority-based attention method to\ndistinguish each turn explicitly by adding di-\nalogue features into the attention mechanism,\ncalled Turn Emphasis with Dialogue (TED). It\nhas a priority for each turn according to turn po-\nsition and speaker information as dialogue fea-\ntures. It takes multi-head self-attention between\nturn-based vectors for multi-turn input and ad-\njusts attention scores with the dialogue features.\nWe evaluate TED on four typical benchmarks.\nThe experimental results demonstrate that TED\nhas high overall performance in all datasets\nand achieves state-of-the-art performance on\nIEMOCAP with numerous turns.", "sections": [{"title": "1 Introduction", "content": "Emotion recognition in conversation (ERC) has\nbeen discussed for over decade (Yang et al., 2007;\nDevlin et al., 2019). In ERC, an emotion label is\nestimated for the current turn from multiple turns\u2019\nworth of utterances and speaker information. Emo-\ntion understanding such as ERC has the potential\nto be used in chatbots, medical situations, and call\ncenters.\nThe methods of ERC are often used on the basis\nof past and future contexts, external commonsense\nknowledge, and speaker information (Majumder\net al., 2019; Ghosal et al., 2020; Zhu et al., 2021;\nShen et al., 2021). Especially, studies that involve\nmodeling multiple turns have grown in number\nsince the appearance of ERC datasets that embody\ndifferent perspectives including multiple speakers\nand topics (Li et al., 2017; Zahiri and Choi, 2017;\nPoria et al., 2018).\nThe mainstream multi-turn methods can exploit\npast and future turns for more contexts through\ntheir use of recurrent neural networks (RNNs),\ngraph neural networks (GNNs), and Transformer\n(Vaswani et al., 2017). These methods often input\nthe token sequence for only the current turn into\na pretrained model, such as BERT (Devlin et al.,\n2019) and RoBERTa (Liu et al., 2019). They can-\nnot use the context other than the current turn for\nsingle-turn input. Therefore, the methods of multi-\nturn input have been recently proposed by inserting\nspecial tokens into the input sequence as shown\nin Figure 1 (Li et al., 2020a; Gu et al., 2020; Lee\nand Choi, 2021; Kim and Vossen, 2021). These\nmethods can obtain deeper contexts by adding the\nutterances in multiple turns into the input sequence.\nOn the other hand, they design to distinguish each\nturn by including information of turn position and\nspeaker as special tokens. However, this multi-\nturn input implicitly expects to be distinguished\nbetween the current turn and other turns to be mod-\neled in the process of machine learning.\nWe introduce a concept to distinguish explicitly\nbetween each turn and to control a degree of the"}, {"title": "2 Related work", "content": "ERC has recently attracted attention because it\ncan handle more complex contexts using multi-\nple turns. Numerous powerful approaches have\nbeen proposed on the basis of RNNs, GNNs, and\nTransformers. Many use a pretrained model, such\nas BERT and RoBERTa, to make sequence repre-\nsentations corresponding to input tokens.\nHere, we review recent models based on three dif-\nferent neural networks.\nRNN-based models\nICON (Hazarika et al.,\n2018a) and CMN (Hazarika et al., 2018b) have\ngated recurrent unit (GRU) and memory networks.\nHiGRU (Jiao et al., 2019) has two GRUs for\nthe utterance and conversation. BiF-AGRU (Jiao\net al., 2020) has a hierarchical memory network\nwith an attention GRU for historical utterances.\nDialogueRNN (Majumder et al., 2019) utilizes\nthe global state as a context and the party state\nfor individual speakers by incorporating bidirec-\ntional GRUs for emotional dynamics. COSMIC\n(Ghosal et al., 2020) has a similar structure to Dia-\nlogueRNN but with added external commonsense\nknowledge. BiERU (Li et al., 2020b) devised an\nefficient and party-ignorant framework by using\na bi-recurrent unit. CESTa (Wang et al., 2020)\nhandles the global context by using Transformer\nand individual speakers by using BiLSTM-CRF.\nDialogueCRN (Hu et al., 2021) has contextual rea-\nsoning networks that have long short-term memory\n(LSTM) to understand situations and speaker con-\ntext.\nGNN-based models DialogGCN (Ghosal et al.,\n2019) handles the dependency and positional rela-\ntionship of speakers as a graph structure. RGAT\n(Ishiwatari et al., 2020) has a similar strategy to\nDialogGCN but with added positional encodings.\nConGCN (Zhang et al., 2019) builds an entire\ndataset including utterances and speakers as a large\ngraph. SumAggGIN (Sheng et al., 2020) has two\nstages of summarization and aggregation graphs\nfor capturing emotional fluctuation. DAG-ERC\n(Shen et al., 2021) models the flow between long\ndistance and nearby contexts. TUCORE-GCN (Lee\nand Choi, 2021) constructs a graph of relational in-\nformation in a dialogue with four types of nodes\nand three types of edges."}, {"title": "3 Methodology", "content": "The following three subsections describe TED as\na classification problem using a pretrained model\nand multi-turn contexts.\nHere, we introduce the turn-based encoding model\n(TBE), which has two components, i.e. input en-\ncoding and creation of the mean vector of utterance\nto encode multi-turn utterances. Figure 3 shows\nthe structure of TBE.\nFirst, TBE creates token-based\nvectors by concatenating multi-turn utterances in-\ncluding past and future turns with special tokens\n(called \"Concat Utterance with Special Token\u201d, or\nCUST). As indicated in Figure 3, CUST outputs a\ntoken sequence including past and future turns as\nfollows:\n$\\text{CUST} := \\text{Concat} (x_c, x_p, X^P, X^f, t_r, t_s)$", "3.1 Turn-Based Encoding": ""}, {"title": "3.2\nTurn-Based Multi-Head Self-Attention", "content": "We use the multi-head self-attention (MHSA) of\nTransformer (Vaswani et al., 2017) between turns\nto obtain more context based on TBE. Figure 4\nshows our turn-based MHSA (TBM) model. The\noutput of MHSA is added to the original input;\nthen, the layer norm is taken in the same way as in\nthe vanilla Transformer.\nTUCORE-GCN uses MHSA masked with a turn-\nbased window size and performs attention on a\ntoken-by-token basis. HiTrans also establishes\ntoken-based MHSA between CLS tokens, which\nis added to the input at the beginning of each turn.\nOur model differs from TUCORE-GCN in that it\nis MHSA with turn-based vectors of positions that\nexactly match the tokens of utterances."}, {"title": "3.3 Turn Emphasis with Dialogue (TED)", "content": "We propose \"Turn Emphasis with Dialogue (TED)\u201d\nto emphasis the current turn with two dialogue fea-\ntures, called \"priority\" and \"speaker\u201d, as shown\nFigure 2. We add a dialogue layer to TBM to ad-\njust the attention scores with the turn priority and\nspeaker information, as shown in Figure 5.\nThe multi-head self-attention in Figure 5 has the\nsame structure as the vanilla Transformer. $\\tilde{a_i}$ is the\nattention score matrix for all m turns at the jth head\nof MHSA as follows:\n$\\tilde{a_j} = (\\tilde{a_j}^1,...,\\tilde{a_j}^t,...,\\tilde{a_j}^m)$", "3.1 Turn-Based Encoding": ""}, {"title": "4 Experiment setting", "content": "We evaluate TED on four ERC datasets. The\ndatasets include speaker IDs for every turn. The\ntraining, development, and test data split follow\nthe related work, such as COSMIC (Ghosal et al.,\n2020).\nTable 1 shows the statistics of the datasets. IEMO-\nCAP (Busso et al., 2008) is a multimodal dataset\nincluding text transcriptions of two speakers and it\nis annotated with six emotions.\nMELD (Poria et al., 2018) is a multimodal dataset\ncreated from a TV show, Friends. It includes 260\nspeakers and is labeled with seven emotions.\nEmoryNLP (Zahiri and Choi, 2017) uses the same\ndata source as MELD, i.e., the TV show Friends,\nand it annotates different utterances as compared\nwith MELD by seven emotions. The utterances are\nfrom 225 speakers.\nDailyDialog (Li et al., 2017) contains utterances\nof two speakers communicating on various topics\nrelated to daily life. It is annotated with seven\nemotions.\nIEMOCAP is different from the other datasets\nin that its standard deviation (Std.) of the number\nof turns in one dialogue is 16.8, while the Std. of\nthe others range from 3.99-5.79. This means it\nis possible to use more surrounding contexts for\nIEMOCAP. Regarding the speaker IDs of MELD", "4.1 Dataset": ""}, {"title": "5 Results and Analysis", "content": "Table 2 compares the performance of TED and the\nother latest models listed in Section 2. TED had\ngood overall performance, while the other models\nhad good performance on certain datasets. More-\nover, TED had an advantage in terms of the dia-\nlogue features on datasets with a lot of turns, such\nas IEMOCAP.\nIn Formula (7), the attention score vector is a\nfunction of\n\u2022\n\u2022 all turns, turns that have the same speaker as\nin the current turn, or listener turn.\na decay factor determined by turn priority\n(No decay (i.e., Constant) or a Normal\ndistribution centered on a current turn (i.e.,\nNormdist).\nIn terms of the context turn in CUST, we target\nthe past only or both the past and the future. We de-\nscribe the above combination of parameters of the\nresults in Table 2. The IEMOCAP column in Table\n2 shows results for past, Listener, and Normdist;\nthe MELD column those for past, Listener, and\nConstant; the EmoryNLP column those for both\nthe past and the future, All turns, and Normdist,\nand the DailyDialog those for both the past and the\nfuture, Listener, and Constant.\nThe results suggest that TED strengthens the\nrelationships between turns and accelerates the turn\nemphasis by specially treating the current turn.", "5.1 Overall Performance": ""}, {"title": "5.2 Detailed Performance", "content": "Here, we examine the results of TED in more detail\nby analyzing the effect of different parameters in"}, {"title": "5.3 Effect of Input Encoding and Mean\nVector of Utterance (TBE)", "content": "Table 4 compares the TBE models that have only\ninput encoding part or both the input encoding part\nand mean vector of utterance part with a baseline\nthat does not have either part. The \u201cContext turn\"\ncolumn indicates whether to include past, future,\nor both kinds of turn in CUST; \u201cno use\u201c indicates\nonly the current turn for the baseline.\nThe results show the effectiveness of the past\nturns and TBE. The future turns do not contribute\nto a performance improvement. We consider that\nthe current state is influenced by the past states in a\nchain reaction, while the current state is not directly\naffected by the future states.\nThe performance on IEMOCAP is significantly\nimproved in the case of past turns. The perfor-\nmance of EmoBERTa shows the same tendency as\nTBE on IE\u041c\u041e\u0421\u0410\u0420.\nThese results suggest that the turn-based vector\nof TBE is effective."}, {"title": "5.4 Comparison with related work using\nspeaker information", "content": "TBM performs MHSA on turn-based vectors and\ndoes not use speaker information, as stated in Sec-\ntion 3.2. Herein, we applied speaker information\nto TBM in two ways.\nThe first way involved adding special tokens\ncorresponding to speaker IDs to the input sequence,\nas is done in TUCORE-GCN. For instance, when\nthe speaker IDs are 0 and 1, the speaker token\nbecomes [SPK0] or [SPK1], respectively. Note\nthat while EmoBERTa uses real names as speaker\nIDs, we added speaker tokens to use the same IDs\nas the original data as accurate as possible.\nThe other way was to attend only to the turns of\nthe target speakers. This is called local attention,\nand it was originally applied to an encoder-decoder\n(Luong et al., 2015).\nIn Table 5, the \"Speaker token\" column shows re-\nsults for speaker IDs, while the \"Speaker attention\"\nand \"Listener attention\" columns show results for\nthe attention on turns with the same speaker as the\ncurrent one and on listener turns. The performance\nof TBM was significantly higher than that of TBE.\nHowever, neither way of adding speaker informa-\ntion showed any advantage. On the contrary, the\naddition of speaker tokens caused a noticeable loss\nin performance.\nThe results also show that it is better to attend\nto all turns than to limit the attention to certain\nturns. Moreover, DialogXL reported that global\nattention contributed more than speaker or listener\nattention. Accordingly, the results suggests that\nthe high performance of TBM is based on global\nattention."}, {"title": "5.5 Variants of TBM Structure", "content": "The MHSA of TBM is the same as that of the\nvanilla Transformer. Because Transformer-based"}, {"title": "6 Conclusion", "content": "For modeling multi-turn contexts of conversation,\nwe presented TED, which emphasizes the cur-\nrent turn and explicitly distinguishes each turn\nby adding the dialogue features into the attention\nmechanism. The results in four ERC datasets show\nthat TED had good overall performance, while\nthe other models had good performance on cer-\ntain datasets. Moreover, TED had an advantage in\nterms of the dialogue features on datasets with a lot\nof turns, such as IEMOCAP. Further experiments\ndemonstrated the effectiveness of TED's key com-\nponents; TBE, TBM, and the dialogue features of\nthe turn position and speaker information. Our pri-\nority factor for the distinction of the turns has made\nit possible to emphasize the emotional target turn\nand can adapt to diverse datasets by controlling the\ndialogue features."}, {"title": "A Attention score (supplementary)", "content": "The attention score $a_j^t$ of Formula (5) is as follows:\n$\\alpha_j^t = \\frac{Q_j^t K^t}{\\sqrt{d_k}}$\nwhere $Q_j^t = H^t W_j^Q , K^t = H W^K$\nwhere $d_k$ denotes the head dimension to correct\nthe inner product. It is obtained by dividing the\nmodel dimension by the number of J heads. $W_j^Q$\nand $W^K$ denote learning parameters for the query\nand key process, respectively. $Q_j^t$ and $K^t$ take a\nlinear transformation to $d_k$ from the turn-based\nvector at the $t^{th}$ turn for the query process and the\nlist of the scaled vectors for the key process in\nall turns, respectively. The superscript T denotes\ntranspose."}, {"title": null}]}