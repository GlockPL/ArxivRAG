{"title": "Non-Halting Queries: Exploiting Fixed Points in LLMs", "authors": ["Ghaith Hammouri", "Kemal Derya", "Berk Sunar"], "abstract": "We introduce a new vulnerability that exploits fixed points in autoregressive models and use it to craft queries that never halt, i.e. an LLM output that does not terminate. More precisely, for what we call non-halting queries, the LLM never samples the end-of-string token <eos>. We rigorously analyze the conditions under which the non-halting anomaly presents itself. In particular, at temperature zero, we prove that if a repeating (cyclic) sequence of tokens is observed at the output beyond the context size, then the LLM does not halt.\nWe demonstrate the non-halting anomaly in a number of experiments performed in base (unaligned) models where repeating tokens immediately lead to a non-halting cyclic behavior as predicted by the analysis. Further, we develop a simple recipe that takes the same fixed points observed in the base model and creates a prompt structure to target aligned models. We study the recipe behavior in bypassing alignment in a number of LLMs including gpt-40, llama-3-8b-instruct and gemma-2-9b-it where all models are forced into a non-halting state. Further, we demonstrate the recipe's success in sending most major models released over the past year into a non-halting state with the same simple prompt even at higher temperatures. Further, we study direct inversion based techniques to craft new short prompts to induce the non-halting state. Our experiments with the gradient search based inversion technique ARCA show that non-halting is prevalent across models and may be easily induced with a few input tokens.\nWhile its impact on the reliability of hosted systems can be mitigated by configuring a hard maximum token limit in the sampler, the non-halting anomaly still manages to break alignment. This underlines the need for further studies and stronger forms of alignment against non-halting anomalies.", "sections": [{"title": "1 Introduction", "content": "Since their emergence, Large Language Models (LLMs) have been shown to be vulnerable to an array of attacks. For instance, by using carefully crafted prompts, LLMs may be tricked into revealing proprietary information such as training data or application prompts (prompt injection) [1], or to bypass safety filters (jailbreaking) [2, 3, 4, 5]. In general, attackers may use prompt injection to extract prompts used by the target application [6], to extract memorized training data [7, 8], to redirect the prompt [6, 9], or to control the output of the query [10]. To this end [8, 10] advocate adversarial alignment, i.e. fine-tuning the model with malicious adversaries in mind. The goal of alignment is for the LLM-generated content to align with human values i.e. to be helpful, truthful, and harmless [11]. Misaligned LLMs may fail to follow user's instructions, may not be able to carry on conversations or respond to queries, may make up \u201cfacts\u201d, or generate harmful content.\nLLMs are also susceptible to Denial of Service (DoS) attacks which were designated as one of the top 10 security risks for LLMs\u00b9. An example of DoS attacks on LLM is [13], where specific inputs are designed to maximize the energy consumption and the latency during inference. More traditional examples include attacks that overwhelm the LLM by sending a large volume of queries that are longer (or just below) the context window size to trigger a large number of costly web requests. While Denial of Service (DoS) attacks are well recognized as a potentially serious threat, they have yet to be fully explored in the domain of LLMs.\nThe Non-halting Anomaly We introduce a new vulnerability that exploits fixed points in autoregressive models to craft queries that never halt. More precisely, for what we call non-halting queries the LLM never samples the end-of-string token <eos>. Unlike traditional DoS attacks that require sustained efforts to overwhelm a victim, the non-halting vulnerability requires minimal effort, i.e. a single well crafted query can force the victim LLM into a non-halting state. Note that the prompt does not directly ask the LLM to produce an infinite repeating sequence. Aligned LLMs are already hardened against such questionable requests.\nWe first encountered the non-halting anomaly in Retrieval Augmented Generation (RAG) systems [14]. While examining the RAG system behavior against randomly formed non-sensical queries, we observed a non-halting behavior where the LLM was stuck outputting the same cycle of tokens. We rigorously analyzed the conditions under which the non-halting anomaly presented itself and isolated the behavior to a general setting not specific to a RAG system. In particular, we prove that at temperature zero, if a repeating (cyclic) sequence of tokens is observed at the output beyond the context size, then the LLM will never halt. The analysis we present shows this vulnerability to be theoretically possible. We performed a number of experiments in base models where we observe cyclic outputs that extend beyond the context window which immediately leads to a non-halting cyclic behavior as predicted by the analysis. Of course, base language models are not meant to be used directly. They are fine-tuned to produce output as expected by humans through the process of alignment, typically to teach the model to follow instructions (instruct) or to engage in conversation"}, {"title": "2 Related Work", "content": "One of the active areas in AI security is that of watermarking where a scheme is developed to actively embed identifiable markers within the AI text output. These markers help attribute generated content to specific models. LLM watermarking schemes are crucial for copyright protection, content authenticity, and preventing misuse of AI-generated content. The work in [15] demonstrated that watermarks can be inserted in the output text if there is sufficient entropy by modifying the output distribution. Inspired by cryptographic primitives, [16] introduced a watermark that is only detectable by the secret key holder without altering the LLM output distribution.\nOn the AI attacks front, the work in [8] shows how an adversary can extract gigabytes of training data from open-source/semi-open models like LLaMA or Falcon, and even closed models like ChatGPT. The authors also introduce a divergence attack that is able to circumvent alignment to emit data at much higher rates than previously thought. Most interestingly, the authors note that their data emission might be enabled by repeated token sequences collectively emulating the beginning of sequence token. The work in [17] proposed an efficient gradient-based optimization method to manipulate discrete text structure at its one-hot representation. A more improved gradient ascent based discrete optimization algorithm is introduced in [18] that jointly optimizes over inputs and outputs. This technique is used to build an auditing tool that may be used to scan models before deployment e.g. to uncover derogatory completions about celebrities, to produce French inputs that complete English outputs, and find inputs that generate a specific name. Motivated by limitations of earlier manually crafted jailbreaking attacks, [2] introduces a technique for automatically producing malicious prompts using adversarial suffixes. Their approach works by applying a combination of greedy and gradient-based search techniques improving on earlier automated approaches. Another important consequence is that adversarial prompts generated by this approach are highly transferable even to publicly released, closed production LLMs.\nA more recent work [10] introduces a prompt injection countermeasure that leverages task-specific fine-tuning. They first use a base model which is called the Teacher to generate input/output pairs designed for specific tasks. Since the inputs are non-malicious neither are the outputs. Then this golden dataset is used to fine-tune the target model. The technique leverages the fact that LLMs can only follow instructions once they have undergone instruction tuning. The approach significantly reduces the prompt injection success rate while retaining the output quality."}, {"title": "3 Formal Analysis", "content": "In this section, we explore the root cause that enables fixed points to naturally occur in language models. We establish the necessary conditions for a non-halting generative model. Specifically, what we need from the theory is to tell us once we observe a repeating (cyclic) output sequence how far we have to sample the LLM output to be certain a non-halting state is achieved. In other words, we want to be able to recognize non-halting cyclic anomalies from empirical data."}, {"title": "3.1 Definitions", "content": "We start by presenting the following two definitions that capture a high-level mathematical abstraction of a language model. These definitions have been previously introduced by [16]"}, {"title": "Definition 1.", "content": "A language model M is a deterministic probability distribution generator expressed as M : T* \u2192 D, where T is a set of tokens, and D is the set of probability distributions over T. For any prompt q \u2208 T*, we write D = M(q) where D \u2208 D.\nIn order to generate a linguistic output from the language model we have to run it through a sampling function."}, {"title": "Definition 2.", "content": "(Sampled) A sampled language model's output to an input-prompt q is a random variable x = M(q) \u2208 T* that is defined algorithmically as follows. A sampler S begins with an empty list of tokens x = (), as long as the last token in x is not a special token < eos > which halts the sampler, it samples a token xi from the distribution D\u2081 := M(q, x) and appends xi to x, starting at i = 1. This allows us to set M(q) = S(M(q)) = x.\nIn a sampled setting, the output x depends on the sampler S. Typically, S is assigned a temperature variable \u03c4\u2208 [0,\u221e] which determines the entropy in the sampling. For t = 0, the output becomes deterministic by sampling the token with the highest probability. On the other hand, if \u03c4 = 1 the output enjoys the full entropy of the distribution Di. In the extreme case of t = \u221e, the distribution becomes uniform overall tokens. In essence, the temperature is a dial that smooths the probability distribution with a larger standard deviation or sharpens it with a smaller standard deviation. It can assume any value in the positive reals, however, typically \u03c4\u2208 (0,2]. Here we state the following fact regarding sampling at temperature T = 0."}, {"title": "Fact 1.", "content": "The output of a sampled language model M becomes deterministic when sampled at temperature \u0442 = 0.\nIn general, the sampler can be assigned a number of hyper-parameters that can impact the behavior of the sampler. In this work, we will focus on the general conditions that lead the language model to behave in a specific way. Our analysis will mainly discuss the effects of changing the temperature under certain conditions."}, {"title": "Definition 3.", "content": "(Cyclic Anomaly) For q \u2208 T*, \u03c4\u2208 [0, \u221e], and l > b + c for some b, c, l \u2208 Z+, we say that input q is a (b,c,l) cyclic-anomaly for model M at temperature \u0442, if, for any i such that l > i > b + c where xi = M(q, x1,...,xi\u22121) is sampled by sampler S at temperature \u0442, the following is true:\nXi = x; where j = (i-b-1 mod c) + 1 + b.\nThat is, for xb \u2208 T\u266d and x \u2208 T\u00a3 where xb = x\u2081,...,xb and x = x\u2081,...,x, we have:\nx:= (X1... xe) = (xb, X, ..., X, X\u2081, ..., X),\nwhere l = b + r. c + j for j < c and r \u2208 Z+. Here we say, b is the size of the anomaly's beginning, c is the cycle size, r is the number of cycle repetitions, and l is the number of the last generated token."}, {"title": "Definition 4.", "content": "A w-context language model Mw is a language model where the maximum input size is w-tokens. When sampled by a sampler S, the language model only inputs the most recent w tokens of (q, x), thus for i > w, xi = Mw(Xi-w, ..., Xi\u22121) = S(Mw(Xi-w, ..., Xi\u22121)).\nCurrent language models all come with the restriction of a finite context size. In fact, the race to increase the context size is a major area of research and development. The current models have contexts of size w somewhere between thousands to a million tokens. Currently, there is a race of sorts to achieve a trillion token context."}, {"title": "3.2 Non-Halting Anomalies", "content": "Here we will use the notation Z[a, b] to define the integer range from a to b. We start by defining the concept of a non-halting cyclic anomaly."}, {"title": "Definition 5", "content": "(Non-Halting Cyclic-Anomaly). For q \u2208 T*, we say that q is a (b,c) non-halting cyclic anomaly for model Mw at temperature \u315c, if, \u2203l\u2217 such that q is a (b, c, l) cyclic anomaly for model Mw at temperature r, Vl \u2208 Z[l*, \u221e]."}, {"title": "Proposition 1.", "content": "Let q be a (b, c, l) cyclic-anomaly for model Mw at temperature \u0442 = 0, then q is a (b, c, l') cyclic-anomaly for model Mw at temperature \u0442 = 0 where l' \u2208 Z[b + c + 1, l]."}, {"title": "Proof.", "content": "At temperature \u0442 = 0, Mw is a deterministic function. Thus, q has the same output at every length l' of a running sampling algorithm Sat\u0442 = 0. As per Definition 3, a cyclic anomaly is observed at l' > b + c, and since q is a cyclic anomaly at l', the cyclic behavior is observed by every l' \u2208 Z[b + c + 1, l]."}, {"title": "Proposition 2.", "content": "Let q be a (b,c) non-halting cyclic-anomaly for model Mw at temperature \u03c4 = 0, then q is (b,c,l') cyclic-anomaly for l' \u2208 Z[b + c + 1, \u221e]."}, {"title": "Proof.", "content": "Since q is a (b, c) non-halting cyclic-anomaly, then \u2203l\u2217 such that q is a (b, c, l') cyclic anomaly for model Mw at temperature \u03c4 = 0, \u2200l' \u2208 Z[l*, \u221e]. And according to Proposition 1, q is a (b, c, l') cyclic anomaly for M\u300f at \u03c4 = 0 where l' \u2208 Z[b + c + 1, l*] thus proving the claim."}, {"title": "Lemma 1.", "content": "Let q \u2208 T* be a (b, c, l) cyclic-anomaly for model Mw at temperature \u0442 = 0, and let l* := w + b + c. If l = l*, then, q is a non-halting cyclic anomaly."}, {"title": "Lemma 2.", "content": "If q \u2208 T* is a non-halting cyclic anomaly for model Mw at temperature \u0442 = 0, then q is a (b, c, l\u2217) cyclic-anomaly for model Mw at temperature \u0442 = 0 where l* = b + c + w."}, {"title": "Proof.", "content": "Since q is a non-halting cyclic anomaly, by Proposition 2, it must also be a cyclic anomaly for any l \u2208 Z[b + c + 1,\u221e], thus q must be a cyclic anomaly for Mw at \u03c4 = 0 and l=b+c+w > b + c."}, {"title": "Theorem 1", "content": "(Non-Halting LLM). q \u2208 T* is a non-halting cyclic anomaly for model Mw at temperature \u0442 = 0, if and only if, q is a (b, c, l\u2217) cyclic-anomaly for Mw at temperature \u0442 = 0 where l* = b + c + w."}, {"title": "Proof.", "content": "Lemma 1 proves the (if) part of the theorem, and Lemma 2 proves the (only if) part of the theorem."}, {"title": "Corollary 1.", "content": "Let q \u2208 T* be a (b, c) non-halting cyclic anomaly for Mw at temperature \u0442 = 0, then for w > c,\n \u03c7\u03c0\u03af (c,w)= x,...,X, X (1:j+i mod c)\nwhere i \u2208 Z[0, c \u2212 1], j = w_mod c, and r = [w/c], x\u03c0\u03af(c) is a (0,c) non-halting cyclic anomaly for Mw at temperature \u0442 = 0."}, {"title": "Proof.", "content": "As can be seen for Equation 1, when i = 0, xi(c,w) is the input where the proof of Lemma 1 starts. Thus, at this input, we will have passed all input tokens corresponding to the query q and the beginning tokens of the cycle corresponding to xb. This means we will only be left with w tokens corresponding to the non-halting cycle proven in Lemma 1, with beginning xb = $ and b = 0. For i \u2208 Z[1, j] we will only be moving the window as done in Lemma 1 to another point of the non-halting cycle. Thus, every input of the form xi(c,w) will be a (0, c) non-halting cyclic anomaly for Mw at temperature \u03c4 = 0."}, {"title": "Corollary 2.", "content": "Let q \u2208 T* be a (b, c) non-halting cyclic anomaly for Mw at temperature \u0442 = 0, then for w < c, we have xi(c,w), defined for i \u2208 Z[0, c \u2013 w] as x\u03c0\u03ad(c,w) = x(1+i:w+i), and for i \u2208 Z[c \u2013 w + 1, c \u2212 1] \u03b1\u03c2 \u03c7\u03c0\u03af(c,w) is a (0,c) non-halting cyclic anomaly for Mw at temperature \u0442 = 0."}, {"title": "Proof.", "content": "From the proof of the case of w < c of Lemma 1, the corollary follows.\nIntuitively, the Corollaries states that at temperature zero, if we observe a cyclic sequence of tokens repeat beyond the context window, due to determinism, it will repeat forever. For instance, for a short cycle sequence, we need the sliding context window to sample the same token it left out for all tokens in the cycle. The outcome of the sampling is highly dependent on the probability distribution of the tokens in relation to each other within the language model."}, {"title": "3.3 What happens at higher temperatures?", "content": "At higher temperatures \u03c4 > 0 the model Mw is no longer deterministic. When a softmax activation function \u03c3 : Rk \u2192 [0, 1]k is used to map the intermediate values within the model to output probabilities where k = |T|, and \u03c3(z) = , the value of 7 determines the type of distribution Di that the sampler chooses the next token from. As \u03c4 \u2192 \u221e the sampler output distribution Di converges to uniform. This means in the extreme case \u03c4 \u2192 \u221e, the model will be sampling random tokens essentially eliminating the possibility of a non-halting anomaly. Alternatively, for smaller values of \u03c4 > 0 we might still observe cyclic behavior with long output sequences. However, in this case, it might be possible for the non-halting state to be observed.\nIn reality, at higher temperatures, a non-halting cyclic behavior is dependent on the entropy of the probability distribution. When \u03c4 = 0 the distribution has no entropy and the non-halting cycles are easily encountered. On the other hand, when \u315c \u2192 \u221e the distribution has full entropy, and observing a non-halting cycle has a probability that converges to zero. The case in between is harder to rigorously analyze as it would require an analysis of all the hyper-parameters used by the sampler as well as consideration of the quantization effect."}, {"title": "4 Attack Validation", "content": "In the formal analysis section we showed that under temperature \u0442 = 0, a language model could observe non-halting cyclic anomalies. In essence, the non-halting behavior is a result of the model not sampling the <eos> token which directs the sampler to stop its sampling procedure. This type of behavior is not too surprising in the unaligned (base) model since they are typically trained in an auto-regressive fashion over long strings of tokens. The challenge here is to show that the same non-halting behavior persists in the aligned version (the instruct or chat version of the model) which is trained to terminate response.\nIn this section, we describe our experimental work to validate the formal analysis presented in the previous section. Our work here will mainly focus on showing a simple recipe that succeeds with a high probability when leveraged against an LLM. Consequently, we conduct a number of experiments that demonstrate the attack validity and then demonstrate the attack on major LLMs released over the past year. Our results in this section are meant as proof that the non-halting attack is quite easy to reproduce across state-of-the-art LLMs."}, {"title": "4.1 Attack Rationale and Recipe", "content": "The key to the non-halting attack is to find a cyclic behavior that can be stretched beyond the window size of the model. As per the analysis, when these conditions are met we are guaranteed that the model will enter into a non-halting cyclic state. To this end, we observe that LLMs are trained with the main objective of predicting the next token. At zero temperature this means that the model will follow the most likely path of tokens as learned from the training data. Given that the token-set used by any model will be finite, the number of possible token combinations, not considering linguistic coherence, will be exponential in the size of the token-set. On the other hand, the training data will always be limited in size (polynomial in the size of the token-set). This means that a model is essentially guaranteed to have token combinations that it has never seen during training. Thus, when prompted with a non-sensical list of tokens not seen before, the model will have no prior knowledge to fall back on. In this case, a model attempting to predict the next token should be equally likely to output any token within the token-set. However, based on its autoregressive nature, the model is expected to have a slight bias towards the patterns introduced in the input prompt due to in-context learning. This suggests that a non-sensical cycle of tokens is expected to induce the model into continuation of the same cycle of tokens over and over without a reason for the model to exit this cycle.\nThis rationale can be expected to apply to the unaligned base model since its natural behavior is to continue generation based on the provided prompt. On the other hand, the aligned model is fine-tuned and optimized for dialogue and chat use cases where a non-sensical input is expected to be rejected by the model as it falls outside of the proper interaction it was fine-tuned to observe. One might conclude that the non-halting attack would not be applicable to a properly aligned model. Unfortunately, this is not the case.\nAs the formal analysis showed, a non-halting cycle in the unaligned base model is an intrinsic model behavior stemming from fixed points. All fine-tuning can do is to prevent the model from engaging the prompt causing this type of non-halting behavior to manifest. However, current fine-tuning procedures do not seem capable of changing the fundamental model behavior. That is, while we can teach the model to avoid non-sensical inputs leading to cyclic behavior, we do not seem to be able to prevent the model from generating these non-sensical inputs itself. Indeed, as can be seen from Figure 1, by providing the aligned model with a proper request that naturally leads to the generation of the same cycle of tokens that lead to a non-halting state, we can essentially bypass the alignment process and revert the aligned model behavior to match that of the unaligned base model. Thus, we have a simple recipe that seems to have a high probability of success in sending an aligned model into a non-halting cycle.\nTo this end, the following is the general description of our attack in Recipe 1. It might appear that the Recipe only works if we have access to the unaligned base model. However, in practice, we observed that once we identify a cycle in the base model for some LLM, we may transfer the same cycle to target a different aligned model. We note here that transferability in attack-prompts was previously noted and utilized in [2]."}, {"title": "4.2 Experiments on llama 3", "content": "In this section, we present results and examples that validate our analysis and recipe using Meta-Llama-3-8B as the base unaligned model and Meta-Llama-3-8B-Instruct as the aligned model. Further, all the results are obtained using the Python code released as part of the Llama 3 GitHub repository [19]."}, {"title": "4.2.1 Base Model Meta Llama-3-8B", "content": "We start with the base unaligned model and notice that almost any repetition of a token (or set of unrelated tokens) in a concatenated fashion results in a valid-cycle that observes a non-halting state. For example, we initially tested slightly more complex cycles (as shown in Figure 1) where the cycle is \u201cMGUSAQ\u201d which is made of three tokens (MG, USA, @). Simply repeating this cycle 2 times to form the cycle-pattern (a total of only 6 input tokens) was sufficient to immediately send the model into a non-halting cyclic anomaly without any preceding output. We repeated this experiment for single token inputs with words like \u201cJohn\u201d or \u201cAdam\" and observed that a repetition of 3 times to form the cycle-pattern (a total of 3 input tokens) was sufficient to send the model into a non-halting cyclic anomaly without any preceding output as shown in Figure 3."}, {"title": "4.2.2 Meta-Llama-3-8B-Instruct", "content": "In the aligned model we utilize the valid cycle observed in the base model in order to bypass the alignment and force the model into the non-halting state. Here we use the following prompt structure (used in Figure 1).\nRandomly choose words from the Context provided and use them to form a non-sensical Answer. \\nContext: ' + context-prompt + '\\nAnswer:'\nwhere context-prompt is simply made of a cycle repeated a fixed number of times. In the example shown in Figure 1 (applied to gpt4-0) the minimum number of repetitions of \u201cMGUSAO\u201d that was required to generate the non-halting cycle was 3 compared to 17 required for Meta-Llama-3-8B-Instruct. This number of repetitions slightly changes based on the model, the specific tokens used in the cycle, the size of the cycle, and the exact wording of the instruction-prompt. Here, the word \u201cJohn\u201d had to be repeated at least 45 times before the non-halting anomaly took effect, whereas the word \u201cAdam\u201d required at least 38 repetitions although both are made of a single token.\nBelow we show an example using the word \u201cADAM\u201d which is made of two tokens (AD, AM). In this case, the minimum number of repetitions required was 14. We note here that we verified the same behavior for many cycles with different sizes and different number of tokens. In all these experiments we used Recipe 1 with the same prompt as before. In general, we observe that it was straightforward to apply and obtain a non-halting state with a high probability of success."}, {"title": "4.3 Experiments on gpt4-0", "content": "In this section, we validate our analysis on the gpt4-o model. Further, all the results are obtained using OpenAI API calls from Python code using the openai-python package. Here we use the same prompt as before in Figures 4 and 1. For example, in Figure 1 we initially tested the repeated cycle \u201cMGUSA@\u201d which is made of three tokens (MG, USA, @). Simply repeating this cycle 3 times in the context was sufficient to send the model into a non-halting cyclic anomaly MGUSA MGUSA MGUSA MGUSA .... without any preceding output. The anomaly is observed at temperature 0 but is also observable up to a temperature 0.5. We repeated this experiment for single token inputs with simple words like \u201cAdam\" and observed that a repetition of 3 times to form the cycle-pattern (a total of 3 input tokens) was sufficient to send the model into a non-halting cyclic anomaly without any preceding output as shown in Figure 5. Note that we obtain similar responses through OpenAI API calls as shown in Figures 1 and 5 as well as through the playground on the OpenAI website. The main difference is that in the playground we need to input a direct command to the query to set the temperature."}, {"title": "4.4 Experiments on gemma-2.", "content": "Here we validate our results using the gemma-2-9b-it model. All the results are obtained using the Hugging Face Transformer library in Python [20]. We use the same prompt and recipe as the previous sections. We test the cycle \u201cJohn \" which is made of one token (John). We observe that simply repeating this cycle 4 times in the context was sufficient to immediately send the model into a non-halting cyclic anomaly without any preceding output and with an extra space between the John leading to a cycle of two tokens. The anomaly is observed at temperature 0 but is also observable up to a temperature of 0.2 with a decreasing likelihood of observing the anomaly. We repeated this experiment for a two-token input with simple words like \u201cJohn@\u201d and observed that a repetition of 55 times was required to form the cycle-pattern before the model entered into a non-halting cyclic anomaly without any preceding output as shown in Figure 8. Further, the anomaly here was observed with a high probability up to a temperature of 0.6."}, {"title": "4.5 Attack Validation on Major LLMs", "content": "Here we use the same prompt and recipe discussed in the previous sections to demonstrate the non-halting attack on a number of top models released over the past year. Our experiments utilize the token Adam as the targeted cycle used for Recipe 1. In each model, we use the same prompt as before and only vary the number of token repetitions in the cycle pattern required to observe a non-halting behavior at different temperatures. Gemma and Llama models are accessed via HuggingFace's Transformers library. OpenAI models are accessed through OpenAI's API. Gemini models are accessed through Google AI Studio. Claude models are accessed through the Anthropic Console.\nTo this end, Figure 9 demonstrates the number of repeated cycle tokens required to send the corresponding model into a non-halting state at different temperatures. The non-halting state is easy to achieve using the recipe in most tested models. Further, the non-halting state is still mostly present at higher temperatures with a clear increase in the number of needed cycle repetitions. What is surprising about the results shown in Figure 9 is that the exact same prompt was used to send all these models into a non-halting state. The only difference from one model to another is the number of cycle repetitions required to guarantee a non-halting behavior.\nAs can be seen from Figure 9, the latest Gemini model Gemini 1.5 Pro Exp 0827 did not produce any non-halting output at any temperature using the same prompt. Earlier models of Gemini 1.5 did produce a non-halting behavior. For instance, Gemini 1.5 Pro model produced non-halting until temperature 0.2. In contrast, gpt4-0613 seemed more resilient than the more recent gpt4-0 as it needs more input tokens to produce a non-halting output. The most recent Claude model Claude-3.5 is more vulnerable compared to the older models. Claude-3.5-sonnet produced non-halting output for the entire temperature range while the Claude-3-opus and Claude-3-sonnet models did not produce after temperature 0.6 and Claude-3-haiku produced a non-halting state for every temperature needing a maximum of 36 repetitions. This makes it the most vulnerable model in the Claude family. We also note that while the LLama-3.1-8B-Instruct and gemma-2-9B-it models have similar sizes gemma-2 is more resilient since it is not possible to produce non-halting output at temperature 1 and it needs slightly more tokens at other temperatures. After our experiments were completed Llama-3.2 was released which we found also to be vulnerable to non-halting at temperature zero.\nWe finally note here that all these models were not investigated for different prompts; rather all experiments simply used the same prompt to produce a non-halting state. This suggests that it is quite simple to find different prompts that could work even better for specific models. In fact, the results here lead us to conjecture that aligning against the non-halting attack might be quite difficult given how prevalent this phenomenon is, even at higher temperatures."}, {"title": "4.6 Observations on Experiments", "content": "We conclude this section by sharing a number of comments and observations regarding our experiments.\nEscaping Alignment: We note that regardless of whether the anomaly is halting or not, in many trials we observed that it manages to escape alignment, i.e. the LLM returns an Answer that does not appear in the form of a proper answer expected by humans. In general, we expect better-aligned models to require more repetitions in the recipe to succeed.\nThe Used Prompt is Fragile: The prompt structure is fragile. Adding/removing spaces or newlines or rewording it, e.g. in Figure 4 changing \u201cchoose many words\u201d to \u201cchoose words\" or \"choose some words\" may break the recipe or require a different number of repetitions of the cycle for the non-halting anomaly to manifest.\nCycle Length and Repetition Matters: The length of the cycle and the number of repetitions in the query context affects the observability of the anomaly as clearly demonstrated by Figure 9. The more aligned a model is, the more repetitions seem to be required. Furthermore, most of our experiments focused on small cycles made of a few tokens (1 token for Figure 9). That said, we did observe non-halting behavior with cycles that contained more than 20 tokens, in which case a few repetitions sufficed to observe a non-halting behavior. Finally, we observe that finding large cycles with many tokens is more difficult which potentially makes them more challenging to identify and handle. This is of particular concern in RAG-based applications as we will discuss in a later section."}, {"title": "5 Gradient Based Inversion for Non-halting Queries", "content": "In the previous section, we saw that it is easy to craft non-halting queries using a recipe that effectively bypasses alignment. Here we set forth to recover non-halting queries directly using model inversion techniques in aligned models. It is conceivable that future alignment will pay more attention to specialized prompts like our non-halting Recipe. However, direct inversion will still be available as an attack strategy. Our objective here is to better understand how common non-halting queries are in aligned models via direct model inversion.\nOne difficulty in applying model inversion techniques is that non-halting queries have an unending output. However, thanks to Theorem 1, we know that a query output only needs to fill the context window plus the cycle length in order to be considered a non-halting query. Thus, in theory, we know the exact finite output that in reality represents a non-halting behavior. Unfortunately, existing model inversion techniques, e.g. [17, 18, 2"}]}