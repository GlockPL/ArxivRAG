{"title": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review", "authors": ["Neha Prakriya", "Jui-Nan Yen", "Cho-Jui Hsieh", "Jason Cong"], "abstract": "Large Language Model (LLM) pretraining traditionally relies on autoregressive language modeling on randomly sampled data blocks from web-scale datasets. We take inspiration from human learning techniques like spaced repetition to hypothesize that random data sampling for LLMs leads to high training cost and low quality models which tend to forget data. In order to effectively commit web-scale information to long-term memory, we propose the LFR (Learn, Focus, and Review) pedagogy, a new dynamic training paradigm which focuses and repeatedly reviews complex data blocks at systematic intervals based on the model's learning pace and progress. LFR records the model perplexities for different data blocks and frequently revisits blocks with higher perplexity which are more likely to be forgotten. We pretrain the GPT-2 models (124M - 1.5B) from scratch on the OpenWebText dataset using LFR. We test on downstream tasks from the language modeling, question answering, translation, and problem solving domains to achieve consistently lower perplexity and higher accuracy than the baseline OpenAI models, while obtaining a 20x pretraining speed-up.", "sections": [{"title": "Introduction", "content": "LLMs have achieved remarkable success in understanding and generating human language. This success is driven by the ever-increasing model parameter sizes which require web-scale training datasets like OpenWebText, CommonCrawl, and Pile, leading to unsustainable training costs. For example, pretraining the GPT-4 model was estimated to have cost around $100M USD over a period of 3-4 months using 25k NVIDIA A100 GPUs. As such, a key challenge for unlocking the next generation of language models is to significantly reduce training costs while retaining or improving downstream task performance.\nPopular LLMs, like the GPT series, Llama series, Mistral and others, were trained utilizing autoregressive/causal techniques, where the goal of training is to generate a plausible next token based on a randomly sampled input data block from the training corpus. However, data scraped from the web is highly redundant and presents varying complexity. Inspired by these observations on different learning methods used by humans and LLMs, our work has the following contributions:\n1. Profile LLM pretraining to observe multiple descent behaviour for 25% of the training tokens that are forgotten multiple times.\n2. Develop a Learn-Focus-Review (LFR) training pipeline that uses perplexity to gauge the LLM's learning pace, focusing on complex data blocks while regularly reviewing all data blocks to prevent forgetting.\n3. Pretrain GPT-2 models (124M-1.5B) on the OpenWebText dataset with 4 AMD MI210 (345M-1.5B) and 4 AMD MI100 GPUs (124M), where the 1.5B model training took less than 20 days, and evaluate them on six\n4. LFR results in significantly lower perplexity and higher accuracy compared to baseline OpenAI models, achieving these improvements with 20x fewer training iterations.\n5. Observe that LLMs first learn conversational and anecdotal data, before being able to retain factual and instructional information in long-term memory.\n6. Demonstrate that text importance varies with training time and model size, driving the need for dynamic data selection methods like LFR.\nIn the following sections, we examine prior works on efficient LLM pretraining before diving deeper into our proposed training strategies and design decisions."}, {"title": "Related Work", "content": "Prior works on efficient pretraining of LLMs using data selection have primarily focused on using distance metrics and clustering techniques. Tirumala et al. (2023) proposes D4, which deduplicates and selects cluster centers in the embedding space generated by pretrained models. SemDeDup (Abbas et al., 2023) prunes semantic duplicates using pretrained models. It can successfully prune 50% of the training data with minimal performance loss. MiniPile (Kaddour, 2023; min) uses the pretrained E5-Large (Wang et al., 2024) model to embed documents in the Pile dataset and clusters them to select a smaller pretraining corpus of ~6GB. DSIR (Xie et al., 2023) proposes selecting subsets from large unlabeled datasets through importance resampling to match the distribution of the target dataset. However, considering the high cost of training, it is unsustainable to sample a new subset and pretrain the LLM from scratch for every new downstream task. While these works address the common redundancy issue present in LLM pretraining corpuses, they face three major drawbacks. First, they require pretrained models for calculating the distance metric on embeddings. This step must ensure that the embeddings found by the pretrained model are similar to those of the target model to ensure the transferability of sample importance. Second, clustering-based methods do not scale to encompass the diversity of data in web-scale datasets. This can lead to a loss in downstream task accuracy (see Section 5). Third, they are slow and can take as long as 2-3 hours to generate 6GB subsets of the OpenWebText dataset (38GB) as shown in Section 5.\nMarion et al. (2023) evaluates data selection by pretrained models used to obtain perplexity, Error L2-Norm (EL2N) (Paul et al., 2021), and memorization rankings (Biderman et al., 2023). They find that perplexity is a good metric to characterize a model's understanding of a data sample and that retaining data samples with middle perplexity values can outperform training on the entire dataset. While this finding inspires our work, their method relies on pretrained model checkpoints to evaluate data sample importance. This limits the scalability."}, {"title": "Problem Formulation and Profiling", "content": ""}, {"title": "LLM Pretraining Objective", "content": "Given an LLM model parameterized by weights 0 and a web-scale dataset D, we first tokenize all the documents in the dataset and obtain contextlength sized sequences of tokens si such that the training corpus becomes D = {S1, S2, S3, ...Sn}. For the OpenWebText dataset used in this paper, the context-length is 1024 and there are a total of 9B training tokens. Given one such sequence of tokens si = {X1, X2, ...Xn}, the training objective is to autoregressively predict the next M tokens:\n$\\P_{\\theta}(y | x) = \\prod_{i=1}^{M}P_{\\theta}(y_i | y_{1:i-1},x)$.\n\nThe batches of sequences (si) used as input to prompt the LLM (shown in Eq. 1) and backpropagate for each step are randomly sampled from the training corpus."}, {"title": "Observations from Training on Randomly Sampled Data", "content": "In order to better understand the drawbacks of this traditional training technique, we profile the pretraining process for the 345M parameter GPT-2 model. The training hyperparameters are provided in Appendix A.1. Similarly to Marion et al. (2023), we use perplexity as a metric to monitor the training progress. Given a token sequence Si = {X1, X2, ..., xn} from the dataset D, perplexity is computed as:\n$\\PPL(s) = exp(\\frac{1}{N} \\sum_{x_j \\in s_i} NLL(x_j))$,\nwhere NLL(xj) is the negative log likelihood of token xj computed as follows:\n$NLL(x_j) = -log P(x_j | x_{<j}; \\theta)$.\nUsing this metric, models exhibiting lower perplexities are considered better since this indicates a high probability of selecting text closest to the raw dataset.\nTo generate profiling information, we record the data block ID (each data block consists of 1024 tokens) and model perplexity each time a block is randomly sampled from the training corpus across all training iterations. At the end of the training process, we have a record of the perplexities over time for each data block. Since we train the GPT-2 345M model for 8 epochs, each data block has at least 8 perplexity values captured at different training iterations. From this record, we classify each data block as one of the following:\n1. Learned: recorded perplexities monotonically decrease.\n2. Unlearned: recorded perplexities monotonically increase.\n3. Forgotten: recorded perplexities first increase and then decrease. Such an upward and downward trend may repeat any number of times during training.\nWe observe that 25% of data blocks are forgotten at least once during training and that of the data blocks that are forgotten, 82% are forgotten multiple times during training, i.e., they display multiple descent behavior (Figure 1). Xia et al. (2022) reported a double-descent behavior for the OPT models (Zhang et al., 2022), and our above experiment further demonstrates that the \"forgotten\" can happen multiple times in LLM pretraining."}, {"title": "LFR Training Methodology", "content": "Based on our profiling observations in Section 3.2 we propose to replace traditional autoregressive language modeling methods with Spaced Repetition (Tabibian et al., 2019). Spaced Repetition is an evidence-based learning method proven to improve information retention and learning pace in humans (Smolen et al., 2016). In this technique, challenging pieces of information are reviewed more often, at regular intervals, and easier pieces of information are presented to the learner less often. We pretrain our models with a combination of the following three steps:\n1. Learn: Initially, we present the model with the entire dataset and train on randomly selected data blocks for p1 epochs, as normally seen in the traditional approach. p\u2081 can be configured by the user based on the available compute budget, model, and dataset. During the p1 epochs, we also record the perplexities for all data blocks. The next 2 phases can be repeated reps number of times based on the available compute budget.\n2. Focus: From the collected perplexities from the previous phase, we discard 81% of the data blocks corresponding with the lowest perplexities. In doing so, we provide a mechanism for shifting the model's focus towards learning data blocks that were determined to be difficult. In this phase of training, we restrict the random sampling of data points to this reduced subset for p2 epochs. 81 and p2 are user-controlled hyperparameters.\n3. Review: Next, we reintroduce all of the removed data blocks and train the model on the entire corpus for p3 epochs (reconfigurable). This ensures that we allow the model to review and revisit data blocks which it may have forgotten. We record the perplexities for all data blocks again in this training phase.\nOur algorithm is detailed in Algorithm 1. For pretraining the GPT-2 models, we choose the following combination of the aforementioned steps:\n1. Phase 1: Learn for 1 epoch (p\u2081 = 1).\n2. Phase 2: Focus on 50% of the data for 1 epoch (81 = 50, p2 = 1).\n3. Phase 3: Review the entire dataset for another epoch (p3 = 1).\n4. Phase 4: Focus on 30% of the data for 5 epochs (reps = 2, s2 = 70, p4 = 1).\nWe choose this configuration based on our limited pretraining budget and the profiling in Section 3.2 which revealed that ~25% of data samples are forgotten at least once during training. From our observations in Figure 1, we notice that the data samples that are forgotten are forgotten multiple times. On average, they had to be presented to the model 4 times for them to be learned. Therefore, we use the first three training phases to identify these samples, and then spend 5 epochs focusing on them. To be on the conservative side, we focus on 30% of the samples in the last 5 epochs, to ensure that the model retains these difficult samples in its long-term memory. Furthermore, we ensure that all data samples are reviewed through Phases 1 and 3 of our approach. Finally, we test other values for p1, 81, p2, p3, and reps and evaluate the sensitivity of LFR to these hyperparameters in Section 5.9."}, {"title": "Evaluation", "content": "In this section, we present a comprehensive evaluation of our proposed data selection method, LFR."}, {"title": "Conclusion", "content": "We present LFR, a new training paradigm which proposes a mechanism for models to learn, focus, revisit and review complex data during training. Our method is inspired by the effective learning methods used by humans. LFR achieves consistently lower perplexity and higher accuracy on downstream task performance over traditional autoregressive modeling-based methods while using 20x fewer training iterations. We also demonstrate that LLMs first learn anecdotal and conversational data, before being able to retain factual information."}, {"title": "Limitations and Ethical Considerations", "content": "LFR presents the following directions for future work:\n1. LFR is evaluated on models up to 1.5B parameters using the OpenWebText dataset, constrained by our compute resources. We encourage researchers to validate such focused learning approaches for different model families, and domains.\n2. The sensitivity study in Section 5.9 reveals that the hyperparameters selected in Section 4 have a large impact on the performance of the trained model. Due to our limited compute budget, we are unable to present more comprehensive hyperparameter tuning experiments than those presented in Section 5.9."}]}