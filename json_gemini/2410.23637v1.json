{"title": "Anytime-Constrained Multi-Agent Reinforcement Learning", "authors": ["Jeremy McMahan", "Xiaojin Zhu"], "abstract": "We introduce anytime constraints to the multi-agent setting with the corresponding solution concept being anytime-constrained equilibrium (ACE). Then, we present a comprehensive theory of anytime-constrained Markov games, which includes (1) a computational characterization of feasible policies, (2) a fixed-parameter tractable algorithm for computing ACE, and (3) a polynomial-time algorithm for approximately computing feasible ACE. Since computing a feasible policy is NP-hard even for two-player zero-sum games, our approximation guarantees are the best possible under worst-case analysis. We also develop the first theory of efficient computation for action-constrained Markov games, which may be of independent interest.", "sections": [{"title": "1 Introduction", "content": "Although multi-agent reinforcement learning (MARL) has made many breakthroughs in game-playing, the literature has long since advocated the importance of constraints in real-world applications [22]. Despite their importance, the literature on constrained MARL is far behind the state-of-the-art in the single-agent setting. Most recently, almost-sure [8] and anytime [30, 29] constraints have emerged in the single-agent setting to capture real-world scenarios, such as medical applications [13, 31, 25], disaster relief scenarios [18, 43, 38], and resource management [27, 26, 33, 4]. Anytime constraints are especially natural for autonomous vehicles [30, 36, 43], which are a primary application of MARL [12, 15, 42]. For example, a GPS client must balance safety with agents' selfish desires lest they risk accidents that would dissuade agents from using the service. Despite these applications, anytime constraints have yet to be studied in the multi-agent setting, which we remedy in this work.\nFormally, we consider a constrained Markov game (cMG) G with a budget vector B. A joint policy \u03c0 satisfies an anytime constraint if every player i's accumulated cost is at most \\(B_i\\) at all times: \\(P_{\\pi}^{G}[\\forall h \\in [H], \\sum_{t=1}^{h} C_{i,s_t} \\leq B_i] = 1\\). Given such a constraint, the natural solution concept is the anytime-constrained equilibrium (ACE). At a high level, an ACE is a feasible joint policy \u03c0 for which no player can gain a higher value from any feasible deviation. Our main question is as follows:\nFor what class of cMGs can ACE be computed (approximately) in polynomial time?"}, {"title": "1.1 Related Work", "content": "Constrained MARL. Ever since constrained equilibria were introduced [2], most works have focused on learning in the regret setting [2, 9, 20, 14, 24]. Outside of these works, the literature has focused on single-agent-constrained Markov Decision Processes (cMDP). It is known that cMDPs can be solved in polynomial time using linear programming [1], and many interesting planning and learning algorithms have been developed for them"}, {"title": "2 Equilibria", "content": "Constrained Markov Games. A (tabular, finite-horizon) n-player Constrained Markov Game (cMG) is a tuple \\(G = (S, A, P, R, C, H)\\), where (i) S is the finite set of states, (ii) \\(A = A_1 \\times \\dots A_n\\) is the finite set of joint actions, (iii) \\(P_h(s, a) \\in \\Delta(S)\\) is the transition distribution, (iv) \\(R_h(s,a) \\in \\Delta(\\mathbb{R}^n)\\) is the reward distribution, (v) \\(C_h(s,a) \\in \\Delta(\\mathbb{R}^n)\\) is the cost distribution, and (vi) H is the finite time horizon. To simplify notation, we let \\(r_h(s, a) \\stackrel{\\text{def}}{=} E[R_h(s, a)]\\) denote the expected reward, \\(S \\stackrel{\\text{def}}{=} |S|\\) denote the number of states, \\(A \\stackrel{\\text{def}}{=} |A|\\) denote the number of joint actions, \\([H] \\stackrel{\\text{def}}{=} \\{1,...,H\\}\\), and |G| be the total description size of the cMG.\nInteraction Protocol. The agents interact with G using a joint policy \\(\\pi = \\{\\pi_h\\}_{h=1}^{H}\\). In the fullest generality, \\(\\pi_h: \\mathcal{H}_h \\to \\Delta(A)\\) is a mapping from the observed history at time h (including costs) to a distribution of actions. Often, researchers study Markovian policies, which take the form \\(\\pi_h : S \\to \\Delta(A)\\), and product policies, which take the form \\(\\pi = \\{\\pi_i\\}_{i=1}^{n}\\), where each \\(\\pi_i\\) is an independent policy for player i.\nThe agents start at an initial state \\(s_1 \\in S\\) with observed history \\(\\tau_1 = (s_1)\\). For any \\(h\\in [H]\\), the agents choose a joint action \\(a_h \\sim \\pi_h(\\tau_h)\\). Then, the agents receive immediate reward vector \\(r_h \\sim R_h(s, a)\\) and cost vector \\(c_h \\sim C_h(s, a)\\). Lastly, G transitions to state \\(s_{h+1} \\sim P_h(s_h, a_h)\\) and the agents update their observed history to \\(\\tau_{h+1} = (\\tau_h, a_h, c_h, s_{h+1})\\). This process is repeated for H steps; the interaction ends once \\(s_{h+1}\\) is reached.\nAnytime Constraints. Suppose the agents have a budget vector \\(B\\in \\mathbb{R}^n\\). We say a joint policy \u03c0 satisfies anytime constraints if,\n\\[P_{\\pi}^{G}[\\forall h \\in [H], \\sum_{t=1}^{h} c_t \\leq B] = 1. \\tag{ANY}\\]\nHere, \\(P_{\\pi}^{G}\\) denotes the probability law over histories induced from the interaction of \u03c0 with G, and all vector operations are performed component-wise. If G only has anytime constraints, which will be the case in this work, we call G an anytime-constrained Markov game (acMG). We refer to any policy \u03c0 satisfying (ANY) as feasible for G, and let \\(\\Pi_G\\) denote the set of all feasible policies for G.\nRemark 1 (Extensions). Our results can also handle multiple constraints per agent, infinite discounting, and the weaker class of almost sure constraints. We defer the details to the appendix."}, {"title": "Solution Concepts", "content": "Solutions to games traditionally take the form of equilibrium. In the MARL realm, the most popular notions include the Nash equilibrium (NE), correlated equilibrium (CE), and course-correlated equilibrium (CCE). Given constraints, the key difference is a focus on feasible policies. Infeasible policies lead to disastrous outcomes for an agent. Thus, not only should a constrained equilibrium be feasible, but agents should only consider deviating if doing so would be feasible.\nDefinition 1 (Anytime-Constrained Equilibria). We call a joint policy \u03c0 an anytime-constrained equilibrium (ACE) for an acMG G if (1) \\(\\pi\\in \\Pi_G\\) and (2) for all players \\(i \\in [n]\\) and potential deviation policies \\(\\pi_i\\), either,\n\\[\\begin{array}{c} (\\pi_i, \\pi_{-i}) \\notin \\Pi_G \\\\ OR \\\\ V_{\\pi_i, \\pi_{-i}}^{G} \\geq V_{\\pi}^{G} \\end{array} \\tag{ACE}\\]\nHere, \\(V_{\\pi}^{G} \\stackrel{\\text{def}}{=} E_{\\pi}^{G}[\\sum_{t=1}^{H} r_{i,t}]\\) denotes i's value from interacting with G under \u03c0, and \\(E_{\\pi}^{G}\\) denotes the expectation defined by the law \\(P_{\\pi}^{G}\\). Lastly, we call \u03c0 an anytime-constrained Nash equilibrium (ACNE) for G if \u03c0 is additionally a product policy.\nRemark 2 (Correlated Equilibrium). Our definition of ACE in Definition 1 technically corresponds to anytime-constrained course-correlated equilibria (ACCCE), which we simplify to ACE for exposition purposes. Our results apply equally well to anytime-constrained correlated equilibria (ACCE). We delay the definition and discussion of ACCE to the appendix. In the main text, we signal when results specialize to each equilibria type by writing ACE (NE/CE/CCE).\nStage Games. It is often useful to consider refinements of equilibrium notions that are more structured and robust. The classical refinement for sequential games is the subgame-perfect equilibrium (SPE). An SPE policy is required to behave optimally under any history, also called subgames, even if those subgames are not realizable. That way, players could still recover if any deviated from the policy's suggestion. In the constrained setting, we only consider feasible subgames, i.e., subgames that are realizable by some feasible policy. This means the players could still adapt and finish the game whenever a player takes an unsupported but feasible action.\nFormally, we let \\(H_h \\stackrel{\\text{def}}{=} \\{\\tau_h \\in \\mathcal{H}_h | P_{\\pi}^{G}[\\tau_h] > 0\\}\\) denote the subset of partial histories at time h that are realizable by a policy \u03c0, and let \\(\\mathcal{F}_h \\stackrel{\\text{def}}{=} \\cup_{\\pi\\in\\Pi_G} H_h\\) denote the set of partial histories at time h realizable by some feasible policy. For any feasible subgame \\(\\tau_h \\in \\mathcal{F}_h\\), we let,\n\\[\\Pi_G(\\tau_h) \\stackrel{\\text{def}}{=} \\{\\pi | P_{\\pi}^{G}[\\forall h \\in [H], \\sum_{t=1}^{h} c_t \\leq B | \\tau_h] = 1\\} \\tag{SUB}\\]\ndenote the set of feasible policies for the subgame \\(\\tau_h\\). To capture our earlier intuition, we require an anytime-constrained SPE to be a policy that is feasible for any feasible subgame, and that beats any feasible deviation for that subgame.\nDefinition 2 (Anytime-Constrained Subgame-Perfect Equilibria). We call a joint policy \u03c0 an anytime-constrained subgame-perfect equilibrium (ACSPE) for an acMG G if for all times \\(h \\in [H + 1]\\), and all feasibly-realizable histories \\(\\tau_h \\in \\mathcal{F}_h\\), \u03c0 satisfies (1) \\(\\pi \\in \\Pi_G(\\tau_h)\\) and (2) for all players \\(i \\in [n]\\), and potential deviation policies \\(\\pi_i\\), either,\n\\[\\begin{array}{c} (\\pi_i^{\\prime}, \\pi_{-i}) \\notin \\Pi_G(\\tau_h) \\\\ OR \\\\ V_{\\pi_i^{\\prime}, \\pi_{-i}}^{i,h}(\\tau_h) \\geq V_{\\pi}^{i,h}(\\tau_h) \\end{array} \\tag{ACSPE}\\]"}, {"title": "3 Feasibility", "content": "Before we can fully understand ACE, we must first understand feasible policies. This section derives characterizations of feasibly realizable histories under anytime constraints. This leads us to design an algorithm that determines if a feasible policy exists, while also producing the set of all feasibly realizable cumulative costs and actions. These sets will be critical to our later equilibria computation in Section 4.\nFirst, observe that for a policy to be feasible, all histories realizable under \u03c0 must obey the budget. If \\(\\tau_h = (s_1, a_1, c_1, ..., s_h) \\in \\mathcal{H}_h\\) is any partial history, we let \\(\\bar{c}_h \\stackrel{\\text{def}}{=} \\sum_{t=1}^{h-1} c_t\\) denote the vector of cumulative costs induced by the history. Given this notation, we see that \\(\\pi\\in \\Pi_G\\) if and only if for all \\(h \\in [H + 1]\\) and all \\(\\tau_h \\in H\\), it holds that \\(\\bar{c}_h \\leq B\\).\nHistory Translation. Consequently, we only need to consider the (state, cost)-pairs induced by a history to determine feasibility. In particular, we can focus on \\(\\Gamma_h \\stackrel{\\text{def}}{=} ((s_1, 0), a_1, (s_2, c_1), a_2, ..., (s_h, \\bar{c}_h))\\), which denotes \\(\\tau_h\\) written in (state, cost)-form. Observe that for any \\(\\tau_h\\), the translation of \\(\\tau_h\\) to (state, cost)-form is well-defined and unique. Specifically, given \\(\\tau_h\\), we can infer any immediate cost \\(c_k\\) uniquely by \\(c_k = \\bar{c}_{k+1} - \\bar{c}_k\\), and given \\(\\Gamma_h\\), we can infer \\(\\bar{c}_k\\) uniquely by \\(\\bar{c}_k = \\sum_{t=1}^{k-1} c_t\\). Given this equivalence, we focus on characterizing the following sets."}, {"title": "4 Reduction", "content": "As hinted in the previous section, we can convert the anytime constraint on full histories into a per-time constraint on the available actions. Specifically, if the agents track their cumulative costs, they can identify actions that satisfy the constraint long term. These actions exactly correspond to those in \\(FA_h(s, \\bar{c})\\)."}, {"title": "5 Computation", "content": "In the last section, we showed how to reduce our anytime-constrained game problem to an action-constrained game problem. However, efficient algorithms for action-constrained games are currently unknown. In this section, we remedy this knowledge gap by designing efficient algorithms for computing MPE of an action-constrained MG. Moreover, we show that feeding our action-constrained method into our reduction yields a polynomial time algorithm for computing ACE so long as the cost precision is logarithmic.\nWe take a backward induction approach similar to other planning algorithms for Markov games. Unlike traditional MGs, here, we must iteratively solve action-constrained matrix stage games. Then, we can combine the constructed policies for each stage to solve the full game. We prove the correctness of this algorithm by deriving a novel theory of equilibria in action-constrained Markov games.\nMatrix Games. The key bottleneck to this backward induction approach is solving the action-constrained matrix games. Formally, let \\((\\mathcal{A}, X, u)\\) denote an action-constrained matrix game, where (i) \\(\\mathcal{A}\\) is the joint action space, (ii) \\(X \\subseteq \\mathcal{A}\\) is the set of feasible actions, and (iii) u is the utility function. We tackle this problem by devising a variation of the standard CE/CCE LP. Importantly, we modify the constraint \\(\\sum_{a\\in \\mathcal{A}}\\sigma(a) = 1\\), which ensures the total probability mass of all joint actions equals one, into the constraint \\(\\sum_{a\\in X}\\sigma(a) = 1\\), which ensures the support of the joint strategy is contained in the valid joint action space. We also define the utility of any infeasible action to be -\u221e so that infeasible deviations will be appropriately ignored by the LP. The full definition of the LP, which has no objective function, is,\n\\[\\begin{aligned} \\sum_{a \\in X} \\sigma(a) (u_i(a) - U_i(a_i^{\\prime}, a_{-i})) &\\geq 0, & \\forall i, a \\in A_i \\\\ \\sum_{a \\in X} \\sigma(a) &= 1, & \\tag{CLP} \\\\ \\sigma(a) &\\geq 0 & \\forall a \\in X \\end{aligned}\\]"}, {"title": "6 Approximation", "content": "In the previous section, we showed our method runs in polynomial time whenever the cost precision is small. However, in cases where the cost precision is large or, even worse infinite, the computation may require exponential time due to the NP-hard nature of finding a feasible solution, Proposition 2. To combat this issue, we slightly relax the feasibility condition. This allows us to compute equilibrium policies that only violate the budget by a given \\(\\epsilon > 0\\) at the cost of an additional poly(1/\\epsilon) factor in running time.\nIn this section, we allow any infinite support cost distributions that are bounded above. We also require that distribution is a product distribution to enable comparisons between supported costs. Technically, we also need the CDF of the distribution to be efficiently computable for use in computation.\nDefinition 7 (Approximate Feasibility). For any \\(\\epsilon > 0\\), a joint policy \u03c0 is \\(\\epsilon\\)-additive feasible for G if,\n\\[P_{\\pi}^{G}[\\forall h \\in [H], \\sum_{t=1}^{h} c_t \\leq B + \\epsilon ] = 1, \\tag{3}\\]\nand \\(\\epsilon\\)-relative feasible for G if,\n\\[P_{\\pi}^{G}[\\forall h \\in [H], \\sum_{t=1}^{h} c_t \\leq B(1 + \\epsilon \\sigma_B)] = 1, \\tag{4}\\]\nwhere \\(\\sigma_B\\) is the sign of B. We then define an \\(\\epsilon\\)-additive/relative approximate ACE to satisfy the usual conditions of Definition 1 but with the feasibility condition (1) relaxed to an \\(\\epsilon\\)-additive/relative feasibility condition.\nRounding. The key idea of our approximations is to have players round down any cost vector they receive to the nearest multiple of some \\(l > 0\\). Doing so allows the players to track far fewer cumulative costs than originally. In addition, we can effectively truncate the lower regime of the distribution using the fact that if player i ever receives an immediate cost smaller than \\(B_i \u2013 Hc_{\\text{max}}^{i}\\), then it may take any action whatsoever going forward without violating its budget. Thus, the player can treat any smaller cost as if it were \\(B_i - Hc_{\\text{max}}^{i}\\). This process of rounding costs then induces a new cMG with finite support cost distributions."}]}