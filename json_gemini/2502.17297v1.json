{"title": "Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts", "authors": ["Zhenghao Liu", "Xingsheng Zhu", "Tianshuo Zhou", "Xinyi Zhang", "Xiaoyuan Yi", "Yukun Yan", "Yu Gu", "Ge Yu", "Maosong Sun"], "abstract": "This paper introduces Multi-Modal Retrieval- Augmented Generation (M\u00b2RAG), a benchmark designed to evaluate the effective- ness of Multi-modal Large Language Models (MLLMs) in leveraging knowledge from multi- modal retrieval documents. The benchmark comprises four tasks: image captioning, multi- modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant informa- tion from a multi-modal document collection and use it as input context for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM- RAIT), an instruction tuning method that opti- mizes MLLMs within multi-modal contexts. Our experiments show that MM-RAIT im- proves the performance of RAG systems by enabling them to effectively learn from multi- modal contexts. All data and code are available at https://github.com/NEUIR/M2RAG.", "sections": [{"title": "Introduction", "content": "With the rapid development of Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023) and LLaMA (Touvron et al., 2023), they have demonstrated strong emergent abilities in many NLP tasks (Wei et al., 2022; Zhao et al., 2023). However, LLMs often face the issue of hallu- cinations, making them produce unreliable re- sponses (Ji et al., 2023; Huang et al., 2023; Shus- ter et al., 2021). Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Asai et al., 2024b; Shi et al., 2024; Yao et al., 2023) has proven effective in mitigating this hallucination problem by integrat- ing external knowledge with LLMs.\nTo enhance LLMs with retrieved knowledge, existing approaches typically feed retrieved doc- uments as input contexts, prompting LLMs to generate responses based on this in-context in- formation (Ram et al., 2023). Existing RAG ap- proaches (Petroni et al., 2021; Lin et al., 2024) usually focus on retrieving textual knowledge from corpora to aid LLMs in answering queries. Re- cent studies (Hu et al., 2024; Sharifymoghaddam et al., 2024) have extended RAG to Multi-modal Large Language Models (MLLMs), enabling them to address knowledge-intensive and information- seeking tasks that involve visual queries. However, these approaches largely rely on text or images as the sole sources of external knowledge, often overlooking the critical role of multi-modal data in providing richer and more comprehensive in- formation that leads to producing more accurate answers (Hu et al., 2024; Liu et al., 2023b).\nTo advance RAG modeling in multi-modal scenarios, we introduce the Multi-Modal RAG (M\u00b2RAG) benchmark, designed to explore the ef- fectiveness of MLLMs by feeding multi-modal re- trieved documents as the input contexts to answer the question. As shown in Figure 1, we can use im- ages or text as queries to retrieve multi-modal doc- uments via multi-modal dense retrievers (Liu et al., 2023b; Zhou et al., 2024b,a). These multi-modal documents are then used as the input contexts to assist MLLMs during generation. Specifically, our M2RAG benchmark includes four distinct tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. Different from existing works (Aghajanyan et al., 2022; Sharifymoghaddam et al., 2024), M\u00b2RAG is built upon high-quality datasets (Chang et al., 2022; Mishra et al., 2022) and designs four evalu- ation tasks in the open-domain setting, aiming to assess the effectiveness of MLLMs in leveraging the knowledge from multi-modal contexts.\nIn this paper, we also propose the Multi-Modal Retrieval Augmented Instruction Tuning (MM- RAIT) method to adapt MLLMs to the multi-modal in-context learning scenario, enhancing the effec- tiveness of MLLMs in utilizing the knowledge from these multi-modal retrieval documents. Specifi- cally, we design task-specific prompt templates and fine-tune MLLMs on the M\u00b2RAG benchmark, making MLLMs maintain contextual awareness during generation. Our experimental results demon- strate that using retrieved knowledge significantly enhances MLLMs' performance, achieving signifi- cant improvements in both zero-shot and few-shot settings. After training with MM-RAIT, MiniCPM- V and Qwen2-VL show an average improvement of 27% and 34% over vanilla RAG modeling methods, showing the effectiveness of MM-RAIT."}, {"title": "Related Work", "content": "Existing RAG models (Shi et al., 2024; Asai et al., 2024a; Yu et al., 2023b; Yan et al., 2024) typically rely on dense retrievers (Karpukhin et al., 2020; Xiong et al., 2021a; Ren et al., 2021; Xiong et al., 2021b; Gao and Callan, 2022) or sparse retriev- ers like BM25 (Robertson et al., 2009) for text document retrieval. More recent efforts have in- tegrated multi-modal retrieval methods, allowing the inclusion of rich external knowledge from dif- ferent modalities within RAG frameworks. For example, some works (Liu et al., 2023b; Zhou et al., 2024b) have introduced unified multi-modal retrieval systems that map images and texts into a shared semantic space. This approach allows for single-modal matching, cross-modal matching, and modality routing within the embedding space. VISTA (Zhou et al., 2024a) further enhances multi- modal retrieval by optimizing synthetic training data and refining training strategies. These advance- ments enable the retrieval of multi-modal knowl- edge, providing a way for evaluating the effective- ness of MLLMs in multi-modal contexts.\nMulti-modal Large Language Models (MLLMs) (Achiam et al., 2023; Team et al., 2023; Sun et al., 2024b,a; Aghajanyan et al., 2022; Lu et al., 2024) have proven their effectiveness in understanding, integrating, and utilizing both visual and textual knowledge in generation tasks. Models like BLIP (Li et al., 2022, 2023), LLaVA (Liu et al., 2023a), and Flamingo (Alayrac et al., 2022) build the MLLMs by combining pre-trained vision encoders with Large Language Models (LLMs) to process multi-modal inputs for generation. Thriving on the advancements in MLLMs, researchers pay more attention to extending the advantages of Retrieval-Augmented Generation (RAG) to these MLLMs, enhancing their generation capability.\nMulti-modal RAG has demonstrated its potential to enhance knowledge-intensive and information- seeking tasks, such as question answering (Chang et al., 2022; Marino et al., 2019) and fact verifi- cation (Mishra et al., 2022). These models utilize retrieval-based multi-modal documents to provide richer and contextually relevant information. Ad- ditionally, other works have applied multi-modal RAG to improve the performance of MLLMs on the tasks like image captioning (Lin et al., 2014; Young et al., 2014) and generation (Yasunaga et al., 2023; Yu et al., 2023a; Sharifymoghaddam et al., 2024). However, existing multi-modal bench- marks (Johnson et al., 2017; Schuhmann et al., 2021; Lin et al., 2014; Young et al., 2014; Marino et al., 2019) are typically tailored to specific tasks and lack a comprehensive framework for evaluating multi-modal RAG systems."}, {"title": "M\u00b2RAG Benchmark for Multi-Modal Retrieval-Augmented Generation", "content": "In this section, we describe our Multi-Modal Retrieval-Augmented Generation (M\u00b2RAG) bench- mark. We first introduce the RAG tasks in M2RAG, followed by a detailed explanation of the construc- tion process. Finally, we present a comparison of existing multi-modal benchmarks with M2RAG.\nTask Definition. As shown in Figure 2, M\u00b2RAG defines four tasks to evaluate the capabilities of MLLMs in open-domain RAG scenarios: image captioning, multi-modal question answering, multi- modal fact verification, and image reranking. For each task, MLLMs are required to retrieve knowl- edge from the multi-modal document collection D and generate responses to answer the question q. Details of the prompt templates of different tasks are shown in Appendix A.3.\nImage Captioning Task. Image Captioning is a widely used task for evaluating the performance of multi-modal RAG models (Aghajanyan et al., 2022; Sharifymoghaddam et al., 2024). In this task, an image is provided as the query q, and the document collection D is constructed using image documents that contain captions. The goal of image captioning is to generate concise and semantically coherent captions that accurately describe the image content. Unlike previous works (Aghajanyan et al., 2022; Sharifymoghaddam et al., 2024), we source image captions from WebQA (Chang et al., 2022), where all image documents are collected from Wikimedia Commons. These captions often include important details, such as named entities, which make the task more challenging and provide crucial informa- tion for query matching (Liu et al., 2023b). More comparison details of the image captioning tasks in different benchmarks are shown in Appendix A.4.\nMulti-Modal Question Answering Task. Follow- ing the WebQA benchmark (Chang et al., 2022), the Multi-Modal QA task involves answering text- based queries q by leveraging both text and image documents. The document collection D consists of both text and image documents containing cap- tions. Additionally, we extend WebQA to an open- domain setting by instructing the retriever to return query-relevant documents from the collection D, as demonstrated by Liu et al. (2023b).\nMulti-Modal Fact Verification Task. The Multi- Modal Fact Verification task challenges MLLMS to verify the accuracy of claims using multi-modal evidence. In this task, the query q can be a multi- modal claim, and the document collection D con- sists of both text and image documents, where the image documents do not contain captions. The rela- tionship between the claim and the evidence is cat- egorized into three possible outcomes: \u201cSupport"}, {"title": "Instruction Tuning for Multi-Modal Retrieval-Augmented Generation", "content": "In this section, we present our Multi-Modal Retrieval-Augmented Instruction Tuning (MM- RAIT) method. First, we describe the framework for multi-modal Retrieval-Augmented Generation (RAG) (Sec. 4.1). Then, we introduce multi-task instruction tuning to enhance the performance of MLLMs in multi-modal RAG tasks (Sec. 4.2)."}, {"title": "The Framework of Multi-Modal Retrieval-Augmented Generation", "content": "Given a query q, multi-modal RAG models first em- ploy a retriever to search for query-relevant multi- modal documents D and then feed these documents to MLLMs to assist them in answering the query q. Each document d \u2208 D can be either an image document or a text document. The multi-modal RAG framework consists of two main components: the multi-modal retrieval module and the retrieval- augmented generation module."}, {"title": "Multi-Modal Retrieval", "content": "To retrieve documents from the multi-modal document collection D, ex- isting methods typically rely on multi-modal dense retrieval models (Zhou et al., 2024b,a).\nGiven a query q and a multi-modal document d, multi-modal dense retrieval models, such as VISTA (Zhou et al., 2024a), encode both as repre- sentations hq and ha, respectively, and map them into an embedding space for retrieval:\n\\(h_q = Enc(q); h_d = Enc(d),\\) (1)\nwhere Enc denotes the encoder model. The query q can be either text or an image, and the multi- modal document d can be a text document or an image document. For documents containing cap- tions, both image features and image captions are fed into the encoder model.\nNext, we compute the similarity score S(q, d) between the representations hq and ha of the query and document:\n\\(S(q, d) = Sim(h_q, h_d),\\) (2)\nwhere Sim denotes cosine similarity. We then per- form a KNN search (Johnson et al., 2019) to re- trieve the top-k most relevant multi-modal doc- uments \\(D = {d_1,..., d_k}\\) to the query q. Dur- ing retrieval, the multi-modal retriever needs to conduct single-modality matching, cross-modality matching and modality routing in the embedding space (Liu et al., 2023b).\nMulti-Modal RAG Module. After retrieval, we input the retrieved documents D and query q into the MLLM (M), such as MiniCPM-V (Yao et al., 2024) or Qwen2-VL (Wang et al., 2024), to generate the output y:\n\\(y = M(D, q).\\) (3)\nThese retrieved documents provide external knowl- edge, which helps to update the parametric memory of the MLLM, enabling it to generate more accu- rate responses to the query q."}, {"title": "MM-RAIT: Multi-Task Multi-Modal Instruction Tuning for MLLMS", "content": "To adapt MLLMs to the multi-modal RAG scenario, we propose the Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT) method, designed to further enhance the performance of MLLMs across various RAG tasks.\nTo improve the MLLM generation process, we incorporate external knowledge to assist in answer- ing the query (Eq.3). Specifically, we follow pre- vious work (Ram et al., 2023) and concatenate the representations of the retrieved documents D along with the query q as the input for the MLLM (M) to generate the output y:\n\\(y = M(Instruct_p, X(D), q),\\) (4)\nwhere Instructp is the instruction for the task p, and X(D) denotes the concatenation of the representa- tions of the retrieved documents:\n\\(X(D) = X(d_1) \\oplus \\cdots \\oplus X(d_k).\\) (5)\nFor the i-th retrieved document di, its representa- tion can be the text sequence for a text document, the image features for an image document, or the concatenation of both image features and caption for an image document that contains a caption.\nNext, we gather queries from three tasks to form the query set Q: image captioning, multi-modal question answering, and multi-modal fact verifica- tion. For each query q in these tasks, the training objective for the model is to minimize the negative log-likelihood of generating the target sequence y*:\n\\(L = - \\sum_{q \\in Q} \\sum_{t=1}^{T} log P(y^*_t | y_{<t}, D, q; \\theta),\\) (6)\nwhere T is the length of the ground truth response, y is the t-th token of the ground truth response, and \\theta represents the parameters of MLLM (M)."}, {"title": "Experimental Methodology", "content": "This section outlines the datasets, evaluation met- rics, baselines, and implementation details used in our experiments.\nDataset. We use the M2RAG dataset to eval- uate the performance of different MLLMs in the multi-modal RAG scenario. The dataset consists of four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and im- age reranking. For multi-modal retrieval, we adopt VISTA (Zhou et al., 2024a), a universal embed- ding model to search for query-related documents. VISTA integrates image token embeddings into the BGE Text Embedding (Xiao et al., 2024) frame- work, enabling flexible processing of the inputs of both text and image data.\nEvaluation Metrics. For image captioning and multi-modal QA tasks, we use BLEU (Pa- pineni et al., 2002), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015) scores to assess performance. In the multi-modal fact verification task, we evaluate the performance of different RAG models using accuracy (ACC) and F1 score. For the image reranking task, we use the Fr\u00e9chet Incep- tion Distance (FID\u2193) (Heusel et al., 2017)\u00b9.\nBaselines. We compare our models with two multi-modal baselines: MiniCPM-V 2.6 (Yao et al., 2024) and Qwen2-VL (Wang et al., 2024). These models are evaluated in a zero-shot setting to as- sess their effectiveness in leveraging multi-modal knowledge from the input context. We feed the top- 1, top-3, and top-5 ranked documents into these MLLMs to evaluate their RAG performance.\nImplementation Details. We apply Low-Rank Adaptation (LoRA) (Hu et al., 2022) to fine-tune both MiniCPM-V 2.6 and Qwen2-VL using the top- 5 retrieved multi-modal documents for 2 epochs. The batch size is 4, with a maximum token limit of 4,096. A cosine learning rate scheduler is used, with the learning rate set to le \u2013 6 for MiniCPM-V and 1e-4 for Qwen2-VL. We fine-tune Qwen2-VL using LLaMA-Factory (Zheng et al., 2024) and set max_pixels=512 \u00d7 512 for training and inference."}, {"title": "Evaluation Result", "content": "In this section, we first evaluate the performance of MLLMs on the M\u00b2RAG benchmark. We then conduct ablation studies to assess the impact of varying numbers of retrieved documents of differ- ent modalities. Following that, we analyze the role of different retrieval modalities in RAG models. Finally, case studies are shown."}, {"title": "Overall Performance", "content": "As shown in Table 2, we report the performance of various RAG models on the M2RAG benchmark. The vanilla RAG models directly use retrieved doc- uments to augment LLMs, while MM-RAIT mod- els fine-tune MLLMs within the RAG framework.\nFor these vanilla RAG models, performance gen- erally improves as the number of retrieved doc- uments increases. However, when retrieving the top-5 ranked documents, the overall performance of vanilla RAG models on most tasks is lower com- pared to using top-1 or top-3 documents. This sug- gests that vanilla LLMs still struggle to fully lever- age multi-modal knowledge to enhance MLLMs. Although some related works also use image cap- tioning tasks to evaluate RAG performance (Shar- ifymoghaddam et al., 2024), the performance of these MLLMs on M2RAG is considerably worse, indicating that M\u00b2RAG offers a more challenging dataset for image captioning. In contrast to vanilla RAG models, both MiniCPM-V 2.6 and Qwen2- VL demonstrate strong performance across all tasks on the M2RAG benchmark after training with MM- RAIT. Specifically, MiniCPM-V 2.6 achieves an av- erage improvement of over 27% across all tasks in M\u00b2RAG, while Qwen2-VL shows an even greater improvement of 34%. These results highlight the effectiveness of MM-RAIT, showcasing its ability to help MLLMs better utilize multi-modal contexts to enhance their performance."}, {"title": "Ablation Study", "content": "As shown in Table 3, we conduct ablation stud- ies to evaluate RAG effectiveness with retrieved documents of different modalities and numbers. Specifically, we conduct two evaluation settings to evaluate the roles of different modalities: Only Text and Only Image. Only Text indicates removing all image features from multi-modal input contexts to enhance the MLLM, while Only Image removes all texts from top-ranked multi-modal documents. Compared with the RAG models using top-3 ranked multi-modal documents for augmentation, the performance of vanilla RAG models usually de-\ncreases with top-5 ranked documents, while MM- RAIT alleviates the performance decreases but also shows limited improvements. It illustrates that effectively using the multi-modal contextual knowledge is still challenging for existing MLLMs. Moreover, we further remove all texts or image features to show the roles of different modalities in RAG modeling. For all tasks, the RAG perfor- mance of the Only Text model slightly decreases, showing that these texts contribute to the primary knowledge source for these RAG models. After adding the image features, the RAG performance usually increases, showing that these image fea-\ntures can improve the performance of RAG models. Even though different modalities show the effec- tiveness in multi-modal RAG modeling, it is still hard to effectively learn more crucial semantics from these image features to improve the RAG performance within multi-modal contexts."}, {"title": "Effectiveness of MLLMs in Different Modality-based RAG Scenarios", "content": "In this experiment, we investigate the impact of retrieved documents from different modalities on the effectiveness of RAG models.\nAs shown in Figure 3, we divide the multi-modal QA dataset of M\u00b2RAG into two groups: image- answerable queries and text-answerable queries. These categories represent queries that can be an- swered by image or text documents, respectively. We compare both vanilla RAG and MM-RAIT, im- plemented using MiniCPM-V and Qwen2-VL. Top- 5 ranked documents from texts, images, and both modalities are fed to the different RAG models to evaluate the QA performance.\nFigures 3(a) and 3(b) present the RAG perfor- mance on text-answerable queries. Overall, the RAG models using multi-modal retrieved docu- ments exhibit comparable performance to those using only text-based documents, indicating that MLLMs can effectively learn from text documents to answer queries. Notably, vanilla RAG models show minimal differences in performance when using text, image, or both types of documents, whereas MM-RAIT significantly improves perfor- mance when leveraging documents from multiple modalities. This highlights the effectiveness of MM-RAIT in enabling MLLMs to learn from multi- modal contexts. Interestingly, vanilla MLLMs ap- pear insensitive to the retrieved contexts, likely because they rely heavily on internal knowledge"}, {"title": "Case Study", "content": "In this section, we show two cases from Qwen2-VL in the Multi-Modal QA task of M\u00b2RAG to evaluate the effectiveness of the MM-RAIT method within the multi-modal retrieval contexts. More cases are shown in Appendix A.5.\nAs illustrated in Figure 4, in the first case, the question asks, \"What animal is included in the painting of John Campbell, 1st Baron Cawdor?\". This requires the MLLM to match the \u201c1st Baron Cawdor\" and extract information about animals in the painting. Due to limited internal knowl- edge, the model encounters hallucination issues and generates an incorrect answer, \u201ca lion\". When the retrieved multi-modal document of \"1st Baron Cawdor\" is fed into the MLLM, the vanilla RAG model can directly extract \u201cdog\u201d from the painting, thus providing the correct response. This highlights the importance of multi-modal information in offer- ing more intuitive and richer semantic insights to answer the question, underscoring the effectiveness of constructing the M2RAG benchmark.\nIn the second case, the question asks that, \u201cWhat weapon is the man in Daniel Maclis's A Scene from \u2018Undine' (detail) holding?\u201d Based on retrieved doc- uments, the vanilla RAG model focuses on the fifth document, which depicts a \"Scottish dirk\u201d. This leads the vanilla RAG model to generate an incor- rect response, \u201cholding a dirk\u201d. After MM-RAIT training, the model can accurately identify the rele- vant document describing the man holding a sword and extract pertinent information from it, thereby generating the correct response."}, {"title": "Conclusion", "content": "This paper introduces Multi-Modal Retrieval- Augmented Generation (M\u00b2RAG), a benchmark designed to evaluate MLLMs with retrieved multi- modal contexts across four tasks. To further en- hance the utilization of retrieved information, we also propose a Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT) method, which optimizes MLLMs with multi-modal contexts as inputs, thereby improving their ability to effectively utilize retrieved information."}, {"title": "Limitations", "content": "Although our M\u00b2RAG benchmark includes four common multi-modal tasks, incorporating addi- tional tasks can provide a more comprehensive evaluation of the capabilities of MLLMs. Further- more, while MLLMs perform satisfactorily within retrieved multi-modal contexts, they still rely pre- dominantly on textual data for some tasks. Find- ing ways to enable MLLMs to more effectively leverage multi-modal contexts remains a critical challenge that requires further exploration. Ad- ditionally, due to the performance limitations of multi-modal retrieval models, the quality of the retrieved multi-modal documents directly impacts the overall performance of MLLMs. Improving the accuracy of multi-modal retrieval remains a vital area for future research."}]}