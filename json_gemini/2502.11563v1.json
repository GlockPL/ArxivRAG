{"title": "Leader and Follower: Interactive Motion Generation under Trajectory\nConstraints", "authors": ["Runqi Wang", "Caoyuan Ma", "Jian Zhao", "Hanrui Xu", "Dongfang Sun", "Haoyang\nChen", "Lin Xiong", "Zheng Wang", "Xuelong Li"], "abstract": "With the rapid advancement of game and film pro-\nduction, generating interactive motion from texts\nhas garnered significant attention due to its poten-\ntial to revolutionize content creation processes. In\nmany practical applications, there is a need to im-\npose strict constraints on the motion range or trajec-\ntory of virtual characters. However, existing meth-\nods that rely solely on textual input face substan-\ntial challenges in accurately capturing the user's\nintent particularly in specifying the desired tra-\njectory. As a result, the generated motions often\nlack plausibility and accuracy. Moreover, exist-\ning trajectory-based methods for customized mo-\ntion generation rely on retraining for single-actor\nscenarios, which limits flexibility and adaptabil-\nity to different datasets, as well as interactivity in\ntwo-actor motions. To generate interactive motion\nfollowing specified trajectories, this paper decou-\nples complex motion into a Leader-Follower dy-\nnamic, inspired by role allocation in partner danc-\ning. Based on this framework, this paper explores\nthe motion range refinement process in interactive\nmotion generation and proposes a training-free ap-\nproach, integrating a Pace Controller and a Kine-\nmatic Synchronization Adapter. The framework\nenhances the ability of existing models to gener-\nate motion that adheres to trajectory by control-", "sections": [{"title": "1 Introduction", "content": "With advances in game and film production, generating in-\nteractive motion from textual descriptions has gained signifi-\ncant attention due to its potential to replace traditional motion\ncapture and drive the transformation and enhancement of con-\ntent creation workflows [Guo et al., 2022; Zhang et al., 2023;\nLin et al., 2023]. In practical applications such as fight chore-\nography and animation production, characters often need to\nperform actions along specific trajectories or within desig-\nnated areas. Therefore, generating motions that not only sat-\nisfy the textual pose requirements but also adhere to the spec-\nified trajectory has become an important research direction.\nCurrently, methods for generating interactive motion con-\nditioned on both text and trajectory can be broadly catego-\nrized into two types. The first category directly incorporates\ntrajectory descriptions into the text conditions (see Fig.1(a)).\nHowever, the text-based descriptions of trajectories are im-\nprecise, and in multi-agent scenarios, issues such as inter-\npenetration arise. The second category consists of trajectory\nconstraint methods for single-actor motion [Wan et al., 2023;\nXie et al., 2023; Karunratanakul et al., 2023; Song et al.,\n2024], which typically require retraining based on different"}, {"title": "2 Related Work", "content": "2.1 Interactive Motion Generation\nRecently, various methodologies [Liang et al., 2024; Zhou et\nal., 2023; Wang et al., 2023; Ghosh et al., 2024; Xu et al.,\n2024; Lee et al., 2024; Shan et al., 2024; Li et al., 2024;\nJang et al., 2024] have been developed for Interactive Mo-\ntion Generation with humans. However, existing methods\nprimarily depend on textual prompts to guide motion genera-\ntion, which limits their ability to control complex interactive\nmovements, especially in terms of trajectory control. Our ap-\nproach leverages key trajectory constraints to enable precise\ncontrol over interactive motion generation, enhancing adapt-\nability to complex scenarios and aligning generated motions\nwith diverse space demands in real-world situations.\n2.2 Trajectory Guided Motion Generation\nIn the field of single-person motion generation, prior stud-\nies have explored the integration of trajectories into the hu-\nman motion generation process. GMD [Karunratanakul et\nal., 2023] proposed a two-stage framework that first gener-\nates trajectories satisfying keyframe constraints and then syn-\nthesizes complete motions. OmniControl [Xie et al., 2023]\ndemonstrated the capability to control multiple joints using\na single model, but it may produce unnatural motions when\nspatial control signals for different joints conflict. TLCon-\ntrol [Wan et al., 2023] employs a VQ-VAE [Van Den Oord\net al., 2017] and a Masked Trajectory Transformer to pre-\ndict motion distributions based on user-specified partial tra-\njectories and textual descriptions. However, these approaches\nare confined to single-person motion generation and fail to\naccount for the interactive dynamics of multi-person move-\nments. Moreover, they often require redesigning and retrain-\ning models for different tasks, architectures, or new datasets.\nWe pioneer the exploration of diffusion models in the context\nof interactive-person motion generation, introducing trajecto-\nries into this domain for the first time without necessitating\nany additional retraining of existing models."}, {"title": "3 Method", "content": "3.1 Preliminary\nDiffusion-based probabilistic generative models (DPMs) are\nmodeled as a Markov noising process, where the input xt\nis progressively noised with varying noise levels t. The\nnoising process is defined cumulatively as $q(x_t|x_0) = \\mathcal{N}(\\sqrt{\\alpha_t}x_0, (1 - \\alpha_t)I)$, where xo is the clean input, $\\alpha_t =\n\\prod_{s=1}^{t} (1 - \\beta_s)$, and \u1e9et is a noise scheduler. The denoising\nmodel $p_\\theta(x_{t-1}|x_t)$ with parameters @ learns to reverse the\nnoising process by modeling the Gaussian posterior distri-\nbution $q(x_{t-1}|x_t, x_0)$. After T successive denoising steps,\ndiffusion models can map a prior distribution $\\mathcal{N}(0, I)$to any\ntarget distribution p(x).\nWe model a two-person interaction x as a collection of two\nsingle-person motion sequences xh, i.e., x = {xa, xb}, where\nxh = {$x_i$}$_{i=1}^{L}$ is a fixed-framerate sequence of motion poses\nXi. Multi-Human Motion Generation [Liang et al., 2024]\nis based on a fundamental assumption, commutative prop-\nerty, which means that two-person {xa, xb} and {xb, xa} are\nequivalent, the order of every single motion does not change\nthe semantics of the interaction itself, the distribution of in-\nteraction data satisfies the following property:$p(x_a,x_b) =\np(x_b, x_a)$. Based on this principle, Multi-Human Motion\nGeneration [Liang et al., 2024] addresses the symmetry\nof human identities in interaction by utilizing cooperative\ntransformer-based denoisers with shared weights.\nThe base model InterGen [Liang et al., 2024] is designed\nto ensure that interactive motion adheres to physical laws and\nits process characteristics (such as interactivity). To ensure\nphysical plausibility and consistency, geometric loss func-\ntions such as foot contact, joint velocity, and bone length loss\nare used, following the approach in MDM [Tevet et al., 2023].\nAdditionally, interactive losses, including masked joint dis-\ntance map (DM) loss and relative orientation (RO) loss, are\nintroduced to address spatial complexity in Multi-Human in-\nteractions [Liang et al., 2024].\n3.2 Pace Controller\nThe Pace Controller aims to guide the leader's trajectory\nto align with the input conditions. However, the bidirec-"}, {"title": "Algorithm 1 Pace Controller", "content": "Input: Initial motion $x_t = {x_t^a, x_t^b}$, ground truth trajectories\n$x^{proj}_{traj}$, trajectory period $[T_1, T_2]$, Trajectory loss function $G_t(\\cdot)$\nParameter:Motion diffusion model $M_\\theta$\nOutput:Motion $x_0 = {x_0^a, x_0^b}$\n\nfor t = 1 to T do\nif $t \\in [T_1, T_2]$ then\n$x_t^a \\leftarrow {x_t^a, x_t^b} - x^{proj}_{traj}(t)$\nend if\n$x_0 \\leftarrow M_\\theta(x_t)$\n$x_0 \\leftarrow G_{opt}(x_0; ||x_0 - x_{traj}^{proj}(t)||_2)$\n$\\mu, \\Sigma \\leftarrow \\mu(x_0, x_t), \\Sigma_t$\n$x_{t-1} \\sim \\mathcal{N}\u2019(\\mu, \\Sigma)$\nend for\n\nreturn Optimized motion $x_0 = {x_0^a, x_0^b}$.\nguidance. Specifically, inspired by nonequilibrium thermo-\ndynamics, the diffusion model establishes a Markov chain\nbetween the target data distribution and a Gaussian distribu-\ntion. In this process, data interactions between the root joint\nand other joints influence each other (i.e., the root joint data\nis also modified by other joints), gradually transitioning from\nthe Gaussian distribution to the target distribution. Therefore,\nthe core challenge of Pace Controller lies in effectively ensur-\ning the accuracy of leader's root joint trajectories amidst such\ninteractions, protecting them from interference by other joint\ncomponents, and leveraging trajectory information to guide\nand refine the generation of leader motion effectively.\nBased on the above analysis, we propose a unidirectional\ndiffusion-guided strategy to ensure the leader's trajectory\nconsistency. During the trajectory formation phase, we em-\nploy a strategic intervention to directly replace the relevant\nportion of xt at time step t with the real trajectory segment\nfrom the input condition $x^{proj}_{traj}$. This replacement ensures that\nthe trajectory segment associated with the leader is main-\ntained with absolute accuracy at the start of the denoising\nprocess.\n$x_t = (x_t \\leftarrow x^{Proj}) \\cdot I_{[T_1,T_2]}(t) + X_t \\cdot I_{[T_1,T_2]^=(t)$, (1)\nwhere $I_{[T_1,T_2]}(t)$ equals 1 if $t \\in [T_1, T_2]$ and 0 otherwise,\nwhile $I_{[T_1,T_2]^=(t)$ equals 1 if t \u00a2 [T_1, T_2] and 0 otherwise.\nMoreover, throughout the reverse denoising process, for\neach predicted clean state xo at time step t, we extract the\ntrajectory portion and compute the per-frame MSE loss with\nrespect to the given trajectory condition, thereby optimizing\nthe current state prediction. After optimization, the updated\nstate xo is further involved in the diffusion process, leading\nto an improvement in the accuracy of the final generated tra-\njectory.\n$X_0 = G_{opt} (x_0; ||x_0 - x^{proj} (t)||_2)$, (2)\n$G_{opt}$ represents the optimization function, applied to mini-\nmize MSE loss and update the current state xo.\n3.3 Kinematic Synchronization Adapter\nForcibly modifying the trajectory of the leader during an in-\nteraction may adversely affect the interactivity and compati-"}, {"title": "ity of both participants. To address this issue, we have de-\nsigned the Kinematic Synchronization Adapter, which lever-\nages the interactive relationship between two participants to\nensure the motion of the follower remains consistent and rea-\nsonable. Specifically, we first develop a conflict detection\nmodule that utilizes the SMPL model [Loper et al., 2023] to\ndelineate the interaction domain between two bodies.", "content": "$C_t = I_{|D_L \\bigcap D_F|>0}$ (3)\nwhere $D_L = M(x_t^a)$ and $D_F = M(x_t^b)$ represent the in-\nteraction domains of Leader xt and Follower xt, at time step\nt, obtained through the SMPL model mapping. Upon detect-\ning an overlap within this domain, the position state of the\nfollower is adjusted.\n$x_t^b = I_{C_t=\\emptyset} (x_t^b) + I_{C_t \\neq \\emptyset} (F_{opt} (x^b; S(x_t^a,x_t^b)))$ (4)\n$I_{C_t \\neq \\emptyset}$ and $I_{C_t=\\emptyset}$ are indicator functions. The optimal ad-\njustment function $F_{opt}$ modifies the trajectory of the fol-\nlower based on the Leader-Follower interactive relationship\n$S(x_t^a,x_t^b)$ between the participants. We still opt for 2-3 de-\nnoising steps during the trajectory formation phase, applying\nreverse motion to the detected colliding individuals first and\nthen imposing a joint distance loss function to constrain the\nrelative position between the two individuals. The distance\nloss function may lead to highly similar human movements\nat the end frame of the motion. Therefore, we introduce a\ndiscrepancy velocity loss function as a penalty term.\n$S(x_t^a, x_t^b) = L_{joint} + L_{velocity}$ (5)\n$L_{joint} = \\sum_{j} ||max(0,8 \u2013 ||P_a(t)(j) \u2013 P_b(t)(j)||_2)^2||_2$ (6)\n$L_{velocity} = \\sum_{t=1}^{n-1} \\sum_{j=1}^{22} \\frac{(v_a (t, j) \\cdot v_b (t, j))}{\n||v_a(t, j)||_2 ||v_b(t, j)||_2}$ (7)\nThe function $S(x_t^a, x_t^b)$ represents the combined loss func-\ntion, consisting of the joint distance loss $L_{joint}$ and the veloc-\nity loss $L_{velocity}$. The joint distance loss is used to constrain\nthe relative positions of the Leader and Follower at each time\nstep, ensuring that the joints of the two participants main-\ntain an appropriate distance throughout the interaction. The\nvelocity loss applies a similarity penalty to prevent the ho-\nmogenization of the two participants in the final frames, thus\npreserving the dynamic nature of the interaction."}, {"title": "4 Experiment", "content": "Evaluation dataset. We utilize the InterHuman [Liang et al.,\n2024] as our primary dataset. This dataset comprises 7,779\nmotions spanning various categories of human motions, an-\nnotated with 23,337 unique descriptions containing 5,656 dis-\ntinct words, and encompassing a total duration of 6.56 hours.\nEvaluation metrics. We adopt the evaluation metrics con-\nsistent with InterGen [Liang et al., 2024], which are listed as\nfollows:\n1. R-Precision. To measure the text-motion consistency,\nwe rank the Euclidean distances between the motion and\ntext embeddings. Top-1, Top-2, and Top-3 accuracy of\nmotion-to-text retrieval are reported."}, {"title": "4.1 Implementation Details.", "content": "We implement InterGen [Liang et al., 2024] as the baseline\nmodel, which consists of N = 8 blocks, each with a latent\ndimension of 1024. Each attention layer within the model is\ncomposed of 8 heads. During training, the number of diffu-\nsion timesteps is set to 1,000. For sampling, we apply the\nDDIM [Song et al., 2020] strategy with 50 timesteps and set"}, {"title": "4.2 Comparison with State-of-the-art Methods", "content": "Table 1 presents a comparison between our method and other\nstate-of-the-art approaches across multiple dimensions. In\nterms of R-Precision, our method significantly outperforms\nexisting methods across all Top-K evaluations, indicating\nthat the trajectory-guided signal provides clearer spatiotem-\nporal alignment information, which effectively improves the\nmatching between the generated motion and the target ref-\nerence. Regarding the FID metric, our method further re-\nduces to 5.352 compared to InterGen [Liang et al., 2024],\nhighlighting the crucial role of the trajectory signal in guid-\ning interactive motion generation. This signal enhances both\nthe consistency and realism of the generated motion. In terms\nof diversity, our method achieves high diversity scores, sug-\ngesting that although the trajectory imposes some constraints\non the generated samples, it remains a low-dimensional sig-\nnal and does not limit the diversity of the generated motions.\nHowever, the lower MM modality score of 1.174 indicates\nthat while trajectory guidance enhances the spatiotemporal\nconsistency of the generated samples, it also somewhat sup-\npresses the divergence of the multimodal distribution in the\ngenerated results."}, {"title": "4.3 Qualitative Results", "content": "Analysis of Visual Comparison with Other Methods. Fig-\nure 4 illustrates the visual results of our method compared to\nother approaches. In complex scenarios requiring close con-\ntact and interaction, such as martial arts combat and Latin\ndance, the baseline model generates unnatural overlaps, in-\ntersections, or penetrations (highlighted by red circles), com-\npromising motion authenticity and fluidity. In the martial arts\nscene, overlapping actions between characters violate physi-\ncal and behavioral logic, while in the Latin dance scene, pen-\netration and distortion disrupt the dance's fluidity and aes-\nthetic. By incorporating trajectory range and distance con-"}, {"title": "straints, our method effectively resolves these issues, ensur-\ning spatiotemporal consistency in the generated motions. The\nresulting actions are not only more natural and realistic but\nalso adhere to physical laws and behavioral logic, exhibiting\nenhanced interactivity and coherence.\nAnalysis of Trajectory Guidance.", "content": "Figure 5 presents the\nmotion sequences generated under the same text (Two peo-\nple are dancing together) with different trajectory signals for\nguidance. The experimental results indicate that the proposed\nmodel effectively leverages various trajectory signals to gen-\nerate motion sequences that align with both specific textual\nand trajectory features. By incorporating diverse trajectory\nconditions, the model not only generates richer motion se-\nquences but also ensures that the introduction of trajectory\nsignals does not cause distortion or unreasonable outcomes.\nEven under trajectory constraints, the generated sequences\nmaintain high-quality fluidity and consistency, demonstrat-\ning an excellent balance between diversity and consistency,\nsurpassing the capabilities of existing methods."}, {"title": "4.4 Ablation Studies", "content": "Analysis of the Trajectory Guidance Period. Figure 6 illus-\ntrates the impact of different trajectory guidance periods on\nthe final generated results. We selected several time intervals\nduring the denoising process as trajectory guidance and re-\nported R-Precision and FID metrics. The results indicate that\nwhen the trajectory guidance period is set to the trajectory\nformation phase (0.7T \u2264 t \u2264 0.3T), the generated results\nexhibit optimal performance in both semantic consistency (R-\nPrecision) and action realism (FID). In contrast, when trajec-\ntory guidance is applied during the early stage of the denois-\ning process (0.8T \u2264 t \u2264 0.2T), the accumulated redundant\nnoise during this stage leads to chaotic generation, signifi-\ncantly reducing semantic consistency. Additionally, when the\ntrajectory guidance period is too short (0.6T \u2264 t \u2264 0.4T), it\nis insufficient to provide the necessary semantic guidance, re-\nsulting in a decrease in the alignment between the generated\nactions and the input conditions.\nAnalysis of Trajectory Guidance of Different Individuals.\nTable 2 shows the results of motion generation for the trajec-\ntories of different individuals as input conditions. Due to the\nequivalence of two-actor motion, using the trajectory of either\nindividual as the input condition results in minimal differ-\nences. In this case, the FID and R-Precision yield consistent\nresults, achieving both realistic motion generation and a high\ndegree of textual alignment. However, when both individuals'\ntrajectories are used as input conditions simultaneously, FID\nand R-Precision worsened. This is because the constraints\nimposed on both actors' conditions are excessively rigid, po-\ntentially causing a misalignment with the textual description,\nleading to unnatural generation outcomes such as model clip-\nping. Therefore, further refinement of the constraint condi-\ntions is necessary to ensure the coherence and naturalness of\nthe interactive motion.\nAnalysis of Joint Loss. Figure 7 illustrates the corrective\neffect of Joint Loss within our Kinematic Synchronization\nAdapter. We select a textual description with close contact\nbetween individuals. The introduction of Joint Loss effec-\ntively constrains the spatial relationship between the two, im-\nproving the plausibility of relative joint positions and prevent-\ning potential collisions in intermediate frames during the mo-\ntion correction by the Pace Controller. To quantitatively eval-\nuate this effect, we employed a collision detection module to\nanalyze the number of penetration frames across 210 frames\nof motion before and after correction. Our correction reduce\nthe number of penetration frames by nearly half.\nAnalysis of Velocity Loss. Figure 8 (a) illustrates the effect"}, {"title": "of the Velocity Loss correction within the Kinematic Syn-\nchronization Adapter.", "content": "When only joint loss is applied, the\nexperiment reveals an issue in the final frames: the corrected\ncharacter undergoes an abrupt reversal, and the motions of\nboth characters gradually converge. Upon introducing Ve-\nlocity Loss, as shown in Figure 8 (b), the diversity penalty\neffectively prevents the homogenization of the last frames,\nensuring that the movements remain natural and coherent."}, {"title": "Analysis of Inference Time.", "content": "Table 3 presents results re-\ngarding inference time. We randomly select the same 20\nprompts, generate 210 frames for each movement, and per-\nform the computation using an Nvidia RTX 3090 GPU, re-\nporting the average inference time. The base model employs\nthe DDIM sampling method, which facilitates rapid gener-\nation. The introduction of Pace Controller and Kinematic\nSynchronization Adapter does not significantly increase the\ncomputational overhead during inference; the inference time\nonly increases marginally, by approximately four seconds."}, {"title": "5 Conclusion", "content": "This paper investigates the problem of generating multi-\nperson interactive motions based on textual descriptions and\n3D trajectory conditions. Inspired by the role allocation in\npartner dances, the study introduces a Lead-Follow Paradigm"}]}