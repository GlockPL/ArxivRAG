{"title": "Video-CCAM: Enhancing Video-Language Understanding with Causal Cross-Attention Masks for Short and Long Videos", "authors": ["Jiajun Fei", "Dian Li", "Zhidong Deng", "Zekun Wang", "Gang Liu", "Hui Wang"], "abstract": "Multi-modal large language models (MLLMs) have demonstrated considerable potential across various downstream tasks that require cross-domain knowledge. MLLMs capable of processing videos, known as Video-MLLMs, have attracted broad interest in video-language understanding. However, videos, especially long videos, contain more visual tokens than images, making them difficult for LLMs to process. Existing works either downsample visual features or extend the LLM context size, risking the loss of high-resolution information or slowing down inference speed. To address these limitations, we apply cross-attention layers in the intermediate projector between the visual encoder and the large language model (LLM). As the naive cross-attention mechanism is insensitive to temporal order, we further introduce causal cross-attention masks (CCAMs) within the cross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a straightforward two-stage fashion: feature alignment and visual instruction tuning. We develop several Video-CCAM models based on LLMs of different sizes (4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows outstanding performance from short videos to long ones. Among standard video benchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding per- formances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA, MSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos, Video-CCAM models can be directly adapted to long video understanding and still achieve exceptional scores despite being trained solely with images and 16-frame videos. Using 96 frames (6\u00d7 the training number of frames), Video-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among all open-source Video-MLLMs, respectively. We provide a theoretical analysis of its temporal consistency and emphasize several key factors in its architecture through exper- iments. We hope that Video-CCAM can serve as a straightforward yet robust baseline for future Video-MLLM development. The code is publicly available in https://github.com/QQ-MM/Video-CCAM.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) such as GPT-4 [33], Gemini [44], and LLaMA3 [32], have signifi- cantly reshaped the landscape of artificial intelligence, profoundly impacting our daily lives. These LLMs can engage in text-based conversations with users, meeting their needs and completing specific tasks [61]. Despite their potential as a step towards artificial general intelligence (AGI) assistants, their capabilities are confined to processing natural language. However, human interaction with the world is not limited to language alone; it also encompasses a variety of multi-modal information, such as vision, speech, audio, etc.\nTo address the language-only limitation, the research community has recently proposed various multi-modal large language models (MLLMs) that integrate additional modalities. Visual modality, especially image, has garnered considerable interest among all modalities. Notable developments include Flamingo [3], which combines pre-trained vision and language models, exhibiting impressive multi-modal few-shot learning capabilities. MiniGPT-4 [64] aligns the visual encoder and Q-Former from BLIP-2 [15] with Vicuna [7] through a single trainable projection layer, achieving advanced vision-language performance. LLaVA [26] further introduces the concept of visual instruction tuning and showcases superior multi-modal abilities across various benchmarks. These pioneering approaches have collectively established a standard pipeline for MLLMs, typically including pre- trained large language models, modality-specific pre-trained encoders, trainable projectors, and datasets for feature alignment and instruction tuning. This framework has proven effective for integrating and leveraging image-text data.\nThe research field of MLLMs has recently seen a surge in Video-MLLMs [16, 31, 29, 14, 63, 21, 17]. Compared to images with two spatial dimensions, videos have an additional temporal dimension. Therefore, the number of visual tokens is not only related to the spatial resolution but also proportional to the number of video frames, which is difficult to accommodate within the limited context size of LLM. Existing works address this issue from modifying three components of Video-MLLMs, i.e., the LLM, the visual encoder, and the intermediate projector. Some works directly extend the context size of LLMs to hold more visual tokens. LWM [23] gradually increase the number of frames and the context size through multi-stage vision-language training. LongVA [57] first trains long-context LLM and then aligns it with images. However, the computational burden of long-context LLMs is significantly larger than normal ones. Other works adopt pooling [31], downsampling [19, 29, 60], or clustering [14] to directly reduce the number of visual tokens. These approaches are effective but come at the expense of fine-grained information loss. Unlike MLP projectors that do not alter the number of output tokens, cross-attention based projectors (Perceiver [3] and Q-Former [15]) adopt a fixed number of queries to extract relevant information from visual inputs. For example, VideoChat2 [17] uses 96 queries to process video inputs. However, cross-attention mechanism is insensitive to temporal order, which is crucial for accurate video understanding. Besides, these projectors generally have more parameters than MLP projectors.\nIn this work, we concentrate on the projector to address the abundant visual tokens. Our projector is centered around cross-attention layers, where a fixed number of queries is employed to process videos with different number of frames. This architecture makes it possible to handle extremely large number of video frames at no risk of exceeding the context length. Besides, we make several modifications to better process videos and simplify the training process. First, we propose causal cross-attention masks (CCAMs) within the cross-attention layer, making learnable queries temporally ordered and enhancing the model's video understanding ability. Second, we simplify the projector structure through reducing the number of layers and increasing the number of queries. We encapsulate our contributions as follows:\n\u2022 We propose Video-CCAM, an innovative Video-MLLM designed for advanced video- language understanding. Video-CCAM is a flexible model composed of a visual encoder, an LLM, and a projector, which employs cross-attention mechanism to process videos of variable frames and CCAMs to capture the temporal relationship within videos.\n\u2022 We provide a theoretical analysis on the temporal consistency of CCAM. By treating videos as continuous signals, we demonstrate that the CCAM projector remains consistent for videos with different numbers of frames, making Video-CCAM a reliable Video-MLLM.\n\u2022 We conduct extensive experiments to highlight Video-CCAM's outstanding performance. Among all open-source Video-MLLMs, Video-CCAM ranks 1st in MVBench [17], 1st in VideoVista [20], 1st in MLVU [62], and 3rd in Video-MME [10]."}, {"title": "2 Related Work", "content": "2.1 Image-MLLMS\nImages serve as a vital complement to the visual details that text alone cannot convey, thus playing a crucial role in multi-modal learning. Flamingo [3] leverages a pre-trained, frozen vision encoder to process input images and introduces the GATED XATTN-DENSE layer to integrate visual information into language models. However, the high training costs limit its accessibility. With the advancement of LLMs and the emergence of open-source projects [47, 32, 46, 4], there has been a proliferation of studies utilizing these powerful LLMs to create Image-MLLMs. BLIP-2 [15] introduces the Q-Former that connects frozen visual encoders with LLMs, facilitating various image-to-text tasks, including visual knowledge reasoning and conversation. MiniGPT-4 [64] further refines this approach by aligning the visual encoder and Q-Former in BLIP-2 with Vicuna [7] through a single trainable projection layer, resulting in enhanced downstream performance. LLaVA [26] expands the concept of instruction tuning to the visual domain, proposing visual instruction tuning as a follow-up to feature alignment pre-training. LLaVA and its successors [28, 24, 25] demonstrate significant promise in addressing a variety of vision-language tasks. Recent works [52, 5, 48] enhance the capabilities of MLLMs by innovating model architectures, introducing additional training stages, and curating high-quality training datasets, among other strategies.\n2.2 Video-MLLMs\nAs Image-MLLMs continue to mature, researchers are increasingly focusing on videos. Compared to images, videos have an additional temporal dimension, posing additional difficulties and challenges to Video-MLLMs. Similar to their image counterparts, Video-MLLMs primarily utilize two types of projectors: MLPs and Q-Formers [15]. MLP projectors directly convert visual features from the encoder into embeddings. For instance, Video-ChatGPT [31] employs a linear layer to align spatially and temporally pooled video features with the LLM. PLLaVA [51] proposes an pooling strategy to reduce the domain differences between pre-trained image features and video ones. However, MLPs struggle to handle many frames, often forcing a trade-off between spatial resolution and temporal sampling density. Q-Formers output the same number of tokens as the number of learnable queries, independent of the input size. For example, VideoChat [16] employs additional learnable queries to produce aligned visual embeddings. To address the Q-Former's limited frame differentiation, Vista- LLaMA [30] recursively applies the Q-Former to model the temporal relationships. ST-LLM [27] also applies pre-trained Q-Formers on video frames to obtain compact visual representations. Beyond projectors, Video-MLLMs also face other challenges, particularly regarding the choice of video and image encoders. Since videos are often treated as sequences of images, most studies utilize image encoders to extract frame features, which are subsequently aggregated to represent video features. A majority of works, including VideoChat [16], Video-ChatGPT [31], Valley [29], and Chat-UniVi [14], employ CLIP ViT [40] for processing both video frames and images. Others, such as LLaMA-VID [19], TimeChat [41], and Emu2 [42], opt for EVA CLIP ViT [43] as the visual encoder. Some researchers advocate that pre-trained video encoders are more suitable to capture temporal features. Video-LLaVA [21] underscores the significance of feature alignment across visual modalities and utilizes the visual encoders from LanguageBind [63] for processing visual inputs. UMT-L [18], a pre-trained video foundation model, is employed by VideoChat2 [17] and has shown impressive performance across a range of downstream video-language tasks."}, {"title": "3 Method", "content": "As illustrated in Fig. 1, Video-CCAM consists of three principal components: the visual encoder that processes images and videos, the LLM that handles visual and textual embeddings, and the CCAM projector that connects them.\n3.1 Visual Encoder\nExisting Video-MLLMs generally employ three visual encoding strategies: using an image encoder, a video encoder, or both. In this work, we adopt image encoders for three reasons. Firstly, the generalization capabilities of pre-trained image encoders [40, 43, 54] have been extensively validated, whereas the generalization capabilities of their video counterparts remain underexplored. Secondly, some video encoders have constraints on the number of input frames, whereas image encoders can be applied to arbitrary frames. Video-MLLMs built with these video encoders may give inaccurate responses if the input number of frames is different from that used during training. Lastly, Video- MLLMs with both image and video encoders require additional feature alignment efforts, which are not needed by those with a single encoder. Although most image encoders are not optimized for video processing, we argue that the autoregressive nature of LLMs can compensate for this limitation and enable them to interpret temporal visual tokens effectively.\n3.2 Projector\nThe projector is a crucial intermediary that connects the visual and textual embedding spaces in MLLMs. In this work, we focus on the projector, specifically the cross-attention based projector, to hold the large number of visual tokens in videos. However, naive cross-attention mechanism is insensitive to the temporal order within the video frames, since all queries can attend to all spatial and temporal visual tokens indiscriminately. For simplicity, we focus on one query embedding $Q_i \\in \\mathbb{R}^{1\\times C}$ (0 \u2264 i \u2264 N \u2212 1) and one attention head. We denote the key and value functions as $K, V : \\mathbb{R}^{L\\times C'} \\rightarrow \\mathbb{R}^{L\\times C}$, respectively. For image embeddings with length as L = H \u00d7 W, the output of the cross-attention layer is computed as follows:\n$Y_i = \\frac{\\exp \\left(Q_i K^T(x)\\right) V(x)}{\\exp \\left(Q_i K^T(x)\\right) 1_L}, \\in \\mathbb{R}^{1\\times C}$\nwhere $x \\in \\mathbb{R}^{L\\times C'}$ represents the image embeddings, and $1_L = [1,\\dots,1]^T \\in \\mathbb{R}^{L\\times 1}$ is a vector of ones. Subsequently, each query can integrate visual features from all positions. However, when it comes to video embeddings, each query considers visual features from not only all positions but also all moments:\n$Y_i = \\frac{\\sum_j \\exp \\left(Q_i K^T(x_j)\\right) V(x_j)}{\\sum_j \\exp \\left(Q_i K^T(x_j)\\right) 1_L},$\nwhere $[x_0,x_1,\\dots]$, $x_i \\in \\mathbb{R}^{L\\times C'}$ represents the video embeddings. Under these circumstances, it is possible that the initial queries may focus on the later visual embeddings while the last queries may concentrate on the earlier visual embeddings, which contradicts the LLM's autoregressive nature and may lead to incorrect responses with respect to the video inputs.\nTo mitigate this issue, we propose a simple approach by applying causal cross-attention masks (CCAMs), where the cross-attention output for video embeddings is computed as follows:\n$Y_i = \\frac{\\sum_j M_{ij} \\exp \\left(Q_i K^T(x_j)\\right) V(x_j)}{\\sum_j M_{ij} \\exp \\left(Q_i K^T(x_j)\\right) 1_L},$\nwhere $M_{ij} = 1$ if the i-th query $Q_i$ is accessible to the j-th frame $x_j$. As T is generally smaller than N, $M_{ij} = 1$ if $i \\geq \\frac{j}{\\lceil \\frac{N}{T} \\rceil}$ else $M_{ij} = 0$, and $\\lceil \\rceil$ is the floor function. We visualize the conventional cross-attention mask and our CCAM in Fig. 2. As depicted in Fig. 2a, conventional cross-attention masks allow queries to attend to all visual tokens indiscriminately, which hinders the model's ability to discern temporal order across frames. In contrast, our CCAM, as illustrated in Fig. 2b, ensures the initial queries focus on the early visual embeddings while allowing the last queries to access visual embeddings across different moments.\n3.3 Temporal consistency\nIn this work, we use temporal consistency to refer to the ability of the Video-MLLM to 1) effec- tively process videos with varying, often significantly larger, numbers of frames compared to those encountered during training; and 2) produce consistent outputs for the same video regardless of the number of sampled frames. Owing to the model structure and the training data distribution, some Video-MLLMs may encounter severe performance drops with different sampling strategies. However, a large number of frames is essential for long video understanding, which is significantly different from the training number of frames (\u2264 32 for most Video-MLLMs). Therefore, existing works mostly employ an additional training stage to bridge this gap. Unlike these Video-MLLMs, Video-CCAM, despite being trained with images and 16-frame videos, can directly handle a large number of frames (e.g., 96 in VideoVista [20], MLVU [62], and Video-MME [10]) and shows outstanding performance without additional tuning. We attribute such experimental results to the temporal consistency of the CCAM projector, as illustrated below from the perspective of continuous signals.\nFirst, we treat the video as a continuous signal instead of sampled frames, and then apply the visual encoder on the signal to get the visual embedding as $x (t) : [0, T] \\rightarrow \\mathbb{R}^{L\\times C'}$. Next, we replace the summation with the integral in eq. (2) and gradually increase the upper limit of the integral to make the output sensitive to the temporal order:\n$Y_i = \\frac{\\int_0^{T_i} \\exp \\left(Q_i K^T(x (\\tau))\\right) V(x (\\tau)) d\\tau}{\\int_0^{T_i} \\exp \\left(Q_i K^T(x(\\tau))\\right) 1_L d\\tau}$\nwhere $T_i = \\frac{i}{N} T, 0 \\leq i \\leq N - 1$ and T is the duration. Suppose that we sample one frame every \u2206T, then the discrete version of eq. (4) becomes:\n$Y_i = \\frac{\\sum_{\\tau<T_i} \\exp \\left(Q_i K^T(x_j)\\right) V(x_j) \\Delta \\tau}{\\sum_{\\tau\\leq T_i} \\exp \\left(Q_i K^T(x_j)\\right) 1_L \\Delta T},$\nwhich is equivalent to eq. (3) for $M_{ij} = 1(j\\Delta \\tau < T_i)$. It is straightforward to prove that $\\lim_{\\Delta \\tau \\rightarrow 0} Y_i = y_i$ if $K (\\cdot), V (\\cdot)$ are bounded (they are linear modules with bounded inputs in the implementation, so they are bounded). Given two differently sampled visual embeddings of the same video, their corresponding CCAM outputs are just approximations of eq. (4) with different precision. In a word, the CCAM projector is able to not only handle videos of different length but also give consistent outputs for the same video with different numbers of sampled frames.\n3.4 Training Pipeline\nVideo-CCAM is trained using a standard autoregressive loss, where the objective is to maximize the likelihood of the target textual outputs given the visual inputs and textual inputs. We take a simple two-stage training strategy. In the first pre-training stage, We randomly initialize the CCAM projector and leverage it to bridge the pre-trained visual encoder and LLM, both of which remain frozen. Only image-text data is utilized in this stage. In the second visual instruction tuning stage, more parameters in the visual encoder and LLM become tunable in addition to the projector. The instruction tuning dataset is composed of image-text and video-text pairs, thereby providing the model with richer context and more challenging tasks."}, {"title": "4 Experiments", "content": "4.1 Setup\nModel. We use SigLIP-SO400M [54] as the visual encoder and conduct experiments on three LLMs, i.e., Phi-3-mini-4k-instruct [1] (4B), Yi-1.5-9B-Chat [2], and Phi-3-medium-4k-instruct [1] (14B). The resulting models are denoted as Video-CCAM-4B, Video-CCAM-9B, and Video-CCAM-14B. Our implementation is based on the xtuner [8] repository. In the first stage, only the projector is tuned. In the second stage, we incorporate LoRA [11] on the visual encoder and the LLM. All CCAMS are composed of one causal cross-attention layer and one feed-forward layer with 1,024 learnable queries.\nDataset. In the first stage, we use the LCS-558K [26] for alignment. In the second stage, we combine the instruction tuning datasets of VideoChat2 [17] and LLaVA-Hound [59]. To enrich the data diversity, we further add several question answering and caption datasets (the training split), including EgoTaskQA [13], PerceptionTestQA [39], ActivityNetQA [53], STAR [49], etc. For short or incomplete responses, some are abandoned while the others are rephrased into long and complete sentences by GPT-40-mini [35] and Gemini 1.5 Flash [45]. Finally, we get 4.4M samples in total. Video-CCAM is trained for 1 epoch with images and 16-frame videos. All experiments are done with 8\u00d7 NVIDIA H800 GPUs. The total training duration of Video-CCAM-4B and Video-CCAM-14B are 2.5 days and 6 days, respectively.\nEvaluation. We evaluate our Video-CCAMs with several benchmarks, i.e., MVBench [17], Video- Vista [20], MLVU [62], VideoChatGPT-QA [31], and Video-MME [10]. As shown in Table 1, the videos in all benchmarks except MVBench [17] are significantly longer than those in the training data on average, posing great challenges on our Video-CCAM models."}, {"title": "4.2 MVBench [17]", "content": "MVBench [17] is a comprehensive benchmark that includes 20 distinct video tasks, each with 200 questions designed to probe the model's understanding of video content. As shown in Table 2, Video-CCAM-4B surpasses all previous MLLMs despite its small size, demonstrating its efficiency and effectiveness in handling video-language understanding tasks. Meanwhile, Video-CCAM-9B sets a new SOTA result, further showcasing its superior performance in this benchmark."}, {"title": "4.3 Video Vista [20]", "content": "VideoVista [20] is another comprehensive benchmark tailored for video understanding and reasoning, including 3,400 videos and 25,000 questions across 14 categories. As the experimental results in Table 3 show, Video-CCAM-4B surpasses all previous open-source Video-MLLMs, while Video- CCAM-14B sets a new SOTA result among open-source Video-MLLMs and demonstrates similar performance to GPT-40-mini [35] and Gemini 1.5 Flash [45]."}, {"title": "4.4 MLVU [62]", "content": "MLVU [62] is a long video understanding benchmark with 9 distinct tasks divided into Multi-Choice (M) and Generation (G) categories. For the Generation tasks, MLVU utilizes GPT-4-Turbo [36] to assign scores to model responses. While Video-CCAM models do not achieve top results, the performance gaps between them and the best open-source results are small. For the Multi-Choice tasks, Video-CCAM-4B is comparable to previous open-source SOTA Video-MLLMs, while Video- CCAM-14B sets a new SOTA result among open-source Video-MLLMs. Despite the duration differences between training data and MLVU [62] in Table 1, Video-CCAM is still proficient at handling long video understanding."}, {"title": "4.5 VideoChatGPT-QA [31]", "content": "VideoChatGPT-QA [31] encompasses a variety of validation/test datasets from MSRVTT-QA [50], MSVD-QA [50], TGIF-QA [12], and ActivityNet-QA [53]. Following VideoChatGPT [31], we employ GPT-3.5-Turbo [37] to evaluate the predictions. As shown in Table 5, Video-CCAM-4B outperforms all previous works except PLLaVA-34B [51], and Video-CCAM-14B further closes the gap between medium-sized Video-MLLMs and PLLaVA-34B [51]. Notably, both Video-CCAM models have better accuracies and scores in TGIF-QA [12] than all previous models."}, {"title": "4.6 Video-MME [10]", "content": "Video-MME [10] is another comprehensive multi-modal evaluation benchmark for Video-MLLMs, offering a highly diverse range of video types and temporal durations and posing significant challenges for Video-MLLMs trained with few frames. As shown in Table 6, Video-CCAM-4B demonstrates competitive performance and is only slightly weaker than the much larger InternVL-Chat-V1.5 [6]"}, {"title": "4.7 Ablation Study", "content": "We conduct several ablation studies with Video-CCAM-4B.\nNumber of Inference Frames. We validate the temporal consistency of Video-CCAM by varying the number of inference frames in Figure 3. In MVBench [17], mostly composed of short videos, its influence is small. However, the number of inference frames plays a vital role in other benchmarks consisting of many long videos, where the score significantly increases from 16 to 96 frames and plateaus around 96 and 128 frames. Besides, no sudden improvement or degradation is observed for all benchmarks.\nCCAM. We replace the CCAM in Video-CCAM-4B with full masks to demonstrate its necessity. We also conduct ablation studies on temporal position embeddings (TPE) as some MLLMs [17] use"}, {"title": "5 Conclusion", "content": "In this work, we introduce Video-CCAM, a novel Video-MLLM specifically designed to tackle video- language understanding tasks for both short and long videos. We integrate the causal cross-attention mask within the cross-attention layer and develop the CCAM projector to handle a large number of visual tokens and effectively model temporal dynamics. To validate its effectiveness, we conduct experiments with LLMs of different sizes on a diverse range of tasks involving both short and long videos, where Video-CCAM models consistently achieve top ranks. Our theoretical analysis and empirical studies on CCAM elucidate the factors contributing to the model's exceptional performance. Through this work, we aim to simplify the complexities of Video-MLLM development and encourage continued innovation in video-language understanding."}]}