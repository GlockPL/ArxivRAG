{"title": "Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations", "authors": ["DAVID N. PALACIO", "DANIEL RODRIGUEZ-CARDENAS", "ALEJANDRO VELASCO", "DIPIN KHATI", "KEVIN MORAN", "DENYS POSHYVANYK"], "abstract": "Trustworthiness and interpretability are inextricably linked concepts for LLMs. The more interpretable an LLM is, the more trustworthy it becomes. However, current techniques for interpreting LLMs when applied to code-related tasks largely focus on accuracy measurements, measures of how models react to change, or individual task performance instead of the fine-grained explanations needed at prediction time for greater interpretability, and hence trust. To improve upon this status quo, this paper introduces ASTrust, an interpretability method for LLMs of code that generates explanations grounded in the relationship between model confidence and syntactic structures of programming languages. ASTrust explains generated code in the context of syntax categories based on Abstract Syntax Trees and aids practitioners in understanding model predictions at both local (individual code snippets) and global (larger datasets of code) levels. By distributing and assigning model confidence scores to well-known syntactic structures that exist within ASTs, our approach moves beyond prior techniques that perform token-level confidence mapping by offering a view of model confidence that directly aligns with programming language concepts with which developers are familiar.\nTo put ASTrust into practice, we developed an automated visualization that illustrates the aggregated model confidence scores superimposed on sequence, heat-map, and graph-based visuals of syntactic structures from ASTs. We examine both the practical benefit that ASTrust can provide through a data science study on 12 popular LLMs on a curated set of GitHub repos and the usefulness of ASTrust through a human study. Our findings illustrate that there is a causal connection between learning error and an LLM's ability to predict different syntax categories according to ASTrust \u2013 illustrating that our approach can be used to interpret model effectiveness in the context of its syntactic categories. Finally, users generally found ASTrust's visualizations useful in understanding the trustworthiness of model predictions.", "sections": [{"title": "1 INTRODUCTION", "content": "The proliferation of open-source software projects and rapid scaling of transformer-based Large Language Models (LLMs) has catalyzed research leading to the increased effectiveness of auto-mated Software Engineering (SE) tools. LLMs have demonstrated considerable proficiency across a diverse array of generative SE tasks [7, 62], including, but not limited to, code completion [10, 50], program repair [3, 9], and test case generation [64]. Current research in both designing LLMs for code and applying them to programming tasks typically makes use of existing benchmarks (e.g., CodeSearchNet [22], or HumanEval [8]) and canonical metrics (by canonical, we refer to metrics that reflect an aggregate performance across many model predictions, for example, percentage accuracy). These canonical metrics have been adapted from the field of Natural Language Processing (NLP) to evaluate the performance of deep code generation models.\nRecent work has illustrated the limitations of benchmarks such as HumanEval [30] and there has been growing criticism of canonical metrics within the NLP community due to the lack of an interpretable context that allows for a deeper understanding of LLMs' predictions or outputs [13, 27, 32, 40, 61]. While code-specific metrics such as CodeBLEU [51] may provide more robust aggregate pictures of model accuracy, they cannot provide the fine-grained context required to truly explain model predictions. The general lack of widely adopted interpretability or explainability tools is a barrier to the adoption of any deep learning model, and in particular LLMs of code, as practitioners are skeptical of models' trustworthiness [34]. This deficiency largely stems from the fact that such benchmarks and canonical metrics are often aimed at evaluating functional correctness or standard performance of generated code at a glance. That is, the evaluation is reduced to a single aggregate metric in which relevant information related to individual predictions is obfuscated [6].\nMethods for interpreting and trusting LLMs for code are inextricably linked. A trustworthy LLM for code requires some degree of interpretability of its predictions, such that model behavior can be under-stood at a fine-grained enough level to judge which parts of the output are correct or not, and why. The more interpretable an LLM for code is, the higher the confidence and trust in the deployment and use of the model [15, 23]. Notably, interpretability has been identified as an important component for enhancing trustworthiness in various studies [29, 35, 65]. When evaluating trust-worthiness, a clear understanding of how and why a model reaches specific predictions is critical. This transparency not only addresses challenges related to uncertainty and the potential for bugs or vulnerabilities but also plays a pivotal role in transforming a model perceived as untrustworthy into one deemed as reliable [53].\nWe assert that a LLM for code is interpretable, and hence more trustworthy, if the reasoning behind its predictions is easy for a practitioner to comprehend. In other words, a useful interpretability technique must provide a conceptual mapping between descriptions of a model's reasoning process and concepts inherently understood by programmers. In this paper, we explore the possibility of using a model's confidence in its predictions as a proxy for describing its reasoning process and develop a technique, which we call ASTrust that automatically aligns and clusters model confidence measures with groups of tokens based on syntactic categories derived from Abstract Syntax Trees (ASTs) that we call Syntax Categories (SCs). This method enables a fine-grained understanding of the correctness of model predictions rooted in syntax-grounded explanations. As illustrated by the overview of our approach in Fig. 1, ASTrust enables two different granularities of interpretability, local explanations at the code snippet level, and global explanations for large collections of code. ASTrust also makes two main contributions: (i) a statistical technique for aligning and aggregating confidence scores to syntactic code structures of different granularities, and (ii) an automated technique for generating visualizations of these aligned confidence scores. At the local level these visualizations take the form of model confidence scores overlaid on both sequence and graph-based illustrations of ASTs and different syntactic structures. At the global level, these take the form of a heat map with confidence values clustered around higher-level syntactic categories. An example of the type of explanation that a developer may derive from ASTrust's visualizations is as follows, \u201cThe model's prediction of the type of the character parameter may be incorrect due to low confidence.\u201d Grounding explanations of model confidence in code syntax provides an informative context to practitioners allowing for interpretability. This is due to the fact that code semantics and syntax are tightly coupled. That is, descriptions of code meaning, or semantics, are often grounded in syntax. For instance, consider the following example of a developer describing program behavior in numpy in which the description of functionality is grounded in terms of data structures, \u201cConvert an array representing the coefficients of a Legendre series,\u201d where the underlined word refers explicitly to the syntactic category of a data structure. One may ask \u201cwhy not ground explanations in code semantics directly?\u201d However, such semantic-based grounding is difficult to achieve, as it requires reasoning among model confidence, input code, predicted code, and widely variable interpretations of code meaning - leading to the potential for incorrect explanations that would undermine a technique ultimately meant to build trust. However, as we illustrate in this paper, it is possible to directly map measures of model confidence to different syntactic categories of code, providing a statistically sound method of understanding the potential correctness of model predictions rooted in concepts that developers can easily understand.\nWe explore the practical benefit of ASTrust through a large-scale data science study examining the relationship between model effectiveness and global explanations and evaluate the usefulness of our method through a human study targeted at local explanations of code snippets using ASTrust's different visualizations. The context of our empirical evaluation includes 12 popular LLMs for code and a curated set of code taken from recent commits of the 200 most popular Python projects on GitHub. Using a carefully crafted causal inference study, our analysis illustrates causal connections between learning error and a model's ability to predict different syntax categories according to ASTrust \u2013 showing that our approach can be used to interpret model effectiveness in the context of its syntactic categories. Our human study included 27 participants who examined code snippets completed by GPT 3 and one of four of ASTrust's visualization techniques for local explanations. Our results illustrate that developers generally found ASTrust and its visualizations useful in understanding model predictions.\nThe results of our studies illustrate that mapping token-level predictions of LLMs to segregated Syntax Categories are of considerable practical benefit to SE researchers and practitioners because it allows them to interpret and trust parts of generated code based on the structural functionality, which contextualizes model predictions beyond the canonical evaluation (i.e., measuring intrinsic and extrinsic metrics). We hope other researchers build upon our method to create new types of"}, {"title": "2 BACKGROUND & RELATED WORK", "content": "In this section, we present background on interpretability and trustworthiness as complementary terms for generating syntax-grounded post hoc (e.g., generated after training) explanations for LLMs of code.\nInterpretability. The brittleness of LLMs can be formulated as an incompleteness in problem formalization [14], which means that it is insufficient that models only infer predictions for certain tasks (the what?). The models must also explain how they arrive at such predictions (the why?). To mitigate such incompleteness in problem formalization, the field of interpretability has risen to encompass techniques and methods that aim to solve the why question. Although authors in this field generally use the terms explainability and interpretability interchangeably, these definitions are inconsistent throughout the literature [16]. We distinguish between the terms to avoid confusion with the purposes of our approach. We will use explainability for methods whose goal is to understand how a LLM operates and comes to a decision by exploring inner mechanisms or layers. Conversely, we will use interpretability for methods that define conceptual mapping mechanisms whose goal is to contextualize models' predictions by associating them with an understandable concept, which in this paper is the syntax of programming languages.\nRelated Work on Interpretability in NLP. There are existing techniques in both natural language processing (NLP) and SE literature focused on interpretability, including LIME [52], Kernel SHAP [35], Integrated Gradient [58] and Contextual Decomposition [42]. These techniques generally try to approximate an interpretable model that either attempts to attribute meaning to hidden representations of neural networks, or illustrate the relationship between input features and model performance. However, we argue that such techniques are difficult to make practical in the context of LLMs for code, given the lack of conceptual mappings explained earlier. However, the most closely related interpretability technique to ASTrust, and one of the only to have adapted to LLMs of code is that of probing which is a supervised analysis to determine which type of parameters (e.g., input code snippets, tokenization process, number of hidden layers, and model size) influence the learning process in ML models [60]. Probing aims to assess whether hidden representations of LLMs encode specific linguistic properties such as syntactic structures of programming languages. Given our generated visualizations, there may be an inclination to characterize ASTrust as a probing technique. However, it is important to note that ASTrust is focused on estimating the correctness of predicted syntactic code elements rather than mapping meaning to internal model representations of data.\nRelated Work on Interpretability in SE. In the realm of SE research, prior work has taken two major directions: (i) techniques for task-specific explanations [17, 31, 49], and (ii) empirical interpretability studies using existing NLP techniques [33, 38, 59]. Previous authors have proposed techniques for explaining specific tasks including vulnerability explanation [17], vulnerability prediction for Android [31], and defect prediction models [49]. More recently Liu et al. conducted large empirical study using existing explainability techniques for global explanations of code to better understand generative language models of code [33]. Mohammadkhani et al. conducted a study using LLM's attention mechanism to interpret their performance on generating code. Finally, one paper that proposed a code-specific interpretability technique is that of Cito et al. [11] who formulated a method to generate explanations using counterfactual reasoning of models. Our work on ASTrust complements this body of past work by developing a new, generally applicable interpretability method that can be applied to both local and global explanations of code, which no prior study or technique has done.\nTrustworthiness. This research is inspired by definitions of trust from automated systems, SE, and NLP. In automated systems, trust is defined as \u201cthe attitude that an agent will help achieve an individual's goal in a situation characterized by uncertainty and vulnerability\u201d [28]. Bianco et al. define software trust as the degree of confidence when the software meets certain requirements [12]. In NLP, Sun et al. argue that LLMs must appropriately reflect truthfulness, safety, fairness, robustness, privacy, machine ethics, transparency, and accountability for them to be trustworthy [57]. We define trust as the confidence that practitioners and researchers have in LLMs' code prediction, anticipating that these predictions will effectively align with their intended goals. Trustworthiness in LLMs implies a sense of interpretability in a given LLM's performance, instilling confidence among practitioners in their abilities to perform code-related tasks. To the best of our knowledge, no paper proposes a concrete definition of trust based on interpretability within the SE research community. Yet, several researchers have called for the importance of trustworthiness in LLMs for code [34, 56]. In our work we present a concrete definition of trustworthiness, highlight its importance, and show how syntax-grounded explanations such as ASTrust contribute to more trustworthy LLMs."}, {"title": "3 SYNTAX-GROUNDED EXPLANATIONS", "content": "At a high level, ASTrust queries a LLM for probabilities per token, estimates the median across tokens that are part of one AST node, and presents those averages as confidence performance values segregated by hand-assigned syntax categories. We also refer to this confidence performance as ASTrust Interpretability Performance.\nASTrust consists of four steps depicted in Fig. 1. In step 1, a code snippet for local or a testbed for global explanations is the starting point of the interpretability process. Each sequence within the snippet or the testbed is processed by a tokenizer (e.g., Byte-Pair Encoding (BPE)). In step 2, the tokenizer sets a vocabulary we named token set. Once code sequences are preprocessed, an LLM under analysis generates token-level predictions (TLP) for each position in a sequence. Next, in step 3, the generated token-level predictions are aligned with the associated Abstract Syntax Tree (AST) terminal nodes. Terminal nodes only store TLP, while non-terminal nodes hierarchically store clustered and aggregated TLP. Terminal and non-terminal nodes comprise the subcategory set. For example, consider if_ BPE token from the token set. This token is aligned with the 'if' terminal AST node while clustered in the \u2018if_statement' non-terminal node. Finally, in step 4, ten syntax categories are proposed to summarize a model's predictions. Syntax Categories aim to group the sub-categories into higher-level, more human-understandable categories. These syntax categories are a fixed category set that comprises more interpretable elements and include:\n\u2022 Decisions\n\u2022 Data Structures\n\u2022 Exceptions\n\u2022 Iterations\n\u2022 Functional Programming\n\u2022 Operators\n\u2022 Testing\n\u2022 Scope\nData Types\n\u2022 Natural\nLanguage\nFor instance, the sub-categories \u2018if_statement' and \u2018if' are both clustered into one syntax category Decisions. In the end, ASTrust generates an averaged score per category for global ex-planations and an AST tree visualization with stored scores at each node for local explanations. In essence, we propose that syntax elements contain semantic information that contextualizes predicted probabilities. However, this semantic information varies across the granularity of these elements. We can claim, for example, that token-level elements carry less interpretable information than category-level elements.\nASTrust produces post-hoc local and global explanations of generated code snippets. A local explanation intends to interpret the generation of a code snippet by decomposing it into AST elements. Conversely, a global explanation uses a set of generated snippets (or existing bench-mark dataset) to interpret a given model holistically into Syntax Categories (SCs). The following sub-sections introduce the building blocks of syntax-grounded explanations. Sec. 3.1 defines the interpretable sets (e.g., Token, Subcategory, and Category) that contain the syntax elements employed for the interpretability process. Sec. 3.2 formalizes two function interactions that communicate previously interpretable sets. Such communication consists of aligning and clustering elements from code tokens to syntax categories. Finally, Sec. 3.3 shows the process of generating local and global explanations.\n3.1 Interpretable Syntax Sets\nToken Set V. Although ASTrust was designed to be compatible with different types of LLMs, this paper concentrated on Decoder-Only models due to their auto-regressive capacity to generate code [67] by preserving long-range dependencies [26]. A Decoder-only model can be employed as a generative process such as any token $w_i$ is being predicted by $\\hat{w}_i \\sim P(w_i | w_{<i}) = \\sigma(y)_i = \\frac{e^{yw_i}}{\\sum_j{e^{y_j}}}$. The term $y_j$ represents the non-normalized log-probabilities for each output token j (see Fig. 3). We extracted and normalized these log-probabilities from the last layer of LLMs to estimate Token-Level Predictions (TLP). This estimation relies on the softmax function. The softmax $\\sigma_i$ returns a distribution over predicted output classes, in this case, the classes are each token in the token set V. The predictions $\\sigma_i$ are expected to be influenced by previous sequence inputs $w_{<i}$.\nSubcategory Set N. This set comprises elements of Context-Free Grammars (CFGs). Such elements are rules containing the syntax and structural information of a programming language [21]. CFGs define instructions that specify how different tokens (i.e., Lexemes) are assembled to form valid statements for each language. Formally, a CFG is defined as $G = (\\alpha, \\lambda, \\omega, \\beta)$ where $\\alpha$ denotes the finite set of non-terminal nodes, $\\lambda$ the finite set of terminal nodes, $\\omega$ the finite set of production rules and $\u03b2$ the start symbol. CFGs use the terminal and non-terminal nodes (or subcategories) to define the production rules \u03c9 for any statement (e.g., conditional, assignation, operator). Furthermore, these terminal and non-terminal nodes retain different meanings. Note that these nodes are the elements of the subcategory set $\\lambda, \\alpha \\epsilon N$.\nCategory Set C. The steps three and four in Fig. 1 illustrate the binding of \u03b1 and \u03bb into a category $c \\epsilon C$. We pose the term Syntax Categories (SCs) as the elements within the Category Set C. We propose ten different SCs based on tree-sitter bindings [5] for Python. SCs are the semantic units to enable the syntax interpretability of LLMs. As such, ASTrust allows for Token-Level Predictions (TLP) to be explained in a developer-centric way. In summary, each token in a sequence s can be mapped to a category $c \\epsilon C$. With ASTrust, practitioners can easily associate LLMs\u2019 code predictions to specific structural attributes. For instance, \u2018identifier' and \u2018string\u2019 nodes correspond to a common Natural Language category in Fig. 4. As such, we can group nodes \u03bb and \u03b1 into semantically meaningful categories C.\n3.2 Alignment and Clustering Formalism\nThe previous subsection describes the syntax elements for enabling LLMs interpretability (i.e., token-set, \u03b1 and \u03bb subcategories, and Syntax Categories (SCs)). This section elaborates on the interaction among these elements. Two interactions in the form of a function are defined. First, the alignment function \u03b4 links code tokens from the Token Set V to terminal nodes \u03bb. Second, the clustering function \u03b8 groups the subcategories \u03bb (terminal nodes) and \u03b1 (non-terminal nodes) by syntax categories (SCs) from the Category Set. Fig. 2 showcases both function interactions \u03b4 and \u03b8 respectively.\nDEFINITION 1. Alignment (\u03b4). The function $\u03b4: W_{<=i} -> \\lambda$ where $W_{<=i}$ corresponds to a code sub-sequence whose tokens are many-to-one associated to the corresponding terminal node vector \u03bb of syntax subcategories.\nClustering Interaction. A clustering function \u03b8 estimates the confidence performance of \u03bb and \u03b1 nodes (subcategories) from an AST by hierarchically aggregating the Token-Level Predictions (TLP) to a Category $c \\epsilon C$. Once the tokens are aligned with their corresponding nodes using \u03b4 from Def.1, ASTrust clusters them into their respective category or non-terminal \u03b1 node according to the AST representation. Some terminal \u03bb nodes can directly be aggregated into a category without considering intermediate non-terminal \u03b1 nodes. A terminal \u03bb node can initiate a block sentence (i.e., a category) and a block sequence parameters (i.e., non-terminal if_statement node). For instance, Fig. 2-1 depicts the terminal \u03bb \u2018if' node aggregated into the Decisions category and also starts the non-terminal \u03b1 \u2018if_statement' node. To estimate the confidence performance, we traverse the entire AST and aggregate the TLP probabilities of respective tokens.\nThe \u03b8 function can adopt average, median, or max aggregations depending on the user con-figuration. Fig. 3 shows the clustering function applied to a concrete code generation sample. This application constitutes a local post hoc explanation: the parent node \u2018parameters' has a 0.23 associated confidence performance. This parent node average was aggregated with its terminal values: '(' with 0.07, \u2018identifier' with 0.4 and 0.1, \u2018,' with 0.5, and ')' with 0.1. Formally, $\u03b8(\u03bb = [0.07, 0.4, 0.1, 0.5, 0.1]) -> [(parameters, 0.23)]$. If a sample snippet does not contain any particular syntax element (i.e., token, subcategory, or category), such an element is therefore never considered for clustering. An absent syntax element is reported as a null value to avoid biased syntax-grounded explanations.\nDEFINITION 2. Clustering (\u03b8). The function $\u03b8: \u03bb -> median(\u00f1)$ where $\u03bb$ is the resulting vector of a sub-sequence and $\u00f1$ is the vector of hierarchical associated non-terminal nodes for each terminal \u03bb. The vector n, therefore, contains the TLP of non-terminal and the corresponding terminal nodes\u00b2.\n3.3 Post Hoc Local and Global Explanations\nLLMs are more understandable when they reflect human knowledge [27]. One way of determining whether an LLM trained on code reflects human knowledge is testing it to see whether or not it operates similar to how a developer would estimate the prediction of a sequence [45]. ASTrust can adopt the form of a post-hoc local or a global explanation to make code predictions humanly understandable.\nASTrust for local interpretability allows us to interpret a single snippet s by generating a visual explanation based on an Abstract Syntax Tree (AST) as illustrated in Fig. 3. A practitioner can explain the code predictions observing the probabilities associated with each element on the AST. In other words, we use \u03b8 from Def. 2 to cluster around AST nodes across all levels (i.e., AST probability annotations). Therefore, the syntax-grounded local explanation comprises a conceptual mapping from the code prediction to a terminal and non-terminal node (or sub-categories). Fig. 3 is a visual representation of the conceptual mapping using code predictions by gpt-3 [1.3B] model. The visualization displays a confidence value for each \u03bb and \u03b1 sub-categories after parsing the AST. The auto-completed snippet is processed with the clustering \u03b8 function.\nASTrust for global interpretability allows us to interpret a LLM by decomposing the canonical performance into segregated confidence performance. This segregated confidence is attached to"}, {"title": "4 EMPIRICAL STUDY DESIGN", "content": "We study the applicability of ASTrust in interpreting code completion tasks. We conducted a human study to investigate the usefulness of local explanations in real-world settings. In contrast, we conducted a data science study to showcase the effectiveness of global explanations on a diverse set of LLMs. Finally, we carried out a causal inference study to assess the validity of the syntax-grounded explanations as they relate to the statistical learning error of the studied models. The following research questions were formulated:\nRQ1 [Usefulness] How useful are local explanations in real-world settings? We validate the extent to which AST probability annotations are useful in locally explaining code predictions. We measure usefulness in three key factors: complexity, readability, and LLMs' reliability.\nRQ2 [Effectiveness] To what extent do LLMs for code correctly predict different syntactic structures? We interpret the performance of 12 LLMs on each Syntax Category (SC). The conceptual mapping allows us to obtain an interpretable and segregated confidence value per category, so we can detect categories that are easier or harder to predict \u2013 moving beyond canonical aggregate metrics.\nRQ3 [Validity] How do Syntax Concepts impact LLMs' statistical learning error? We validate the causal connection between learning error and LLMs' ability to predict different syntax categories using ASTrust.\n4.1 Experimental Context\n4.1.1 Model Collection. To perform our global analysis, we conducted an interpretability analysis of 12 open Decoder-only LLMs, selected based on their popularity. The largest among these models boasts 2.7 billion parameters. Tab. 1 categorizes these LLMs into four distinct groups, each aligned with a specific fine-tuning strategy. The initial category comprises GPT-3-based models primarily trained on natural language, exemplified by Pile [19]. The second category encompasses models trained on natural language but constructed upon the codegen architecture [43]. Moving to the third"}, {"title": "4.2 Human Study for ASTrust Usefulness", "content": "This section presents a preliminary human study comprising a control/treatment experimental design to assess ASTrust usefulness in practical settings. We followed a purposive sampling approach [4] since our primary goal was to gather preliminary data and insights from practitioners with expertise in ML and SE combined. We selected our subjects carefully rather than randomly to study the usefulness of ASTrust (at local explanations). ASTrust is designed to enhance the inter-pretation of model decisions in code completion tasks for practitioners with diverse backgrounds, including researchers, students, and data scientists. By targeting individuals with specific expertise, we ensured that the feedback received was relevant and informed, thereby enhancing the quality of our preliminary findings.\n4.2.1 Survey Structure. Each survey consists of three sections. The first section is aimed at gathering participant profiling information. The profiling section aims to collect information related to how proficient the participants are when using Python and AI-assisted tools in code generation tasks. In particular, we asked about their level of expertise in Programming Languages (PL) and how familiar"}, {"title": "4.2.2 Survey Treatments", "content": "To collect the perception of practitioners regarding the usability of ASTrust, we devised a control survey (UCTR) and three treatments with two types of local expla-nations: sequential (USEQ) and AST-based (UAST) explanations. UCTR represents the absence of a local explanation and only collects the participants' perceptions regarding the correctness of LLMs\u2019 output. By contrast, treatment surveys USEQ and UAST yield syntax-grounded explanations. It is worth noting that we define correctness as the degree to which the generated code reflects the purpose of the algorithm contained in the prompt. In other words, we ask participants to judge whether the model predicted a valid or closely accurate set of tokens given the information context within the prompt."}, {"title": "4.2.3 Survey Metrics", "content": "When evaluating the usefulness of our approach to answer RQ1, we measure the qualitative features of local explanations depicted in Fig. 5. More precisely, we proposed five qualitative metrics to evaluate the usefulness of our approach: Information Usefulness, Local Explanation Complexity, Local Explanation Readability, Visualization Usefulness, and LLM's reliability. We used a Likert scale with three options for quantitatively measuring the responses. Specifically for Information Usefulness: Agree, Neutral and Disagree. For Local Explanation Complexity, Local Explanation Readability and Visualization Usefulness: Useful, Slightly useful and Not useful. Finally, for LLM's Reliability: Not reliable, Highly reliable and Impossible to tell. Each of the survey metrics corresponds to one of the following survey questions.\nMetric\u2081: Information Usefulness - \u2018Q: How useful was the information for interpreting the model's decisions?' In the treatment surveys, we ask the participants to explain the LLM's behavior when"}, {"title": "4.2.4 Open Questions", "content": "In addition to survey metrics, we formulated several open-ended questions for collecting the participants' perception about the correctness of the predictions (Open\u2081) and the most helpful parts of the visual explanations including potential improvement aspects (Open2). Each of these open metrics corresponds to one or more survey questions.\nOpen1: LLM's Prediction Correctness - \u2018Q: If the generated code is incorrect, can you explain why the model might have made the mistake? Otherwise, If the generated code is correct, can you speculate on why the model may have been able to correctly predict the above snippet?\u201d. We asked the participants to use the provided information per sample to analyze whether the prompt or the generated code contained any syntax or semantic error. In UCTR, we aimed to assess the extent to which partici-pants could reason about the source code correctness without any type of explanation provided. Conversely, in USEQ and UAST, we inspected if the layout information somehow contributed to detecting and reasoning about the cause of the error.\nOpen2: Importance of visual explanations - \u2018Q1 : What information from the visualization did you find useful in explaining the model's predictions?', 'Q2 : What information from the visualization did you find useful in explaining the model's predictions?', 'Q3 : What other information (if any) would you like to see in the visualization?', 'Q4 : What elements of the visualization did you like most?', 'Q5 : What elements of the visualization did you like least?'. We asked the participants to provide overall feedback about the type of representation used in the treatment surveys (USEQ and UAST). We aimed to identify the most and least useful elements, as well as gather potential ideas for improvement.\nTo collect, standardize, and analyze the previous group of open-ended questions, two authors independently gathered and reviewed each survey's responses. Any differences were resolved through discussion to reach a consensus.\n4.2.5 Population Profiling. The target population consists of software engineering practitioners experienced in using AI tools for code generation (e.g., ChatGPT, Copilot). Participants were meant to be knowledgeable in Python and understand how algorithms are structured in programming languages and represented in Abstract Syntax Trees (ASTs). While certain knowledge in Deep"}, {"title": "4.2.6 Data Collection", "content": "We reached out to 50 potential participants who were unaware of the purpose of this work, from industrial and academic backgrounds with varying levels of expertise in machine learning and Python. Participants were contacted via email invitations. Out of this group, 27 completed one of the surveys, with the assignment uniformly distributed among the surveys. but we excluded three for low-quality responses, leaving 24 valid submissions. The study was performed on Qualtrics [2] and the anonymized survey data can be found in our appendix [44]."}, {"title": "4.2.7 Statistical Analysis", "content": "We use USEQ as a baseline for our study. We expose the participants to ASTrust with two treatments: UAST[p] and UAST[c] (refer to Fig. 5). The result of each question is influenced by these two treatments. To compare the influence of UAST[p] and UAST[c] against USEQ, we compute the weighted average of the responses from surveys UAST[p] and UAST[c]. We refer to the weighted average as UAST. First, we calculate the results of each treatment individually for all the answers. Then, the weight of each answer is estimated by averaging the number of responses per answer across all samples. We then normalize this weight to get the final weighted average for UAST. We use this weighted average for all our statistical analyses in the paper."}, {"title": "4.2.8 Survey Validity", "content": "To validate the design of the human study, we conducted a pilot experiment with 10 individuals excluded from the pool of participants. Based on this pilot, the quality and appropriateness of the control and treatment surveys were solidified. Initially, the pilot survey included only the UCTR control and the UAST[c] treatment. However, the pilot revealed the need for an intermediate representation serving as a baseline explanation, which is less complex than an AST visualization, to ensure a fair comparison. Consequently, we introduced USEQ, inspired by techniques such as SHAP [35], as a baseline treatment with a less complex representation. Additionally, we"}, {"title": "4.3 Data Science Study for ASTrust Effectiveness", "content": "To answer RQ2 we implemented a data science study to globally interpret 12 LLMs' performance described in Tab. 1 on the SyxTestbed dataset. We performed code completion with different input prompts. The input prompt combines the code completion task, a description, and a partial code snippet. Each prompt has a standard maximum size of 1024 tokens for all considered LLMs.\nWe first compute the normalized log-probabilities (Sec.3.1) or TLP $\\hat{w}_i$ for each SyxTestbed snippet $s \\epsilon S$. These log-probabilities were obtained across the 12 LLMs for every token position. The log-probability distributions maintain a consistent vector size |V| for each token position. Subsequently, these distributions underwent processing to extract the log-probability aligned with the expected token at position i. As a result, each token position corresponds to a stored prediction value $w_i$ for constructing the TLP sequence $w_{<=i}$. As discussed earlier, this experimental setting is based on the premise that token probabilities are well-calibrated to model correctness, which has been confirmed in code completion settings by prior work [56]. Additionally, we confirm this finding in answering RQ3 by observing a causal link between learning error and the probabilities used within ASTrust.\nWe used the alignment function \u03b4 to obtain the terminal node \u03bb vector (see Def.1). Next, we traversed the AST for each terminal node \u03bb and clustered them into the corresponding final \u03bb, \u03b1 node and their correspondent TLP by applying the \u03b8 function (see Sec.3). The clustering was fixed to generate 32 subcategories and their probability values. We estimated a single confidence performance metric (a.k.a. ASTrust Interpretability Performance) per model by averaging the subcategories probabilities. The confidence performance per model was bootstrapped with the median (size of 500 samplings) to ensure a fair comparison. Lastly, we mapped the subcategories to the SCs obtaining a value per Category C (e.g., Data Structures, Decision, or Scope).\nTo provide a baseline comparison, we calculated canonical extrinsic metrics BLUE-4 [46] and CodeBLEU [51], and intrinsic performance. Extrinsic metrics evaluate downstream tasks directly (i.e., code completion), while intrinsic metrics assess how well a language model can accurately predict the next word given an incomplete sequence or prompt [25]."}, {"title": "4.4 Causal Inference Study for ASTrust Validity", "content": "We validate our ASTrust approach using causal inference to answer RQ3. To accomplish this, we formulated a Structural Causal Model (SCM) designed to estimate the impact of SC predictions on the overall learning error of LLMs [48]. We consider that the learning error (i.e., cross-entropy loss) of an LLM is causally impacted by the predicted probabilities of syntax elements. This impact indicates that SCs influence the quality of an LLM. We conducted a causal inference analysis using the docode technique [45] to estimate SCs influence. Inherently, a developer mentally rationalizes several things such as the"}, {"title": "5 RESULTS", "content": "In this section, we present our findings for human, data science, and causal studies. The local analysis is focused on answering RQ1 by using ASTrust to interpret concrete snippets. Similarly, we provide insights into our global analysis to answer RQ2 and RQ3, which incorporates the interpretation of LLMs' performance segregated by Syntax Categories, a comparison of edge cases, and a causal assessment of ASTrust validity.\nBefore presenting the results, we point out basic stats about AST data processing: The average tree height of the samples in the empirical study was 30, with an average of 104 tokens and 166 nodes. In the human study, the four samples have distinct complexity levels. The smallest sample has 80 tokens, with 47 AST nodes and a tree of height eight. The biggest sample has a token length of 139, with 117 AST nodes and a tree height of 14.\n5.1 RQ1 ASTrust Usefulness\nBelow, we present the results for each survey question as introduced in Sec. 4.2. Quantified responses are detailed in Tab. 2. In addition, we summarize the most relevant feedback received in the open-ended questions. The full human study's results can be accessed in the appendix [44].\nMetric\u2081: Information Usefulness. The data reveals that 67.48% of participants who evaluated UAST explanations, found the presented information useful or slightly useful, with a slight preference for UAST[p] (67.86%) over UAST[c] (62.5%). However, 75% of participants who evaluated USEQ felt that it was useful, indicating a stronger preference towards it.\nMetric2: Local Explanation Complexity. Participants found UAST explanations slightly more com-plex (44%) than USEQ (42%). In particular, UAST[c] was found substantially more complex (67%) than UAST[p]. This is not surprising, given that complete ASTs, even for small code snippets can appear complex.\nMetric3: Local Explanation Readability. Both UAST and USEQ were found to be similarly readable: 35% participants found UAST easy to read and use, compared to 29% for USEQ. However, between the two AST types UAST[p] (57%) was far preferred in contrast to UAST[c] (17%), again likely due to the complexity of UAST[c]\u00b7\nMetric4: Visualization Usefulness. USEQ visualization was found useful by more than half of the participants who evaluated it (57%). Similarly, 49.8% considered the UAST visualizations useful, with an appreciable preference for UAST[c] (50%) over UAST[p] (42%)."}, {"title": "5.2 RQ2 ASTrust Effectiveness", "content": "To answer RQ2 we computed both the canonical intrinsic performance and the ASTrust inter-pretability performance for 12 LLMs (Tab. 1). Fig. 7 depicts the canonical intrinsic performance for each LLM (i.e., box-plot) and the density canonical intrinsic performance (i.e., density plot) by model type (e.g., NL GPT-3, NL Codegen, Mono-Language-Type, and Multi-Language-Type). The intrinsic performance comprises an aggregated metric that allows us to compare models at a glance. For instance, on average the smallest mono-lang [110M] has a similar intrinsic performance as the largest GPT-based gpt-3 model with intrinsic performance of 0.61 and 0.62 respectively. After grouping the models by types, we observe that Mono-Language-Type models excel in the intrinsic performance with the highest density of 0.9 for performance values between (0.6 \u2013 0.8) and an average intrinsic performance of \u2248 0.7. Despite the fact canonical intrinsic performance can statistically describe, on average, how the model performs at generating code, these metrics are limited to explaining which categories are being predicted more confidently than others."}, {"title": "5.3 RQ3 ASTrust Validity", "content": "We quantitatively demonstrate that cross-entropy loss of LLMs tends to be negatively impacted by ASTrust probabilities. Therefore, we can explain at syntax category granularity which parts of the code LLMs perform poorly (see red boxes in Tab. 3). We showcase empirical evidence that the previous statement holds for correlations \u03c1 and causal effects p(y|do(t)). Tab. 4 shows, in general, SCs (e.g., Iterative, Scope, or Operator) negatively influence the cross-entropy loss for our best (i.e., M12) and worst (i.e., M\u2081) models. Negative effects indicate that the better a syntax category is predicted, the lower the learning error associated."}, {"title": "6 DISCUSSION", "content": "Below, we pose three aspects for discussion: 1) some general insights (GIs) from the empirical study, 2) a logical analysis of the connection between trustworthiness and interpretability, and 3) the threats to validity of our approach.\n6.1 Empirical Study Insights\nGI\u2081: Token Predictions Reliability. ASTrust relies on logit extraction to generate post-hoc explanations as syntax categories. If logits are wrongly predicted (by over/underfitting), our causal validation process detects such inconsistency by reducing the Average Treatment Effect (ATE) of syntax categories on the statistical learning error. Our Structural Causal Model (SCM) was designed to test the robustness and fidelity of our approach under models' misconfigurations or unreliable performance. Also, as stated earlier in the paper, recent work on calibration for LLMs of code has illustrated that, for code completion (which subsumes the experimental settings in this paper), LLMs tend to be well calibrated to token probabilities/logits [56]. This helps to mitigate issues that may arise due to model confidence and correctness being misaligned.\nGI2: Syntax Aggregations Improves Explanations. Due to its granular nature, token-level predictions are less informative than a hierarchical aggregated level. BPE can make the interpretation of individual tokens much more difficult when code-based sequences are split into tokens that may be meaningless. We posit that practitioners can more easily understand syntax categories rather than individual tokens because these categories are already defined by context-free grammars, which are semantically rich. Moreover, our human study provides evidence of this claim since AST-based explanations were found to be easy to read and use by participants. AST-based explanations also capture semantics by allowing visualization of the full AST structure. This approach helps practitioners evaluate the model's implementation more effectively by providing a clearer, structured view of the code's semantics.\nG13: Natural Language Imbalance. Our approach indicates a poor performance on NL sub-categories. We hypothesize this low performance is due to an unbalanced distribution of NL training samples compared to other categories. Before increasing the context window, we believe that a better analysis would be measuring the proportionality of NL sub-categories on the training set and, then, fine-tuning current LLMs to fix possible data bias. Unfortunately, this analysis is currently out of scope since it demands a complementary Exploratory Data Analysis that we envision for future research stages.\nGI4: Foundational Interpretability Research. ASTrust is meant to serve as a more foundational approach required to guide the future development of interpretability tools for users of different backgrounds (e.g., researchers, students, and data scientists). We aimed to not only propose a formal methodology to conduct interpretability in our field but also perform a preliminary assessment of ASTrust 's usefulness by conducting a control/treatment experiment (i.e., with and without the approach) on a visualization technique based on ASTrust under clearly defined qualitative metrics.\nGI5: Contradictions about the Usefulness of Explanations. In our human study, we found that AST-based explanations were preferred over sequential-based ones. Results revealed that the AST-partial representation was considered more useful than AST-Complete, as it presents the AST representation and ASTrust confidence performance only for the generated portion of the code. However, the feedback received in the open-ended questions revealed contradictory opinions. Some participants indicated that the AST-partial representation missed important details, while others felt that the AST-Complete representation was excessively detailed. These findings suggest the need for more tailored representations for explanations, aiming to present useful information while maintaining readability. We envision incorporating ASTrust into a tool that adds interactivity to navigate the explanations.\n6.2 Trustworthiness & Interpretability Connection\nWe outline two premises based on state-of-the-art definitions of trustworthiness and direct obser-vations from our quantitative and qualitative analyses. Then, we use logical deduction supported by documented and empirical evidence to link the concept of trustworthiness with ASTrust highlighting the significance of syntax-grounded explanations.\nPremise\u2081: Interpretability is a cornerstone for trustworthiness in Language Models for Code (LLMs). The interpretability field enhances transparency and provides insights into the decision-making process serving as a key factor in fostering practitioner trust and adoption. In the realm of Deep Learning for Software Engineering (DL4SE) [63], the significance of interpretability"}, {"title": "6.3 Threats to Validity", "content": "Threats to construct validity concern the intentionality of ASTrust in providing useful explana-tions. Instead of attempting to disentangle information represented between the layers learned by LLMs (i.e., probing [36]), ASTrust focuses on conceptually mapping LLMs' code predictions to present the accuracy in a segregated way. We quantitatively and qualitatively validated the extent to which ASTrust is interpretable through causal analyses and a human study. While we cannot claim that the results from our study generalize beyond the population of users that participated in our study, our participants represent a diverse range of backgrounds mitigating this threat. Nonetheless, the purpose of our study was to conduct a preliminary assessment of ASTrust representations. As such, all code completion scenarios were designed to include a syntax or semantic error since we assessed how useful our approach is in assisting users in understanding models' incorrect behavior, increasing the reliability of our findings.\nThreats to internal validity refer to the degree of confidence in which the ASTrust study results are reliable. Firstly, in our causal study, the potential for unidentified confounders in the code may bias the causal relationship between cross-entropy loss and the Syntax Categories. That is why we ensured the robustness of the Structural Causal Model by performing placebo refutations, which involves simulating unrelated treatments and then re-estimating the causal effects. Secondly, we"}, {"title": "7 LESSONS LEARNED & CONCLUSIONS", "content": "Lesson1: Aggregated metrics may give false impressions about LLMs' capabilities. The research community should incentivize researchers to report AI4SE results in a granular way, as opposed to more traditional aggregated accuracy metrics. After controlling for code confounders, we demonstrated that segregated syntax elements influence the cross-entropy loss of LLMs. This influence persists across models at different parameter sizes and fine-tuning strategies. Syntax information is also relevant for any posterior static analysis of code enabling further evaluations of LLMs in downstream tasks that entail elements of software design (e.g., refactoring).\nLesson2: New interpretability methods are required to enable trustworthiness. In our studies, we have noted an absence of a concrete definition for the term trust in the Software Engineering research. However, several researchers have highlighted the importance of establish-ing trust in work on AI4SE. Research has also shown that interpretability is one of the keys to improving trustworthiness, but at the same time, there is a scarcity of interpretable methods linked to trustworthiness. Despite this limitation, surveyed participants agreed that ASTrust was useful to understand why and how a LLM produced certain errors in code-completion tasks.\nLesson3: Grounding model explanations in the relationship between syntactic structures and prediction confidence is useful. It is feasible to segregate intrinsic metrics (i.e., standard accuracy) into interpretable Syntax Categories revealing the LLMs' inner workings concerning code structure and contributing towards interpretability. By conducting extensive qualitative and quantitative studies involving 12 prominent LLMs, we have demonstrated the effectiveness of ASTrust in enhancing interpretability. We do not claim that our set of categories is complete; however, we consider that a good alignment of the generated categories by the LLM with the ones expected by humans configures a good explanation [20]. Our ASTrust clusters tokens to meaningful categories that are easier for human concept association. Furthermore, we uncovered valuable insights, such as the causal influence of AST categories on the cross-entropy loss of LLMs after accounting for confounding factors. Our human study participants attested to the usefulness of our ASTrust in explaining the predictions of Python code snippets by a LLM 5.1. By breaking down intrinsic metrics into segregated and interpretable terminal and non-terminal nodes, our approach not only enhances the understandability of LLMs but also unveils crucial insights into the inner workings of syntax elements.\nLesson4: The usability of proposed techniques must be further evaluated for indus-try adoption. We adapted the non-mathematical definition of interpretability by Doshi-Velez & Kim [13], Molnar [41] and Miller [37] to the field of AI4SE [63]. However, as our preliminary human study suggests, ASTrust solution is incomplete until being extensively evaluated for industry settings.\nArtifact Availability: Experimental data, curated datasets, source code, and complementary statistical analysis used in this research are published in an open-source repository [44]."}]}