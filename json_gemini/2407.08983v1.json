{"title": "Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations", "authors": ["DAVID N. PALACIO", "DANIEL RODRIGUEZ-CARDENAS", "ALEJANDRO VELASCO", "DIPIN KHATI", "KEVIN MORAN", "DENYS POSHYVANYK"], "abstract": "Trustworthiness and interpretability are inextricably linked concepts for LLMs. The more interpretable an LLM is, the more trustworthy it becomes. However, current techniques for interpreting LLMs when applied to code-related tasks largely focus on accuracy measurements, measures of how models react to change, or individual task performance instead of the fine-grained explanations needed at prediction time for greater interpretability, and hence trust. To improve upon this status quo, this paper introduces ASTrust, an interpretability method for LLMs of code that generates explanations grounded in the relationship between model confidence and syntactic structures of programming languages. ASTrust explains generated code in the context of syntax categories based on Abstract Syntax Trees and aids practitioners in understanding model predictions at both local (individual code snippets) and global (larger datasets of code) levels. By distributing and assigning model confidence scores to well-known syntactic structures that exist within ASTs, our approach moves beyond prior techniques that perform token-level confidence mapping by offering a view of model confidence that directly aligns with programming language concepts with which developers are familiar. To put ASTrust into practice, we developed an automated visualization that illustrates the aggregated model confidence scores superimposed on sequence, heat-map, and graph-based visuals of syntactic structures from ASTs. We examine both the practical benefit that ASTrust can provide through a data science study on 12 popular LLMs on a curated set of GitHub repos and the usefulness of ASTrust through a human study. Our findings illustrate that there is a causal connection between learning error and an LLM's ability to predict different syntax categories according to ASTrust \u2013 illustrating that our approach can be used to interpret model effectiveness in the context of its syntactic categories. Finally, users generally found ASTrust's visualizations useful in understanding the trustworthiness of model predictions.", "sections": [{"title": "1 INTRODUCTION", "content": "The proliferation of open-source software projects and rapid scaling of transformer-based Large Language Models (LLMs) has catalyzed research leading to the increased effectiveness of auto-mated Software Engineering (SE) tools. LLMs have demonstrated considerable proficiency across a diverse array of generative SE tasks [7, 62], including, but not limited to, code completion [10, 50], program repair [3, 9], and test case generation [64]. Current research in both designing LLMs for code and applying them to programming tasks typically makes use of existing benchmarks (e.g., CodeSearchNet [22], or HumanEval [8]) and canonical metrics (by canonical, we refer to metrics that reflect an aggregate performance across many model predictions, for example, percentage accuracy). These canonical metrics have been adapted from the field of Natural Language Processing (NLP) to evaluate the performance of deep code generation models.\nRecent work has illustrated the limitations of benchmarks such as HumanEval [30] and there has been growing criticism of canonical metrics within the NLP community due to the lack of an interpretable context that allows for a deeper understanding of LLMs' predictions or outputs [13, 27, 32, 40, 61]. While code-specific metrics such as CodeBLEU [51] may provide more robust aggregate pictures of model accuracy, they cannot provide the fine-grained context required to truly explain model predictions. The general lack of widely adopted interpretability or explainability tools is a barrier to the adoption of any deep learning model, and in particular LLMs of code, as practitioners are skeptical of models' trustworthiness [34]. This deficiency largely stems from the fact that such benchmarks and canonical metrics are often aimed at evaluating functional correctness or standard performance of generated code at a glance. That is, the evaluation is reduced to a single aggregate metric in which relevant information related to individual predictions is obfuscated [6].\nMethods for interpreting and trusting LLMs for code are inextricably linked. A trustworthy LLM for code requires some degree of interpretability of its predictions, such that model behavior can be under-stood at a fine-grained enough level to judge which parts of the output are correct or not, and why. The more interpretable an LLM for code is, the higher the confidence and trust in the deployment and use of the model [15, 23]. Notably, interpretability has been identified as an important component for enhancing trustworthiness in various studies [29, 35, 65]. When evaluating trust-worthiness, a clear understanding of how and why a model reaches specific predic-tions is critical. This transparency not only addresses challenges related to uncertainty and the potential for bugs or vulnerabilities but also plays a pivotal role in transforming a model perceived as untrustworthy into one deemed as reliable [53].\nWe assert that a LLM for code is interpretable, and hence more trustworthy, if the reasoning behind its predictions is easy for a practitioner to comprehend. In other words, a useful interpretability technique must provide a conceptual mapping between descriptions of a model's reasoning process and concepts inherently understood by programmers. In this paper, we explore the possibility of using a model's confidence in its predictions as a proxy for describing its reasoning process and"}, {"title": "SYNTAX-GROUNDED EXPLANATIONS", "content": "At a high level, ASTrust queries a LLM for probabilities per token, estimates the median across tokens that are part of one AST node, and presents those averages as confidence performance values segregated by hand-assigned syntax categories. We also refer to this confidence performance as ASTrust Interpretability Performance.\nASTrust consists of four steps depicted in Fig. 1. In step 1, a code snippet for local or a testbed for global explanations is the starting point of the interpretability process. Each sequence within the snippet or the testbed is processed by a tokenizer (e.g., Byte-Pair Encoding (BPE)). In step 2, the tokenizer sets a vocabulary we named token set. Once code sequences are preprocessed, an LLM under analysis generates token-level predictions (TLP) for each position in a sequence. Next, in step 3, the generated token-level predictions are aligned with the associated Abstract Syntax Tree (AST) terminal nodes. Terminal nodes only store TLP, while non-terminal nodes hierarchically store clustered and aggregated TLP. Terminal and non-terminal nodes comprise the subcategory set. For example, consider if_ BPE token from the token set. This token is aligned with the 'if' terminal AST node while clustered in the \u2018if_statement' non-terminal node. Finally, in step 4, ten syntax categories are proposed to summarize a model's predictions. Syntax Categories aim to group the sub-categories into higher-level, more human-understandable categories. These syntax categories are a fixed category set that comprises more interpretable elements and include:\n\u2022 Decisions  \u2022 Data Types\n\u2022 Data Structures  \u2022 Natural\n\u2022 Exceptions  \u2022 Language\n\u2022 Iterations \u2022 Functional Programming\n\u2022 Operators\n\u2022 Testing\n\u2022 Scope\nFor instance, the sub-categories \u2018if_statement' and \u2018if' are both clustered into one syntax category Decisions. In the end, ASTrust generates an averaged score per category for global ex-planations and an AST tree visualization with stored scores at each node for local explanations.\nIn essence, we propose that syntax elements contain semantic information that contextualizes predicted probabilities. However, this semantic information varies across the granularity of these elements. We can claim, for example, that token-level elements carry less interpretable information than category-level elements."}, {"title": "3.1 Interpretable Syntax Sets", "content": "Token Set V. Although ASTrust was designed to be compatible with different types of LLMs, this paper concentrated on Decoder-Only models due to their auto-regressive capacity to generate code [67] by preserving long-range dependencies [26]. A Decoder-only model can be employed as a generative process such as any token $w_i$ is being predicted by $\\hat{w}_i \\sim P(w_i|w_{<i}) = \\sigma(y)_i = e^{y_{w_i}}|\\sum_j e^{y_j}$.\nThe term $y_j$ represents the non-normalized log-probabilities for each output token j (see Fig. 3). We extracted and normalized these log-probabilities from the last layer of LLMs to estimate Token-Level Predictions (TLP). This estimation relies on the softmax function. The softmax $\\sigma_i$ returns a distribution over predicted output classes, in this case, the classes are each token in the token set V. The predictions $\\sigma_i$ are expected to be influenced by previous sequence inputs $w_{<i}$.\nSubcategory Set N. This set comprises elements of Context-Free Grammars (CFGs). Such elements are rules containing the syntax and structural information of a programming language [21]. CFGs define instructions that specify how different tokens (i.e., Lexemes) are assembled to form valid statements for each language. Formally, a CFG is defined as G = (\u03b1, \u03bb, \u03c9, \u03b2) where \u03b1 denotes the finite set of non-terminal nodes, A the finite set of terminal nodes, \u03c9 the finite set of production rules and \u03b2 the start symbol. CFGs use the terminal and non-terminal nodes (or subcategories) to define the production rules \u03c9 for any statement (e.g., conditional, assignation, operator). Furthermore, these terminal and non-terminal nodes retain different meanings. Note that these nodes are the elements of the subcategory set \u03bb, \u03b1 \u03b5 \u039d.\nCategory Set C. The steps three and four in Fig. 1 illustrate the binding of \u03b1 and A into a category c\u2208 C. We pose the term Syntax Categories (SCs) as the elements within the Category Set C. We propose ten different SCs based on tree-sitter bindings [5] for Python. SCs are the semantic units to enable the syntax interpretability of LLMs. As such, ASTrust allows for Token-Level Predictions (TLP) to be explained in a developer-centric way. In summary, each token in a sequence s can be mapped to a category c \u2208 C. With ASTrust, practitioners can easily associate LLMs\u2019 code predictions to specific structural attributes. For instance, \u2018identifier' and \u2018string\u2019 nodes correspond to a common Natural Language category in Fig. 4. As such, we can group nodes A and \u03b1 into semantically meaningful categories C."}, {"title": "3.2 Alignment and Clustering Formalism", "content": "The previous subsection describes the syntax elements for enabling LLMs interpretability (i.e., token-set, A and \u03b1 subcategories, and Syntax Categories (SCs)). This section elaborates on the interaction among these elements. Two interactions in the form of a function are defined. First, the alignment function \u03b4 links code tokens from the Token Set V to terminal nodes \u03bb. Second, the clustering function @ groups the subcategories \u03bb (terminal nodes) and \u03b1 (non-terminal nodes) by syntax categories (SCs) from the Category Set. Fig. 2 showcases both function interactions \u03b4 and @ respectively."}, {"title": "Alignment Interaction", "content": "Fig. 2 illustrates the process of aligning the terminal nodes d in the AST to their corresponding code tokens wi. This alignment starts by decomposing an input snippet s into tokens W<=i \u2208 V. For instance, Fig. 2-2 depicts the alignment of try_token to the terminal \u03bb 'try' node. Note that the alignment ignores the character \"_\" from | try. A tokenizer may produce a sequence in which each token does not necessarily match one-to-one with a terminal \u03bb node, e.g., Fig. 2-3 illustrates the tokens flo_ and at are aligned with the A node \u2018float'. Formally, \u03b4(flo_, at) \u2192 [float] in a many-to-one interaction. Consequently, the alignment between code tokens and terminal nodes is certainly many-to-one, including one-to-one, but never one-to-many or many-to-many.\nDEFINITION 1. Alignment (\u03b4). The function \u03b4: W<=i \u2192 A where w<=i corresponds to a code sub-sequence whose tokens are many-to-one associated to the corresponding terminal node vector \u03bb of syntax subcategories."}, {"title": "Clustering Interaction", "content": "A clustering function \u03b8 estimates the confidence performance of \u03bb and \u03b1 nodes (subcategories) from an AST by hierarchically aggregating the Token-Level Predictions (TLP) to a Category c \u2208 C. Once the tokens are aligned with their corresponding nodes using \u03b4 from Def.1, ASTrust clusters them into their respective category or non-terminal \u03b1 node according to the AST representation. Some terminal A nodes can directly be aggregated into a category without considering intermediate non-terminal \u03b1 nodes. A terminal A node can initiate a block sentence (i.e., a category) and a block sequence parameters (i.e., non-terminal if_statement node). For instance, Fig. 2-1 depicts the terminal \u03bb \u2018if' node aggregated into the Decisions category and also starts the non-terminal \u03b1 \u2018if_statement' node. To estimate the confidence performance, we traverse the entire AST and aggregate the TLP probabilities of respective tokens.\nThe @ function can adopt average, median, or max aggregations depending on the user con-figuration. Fig. 3 shows the clustering function applied to a concrete code generation sample. This application constitutes a local post hoc explanation: the parent node \u2018parameters' has a 0.23 associated confidence performance. This parent node average was aggregated with its terminal values: '(' with 0.07, \u2018identifier' with 0.4 and 0.1, \u2018,' with 0.5, and ')' with 0.1. Formally, \u03b8(\u03bb = [0.07, 0.4, 0.1, 0.5, 0.1]) \u2192 [(parameters, 0.23)]. If a sample snippet does not contain any particular syntax element (i.e., token, subcategory, or category), such an element is therefore never considered for clustering. An absent syntax element is reported as a null value to avoid biased syntax-grounded explanations."}, {"title": "3.3 Post Hoc Local and Global Explanations", "content": "LLMs are more understandable when they reflect human knowledge [27]. One way of determining whether an LLM trained on code reflects human knowledge is testing it to see whether or not it operates similar to how a developer would estimate the prediction of a sequence [45]. ASTrust can adopt the form of a post-hoc local or a global explanation to make code predictions humanly understandable.\nASTrust for local interpretability allows us to interpret a single snippet s by generating a visual explanation based on an Abstract Syntax Tree (AST) as illustrated in Fig. 3. A practitioner can explain the code predictions observing the probabilities associated with each element on the AST. In other words, we use 0 from Def. 2 to cluster around AST nodes across all levels (i.e., AST probability annotations). Therefore, the syntax-grounded local explanation comprises a conceptual mapping from the code prediction to a terminal and non-terminal node (or sub-categories). Fig. 3 is a visual representation of the conceptual mapping using code predictions by gpt-3 [1.3B] model. The visualization displays a confidence value for each \u03bb and d sub-categories after parsing the AST. The auto-completed snippet is processed with the clustering 0 function.\nASTrust for global interpretability allows us to interpret a LLM by decomposing the canonical performance into segregated confidence performance. This segregated confidence is attached to"}, {"title": "EMPIRICAL STUDY DESIGN", "content": "We study the applicability of ASTrust in interpreting code completion tasks. We conducted a human study to investigate the usefulness of local explanations in real-world settings. In contrast, we conducted a data science study to showcase the effectiveness of global explanations on a diverse set of LLMs. Finally, we carried out a causal inference study to assess the validity of the syntax-grounded explanations as they relate to the statistical learning error of the studied models. The following research questions were formulated:\nRQ1 [Usefulness] How useful are local explanations in real-world settings? We validate the extent to which AST probability annotations are useful in locally explaining code predictions. We measure usefulness in three key factors: complexity, readability, and LLMs' reliability.\nRQ2 [Effectiveness] To what extent do LLMs for code correctly predict different syntactic structures? We interpret the performance of 12 LLMs on each Syntax Category (SC). The conceptual mapping allows us to obtain an interpretable and segregated confidence value per category, so we can detect categories that are easier or harder to predict \u2013 moving beyond canonical aggregate metrics.\nRQ3 [Validity] How do Syntax Concepts impact LLMs' statistical learning error? We validate the causal connection between learning error and LLMs' ability to predict different syntax categories using ASTrust."}, {"title": "4.1 Experimental Context", "content": "4.1.1 Model Collection. To perform our global analysis, we conducted an interpretability analysis of 12 open Decoder-only LLMs, selected based on their popularity. The largest among these models boasts 2.7 billion parameters. Tab. 1 categorizes these LLMs into four distinct groups, each aligned with a specific fine-tuning strategy. The initial category comprises GPT-3-based models primarily trained on natural language, exemplified by Pile [19]. The second category encompasses models trained on natural language but constructed upon the codegen architecture [43]. Moving to the third"}, {"title": "4.2 Human Study for ASTrust Usefulness", "content": "This section presents a preliminary human study comprising a control/treatment experimental design to assess ASTrust usefulness in practical settings. We followed a purposive sampling approach [4] since our primary goal was to gather preliminary data and insights from practitioners with expertise in ML and SE combined. We selected our subjects carefully rather than randomly to study the usefulness of ASTrust (at local explanations). ASTrust is designed to enhance the inter-pretation of model decisions in code completion tasks for practitioners with diverse backgrounds, including researchers, students, and data scientists. By targeting individuals with specific expertise, we ensured that the feedback received was relevant and informed, thereby enhancing the quality of our preliminary findings.\n4.2.1 Survey Structure. Each survey consists of three sections. The first section is aimed at gathering participant profiling information. The profiling section aims to collect information related to how proficient the participants are when using Python and AI-assisted tools in code generation tasks. In particular, we asked about their level of expertise in Programming Languages (PL) and how familiar"}, {"title": "4.2.2 Survey Treatments", "content": "To collect the perception of practitioners regarding the usability of ASTrust, we devised a control survey (UCTR) and three treatments with two types of local expla-nations: sequential (USEQ) and AST-based (UAST) explanations. UCTR represents the absence of a local explanation and only collects the participants' perceptions regarding the correctness of LLMs\u2019 output. By contrast, treatment surveys USEQ and UAST yield syntax-grounded explanations. It is worth noting that we define correctness as the degree to which the generated code reflects the purpose of the algorithm contained in the prompt. In other words, we ask participants to judge whether the model predicted a valid or closely accurate set of tokens given the information context within the prompt."}, {"title": "4.2.3 Survey Metrics", "content": "When evaluating the usefulness of our approach to answer RQ1, we measure the qualitative features of local explanations depicted in Fig. 5. More precisely, we proposed five qualitative metrics to evaluate the usefulness of our approach: Information Usefulness, Local Explanation Complexity, Local Explanation Readability, Visualization Usefulness, and LLM's reliability. We used a Likert scale with three options for quantitatively measuring the responses. Specifically for Information Usefulness: Agree, Neutral and Disagree. For Local Explanation Complexity, Local Explanation Readability and Visualization Usefulness: Useful, Slightly useful and Not useful. Finally, for LLM's Reliability: Not reliable, Highly reliable and Impossible to tell. Each of the survey metrics corresponds to one of the following survey questions.\nMetric\u2081: Information Usefulness - \u2018Q: How useful was the information for interpreting the model's decisions?' In the treatment surveys, we ask the participants to explain the LLM's behavior when"}, {"title": "4.2.5 Population Profiling", "content": "The target population consists of software engineering practitioners experienced in using AI tools for code generation (e.g., ChatGPT, Copilot). Participants were meant to be knowledgeable in Python and understand how algorithms are structured in programming languages and represented in Abstract Syntax Trees (ASTs). While certain knowledge in Deep"}, {"title": "4.3 Data Science Study for ASTrust Effectiveness", "content": "To answer RQ2 we implemented a data science study to globally interpret 12 LLMs' performance described in Tab. 1 on the SyxTestbed dataset. We performed code completion with different input prompts. The input prompt combines the code completion task, a description, and a partial code snippet. Each prompt has a standard maximum size of 1024 tokens for all considered LLMs.\nWe first compute the normalized log-probabilities (Sec.3.1) or TLP \u0175\u00a1 for each SyxTestbed snippet s \u2208 S. These log-probabilities were obtained across the 12 LLMs for every token position. The log-probability distributions maintain a consistent vector size |V| for each token position. Subsequently, these distributions underwent processing to extract the log-probability aligned with the expected token at position i. As a result, each token position corresponds to a stored prediction value wi for constructing the TLP sequence w<=i. As discussed earlier, this experimental setting is based on the premise that token probabilities are well-calibrated to model correctness, which has been confirmed in code completion settings by prior work [56]. Additionally, we confirm this finding in answering RQ3 by observing a causal link between learning error and the probabilities used within ASTrust.\nWe used the alignment function \u03b4 to obtain the terminal node \u03bb vector (see Def.1). Next, we traversed the AST for each terminal node A and clustered them into the corresponding final \u03bb, \u03b1 node and their correspondent TLP by applying the @ function (see Sec.3). The clustering was fixed to generate 32 subcategories and their probability values. We estimated a single confidence performance metric (a.k.a. ASTrust Interpretability Performance) per model by averaging the subcategories probabilities. The confidence performance per model was bootstrapped with the median (size of 500 samplings) to ensure a fair comparison. Lastly, we mapped the subcategories to the SCs obtaining a value per Category C (e.g., Data Structures, Decision, or Scope).\nTo provide a baseline comparison, we calculated canonical extrinsic metrics BLUE-4 [46] and CodeBLEU [51], and intrinsic performance. Extrinsic metrics evaluate downstream tasks directly (i.e., code completion), while intrinsic metrics assess how well a language model can accurately predict the next word given an incomplete sequence or prompt [25]."}, {"title": "4.4 Causal Inference Study for ASTrust Validity", "content": "We validate our ASTrust approach using causal inference to answer RQ3. To accomplish this, we formulated a Structural Causal Model (SCM) designed to estimate the impact of SC predictions on the overall learning error of LLMs [48]. We consider that the learning error (i.e., cross-entropy loss) of an LLM is causally impacted by the predicted probabilities of syntax elements. This impact indicates that SCs influence"}, {"title": "5 RESULTS", "content": "In this section, we present our findings for human, data science, and causal studies. The local analysis is focused on answering RQ1 by using ASTrust to interpret concrete snippets. Similarly, we provide insights into our global analysis to answer RQ2 and RQ3, which incorporates the interpretation of LLMs' performance segregated by Syntax Categories, a comparison of edge cases, and a causal assessment of ASTrust validity.\nBefore presenting the results, we point out basic stats about AST data processing: The average tree height of the samples in the empirical study was 30, with an average of 104 tokens and 166 nodes. In the human study, the four samples have distinct complexity levels. The smallest sample has 80 tokens, with 47 AST nodes and a tree of height eight. The biggest sample has a token length of 139, with 117 AST nodes and a tree height of 14."}, {"title": "5.1 RQ1 ASTrust Usefulness", "content": "Below, we present the results for each survey question as introduced in Sec. 4.2. Quantified responses are detailed in Tab. 2. In addition, we summarize the most relevant feedback received in the open-ended questions. The full human study's results can be accessed in the appendix [44].\nMetric\u2081: Information Usefulness. The data reveals that 67.48% of participants who evaluated UAST explanations, found the presented information useful or slightly useful, with a slight preference for UAST[p] (67.86%) over UAST[c] (62.5%). However, 75% of participants who evaluated USEQ felt that it was useful, indicating a stronger preference towards it.\nMetric2: Local Explanation Complexity. Participants found UAST explanations slightly more com-plex (44%) than USEQ (42%). In particular, UAST[c] was found substantially more complex (67%) than UAST[p]. This is not surprising, given that complete ASTs, even for small code snippets can appear complex.\nMetric3: Local Explanation Readability. Both UAST and USEQ were found to be similarly readable: 35% participants found UAST easy to read and use, compared to 29% for USEQ. However, between the two AST types UAST[p] (57%) was far preferred in contrast to UAST[c] (17%), again likely due to the complexity of UAST[c]\u00b7\nMetric4: Visualization Usefulness. USEQ visualization was found useful by more than half of the participants who evaluated it (57%). Similarly, 49.8% considered the UAST visualizations useful, with an appreciable preference for UAST[c] (50%) over UAST[p] (42%)."}, {"title": "5.2 RQ2 ASTrust Effectiveness", "content": "To answer RQ2 we computed both the canonical intrinsic performance and the ASTrust inter-pretability performance for 12 LLMs (Tab. 1). Fig. 7 depicts the canonical intrinsic performance for each LLM (i.e., box-plot) and the density canonical intrinsic performance (i.e., density plot) by model type (e.g., NL GPT-3, NL Codegen, Mono-Language-Type, and Multi-Language-Type). The intrinsic performance comprises an aggregated metric that allows us to compare models at a glance. For instance, on average the smallest mono-lang [110M] (M9) has a similar intrinsic performance as the largest GPT-based gpt-3 [2.7B] (M3) model with intrinsic performance of 0.61 and 0.62 respectively. After grouping the models by types, we observe that Mono-Language-Type models excel in the intrinsic performance with the highest density of 0.9 for performance values between (0.6 \u2013 0.8) and an average intrinsic performance of \u2248 0.7. Despite the fact canonical intrinsic performance can statistically describe, on average, how the model performs at generating code, these metrics are limited to explaining which categories are being predicted more confidently than others."}, {"title": "5.3 RQ3 ASTrust Validity", "content": "We quantitatively demonstrate that cross-entropy loss of LLMs tends to be negatively impacted by ASTrust probabilities. Therefore, we can explain at syntax category granularity which parts of"}, {"title": "6 DISCUSSION", "content": "Below, we pose three aspects for discussion: 1) some general insights (GIs) from the empirical study, 2) a logical analysis of the connection between trustworthiness and interpretability, and 3) the threats to validity of our approach."}, {"title": "6.1 Empirical Study Insights", "content": "GI\u2081: Token Predictions Reliability. ASTrust relies on logit extraction to generate post-hoc explanations as syntax categories. If logits are wrongly predicted (by over/underfitting), our causal validation process detects such inconsistency by reducing the Average Treatment Effect (ATE) of syntax categories on the statistical learning error. Our Structural Causal Model (SCM) was designed"}, {"title": "6.2 Trustworthiness & Interpretability Connection", "content": "We outline two premises based on state-of-the-art definitions of trustworthiness and direct obser-vations from our quantitative and qualitative analyses. Then, we use logical deduction supported by documented and empirical evidence to link the concept of trustworthiness with ASTrust highlighting the significance of syntax-grounded explanations.\nPremise\u2081: Interpretability is a cornerstone for trustworthiness in Language Models for Code (LLMs). The interpretability field enhances transparency and provides insights into the decision-making process serving as a key factor in fostering practitioner trust and adoption. In the realm of Deep Learning for Software Engineering (DL4SE) [63], the significance of interpretability"}, {"title": "6.3 Threats to Validity", "content": "Threats to construct validity concern the intentionality of ASTrust in providing useful explana-tions. Instead of attempting to disentangle information represented between the layers learned by LLMs (i.e., probing [36]), ASTrust focuses on conceptually mapping LLMs' code predictions to present the accuracy in a segregated way. We quantitatively and qualitatively validated the extent to which ASTrust is interpretable through causal analyses and a human study. While we cannot claim that the results from our study generalize beyond the population of users that participated in our study, our participants represent a diverse range of backgrounds mitigating this threat. Nonetheless, the purpose of our study was to conduct a preliminary assessment of ASTrust representations. As such, all code completion scenarios were designed to include a syntax or semantic error since we assessed how useful our approach is in assisting users in understanding models' incorrect behavior, increasing the reliability of our findings.\nThreats to internal validity refer to the degree of confidence in which the ASTrust study results are reliable. Firstly, in our causal study, the potential for unidentified confounders in the code may bias the causal relationship between cross-entropy loss and the Syntax Categories. That is why we ensured the robustness of the Structural Causal Model by performing placebo refutations, which involves simulating unrelated treatments and then re-estimating the causal effects. Secondly, we"}, {"title": "7 LESSONS LEARNED & CONCLUSIONS", "content": "Lesson1: Aggregated metrics may give false impressions about LLMs' capabilities. The research community should incentivize researchers to report AI4SE results in a granular way, as opposed to more traditional aggregated accuracy metrics. After controlling for code confounders, we demonstrated that segregated syntax elements influence the cross-entropy loss of LLMs. This influence persists across models at different parameter sizes and fine-tuning strategies. Syntax information is also relevant for any posterior static analysis of code enabling further evaluations of LLMs in downstream tasks that entail elements of software design (e.g., refactoring).\nLesson2: New interpretability methods are required to enable trustworthiness. In our studies, we have noted an absence of a concrete definition for the term trust in the Software Engineering research. However, several researchers have highlighted the importance of establish-ing trust in work on AI4SE. Research has also shown that interpretability is one of the keys to improving trustworthiness, but at the same time, there is a scarcity of interpretable methods linked to trustworthiness. Despite this limitation, surveyed participants agreed that ASTrust was useful to understand why and how a LLM produced certain errors in code-completion tasks.\nLesson3: Grounding model explanations in the relationship between syntactic structures and prediction confidence is useful. It is feasible to segregate intrinsic metrics (i.e., standard accuracy) into interpretable Syntax Categories revealing the LLMs' inner workings concerning code structure and contributing towards interpretability. By conducting extensive qualitative and quantitative studies involving 12 prominent LLMs, we have demonstrated the effectiveness of ASTrust in enhancing interpretability. We do not claim that our set of categories is complete; however, we consider that a good alignment of the generated categories by the LLM with the ones expected by humans configures a good explanation [20]. Our ASTrust clusters tokens to meaningful categories that are easier for human concept association. Furthermore, we uncovered valuable insights, such as the causal influence of AST categories on the cross-entropy loss of LLMs after accounting for confounding factors. Our human study participants attested to the usefulness of our ASTrust in explaining the predictions of Python code snippets by a LLM 5.1. By breaking down intrinsic metrics into segregated and interpretable terminal and non-terminal nodes, our approach not only enhances the understandability of LLMs but also unveils crucial insights into the inner workings of syntax elements.\nLesson4: The usability of proposed techniques must be further evaluated for indus-try adoption. We adapted the non-mathematical definition of interpretability by Doshi-Velez & Kim [13], Molnar [41] and Miller [37] to the field of AI4SE [63]. However, as our preliminary human study suggests, ASTrust solution is incomplete until being extensively evaluated for industry settings."}]}