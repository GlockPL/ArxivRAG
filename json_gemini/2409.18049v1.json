{"title": "Revisit Anything: Visual Place Recognition via Image Segment Retrieval", "authors": ["Kartik Garg", "Sai Shubodh Puligilla", "Shishir Kolathaya", "Madhava Krishna", "Sourav Garg"], "abstract": "Accurately recognizing a revisited place is crucial for embodied agents\nto localize and navigate. This requires visual representations to be distinct, de-\nspite strong variations in camera viewpoint and scene appearance. Existing vi-\nsual place recognition pipelines encode the whole image and search for matches.\nThis poses a fundamental challenge in matching two images of the same place\ncaptured from different camera viewpoints: the similarity of what overlaps can\nbe dominated by the dissimilarity of what does not overlap. We address this by\nencoding and searching for image segments instead of the whole images. We\npropose to use open-set image segmentation to decompose an image into \u2018mean-\ningful' entities (i.e., things and stuff). This enables us to create a novel image\nrepresentation as a collection of multiple overlapping subgraphs connecting a\nsegment with its neighboring segments, dubbed SuperSegment. Furthermore, to\nefficiently encode these SuperSegments into compact vector representations, we\npropose a novel factorized representation of feature aggregation. We show that re-\ntrieving these partial representations leads to significantly higher recognition re-\ncall than the typical whole image based retrieval. Our segments-based approach,\ndubbed SegVLAD, sets a new state-of-the-art in place recognition on a diverse\nselection of benchmark datasets, while being applicable to both generic and task-\nspecialized image encoders. Finally, we demonstrate the potential of our method\nto \"revisit anything\u201d by evaluating our method on an object instance retrieval task,\nwhich bridges the two disparate areas of research: visual place recognition and\nobject-goal navigation, through their common aim of recognizing goal objects\nspecific to a place.", "sections": [{"title": "1 Introduction", "content": "Visual Place Recognition (VPR) is an important capability for embodied agents to\nlocalize and navigate autonomously. A predominant solution for VPR is to encode"}, {"title": "2 Related Works", "content": "Image retrieval-based Visual Place Recognition (VPR) is a well-established area of re-\nsearch in visual localization [19,44,63,70]. It is important both during mapping for loop"}, {"title": "2.1 Whole Image Encoders", "content": "Earlier works in whole-image representation used methods like Gist [50], BoW (Bag\nof Word) [66], and VLAD (Vector of Locally Aggregated Descriptors) [30], often\ndefined using hand-crafted features such as SIFT [41]. In recent years, deep learn-\ning based methods have demonstrated remarkable performance, with initial successful\nmethods like NetVLAD [3] now rapidly outperformed by better alternatives such as\nCosPlace [6], MixVPR [2], EigenPlaces [8], TransVPR [71], and more recently Any-\nLoc [32], SALAD [29] and VLAD-BuFF [35]. All these learning-based methods im-\nprove different aspects of representation learning: training datasets [1, 6, 72], objec-\ntive/loss functions [2, 8], aggregation methods [29, 56, 59, 71], and generalization [32].\nOur approach complements these existing methods as we mainly focus on the use of\nsegment-based information, where the segments can be described by any of the image\nencoders from the aforementioned techniques. In particular, we demonstrate that both\n\u2013 an off-the-shelf encoder, e.g., DINOv2-AnyLoc [32, 51] or that finetuned specifically\nfor the VPR task, e.g., DINOv2-NetVLAD [35] \u2013 can be used in conjunction with our\nsegment-based approach to further elevate place recognition capability."}, {"title": "2.2 Region/Patch Based Methods", "content": "There exist several methods that employ region or patch level information to enhance\nrepresentational power [13, 14, 34, 40, 54, 74, 76]. However, most of these methods only\nuse this additional information to generate a single (or concatenated) compact vector\nrepresentation of an image. Other methods such as Patch-NetVLAD [25] create mul-\ntiple features per image but their primary purpose is to perform local matching based\nreranking. In contrast to these methods, we aim to use multiple segment descriptors per\nimage to directly retrieve from a database of segments, without using any geometric\ninformation or reranking. The motivation behind this stems from the very nature of hi-\nerarchical VPR pipelines: reranking recall is upper bounded by the coarse retriever. A\nbetter coarse retriever can improve reranking performance without needing to rerank\nfrom a longer list of top hypotheses. MultiVLAD [4] is similar to our method in the\nspirit of retrieving multiple features per query image. However, like aforementioned"}, {"title": "2.3 Segments-Enhanced Methods", "content": "There exist several methods that use semantic segmentation information to improve\nVPR, as also surveyed in [22]. These methods vary in terms of type of segmentation\nused and the specific ways in which it is integrated in the VPR pipeline, e.g., planes [17],\nobjects [15,31,46], landmarks [68], outdoor semantics [21,23,47,48], utility/confusion\nbased [33, 38], domain adaptation [26] and even learning to segment for VPR [52].\nHowever, neither these methods aim to perform segment-level retrieval nor do they use\nopen-set segmentation. We also review two concurrent works: MESA [78] and Region-\nRevisited [64]. Similar to our method, they both use SAM to segment images but for\ndifferent specific tasks. MESA [78] proposes a graph-based local feature/area matching\nmethod to obtain point correspondences. Our method complements this, as we per-\nform coarse retrieval for VPR, which could potentially use MESA for reranking. Re-\ngions Revisted [64] delves into the advantages of using SAM masks in conjunction with\nSLIC [36] to improve semantic segmentation, activity recognition and object category\nretrieval. In contrast, we aim to improve instance-level recognition by recognizing spe-\ncific things belonging to specific places that a robot encounters during a revisit. Similar\nto our work, [20] creates an image sequence-based topological graph of segments where\nits segment neighbourhood aggregation is based on average pooling, similar to [64]. In\nSection 5.3, we show that such segment average pooling deteriorates recognition per-\nformance for the VPR task."}, {"title": "2.4 Open-set VPR", "content": "Researchers have recently started to shift their focus to open-set, generally-applicable\ntechniques, including that for VPR [31, 32, 46, 65]. FM-Loc [46] uses GPT [9] to rec-\nognize object and place labels, whereas [31] uses CLIP [57] for open-set place recog-\nnition. LIP-Loc [65] proposes pretraining for cross-modal VPR, but is limited in its\nzero-shot capabilities. AnyLoc [32] proposes to use DINOv2 with domain-level vocab-\nularies and hard-assignment based VLAD. It achieves state-of-the-art performance par-\nticularly on non-streetview datasets, where current VPR-trained methods tend to fail. In\nthis work, we propose a generally-applicable approach which is built on top of models\nlike SAM [37] and DINOv2 [51], and works with both VPR-agnostic [32] and VPR-\nspecific [35] backbone models. We particularly aim for a new paradigm in retrieval\nbased VPR, where we move away from the conventional whole image global descrip-\ntors to segments based descriptors and retrieval, which achieves a new state-of-the-art\non diverse domains under wide viewpoint variations."}, {"title": "3 Proposed Approach", "content": "Despite recent advances in place recognition, viewpoint variations continue to be an\nopen challenge for an embodied agent to recognize the same specific things in its envi-\nronment. Current methods in visual place recognition tackle this problem by converting"}, {"title": "3.1 Problem Formulation", "content": "We represent an image as a set of segment descriptors instead of a single global de-\nscriptor. For an image I, we obtain binary image segment masks $M \\in {0,1}^{S\\times N}$ and\ndense pixel-level descriptors $f_p \\in \\mathbb{R}^D$, where S represents the number of segments per\nimage, D is the descriptor dimension, and $p \\in [1, N]$ represents spatial elements across\nthe width (W) and height (H) of the image encoder's output, flattened into $N = W \\times H$\nfor convenience. Figure 1 shows an illustration of our proposed pipeline, as explained\nin the following subsections."}, {"title": "3.2 Super Segments", "content": "Humans are remarkable at visual recognition, where existing studies suggest that we\noften leverage spatial associations among objects in an environment to represent it in-\nternally [5, 28]. This enables us to distinguish between two different scenes through\nthe surrounding context of the objects of interest. In this work, we imbibe this context\nthrough the spatial neighbourhood of the image segments. For each image, we con-\nstruct a graph of segments through their pixel centers using Delaunay Triangulation."}, {"title": "3.3 SuperSegment Descriptors", "content": "In this section, we describe our feature aggregation method to obtain SuperSegment\ndescriptors. Recent state-of-the-art method AnyLoc [32] demonstrated that using off-\nthe-shelf powerful image encoders such as DINOv2 with hard assignment based VLAD"}, {"title": "3.4 Image Retrieval via Segments", "content": "Existing global descriptor based VPR techniques produce a single vector per image to\nsearch against a database of reference image vectors. In our method, we obtain multiple\nSuperSegment descriptors per image. We perform retrieval at segment-level, that is, we\nsearch for the top matches for each query segment against a flat index of all segments\nfrom all the images of the reference database/map. To evaluate in the form of image\nretrieval-based VPR, we convert the top retrieved segment indices across all segments\nof a query image into top reference image indices. This is achieved through a weighted\nfrequency measure (i.e., weighted bin/word counting). We first map the top $K'$ retrieved\nsegment indices for each of the query segments $s \\in [1, S]$ to their respective reference\nimage indices, denoted with r. Then, for each of the unique retrieved image indices $r_j$,\nwe accumulate its segment similarity $\\theta$ and then use the cumulative similarity score $\\Theta$\nto rank the image indices to obtain the top image match $r_j^*$:\n\n$r^*_j = \\underset{r_j}{\\text{argmax}} \\Theta(r_j); \\,\\, \\, \\, \\Theta(r_j) = \\sum_{s=1}^S \\sum_{k=1}^{K'} \\theta_{sk} .\\mathbb{1}\\{r_{sk}=r_j\\}$  (4)\n\nIn Section 5.3, we compare our similarity-weighted ranking with other alternatives\nbased on frequency or similarity alone."}, {"title": "4 Experimental Setup", "content": "Datasets: VPR datasets are in abundance, as can be found in several benchmarks\nincluding VPR-Bench [77], Deep Visual GeoLocalization Benchmark [7], and Any-\nLoc [32]. In this work, we used a variety of datasets covering both outdoor and in-\ndoor environments. Outdoor datasets include Pitts30k [69], AmsterTime [75], Mapil-\nlary Street Level Sequences (MSLS) [72], SF-XL [6], VPAir [62], Revisted Oxford5K\nand Revisited Paris6k [55]. Indoor datasets include Baidu Mall [67], 17Places [80] and\nInsideOut [27]. Additional datasets-related details are provided in the supplementary.\nEvaluation and Benchmarking: We evaluate our method as an image retrieval task\nusing Recall@K metric, where top $K'$ (= 50) retrieved segments per query segment are\nused to obtain top K (= 5) images (see Eq. 4). We compare against the most recent\nand high-performing VPR baseline methods. This includes CosPlace [6], MixVPR [2]\nand EigenPlaces [8], which are trained on large-scale urban datasets for VPR tasks.\nWe further include two very recent state-of-the-art methods that use DINOv2 as the\nbackbone. These include AnyLoc [32] which uses an off-the-shelf DINOv2 model and\nSALAD [29] which uses a finetuned DINOv2 backbone. Given the dichotomy between\ngeneral-purpose VPR benchmarking of AnyLoc and the typical outdoor-focused bench-\nmarking [7], we evaluated our method using two different backbones. a) SegVLAD-\nPreT: we use the same backbone and aggregation as AnyLoc, i.e., off-the-shelf pre-\ntrained DINOv2 (ViT-G) backbone with hard VLAD assignment, but the key difference\nis in the use of SuperSegments for our method as opposed to whole-image description\nof AnyLoc. b) SegVLAD-FineT: as our default aggregation method is VLAD, we use\na finetuned DINOv2 (ViT-B) backbone which is similar to SALAD but we replace its\naggregation layer with the original NetVLAD aggregation [3] using 64 clusters, as de-\nscribed in [35]. We use this finetuned backbone with hard VLAD based assignment,\nsimilar to AnyLoc. For both these models, we reduce the descriptor dimensions of the\nVLAD descriptor to 1024 using PCA, as commonly done in previous works [3,32]. We\ntrain PCA transform in a map-specific manner using the database images of the dataset.\nFollowing AnyLoc [32], we report results using two different sources of VLAD vocab-\nulary: map-specific (M) and domain-specific (D)."}, {"title": "5 Results", "content": "We first present benchmark comparison of our method against state-of-the-art VPR\nmethods. This is followed by detailed analysis of our proposed aggregation technique.\nLastly, we demonstrate results on a downstream task of Object-of-Interest (OOI) re-\ntrieval, showcasing the versatility of our method."}, {"title": "5.1 State-of-the-art comparisons", "content": "Table 1 presents Recall@1/5 comparison against state-of-the-art VPR methods on stan-\ndard outdoor street-view datasets, which are similar to the typical training datasets used\nfor VPR [2,3,6]. Table 2 covers \u2018out-of-distribution' datasets, inspired by AnyLoc [32],"}, {"title": "5.2 Revisiting Objects of Interest (OOI): Object Instance Retrieval", "content": "A typical requirement of an embodied agent is to understand the context of its task\nthrough its memory/map information, which is composed of visual and/or semantic\ncues. For example, navigating to a given object goal requires a robot to visually rec-\nognize the goal and not be confused by perceptually-similar items. In this section, we\ndemonstrate our method's ability to retrieve the correct image given just an Object Of\nInterest (OOI) as a query segment. For this purpose, we use an extended version of the\nBaidu dataset [73] which annotates OOI as various discriminative areas that can be re-\nliably detected under variable viewpoint and lighting conditions. In total, there are 220\nOOI, which cover various things such as logos, brand names, posters, etc., in a highly\ncluttered mall environment. To cast this dataset in terms of revisiting things, we use the\noriginal query images of the Baidu Mall [67] dataset as the database and the images\nwith OOI as the queries. This allows us to evaluate the OOIs directly. This is similar\nto VPR evaluation of recall in terms of image retrieval but with querying of a specific\nsegment instead of using all the segments of the query image.\nWe consider four different methods of recognizing known objects in this study. i)\nGlobal-to-Global: as a baseline method, we use whole images to represent and\nretrieve, i.e., without using the OOI mask; this resembles object-goal recognition prob-\nlem for an InstanceImageNavigation task [39]. ii) Segment-to-Global: this is the\nsame as the previous setting except that the query image descriptor is aggregated only\nusing the OOI mask; this tests the ability of the image encoder/aggregator to match\nsegment-level descriptor against global descriptors. iii) SegVLAD and iv) SegVLAD\nNoNbrAgg, which are our proposed methods but the latter does not use any neighbor-\nhood information; this highlights the relevance of spatial context around the OOI for\nrecognition. For SegVLAD, we create a virtual segment mask for the OOI, append it\nto the other masks of the image, and then perform our neighborhood expansion and\nfeature aggregation, as described in Section 3."}, {"title": "5.3 Ablation Studies", "content": "Aggregation method & Order of Neighborhood Expansion Previous studies on\nVPR such as AnyLoc [32] have shown VLAD to be better than other aggregation\nmethods for whole-image based global descriptors. However, in an increasing number\nof segment-based approaches [12, 20, 24, 64], segment average pooling (SAP) is used\nmore commonly. Thus, we compare hard-assignment VLAD against SAP on Baidu\ndataset. For SAP, we upsample the DINOv2 features to match the resolution of our\nSAM masks \u2013 this upsampling is shown to enhance performance in [64]. For VLAD\naggregation, we use our proposed method SegVLAD, where we downsample masks to\nmatch with the low resolution of DINOv2. Table 4 shows that SAP performs well for\norder 0 aggregation (i.e., no neighborhood aggregation) but its performance reduces as\nthe neighborhood expands. On the other hand, SegVLAD has low recall when no neigh-\nborhood is considered but benefits significantly even with its immediate neighborhood\n(order 1). We attribute these inverted trends of SegVLAD and SAP to the very nature of\nthese aggregation methods: as more information becomes available SAP smooths out\nthe overall information content whereas SegVLAD benefits from additional informa-\ntion which gets distributed across its clusters, thus minimizing any possible smoothing\neffect. It can be observed that R@5 increases for SegVLAD with an increasing order\nof neighborhood expansion but margins diminish for higher orders. Overall, SegVLAD\n(order 3) achieves the best results, despite aggregating at a 14\u00d7 lower resolution than\nSAP's upsampling based aggregation.\nSegment to Image Retrieval Unlike conventional global descriptor retrieval based\nVPR, we perform retrieval for multiple SuperSegments of the query image. To obtain\nretrieval output in terms of images (as that is what VPR is typically evaluated on), there\nexist multiple ways to combine the top segment-level matches across all the query seg-\nments. We consider the following alternative options. i) MaxSeg: we obtain the best\nmatching segment for each query segment, associate the matched segments to their"}, {"title": "5.4 Limitations, Storage, Compute Time & IOU Based Filtering", "content": "A key limitation of our method is its large map size, i.e., large storage requirement for\nsegment-level descriptors (see supplementary for further details). In this section, we an-\nalyze the resource requirements in terms of database (index) storage and query retrieval\ntime for our method, along with a preliminary study on IOU (Intersection over Union)\nbased filtering of SuperSegments to reduce such costs. We compute IOU between all\npairs of SuperSegments in a given image, and remove segments with $IOU(s_i, s_j) >$\n$\\psi \\forall i\\in [1, S], j \\in [i, S]$, where $\\psi \\in [0, 1]$ is a threshold on IOU and $s_i$ refers to the list\nof SuperSegments sorted by their pixel area in a descending order. We only perform this\nculling on the database segments. We use the outdoor-finetuned DINOv2 backbone for\nthis purpose and compare SegVLAD with SALAD on AmsterTime (see supplemen-\ntary for additional results on Pitts30K). Figure 5 shows that SegVLAD outperforms\nSALAD while requiring less storage (annotated on points) and comparable retrieval\ntime (excludes extraction time), using IoU-based filtering threshold ranging from 0.2 to\n0.8 (left to right) with a step size of 0.2. In particular, at 0.4 IOU threshold, only 20%\nof SuperSegments are retained (0.05 GB) while still outperforming the baseline."}, {"title": "5.5 Qualitative Analyses", "content": "In this section, we further demonstrate the capabilities of our method through qualita-\ntive visualizations. We compare our method against AnyLoc, where the only difference\nbetween the two methods is Global aggregation/retrieval vs SuperSegment aggrega-\ntion/retrieval. We particularly consider the queries for which our approach successfully\nretrieved the correct match but AnyLoc failed to do so (additional examples can be\nfound in the supplementary). Figure 4 shows triplets of images in the order of query,\ncorrect match (ours), and incorrect match (Anyloc). The segmented part shows one of\nthe correctly matched SuperSegments, displayed as a subgraph in white color overlaid\non the corresponding segment masks.\nThe first row shows a triplet from AmsterTime where our proposed method is able\nto correctly recognize a subgraph of building across the image pair, whereas Anyloc\nretrieves an incorrect image of a street-view with buildings, cars, and road, laid out\nsimilarly across the image pair. This highlights that a global descriptor can get con-\nfused with perceptually-aliased global context. In contrast, our SuperSegment based"}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we presented a novel visual place recognition pipeline SegVLAD based\non image segments-based description and retrieval, which is akin to \u2018revisting things'\nas a means to recognize specific instances of what constitute a place. Our proposed\nSuperSegments based image representation and a novel factorization based feature ag-\ngregation enables us to effectively represent and retrieve images using our segment\nsimilarity-weighted image ranking. Our results show that despite using powerful im-\nage encoders such as DINOv2 (pretrained or VPR-finetuned), existing global descrip-\ntor based techniques are unable to deal with the challenges of viewpoint variations.\nIn contrast, SegVLAD is able to correctly retrieve images through its ability to match\npartially-overlapping images with its partial image representation in the form of semi-\nglobal subgraphs of segments, i.e., SuperSegments. Thus, our method achieves state-of-\nthe-art results on three diverse datasets (indoor and outdoor) that exhibit strong view-\npoint variations on top of other challenges of appearance shift and high perceptual alias-\ning. Through an additional object instance retrieval study, we demonstrate the unique\nability of our method to recognize objects instances within their specific place contexts\n\u2013 an open-set recognition capability that existing VPR methods lack.\nOur approach shifts the paradigm in retrieval based VPR research, as the conven-\ntional methods predominantly classify into either whole-image global descriptor based\ncoarse retrieval or local feature based geometric reranking. Our approach complements\nrecent concurrent works like MESA [78]; future work can explore a hierarchical VPR\npipeline that closely integrates a segment-based coarse retriever with segments-based\nreranker such as MESA, thus doing away entirely with global whole-image descriptors.\nFurthermore, segments-based representation with implicitly baked semantics provide\na natural way for creating text-based interfaces through CLIP [57] and LLMs (Large\nLanguage Models) [9], which can be easily integrated with recent efforts in this direc-\ntion [12, 18, 20, 24, 42, 43, 81]."}, {"title": "Supplementary Material", "content": "In this supplementary, we first present the limitations of our work, coupled with addi-\ntional results on IOU-based filtering of SuperSegments (Sec. 8). This is followed by ab-\nlation studies on local feature based retriever (Sec. 9.1) and an efficient version of SAM\n(Sec. 9.2). We then present implementation and benchmarking details relating to the\nproposed factorized feature aggregation method (Sec. 10.1), backbone models (SAM\nand DINOv2) (Sec. 10.2), and benchmark datasets (Sec. 10.3). Finally, we present ad-\nditional qualitative examples for retrieval (Sec. 11)."}, {"title": "7 Limitations", "content": "While our proposed method SegVLAD achieves state-of-the-art results on a diverse\nset of VPR benchmark datasets, there are notable limitations of our approach. i) Re-\ndundancy: we create several overlapping SuperSegments per image. While these are\nsomewhat necessary to enable accurate partial image matching via partial representa-\ntions, SuperSegments formed through neighboring central segments will have a very\nhigh overlap. ii) Map size, we need to store several SuperSegment descriptors (far more\nthan a typical global descriptors database). These limitations to some extent can be ad-\ndressed through simple measures, e.g., masks IOU based filtering of the database seg-\nments to reduce both the redundancy and storage, as detailed in the subsequent section.\nThe value of our research primarily lies in the demonstration of a novel approach to\nVPR that not only addresses the fundamental limitation of global descriptors but is also\ncharacterized as an open-set, object-based, text-interface-friendly representation, which\nis more likely to plug in to similar recent approaches aimed at embodied intelligence."}, {"title": "8 Storage, Compute Time & IOU Based Filtering Additional\nResults", "content": "Table 7 shows results for IOU-based filtering for different thresholds, corresponding\nnumber of database segments, their storage consumption, and the average retrieval time\nper query. As can be observed from the results for AmsterTime, 0.4 IOU threshold\nremoves up to 80% of the original segments, and our method still outperforms the global\ndescriptor baseline while only requiring roughly half its storage. For Pitts30K, both 0.4\nand 0.6 IOU thresholds remain reasonable choices, with improved recall at reduced\nstorage and time. Furthermore, in absolute terms, both storage and retrieval time for\nour method are practically viable, and comparable to the global descriptor baseline."}, {"title": "9 Additional Results and Ablation Studies", "content": "This section describes two ablation studies: i) local feature based retrieval, which high-\nlights the role of SuperSegments in capturing sufficient pixel scope to retrieve correctly,"}, {"title": "9.1 Local Feature Retrieval", "content": "There exist retrieval-based place recognition methods that directly use the local fea-\ntures with efficient inverted indexing and searching, e.g., DeLF [49]. In this section, we\ncompare our segments-based retriever against such a local feature-based retriever, while\nusing the same feature backbone (AnyLoc's DINOv2). We sample S local features uni-\nformly at random, where S is the average number of segments for that image. As we\nintend to compare the role of local features in contrast to aggregation at segment/global\nlevel, we directly construct a flat index of these local features using the reference im-\nages of the given dataset. We then use our retrieval pipeline, considering local features\na drop-in replacement of segment descriptors."}, {"title": "9.2 SAM vs FastSAM", "content": "Section 10 provides details of the configuration used for the original SAM model, where\nsegment extraction becomes time consuming due to grid-based sampling/prompting (up\nto 3.5 seconds per image). Therefore, in our pipeline, we drop-in replace the original\nSAM with an efficient version of SAM, i.e., FastSAM [79] to analyse the recall-speed\ntrade-off for our proposed method. FastSAM replaces the transformer architecture of"}, {"title": "9.3 HardVLAD vs (Soft) NetVLAD", "content": "In the main paper, we used hard assignment based VLAD aggregation for both pre-\ntrained and finetuned DINOv2 backbones. The alternative choice, particularly for the\nfinetuned version, is to use soft assignment as defined in NetVLAD [3]. Table 10 shows\nthat hard assignment of VLAD performs similar to NetVLAD."}, {"title": "9.4 Different feature extractor", "content": "Other than the DINOv2 as our backbone feature extractor, here we consider another\nsimilar pretrained off-the-shelf backbone to highlight that our method is generally ap-\nplicable to any dense feature extractor. Table 11 shows results for a more recent fea-\nture extractor, RADIOv2 [58]. Our segments-based approach applied on top of this en-\ncoder substantially improves its performance, although our DINOv2-based SegVLAD\nachieves the best performance."}, {"title": "9.5 17Places dataset: Ground Truth Vagueness", "content": "For VPR, ground truth is often defined in terms of either GPS coordinates [3] or image\nframe separation [45]. There is also often a discrepancy in defining ground truth for\nthe VPR task, depending on how a 'place' is defined and whether the camera position\nor visual overlap is used as a criterion for correct recognition, as discussed in [19].\nIn Figure 6, we present extended results for 17Places dataset to indicate how relative\nranking of two methods can potentially switch depending on the choice of localization\nradius (in terms of frame separation)."}, {"title": "10 Implementation and Benchmarking Details", "content": "In this section, we provide detailed implementation of our factorized representation for\naggregation, backbone networks and benchmark datasets."}, {"title": "10.1 Factorized Aggregation", "content": "We proposed a factorized representation for feature aggregation as a unified method\nto aggregate at segment/global level for different aggregation types (see Section 3.3,\nEquation 2 in the main paper). In this section, we further elaborate this with explicit\nformulations for computing SegVLAD, GlobalVLAD, Global Average Pooling (GAP),\nSegment Average Pooling (SAP), and Generalized Mean Pooling (GeM) using the pro-\nposed factorization:\n$F_{S \\times D} = A'_{S \\times S} . M_{S \\times N} . T_{N \\times D}$   (5)"}, {"title": "10.2 Backbone Networks", "content": "DINOv2: We follow AnyLoc [32] and use its default ViT-G backbone with the value\nfacet features from layer 31. For the DINOv2 finetuned model, we followed SALAD [29]\nwhich by default uses ViT-B backbone. Note that SALAD's aggregation is different\nfrom the soft assignment based VLAD aggregation proposed in NetVLAD [3]. Since\nour method is based on Hard-VLAD aggregation, we followed SALAD's finetuning ap-\nproach but replaced their aggregation with NetVLAD. Similar to SALAD, we only train\nthe last 4 layers of DINOv2 (ViT-B) on the GSV dataset [1] with training image reso-\nlution as 224 \u00d7 224. Similar to NetVLAD, we used 64 clusters which were initialized\nby randomly sampling images from the GSV training set.\nSAM: We use its ViT-H model with default parameters for segmentation. It generates\nmasks for the entire image, using a grid of point prompts (32 along each edge), which\nare subsequently filtered based on IOU and stability score.\nEvaluation: For evaluation, we used 640 \u00d7 480 image resolution for DINOv2 encoder\nand 320 \u00d7 240 for SAM. For AmsterTime, we followed [75] and used a fixed resolution\nof 256 \u00d7 256 for both the models. Note that we follow the exact same procedure of\nimage resizing when comparing our segments based approach with their global coun-\nterparts, i.e., AnyLoc [32] and SALAD [29]. Additionally, for the baseline methods"}, {"title": "10.3 Datasets", "content": "In this work, we used a variety of datasets covering both outdoor and indoor environ-\nments. Outdoor datasets include Pitts30k [69], AmsterTime [75], Mapillary Street Level\nSequences (MSLS) [72], SF-XL [6], Revisted Oxford5K and Revisited Paris6k [55]\nIndoor datasets include Baidu Mall [67], 17Places [80] and InsideOut [27]. While\nall these datasets exhibit strong viewpoint variations, they are significantly diverse in\nterms of appearance changes, perceptual aliasing (repetitive elements), type of environ-\nment/domain (indoor vs outdoor) and extent of temporal changes (e.g., matching his-\ntorical images). We elaborate on each of the datasets in detail in the supplementary. In\nTable 12, we report the number of reference and query images in each of these datasets.\nFurthermore, for our segments-based approach, we also note the total number of seg-\nments (based on SAM [37]) across all the queries and references for each dataset, with\nthe final columns listing the average number of segments per image. This highlights the\nscale at which we perform segments based retrieval, as opposed to global descriptors\nbased retrieval."}]}