{"title": "On Limitations of the Transformer Architecture", "authors": ["Binghui Peng", "Srini Narayanan", "Christos Papadimitriou"], "abstract": "What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.", "sections": [{"title": "Introduction", "content": "The Transformer architecture [VSP+17], an otherwise singularly promising approach to AI, is known to be plagued by the problem of hallucinations: an answer to a user prompt is too often incompatible with the device's training data and prompt. There is now a vast literature on hallucinations and their nature, typology, and remedies, see for example the survey [JLF+23].\nAre there root causes of the hallucination phenomenon that can be imputed to the Transformer architecture? Theoretical limitations of Transformers have been pointed out in the past, starting with Hahn's 2020 paper [Hah20], where it was proved that Transformers cannot recognize simple patterns such as parity (e.g., whether a phrase contains an even number of negations) or balanced parentheses. However, the elegant proofs in [Hah20], inspired by Complexity Theory, are asymptotic in nature, and it appears that the proven limitations take hold only for unrealistically large inputs; in fact, it has been demonstrated that there are Transformers on which these functions can be computed reliably for all practical purposes [EGZ20, YPPN21]. Transformers have also been studied through Complexity Theory an important lens for understanding the limitations of computational systems culminating to [MS23b], where it was shown that, computationally, Transformers belong to a rather weak complexity class, namely logspace-uniform TC; we elaborate on this point of view in Section 4.\nAlso recently, Sanford, Hsu, and Telgarsky [SHT23] identified a particular mathematical problem called 3-Matching, which cannot be computed by single-layer multi-head Transformers: Given a sequence of integers, identify three numbers in the sequence that add up to zero modulo a given large number. Very interestingly, (a) the easier problem of 2-Matching can be solved by Transformers, but cannot be solved by feed-forward deep nets, establishing a kind of supremacy of Transformers over the rest of ML; and (b) these negative results manifest themselves for rather small prompt sizes. Note that, the conceptual contribution of this result notwithstanding, the 3-matching function identified in [SHT23] is not a compelling example of the kinds of problems that Transformers are meant to solve. The question arises, can we identify impossible tasks that are closer to the intended use of the architecture? This is our focus here. We point out a new fundamental limitation of the Transformer architecture, namely that it has serious difficulties computing a very simple, and practically important, semantic operation we call function composition, explained next.\nIn a recent paper on the topic of hallucinations [GLL+24], the opening example is a wrong answer to the question what is the birthday of Fr\u00e9d\u00e9ric Chopin's father?, when these two facts were included in the prompt: (a) the father of Fr\u00e9d\u00e9ric Chopin was Nicolas Chopin and (b)\nTransformers occasionally hallucinate, that is, they generate output inconsistent\nwith the training data and prompt. However, this deficiency is rather rare,\nand techniques such as post-filtering can alleviate it.\nUnderstanding what is meant by the last word it of this utterance, one needs to compose two index-icals: First one has to recognize that the indexical it points to this deficiency, and then that this deficiency refers to the particular deficiency that Transformers occasionally hallucinate. It appears that humans have little difficulty composing idexicals but how about Transformers?\nSo it seems desirable indeed important that LLMs perform function composition reliably. In this paper we prove that they cannot: function composition is an inherent weakness of the Transformer architecture. In particular, we show that a single Transformer attention layer cannot compute the answer to a function composition query correctly with significant probability of success, as long as the size $n$ of the domain of the function satisfies $n \\log n > H(d+1)p$, where $d$ is the embedding dimension, $p$ is the precision, in bits, required for the calculation, and $H$ is the number of attention heads. In fact, the proof of our impossibility theorem suggests that this weakness has its roots in the nature of the softmax computation that allows the next embedding of a token to be computed with very scant non-local information.\nOur impossibility result holds for a single, multi-headed attention layer; however, we suspect that the weakness that it exposes also burdens multi-layer Transformers, and in Appendix A we see anecdotal evidence that LLMs appear unable to reliably compose functions with domains that are far smaller.\nThe chain of thought maneuver (CoT) [WWS+22] is known to help with the problem of hallucinations by inducing the LLM to generate prompts which break down the task in hand into smaller steps eventually leading to the correct answer. Indeed, a simple CoT scheme can plausibly mitigate our impossibility result on composition by generating a short prompt. However, we also prove a theorem implying that a Transformer layer with CoT needs far more tokens in the generated CoT prompt to solve the composition problem for a cascade of many compositions (Theorem 2)\nWe also provide a different impossibility argument that holds for any number of layers, and concerns a different genre of hallucinations: wrong answers to compositionality tasks. In [DLS+23], it is demonstrated through extensive experimentation that Transformers have trouble carrying out tasks that require sequential composition of elementary tasks, such as multiplying multi-digit integers and solving logical puzzles, and in fact this failure grows quickly with the required depth of composition. We point out (Theorem 3) that, under a widely accepted complexity assumption akin to P\u2260 NP albeit in the domain of memory multi-layer Transformers are incapable of performing several elementary computations that are crucial in carrying out compositionality tasks such as those discussed in [DLS+23].\nFinally, it has been often pointed out that it is the very nature of Transformers as probabilistic language generators that renders them likely to veer off their grounding on the training data and prompt see for example [MYF+23], where it is demonstrated through extensive experimentation"}, {"title": "Preliminary definitions", "content": "Transformer. To model mathematically computation by Transformers, we adapt slightly the formal model of [SHT23]. A self-attention unit is a function $A : (\\mathbb{R}^D)^N \\rightarrow (\\mathbb{R}^d)^N$, where $N$ is the sequence length, $D$ is the embedding dimension, and $d$ is the output dimension. $A$ is defined in terms of three real matrices $K, Q, V \\in \\mathbb{R}^{d \\times D}$. For simplicity, we assume that the key, query, value matrices $K, Q, V$ share the same dimension. On input $X = (x_1,...,x_n) \\in (\\mathbb{R}^D)^N$, the attention unit $A$ calculates, for each $i = 1,..., N$, the output\n$Y_i = \\sum_{j\\in[N]} r_{i,j}Vx_j \\in \\mathbb{R}^d$\nwhere the attention probability\n$(r_{i,1},...,r_{i,n}) = softmax(x_i^TQx_1,...,x_i^TQKx_N)$\n$= \\frac{exp(x_i^TQKx_1)}{\\sum_{j\\in[N]} exp(x_i^TQKx_j)} = \\frac{exp(x_i^TQKx_N)}{\\sum_{j\\in[v]} exp(x_i^TQKx_j)}$\nWe assume that the computations of the self-attention unit are carried out with a precision of p bits.\nAn H-headed transformer layer L consists of H self-attention units sharing the same input, together with a combining function which maps, for each $i$, the H outputs of the layer to an output token in $\\mathbb{R}^d$. Finally, a Transformer T is the cascade of several transformer layers.\nNotice that our definition ignores certain features of the Transformer architecture, such as input embedding and pre- and post-processing of individual tokens by feed-forward networks; however, it is easy to see that input embedding and the pre-processing can be absorbed in the input tokens, while post-processing can be incorporated into the combining function $\\Phi$, and therefore, this omission does not affect the validity of our argument.\nFunction composition. We next define the function composition problem. Consider two functions, $g$ mapping a domain $A$ to a domain $B$, and $f$ mapping $B$ to another domain $C$ for example, $g(a)$ could be the mother of person $a \\in A$, and $f(b)$ is the profession of person $b \\in B$. These functions will be described in a prompt X. The N tokens of X are divided into three parts:\nPart 1. The first part describes the function $g$ through $|A|$ sentences in simple, unambiguous language separated by punctuation, e.g. the mother of John is Helen; the mother of Charlotte is Eve; etc.;\nPart 2. Similarly, the second part consists of $|B|$ sentences describing the function $f$, e.g. Helen is a doctor; Jane is a teacher; etc. and"}, {"title": "The Impossibility of Composition", "content": "We prove the following:\nTheorem 1. Consider a function composition problem with input domain size $|A| = |B| = |C| = n$, and an H-headed transformer layer L with embedding dimension d and computation precision p, and assume that $H(d+1)p < n \\log n$. Then L cannot solve correctly the function composition problem. In particular, if $R = n \\log n - H(d+1)p > 0$, then the probability, over all possible functions and queries, that L answers the query incorrectly is at least $\\frac{R}{3n \\log n}$.\nThe proof relies on Communication Complexity [KN96], a subfield of Complexity Theory in which one measures the number of bits that need to be exchanged between distributed computational agents possessing different parts of the input, in order for one of them to obtain the result of the computation. The agents are assumed to have unbounded computational capabilities, and to be restricted only in terms of their communication. One important classical result in Communication Complexity is the communication difficulty of the set disjointness problem: If Bob and Alice are each given a vector of n bits and they want to compute the Boolean inner product of these bit vectors that is, to tell if there is an index $i$ such that the $i$th bit of both Bob and Alice is al then they must communicate n bits. In fact, this result used is in the impossibility proofs in [SHT23]. Another classical problem in this field is pointer chasing: Alice and Bob are given two functions A and B, respectively, from [n] to [n], and they need to compute a composition of these functions, say $A(B(A(B(A(0)))))$. Compositions can obviously be computed by the agents alternatingly communicating log n bits to each other: in this example, Alice communicates to Bob the value of A(0), then Bob tells Alice the value of B(A(0)), and so on. But what if one less rounds of communication is allowed? Or, if the same number of rounds is allowed, but Bob must start? Over the four decades since this problem was first posed [PS82], it has been shown that, if one less round of communication is desired, or if Bob has to start the communication, then exponentially more bits must be exchanged. These lower bounds have been used to inform many fields of Complexity, including lower bounds of bounded-depth circuits, see for example [KN96], and even the complexity of Machine Learning [CPP22].\nHere we use a slight variant of this classical framework. We assume three agents, whom we call Faye, Grace, and Xavier. Faye knows a function $f$ from $[n]$ to $[n]$, Grace knows another such function $g$, and Xavier knows a number $x \\in [n]$. We can actually assume that Faye and Grace both know Xavier's value $x$. The only communication allowed is from Faye and Grace to Xavier not between Faye and Grace. Our goal is for Xavier to know the value $f(g(x))$ so that Faye communicates to Xavier as few bits as possible. Notice that we do not restrict the number of bits Grace can communicate to Xavier.\nLemma 1. If fewer than $n \\log n$ bits are communicated by Faye to Xavier, then Xavier cannot know the value $f(g(x))$. In particular, if only $n \\log n-R$ bits are communicated from Faye to Xavier for some $R > 0$, then the probability, over all pairs of functions, that the composition is computed incorrectly is at least $\\frac{R}{3n \\log n}$.\nProof. The proof of the first statement of the lemma is elementary. Since Faye and Grace cannot communicate, and communication from Grace to Xavier is free, we may assume that Grace communi-cates to Xavier the value of $g(x)$ intuitively, the only information he needs from her. Even though progress seems to have been made, Xavier must now apply $f$ to $g(x)$, and he knows nothing about $f$. This difficulty must be solved through communication from Faye to Xavier, as further communication from Grace obviously cannot help. There are $n^n$ possible functions $f$, and to describe the actual one completely Faye needs to send Xavier at least $n \\log n$ bits, the logarithm base two of $n^n$ in fact $n \\lceil \\log n \\rceil$ bits suffice, because Faye can send to Xavier the values $f(0), f(1), ...$ in this order. If Faye sends to Xavier fewer than $n \\log n$ bits, say $B$ bits, the precise bitstring she sends is determined by the function $f$ she knows, and thus this communication subdivides the $n^n$ candidates for the $f$ function into $2^B$ categories one for each value of the B bits sent by Faye to Xavier. Since $B < n \\log n$, we have that $2^B < n^n$, and hence, by the pigeonhole principle, at least two different candidate functions $f$ and $f'$ result in the same communication from Faye to Xavier, and Xavier has no way of knowing which of the two functions is the correct one. These two functions must differ in at least one value, say $f(z) \\neq f'(z)$. But what if it so happens that $g(x) = z$? In this case, Xavier has no way of knowing the correct answer to the problem he is supposed to solve, completing the proof of the first statement.\nThe proof of the second, quantitative, statement is more technical and relies on Information Theory and mutual information. Recall the input of Grace is a random mapping $g : [n] \\rightarrow [n]$, for any fixed input of Xavier $x \\in [n]$. Let $i^* = g(x) \\in [n]$, then we know that $i^*$ is uniform over $[n]$. Let $\\Pi$ be the message sent from Faye to Xavier, it has $n \\log(n) - R$ bits. We first bound the mutual information between $\\Pi$ and $f(i^*) = f(g(x))$:\n$I(\\Pi; f(i^*)|i^*) = \\sum_{i=1}^n Pr[i^* = i] \\cdot I(\\Pi; f(i^*)|i^* = i)$\n$= \\frac{1}{n} \\sum_{i=1}^n I(\\Pi; f(i))$\n$\\leq \\frac{1}{n} \\sum_{i=1}^n I(\\Pi; f(i)|(f(j))_{j<i})$\n$= \\frac{1}{n} I (\\Pi; f (1), ..., f (n))$\n$\\leq \\frac{n \\log n - R}{n} = \\frac{R}{\\log n}$\nThe first step follows from the definition of conditional mutual information. The second step follows from the facts that $i^*$ is uniform over $[n]$ and $\\Pi$ is independent of $i^*$. The third step follows from $(f(j))_{j<i}$ are independent of $f(i)$, and the fourth step is the chain rule.\nNotice that Xavier's output is just a post-processing of $\\Pi$ and $i^*$, and denote by $\\delta$ its error probability. Then by Fano's Inequality, we have\n$H(\\delta) + \\delta \\log(n) \\geq H(f(i^*)|\\Pi, i^*) = H(f(i^*)|i^*) - I(\\Pi; f(i^*)|i^*) \\geq \\log n - \\frac{R}{\\log n}$\nHere the first step follows from Fano's inequality, the third step follows from Eq. (2).\nFrom Eq. (3), we can derive that $\\delta \\geq \\frac{R}{3n \\log n}$.\nRemark 1. The lower bound on the error probability in the statement of the Lemma, $\\frac{R}{3n \\log n}$, is optimal within a small constant factor. In proof, an upper bound of $\\frac{R}{[n] \\log n}$ is possible, by the following construction (we assume n is a power of 2 for simplicity): Faye sends to Xavier $n \\log n - R$ bits, by which she encodes the the first $n - [\\frac{R}{\\log n}]$ values of the function. With this scheme, an error happens if one of the last $[\\frac{R}{\\log n}]$ values of the function is queried, and answered the wrong way. This probability is at most $\\frac{R}{n \\log n}[\\frac{R}{\\log n}]$, which is just a factor of three greater than the lower bound.\nWe now turn to the proof of the theorem.\nProof. For the purposes of contradiction, suppose that there is a self-attention layer L that can reliably combine any two functions $f$ and $g$ on any domains A, B, C of size n, such that $n \\log n > H(d + 1)p$. We will show that this contradicts Lemma 1.\nSuppose that the three agents Faye, Grace, and Xavier are as in the lemma, and they wish to compute $f(g(x))$; we claim that they could do so by using L as follows: They put together the prompt of a function composition problem to be solved by L, where Faye supplies the token strings associated with function $f$ say, the value of $f$ applied to 0 is 3, etc. Grace supplies similarly the tokens associated with function $g$, and Xavier supplies the query part: what is the result of $f$ applied to $g$ applied to 23? recall that Xavier knows x, and it happens to be 23. We assume the token corresponding to 23 is token t, that is, $x_t = 23$. Then the three agents compute the result of the computation of L that corresponds to the token t, as explained below. Recall that, by our assumption that L performs function composition, this result must be the required answer $f(g(x))$.\nThe three agents communicate to compute, for each head, the final embedding that corresponds to the token t, and then Xavier applies the finishing function $\\Phi$ to compute the final result, which will be the answer of the composition problem. For each head, the result at token N can be written as\n$Y_t = \\frac{\\sum_{j=1}^N r_{t,j}Vx_j}{\\sum_{j=1}^N t,j}$, where $r_{t,j} = exp(x_t^TQKx_j)$.\nThe key observation now is that this expression can be written as $y_t = \\frac{F+G+X}{F'+G'+X'}$, where F is the part of the numerator that corresponds to Faye's tokens $x_j$, similarly G corresponds to Grace's tokens, X corresponds to Xavier's tokens, and similarly for the denominator. Hence, Faye can compute and communicate to Xavier quantities F and F', and similarly for Grace and G, G'; then Xavier can add to these the terms X and X', divide, and thus compute $y_N$. Repeating for all heads and combining with $\\Phi$, Xavier can compute $f(g(x))$ and obtain the desired answer to the composition problem. But now notice that this was accomplished through a communication of only $H(d+1)p$ bits from Faye to Xavier dp bits for F and p bits for F' for each of the H heads. By hypothesis, this quantity is less than $n \\log n$, contradicting Lemma 1. The second part on error probability follows from the same reduction, completing the proof.\nChain of Thought\nCan CoT help solve the composition problem? Intuitively, the answer is \"yes.\" For any composition problem for example, the prompt about Turing, London, and England in the introduction we can help the LLM successfully answer the question \"In which country was Turing born?\" by gen-erating a short CoT prompt that breaks the question into simpler ones, e.g. \"Let's see, Turing was born in GENERATE, and GENERATE is in the country of GENERATE, so Turing was born in GENERATE.\" However, we prove below that an arbitrarily large number of CoT steps are needed to solve the generalization of composition to many function applications. In the iterated function composition problem we are given K functions $f_1, f_2, ..., f_K$, and we need to calculate $f_K (f_{K-1}(... (f_1(x))))$ for $x \\in [n]$. In fact, in our proof we shall consider $f(K)(x) = f(f(... f(x)))$ that is, we shall take $f_1 = ... = f_K$.\nTheorem 2. Let H be the number of attention heads, d the embedding dimension, p the computation precision, and n be the domain size of the iterated composition problem. A Transformer layer requires $(\\frac{n}{\\sqrt{Hdp}})$ CoT steps for answering correctly iterated function composition prompts.\nProof. We reduce from another classical problem in Communication Complexity called pointer chasing. Let n and c be two positive integers. In the (n, c)-pointer chasing problem, Alice knows a function $f_A : [n] \\rightarrow [n]$ and Bob knows another function $f_B : [n] \\rightarrow [n]$. The pointers $w(1), w(2),...$ are recursively defined as\n$w(1) = 1, w(2) = f_A(w(1)), w(3) = f_B(w(2)), w(4) = f_A(w(3)), w(5) = f_B(w(4)),$\nThe communication proceeds for 2r rounds, with Alice starting. The goal is for Bob to output the binary value of $w(2r+2) (mod 2)$. The following summarizes what is known about this problem:\nLemma 2 ([NW91, Kla00, Yeh20]). Any randomized protocol for the pointer chasing problem with error probability at most 1/3 under the uniform distribution must involve the transmission of at least $n/(2000c) - 2c \\log n$.\nLemma 3. For any $K \\geq 1$, suppose there is a Transformer layer L with H attention heads, dimension d, and precision p that solves the K-iterated function composition within R CoT steps, then there is a communication protocol for (n, K 1)-pointer chasing, communicates in 2R rounds and exchanges $2RH(d+1)p$ bits.\nProof. Recall that in the pointer chasing problem, Alice receives a function $f_A: [n] \\rightarrow [n]$ and Bob receives a function $f_B : [n] \\rightarrow [n]$. Define a single mapping $f : [2n] \\rightarrow [2n]$ such that\n$f(i) = \\begin{cases} f_A(i) + n & i \\in [n] \\\\ fB(i-n) & i \\in [n + 1 : 2n] \\end{cases}$\nFor any $i \\in [n]$, we have that $f^{(k)} (i) = (f_B \\circ f_A)^{(k)} (i)$ holds for any integer k > 1.\nSuppose there is a Transformer layer L that solves the K-iterated function composition problem using R CoT steps; we shall construct a communication protocol for pointer chasing based on it. Alice and Bob put together the function composition task for L, where Alice supplies the description of $f(1),..., f(n)$, using her knowledge of $f_A$, and Bob supplies the description of $f (n + 1), ..., f (2n)$, using his knowledge of $f_B$. For simplicity, we assume the i-th input token, $x_i$, contains the information of $f(i)$, for any $i \\in [2n]$; the query $X_{2n+1}$ appears at position 2n + 1 and it asks for $f(K)(1)$. Let $K_h, Q_h, V_h$ be the key, query, value matrices of the h-th attention head.\nThe communication protocol proceeds in 2R rounds, where the 2r - 1, 2r rounds simulate the r-th step of CoT. Formally, for r = 1, 2, . . ., R, the protocol proceeds as follows:\nAt round 2r - 1, for each attention head $h \\in [H]$, Alice computes $\\sum_{i\\in[n]} exp(x_{2n+r}Q_h^TK_hx_i) \\in \\mathbb{R}$ and $\\sum_{i\\in[n]} exp(x_{2n+r}Q_h^TK_hx_i) V_hx_i \\in \\mathbb{R}^d$, and sends them to Bob;\nAt round 2r, for each attention head $h \\in [H]$, Bob computes $\\sum_{i\\in[n+1:2n]} exp(x_{2n+r}Q_h^TK_hx_i) \\in \\mathbb{R}$, $\\sum_{i\\in[n+1:2n]} exp(x_{2n+r}Q_h^TK_hx_i) V_hx_i \\in \\mathbb{R}^d$ and sends them to Alice;\nAlice and Bob compute, locally, the output $Y_{2n+r,h} (h \\in [H])$ as\n$Y_{2n+r,h} = \\frac{\\sum_{i\\in[2n]} exp(x_{2n+r}Q_h^TK_hx_i) V_hx_i + \\sum_{i \\in [r]} exp(x_{2n+r}Q_h^TK_hx_{2n+i})V_hx_{2n+i}}{\\sum_{i\\in[2n]} exp(x_{2n+r}Q_h^TK_hx_i) + \\sum_{i\\in[r]} exp(x_{2n+r}Q_h^TK_hx_{2n+i})}$\nand the next token $X_{2n+r+1} = \\Phi (Y_{2n+r,1},..., Y_{2n+r,H})$\nAfter 2R rounds, Bob knows the output of R-fold CoT, and can compute $f^{(K)} (1) = (f_B \\circ f_A)^{(K)} (1)$, this resolves the (n, K \u2212 1)-pointer chasing task. The total number of bits communicated are 2R\u00b7H(d+1)p, as required by the lemma.\nCombining Lemma 2 and Lemma 3, and taking $K = \\frac{n}{\\sqrt{100 \\frac{Hdp}{n}}}$, we complete the proof of theorem."}, {"title": "Compositionality and Logarithmic Space", "content": "In a recent paper [DLS+23] a novel genre of hallucinations was identified: extensive experimental evidence is presented that Transformers perform poorly on compositional tasks, that is, tasks requiring the repeated iteration of elementary tasks; similar phenomena have been observed elsewhere [MS23b, FGZ+23, MS23a]. The main examples of compositional tasks explored in [DLS+23] are:\nmultiplication of multi-digit integers modeled as an arithmetic circuit with single-digit values and inputs;\na simple sum maximization problem over a sequence of integers under the restriction that two successive integers cannot be both added to the sum; this again can be reduced through dynamic programming to an arithmetic circuit with plus and max gates; and\nLogic puzzles such as \"Einstein's Riddle\"\nWrong answers to large families of simple questions such as these constitute a special category of hallucinations, and it is of interest to explore its causes. It turns out that, to do so, we must turn the page of our negative results of the previous section and Communication Complexity arguments, and employ the theory of Computational Complexity [Pap93, AB09] to study certain basic computational problems underlying the tasks studied in [DLS+23]:\nCircuit evaluation: Given the description of a circuit with gates, which can be either Boolean or arithmetic operations, as well as the values of all input gates of the circuit, evaluate the output(s) of the circuit. Multiplying decimal integers with multiple digits is an example of such a circuit; solving the adjacency-restricted largest sum problem of [DLS+23] is also the evaluation of a circuit, this time with + and max gates.\nDerivability is yet another generalization of our composition task of the previous section which we believe captures many aspects of the informal notion of compositionality. We are given a finite domain S intuitively, the partial solutions of the problem in hand and a relation D \u2286 S \u00d7 S intuitively, legal one-step derivations. We are also given two subsets of S, I (for initial partial solutions) and F (for final partial solutions). The question is: are there elements $a_1, a_2,..., a_k \\in S$ such that (a) $a_0 \\in I$; (b) $a_k \\in F$, and (c) for all j such that 0 < j \u2264k, $D(a_{j\u22121}, a_j)$?\nLogical reasoning: Logic puzzles can be typically formulated as instances of satisfiability (or SAT). This problem is NP-complete and, even though large instances arising in practice can be solved by sophisticated techniques developed over decades of research, it would not be surprising if LLMs may have problems in dealing with arbitrary logical expressions. There are, however, three tractable special cases of SAT that underlie much of tractable common-sense reasoning: 2-SAT, the satisfiability of CNF formulas with two literals in each clause; Horn SAT, the corresponding problem for Horn clauses, that is, clauses with at most one positive literal; and Mod 2 SAT, the solution of a system of linear equations modulo 2. Note that these are the only nontrivial tractable special cases of SAT. Can we expect LLMs to handle them?\nWe point out below that, assuming certain well accepted conjectures in Computational Complexity, all of these tasks are impossible for a multi-layer Transformer to perform reliably and for large enough prompts. We start with the following:\nObservation 1. The computation of a multi-layer Transformer on a prompt of length N can be carried out with O(log N) bits of memory.\nThis is not a new result; it has been recently established in [MS23b] that the computation of Transformers lies in the complexity class log-uniform TC, which is more restrictive than logarithmic memory; in fact, the implications of this result for the capabilities of Transformers are also briefly"}, {"title": "Discussion", "content": "We used complexity arguments of two different genres Communication Complexity and Computational Complexity in order to elucidate certain shortcomings of the transformer architecture resulting in different kinds of hallucinations. We showed that the elementary function composition problem cannot be solved by a single Transformer layer, and that CoT can solve the iterated composition problem only by generating a prompt that has length $\u03a9(\\sqrt{N})$. These mathematical results are limited in two ways: (a) they take hold when the domain size is larger than the dimension parameters (which are typically in the hundreds), and (b) they break down for multiple layers. We also provide evidence from Complexity Theory of the classical Turing machine variety that the compositionality tasks known empirically to be hard for Transformers contain computational ingredients and primitives that are impossible for Transformers to deal with.\nThe reader is reminded that the complexity arguments we employ here come with caveats. The impossibility result for composition holds for a single layer, in a probabilistic way (the error probability is nonzero but not one), and only when the domain of the functions is larger than the parameters of the Transformer layer. The results based on Computational Complexity come with a different set of caveats: They hold only if certain yet unproven, if widely accepted, conjectures are true, and even then they are asymptotic, holding for instances larger than an unknown size, for which the assumed asymptotic conjectures take effect. Complexity results such as these are mere warnings that these problems have a fundamental incompatibility with with the Transformer architecture, and therefore one should not expect that these problems to be solvable in practice ad infinitum. However, as with other complexity conjectures such as P \u2260 NP, it is often the case that computational difficulties start happening for instances of reasonably small size. For example, we already know [DLS+23] that Transformers have difficulties with compositionality tasks of rather small sizes, and in Appendix A we present anecdotal evidence that LLMs often respond erroneously to prompts of small sizes related to function composition. Naturally, the opposite phenomenon is also common: some NP-complete problems such as 3SAT seem to be amenable to efficient solutions in practice for very large instances; however, this is typically accomplished by extensive exploration over decades of a large arsenal of algorithmic techniques relying on the kinds of instances that appear in practice, and not through a fixed algorithmic architecture.\nThe real tragedy of complexity lower bounds is not their tentative and asymptotic nature; it is that (a) they are rare and hard to come by, and they come in very few known kinds; and (b) they tend to be overly conservative, in that they often vastly overestimate the capabilities of the computational agents they are trying to delimit. Lemma 1 is a good example: it is designed to hold even if Grace uses the most sophisticated math to encode her input into her message. But when the lemma is applied in Theorem 1, Grace is very restricted, in that her message to Xavier is not a clever coding of her tokens, but the two particular simple numerical expressions that we call G and G' in the proof. It is intuitively clear that this computation the token values projected, exponentiated, and then cross-multiplied and averaged is a very poor way to encode the n values of function g so that Xavier can recover each one of them readily. This observation highlights an interesting open problem, the opportunity to develop a more sophisticated variant of Communication Complexity for computationally restricted agents in order to study the limitations of devices such as the Transformer.\nFinally, our two genres of negative results suggest an intriguing challenge: What would it take to design a different attention layer that is immune to these two lower bound techniques, while maintaining the architecture's efficiency and effectiveness in practice? Our proof suggests a version of the softmax computation in the attention layer that is either not commutative or not associative, or one that requires more than logarithmic space. But, of course, simply evading a lower bound technique does not guarantee a tangible improvement in performance..."}, {"title": "Examples", "content": "We show here a few qualitative results that illustrate the difficulty of composition for state-of-the-art LLMs. A full empirical exposition for the case of compositional failures can be found in [DLS+23"}]}