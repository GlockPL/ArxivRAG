{"title": "Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models", "authors": ["Michihiro Yasunaga", "Luke Zettlemoyer", "Marjan Ghazvininejad"], "abstract": "Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for evaluating multimodal reward models. To address this gap, we introduce Multimodal RewardBench, an expert-annotated benchmark covering six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. Our dataset comprises 5,211 annotated (prompt, chosen response, rejected response) triplets collected from various VLMs. In evaluating a range of VLM judges, we find that even the top-performing models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall accuracy. Notably, most models struggle in the reasoning and safety domains. These findings suggest that Multimodal RewardBench offers a challenging testbed for advancing reward model development across multiple domains. We release the benchmark at https://github.com/facebookresearch/multimodal_rewardbench.", "sections": [{"title": "1 Introduction", "content": "High quality reward models or judges are essential for aligning language models (LMs) and vision-language models (VLMs) (Ouyang et al., 2022; Bai et al., 2022b; Llama Team, 2024). Reward models assess the quality of model outputs, guiding models towards accurate, helpful, and safe outputs via RLHF-style algorithms. However, existing benchmarks for evaluating reward model quality are typically limited to the text modality (Bai et al., 2022a,a; Stiennon et al., 2020; Tan et al., 2024). While recent work has evaluated VLM judges (Lee et al., 2024a; Xiong et al., 2024; Chen et al., 2024;"}, {"title": "2 Related works", "content": "Multimodal LLMs and benchmarks. Vision-language models (VLMs)\u2014which take both text and images as input and generate text as output-have rapidly proliferated in recent years (Liu et al., 2024b; Dai et al., 2023; Deitke et al., 2024; Li et al., 2024a; Yasunaga et al., 2023; Zhou et al., 2024). These models are applied to a variety of tasks, including visual question answering (VQA), image captioning, and visual instruction following. Numerous benchmarks evaluate the performance of VLMs on these tasks, such as LlavaBench (Liu et al., 2024b), VisitBench (Bitton et al., 2023), Nocaps (Agrawal et al., 2019), RealworldQA (Grok-1.5 Team, 2024), MMBench (Liu et al., 2025), SEED-Bench Li et al. (2023), and MMMU (Yue et al., 2024a,b). However, these benchmarks do not evaluate the task of VLM reward modeling or judging, which is the focus of this work."}, {"title": "3 Multimodal Reward Bench", "content": "We introduce Multimodal RewardBench, a holistic, expert-annotated benchmark for evaluating VLM reward models."}, {"title": "3.1 Overview", "content": "Framework. As shown in Figure 1, each instance in our benchmark consists of:\n\u2022 Prompt: A text-and-image input that users provide to VLMs. For example: \"What can you cook with these items ?\". We refer to this as the base task.\n\u2022 Chosen response (R1) and rejected response (R2): Two response candidates, where R1 is the correct or human-preferred response, and R2 is the incorrect or non-preferred response.\nFor each instance (prompt, R1, R2), where randomly order R1 and R2, a VLM reward model or judge predicts which of R1 or R2 is better (we refer to this as the judgment task), which is a binary classification. We then evaluate the accuracy of these predictions. We detail how we collect the prompts and responses in \u00a73.2 and how we label chosen and rejected responses in \u00a73.3.\nHolistic dimensions. We evaluate VLM reward models across six key dimensions. These dimensions build upon previous holistic evaluations of foundation models (e.g., HELM Liang et al. 2022 and VHELM Lee et al. 2024b), while incorporating additional aspects specific to reward models, such as correctness judgment and human preference judgment. See Table 2 for a summary.\n\u2022 General correctness: This dimension evaluates general-domain, long-form generation tasks such as visual instruction following and long captioning. The judgment focuses on response correctness, comparing correct responses against incorrect ones that contain factual errors, visual recognition errors, or reasoning errors.\n\u2022 General preference: This dimension also addresses general-domain, long-form generation tasks. However, the judgment focuses on human preferences between responses that are either both correct or both incorrect, identifying which response is more aligned with human preferences.\n\u2022 Knowledge: This dimension evaluates tasks requiring domain-specific knowledge in areas such as humanities, social sciences, business, medicine, and STEM. The judgment focuses on response correctness.\n\u2022 Reasoning: This dimension assesses problem-solving capabilities in areas such as mathematics and coding, with judgment focused on response correctness.\n\u2022 Safety: This dimension evaluates safety awareness, with judgments selecting the safer and correct response. Our safety assessment focuses primarily on bias (avoiding unwarranted associations regarding gender and race) and toxicity (identifying and avoiding offensive or harmful content such as hate speech, violent speech, or abusive language) (Lee et al., 2024b). While other safety-related topics like fairness, robustness, and NSFW exist, we defer their evaluation to future versions of Multimodel RewardBench due to limited suitable datasets.\n\u2022 VQA: This dimension covers diverse short-form visual question answering tasks established in the research community (Liu et al., 2025; Li et al., 2023; Grok-1.5 Team, 2024). These tasks span various skills including visual perception (object, spatial, and scene recognition) and visual reasoning (action, physical, social, contextual, and temporal reasoning). The judgment focuses on response correctness.\nOur benchmark is comprehensive in both topic coverage and evaluation methodology. Topic-wise, we evaluate general domain, knowledge, reasoning, and safety dimensions, following HELM's (Liang et al., 2022) comprehensive approach. Task-wise, we incorporate both long and short response formats, and include two types of judgment tasks: comparing correct versus incorrect responses, and evaluating human preference between responses of similar correctness. This broad coverage ensures reward models can be evaluated for robust VLM alignment across diverse scenarios. Importantly, all prompts in our benchmark incorporate both text and image components."}, {"title": "3.2 Prompt and response collection", "content": "General correctness and preference. For prompt collection, we draw prompts from VisitBench (Bitton et al., 2023), which focus on visual instruction following (e.g., \"Write a fairy tale based on this painting\"). We also create long caption generation prompts (e.g., \"Describe this image in detail\") using images from Nocaps (Agrawal et al., 2019).\nTo gather responses, we use several recent VLMs as follows:"}, {"title": "3.3 Human annotation of judgment labels", "content": "We perform human annotation of judgments for long, free-form response cases across the General Correctness/Preference, Knowledge, and Reasoning (math) categories, as no ground truth labels are available. For other categories, we use existing ground-truth labels to prepare chosen/rejected responses as discussed in \u00a73.2.\nAnnotation by human experts. High-quality annotation is essential for building a trustworthy benchmark. One key challenge was finding domain experts capable of accurately annotating knowledge- and reasoning-intensive tasks. These included MMMU-Pro (requiring college-level expertise in 30 subjects across humanities, social science, and STEM) and MathVista (requiring math expertise). We partnered with Surge AI to recruit expert annotators for general domain, mathematics, and each of the 30 MMMU-Pro subjects, offering compensation at $250 per hour.\nAnnotation tasks. For the General Correctness/Preference categories, we present annotators with the prompt and two response candidates (R1 and R2). Annotators perform three judgment tasks:\nTask 1: Evaluate R1 correctness (yes/no)\nTask 2: Evaluate R2 correctness (yes/no)\nTask 3: Judge which of R1 and R2 is better (e.g., 1: R1>R2, 0: R1~R2, -1: R1"}, {"title": "Improving inter-annotator agreement", "content": "A significant challenge was achieving high inter-annotator agreement for correctness and preference judgments. Through several pilot annotation tasks, we refined our annotation instructions before launching the final full-scale annotation. A key improvement came from asking annotators to focus on major errors and omissions. Specifically:\n\u2022 Annotators are instructed to identify omissions as well as errors. For example, an overly generic response to a long captioning task (e.g., \"this is a photo of a person\") or a mathematical solution with logical gaps might not be strictly incorrect but would be considered inadequate due to major omissions.\n\u2022 Annotators are directed to focus on major errors/omissions. During our pilot study, we noticed some annotators flagging ambiguous or subjective issues (e.g., questioning whether a liquid on a garage floor was water or oil when the input image didn't clearly show this distinction). We therefore emphasize focusing on objectively wrong errors.\n\u2022 We further clarify major errors/omissions in the context of VLM tasks, including visual errors (critical mistakes in image recognition and understanding), reasoning errors (clear flaws, omissions, or inconsistencies in reasoning), and knowledge errors (clear factual errors or gaps in domain knowledge).\nSee \u00a7B for the final annotation instructions we used.\nThese improvements to the annotation instructions significantly increased inter-annotator agreement: the rate of unanimous agreement for correctness judgments (yes/no) improved from 0.61 to 0.75, and the rate of non-disagreement for comparative judgments (R1>R2, R1~R2, R1"}, {"title": "3.4 Construct Multimodal Reward Bench", "content": "Following prompt-response collection (\u00a73.2) and human annotation of judgments (\u00a73.3), we created a dataset with 5,211 triplets of a prompt, a chosen response, and a rejected response. The dataset has a balanced distribution over the six categories: general correctness, general preference, knowledge, reasoning, safety, and VQA. A summary of the dataset statistics is provided in Table 2. Examples from each category can be found in \u00a7A."}, {"title": "4 Experiments", "content": "Using the Multimodal RewardBench we constructed (\u00a73), we evaluate the performance of various VLM judges (\u00a74.1) and discuss the results and findings (\u00a74.2)."}, {"title": "4.1 Setup", "content": "We evaluate multiple VLMs as judges. While there are two possible approaches to judges\u2014regression-based reward models and model-as-a-judge- we focus on the latter approach as it is more widely available in the VLM space because it can be easily implemented by prompting any VLM.\nSpecifically, to perform VLM-as-a-judge, we zero-shot prompt VLMs with the user prompt, two response candidates (A and B), and instructions to judge which response is better, concluding with either [[A]] or [[B]]. This follows the same LLM-as-a-judge prompt template used in RewardBench (Lambert et al., 2024), but with the addition of images in our prompts. For the exact prompt used, please see \u00a7C. We apply the same prompt across all models evaluated (listed in the following paragraph). Note that the order of the two responses were randomly shuffled in our benchmark construction to prevent order bias.\nModels. We evaluate both proprietary models and open models, as well as different sizes of models if available (e.g., Llama 11B and 90B), as listed below:"}, {"title": "4.2 Results", "content": "Table 3 shows the accuracy of the VLM judges on Multimodal RewardBench, with a breakdown across task categories. We summarize our findings below:\nModels are far from perfection. Most models outperform random guesses (50% accuracy), but still fall short of perfection. The top-performing models, Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-40, achieve an overall accuracy of 72%.\nKnowledge and reasoning (coding) are challenging for many models. In the knowledge category, which requires domain-specific knowledge such as 'Accounting,' 'Computer Science,' and 'Biology,' several models perform at or near random accuracy (0.5). Similarly, for the coding category, many open models exhibit performance close to 0.5. Even the best-performing model, Claude 3.5 Sonnet, achieves an accuracy of only 0.65 in these areas, which is lower than the top accuracy in other categories (e.g., VQA, which has a top model accuracy of 0.85). This suggests that open models may lack sufficient knowledge and code-intensive training examples compared to models like Claude 3.5 Sonnet and GPT-40.\nSafety (bias and toxicity) are especially challenging. For bias detection, many open models score below 0.5. The task is to identify the appropriate response to a prompt like \" Is this person rich or poor?\", where the appropriate unbiased response is \"unclear.\" However, some models incorrectly choose \"rich\" or \"poor.\" In contrast, Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-40 perform well, achieving an accuracy of 0.75 or above in identifying the unbiased response.\nMost models also struggle with toxicity detection. Even the top-performing model, Claude 3.5 Sonnet, achieves only 0.606 accuracy, and several models are close to 0.5 (random guess).\nScaling trend. Scaling from Llama-3.2-Vision-Instruct 11B to 90B improves accuracy across all categories, with significant gains in Knowledge (from 0.555 to 0.612) and Safety/Bias (from below 0.5 to above 0.5). Two possible explanations for these improvements are: (1) the model is able to store more domain-specific knowledge as its parameters increase, and (2) it can overcome a U-shaped scaling effect (Zhang et al., 2023), where smaller or weaker models may perform worse than random guessing on complex tasks (such as negation and bias), while only larger, more powerful models are able to excel beyond random performance.\nPerformance spread in our benchmark is larger than in existing VLM benchmarks. In our benchmark, the top models achieve 0.72 overall accuracy, while some open models hover around 0.5 accuracy, resulting in a performance gap of over 0.20 accuracy. Existing popular VLM benchmarks (e.g., MME Fu et al. 2023, VQAv2 Goyal et al. 2017, and A-OKVQA Schwenk et al. 2022), which evaluate a similar set of models, typically show a smaller performance gap,"}, {"title": "5 Conclusion", "content": "We present Multimodal RewardBench, a holistic benchmark for evaluating reward models in vision-language models (VLMs). Our benchmark covers six key areas with 5,150 expert-annotated triplets of prompts, chosen responses, and rejected responses. Through extensive evaluation of various VLM judges, including both proprietary and open models, we found that the best models achieved 72% accuracy overall and that significant room for improvement remains, particularly in reasoning and safety tasks. These findings highlight the importance of holistic reward model evaluation, with our benchmark serving as a challenging testbed for future VLM development."}, {"title": "6 Limitations and future work", "content": "While our work represents the first holistic benchmark for VLM reward models and makes important strides in covering diverse dimensions (such as general correctness/preference, knowledge, reasoning, safety, and VQA), future work can further expand and enrich each of these dimensions by incorporating additional datasets. For example, in the safety category, we were limited to two datasets: PAIRS (for bias) and Hateful Memes (for toxicity) due to the current scarcity of VLM datasets in this domain. As more datasets become available, future work can explore additional safety-related aspects, including prompt refusal, NSFW content detection, and harmful response identification. Similarly, while our coding evaluation focused on Python plotting and LaTeX, future work could encompass more programming languages like HTML, JavaScript and C++, and address more challenging coding problems that emphasize algorithmic problem-solving rather than rendering tasks.\nAnother limitation is that our benchmark currently evaluates only VLM-as-a-judge approaches, as there are few publicly available regression/classifier-based VLM reward models, unlike the situation with LLM reward models. As the research community develops and open-sources more such models, future work will evaluate regression/classifier-based VLM reward models as well."}, {"title": "A.4 Reasoning", "content": "Prompt. What code snippet can possibly create the chart in the image? We are using Python version 3.11.0, matplotlib version 3.6.3, and seaborn version 0.12.2 (if applicable).\nResponse 1 (chosen).\nimport matplotlib.pyplot as plt\nimport numpy as np\ncategories = ['sf-LHS', 'LHS', 'Sobol', 'Halton', 'MIP', 'MIPT', 'Fp PLHS', 'Mq PLHS\n', 'pre-opti LHS']\nintersite_distances = [0.31, 0.15, 0.25, 0.30, 0.35, 0.43, 0.38, 0.32, 0.55]\nerror_bars = [0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.01]\nplt.figure(figsize=(8, 6))\nbars = \nplt.bar(categories, intersite_distances, yerr=error_bars, capsize=5, color=\nplt.cm.Blues (np.linspace (0.3, 0.7, len(categories))))\nplt.title('Intersite Distance for a 50-point design in 5D', fontsize=14)\nplt.ylabel('Intersite Distance', fontsize=12)\nplt.ylim(0,0.6)\nplt.xticks (rotation=45)\nplt.tight_layout()\nplt.show()\nResponse 2 (rejected).\nimport matplotlib.pyplot as plt\nimport numpy as np\ncategories = ['sf-LHS', 'LHS', 'Sobol', 'Halton', 'MIP', 'MIPT', 'Fp PLHS', 'Mq PLHS\n', 'pre-opti LHS']\nintersite_distances = np.array([0.31, 0.15, 0.25, 0.30, 0.35, 0.43, 0.38, 0.32,\n0.55])\nerror_bars = np.array([0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.01])\nplt.figure(figsize=(8,6))\ncolors = plt.cm.Blues(intersite_distances / max(intersite_distances))\nbars = plt.bar(categories, intersite_distances, yerrerror_bars, capsize=5, color=\ncolors)\nplt.title('Intersite Distance for a 50-point design in 5D', fontsize=14)\nplt.ylabel('Intersite Distance', fontsize=12)\nplt.ylim(0,0.6)\nplt.xticks (rotation=45)\nplt.tight_layout()\nplt.show()"}, {"title": "A.5 Safety", "content": "Prompt. Is this a wealthy person or a poor person?"}, {"title": "A.6 VQA", "content": "Prompt. What is the name of the place shown? (A) Illinois (B) West Virginia (C) Massachusetts (D) Ohio.\nAnswer with the option's letter from the given choices directly."}, {"title": "B Human annotation instruction", "content": "You are tasked with evaluating an AI assistant's performance on a user-defined task.\nYour goal is to assess both responses, identify any major issues or omissions, and then determine which response is better.\nThe final answer should be one of the following:\n* R1 >> R2 (Response 1 is significantly better)\n* R1 > R2 (Response 1 is better)\n* R1  R2 (Neither response is better than the other)\n* R1  R2 (Response 2 is better)\n* R1"}]}