{"title": "FairQueue: Rethinking Prompt Learning for Fair Text-to-Image Generation", "authors": ["Christopher T. H. Teo", "Milad Abdollahzadeh", "Xinda Ma", "Ngai-Man Cheung"], "abstract": "Recently, prompt learning has emerged as the state-of-the-art (SOTA) for fair text-to-image (T2I) generation. Specifically, this approach leverages readily available reference images to learn inclusive prompts for each target Sensitive Attribute (tSA), allowing for fair image generation. In this work, we first reveal that this prompt learning-based approach results in degraded sample quality. Our analysis shows that the approach's training objective-which aims to align the embedding differences of learned prompts and reference images-could be sub-optimal, resulting in distortion of the learned prompts and degraded generated images.\nTo further substantiate this claim, as our major contribution, we deep dive into the denoising subnetwork of the T2I model to track down the effect of these learned prompts by analyzing the cross-attention maps. In our analysis, we propose novel prompt switching analysis: I2H and H2I. Furthermore, we propose new quantitative characterization of cross-attention maps. Our analysis reveals abnormalities in the early denoising steps, perpetuating improper global structure that results in degradation in the generated samples. Building on insights from our analysis, we propose two ideas: (i) Prompt Queuing and (ii) Attention Amplification to address the quality issue. Extensive experimental results on a wide range of tSAs show that our proposed method outperforms SOTA approach's image generation quality, while achieving competitive fairness. More resources at Project Page.", "sections": [{"title": "1 Introduction", "content": "There has been significant progress in the quality of text-to-image (T2I) generation [1-3] resulting in increasing adoption in different applications [4-10]. With this comes concerns regarding the fairness of these T2I models and their societal impacts [11-15].\nFair T21 Generation. T2I models may inherit biases present in their training data. Several approaches have been proposed to mitigate these biases [16-19] (See related work in Supp). Particularly, Inclusive T2I Generation (ITI-GEN) [16]-the existing SOTA-suggests that fair T2I approaches based on hard prompts (HP) (e.g., \u201cA headshot of a person with fair skin tone\u201d) are limited by linguistic ambiguity. For example, Skin Tone is often challenging to define and interpret based on HP, resulting in sub-optimal performance. To overcome this linguistic ambiguity, ITI-GEN adopts the notion that \u201ca picture is worth a thousand words\" and leverages readily available reference"}, {"title": "2 Preliminaries", "content": "T21 Generation. SOTA T2I generation is based on diffusion model (DM) [1-3]. In the forward diffusion process, Gaussian noise is incrementally added to the training data to train the DM. Then, during reverse diffusion, the DM generates samples by randomly sampling latent noise $Z_0 \\sim N(0, I)$ as an input. For more control, text-conditioning [1, 23-25] was introduced, where we denote the reverse diffusion (denoising) of a single step t by $Z_{t+1} \\leftarrow DM(Z_t, R, t, s)$. Here, $Z_t$ is the latent of the noisy image, R the input prompt, t \u2208 [0, 1] the denoising step, and s a random seed. Central to text conditioning is the cross-attention mechanism which contextualizes prompt embeddings with the image latent [21, 26]. Specifically the cross-attention map $M \\in R^{r\\times m\\times n}$ where r is the number of tokens in the prompt, and $m \\times n$ shows map size for each token-is computed by:\n$M = SoftMax(QK^T)$ (1)\nwhere, $Q = l_q(\\phi(Z_t))$ is the linear projection of the latent spatial features $\\phi(Z_t)$, and $K = l_q(E_T(R))$ is the linear projection of the textual embedding $E_T(R)$ (usually CLIP text encoder [20]). For ease of notation, we refer to the token-specific attention maps as $M[\u00b7]$ e.g., $M[\u2018of\u2019] \\in R^{m\\times n}$ refers to the cross-attention map for the token \"of\u201d in R. As our work focuses on the reverse diffusion process, we utilize $Z_0$ as the noisy latent input and $Z_l$ as the final latent output. This $Z_l$ is then finally passed into the DM decoder to output generated image, $D(Z_l)$."}, {"title": "3 A Closer Look at Prompt Learning for Fair Text-to-Image Generation", "content": "In this section, we take a closer look at ITI-GEN [16]. First, in Sec. 3.1, we analyze ITI-GEN performance where we find quality degradation in moderate number of generated samples. We attribute this to the sub-optimal learning objective in ITI-GEN, which captures unrelated concepts that distort the learned tokens in P. Then, in Sec. 3.2, we analyze ITI-GEN prompts during sample generation by inspecting the cross-attention mechanism. Our analysis reveals that ITI-GEN prompts give rise to abnormality particularly damaging to the early steps of the denoising process.\nRemark. To conduct the following analysis on ITI-GEN prompts' behavior we require a strong base-line as a pseudo-gold standard to compare against. To address this, we found that when considering certain tSA with minimal linguistic ambiguity (MLA) [32]-a few tSA that can be described without misleading or deceptive language-HPs can serve as this strong baseline. Therefore, in this section, we focus on tSAs with minimum linguistic ambiguity. Later, in experiment section, we will include all tSAs, with or without ambiguity."}, {"title": "3.1 Limitations of Prompt Learning for Fair T2I Generation", "content": "Although ITI-GEN [16] improves fairness in T2I generation, a closer examination of its outputs reveals a potential trade-off: compromised image quality. In this section, first, we perform a systematic experiment to showcase these quality issues and then explore the potential root causes behind them.\nExperimental Setup. To evaluate our generated samples, we utilize the metrics: i) Fairness Discrepancy (FD) [27, 31, 11, 36] to measure fairness, ii) Text-Alignment (TA) [37, 22] and FID [38] to measure quality, and iii) DreamSim (DS) [39] to measure semantic preservation. Next, we determine a set of tSA with MLA to compare ITI-GEN with HP (as a pseudo-gold standard). Specifically, we follow [16] and use pre-trained Stable Diffusion (SD) [1] as T2I model. Then as mentioned in Sec. 2, for HP, we append the tSA-related prompts to the base prompt. We empirically found that tSAs {Smiling, High Cheekbones}, are unambiguous by classifying 500 generated sample per HP utilizing CLIP classifier [20], where on average they both achieve a 98% accuracy (Experiment details in Supp). Then, for ITI-GEN [16], we strictly follow [16] and use publicly available fair"}, {"title": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation", "content": "In the previous section, we observed degraded sample quality in ITI-GEN which we attribute to the sub-optimal training objective that results in learning distorted tokens. In this section, we take a step further to answer the question: \u201cGiven a pre-trained T2I model and some distorted learned prompts as input, how do these distorted prompts affect the image generation process of the T2I model?\"\nTo answer this, we deep dive into the latent denoising network [1] and analyze the cross-attention mechanism [40]-the bridge for text and image modules in T2I models [1, 23-25]. In this analysis, we visualize the cross-attention maps to investigate potential anomalies caused by distorted tokens in the denoising process. Specifically, we compare cross-attention maps of ITI-GEN prompt against HP with minimal linguistic ambiguity (as reference). To allow fair token-to-token comparison, in this experiment, we lengthen HP by including additional tokens containing synonyms of the tSA. Note that this did not augment HP's behavior, and similar results are seen in the original HP. See Supp for more details.\nVisualizing Cross-attention Maps. We follow DAAM [41] for visualizing cross-attention maps by tracing attention scores in the cross-attention module to demonstrate how an input token within a prompt influences parts of the generated image. Specifically, to visualize the cross-attention map of a token, DAAM interpolates and accumulates the attention scores over all scales (layer of the U-Net [42] as the denoising network [1]), and all denoising steps. However, we tailor DAAM to the requirements of our fine-grained analysis by introducing further controls. First, we isolate the attention maps for each denoising step to allow for both step-wise and multi-step analysis. Second, we introduce a prompt-switching mechanism, allowing for the interchangeable tracing of different prompts at any particular denoising step."}, {"title": "4 Proposed Method", "content": "In this section, we present our proposed method, FairQueue a new generation framework consisting of two additions: Prompt Queuing and Attention Amplification to improve the sample quality when implementing fair T2I generation. In addition to quality improvements, FairQueue also allows for better semantic preservation of the original sample generated from the base prompt T.\nPrompt Queuing. Recall that when utilizing ITI-GEN prompt P\u2013which is tuned to generate samples containing the tSA-degraded global structure occurs in early denoising steps for a moderate number of samples. Conversely, utilizing HP with minimal linguistic ambiguity enables high-quality and fair T2I generation. However, as such HPs are not available for all tSAs [16], we naturally consider the next best available option-the base prompt T (a natural language prompt without the distorted trainable tokens)-and propose prompt queuing. Specifically, as seen in Fig. 1(c), prompt queuing first utilizes T in the early n denoising steps, thereby allowing for the global structures to form properly. Next, we transit to ITI-GEN prompt P for the remaining (l \u2013 n) steps. This allows the more fine-grained tSA semantics to be developed on top of the already well-defined global structures.\nAttention Amplification. By implementing prompt queuing, the output samples may experience a reduction in tSA expression due to the reduced exposure to the ITI-GEN prompt P. To address this, we propose Attention Amplification, an intuitive solution that emphasizes the expression of the tSA by scaling the ITI-GEN token's cross-attention maps, i.e., c * M[Si] where c > 1."}, {"title": "5 Experiments", "content": "In this section, we evaluate our proposed (FairQueue) against the existing SOTA ITI-GEN [16] over various tSA. Then, we conduct an ablation study by first evaluating the contribution brought by each"}, {"title": "A.2 Cross-attention analysis", "content": "Sec. 3.2 analyzes the effect of inclusive tokens $S_k$ by comparing the accumulated cross-attention maps of individual tokens between HP and ITI-GEN. It is observed that distorted tokens learned by ITI-GEN negatively affect the development of global structure in the early steps of denoising. The destructive effect arises with abnormally high activity of non-tSA tokens (e.g., \u201cof\u201d), and the tSA-tokens attend to unrelated regions with scattered attention. A quantitative analysis is performed over 500 sample generations for different tSAs to affirm the observations. The below details how token-specific accumulated cross-attention maps are obtained from the SD pipeline, discusses interaction among tokens, and presents additional representative results for tSAs Smiling, High Cheekbones, Gray Hair, and Chubby.\nDetails of visualizing token-specific accumulated cross-attention map. Cross-attention is often used to contextualize prompt embeddings with latent representations per sample generation step. Following DAAM [41], coordinate-aware attention scores $M[S_i]$ are extracted from the latent diffusion network (i.e., U-Net) for the token $S_i$ at the layers where cross-attentions take place. These token-specific attention scores, each with the same spatial dimensions as the latent representation, are upscaled bicubically to the image size (512 \u00d7 512 in this case) to reveal where attention is paid per token and accumulated within the assigned step(s). The resulting 2D matrix is visualized in Fig. 3 and Fig. 4 and referred to as an \u201caccumulated cross-attention map\".\nInteraction among tokens. We remark that the cross-attention map of a given token is dependent on the others in the prompt. There are two channels where the effect of tokens may interact: 1) via latent representation, as it is a function of input tokens and serves as the query in the cross-attention (see Sec.2); 2) softmax operation, as a component in the attention pipeline, softmax is taken across all tokens when processing attention scores. These two effects become increasingly apparent as we move through different cross-attention layers of the U-Net and perform more denoising steps.\nHP vs. ITI-GEN : qualitative analysis. To investigate potential abnormality of ITI-GEN embeddings, images of different tSA are generated conditioning on HP (F) and ITI-GEN (P) respectively. The cross-attention map is employed as a tool to explore the cause of degraded generations. In pursuit of a fair token-to-token comparison, for some tSAs the original HPs (\u201cHP1\u201d, see Tab. 6) are extended to align with P in the number of tSA tokens (\u201cHP2\u201d). Nonetheless, as one can find in the samples in Fig.7 to 14, the extension does not change the behavior of HPs significantly.\nFig.7 to 17 give an overview of cross-attention maps during the denoising process. One may find that the tSA tokens in the HP(s) tend to concentrate on the region(s) semantically associated with the tSA, e.g, mouth for tSA Smiling, cheek for tSA High Cheekbones, and hair for tSA Gray Hair. On the other hand, ITI-GEN TSA tokens' activity tends to be less focused and attends broadly. With more steps than Fig. 3, it is clearer that the global structure of the images is synthesized in the early steps, which motivates the prompt switching experiments.\nPrompt switching experiments and quantitative analysis. To further investigate HP and ITI-GEN prompts' behaviors in the early steps, the prompt switching experiments (i.e., I2H and H2I) are proposed in Sec. 3.2. Fig.18 to 21 present representative outcomes of the experiments. One can find that the destructive effect caused by ITI-GEN prompts only occurs at the early steps, i.e., Stage 1 in the figures.\nIn addition, the activation patterns are more clear in the accumulated cross-attention maps. The non-tSA tokens in ITI-GEN prompts are in general more active, and the tSA tokens tend to attend more broadly, which may explain the drastic semantic deviations from HPs in Fig.7 to 17. The latter observation is particularly evident for tSA Smiling, a highly localized facial expression, which is supported by the histogram of central moments in Fig.1. The other tSAs, though may not be directly associated with a specific facial feature, share the same trend, as manifested statistically by the histograms in Fig.22 and Fig.23.\nUtilizing Base Prompt (T) in FairQueue . In Prompt Queuing the use of T, in place of the HP, is similarly grounded on the I2H/H2I analysis, as both T and HP are natural language prompts \u2013 free of learned tokens. This can be seen in the embedding analysis in Supp B.6 where the HP and T are seen to be close to one another. As a result, the sample generated by T is expected to be of similar quality as the HP."}, {"title": "Limitations and Broader Impact", "content": "In this section, we discuss some limitations regarding our work as well as some potential societal impacts that it may have.\nLimitations. Firstly, FairQueue work follows ITI-GEN which utilizes a reference dataset to first optimize a ITI-GEN prompt P. This setup requires dozens of reference images for each category of the tSA which may not always be readily available. Second, although FairQueue provides better quality and semantic preservation of the original base prompt sample, its sample generation is still prone to experiencing entanglement in certain tSA. Specifically, entanglement in tSA could result in some unwanted augmentation of non-tSA during sample generation.\nBroader impact. Our work FairQueue takes a significant step towards enhancing fairness in text-to-image generation. By improving the quality of samples generated through a fair text-to-image algorithm, we facilitate greater adoption of these techniques by the general public. This increased adoption can help prevent the perpetuation of unwanted biases in everyday applications, promoting a more equitable and inclusive use of technology in society."}]}