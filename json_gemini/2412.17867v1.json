{"title": "Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types", "authors": ["Ziming Guo", "Chao Ma", "Yinggang Sun", "Tiancheng Zhao", "Guangyao Wang", "Hai Huang"], "abstract": "Recent advancements in large language models (LLMs) have significantly advanced text-to-SQL systems. However, most LLM-based methods often narrowly focus on SQL generation, neglecting the complexities of real-world conversational queries. This oversight can lead to unreliable responses, particularly for ambiguous questions that cannot be directly addressed with SQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed to evaluate the question classification and SQL generation capabilities of LLMs by simulating real-world scenarios with diverse question types and multi-turn Q&A interactions. Using MMSQL, we assessed the performance of popular LLMs, including both open-source and closed-source models, and identified key factors impacting their performance in such scenarios. Moreover, we introduce an LLM-based multi-agent framework that employs specialized agents to identify question types and determine appropriate answering strategies. Our experiments demonstrate that this approach significantly enhances the model's ability to navigate the complexities of conversational dynamics, effectively handling the diverse and complex nature of user queries.", "sections": [{"title": "1 Introduction", "content": "Text-to-SQL bridges natural language and SQL queries, empowering non-technical users to access and analyze data without mastering complex SQL knowledge. The advent of LLMs, with their remarkable capacity for following instructions, has transformed the text-to-SQL domain. LLM-based methods have delivered remarkable outcomes across various text-to-SQL tasks [1, 2], while the assessment of their robustness is increasingly drawing attention. Current research often assumes that user queries are unambiguous, focusing on expanding the scope of databases and increasing the complexity of questions to elevate the difficulty of question-to-SQL mapping, thereby pushing the boundaries of text-to-SQL systems. [3, 4, 5, 6]. However, in practical scenarios, user queries are inherently dynamic and uncertain [7]. Assuming that all user queries can be answered may lead to the generation of hallucinated content and unreliable predictions, thereby undermining the dependability of these systems [8, 9]. Observing real-world conversations, it is common for users to engage in multi-turn dialogues with interrelated questions that do not readily translate into SQL [7, 10, 11]. \nThere is a need for more robust models that can effectively manage such challenges. Existing studies have only sporadically addressed the effective assessment and handling of multi-type questions. Most text-to-SQL research focuses on achieving high accuracy for single-type or single-round user questions [4, 6, 12, 13], often"}, {"title": "2 Related Work", "content": "Text-to-SQL involves transforming natural language queries into SQL, representing a crucial area of natural language processing with many practical applications. Crucial datasets include single-turn datasets like WikiSQL [16], Spider, and BIRD [3], as well as multi-turn datasets such as CHASE [13], SParC [12], and CoSQL [17]. These datasets serve as benchmarks for evaluating the performance of text-to-SQL systems. In recent years, the advent of pre-trained language models (PLMs) has driven significant progress in this domain. Models like SQLova [18], PICARD [19], and RAT-SQL [20] have achieved impressive results. However, despite their effectiveness, these specialized models require considerable resources and time for training. LLMs have revolutionized numerous NLP tasks, including text-to-SQL, surpassing previous models and establishing a new paradigm [21, 22]. Recent studies have focused on enhancing prompt design and developing complex multi-stage frameworks that leverage LLMs to boost performance [23]. Notably, LLM-based frameworks like DIN-SQL [24], which utilize chain-of-thought reasoning [25], decompose the problem into simpler sub-problems. MAC-SQL [6] and CHASE-SQL [5] employ a multi-agent collaborative framework, demonstrating significant advancements in this field.\nRecent works highlight natural language's inherent diversity and ambiguity, noting that not all user queries can be effectively translated into accurate SQL as the answer in practical applications [7, 10]. Datasets such as TriageSQL [14], NoisySP [7], and CoSQL have incorporated question type detection as part of the model's task. Previous work, TrustSQL [10] demands that models opt for abstention when faced with diverse unanswerable questions. Additionally, AMBROSIA [9] and [26] propose testing text-to-SQL systems by parsing ambiguous questions into multiple potential SQL queries, thereby enhancing model usability in real-world scenarios. [26] improves beam search on T5 [27] to provide possible SQL responses for ambiguous questions and evaluate their effectiveness. Additionally, [15] employs questions enhanced dialogue augmentation to train open-source LLMs to identify question types. These studies focus solely on the model's ability to recognize question types or address specific problems, which may result in unhelpful abstentions or hallucinations due to specialized question-and-answer tailoring. Moreover, most work is limited to single-turn settings, whereas real interactions often involve ambiguity that must be addressed in multi-turn contexts. These limitations motivate our work, which evaluates and enhances LLMs for multi-turn dialogues with multiple question types."}, {"title": "2.2 LLM-based Agents", "content": "LLM-based agents have long captured the attention of researchers in both academia and industry [28]. With the expansion of web knowledge, LLMs are increasingly demonstrating intelligence levels comparable to humans. This evolution has sparked a growing interest in creating autonomous agents driven by LLMs. AutoGPT [29] and MetaGPT [30] enhance AI models by integrating a range of useful tools, allowing developers to create adaptable, conversational agents that can function in various"}, {"title": "3 MMSQL: Multi-type and Multi-turn Text-to-SQL Test Suite", "content": "MMSQL aims to assess the effectiveness of language models in managing multi-type and multi-turn text-to-SQL tasks. By analyzing real-world text-to-SQL interactions, we have categorized four types of question-answer scenarios, as shown in Figure 1.\n1.  Ambiguous: These questions include terms that might correspond to multiple columns or cell values in the database, causing potential ambiguity in SQL query generation. The system should seek clarification from the user to resolve this ambiguity while simultaneously attempting to predict possible SQL queries. For example, if a term like \"Glenn\" could refer to different columns such as \"donator_name\" or \"School_name,\" the system should ask for clarification (e.g., \"What did you mean by...?\") or inquire about which columns to return, while also generating SQL queries based on these interpretations.\n2.  Answerable: These questions can be directly answered using SQL queries based on the available database information. The system must generate an accurate SQL query reflecting the user's request and execute it to retrieve the required data. For instance, if the user asks for the \"id of Glenn\" and it is clear that \"Glenn\" refers to the \"donator_name\" column, the system should generate the appropriate SQL query.\n3.  Unanswerable: These questions pertain to information that is not present in the database or exceeds the system's operational scope, such as when they involve external knowledge or web search capabilities beyond the SQL system. The system should acknowledge its inability to answer the question and explain the reason to the user. For example, a question about the \"country\" of donors, if not present in the database, should be identified by the system as unanswerable.\n4.  Improper: These questions are irrelevant to the database or pertain to everyday conversation that does not require an SQL response. The system should recognize these questions and respond appropriately, without attempting to generate an SQL query. For example, a casual \"thank you\" from the user should be met with a simple \"you're welcome\" rather than an SQL statement.\nIn this task, the model is required to perform two key functions: first, to accurately determine the type of each question; and second, to attempt resolution and"}, {"title": "3.4 Evaluation Metrics", "content": "Following BIRD [3] and CoSQL [17], our evaluation metrics include Exact Matching (EM) and Execution Accuracy (EX). EM compares each predicted clause to the reference query, considering it is correct only if all components match, excluding values. EX evaluates the proportion of SQL where the execution results of both predicted and ground-truth SQL are identical. Interaction Execution Accuracy (IEX) is achieved when all SQL queries in a multi-turn interaction execute correctly. We also developed the Dual Assessment of Question Type Detection and Execution Accuracy (TDEX) to assess the comprehensive capability of text-to-SQL models with complex queries. Additionally, the Response Quality Score (RQS) measures the quality of the model's natural language responses.\nTDEX is a comprehensive metric that evaluates the accuracy of user query type classification and execution accuracy. For a set of N questions, where $C_i$ denotes the expected classification and $\\hat{C}_i$ represents the predicted classification for the i-th question, $S_i$ denotes the ground truth SQL query and $\\hat{S}_i$ represents the predicted SQL query for the i-th question, TDEX is computed as:\n$\\text{TDEX} = \\frac{1}{N} \\sum_{i=1}^N \\begin{cases} \\varepsilon_{\\text{exec}}(S_i, \\hat{S}_i) & \\text{if } C_i = \\text{'Answerable' or 'Ambiguous'} \\\\ \\delta_{\\text{type}}(C_i, \\hat{C}_i) & \\text{otherwise} \\end{cases}$\nwhere $\\varepsilon_{\\text{exec}} = 1$ if the execution result of $S_i$ matches the execution result of $\\hat{S}_i$, and $\\varepsilon_{\\text{exec}} = 0$ otherwise; $\\delta_{\\text{type}} = 1$ if $\\hat{C}_i$ matches $C_i$, and $\\delta_{\\text{type}} = 0$ otherwise.\nTo evaluate the quality of a model's responses, we use an LLM-assisted rating method assessing Utility, Accuracy, Completeness, Clarity, and Relevance on a 0-to-2 scale, with a maximum score of 10. This approach surpasses traditional metrics like BLEU, ROUGE, and BERT-Score [32, 33]. We employ GPT-40-mini due to its strong alignment with human judgment [34, 35, 36] and enhance evaluation calibration using the Multiple Evidence Calibration methodology [37]. To ensure alignment with human judgment, three experts scored each sample, and their scores were averaged. Pearson, Spearman, and Kendall's Tau correlations were calculated, confirming a strong positive correlation between LLM and human scores,"}, {"title": "3.5 Baselines", "content": "We evaluate eight popular LLMs' performance, including close-source and open-source options. The close-source LLMs evaluated are GPT-4 Turbo\u00b9, GPT-3.5 Turbo\u00b2, and Gemini-1.5 Flash\u00b3. The open-source LLMs include Llama-3 (70B and 8B)4, Llama-3-SQLCoder-8B5 (a Llama-3 model specifically trained in the text-to-SQL domain), Codellama-7B6, and Mistral-7B-v0.27. See Appendix A for experimental details."}, {"title": "3.6 Result of MMSQL Evaluation", "content": "Table 3 presents the performance of the evaluated LLMs in the zero-shot evaluation on multi-turn text-to-SQL tasks of the MMSQL test set. This test set encompasses four types of questions. We assessed the models based on overall performance (TDEX), SQL generation performance (EX and EM), and their ability to recognize different types of questions. We provide a detailed discussion of the results in the following sections."}, {"title": "3.7 Clarification for Ambiguous Queries", "content": "As illustrated in Figure 2, several representative models exhibit a notable decline in execution accuracy when addressing ambiguous queries, compared to their performance with clear, answerable queries. For example, GPT-4 Turbo's execution accuracy decreases from 51.0% with answerable queries to 41.3% when handling ambiguous ones. Similarly, GPT-3.5 Turbo and Llama3-70B show significant declines, with accuracy falling from 47.3% to 34.5% and from 47.4% to 34.7%, respectively. However, execution accuracy improves significantly when models detect ambiguity and prompt users for clarification. For instance, when ambiguous queries are clarified, GPT-3.5 Turbo's query match accuracy rises from 34.5% to 49.0%. This suggests that generating SQL for ambiguous queries without clarification results in diminished performance. Therefore, it is crucial for models to not only generate SQL queries but also effectively communicate any ambiguities to users. By clarifying uncertainties and helping users understand them, models can assist in obtaining accurate information. By incorporating the clarification process across multiple interactions, models can formulate more precise SQL queries, enhancing text-to-SQL systems' usability and effectiveness."}, {"title": "4 Methodology", "content": "As illustrated in Figure 3, we introduce an LLM-based multi-agent collaborative framework that utilizes intelligent agents with distinct functionalities to enhance text-to-SQL parsing. This framework comprises a Question Detector agent, which identifies the type of question and determines the appropriate response strategy. Accompanying it is the Question Decomposer agent, responsible for generating SQL queries by breaking down complex questions into simpler sub-questions. Additionally, two auxiliary agents: the Schema Selector and the SQL Refiner aid in refining the process."}, {"title": "4.1 Schema Selector", "content": "The Schema Selector is designed to identify the essential subset of a database schema necessary for answering a given question. By selectively focusing on the most relevant tables and columns, the Selector curtails the interference from extraneous data,"}, {"title": "4.2 Question Detector", "content": "The Question Detector is responsible for detecting the question type and deciding on an appropriate answering strategy, whether to directly answer or attempt SQL generation, based on the subset of the database schema and the question-answer history.\nFor answerable questions, the system proceeds to the Question Decomposer for further processing. In the case of ambiguous questions, where the query cannot be precisely mapped to the database schema, the system explains the ambiguity and attempts to rewrite it into answerable forms. For example, as shown in Figure 3, the question \"What is the flight number of Delta Airlines?\u201d (Q2) could be interpreted in multiple ways: it might refer to flights departing from APG, as inferred from the previous context (Q1), or it could pertain to all flights in the database. Instead of rejecting such ambiguous queries, the system attempts to rewrite them into answerable questions and provides multiple potential answers in the final response.\nFor unanswerable questions, where the query is not ambiguous but cannot be answered due to data limitations or inappropriateness, the system informs the user of the reason. For improper questions, the LLM is tasked with providing a helpful response to assist the user as much as possible. In both scenarios, the system aims to guide the user towards obtaining a meaningful and helpful response as the final answer."}, {"title": "4.3 Question Decomposer", "content": "The Question Decomposer plays a critical role in the multi-agent framework by breaking down complex questions into simpler, manageable Sub-Questions {$Q_1, Q_2,...,Q_Q$}. For each Sub-Question $Q_i$, it generates a corresponding sub-SQL $S_j$ based on $S_j$, and the schema subset to address specific parts of the original query. This process involves applying chain-of-thought reasoning, allowing the system to progressively solve each sub-question. As shown in Fig. 3, the Question Decomposer takes an ambiguous question, \"What are the Delta Airlines flight numbers for flights departing from APG?\u201d, and decomposes it into sub-questions like \"Retrieve airlines for flights from SourceAirport APG\" and \"Retrieve flight numbers for Delta Airlines from those flights.\u201d Each sub-question is then translated into a sub-SQL, collectively forming the final SQL output."}, {"title": "4.4 SQL Refiner", "content": "The SQL Refiner, as an integral auxiliary operation of the Question Decomposer, serves a critical role in ensuring the precision and reliability of the SQL queries generated by our multi-agent framework. This component is indispensable for the inspection and correction of the responses produced, particularly when addressing the complex demands of text-to-SQL tasks. Frequently utilized, the Refiner has demonstrated its effectiveness in enhancing the output to meet stringent accuracy requirements [5, 6, 31]. Drawing a parallel to its counterparts in software development, such as metaGPT [30] and ChatDev [40], where intelligent agents are tasked with overall architectural design, code writing, and comprehensive testing, the SQL Refiner plays a similar role in the domain of database interactions. It adeptly adjusts SQL queries to accommodate various datasets, database schemas, and SQL generation styles, thereby ensuring the accuracy and effectiveness of the generated queries."}, {"title": "5 Experiments", "content": "We conducted a comprehensive experiment utilizing the MMSQL test suite to rigorously evaluate the efficacy of our multi-agent framework in managing intricate multi-turn text-to-SQL tasks. The experimental design was meticulously crafted to evaluate the framework's proficiency in producing valid SQL and coherent natural language responses, alongside its capacity to categorize diverse question types effectively. To quantitatively assess the quality of the natural language responses, we employed a suite of metrics, including the average RQS, EM, average F1 score across all categories, and TDEX, to provide a holistic evaluation of the models' performance.\nWe conduct experiments on three prominent models: GPT-4 Turbo, Gemini-1.5 Flash, and Llama3-70b, as delineated in Section 3.6. These models were selected as our baselines to underscore the comparative effectiveness of our methodology. The objective of our experiments was to conduct a meticulous analysis elucidating how the incorporation of the multi-agent framework augments the performance of these models across a spectrum of evaluation metrics."}, {"title": "5.2 Overall Performance", "content": "Table 5 illustrates the performance of our method and baseline models on the MMSQL test set, highlighting the enhancements achieved by integrating our multi-agent framework with different models. The results demonstrate the benefits of our approach, with significant improvements across all metrics for each model. For example, the TDEX score increases from 62.8 to 70.7 and the F1 Score improves from 59.8 to 70.8 for Llama3-70B, showcasing the robustness of our framework in generating accurate SQL queries and natural language responses. These improvements underscore the effectiveness of our multi-agent framework in bolstering the capabilities of existing models to handle complex text-to-SQL tasks on the MMSQL benchmark."}, {"title": "5.3 Ablation Study", "content": "In Table 6, we presents the findings from an ablation study on the MMSQL test set, examining the impact of removing individual components from the multi-agent framework when integrated with Gemini-1.5 Flash. The complete model achieves its best performance with scores of 69.4 in TDEX, 73.7 in EX, 7.05 in Average RQS, and an F1 Score of 70.7. The study reveals that the removal of any component results in a decrease in performance across these metrics.\nNotably, the absence of the Detector results in the poorest performance with a sharp decline to 5.57 in Average RQS and 64.4 in F1 Score, indicating its pivotal role in selecting answering strategies. The Selector and Decomposer also contribute notably to the model's accuracy and response quality. The Refiner, also shows a notable impact when removed, particularly affecting the EX, dropping to 70.5, suggesting its role in correcting errors is indispensable for ensuring the accuracy of the final SQL queries generated."}, {"title": "6 Discussion", "content": "In this study, we introduced the MMSQL test suite, a pioneering benchmark specifically designed to evaluate LLMs across complex multi-turn and multi-type text-to-SQL"}, {"title": "7 Conclusion", "content": "Existing text-to-SQL evaluation methods often focus on increasing the complexity of SQL queries to enhance model performance. However, these approaches frequently encounter significant challenges in handling conversational dynamics, which further impede their development. In response to these challenges, this paper introduces the MMSQL test suite and proposes a novel multi-agent framework. The MMSQL test suite provides a structured approach to assess the capabilities of LLMs and offers an environment with diverse question types from a multi-turn perspective. Our analysis and experiments based on MMSQL illuminate the current limitations of LLMs."}, {"title": "Appendix A Implementation Details", "content": "In all experiments, we employed the same chatting format for each LLM across all experiments. We applied greedy decoding strategies across all LLMs during inference and evaluation to ensure reproducibility. Training settings include lora rank set to 64, lora_alpha set to 16, and learning rate set to 5e-5. The experiments were performed on a server with an Intel(R) Xeon(R) Gold 6133 CPU @ 2.50GHz and 4 NVIDIA A800 80GB PCIe GPU."}, {"title": "Appendix B Availability of data and material", "content": "We acknowledge the use of the CoSQL and SParC datasets, both licensed under the CC BY-SA 4.0 license. This license allows for the sharing and adaptation of the datasets, provided that appropriate credit is given to the original creators and any modifications are distributed under the same license.\nThe MMSQL repository provides access to the augmented datasets generated by QDA-SQL, testing scripts, and experimental records. The implementation of the QDA-SQL method is available in a separate repository."}]}