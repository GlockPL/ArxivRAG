{"title": "Shaking the Fake: Detecting Deepfake Videos in Real Time via Active Probes", "authors": ["Zhixin Xie", "Jun Luo"], "abstract": "Real-time deepfake, a type of generative AI, is capable of \"creating\" non-existing contents (e.g., swapping one's face with another) in a video. It has been, very unfortunately, misused to produce deepfake videos (during web conferences, video calls, and identity authentication) for malicious purposes, including financial scams and political misinformation. Deepfake detection, as the countermeasure against deepfake, has attracted considerable attention from the academic community, yet existing works typically rely on learning passive features that may perform poorly beyond seen datasets. In this paper, we propose SFake, a new real-time deepfake detection method that innovatively exploits deepfake models' inability to adapt to physical interference. Specifically, SFake actively sends probes to trigger mechanical vibrations on the smartphone, resulting in the controllable feature on the footage. Consequently, SFake determines whether the face is swapped by deepfake based on the consistency of the facial area with the probe pattern. We implement SFake, evaluate its effectiveness on a self-built dataset, and compare it with six other detection methods. The results show that SFake outperforms other detection methods with higher detection accuracy, faster process speed, and lower memory consumption.", "sections": [{"title": "1. Introduction", "content": "In recent years, real-time deepfake technology has received much attention [68]; it can replace a person's facial characteristics with those of someone else, creating convincingly altered footage that maintains the original video's fluidity in real time. Such application has significantly influences various fields, such as entertainment [63] and education [47]. Unfortunately, deepfake has also been extensively misused in scenarios such as video calls or web conferences, where it is employed to impersonate one of the participants, deceiving others involved in the call for nefarious purposes such as financial fraud and political misinformation. As reported by CNN, a financial employee at a global company was deceived into transferring 25 million dollars to swindlers who employed real-time deepfake techniques to impersonate the firm's chief financial officer during a video conference [25]. Similar news stories of individuals and organizations being swindled by real-time deepfake videos have become increasingly common in today's digital landscape [8], [42], [49], [57], [70]. Therefore, this paper aims to design a detection method especially for thwarting real-time deepfake during video conferencing or calls on mobile devices.\n The significant losses caused by deepfake have already alerted the academic community. Existing works about real-time deepfake video detection mainly focus on passively seeking the differences between real and fake videos induced by the deepfake algorithm [46]. Although these detection methods have achieved various degrees of success, they also exhibit limited generalizability across different datasets [12], [30], [48]. One major reason for the poor generalization is the reliance on detecting passively identified features that may not be universally applicable to all deepfake algorithms and their diversified application scenarios, which has led to three main consequences. First of all, switching to different deepfake algorithms may lead to distinct features that invalidate the trained detection algorithm [46]. Moreover, deepfake algorithms are under fast developments, forcing (passive) detection methods to keep up [76]. Last but not least, diversified application scenarios (e.g., distinct backgrounds) where a deepfake algorithm is deployed can significantly affect the effectiveness of passive detection [75].\n The above analysis brings us to think about a research problem: can the detector actively introduce changes (or features) into videos that are i) controllable, ii) readily recognizable, and iii) affect the real and fake parts in a video distinctively? Our answer, also the key idea of SFake, is to induce vibration to the attacker's smartphone via active probes. As shown by the basic idea of SFake in Figure 1, causing a smartphone to vibrate can blur the video (hence the real face in it) captured by the camera, yet such blur may not be fully adapted by deepfake. Consequently, the blur can act as a controllable and ready-to-recognize feature. Although deepfake algorithms can attempt to adapt to the blur caused by vibration\u00b9, we \u201cturn the tables\u201d by forcing the attacker to keep up with the defensive side, rather than the another way around like existing passive deepfake detection methods. In particular, we parameterize the probes to adjust the induced vibration pattern, so as to actively introduce controllable features to video footage, endowing the defensive side with prior knowledge as a key leverage. Building upon this principle, we propose and design SFake as the first deepfake detection method that may potentially offer universal defence to deepfake video calls on smartphones in real time.\n To implement SFake, we have to face two challenges: i) how to induce vibration to the attacker's smartphone and ii) how to detect the deepfake video based on the probe-induced blur in the footage. To address the first challenge, we give a detailed study on the smartphone construction and potential schemes for causing vibrations. Our experience indicates that, though acoustic signals from external speakers can trigger vibration on smartphone [41], the built-in speaker of the smartphone barely causes any noticeable blur features on video footage, likely due to its very-low sound volume of the speakerphone. Fortunately, our study also reveals that the built-in \"vibration\" effects of the smartphone can be remotely activated to introduce sufficient blur to the footage, yet at no cost of user experience. To address the second challenge, we strategically choose not to target any specific detection algorithms; this avoids leaving vulnerabilities for potential attackers to exploit. Instead, we focus on deriving a universal blur feature sequence that may drive virtually any detection algorithms (essentially binary classifiers), hence leaving the attackers to guess what strategy (including both vibration partten and detection algorithm) is taken by the defensive side at any point in time.\n Overcoming these challenges, we implement the SFake and evaluate it on our self-built dataset with 8 brands smartphone, 15 participants and 5 existing deepfake algorithms. We compare the performance of SFake with 6 existing deepfake detection methods. SFake outperforms other detection methods with accuracy over 95.2%, time period less than 5 seconds and memory consumption less than 450 MB. In summary, our contributions are as follows:\n\u2022 We propose SFake, the first real-time deepfake video detection scheme on smartphones that issues active probes to induce controllable blur features.\n\u2022 We design and implement SFake by actively introducing physical probes, recognizing the feature patterns, and checking for their consistency.\n\u2022 We evaluate SFake on our self-built dataset. The results show that SFake outperforms other six detection methods in accuracy, process speed and memory consumption.\nWhen put into practical use, SFake can be integrated as a specific security functionality into video communication"}, {"title": "2. Background", "content": "In this section, we present the background knowledge of i) the threat model of our detection method, ii) the workflow of deepfake algorithms and limitations of the existing detect methods, iii) the intuition of our detection method, and iv) the related components of the smartphone."}, {"title": "2.1. Threat Model", "content": "Deepfake attack model. The scenarios of deepfake attacks mainly involve video communication where the participants use digital video and audio to communicate. Generally speaking, deepfake attacks focuses on replacing the attacker's faces with that of a legitimate participant, aiming to deceive other participants during video communication.\nDefense goals. SFake should detect the deepfake-generate videos in different scenarios as accurately as possible. Besides that, The detection process should be resource-saving as it is designed for the mobile device.\nDefender's capability. Like other facial recognition functionality, SFake requires the user to remain as still as possible for a short period (e.g., 4 s) for detection. SFake requires cameras with resolutions of 1920*1080 pixels or higher, and they must support at least 2x zoom capabilities. If a phone's front camera does not meet these requirements, the rear camera can be used as an alternative. SFake should to some extent be able to control the hardware, such as zooming out the camera and playing the \"vibration\" sound effect during the detection process."}, {"title": "2.2. Limitation of Existing Methods", "content": "In this part, we briefly illustrate the basic principle of the face swap algorithm (FSA) and the limitations of existing detection methods. The main steps of the FSA are summarized in Figure 2. The FSA detects the face area of the target face and extracts several tens of facial landmarks. Based on the facial landmarks, the FSA generates the fake face based on the source face and blends it with the the background of the real image, thus get the result face. To generate a fake video, FSA applies the above steps to each"}, {"title": "2.3. Intuition of Our Method", "content": "As passively extracting features depends much on the attacker's strategies and has limited robustness, we think about how to actively introduce the features that are controllable to the defender and insensitive to alteration of the attacker's strategies. The very straightforward idea is to shake the attacker's smartphone intensively, which blurs the captured video. As the source face remains unchanged, the different sharpness of the source face and target face is likely to cause inconsistency on the result face, no matter what algorithm the attacker uses. As shown in Figure 4, the circled part of the fake picture on the right is sharper than other parts, which can even be observed by raw eyes, not to mention the well-designed detection algorithm. Therefore, the shake-caused blur can act as a feature to detect the fake faces. However, the defender cannot shake the attacker's smartphone as they are typically not in the same space. Therefore, we conduct a comprehensive study of the smartphone's structure to find methods that can make smartphones move controllably even far away from it. Finally, we find the \"vibration\" sound effect meets our requirement, because it not only generates mechanical vibrations sufficiently large but also is remotely controllable."}, {"title": "2.4. Related Smartphone Components", "content": "\"Vibration\" sound effect. \u201cVibration\u201d, a widely used sound effect of the smartphone, is triggered by a built-in motor. Traditionally, with an off-centered weight attached, the center of gravity moves as the motor spins. Recently, linear motors which utilizes the electromagnetic effect as drive source are also commonly used in smartphone applications. Regardless of their running principles, the motor inside the smartphone can be controlled by the software, so the period and duty cycle of the vibration can be customized, enabling different vibration patterns for various applications."}, {"title": "3. Feasiblity Analysis", "content": "Before we present the design of SFake, in this section, we illustrate the feasibility of inducing slight movements to the smartphone as the controllable physical probe and blurring the video footage as the readily recgnised feature by playing the \u201cvibration\u201d sound effect."}, {"title": "3.1. Induce Movement to Smartphone", "content": "To remotely vibrate the smartphone, we are initially inspired by the idea in SideEye attack\u00b2 [41] of using the smartphone's speaker to generate a sound wave that shakes the camera. However, based on our experiments, the phone's volume is too low to cause sufficient vibration. We collect the acceleration data of the smartphone by its inertial measurement unit (IMU) while it is playing a piece of music at its maximum volume, and the result in Figure"}, {"title": "3.2. Induce Blur to Video", "content": "The structure of the camera and the blurriness under ideal conditions. In this part, we briefly introduce the fundamental structure of the imaging system and illustrate the theoretically minimum blurriness. An imaging system is simplified to consist of an object, a lens, and a photosensitive sensor array, as shown in Figure 6. All light rays originating from one point of the object passing through the lens converge to the image point. To get a sharp image, the sensor array should adjust its position along the optical axis until it contains the image point. Under ideal conditions, one object point maps precisely to one image point, shown as the black point on the right side of Figure 6. However, when the object point vibrates, the image point also vibrates, making light rays not focus ideally on the sensor array. The imaging result is no longer an ideal geometric point but becomes a circle with a physical size in space, termed the \"circle of confusion\" (CoC), shown as the blue and green circles in Figure 6. The size of the CoC represents the degree of the image's blurriness.\nEven with the smartphone being stationary and focused, an ideal camera exhibits inherent blurriness due to several factors. First, the sensor array's pixel size limits the imaging resolution. Even if the light rays focus ideally on the sensor array, the image is not a perfect point but a rectangle with the size of a pixel. Second, due to the diffraction of light, an object point is naturally imaged to a circle known as the Airy disk [52], whose size theoretically limits the upper bound of imaging resolution with a particular aperture [69]. We take the camera on Xiaomi Redmi 10X [20] as an example to calculate the ideal resolution. First, the size of each pixel is 0.8 \u00b5m, as referred to in the specifications. Second, according to Rayleigh criterion [69], the diameter of the Airy disk is 0.87 \u00b5m with the aperture of f/1.8. Therefore, the smallest CoC diameter of the camera on Xiaomi Redmi 10X is limited by its sensor arrays, which is 0.8 \u00b5m.\nThe theoretical analysis of blurriness induced by vibration. As illustrated in Section 2, the movement along the z-axis leads the camera to refocus and introduce blur on the video. In this part, We further explore the quantitative relationship between the movement of the smartphone and the degree of blurriness represented by the CoC's radius. We assume the object distance as $u$, the image distance as $w$, the focus length as $f$, and the aperture as $F$ (then the size of the aperture equals $\\frac{f}{F}$ by definition) and the diameter of the CoC as $r$. According to the thin lens equation\n$\\frac{1}{f} = \\frac{1}{u} + \\frac{1}{w}$ (2)\n$u = \\frac{f * w}{w - f}$\n$w = \\frac{f * u}{u - f}$\nWe first consider the situation in which the object point moves towards the lens, represented as the blue lines and blue circle in Figure 6. According to the geometric properties of similar triangles\n$\\frac{w_2}{w_f} = \\frac{L_2}{F} \\Rightarrow w_2 = \\frac{f * w_f}{f - F * L_2}$ (3)\nWe substitute Eq. 2 to Eq. 3, and obtain the equation set\n$\\begin{cases}w_f = \\frac{f*u_f}{u_f-f}\\\\w_2 = \\frac{f*w_f}{w_f-f}\\\\W_2 = \\frac{f*w_f}{f-FL_2}\\end{cases}$ (4)\nSolving the equation set, we get the expression of $L_2$, the radius of CoC caused by the object point moving towards the lens\n$L_2 = \\frac{f^2 * (u - u_2)}{F * (u - f) * u_2}$ (5)\nBy analogy, when the object point moves away from the lens, the radius of CoC can be represented by the expression\n$L_1 = \\frac{f^2 * (u_1 - u)}{F * (u - f) * U_1}$ (6)\nTo illustrate the impact of vibration on the degree of image blurriness, we take the Xiaomi Redmi 10X smartphone as an example where $f$ is 26 mm, and $F$ is 1.8. According to Eq. 1, a typic value of the maximum vibration amplitude is 1.5 \u00b5m. We assume the distance $u$ between the lens and the object is 10 cm, and then $u_1$ and $u_2$ are respectively 10cm + 1.5\u00b5m and 10cm - 1.5\u00b5m. According to Eq. 6 and Eq. 5, $L_1$ and $L_2$ are approximately equal to 0.095 \u00b5m, which represents the blurriness caused by vibration. Compared to the inherent error of 0.8um in an ideal camera, the error caused by vibration is about one-tenth of it. That means the light rays initially contained within a single pixel's sensor now partially affect the surrounding pixel, leading to mutual interference between the pixels. Since the edges of color blocks experience the most significant color changes, the blurriness caused by vibration is primarily manifested in the slowing of color transitions at the edges and the widening of the edges. When the camera vibrates rapidly along with the smartphone, the camera does not have enough time to complete the focusing process, resulting in a continuous defocus state, leading to sustained blurriness in the video.\nThe experimental verification of blurriness induced by vibration. To validate the above theoretical hypothesis, We record an 8-second video with the vibration period of 2 seconds and the duty cycle of 0.3. We observe the vibration of the subject's collar in the video as an example. The result is shown in Figure 7. When the smartphone does not vibrate, the collar area is sharp and the edge between the skin and the clothing is distinct. When the smartphone vibrates, the skin and clothing are blended along the edge of the two parts. We calculate the gradient of two images, which can better represent the distribution of edges. The result shows that gradients of the vibrating image are more widely distributed in space, whereas they are more concentrated in the non-vibrating images."}, {"title": "How deepfake algorithm deals with blurriness.", "content": "Therefore, when the camera defocuses, the edges of the imaging will be less sharp, resulting in a more gradual variation in pixel values. We can use variance to measure the degree of variation of an image, which is calculated by Eq. 7\n$\\sigma\u00b2 = \\frac{1}{N} \\sum_{i=1}^{N}(X_i - \\mu)^2$ (7)\nHere, \u03c3\u00b2 represents the variance, N is the number of pixels, $X_i$ represents the value of each pixel and $\u03bc$ is the mean of pixel values. We calculate the variance of the gradient of the collar area in Figure 7 frame by frame in the 8-second video and collect acceleration data from the IMU during video recording. The relationship between IMU readings and video variance is shown in Figure 8. When the smartphone vibrates, the variance of the images decreases, which means the footage gets blurred. The experiment shows that vibration blurs the video, which can be detected by variance sequence.\n FSA exhibits a certain adaptability for blurry images. For example, when it performs a face swap on a blurry target image, the result image is also quite blurry, as shown in Figure 9(a). However, the vibration caused blurriness is too small to be recognized by FSA. To prove this conclusion, we select a facial area of the first frame of the 8-second video and use Gaussian filtering to blur it, simulating the effect of vibration on the video. To choose the proper size of the Gaussian kernel, we compare the variance of the video with and without vibration. we find that when the kernel size is between 1 and 3, the blurriness effect of Gaussian filtering is similar to the vibration. Therefore, we Gaussian filter the same sample image 100 times with random size of kernel and apply the FSA algorithm to generate 100 fake faces. The variance of real and fake faces are respectively shown as blue and green curves in Figure 9(b). The trends of the green and blue curves are not consistent, which indicates that FSA cannot recognize the slight variation of blurriness caused by vibration.\n The above analysis leads to a conclusion: for the real face, artificially controlled vibrations cause a regular blurriness which can be reflected by the variance. However, the FSA can not precisely determine the vibration-caused blurriness of the target image and the fake face does not exhibit regular blurriness. Therefore, vibration-caused blurriness can serve as the feature that differentiates the real and fake videos."}, {"title": "4. System Design", "content": "We design the SFake with three main steps, as shown in Figure 10. First, the SFake sets the artificially designed or randomly generated vibration pattern. The SFake also sets a proper focus length of the camera and focuses on the facial area, as it can influence the degree of the blurriness of the video. Meanwhile, the smartphone starts to vibrate and capture the facial area. Afterward, SFake selects areas most likely to reflect the vibration pattern by detecting the facial landmarks and analyzing the image's gradient information. Finally, we obtain the feature by calculating the variance of these areas frame by frame and filtering out the noise. We determine the authenticity of a video by assessing whether its features reflect the vibration pattern."}, {"title": "4.1. Preparation to Detection", "content": "Selection of vibration patterns. SFake can set the period and duty cycle of the vibration to configure its pattern. Theoretically, the variance sequence should be directly proportional to the vibration pattern. However, the sensitivity of the variance sequence to vibration is limited. For example, if the duty cycle of vibration changes from 0.50 to 0.51, the variance sequence may not change so much. To quantitatively explore the relationship between the vibration patterns and variance sequence, we set the vibration period from 1 to 5 seconds with a step size 1 second and the duty cycle from 0 to 1 with a step size 0.05. For each vibration pattern, we record a video, calculate the variance sequence frame by frame, and utilize the middle value of the variance sequence as a threshold to calculate its duty cycle. The experiment result is shown in Figure 11. In general, the variance sequence can reflect the pattern of vibration. As for period, Figure 11(b) shows that under different duty cycles, the period of variance sequence equals that of vibration. As for the duty cycle, the overall trends of variance sequence reflect the duty cycle of the vibration, but"}, {"title": "4.2. Video Processing", "content": "The fundamental idea of SFake is to determine the authenticity of a video by comparing the changes in video blurriness and vibration patterns. To accurately measure the blurriness, we calculate the gradient of the image. However, to conserve computational resources, we only select several representitive areas to perform gradient calculation.\nGradient calculation. As previously analyzed, the blurriness caused by vibration primarily manifests at the edges of color blocks in the footage. In previous discussions, we directly use the variance of the image as a measure to assess the level of blurriness for further analysis. This method is applicable under good shooting conditions because the distribution of pixel values inside the color blocks is relatively uniform with slight variation in such cases. However, when the shooting conditions are poor, noise may occur within the color blocks due to lighting fluctuations, sensor noise, etc. The noise is widespread throughout the entire image and constantly changes over time, thereby affecting the value of the variance, making the variance not solely affected by the vibration.\nTo eliminate the influence of noise, we apply gradient processing to the image and remove areas with minimal gradient values to prevent the noise within the color blocks from interfering the variance calculation. we calculate the gradient of the image by Eq. 8\n$g[i, j] = \\sum_{u=i-1}^{i+1} \\sum_{v=j-1}^{j+1} \\frac{abs(f[u, v] - f[i, j])}{9}$ (8)\nwhere $f[i, j]$ refers to the pixel value at $i^{th}$ row and $j^{th}$ column, while $g[i, j]$ denotes the grayscale value at the same coordinate. After that, we zero out the gradients smaller than one-tenth of the maximum gradient value. Figure 12(a) shows the raw and gradient image. To demonstrate the effect of the gradient calculation, we record a 4-second video with"}, {"title": "4.3. Data Processing", "content": "Although at most times, the variance sequence of the selected area can reflect the vibration patterns, it can also be affected by other factors, such as lighting changes and subtle body movements. We capture a 4-second video indoors, deliberately opening the curtains slowly during the recording to introduce subtle lighting variations. The vibration pattern is set with a period of 1 second and a duty cycle of 0.5. The variance sequence of the selected area is shown as the blue waveform in Figure 14(a). The sequence does not reflect the vibration pattern clearly because the lighting variations (and other environmental factors) also influence the variance calculation.\nTo extract vibration-related components from the variance sequence, we analyze the frequency domain of the ideal variance sequence, which is a square wave with a period of 1 second and a duty cycle of 0.5. In theory, the actual variance sequence can be considered as the cumulative effect of various factors influencing the blurriness of the video, whereas the ideal variance sequence can be viewed as the isolated impact of vibration on the blurriness. Therefore, to extract the feature reflecting the vibration patterns, we combine the information of both ideal and actual variance sequences. We perform a Fourier transform on the ideal variance sequence to obtain its frequency spectrum and sort the top 80% of its non-zero frequency components as the effective ones. We filter out all other frequency components of the actual variance sequence, and the filtered sequence"}, {"title": "5. Evaluation", "content": null}, {"title": "5.1. Experiment Setup", "content": "Dataset. Considering the absence of available deepfake datasets including physical probe mechanisms, we use 8 different brands of smartphones to record 15 participants of varying genders and ages to build our own dataset. We place the smartphone on the phone holder 20 cm away from the participant and zoom in twice, aiming at the participant's face to encompass all his facial features while vibrating the smartphone in different patterns. For phones whose front cameras cannot zoom, we use the rear cameras as a substitute. We record 150 long videos, each 20 seconds in duration. By default, we assume the detection period lasts 4 seconds. We trim 10 clips of 4 seconds long from one long video by randomizing the start time. Therefore, we get a total of 1500 real clips, each 4 seconds long.\nBased on the real videos, we use five different deepfake algorithms to generate fake videos: Hififace [67] and FS-GANV2 [51] which represent the new deepfake algorithm proposed in the academic community, DeepFaceLive [28] and RemakerAI [1] which represent the widely used online face swap applications, and MobileFaceSwap [71] which represents the lightweight deepfake algorithm specifically designed for the mobile devices. For each deepfake algorithm, we generate 1500 fake videos, each corresponding to a real video.\nEthical Considerations. We prioritize societal security and ethical concerns. All participants comply with approved IRB protocols, ensuring participant awareness of the usage of their images. Additionally, the deepfake samples created for the study are not utilized beyond its scope and securely discarded post-research.\nObject of comparison. We test the performance of 5 existing detection methods from the academic community (SBI [59], FaceAF [37], CnnDetect [65], LRNet [62], DFHob [11]), including two of them specifically designed to be lightweight networks [11] [62]. Additionally, we test Deepaware [16], an online deepfake website, considering its widespread use due to its simplicity and being free of charge.\nClassifier design. We employ a simple two-layer neural network as our classifier with dimensions 120x30 for the hidden layer and 30x1 for the output layer. We utilize ReLU as the activation function and set the learning rate as 0.01. We randomly select 1000 real videos and 1000 fake videos generated solely by DeepFaceLive. For each video, we select 3 areas as the training data. The network converges after 40 epochs. When testing, we also select three areas for every video and determine the authenticity by the majority classfication results among the three areas. It is worth noting that there are possibly better choices for the classifier design, and we will explore the impact of different classifiers on the detection task later.\nEvaluation metrics and devices. We use NVIDIA RTX 3060 to build the dataset and implement our detection method. The detection methods run in Ubuntu 22.04.2 LTS. We record the videos using the Xiaomi Redmi 10x, Xiaomi Redmi K50, OPPO Find x6, Huawei Nova9, Xiaomi 14 Ultra, Honor 20, Google Pixel 6a, and Huawei P60. We use Pytorch to reproduce existing detection methods. We utilize standard metrics for assessing the detection methods: the Area Under the Receiver Operating Characteristic Curve (AUC) and Accuracy (ACC)."}, {"title": "5.2. Overall Performance", "content": "In this part, we compare SFake with other detection models on the dataset to evaluate its capability to classify real and fake videos. Additionally, we measured the computational speed and overhead of various detection methods, demonstrating the efficiency advantages of SFake."}, {"title": "5.2.1. Detection Performance", "content": "Table 1 shows the results of detecting fake faces generated by various deepfake algorithms by different methods. In all cases, the detection accuracy of SFakeexceeded 95%. Among the five deepfake algorithms, except for Hififace, SFake performs better"}, {"title": "5.2.2. Computation Performance.", "content": "As SFake is designed for mobile devices where the computational resource is limited, we discuss the time and memory required by the different detection methods in this part. We use pmamx pid to check the physical memory consumption of the detection process by pid and use the built-in timing function in Python to measure the time consumption. To compare the computational performance, we record a 4-second video and measure the memory and time consumption when detecting it with the 6 detection methods, except for Deepaware, which is deployed online and hides its memory information.\nThe result is shown in Figure 16. Other detection methods require several gigabytes of memory space to load the memory-consumed models. In contrast, SFake consumes less than 450MB of memory space, which is about onetenth of the other detection methods. This is because SFake extracts the features by memory-saving operations like computing the variance, calculating the gradient, performing the Fast Fourier Transform, and using the two-layer neural network for classification. Regarding processing time, SFake takes 4.52 seconds to complete the processing, which is only 0.56 times the shortest duration among the other methods. The most time-consuming part of SFake's detection is calculating the gradient frame by frame within multiple selected areas. However, considering that the variance calculation for each area is independent, we create multiple processes where each process computes the gradient of one area. In this way, calculating the gradient for more areas will not significantly increase the time consumption."}, {"title": "5.3. Impact Factors", "content": "Considering the various scenarios of video communications in real life, we explore the relationship between environmental factors and the distinctness of the feature. We launch several experiments to illustrate the relationship between the performance of SFake and several factors such as lighting conditions, camera resolution, shooting distance, time consumed during the detection process, zoom factor, and classifiers.\nMetrics and setup. To measure the distinctness of features, we define the Proportion of Sequence (POS), which means the ratio of the ideal frequency components to all the frequency components in the actual variance sequence. The ideal frequency components are defined as the non-zero frequency bands of the ideal variance sequence. The mathematical expression for POS is\n$POS = \\frac{sum(f_{avs}[where f_{ivs} != 0])}{sum(f_{avs})}$ (9)\nWhere $f_{ivs}$ and $f_{avs}$ represent the discrete frequency spectrum of the ideal and the actual variance sequence, respectively. A higher POS value suggests that the vibration more significantly influences the variance sequence. We record a 40-second video for each environmental circumstance and generate the corresponding fake video by RemarkerAI as it presents the most significant challenge for detection models according to our experiment results. Subsequently, we trim 100 video clips of four seconds each from the real and"}, {"title": "5.3.1. Lightening Level.", "content": "We explore the performance of SFake under different lighting conditions. We place the smartphone in a small room, adjusting the luminous intensity by controlling the room's lights. We measure the light intensity by the average grayscale value of the video's first frame. The experimental results are shown in Figure 17(a). SFake performs well with a lighting level of 35, akin to illuminating one's face with the light from a computer screen in a dark room. When the lighting level is below 35, the detection accuracy and the real video's POS value decrease rapidly. That may be because insufficient lighting reduces the sharpness of the edges between different color blocks in the footage, resulting in a decrease in the blurriness caused by the vibration. It indicates that SFake works well with even poor lighting conditions."}, {"title": "5.3.2. Resolution.", "content": "Considering SFake may work under poor network conditions which influence the video quality, we explore the detection results at different video resolutions. We first record videos with a resolution of 1920*1080 and then compress the original videos by cv2.resize function to generate videos with smaller resolutions. The result is shown in Figure 17(b). When the resolution is lower than 1920*1080, the accuracy sharply decreases, along with a decrease in real video POS. That is because the blurring caused by vibration is relatively minor, and reducing the image from high resolution to low resolution is essentially a manual process of blurring the image, which overshadows the blurring caused by vibration, resulting in the decrease of real video POS and accuracy. We find that Tencent Meeting [4], Microsoft Teams [3], Zoom [2], and Skype [5] all support resolutions of 1920*1080 or even higher. Additionally, even if the cloud services for SFake are inaccessible due to poor network conditions, we can consider deploying SFake locally. According to the experiment about memory consumption, SFake consumes less than 450 MB, which can run locally on a mobile device and thereby ensure that the video footage is not affected by the network conditions."}, {"title": "5.3.3. Shooting distance.", "content": "This part explores the relationship between detection performance and shooting distance. We place the smartphone at different distances from the subject's face and record the videos. The result is shown in Figure 17(c). When the distance is 20cm, the POS value of the real video reaches its maximum, and the feature is most prominent. Within the range of 20cm to 60cm, the POS value of the real video is significantly greater than that of the fake video, indicating a good detection performance. The detection accuracy decreases when the distance is too close or too far. When the camera is close to the subject, the camera may not encompass the entire facial contour, which reduces the search range when selecting the landmarks with the most gradient value. When the camera is too far from the facial area, the details of the face may not be clear, so vibrations are less likely to affect the variance of the face. As reported, the average distance at which women use mobile devices is 34.7 cm, while for men, it is 38.2 cm [9]. SFake performs well in this distance range."}, {"title": "5.3.4. Detection time.", "content": "In Section 5.2, we set the detection time as 4 seconds, which indicates the classifier's input should be a vector with a length of 120 (as the frame rate is 30 fps). To evaluate different lengths of detection time, we trim or splice the existing variance segmentations to match the length of the input and retrain the model. The results shown in Figure 17(d) indicate that, apart from 4s or 8s, the accuracy decreases to varying degrees at other detection times."}]}