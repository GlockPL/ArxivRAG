{"title": "Expediting and Elevating Large Language Model Reasoning via Hidden Chain-of-Thought Decoding", "authors": ["Tianqiao Liu", "Zui Chen", "Zitao Liu", "Mi Tian", "Weiqi Luo"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in tasks requiring reasoning and multi-step problem-solving through the use of chain-of-thought (CoT) prompting. However, generating the full CoT process results in significantly longer output sequences, leading to increased computational costs and latency during inference. To address this challenge, we propose a novel approach to compress the CoT process through semantic alignment, enabling more efficient decoding while preserving the benefits of CoT reasoning. Our method introduces an auxiliary CoT model that learns to generate and compress the full thought process into a compact special token representation semantically aligned with the original CoT output. This compressed representation is then integrated into the input of the Hidden Chain-of-Thought (HCoT) model. The training process follows a two-stage procedure: First, the CoT model is optimized to generate the compressed token representations aligned with the ground-truth CoT outputs using a contrastive loss. Subsequently, with the CoT model parameters frozen, the HCoT model is fine-tuned to generate accurate subsequent predictions conditioned on the prefix instruction and the compressed CoT representations from the CoT model. Extensive experiments across three challenging domains - mathematical reasoning, agent invocation, and question answering - demonstrate that our semantic compression approach achieves competitive or improved performance compared to the full CoT baseline, while providing significant speedups of at least 1.5x in decoding time. Moreover, incorporating contrastive learning objectives further enhances the quality of the compressed representations, leading to better CoT prompting and improved task accuracy. Our work paves the way for more efficient exploitation of multi-step reasoning capabilities in LLMs across a wide range of applications.", "sections": [{"title": "1 Introduction", "content": "Chain-of-Thought (CoT) prompting, as introduced by (Wei et al., 2022), involves prompting large language models (LLMs) to generate explicit reasoning steps, significantly enhancing their perfor-mance in various reasoning tasks such as mathematical problem solving (Hendrycks et al., 2021, Cobbe et al., 2021) and science question answering (Lu et al., 2022). Subsequent CoT variants (Zhou et al., 2023, Chen et al., 2022, Gao et al., 2023) have aimed at further improving its efficacy across diverse domains. However, these methods enhance LLMs' reasoning capabilities by extending the duration and complexity of reasoning processes and incorporating external computational resources to achieve superior outcomes. This increased computational demand may limit their applicability in real-world development scenarios.\nTo address this challenge, we draw upon the human cognitive process, where chains of thought are often implicitly and instantaneously formed within the mind. This introspection led us to hypothesize that during the decoding phase, a single token could represent the forthcoming cognitive process, effectively compressing the semantic content of an extensive reasoning chain into a specialized token. This approach aligns with recent findings in the domain of In-context Learning (ICL) for LLMs. Research by Wang et al. (2023a) has demonstrated the feasibility of employing 'anchor tokens' as potent conduits for aggregating and transmitting complex information.\nLeveraging this foundation, we propose a novel two-stage fine-tuning framework aimed at generating subsequent outputs, such as precise answers or computational formulas, by utilizing a compressed special token representation in conjunction with the preceding context. The first stage of this framework involves the training of an auxiliary CoT model. This model employs a contrastive loss function to effectively condense an elaborate thought process into a specialized token, herein referred to as [CoT]. Subsequently, we fine-tune our Hidden CoT (HCoT) model to generate the desired output based on the representation of the special token encoded by the CoT model and the preceding instructions, with the parameters of the CoT model remaining frozen. During inference, as shown in Figure 1, our HCoT model halts upon encountering the [CoT] token, at which point it feeds the preceding information to the auxiliary CoT compression model. The auxiliary model then generates a compressed CoT representation encapsulating the subsequent thought process. This compressed representation is then reinserted into the HCoT model to complete the inference. Concurrently, the auxiliary CoT compression model can either continue to generate the full thought process or opt not to, in order to conserve computational resources. Building on the inherent parallelizability of the LLM encoding process, the encoding phase that yields the special token representation is markedly more time-efficient when compared to the time-consuming process of decoding a complete chain of thought. Consequently, this optimization significantly accelerates the rate of inference.\nOur extensive experiments show the potential of HCoT method on four datasets in three challenging domains: mathematical reasoning (Hendrycks et al., 2021, Cobbe et al., 2021), agent invocation (Yao et al., 2023), and science question answering (Lu et al., 2022). The results demonstrate that our HCOT model achieves competitive or improved performance compared to the full CoT baseline, while providing significant speedups of over 1.5x to 3.8x in decoding time.\nTo summarize, our major contributions are:\n\u2022 We propose the Hidden Chain-of-Thought (HCoT) framework, a novel approach that accelerates the inference process of large language models by compressing the multi-step reasoning process into a specialized token representation, thereby reducing computational overhead during decoding.\n\u2022 We introduce a disentangled training paradigm for the multi-step CoT reasoning, enabling isolated error correction and specialized optimization for each component.\n\u2022 Our compression model effectively condenses the entire thought process into a compact special token while maintaining interpretability, allowing for parallel generation of CoT content.\n\u2022 By incorporating a contrastive learning objective, we further enhance the quality of the compressed CoT model. This approach improves CoT prompting and task accuracy through the application of a span-level loss function during supervised fine-tuning.\nWe believe our work paves the way for more efficient exploitation of multi-step reasoning capabilities in LLMs across a wide range of applications."}, {"title": "2 Background", "content": "We first formalize some existing methods in this section. Our approach is not only inspired by these prior techniques but is also benchmarked against them for comparative analysis. We denote a pre-trained LLM with parameters \u03b8. We use lowercase letters such as x, c, and z to represent the user's question, the generated content, and the CoT reasoning process, respectively. For instance, a user question is denoted as x = (x[1],\u00b7\u00b7\u00b7, x[n]), where each x[i] is an individual token, and the probability of the sequence under our model is given by $p_{\\theta}(x) = \\prod_{i=1}^{n} p_{\\theta}(x[i]|x[1]...x[i - 1])$. To accommodate the complexity of reasoning that involves multiple interleaved sequences of content and thought, we extend our notation. For the i-th element in the reasoning process, we use subscripts: zi represents the i-th chain of thought, and ci represents the i-th output content sequence. We use uppercase letters C and Z to denote a collection of output contents and thoughts respectively.\nChain-of-Thought (CoT) Reasoning introduces intermediate steps that guide the model towards generating a more structured and potentially more accurate response. In this framework, the output is divided into several components: intermediate steps zi and content parts ci. The process involves iteratively generating these components based on previous steps: where model sample i-th thought $Z_{i} \\sim p_{\\theta}(z_{i} | X, C_{0}, z_{0}\u00b7\u00b7\u00b7c_{i})$ and then sample following content $C_{i+1} \\sim p_{\\theta}(C_{i+1} | X, C_{0}, Z_{0}\u00b7\u00b7\u00b7 C_{i}, Z_{i})$ given the sampled i-th thought.\nReasoning w/o Chain-of-Thought (CoT) contrasts with the CoT methodology by directly generating the final answer without explicitly modeling the intermediate reasoning steps. In this approach, the model aims to produce the output content C directly from the user's question x: $C \\sim p_{\\theta}(C | x)$. Here, the the reasoning process is implicit within the model's parameters \u03b8 and is not visible."}, {"title": "3 HCoT: Hidden Chain-of-Thought Reasoning", "content": "In this section, we present our novel two-stage training method that incrementally develops the auxiliary CoT model followed by the Hidden CoT (HCoT) model. The auxiliary CoT model is ingeniously crafted to distill the reasoning process into a singular [CoT] token. Subsequently, the HCoT model leverages the encapsulated reasoning within the [CoT] token to facilitate swift and efficient chain-of-thought reasoning. As depicted in Figure 2, our training paradigm encompasses three components: (i) the generation of HCoT training instances from the original dataset to construct data that embodies CoT reasoning, (ii) the auxiliary CoT model training that employs these HCOT instances to create specialized training samples aimed at enhancing CoT reasoning, and (iii) the HCOT Model training phase that utilizes the same HCoT training instances, first replacing the intermediate reasoning steps zi with the special [CoT] token, and upon encountering this special token, leveraging the encoded hidden representation from the frozen Auxiliary CoT model to replace the original input embedding for the special [CoT] token in the HCoT Model, thereby guiding the generation of the subsequent content Ci+1. In the following subsections, we delineate the methodology for constructing HCoT training samples in Section 3.1, elucidate the training dataset preparation and the training procedure for the auxiliary CoT model in Section 3.3, and detail the dataset construction and training process for the HCoT model in Section 3.4."}, {"title": "3.1 HCoT Training Sample Construction", "content": "Constructing training samples for HCoT is a flexible process that can be tailored to the specific requirements of the task at hand and the anticipated format of the CoT. For instance, in our experiments on math reasoning tasks, as depicted in Figure 2, data from sources such as Math and GSM8K are utilized to construct training samples employing a GPT-4 based ICL method. (Specific prompts are in AppendixB.) This approach enables the model to output a series of contents and thoughts denoted as $C_{0}, z_{0},..., z_{n-1}, C_{n}$ which is sampled from:\n$p(C, Z | x) = \\prod_{i=1}^{n} p_{\\theta}(C_{0} | X)p_{\\theta}(z_{i-1} | X,..., C_{i-1})p_{\\theta} (C_{i} | X, C_{0},..., Z_{i-1}), \\qquad(1)$"}, {"title": "3.2 Disentangled Training Paradigm", "content": "Delving deeper into Equation 1, we have disentangled the probability distribution into distinct components, we can identify the segment $p_{\\theta}(Z_{i-1} | X,..., C_{i-1})$ as the generation phase of CoT information, namely auxiliary CoT model. Conversely, the segment $p_{\\theta}(c_{i} | X, C_{0},..., Z_{i-1})$ is re-sponsible for harnessing the generated CoT to produce subsequent content, namely content generation model. This observation has led to the conceptualization of disentangled training paradigm which first compressing the high-dimensional discrete distribution of $p_{\\theta}(Z_{i-1} | X,..., C_{i-1})$ into a more compact thought representation via encoding the input content with $p_{\\theta}^{COT}$, which corresponding to the auxiliary CoT model trianing. Subsequently, the content generation model $p_{HCoT}$ is fine-tuned to maximize $p(c_{i} | X, C_{0},..., Z_{i-1})$, where $[z_{0} ... Z_{i-1}]$ denote the compressed representations provided by the CoT model. This disentangled training paradigm offers several beneficial training dynamics:\nError Isolation: By decoupling the training of the auxiliary CoT model ($p_{\\theta}^{COT}$) from the content generation model ($p_{\\theta}^{HCoT}$), errors in reasoning can be isolated within the auxiliary CoT model. This facilitates targeted corrections without affecting the content generation model, thereby preventing the propagation of errors and enhancing the overall robustness of the system.\nSpecialized Optimization: The disentangled training paradigm allows for specialized optimization strategies. The CoT model can be honed to refine reasoning abilities and logical coherence, while the content generation model can concentrate on articulating clear and pertinent content. This specialization ensures that each model maximizes its performance in its respective domain, leading to a more effective training process.\nParallel Development and Improved Interpretability: The CoT generation operates in parallel with the generation of actual content. This not only accelerates the inference speed but also preserves the interpretability of the model. Unlike a black box approach, the reasoning steps generated by the CoT model are explicit and can be scrutinized, allowing for a better understanding of the model's thought process and facilitating easier debugging and refinement.\nTo be noticed, We also include the part of $p_{\\theta}(C_{0} | x)$ in our content generation model."}, {"title": "3.3 Auxiliary CoT Model", "content": "In this section, We use lowercase letter r to represent special [CoT] token's representation, and $r_{i}$ denotes the i-th special [CoT] representation. Given the user's question and the preceding content, the objective of the auxiliary CoT model is to distill the reasoning process into a compact representation by maximizing $p_{COT} (z_{i} | x, ..., C_{i}, r_{i})$, where zi is the desired thought process.\nTraining Data Configuration: As depicted in the top right corner of Figure 2, the training data for the auxiliary CoT model is constructed by first extracting all the thought processes from the original HCoT training samples. Subsequently, between each content segment ci and each thought segment zi, we insert a special token, denoted as [CoT], where this unique token serves as an anchor to facilitate the generation of the subsequent thought process. We then segment the entire training sample into individual instances, each treating a thought process zi as the target output, conditioned on the preceding context comprising the question x, special [CoT] tokens and the content segments up to $C_{i-1}$.\nThought Compression: The auxiliary CoT model $p_{COT}^{\\theta}$ is trained by maximizing the likelihood $p_{COT} (z_{i} | X,..., c_{i}, r_{i})$, where we expect the model to generate the most accurate thought represen-tation based on the preceding information. In addition to the conventional cross-entropy loss, we incorporate symmetric contrastive loss between the thought process respresentations mean-pooling and the [CoT] token representation to enhance the thought compression capability of the model. The underlying assumption is that the compressed thought representation should exhibit a higher affinity with its corresponding special [CoT] token than with other [CoT] tokens, and vice versa. The final loss function for the auxiliary CoT model is as follows:\n$L_{COT} = L_{CE} + \\lambda \\cdot L_{contrastive} = -log p_{COT}(z_{i} | x, ..., C_{i}, r_{i}) \\\\\n-\\frac{\\lambda}{2} (log \\frac{exp(z_{i}r_{i})}{\\sum_{k\\neq i}^{n} exp(z_{i}r_{k})}+ log \\frac{exp(z_{i}r_{i})}{\\sum_{k\\neq i}^{n} exp(z_{k}r_{i})}),  \\qquad(2)$\n, where n denotes the batch size during the auxiliary CoT model's training, $z_{i} \\in R^{d}$ represents the normalized representation of the i-th target thought process, obtained by mean-pooling the final hidden states from $p_{\\theta}^{COT}$ when the input is the sequence $[z_{i}[0], z_{i}[1], . . ., z_{i}[t]]$. Additionally, $r_{i} \\in R^{d}$ denotes the normalized representation of the corresponding special [CoT] token generated by the auxiliary CoT model. The $L_{contrastive}$ is introduced to enhance the compactness of the thought process representation, ensuring that it exhibits a higher affinity towards the corresponding target thought process representation. Here, \u03bb is a hyperparameter that governs the trade-off between the contrastive loss and the primary cross-entropy loss term."}, {"title": "3.4 HCOT Model", "content": "Training Data Configuration: The training data construction process for the HCoT model is illustrated in the bottom right corner of Figure 2. We replace all the thought processes zi in the original HCoT training samples with the special [CoT] token. This transformation aligns the training paradigm seamlessly with the auxiliary CoT model's training, as they share the same input format. By substituting the explicit thought processes with the compact [CoT] token, we effectively leverage the distilled reasoning encapsulated within the auxiliary CoT model's output representation. This approach ensures that the HCoT model's training is closely tied to the learned thought representations from the auxiliary CoT model, facilitating the transfer of reasoning capabilities.\nSupervised Fine-tuning of HCoT Model: Given the user's question x, the objective of the HCOT model is to maximize -1 $p_{HCoT} (C_{0} | x)p_{HCoT} (C_{i} | x, C_{0},..., Z_{i-1})$, where $z_{i-1}$ is the com-pressed thought representation obtained from the auxiliary CoT model. In this stage, we freeze the parameters of the auxiliary CoT model and fine-tune the HCoT model to effectively leverage the reasoning encapsulated within the compressed representations. Notably, the training target sequence comprises both content segments and special [CoT] tokens, requiring the model to learn not only the generation of content but also the appropriate insertion of the compressed thought representations denoted by the [CoT] tokens. We employ the standard cross-entropy loss function to supervise the fine-tuning process of the HCOT model."}, {"title": "4 Experiment", "content": "We conducted experiments on three downstream tasks including four seed datasets: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), ScienceQA (Lu et al., 2022), and HotpotQA (Yang et al., 2018). The ScienceQA dataset was categorized into three subjects: ScienceQA (natural science), ScienceQA (social science), and ScienceQA (language science). We prepared the training, validation, and test data for the auxiliary CoT model and the HCoT model based on the seed datasets, as detailed in Table 3. It is worth noting that there is no dedicated test data for the auxiliary CoT model, as we evaluate the end-to-end performance of the HCoT model. During training, we select the best-performing auxiliary CoT model by identifying the model with the lowest perplexity score on the auxiliary CoT model validation set. The train, validation, and test datasets remain consistent across other baselines for fair comparison. Notably, the auxiliary CoT training and validation data were constructed from the HCoT training and validation portions of the data, preventing the usage of more data than other baseline methods.\nFor the math reasoning datasets, GSM8K and MATH, we generated separate fields for thoughts and contents based on the input questions using the method described in Section 3.1, implemented by GPT-4. For the question answering task, we directly utilized the original fields from the ScienceQA dataset\u00b9. Notably, for the agent invocation task, we employed fields generated through the ReAct\u00b2"}, {"title": "4.5 Discussion", "content": "The Speedup of HCoT Model: Table 2 presents the compression and speedup rates of the HCOT model during the inference stage across four datasets compared to the explicit CoT model, both of which are based on LLaMa2-7B. The results for the LLaMa2-13B model are included in the Appendix due to space limit. Firstly, let's clarify the metrics used in the table. S-CR (Sequence-Level Compression Rate) refers to the average number of completion tokens of the HCOT model compared to the CoT model. S-S (Sequence-Level Speedup) is the reciprocal of S-CR, representing how many times faster the HCoT model is compared to the Full CoT model. W-CR (Wall-clock Time Compression Rate) provides a realistic measure of user-perceived speed-up, by comparing the actual inference time of the HCoT model to the Full CoT model. W-S (Wall-clock Time Speedup) is the reciprocal of W-CR. The table shows that S-CR values range from 23.78% to 66.91%, indicating that the sequence length of the HCoT model's output is significantly shorter than that of the Full CoT model. Correspondingly, the W-CR values range from 35.82% to 71.04%, and W-S values range from 1.41x to 2.79x, showcasing a substantial acceleration. It's important to note that the sequence-length-based acceleration rate (S-S) is generally higher than the real-time acceleration rate (W-S). This discrepancy arises because the sequence-length-based measure does not account for the encoding time of the Auxiliary CoT Model in HCoT. These times were tested on an H800 cluster using a single 80GB GPU. Despite achieving a speedup range of 1.41x to 2.79x, the HCoT model also delivers superior performance compared to the Full CoT model, demonstrating its efficiency and effectiveness. More fine-grained analysis of the length distribution before and after compression are detailed in Appendix.\nThe recovery of CoT process: Although in general scenarios, people prefer concise yet accurate responses that have undergone CoT reasoning, it is reasonable to compress the CoT process simply at this time. However, in certain situations, people wish to see the complete CoT process, thus maintaining the option to retain full output of CoT is important. Our model employs a method similar to \"hiding\", where the complete CoT process is concealed during the main reasoning process of the HCOT model, but the Auxiliary CoT model can still produce it normally when required. When asked to display the complete CoT process, we can simply keep the other settings unchanged and request the CoT model to continue outputting as needed. The case study in Appendix D shows this process."}, {"title": "5 Related Work", "content": "Chain-of-thought prompting (CoT) enhances the emergent reasoning abilities of LLMs by prompt-ing them to use explicit reasoning steps. Zero-shot-CoT (Kojima et al., 2023) demonstrates notable improvements in diverse reasoning tasks by merely prefacing solutions with the phrase \"Let's think step by step,\". The least-to-most prompting (Zhou et al., 2023) approach effectively addresses com-plex problems by decomposing them into manageable subproblems and resolving them sequentially. Wang et al. (2023b) considers the self-consistency of CoT, enhancing its performance through major-ity voting. Furthermore, Gou et al. (2023) and Yuan et al. (2023) employ CoT on GPT-4, stabilizing the CoT capabilities of open-source models through fine-tuning on sampled data. Recent studies like ? and? highlight the impact of reasoning step length on performance, suggesting that extending reasoning steps can improve outcomes. Our method achieves a similar effect by effectively extending CoT reasoning length, but with the added benefit of saving time during the decoding phase.\nEfficient model inference often utilizes model compression techniques (Han et al., 2016) such as pruning or quantization. LLM-Pruner (Ma et al., 2023) implements structural pruning, which selectively removes non-critical coupled structures based on gradient information. LLM-QAT (Liu et al., 2023) leverages generations produced by the pre-trained model, enabling quantization of any generative model independently of its training data, akin to post-training quantization methods. Additionally, Gloeckle et al. (2024) considers multi-token prediction as an auxiliary training task, asking the model to predict the following n tokens using n independent output heads at each position in the training corpus, which not only accelerates inference but also enhances performance. ? proposes compressing prompts with gist tokens, focusing on efficient encoding. Deng et al. (2023) is closely related to ours, employing a method where the model is trained to predict hidden states for implicit CoT reasoning, aiming to reason more effectively. However, this approach exhibits a significant performance decline compared to explicit CoT reasoning, lacks interpretability, and involves a more complicated training process. In contrast, our method streamlines training, enhances reasoning path optimization, and sustains robust performance across tasks using models with over 7B parameters, offering a more interpretable, practical, and efficient solution."}, {"title": "6 Conclusion", "content": "We proposed HCoT, an innovative framework designed to accelerate the inference process of large language models while preserving their multi-step reasoning capabilities. At its core, HCoT employs a disentangled training paradigm that decouples the reasoning process into two specialized components: an auxiliary CoT model and a content generation model. The auxiliary CoT model is trained to compress the entire thought process into a compact, specialized token representation through a contrastive loss objective. This compressed representation effectively encapsulates the core reasoning steps, enabling efficient parallel computation during inference. The HCoT model, in turn, is fine-tuned to leverage this compressed reasoning representation, seamlessly integrating it into the content generation process. Our extensive experiments across diverse domains demonstrate the efficacy of the proposed HCoT framework. The results highlight its ability to achieve competitive or improved performance compared to the full CoT baseline while providing significant speedups of at least 1.5x in decoding time. However, it is important to acknowledge that this increase in efficiency comes at the cost of a more complex training phase and the necessity for additional model parameters, which may present scalability and resource challenges. In the future, we hope to address these limitations by optimizing the training phase and enhancing the model's scalability, potentially reducing the need for additional parameters and lessening the training resource burden."}]}