{"title": "Path-based summary explanations for graph recommenders", "authors": ["Danae Pla Karidi", "Evaggelia Pitoura"], "abstract": "Path-based explanations provide intrinsic insights into graph-based recommendation models. However, most previous work has focused on explaining an individual recommendation of an item to a user. In this paper, we propose summary explanations, i.e., explanations that highlight why a user or a group of users receive a set of item recommendations and why an item, or a group of items, is recommended to a set of users as an effective means to provide insights into the collective behavior of the recommender. We also present a novel method to summarize explanations using efficient graph algorithms, specifically the Steiner Tree and the Prize-Collecting Steiner Tree. Our approach reduces the size and complexity of summary explanations while preserving essential information, making explanations more comprehensible for users and more useful to model developers. Evaluations across multiple metrics demonstrate that our summaries outperform baseline explanation methods in most scenarios, in a variety of quality aspects.", "sections": [{"title": "I. INTRODUCTION", "content": "The interpretability and transparency of machine learning models have emerged as critical areas of research, particularly within the domain of explainable AI [1], [2]. Although substantial research has been done on explaining recommendations [3], most of this research focuses on explaining individual recommendations, hence explanations for each specific recommendation made to a user. However, users often seek a broader understanding of how the entire system functions towards them [4], while item providers want to comprehend the overall system behavior regarding the items they offer [5], [6]. For example, imagine a music streaming user who receives several recommendations for songs. Explanations relate the recommended songs to several songs, artists and genres that the user has enjoyed in the past. The user would like a single explanation highlighting the most common reasons for the recommendations received. To address these needs, we propose summary explanations.\nWe introduce two types: user-based summary explanations, which explain the overall behavior of the system to a specific user -explaining why they receive specific item recommendations and item-based summary recommendations, which explain the global behavior of the system for a specific item based on the users to whom the item is recommended. By summarizing multiple explanations, users can determine the most relevant aspects and the overall rationale behind the recommendations they receive, avoiding information overload [7] and enabling them to make more informed decisions. Item providers can identify key features that appeal to users or require improvement, while model developers can detect underlying patterns in model behavior, facilitating targeted and effective system enhancements.\nIn addition, we extend our summary recommendations to groups of users and items. Analyzing group-based summaries helps identify potential biases affecting specific user groups or item categories. These summaries are also useful for targeted marketing and advertising by enabling marketers to design more effective, personalized strategies, while item providers can tailor their advertising based on insights into the interests of specific user groups.\nWe focus on four key summarization scenarios: user-centric summarizes the explanations to a user, item-centric summarizes the explanations to various users for a specific item, user-group summarizes the explanations for a group of users, and item-group summarizes the explanations to various users for a group of recommended items.\nMoreover, we introduce efficient algorithms for computing summary explanations tailored for graph-based recommenders, which leverage graph structures to capture complex relationships between users, items, and their interactions, enhancing recommendation accuracy [8], [9]. Graph-based recommendation systems are used in major platforms. Twitter recommends connections between users and content based on shared interests and activities [10], while Spotify uses a co-listening graph linking podcasts and audiobooks [11]. Amazon combines co-purchase and co-view data to build a product recommendation graph [12]. Similarly, Facebook [13] and Pinterest [14] incorporate graph algorithms to personalize content recommendations. A common method for explaining graph-based recommendations is path-based explanations, which trace the paths from users to recommended items using the graph structure [15]. Path-based approaches (also called meta-path or path-embedding techniques), such as PGPR [16] and CAFE [17], are considered state-of-the-art for both recommendation accuracy and explanation quality [18], [19]. Recent models PLM [20], and PEARLM [21] enhance these explanations by using pre-trained language models.\nWe propose a novel approach based on the Steiner Tree problem, aggregating individual path-based explanations into"}, {"title": "II. RELATED WORKS", "content": "Previous research has not specifically addressed the task of explanation summarization in recommendation systems. Our work fills this gap by contributing to the broader field of explainable recommendations, with a particular focus on intrinsic graph-based explainable recommendations. Additionally, our research intersects with techniques in graph summarization.\nExplainable recommenders aim to explain why items are recommended to users. Recent machine learning advancements have heightened the focus on explainability [3], leading to new models such as Latent Factor Models (LFMs) [24], representation learning methods [25], and generative models using large language models (LLMs) for personalized textual explanations [26], [27]. Explainable models can be classified into interpretable models, with transparent decision mechanisms [28], and black-box models, which generate explanations post-recommendation [29]. Intrinsic explanations [30], [31] applied on interpretable models [30], [31] provide insights directly from the recommendation pipeline itself, aiding user comprehension.\nGraph-Based Explanations. Our work focuses on graph-based explanations, leveraging knowledge-enhanced graphs where nodes represent users and items, and edges represent interactions. These graphs inject semantic content and extend the graph beyond a bipartite structure, offering efficient recommendations and meaningful explanations [18]. Graph-based explanations can be black-box, using embeddings for historical interaction representations [32]-[36], or intrinsic, using graph reasoning and link prediction [37]. Our method summarizes explanation paths over knowledge-enhanced graphs and is suitable for intrinsic models that naturally provide explanation paths. Importantly, our approach is compatible with any recommendation method that outputs explanation paths, regardless of the specific reasoning algorithms it uses. Additionally, for methods that do not output paths but provide recommended items and access to underlying graph data, our approach can generate new path explanations based on the graph structure. We benchmark against several intrinsic path-based methods: PGPR [16], which uses reinforcement learning for path reasoning, and CAFE [17], which uses historical patterns and adversarial training to guide path-finding. Additionally, we provide experimental results against PLM-Rec [20] and PEARLM [21], [38]. Both methods employ language models over knowledge graphs to generate explainable recommendations. PLM-Rec generates novel paths beyond the static KG topology, while PEARLM improves upon this by ensuring that generated paths faithfully adhere to valid KG connections.\nGraph Summarization Techniques. Summarizing explanation paths can be viewed through the lens of the general problem of graph summarization [39], which often involves clustering nodes into super-nodes [40], [41] or merging edges [42]. Other graph-summarization methods involve selecting a subset of \"important\" nodes or edges, resulting in a sparsified graph [43], [44]. For instance, some of the work in [45] creates a summarization by identifying the most important nodes using centrality measures and then finding the Steiner Tree that connects them [45].\nExisting graph summarization techniques are primarily designed for general applications and focus on reducing graph complexity and achieve storage efficiency. However, in the context of recommendation systems, these methods often lead to the loss of crucial local structures and detailed information that are vital for understanding user-item interactions. Moreover, node and edge aggregation methods prioritize overall structure over detailed interactions. Hence, they can obscure the specific paths and relationships that explain why certain items are recommended to users. Additionally, the optimization functions used are application-dependent, limiting their generalizability across different types of graphs. Our work, instead, focuses on summarizing recommendation paths by directly including recommended items as terminal nodes and leveraging information from individual explanation paths. We apply a prize-collecting Steiner Tree method to optimize the"}, {"title": "III. PROBLEM DEFINITION", "content": "Let $U = \\{u_1,u_2,...,u_n\\}$ be a set of users and $I = \\{i_1, i_2,..., i_m\\}$ be a set of items. The $n \\times m$ rating matrix $M$ is defined such that $M[u, i] = (r, t)$, where r is the positive rating and t is the timestamp of the rating, if $u \\in U$ has rated $i \\in I$, and $M[u, i] = (0,0)$ otherwise (indicating no rating). As many graph recommenders [16], [17], [51], we construct from M, a directed weighted graph $G_M(V_M,E_M,w_M)$, where $V_M = U \\cup I$ is the set of nodes corresponding to users and items. $E_M \\subset U \\times I$, meaning there is an edge from u to i if $M[u, i] \\neq (0,0)$ [16], [17], [51].\nIn recommendation systems, it is essential to consider both the history and recency of user-item interactions. Older interactions may be less indicative of current preferences, while higher ratings reflect stronger user preferences. To address this, the weight function $w_M$ is defined on $E_M$ and maps to $R$, combining the rating r and the timestamp t from the rating matrix M. For an edge $(u, i) \\in E_M$, where $M[u, i] = (r, t)$:\n$w_M(u,i) = \\beta_1 \\cdot r + \\beta_2 \\cdot f(t)$. Here, $\\beta_1$ adjusts the importance of rating, $\\beta_2$ adjusts the importance of recency, and $f(t_{ui})$ is the recency function that assigns higher values to more recent interactions. The dependency on the rating ensures that interactions with higher ratings have a greater impact on the summarization process. The recency function is defined as: $f(t_{ui}) = e^{-\\gamma(t_0-t_{ui})}$, where $t_0$ is the current time, and $\\gamma$ is a decay parameter controlling how quickly the weight decreases with time. The exponential decay function is chosen to reflect the natural diminishing influence of older interactions. This combined use of recency and rating scores ensures that the model prioritizes recent and highly rated interactions, which are more likely to align with the user's current preferences.\nWe extend $G_M$ with a set of external nodes $V_A$ that represent additional information such as item categories, user attributes, etc., and edges $E_A \\subset V_M \\times V_A$ that connect users and items with the external nodes. The weight of these edges is defined by $w_a$, a weight function $E_A \\rightarrow R$. The weight $w_A(v, a)$ represents the relevance score of the external node $a \\in V_A$ to the user or item $v \\in V$.\nThe extended graph $G(V, E, w)$ of $G_M(V_M,E_M,w_M)$, called the knowledge-based graph, is a directed weighted graph where $V = V_M \\cup V_A$, $E = E_M \\cup E_A$, and a weight function $w : E \\rightarrow R$, where $w(e) = w_M(e)$ if $e \\in E_M$, and $w(e) = w_A(e)$ if $e \\in E_A$.\nThe knowledge-based graph is used to generate recommendations, where a recommendation is a set of items for each user and the explanation is a path [16]. This means that the explanation $E(u, i)$ for the recommendation of item i to user u is a path, starting from a user node $u \\in U$ and ending at an item node $i \\in R_u$: $E(u,i) = (u, v_1, v_2, ..., v_k, i)$, where $u, v_1, v_2, ..., v_k, i \\in V$ and $(u, v_1), (v_1, v_2), ..., (v_k, i) \\in E$.\nWe use $R_u$ to denote the set of recommended items to user u: $R_u = \\{i | i \\in I$ and i is recommended to u$\\}$. We use $E_u$ to denote the corresponding explanation paths leading from u to the items in $R_u$: $E_u = \\{E(u, i) | i \\in R_u\\}$.\nWe use $C_i$ to denote the set of users to whom the item i is recommended: $C_i = \\{u | u \\in U$ and $i \\in R_u\\}$. Analogously, we use $E_i$ to denote the corresponding explanation paths leading from users in $C_i$ to item i: $E_i = \\{E(u, i) | u \\in C_i\\}$.\nIn this paper, we propose explanations for a set of recommendations as opposed to individual recommendations. While individual explanations are paths of G, summary explanations are subgraphs of G. We start by defining summary explanations for users and items and then generalize our definitions for groups of users and items.\n1) User-centric and item-centric summary explanations: Summary user-centric explanations summarize the explanation paths leading from a user to their recommended items. An aggregated view of the individual recommendation paths, enables users to determine more effectively the key aspects of the recommendations they get.\nThe straightforward definition of summary user-centric explanation is as a union graph that includes the explanation paths $E_u$ of the items in $R_u$. However, this simplistic consolidation can lead to information overload, obscuring the overarching rationale and systems behavior towards the user. Users may be overwhelmed by the volume of information and struggle to discern the key reasons behind their recommendations. For explanations to be interpretable, they must be concise and manageable in size [52] to highlight the most relevant paths and connections and enhance user comprehension.\nFor example, consider User 1, who receives recommendations for the movies Eternity and a Day (Item A), The Beekeeper (Item B), and The Suspended Step of the Stork (Item C). While the original explanations had a total length of 13, the summarization achieves a length of 6 edges, significantly increasing the comprehensibility of the explanation. In addition, before the summarization, User 1 needed to understand how she connected to each movie through multiple separate and convoluted paths. Specifically, each individual path includes intermediary steps that, while part of the explanation, may not all be crucial for understanding"}, {"title": "IV. COMPUTING SUMMARY EXPLANATIONS", "content": "In this section, we present efficient algorithms for computing summary explanations for graph-based recommenders. We employ two graph-based approaches: the Steiner Tree and the Prize Collecting Steiner Tree.\nA. Steiner-Tree (ST) Summary Explanations\nTo solve the problem of summarizing explanation paths, we leverage the concept of the Steiner Tree. The Steiner Tree problem, in its classical form, seeks the shortest tree that spans a given set of terminal nodes in a graph, potentially including additional intermediate nodes (Steiner nodes) to minimize the total edge weight. However, our objective is to maximize the total weight of the subgraph while minimizing the number of edges, effectively balancing between weight maximization and edge minimization. To adapt the problem for the Steiner Tree solution, we align the edge weights with the Steiner Tree's minimization objective by multiplying all edge weights by -1, converting the problem into one that seeks to maximize the weights, consistent with the Steiner Tree approach. The set of terminal nodes for user-centric summary is $T = u \\cup R_u$, for item-centric summary $T = i \\cup C_i$, for user group summary $T = D \\cup R_D$, and for item-group summary $T = F \\cup C_F$. \n\nThe ST-based algorithm has time complexity $O(|T|(|E|+|V|log |V|))$ under typical conditions, where T (terminals) is small relative to large V (nodes), and E (edges). Its approximation ratio to the optimal Steiner Tree solution is at most 2 [53].\nThe initial weights of edges in the graph $G_M$ are determined based on the user-item interaction matrix M. While this captures the historical interactions between users and items, it does not account for the specific explanation paths that are generated for individual recommendations. The problem with this approach is that without adjusting the weights to reflect the importance of edges in the explanation paths, the summarization algorithms may not prioritize the most relevant paths. This could lead to summaries that merely take into account the individual explanations and create entirely new explanations instead of summarizing the individual ones. To address this, we propose a solution where the weight function w(e) increases the initial weights of edges of $G_M$ based on their inclusion in the individual explanation paths. This adjustment is controlled by the parameter $\\lambda$, which allows us to balance the influence of the initial weights and the frequency of edges in the explanation paths. By incorporating both the initial user-item interaction matrix M and the presence of edges in the explanation paths, our weight function ensures that the summarization algorithms prioritize paths that are part of the individual explanation paths, effectively summarizing them.\nThe weight function w(e) is defined as:\n$w(e) = w_M(e) (1 + \\lambda \\cdot \\frac{1_{e \\in P}}{\\|S\\|}),$\nwhere:\n* $w_M(e)$ is the initial weight assigned to edge e in $G_M$.\n* P varies based on summarization type: user-centric: $P = E_u$, item-centric: $P = E_i$, user-group: $P = E_D$, item-group: $P = E_F$.\n* $\\lambda$ is a positive scaling factor adjusting the impact of the input explanation path frequency to the summarization. If $\\lambda$ is set to zero, the summarization algorithm generates a new explanation, as the impact of the existing explanation paths is effectively nullified. Consequently, the algorithm creates a new explanation that includes S.\n* $1_{e \\in P}$ is an indicator function that returns 1 if edge e is part of the explanation paths P and 0 otherwise.\n* S varies based on summarization type: user-centric: $S = R_u$, item-centric: $S = C_i$, user-group: $S = R_D$, item-group: $S = C_F$.\nB. Prize-Collecting ST Summary Explanations\nTo address the problem of a significant increase in the number of terminal nodes when creating group summaries, we leverage the concept of the Prize-Collecting Steiner Tree (PCST) to balance maximizing the total weight of the subgraph with minimizing the number of edges.\nThe PCST is a generalization of the classical Steiner tree problem. Given a weighted graph and a set of terminal nodes, the Steiner tree problem seeks to find a minimum-cost spanning tree of the terminal nodes. The prize-collecting variant relaxes the connectivity constraint by assigning a prize to each terminal node. Instead of connecting a terminal node to our spanning tree, we can choose to forego the prize of the omitted terminal. The PCST problem aims to find a subgraph that minimizes the total cost, which is the sum of the weights of the included edges minus the prizes of the included nodes. This formulation captures the trade-off between the costs of connecting a terminal and omitting it from the solution. Hence, in the context of group-centric summarization, this approach allows us to prioritize important nodes and edges while reducing the overall complexity of the explanation paths.\nTo this end, we normalize the edge weights w(e) to set the prizes a and $\\beta$ as follows: $a = max(w(e))$ and $\\beta = min(w(e))$. Following, we assign node prizes p(v) such that $p(v) = a$ for nodes in the set of terminal nodes T, and $p(v) = \\beta$ for nodes not in T. High prizes for terminal nodes (a) prioritize their inclusion, while low prizes for non-terminal nodes ($\\beta$) discourage their inclusion unless they are essential for connecting terminals. This approach balances the inclusion of terminal nodes and edge costs, ensuring non-terminal nodes are added only if they significantly reduce connection costs.\nThe total cost to be minimized is: $C(S) = \\sum_{e\\in E_s} w'(e) - \\sum_{v\\in V_s} p(v)$, where C(S) represents the total cost of the subgraph S, w'(e) is the modified edge weight for edge $e \\in E_s$, and p(v) is the prize for node $v \\in V_s$.\nThe algorithm runs in $O((|V| + |E|) log |V|)$ time."}, {"title": "V. EXPERIMENTAL EVALUATION", "content": "In this section, we present experimental results that evaluate the quality of our summaries and the performance of our algorithms. We primarily use the MovieLens 1M (ML1M) dataset [22] and the DBpedia Knowledge Graph (KG) [55]. To enrich ML1M, we extract contextual information from DBpedia dumps [56], integrating attributes such as director, actors, genre, composers, and other relevant properties for movies. We construct the knowledge-based graph by integrating user-item interactions from ML1M with DBpedia. In these graphs, nodes represent users, movies, and DBpedia entities, while edges connect users to movies based on interactions, with weights indicating user ratings (see Section IV-A).\nWe also present experiments using the LFM1M dataset [23] enriched with relevant information about songs and their properties from DBpedia [56] at the end of this section.\nA. Experiment setup\nUsing the Steiner and PCST algorithms we created summaries for each of the following scenarios: user-centric, item-centric, user-group, and item-group summaries.\nUser and Item Sampling. For user-centric summarization, we selected 100 male and 100 female users, preserving the original rating distribution to reduce bias. For item-centric summarization, we chose 100 items, split equally between the 50 most and 50 least popular items, ensuring both popular and less popular items were evaluated. These sampled subsets also form the user-group and item-group scenarios. We also conducted an additional experiment for user-group and item-group scenarios with varying group sizes to evaluate scalability.\nBaseline explanation paths. The main experiments utilize individual explanation paths generated with the PGPR [16] and CAFE [17] methods for a sampled dataset of 200 users. These paths explain the top-10 item recommendations, each reaching the recommended item within a maximum of three edges. The preprocessing of these paths involved generating"}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce summary explanations and provide efficient algorithms based on the Steiner Tree and Prize-Collecting Steiner Tree for graph-based recommenders. To the best of our knowledge, this is the first work to summarize path-based recommendation explanations for individual users, individual items, groups of users, and groups of items. Our results show that these summary explanations outperform baseline methods across multiple evaluation metrics. Future work will explore explanation summaries to assess explanation fairness across user demographic and item category groups, as well as various refinements of our algorithms, including testing additional PCST prize assignment policies and considering incorporating node centrality measures. We also plan to study summaries to non-graph-based recommenders."}]}