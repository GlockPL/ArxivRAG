{"title": "Proactive Adversarial Defense: Harnessing Prompt Tuning in Vision-Language Models to Detect Unseen Backdoored Images", "authors": ["Kyle Stein", "Andrew Arash Mahyari", "Guillermo Francia", "Eman El-Sheikh"], "abstract": "Backdoor attacks pose a critical threat by embedding hidden triggers into inputs, causing models to misclassify them into target labels. While extensive research has focused on mitigating these attacks in object recognition models through weight fine-tuning, much less attention has been given to detecting backdoored samples directly. Given the vast datasets used in training, manual inspection for backdoor triggers is impractical, and even state-of-the-art defense mechanisms fail to fully neutralize their impact. To address this gap, we introduce a groundbreaking method to detect unseen backdoored images during both training and inference. Leveraging the transformative success of prompt tuning in Vision Language Models (VLMs), our approach trains learnable text prompts to differentiate clean images from those with hidden backdoor triggers. Experiments demonstrate the exceptional efficacy of this method, achieving an impressive average accuracy of 86% across two renowned datasets for detecting unseen backdoor triggers, establishing a new standard in backdoor defense.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks (DNNs) have revolutionized fields ranging from object classification [1] and face recognition [2] to reinforcement learning [3] and natural language processing [4], setting new benchmarks in performance and innovation. However, this remarkable success has made them prime targets for sophisticated adversarial manipulations. Among the most insidious threats are backdoor attacks, which stealthily embed hidden patterns-known as triggers into models, causing them to misclassify inputs into an adversary's chosen target label. These backdoors can be implanted through malicious techniques like data poisoning [5] or neuron hijacking [6], posing an immediate and formidable challenge. In response, the research community has developed numerous defense and detection strategies [7], [8], [9], [10], [11]. Early approaches focused on purifying compromised models using methods such as fine-tuning [12], [13] or distillation [14]. More recently, cutting-edge techniques have attempted to neutralize adversarial triggers by leveraging limited training or in-distribution samples [9], [15]. Furthermore, researchers in [16] utilize a Vision Transformer (ViT) to classify previously seenadversarial attack patterns targeting traffic sign recognition systems in autonomous vehicles. These advances mark significant strides in safeguarding DNNs, but the persistence and evolution of adversarial threats demand continued innovation to stay ahead in this escalating arms race.\nContribution: Despite advancements in adversarial defense and training algorithms, achieving 100% protection against adversarial attacks remains elusive. These traditional methods are reactive, focusing on cleansing already-compromised models of embedded backdoor triggers. In contrast, this paper introduces a revolutionary and complementary strategy: a proactive algorithm designed to detect adversarial images before they wreak havoc. Our approach serves two critical purposes: i) Pre-Training Defense: Before training begins, the algorithm meticulously scans the dataset to identify and eliminate adversarial (backdoored) images that could poison object recognition models. This ensures the integrity and purity of the training data, safeguarding the foundation of model learning. 2) Inference-Time Shielding: During inference, the algorithm acts as a vigilant gatekeeper, inspecting incoming images to block adversarial content from reaching the object recognition system. This prevents adversarial images from manipulating the model to misclassify inputs into the adversary's target class. By proactively identifying and neutralizing adversarial threats, this approach works in tandem with traditional defense algorithms, creating a robust and comprehensive safeguard against adversarial attacks. It represents a significant leap forward in securing object recognition systems."}, {"title": "II. PROPOSED METHOD", "content": "Let $D = \\{(x_i, Y_i)\\}_{i=1}^N$ represent a benign training set with N images, where $x_i \\in X$ is the i-th image and $Y_i \\in Y = \\{1,...,K\\}$ is its corresponding label, with K being the number of classes. If the goal of adversaries is to poison object recognition systems, they generate a poisoned dataset $D_p$ to train the attacked model using either a standard loss function or an adversary-specified one. Specifically, $D_p$ is composed of a modified subset of D, denoted as $D_m$, and a benign subset $D_b$, such that $D_p = D_m \\cup D_b$, where $D_b \\subset D$. The modified subset $D_m$ is defined as $D_m = \\{(x', y') | x' = G_x(x), y' = G_y(y), (x,y) \\in D_s\\}$, where $\\alpha = \\frac{D_s}{D}$ is the poisoning rate, $D_s$ is the subset of D selected to be modified, and $G_x, G_y$ are the adversary-specified generators for poisoned images and labels, respectively. For example, in BadNets [17], $G_x(x) = (1 - \\beta) \\odot x + \\beta \\odot t$, where $\\beta \\in \\{0,1\\}^{C\\times W\\times H}$, where C, W and H are dimensions of input, t is the trigger pattern, and $\\odot$ denotes the element-wise product. If the goal of advesaries is to attack object"}, {"title": "B. Backdoor Image Detection Framework", "content": "The framework of our proposed method for detecting unseen backdoored images is illustrated in Figure 1, showcasing a novel and powerful approach to tackling the complex challenge of detecting unseen backdoored images. We leveraged prompt tuning of VLMs to transcend their general-purpose design to become specialized tools for detecting and classifying backdoored images with unparalleled precision and adaptability [22], [23], [24]. Figure 1(a) highlights the diverse permutations of backdoor attack images generated across the CIFAR-10 dataset, emphasizing the robustness of our method in handling a wide range of malicious image manipulations. Unlike traditional defenses focused on protecting against backdoored models, our goal is to directly detect backdoored images, combining training and test images into a unified dataset for this purpose.\nIn the training phase, depicted in Figure 1(b), we leverage the state-of-the-art architecture of CLIP, utilizing both a text encoder and an image encoder to project prompts and input images into a shared, semantically rich embedding space. Learnable soft prompts [P1, P2, P3] are prepended to the word embedding of the \"class\" label, which is either clean for normal images or backdoored for malicious ones. To speed up the convergence time, we initialize [P1,P2,P3] with the word embeddings of \"a photo of\", where $p_i \\in R^d$ and d is the dimension of the output of CLIP's word embedding E(\u00b7), and is equal to 512. This sequence, combining the learnable soft prompts and the word embedding of the \"class,\" is passed through the text encoder of CLIP and normalized, producing embedding vectors $T_1 = \\frac{f_t([P_1, P_2, P_3, E(\u2018clean\u2019)])}{||f_t([p_1, P_2, P_3, E(\u2018clean\u2019)])||}$ for clean images and $T_2 = \\frac{f_t([P_1, P_2, P_3, E(\u2018backdoored\u2019)])}{|| f_t ([P_1, P_2, P_3, E(\u2018backdoored\u2019)]) ||}$ for backdoored images. These embeddings capture the contextual nuances of the respective image classes.\nSimultaneously, the image encoder processes all clean and backdoored images to generate high-dimensional embeddings $(I_1, I_2,..., I_n)$, where $I_i = \\frac{f_I(x_i)}{||f_I(x_i)||}$ . To optimize the model, similarity scores $(I_j \\times T_1, I_j \\times T_2)$ are computed between the jth image embedding and text embeddings $T_1$ and $T_2$, and a cross-entropy loss function is employed to fine-tune the system. This comprehensive framework not only ensures precise detection of seen backdoored images but also paves the way for identifying unseen backdoor attacks with exceptional accuracy and adaptability.\nDuring inference, as shown in Fig. 1(c), the learned prefix embeddings $[P_1, P_2, P_3]$ are appended to the word embeddings of \"clean\" and \"backdoored\" and passed through the text encoder to generate the frozen text embeddings: $T_1 = f_t([P_1, P_2, P_3, E(\u2018clean\u2019)])$, and $T_2 = f_t([P_1, P_2, P_3, E(\u2018backdoored\u2019)])$. It is important to highlight that, although this process resembles the training phase, the prefix embeddings $p_1, p_2$, and $p_3$ are frozen during inference and remain unaltered. Meanwhile, the image encoder processes the input image $x_j$ to compute its corresponding embedding $I_j$. Finally, the similarity scores between $I_j$ and $T_1$, as well as $I_j$ and $T_2$, are calculated and compared: $Similarity(I_j, T_1), Similarity(I_j, T_2)$. These similarity scores determine whether $x_j$ is classified as clean or backdoored. This architecture provides robust detection of unseen"}, {"title": "III. EXPERIMENTS", "content": "Attack Models. We have selected six renowned backdoor attacks to evaluate our proposed architecture: Badnets Square (Badnets-SQ) [17], Badnets Pixels (Badnets-PX) [17], Trojan Square (Trojan-SQ) [6], Trojan Watermark (Trojan-WM) [6], 12-inv [26], and l0-inv [26]. These attacks cover a wide range of backdoor conditions, including universality, label specificity, and variations in backdoor shape, size, and location.\nDatasets. We conduct experiments using two datasets: CIFAR-10 [27] and GTSRB [20]. CIFAR-10 includes 50,000 training images and 10,000 test images across 10 classes. GTSRB consists of 39,209 training images and 12,630 test images of traffic signals, spanning 43 classes.\nExperiment Settings. It is important to note that unlike adversarial attacks and defense literature that work on the model, we are working on images solely to detect backdoored images. Therefore, we do not train any model (e.g. ResNet-18) for our evaluation. We use CLIP's ViT-B/32, the smallestarchitecture in the CLIP family, chosen for its efficiency in balancing performance and computational demands. We leverage the predefined training and testing splits from the previously mentioned datasets. Clean images are taken directly from the train and test split without modification, while backdoored images are generated by applying the predefined attack types to all clean images. To evaluate the performance of our proposed method in detecting unseen attacks, only five of six attack types are selected for training. To maintain balance, we randomly select an equal number of images from each attack type to match the total number of clean images. At inference, the model is tested on all clean test images and their backdoored version by the unseen attack, which is the attack type excluded during training. Furthermore, we set the scaling factor $\\alpha = 100$ and the Adam optimizer's learning rate for the learnable prefix embeddings to $10^{-5}$. Training is conducted over 10 epochs with a batch size of 128."}, {"title": "A. Experimental Results", "content": "To compare the performance of our methods in detecting unseen backdoored images, we train three widely used CNN architectures: Simple-CNN [19] consists of three convolutional layers and a fully connected, Deep-CNN [20] consists of six convolutional layers with a dropout layer and fully connected layer for classification, and ResNet-18 [21].\nTable I presents the evaluation results, showcasing the exceptional performance of our proposed method in classifying unseen backdoor attack images. For example, on the CIFAR-10 dataset, our method achieves detection accuracies exceeding 95% for Trojan-WM, Trojan-SQ, and 10-inv triggers, alongside an impressive 93.85% accuracy for 12-inv triggers. Furthermore, the results highlight a 25% increase for detecting Badnets-SQ triggers and over 4.5% on Badnets-PX. To further validate the robustness of our approach, we extend our experiments to the GTSRB dataset. While prior CNN-based methods show promise in detecting Trojan triggers, our proposed method exhibits a remarkable performance increase, achieving an accuracy improvement of approximately 11% and 8% on Trojan-WM and Trojan-SQ, respectively. Additionally, our method significantly enhances detection accuracy by 35% for unseen Badnets-SQ triggers on the GTSRB dataset. While our method performs well across most unseen attack types, the lower accuracy on Badnets-PX attacks highlights a limitation in detecting subtle, pixel-level triggers. The minimal changes may not significantly alter the global image features captured by the frozen visual encoder, making them more challenging to detect. Overall, these results validate the value of our approach in detecting unseen backdoor attacks across both datasets."}, {"title": "B. Cross-Generalization Experiment", "content": "Ensuring robust generalization across datasets is crucial for backdoored image detection, particularly when facing unseen triggers. To evaluate the strength of our proposed approach regardless of the dataset used during the training, we conduct experiments to train on one dataset (e.g. CIFAR-10) and test on the other (e.g. GTSRB), while ensuring that the model is still trained on seen triggers and tested on unseen triggers. Table II illustrates the impressive results of these experiments. For instance, in the initial tests (CIFAR-10 \u2192 GTSRB), our model achieves an average accuracy of 77.54% on unseen Trojan triggers. The model retains robust performance across unseen 12-inv and l0-inv and triggers, achieving accuracies of 78.81% and 73.68%, respectively.\nWhen reversing the train and test sets (GTSRB \u2192 CIFAR-10) Trojan triggers achieve a higher average accuracy of 81.01%. In this scenario, the model once again performs well in identifying unseen 12-inv and l0-inv and triggers. Interestingly, the model detects 62.84% of unseen Badnets-PX triggers, which outperforms the performance when training directly on CIFAR-10. This improvement likely occurs due to the greater diversity and visual complexity of the GTSRB dataset (i.e. 43 classes) compared to less diverse CIFAR-10 dataset (i.e. 10 classes). This diversity appears to enable the model to learn more generalized representations, improving its ability to detect subtle pixel-level triggers like Badnets-PX."}, {"title": "C. Visual Analysis", "content": "To demonstrate the separation between clean and adversarial embeddings, we present t-SNE visualizations [28] of the test image and text embeddings within the embedding space for unseen backdoor triggers Trojan-WM and Badnets-PX, shown in Figure 2. These attacks were selected for illustration due to their contrasting detection performances: Trojan-WM achieves over 96% accuracy, while Badnets-PX achieves around 59%. For Trojan-WM, the correct text embeddings are closely aligned with their corresponding image embedding clusters, helping in create a distinct separation between clean and backdoor images. In contrast, the embedding space for Badnets-PX reveals a less distinct clustering pattern. While some separation occurs between the text embeddings, there is a significant overlap in image embeddings. This misalignment makes it more challenging to distinguish the unseen Backdoor-PX images from clean images during inference."}, {"title": "D. Ablation on Fixed Prefix", "content": "While our approach is built on leveraging a learnable prefix to adapt the textual representation in detecting unseen backdoored images, it is important to examine the effect on the model if the prefix remains static. We compare the results when using a fixed prompt \"a photo of\" \u2013 with the performance using our learned prefix, shown in Table III. When the prompt remains static, the model relies heavily on the understanding of the fixed prefix, providing no additional semantic context that highlights backdoored features. Furthermore, the base model is applying only pre-trained knowledge without any fine-tuning, leading to a lack of generalization to previously unseen images of backdoored images. However, the learnable prefix helps guide the model's attention by enabling the prompt to dynamically adapt to associated backdoor patterns. This helps align visual and textual embeddings in the multimodal embedding space, making it more effective at detecting unseen backdoor triggers."}, {"title": "IV. CONCLUSION", "content": "Defending object recognition systems against adversarial attacks has traditionally centered on reactive strategies like cleansing backdoored models or adversarially training them. In this groundbreaking work, we introduced a paradigm shift: a proactive method for detecting unseen backdoored (poisoned) images before they can infiltrate object recognition systems. Our approach serves dual purposes\u2014vetting training datasets to ensure integrity and safeguarding inference by blocking adversarial images before they reach the model. We achieved this by harnessing the unparalleled generalization capabilities of vision-language models like CLIP, leveraging prompt tuning to exploit their training on vast and diverse datasets. Extensive experiments across six distinct types of unseen attacks demonstrate the robustness and effectiveness of our approach, setting a new benchmark for proactive defense mechanisms. While this pioneering work represents the first step toward detecting backdoored images, future research must delve deeper into improving detection of pixel-based attacks, where subtle, localized triggers present a formidable challenge. This study paves the way for a new era in securing object recognition systems against adversarial threats."}]}