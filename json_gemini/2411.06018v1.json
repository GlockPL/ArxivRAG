{"title": "A Picture is Worth A Thousand Numbers:\nEnabling LLMs Reason about Time Series via Visualization", "authors": ["Haoxin Liu", "Chenghao Liu", "B. Aditya Prakash"], "abstract": "Large language models (LLMs), with demon-\nstrated reasoning abilities across multiple do-\nmains, are largely underexplored for time-\nseries reasoning (TsR), which is ubiquitous\nin the real world. In this work, we propose\nTimerBed, the first comprehensive testbed for\nevaluating LLMS' TsR performance. Specif-\nically, TimerBed includes stratified reasoning\npatterns with real-world tasks, comprehensive\ncombinations of LLMs and reasoning strate-\ngies, and various supervised models as com-\nparison anchors. We perform extensive experi-\nments with TimerBed, test multiple current be-\nliefs, and verify the initial failures of LLMs\nin TsR, evidenced by the ineffectiveness of\nzero shot (ZST) and performance degradation\nof few shot in-context learning (ICL). Further,\nwe identify one possible root cause: the nu-\nmerical modeling of data. To address this, we\npropose a prompt-based solution VL-Time, us-\ning visualization-modeled data and language-\nguided reasoning. Experimental results demon-\nstrate that VL-Time enables multimodal LLMs\nto be non-trivial ZST and powerful ICL rea-\nsoners for time series, achieving about 140%\naverage performance improvement and 99%\naverage token costs reduction\u00b9.", "sections": [{"title": "1 Introduction", "content": "With the recent surge in large language models\n(LLMs), their reasoning abilities have exhibited\npowerful performance with unique interpretability\nacross multiple domains, including logic, mathe-\nmatics, and symbolic reasoning (Wei et al., 2022;\nKojima et al., 2022; Achiam et al., 2023; Dubey\net al., 2024; Yang et al., 2024). As a type of data\nwidely present in the real world, time series records\nthe dynamic evolution of data over time. Time-\nseries reasoning (TsR) (Merrill et al., 2024) has\nextensive real-world applications, such as patho-\nlogical diagnosis (Dingwell and Cusumano, 2000),\nmarine biological monitoring (Baumgartner and\nMussoline, 2011), and human activity recogni-\ntion (Yang et al., 2015). Therefore, in this paper,\nwe aim to explore the question: \"Can LLMs han-\ndle time-series reasoning? If not, how to enable\nthem?\"\nHowever, this question remains largely underex-\nplored. The first key obstacle is \"How to effec-\ntively evaluate LLMs' TsR abilities?\". Existing\nworks face limitations at each component:\n(1) Task and Data. (a) Unstructured task de-\nsign: Existing TsR evaluations (Merrill et al., 2024;\nChow et al., 2024) simply mix multiple tasks with-\nout any taxonomy for reasoning patterns, making\nit challenging to understand LLMs' specific TsR\nlimitations. (b) Synthetic data. Due to the chal-\nlenges of curating suitable datasets, existing TsR\nworks (Merrill et al., 2024) use synthetic data, fail-\ning to effectively reflect real-world validity.\n(2) LLMs and Reasoning Strategies: (a) Out-\ndated LLMs: Evaluations (Merrill et al., 2024), still\nusing Llama of March 2023, have limited reference\nvalue for current research due to the rapid develop-\nment of LLMs. (b) Only Zero Shot: Current TsR\nevaluations (Merrill et al., 2024; Chow et al., 2024)\nonly adopt the zero-shot setting to assess LLMs for\nTsR, which fails to reveal their actual effectiveness.\n(3) Comparison Anchors: Existing TsR works\neither lack comparison anchors (Chow et al., 2024),\nonly comparing different LLMs, thus failing to\nquantify success, or adopt human-level perfor-\nmance as a reference (Merrill et al., 2024), which\nis costly and difficult to scale to other datasets.\nTo address these issues, we propose TimerBed,\nthe first comprehensive and progressive testBed for\nevaluating LLMs reasoning about Time series. Us-\ning TimerBed, we conduct extensive experiments\nto assess LLMS' TsR effectiveness, test several\ncurrent beliefs about LLMs, and discuss possible"}, {"title": "2 Testbed for LLMs Reasoning about\nTime Series", "content": "As discussed in Section 1, existing evaluation ef-\nforts have limitations at every component of bench-\nmarking. To address the question of \"How to eval-\nuate and understand the actual TsR capabilities\nof LLMs?\", we present TimerBed, the first-of-this-\nkind testbed for evaluating LLM reasoning about\ntime series, as shown in Figure 1. We further detail\neach component of TimerBed by reasoning pat-\nterns with tasks, LLMs and reasoning strategies,\nand comparison anchors."}, {"title": "2.1 Stratified Reasoning Patterns with\nCurated Real-world Tasks", "content": "Inspired by the classic reasoning problem taxon-\nomy(Jaeger, 1994; Dechter, 2022), we stratify TsR\ntasks into three patterns of increasing difficulty:\nsimple deterministic reasoning, complex determin-\nistic reasoning, and probabilistic reasoning. Specif-\nically, \"Simple\" and \"Complex\" are defined based\non whether multiple factors need to be integrated\nfor reasoning. \"Deterministic\" and \"Probabilistic\"\nare based on whether the relationship between in-\nput and output is deterministic. We practically\ndistinguish them by the method of data annota-\ntion, i.e., whether labeled by human experts or\ndirectly derived from data. The former ensures a\ndefinite reasoning path from the input data to the\nanswer, whereas the latter involves potential insta-\nbility, such as the influence of unobserved variables.\nTo the best of our knowledge, this is the first strati-\nfied evaluation framework for LLMs' TsR capabil-\nity, supporting a progressive analysis.\nWe further aligned two real-world tasks with\neach pattern for an effective evaluation. Following\nexisting works on benchmarking LLMs' reason-\ning abilities(Kojima et al., 2022; Wei et al., 2022;\nSprague et al., 2024), we standardized the organi-\nzation of datasets in a classification task format.\nThese datasets cover multiple domains (biology,\nhealthcare, electricity, physics, and nature), various\ntask types (binary and multi-class classification,\nfine-grained and coarse-grained classification), and\nmultiple data types (univariate and multivariate,\nvarying size, precision, value ranges and sequence\nlengths), thereby enable a comprehensive evalua-\ntion."}, {"title": "2.2 Comprehensive Combination of LLMS\nand Reasoning Strategies", "content": "Considering the rapid development of LLMs,\nTimerBed explores a comprehensive evaluation\nby combining various types of LLMs and rea-\nsoning strategies. Specifically, TimerBed covers\nfour types of LLMs, including open-source uni-\nmodal models with the corresponding advanced\nLLMs: Qwen2.5-72B8 for open-source unimodal\nLLMs, Qwen2-VL-72B\u00ba for open-source multi-\nmodal LLMs, GPT-410 for closed-source unimodal\nmodels, and GPT-4011 for closed-source multi-\nmodal LLMs. The open-source or closed-source\nnature determines development methods, while\nunimodal or multimodal models affect their adap-\ntations to the underexplored time-series modal-\nity. Note that we consider multimodal LLMs be-\ncause they include audio data in pretraining, which\nis more similar to time-series data. For reason-\ning strategies, we consider three widely used ap-\nproaches: zero shot (ZST) (Kojima et al., 2022),\nzero-shot chain of thought (CoT) (Wei et al., 2022),\nand few-shot in-context learning (ICL) (Brown\net al., 2020)."}, {"title": "2.3 Anchors for Quantifying Success", "content": "Considering the differences in reasoning patterns\nand task difficulty, it is necessary to use anchors to\nquantify the success of LLMs. We include random\nguessing and nine supervised time-series models\nas basic and strong anchors, respectively. Specif-\ncally, the supervised time-series models cover both\ntime and frequency domain with three main ar-\nchitectures including transformer-based models of\ntime domain(Vaswani, 2017; Zhou et al., 2021; Wu\net al., 2021; Nie et al., 2023; Liu et al., 2024e) and\nfrequency domain(Zhou et al., 2022), convolution\nneural network-based models(Wu et al., 2023), and\nmulti-layer perceptron-based models(Zeng et al.,\n2023). Details for these time-series models are\nprovided in Appendix G."}, {"title": "3 Evaluations and Insights", "content": ""}, {"title": "3.1 Evaluation Setups", "content": "We model time-series data as numerical series\nto feed into LLMs, following existing works of\nprompting LLMs for time-series analysis (Xue and\nSalim, 2023; Gruver et al., 2024; Liu et al., 2024c;\nDong et al., 2024). We apply format instructions,\nincluding mandatory descriptions of tasks, data,\nand options, as well as optional instructions, such\nas the CoT instruction: \"Please solve this prob-\nlem step by step.\" For ICL strategies that require\ncontext, we randomly sampled examples from the\noriginal training set to construct the demo set and\nselected the number of demos per category from 1\nto 6 as a standard few-shot setting following Jiang\net al.. Details of the prompts are in Appendix E.\nFollowing existing LLM evaluation works (Wang\net al., 2023a; Sun et al., 2024; Guha et al., 2024;\nNiklaus et al., 2024), we employ a temperature set-\nting of 0 to ensure more precise outputs. Accuracy\nis used as the evaluation metric, where a higher\nvalue indicates better performance. We run each\nsetting three times and report the average. More\nsetup details are provided in Appendix F"}, {"title": "3.2 Results and Insights", "content": "We conduct evaluations with TimerBed with the\naforementioned setups to answer the question:\n\"How effective LLMs are for TsR under different\nreasoning patterns and strategies?\" We have the\nfollowing key observations:\nDifferent Reasoning Patterns: Usually Near-\nRandom Performance with ZST. We visualize\nZST results in Figure 2, normalized by random\nguessing's accuracy. Detailed original results are\nin Table 3. We observe that for all LLMs, their ZST\nperformances across different reasoning patterns\nare Ineffective, usually close to or sometimes even\nworse than random guessing. This finding contra-\ndicts the prevailing belief that LLMs are generally\nnon-trivial ZST reasoners (Kojima et al., 2022), es-\npecially considering that we have provided clear\nand informative instructions for these advanced\nLLMs.\nDifferent Reasoning Strategies: CoT > ZST >\nICL. We visualize the results under different rea-\nsoning strategies in Figure 3, normalized by the\nperformance of random guessing. Detailed original\nresults are in Table 3.\nAcross LLMs, we find that CoT consistently\noutperforms ZST for TsR, validating the rationality\nof our reasoning tasks, which is consistent with the\nprevailing belief that CoT is primarily useful for\nreasoning tasks(Sprague et al., 2024).\nHowever, the few-shot ICL strategy for TsR\noften results in performance degradation, which\ncontradicts the prevailing belief that LLMs' per-\nformance can generally benefit from few-shot\nICL(Brown et al., 2020).\nAdditionally, we observe that multimodal LLMs\ngenerally perform worse than unimodal LLMs,\nconsistent with the existing belief about potential\ndegradation of language capabilities in multimodal\nLLMs(Lu et al., 2024)."}, {"title": "4 Failure Analysis", "content": "Further, we attempt to answer \"What cause the\nfailures of LLMs for TsR?\". Focusing on the"}, {"title": "5 Proposed Solution", "content": "This section aims to answer the question: \"How to\naddress the original failure of LLMs in TsR?\" by\nfocusing on the two observed anomalies, i.e., inef-\nfectiveness of ZST and performance degradation\nwith ICL.\nHigh-Level Idea. Our high-level idea is to\nreplace existing numerical modeling with\nvisualization-based modeling of time-series data.\nThe rationale for why visualization is key to ad-\ndressing the feature extraction and context length\nissues is as follows:\n(1) Visualization empowers feature extraction.\nFirst, visualization naturally reveals pictorial fea-\ntures. Second, visualization can directly reflect\ntime-related features by indicating time stamps us-\ning axis indices. Additionally, visualization nat-\nurally aligns different variables and uses color\nand legends to annotate them, thus clearly denot-\ning cross-dimension features. Finally, frequency-\ndomain features can be effectively represented\nthrough frequency-domain visualization.\n(2) Visualization compresses token length. Com-\npared to numerical modeling, where token usage\nis proportional to the data precision and sequence\nlength, visualization serves as an effective compres-\nsion method. By adjusting the image size, visual-\nization can effectively control token count while\nlargely preserving information. For example, on\nthe RCW task, a time-series sample using numer-\nical modeling needs up to 60,000 tokens, while\nvisual modeling only requires 85 tokens, based on\nthe \"gpt-40-2024-08-06\" endpoint."}, {"title": "6 Experiments for Proposed Solution", "content": "This section aims to answer the question: \"How\nwell can VL-Time unlock multimodal LLMs' rea-\nsoning ability for time series?\""}, {"title": "6.1 Performance Comparison", "content": "We conduct experiments using TimerBed, follow-\ning the same experimental setup as in Section 3.2.\nWe use GPT-40 as the representative by default,\nwith other LLMs in Section 6.3. To avoid con-\nfusion, we use GPT-40 (numeric) to denote ex-\nisting numerical modeling solution evaluated in\nSection 3.2. We use the low resolution option of\nGPT-40 API. For supervised models, we retained\neach dataset's original train-test split or used an 8:2\nsplit of the original training set when the test set\nlacked available labels. As shown in Table 1, we\nmake the following observations:\nVL-Time enables multimodal LLMs as non-\ntrivial zero-shot time-series reasoners. First,\nVL-Time significantly improves GPT-40's perfor-\nmance by an average of 17% and up to 40%. Ad-\nditionally, VL-Time allows GPT-40 to surpass ran-\ndom guessing on all tasks and outperform part of su-\npervised models on 4/6 tasks. Compared to GPT-40\n(numeric)'s random-level performance, VL-Time\nenables a non-trivial improvement.\nVL-Time enables multimodal LLMs as powerful\nfew-shot in-context time-series reasoners. First,\nVL-Time boosts GPT-40's performance by an im-\npressive average of 140% and up to 433%. Be-\nsides, VL-Time enables GPT-40 to outperform all\nsupervised time-series models on all tasks with sim-\nple and complex deterministic reasoning patterns.\nMoreover, VL-Time even matches the performance\nof supervised models (surpassing parts of them)\non all tasks with probabilistic reasoning patterns.\nRecall that probabilistic reasoning demands more\ndata to capture uncertainty. However, VL-Time\nremarkably enables multimodal LLMs to achieve\nstrong performance with fewer than six examples\nper class, demonstrating unique data efficiency.\nCost Comparison. VL-Time introduces no ad-\nditional training cost, only requires running the\nplanning stage once per task, and maintains the\nsame output format as traditional prompts. The"}, {"title": "6.2 Ablation Studies", "content": "We choose the RCW and HAR tasks to conduct\nablation study of the proposed VL-Time. We apply\nthe zero-shot setting.\nEffectiveness of the Planning Stage. To validate\nthe effectiveness of the first stage, i.e., planning, we\nuse only the second stage, i.e., planning, referred\nto as VL-Time\\Planning. Due to the lack of a\ndecision between time- and frequency-domain vi-\nsualization, VL-Time\\Planning uniformly adopts\nthe more common time-domain visualization.\nAs shown in Figure 5, the planning stage proves\neffective. Firstly, VL-Time\\Planning performs"}, {"title": "6.3 Exploratory Studies", "content": "ICL capability with VL-Time. To deeply an-\nalyze how the ICL capability is unlocked by\nVL-Time, we conduct experiments by varying the\nnumber of examples in the context.\nAs shown in Figure 6, we have the following\nobservations: (1) VL-Time demonstrates high ICL\ndata efficiency by typically achieving strong perfor-\nmance with just 4 examples per class. (2) VL-Time\nimproves the ICL data capacity. Theoretically,\nVL-Time can accommodate at least 400 examples\nwithin a 128K context length, as shown in Table 5.\nEmpirically, we demonstrate that VL-Time allows\nmore than five examples per class with performance\nimprovements across datasets. In contrast, numeri-\ncal modeling highly limits the number of examples\nand experiences performance degradation, as dis-\ncussed in Section 3.2 and 4.\nMultimodal LLMs Selection within VL-Time.\nWe analyze the impact of LLM selection to answer\nthree questions: (1) Is VL-Time effective across\ndifferent LLMs? (2) How does model size affect\nTsR performance? (3) What is the gap between\nopen-source and closed-source models?\nTo answer these questions, we introduce two\nadditional multimodal LLMs: GPT-40-mini12 for\nQuestion (2) and Qwen2-VL-72B for Question (3).\nAs shown in Figure 7a and 7b, we make the\nfollowing observations: (1) VL-Time is effective\nacross different multimodal LLMs. (2) LLM's TsR\ncapability relies on model size. (3) State-of-the\nart open-source multimodal LLMs still lag behind\nclosed-source models on TsR tasks."}, {"title": "7 Conclusion", "content": "In this paper, we explore the area of LLMs reason-\ning about time series. We present the first compre-\nhensive evaluation suite, conduct extensive evalua-\ntions, provide insights and verify the initial failures.\nWe identify the traditional numerical modeling of\ntime-series data as one possible key issue and fur-\nther propose a prompt-based solution via visualiza-\ntion modeling. Experiments show that the proposed\nsolution enables multimodal LLMs as non-trivial\nzero-shot and powerful in-context-learning reason-\ners about time series, and significantly reduces to-\nken costs."}, {"title": "Appendix", "content": ""}, {"title": "A Additional Related Works", "content": "First, we introduce related works on prompting\nLLMs for time series analysis. These works (Xue\nand Salim, 2023; Gruver et al., 2024; Liu et al.,\n2024c; Dong et al., 2024) mainly model time series\nnumerically as language inputs to LLMs. This ap-\nproach is somewhat limited by the fact that LLMs\nat that time are mainly unimodal. With the re-\ncent rapid development of multimodal LLMs (Yin\net al., 2023; Zhang et al., 2024) and multimodal\ntime-series analysis (Liu et al., 2024b), our pro-\nposed solution aims to demonstrate that visualiza-\ntion modeling is the key for LLMs reasoning about\ntime series.\nNext, we discuss differences between related\nworks on LLMs reasoning about time series. Re-\ncently, Merrill et al. pointed out that LLMs still\nstruggle with zero-shot reasoning about time se-\nries. Our concurrent work (Chow et al., 2024) also\nconfirms this viewpoint. From an evaluation per-\nspective, our contribution lies on two levels: (1)\nwe develop the first-of-this-kind testbed to address\ntheir evaluation limitations including tasks and data,\nLLMs and reasoning strategies, and comparison an-\nchors, as detailed in Section 2; and (2) we offer\ndeeper analysis and provide insights. Specifically,\nwe make more comprehensive and progressive eval-\nuations, test multiple existing beliefs about LLMs\nand verify initial failures of LLMs reasoning about\ntime series, as detailed in Section 3.\nBeyond evaluations, we propose solutions to re-\nsolve these failures and enable LLMs as non-trival\nzero-shot and powerful in-context reasoners about\ntime series. We further point out current limitations\nand future research directions. Specifically, LLMs\nstill lag behind supervised models in probabilistic\nreasoning, detailed in Table 1; both the effective-\nness and efficiency of LLMs reasoning time series\ncan benefit from visualization-based modeling."}, {"title": "B Limitations", "content": "According to the three levels of causality defined\nby Pearl and Mackenzie, this paper focuses on rea-\nsoning problems at the basic \"Correlation\" level.\nFor reasoning problems at the further \"Interven-\ntion\" and \"Counterfactual\" levels, we look forward\nto future works, especially in addressing the lack\nof suitable datasets.\nBesides, all reasoning tasks in this work are for-"}, {"title": "C Detailed Evaluation Results", "content": "Detailed Evaluation Results are provided in Ta-\nble 3."}, {"title": "D Details of Datasets", "content": "Satistics of original dataset are provided in Table 2."}, {"title": "E Details of Prompts", "content": ""}, {"title": "E.1 Prompt used for Zero-Shot Setting", "content": "prompt =\nprompt += f\"\"\"{TimeSeries_Number}.\nGiven the time series data above,\nanswer the following question\nusing the specified format.\nQuestion: {task_description}\nChoices: {str(class_desprtion)}\nPlease respond with the following format:\n---BEGIN FORMAT TEMPLATE---\nAnswer Choice: [Your Answer Choice Here]\n---END FORMAT TEMPLATE---\nDo not deviate from the above format.\nRepeat the format template for the answer.\"\"\""}, {"title": "E.2 Prompt used for Zero-Shot\nChain-of-Thought Setting", "content": "prompt =\nprompt += f\"\"\"{TimeSeries_Number}.\nGiven the time series data above,\nanswer the following question\nusing the specified format.\nQuestion: {task_description}\nChoices: {str(class_desprtion)}\nPlease solve this problem step by step.\nPlease respond with the following format:\n---BEGIN FORMAT TEMPLATE---\nAnswer Choice: [Your Answer Choice Here]\n---END FORMAT TEMPLATE---\nDo not deviate from the above format.\nRepeat the format template for the answer.\"\"\""}, {"title": "E.3 Prompt used for Few-shot In-Context\nLearning Setting", "content": "prompt =\nfor demo in demo_examples:\nprompt += f\"\"\"{TimeSeries_Number}.\nGiven the time series data above,\nanswer the following question\nusing the specified format.\nQuestion: {task_description}\nChoices: {str(class_desprtion)}\nAnswer Choice: {demo.answer}\n\"\"\"\nprompt += f\"\"\"{TimeSeries_Number}.\nGiven the time series data above,\nanswer the following question\nusing the specified format.\nQuestion: {task_description}\nChoices: {str(class_desprtion)}\nPlease solve this problem step by step.\nPlease respond with the following format:\n---BEGIN FORMAT TEMPLATE---\nAnswer Choice: [Your Answer Choice Here]\n---END FORMAT TEMPLATE---\nDo not deviate from the above format.\nRepeat the format template for the answer.\"\"\""}, {"title": "E.4 Details of Task Description", "content": ""}, {"title": "E.4.1 RCW", "content": "Play the role of a marine biology expert: is there a\nright whale calls in the record?"}, {"title": "E.4.2 TEE", "content": "Based on the power density time series data and\nselect the transient electromagnetic event that best\nmatches. The FORTE satellite detects transient\nelectromagnetic events associated with lightning\nusing a suite of optical and radio-frequency (RF)\ninstruments. There are 7 event types. CG Positive\nInitial Return Stroke: A positive charge is lowered\nfrom a cloud to the ground. The characteristic\nfeature of this type of event in the power density\ntime series is a sharp turn-on of radiation, followed\nby a few hundreds of microseconds of noise; IR\nNegative Initial Return Stroke: A negative charge\nis lowered from a cloud to ground. The power\nwaveform slowly ramps up to a level known as\nan attachment point, where a large surge current\ncauses the VHF power to 'spike'. This attachment\nis followed by an exponentially shaped decline in\nthe waveform.; SR Subsequent Negative Return\nStroke: A negative charge is lowered from a cloud"}, {"title": "E.4.3 ECG", "content": "As a cardiologist, you are tasked with classifying a\npatient's heart condition based on single-lead ECG\nrecordings."}, {"title": "E.4.4 EMG", "content": "As an Electromyograms (EMG) analysis expert,\nyou are tasked with determining the type of the\nsubject based on the EMG record."}, {"title": "E.4.5 CTU", "content": "Play as a computer energy consumption analysis ex-\npert, please correctly determine whether this com-\nputer is a desktop or a laptop based on the 24-hour\npower consumption data."}, {"title": "E.4.6 HAR", "content": "As a human activity recognition expert, you are\ntasked with determining the type of activity per-\nformed by the subject based on the accelerometer\nrecord series along the x, y, and z axes over time."}, {"title": "F More Details of Setup", "content": "For the Qwen model, we utilize four NVIDIA A100\n80GB servers. We use the API service provided by\nOpenAI for GPT-40 and GPT-4. The specific end-\npower consumption data.I will extract the follow-\npower consumption data.I will extract the follow-\npattern of interest on this data is consistent. Theis \u201cgpt-4o-2024-08-06\", for\nGPT-4 is \"gpt-4-1106-preview\u201d, To prevent mod-\nels from abstaining (which is common in ICL set-\nting with numerical modeling but rarely for our\nVL-Time), we rerun the query until an answer is\nprovided. We follow the most recent evaluation\nwork on multimodal LLMs (Jiang et al., 2024) to\nperform sampling, aiming to control class balance\nand evaluation costs.\""}, {"title": "G Included Eight Supervised Time-Series\nModels", "content": "\u2022 Transformer(Vaswani, 2017), a classic\nsequence-to-sequence model basing on multi-\nhead attention mechanism.\n\u2022 Informer(Zhou et al., 2021), an advanced\nTransformer designed to tackle long-term\nmodeling problem with sparse attention layers\nand self-attention distilling.\n\u2022 Autoformer(Wu et al., 2021), a Transformer-\nbased model that keeps encoder-decoder struc-\nture but alters attention computations by auto-\ncorrelation mechanism in order to benefit\nlong-term modeling.\n\u2022 FEDformer(Zhou et al., 2022), a Transformer\nthat explicitly use Fourier decomposition re-\nsults to enhance long-term modeling ability.\n\u2022 PatchTST(Nie et al., 2023), a transformer-\nbased models using segmentation of time se-\nries and channel-independence design.\n\u2022 iTransformer(Liu et al., 2024e), a inverted\nTransformer that tokenizes multivariate time-\nseries upon each timestamps.\n\u2022 DLinear(Zeng et al., 2023), a linear model\nthat performs modeling by a direct regression\nupon historical time series with a one-layer\nlinear model.\n\u2022 TimesNet(Wu et al., 2023), a temporal 2D-\nvariation modeling for general yime series\nalysis."}, {"title": "H Verbalized Feature Extraction Plans", "content": ""}, {"title": "H.1 RCW", "content": ""}, {"title": "H.1.1\nPrompt", "content": "Given the task: Play the role of a marine biology\nexpert: is there a right whale calls in the record?\nPlease tell me what features or patterns do you plan\nto extract from input data to correctly complete this\ntask?"}, {"title": "H.1.2\nGPT-4", "content": "To determine if there are right whale calls in a\nrecording while playing the role of a marine biol-\nogy expert, several key features and patterns can\nbe extracted from the audio data. Here's a plan to\napproach this task:\nFrequency Analysis: Right whale calls typically\nfall within specific frequency ranges, often between\n50 Hz to 500 Hz. Analyzing the frequency content\nof the audio can help identify segments where these\ncalls might occur.\nTemporal Patterns: Right whale calls have dis-\ntinctive durations and temporal patterns. For exam-\nple, up-calls usually last about one second and rise\nin frequency. Identifying these patterns can help\ndifferentiate right whale calls from other sounds.\nEnergy and Power: Examining the energy or\npower of the audio signal over time can help to\ndetect calls, as vocalizations tend to have higher\nenergy levels compared to background noise.\nSpectral Characteristics: Spectrogram analysis\ncan be very useful. Right whale calls exhibit spe-\ncific shapes on spectrograms, such as the curved\nshape of up-calls. Machine learning models can be\ntrained to recognize these shapes.\nHarmonic Patterns: Whale calls often consist\nof harmonic series where multiple harmonics of\na fundamental frequency are present. Detecting\nthese can help confirm the presence of whale calls.\nTemporal Consistency: Repeated patterns over\ntime could indicate the presence of biological\nsources (like whales) as opposed to random noise\nor other transient sounds.\nMachine Learning Classification: After extract-\ning features, employing a classifier such as a Sup-\nport Vector Machine (SVM) or a neural network\nthat has been trained on labeled examples of right\nwhale calls and other sounds can automate the de-\ntection process.\nAudio Quality Assessment: Ensuring the record-\ning's quality and clarity can aid in more accurate\nanalysis, considering factors like signal-to-noise\nratio and possible interference.\nBy combining these methods, the task of de-\ntecting right whale calls in recordings can be ap-\nproached systematically and effectively."}, {"title": "H.1.3\nGPT-40", "content": "To detect right whale calls in a sound recording,\nthe following features and patterns are essential\nfor accurate analysis. I will approach this task\nsystematically, as it typically involves bioacoustic\nsignal processing. Below are the key features and\npatterns I would focus on:\n1. Spectral Features (Frequency Domain) Spec-\ntrogram Analysis: Visual representation of frequen-\ncies over time to identify characteristic call patterns.\nPeak Frequency: Right whale calls often occur be-"}, {"title": "H.1.4\nQwen2.5-72B", "content": "Certainly! To determine whether a recording con-\ntains right whale calls, we need to analyze the"}, {"title": "H.1.5\nQwen2-VL-72B", "content": "To determine if there are right whale calls in a given\naudio record", "data": "nSpectrogram: A spectrogram is a visual represen-\ntation of the spectrum of frequencies in a sound as\nthey vary with time. By analyzing the spectrogram\nof the audio record", "sounds.\nPitch": "Right whale calls have a distinct pitch that is\ntypically between 20 and 30 Hz. By analyzing the\npitch of the audio record", "Duration": "nRight whale calls typically last between 1 and 10\nseconds. By analyzing the duration of the sounds\nin the audio record", "Intensity": "Right whale calls\nare typically loud and can be heard over long dis-\ntances. By analyzing the intensity of the sounds in\nthe audio record, I can determine if there are any\nsounds that are loud enough to be right whale"}]}