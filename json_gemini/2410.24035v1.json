{"title": "State- and context-dependent robotic manipulation and grasping via uncertainty-aware imitation learning", "authors": ["Tim R. Winter", "Ashok M. Sundaram", "Werner Friedl", "Maximo A. Roa", "Freek Stulp", "Jo\u00e3o Silv\u00e9rio"], "abstract": "Generating context-adaptive manipulation and grasping actions is a challenging problem in robotics. Classical planning and control algorithms tend to be inflexible with regard to parameterization by external variables such as object shapes. In contrast, Learning from Demonstration (LfD) approaches, due to their nature as function approximators, allow for introducing external variables to modulate policies in response to the environment. In this paper, we utilize this property by introducing an LfD approach to acquire context-dependent grasping and manipulation strategies. We treat the problem as a kernel-based function approximation, where the kernel inputs include generic context variables describing task-dependent parameters such as the object shape. We build on existing work on policy fusion with uncertainty quantification to propose a state-dependent approach that automatically returns to demonstrations, avoiding unpredictable behavior while smoothly adapting to context changes. The approach is evaluated against the LASA handwriting dataset and on a real 7-DoF robot in two scenarios: adaptation to slippage while grasping and manipulating a deformable food item.", "sections": [{"title": "I. INTRODUCTION", "content": "Learning from demonstration (LfD), also referred to as imitation learning, has demonstrated promising results in the field of robotic manipulation in recent years [1]. The possibility to learn from small amounts of data and the often minimal expert knowledge required, enable direct training on the real system. Despite the historical focus on adaptability and generalization [2]\u2013[6], recent approaches have shown the potential to adapt to external task parameters that describe a specific context, environment or robot state, further increasing LfD's generalization capabilities [7], [8]. This becomes particularly relevant in the case of deformable object manipulation (DOM), where the grasp strategy often depends on the variable object shape. Furthermore, recent approaches [9]-[12] propose state-dependent implementations of LfD, in which new robot commands are determined based on the current pose of the robot in order to achieve reactivity and adaptability. However, due to disturbances, different initial conditions or as a result of distribution shifts, deviations from the leaned trajectories can occur with these state-based approaches. To compensate for deviations and therefore guarantee appropriate robot behavior, [11] proposed to fuse the learned policy with additional policies that improve the stability and convergence while maintaining the approach flexible. Despite these promising developments, none of the above state-based references incorporate context information. The application of LfD has therefore yet to reach a significant breakthrough in context-adaptive manipulation. An in-depth review of related work can be found in Section II.\nWe take inspiration from previous achievements and propose a state-based imitation learning approach for context-adaptive manipulation. To achieve this, we use Kernelized Movement Primitives (KMP) [5] due to their ability to learn trajectories associated with high-dimensional inputs while preserving the probabilistic properties exhibited in the demonstrations to learn a joint distribution of the multidimensional robot state and external context variables (see Section III for an overview). Moreover, we make use of the epistemic uncertainty [13] to obtain a stabilizing policy that guides the robot back to the demonstrated trajectories and prevent undesired behavior. This combination allows the robot to smoothly adapt to context changes during runtime and therefore permits the consideration of changing context inputs directly into the learned policy in a robust way. Furthermore, we improve the convergence using a goal attractor policy and combine those three policies as a mixture of experts (MoE). Section IV introduces our approach. Using intuitive 2D scenarios and a real 7-DoF robot in two different manipulation tasks, in Section V we show that our approach is able to reproduce the demonstrated trajectories while adapting to the changing context, maintaining stability and converging on the goal. Finally, we discuss our approach in Section VI and a conclusion is drawn in Section VII."}, {"title": "II. RELATED WORK", "content": "Context-adaptive manipulation and grasping is crucial in tasks where the robot needs to adapt its motion to external task parameters, such as object shapes and configurations. In the field of deformable object manipulation this becomes particularly relevant, as the grasp strategy for an object is often influenced by its changeable shape. Most recent data-driven approaches focus on solving this highly complex task using machine learning techniques [14]. Several methods tackle this problem using neural networks [15]\u2013[23], reinforcement learning (RL) [24]\u2013[27] or a combination of both [28], [29]. Due to the large amounts of data required, these methods often rely on physic simulators in order to generate the necessary data or time-consuming annotation.\nIn contrast, the application of LfD approaches as function approximators in the domain of robotic manipulation has gained popularity in recent years as they have demonstrated to be an alternative to classical control and planning algorithms as well as supervised deep learning methods due to the small amount of data required and the adaptability to new tasks. Thereby classical regression approaches as Gaussian Mixture Regression (GMR) [30] and Gaussian Process Regression (GPR) [31], that approximate the demonstrated trajectories using a set of Gaussian basis functions, are exploited as well as movement primitives (MP), which present the trajectories using superpositions of basis functions [2]-[12], [32]-[37]. Building on these approaches, several methods tackled the problem of distribution shifts. In [9] the authors constrain the outcome of GMM using stabilization criteria in order to ensure convergence and stable behavior, while [11], [12], [38] compensate for the deviations by fusion of the learned behavior with additional policies. Furthermore, several works tackle the problem of context aware imitation learning. In [4], [8], [32], [33] the authors focus on having different initial, intermediate or final robot poses as context, which are known before execution. On the other hand [7], [34], [35] consider more general cases in which the context variable can be any environmental property. Nevertheless, none of these works consider both continuously changing context during execution and state-driven policies, and therefore fail to provide context-aware, reactive behavior. As providing strong environmental priors (e.g., Cartesian frames [4]) can be challenging in domains like DOM, we adopt a more general approach by using generic context parameterizations."}, {"title": "III. BACKGROUND", "content": "In this section we present the methodological background required for the development of our approach (Section IV).\nA. Kernelized Movement Primitives (KMPs)\nA KMP [5] is initialized using GMR, to obtain the conditional probability distributions $\\xi_n|s_n \\sim \\mathcal{N}(\\mu_n, \\Sigma_n)$, n = 1,..., N, from a set of H demonstrated trajectories $\\{\\{s_{m,h},\\xi_{m,h}\\}_{m=1}^H\\}$, where $s_{m,h} \\in \\mathbb{R}^I$ specifies the input and $\\xi_{m,h} \\in \\mathbb{R}^O$ denotes the resulting output. Here, $s_n = [s_{1,n},..., s_{I,n}]$, used for the regression, is sampled from the Gaussian mixture model (GMM) with C components, which is derived by expectation maximization (EM). Moreover, I and O are the dimensions of input and output space. By minimizing the KL-divergence between the reference distributions $\\mathcal{N}(\\hat{\\mu}_n, \\hat{\\Sigma}_n)$ and the probability distribution of a parametric trajectory $\\xi(s)$, represented as a weighted superposition of basis functions, it is possible, using the kernel trick [31], to predict mean and covariance at new input data s* by\n$\\mu_{kmp} = k^*(K + \\lambda \\Sigma)^{-1}\\mu$, (1)\n$\\Sigma_{kmp} = (k^{**} - k^* (K + \\lambda \\Sigma)^{-1}k^{*T})$. (2)\nHere, the mean vector $\\mu = [\\mu_1^T \\cdots \\mu_N^T]^T$ and the covariance matrix $\\Sigma = \\text{blockdiag}(\\Sigma_1, \\Sigma_2, ..., \\Sigma_N)$ are obtained from the means and covariances computed with GMR. Moreover, $k^{**} = k(s^*, s^*)$ denotes the kernel matrix computed at s*, $k^* = [k(s^*,s_1) \\cdots k(s^*, s_N)]$ refers to the kernel matrix combining new input and $s_n$ and\n$K = \\begin{bmatrix} k(s_1, s_1) & \\cdots & k(s_1, s_N) \\\\ \\vdots & \\ddots & \\vdots \\\\ k(s_N, s_1) & \\cdots & k(s_N, s_N) \\end{bmatrix}$ (3)\ndescribes the kernel matrix evaluating the similarity of the demonstrated trajectories represented by $s_n$. Additionally, $k(s_i, s_j) = k(s_i, s_j)\\mathbb{I}_O$, with $\\mathbb{I}_O$ an O \u00d7 O identity matrix, where the kernel function k(si, sj) is chosen according to the characteristics of the data. In this work, the radial basis function (RBF) kernel, given by\n$k(s_i, s_j) = \\exp \\big( -\\frac{1}{2}(s_i-s_j)^T\\Lambda(s_i-s_j) \\big)$, (4)\nis used as we assume smooth, continuous trajectories. The parameters $\\Lambda = \\text{diag}(l)^{-2}$, with the kernel length vector $l \\in \\mathbb{R}^I$, and \u03bb denote the hyperparameters of the algorithm. The covariance prediction of KMP (2), includes both the epistemic and aleatoric uncertainties of the learned model [36]. In [37], it is shown that it can be re-written as a sum of aleatoric and epistemic terms (omitting for simplicity):\n$\\Sigma_{kmp} = k^{**} - k^*K^{\\oplus^{-1}}k^{*T} +k^* (K + K(\\lambda \\Sigma)^{-1}K)^{-1}k^{*T}$. (5)\n$\\Sigma_{al}$ $\\Sigma_{ep}$\nThe term $\\Sigma_{al}$ represents the aleatoric uncertainty encoding the covariance in the training data. In contrast, $\\Sigma_{ep}$ models the epistemic uncertainty similar to GPR [31] and can be written as $\\Sigma_{ep} = \\Sigma_{ep}\\mathbb{I}_O$ since it does not include correlation.\nB. Probabilistic Mixture of experts (MoE)\nThe combination of experts is a common technique in the domain of machine learning to model high-dimensional data that adheres to multiple low-dimensional constraints simultaneously. It allows to learn or formulate several simpler probability distributions that cover each a low-dimensional constraint and combine their respective outputs. The two most common methods are the Mixture of Experts (MOE) [39] and Product of Experts (PoE) [40]. Whereas with PoE, all conditions must be approximately satisfied to obtain the"}, {"title": "IV. STATE AND CONTEXT-DEPENDENT IMITATION LEARNING", "content": "In this work, we exploit the usage of KMP in a context- and state-based manner with the current end-effector pose x and context parameter $c \\in \\mathbb{R}^C$ as inputs s = [c x] and an end-effector velocity command $\\dot{x}$ as output, i.e. $\\xi = \\dot{x}$. Our aim with such a formulation is to be able to modulate the robot behavior in response to c, which can contain task parameters, environmental conditions or low-dimensional representations of the object, in runtime. In combination with the state input x, which introduces reactivity to the current state of the robot, we additionally aim to represent the robot motion in a time-independent way, which facilitates adaptation. However, since KMP is able to reproduce the demonstrated behavior only along the learned trajectories and close to them, deviations from the learned trajectory due to disturbances, different initial conditions or as a result of distribution shifts, can cause undesirable or even dangerous behavior when applied on the robot. To prevent such behavior, we take inspiration from [11] and combine KMP with supplementary policies that improve the stability and convergence of the approach. Doing so, allows us to preserve the flexibility of KMP as we do not constrain the output of the LfD approach like [9]. We formulate independent policies for stabilization and convergence in form of Gaussian distributions which we combine using MoE. We focus in the subsequent work on the MoE mean $\\hat{\\mu}$ as we use single actions u to control the robot. Our formulation consists of three distinct types of policies:\n\u2022 LfD policy ($\\pi_{kmp}, \\mu_{kmp}$): a state-dependent policy that encodes demonstrated control actions (in our experiments Cartesian velocities).\n\u2022 Stabilizing policy ($\\pi_{sp}, \\mu_{sp}$): an epistemic-uncertainty- dependent policy that brings the robot to the regions of the state space where demonstrations were provided.\n\u2022 Goal attractor policy ($\\pi_g, \\mu_g$): a linear state feedback policy that ensures the robot motions end at one of the demonstrated final poses.\nThis results in the following MoE mean (6):\n$\\mu = \\pi_{kmp}\\mu_{kmp} + \\pi_{sp}\\mu_{sp} + \\pi_{g}\\mu_{g}$. (7)\nIn the following sections we describe the formulation of the weights and means in the model.\nA. MoE mixing coefficient design\nWe here outline the design choices for the policy coeffi- cients in the mixture (7).\n1) Stabilizing policy coefficients $\\pi_{sp}$: We assign a high priority in our framework to staying close to demonstrations, avoiding dangerous, unseen behaviors. Thus, we assume that the stabilizing policy is active all the time, assigning a constant value for its mixing coefficient given by $\\pi_{sp}$. Despite having a re-scaling effect in the velocity learned by KMP, this ensures predictable movements and smoother transitions between stabilizing policy and KMP. The effect of the stabilizing policy in isolation can be seen in Fig. 2b.\n2) Goal attractor policy coefficients $\\pi_{g}$: To en- sure the local activation of the goal attractor pol- icy as well as a smooth transition between itself and the LfD policy, we use the maximum kernel activa- tion $k_{max} = \\text{max}(k(s^*, s_{1,M_1}), \\ldots, k(s^*, s_{H,M_H}))$ calcu- lated based on the current input s* and demonstrated goal poses $s_{h, M_h}$. We thus define $\\pi_{g} = (1 - \\pi_{sp})k_{max}$. The behavior of the goal attractor policy is shown in Fig. 2c.\n3) LfD policy coefficients $\\pi_{kmp}$: Taking into account $\\sum_{i=1}^P \\pi_i = 1$ the above results in a mixing coefficient for the LfD policy of $\\pi_{kmp} = (1 - \\pi_{sp})(1 - k_{max})$. The combined output $\\hat{\\mu}$ of all policies, applied to the 2D example, can be seen in Fig. 2d. Here, each policy acts as an expert in its respective area, while the combined policy unifies the advantages of the individual policies."}, {"title": "B. Stabilizing policy distribution", "content": "Since KMP is only guaranteed to reproduce the demon- strated behavior along the learned trajectories and close to them, we introduce a policy that automatically guides the robot back to the demonstrations, in case of deviations. To achieve that, we make use of the KMP epistemic uncertainty (5), which is high in underrepresented regions and low on the demonstrations, and, similarly to [12], formulate a policy that generates actions in the direction of the gradient descent. Therefore, we compute the gradient of $\\Sigma_{ep}$ with respect to the current input s*, assuming the RBF kernel (4):\n$\\nabla \\Sigma_{ep} = -2\\nabla k^*K^{-1}k^{*T}$, (8)\nwhere $\\nabla k^* = [\\nabla k(s^*, s_1) \\cdots \\nabla k(s^*, s_N)]$ and $\\nabla k(s^*, s_n) = k(s^*,s_n)\\Lambda(s^* - s_n)$ is the gradient of the RBF kernel. For notation simplicity, we subse- quently use the scalar expression $\\Sigma_{ep}$ and its derivative $\\nabla \\Sigma_{ep} = [\\nabla_{s_1}\\Sigma_{ep} \\cdots \\nabla_{s_I}\\Sigma_{ep}]$, discarding the identity ma- trix $\\mathbb{I}_O$, which allows us to treat the different dimen- sions in vector form. Note that for an input of the form s = [c x]', the gradient contains two distinct terms, $\\nabla \\Sigma_{ep} = [(\\nabla_{c}\\Sigma_{ep})^T (\\nabla_{x}\\Sigma_{ep})^T]^T$. In this work we assume to have no control of the object shape and thus focus on the state-dependent term $\\nabla_{x}\\Sigma_{ep}$ to design the stabilizing policy. In order to be able to regulate the magnitude of the velocities when far from the data, we normalize the gradient $\\nabla_{x}\\Sigma_{ep}$ when its norm or the uncertainty are above pre- defined thresholds $\\gamma_{\\Sigma}, \\gamma_{\\nu}$:\n$\\nabla_\\Sigma = \\begin{cases} \\frac{\\nabla_{x}\\Sigma_{ep}}{\\|\\nabla_{x}\\Sigma_{ep}\\|} & \\text{if } \\Sigma_{ep} < \\gamma_{\\Sigma} \\text{ and } \\|\\nabla_{x}\\Sigma_{ep}\\| < \\gamma_{\\nu}, \\\\ \\text{otherwise}. \\end{cases}$ (9)\nNote that the gradients vanish near the data, an intended effect as the LfD policy is expected to take over. We further introduce the hyperparameter $K_{sp}$ that limits the velocity to an upper bound, resulting in the following mean velocity for the stabilizing policy:\n$\\mu_{sp} = - K_{sp}\\nabla_\\Sigma(\\Delta t)^{-1}$, (10)\nwhere $\\Delta t^{-1}$ is the control rate. Equation (10) guides the robot towards regions of the state space with low epistemic uncer- tainty. This effect can be observed on the vector fields in"}, {"title": "C. Goal attractor policy", "content": "Since KMP is not guaranteed to converge to the final demonstrated poses, and the stabilizing policy only returns the robot to low uncertainty areas, we introduce a policy that improves the convergence properties of our approach. For this, we use a linear state feedback controller with gain $K_g$ that is activated near the demonstrated final poses according to the mixing coefficients defined in Section IV-A,\n$\\mu_{g} = K_g(x_g - x^*)(\\Delta t)^{-1}$. (11)\nHere, $x_g$ describes the demonstrated final pose with the highest kernel activation $k_{max}$ for the current pose $x^*$. The vector field resulting from (11) can be visualized in Fig. 2c for the considered illustrative example."}, {"title": "V. EVALUATION", "content": "We evaluate our approach in four experiments of increas- ing complexity. First, we consider two 2D experiments (V-A) using 1) the LASA handwriting dataset [9] and 2) a letter dataset of our own including context variables. We then use a real 7-DoF robot in two manipulating scenarios, namely adaptation to slippage (V-B) and food handling (V-C). The hyperparameter values used in the experiments are listed in Table I. Due to the different scales of input dimensions, we define different kernel lengths, $l = [l_c l_p l_o]^T$. In the table, only the scalar values of the kernel lengths that are the same within the sub-dimensions of l are listed.\nA. Handwriting datasets experiments\nIn the first experiment we quantitatively evaluate our approach on 30 human handwriting motions from the LASA handwriting dataset [9]. We train a KMP using s = X, $\\xi = \\dot{x}$ (I = 2, O = 2). The learned trajectories are reproduced by iterative computing $\\dot{x}$ both with our approach and with various sub-combinations of the individual policies and estimating the next position as $x_{t+1} = x_t+\\Delta t\\dot{x}_t$ in a 20 Hz loop. We perform two sets of trials, one in which $x_0$ is placed in the initial position of each demonstration, and one in which 10 starting positions $x_0$ are randomly generated across the input space. The loop is exited when the distance to the goal pose is less than 0.01 (success) or 500 iterations are reached (failure). The performance is measured using the success rate, average iterations and Root Mean Square (RMS) error over the whole dataset. For RMS, the smallest distance between each visited state and the demonstrations is considered. Table II shows the results of the experiment in the two upper sections.\nB. Re-grasp experiment\nIn V-A we have shown the adaptation to context values that remain static during execution. However, thanks to the state-based implementation, our approach is also able to react to changing context values during the execution. We show this in a re-grasp experiment, where the object slips from the robot hand, using a 7-DoF KUKA LWR robotic arm equipped with a variable stiffness parallel gripper. For the object we use a 3D-printed cylinder, connected to a servomotor via a cable attached to the curved side. This allows pulling the object down with an adjustable force, creating slippage and resulting in the loss of the object. For the demonstrations, the cylinder is placed on the curved side and no force is applied to the cable. We record two trajectories via kinesthetic teaching. In one, we move the robot from the starting position to the cylinder and close the gripper at the grasping point. In the other, we move the robot with the grasped object to the end pose.\nWe make use of the incorporated tactile sensors in both fingertips to derive a binary context value c indicating contact with the object (c = 1 if grasped, c = 0 otherwise). This results in an input vector s = [c p r T o T ]', consisting of context, robot position and orientation as a rotation vector, thus C = 1, I = 7. Based on the learned joint distribution provided by the demonstrations and the given current robot pose, we predict the corresponding end-effector velocity $\\dot{r} = [\\dot{p} \\dot{o}]^T$ and the gripper opening width q in a 20 Hz loop, hence $\\xi = [\\dot{r} q]$ and O = 7. The end-effector velocity is used as a control command for the robot and the gripper opening width is applied to the gripper continuously. During execution, we apply a force of 8-9 N to the cable attached to the cylinder, making it slip out of the gripper during each lifting attempt. As soon as the object slips, the context value switches from 1 to 0, modifying the velocity field generated by our approach to match the one from the approach-and-grasp demonstration. We lower the force in the end to visualize the convergence of the approach.\nC. Fish layout experiment\nIn a final, more complex, experiment we show the ability to adapt to continuous context changes. We use the same robot as in V-B with different gripper fingertips, in order to show the adaptation to different fish placing strategies. We demonstrate the placing of a deformable fish fillet on a tray via kinesthetic teaching. Depending on the grasp, the placing should be performed differently, by approaching the tray from the side where the fillet hangs. We provide three demonstrations of placing the fish from the left and three from the right. We also use a shorter, less deformable, fillet than in and demonstrate twice how to place it from the top. Finally, we demonstrate the movement back to the initial pose when no fish is in the scene.\nIn order to adapt to the fish geometry, we capture its shape using an Azure Kinect camera and compute the skeleton of the fillet using color segmentation from OpenCV [41] and medial axis transformation. From that, we derive the grasping point by checking for gaps in the segmented skeleton. As context values we use a 2D representation of the fish con- sisting of the skeleton height, $C_{sh}$, and the Euclidean distance between grasping point and lowest point, only considering the left side of the grasping point for both values, $C_{dist}$. We thus have c = [$C_{sh}$ $C_{dist}$]', C = 2. Both values are relative to the total pixel height and width. Due to the continuous tracking and the changing distance and angle to the camera, the context value changes continuously. If there are no gaps in the segmented skeleton, we define c = [-1 -1] which applies for both when no fish is in the scene and when it is on the tray. We further have s = [c p r T o T ]', I = 8, and the output as in V-B."}, {"title": "VI. DISCUSSION", "content": "The results in V-A support the combination of multiple policies, specialized in different types of behavior, when learning skills with state-based formulations, particularly when the input to the policy goes beyond the robot state. The experiments on the LASA dataset validate our assumption that the need for additional policies increases when the system state deviates from the demonstrations and that our chosen expert policies provide the required functionality to improve performance. This is further exemplified in the experiment with context inputs, where the combination of policies proved critical to achieving a high success rate. Note that the model hyperparameters, chosen empirically, were the same for all letters in the LASA dataset. Using more sophisticated hyperparameter optimization routines is likely to further improve performance. Furthermore, the EM step required to initialize each KMP, despite being run for each new letter, is a light-weight one compared to approaches such as [9], which require the fulfillment of formal stability guarantees. Such requirements also hinder the extension to formulations like ours, i.e. with additional inputs, as, in principle, parameters that guarantee stability would need to be estimated for each new condition.\nThe experiments in V-B, V-C suggest that the approach can be robustly deployed on real robots, providing important properties like intuitive and data-efficient skill transfer as well as reactivity to changes in the environment. Particularly, Fig. 4b shows swift switching behaviors between lifting and re-grasping policies depending on the grasping state. The vector fields in attests the smooth modulation of the policy space depending on both context and state inputs, with the regions of low uncertainty changing with task progress while guaranteeing successful execution. Current limitations include the choice of context repre- sentation, which has been done ad hoc depending on the experiment, as well as the non-negligible number of hyper- parameters. Note that other approaches that encode epistemic uncertainty, e.g., GPR [31], are also suitable LfD policies in our formulation. However, the encoding of aleatory uncer- tainty by KMP offers additional interesting properties that we intend to exploit in extensions of our approach."}, {"title": "VII. CONCLUSION", "content": "We introduced an approach for context-aware LfD that has the ability to respond to changing environment inputs in a time-independent manner, making it well-suited to reactive object manipulation including soft, deformable objects. We showed the applicability of our solution in both simulation and two manipulation tasks on a real 7-DoF robotic system. In future work we will investigate hyperparameter optimiza- tion as well as active learning formulations that actively request new data when the context changes significantly."}]}