{"title": "STACKING SMALL LANGUAGE MODELS FOR GENERALIZABILITY", "authors": ["Laurence Liang"], "abstract": "Recent advances show that large language models (LLMs) generalize strong per-formance across different natural language benchmarks. However, the large size of LLMs makes training and inference expensive and impractical to run in resource-limited settings. This paper introduces a new approach called fine-tuning stacks of language models (FSLM), which involves stacking small language models (SLM) as an alternative to LLMs. By fine-tuning each SLM to perform a specific task, this approach breaks down high level reasoning into multiple lower-level steps that specific SLMs are responsible for. As a result, FSLM allows for lower training and inference costs, and also improves model interpretability as each SLM communicates with the subsequent one through natural language. By evaluating FSLM on common natural language benchmarks, this paper highlights promising early results toward generalizable performance using FSLM as a cost-effective alternative to LLMs.", "sections": [{"title": "INTRODUCTION", "content": "Since the publication of the transformer paper Vaswani et al. (2017), a considerable amount of research devoted to large language models (LLMs) has shown that LLMs are capable of generalizing well on natural language benchmarks and that new emergent properties appear as LLMs increase in scale. Devlin et al. (2019); Wei et al. (2022). LLMs seem to follow some empirical scaling laws, where larger datasets, compute and model size contribute to improvements in model performance. Kaplan et al. (2020)\nAs language models and datasets increase in size, a growing need emerges to identify methods to run language models in resource-limited settings where large amounts of compute are inaccessible. In fact, multiple methods have been documented and researched in recent years to make LLM training or inference more computationally efficient. One such method is fine-tuning: given a pre-trained model, fine-tuning that model for specific tasks can cause that model to score better on benchmarked tasks downstream. Brown et al. (2020) Furthermore, more efficient methods of fine-tuning such as LORA and QLoRA also show that adding a trainable adapter to LLMs whose weights are frozen also allows for faster fine-tuning while showing strong signs of solid model performance. Hu et al. (2021); Dettmers et al. (2023)\nAdditionally, recent work indicates that small language models (SLM), such as Microsoft's Phi-3, can still achieve decent performance on natural language benchmarks. This finding is important, as it suggest that small language models, which are a few orders of magnitude smaller than state-of-the-art LLMs, can still achieve solid performance on various benchmarks. Abdin et al. (2024)\nThis paper aims to build on both the fine-tuning and small language model directions, in order to identify methods that allow for cost-effective training and inference in resource-limited settings. As a result, this paper proposes a new model framework called Fine-tuning Stacks of Language Models (FSLM) - or \"stacking\" - which involves chaining multiple specialized small language models together such that the framework's input and output resemble those of performant language models.\nFSLM takes loose inspiration from the human brain, where different components specialize in different tasks. For small language models, because each SLM has limited capabilities due to its small"}, {"title": "RELATED WORK", "content": ""}, {"title": "MODEL FINE-TUNING", "content": "In recent years, researchers have shown that pre-training a language model in a self-supervised fashion, followed by fine-tuning that same model to a variety of tasks, improves model perfor-mance downstream on natural language benchmarks. OpenAI's GPT is a notable example of fine-tuning a pre-trained model. Brown et al. (2020) Because fine-tuning entire models is expensive, researchers have developed different methods to minimize computational cost while still achieving similar model performance.\nHu et al. (2021) introduced Low-Rank Adaptation (LoRA) as a fine-tuning approach. LoRA freezes the weights of the original pre-trained model, and adds an \"adapter\u201d component, located between the original model output and the actual text output. Instead of the adapter being a fully connected layer, the adapter uses matrix factorization to generate low-rank matrix multiplications that approximate the fully connected equivalent. Low-rank matrix multiplication, however, is less computationally expensive than running inference on a fully connected layer. Hu et al. (2021) then show that LoRA can maintain or even improve model performance. Dettmers et al. (2023) devel-oped QLORA, which performs quantization to further improve LoRA. Both QLoRA and LoRA are considered to be Parameter-Efficient Fine-Tuning (PEFT) methods, a group of methods that aim to increase the efficiency of fine-tuning models. Xu et al. (2023)"}, {"title": "MODEL COMPRESSION", "content": "Model compression techniques aim to either shrink a given model's size, or to train a smaller model to learn from a larger one.\nFor instance, quantization reduces the precision of the model weights, thus decreasing the overall size of the model. Even though the model loses precision, if quantization is implemented correctly, the model should maintain a similar level of performance while experiencing a speedup for training and inference. Jacob et al. (2017)\nModel pruning removes weights whose values are close to zero, thus eliminating weights that may not be contributing to the model's main inference. Cheng et al. (2024)\nModel distillation is another method of interest: using a teacher-student architecture, a smaller \"student\" model learns from a larger \"teacher\" model that should be already well-trained. As a result, the teacher model distills its internal knowledge to the student model, by providing the student model inputs and outputs to learn from during this training process. Hinton et al. (2015); Sanh et al. (2020)"}, {"title": "METHOD", "content": ""}, {"title": "FSLM METHOD OVERVIEW", "content": "The FSLM framework consists of four small language models (SLM) that each specialize in a spe-cific task, as shown in Fig. 1. A human user would supply a prompt to the FSLM framework, and the FSLM framework responds with a textual output. Internally, the SLMs look for specific textual elements from either the user's input or another SLM's output. As a result, each individual SLM is compensating for its limited capabilities by instead specializing in a specific task. As a result, the overall framework follows an information flow where textual information is slowly processed towards the intended model output."}, {"title": "CHOICE OF MODELS", "content": "We use the Pythia 160M GPT-NeoX architecture from the Pythia suite, as Pythia allows for ease of future scalability as we can evaluate on different model sizes. Biderman et al. (2023) Pythia also integrates well with LM-eval, which we use to evaluate FSLM on natural language benchmarks. Gao et al. (2024)"}, {"title": "CHOICE OF DATASET", "content": "We use the Alpaca dataset to train FSLM in an instruction-tuning manner. Taori et al. (2023) Alpaca contains 52,000 self-instruct generated instructions covering a wide array of applications. As of this writing, we selected a subsample of 5,000 instructions to fine-tune FSLM."}, {"title": "TRAINING DATA GENERATION", "content": "In order to properly distill the intermediary texts between SLMs, we use the Llama 3.2 (3B) model to generate texts, a recent addition to the Llama family of LLMs. Touvron et al. (2023)"}, {"title": "FINE-TUNING", "content": "We use HuggingFace's PEFT implementation to run LoRA for fine-tuning."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "NATURAL LANGUAGE BENCHMARKS", "content": "We use Eleuther AI's LM-Evaluation Harness to run natural language tasks from TinyBenchmarks. Gao et al. (2024); Polo et al. (2024)"}, {"title": "QUALITATIVE ANALYSIS FOR MODEL INTERPRETABILITY", "content": "Our FSLM implementation with four Pythia-160M is capable of simple question and answering in a coherent manner, as shown in Table 2. Since our FSLM framework (approximately 640M param-eters, or around 1.4 GB) is sufficiently small to run on most mobile phones or personal computers, the coherence and natural-sounding response further show promise that FSLM can run in resource-limited settings and exhibit human-like responses.\nThe intermediary outputs of SLMs within FSLM is of particular interest, because these responses allow us to directly evaluate model interpretability. Accordingly, we observe in Table 2 that the intermediary SLM outputs match very strongly with each pre-defined task, at least from the per-spective of a human observer. While this shows that LoRA fine-tuning for FSLM is cost-effective, these intermediate SLM responses also serve as a checkpoint to flag potential mistakes or halluci-nations. Because each SLM is specialized for a specific task, we expect the scope of the responses for each SLM to be somewhat bounded. As a result, if we detect that one of the responses seems wrong, it may be sufficient to only re-tune that single SLM, instead of the whole FSLM stack. In addition to promoting model explainability, this design would also minimize compute costs needed to fix overall model performance throughout model deployment."}, {"title": "CONCLUSION AND DISCUSSION", "content": "The objective of this paper was to evaluate whether FSLM, a stack of task-specific SLMs, can per-form well on natural language benchmarks and also exhibit natural-sounding text responses. By running natural language benchmarks, we determined that there were promising signs showing that FSLM's Pythia models perform on par with vanilla Pythia models of comparable sizes, suggesting that stacking fine-tuned specialized models can lead to accurate models at small scales. Addition-ally, by observing the full response of a sample model output, we determined that the final output was coherent and natural-sounding, and that the intermediary outputs were also highly aligned to each SLM's intended task. Additionally, FSLM's modular design could allow for easy model de-bugging and replacement of faulty SLMs. These results demonstrate encouraging signs that stacks of highly specialized small language models can perform as well as equivalent models of the same size, making FSLM architectures a potential area of interest for resource-limited compute settings.\nOne main limitation concerns the limited scope for natural language benchmark evaluations. Be-cause FSLM is a new implementation, we needed to write additional code to integrate it with existing Im-eval tasks, which initially limited the scope of tasks we could run as of this writing. Conse-quently, future work should increase the number of natural language benchmarks, and also evaluate model perplexity for token generation, and rouge scores for model summarization. Furthermore, surveys with human observers interacting with FSLM would be beneficial, as we would be able to quantitatively assess the quality and helpfulness of human-to-model interactions.\nAnother limiting factor is the fine-tuning scope. Future work should try different fine-tuning datasets and determine to what extent dataset quality influences model performance downstream. On a sim-ilar topic, model pre-training should also be documented, as shown by the flan-T5 models' superior performances. Future work should investigate fine-tuning SLMs across different architectures that underwent different pre-training processes."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "All the code used in this paper is accessible publicly on GitHub. The code is written in Jupyter Notebooks, which makes it easy for researchers to run and reproduce these results. Due to the double-blind submission, the GitHub link is not displayed here, though the codebase is available upon request."}]}