{"title": "Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data Contamination's Impact on Machine Translation", "authors": ["Muhammed Yusuf Kocyigit", "Eleftheria Briakou", "Daniel Deutsch", "Jiaming Luo", "Colin Cherry", "Markus Freitag"], "abstract": "Data contamination\u2014the accidental consumption of evaluation examples within the pre-training data\u2014can undermine the validity of evaluation benchmarks. In this paper, we present a rigorous analysis of the effects of contamination on language models at 1B and 8B scales on the machine translation task. Starting from a carefully decontaminated train-test split, we systematically introduce contamination at various stages, scales, and data formats to isolate its effect and measure its impact on performance metrics. Our experiments reveal that contamination with both source and target substantially inflates BLEU scores, and this inflation is 2.5\u00d7 larger (up to 30 BLEU points) for 8B compared to 1B models. In contrast, source-only and target-only contamination generally produce smaller, less consistent over-estimations. Finally, we study how the temporal distribution and frequency of contaminated samples influence performance over-estimation across languages with varying degrees of data resources.", "sections": [{"title": "1. Introduction", "content": "Scaling laws have reshaped our understanding of the data requirements for training language models (LM), leading to a rapid expansion in data collection efforts. This expansion has inadvertently increased the probability of evaluation data contamination (Sainz et al., 2024): fragments or even entire test sets are accidentally consumed within the pre-training data, thereby invalidating the assumption that models are evaluated on unseen data.\nAlthough several studies have acknowledged the contamination issue and shown that it can contribute to performance overestimations (Zhou et al., 2023; Jiang et al., 2024; Yang et al., 2023), we still lack a large-scale controlled analysis to better characterize the phenomenon. Concretely, prior efforts are commonly limited to smaller scales, both in terms of model and data (Jiang et al., 2024), treating contamination as a fine-tuning (Yang et al., 2023) or extended pre-training problem (Zhou et al., 2023), rather than addressing it directly in the larger-scale pre-training setting. Meanwhile, computational and logistical constraints have deterred large-scale, systematic experiments that would clarify how contamination interacts not only with model size, but also with training dynamics, and data composition.\nIn this paper, we present a controlled study to isolate and measure the impact of contamination during pre-training under diverse contamination conditions and two model scales (1 and 8 billion parameters). For this purpose, we take machine translation (MT) test suites as our case study, covering eight languages, thereby gaining insight into the interaction between contamination and linguistic resource availability.\nStarting with a multilingual mixture of public pre-training corpora, we first decontaminate our train-test set splits and train a base LM on the training data (Figure 1). Then, we systematically introduce MT test examples into pre-training by varying their mode (presenting source and target in isolation or as full, prompted parallel texts), temporal distribution (controlling for when contamination happens), and frequency (controlling for the number of contamination copies). To efficiently explore various contamination conditions without the prohibitive cost of training from scratch, we adopted a branching strategy-resulting in more than 50% reduction in our training computational budget. Concretely, each experiment branched from a base model checkpoint and continued training on a modified data mixture, which replaces original samples with test contamination instances. Following our branching strategy, we studied a total of 42 contamination conditions for each model scale, by comparatively studying the performance of contaminated and uncontaminated models across contaminated and non-contaminated datasets.\nOur findings are summarized as follows:"}, {"title": "2. Related Work", "content": "Prior work has studied contamination mainly by detecting it post-hoc or training models with contaminated data to understand its impact on model performance. The former involves examining token probabilities (Shi et al., 2024) or analyzing model preferences for specific orderings (Oren et al., 2023), while the latter assesses contamination's impact by intentionally contaminating test sets into LLMs' pre-training mixtures (Jiang et al., 2024; Yang et al., 2023; Magar & Schwartz, 2022; Zhou et al., 2023).\nWe present a comparative summary of our method and previous work in Table 1. Several studies have explored the impact of data contamination by extended pre-training or fine-tuning on contaminated data (Table 1, Training Process), leaving the generalization of their results to contamination in pre-training an open question. Zhou et al. (2023) set up their experiments as extended pre-training, which could overstate contamination's impact, as models are more susceptible to performance inflation when contamination is introduced later in training (Magar & Schwartz, 2022). On the other hand, studying contamination during fine-tuning, as in Yang et al. (2023), involves substantial training changes\u2014from learning rate to batch size choices\u2014 further complicating its generalizability to pre-training.\nPrevious works analyze models and pre-training corpora of limited sizes (Table 1, Model Size and Data Size), primarily restricted by resource constraints. For example, Jiang et al. (2024) use models ranging from 124M to 774M parameters, while Magar & Schwartz (2022) use models with 110M to 345M parameters. Our work contributes an analysis at larger scales (1B and 8B parameters and 325B tokens) to shed more light on how contamination behaves at scale.\nMoreover, methods measuring the impact of contamination or detecting it via model activations often assume a well-understood pre-training mixture and that any unintended data is absent from the corpus (Table 1, Data Control). Consequently, prior work usually omits a rigorous decontamination step to establish a clean base model performance (Jiang et al., 2024; Magar & Schwartz, 2022; Zhou et al., 2023). However, existing work indicates that contamination frequently occurs in public pre-training corpora, highlighting"}, {"title": "3. Large-Scale Contamination Analysis", "content": "Our experimental pipeline comprises four steps (Figure 1). We start by searching the pre-training mixture with an n-gram search algorithm to detect existing overlap with our evaluation datasets (\u00a73.1). Then, we decontaminate our test set, and train a baseline model on the training split (\u00a73.2). This baseline is used as a reference, uncontaminated model in our experiments. As a next step, we systematically contaminate MT evaluation sets within the baseline's pre-training mixture by defining a wide range of contamination conditions (\u00a73.3). Finally, to efficiently manage the large-scale nature of our analysis, we developed a checkpoint-branching approach, resulting in a jungle of contaminated checkpoints, which are compared against the baseline to isolate the effect of contamination (\u00a73.4)."}, {"title": "3.1. Search and Decontaminate Test Sets", "content": "We implement an 8-gram search to find matches between the test sets and the pre-training data. We do not normalize the text and work with sub-word tokens instead of white space-split text. We search for source and target contamination separately. An example is labelled as contaminated if the longest matching sub-sequence (Singh et al., 2024) matches more than 70% (Chowdhery et al., 2022) of their source or target tokens. By running this algorithm against our pre-training mixture, we found around 10% of test examples being already contaminated. We removed those examples from our test sets to ensure a clean setup. Detailed statistics are found in Appendix G."}, {"title": "3.2. Uncontaminated Baseline Model", "content": "Our 1B and 8B models are decoder-only transformer models (Vaswani, 2017) trained with a casual language modelling objective. We use a sentence piece tokenizer (Kudo, 2018) with a vocabulary size of 256K. We use a 4,096 token context window and a batch size of 512 for 155K steps, bringing our training budget to 325B tokens. The 8B and the 1B models are trained using the same hyper-parameter settings and data.\u00b9 During training, we track loss on the validation set a small random sample from the training mixture. We use the ADAM (Kingma & Ba, 2014) optimizer with cosine learning rate decay with a warmup phase."}, {"title": "3.3. Contamination Conditions", "content": "We experimented with various ways of contaminating MT samples along three dimensions (Table 2). First, we define different modes of contamination-one where a sample is presented as a full, prompted\u00b2 source-target instance and two partial contamination cases where the model is only exposed to either the source or the target. For source-target contamination, we introduce two additional variants by injecting each side as independent, unformatted samples either within or across different batches. Second, we vary the temporal distribution of contaminated samples by injecting them at different pre-training points or uniformly distributing them throughout. Third, we vary the number of times each contaminated sample is presented to the model.\nContaminated samples are mixed with existing training data, ensuring they are not seen in isolation. Contamination for each setting is introduced within a pre-determined window of training steps to enable fair comparisons."}, {"title": "3.4. Checkpoint-branching", "content": "To avoid training multiple LLMS from scratch, each contamination setting branches out from the baseline checkpoint and continues pre-training by inheriting all its training hyperparameters (see Figure 1). This gives us two main benefits: efficiency-by avoiding training from scratch for each contamination setting, we reduced the total training budget by 53.6%, and reduced variance between contaminated and baseline runs allowing us for better isolating the impact of contamination by increasing the overlap in the compared models' training and initialization."}, {"title": "4. Experimental Setup", "content": "Pre-training Data Our pre-training data are drawn from multiple public resources. Monolingual texts are sourced from Dolma (Soldaini et al., 2024) for English and Madlad (Kudugunta et al., 2024) for covering non-English languages. In addition to monolingual resources, we add parallel data into our pre-training mixture, which has shown to be critical in enabling translation capabilities at the scales we study (Briakou et al., 2023; Chowdhery et al., 2022; Alves et al., 2024). Our parallel data are sourced from the WMT'23 translation task (Kocmi et al., 2023). The total size of these datasets is about 2T tokens, which we have downsampled for our purposes. The total pre-training mixture consists of 325 billion tokens based on our multilingual sentence-piece tokenizer. The different sources are mixed based on the ratios of 60% for Dolma, 35% for Madlad, and 5% for parallel texts. The exact token counts and Madlad languages can be seen in Table 3. We randomly sampled from Dolma and Madlad sources without altering their domain or language distributions. All parallel texts from WMT are used and up-sampled to fit our 5% parallel data budget. When we insert contamination into the data, we try to keep the ratio of parallel data constant by only randomly replacing less than 5% of the examples in a batch with contamination."}, {"title": "5. Results", "content": "Main Findings We present an overview of our results in Figure 2, which shows the absolute BLEU score differences between contamination models of different flavors, each compared against the baseline, uncontaminated model on the WMT'23 contaminated datasets. Several clear trends emerge. First, contaminating both source and target in a prompted format (Full) into the training data-regardless of when contamination happens during training or how many times the contaminated instances are seen by the model\u2014consistently inflates model performance. Second, this inflation becomes more pronounced as model size increases. For the 1B model, maximum performance over-estimates"}, {"title": "5.1. Are performance over-estimations actually over-estimations?", "content": "The contaminated MT test examples are arguably a high-quality source of parallel texts. Therefore, it is reasonable to ask: to what extent does the increased performance come from contamination inflation rather than genuine improvements in the model's translation capabilities? If the latter was true, we would expect the performance improvements to generalize to other test sets of the same task that are not contaminated. To account for this, we evaluate our models on the WMT' 24 non-contaminated test sets."}, {"title": "5.2. How does the temporal distribution of contamination impact performance inflation?", "content": "As discussed in \u00a72, previous works studied contamination in extended pre-training or fine-tuning settings. However, as shown in Figure 4, the temporal distribution of contamination, i.e., the time-step(s) at which a pre-training model is exposed to contamination, impacts the inflation we observe at the end of pre-training. Concretely, we notice that the earlier the contamination is introduced, the larger the immediate spike in performance. As training continues, those earlier momentary spikes are rapidly wearing off, with their ultimate effect being way smaller than their initial intensity, i.e., a spike of ~ 70 BLEU points is mapped to ~ 40 points after ~ 100K steps. As a result, observing contaminated data later during training has a bigger footprint than early exposure, which questions whether studying contamination within extended pre-training or fine-tuning settings exaggerates its measurable impact in real pre-training scenarios.\u2074\nFurthermore, we observe that uniform contamination- contaminated examples being uniformly spread out during pre-training instead of being presented within concentrated time steps-results in larger performance inflation across all temporal settings, even the one where contamination is exposed late at pre-training. This finding has important implications, given that uniform contamination reflects a more realistic contamination scenario in the wild, and randomizing the pre-training data order is common practice."}, {"title": "5.3. How does the frequency of contamination impact performance inflation?", "content": "Figure 5 presents how the performance of the contaminated models improves on the contaminated datasets as the frequency of contamination increases. As seen, for particular language pairs (dashed lines), performance inflation follows a A-Shaped curve, in line with what is observed in prior works (Jiang et al., 2024). Although some languages follow this trend, most of them do not. On average, for Full contamination, the inflation from contamination increases with the number of copies. On the other hand, partial contamination exhibits a different trend as the performance does not"}, {"title": "5.4. How is contamination format impacting performance inflation?", "content": "Contaminated data can be presented within pre-training mixtures in different formats depending on a variety of reasons, starting from how they naturally occur on the web to how curated data are pre-processed before consumed by LLMS. When it comes to contamination of source-target MT examples, we have so far explored the special case where the contaminated dataset is consumed as Full, formatted examples in the same prompt format used at test time. However, test data on the Internet is not necessarily stored in that same format, while source and target texts can be maintained in separate files, which means that source-target pairs can be contaminated as unpaired examples. To simulate such scenarios, we create two variations of source-target contamination: starting from a given source-target text; we insert each side as a separate, unpaired example into the same batch (named Source and Target, Batched) or in different batches (named Source and Target, Split).\nFigure 6 compares these two settings with the prompted format (Source and Target, Prompted also named Full) and the case where we only contaminate the target text which we add as an additional reference point. Comparing with these baselines, we see that both the Split and Batched settings perform better than target-only contamination however, the prompted format still results in higher inflation. Comparing Split and Batched, we see that consuming the unpaired texts within the same batch causes larger performance over estimations compared to spreading them across different batches, even though the examples are not presented as paired, prompted translation texts. We also observe that the additional performance inflation caused by Batched and especially Split contamination compared to target-only contamination is larger for the 8B model than the 1B model."}, {"title": "5.5. How does contamination impact performance of near zero-resourced languages?", "content": "To understand the impact of contamination for languages with no intentional language representation during pre-training, we contaminate MT examples from three languages\u2014Achenese, Wolof and Yoruba\u2014sampled from FLORES. We note that we intentionally do not include any monolingual or parallel data for those languages in our pre-training mixtures.\nFor these language pairs, we observed no performance inflation from contamination (Table 4). Even right at the contamination, the BLEU score increased only by 1 to, at most, 3 points for both the 8B and 1B models, as seen in Appendix D. This suggests that gains from contamination require the model having some of representation in that language, at least at the scales investigated in this work."}, {"title": "5.6. How does contamination impact performance into versus out of English directions?", "content": "Figure 7 illustrates the percentage BLEU score improvements grouped by out and into English translation directions (En\u2192X and X\u2192En, respectively). As shown, contamination has a more significant impact on the En\u2192X translation direction compared to X\u2192En, for all Full contamination settings. While we present percent improvements, the absolute improvements are also larger for the En\u2192X direction for almost all cases. Considering the results from Section 5.5, one could naturally expect the model to benefit more in the X\u2192En since it has better performance and hence better representations in English. Contrarily, we observe that the model can benefit more from contamination in En\u2192X, where the original performance is lower. These results, together with our findings from Section 5.5, show that while a certain level of representation is necessary to benefit from contamination, the performance gains do not continue to constantly increase as model capabilities improve, highlighting the complex relationship between base model capabilities and the effects of contamination. This finding highlights the complex relationship between contamination and the availability of language resources and demonstrates how previously observed model behaviors under well resourced settings (English) might fall short when moving to multilingual settings."}, {"title": "6. Limitations and Discussion", "content": "Drawing definitive conclusions on whether contamination matters is very challenging due to sources of variance that are challenging to control for. Initialization seed, data ordering, among others, introduce variance into the analysis. In an ideal world, one would run each contamination experiment for multiple random seeds of the model and data orders. However, resource constraints deem this setup impractical.\nDespite those challenges, our experimental setup takes steps to control for variance as much as possible. For instance, we fixed the order of the data and the model initialization. While these deterministic systems helped minimize random variations between runs and allowed us to isolate the impact of contamination, they also imposed certain limitations on our findings. Specifically, our results are based on a single canonical ordering of the training dataset and a single initialization of the two models used in the experiments.\nAnother source of variation we observed was between different checkpoints (stopping points). This pattern is further elaborated in the Appendix D, where the model's performance can vary for different language pairs between different training steps. This makes interpreting individual data points for language pairs at the end of training more challenging. To tackle this problem, we focused on analyzing meaningful aggregates and general trends rather than changes and variations for individual language pairs.\nOne challenge this introduced is that changes in BLEU score across different scales do not correspond to comparable differences in quality. This aggregated BLEU scores across different value ranges can be tricky and must be done carefully. To tackle this problem, we either use the same language pairs in all aggregates that are compared at a single experiment or when we split language pairs, we group language pairs that are on similar BLEU scales together. We also compare average BLEU changes when comparing WMT'23 and wMT'24 performances but acknowledge the caveat explained here.\nFinally, our experiments on models with up to 8B parameters indicate that the impact of contamination grows with model size, although this trend is not guaranteed to hold for larger models."}, {"title": "7. Conclusion", "content": "In this work, we study the impact of contamination on large language model pre-training, with a focus on the task of machine translation. Our work employs a checkpoint-branching strategy that allows us to efficiently scale up our study to 46 contamination conditions across two-model scales, 1 and 8 billion parameters, and 13 language-pairs. The key experimental results include that (1) contaminating both the source and target text leads to substantial performance inflation; (2) when the contamination is observed during training influences the size of its impact, with uniform contamination having the biggest effect; (3) larger models benefit more from contamination, and (4) contamination requires sufficient language representation to have a measurable effect. This work sheds light on the nuanced ways in which data contamination affects model performance, and underscores the need for more reliable evaluation practices in large language model development."}]}