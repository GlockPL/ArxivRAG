{"title": "Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models", "authors": ["Yida Zhao", "Chao Lou", "Kewei Tu"], "abstract": "Syntactic Transformer language models aim to achieve better generalization through simultaneously modeling syntax trees and sentences. While prior work has been focusing on adding constituency-based structures to Transformers, we introduce Dependency Transformer Grammars (DTGs), a new class of Transformer language model with explicit dependency-based inductive bias. DTGs simulate dependency transition systems with constrained attention patterns by modifying attention masks, incorporate the stack information through relative positional encoding, and augment dependency arc representation with a combination of token embeddings and operation embeddings. When trained on a dataset of sentences annotated with dependency trees, DTGs achieve better generalization while maintaining comparable perplexity with Transformer language model baselines. DTGs also outperform recent constituency-based models, showing that dependency can better guide Transformer language models. Our code is released at https://github.com/zhaoyd1/Dep_Transformer_Grammars.", "sections": [{"title": "1 Introduction", "content": "Transformer language models have shown strong performance on language modeling tasks and a broad spectrum of downstream tasks (Radford et al., 2019; Devlin et al., 2019; Brown et al., 2020). Despite the great power of the Transformer architecture (Vaswani et al., 2017), it lacks the inductive biases of syntactic structures, which has been hypothesized to improve generalization (Everaert et al., 2015). A straightforward way to incorporate such biases into Transformers is explicit modeling of syntactic structures.\nInspired by earlier work of generative parsing as language modeling that integrates syntactic structures into RNNs (Dyer et al., 2016; Choe and Charniak, 2016), recent studies have focused on adapting this method to Transformer architectures (Qian et al., 2021; Yoshida and Oseki, 2022; Sartran et al., 2022; Murty et al., 2023). The models proposed by these studies are categorized as syntactic language models because they jointly model the distribution of surface strings and their corresponding syntactic trees. Experiments show that these models achieve competitive perplexity in language modeling and gain better syntactic generalization, supporting the above hypothesis on the benefits of introducing inductive bias of syntactic structures. However, the structural supervision that has been used in all these models is based on constituency trees and it is unclear of the performance of dependency-based Transformer syntactic language models. Different from constituency structures, which model recursive syntactic compositions, dependency structures focus more on the relationship between tokens, which is similar to the self-attention mechanism in Transformer, hinting at potential synergy between the two.\nIn this paper, we propose Dependency Transformer Grammars (DTGs), dependency-based syntactic language models that learn joint distributions of sentences and dependency trees. DTGs introduce an inductive bias of dependency structures to Transformers by (i) modeling transition sequences of transition-based dependency parsers instead of sentences, (ii) simulating the stack operations in transition-based dependency parsers through modification of attention masks, (iii) incorporating the stack information of transition-based systems through relative positional encoding of stack depth, and (iv) representing head-dependent relations through a combination of head token embeddings and transition operation embeddings. Following a line of previous work in generative dependency parsing (Titov and Henderson, 2007; Cohen et al., 2011; Buys and Blunsom, 2015), the generative formulation of our model is based on the"}, {"title": "2 Preliminaries: Transition-based Dependency Parsing", "content": "Given a sentence, transition-based dependency parsing predicts a sequence of predefined transitions between states that incrementally build a dependency parse tree. A state contains a stack \\sigma with token i on the top, a buffer \\beta with j at its leftmost side, and a set A of dependency arcs, denoted as (\\sigma_i, j\\beta, A).\nIn this work, we focus on unlabeled projective dependency parsing for the simplicity of its transition systems. There are several different transition systems for projective dependency parsing, as shown in Table 1. Arc-standard (Nivre, 2004) is a widely used transition system that defines three transitions: SHIFT, LEFTARC and RIGHTARC. Arc-standard builds dependency trees in a bottom-up manner, that is, every token is not connected to its head token until it gathers all of its dependents. Arc-eager (Nivre, 2003) is another transition system that adds one more transition: POP. The main difference between arc-standard and arc-eager lies in the scope of arcs. Arc-standard only allows inducing arcs in the stack while arc-eager eases the restriction by defining arc transitions between the stack and the buffer. As a result, dependency trees are no longer built from bottom to up in arc-eager. A later system arc-hybrid (Kuhlmann et al., 2011) combines LEFTARC in arc-eager and RIGHTARC in arc-standard. Another more recent system arc-swift (Qi and Manning, 2017) extends arc-inducing to non-local cases: transition LEFTARC/RIGHTARC[k] in arc-swift can be seen as k-1 POP operations followed by one arc-inducing in arc-eager.\nThe above dependency parsing transition systems can be changed into a generative form, such that they generate sentences along with their associated dependency trees. The main change to the transition systems is that tokens need to be generated instead of being shifted from the buffer. Specifically, in arc-standard we substitute SHIFT with a token generation transition GEN, while retaining the other transitions (Titov and Henderson, 2007; Cohen et al., 2011; Buys and Blunsom, 2015). Other systems require additional efforts to obtain a generative form because they contain the usage of the buffer head in LEFTARC and/or RIGHTARC before shifting it to the stack. Simply replacing SHIFT with GEN cannot ensure the existence of the two tokens involved in a newly generated arc. Therefore, we need to insert a GEN' transition, which generates a new token but puts it in the buffer, before any LEFTARC/RIGHTARC transition that involves an un-generated token. The SHIFT transitions are omitted because any generated token will be shifted to the stack once a new token is generated.\nWe can use an oracle to extract a transition sequence from a dependency parse tree: An arc-inducing transition is generated whenever possible, and a POP transition (in arc-eager) is generated when it is impossible to generate other transitions, i.e., the transition preference order is LEFTARC/RIGHTARC > GEN > POP."}, {"title": "3 Model", "content": "DTG follows the generative form of the arc-standard dependency transition system and generates a sequence of transitions that construct a sentence x and its dependency tree y incrementally. The sequence consists of three types of transitions:\n* GEN(x): generating a token x, which corresponds to the GEN operation in generative arc-standard and is exactly what a standard Transformer decoder does at each step;\n* LEFTARC or LA: inducing an arc from the most recent unconnected token (i.e., a token that has not been connected to its head) to the second most recent unconnected token, which corresponds to the LEFTARC operation in arc-standard;\n* RIGHTARC or RA: inducing an arc from the second most recent unconnected token to the most recent unconnected token, which corresponds to the RIGHTARC operation in arc-standard.\nAn example is shown in Figure 1.\nWe write a(x,y) = (a_0, a_1, ..., a_{T-1}) as the transition sequence of length T of sentence x and parse tree y, where each a_t belongs to one of the three types mentioned above. DTG is a Transformer decoder that models the distribution of a(x, y) in the manner of causal language modeling, that is, p(a(x,y)) = \\prod_i p(a_i|a_{<i}). It differs from a standard Transformer in several aspects in order to incorporate the dependency inductive bias, including attention masks, positional encoding, augmented representation of arcs, and constrained generation, which we discuss in the following subsections."}, {"title": "3.1 Arc-Standard via Attention Mask", "content": "DTGs generate the transition sequence autoregressively. A standard Transformer language model makes predictions based on the complete generation history. In contrast, to incorporate the dependency inductive bias into DTGs, we generate transitions based on the stack in arc-standard. The stack is encoded into the model with different attention forms and is updated by input transitions.\nWhen a GEN transition comes, the transition system pushes a new token onto the stack and then gathers the stack information to generate the next transition, which we realize by the first attention form, STACK attention. When a transition changing the dependency structure comes, i.e., a LEFTARC/RIGHTARC transition, the stack is updated in two steps: (i) pop two tokens from the stack and designate one as the head of the other and (ii) push the head token back onto the stack. The two steps are realized by the second form of attention, COMPOSE attention, which updates the representation of the head by consuming its dependent but ignoring everything else to reflect the newly induced dependency arc. Then all the stack information is gathered for generating the next transition, which is again realized by STACK attention. Therefore, two forms of attention are required for one transition. As each transition can only use one form of attention in Transformer, we duplicate the arc transitions, namely LEFTARC/RIGHTARC and LEFTARC2/RIGHTARC2. The former encodes dependency information with COMPOSE attention and makes no generation, while the latter triggers the generation of the next transition with STACK attention. After the duplication, the sequence length increases from T to T'. We denote the new sequence as a', which is the exact input sequence of our model. Note that this does not change the distribution of a(x, y), as the generation sequence remains unchanged. An example of the expanded transition sequence and the corresponding attention forms is shown in Figure 2a.\nThe two forms of attention can be realized by leveraging different attention masks. We represent the attention masks as A \\in R^{T'xT'}, where A_{ij} = 1 means position j can be attended from i and A_{ij} = 0 means position j is masked from i. Our models generate transitions in an autoregressive manner, so the attention mask is causal, i.e., A_{ij} = 0 for j > i.\nSTACK attention is performed at each position i which needs to predict a new transition, i.e., a \\in {GEN(x), LEFTARC2,RIGHTARC2}. From position i, we attend to all the unmasked positions before i (including i) to collect all the information on the stack for generation.\nCOMPOSE attention is performed at each position i where a \\in {LEFTARC,RIGHTARC}. From position i, we attend to the positions of the most recent two unmasked tokens, i.e., the top two tokens on the stack in arc-standard, which forms a head-dependent pair. Then we mask the two attended positions from subsequent positions, effectively popping the two tokens from the stack. The newly computed representation serves as a substitute for the head token that has absorbed the information of its dependent and is pushed back onto the stack. Algorithm 1 shows how to compute attention masks for a transition sequence as described above. We also show attention masks of an example transition sequence in Figure 2b."}, {"title": "3.2 Relative Positional Encoding", "content": "We design the positional encoding for DTGs based on the relative positional encoding in Transformer-XL (Dai et al., 2019). In Transformer-XL, the positional encoding is based on the distance between the attending position i and the attended position j, i.e., R_{ij} = i \u2013 j. In DTGs, we modify the formulation to reflect the stack information. Crucially, R_{ij} is only computed when A_{ij} = 1. For STACK attention, we define d(i) as the depth in the stack, which increases from the top to the bottom. We then define R_{ij} = d(i) \u2013 d(j). For COMPOSE attention, we define two positions, 0 and -1, to distinguish between the head and the dependent to be composed, i.e., R_{ij} = 0 if token j is the head token and R_{ij} = -1 if token j is the dependent token. The new representation computed with COMPOSE inherits the depth of the head token, i.e., d(i) = d(j) if token j is the composed head token."}, {"title": "3.3 Arc Representation", "content": "In standard language models, generated tokens are fed back into models as history. For arc-inducing transitions in DTGs, the generated transitions have surface forms of LEFTARC or RIGHTARC while the tokens ought to be pushed back are the head tokens. We propose to feed a combination of LEFTARC/RIGHTARC and the head token via summing the embedding of these two parts. This formulation stems from the following two considerations: (i) the attention in DTGs cannot distinguish between LEFTARC and RIGHTARC, so the embedding of LEFTARC/RIGHTARC acts as an indicator of the arc direction; (ii) the representation computed with COMPOSE is viewed as a substitute of the composed head token by subsequent positions, so we add the embedding of the head token to bias the representation."}, {"title": "3.4 Other Transition Systems via Attention Mask", "content": "We also design the attention mechanism for generative arc-eager and arc-swift and name the resulting models DTG-eager and DTG-swift. We do not work on generative arc-hybrid because its transition sequences are exactly the same as that of generative arc-standard.\nFor DTG-eager, we make two modifications based on DTG: (i) Change the COMPOSE attention of RIGHTARC by not masking the position of the dependent token because in arc-eager, the dependent token can still induce arcs to subsequent tokens. (ii) For transition POP, we define POP-STACK attention, which pops the stack top. The stack top is the second most recent unmasked token in most cases, and the most recent one is the head of the buffer. However, if all tokens have been generated and thus the buffer is empty, the stack top is the most recent unmasked token.\nFor DTG-swift, LEFTARC and RIGHTARC are decorated by an additional positive number k. This affects ranges of attending and masking in COMPOSE attention. That is, we attend to not only the head-dependent pair but also the k + 1 tokens between them, and we mask all these k + 1 tokens for subsequent positions."}, {"title": "3.5 Constraints on Inference", "content": "We define several constraints on transition generation during DTGs inference to make it consistent with the corresponding transition-based dependency parsing systems:\n* For all the systems, the LEFTARC and RIGHTARC transition can only be generated if at least two tokens exist in the stack.\n* For arc-eager, POP can only be generated if the top of the stack has been recognized as a right dependent of some head token.\n* For arc-swift, the value of k in LEFTARC/RIGHTARC[k] must not exceed the size of the stack."}, {"title": "4 Experiments", "content": "We compare DTGs with DTG-eager, DTG-swift, two Transformer-XL baselines, and constituency-based syntactic Transformer language models. The two Transformer-XL baselines follow those of Sartran et al. (2022): (i) TXL (tokens) is a standard Transformer-XL that generates sentences only, and (ii) TXL (trans) is Transformer-XL that generates transition sequences just like DTG, but uses standard attention masks and positional encoding. Constituency-based syntactic Transformer language models include: (i) the \u201cgenerative parsing as language modeling\u201d of Qian et al. (2021) (PLM), (ii) Transformer Grammars of Sartran et al. (2022) (TG) and (iii) Pushdown Layers of Murty et al. (2023) (Pushdown).\nDataset and Preprocessing All the models are trained on the BLLIP-LG dataset of Charniak et al. (2000), with training splits from Hu et al. (2020)."}, {"title": "4.1 Sentence-Level Language Modeling", "content": "We evaluate the perplexity of the models on the BLLIP-LG dataset of Charniak et al. (2000), with test splits from Hu et al. (2020).\nSetup For syntactic language models that jointly model the distributions of sentences and syntactic trees, i.e., p(x, y), we compute the string probability p(x) = \\sum_y p(x, y). It is impossible to compute p(x) precisely due to the large space of all possible trees, so we follow Sartran et al. (2022) to approximate it using a relatively small set of trees sampled from a proposal model q(y|x). For our depdendency-based models, we use the Biaffine-roberta (Dozat and Manning, 2017) parser as the proposal model to sample 300 unlabeled projective dependency trees without replacement as a proposal tree set Y'. p(x) is then approximated by \\sum_{y \\in Y'} p(x, y), which is an exact lower bound of the true value of p(x) (hence leading to an upper bound of perplexity). We evaluate the models by sentence-level perplexity.\nResults We report the perplexity of all the models in Table 2. DTG achieves comparable perplexity with TXL (tokens) and DTG-swift, outperforming DTG-eager. TXL (trans) achieves lower perplexity than TXL (tokens) even though the reported result of TXL (trans) is an upper bound of its true perplexity. It shows that jointly modeling dependency trees and sentences is helpful for sentence-level language modeling.\nThe perplexity upper bound of DTG can be seen to be lower than that of TG. There are two possible interpretations of this result: (i) Dependency trees give better guidance than constituency trees in syntactic language modeling. (ii) 300 trees may be too few to get an accurate approximation of perplexity when sampling from a large set of possible trees. Evaluating DTG and TG requires samples of unlabeled projective dependency trees and labeled constituency trees, respectively. The number of the former is much smaller than the number of the latter. Therefore, sampling 300 trees may give a much tighter perplexity upper bound for DTG than for TG, resulting in a gap in the reported results. Unfortunately, it requires nontrivial work to distinguish between the two possibilities and we leave it for future work."}, {"title": "4.2 Syntactic Generalization", "content": "To measure the syntactic generalization, we evaluate our models on BLiMP (Warstadt et al., 2020) and SG test suites (Hu et al., 2020).\nSetup on BLIMP BLIMP contains 67 generalization tests, each with 1000 sentence pairs. Each sentence pair consists of a grammatical sentence and an ungrammatical sentence. Models are evaluated by whether they assign a higher probability to the grammatical one. We use the same setup as in Section 4.1, sampling 300 trees for each sentence and calculating a lower bound of marginal probability p(x) for comparison."}, {"title": "4.3 Parse Reranking", "content": "Setup We study to what extent DTG and TXL (trans) have learned to produce correct dependency structures. We still sample 300 trees with the Biaffine-roberta parser and rerank them using the two models. We convert human-annotated constituency trees in the Penn Treebank (PTB) (Marcus et al., 1993) test split into dependency trees with CoreNLP 3.3.0 (Manning et al., 2014) and then evaluate the UAS of the reranked trees on them.\nResult We present the results in Table 3. TXL (trans) and DTG both achieve a slightly higher score than the proposal model Biaffine-roberta. Note that both models are trained on the dependency parse trees produced by Biaffine-roberta. The results show that both models successfully learn about dependency structures from Biaffine-roberta."}, {"title": "5 Analysis", "content": "We compare three different representations of LEFTARC/RIGHTARC in DTG : (i) the default formulation of summing the LEFTARC/RIGHTARC embedding and the embedding of the head token x, (denoted as w + arc); (ii) the embedding of the LEFTARC/RIGHTARC alone (denoted as arc); (iii) the embedding of the head token alone (denoted as w). DTG models with these representations are trained and evaluated with the same setting as in Section 4.\nThe result is reported in Table 4. The default formulation outperforms the other two representations, showing that both the head token embedding and the LEFTARC/RIGHTARC embedding play a positive role in arc representation."}, {"title": "5.2 Dependency Parses for Training", "content": "We use an external parser to provide dependency trees in the training data and sample 300 trees in sentence probability evaluation. Here, we study how the quality of the external parser affects our model's performance. We compare two parsers, vanilla Biaffine without pre-trained token embeddings and Biaffine-roberta, as the external parser used in training and evaluation. Note that Biaffine-roberta is more accurate than vanilla Biaffine.\nThe result is reported in Table 5. We see an improvement in both perplexity and generalization when using a better parser."}, {"title": "6 Related Work", "content": "Augmenting language models with syntactic bias has been studied for a long time. One line of work adds constituency-based syntactic structures to language models through jointly modeling the distribution of sentences and structures (Chelba, 1997; Roark, 2001; Henderson, 2004; Choe and Charniak, 2016; Kim et al., 2019). The RNNG model (Dyer et al., 2016) is a representative work of syntactic language models, using recursive networks to build representations of phrases. More recent work of syntactic language models is based on Transformers (Qian et al., 2021; Yoshida and Oseki, 2022; Sartran et al., 2022; Murty et al., 2023). Qian et al. (2021) and Sartran et al. (2022) constrain the attention with syntactic bias, while Pushdown Layers (Murty et al., 2023) enforce structural constraints via gradient based learning. The above work is all based on constituency structures, and there has been some work considering dependency trees with simple neural networks (Titov and Henderson, 2007; Cohen et al., 2011; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015). Most of them, however, focus more on generative dependency parsing while scratching the surface of a language modeling setting. A more general work is Prange et al. (2022), which both introduces constituency and dependency graphs to augment Transformer language modeling, but it requires given gold trees for generation. Following the work of generative dependency parsing and the constrained attention patterns used in Sartran et al. (2022) and other work (Strubell et al., 2018; Peng et al., 2019; Zhang et al., 2020; Nguyen et al., 2020; Fernandez Astudillo et al., 2020; Lou and Tu, 2023), we propose DTG, a novel class of dependency-based syntactic language models. It is the first syntactic language model that designs a dependency-based constrained attention mechanism for Transformers.\nAnother line of work augments models by learnable structures. Some studies integrate stack-structured memory into models, where updating patterns are learned from data rather than being dictated by predefined syntactic inductive bias (Joulin and Mikolov, 2015; Yogatama et al., 2018; DuSell and Chiang, 2021, 2023). Besides, some studies propose to learn structural attention patterns (Kim et al., 2017; Wang et al., 2019; Shen et al., 2021, 2022). For example, Kim et al. (2017) assumes that the attention scores are subject to linear-chain or tree conditional random fields (CRFs; Lafferty et al., 2001). These kinds of augmentation lead to better generalization but usually cost longer running time than naive counterparts.\nSome other studies focus on examining the syntactic knowledge acquired by standard attention after pretraining (Htut et al., 2019; Kovaleva et al., 2019; Kim et al., 2020; Ravishankar et al., 2021). These studies have identified that certain attention heads align their attention patterns with syntactic structures, thereby providing substantial evidence for the benefits of introducing syntactic inductive bias. In addition, some work re-invents attention using dependency structures and CRFs (Wu and Tu, 2023), motivating more linguistically principled studies."}, {"title": "7 Conclusion", "content": "We propose DTGs, a new type of syntactic language models that add explicit dependency bias into Transformers. DTGs simulate dependency transition systems with constrained attention patterns and incorporate stack information through relative positional encoding. Experiments show that DTGs surpass Transformer language model baselines and other constituency-based syntactic language models on syntactic generalization while maintaining competitive perplexity. This implies that the presence of dependency information does improve the performance of Transformer language models."}, {"title": "Limitations", "content": "DTGs rely on dependency trees for training, which are predicted by an external parser in this study. However, for languages lacking accurate dependency parsers, our methods might not offer benefits. Additionally, we restrict trees in our study to be in the Standard Dependency representation (de Marneffe and Manning, 2008) and only consider non-labeled projective dependency trees at the sentence level. The investigation of other dependency representations, such as Universal Dependencies (Nivre et al., 2020), more complex trees and document-level settings is left for future research.\nFor training and inference, DTGs cannot utilize some recent advancements for Transformers easily, including rotary position embeddings (Su et al., 2021) and Flash attention (Dao et al., 2022), due to our attention mask patterns and relative position encodings. Moreover, evaluating a sentence's probability with DTGs requires marginalizing over all possible trees, which is intractable. In this study, we approximate this by sampling 300 trees. However, this is still time-consuming and only provides an upper bound for the perplexity metric."}, {"title": "A Examples of DTG-eager and DTG-swift", "content": "The example of DTG-eager is shown in Figure 4. The main difference with DTGs is the new transition POP. It directly masks the top of the stack and attends to other positions, denoted as POPSTACK attention.\nThe example of DTG-swift is shown in Figure 5. The newly introduced ARC number is represented within []."}, {"title": "B Other Experimental Details", "content": "Using subword tokenizers Previously, we always assume that each word corresponds to a single token. However, subword tokenizers (e.g., SentencePiece) may divide a word into several subtokens. In our work, we do not consider dependencies among subtokens within a word. All dependencies between words are converted to arcs between the last subtokens of these words. For masking, once a word should be masked, all of its subtokens are masked. For arc representation, we use the embedding of the last subtoken of the head word.\nComputational costs We spent one NVIDIA A6000 GPU for each training, which lasted approximately 35 hours."}, {"title": "C Discussion on the Results of BLIMP", "content": "An example testcase in the QUANTIFIERS category of BLiMP is to judge whether \"An actor arrived at at most six lakes\" or \"No actor arrived at at most six lakes\" is acceptable. The correct answer is that the former is acceptable while the latter is not, because superlative quantifiers cannot embed under negation. A stardard Transformer language model could assign a lower probability to the second sentence because it could lower the probability of generating \u201cat most\u201d by attending to \u201cNo\u201d. In DTG, however, \"No\u201d as a determiner is absorbed into \u201cactor\u201d and hence masked from the attention when generating \u201cat most\u201d. While doing this can be beneficial to syntactic generalization, it hinders semantic judgment in this case."}, {"title": "D Discussion on the Results of SG", "content": "To measure the overlap that DTG and TXL (trans) get right on SG, we count the numbers of correct examples and wrong examples of both models and the results are shown in Table 6."}]}