{"title": "Learning more with the same effort: how randomization improves the robustness of a robotic deep reinforcement learning agent", "authors": ["Luc\u00eda G\u00fcitta-L\u00f3pez", "Jaime Boal", "\u00c1lvaro J. L\u00f3pez-L\u00f3pez"], "abstract": "The industrial application of Deep Reinforcement Learning (DRL) is frequently slowed down because of the inability to generate the experience required to train the models. Collecting data often involves considerable time and economic effort that is unaffordable in most cases. Fortunately, devices like robots can be trained with synthetic experience thanks to virtual environments. With this approach, the sample efficiency problems of artificial agents are mitigated, but another issue arises: the need for efficiently transferring the synthetic experience into the real world (sim-to-real). This paper analyzes the robustness of a state-of-the-art sim-to-real technique known as progressive neural networks (PNNs) and studies how adding diversity to the synthetic experience can complement it. To better understand the drivers that lead to a lack of robustness, the robotic agent is still tested in a virtual environment to ensure total control on the divergence between the simulated and real models. The results show that a PNN-like agent exhibits a substantial decrease in its robustness at the beginning of the real training phase. Randomizing certain variables during simulation-based training significantly mitigates this issue. On average, the increase in the model's accuracy", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) and its deep-learning variant (DRL) are used in many disciplines since they are powerful tools to solve complex problems that involve sequential decision-making and require extensive experience. Unlike supervised learning, in which an external intelligence labels the training data, or unsupervised learning, in which the goal is to find hidden patterns in unlabeled data, RL is based on learning, by trial and error, from the samples generated within the interaction of an agent and its environment. In each step, the agent, which is the decision-maker, observes the current environment state and the reward obtained and, following a policy, decides which action to perform next. The agent's final goal is to maximize the cumulative reward, named expected return [1].\nRL algorithms can be classified into model-free or model-based methods. The main difference between them is that, in the former, the agent only learns by trial and error the policy that maximizes the objective, whereas in the latter, the agent can also employ a function that predicts the state transitions and rewards from the environment. Model-based algorithms [2] allow the agent to plan ahead and decide among the best possible options, reducing the number of samples needed in a model-free approach and achieving an outstanding"}, {"title": "2 Preliminaries", "content": "RL is a Machine Learning (ML) area in which an agent learns from the feedback given by the environment during the interplay between them. The elements that define an RL problem are the environment's state, which the agent perceives as an observation $S_t \\in S$ where S is the set of states; the action the agent performs $A_t \\in A(S_t)$, being A($S_t$) the set of actions available in $S_t$; and the reward $R_{t+1} \\in \\mathbb{R} \\subset \\mathbb{R}$, which is the result of the agent's decision and the environment dynamics. Besides, the agent acts following a policy $\\pi(a | s)$, which maps states to actions. The agent seeks to maximize the reward received over time, being this expressed as $G_t = \\Sigma_{k=0}Y^kR_{t+k+1}$ where \u03b3\u2208 [0, 1] is the discount rate that modulates the effect of future rewards in the present moment, k denotes the number of time steps, and t refers to the current time step.\nRL problems can be formulated as a Markov Decision Process (MDP) [7] or a Partially Observable Markov Decision Process (POMDP) [8]. However, since the mathematical tools available to solve POMDPs do not usually scale well, these problems must be converted to an MDP by means of a transformation function that allows obtaining the environment state from the observations [1].\nA finite MDP is a tuple (S, A, P, R, \u03b3) [9] consisting of a finite set of states S, a finite set of actions A, a state transition probability matrix P, a reward function R, and a discount factor \u03b3.\nRL model-free algorithms can be classified into two main groups: value- based or policy-based methods [4], [10]. Let's focus on the second category, to which the algorithm applied in this research belongs. Policy-based methods"}, {"title": "3 Related work", "content": "RL approaches have traditionally failed to scale to high dimensional spaces like the ones modern artificial agents face when they need to learn from camera images that monitor the environment [12]. DRL overcomes the limitations of RL methods like memory and computational complexity thanks to the function approximation and representation learning properties that deep neural networks exhibit [13]. Therefore, the burst of DRL comes along with the growth of some deep learning algorithms such as Convolutional"}, {"title": "4 Materials", "content": "The experiments performed in this research make use of a set of virtual and physical elements listed in this section.\nDeep-learning computer\nThe hardware used to perform the experiments was a PC running Ubuntu 20.04 equipped with an Intel Core i9-10900KF processor at 3.70 GHz, 64 GB of DDR4 RAM, an NVIDIA GeForce RTX 2080 Ti GPU, and a 2TB M.2 SSD. The algorithms are programmed in Python 3.8, using PyTorch [47] to implement the advanced neural model in the core of the artificial agents and other libraries for basic computation, ancillary graphic services, etc. (e.g., NumPy, Matplotlib)."}, {"title": "5 Method", "content": "This section presents the general details of the research approach followed, the model design and training details, the virtual test bench used to perform robustness test cycles, and the analyses performed after them."}, {"title": "Research approach", "content": "summarizes the procedure carried out in this research. On the top branch, there is an artificial agent to control the robot following the PNN approach found in the literature. This is the reference model, coined Baseline model (BM) or Baseline agent, that allows quantifying the improvement introduced by the proposed approach. Then, a virtual test bench measures the robustness of this BM to changes in the camera setting, and finally, the results are structured and analyzed.\nAfterward, shown on the bottom branch, the same method is followed to obtain the robustness results of an improved artificial agent in which the training phase is enriched by adding variability, that is, applying DR to the camera position. For the sake of simplicity, only the camera position is used to represent discrepancies between the virtual and the real worlds, acknowledging that the model is sensitive enough to changes in this variable. We refer to this model as the DR model (DRM) or DR agent.\nThe results and insights extracted from comparing the outcomes of both models are presented in Section 6."}, {"title": "Training and model design details", "content": "Agent training is made up of episodes. At the beginning of each one, the target and robot positions are initialized randomly according to a uniform distribution whose limits are the working area for the target and the joints' degrees that prevent the robot position from exceeding the image limits, that is, a 15% of its operating range (Table 1). An episode ends when the number of steps reaches 50 or if the distance from the gripper to the target is lower or equal to 5 cm.\nThe discount factor (\u03b3) is fixed to 0.99 in the experiments since the agent should evaluate each action regarding the future rewards in this problem. The experience presented to the agent during training amounts to 70 million steps. Every 50 thousand steps, training is halted to perform 40 evaluation episodes and compute the average, maximum, and minimum distance from the gripper to the target position, obtaining the training curves that allow measuring the effectiveness of training, sample efficiency, etc.\nAs aforementioned, the BM is designed following a large extent of the PNN approach. However, an appreciable effort has also been put into studying the effect of the MDP's action space and the design of the reward signal to acquire a better understanding of the drivers of control performance. Although we"}, {"title": "Evaluation details: the virtual test bench", "content": "After the design and training phases, the control robustness is methodically measured by assessing the performance of the given model when it only uses the virtual column (i.e., right before including real experience into the lateral connections and real columns of the PNN architecture). Our test bench allows changing almost all parameters in the virtual environment. To make the training-test pipeline clearer: the agent is first trained with a virtual model; then, the agent's policy is extracted from the neural network and tested against"}, {"title": "6 Results and discussion", "content": "This section presents the BM results first, followed by the DRM right after, repeating the same fixed scheme for both models to enhance understanding. In each of these parts, the training and the robustness"}, {"title": "6.1 Baseline Model (BM)", "content": "Fig. 7 presents the training curve for the BM. The learning process meets the steady-stable regime in around 35 million steps. Note that around 40 million steps and 60 million steps, the agent seems to explore and exploit a sub-optimal policy, leading to a temporary decrease in the average reward. Under the conditions mentioned in Section 4 and Section 5, the training time was 14 hours."}, {"title": "Robustness", "content": "To better understand the accuracy results, the outcomes have been analyzed using a figure inspired in the orthographic projection system (Fig. 8). The matrix, which corresponds to the floor view, is a heat map that presents the average accuracy obtained after evaluating the BM across all combinations of the z- and the y-axes. On top of it, the average accuracy is displayed along the z-axis when the y-axis orientation is fixed to -30\u00b0, emulating a front view where the heat map surface intersects with a plane perpendicular to the y-axis at -30\u00b0. Finally, the left plot shows the average accuracy when the z-axis is kept at 180\u00b0. This graphic can also be regarded as the surface projection that results from the intersection between the heat map and a plane perpendicular to the z-axis at 180\u00b0.\nAlthough the Baseline agent was trained with a fixed camera position at (180\u00b0, -30\u00b0), it keeps the accuracy above 90% for the interval [165\u00b0, 195\u00b0] in the z-axis and [-35\u00b0, -25\u00b0] in the y-axis. It achieves 100% accuracy at (175\u00b0, -30\u00b0), (180\u00b0, -30\u00b0), and (185\u00b0, -30\u00b0), all of them with the height perspective in which the robot was trained. The maximum failure distance was 48 cm, registered at (140\u00b0, -25\u00b0) and at (220\u00b0, -50\u00b0). Both positions correspond to unfavorable scenarios, where the camera is visibly below and over the initial y-axis orientation.\nAccuracy around the z-axis is distributed almost symmetrically for all the combinations from -35\u00b0 to -20\u00b0 in the y-axis. However, the results obtained for [-50\u00b0, -40\u00b0] show that there is a divergence in the accuracy reached at [140\u00b0, 170\u00b0] with respect to the ones in [190\u00b0, 220\u00b0], especially at -40\u00b0. We ponder that this behavior is caused by the effect of the robot's shadow in the picture captured when the camera was clearly above -30\u00b0. This detail might increase in the difficulty of analyzing the input image since the contrast between the target's and the floor's colors, both affected by the shade, is reduced. This phenomenon produced a substantial decrease in the agent's performance. The same incident happened at -15\u00b0 and -10\u00b0 for the intervals [140\u00b0, 150\u00b0] when the outcomes are compared to those within [210\u00b0, 220\u00b0]. Apparently, the agent analyzes differently two pictures with the same camera"}, {"title": "6.2 Domain Randomization Model (DRM)", "content": "displays the training curve for the DRM. In this case, the steady- stable regime is reached at around 40 million steps, which is consistent with"}, {"title": "Robustness", "content": "As in the analysis of the BM, the results of the DRM performance have been analyzed using a representation inspired in the orthographic projection system. The matrix on the center of Fig. 10 displays a heat map whose values are the average accuracy obtained after evaluating the DRM across all the orientation combinations. On top of it, the DRM and BM average accuracy are exhibited along the z-axis when the y-axis orientation is kept to -30\u00b0, being this graphics the result of the intersection between each heat map and a plane perpendicular to the y-axis at -30\u00b0. Fig. 10's left plot presents the average accuracy when the z-axis is fixed at 180\u00b0. This surface projection emulates the intersection between each heat map and a plane perpendicular to the z-axis at 180\u00b0.\nOwing to the spread experience and thus, the diversity achieved during the learning period, the agent maintain the accuracy above 90% for approximately all the orientations between 155\u00b0 to 210\u00b0 and -40\u00b0 to -25\u00b0, which is clearly a larger range than that in the BM. 100% accuracy is obtained in 11 positions, most of them at -30\u00b0 and within the interval [165\u00b0, 200\u00b0]. The maximum failure distance was 81 cm at (140\u00b0, -10\u00b0), one of the most challenging environment configurations for the DRM.\nThe distribution of the average accuracy around the z-axis is more symmetrical than in the BM, especially in the y-axis intermediate degrees. Nevertheless, the shadow effect that appeared in the BM causing a considerable"}, {"title": "Training"}, {"title": "7 Conclusions and future work", "content": "This research compares the performance of an artificial agent trained with domain randomization (DR) techniques with respect to a baseline designed following a vanilla PNN approach. The setup used was a virtual environment composed of a robotic arm whose task is to arrive at a picking area using only the images provided by an externally-mounted camera as input. Both models were exposed to a robustness analysis where the camera orientation changed along two axes.\nThe results show that right before being transferred to reality in the sim- to-real pipeline (i.e., after the virtual training phase), the DR agent is more robust to moderate changes in the setup than the Baseline PNN model, at the expense of a slightly higher computational effort, if any. The conclusion holds even if two camera angles are changed simultaneously. This finding suggests that applying DR in the virtual environment should boost the vanilla PNN approach's performance and reduce the amount of training and experience required when transferring this knowledge to the real world, which will never exactly replicate the virtual setup. We leave for future work the quantification of the actual reduction of the total amount of real experience required to make agents fully operative and the point where adding more randomization starts yielding only marginal benefits."}, {"title": "Author contributions", "content": "Conceptualization and design of this study,\nMethodology, Formal analysis and investigation, Software, Data curation, Writing - original draft preparation, Writing - review and editing.\nConceptualization and design of this study, Methodology, Formal analysis and investigation, Supervision, Writing - review and editing.\nConceptualization and design of this study, Methodology, Formal analysis and investigation, Supervision, Writing - review and editing.\nLuc\u00eda G\u00fcitta-L\u00f3pez:\nJaime Boal:\n\u00c1lvaro J. L\u00f3pez-L\u00f3pez:"}, {"title": "Declarations", "content": "Funding: This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.\nConflict of interest/Competing interests: All authors certify that they have no affiliations with or involvement in any organization or entity with any financial interest or non-financial interest in the subject matter or materials discussed in this manuscript.\nEthics approval: Not applicable\nConsent to participate: Not applicable\nConsent for publication: Not applicable\nAvailability of data and materials: Researchers or interested parties are welcome to contact the corresponding author L.G-L. for further explanation, who may also provide the data upon request.\nCode availability: Researchers or interested parties are welcome to contact the corresponding author L.G-L. for further explanation, who may also provide the Python code upon request."}, {"title": "Appendix A Design of the Baseline model (BM)", "content": "The problem is solved as a fully observable MDP, where the state signal is the image resulting from rendering the virtual environment. The agent implemented is an A3C whose architecture is the same as that presented by Rusu et al. [5]. The agent's observation (i.e., the model input) is a 64x64 RGB environment image. There are seven outputs, being the Actor composed of six of them, which are the policies applied to each joint, that is, the discrete probability functions that map the probability of applying a certain action from the action-set to modify the joint's position. The softmax function is used to obtain the actions' likelihood. The other model output is the Critic, which determine the value of the value-state function. From the premise that the MDP design was fundamental, various models were developed and evaluated to select the most appropriate action set and reward strategy. In this research, the agent command the position of the actuators rather than their speed, as [5] implemented, since that is the attribute embedded controllers allow the user to interact within these types of commercial industrial robots. Hence, the actions are applied as increments of each joints' orientation. Therefore, the proposal made in [5] is used as the initial framework due to its encouraging results, but our problem characterization and goals differed.\nIn order to analyze the impact of the action sets and the reward on the agent's behavior, several models with different reward and action set configurations were designed and trained"}]}