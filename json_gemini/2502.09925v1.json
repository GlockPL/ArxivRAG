{"title": "TASKGALAXY:\nSCALING MULTI-MODAL INSTRUCTION FINE-TUNING\nWITH TENS OF THOUSANDS VISION TASK TYPES", "authors": ["Jiankang Chen", "Tianke Zhang", "Changyi Liu", "Haojie Ding", "Yaya Shi", "Feng Cheng", "Huihui Xiao", "Bin Went", "Fan Yang", "Tingting Gao", "Di Zhang"], "abstract": "Multimodal visual language models are gaining prominence in open-world appli- cations, driven by advancements in model architectures, training techniques, and high-quality data. However, their performance is often limited by insufficient task- specific data, leading to poor generalization and biased outputs. Existing efforts to increase task diversity in fine-tuning datasets are hindered by the labor-intensive process of manual task labeling, which typically produces only a few hundred task types. To address this, we propose TaskGalaxy, a large-scale multimodal instruc- tion fine-tuning dataset comprising 19,227 hierarchical task types and 413,648 samples. TaskGalaxy utilizes GPT-40 to enrich task diversity by expanding from a small set of manually defined tasks, with CLIP and GPT-4o filtering those that best match open-source images, and generating relevant question-answer pairs. Multiple models are employed to ensure sample quality. This automated pro- cess enhances both task diversity and data quality, reducing manual interven- tion. Incorporating TaskGalaxy into LLaVA-v1.5 and InternVL-Chat-v1.0 models shows substantial performance improvements across 16 benchmarks, demonstrat- ing the critical importance of task diversity. TaskGalaxy is publicly released at https://github.com/Kwai-YuanQi/TaskGalaxy.", "sections": [{"title": "INTRODUCTION", "content": "Recent breakthroughs in artificial intelligence have been fueled with the development of a large\nnumber of large multimodal models (LMMs) (Liu et al., 2024b; Bai et al., 2023; Liu et al., 2024a;\nZhang et al., 2024c). These models are typically composed of a pre-trained visual encoder (Radford\net al., 2021), a pre-trained large language model (Touvron et al., 2023a), and a lightweight structure\n(Q-former for BLIP2 (Li et al., 2023c), two layers of MLP for LLaVA (Liu et al., 2024b), etc.)\nconnecting the above two, which have been adopted in various domains such as image captioning,\nobject detection, visual question answering and other related fields. How to improve the model's\nperformance in various mission scenarios is of great importance for deploying such a model into an\nopen-world system.\nTo enhance the performance of LMMs in specialized and general-purpose domains, mainstream ap-\nproaches focus on three key areas: optimizing model architectures (Alayrac et al., 2022; Wang et al.,\n2023; Zhang et al., 2023a; Li et al., 2023c; Liu et al., 2024a), improving training strategies (Wang\net al., 2024; Sun et al., 2023; Banerjee et al., 2024; Zhu et al., 2024), and constructing high-quality\ndata (Xu et al., 2024; Chen et al., 2023b; Shi et al., 2024). While advances in model architectures\nand training strategies are crucial, our work focuses on addressing the critical challenges in the data\ndomain. Current multimodal models typically undergo a biphasic training process: a pre-training\nphase with image-text pairs for visual-textual alignment, followed by a supervised fine-tuning (SFT)\nphase with command-format data to refine multimodal abilities. However, the diversity of tasks in\nthe pre-training phase is limited, affecting the generalization ability of visual language models (Xu\net al., 2023). Recent research (Hu et al., 2024; Li et al., 2024b; Shi et al., 2024; Xu et al., 2023;\nYin et al., 2024; Liu et al., 2024b; Zhao et al., 2024) has focused on expanding task diversity in the"}, {"title": "TASKGALAXY DATASET", "content": "The TaskGalaxy dataset consists of 19,227 hierarchical task types, ranging from OCR and image\ndescription to fine-grained object recognition and complex logical reasoning and so on. These task\ntypes originate from a small set of manually defined seeds, further expanded by GPT-40 through\nprompt design. CLIP's graphical similarity is used to filter task types that best match specific im-\nages. GPT-40 then generates corresponding question-answer pairs, which are evaluated by three\nopen-source models to select the highest-quality samples. This process yields 413,648 high-quality\nquestion-answer pairs aligned with these hierarchical task types."}, {"title": "DATASET GENERATION PIPELINE", "content": "In this section, we outline the pipeline for creating the TaskGalaxy dataset, highlighting the reduc-\ntion of human intervention and leveraging multiple advanced LLMs. The process encompasses task"}, {"title": "EXPERIMENT", "content": null}, {"title": "EXPERIMENT SETUP", "content": "Model Architecture. In the matching and filtering stage, the CLIP-L/14Radford et al. (2021) model,\ndeveloped by OpenAI, is employed. This model utilizes a ViT-L/14 Transformer architecture as the\nimage encoder and a masked self-attention Transformer as the text encoder. In the stage of evaluating\nthe TaskGalaxy dataset, We use the LLaVA (Liu et al., 2024b) and InternVL-Chat-v1.0 (Chen et al.,\n2023c) models. Both models feature a pre-trained visual encoder and a large language model, linked\nby a two-layer MLP projection layer. LLaVA employs a two-stage training: initially, a subset of the\nCC3M (Sharma et al., 2018) dataset pretrains only the projection layer for multimodal alignment.\nWe use the model pre-trained in this phase as the basis for fine-tuning in the subsequent phase. For\nvalidation, we selected two variants: LLaVA-v1.5-7B and LLaVA-v1.5-13B. Similarly, InternVL-\nChat-v1.0 undergoes two training phases: first, the MLP layers are trained with the LGS-558K (Liu\net al., 2024b) dataset, followed by training the language model with the LLaVA-Mix-665K (Liu\net al., 2024a) dataset."}, {"title": "QUANTITATIVE COMPARISON", "content": "Table 2 presents a quantitative comparison of LLaVA-v1.5 and InternVL-Chat-v1.0 models trained\non the original fine-tuned data versus those fine-tuned with TaskGalaxy. The new models show\nimprovements of 4.5 and 3.83 points across all 15 benchmarks for LLaVA-v1.5-7B and 13B, re-\nspectively, excluding MME. For InternVL-Chat-v1.0, the improvements are 3.0 and 3.64 points. It\nis worth noting that LLaVA-v1.5-13B sees a performance increase of 68 points with TaskGalaxy on\nthe MME Benchmark.\nTaking LLaVA-v1.5-7B as an example, we observed a 3.35% and 3.1% improvement over the origi-\nnal baseline on MMBench and MMBench_CN, respectively. For LLaVA-in-the-wild, we achieved a\n3.3 points increase, demonstrating that the TaskGalaxy fine-tuning dataset enhances the model's per-\nformance in detailed description and complex reasoning tasks. Notably, incorporating TaskGalaxy\nresulted in improvements of 0.77, 1.75, 4.7, 5.48, and 12.94 points on TQA, SQA, MathVista,\nChartQA, and AI2D, respectively, highlighting the dataset's broad coverage. On hallucination mit-\nigation tasks, improvements of 1 to 2 points on POPE and HalluBench suggest that a diverse range\nof tasks helps address hallucination issues. Additionally, on the SEED benchmark, which includes\n12 evaluation latitudes and 19k questions, there was a modest 1.7 points improvement over the\nmodel trained solely on raw fine-tuned data. In low-level image evaluation tasks, Q-Bench and\nChinese-Q-Bench, models fine-tuned with TaskGalaxy showed gains of 17.5 and 1.27 points, re-"}, {"title": "ABLATION STUDY", "content": "The number of task types. The primary objective of TaskGalaxy is to enhance the generalization\ncapabilities of multi-modal models by encompassing a broad array of visual-language task types.\nTaskGalaxy includes a diverse set of 19,227 distinct task types. In this subsection, we examine how\nthe number of task types affects the performance of multimodal models. We selected task types in\nincrements of 2k, 3k, 5k, 10k, 15k, 18k, and 19,227 from TaskGalaxy, maintaining a constant total\nof 100k images, and conducted ablation experiments using the LLaVA-v1.5-7B model. As shown\nin Figure 6(left), benchmarks such as LLaVA-in-the-wild, ChartQA, AI2D, Q-Bench, and MMMU\nconsistently improved with an increasing number of task types. The 'Average' performance across\nthe 15 benchmarks, excluding MME, also shows a clear trend of enhancement with more task types,\nwhich is corroborated by MME performance changes in Figure 7. These results highlight the critical\nrole of task type diversity in enhancing the capabilities of modern multimodal models.\nThe number of samples. In addition to the impact of the number of tasks, it is well established\nthat the amount of sample data in the instruction fine-tuning dataset also significantly affects model\nperformance. To investigate this, we conducted ablation experiments to assess the effect of varying\ndata volumes on model performance. As depicted in Figure 6(right), we included all task types and\ncontrolled the variation in total sample size by setting the maximum number of samples for each task\ntype, ranging from 5 up to 55 which corresponds to the final TaskGalaxy dataset. The results show\nthat for benchmarks such as MMVeT, ChartQA, AI2D, and Q-Bench, as well as the average perfor-"}, {"title": "RELATED WORK", "content": "Large Multi-modal Models. With the rapid advance- ment of large language models (LLMs), such as GPT- 3 (Brown et al., 2020), LLama2 (Touvron et al., 2023b), InternLM (Team, 2023), and Baichuan 2 (Yang et al., 2023), there has been a growing focus on integrating vi- sual knowledge into LLMs, exemplified by models like CLIP (Radford et al., 2021), BLIP (Li et al., 2022), and BLIP2 (Li et al., 2023c). While these models ex- hibit strong performance in graphic alignment and im- age captioning, they continue to face significant chal- lenges in handling more complex visual question answer- ing tasks. To enhance the model's instruction adherence and content understanding in visual question answering (VQA), visual instruction fine-tuning strategies have gar- nered increasing attention in the training of large multi- modal models. For instance, models like LLaVA (Liu et al., 2024b), MiniGPT-4 (Zhu et al., 2023), and Instruct- BLIP (Dai et al., 2023) leverage large language models from the GPT4 (Achiam et al., 2023) family to gener- ate fine-tuned instruction data, thereby enhancing per- formance in complex VQA scenarios. Furthermore, to expand the range of VQA task scenarios, recent models such as LAMM (Yin et al., 2024) and MIMIC-IT (Li et al., 2023a), following the example of LLaVA, have extended their VQA capabilities to encompass 3D scenarios, multi-graph tasks, videos, and other complex domains. Recently, a series of open-source large multi-modal models with enhanced performance have consistently outperformed existing benchmarks. Notable examples include GLM- 4v (GLM et al., 2024), Qwen-VL (Bai et al., 2023), InternLM-XComposer-2.5 (Zhang et al., 2024b), and InternLM2 (Zhang et al., 2024c), which are leading the field in various multimodal tasks. In addition to open-source models, recently developed closed-source models such as GPT-4v (OpenAI, 2023), GPT-40 (Hurst et al., 2024), and Claude-3.5 (Anthropic, 2024) continue to lead the field, of- ten matching or surpassing open-source models in various VQA tasks. To facilitate comprehensive"}, {"title": "CONCLUSION", "content": "In this study, we present TaskGalaxy, a multi-modal instruction fine-tuning dataset comprising ap- proximately 20,000 multi-modal task types and around 410k instruction Q&A samples. Addition- ally, we propose a pipeline for the systematic construction and generation of a diverse range of task types and corresponding high quality instruction Q&A samples. This approach addresses the limitations of existing multi-modal instruction fine-tuning datasets, particularly the narrow scope of task types and the excessive reliance on human intervention. TaskGalaxy encompasses an exten- sive range of multimodal visual Q&A tasks and offers a highly extensible pipeline that facilitates the addition of new task types and the generation of high-quality fine-tuned instructional data. We fine-tuned the LLaVA-v1.5 and InternVL-Chat-v1.0 models using TaskGalaxy, resulting in a signif- icant improvement compared to using only raw fine-tunning data, respectively. Adequate empirical evaluation confirms the effectiveness of our broader task type data in enhancing the performance of multimodal models, highlighting the critical importance of task type diversity in the instruction fine-tuning dataset. We hope that our approach of constructing a dataset with a broad range of task types and reduced manual labor will guide future development of multi-modal instruction fine-tuning datasets and we plan to make the dataset publicly available for community research."}, {"title": "ETHICS STATEMENT", "content": "This study upholds rigorous ethical standards to ensure the credibility and confidentiality of the findings. All data underwent thorough de-identification procedures to protect privacy and main- tain anonymity. The study followed ethical guidelines and obtained informed consent from partici- pants while prioritizing their rights and autonomy. Transparency and accountability were maintained throughout the research process to minimize biases and conflicts of interest. No academic ethical issues or misconduct were encountered, and the authors affirm their unwavering commitment to upholding ethical research practices and promptly addressing any unintentional errors or oversights."}, {"title": "APPENDIX", "content": null}, {"title": "OVERALL REVIEW OF IMAGE SOURCES", "content": "Considering the accessibility of data sources and the task-related nature of the image data we aim to mine, we have opted for open-source image data. The approximate data sources and their corre- sponding sample sizes are presented in Table 1 of the main text. To provide further insight into the image data, the Table A-1 presents the statistics of the sample sizes for the different data sources collected."}, {"title": "MORE DETAILS ON THE GENERATION PIPELINE PROCESS OF TASKGALAXY", "content": "About Prompt: In the TaskGalaxy dataset pipeline, the first step involves using GPT-40 to continu- ously expand new task types based on a set of human-defined task type seeds. This process requires designing distinct prompts for different levels of task types and determining whether lower-level task types exist within each hierarchy, allowing GPT-40 to systematically expand and populate the dataset. For level-1 task types, we focus solely on extending the existing prompt, as detailed in the main text. For generating two-level task types, the approach varies depending on whether the one-leveltask types have corresponding two-level tasks; prompts are designed accordingly to either continue expansion or generate new prompts. The same methodology applies to three-leval tasks. The detailed prompts are provided in Table A-2.\nAfter generating a large number of hierarchical task types and collecting a substantial amount of open-source image data, the third step in our pipeline involves matching and filtering. Following the image-text cosine similarity matching conducted by CLIP, we proceed to further refine the selection of task types that are most compatible with specific images. In this stage, we employ a specially designed prompt for GPT-40, denoted as p_filter in Table A-3, to filter and select the task types that best match each particular image.\nAfter completing the matching and filtering of task types and images in the third stage, the fourth stage involves generating question-answer pairs related to all task types matched with the images. The prompt templates guiding GPT-40 to generate these task-type-related answer texts are denoted as p_Q&A in Table A-3.\nAfter generating all the Q&A samples related to the task types, to further refine the selection of task types and questions that best match the images and ensure higher quality, we employ three open-source models in the final stage. This step focuses on filtering the images, task types, and their corresponding questions to identify the most suitable and coherent matches, while also considering"}, {"title": "ILLUSTRATIONS AND ANALYSIS OF SAMPLES FILTERED DURING THE DATA\nGENERATION PIPELINE", "content": "In TaskGalaxy generation pipeline, there are two parts involved in matching and screening, and in the first part, we employ a two-phase process for matching and screening task types and images. In the first phase, task types are generated and open-source image data is collected. We use CLIP to perform an initial screening, matching task types with images based on their textual and visual similarity. However, the performance of CLIP's image-text matching is inherently limited. This sometimes leads to overestimation of similarity scores, resulting in mismatches where task types are paired with images that do not accurately represent their content. To address this limitation, the second phase involves leveraging GPT-40 with carefully designed prompts to refine the matches. This step effectively filters out task types that are not contextually related to the content of the images. Below, we provide a comparison illustrating the image-task pairings before and after the second-phase refinement by GPT-40, demonstrating the improvement in alignment between task types and image content."}, {"title": "LICENCES OF DATA", "content": "The licensing information for the image sources listed in Table 1 is as follows: ALLaVA (Apache License 2.0), Visual Genome (CC BY 4.0), MathV360K (Apache License 2.0), and ShareGPT4V (CC BY-NC 4.0). The proposed dataset, upon its open-source release, will be licensed under CC BY-NC 4.0.\nWe acknowledge our responsibility for ensuring legal compliance in data usage. The dataset licenses have been carefully reviewed, and our release under CC BY-NC 4.0 aligns with the restrictions of certain sources. Steps have been taken to mitigate potential legal risks and ensure adherence to the respective terms."}, {"title": "EVALUATION REGULATIONS", "content": "For the evaluated benchmarks MME, MMBench, MMBench_CN, MM-VeT, POPE, SEED, SQA, and TextVQA, we utilized the official evaluation code provided by LLaVA. For AI2D, ChartQA, HallusionBench, LLaVA-in-the-wild, MMMU, Q-Bench, and Chinese-Q-Bench, we referred to the evaluation code that follows the official evaluation protocol of InternLM-XComposer. The Math- Vista evaluation baseline was conducted using the official evaluation code of MathVista. In bench- marks such as MM-VeT, LLaVA-in-the-wild, and MathVista, we replaced the original GPT-4 API with GPT-40, which offers more stringent criteria and improved performance for scoring, answer extraction, answer matching, and related tasks."}, {"title": "EXPERIMENTAL EVALUATION USING TASKGALAXY, BASELINE DATASET, AND OTHER\nINSTRUCTION-TUNING DATASETS INDIVIDUALLY", "content": "To demonstrate the impact of TaskGalaxy's task diversity on model performance, we compare the baseline fine-tuning data with TaskGalaxy fine-tuning data, as well as several other instruction- tuning datasets, including ShareGPT-4V Chen et al. (2023b), LLaVA-OneVision Li et al. (2024a), ALLaVA-4V Chen et al. (2024a), and Cambrian-1 Tong et al. (2024). For a fair comparison, we ran- domly sampled the same number of samples from each dataset as in TaskGalaxy for fine-tuning. The results, summarized in Table A-5, demonstrate that TaskGalaxy consistently achieves the highest performance on most benchmarks across multiple model architectures, validating its effectiveness."}, {"title": "MORE ADVANCED MODEL ARCHITECTURE", "content": "For more advanced models, we utilize the InternVL-Chat-V2.0-8B model, which has made its second-stage instruction fine-tuning data publicly available. For the comparison, we randomly sam- ple the same number of samples as TaskGalaxy from the officially disclosed instruction fine-tuning dataset.\nWe fine-tune InternVL-Chat-V2.0-8B using both the original instruction fine-tuning dataset and the TaskGalaxy instruction fine-tuning dataset, ensuring that the number of samples for each is consistent with TaskGalaxy. The Table A-6 shows the performance comparison between the original instruction fine-tuning dataset and the TaskGalaxy instruction fine-tuning dataset on InternVL-Chat- V2.0-8B."}, {"title": "THE BENEFITS OF CHAIN-OF-THOUGHT(COT)", "content": "Numerous studies (Zhang et al., 2023b; Zheng et al., 2023; Zhang et al., 2024a; Mitra et al., 2024) have highlighted the significant impact of Chain-of-Thought (CoT) prompting on enhancing MLLM performance. In this section, we investigate whether CoT prompting improves performance with in- creased task types. We constrained the maximum number of samples per task type to 5, resulting in a total of 76k samples (as indicated by max_5 in Table A-7). We compared the performance using original TaskGalaxy Q&A data with CoT-generated answers from GPT-40, designed through specific prompts detailed in the Appendix. The results show significant improvements for the CoT versions of TaskGalaxy (max_5) in benchmarks such as MME, LLaVA-in-the-wild, and Q-Bench. Additionally, the average performance across 15 benchmarks, excluding MME, increased by approx- imately 1.3 points with CoT. These findings underscore the value of incorporating CoT prompting into multimodal models."}, {"title": "TASK TYPES IN TASKGALAXY", "content": "One of the key challenges addressed by the TaskGalaxy instruction fine-tuning dataset is the sub- stantial expansion of task type diversity. Initially, we manually defined a small set of task types,"}], "equations": ["sj = I(xi) \u00b7T(tj),"]}