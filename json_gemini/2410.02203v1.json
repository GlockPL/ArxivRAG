{"title": "GRAPHIC: A GRAPH-BASED IN-CONTEXT EXAMPLE RETRIEVAL MODEL FOR MULTI-STEP REASONING", "authors": ["Jiale Fu", "Yaqing Wang", "Simeng Han", "Jiaming Fan", "Chen Si", "Xu Yang"], "abstract": "In-context learning (ICL) enables large language models (LLMs) to generalize to new tasks by incorporating a few in-context examples (ICEs) directly in the input, without updating parameters. However, the effectiveness of ICL heavily relies on the selection of ICEs, and conventional text-based embedding methods are often inadequate for tasks that require multi-step reasoning, such as mathematical and logical problem solving. This is due to the bias introduced by shallow semantic similarities that fail to capture the deeper reasoning structures required for these tasks. We present GraphIC, a novel approach that leverages graph-based representations of reasoning processes, coupled with Bayesian Networks (BNs) to select ICEs. Graph structures inherently filter out shallow semantics while preserving the core reasoning structure. Importantly, BNs capture the dependency of a node's attributes on its parent nodes, closely mirroring the hierarchical nature of human cognition-where each thought is shaped by preceding ones. This makes BNs particularly well-suited for multi-step reasoning tasks, aligning the process more closely with human-like reasoning. Extensive experiments across three types of reasoning tasks (mathematical reasoning, code generation, and logical reasoning) demonstrate that GraphIC outperforms both training-free and training-based models in selecting ICEs, excelling in terms of both effectiveness and efficiency. We show that GraphIC enhances ICL's performance and interpretability, significantly advancing ICE selection for multi-step reasoning tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "In-context learning (ICL) (Brown et al., 2020) represents a paradigm in how large language models (LLMs) perform inference by using a small number of in-context examples (ICEs) within the input prompt. This technique enables LLMs to generalize to new tasks or enhance their performance on existing tasks without updating parameters. However, previous studies have highlighted the sensitivity of ICL performance to the specific ICEs selected (Zhao et al., 2021; Liu et al., 2022), underscoring the importance of strategic ICE selection. Consequently, numerous methods have been proposed to optimize the selection of ICEs, focusing on improving task performance and ensuring greater robustness (Liu et al., 2022; Rubin et al., 2022; Ye et al., 2023; Gupta et al., 2024). These methods frequently rely on text-based embeddings, where both the query and candidate ICEs are embedded using a language encoder, with similarity scores guiding the selection process.\nCurrent text-based embedding selection methods primarily focus on capturing semantic-level similarity, demonstrating their utility in tasks such as sentiment analysis (Liu et al., 2022) and machine translation (Agrawal et al., 2023). However, these approaches encounter significant limitations in multi-step mathematical and logical reasoning tasks, such as GSM8K (Cobbe et al., 2021) and ProofWriter (Tafjord et al., 2021). The core issue lies in the fact that textual data often encodes a substantial amount of shallow semantic information, which is largely irrelevant to the underlying reasoning processes required for math and logic tasks. This extraneous information introduces bias in the selection of ICEs (An et al., 2023), and can even lead the LLM to adopt misleading reasoning strategies, thereby degrading task performance. For example, in a problem involving speed calculation (i.e., determining the rate of change of distance over time), text-based embeddings may"}, {"title": "2 RELATED WORK", "content": "Existing ICE selection techniques can be classified as either training-free or training-based, depending on whether a retriever needs to be trained.\nTraining-free approaches are generally divided into two types: (i) those that use heuristic criteria such as similarity (Liu et al., 2022; Hu et al., 2022), diversity (Cho et al., 2023; Zhang et al., 2022b; Levy et al., 2023; Hongjin et al., 2022; Zhang et al., 2023), complexity (Fu et al., 2022), or combinations of these (Agrawal et al., 2023; Tonglet et al., 2023; Gupta et al., 2023) to select in-context examples (ICEs); (ii) those that leverage feedback from LLMs, such as probability distributions (Wu et al., 2023; Nguyen & Wong, 2023; Li & Qiu, 2023; Yang et al., 2023), perplexity (Gonen et al., 2023), or the model's generated output (An et al., 2023) to guide the selection process. While training-free approaches avoid the computational and time overhead associated with model training, their relatively simplistic architecture often results in sub-optimal performance compared to training-based methods.\nTraining-based methods are typically divided into two main categories. The first learns to select individual examples and then extends this to k-shot scenarios (Rubin et al., 2022; Xiong et al., 2024; Gupta et al., 2024). The second models the selection of a group of examples as a whole (Ye et al., 2023; Wang et al., 2023; Zhang et al., 2022a; Scarlatos & Lan, 2023; Lu et al., 2022; Peng et al., 2023; Xu et al., 2024). While training-based approaches usually achieve superior performance, their reliance on repeated LLM queries and model training makes them both computationally intensive and time-consuming.\nOur proposed GraphIC method is not only training-free and inherently efficient but also incorporates an advanced graph-based example retriever specifically designed for multi-step reasoning tasks. This sophisticated design enables GraphIC to achieve a significant performance advantage, even surpassing training-based methods."}, {"title": "3 PRELIMINARIES: BAYESIAN NETWORK", "content": "A Bayesian Network (BN) (Pearl, 2014) is a probabilistic graphical model that represents conditional dependencies among random variables via a directed acyclic graph (DAG). In a DAG G = (V, E), V = {v1,..., vn} denotes the vertices corresponding to random variables, and E denotes the conditional dependencies. Each vertex vi is associated with a random variable Xi, and the joint probability distribution is factorized as:\n$\\displaystyle p(x_1, x_2,...,x_n) = \\prod_{i=1}^{n} P(x_i|pa(v_i)),$ (1)\nwhere xi \u2208 Rnf denotes the value of the random variable Xi, pa(vi) refers to the set of parent variables for vi, and p(xi|pa(vi)) is typically modeled as:\n$\\displaystyle p(x_i|pa(v_i)) = g(dist(x_i, \\hat{x}_i)),$ (2)\nwith $\\hat{x}_i = Wz_i$ and $z_i = f(pa(v_i))$. Here, $\\hat{x}_i$ represents the predicted value of xi based on zi, where zi aggregates information from the parent nodes pa(vi). The weight matrix W is used to predict $\\hat{x}_i$, f(.) denotes the aggregation function, dist(\u00b7,\u00b7) is a distance metric between xi and $\\hat{x}_i$, and g(\u00b7) is a function that satisfies: 1). monotonicity: g'(u) \u2264 0 for u > 0; and 2) normalization: $\\int g(dist(x, x_i))dx = 1$.\nGiven the aggregated features $Z = (z_1,...,z_n)^\\mathsf{T}$ where T denotes the transpose operation, organizing the individual feature vectors zi into a matrix where each row corresponds to a feature vector, along with the distance function dist(\u00b7,\u00b7) and function g(\u00b7), the joint probability density of the dataset $X = (x_1,x_2,...,x_n)$ can be computed."}, {"title": "4 THE PROPOSED GRAPHIC", "content": "In this work, we propose a novel approach called GraphIC for representing the problem-solving process through a graph-based model, intending to select examples that maximize the probability density of capturing the correct reasoning process. First, we introduce \"thought graphs\", a formal"}, {"title": "4.1 THOUGHT GRAPH AND ITS CONSTRUCTION", "content": "We begin by introducing the concept of a thought graph and its construction, inspired by the hierarchical structure of human cognition in problem-solving. The human thought process, especially when addressing complex problems, can be naturally modeled as a graph-like structure (Friston, 2008; Besta et al., 2024; Yao et al., 2023). In this work, we present the \"thought graph\" as a structure for modeling the cognitive process of LLMs during multi-step reasoning tasks. Formally, a thought graph G is represented as a vertex-attributed graph, where each vertex is associated with a natural language text, corresponding to the description of the operation performed or the intermediate conclusion reached at that step. To facilitate computation, we further represent the vertex attributes as the BERT embedding (Devlin et al., 2019) of corresponding text, denoted as xi.\nSince LLMs are not natively equipped to output graph structures, we propose a methodology to generate these graphs from LLM outputs. As illustrated in Figure 2, we prompt the LLM to generate a \"formalized reasoning representation\u201d (FRR), which is subsequently parsed to construct the thought graph. The detailed prompt and parser pseudo-code are provided in Appendix D.\nAfter constructing the thought graph, a straightforward way to select in-context examples (ICEs) is to compare the similarities between graph embeddings. To compute these embeddings, we employ a widely adopted method where each graph embedding is generated through iterative aggregation of node information, as outlined by Togninalli et al. (2019). Specifically, this process is formalized as:\n$X^{h+1} = AX^h, X^0 = X = (x_1, x_2,...,x_n),$ (3)\nwhere $A = \\tilde{D}_A^{-1}(A+I)\\tilde{D}_A^{-1}, \\tilde{D}_A = 1+\\sum_j A_{ij}$. A represents the adjacency matrix of the thought graph, where Aij = 1 indicates a directed edge from node vi to node vj, and Aij = 0 otherwise.\nWhile this approach effectively captures the structural properties of human thought as represented in the graph, it is constrained by its focus on graph similarity alone. Importantly, selecting an example based solely on the similarity does not necessarily optimize the possibility of an LLM generating a correct reasoning trajectory. To overcome this, we further propose a novel example retrieval model that prioritizes the optimization of the probability density of producing a correct reasoning process detailed in the next subsection, moving beyond a mere reliance on graph similarity."}, {"title": "4.2 PROBABILISTIC MODEL ON THOUGHT GRAPH", "content": "Building on the method for constructing thought graphs, we now turn to developing a probabilistic model for this structure. BNs model the dependencies of a node's attributes on its parent nodes, which closely mirror the way human cognition functions\u2014where new thoughts are informed by prior ones (Oaksford & Chater, 2007; Jacobs & Kruschke, 2011). This makes them a well-suited framework for modeling the thought graphs. In this section, we outline the construction of BNs for thought graphs. As described in Section 3, calculating the joint probability density on the thought graph requires the aggregate feature Z, the distance metric dist(\u00b7,\u00b7), and the function g(\u00b7). We now provide a detailed discussion of how each of these components is constructed.\nComputing the Aggregated Feature Z. Traditional BNs, which rely on the Markov assumption that a node's feature distribution depends solely on its parent nodes, are insufficient for modeling a thought graph where reasoning often requires referencing multiple prior steps. For example, problem solvers may need to iteratively review information from earlier steps or return to the beginning to re-examine the entire reasoning process. To address this limitation, we first employ an iterative aggregation mechanism that better captures the human reasoning processes. This iterative approach is formalized in Equation (3). Next, inspired by the Personalized PageRank (PPR) algorithm (Page, 1999; Gasteiger et al., 2018), we refine this method to more accurately simulate the flow of information during problem-solving. The PPR framework models a random walk where a user transitions between web pages with some probability of returning to the start. This closely parallels the cognitive process in complex problem-solving, where solvers often revisit initial hypotheses to reassess"}, {"title": "4.3 PROBABILISTIC EXAMPLE RETRIEVAL", "content": "As outlined in Section 4.2, the parameter W is meant to capture and represent the underlying structure of reasoning or connections between different concepts or ideas within the thought graph. The task of computing the probability density of generating a particular thought graph given W can be interpreted as evaluating the likelihood of producing the associated reasoning process based on the thought pattern encoded within W. Building on this idea, we design an example retrieval mechanism that estimates the model parameters for each candidate example and prioritizes those that maximize the probability density of the thought graph corresponding to the query. These selected examples serve as ICEs, offering the highest potential for accurately solving the problem at hand.\nEstimation of Model Parameters. We estimate the parameter matrix W by maximizing the likelihood function Lw of the thought graph features, which is computed as\n$\\displaystyle L_W = \\prod_{i=1}^{n} p(x_i|G), \\text{ log } L_W = \\sum_{i=1}^{n} \\text{log} p(x_i|G).$ (9)\nTo simplify the computation, this reduces to the following:\n$\\displaystyle \\text{log } L_W = \\sum_{i=1}^{n} \\text{log } C_i + \\sum_{i=1}^{n} [-(1-\\hat{x}_i^\\mathsf{T} x_i)] = -\\sum_{i=1}^{n} \\text{log } C_i - nl + tr(ZW^\\mathsf{T}X^\\mathsf{T}).$ (10)\nHence, maximizing Lw is equivalent to maximizing tr(ZWTXT), formally expressed as:\n$\\displaystyle \\max_W tr(ZW^\\mathsf{T}X^\\mathsf{T}), W \\in \\mathbb{R}^{n_f \\times n_f}, s.t. ||W||_F = (\\sum_{i=1}^{n_f} \\sum_{j=1}^{n_f} W_{ij}^2)^{\\frac{1}{2}} = 1.$ (11)\nThis constraint ensures that the magnitude of W does not influence the optimization.\nTypically, the number of vertices n in the thought graph is much smaller than the embedding dimensionality nf (i.e., n < nf). For instance, in the GSM8K dataset, thought graphs often contain fewer"}, {"title": "5 EXPERIMENTS", "content": "We conduct a comprehensive evaluation of GraphIC model across four multi-step reasoning benchmarks: two for mathematical reasoning (GSM8K (Cobbe et al., 2021) and AQUA (Ling et al., 2017)), one for code generation (MBPP (Austin et al., 2021)), and one for logical reasoning (ProofWriter (Tafjord et al., 2021)). For both GSM8K and MBPP, we utilize the original datasets without further preprocessing. For AQUA and ProofWriter, we refine the original dataset to improve the experimental setup, as detailed in Appendix A.\nFor GSM8K, AQUA, and ProofWriter, model performance is evaluated based on the accuracy of the LLMs' final answers. For MBPP, we adopt the pass@1 metric (Chen et al., 2021) to assess the quality of code generation.\nWe employ GPT-40-mini and Llama-3.1-8B-Instruct as LLMs. Unless explicitly mentioned otherwise, all evaluations are conducted under an 8-shot paradigm. We set the temperature to le-5. We set iterations H in Equation 5 to 3, with \u03bb values from {0,0.1, 0.2, 0.3}, based on the LLM and dataset (see Appendix D for details). For GSM8K, AQUA, and ProofWriter, we prompt the LLM to create a formalized reasoning representation (FRR) for thought graph construction, using vertex features from a BERT model. For MBPP, we use the staticfg module to parse Python code and generate the control flow graph, embedding each vertex's features with CodeBERT (Feng et al., 2020). Variable names in the code are anonymized with arbitrary symbols like 'a', 'b', and 'c'."}, {"title": "5.2 BASELINES", "content": "Our model, GraphIC, is designed as a training-free retriever for ICE selection. We compare GraphIC against six training-free retrieval methods spanning random, similarity-based, diversity-based, and complexity-based approaches, including: 1) Random randomly selects k unique ICEs from the candidate set; 2) BM25 (Robertson et al., 2009) selects the top k examples based on BM25 scoring; 3) BERT (Devlin et al., 2019) is a dense retriever using cosine similarity with BERT-base-uncased embeddings; 4) Complex-CoT (Fu et al., 2022) selects k examples based on complexity, quantified by newline characters; 5) Auto-CoT (Zhang et al., 2022b) clusters candidates and selects the closests to each cluster center; and 6) Skill-kNN (An et al., 2023) prompts LLM to generate task-relevant skills for query and candidates, followed by dense retrieval. Since Skill-kNN does not natively support datasets like GSM8K, we manually craft the instructions and examples, detailed in Appendix C.\nWe also compare with four training-based retrievers, which encompass both single-example and combination-example retrieval strategies, including: 1) EPR (Rubin et al., 2022) is trained to retrieve the single most relevant ICE, with top k examples being selected during inference;"}, {"title": "5.3 MAIN RESULTS", "content": "Figure 3 illustrates the thought graphs corresponding to each dataset. Table 1 evaluates our GraphIC model against 10 baselines across two LLMs and four datasets. As a training-free method, GraphIC consistently outperforms both training-free and training-based baselines in most settings. With the GPT-40-mini model, GraphIC achieves the highest performance, averaging 2.57% above the leading training-free model and 1.18% above the best training-based model. For the Llama-3.1-8B-Instruct model, GraphIC ranks first in three out of four datasets, with an average gain of 4.29% over the top training-free competitor and 2.5% over the strongest training-based method. Our analysis shows that the GraphIC model significantly enhances performance in mathematical and logical reasoning tasks versus code generation, especially for complex problems. For instance, in the GSM8K dataset, GraphIC outperforms all baselines by an average of 0.65% and 3.57% with two LLMs. In the more challenging AQUA dataset, improvements rise to 3.47% and 7.64%."}, {"title": "5.4 ABLATION STUDY", "content": "We perform a series of ablation studies to systematically evaluate the contribution of each component within the GraphIC framework, which is built upon three key pillars: the incorporation of thought graphs, PPR for aggregating features, and BN-based retrieval.\nTo this end, we develop several variants of the GraphIC model: 1) Text relies solely on text embeddings, the same as the BERT approach; 2) FRR retrieves examples using BERT embeddings derived from FRRs (or CodeBERT embeddings for the MBPP dataset); 3) Graph utilizes the formula (3) to generate graph embeddings, which are employed for dense retrieval; 4) Graph+PPR uses the formula (5) to obtain graph embeddings for dense retrieval; 5) Graph+BN excludes the backtracking (or PPR) mechanism from the full GraphIC model during computing Z; and 6) Graph+PPR+BN represents the full GraphIC model, integrating all components."}, {"title": "5.5 ANALYSIS", "content": "Impact of ICE Examples on Model Performance. We conduct an in-depth investigation into the influence of the number of ICEs on the performance of our proposed GraphIC model and several competitive baselines across four datasets. For each dataset, we select the top three baselines and varied the number of examples in the set {1, 2, 4, 8}. Llama-3.1-8B-Instruct is employed as the underlying LLM. Results in Figure 4 indicate a general trend of improved model performance with an increase in the number of examples. Notably, The performance of GraphIC steadily improves as the number of ICEs increases, unlike some baseline methods, which may experience performance degradation when the number of ICEs increases. Furthermore, while GraphIC initially lags behind the baselines in the low-shot settings, its performance exhibits a more pronounced improvement as the number of examples grew. One can observe that GraphIC surpasses the baselines, demonstrating superior performance and underscoring its robustness as the number of examples increases."}, {"title": "6 CONCLUSION", "content": "We introduce GraphIC, a graph-based method for in-context example (ICE) retrieval aimed at enhancing LLM performance on multi-step reasoning tasks. By modeling reasoning as \"thought graphs\" and utilizing Bayesian Networks and personalized PageRank, GraphIC selects ICEs that align with the task's cognitive structure, overcoming the limitations of text-based embedding methods. Extensive experiments on four benchmarks show that GraphIC consistently outperforms both training-free and training-based baselines, especially in mathematical and logical reasoning. Our analysis of symmetry assumptions highlights the advantage of asymmetric retrieval models. A limitation of the GraphIC model is that, as a training-free framework, it may face difficulties in capturing more intricate thought patterns. Beyond this, GraphIC not only introduces a powerful ICEs retrieval method, but more crucially, it provides a way to represent and understand the reasoning process. This capability can be applied to various domains related to LLM reasoning, such as developing novel graph-based reasoning methods, selecting high-quality and diverse training datasets, and more."}, {"title": "A PROCESSING OF AQUA AND PROOFWRITER", "content": "Given the substantial size of the AQUA dataset, which incurs significant retrieval overhead during testing, we followed the methodology outlined in DQ-LoRe (Xiong et al., 2024), using a 1,000-sample subset for efficient evaluation.\nFor the ProofWriter dataset, we refined the subset selected by Logic-LM (Pan et al., 2023), excluding instances labeled as \u201cUnknown,\u201d as these samples lacked explicit reasoning chains. Furthermore, because the original training set did not provide reasoning in natural language, we leveraged the GPT-40-mini model to generate reasoning sequences for the training set, discarding any generated outputs deemed incorrect. We evaluate the correctness of the reasoning process by the correctness of the final result, which is a commonly used approach (Lightman et al., 2024; Xiong et al., 2024; Khattab et al., 2022). This process resulted in a refined training set of 1,358 examples with their Chains of Thought and 400 test samples from the original ProofWriter dataset."}, {"title": "B PROMPT TEMPLATES", "content": "For the four datasets under consideration, we design the following prompt templates to format the ICEs and the question into a prompt, which is then fed into an LLM to generate answers.\nGSM8K & AQUA:\nQ: {{ice_question_1}}\nA: {{ice_answer_1}}\nQ: {{ice_question_k}}\nA: {{ice_answer_k}}\nQ: {{question}}\nA:\nMBPP:\nText: {{ice_question_1}}\nTest Cases: {{ice_test_cases_1}}\nCode: {{ice_code_1}}\nText: {{ice_question_k}}\nTest Cases: {{ice_test_cases_k}}\nCode: {{ice_code_k}}\nText: {{question}}\nTest Cases: {{test_cases}}\nCode:\nProofWriter:\nQ: {{ice_question_1}}\nProof: {{ice_answer_1}}\nQ: {{ice_question_k}}\nProof: {{ice_answer_k}}\nQ: {{question}}"}, {"title": "C SKILL-KNN", "content": "Since Skill-kNN does not offer prompts for skill generation in these three tasks, we referred to the prompt designed for the semantic parsing task in the original paper to write prompts for the four datasets we used. First, we applied the Complex-CoT method to select 8 examples, then employed the GPT-40 model to generate skills in a zero-shot setting. Finally, we integrated these results to construct the final prompt.\nGSM8K:\nGenerate the skills needed to solve the following math problems.\nQ: You can buy 4 apples or 1 watermelon for the same price. You bought 36 fruits evenly split between oranges, apples and watermelons, and the price of 1 orange is $0.50. How much does 1 apple cost if your total bill was $66?\nSkills:\n1. Algebraic Reasoning\n2. Proportional Thinking\n3. Numerical Operations\n4. Logical Analysis\n5. Problem Solving\n6. Cost Analysis\nQ: {{question}}\nSkills:\nAQUA:\nGenerate the skills needed to solve the following math problems.\nQ: In a group of 6 boys and 4 girls, four children are to be selected. In how many different ways can they be selected such that at least one boy should be there?\nOptions: A) 209, B) 210, C) 211, D) 213, E)215\nSkills:\n1. Selection Principles\n2. Inclusion-Exclusion\n3. Logical Analysis\n4. Quantitative Reasoning\nQ: {{question}}\nSkills:\nMBPP:\nGenerate the skills needed to solve the following coding problems.\nText: Write a function to generate a square matrix filled with elements from 1 to n raised to the power of 2 in spiral order.\nTest Cases:\nassert generate_matrix (3)== [[1, 2, 3], [8, 9, 4], [7, 6, 5]]\nassert generate_matrix(2)==[[1,2], [4,3]]\nassert generate_matrix (7)==[[1, 2, 3, 4, 5, 6, 7], [24, 25, 26, 27, 28, 29, 8], [23, 40, 41,\n42, 43, 30, 9], [22, 39, 48, 49, 44, 31, 10], [21, 38, 47, 46, 45, 32, 11], [20, 37, 36,\n35, 34, 33, 12], [19, 18, 17, 16, 15, 14, 13]]\nSkills:\n1. Matrix Manipulation\n2. Spiral Algorithm Design\n3. Loop Control Flow\n4. Boundary Handling\n5. Efficient Implementation\n6. Testing & Debugging\n7. Sequence-to-Matrix Mapping"}, {"title": "D.1 FORMALIZED REASONING REPRESENTATION", "content": "The prompt examples below are used to generate formalized reasoning representations for the four datasets being considered. For the test question, since no answer is provided, we will remove the section of the prompt highlighted in blue. This will allow the LLM to generate both the answer and the formalized reasoning representation simultaneously, from which we can then extract the formalized reasoning representation.\nGSM8K:\nTranslate the given calculations into code form. Each line of code MUST follow the format specified below:\noutput_variable = [description of operation] (input_variable_1, ..., input_variable_n)\nQ: You can buy 4 apples or 1 watermelon for the same price. You bought 36 fruits evenly split between oranges, apples and watermelons, and the price of 1 orange is $0.50. How much does 1 apple cost if your total bill was $66?\nA: If 36 fruits were evenly split between 3 types of fruits, then I bought 36/3 =\n<<36/3=12>>12 units of each"}, {"title": "D.2 VALUES OF A", "content": "We select hyper parameter A values from {0, 0.1, 0.2, 0.3}, and report the \u00c0 values chosen on various datasets and LLMs in Table 4."}]}