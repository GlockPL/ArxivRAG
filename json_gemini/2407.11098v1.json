{"title": "Inertial Confinement Fusion Forecasting via LLMs", "authors": ["Mingkai Chen", "Taowen Wang", "James Chenhao Liang", "Chuan Liu", "Chunshu Wu", "Qifan Wang", "Michael Huang", "Ying Nian Wu", "Ang Li", "Chuang Ren", "Tong Geng", "Dongfang Liu"], "abstract": "Controlled fusion energy is deemed pivotal for the advancement of human civilization. In this study, we introduce FUSION-LLM, a novel integration of Large Language Models (LLMs) with classical reservoir computing paradigms tailored to address challenges in Inertial Confinement Fusion (ICF). Our approach offers several key contributions: Firstly, we propose the LLM-anchored Reservoir, augmented with a fusion-specific prompt, enabling accurate forecasting of hot electron dynamics during implosion. Secondly, we develop Signal-Digesting Channels to temporally and spatially describe the laser intensity across time, capturing the unique characteristics of ICF inputs. Lastly, we design the Confidence Scanner to quantify the confidence level in forecasting, providing valuable insights for domain experts to design the ICF process. Extensive experiments demonstrate the superior performance of our method, achieving 1.90 CAE, 0.14 top-1 MAE, and 0.11 top-5 MAE in predicting Hard X-ray (HXR) energies of ICF tasks, which presents state-of-the-art comparisons against concurrent best systems. Additionally, we present FUSION4AI, the first ICF benchmark based on physical experiments, aimed at fostering novel ideas in plasma physics research and enhancing the utility of LLMs in scientific exploration. Overall, our work strives to forge an innovative synergy between AI and plasma science for advancing fusion energy.", "sections": [{"title": "1 Introduction", "content": "After National Ignition Facility (NIF) achieving ignition in December 2022 [1], the focus of current inertial confinement fusion (ICF) research shifts to exploring high gain schemes required to make fusion a practical and sustainable energy source for humankind. Fusion represents a potential key enabler for advancing humanity towards a Type I civilization on the Kardashev scale [91], offering a virtually limitless and clean energy source that could power our civilization globally. This advancement could potentially resolve numerous crises we currently face e.g., economic recessions and climate change - by eliminating the need for finite resources like fossil fuels.\nThe optimization of ICF designs and the achievement of reliable ignition face formidable constraints [8, 14] due to laser-plasma instabilities (LPI)[22, 54]. Efficient and symmetrical driving of the target, vital for ICF, is impeded by LPI phenomena such as stimulated Raman and Brillouin backscatterings (SRS and SBS), which can disrupt implosion symmetry and reduce efficiency through cross-beam energy transfer (CBET) [68, 21]. Hot electrons, a byproduct of LPI processes like SRS and Two-Plasmon-Decay (TPD), can both hinder and assist ignition, showcasing the complexity of the interaction. Despite efforts to measure and simulate hot electrons, obtaining predictive scaling laws remains challenging due to the dynamic nature of laser/plasma conditions and computational limitations [38, 57, 89, 66, 42], highlighting the current gap in establishing a predictive capability based on first principles that aligns with experimental data. These constraints highlight the need for an innovative approach to overcome these obstacles.\nCurrently, Large Language Models (LLMs) exhibit versatile capabilities across diverse disciplines (e.g., robotics [44, 73, 90, 26], medical health [41, 67, 77, 50], agriculture [56, 80]), adeptly capturing intricate patterns in multimodal data. Due to their success in other domains [39, 93, 46], we are convinced that LLMs may also potentially excel in generalizing to plasma physics, particularly in forecasting the behavior of hot electrons generated during implosion in the real-world, physics experiments. Leveraging their vast pre-trained knowledge base, LLMs could optimize ICF designs by efficiently evaluating numerous scenarios, aiding researchers in identifying promising approaches expediently, and generating insights to enhance our understanding and control of the fusion process.\nIn light of this perspective, we intend to harness the power of LLMs to overcome the barrier to the vertical advancement of the next stage of human civilization: fusion energy. This science problem manifests as two sub-problems in a unique and challenging setup: 1 how to tailor pre-trained LLMs in order to accurately predict the behavior of hot electrons based on laser intensity inputs? and how to evaluate the trustworthiness of the LLM's predictions in order to guide the ICF design?\nWe conceptualize LLMs as a computational reservoir to unlock their potential for robust domain adaptation and generalization for ICF task, titled FUSION-LLM (see Fig. 1). In order to address question 0, we propose the LLM-anchored Reservoir, augmented with a fusion-specific prompt, to facilitate the interpretation of plasma physics. The incorporated prompts encompass domain-specific knowledge, instructional cues, and statistical information, thereby enabling the LLMs to accommodate the specific demands of the given task. Additionally, we develop the Signal-Digesting Channels to capture the distinctive characteristics of ICF inputs. It features a temporal encoder to better align the laser signals with the pre-trained, time-series space, and a spatial encoder to provide a global description of the input landscape. To tackle question \ufffd, we introduce the Confidence Scanner to assess the trustworthiness in predictions. Specifically, we couple the gradient saliency in prediction head and token entropy in LLM outputs to obtain the model's confidence scores.\nOverall, this study aims to provide an exciting synergy between the domain of AI and plasma science for fusion energy development. The core contributions of this paper can be summarized as follows:"}, {"title": "2 Methodology", "content": "2.1 Preliminary\nThe ICF Overview. A ICF shot is depicted in Fig. 2. Laser beams are directed towards a target fuel pellet, rapidly heating its surface to generate a plasma envelope. This plasma envelope subsequently undergoes implosion through a rocket-like expulsion of material, compressing the pellet to densities exceeding that of lead by twenty-fold and attaining ignition temperatures of approximately 100,000,000\u00b0C, thus initiating fusion reactions. Throughout this process, laser-induced plasma instabilities give rise to hot electrons, which may prematurely heat the pellet and emit Hard X-Rays (HXR) emissions as they interact within the plasma environment. Understanding the hot electron dynamics is crucial for guiding the design of the ICF. In the context, HXR diagnostics have been extensively applied in studies at facilities such as OMEGA [69, 88, 71, 87] and the NIF [58, 59, 70]. It is however worth noting that conducting these experiments is extremely expensive (around $1m for one shot).\nExperiment and Data Collection. To collect the fusion data, we conduct 100 real-world ignitions of ICF. Each ignition is accompanied by detailed configurations including the size of the fuel pellet, the phase plate of the laser, and the input laser profile. We also measure raw sensor readings regarding HXR, which can be further converted into charges of hot electrons, allowing for the subsequent calculation of the total energy of these electrons based on their temperature. In this study, we focus on forecasting the energy from HXR emissions by given laser intensity inputs throughout the ignition. This data is indispensable for physicists to ascertain and analyze the state of ICF.\n2.2 FUSION-LLM\nIn this section, we introduce FUSION-LLM, a novel approach for predicting hot-electron energy in ICF. Illustrated in Fig. 3, the inputs comprise fusion-specific prompts and HXR signals, which are processed through LLM for feature extraction. Subsequently, the output module makes predictions of hot-electron energy along with confidence scores. Our approach is characterized by three core modules: LLM-anchored Reservoir, which establishes a reservoir foundation using LLMs to comprehend the dynamic impact of laser intensity on hot-electron energy emission; Signal-Digesting Channels is responsible for encoding HXR signals, capturing the temporal characteristics of sequential details, and spatial distinctiveness of data landscapes; and Confidence Scanner, tasked with estimating prediction confidence for each shot, thereby providing trustworthy guidance for practical ICF design.\n2.2.1 LLM-anchored Reservoir\nIn classic reservoir computing (RC), a fixed, randomly generated reservoir (e.g., RNN) transforms input data into a high-dimensional representation. A trainable output layer then maps this representation to the desired output. RC has gained popularity in the scientific community due to its ability to enable efficient processing of sequential data with simple training methods. Following the RC convention, we construct a reservoir using LLM with a shallow prediction head (see \u00a7S1). Due to the extensive pre-training, LLMs are equipped with robust generalization capabilities for in-context reasoning and time-series forecasting. To leverage the physics knowledge embedded in LLMs, we design fusion-specific prompts (FSP) that strategically connect the LLM's vast knowledge base to the"}, {"title": "2.2.2 Signal-Digesting Channels", "content": "are observed. This laser-energy signal landscape exhibits significant discrepancies between the uniformity of the plain phase and the peaks of the impact phase. This physics insight guides our SDC design (see Fig. 3b), which comprises two components: a temporal encoder to align the laser signals with the pre-trained time-series space, and a spatial encoder to delineate the landscape of input data.\n\u27a4 Temporal encoder is designed to extract time-series features using a windowing mechanism that constructs consistent signal patches across sequential time steps. It employs a set of Transformer layers [85] to capture time-series distributions over the forecast horizon. This process is formulated as $f_{\\text{tmp}}: (X_{t-l:t+h}, Y_{t-l:t}) \\rightarrow \\downarrow$, where X and Y represent the input data and target data spanning time windows of length l and h at time t. The encoder is pre-trained on a large-scale time-series dataset (see \u00a7S1 for details) and is optimized using the log-likelihood of the forecast:\n$\\arg \\max_{\\theta} \\mathbb{E}_{(X, Y) \\sim p(\\mathcal{D})} \\log p(Y_{t:t+h} | f_{\\text{tmp}}(X_{t-l:t+h}, Y_{t-l:t}))$,\nwhere $p(\\mathcal{D})$ represents the data distribution from which the time-series samples are drawn. During fine-tuning on the target-domain ICF data, all pre-trained parameters are frozen, except for the last linear layer. The temporal encoder is utilized for feature extraction over the input laser signals I, producing temporal tokens denoted as $E_{\\text{tmp}} = f_{\\text{tmp}}(I)$ for subsequent processing.\n\u27a4 Spatial encoder is designed to analyze the input signals by providing a qualitative overview of the in-put landscape. Specifically, it is structured to characterize the spatial patterns of laser intensity signals throughout the ICF process. We utilize the projection block $f_{\\text{LLM}}$ to project sets of critical contexts into spatial features, denoted as $E_{\\text{spt}} = \\{f_{\\text{LLM}}(\\text{``pulse''}), f_{\\text{LLM}}(\\text{``peak''}), ..., f_{\\text{LLM}}(\\text{``trailing''})\\}$. In practice, $E_{\\text{spt}}$ is further processed by a cross-attention layer with temporal features $E_{\\text{tmp}}$. This step couples the contextual description to the actual signal distribution within the ICF, enabling the LLM to better capture the observed physical phenomena for predictions.\nAfter acquiring the features from both the temporal and spatial encoders, we concatenate them $E = [E_{\\text{tmp}}; E_{\\text{spt}}]$ to form the output of DSC. Here, we use \u00c9 as augmented inputs to replace the vanilla $E_{\\text{las}}$ in Eq.1. Fundamentally, DSC introduces a novel method for input encoding in reservoir computing, which enhances overall system performance. This design offers the following advantages:"}, {"title": "2.2.3 Confidence Scanner", "content": "Trustworthiness is pivotal for AI in science. Under-confident predictions may lead to misguided conclusions or improper actions. Some approaches [45, 27] directly utilize the entropy observed in the output tokens of the LLMs to gauge the confidence. However, these methods encounter a chal-lenge in our study of ICF whereby the entropy of LLM's output to-kens does not consistently reflect confidence of prediction at each time step. This discrepancy arises due to the non-linear transformation undertaken by the multi-layer perception within the prediction head, which alters the embedding of token counterparts, thereby distorting the relation between tokens and their corresponding confidence estimations.\nTo this end, we propose Confidence Scanner that incorporates a confidence reweighing mechanism to assess the confidence level of each prediction systematically. Concretely, our approach re-calibrates the allocation of confidence across tokens to implicitly reflect their actual influence on predictions. As shown in Fig. 4, we extract the embedding $E_k = \\{e_{n-k}, ..., e_n\\}$ for the last layer of LLM, which specifically analyze the embedding of the last k tokens. The confidence level H is then formulated as:\n$H = [h(e_{n-k}), ..., h(e_n)],$\nwhere h(\u00b7) map the embedding to word probabilities, which are subsequently used to calculate the entropy. Furthermore, we derive a reweigh matrix S from the task prediction head $H_{\\text{task}}(\u00b7)$ by performing a forward process to predict hot-electron energy by $P = H_{\\text{task}}(E_k)$. The matrix S is then obtained through saliency, reflecting the contribution to the i-length of predictions:\n$S = \\sigma (\\frac{\\partial P}{\\partial e_{k}}, ..., \\sigma \\frac{\\partial P}{\\partial e_{k}})$\nwhere the \u03c3 is the softmax function to normalize the saliency scale. Finally, entropy H is combined with reweigh matrix S to produce the confidence score $C = \u2212H \u00d7 S$ for the hot electron energy predictions. Through our design, we align the entropy derived from LLM embeddings with the hot electron energy forecasting. This alignment allows us to directly obtain a confidence level that can serve as a trustworthiness indicator for our system. We provide empirical evidence in Fig. 6."}, {"title": "3 Empirical Findings", "content": "3.1 Main Results\nDataset. We develop a new benchmark, FUSION4AI, to support AI research in ICF. This benchmark consists of 40,000 LPI samples containing 100 shot sequences with 400 time steps/shot. Each shot is documented with key parameters such as target size, laser intensity, and energy of hot electrons (see \u00a72.1). The dataset has been systematically divided into 80/10/10 for train/val/test splits respectively. We will release this dataset upon acceptance to advance research for the fusion reaction.\nBaselines. We choose a classic physical Particle-In-Cell (PIC) simulation method [13], a number of classic AI models (i.e., LSTM [25], Autoformer [86]), reservoir computing models (i.e., HoGRC [43], RCRK [17], NGRC [20]), and concurrent time-series LLM-based models (i.e., GPT4TS [94] and Time-LLM [31]) as baseline models for performance comparison on proposed FUSION4AI dataset.\nExperimental Setup. Our experiments are trained with 100 epochs and a batch size of 5, which is adequate to achieve convergence based on our empirical findings. In addition, we utilize a fixed learning rate of 0.0004 and the Adam optimizer [36]. A loss function defined by the cumulative absolute error across each time-steps $l_{\\text{loss}}(Y, G) = \\Sigma_{\\text{pred}\\_{\\text{len}}} | Y_n - g_n |$ is used, where Y and G represent the sequences of predictions and ground truths, respectively, $Y_n$ and $g_n$ denote the values at the n-th time-step, and pred_len is the length of prediction. For all other counterpart methods, we follow the original experimental setting and training configurations to reproduce the results.\nMetric. We employ cumulative absolute error (CAE) as our primary metric. The sole distinction in its implementation involves nullifying values that are less than 0.03 of the predicted value. In addition, we incorporate two supplementary metrics: top-1 and top-5 MAE, which represent mean absolute error focusing exclusively on the top one percent or five percent errors, respectively, thereby highlighting the performance where the highest errors are observed."}, {"title": "3.2 Diagnostic Experiment", "content": "This section ablates FUSION-LLM's systemic design on val split of FUSION4AI. All experiments use the Llama 3 8B variant. Appendix \u00a7S2 has more experimental results.\nKey Component Analysis. We first investigate the two principal modules of FUSION-LLM, specifically, Fusion-Specific Prompt and Signal-Digesting Channels. We construct a baseline model with generic dummy prompts which only provide broad, non-specific instructions regarding fusion, and a rudimentary encoder composed of a single linear layer. As shown in Table 2a, the baseline model achieves 3.57 CAE. Upon applying Fusion-Specific Prompt to the baseline, we observe significant improvements for CAE from 3.57 to 2.59. Furthermore, after incorporating Signal-Digesting Channels into the baseline model, we achieve significant gains of 1.56 CAE. Finally, by integrating both core techniques, our FUSION-LLM delivers the best performance of 1.19 CAE. These findings affirm that the proposed Fusion-Specific Prompt and Signal-Digesting Channels operate synergistically, and validate the effectiveness of our comprehensive model design.\nFusion-Specific Prompt. We next study the impact of our Fusion-Specific Prompt by contrasting it with a constructed baseline. This baseline incorporates Signal-Digesting Channels and employs generic prompts that provide broad, nonspecific instructions unrelated to the process of fusion. As shown in Table 2b, the baseline yields a performance measure of 2.01 in terms of CAE. Upon substituting the generic prompt with one that integrates discipline-specific information, including"}, {"title": "4 Related Work and Discussion", "content": "AI for Science. Al is increasingly acknowledged as a critical instrument and recently led trending of significant scientific discovery [32, 40, 55, 5]. The trajectory of AI in scientific research commenced with different elementary data analysis methods (e.g., Rule-based [11, 60], Bayesian [19], Analogy [24, 30, 76], Evolutionary [35, 16], Connectionism [83, 40], etc) and has evolved into the advanced foundation models [23, 81, 15, 18, 10]. This development is underscored by considerable progress in AI recently, particularly the emergence of sophisticated constructs such as LLMs [2, 78, 75, 95], which have fundamentally altered our approach to scientific challenges. By incorporating expansive knowledge bases, AI is indispensable for deciphering vast and varied scientific data.\nThe uniqueness of our work lies in the challenging nature of accessing ICF data, given the difficulty in observing LPI, which renders conventional AI tools less effective in this crucial area of physics. Building on the success of LLMs, this study harnesses their extensive knowledge base to unlock robust predictive capabilities for physical phenomena. Our empirical findings validate our methodological approach, positioning our work as pioneering and innovative. This not only demonstrates the applicability of LLMs in the scientific domain but also fosters a dynamic interplay between AI and scientific exploration. We showcase the potential of our LLM-based solution to make meaningful contributions in addressing complex scientific challenges, thus advancing both AI and science.\nPlasma Physics for Fusion. In the realm of plasma physics, particularly in ICF, efficient and symmetric driving of a target is imperative. Specifically, LPI represents significant impediments due to the complex dynamics of phenomena in fusion. For instance, Stimulated Raman and Brillouin Scattering (SRS and SBS) can cause the reflection of laser beams [37]. Moreover, Cross-Beam Energy Transfer [28] may adversely influence the symmetry of implosion. A critical issue particularly in direct drive scenarios is the preheating caused by hot electrons produced via SRS and Two-Plasmon Decay, which potentially increases the shell entropy and diminishes the implosion efficiency [68, 21, 14, 54]. Conversely, these hot electrons might contribute positively by depositing their energy in the compressed shell, thereby enhancing the ignition shock and aiding ignition processes such as those observed in shock ignition, a novel high-gain strategy [9, 53]. Understanding the dynamics ofLPI and establishing predictive models for hot electron generation in direct drive configurations are crucial.D In response to these issues and challenges, extensive physical experimentation and simulations [68, 21, 14, 54, 9, 53] are necessary, which are both costly and time-consuming.\nTo address the above challenges, our AI-based approach emerges as a potential alternative. By reformulating LLMs with a simple yet effective pipeline, our model can effectively engage in such ICF tasks as Computational Reservoir to encapsulate domain-specific knowledge and generalize within the domain. This capability potentially allows LLMs to guide ICF design, circumventing the need for extensive and expensive experimental setups and simulations. Empirical evidence indicates that the incorporation of LLMs had promising to revolutionize the predictive modeling in plasma physics by providing quick, cost-effective insights that are grounded in generalizable knowledge.\nReservior Computing. The ICF data exhibits a time-series nature, where laser intensity influences the target, correlating with the volume of hot electron energy emission. Traditional machine learning approaches [72, 7, 3, 84, 61, 62, 33] for time-series data typically involve dynamically transforming temporal inputs into a high-dimensional state space through nonlinear mappings. Revolutionary advancements in the field, such as echo state networks [29, 47] and liquid state machines [49, 92], have introduced the concept of Reservoir Computing (RC). RC presents an innovative framework [51, 63, 20, 48] characterized by a static \u201creservoir\". This reservoir is pre-trained, reducing the computational expenses associated with training only the readout parameters, rather than the entire network. This feature makes RC advantageous for applications involving temporal data [48, 82]. In this paradigm, the input signals are transformed non-linearly into a dynamic time-series system's state space, from which outputs are linearly extracted [74].\""}, {"title": "5 Conclusion", "content": "Fusion energy stands as a pivotal pathway toward advancing human civilization to a Type I status on the Kardashev scale [34]. The key to realizing this potential lies in mastering Inertial Confinement Fusion, where understanding laser-plasma instabilities is paramount. To address this challenge, we present FUSION-LLM, a groundbreaking framework merging LLMs with reservoir computing. Our approach not only provides a cost-effective solution but also emerges as a top-tier contender in forecasting hot electron dynamics, offering invaluable insights for plasma scientists in refining ICF designs. Beyond its immediate impact on ICF, employing LLMs for scientific exploration holds promise forcross-domain applications, potentially catalyzing advancements in AI-driven scientific endeavors."}, {"title": "S1 Implementation Details", "content": "The overall pipeline of FUSION-LLM is shown in Fig. 3. Experiments are conducted on two NVIDIA A100-40GB GPUs. For our approach, we keep all parameters of the LLMs and most of the SDC frozen during the fine-tuning. Only parameters pertaining to the Prediction Head and partial Spatial Encoder are trainable. The codes and dataset shall be publicly released upon paper acceptance.\n\u2022 FUSION-LLM is built from Llama-2-7B-hf / 3-8B [78, 4] to construct reservoir without tuning.\n\u2022 Fusion-specific prompts structure the textual prompts with three descriptors: context descriptor, task descriptor, and input descriptor. Each descriptor is initialized with specialized tokens for indication (e.g.,  ,  ,  , etc) and input scalars as context descriptions (e.g., , , , etc). These prompts are subsequently concatenated and input into the projection layer from LLMs for feature embedding.\n\u2022 Signal-digesting channels are composed of two components, temporal encoder and spatial encoder. The former one, which incorporates 24 Transformer layers and a linear layer, captures temporal features over the input laser signal. This module has been pre-trained on the Large-scale Open Time Series Archive (LOTSA) dataset [85], which covers nine varied domains and compiles over 27 billion timestamped instances. The spatial decoder first uses a projection block from Llama [78, 4] to encode the context description of the input signal, followed by a leaner transformation. Outputs are fed into a cross-attention layer, where Key and Value are derived from the contextual embedding and query stems from temporal features, to generate the final spatial features. We concatenate the spatial and temporal features before feeding them into a linear layer to produce the final, augmented input signals.\n\u2022 Confidence scanner has been described in \u00a72.2.3 and it has no consumption of parameters. The default number of tokens k used in confidence calculation is set to 50 in the implementation.\n\u2022 Prediction head consists of two layers: a convolution layer with the kernel size of 32 and stride of 32, followed by batch normalization and GELU activation, connected to LLM, then fed to a linear layer with the input dimension of 128 that produces the final prediction."}, {"title": "S2 Quantitative Results", "content": "This section elaborates on a detailed analysis of quantitative results in Table S1, focusing specifically on in-context learning [12] performance and a runtime assessment of the models under investigation. Initially, we present supplementary in-context learning results obtained directly from various LLMs (i.e., Llama 2 [79], Llama 3 [4], and Claude 3 Opus [6]). These findings indicate that, even without an additional fine-tuning process, the LLMs exhibit substantial proficiency in the in-context learning scheme within the ICF task. For instance, Claude 3 Opus [6] achieves CAE scores of 12.19, 10.67, and 9.46 for the 1-shot, 2-shot, and 3-shot scenarios, respectively. It is amply demonstrated that the vanilla LLM has the ability to make inferences and predictions on empirical scientific data even if it is not fine-tuned at all. This underscores that our approach, leveraging these LLMs, represents a notable advancement, particularly in forecasting the energy dynamics of hot electrons."}, {"title": "S3 More Qualitative Result", "content": "This section expands to include more qualitative results that help to understand the capabilities and effectiveness of this model. Initially, we release all visualized prediction results of our model FUSION-LLM on test split of our dataset FUSION4AI in Fig. S2. From these qualitative results, it can be found that our model achieves accurate predictions on all unseen data, especially conforming"}, {"title": "S4 Failure Case Analysis", "content": "In this section, we examine a significant outlier with the largest error forecasts generated by the FUSION-LLM on the test split. This particular instance serves as a critical case study for understanding the limitations and challenges faced by our model. Fig. S3 illustrates that the shot markedly deviates from the typical scenarios. Notably, this shot exhibits an exceptionally low peak hot electron energy, registering less than 0.15, whereas the majority of other cases yield values ranging between 0.25 and 0.5 under a comparable input laser profile. This anomaly categorizes this shot as an out-of-distribution (OOD) instance. The limited volume of training data available for FUSION-LLM is a plausible explanation for the model's diminished performance on this OOD data. In scenarios where training data is sparse, the model's capability to generalize to new, especially atypical, data points is inherently restricted. Consequently, this case highlights the importance of enhancing the dataset's diversity and volume for ICF tasks. We hope the community can share more data points to improve and enlarge FUSION4AI dataset (\u00a73.1) together."}, {"title": "S5 Confidence Analysis", "content": "In this section, we provide a comprehensive discussion and analysis of the confidence scores associated with the FUSION-LLM. As elaborated in \u00a72.2.3, our confidence scores offer per-step evaluations, thereby aiding physicists to gain deeper insights into the reliability across various segments of predictions. This functionality is particularly vital for understanding the model's performance dynamics within specific contexts of its predictive output.\nTo visually represent this, we have plotted the prediction errors at each time step for the four test sets alongside their respective confidence scores in Fig. S4. The visual analysis reveals that the confidence scores exhibit a discernible decline in the intervals where the model's predictions incur larger errors compared to other intervals. This observation substantiates the existence of a correlation between the model's diminished confidence and the occurrence of higher prediction inaccuracies.\nSuch an analysis underscores the importance of confidence scores as a diagnostic tool, highlighting intervals where the model's predictions are potentially less reliable. By mapping these confidence scores to the corresponding prediction errors, physicists can identify specific phases within the prediction and temporal sequence where the model's forecasting should be interpreted with caution. This capability not only enhances the trustworthiness of the FUSION-LLM but also provides critical feedback for further refinement of the model in the future research.\nMoreover, the integration of confidence scores into the model's predictive framework offers a robust mechanism for assessing the model's performance in real-time applications. By continuously monitoring these scores, physicists can make informed decisions about the reliability of the predictions, ensuring that critical assessments and subsequent actions are based on the most credible forecasting."}, {"title": "S6 Social Impacts and Limitations", "content": "The introduction of FUSION-LLM represents a significant advancement in integrating LLMs with classical reservoir computing paradigms to enhance predictive capabilities in Inertial Confinement Fusion. This novel approach not only meets but exceeds several existing state-of-the-art models in performance benchmarks. From a societal perspective, the implications of FUSION-LLM are"}, {"title": "S7 Ethical Safeguards", "content": "In our paper, which involves a new dataset, we will establish comprehensive ethical safeguards to mitigate potential misuse and ensure responsible utilization, as outlined in the detailed protocols in the final release of models and datasets. These protocols include strict usage guidelines, access restrictions, integration of safety filters, and monitoring mechanisms. We conduct thorough risk assessments to identify potential misuse scenarios, developing tailored mitigation strategies such as robust data governance frameworks. Although not all research may require stringent safeguards, we adhere to best practices, promoting ethical awareness encouraging researchers to consider the broader impacts of their work and maintain detailed documentation for transparency and accountability. These efforts demonstrate our commitment to upholding the highest standards of ethical conduct in scientific inquiry, aiming to safeguard the interests and privacy of all people involved."}, {"title": "S8 Reproducibility", "content": "FUSION-LLM is implemented in PyTorch [52]. Experiments are conducted on two NVIDIA A100-40GB GPUs. To guarantee reproducibility, our full implementation shall be publicly released upon paper acceptance."}, {"title": "S9 Licenses for existing assets", "content": "All the methods we used for comparison are publicly available for academic usage. PIC Simulation is implemented based on the reproducing by osiris-code/osiris with AGPL-3.0 license. We use huggingface/transformers for the implementations of Autoformer [86], Llama 2 [79] and Llama 3 [4] with Apache-2.0 license and Llama 2/3 Community License Agreement. We used the official repositories DAMO-DI-ML/NeurIPS2023-One-Fits-All (GPT4TS [94]), KimMeen/Time-LLM [31], rubenohana/Reservoir-computing-kernels (RCRK [17]), CsnowyLstar/HoGRC [43] and quantinfo/ng-rc-paper-code (NGRC [20]) for our comparison experiments, where Time-LLM [31] is licensed under Apache-2.0, HoGRC [43] and NGRC [20] are licensed under MIT, the rest did not mention their licenses."}]}