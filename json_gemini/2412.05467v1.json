{"title": "The BrowserGym Ecosystem for Web Agent Research", "authors": ["Thibault Le Sellier De Chezelles", "Maxime Gasse", "Alexandre Lacoste", "Alexandre Drouin", "Massimo Caccia", "L\u00e9o Boisvert", "Megh Thakkar", "Tom Marty", "Rim Assouel", "Sahar Omidi Shayegan", "Lawrence Keunho Jang", "Xing Han L\u00f9", "Ori Yoran", "Dehan Kong", "Frank F. Xu", "Siva Reddy", "Quentin Cappart", "Graham Neubig", "Ruslan Salakhutdinov", "Nicolas Chapados"], "abstract": "The BrowserGym ecosystem addresses the growing need for efficient evaluation and benchmarking of web agents, particularly those leveraging automation and Large Language Models (LLMS) for web interaction tasks. Many existing benchmarks suffer from fragmentation and inconsistent evaluation methodologies, making it challenging to achieve reliable comparisons and reproducible results. BrowserGym aims to solve this by providing a unified, gym-like environment with well-defined observation and action spaces, facilitating standardized evaluation across diverse benchmarks. Combined with AgentLab, a complementary framework that aids in agent creation, testing, and analysis, BrowserGym offers flexibility for integrating new benchmarks while ensuring consistent evaluation and comprehensive experiment management. This standardized approach seeks to reduce the time and complexity of developing web agents, supporting more reliable comparisons and facilitating in-depth analysis of agent behaviors, and could result in more adaptable, capable agents, ultimately accelerating innovation in LLM-driven automation. As a supporting evidence, we conduct the first large-scale, multi-benchmark web agent experiment and compare the performance of 6 state-of-the-art LLMs across all benchmarks currently available in BrowserGym. Among other findings, our results highlight a large discrepancy between OpenAI and Anthropic's latests models, with Claude-3.5-Sonnet leading the way on almost all benchmarks, except on vision-related tasks where GPT-4o is superior. Despite these advancements, our results emphasize that building robust and efficient web agents remains a significant challenge, due to the inherent complexity of real-world web environments and the limitations of current models.", "sections": [{"title": "1 Introduction", "content": "In recent years, the emergence of powerful large language models (LLMs) and vision-language models (VLMs) (OpenAI, 2024a; Anthropic, 2024a; Meta, 2024) has led to a revolution in the field of conversational assistant, a.k.a. chatbots. Since then, the capabilities of these traditional, chat-only assistants have been expanded to include running search queries on the web, reading user documents, executing sand-boxed code, or generating images. Of particular interest in this work is the capability for conversational assistants to perform actions in a web browser on behalf of the user, by directly manipulating its human-facing graphical user interface (UI).\nMany everyday tasks we perform on the web are simple, yet repetitive and time-consuming. Filling out redundant administrative forms, visiting e-commerce websites in search of the best deal, or manually synchronizing different calendars are all tedious tasks that involve multi-step execution. Autonomous web assistants that can follow"}, {"title": "3 BrowserGym", "content": "BrowserGym provides a unified interface for web agents, i.e., conversational assistants that can use a web browser as a tool. As showcased in Figure 2, both the user and the agent have access to a chat where they can exchange messages, and a browser in which they can perform actions, such as typing text, clicking buttons, or opening tabs. The user typically writes instructions in the chat, and the agent tries to follow them by navigating pages, performing UI actions, extracting information, and writing back messages in the chat.\nImplementation From the agent's perspective, this interaction loop can be conceptualized as a Partially Observ- able Markov Decision Process (POMDP), where the environment (chat and browser) provides the current observation and reward, and the agent produces the next action. BrowserGym environments follow the standard OpenAI Gym API (Brockman et al., 2016) and are made available in Python through the gymnasium5 interface (Figure 3). Internally, BrowserGym relies on the Chromium browser and the Playwright library to automate browser interactions."}, {"title": "3.1 Extensive Observation Space", "content": "BrowserGym provides a rich observation space, with its main components being the task's goal (or chat history), the list of open tabs, and the current page's content.\nStructured page description BrowserGym provides the raw DOM (Document Object Model) and the AXTree (accessibility tree) as structured representations of the current page, which are extracted using the Chrome Developer Protocol (CDP). They are both provided as-is (dom_object, axtree_object) with only minimal"}, {"title": "3.2 Expandable Action Space", "content": "The fundamental action space of BrowserGym is raw executable Python code (Figure 6a). Agents have access to a Playwright page object along with two functions send_message_to_user(text) and report_infeasible(reason). These respectively allow agents to interact with the browser, and the chat, and terminate the task. This design choice offers a powerful, unrestricted action space for researchers to play with, but also exposes web agent users to vulnerabilities and safety risks due to the possibility of executing arbitrary, untrusted code. To mitigate this, BrowserGym offers a control mechanism to further restrict the action space available to web agents, through the use of an action_mapping.\nAction mapping In BrowserGym, users can instantiate environments with an optional action_mapping, which the environment will use to convert input actions to executable Python codes. It is then easy to define and experiment with new, custom action spaces in Browser Gym, for example by allowing a pre-defined list of actions"}, {"title": "3.3 Extensibility: create your own task", "content": "BrowserGym minimizes the effort needed for practitioners to implement new web tasks. Doing so only requires writing the logic of the task, which takes the form of a setup() and a validate() method.\n\u2022 Setup Starting from a blank page, this method must bring the browser to the starting point of the task, and return the goal the web agent must achieve. This will typically involve logging into a system, creating data entries if needed, and navigating to a task-specific starting URL. Then, the method must return a goal which can either be a raw string as in Figure 7 or a list of OpenAI-style messages that can include text and images, to allow for vision-conditioned goals (e.g., \u201cFind a pair of shoes that look like this image\").\n\u2022 Validate Called after each agent action is executed, this method must check whether the agent has completed the task. It has access to the current state of the browser through the active page, as well as the history of chat_messages which contains all previous user and assistant messages. Typical validation steps involve looking at the content of the current page, searching for database entries and updates, or reading assistant messages in the chat in search of a correct answer. At the end, this method must produce a scalar reward for the agent, a boolean done flag that indicates task termination, and an optional message to be added to the chat, to simulate user interaction.\""}, {"title": "4 Unification of Web Agent Benchmarks", "content": "BrowserGym currently supports six popular web agent benchmarks, listed in Table 1. Each benchmark consists of a set of BrowserGym tasks, accessible as a BrowserGym environment through the gymnasium interface (Figure 8). Any task from any of these benchmarks can then be executed using the same code base, through the same observation/action API, and any web agent implemented for one benchmark can readily be evaluated on another. The advantages are the following:\n\u2022 Breaking silos. Unifying benchmarks encourages the development of general-purpose web agents. While benchmark-specific solutions have their own merit (Kim et al., 2023; Sodhi et al., 2024) and correspond to real use cases (e.g., a company developing a web agent for an internal website), we believe that the pursuit of generic solutions is more likely to bring long-term, impactful innovation.\n\u2022 Accelerating research. Proposing a unified interface for web agents will also encourage the development of new, compatible benchmarks, stimulate progress in the field, and accelerate research. For example, a new BrowserGym-compatible security benchmark could easily evaluate all existing web agent implementations, without the need to spend time and effort adapting agent codes.\n\u2022 Reducing noise. By evaluating across benchmarks, practitioners can collect stronger statistical signals for testing design choices and scientific hypotheses. Does using screenshots improve the performance of web agents? Is LLM-A a better web agent than LLM-B? Evaluating on a single benchmark with its own biases and specificities only provides a partial answer, while cross-benchmark evaluation gives a wider picture."}, {"title": "5 AgentLab", "content": "As depicted in Figure 1, AgentLab is a set of tools to simplify experimenting with agents over BrowserGym. In the high-level API, we define the Study object which organizes the experiments of multiple agents on a benchmark. The study is materialized on disk and can be launched to run in parallel across multiple processes (Section 5.2). Once a study is completed it can be examined using AgentXRay (Section 5.3), a custom UI made to introspect the behavior of agents on individual steps of each task in the study. Due to the challenges of reproducing experiments in dynamic environments, we implemented a few features to help reproducibility (Section 5.4). Finally, we also provide a building blocks to simplify making your agent (Section 5.5)."}, {"title": "5.1 Launching experiments", "content": "Once your agent is implemented (See section 5.5), you can evaluate it by simply creating a Study and running it:\nStudy objects are in charge of setting up the experiment and running and saving the reproducibility checks. They also manage the relaunching of failed experiments. In such a broad range of environments, it is frequent to experience system failures. A server not responding, simple edge case bugs in 3rd party libraries, or rate limits from LLM servers. Thus, it is crucial to be able to relaunch failed experiments. The function study.run is designed to relaunch up to 3 times the failed tasks, but a manual relaunch can also be done as follows:"}, {"title": "5.2 Parallel experiments", "content": "Evaluating a single agent on a benchmark with a reasonable number of seeds per task yields several hundreds of episodes. When exploring multiple agent configurations this can lead to several thousands of episodes in a single Study. Hence, it is crucial to parallelize efficiently the execution of these episodes.\nWe use a multiprocess approach with the possibility to use ray (Moritz et al., 2018) or joblib as a backend. Since most of the compute requirements are outsourced to servers (LLM API, TGI server, or web server), the actual compute requirement of the job is relatively small and we can launch up to 20 tasks in parallel on a single laptop or even 50-100 on a server machine with many CPU cores and large memory. With such parallelization, the rate limits of API-based LLMs are usually the bottleneck. If more parallelism is required, ray can connect multiple machines into a single pool of workers.\nTask dependencies Benchmarks like (Visual) WebArena have task dependencies to ensure that the execution of a task will not corrupt the instance and prevent it from properly solving other tasks. The ray backend is capable of handling such dependencies, but unfortunately this also greatly limits the potential parallelism to 2-4 tasks in parallel for the (Visual) WebArena benchmarks. Also, if the study is created through make_study, AgentLab will ensure to sequentially execute all agents in the study with a proper instance reset between each evaluation."}, {"title": "5.3 AgentXRay", "content": "To facilitate deeper analysis of agent behavior, AgentLab includes AgentXRay (not to confuse with the ray parallel backend), a Gradio-based interface for diving deep into the logged traces. A visual of this interface is shown in Figure 9. After selecting the trace of interest AgentXRay displays the profiling, the goal, the different parts of the observation, the action taken, and the prompts, providing a comprehensive view of the agent's decisions and interactions. This interface allows users to visualize the step-by-step decision-making process of an agent, helping them gain valuable insights and refine their agent for better performance. This is particularly useful for identifying areas where the agent may be struggling, as it allows researchers to pinpoint the exact moment when an incorrect or suboptimal decision is made."}, {"title": "5.4 Reproducibility", "content": "Several factors can influence the reproducibility of results in the context of evaluating agents on dynamic benchmarks. Having a framework robust to the noisy nature of web tasks is necessary. We first discuss core factors that can lead to discrepancies in web agent evaluations and then present the corresponding crucial features that AgentLab implements to mitigate these discrepancies."}, {"title": "5.4.1 Factors affecting reproducibility", "content": "Software version Different versions of Playwright or any package in the software stack could influence the behavior of the benchmark or the agent. Fixing package versions can help, but it would prevent bug fixes, security updates, and new features.\nAPI-based LLMs silently changing Even for a fixed version, a commercial LLM may be updated, for example, to incorporate the latest web knowledge.\nLive websites WorkArena: The demo instance is mostly fixed in time to a specific version, but ServiceNow sometimes pushes minor modifications. Assistant Bench and GAIA: These rely on the agent navigating the open web. The experience may change depending on the country or region; some websites might be in different languages by default. Mind2Web-live: Again, this relies on live websites that may change over time, and the content may be affected by the region.\nStochastic Agents LLMs are non-deterministic, but this is less concerning since it is independent and identically distributed (IID) noise, and setting the temperature of the LLM to 0 can reduce most stochasticity."}, {"title": "5.4.2 Reproducibility Features", "content": "Standard observation and action space One core feature of BrowserGym is to provide a standardized observation and action space. While it allows for customization, each benchmark is defined with its own default action space. Similarly, the observation space of agents in AgentLab comes with a default way to represent the DOM, the AXTree, and other components.\nPackage versions The Study object contains a dictionary of information about reproducibility, including benchmark version, package version, commit hash, os version, and time stamp. By keeping track of these variables we could isolate versions or systems that could affect the performances.\nReproducibility Journal By calling study append_to_journal(), this will automatic upload your results to reproducibility_journal.csv . This makes it easier to populate a large number of reference points.\nReproduced results in the leaderboard For agents that are reproducible, we encourage users to try to reproduce the results and upload them to the leaderboard. There is a special column containing information about all reproduced results of an agent on a benchmark. It is displayed as a range of scores alongside the original score on the leaderboard.\nReproducibility Agent By running the ReproducibilityAgent on an existing study, it will re-execute the same action sequence on the same task seeds. A visual diff of the two prompts will be displayed in the AgentInfo HTML tab of AgentXray. The user can inspect what changes that occurred between the two executions. While the two executions may diverge rapidly if there are some differences at the beginning of the episode, the diffs between the first few steps will be insightful on what has changed since the last execution."}, {"title": "5.5 Extensibility: create your own agent", "content": "At the core, BrowserGym and AgentLab are designed to have a minimalist API for implementing a new agent. Only a few key components need to be implemented in the Agent class:"}, {"title": "5.6 Extensibility: Plug your own LLM / VLM", "content": "AgentLab also offers a base class for LLMs. This allows users to plug their models into any existing agent by just sub-classing our objects. More precisely, we provide a BaseModelArgs, to configure and log your LLM information, as well as instantiating your model, and an AbstractChatModel, which just needs a __call__ method defined to query the LLM. AgentLab comes with different model objects readily available, such as a class for OpenAI models, Azure OpenAI models, or OpenRouter models. Using our chat model classes allows us to add some useful tools such as a cost and token tracker that can be automatically added to the AgentInfo."}, {"title": "6 Experiments", "content": "This section showcases large-scale experimentation that leverages the BrowserGym ecosystem and evaluates the capabilities of several modern LLMs and VLMs in performing web tasks. We focus on two objectives:\n\u2022 First, showcasing how AgentLab, integrated with BrowserGym, facilitates the creation and management of experiments with web agents.\n\u2022 Second, assessing the current performance levels of various state-of-the-art models in tackling these benchmarks.\nThe results of this experiment provide an interesting overview of the strengths and limitations of the evaluated models, offering insights into axes where they require improvements. The results are consolidated into an online leaderboard, allowing for comparisons and updates as new models and benchmarks emerge."}, {"title": "6.1 Setup", "content": "We evaluate AgentLab's default agent, referred to as GenericAgent, on all available benchmarks in the Browser- Gym ecosystem. GenericAgent employs a modular prompting mechanism. Depending on the configuration, it can use tools such as Chain-of-Thought (CoT) reasoning, screenshots with Set-of-Marks (SOM), a memory mechanism, self-criticism, and in-context examples to enhance its responses. The observation displayed in the"}, {"title": "6.2 Results", "content": "We present the results obtained using various open-source and closed-source LLMs as GenericAgents across standard web agent benchmarks in Table 2. We observe that Claude obtains the best performance across the majority of the benchmarks, with o1-Mini falling second. Claude surprisingly improves substantially over the previously unsolved WorkArena-L2 benchmark and obtains an average task success rate of 39.1%. One explanation for this impressive result could be Claude's specific training for computer use (Anthropic, 2024b), which bears a close resemblance to using a web browser. We also observe that Llama-3.1 70B obtains very close performance to GPT-4o Mini, and Llama-3.1-405B surpasses GPT-4o-mini significantly on numerous benchmarks. While beaten on most complex reasoning benchmarks, 405B still obtains more than decent performances overall, which comes as a promising omen for the open-source community.\nWe also observe an improvement in performance for GPT-4o compared to the performance measured by Boisvert et al. (2024), which goes from 23.5% to 31.4% on WebArena, and from 3.8% to 8.5% on WorkArena L2. Since they were using a very similar web agent implementation but an older model checkpoint, this suggests that the reasoning"}, {"title": "7 Discussions", "content": "In this work, we introduced the BrowserGym ecosystem, which contributes to the standardization of web agent research by providing a unified environment for evaluating web agents. We make our contribution available as two easy-to-use Python packages, BrowserGym and AgentLab. BrowserGym on one side provides a unified platform that exposes all existing web agent benchmarks under the same interface, and is designed to facilitate the addition of new benchmarks. AgentLab, on the other side, provides a complementary interface and set of tools for the implementation and evaluation of web agents, with a set of flexible web agent implementations, an expandable LLM / VLM API, reproducibility features, trace analysis tools, as well as an online leaderboard.\nTo showcase our main contribution, we conducted the first large-scale empirical evaluation of a representative set of state-of-the-art LLM / VLM models on six popular web agent benchmarks, using AgentLab and BrowserGym. While running such an experiment using the original, fragmented benchmark codes would be extremely cumbersome, BrowserGym makes all of them accessible through the same interface and allows for large-scale, unified experiments in a single code base with AgentLab. By fostering reproducibility and facilitating consistent benchmarks, the BrowserGym ecosystem lays the groundwork for more dependable and insightful evaluations of web agent capabilities, thus accelerating the advancement of LLM-driven automation."}, {"title": "7.1 Limitations", "content": "Reproducibility Despite our efforts to improve reproducibility, stochasticity, and external factors remain significant challenges. Localization differences, such as time zones, default languages, and geographic settings, can lead to inconsistent agent behaviors when interacting with live websites. Additionally, variations in operating systems and browsers can affect how web pages are rendered, impacting the uniformity of results. The presence of dynamic elements, such as advertisements, adds to non-determinism, leading agents to face different environments during repeated tasks, which affects their consistency. Addressing these challenges is essential for reliable and repeatable benchmarks in web agent research."}, {"title": "7.2 Avenue for Future Work", "content": "To further develop the field, several areas require attention. One promising direction is improving safety mechanisms for web agents. Currently, safety evaluation in web agent benchmarks, including ensuring agents' compliance with data privacy and safety policies, as well as their resilience to malicious interactions, is underexplored. The development of secure environments is crucial for adopting web agents in enterprise or sensitive applications.\nAnother avenue for future work involves the creation of real-time web agents capable of interacting with live environments at speeds comparable to human users. This necessitates improvements in both latency reduction and real-time decision-making, which would enhance the applicability of web agents in dynamic settings, such as customer service or rapid data retrieval.\nThe potential for smaller, yet efficient LLMs is also an important focus. Developing smaller models that retain the ability to understand and reason through complex web interactions would lower computational demands, making web automation more accessible and sustainable. This aligns with efforts to increase the efficiency of LLM-based agents, particularly for real-world deployment where computational resources may be constrained.\nIn addition, enhancing reactivity through the use of computer-level vision-language models (VLMs) can provide agents with a more nuanced understanding of the visual context on web pages. Current models often lack the capability to fully interpret complex visual layouts or graphical information, limiting their interaction fidelity with certain websites. Integrated models like Anthropic's computer use model could make web agents more versatile and capable of handling tasks that require both visual reasoning and textual inputs.\nAnother direction is to leverage inference time scaling, optimizing it specifically for web tasks and web agents. This could involve making better use of the model's self-reflection capabilities to further improve task performance, adapting dynamically to various complexities encountered during web navigation, and efficiently managing resources to reduce latency. By tailoring these improvements, web agents can become more effective at real-time, adaptive decision-making, ultimately enhancing user experiences and broadening their applicability.\nAs BrowserGym saves and logs every piece of information seen in its experiments, it could serve as a valuable collection tool for finetuning models. This collected data could be utilized to further refine agent behaviors, improve model performance on specific web tasks, and create efficient agents capable of handling complex scenarios more efficiently."}, {"title": "8 Broader Impact", "content": "The development of more general UI-based agents holds the potential to significantly transform how work is performed on computers. These agents promise to automate tasks at an unprecedented pace, with reduced costs, and with a broader and deeper scope of expertise. While current benchmarks demonstrate that this level of sophistication has not yet been achieved, the rapid pace of advancements in the field suggests that such a future may not be far off. This progress could unlock immense economic value but is also likely to result in substantial and rapid job displacement, presenting a critical societal challenge.\nAI safety must be central to this emerging agentic revolution. Presently, jailbreaking a large language model (LLM) might only expose knowledge already available on the web. However, as agents become more sophisticated, vulnerabilities such as prompt-injection attacks could be exploited to leak sensitive company information, authorize unwanted transactions, or cause other unintended harm. The widespread deployment of agents at scale will create enormous financial incentives for malicious actors to develop targeted attacks that uncover and exploit weaknesses in these systems.\nAs these technologies evolve, society must proactively address the accompanying challenges by establishing robust policies and frameworks. Rapid action will be essential to ensure that the benefits of these advancements are maximized while minimizing potential risks and harm."}]}