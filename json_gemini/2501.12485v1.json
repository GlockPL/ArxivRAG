{"title": "R2D2: Remembering, Reflecting and Dynamic Decision Making for Web Agents", "authors": ["Tenghao Huang", "Kinjal Basu", "Ibrahim Abdelaziz", "Pavan Kapanipathi", "Jonathan May", "Muhao Chen"], "abstract": "The proliferation of web agents necessitates advanced navigation and interaction strategies within complex web environments. Current models often struggle with efficient navigation and action execution due to limited visibility and understanding of web structures. Our proposed R2D2 framework addresses these challenges by integrating two paradigms: Remember and Reflect. The Remember paradigm utilizes a replay buffer that aids agents in reconstructing the web environment dynamically, thus enabling the formulation of a detailed \"map\" of previously visited pages. This helps in reducing navigational errors and optimizing the decision-making process during web interactions. Conversely, the Reflect paradigm allows agents to learn from past mistakes by providing a mechanism for error analysis and strategy refinement, enhancing overall task performance. We evaluate R2D2 using the WebArenabenchmark, demonstrating substantial improvements over existing methods, including a 50% reduction in navigation errors and a threefold increase in task completion rates. Our findings suggest that a combination of memory-enhanced navigation and reflective learning promisingly advances the capabilities of web agents, potentially benefiting various applications such as automated customer service and personal digital assistants.", "sections": [{"title": "1 Introduction", "content": "Web agents\u2014autonomous AI agents designed to navigate and perform natural language-described tasks within web environments--have become increasingly integral to applications such as online customer service (Huang et al., 2024a), automated data retrieval (Huang et al., 2024b), and personalized digital assistants.\u00b9. These agents interact with complex web interfaces to execute user-described tasks, often emulating human actions like clicking buttons, filling forms, and extracting information (Shi et al., 2017; Liu et al., 2018; Yao et al., 2022; Zhou et al., 2023). Despite recent advancements in web agents' capabilities, a persistent challenge remains: agents frequently fail to navigate effectively within intricate web environments. As illustrated in Fig. 1, the successful resolution of a user query usually requires a long series of actions."}, {"title": "2 Related Works", "content": "Enhancing Web Agents. Existing research has illuminated that language models, without intervention, struggle to express linguistic intent in formal instruction that can control an extra-linguistic environment, such as web site navigation (Shi et al., 2017; Liu et al., 2018; Yao et al., 2022; Zhou et al., 2023; Deng et al., 2023). This inadequacy stems primarily from the intrinsic challenges associated with perception, strategic planning, and task execution in the intricate web environments.\nTo mitigate these challenges, enhancements to web agents have been categorized into two following principal strategies. (1) Perception Alignment: This strategy aims to augment agents' capabilities in interpreting graphical user interface elements by integrating multimodal data from webpages, enhancing both textual and visual comprehension (Gou et al., 2024; Anonymous, 2024). (2) Post-hoc Reflection: Studies indicate that enabling agents to engage in reflective practices post interaction can facilitate learning from historical trajectories, thereby improving future task executions (Shinn et al., 2023; Song et al., 2024; Pan et al., 2024; Wang et al., 2024a). (3) Online Search Algorithms: This involves the adoption of sophisticated search algorithms, including Monte Carlo Tree Search and other tree-based exploration methods, integrated with high-level planning driven by LLM-derived knowledge (Zhang et al., 2024; Koh et al., 2024; Meng et al., 2024). Furthermore, Gu et al. 2024 discusses speculative planning that leverages simulations of world models.\nDespite these enhancements, the performance of current web agents are constrained by the assumptions of Unknown MDP, where the potential outcomes of actions are not available. In contrast, this paper proposes a novel approach where we reconstruct the web environment's structure based on the agents' exploratory actions, thereby furnishing them with outcome information crucial for making informed and grounded decisions.\nContinual Exploration of the Agentic Environment. Tasking agents to explore an unknown environment has been an active research direction in the community (Brohan et al., 2023). Recent studies have focused on how agents abstract experiences into actionable skills, a development that is becoming increasingly central to advancements in this field (Wang et al., 2023a,b; Liu et al., 2024). Within the domain of web-based agents, these skills are often conceptualized as workflows. Sodhi et al. (2024) have introduced a novel framework that leverages human-engineered workflows to compose policies to tackle web tasks. Although improving agents' performance, manually crafting workflows can be a tedious process.\nUnlike previous strategies that rely solely on high-quality successful trajectories or hand-crafted workflows, R2D2 introduces a two-part mechanism that continuously learns from the full range of agent experiences, including failed attempts. R2D2 moves beyond the limitations of purely Unknown-MDP-based assumptions and handcrafted workflows, resulting in more informed, robust decision-making and improved overall performance."}, {"title": "3 Method", "content": "In this section, we present R2D2, a framework for tackling complex web navigation tasks by integrating two paradigms: Remember and Reflect. The Remember paradigm constructs a structured replay buffer from past observations(\u00a73.2), while the Reflect paradigm diagnoses and corrects errors in failed trajectories(\u00a73.3). We then introduce mechanisms of the reflective memory (\u00a73.4). Finally, we illustrate how these paradigms interact to improve the agent's performance through retrieval and in-context learning demonstrations (\u00a73.5).\n3.1 Method Overview\nGiven a user's task query q and an initial observation 00 from the environment, the agent must produce a sequence of actions to address q. We define an episode as the process where the agent starts from 00 and executes a trajectory t. Let $t = \\{a_1,a_2,...,a_h\\}$ be the trajectory of length H, where each $a_h$ is an action at step h. After each action an, the agent receives an observation on, thereby forming an observation sequence"}, {"title": "3.2 Remember Paradigm", "content": "Building the Replay Buffer. To effectively represent the web environment and enable its systematic reconstruction from the observation sequence Oall, R2D2 structures the replay buffer as a directed graph G = (O, E). Here, the vertex set O includes the root node 00, which corresponds to the homepage observation, and each subsequent vertex represents another webpage observation of. The edge set E consists of tuples ((0i, 0j), a) where each edge corresponds to an action a that transitions the agent from one observation or to another observation oj. Due to the noisy and dynamic nature of web pages, R2D2 store the differences between consecutive observations at each vertex rather than the full webpage state.\nA* Search within the Buffer. The search algorithm employs a breadth-first search (A*) strategy (Hart et al., 1968; Meng et al., 2024) to navigate and evaluate web environments effectively. Instead of expanding nodes level-by-level, R2D2 incorporates a heuristic that guides the search toward potentially relevant and promising webpages more efficiently (Bonet and Geffner, 1999; Guez et al., 2018; Moldovan and Abbeel, 2012). This heuristic, provided by the LLM, estimates the relevance and utility of exploring a particular webpage node, thereby reducing unnecessary expansions and focusing on paths that are more likely to yield correct information.\nIn A* search, each node o in the replay buffer graph is associated with both a cost (e.g., the depth from the start node or the number of steps taken) and a heuristic h(o), which estimates how close o is to a relevant webpage that can answer the query q. We derive the heuristic by prompting the LLM to assess the likelihood that the subtree rooted at o will yield information relevant to q. A* search proceeds by maintaining a priority queue that selects which node to expand next based on the sum of the cost-to-come and the heuristic estimate. Webpages that are deemed relevant are added to a candidate queue, prioritizing content that potentially answers the query, while non-relevant pages are bypassed to streamline the search. This exploration continues until all reachable nodes have been evaluated. Subsequently, for each candidate node in the queue, paths are constructed back to the root, mapping feasible routes that could satisfy the query. The LLM then ranks these paths based on relevance and utility, culminating in the selection of the optimal trajectory P*."}, {"title": "3.3 Reflect Paradigm", "content": "R2D2 first determines\u00b2 the error type of a failed trajectory t. For all navigation failures, R2D2 performs search within the replay buffer to correct their navigation behaviors. We discuss details of how we address navigation failure in \u00a73.2. We now discuss using reflection techniques to address execution errors.\nThe reflection process is designed to enhance the system's capability to learn from mistakes within trajectories rather than only successes (Madaan et al., 2023; Shinn et al., 2023). When the failure reason of a trajectory t is classified as an execution failure, we prompt the LLM to identify the first erroneous action ai. The trajectory is then truncated to include only the actions before the error point, {a1,a2,...,Ai\u22121}, which are considered correct. Following this, a detailed reflection on the erroneous action a\u017c is generated, providing a rationale for its failure and potential strategies for avoidance in the future. This reflection, along with the truncated trajectory, is stored in the reflective memory (Weston et al., 2015; Mirowski et al., 2017; Wayne et al., 2018) that is to be introduced in \u00a73.4."}, {"title": "3.4 Reflective Memory Mechanism", "content": "We hereby introduce the reflective memory mechanism that stores corrected and truncated trajectories for future retrieval. The reflective memory is structured as a key-value store:\nKey-Value Architecture. The reflective memory mechanism functions as a key-value store where each user query is encoded into a unique query vector serving as the key, encapsulating the query's semantic intent for efficient retrieval via vector similarity metrics. The corresponding value comprises a truncated and corrected trajectory, as described in \u00a73.1, along with reflective insights. Specifically, for execution failures, only steps up to the first error are stored, while for navigation failures, corrected trajectory segments are retained once identified. During inference, a new query vector is generated and matched against existing keys to retrieve the most relevant trajectories.\nBasic Operations. In alignment with conventional memory module architectures, the reflective memory mechanism defines two basic operations: (1) Lookup. Given a query, the memory retrieves the value(s) associated with the closest key vectors. (2) Update. If a newly truncated trajectory provides a more accurate or enriched reflection for an existing query, the memory updates the current value. R2D2 uses an LLM to make such decision."}, {"title": "3.5 Paradigm Coordination and Inference", "content": "Exploration Phase. Using a ReACT agent (Yao et al., 2023), R2D2 processes user queries and collects observational data to build Oall. Trajectories are classified and corrected via Remember and Reflect paradigms, then stored in the memory.\nInference Phase. During inference, user queries are encoded into vectors and matched against reflective memory to retrieve relevant trajectories as in-context demonstrations (Karpukhin et al., 2020; Brown et al., 2020). These demonstrations guide the agent's response. Failed trajectories undergo reflection, and the memory is updated to improve future performance. This coordination allows R2D2 to leverage past experiences and reflections, ensuring continuous learning and enhanced handling of complex queries."}, {"title": "4 Experiments", "content": "In this section, we evaluate the proposed R2D2 framework for web agent tasks and compare it with baseline methods. We first delve into the details of our experimental setup (\u00a74.1), discuss the results obtained(\u00a74.2), and perform ablation studies to understand the strengths of different components (\u00a74.3). Furthermore, we provide a comprehensive error analysis (\u00a74.4).\n4.1 Experimental Setup\nBenchmark. We use the WebArena benchmark (Zhou et al., 2023). This benchmark comprises diverse web interaction scenarios, ranging from web shopping to customer relationship management system (CMS). The dataset consists of 812 user queries with annotated ground truth trajectories. The Webarena benchmark further provides a set of validators to programmatically validate the functional correctness of each task.\nImplementation Details. We choose gpt-403 as our main LLM for both Remember and Reflect paradigm. We use the retriv\u2074 framework as the backbone of the reflective memory index, and select \"sentence-transformers/all-MiniLM-L6-v2\" as the dense embedding model for our retriever.\nBaselines. We compare our framework against several represetative agent frameworks: (1) Re-ACT (Yao et al., 2023): a widely-used framework, which takes an observation of the environment as input, performs Chain-of-Thought reasoning, and then generates the next action. (2) Tree-Search (Koh et al., 2024): an inference-time tree-search strategy to perform best-first tree-search in web environments. It enables agents to revert to the most recently validated state upon encountering a failed trajectory. (3) LATS (Zhou et al., 2024): a method based on Monte Carlo tree search that employs LLMs as agents, value functions, and optimizers for decision-making. (4) Anticipatory Reflection (Wang et al., 2024a): a method that explicitly considers potential failures before action, alignment and backtracking after actions to maintain task objectives. (5) AutoEval (Pan et al., 2024): methods that boost agent performance using domain-general automatic evaluators. (6) BrowserGym (Chezelles et al., 2024): a framework that incorporates additional actions and observation tools for agents to interact with the environment.5"}, {"title": "4.2 Main Results", "content": "Outperforming Tree-search and ReACT Methods: As illustrated in Tab. 1, the R2D2 model achieves superior success rates across all tasks when compared to both the Tree-search and ReACT methods, demonstrating its effectiveness. For instance, in the CMS and Reddit tasks, R2D2 outperforms Tree-search by substantial margins (30.4% vs. 16.5% in CMS and 20.8% vs. 10.5% in Reddit). The improvement suggests that the integration of a systematic replay buffer and a reflective memory paradigm in R2D2 enhances the agent's ability to navigate and learn from past interactions, avoiding repeated errors and optimizing the decision-making process based on agents' experiences. This highlights the advantage of R2D2's enhanced observability in the MDP, which enables a more comprehensive understanding of the web environment, leading to more accurate and efficient strategies."}, {"title": "4.3 Ablation Study", "content": "Ablating rounds of execution. To better understand the strength of our proposed framework, we compare R2D2 with advanced baselines that emphasize reflection techniques. Fig. 4 illustrates a marked increase in the success rate of R2D2 during initial episodes. Upon manual inspection, we attribute this early performance enhancement primarily to the effective resolution of navigation failures. By the fifth episode, R2D2 substantially outperforms AR and LATS, confirming its methodological superiority. This highlights R2D2's ability to leverage historical data and adaptive strategies effectively. While AR demonstrates commendable learning capabilities through its anticipatory reflection, it fails to match R2D2's effectiveness. LATS, in contrast, shows minimal improvement. These findings support the practical superiority of the R2D2 model in dynamic learning environments.\nAblating Remember & Reflect paradigms. In this ablation study focused on the CMS domain, the full R2D2 model substantially outperforms its variants, as shown in Fig. 5. The \"- Reflection\" variant, which lacks advanced reflection capabilities, shows moderate gains, while the \u201c- Navigation\u201d variant, which removes navigation, achieves only marginal improvement. Notably, the \"- Reflection\u201d variant, though initially showing some improvement, demonstrates a limited performance increase in later episodes, suggesting that while navigation capabilities can provide early benefits, their effectiveness without reflection support plateaus quickly. This observation highlights the critical role of navigation in sustaining performance improvements over time, reinforcing that reflection alone is insufficient for long term success in complex web environments.\nAblating Failed Trajectories. To elucidate the learning dynamics of R2D2, we conduct a study to isolate the impact of failed trajectories. During this study, only successful trajectories are provided as in-context demonstrations at inference time, thereby restricting R2D2 to learning exclusively from positive examples. This variant falls 7.5% from the full implementation of R2D2 to 20.5%. This also reveals a critical limitation: the number of positive examples is insufficient to provide robust navigation and reflection to the agent during inference. Consequently, if no relevant successful trajectory is identified at retrieval time. These findings substantiate the hypothesis that failed trajectories, despite not directly addressing user queries, are instrumental in enriching R2D2's strategic repertoire, and R2D2 extend beyond the mere memorization of positive examples."}, {"title": "4.4 Error Analysis", "content": "As shown in Fig. 6, we manually inspect the trajectories of the same 60 queries executed by the vanilla ReACT agent and R2D2 agent. About 60% of the vanilla ReACT agent trajectories stall at the navigation stage, so there is not even an opportunity to fail in execution. In contrast, the R2D2 agent substantially reduces navigation failures, reliably guiding itself toward the right content and thereby reaching a point where it is possible to fail in execution more frequently. As a result, R2D2 achieves a higher pass rate overall. We further annotate and discuss erroneous trajectories in Appx. \u00a7A."}, {"title": "5 Qualitative Study", "content": "We present a case study to conduct a qualitative evaluation of the R2D2 framework, as detailed in Appx. \u00a7C. Our framework uses precise navigation and advanced reflective capabilities, enabling effective interaction with and extraction of information from intricate web environments. In contrast, the baseline struggles with navigation and data analysis, resulting in incomplete and inaccurate outcomes. These findings emphasize the importance of meticulous navigation strategies and reflective processing for successful information retrieval in sophisticated web interfaces."}, {"title": "6 Conclusion", "content": "The R2D2 framework significantly enhances web agents' capabilities by integrating Remember and Reflect paradigms, enabling more effective navigation and interaction in complex web environments. This approach leads to measurable improvements in performance, reducing errors and increasing task completion rates. Through structured memory use and reflective learning, R2D2 not only outperforms existing models but also offers a scalable solution adaptable to various domains. Future work could extend its application, further optimizing agent functionality across broader scenarios."}, {"title": "Limitations", "content": "Our experiments were exclusively conducted in English. This limitation restricts our understanding of the model's efficacy across different linguistic contexts, potentially overlooking cultural and language-specific nuances that could affect the agent's performance in non-English web environments."}, {"title": "A Error Analysis", "content": "Among all the execution failures of R2D2, the errors can be classified as following:\nPessimistic Reflection (30.3%). When the agent makes a mistake and enters the reflection phase, it occasionally produces overly pessimistic rationales. Instead of proposing a plausible alternative action or a corrective step\u2014such as trying a different button or re-verifying information on the same page\u2014the agent may hastily conclude that the service is unavailable, broken, or that no solution exists. This pessimism not only mischaracterizes the underlying issue but also inhibits effective learning from the mistake. By prematurely giving up, the agent misses opportunities to refine its approach, explore subtle variations in the action sequence, or simply retry a failed step with slight modifications.\nLack of GUI understanding (24.2%). In certain scenarios, the agent struggles to properly interpret or interact with the graphical user interface (GUI) elements of the webpage. For example, when the user's query requires submitting information through an online form, the agent may fail to pinpoint the correct input fields or submission buttons, even after correctly navigating to the right page. As a result, it may click on the wrong element, repeatedly fail to submit required information, or get stuck trying to identify how to move forward.\nDifficulty with Executing Complex Plan (20.2%). After reaching the desired section of a website, the agent may still falter when asked to carry out intricate, multi-step tasks. For instance, it could be instructed to iterate through a list of items, adding each one to a cart, verifying their details, and then proceeding to a checkout process. This difficulty suggests that, although navigation is now more reliable, the agent still needs improved reasoning capabilities and better long-term action planning to handle scenarios that demand careful step-by-step execution."}]}