{"title": "ELEMENT: Episodic and Lifelong Exploration via Maximum Entropy", "authors": ["Hongming Li", "Shujian Yu", "Bin Liu", "Jos\u00e9 C. Pr\u00edncipe"], "abstract": "This paper proposes Episodic and Lifelong Exploration via Maximum ENTropy (ELEMENT), a novel, multiscale, intrinsically motivated reinforcement learning (RL) framework that is able to explore environments without using any extrinsic reward and transfer effectively the learned skills to downstream tasks. We advance the state of the art in three ways. First, we propose a multiscale entropy optimization to take care of the fact that previous maximum state entropy, for lifelong exploration with millions of state observations, suffers from vanishing rewards and becomes very expensive computationally across iterations. Therefore, we add an episodic maximum entropy over each episode to speedup the search further. Second, we propose a novel intrinsic reward for episodic entropy maximization named average episodic state entropy which provides the optimal solution for a theoretical upper bound of the episodic state entropy objective. Third, to speed the lifelong entropy maximization, we propose a k nearest neighbors (kNN) graph to organize the estimation of the entropy and updating processes that reduces the computation substantially. Our ELEMENT significantly outperforms state-of-the-art intrinsic rewards in both episodic and lifelong setups. Moreover, it can be exploited in task-agnostic pre-training, collecting data for offline reinforcement learning, etc.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement Learning (RL) has achieved remarkable success in fields like robotics [1] and games [2]. Nonetheless, its practical applications in real-world scenarios are still restricted due to the high variability and lack of user control on the density of rewards, which are critical for sample efficiency and success of RL. To counteract this shortcoming, intrinsically motivated exploration [3] has been put forth to encourage the agent to explore unknown states in the absence of extrinsic rewards, by offering an intrinsic reward.\nRecently, the state entropy $H(s)$ serves as a well-defined quantity to measure the diversity of state visitations, making it a popular intrinsic reward for exploration [4, 5, 6, 7] and various sampling methods [8, 9, 10, 11, 12]. For non-tabular state entropy maximization, Liu et.al [4] propose a popular method to estimate state entropy by measuring distances between states and their k-nearest neighbors, a.k.a., Kozachenko-Leonenko kNN entropy estimator [13]. However, these distance-based methods [4, 6, 7] face a fundamental limitation due to the vanishing trend of lifelong rewards: after a state has been visited, its novelty/reward vanishes drastically, and then the agent is not encouraged to revisit it, regardless of the potential downstream exploration opportunities it might allow [14, 15, 16, 17]. This approach diminishes the likelihood of exploring further regions via similar routes because the lifelong rewards on these paths decrease rapidly.\nTo overcome this issue, we offer an elegant solution by incorporating an episodic exploration mechanism [16] under the motivation that episodic setting encourages an agent to revisit familiar (but not fully explored) states over recent episodes [17]. Our approach, termed ELEMENT, aims to jointly fostering Episodic and Life-long Exploration under the Maximum ENTropy principle. Maximizing lifelong state entropy across all historically visited states slowly discourages revisits to frequently visited states over all episodes. Conversely, maximizing episodic state entropy quickly discourages revisiting the same state within a single episode.\nAlbeit the simplicity of our idea, implementing ELEMENT is a non-trivial task. One reason is that the state entropy in an episode provides one single trajectory-wise bonus only at the conclusion of each long-term episode, which is a Partially Observed Markovian decision process (POMDP). In contrast, standard RLs learn by exploiting dense Markovian rewards for each state in episodes. To solve this problem, we propose a novel intrinsic reward termed average episodic state entropy, which is a Markovian proxy reward function, offering a theoretically grounded sub-optimal solution. This strategy enables the usage of any entropy estimator in the episodic exploration, but is infeasible for the lifelong state entropy maximization due to the computational complexity burden which escalates as the number of visited states grows. To this end, we further identified that popular entropy estimators, such as kernel density estimator, k-nearest neighbors (kNN) estimator [13] and matrix-based R\u00e9nyi's entropy functional estimator [18], are essentially (approximately) proportional to a sum decomposition based on kNN distances. This motivates us to leverage a fast kNN graph to make these estimator practical in lifelong exploration, by integrating a modified greedy graph search algorithm [19] with an online updating mechanism [20]. To summarize, our main contributions include:\n\u2022 We propose a novel intrinsically motivated policy learning method for joint Episodic and Life-long Exploration using Maximum Entropy principle (ELEMENT), which resides on the multi-scale entropy estimation to derive internal rewards for pure exploration in RL.\n\u2022 To address the POMDP challenge inherent in maximizing episodic entropy, we propose a theoretical grounded"}, {"title": "II. BACKGROUNDS", "content": ""}, {"title": "A. Entropy Estimators", "content": "For a random variable $x \\in X$ with probability density function $p(x)$, R\u00e9nyi's $\\alpha$-entropy [21] generalizes the definition of Shannon by introducing a hyperparameter $\\alpha$:\n$H_{\\alpha}^{ex}(x) = \\frac{1}{1-\\alpha}log_2(\\int_{x\\in X} p(x) dx),$ (1)\nwhen $\\alpha \\rightarrow 1$, we get back to Shannon's definition $\\int_{x\\in X} p(x) log_2 p(x) dx$.\nR\u00e9nyi entropy has been widely studied in statistics [22] and gained immense popularity in a wide range of applications in exploration RL [7, 9]. It nice property is that it can be easily estimated with the matrix-based entropy functional [18]. Specifically, given N observations ${s_i}_{i=1}^N$, its Gram or kernel matrix $K \\in R^{N \\times N}$ can be obtained by $K(i, j) = k(s_i, s_j)$, with denotes a kernel function which usually takes the form of Gaussian with width $\\sigma$ (i.e., $k_{\\sigma}(s_i, s_j) = exp(-\\frac{||s_i - s_j||^2}{2\\sigma^2})$), the state entropy can be evaluated as:\n$H_{\\alpha}(s) = \\frac{1}{1 - \\alpha}log_2[tr(A)] = \\frac{1}{1 - \\alpha}log_2[\\sum_{i=1}^N \\lambda_i(A)],$ (2)\nwhere \"tr\" is the matrix trace, $A = K/ tr (K)$, $\\lambda_i(A)$ denotes the i-th eigenvalue of A.\nIn this paper, we consider two other popular estimators including kernel density estimator (KDE) and kNN. KDE of the state set ${s_i}_{i=1}^N$ is given by [23]:\n$H_{KDE}(s) = \\frac{1}{N} \\sum_{i=1}^N -log(\\frac{1}{N} \\sum_{j=1}^N k_{\\sigma}(s_i, s_j)),$ (3)\nLet $s^{kNN}$ be the kNNs of $s_i$, the kNN entropy estimator $H_{kNN}(s)$ is given by [13]:\n$H_{kNN}(s) = \\frac{1}{N} \\sum_{i=1}^N log (\\frac{N. ||s_i - s_i^{kNN}|| . \\pi^{d/2}}{k . \\Gamma(d/2+1)} + C_k)$ (4)\nin which $C_k = log k - \\Psi(k)$ is a bias correction constant, in which $\\Psi$ is the digamma function; $\\Gamma$ is the gamma function; d is the dimentionality of s."}, {"title": "B. Reinforcement learning for exploration with maximum state entropy", "content": "The standard RL problem can be defined as policy search in an infinite-horizon Markov decision process (MDP) defined by a 5-tuple (S, A, p, r, $\\gamma$), where S is the set of all possible states, A is the set of actions, $p(s_{t+1}|s_t, a_t): S \\times A \\rightarrow S$ is the transition probability density function. $\\gamma \\in [0,1)$ is a discount factor. The episode $\\tau = {s_1, s_2 ...}$ constitutes state samples induced by a policy $\\pi(a_t|s_t) : S \\rightarrow A$. Meanwhile a stationary reward function $r(s_t, a_t) : S \\times A \\rightarrow R$ is given, and the objective can be written as a maximization of the infinite-horizon discounted rewards:\n$J(\\pi) = E_{\\tau \\sim \\pi} [\\sum_{t=0}^{\\infty}[r(s_t, a_t)]] ,$ (5)\nIn scenarios where extrinsic/environmental rewards are unavailable, Early works by [8, 24] proposed to explore the environment by maximizing state entropy. More formally, the goal is to find an optimal policy $\\pi^*:\n$arg \\underset{\\pi}{max} H(\\rho^{\\pi}(s)) = arg \\underset{\\pi}{max} [-\\int_{s \\in S} (p_{\\pi}(s)log(p_{\\pi}(s)))],$ (6)\nwhere $\\rho$ denotes state distribution induced by $\\pi$. Hazan et.al [8] offer a provably efficient algorithm which internally splits state entropy in entropy gradients defined using probability density function of state. Multiple approaches have been proposed to enhance the method including R\u00e9nyi variant [9], and efforts to reduce sample complexity [12]. Subsequently, Mutti et al. [5] consider a finite-horizon setting and introduce importance-weighted kNN entropy estimation for $p_{\\pi}$. This method computes the implicit gradient of the state entropy estimate and maximizes it, resembling the framework of TRPO [25].\nBuilding on previous works, Active Pre-Training (APT) [4] first adopts nearest neighbors as the intrinsic rewards $r^I$, which are directly proportional to the classic Kozachenko-Leonenko kNN entropy estimation in a non-parametric and model-free manner. The goal is simply to develop a reward function $r^I(s_t, a_t)$ as a substitute for r in Eq. (5). More formally:\n$r^I(s) := log(||s - s^{kNN}||_2 + 1),$ (7)\nwhere $s^{kNN}$ denotes the neighbors of s, and $||s-s^{kNN}||_2$ represents the norm of all k nearest neighbor distances to the state s. It is important to note that most intrinsic rewards are non-stationary, which contradicts the standard MDP framework. Nonetheless, they achieve state-of-the-art performance when utilized to train deep reinforcement learning (DRL) agents in non-tabular environments, where $r^I(s_t, a_t)$ does not change rapidly.\nAmong all methods using this intrinsic reward function, ATP [4] first computes kNN distances in a representation space obtained through contrastive learning. Subsequently, [11] propose training an encoder simultaneously during exploration to obtain a prototypical representation space for the kNN computation in Eq. (7). RE3 [6] asserts that random encoders are sufficient in many cases, rather than requiring contrastive"}, {"title": "III. INTRINSIC REWARD WITH ELEMENT", "content": "We focus on a fully unsupervised scenario in the absence of any extrinsic reward. The ELEMENT reward consists of two terms represented in blue and red respectively in Fig. 1: (1) episodic reward $r_{ep}$ rapidly encourages visiting different states within the same episode, i.e., maximizing episodic state entropy, (2) lifelong reward $r_l$ slowly discourages visits to past states, visited many times in the history, i.e., maximizing lifelong state entropy. Both modules share the same fixed encoder $f(o)$, mapping the current observation o to a vectorized representation that we refer to as state s. More formally, the objectives of these two modules can be written as $max H_{s \\in T}(s)$, where $T = {s_t}_{t=1}^{T_r}$ represents an episode that constitutes T, cascade state samples, and $max H_{s \\in D}(s)$, with $D = {s_t}$ being a global memory that archives all previous visited states, respectively. The intrinsic reward with ELEMENT is constructed by adding both rewards:\n$r^I(s) = r_{ep}(s) + \\beta r_l(s),$ (8)\nwhere $\\beta$ is a positive hyper-parameter weighting the relevance of the latter. An illustrative example demonstrating the impact of episodic and lifelong exploration is shown Fig. 2. The Mujoco ant agent is capable of navigating freely in all directions within a 3D space, starting from the center. When trained solely with $r_{ep}(s)$, the agent tends to move in a single direction, maximizing the number of distinct states in a single episode. Conversely, when trained only with $r_l(s)$ alone, the agent is discouraged from exploring previously visited directions, and thus hard to fully explore a direction. Our approach integrates both motivations: $r_{ep}(s)$ constrains the agent to generate episodes consisting exclusively of distinct states, while $r_l(s)$ persistently encourages the agent to explore unvisited states by fostering the creation of such episodes. As a result, during each episode, the agent consistently moves radially in one direction. If we continue the training process, the ELEMENT agent explores the world more radially, akin to \"fireworks\". Ablation studies examining the effects of varying $\\beta$ can be found in Section IV-F."}, {"title": "A. Fixed neural encoder f: O \u2192 S.", "content": "The parametric encoder maps the current observation to a d-dimensional state s. Consider a changing encoder that leads to very different representation (state) for the same observation, an agent could visit a large number of different 'states' without"}, {"title": "B. Episodic intrinsic reward.", "content": "The major challenge for episodic entropy maximization is that intrinsic rewards based on episodes instead of current state-action pairs subtly changes the underlying MDP into an episodic POMDP problem: an agent receives a single feedback $R_{ep}(T) = H_{s \\in T}(s)$ only at the conclusion of each episode, as discussed above. To solve this problem, we assume the existence of an underlying standard MDP reward function $r_{ep}(s_t, a_t)$ that approximates the episodic reward $R_{ep}(T)$ in a sum-form decomposition $R_{ep}(T) \\approx \\sum_{t=1}^{T_r} r_{ep}(s_t)$, which is a common trick for trajectory-wise feedback problems [31, 32, 33]. More formally, the episodic objective is decomposed to learn an optimal policy by maximizing:\n$J_{ep}(\\pi) = E_{\\tau \\sim \\pi} [R_{ep}(T)],\\newline = E_{\\tau \\sim \\pi} [H_{s \\in T}(s)],\\newline = E_{\\tau \\sim \\pi} [\\sum_{t=0}^{T_r-1} r_{ep}(s_t, a_t)],$ (9)\nGiven such a Markovian proxy reward function $r_{ep}(s_t, a_t)$, the agent can be trained using any standard RL algorithms. A popular approach to model $r_{ep}$ involves parameterization by effectively minimizing the objective function[32, 33] :\n$\\mathcal{L}(r_{ep}, \\theta) = E_{\\tau \\in D} [H_{s \\in T}(s) - [\\sum_{t=0}^{T_r-1} r_{ep}(s_t, a_t, , \\theta)]^2],$ (10)\nwhere $\\theta$ denotes the parameters of the learned reward model. However, parametric modeling introduces additional uncertainties due to hyper-parameters, fine-tuning, etc. Instead, inspired by episode-space smoothing [31], we define our non-parametric episodic intrinsic reward as:\n$r_{ep}(s) := E_{\\tau \\in D}[H_{s_i \\in \\tau}(S_i) \\cdot I(s \\in T)]$ (11)\nwhere $H_{s \\in T}(S_i)$ denotes the estimated state entropy value using all states within the episode $\\tau$. $I(s \\in T)$ is an indicator function representing whether the trajectory includes (s). This reward function allows for flexibility in selecting any suitable estimator. In this paper, we consider three different estimators defined in Section II and select one based on their experimental performance. The intuitive aim of this reward function is to gauge the \"average entropy\" of a state s based on the episodes accumulated during the training process. States that consistently appear in high-entropy episodes are assigned larger rewards. Theoretically, implementing Eq. (11) provides an optimal solution for an upper bound to the loss function described in Eq. (10):\nProposition 1. Assume all episodes have the same length $T_r$, we define an upper bound of Eq. (10) given by the Monte-Carlo estimation [32]:\n$\\mathcal{L}(r_{ep}) = \\mathcal{L}(r_{ep}) + E_{\\tau \\in D}[T_r^2 \\cdot V_{s \\in T}(r_{ep}(S))],$ (12)\nwhere $V_{s \\in T}(r_{ep}(s)) \\geq 0$ denotes the expected variance of rewards within the episodes. The optimal solution $r_{ep}$ to minimize $\\mathcal{L}(r_{ep})$ is given by:\n$r_{ep}(s) = E_{\\tau \\in D}[\\frac{H_{s \\in T}(S_i)}{T_r} I(s \\in T)]$ (13)\nA minor difference between Eq. (11) and Eq. (13) is a scale $\\frac{1}{T_r}$, which is a constant. Proofs and extended discussion for different $T_r$ are provided in Appendix A."}, {"title": "C. Lifelong intrinsic reward.", "content": "The trick of the sum decomposition for episodic state entropy, i.e., $H_{s \\in T}(s) \\approx \\sum_{t=1}^{T_r} r_{ep} (s_t)$ in Eq. (9), and its solution in Eq. (11) are not suitable for lifelong scenario, because no estimator can effectively estimate the entropy across all visited states, which could number more than dozens of millions. To make the above mentioned estimators amenable to lifelong scenario, we instead pursuit an intrinsic reward propositional to the entropy estimation: $H_{s \\in D}(s) \\propto \\sum_{t=1}^{T_r} r_I(s_i)$. More formally, we provide Proposition 2 which identifies the re-lationships between entropy estimators and kNN distances."}, {"title": "IV. EMPIRICAL ANALYSIS", "content": "The experimental section is organized as follow. In Section IV-A, we qualitatively illustrate how ELEMENT explores the environment. Afterwards, we quantitatively demonstrate that ELEMENT outperforms other SOTA intrinsic rewards, including RND [27], NGU [17], RE3 [6] and RISE [7], in maximizing episodic state entropy (Section IV-B) and number of visits in lifelong mode (Section IV-C). Importantly, the samples collected by ELEMENT can be effectively used for offline learning, as illustrated in Section IV-D. Besides, we show that ELEMENT is an effective unsupervised pre-training method for downstream tasks in Section IV-E. Additional experiments including ablation study and visualizing reward distribution are provided in Section IV-F and IV-G, respectively. We implement the Soft Actor Critic (SAC) [34] for the Mujoco environments and Asynchronous Advantage Actor-Critic (A3C) [35] for Mario as the respective backbone algorithms.In the subsequent experiments, we set $\\beta$ to 0.5 after normalizing both $r_{ep}$ and $r_l$ to the range [0, 1]. Details of other experimental settings are available in Appendix 1b."}, {"title": "A. How Does ELEMENT Drive the Agent?", "content": "We employ Walker2D, Ant and Mario environments from the Mujoco and NES simulation suite to qualitatively visualize the exploration process during training. In Walker2D, the agent can only move forward or backward within a 2D spatial plane, whereas the Ant agent can navigate freely in all directions within a 3D space. Both agents are reset to starting points near (0,0) if they fail to meet the health conditions specified by the default setting [36]. The default number of steps for truncation in these environments is directly adopted as $T_r$, without any fine-tuning.\nTo observe policies in the entire training process, we establish checkpoints every 500,000 steps and execute the trained policies $\\pi_c$ to collect 30,000 samples at these checkpoints, where $\\pi_c$ denotes the policy at c-th checkpoint. We also synchronize the graph update interval with these checkpoints, setting U = 500,000. The trajectories executed by $\\pi_c$ are depicted using different colors in Fig. 4. For visualization, we track and record the x-z and x-y coordinates, respectively.\nAs shown by the trajectories generated by $\\pi_1$, the agent learns to move backward in Walker2D and upward in Ant, driven purely by episodic entropy maximization, as the kNN graph remains empty before the first checkpoint. This episodic strategy leads to sequences characterized by high episodic state entropy but lacks variance across different episodes. This finding empirically validate that ELEMENT achieves the episode goal which encourages an agent to revisit familiar states over several recent episodes, but not within the same episode. Subsequently, as the kNN graph begins to update periodically, it prompts agents to explore novel states while still adhering to policies that maximize episodic state entropy. Consequently, this exploration strategy uncovers a variety of episodes with diverse states. These episodes include various motion skills including moving in different directions, walking, running, and jumping with different legs. Videos detailing these behaviors are available in the supplementary files."}, {"title": "B. Can ELEMENT Maximize Episodic State Entropy?", "content": "Recall that ELEMENT assumes the existence of an additive form of the episodic state entropy, i.e., $E_\\tau [H_{s \\in T}(S)] \\approx E_\\tau [\\sum_{t=1}^{T_r-1} r_{ep}(S_t)]$. In this section, we first empirically validate this underlying assumption. We train a SAC agent in the Ant environment using default task-driven rewards and record all historical states during the training process. Subsequently, we train a deep neural network (DNN) $r_e(s)$ to model $r_{ep}(s)$ using these approximately 2,000 historical trajectories by minimizing the objective function $||H_{s \\in T}(s)-r_e(S_t)||^2$ over all trajectories in the dataset. Fig. 5 (a) presents the learning curve, where the loss is minimized, and the predicted episodic entropy (b) closely aligns with the ground truth. This demonstrates that it is reasonable to assume the existence of such a reward function.\nFurthermore, we compare our approach with baselines in Mujoco and Mario environments, using Shannon entropy within each trajectory as our evaluation metric. This comparison aims to empirically demonstrate our method's superiority in maximizing episodic state entropy, i.e., $max H_{s \\in T}(s)$. Therefore, we solely implement the proposed episodic rewards, setting $\\beta$ = 0 in Eq. (8). The episodic state entropy for evaluation in this section is estimated with the matrix-based entropy functional in Eq. (2) with a = 1.001. We train the ELEMENT framework using only episodic rewards, $r_{ep}$. The learning curves, with the y-axis representing Shannon state entropy, are illustrated in Fig. 6. Experiments are repeated for 10 times, and we report average scores with variance. Overall, our method surpasses baselines in both exploration range and sample efficiency."}, {"title": "C. Can ELEMENT Explore New States Efficiently?", "content": "In the following, we compare our approach with baselines using unique visited state in the entire training process as our evaluation metric. Given the continuous high-dimensional state spaces, counting visited states becomes practically challenging. To manage this, we discretize only x-y or x-z coordinates into 100 \u00d7 100 histograms. Subsequently, we count unique visited states in the entire training process. We train the ELEMENT framework using both episodic and lifelong rewards. The learning curves, with the y-axis representing unique visited spatial coordinates, are illustrated in Fig. 7. Experiments are repeated for 5 times, and we report average scores with variance. Overall, our method surpasses the baseline methods in terms of both exploration range and sample efficiency."}, {"title": "D. Can the Samples Collected by ELEMENT Agents be Used for Offline learning?", "content": "The results in Section IV-C indicate that ELEMENT could have a significantly potential impact on unsupervised offline RL[37], which first collect state samples during a sampling phase and then learn from these samples during the training phase. It is widely accepted that more visited states will result in improved performance in offline learning [38].\nIn this section, we conduct offline reinforcement learning (RL) on samples induced by ELEMENT and baseline agents. During the exploration phase, we interact with the environment using the trained exploration policies recorded throughout the unsupervised training process, i.e., {$\\pi_1, \\pi_2, . . . $}, as illustrated in Fig. 4, to collect samples {$s_{t+1}, a_t, s_t, r(s_t, a_t)$} over 1,000,000 steps. Here, r(st, at) represents the task-driven extrinsic reward. In the planning phase, we train an offline RL algorithm using the collected dataset, thus eliminating the need for further interaction with the environment. For our experiments, we employ the Conservative Q-Learning (CQL) algorithm [39] as the offline RL method."}, {"title": "E. Can ELEMENT agents Conduct Task-Agnostic Exploration for Downstream Tasks?", "content": "In this section, we demonstrate the benefits to an agent from an exploration policy learned by ELEMENT. After unsupervised pre-training with ELEMENT and other baselines, the learned policies are assessed on downstream tasks. We establish 10 checkpoints for each method during their learning process. The policy that demonstrates the highest task-driven rewards at these checkpoints is selected for transfer to downstream tasks. The learning curves for episodic extrinsic returns are illustrated in Fig. 8. These experiments were conducted 5 times to ensure reliability, with average scores and variances reported. For clarity, we applied a smoothing window to the curves. ELEMENT pre-training consistently enhanced performance across tasks, contrasting with the notably slower learning from scratch. In the Hopper, Walker2D, Humanoid and Mario, ELEMENT enabled zero-shot policy optimization, as the unsupervised exploration phase had already uncovered optimal behaviors. Ant is an exception where all exploration agents remain stuck in sub-optimal motion modes after pre-training, resulting in their performance being outpaced by SAC agents trained from scratch. However, ELEMENT shows promising initial performance and converges faster than other exploration baselines."}, {"title": "F. Ablation Study for B", "content": "In this section, we extend the experiments presented in Fig. 2 and analyze the impact of $\\beta$ in detail. The Ant agent in Mujoco can navigate freely in all directions within a three-dimensional space. Both agents are reset to randomly initialized starting points near (0, 0) if they fail to meet the health conditions specified by the default gym-Mujoco package. The default number of steps for truncation is set to 1000, which is directly adopted as T, without any fine-tuning. Both $r_{ep}$ and $r_l$ are normalized to the range [0, 1] using min-max normalization. The Soft Actor-Critic (SAC) algorithm serves as the backbone for this analysis. For visualization purposes, we record the x-y coordinates and color them based on logarithmic probability. As illustrated in Fig. 9, the length of exploration episodes (radius) decreases with the increasing impact of lifelong exploration. This observation validates that lifelong exploration constrains \u201cdeep\u201d exploration in a single direction. Conversely, when episodic exploration has a greater contribution (i.e., small $\\beta$), agents are able to explore regions that are farther away from the starting points. Additionally, we note that the number of angles increases with the growing influence of lifelong exploration. Thus, lifelong exploration encourages the agent to investigate a greater variety of directions, whereas episodic exploration tends to limit this. In summary, ELEMENT with an appropriate $\\beta$, such as 0.5, effectively balances episodic and lifelong exploration, thereby promoting exploration in both radius and angular dimensions."}, {"title": "G. How Does ELEMENT Assign Intrinsic Rewards?", "content": "We utilize a 20 \u00d7 20 discrete maze\u00b2 for visualizing intrinsic rewards at each state during training. Agents begin to explore from the top-left corner and navigate for 700 steps per trial, returning to the top-left corner afterwards. We compare episodic and lifelong rewards distributions by training agent with $r_{ep}$ or $r_l$ only. Since the maze is simple and the number is states is finite, we simply implement brute kNN instead of a graph. We employ classic Q-learning as the oracle and update the Q-tables over a course of 300 \u00d7 700 steps.\nFig. 10 displays snapshots of the reward distributions at intervals of 5 \u00d7 700, 50 \u00d7 700, and 300 \u00d7 700 steps. Lifelong rewards initially motivate agents to explore new areas during the early training phase. Notably, by the 50-th episode, they assign large rewards to states deeper in the maze. As the training advances and agents more frequently reach the deeper states, there is a gradual increase in rewards for states near the starting point (shallow states), while the rewards for paths leading to these deeper states correspondingly decrease. As a result, by the 300-th episode, agents often find themselves trapped in dead ends near the starting point.\nIn contrast, episodic agents distributes state entropy rewards smoothly and effectively across the maze, reaching even the deepest parts. Intuitively, previous reward functions overlook a crucial aspect: some of these frequently visited states may serve as essential gateways to the lesser-explored areas that require investigation. Instead, episodic stance is that reward distribution should be designed to smoothly navigate agents towards exploring a broader array of states within each episode."}, {"title": "V. LIMITATION AND FUTURE WORKS", "content": "a) Fixed Encoders.: The primary limitation of ELEMENT is its reliance on a fixed encoder, avoiding that distinct representations correspond to the same observation. However, this introduces a chicken-and-egg dilemma in long-horizon environments: obtaining diverse observations for robust representation learning is challenging without effective exploration strategies, and conversely, it's difficult to develop proficient exploration RL without a solid foundation in representation learning. This issue is common across nearly all non-parametric exploration RL methods. Addressing this challenge is a key objective for our future research.\nb) Trade-off influenced by episode length $T_r$.: Given the proposition 1, the tightness of the upper bound is determined by the expected variance by the expected variance $E_{\\tau \\in D} [T_r^2 V_{s \\in T}(r_{ep}(s))]$. Typically, longer episodes lead to higher variances in state rewards, resulting in a looser bound. Conversely, estimation accuracy generally improves as the number of samples increases, which is typical of longer episodes. Here, we identify a trade-off influenced by the episode length $T_r$. Specifically, a longer episode leads to a reduced entropy estimation error, but at the cost of an increased Monte-Carlo estimation error."}, {"title": "VI. CONCLUSION", "content": "We develop ELEMENT, a novel intrinsically motivated RL exploration framework to foster state entropy maximization from both episodic and lifelong perspectives. The two objectives inside ELEMENT are complementary to each other and enable the usage of popular entropy estimators. Episodic entropy maximization ensures to search deeper by visiting diverse states in each episode, whereas lifelong entropy maximization enables to search wider by visiting different states across episodes. We test ELEMENT in robotic Mujoco and Mario environments. Qualitatively, we demonstrate how ELEMENT rewards guides agents to produce episodes with high state entropy permanently, and to slowly move away from visited states. Quantitatively, episodes generated by ELEMENT achieve the highest episodic state entropy and largest number of unique visited states, compared to SOTA approaches.Offline RL methods trained with data collected by ELEMENT also outperform those trained with data collected by baseline methods. Additionally, ELEMENT learns various motion skills unsuperivisedly, and thus significantly outperforms baselines in task-agnostic exploration for downstream tasks.\nOur ELEMENT has two limitations, which we leave for future work. First, we utilize a fixed encoder that is obtained through pre-training. In long-horizon environments, it may be challenging to collect sufficient data for effective pre-training. Second, the episodic module presents a trade-off in terms of episodic length where longer episodes lead to reduced entropy estimation errors, but at the cost of increased Monte-Carlo estimation errors."}, {"title": "APPENDIX", "content": "BASELINE DETAILS\nHere is a brief introduction to our baselines.\nThe Random Network Distillation (RND) [27] is a parametric exploration method driven by curiosity. Utilizing prediction errors to measure novelty, this approach predicts the output of a fixed randomly initialized neural network based on the current observation.\nThe Never Give UP (NGU) [17] is a parametric exploration method that introduces episodic mechanism to intrinsically motivated rewards. Utilizing prediction errors to measure lifelong novelty, this approach also implement an episodic memory to maximize state diversity within this memory.\nThe Random Encoders for Efficient Exploration (RE3) [4, 6] APT is the first approach propose kNN particles for state entropy maximization. It learns behaviors and representations by kNN particles and contrastive representation learning, respectively. RE3 replaces the contrastive representation learning process with a random encoder to avoid pre-training for representation.\nThe R\u00e9nyi State Entropy Maximization (RISE) [7] is a combination of RE3 and R\u00e9nyi entropy. It replaces Shannon entropy with R\u00e9nyi entropy and incorporates autoencoders and K-NN methods to estimate entropy in the latent space. RISE emphasizes learning acceleration by leveraging both extrinsic and intrinsic rewards. However, in our experiment, we solely utilize intrinsic rewards."}]}