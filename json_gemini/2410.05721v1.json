{"title": "Mero Nagarikta: Advanced Nepali Citizenship Data\nExtractor with Deep Learning-Powered Text\nDetection and OCR", "authors": ["Sisir Dhakal", "Sujan Sigdel", "Sandesh Prasad Paudel", "Sharad Kumar Ranabhat", "Nabin Lamichhane"], "abstract": "Transforming text-based identity documents, such as Nepali citizenship cards, into a structured digital\nformat poses several challenges due to the distinct characteristics of the Nepali script and minor variations in print\nalignment and contrast across different cards. This work proposes a robust system using YOLOv8 for accurate text\nobject detection and an OCR algorithm based on Optimized PyTesseract. The system, implemented within the\ncontext of a mobile application, allows for the automated extraction of important textual information from both the\nfront and the back side of Nepali citizenship cards, including names, citizenship numbers, and dates of birth. The\nfinal YOLOv8 model was accurate, with a mean average precision of 99.1% for text detection on the front and\n96.1% on the back. The tested PyTesseract optimized for Nepali characters outperformed the standard OCR\nregarding flexibility and accuracy, extracting text from images with clean and noisy backgrounds and various\ncontrasts. Using preprocessing steps such as converting the images into grayscale, removing noise from the images,\nand detecting edges further improved the system's OCR accuracy, even for low-quality photos. This work expands\nthe current body of research in multilingual OCR and document analysis, especially for low-resource languages\nsuch as Nepali. It emphasizes the effectiveness of combining the latest object detection framework with OCR models\nthat have been fine-tuned for practical applications.", "sections": [{"title": "Introduction", "content": "In today's digital world, the management of personal data has become increasingly important, particularly\nwhen it comes to identity documents such as citizenship cards. Back in Nepal, these papers are used for\nalmost every transaction and any kind of legal process, but the users always have to take the pain of writing\ndown the important information from these papers. This manual entry of information is not only very\ntime-consuming, but it is also prone to human error, which can, in turn, cause problems in situations where\nprecise data is of the utmost importance. It is obvious that a system is needed to streamline the process of\ncollecting and managing all of this data.\nThe newest developments in deep learning, especially in Convolutional Neural Networks (CNNs) [1] and\nobject detection methods, however, provide hopeful answers to this dilemma. YOLO (You Only Look\nOnce) and other technologies have changed the game with scene text detection, and it's possible to pull text\nfrom an image with very high accuracy and speed.\nLatha et al. [2], in their research, utilized the YOLO model on the COCO \u2018Common Objects in Context'\ndataset for real-time multilingual scene text detection and addressed the challenges caused by varying text\ncharacteristics and backgrounds of the image. They also highlighted the effectiveness of the convolutional\nneural network(CNN) in processing and effectively identifying the scene text in various contexts. Similarly,\nin 2024, Liu et al. [3] presented YOLOv5ST, a lightweight model fine-tuned specifically for fast scene text\ndetection with increased speed and accuracy in inference. Their experiment showed an impressive 26%\nincrease in speed during inference compared to the baseline model. In 2023, Gao and Liu [4] proposed\nInvo-YOLOv5, an enhanced deep learning-based text detection algorithm that replaces traditional\nconvolution layers with inverse convolution in YOLOv5. Their study demonstrated an improved efficiency"}, {"title": "Methodology", "content": "In this project, we first briefly describe the related works and the current approaches for scene text\ndetection and recognition. In section 2, we discuss specific information about the dataset and the\npreprocessing stage. Section 3 discusses the evaluation and performance metrics. We then outline the\nresults of the experiments that were carried out (Section 4). We analyze the results in the last section\n(Section 5) and recommend further studies."}, {"title": "Dataset Interpretation", "content": ""}, {"title": "Collection", "content": "For this project, the four members of the group contacted their closest friends and asked\nfor copies of their citizenship cards, while also assuring them of confidentiality through a\nNon-Disclosure Agreement (NDA) to prevent misuse. Some of the images were captured\nusing our own camera to ensure clarity. Subsequently, all these pictures were compiled in\nGoogle Drive and organized into folders to establish a dataset with diverse variations. We\ncollected 250 images with different text types, formats, and lighting conditions."}, {"title": "Description", "content": "The dataset includes pictures of the front and back of the Nepali citizenship card. We\ntook photos of some citizenship cards while they were still enclosed in transparent plastic\ncases and captured others without the case. This approach gave the model more diversity\nin clarity, glare, and reflections, addressing real-world situations. Also, the pictures of the\ncard were taken in different lighting, angles, and other atmospheric conditions to increase\nthe variance in the dataset."}, {"title": "Data Preprocessing", "content": "This project followed the following steps for the preprocessing of the data:"}, {"title": "Data Organization", "content": "The photos of the citizenship cards were carefully collected, archived, and then organized into\nfolders. During this process, duplicate photos were discovered which were later removed from the\ndataset."}, {"title": "Data Normalization", "content": "The pixel intensities of each photo were normalized to a standard range between 0 and 1. This\nhelped us speed up and stabilize the learning process by ensuring that the model processed each\nimage uniformly, regardless of differences in quality between images within the dataset [8]."}, {"title": "LabelImg", "content": "We used an open-source labeling tool called LabelImg to facilitate the labeling process for the\nfront and back sides of the citizenship images [9]. We surrounded the areas containing the relevant\ntext with rectangles and made annotations in YOLO format. Each annotation file included class\nlabels followed by the coordinates of the top-left corner and the dimensions to the bottom-right\ncorner for each image. We saved the files in text format corresponding to each image."}, {"title": "Data Splitting", "content": "We divided the datasets into 210 instances of preprocessed citizenship images for training and 40\nfor validation. We used the popular Python package split-folders inorder to shuffle and split the\ndataset while also preserving the original distribution [10]. This division ensured enough data for\nboth the training and during the testing of model's performance [11]."}, {"title": "Data Augmentation", "content": "Data Augmentation reduces overfitting when it is used with pre-trained models. It makes the\nmodel aware of a more comprehensive array of features in the images. Similarly, the augmented\ndataset also increases the robustness of the model when fine-tuning for specific objectives such as\nobject detection and text recognition. So, to increase the number of images and variability in the\naugmented dataset, we used image augmentation [12]. The various manipulation techniques\ninclude basic operations like rotation or flipping of the image, scaling of the image, and the\nimage's brightness and contrast. It is beneficial for transfer learning as it enhances the size of the\ndataset."}, {"title": "Creating YOLO Configuration Files", "content": "After that, we defined the YOLO configuration file (.yaml file), which consists of the model\ndefinition, training procedure, and paths to the training and validation datasets. In YOLO, this\nconfiguration file is crucial for initializing the architecture, the number of classes, class labels, and\nother vital parameters when training and optimizing the model."}, {"title": "YOLOv8 and Transfer Learning", "content": "This project used transfer learning to efficiently adapt a YOLOv8 model for training on our\ncustom dataset. Transfer learning is a technique whereby a model trained on some significant data,\nsuch as the COCO set, is fine-tuned on a specific data set [13]. This approach allowed us to take\nadvantage of the knowledge already embedded in YOLOv8 and train the model quickly.\nThe new YOLOv8 [7] model, which builds on previous YOLO versions represents the cutting\nedge in object detection, classification, and segmentation tasks. This version improves both speed\nand accuracy through architectural improvements. The model incorporates a CSP-based\nBackbone, which efficiently processes feature maps, and a robust Head for precise object\nlocalization and classification. By utilizing this pre-trained model and fine-tuning it on our dataset,\nwe achieved high performance while minimizing training time and computational costs."}, {"title": "Model Building", "content": "We used YOLOv8 along with it's pretrained weights and added fully connected layers followed by\na softmax layer and used transfer learning to develop two models each for front and back sides of\ncitizenship to detect the desired classes. This dual-model approach covers all relevant information,\nimproving the accuracy of our detection system."}, {"title": "Hyperparameters", "content": "In order to get the best results, we experimented with different sets of hyperparameter as\nhyperparameters are crucial in learning and generalizing the model."}, {"title": "Categorical Cross-Entropy (CCE) for Classification:", "content": "Since we dealt with multiple classes (different labels on the citizenship card), we used Categorical\nCross-Entropy (CCE) as the primary loss function for classification. CCE calculates the\ndissimilarity between the predicted class probabilities and the actual one-hot encoded ground truth\nlabels for multiple classes.\nThe formula for Categorical Cross-Entropy is as follows:\nCategorical Entropy = $-\\sum_{\\text{neuron}=1}^{\\text{classes}} y^{\\text{true}}_{\\text{neuron}} * \\ln y^{\\text{pred}}_{\\text{neuron}}$\nThis loss encourages the model to maximize the likelihood of the correct class for each label on\nthe citizenship card."}, {"title": "DFL (Distribution Focal Loss) for Bounding Box Regression:", "content": "This project utilizes Distribution Focal Loss (DFL) to refine the object localization. DFL helps to\npredict the boundaries more accurately by applying a higher penalty to errors on smaller and more\ndifficult-to-detect objects, focusing the model's attention on more challenging regions."}, {"title": "CIoU (Complete Intersection over Union) for Bounding Box Regression:", "content": "CIoU is a specialized loss function which is used to measure how well the predicted bounding box\noverlaps with the ground truth box. CIoU goes beyond just calculating overlap, as it considers:\n\u2022 The aspect ratio of the bounding boxes,\n\u2022\nThe distance between the centers of the predicted and the ground truth boxes,\n\u2022 Size consistency."}, {"title": "Adam W:", "content": "AdamW is an optimization algorithm that modifies the traditional Adam optimizer by\ndecoupling weight decay from the gradient update, which helps to improve\ngeneralization. This adjustment prevents the weights from growing too large during\ntraining, leading to more stable and efficient convergence.\nAdamW = $\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} * \\hat{m_t} - \\alpha * \\text{weight\\_decay} * \\theta_t$\nWhere:\n\u2022\na is a learning rate\n\u2022\n$\\theta_t$ is the parameter at time step t\n\u2022\n$\\epsilon$ is a small constant to avoid division by zero\n\u2022\n$\\hat{V_t}$ is the biased second raw moment estimate\n\u2022\n$\\hat{m_t}$ is the biased first-moment estimate\n\u2022 weight_decay is the weight decay term [15]."}, {"title": "Preprocessing Images for Inference", "content": "To improve the model's inference by feeding it with less noisy and more consistent images,\nseveral preprocessing techniques were applied."}, {"title": "Grayscale Conversion", "content": "This technique reduces an image's complexity by removing color data and preserving only the\nintensity values [16]. It helps to simplify images, mainly when color is not a critical model feature."}, {"title": "Gaussian Blur", "content": "Gaussian Blur is a technique to reduce image noise and detail by convolving the image with the\ngaussian function [16]. It helps to smooth out image areas, making it easier for the edge-detection\nalgorithms to perform effectively."}, {"title": "Canny Edge Detection", "content": "This technique marks the boundaries of objects within an image by identifying and highlighting\nsharp changes in intensity[18]. It begins with Gaussian smoothing to reduce noise, followed by\ngradient calculation and edge detection."}, {"title": "Cropping and Perspective Transformation", "content": "Cropping removes unwanted parts of the image, thus improving the models' ability to focus on the\nregion of interest (ROI). Perspective transformation adjusts the spatial orientation of the image,\nmaking it more suitable for analysis."}, {"title": "Application Development", "content": ""}, {"title": "Frontend App Development", "content": "The application Mero Nagarikta was built and deployed using the Flutter framework,\nwhich was built on Dart technology. Flutter allows the project to develop the application\nfor both iOS and Android using the same codebase. It enables users to select photos of\ncitizenship documents' front and back sides, facilitating interaction with the deployed\nmodel.\n\u2022\nImage Selection and Upload: The App incorporates the ImagePicker package\nso that users can choose images from the device storage.\n\u2022\nUser Interaction: When the images are uploaded, users can click a \"Fetch\"\nbutton to forward them to the backend, displaying the extracted information\nimmediately.\n\u2022\nData Extraction and Saving: Users can modify and save extracted information\nin the .txt file Format. The application has a separate section at the bottom of the\nscreen showing the work history."}, {"title": "Backend Development and Backend Hosting", "content": "For the backend, the project used Django REST Framework to build APIs that accept\nPOST requests with images in base64-encoded JSON format. The requested images\nundergo preprocessing before the trained model analyzes them for regions of interest\n(ROIs). After detection, PyTesseract performs Optical Character Recognition (OCR) to\nextract text. The project then obtains accurate and validated data through additional\nprocesses, such as using the RapidFuzz library for string matching and performing gender\nand district corrections for specific fields. The cleaned data is returned in JSON format to\nensure efficient and accurate interaction between the App and the backend at runtime."}, {"title": "Evaluation Metrics", "content": "For the evaluation of the models, several evaluation metrics were utilized:"}, {"title": "Intersection over Union (IoU):", "content": "IoU measures the overlap between predicted and ground truth bounding boxes, providing a\nfundamental measure for assessing object localization accuracy.\n$IoU = \\frac{Area of Overlap}{Area of Union}$"}, {"title": "Accuracy:", "content": "Accuracy measures the proportion of correct predictions made by the model compared to the total\nnumber of forecasts, calculated as follows:\n$Accuracy = \\frac{True Positives + True Negative}{Total number of sample data}$"}, {"title": "Recall:", "content": "It measures the ratio of true positives to all actual positives, reflecting the model's ability to detect\nall instances of a class:\n$Recall = \\frac{Total Number of True Positives}{True Positives + False Negatives}$"}, {"title": "Precision:", "content": "This metric calculates the ratio of true positives to all optimistic predictions, indicating the model's\nability to minimize false positives:\n$Precision = \\frac{Total Number of True Positives}{True Positives + False Positives}$"}, {"title": "F1 Score:", "content": "The F1 Score, which is the harmonic mean of precision and recall, provides a comprehensive\nevaluation of model performance by taking into account both false negatives and false positives:\n$F1 score = 2 \\frac{Precision * Recall}{Precision + Recall}$"}, {"title": "Support:", "content": "Support helps with performance evaluations and identifies potential class imbalances that can have\nan impact on model results by indicating the number of instances that belong to a particular class."}, {"title": "Results And Discussion", "content": "This project assessed the effectiveness of the YOLOv8 model in detecting text on Nepali citizenship cards\nthrough transfer learning. Both models for the front and back are evaluated on the F1 curve, confusion\nmatrix, and precision and recall curves."}, {"title": "Model Performance on Front and Back Models:", "content": ""}, {"title": "Front Model Performance", "content": "The confusion matrix shows how well a model made predictions by comparing the actual true classes to the\npredicted results, breaking down the counts of correct and incorrect predictions. Figure 7 depicts the\nconfusion matrix of the front model. The front model precisely recognized fields such as name, date of\nbirth, and citizenship number with high recall metrics, showcasing its strong ability to detect these features.\nMoreover, the confusion matrix highlighted minor challenges faced by the model in recognizing the\ncitizenship number, primarily caused by obstructions from government stamps on the cards."}, {"title": "Back Model Performance", "content": "Figure 8, is the confusion matrix for the back model. Although the classes to be detected were fewer for the\nback model,its result is slightly less accurate than that of the front model as it faces challenges in correctly\ndetecting the issuing officer's name and date of issue."}, {"title": "Post-processing Techniques and Challenges", "content": "After extraction, several post-processing techniques were utilized, including text correction, noise\nreduction, character replacement, and data organization. These techniques helped to improve the overall\naccuracy of the extracted data by addressing common OCR errors and misinterpretations. Moreover, the\nuse of language processing techniques including pattern matching and dictionary-based corrections\nmaintained consistency across some fields. For example, the use of sub-tokens in fields like gender and\ndistrict. Gender-specific terms were also standardized through the identification of linguistic differences to\nensure proper categorization. Also, the predefined list of districts was utilized to correct the predicted\noutput for the district field.\nDespite all these improvements, some challenges like poor lighting while clicking the photos, variation in\ntext position due to printing errors, and obstructed text from governmental stamps affected the overall\nperformance of the OCR. Highly advanced image preprocessing techniques and models are necessitated to\nhandle complex document layouts and multi-oriented text."}, {"title": "Conclusion", "content": "This project, therefore, identified an effective strategy for parsing text from Nepali citizenship cards by\nleveraging the power of the deep learning model YOLOv8 in detecting texts and optimized PyTesseract\nOCR in recognizing texts from the cards. Finally, it has turned out with very impressive performance,\nwhich is a mean Average Precision of 99.1% for the front and 96.1% for the back of the cards. It is in this\nchallenging image environment that our preprocessing steps actually proved crucial, such as grayscale\nconversion and smoothing of the image, which are very important for enhancing the accuracy of OCR.\nMero Nagarikta is a mobile application developed as part of the solution-a keen automation approach to\nextract vital citizenship data by providing a user-friendly interface for real-time text recognition.\nIn the future, we would like to investigate the inclusion of advanced transformer models that refine the\nextracted text even further to increase accuracy and precision in information capture. This could then form"}, {"title": "Conflicts of Interest", "content": "The authors declare that there are no conflicts of interest related to the publication of this paper."}]}