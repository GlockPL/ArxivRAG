{"title": "EVOLUTIONARY COMPUTATION FOR THE DESIGN AND ENRICHMENT OF GENERAL-PURPOSE ARTIFICIAL INTELLIGENCE SYSTEMS: SURVEY AND PROSPECTS", "authors": ["Javier Poyatos", "Javier Del Ser", "Salvador Garc\u00eda", "Hisao Ishibuchi", "Daniel Molina", "Isaac Triguero", "Bing Xue", "Xin Yao", "Francisco Herrera"], "abstract": "In Artificial Intelligence, there is an increasing demand for adaptive models capable of dealing with a diverse spectrum of learning tasks, surpassing the limitations of systems devised to cope with a single task. The recent emergence of General-Purpose Artificial Intelligence Systems (GPAIS) poses model configuration and adaptability challenges at far greater complexity scales than the optimal design of traditional Machine Learning models. Evolutionary Computation (EC) has been a useful tool for both the design and optimization of Machine Learning models, endowing them with the capability to configure and/or adapt themselves to the task under consideration. Therefore, their application to GPAIS is a natural choice. This paper aims to analyze the role of EC in the field of GPAIS, exploring the use of EC for their design or enrichment. We also match GPAIS properties to Machine Learning areas in which EC has had a notable contribution, highlighting recent milestones of EC for GPAIS. Furthermore, we discuss the challenges of harnessing the benefits of EC for GPAIS, presenting different strategies to both design and improve GPAIS with EC, covering tangential areas, identifying research niches, and outlining potential research directions for EC and GPAIS.", "sections": [{"title": "1 Introduction", "content": "Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that focuses on developing models capable of learning patterns from data. The optimization of these models has been a prominent area of research, yielding significant results [1] by tailoring their structural design and/or hyper-parameters based on several objectives, such as performance, complexity, or robustness, among others [2,3]. The diversity of optimization goals considered to date in the optimization of ML models reflects the capability of these algorithms to tackle different criteria.\nRecently, advances in several areas of ML research such as Deep Learning (DL) [4] and Large Language Models [5,6] \u2013 with chatbots of unprecedented performance like ChatGPT [7] and models trained to generate code to improve the effectiveness of mutation operators in Evolutionary Computation (EC) [8] \u2013 indicate a notable shift towards more generalized AI systems. The key idea of general-purpose AI systems (GPAIS) is their unique capability to execute several, potentially diverse modeling problems, with potentials to expand to tasks for which they were not originally designed. This ability to model several tasks and generalize beyond known problems has been highlighted in recent definitions of GPAIS (see [9] and references therein discussed). As a result, GPAIS have gained significance in the last year due to their flexibility and adaptability across a wide range of applications. The possibility that GPAIS develop emerging generalization capabilities, though still unproven, has drawn much attention for their practical implications in terms of AI safety [10, 11].\nAs stated previously, multiple approaches to optimize ML models have shown significant competence across various ML domains [12]. However, recent studies signal a new research trend focusing on the optimization of GPAIS, moving beyond traditional ML [9, 13\u201316]. Areas of interest in optimization such as open-ended evolution [17\u201320], quality-diversity optimization [21-23] and novelty search [24] have become crucial for GPAIS. In these optimization areas, GPAIS stand at the forefront of the current research. GPAIS exhibits compelling characteristics such as generating new knowledge, adapting to diverse environments, and seamlessly integrating new tasks without performance degradation. Within GPAIS, as in other complex systems, critical decisions shape their design and optimization, addressing challenges like search space dimensionality, evolving objectives, and the need for continuous adaptation. This dynamic landscape offers opportunities to integrate GPAIS with effective optimization techniques.\nIn this regard, EC stands out as a versatile family of algorithms that have spearheaded the design and optimization of ML models [25, 26], making a substantial impact on both scenarios [27,28]. These algorithms have a great impact on other aspects of ML like learning rules [29]. EC has played a pivotal role in various research areas, yielding valuable contributions to the development of high-quality ML models. A notable example of its success is in the area of Large Language Models merging them with Evolutionary Algorithms (EAs) [30].\nEC has shown its capability to evolve programs, solve dynamic optimization problems, or balance several conflicting objectives, among other optimization scenarios. With its history of successes at optimizing narrow AI systems, EC is particularly attractive for the optimization of GPAIS and for tackling the greater challenges it poses compared to narrow AI systems. Coincidentally, certain research areas of EC match some of the core properties of GPAIS, including adaptability to changing problems over time (evolutionary dynamic optimization) or the confluence of multiple objectives in multitask settings (multitask and multi-objective optimization). Additional strengths of EC for GPAIS we may highlight include: EC is domain-agnostic but can incorporate it if available, and it is free from assumptions about data or model properties. EC can simultaneously build model structures and optimize parameters, allowing for creative method development, and is particularly suitable for multi-objective optimization due to its population-based search approach.\nThis paper analyzes the potential of AI-powered AI that consists of AI models enhanced or even designed by another AI model. When using EC algorithms as an additional layer of abstraction for the design or enhance AI models, we call it EC-powered AI, and we will refer to those models as EC-GPAIS. Besides introducing the concept and exposing the benefits of EC when applied to design or enrich GPAIS, this position paper also surveys recent contributions falling within EC-GPAIS, ending up with a prospect of the research directions that can drive future efforts in this area. In particular, the objectives of this work are:\n\u2022 To match the potential of EC with GPAIS, with the aim of fostering more research in this trending area in AI. To do this, we will carry out the following three objectives:\nTo study the design and enhancement of GPAIS using EC with a taxonomy of EC-powered AI. Each category of the taxonomy serves as a guideline on how EC can design or improve AI.\nTo match GPAIS properties with specific ML areas in which EC has made significant contributions, to show that the connection between EC and GPAIS is well-defined, as EC may enable these properties.\nTo show the recent milestones of EC-GPAIS. Under the definition of GPAIS presented in the next section, we illustrate various works that have contributed to the beginning and progress of this field in recent years, showcasing EC-GPAIS' potential.\n\u2022 To discuss the challenges of harnessing the EC-GPAIS benefits and strategies to address advances, focusing on future challenges. To accomplish this, we break it down into the following two sub-objectives:\nTo study the challenges to obtain the benefits of EC in the GPAIS context. Both designing and enriching GPAIS using EC are complex tasks, and we must ensure that we exploit the benefit of their synergy.\nTo explore strategies that can be implemented with EC to both design and improve GPAIS. These strategies will serve as a guide for present and future developments of GPAIS using EC.\nThe rest of the paper is organized as follows: Section 2 covers the background on EC and ML, its relation with GPAIS, and the paradigm of AI-powered AI for GPAIS. Section 3 examines different options to use EC to design and enhance GPAIS, matches properties of GPAIS with ML areas that have benefited from EC, and shows practical cases as milestones in EC-GPAIS developments. After that, Section 4 emphasizes the challenges for harnessing the benefits"}, {"title": "2 Evolutionary Computation and GPAIS", "content": "This section briefly recalls the importance of EC in ML over time and in the new era of model generation in AI, in Subsection 2.1. Then, it explains several characteristics of GPAIS in Subsection 2.2. Finally, it shows the relevance of the paradigm of AI-powered AI concerning GPAIS in Subsection 2.3.\n2.1 Evolutionary Computation in Machine Learning\nEC has provided intelligent mechanisms capable of optimizing models in various environments. As stated by Li et al. [28], who reviewed more than five hundred proposals, these algorithms have been widely used to optimize different stages of ML pipelines, including pre-processing, post-processing, and modeling itself. It is worth noting that within the entire pipeline, evolutionary computation has achieved significant importance in specific branches, such as feature selection [31], feature extraction [32], ensemble modeling [33], and even in enhancing model design alongside other techniques such as Support Vector Machines [34] and Decision Trees [35].\nThe rapid growth of DL in recent years has expanded the application boundaries of this field. Diverse evolutionary proposals have facilitated the optimization of the weights, architecture, and configuration of DL models [36, 37]. EC has significantly contributed to progress in every new topic in ML, owing to the development of algorithms that advance knowledge in these areas. These algorithms have been widely studied due to their applicability across a range of domains [38]. Therefore, in the modern era of AI, where increasingly complex problems arise, the background in evolutionary computation suggests it is poised to become one of the primary mechanisms for the new generation of AI models.\n2.2 GPAIS: Definitions and Properties\nThe study of GPAIS departs from a clear distinction from narrow AI systems, which are instead designed for specific tasks. This differentiation has sparked discussions about the characteristics of an AI system to be classed as GPAIS [14]. A recent study ([9]) provides a comprehensive analysis of GPAIS, posing a clear landmark definition, a characterization, and a classification of these systems through the use of a taxonomy that distinguishes several strategies to build GPAIS. In particular, GPAIS are defined according to the process followed to incorporate a new task into the system, either by retraining the whole system from scratch with that task or by performing an adaptation to solve it while retaining the knowledge captured by the system from previous data. According to [9]:\nDefinition 1 (GPAIS): A General Purpose Artificial Intelligence System (GPAIS) refers to an advanced AI system capable of effectively performing a range of distinct tasks. Its degree of autonomy and ability is determined by several key characteristics, including the capacity to adapt or perform well on new tasks that arise at a future time, the demonstration of competence in domains for which it was not intentionally and specifically trained, the ability to learn from limited data, and the proactive acknowledgment of its own limitations in order to enhance its performance.\nDefinition 1 [9] provides a comprehensive description of the characteristics that GPAIS may possess. In what follows, and based on [9], we briefly present the concepts in GPAIS that will help understand the role that EAs may undertake.\nThe key distinguishing feature of GPAIS vs classical narrow AI lies in the ability to simultaneously tackle multiple learning tasks (whether known or unknown). Taking into consideration what we know about those tasks, Triguero et al. [9] differentiate between two types of GPAIS:\n\u2022 Closed-world GPAIS: It assumes we have data for a given number of tasks, and those will be the only tasks that will be dealt with.\n\u2022 Open-world GPAIS: These systems acknowledge the fact that new tasks may arise and limited (or none) data may be available.\n2.3 AI-powered AI for GPAIS\nThe concept of AI models enhanced or even designed by another AI model is typically known as AI-powered AI. Following the taxonomy proposed in [9], we show a non-exhaustive list of topics in which an AI may be useful to either design or enrich another AI. In terms of designing a general AI model, we may aim at different levels:\n\u2022 Hyper-parameter optimization: While the use of EC for hyper-parameter optimization is not new or uncommon in narrow AI [41, 42], here we focus on the idea of tuning a set of hyper-parameters to solve a range of tasks [43]. In the open world, the challenge lies in finding the best configuration when a new task, potentially with little or no data, arises, exploiting prior knowledge to generalize.\n\u2022 Automated algorithm selection: The previous approach can be further improved by determining both the most appropriate AI algorithm and its hyper-parameters. This would enable a more general AI solution that automatically decides the AI technique to use for one or multiple problems at once. In ML, this is typically referred to as AutoML"}, {"title": "3 Matching the potential of EC with GPAIS: A Taxonomy for EC-powered AI and notable milestones", "content": "As previously discussed, EC can be employed as a new layer of abstraction to facilitate the design or enhancement of AI, called EC-powered AI. This approach is of great significance for the realization of GPAIS. The aforementioned advantages of EC regarding domain agnosticism, data assumptions, and others make it suitable for GPAIS. Figure 2 presents the taxonomy for this kind of approach based on [9]. To show the potential of EC with GPAIS, Subsection 3.1 and 3.2 describe how EC can help to design and enrich GPAIS, respectively. Furthermore, Subsection 3.3 analyzes the suitability of EC in GPAIS by examining its properties and their connection to research areas of ML in which EC has had a substantial contribution. Finally, Subsection 3.4 shows several milestones of EC-GPAIS during the last years.\n3.1 EC-powered AI for designing GPAIS\nBuilding upon the revision established in Section 2.3 and the taxonomy presented in this section, we now explore each category and examine the strategies in which EC has made contributions, serving as a reference for future de- velopments of GPAIS using EC. We briefly outline and describe them to highlight the potential of EAs in design scenarios:\n\u2022 Hyper-parameter optimization: This category has undergone extensive studies over the years, with Genetic Algorithms emerging as a widely utilized approach for addressing the problem [56]. However, alternative heuristics such as Particle Swarm Optimization or Bayesian Optimization also demonstrate efficacy in this context [2]. In the domain of DL, Evolutionary DL typically focuses on discovering the optimal set of hyper-parameters [36,57,58]. Evolutionary Neural Architecture Search (NAS) methods are particularly adept for identifying the best hyper-parameter configurations for models [59]. Furthermore, recent advancements in Neuroevolution have integrated NAS and co-evolutionary EAs, incorporating hyper-parameters as part of the evolutionary process [60].\n\u2022 Automated algorithm selection: The area of Evolutionary NAS in DL has received considerable attention, with various versions of Genetic Algorithms employed to select the optimal structure for DL models [61, 62]. Their capacity to explore large search spaces renders them well-suited to these tasks, as highlighted in [37, 63] both for single and multi-objective EAs. It is important to note that NAS often encompasses both model selection and hyper- parameter optimization, with many proposals treating them as a unified task.\n\u2022 Algorithm construction: For an algorithm to be well constructed, all its parts should be designed consciously, from data preprocessing to the components of the ML model itself. In this context, EC has achieved great success in data preprocessing [28], including feature selection [31,64] and feature construction [65]. Another area within data preprocessing and EC is evolutionary manifold learning, which aims to construct a lower-dimensional representation (embedding) of the structure of a dataset [66]. Techniques within this area can be regarded as a preprocessing step, as the dimensionality of data is reduced while retaining their essential features [67]. When it comes to the construction of the ML model, evolutionary DL proposals are capable of designing DL models across a wide range of domains [45,68,69]. Genetic Programming has been widely used to automate the heuristic design process [70,71] of most phases of a data modeling pipeline, from data preprocessing to the composition of the ML model itself [25]. This family of EC solvers is at the core of AutoML-zero [46], an evolutionary method capable of constructing neural networks from low-level processing primitives.\n3.2 EC-powered AI for enriching GPAIS\nThis section focuses on how EC can be exploited to enhance GPAIS. The need for generalization and adaptability capabilities of GPAIS are two key issues for which EC has demonstrated to succeed. Based on the taxonomy presented previously, enriching GPAIS using EC may be realized following these avenues:\n\u2022 Discovering new behaviors: For GPAIS to effectively adapt to dynamic environments, evolutionary dynamic op- timization emerges as a critical area, offering innovative proposals [72]. Additionally, when combined with multi- objective EAs, this approach further enhances adaptability [73].\n\u2022 Data generation: In situations where GPAIS confronts limited data, the primary objective of EC is to extract high-quality insights from the available data pool. Evolutionary data generation is widely employed across various domains, including addressing imbalance problems [74]. Additionally, open-ended evolution not only generates data but also creates learning environments, enriching the model's understanding [50]. Moreover, EC can generate diverse configurations of the initial model to adapt the model to the data [75]. Finally, quality-diversity optimization offers a comprehensive approach to this category. These methods ensure the generation of a maximally diverse collection of high-performing individuals with desirable properties such as robustness and adaptability to diverse scenarios [21].\n\u2022 Learning to learn: Given that GPAIS often operates with limited data, leveraging information from similar tasks becomes crucial. Evolutionary transfer learning (ETL) and evolutionary transfer optimization (ETO) may not only pass on learned parameters but also representations and operators [76]. Furthermore, the integration of evolutionary dynamic optimization with transfer learning expands the scope of applications in GPAIS scenarios [77,78].\n\u2022 Active learning: This approach has been explored in conjunction with multi-objective optimization, yielding methods capable of leveraging knowledge and discovering high-quality solutions [79]. The Pareto front delineates a region within the search space where good solutions reside, thus enriching the model by locating high-quality solutions [80]. Also, integrating Active Learning with other EC-based approaches, like co-evolutionary EAs, can further enhance interactions and lead to improved solutions [81].\n\u2022 Cooperative and collective learning: Evolutionary multitask optimization (EMO) [82] facilitates knowledge max- imization among tasks by allowing them to learn from each other [83]. EC plays a crucial role in enhancing this process, particularly in GPAIS scenarios where tasks may vary in type, enabling the EC algorithm to exploit synergistic interactions. Furthermore, co-evolutionary EAs aid in information exploitation by facilitating the exchange of information between populations [53,84]. Ensemble learning leverages collective learning of individual models to exploit the synergy between tasks [33]. This approach allows the system to harness the collective knowledge for improved performance. It is worth noting that this category not only enhances existing capabilities but also has the potential to discover new behaviors through cooperation and information exchange, further enhancing the system.\n3.3 Connection among GPAIS, Machine Learning, and Optimization Research areas\nIn this section, we analyze the capabilities of EC for GPAIS by considering their various needs. We examine which properties of GPAIS can be supported by the knowledge and achievements reported from different areas in EA re- search. We complement this analysis with an assessment of the competences of EC research areas for GPAIS by considering their properties:\n\u2022 GPAIS need to adapt their knowledge to tasks that vary over time by exploiting previous knowledge: For these scenarios, EC algorithms for dynamic optimization could be very useful, as it is necessary to adjust the algorithm's search strategy at runtime to adapt the search to the changing optimization landscape. In particular, they could detect new patterns over time, or decide to ignore previous patterns that no longer reflect the current tasks.\n\u2022 GPAIS can perform several tasks simultaneously, including different priorities or objectives: Multi-objective EAs could be used to optimize GPAIS taking into account different objectives and obtaining models with different balances among them. Also, multitask EAs can learn different tasks simultaneously, allowing for the improvement of GPAIS when tackling multiple tasks. Another area of EC that could be applied is co-evolutionary algorithms, by which many algorithms run in parallel and exchange information to improve the search. This co-evolutionary approach could help GPAIS to exploit synergies between tasks by sharing knowledge between them.\n\u2022 GPAIS can address new unseen tasks with few or no new data: To enforce this capability, EC can be employed for simultaneously optimizing multiple AI models, ensuring diversity among their modeled knowledge. This diversity spans a multitude of options, increasing the likelihood of identifying models that exhibit superior performance for new, unseen tasks.\n\u2022 GPAIS should be able to construct/configure themselves autonomously: EC has traditionally been employed for the automatic tuning of ML models across three different granularity levels. At the highest level, EC is used for algorithm selection, a process where they have seen extensive application. Moving to an intermediate level, once the algorithm is determined, EC can configure its parameters to optimize performance. Finally, at the lowest level of granularity, EC is involved in the configuration or construction of model primitives, showing their versatility across all levels of model tuning.\n\u2022 GPAIS should run efficiently by design, both for the training phase and the inference process: Improving the performance of EC algorithms has been widely studied with different techniques that can be transferred to other contexts. For example, to mitigate training costs, an EC algorithm can be employed to reduce the training dataset, creating a reduced dataset with similar performance a technique known as data distillation. Additionally, EC can be used to reduce model complexity, either through pruning or quantization, thereby decreasing both training and inference costs.\n\u2022 GPAIS should explore, evaluate, and decide on actions or sequences of actions in pursuit of specific goals or objectives: This is a common scenario for EC. EC approaches, such as memetic computing, are a solid and robust alternative for searching in complex domains with its exploration-exploitation combination approach. Even more, EC has been traditionally used for reinforcement learning, where the outcomes of actions are used to reinforce the best actions-a technique applied across various domains from games to robotics. Consequently, EC can offer robustness and adaptability even in dynamic environments.\n\u2022 GPAIS should simultaneously tackle multimodal modeling tasks defined over diverse datasets: EC demon- strates versatility in handling various representations seamlessly. EC algorithms tailored for particular representa- tions can work in cooperation to address multimodal tasks effectively. Consequently, the multi-modality of knowl- edge can be improved through EAs, facilitating the integration of heterogeneous information into a cohesive feature space.\n\u2022 GPAIS should learn cooperatively, exchanging knowledge about their learned task(s) and exploiting synergies therefrom: Numerous techniques have been developed to facilitate the exchange of information among models, such as co-evolutionary EAs or multitask optimization.\n\u2022 GPAIS are capable of estimating their confidence when addressing their task(s) and proactively requesting new information when they are uncertain about their outcome: In scenarios where there is a lack of sufficient information, GPAIS should autonomously decide to request additional data. However, to minimize the demand for new data and ensure its effectiveness, it should be judiciously selected and widely distributed. EAs can generate prototypes of new data and select among them based on their potential contribution to the existing dataset. Moreover, an expert can supervise and evaluate the various options proposed by the EC algorithm. This approach allows the expert to focus on evaluating the proposed options, rather than solely on proposing the need for new information.\n3.4 Notable milestones and achievements in EC-GPAIS\nIn this section, we take a closer look at specific case studies represented by proposals that have significantly contributed to the field's advancement in recent years. We illustrate different studies documented in the literature for closed- and open-world EC-GPAIS in Subsections 3.4 and 3.4, respectively. Table 2 shows these successful proposals across different areas. The first two rows correspond to proposals for closed-world GPAIS in which there are no mechanisms for adapting to new tasks. While these proposals represent progress towards open-world GPAIS, they remain in a closed scenario. The last row, separated with a thicker line, is composed of evolutionary approaches in open-world GPAIS, because they incorporate mechanisms to generate diversity and to share knowledge.\nCurrent milestones in closed-world EC-GPAIS\nProposals listed in this table have set a significant course in research, setting the grounds for initial steps in the synergy between EC and GPAIS. Many works in NAS, evolutionary DL and neuroevolution have spurred substantial developments in such areas, as evidenced by comprehensive surveys published over the years [25\u201328,37,45,57,63,85]. The common points between these proposals are the focus on the configuration of the model, by leveraging on EC to evolve weights or hyper-parameters of a neural network-based GPAIS. We begin by describing several case studies related to Hyper-parameter optimization:\n\u2022 NEAT [86] is the foundational stage in the field of neuroevolution. It uses an EA to evolve minimal neural networks towards the creation of deeper network architectures.\n\u2022 EDEN [87] proposes to evolve different types of convolutional, pooling, and fully-connected layers with their hyper- parameters in deep neural networks.\n\u2022 EvoAAA [88] is a NAS proposal linked to autoencoders. In this case, the configuration of the model (architecture, weights, and hyper-parameters) is evolved towards a network with a better accuracy performance.\n\u2022 DENSER [89] belongs to a branch of proposals in NAS that are characterized by the use of a grammar of operators to evolve neural networks. DENSER uses a genetic approach that encodes the macro-structure of the neural network (layers, learning, parameters, etc), whereas a grammatical evolution specifies the parameters of each evolutionary algorithm unit and the valid range of the parameters.\nThe Algorithm selection category differs from the previous category. The emphasis in this second category is placed on the search for the best algorithm rather than on the best hyper-parameter configuration. In NAS and evolutionary DL, authors often overlap these categories, as the network architecture is often encoded as part of the hyper-parameters to be optimized. Therefore, the evolution of the network architecture can yield the best algorithm (neural network) to address the problem at hand. We highlight several works under this category that have achieved a great influence in this strand of literature:\n\u2022 CoDeepNEAT [60] represents one of the most renowned advances in the field of NAS. A co-evolutionary scheme is applied with two populations of blueprints (the backbone of the neural network) and the modules to be inserted in each part of the blueprint. Then, an evolutionary search is run toward achieving the best backbone and modules, alongside their parameters.\n\u2022 LSEIC [90] provides insights about the evolution of large-scale classifiers, intending to automatically search for the best architecture to address the problem at hand. To this end, this work proposes to resort to several encoding strategies and mutation operators in their evolutionary solver.\n\u2022 LEAF [91] is a framework based on CoDeepNeat as their inner engine to evolve networks. It constitutes an evo- lutionary AutoML framework that optimizes not only hyper-parameters but also network architectures and their size.\n\u2022 A-MFEA-RL [92] harnesses the capability of evolutionary multitask optimization to configure GPAIS capable of simultaneously solving multiple reinforcement learning environments. To this end, an evolutionary search is performed over a unified search space that represents the neural network architecture, the number of neurons of each layer, and the presence of shared layers among models.\nOther NAS propose alternative metrics beyond accuracy, particularly focusing on the complexity of networks. These contributions share similar characteristics with the previous ones but mostly incorporate multi-objective evolutionary algorithms to solve for such objectives. We next summarize some of the most well-known representative approaches, which are deeply discussed on [63]:\n\u2022 NSGA-Net [93] represents an application of an evolutionary multi-objective solver (NSGA-II) to find neural net- works that best balance between modeling performance and complexity. NSGA-Net involves an exploration of the space of potential neural network architectures in three steps: a population initialization step that is based on prior knowledge from hand-crafted architectures, an exploration step using crossover and mutation in the architectures, and an exploitation step based on a history of evaluated neural architectures.\n\u2022 NSGANetv2 [94] extends the previous NAS proposal based on a multi-objective evolutionary algorithm that uses two surrogates, one at the architecture level and another at the weights levels. In the architecture level, the surrogate improves sample efficiency. In the weights level, the weights are evolved through a Supernet based on the candidate architectures.\n\u2022 NAT [95] presents a mechanism to automatically design neural networks by leveraging transfer learning with mul- tiple objectives. To realize this goal, NAT utilizes task-specific Supernets that share their knowledge with other subnets within a many-objective evolutionary search process. While this approach progresses towards open-world GPAIS by featuring knowledge sharing, it embodies a closed-world GPAIS as it lacks adaptation mechanisms for new tasks.\nThe next two proposals relate to Algorithm selection with several objectives, focusing on the achievement of the network itself rather than the optimization of their weights:\n\u2022 LEMONADE [96] uses a multi-objective evolutionary solver to search for architectures under multiple objectives. The novelty relies on how LEMONADE tackles resource consumption, using a Lamarckian inheritance mechanism. This mechanism generates warm child networks, starting with the predictive performance of their trained parents. In doing so, morphism operators are applied, using a similar concept to that used in previously explained proposals.\n\u2022 MOENAS-TF-PSI [97] is another multi-objective EA that aims to improve certain solutions on approximation fronts using a local search called potential solution improving. Moreover, it resorts to a metric based on accuracy as a training-free metric to estimate the performance of the evolved network without running any training epoch, reducing the considerable computation cost that is typical of NAS methods.\nThis short glimpse at the recent literature on closed-world EC-GPAIS ends by assessing efforts towards new algorithm construction. In those cases, the goal is to create an entirely new model and its learning algorithm from low-level processing primitives. Two recent proposals fall within this category:\n\u2022 AutoML-zero [46] employs an EA that disrupts the traditional paradigm in closed-world GPAIS by going beyond performance optimization. It not only discovers the optimal hyper-parameters but also develops an entire algorithm for creating a model tailored to a given modeling problem.\n\u2022 MOAZ [98] represents the multi-objective variant of AutoML-zero. The goal is to find solutions over the entire Pareto front by trading off accuracy against the computational complexity of the algorithm. In addition to generating different Pareto-optimal solutions, MOAZ can effectively traverse the search space to improve search efficiency using specialized crossover and mutation operators.\nThese EC-GPAIS have predominantly focused on the performance-driven optimization of GPAIS and on the addition of additional search objectives, such as complexity. However, there is still a road ahead to make these EC-GPAIS compliant with their assumed properties, especially in what refers to their multimodal, multitask nature and their computational efficiency. Closed-world achievements reported to date only consider one single task, require high computational costs, and assume enough available data for the formulation of an objective function that prevents the optimized GPAIS from overfitting.\nCurrent milestones in open-world EC-GPAIS\nMore recent contributions have shifted their scope towards guaranteeing that GPAIS can integrate new task(s) into their learned knowledge, different from the traditional objective-driven search methods that limit the ability of a system to integrate a new task. While there may not be specific surveys on these emerging diversity-driven concepts, several research areas are closely aligned with the principles of open-world GPAIS. The last studies revisited in what follows are precisely based on the generation of diversity and the discovery of diverse model behaviors:\n\u2022 POET [50] focuses on generating diversity via data synthesis. In this work, agents are paired with these newly generated environments to learn from them and, at the same time, the agents' weights are also optimized. Moreover, these agents can even be transferred from one environment to another, using their knowledge to adapt to the other. Diversity is induced through data synthesis, where knowledge is transferred between the environments via the agents, ultimately optimizing the model through the agents' weights.\n\u2022 EGANs [75], framed in the area of Generative Adversarial Networks, serves as an example of generating diversity through the model, particularly in a zero-shot learning environment. By using an evolutionary approach, EGANS initially learn the optimal generator of models. In a subsequent stage, this generator becomes part of another evolu- tionary process to determine the final model.\n\u2022 EUREKA [99] constitutes another open-world GPAIS in which several reinforcement learning tasks are performed at the same time. The evolutionary algorithm evolves several reward functions in a context based on the environment source code. EUREKA generates executable reward functions, improving them with an evolutionary search that iteratively produces batches of reward candidates.\n\u2022 XferNAS [100] introduces an open-world GPAIS framework for knowledge transfer. XferNAS collects source knowledge from multiple tasks and combines this knowledge to generate an architecture for a new task.\n\u2022 ESBMAL [79] integrates Active Learning and EAs for improving data labeling. The EA optimally reports batches of data to enhance the instance selection process, contributing to the set of open-world proposals together with the aforementioned approaches.\nIn our research, we have not encountered any specific work focused on the utilization of EC for algorithm selection or construction in open-world GPAIS. Differently from closed-world GPAIS, the use of new diversity metrics to develop open-world GPAIS can be regarded as a research niche yet to be explored.\nAnother important feature is that current GPAIS usually consider one task in a closed-world setting, which is even rarer in open-world problems. Surprisingly, the adoption of EC has been used for the design and optimization of multitask learning models [101]. We have also highlighted EUREKA as an open-world GPAIS proposal that works with reinforcement learning agents capable of doing several tasks. These detected research niches should stimulate efforts in prospective studies related to GPAIS and EC.\nGenAI systems are usually more autonomous and do not depend on experts. Still, the use of EC can allow them to achieve even higher levels of autonomy, further reducing the need for such an expert. Using another AI to help improve it can be a viable approach. Within the field of GenAI, more and more works are using EC to improve such systems, which is a major focus in the coming years [102]. In this research line, recent works leverage EAs as key algorithms for tasks such as model merging and creation of foundation models [30], evolving code generated by Large Language Models [103], evolving prompts of Large Language Models [104], and generating optimization algorithms [105]."}, {"title": "4 Challenges of harnessing the EC-GPAIS benefits and strategies to address advances", "content": "The integration of EC with GPAIS poses significant challenges, despite their potential benefits. A major difficulty is ensuring synergy between the adaptive nature of EC and the complex decision-making processes of GPAIS. For this reason, we recall these challenges of how EC is capable of adapting to GPAIS for the design and optimization of these systems in Subsection 4.1. To overcome these challenges, we also present several strategies that can be implemented with EC to both design and improve GPAIS. Specifically, we focus on their goal, their importance in the context of GPAIS, and an analysis of the EC-based research areas that can help realize these strategies. This analysis, backed by the graphical summary in Figure 3 and through Sections 4.2 to 4.5, serves as a motivating evidence for the present and future development of EC-GPAIS.\n4.1 Challenges of harnessing the benefits of EC-GPAIS\nThere are many ways in which EC can benefit both closed-world and open-world GPAIS. EC can enhance closed-world GPAIS by facilitating advanced data preprocessing, optimizing hyper-parameters, and adapting models. Additionally,"}, {"title": "4.2 EC-GPAIS by exploiting and adapting existing knowledge", "content": "Why is this strategy important for GPAIS?\nThe development of a system capable of performing a diverse set of tasks requires a significant effort. Therefore", "knowledge": "n\u2022 ETO combines principles from EAs and transfer learning at its core"}, {"title": "EVOLUTIONARY COMPUTATION FOR THE DESIGN AND ENRICHMENT OF GENERAL-PURPOSE ARTIFICIAL INTELLIGENCE SYSTEMS: SURVEY AND PROSPECTS", "authors": ["Javier Poyatos", "Javier Del Ser", "Salvador Garc\u00eda", "Hisao Ishibuchi", "Daniel Molina", "Isaac Triguero", "Bing Xue", "Xin Yao", "Francisco Herrera"], "abstract": "In Artificial Intelligence, there is an increasing demand for adaptive models capable of dealing with a diverse spectrum of learning tasks, surpassing the limitations of systems devised to cope with a single task. The recent emergence of General-Purpose Artificial Intelligence Systems (GPAIS) poses model configuration and adaptability challenges at far greater complexity scales than the optimal design of traditional Machine Learning models. Evolutionary Computation (EC) has been a useful tool for both the design and optimization of Machine Learning models, endowing them with the capability to configure and/or adapt themselves to the task under consideration. Therefore, their application to GPAIS is a natural choice. This paper aims to analyze the role of EC in the field of GPAIS, exploring the use of EC for their design or enrichment. We also match GPAIS properties to Machine Learning areas in which EC has had a notable contribution, highlighting recent milestones of EC for GPAIS. Furthermore, we discuss the challenges of harnessing the benefits of EC for GPAIS, presenting different strategies to both design and improve GPAIS with EC, covering tangential areas, identifying research niches, and outlining potential research directions for EC and GPAIS.", "sections": [{"title": "1 Introduction", "content": "Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that focuses on developing models capable of learning patterns from data. The optimization of these models has been a prominent area of research, yielding significant results [1] by tailoring their structural design and/or hyper-parameters based on several objectives, such as performance, complexity, or robustness, among others [2,3]. The diversity of optimization goals considered to date in the optimization of ML models reflects the capability of these algorithms to tackle different criteria.\nRecently, advances in several areas of ML research such as Deep Learning (DL) [4] and Large Language Models [5,6] \u2013 with chatbots of unprecedented performance like ChatGPT [7] and models trained to generate code to improve the effectiveness of mutation operators in Evolutionary Computation (EC) [8] \u2013 indicate a notable shift towards more generalized AI systems. The key idea of general-purpose AI systems (GPAIS) is their unique capability to execute several, potentially diverse modeling problems, with potentials to expand to tasks for which they were not originally designed. This ability to model several tasks and generalize beyond known problems has been highlighted in recent definitions of GPAIS (see [9] and references therein discussed). As a result, GPAIS have gained significance in the last year due to their flexibility and adaptability across a wide range of applications. The possibility that GPAIS develop emerging generalization capabilities, though still unproven, has drawn much attention for their practical implications in terms of AI safety [10, 11].\nAs stated previously, multiple approaches to optimize ML models have shown significant competence across various ML domains [12]. However, recent studies signal a new research trend focusing on the optimization of GPAIS, moving beyond traditional ML [9, 13\u201316]. Areas of interest in optimization such as open-ended evolution [17\u201320], quality-diversity optimization [21-23] and novelty search [24] have become crucial for GPAIS. In these optimization areas, GPAIS stand at the forefront of the current research. GPAIS exhibits compelling characteristics such as generating new knowledge, adapting to diverse environments, and seamlessly integrating new tasks without performance degradation. Within GPAIS, as in other complex systems, critical decisions shape their design and optimization, addressing challenges like search space dimensionality, evolving objectives, and the need for continuous adaptation. This dynamic landscape offers opportunities to integrate GPAIS with effective optimization techniques.\nIn this regard, EC stands out as a versatile family of algorithms that have spearheaded the design and optimization of ML models [25, 26], making a substantial impact on both scenarios [27,28]. These algorithms have a great impact on other aspects of ML like learning rules [29]. EC has played a pivotal role in various research areas, yielding valuable contributions to the development of high-quality ML models. A notable example of its success is in the area of Large Language Models merging them with Evolutionary Algorithms (EAs) [30].\nEC has shown its capability to evolve programs, solve dynamic optimization problems, or balance several conflicting objectives, among other optimization scenarios. With its history of successes at optimizing narrow AI systems, EC is particularly attractive for the optimization of GPAIS and for tackling the greater challenges it poses compared to narrow AI systems. Coincidentally, certain research areas of EC match some of the core properties of GPAIS, including adaptability to changing problems over time (evolutionary dynamic optimization) or the confluence of multiple objectives in multitask settings (multitask and multi-objective optimization). Additional strengths of EC for GPAIS we may highlight include: EC is domain-agnostic but can incorporate it if available, and it is free from assumptions about data or model properties. EC can simultaneously build model structures and optimize parameters, allowing for creative method development, and is particularly suitable for multi-objective optimization due to its population-based search approach.\nThis paper analyzes the potential of AI-powered AI that consists of AI models enhanced or even designed by another AI model. When using EC algorithms as an additional layer of abstraction for the design or enhance AI models, we call it EC-powered AI, and we will refer to those models as EC-GPAIS. Besides introducing the concept and exposing the benefits of EC when applied to design or enrich GPAIS, this position paper also surveys recent contributions falling within EC-GPAIS, ending up with a prospect of the research directions that can drive future efforts in this area. In particular, the objectives of this work are:\n\u2022 To match the potential of EC with GPAIS, with the aim of fostering more research in this trending area in AI. To do this, we will carry out the following three objectives:\nTo study the design and enhancement of GPAIS using EC with a taxonomy of EC-powered AI. Each category of the taxonomy serves as a guideline on how EC can design or improve AI.\nTo match GPAIS properties with specific ML areas in which EC has made significant contributions, to show that the connection between EC and GPAIS is well-defined, as EC may enable these properties.\nTo show the recent milestones of EC-GPAIS. Under the definition of GPAIS presented in the next section, we illustrate various works that have contributed to the beginning and progress of this field in recent years, showcasing EC-GPAIS' potential.\n\u2022 To discuss the challenges of harnessing the EC-GPAIS benefits and strategies to address advances, focusing on future challenges. To accomplish this, we break it down into the following two sub-objectives:\nTo study the challenges to obtain the benefits of EC in the GPAIS context. Both designing and enriching GPAIS using EC are complex tasks, and we must ensure that we exploit the benefit of their synergy.\nTo explore strategies that can be implemented with EC to both design and improve GPAIS. These strategies will serve as a guide for present and future developments of EC using EC.\nThe rest of the paper is organized as follows: Section 2 covers the background on EC and ML, its relation with GPAIS, and the paradigm of AI-powered AI for GPAIS. Section 3 examines different options to use EC to design and enhance GPAIS, matches properties of GPAIS with ML areas that have benefited from EC, and shows practical cases as milestones in EC-GPAIS developments. After that, Section 4 emphasizes the challenges for harnessing the benefits"}, {"title": "2 Evolutionary Computation and GPAIS", "content": "This section briefly recalls the importance of EC in ML over time and in the new era of model generation in AI, in Subsection 2.1. Then, it explains several characteristics of GPAIS in Subsection 2.2. Finally, it shows the relevance of the paradigm of AI-powered AI concerning GPAIS in Subsection 2.3.\n2.1 Evolutionary Computation in Machine Learning\nEC has provided intelligent mechanisms capable of optimizing models in various environments. As stated by Li et al. [28], who reviewed more than five hundred proposals, these algorithms have been widely used to optimize different stages of ML pipelines, including pre-processing, post-processing, and modeling itself. It is worth noting that within the entire pipeline, evolutionary computation has achieved significant importance in specific branches, such as feature selection [31], feature extraction [32], ensemble modeling [33], and even in enhancing model design alongside other techniques such as Support Vector Machines [34] and Decision Trees [35].\nThe rapid growth of DL in recent years has expanded the application boundaries of this field. Diverse evolutionary proposals have facilitated the optimization of the weights, architecture, and configuration of DL models [36, 37]. EC has significantly contributed to progress in every new topic in ML, owing to the development of algorithms that advance knowledge in these areas. These algorithms have been widely studied due to their applicability across a range of domains [38]. Therefore, in the modern era of AI, where increasingly complex problems arise, the background in evolutionary computation suggests it is poised to become one of the primary mechanisms for the new generation of AI models.\n2.2 GPAIS: Definitions and Properties\nThe study of GPAIS departs from a clear distinction from narrow AI systems, which are instead designed for specific tasks. This differentiation has sparked discussions about the characteristics of an AI system to be classed as GPAIS [14]. A recent study ([9]) provides a comprehensive analysis of GPAIS, posing a clear landmark definition, a characterization, and a classification of these systems through the use of a taxonomy that distinguishes several strategies to build GPAIS. In particular, GPAIS are defined according to the process followed to incorporate a new task into the system, either by retraining the whole system from scratch with that task or by performing an adaptation to solve it while retaining the knowledge captured by the system from previous data. According to [9]:\nDefinition 1 (GPAIS): A General Purpose Artificial Intelligence System (GPAIS) refers to an advanced AI system capable of effectively performing a range of distinct tasks. Its degree of autonomy and ability is determined by several key characteristics, including the capacity to adapt or perform well on new tasks that arise at a future time, the demonstration of competence in domains for which it was not intentionally and specifically trained, the ability to learn from limited data, and the proactive acknowledgment of its own limitations in order to enhance its performance.\nDefinition 1 [9] provides a comprehensive description of the characteristics that GPAIS may possess. In what follows, and based on [9], we briefly present the concepts in GPAIS that will help understand the role that EAs may undertake.\nThe key distinguishing feature of GPAIS vs classical narrow AI lies in the ability to simultaneously tackle multiple learning tasks (whether known or unknown). Taking into consideration what we know about those tasks, Triguero et al. [9] differentiate between two types of GPAIS:\n\u2022 Closed-world GPAIS: It assumes we have data for a given number of tasks, and those will be the only tasks that will be dealt with.\n\u2022 Open-world GPAIS: These systems acknowledge the fact that new tasks may arise and limited (or none) data may be available.\n2.3 AI-powered AI for GPAIS\nThe concept of AI models enhanced or even designed by another AI model is typically known as AI-powered AI. Following the taxonomy proposed in [9], we show a non-exhaustive list of topics in which an AI may be useful to either design or enrich another AI. In terms of designing a general AI model, we may aim at different levels:\n\u2022 Hyper-parameter optimization: While the use of EC for hyper-parameter optimization is not new or uncommon in narrow AI [41, 42], here we focus on the idea of tuning a set of hyper-parameters to solve a range of tasks [43]. In the open world, the challenge lies in finding the best configuration when a new task, potentially with little or no data, arises, exploiting prior knowledge to generalize.\n\u2022 Automated algorithm selection: The previous approach can be further improved by determining both the most appropriate AI algorithm and its hyper-parameters. This would enable a more general AI solution that automatically decides the AI technique to use for one or multiple problems at once. In ML, this is typically referred to as AutoML"}, {"title": "3 Matching the potential of EC with GPAIS: A Taxonomy for EC-powered AI and notable milestones", "content": "As previously discussed, EC can be employed as a new layer of abstraction to facilitate the design or enhancement of AI, called EC-powered AI. This approach is of great significance for the realization of GPAIS. The aforementioned advantages of EC regarding domain agnosticism, data assumptions, and others make it suitable for GPAIS. Figure 2 presents the taxonomy for this kind of approach based on [9]. To show the potential of EC with GPAIS, Subsection 3.1 and 3.2 describe how EC can help to design and enrich GPAIS, respectively. Furthermore, Subsection 3.3 analyzes the suitability of EC in GPAIS by examining its properties and their connection to research areas of ML in which EC has had a substantial contribution. Finally, Subsection 3.4 shows several milestones of EC-GPAIS during the last years.\n3.1 EC-powered AI for designing GPAIS\nBuilding upon the revision established in Section 2.3 and the taxonomy presented in this section, we now explore each category and examine the strategies in which EC has made contributions, serving as a reference for future de- velopments of GPAIS using EC. We briefly outline and describe them to highlight the potential of EAs in design scenarios:\n\u2022 Hyper-parameter optimization: This category has undergone extensive studies over the years, with Genetic Algorithms emerging as a widely utilized approach for addressing the problem [56]. However, alternative heuristics such as Particle Swarm Optimization or Bayesian Optimization also demonstrate efficacy in this context [2]. In the domain of DL, Evolutionary DL typically focuses on discovering the optimal set of hyper-parameters [36,57,58]. Evolutionary Neural Architecture Search (NAS) methods are particularly adept for identifying the best hyper-parameter configurations for models [59]. Furthermore, recent advancements in Neuroevolution have integrated NAS and co-evolutionary EAs, incorporating hyper-parameters as part of the evolutionary process [60].\n\u2022 Automated algorithm selection: The area of Evolutionary NAS in DL has received considerable attention, with various versions of Genetic Algorithms employed to select the optimal structure for DL models [61, 62]. Their capacity to explore large search spaces renders them well-suited to these tasks, as highlighted in [37, 63] both for single and multi-objective EAs. It is important to note that NAS often encompasses both model selection and hyper- parameter optimization, with many proposals treating them as a unified task.\n\u2022 Algorithm construction: For an algorithm to be well constructed, all its parts should be designed consciously, from data preprocessing to the components of the ML model itself. In this context, EC has achieved great success in data preprocessing [28], including feature selection [31,64] and feature construction [65]. Another area within data preprocessing and EC is evolutionary manifold learning, which aims to construct a lower-dimensional representation (embedding) of the structure of a dataset [66]. Techniques within this area can be regarded as a preprocessing step, as the dimensionality of data is reduced while retaining their essential features [67]. When it comes to the construction of the ML model, evolutionary DL proposals are capable of designing DL models across a wide range of domains [45,68,69]. Genetic Programming has been widely used to automate the heuristic design process [70,71] of most phases of a data modeling pipeline, from data preprocessing to the composition of the ML model itself [25]. This family of EC solvers is at the core of AutoML-zero [46], an evolutionary method capable of constructing neural networks from low-level processing primitives.\n3.2 EC-powered AI for enriching GPAIS\nThis section focuses on how EC can be exploited to enhance GPAIS. The need for generalization and adaptability capabilities of GPAIS are two key issues for which EC has demonstrated to succeed. Based on the taxonomy presented previously, enriching GPAIS using EC may be realized following these avenues:\n\u2022 Discovering new behaviors: For GPAIS to effectively adapt to dynamic environments, evolutionary dynamic op- timization emerges as a critical area, offering innovative proposals [72]. Additionally, when combined with multi- objective EAs, this approach further enhances adaptability [73].\n\u2022 Data generation: In situations where GPAIS confronts limited data, the primary objective of EC is to extract high-quality insights from the available data pool. Evolutionary data generation is widely employed across various domains, including addressing imbalance problems [74]. Additionally, open-ended evolution not only generates data but also creates learning environments, enriching the model's understanding [50]. Moreover, EC can generate diverse configurations of the initial model to adapt the model to the data [75]. Finally, quality-diversity optimization offers a comprehensive approach to this category. These methods ensure the generation of a maximally diverse collection of high-performing individuals with desirable properties such as robustness and adaptability to diverse scenarios [21].\n\u2022 Learning to learn: Given that GPAIS often operates with limited data, leveraging information from similar tasks becomes crucial. Evolutionary transfer learning (ETL) and evolutionary transfer optimization (ETO) may not only pass on learned parameters but also representations and operators [76]. Furthermore, the integration of evolutionary dynamic optimization with transfer learning expands the scope of applications in GPAIS scenarios [77,78].\n\u2022 Active learning: This approach has been explored in conjunction with multi-objective optimization, yielding methods capable of leveraging knowledge and discovering high-quality solutions [79]. The Pareto front delineates a region within the search space where good solutions reside, thus enriching the model by locating high-quality solutions [80]. Also, integrating Active Learning with other EC-based approaches, like co-evolutionary EAs, can further enhance interactions and lead to improved solutions [81].\n\u2022 Cooperative and collective learning: Evolutionary multitask optimization (EMO) [82] facilitates knowledge max- imization among tasks by allowing them to learn from each other [83]. EC plays a crucial role in enhancing this process, particularly in GPAIS scenarios where tasks may vary in type, enabling the EC algorithm to exploit synergistic interactions. Furthermore, co-evolutionary EAs aid in information exploitation by facilitating the exchange of information between populations [53,84]. Ensemble learning leverages collective learning of individual models to exploit the synergy between tasks [33]. This approach allows the system to harness the collective knowledge for improved performance. It is worth noting that this category not only enhances existing capabilities but also has the potential to discover new behaviors through cooperation and information exchange, further enhancing the system.\n3.3 Connection among GPAIS, Machine Learning, and Optimization Research areas\nIn this section, we analyze the capabilities of EC for GPAIS by considering their various needs. We examine which properties of GPAIS can be supported by the knowledge and achievements reported from different areas in EA re- search. We complement this analysis with an assessment of the competences of EC research areas for GPAIS by considering their properties:\n\u2022 GPAIS need to adapt their knowledge to tasks that vary over time by exploiting previous knowledge: For these scenarios, EC algorithms for dynamic optimization could be very useful, as it is necessary to adjust the algorithm's search strategy at runtime to adapt the search to the changing optimization landscape. In particular, they could detect new patterns over time, or decide to ignore previous patterns that no longer reflect the current tasks.\n\u2022 GPAIS can perform several tasks simultaneously, including different priorities or objectives: Multi-objective EAs could be used to optimize GPAIS taking into account different objectives and obtaining models with different balances among them. Also, multitask EAs can learn different tasks simultaneously, allowing for the improvement of GPAIS when tackling multiple tasks. Another area of EC that could be applied is co-evolutionary algorithms, by which many algorithms run in parallel and exchange information to improve the search. This co-evolutionary approach could help GPAIS to exploit synergies between tasks by sharing knowledge between them.\n\u2022 GPAIS can address new unseen tasks with few or no new data: To enforce this capability, EC can be employed for simultaneously optimizing multiple AI models, ensuring diversity among their modeled knowledge. This diversity spans a multitude of options, increasing the likelihood of identifying models that exhibit superior performance for new, unseen tasks.\n\u2022 GPAIS should be able to construct/configure themselves autonomously: EC has traditionally been employed for the automatic tuning of ML models across three different granularity levels. At the highest level, EC is used for algorithm selection, a process where they have seen extensive application. Moving to an intermediate level, once the algorithm is determined, EC can configure its parameters to optimize performance. Finally, at the lowest level of granularity, EC is involved in the configuration or construction of model primitives, showing their versatility across all levels of model tuning.\n\u2022 GPAIS should run efficiently by design, both for the training phase and the inference process: Improving the performance of EC algorithms has been widely studied with different techniques that can be transferred to other contexts. For example, to mitigate training costs, an EC algorithm can be employed to reduce the training dataset, creating a reduced dataset with similar performance a technique known as data distillation. Additionally, EC can be used to reduce model complexity, either through pruning or quantization, thereby decreasing both training and inference costs.\n\u2022 GPAIS should explore, evaluate, and decide on actions or sequences of actions in pursuit of specific goals or objectives: This is a common scenario for EC. EC approaches, such as memetic computing, are a solid and robust alternative for searching in complex domains with its exploration-exploitation combination approach. Even more, EC has been traditionally used for reinforcement learning, where the outcomes of actions are used to reinforce the best actions-a technique applied across various domains from games to robotics. Consequently, EC can offer robustness and adaptability even in dynamic environments.\n\u2022 GPAIS should simultaneously tackle multimodal modeling tasks defined over diverse datasets: EC demon- strates versatility in handling various representations seamlessly. EC algorithms tailored for particular representa- tions can work in cooperation to address multimodal tasks effectively. Consequently, the multi-modality of knowl- edge can be improved through EAs, facilitating the integration of heterogeneous information into a cohesive feature space.\n\u2022 GPAIS should learn cooperatively, exchanging knowledge about their learned task(s) and exploiting synergies therefrom: Numerous techniques have been developed to facilitate the exchange of information among models, such as co-evolutionary EAs or multitask optimization.\n\u2022 GPAIS are capable of estimating their confidence when addressing their task(s) and proactively requesting new information when they are uncertain about their outcome: In scenarios where there is a lack of sufficient information, GPAIS should autonomously decide to request additional data. However, to minimize the demand for new data and ensure its effectiveness, it should be judiciously selected and widely distributed. EAs can generate prototypes of new data and select among them based on their potential contribution to the existing dataset. Moreover, an expert can supervise and evaluate the various options proposed by the EC algorithm. This approach allows the expert to focus on evaluating the proposed options, rather than solely on proposing the need for new information.\n3.4 Notable milestones and achievements in EC-GPAIS\nIn this section, we take a closer look at specific case studies represented by proposals that have significantly contributed to the field's advancement in recent years. We illustrate different studies documented in the literature for closed- and open-world EC-GPAIS in Subsections 3.4 and 3.4, respectively. Table 2 shows these successful proposals across different areas. The first two rows correspond to proposals for closed-world GPAIS in which there are no mechanisms for adapting to new tasks. While these proposals represent progress towards open-world GPAIS, they remain in a closed scenario. The last row, separated with a thicker line, is composed of evolutionary approaches in open-world GPAIS, because they incorporate mechanisms to generate diversity and to share knowledge.\nCurrent milestones in closed-world EC-GPAIS\nProposals listed in this table have set a significant course in research, setting the grounds for initial steps in the synergy between EC and GPAIS. Many works in NAS, evolutionary DL and neuroevolution have spurred substantial developments in such areas, as evidenced by comprehensive surveys published over the years [25\u201328,37,45,57,63,85]. The common points between these proposals are the focus on the configuration of the model, by leveraging on EC to evolve weights or hyper-parameters of a neural network-based GPAIS. We begin by describing several case studies related to Hyper-parameter optimization:\n\u2022 NEAT [86] is the foundational stage in the field of neuroevolution. It uses an EA to evolve minimal neural networks towards the creation of deeper network architectures.\n\u2022 EDEN [87] proposes to evolve different types of convolutional, pooling, and fully-connected layers with their hyper- parameters in deep neural networks.\n\u2022 EvoAAA [88] is a NAS proposal linked to autoencoders. In this case, the configuration of the model (architecture, weights, and hyper-parameters) is evolved towards a network with a better accuracy performance.\n\u2022 DENSER [89] belongs to a branch of proposals in NAS that are characterized by the use of a grammar of operators to evolve neural networks. DENSER uses a genetic approach that encodes the macro-structure of the neural network (layers, learning, parameters, etc), whereas a grammatical evolution specifies the parameters of each evolutionary algorithm unit and the valid range of the parameters.\nThe Algorithm selection category differs from the previous category. The emphasis in this second category is placed on the search for the best algorithm rather than on the best hyper-parameter configuration. In NAS and evolutionary DL, authors often overlap these categories, as the network architecture is often encoded as part of the hyper-parameters to be optimized. Therefore, the evolution of the network architecture can yield the best algorithm (neural network) to address the problem at hand. We highlight several works under this category that have achieved a great influence in this strand of literature:\n\u2022 CoDeepNEAT [60] represents one of the most renowned advances in the field of NAS. A co-evolutionary scheme is applied with two populations of blueprints (the backbone of the neural network) and the modules to be inserted in each part of the blueprint. Then, an evolutionary search is run toward achieving the best backbone and modules, alongside their parameters.\n\u2022 LSEIC [90] provides insights about the evolution of large-scale classifiers, intending to automatically search for the best architecture to address the problem at hand. To this end, this work proposes to resort to several encoding strategies and mutation operators in their evolutionary solver.\n\u2022 LEAF [91] is a framework based on CoDeepNeat as their inner engine to evolve networks. It constitutes an evo- lutionary AutoML framework that optimizes not only hyper-parameters but also network architectures and their size.\n\u2022 A-MFEA-RL [92] harnesses the capability of evolutionary multitask optimization to configure GPAIS capable of simultaneously solving multiple reinforcement learning environments. To this end, an evolutionary search is performed over a unified search space that represents the neural network architecture, the number of neurons of each layer, and the presence of shared layers among models.\nOther NAS propose alternative metrics beyond accuracy, particularly focusing on the complexity of networks. These contributions share similar characteristics with the previous ones but mostly incorporate multi-objective evolutionary algorithms to solve for such objectives. We next summarize some of the most well-known representative approaches, which are deeply discussed on [63]:\n\u2022 NSGA-Net [93] represents an application of an evolutionary multi-objective solver (NSGA-II) to find neural net- works that best balance between modeling performance and complexity. NSGA-Net involves an exploration of the space of potential neural network architectures in three steps: a population initialization step that is based on prior knowledge from hand-crafted architectures, an exploration step using crossover and mutation in the architectures, and an exploitation step based on a history of evaluated neural architectures.\n\u2022 NSGANetv2 [94] extends the previous NAS proposal based on a multi-objective evolutionary algorithm that uses two surrogates, one at the architecture level and another at the weights levels. In the architecture level, the surrogate improves sample efficiency. In the weights level, the weights are evolved through a Supernet based on the candidate architectures.\n\u2022 NAT [95] presents a mechanism to automatically design neural networks by leveraging transfer learning with mul- tiple objectives. To realize this goal, NAT utilizes task-specific Supernets that share their knowledge with other subnets within a many-objective evolutionary search process. While this approach progresses towards open-world GPAIS by featuring knowledge sharing, it embodies a closed-world GPAIS as it lacks adaptation mechanisms for new tasks.\nThe next two proposals relate to Algorithm selection with several objectives, focusing on the achievement of the network itself rather than the optimization of their weights:\n\u2022 LEMONADE [96] uses a multi-objective evolutionary solver to search for architectures under multiple objectives. The novelty relies on how LEMONADE tackles resource consumption, using a Lamarckian inheritance mechanism. This mechanism generates warm child networks, starting with the predictive performance of their trained parents. In doing so, morphism operators are applied, using a similar concept to that used in previously explained proposals.\n\u2022 MOENAS-TF-PSI [97] is another multi-objective EA that aims to improve certain solutions on approximation fronts using a local search called potential solution improving. Moreover, it resorts to a metric based on accuracy as a training-free metric to estimate the performance of the evolved network without running any training epoch, reducing the considerable computation cost that is typical of NAS methods.\nThis short glimpse at the recent literature on closed-world EC-GPAIS ends by assessing efforts towards new algorithm construction. In those cases, the goal is to create an entirely new model and its learning algorithm from low-level processing primitives. Two recent proposals fall within this category:\n\u2022 AutoML-zero [46] employs an EA that disrupts the traditional paradigm in closed-world GPAIS by going beyond performance optimization. It not only discovers the optimal hyper-parameters but also develops an entire algorithm for creating a model tailored to a given modeling problem.\n\u2022 MOAZ [98] represents the multi-objective variant of AutoML-zero. The goal is to find solutions over the entire Pareto front by trading off accuracy against the computational complexity of the algorithm. In addition to generating different Pareto-optimal solutions, MOAZ can effectively traverse the search space to improve search efficiency using specialized crossover and mutation operators.\nThese EC-GPAIS have predominantly focused on the performance-driven optimization of GPAIS and on the addition of additional search objectives, such as complexity. However, there is still a road ahead to make these EC-GPAIS compliant with their assumed properties, especially in what refers to their multimodal, multitask nature and their computational efficiency. Closed-world achievements reported to date only consider one single task, require high computational costs, and assume enough available data for the formulation of an objective function that prevents the optimized GPAIS from overfitting.\nCurrent milestones in open-world EC-GPAIS\nMore recent contributions have shifted their scope towards guaranteeing that GPAIS can integrate new task(s) into their learned knowledge, different from the traditional objective-driven search methods that limit the ability of a system to integrate a new task. While there may not be specific surveys on these emerging diversity-driven concepts, several research areas are closely aligned with the principles of open-world GPAIS. The last studies revisited in what follows are precisely based on the generation of diversity and the discovery of diverse model behaviors:\n\u2022 POET [50] focuses on generating diversity via data synthesis. In this work, agents are paired with these newly generated environments to learn from them and, at the same time, the agents' weights are also optimized. Moreover, these agents can even be transferred from one environment to another, using their knowledge to adapt to the other. Diversity is induced through data synthesis, where knowledge is transferred between the environments via the agents, ultimately optimizing the model through the agents' weights.\n\u2022 EGANs [75], framed in the area of Generative Adversarial Networks, serves as an example of generating diversity through the model, particularly in a zero-shot learning environment. By using an evolutionary approach, EGANS initially learn the optimal generator of models. In a subsequent stage, this generator becomes part of another evolu- tionary process to determine the final model.\n\u2022 EUREKA [99] constitutes another open-world GPAIS in which several reinforcement learning tasks are performed at the same time. The evolutionary algorithm evolves several reward functions in a context based on the environment source code. EUREKA generates executable reward functions, improving them with an evolutionary search that iteratively produces batches of reward candidates.\n\u2022 XferNAS [100] introduces an open-world GPAIS framework for knowledge transfer. XferNAS collects source knowledge from multiple tasks and combines this knowledge to generate an architecture for a new task.\n\u2022 ESBMAL [79] integrates Active Learning and EAs for improving data labeling. The EA optimally reports batches of data to enhance the instance selection process, contributing to the set of open-world proposals together with the aforementioned approaches.\nIn our research, we have not encountered any specific work focused on the utilization of EC for algorithm selection or construction in open-world GPAIS. Differently from closed-world GPAIS, the use of new diversity metrics to develop open-world GPAIS can be regarded as a research niche yet to be explored.\nAnother important feature is that current GPAIS usually consider one task in a closed-world setting, which is even rarer in open-world problems. Surprisingly, the adoption of EC has been used for the design and optimization of multitask learning models [101]. We have also highlighted EUREKA as an open-world GPAIS proposal that works with reinforcement learning agents capable of doing several tasks. These detected research niches should stimulate efforts in prospective studies related to GPAIS and EC.\nGenAI systems are usually more autonomous and do not depend on experts. Still, the use of EC can allow them to achieve even higher levels of autonomy, further reducing the need for such an expert. Using another AI to help improve it can be a viable approach. Within the field of GenAI, more and more works are using EC to improve such systems, which is a major focus in the coming years [102]. In this research line, recent works leverage EAs as key algorithms for tasks such as model merging and creation of foundation models [30], evolving code generated by Large Language Models [103], evolving prompts of Large Language Models [104], and generating optimization algorithms [105]."}, {"title": "4 Challenges of harnessing the EC-GPAIS benefits and strategies to address advances", "content": "The integration of EC with GPAIS poses significant challenges, despite their potential benefits. A major difficulty is ensuring synergy between the adaptive nature of EC and the complex decision-making processes of GPAIS. For this reason, we recall these challenges of how EC is capable of adapting to GPAIS for the design and optimization of these systems in Subsection 4.1. To overcome these challenges, we also present several strategies that can be implemented with EC to both design and improve GPAIS. Specifically, we focus on their goal, their importance in the context of GPAIS, and an analysis of the EC-based research areas that can help realize these strategies. This analysis, backed by the graphical summary in Figure 3 and through Sections 4.2 to 4.5, serves as a motivating evidence for the present and future development of EC-GPAIS.\n4.1 Challenges of harnessing the benefits of EC-GPAIS\nThere are many ways in which EC can benefit both closed-world and open-world GPAIS. EC can enhance closed-world GPAIS by facilitating advanced data preprocessing, optimizing hyper-parameters, and adapting models. Additionally,"}, {"title": "4.2 EC-GPAIS by exploiting and adapting existing knowledge", "content": "Why is this strategy important for GPAIS?\nThe development of a system capable of performing a diverse set of tasks requires a significant effort. Therefore", "76": "EMO [82", "106": "have shown to be strategies to optimize previously acquired knowledge. Therefore", "knowledge": "n\u2022 ETO combines principles from EAs and transfer learning at its core, knowledge or solutions learned from an opti- mization problem in a source domain to improve an optimization process in another related domain. When solu- tions to such problems represent the knowledge of the GPAIS (i.e. its learned parameters), ETO can optimize the adaptation of the transferred knowledge to the target problem by fine-tuning or re-configuring it to better suit the characteristics of the target domain.\n\u2022 ETL aims to leverage knowledge from related domains to improve performance in a different domain, but focuses on improving the learning process of the model using EC through transfer learning based on models, features and/or instances. ETL has been widely used in NAS, with works like XferNAS, in which a process of transfer learning is applied to initiate architectures for a new task, so the knowledge in other related problems is transferred to create a warm-started network [100", "107": "where the process of knowledge transfer between tasks in similar domains is expedited by pruning the unnecessary components of the neural network using EAs, so the GPAIS can autonomously generalize to other tasks while retaining knowledge from the learned source tasks.\n\u2022 Differently, EMO can be used"}]}]}