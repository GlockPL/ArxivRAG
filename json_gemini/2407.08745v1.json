{"title": "EVOLUTIONARY COMPUTATION FOR THE DESIGN AND ENRICHMENT OF GENERAL-PURPOSE ARTIFICIAL INTELLIGENCE SYSTEMS: SURVEY AND PROSPECTS", "authors": ["Javier Poyatos", "Javier Del Ser", "Salvador Garc\u00eda", "Hisao Ishibuchi", "Daniel Molina", "Isaac Triguero", "Bing Xue", "Xin Yao", "Francisco Herrera"], "abstract": "In Artificial Intelligence, there is an increasing demand for adaptive models capable of dealing with a diverse spectrum of learning tasks, surpassing the limitations of systems devised to cope with a single task. The recent emergence of General-Purpose Artificial Intelligence Systems (GPAIS) poses model configuration and adaptability challenges at far greater complexity scales than the optimal design of traditional Machine Learning models. Evolutionary Computation (EC) has been a useful tool for both the design and optimization of Machine Learning models, endowing them with the capability to configure and/or adapt themselves to the task under consideration. Therefore, their application to GPAIS is a natural choice. This paper aims to analyze the role of EC in the field of GPAIS, exploring the use of EC for their design or enrichment. We also match GPAIS properties to Machine Learning areas in which EC has had a notable contribution, highlighting recent milestones of EC for GPAIS. Furthermore, we discuss the challenges of harnessing the benefits of EC for GPAIS, presenting different strategies to both design and improve GPAIS with EC, covering tangential areas, identifying research niches, and outlining potential research directions for EC and GPAIS.", "sections": [{"title": "1 Introduction", "content": "Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that focuses on developing models capable of learning patterns from data. The optimization of these models has been a prominent area of research, yielding significant results [1] by tailoring their structural design and/or hyper-parameters based on several objectives, such as performance, complexity, or robustness, among others [2,3]. The diversity of optimization goals considered to date in the optimization of ML models reflects the capability of these algorithms to tackle different criteria.\nRecently, advances in several areas of ML research such as Deep Learning (DL) [4] and Large Language Models [5,6] \u2013 with chatbots of unprecedented performance like ChatGPT [7] and models trained to generate code to improve the effectiveness of mutation operators in Evolutionary Computation (EC) [8] \u2013 indicate a notable shift towards more generalized AI systems. The key idea of general-purpose AI systems (GPAIS) is their unique capability to execute several, potentially diverse modeling problems, with potentials to expand to tasks for which they were not originally designed. This ability to model several tasks and generalize beyond known problems has been highlighted in recent definitions of GPAIS (see [9] and references therein discussed). As a result, GPAIS have gained significance in the last year due to their flexibility and adaptability across a wide range of applications. The possibility that GPAIS develop emerging generalization capabilities, though still unproven, has drawn much attention for their practical implications in terms of AI safety [10, 11].\nAs stated previously, multiple approaches to optimize ML models have shown significant competence across various ML domains [12]. However, recent studies signal a new research trend focusing on the optimization of GPAIS, moving beyond traditional ML [9, 13\u201316]. Areas of interest in optimization such as open-ended evolution [17\u201320], quality-diversity optimization [21-23] and novelty search [24] have become crucial for GPAIS. In these optimization areas, GPAIS stand at the forefront of the current research. GPAIS exhibits compelling characteristics such as generating new knowledge, adapting to diverse environments, and seamlessly integrating new tasks without performance degradation. Within GPAIS, as in other complex systems, critical decisions shape their design and optimization, addressing challenges like search space dimensionality, evolving objectives, and the need for continuous adaptation. This dynamic landscape offers opportunities to integrate GPAIS with effective optimization techniques.\nIn this regard, EC stands out as a versatile family of algorithms that have spearheaded the design and optimization of ML models [25, 26], making a substantial impact on both scenarios [27,28]. These algorithms have a great impact on other aspects of ML like learning rules [29]. EC has played a pivotal role in various research areas, yielding valuable contributions to the development of high-quality ML models. A notable example of its success is in the area of Large Language Models merging them with Evolutionary Algorithms (EAs) [30].\nEC has shown its capability to evolve programs, solve dynamic optimization problems, or balance several conflicting objectives, among other optimization scenarios. With its history of successes at optimizing narrow AI systems, EC is particularly attractive for the optimization of GPAIS and for tackling the greater challenges it poses compared to narrow AI systems. Coincidentally, certain research areas of EC match some of the core properties of GPAIS, including adaptability to changing problems over time (evolutionary dynamic optimization) or the confluence of multiple objectives in multitask settings (multitask and multi-objective optimization). Additional strengths of EC for GPAIS we may highlight include: EC is domain-agnostic but can incorporate it if available, and it is free from assumptions about data or model properties. EC can simultaneously build model structures and optimize parameters, allowing for creative method development, and is particularly suitable for multi-objective optimization due to its population-based search approach.\nThis paper analyzes the potential of AI-powered AI that consists of AI models enhanced or even designed by another AI model. When using EC algorithms as an additional layer of abstraction for the design or enhance AI models, we call it EC-powered AI, and we will refer to those models as EC-GPAIS. Besides introducing the concept and exposing the benefits of EC when applied to design or enrich GPAIS, this position paper also surveys recent contributions falling within EC-GPAIS, ending up with a prospect of the research directions that can drive future efforts in this area. In particular, the objectives of this work are:\n\u2022 To match the potential of EC with GPAIS, with the aim of fostering more research in this trending area in AI. To do this, we will carry out the following three objectives:\nTo study the design and enhancement of GPAIS using EC with a taxonomy of EC-powered AI. Each category of the taxonomy serves as a guideline on how EC can design or improve AI.\nTo match GPAIS properties with specific ML areas in which EC has made significant contributions, to show that the connection between EC and GPAIS is well-defined, as EC may enable these properties.\nTo show the recent milestones of EC-GPAIS. Under the definition of GPAIS presented in the next section, we illustrate various works that have contributed to the beginning and progress of this field in recent years, showcasing EC-GPAIS' potential.\n\u2022 To discuss the challenges of harnessing the EC-GPAIS benefits and strategies to address advances, focusing on future challenges. To accomplish this, we break it down into the following two sub-objectives:\nTo study the challenges to obtain the benefits of EC in the GPAIS context. Both designing and enriching GPAIS using EC are complex tasks, and we must ensure that we exploit the benefit of their synergy.\nTo explore strategies that can be implemented with EC to both design and improve GPAIS. These strategies will serve as a guide for present and future developments of GPAIS using EC.\nThe rest of the paper is organized as follows: Section 2 covers the background on EC and ML, its relation with GPAIS, and the paradigm of AI-powered AI for GPAIS. Section 3 examines different options to use EC to design and enhance GPAIS, matches properties of GPAIS with ML areas that have benefited from EC, and shows practical cases as milestones in EC-GPAIS developments. After that, Section 4 emphasizes the challenges for harnessing the benefits"}, {"title": "2 Evolutionary Computation and GPAIS", "content": "This section briefly recalls the importance of EC in ML over time and in the new era of model generation in AI, in Subsection 2.1. Then, it explains several characteristics of GPAIS in Subsection 2.2. Finally, it shows the relevance of the paradigm of AI-powered AI concerning GPAIS in Subsection 2.3."}, {"title": "2.1 Evolutionary Computation in Machine Learning", "content": "EC has provided intelligent mechanisms capable of optimizing models in various environments. As stated by Li et al. [28], who reviewed more than five hundred proposals, these algorithms have been widely used to optimize different stages of ML pipelines, including pre-processing, post-processing, and modeling itself. It is worth noting that within the entire pipeline, evolutionary computation has achieved significant importance in specific branches, such as feature selection [31], feature extraction [32], ensemble modeling [33], and even in enhancing model design alongside other techniques such as Support Vector Machines [34] and Decision Trees [35].\nThe rapid growth of DL in recent years has expanded the application boundaries of this field. Diverse evolutionary proposals have facilitated the optimization of the weights, architecture, and configuration of DL models [36, 37]. EC has significantly contributed to progress in every new topic in ML, owing to the development of algorithms that advance knowledge in these areas. These algorithms have been widely studied due to their applicability across a range of domains [38]. Therefore, in the modern era of AI, where increasingly complex problems arise, the background in evolutionary computation suggests it is poised to become one of the primary mechanisms for the new generation of AI models."}, {"title": "2.2 GPAIS: Definitions and Properties", "content": "The study of GPAIS departs from a clear distinction from narrow AI systems, which are instead designed for spe- cific tasks. This differentiation has sparked discussions about the characteristics of an AI system to be classed as GPAIS [14]. A recent study ([9]) provides a comprehensive analysis of GPAIS, posing a clear landmark definition, a characterization, and a classification of these systems through the use of a taxonomy that distinguishes several strate- gies to build GPAIS. In particular, GPAIS are defined according to the process followed to incorporate a new task into the system, either by retraining the whole system from scratch with that task or by performing an adaptation to solve it while retaining the knowledge captured by the system from previous data. According to [9]:\nDefinition 1 (GPAIS): A General Purpose Artificial Intelligence System (GPAIS) refers to an advanced AI sys- tem capable of effectively performing a range of distinct tasks. Its degree of autonomy and ability is determined by several key characteristics, including the capacity to adapt or perform well on new tasks that arise at a future time, the demonstration of competence in domains for which it was not intentionally and specifically trained, the ability to learn from limited data, and the proactive acknowledgment of its own limitations in order to enhance its performance.\nDefinition 1 [9] provides a comprehensive description of the characteristics that GPAIS may possess. In what follows, and based on [9], we briefly present the concepts in GPAIS that will help understand the role that EAs may undertake.\nThe key distinguishing feature of GPAIS vs classical narrow AI lies in the ability to simultaneously tackle multiple learning tasks (whether known or unknown). Taking into consideration what we know about those tasks, Triguero et al. [9] differentiate between two types of GPAIS:\n\u2022 Closed-world GPAIS: It assumes we have data for a given number of tasks, and those will be the only tasks that will be dealt with.\n\u2022 Open-world GPAIS: These systems acknowledge the fact that new tasks may arise and limited (or none) data may be available."}, {"title": "2.3 AI-powered AI for GPAIS", "content": "The concept of AI models enhanced or even designed by another AI model is typically known as AI-powered AI. Following the taxonomy proposed in [9], we show a non-exhaustive list of topics in which an AI may be useful to either design or enrich another AI. In terms of designing a general AI model, we may aim at different levels:\n\u2022 Hyper-parameter optimization: While the use of EC for hyper-parameter optimization is not new or uncommon in narrow AI [41, 42], here we focus on the idea of tuning a set of hyper-parameters to solve a range of tasks [43]. In the open world, the challenge lies in finding the best configuration when a new task, potentially with little or no data, arises, exploiting prior knowledge to generalize.\n\u2022 Automated algorithm selection: The previous approach can be further improved by determining both the most appropriate AI algorithm and its hyper-parameters. This would enable a more general AI solution that automatically decides the AI technique to use for one or multiple problems at once. In ML, this is typically referred to as AutoML [44]."}, {"title": "3 Matching the potential of EC with GPAIS: A Taxonomy for EC-powered AI and notable milestones", "content": "As previously discussed, EC can be employed as a new layer of abstraction to facilitate the design or enhancement of AI, called EC-powered AI. This approach is of great significance for the realization of GPAIS. The aforementioned advantages of EC regarding domain agnosticism, data assumptions, and others make it suitable for GPAIS. To show the potential of EC with GPAIS, Subsection 3.1 and 3.2 describe how EC can help to design and enrich GPAIS, respectively. Furthermore, Subsection 3.3 analyzes the suitability of EC in GPAIS by examining its properties and their connection to research areas of ML in which EC has had a substantial contribution. Finally, Subsection 3.4 shows several milestones of EC-GPAIS during the last years."}, {"title": "3.1 EC-powered AI for designing GPAIS", "content": "Building upon the revision established in Section 2.3 and the taxonomy presented in this section, we now explore each category and examine the strategies in which EC has made contributions, serving as a reference for future de- velopments of GPAIS using EC. We briefly outline and describe them to highlight the potential of EAs in design scenarios:\n\u2022 Hyper-parameter optimization: This category has undergone extensive studies over the years, with Genetic Algo- rithms emerging as a widely utilized approach for addressing the problem [56]. However, alternative heuristics such as Particle Swarm Optimization or Bayesian Optimization also demonstrate efficacy in this context [2]. In the do- main of DL, Evolutionary DL typically focuses on discovering the optimal set of hyper-parameters [36,57,58]. Evo- lutionary Neural Architecture Search (NAS) methods are particularly adept for identifying the best hyper-parameter configurations for models [59]. Furthermore, recent advancements in Neuroevolution have integrated NAS and co-evolutionary EAs, incorporating hyper-parameters as part of the evolutionary process [60].\n\u2022 Automated algorithm selection: The area of Evolutionary NAS in DL has received considerable attention, with various versions of Genetic Algorithms employed to select the optimal structure for DL models [61, 62]. Their capacity to explore large search spaces renders them well-suited to these tasks, as highlighted in [37, 63] both for single and multi-objective EAs. It is important to note that NAS often encompasses both model selection and hyper- parameter optimization, with many proposals treating them as a unified task."}, {"title": "3.2 EC-powered AI for enriching GPAIS", "content": "This section focuses on how EC can be exploited to enhance GPAIS. The need for generalization and adaptability capabilities of GPAIS are two key issues for which EC has demonstrated to succeed. Based on the taxonomy presented previously, enriching GPAIS using EC may be realized following these avenues:\n\u2022 Discovering new behaviors: For GPAIS to effectively adapt to dynamic environments, evolutionary dynamic op- timization emerges as a critical area, offering innovative proposals [72]. Additionally, when combined with multi- objective EAs, this approach further enhances adaptability [73].\n\u2022 Data generation: In situations where GPAIS confronts limited data, the primary objective of EC is to extract high-quality insights from the available data pool. Evolutionary data generation is widely employed across various domains, including addressing imbalance problems [74]. Additionally, open-ended evolution not only generates data but also creates learning environments, enriching the model's understanding [50]. Moreover, EC can generate diverse configurations of the initial model to adapt the model to the data [75]. Finally, quality-diversity optimization offers a comprehensive approach to this category. These methods ensure the generation of a maximally diverse collection of high-performing individuals with desirable properties such as robustness and adaptability to diverse scenarios [21].\n\u2022 Learning to learn: Given that GPAIS often operates with limited data, leveraging information from similar tasks becomes crucial. Evolutionary transfer learning (ETL) and evolutionary transfer optimization (ETO) may not only pass on learned parameters but also representations and operators [76]. Furthermore, the integration of evolutionary dynamic optimization with transfer learning expands the scope of applications in GPAIS scenarios [77,78].\n\u2022 Active learning: This approach has been explored in conjunction with multi-objective optimization, yielding meth- ods capable of leveraging knowledge and discovering high-quality solutions [79]. The Pareto front delineates a region within the search space where good solutions reside, thus enriching the model by locating high-quality solu- tions [80]. Also, integrating Active Learning with other EC-based approaches, like co-evolutionary EAs, can further enhance interactions and lead to improved solutions [81]."}, {"title": "3.3 Connection among GPAIS, Machine Learning, and Optimization Research areas", "content": "In this section, we analyze the capabilities of EC for GPAIS by considering their various needs. We examine which properties of GPAIS can be supported by the knowledge and achievements reported from different areas in EA re- search. We complement this analysis with an assessment of the competences of EC research areas for GPAIS by considering their properties:\n\u2022 GPAIS need to adapt their knowledge to tasks that vary over time by exploiting previous knowledge: For these scenarios, EC algorithms for dynamic optimization could be very useful, as it is necessary to adjust the algorithm's search strategy at runtime to adapt the search to the changing optimization landscape. In particular, they could detect new patterns over time, or decide to ignore previous patterns that no longer reflect the current tasks.\n\u2022 GPAIS can perform several tasks simultaneously, including different priorities or objectives: Multi-objective EAs could be used to optimize GPAIS taking into account different objectives and obtaining models with different balances among them. Also, multitask EAs can learn different tasks simultaneously, allowing for the improvement of GPAIS when tackling multiple tasks. Another area of EC that could be applied is co-evolutionary algorithms, by which many algorithms run in parallel and exchange information to improve the search. This co-evolutionary approach could help GPAIS to exploit synergies between tasks by sharing knowledge between them.\n\u2022 GPAIS can address new unseen tasks with few or no new data: To enforce this capability, EC can be employed for simultaneously optimizing multiple AI models, ensuring diversity among their modeled knowledge. This diversity spans a multitude of options, increasing the likelihood of identifying models that exhibit superior performance for new, unseen tasks.\n\u2022 GPAIS should be able to construct/configure themselves autonomously: EC has traditionally been employed for the automatic tuning of ML models across three different granularity levels. At the highest level, EC is used for algorithm selection, a process where they have seen extensive application. Moving to an intermediate level, once the algorithm is determined, EC can configure its parameters to optimize performance. Finally, at the lowest level of granularity, EC is involved in the configuration or construction of model primitives, showing their versatility across all levels of model tuning.\n\u2022 GPAIS should run efficiently by design, both for the training phase and the inference process: Improving the performance of EC algorithms has been widely studied with different techniques that can be transferred to other contexts. For example, to mitigate training costs, an EC algorithm can be employed to reduce the training dataset, creating a reduced dataset with similar performance a technique known as data distillation. Additionally, EC can be used to reduce model complexity, either through pruning or quantization, thereby decreasing both training and inference costs.\n\u2022 GPAIS should explore, evaluate, and decide on actions or sequences of actions in pursuit of specific goals or objectives: This is a common scenario for EC. EC approaches, such as memetic computing, are a solid and robust alternative for searching in complex domains with its exploration-exploitation combination approach. Even more, EC has been traditionally used for reinforcement learning, where the outcomes of actions are used to reinforce the best actions-a technique applied across various domains from games to robotics. Consequently, EC can offer robustness and adaptability even in dynamic environments.\n\u2022 GPAIS should simultaneously tackle multimodal modeling tasks defined over diverse datasets: EC demon- strates versatility in handling various representations seamlessly. EC algorithms tailored for particular representa- tions can work in cooperation to address multimodal tasks effectively. Consequently, the multi-modality of knowl- edge can be improved through EAs, facilitating the integration of heterogeneous information into a cohesive feature space.\n\u2022 GPAIS should learn cooperatively, exchanging knowledge about their learned task(s) and exploiting synergies therefrom: Numerous techniques have been developed to facilitate the exchange of information among models, such as co-evolutionary EAs or multitask optimization."}, {"title": "3.4 Notable milestones and achievements in EC-GPAIS", "content": "In this section, we take a closer look at specific case studies represented by proposals that have significantly contributed to the field's advancement in recent years. We illustrate different studies documented in the literature for closed- and open-world EC-GPAIS in Subsections 3.4 and 3.4, respectively. shows these successful proposals across different areas. The first two rows correspond to proposals for closed-world GPAIS in which there are no mechanisms for adapting to new tasks. While these proposals represent progress towards open-world GPAIS, they remain in a closed scenario. The last row, separated with a thicker line, is composed of evolutionary approaches in open-world GPAIS, because they incorporate mechanisms to generate diversity and to share knowledge."}, {"title": "Current milestones in closed-world EC-GPAIS", "content": "Proposals listed in this table have set a significant course in research, setting the grounds for initial steps in the synergy between EC and GPAIS. Many works in NAS, evolutionary DL and neuroevolution have spurred substantial developments in such areas, as evidenced by comprehensive surveys published over the years [25\u201328,37,45,57,63,85]. The common points between these proposals are the focus on the configuration of the model, by leveraging on EC to evolve weights or hyper-parameters of a neural network-based GPAIS. We begin by describing several case studies related to Hyper-parameter optimization:\n\u2022 NEAT [86] is the foundational stage in the field of neuroevolution. It uses an EA to evolve minimal neural networks towards the creation of deeper network architectures.\n\u2022 EDEN [87] proposes to evolve different types of convolutional, pooling, and fully-connected layers with their hyper- parameters in deep neural networks.\n\u2022 EvoAAA [88] is a NAS proposal linked to autoencoders. In this case, the configuration of the model (architecture, weights, and hyper-parameters) is evolved towards a network with a better accuracy performance.\n\u2022 DENSER [89] belongs to a branch of proposals in NAS that are characterized by the use of a grammar of operators to evolve neural networks. DENSER uses a genetic approach that encodes the macro-structure of the neural network (layers, learning, parameters, etc), whereas a grammatical evolution specifies the parameters of each evolutionary algorithm unit and the valid range of the parameters."}, {"title": "Current milestones in open-world EC-GPAIS", "content": "More recent contributions have shifted their scope towards guaranteeing that GPAIS can integrate new task(s) into their learned knowledge, different from the traditional objective-driven search methods that limit the ability of a system to integrate a new task. While there may not be specific surveys on these emerging diversity-driven concepts, several research areas are closely aligned with the principles of open-world GPAIS. The last studies revisited in what follows are precisely based on the generation of diversity and the discovery of diverse model behaviors:\n\u2022 POET [50] focuses on generating diversity via data synthesis. In this work, agents are paired with these newly generated environments to learn from them and, at the same time, the agents' weights are also optimized. Moreover, these agents can even be transferred from one environment to another, using their knowledge to adapt to the other. Diversity is induced through data synthesis, where knowledge is transferred between the environments via the agents, ultimately optimizing the model through the agents' weights.\n\u2022 EGANs [75], framed in the area of Generative Adversarial Networks, serves as an example of generating diversity through the model, particularly in a zero-shot learning environment. By using an evolutionary approach, EGANS initially learn the optimal generator of models. In a subsequent stage, this generator becomes part of another evolu- tionary process to determine the final model.\n\u2022 EUREKA [99] constitutes another open-world GPAIS in which several reinforcement learning tasks are performed at the same time. The evolutionary algorithm evolves several reward functions in a context based on the environment source code. EUREKA generates executable reward functions, improving them with an evolutionary search that iteratively produces batches of reward candidates.\n\u2022 XferNAS [100] introduces an open-world GPAIS framework for knowledge transfer. XferNAS collects source knowledge from multiple tasks and combines this knowledge to generate an architecture for a new task.\n\u2022 ESBMAL [79] integrates Active Learning and EAs for improving data labeling. The EA optimally reports batches of data to enhance the instance selection process, contributing to the set of open-world proposals together with the aforementioned approaches.\nIn our research, we have not encountered any specific work focused on the utilization of EC for algorithm selection or construction in open-world GPAIS. Differently from closed-world GPAIS, the use of new diversity metrics to develop open-world GPAIS can be regarded as a research niche yet to be explored.\nAnother important feature is that current GPAIS usually consider one task in a closed-world setting, which is even rarer in open-world problems. Surprisingly, the adoption of EC has been used for the design and optimization of multitask learning models [101]. We have also highlighted EUREKA as an open-world GPAIS proposal that works with reinforcement learning agents capable of doing several tasks. These detected research niches should stimulate efforts in prospective studies related to GPAIS and EC.\nGenAI systems are usually more autonomous and do not depend on experts. Still, the use of EC can allow them to achieve even higher levels of autonomy, further reducing the need for such an expert. Using another AI to help improve it can be a viable approach. Within the field of GenAI, more and more works are using EC to improve such systems, which is a major focus in the coming years [102]. In this research line, recent works leverage EAs as key algorithms for tasks such as model merging and creation of foundation models [30], evolving code generated by Large Language Models [103], evolving prompts of Large Language Models [104], and generating optimization algorithms [105]."}, {"title": "4 Challenges of harnessing the EC-GPAIS benefits and strategies to address advances", "content": "The integration of EC with GPAIS poses significant challenges, despite their potential benefits. A major difficulty is ensuring synergy between the adaptive nature of EC and the complex decision-making processes of GPAIS. For this reason, we recall these challenges of how EC is capable of adapting to GPAIS for the design and optimization of these systems in Subsection 4.1. To overcome these challenges, we also present several strategies that can be implemented with EC to both design and improve GPAIS. Specifically, we focus on their goal, their importance in the context of GPAIS, and an analysis of the EC-based research areas that can help realize these strategies. This analysis, backed by the graphical summary in through Sections 4.2 to 4.5, serves as a motivating evidence for the present and future development of EC-GPAIS."}, {"title": "4.1 Challenges of harnessing the benefits of EC-GPAIS", "content": "There are many ways in which EC can benefit both closed-world and open-world GPAIS. EC can enhance closed-world GPAIS by facilitating advanced data preprocessing, optimizing hyper-parameters, and adapting models. Additionally,"}, {"title": "4.2 EC-GPAIS by exploiting and adapting existing knowledge", "content": "Why is this strategy important for GPAIS?\nThe development of a system capable of performing a diverse set of tasks requires a significant effort. Therefore, every piece of knowledge within the system should be regarded as crucial for its continuous improvement. GPAIS need to leverage all their knowledge from the existing pool of tasks to enhance performance and adapt to new incoming tasks. This forces the system to maintain their performance level on previous tasks, while successfully adapting to new ones appearing over time."}, {"title": "How can EC support this strategy?", "content": "As stated before, ETO [76], EMO [82], and ETL [106] have shown to be strategies to optimize previously acquired knowledge. Therefore, by generalizing learning across problems, ETO, EMO, and ETL can be useful to optimize the exchange of knowledge among models devised for different tasks."}, {"title": "Which EC research areas are useful for this strategy?", "content": "The aforementioned research areas in EC can be linked to the strategy of exploiting and adapting existing knowledge:\n\u2022 ETO combines principles from EAs and transfer learning at its core, knowledge or solutions learned from an opti- mization problem in a source domain to improve an optimization process in another related domain. When solu- tions to such problems represent the knowledge of the GPAIS (i.e. its learned parameters), ETO can optimize the adaptation of the transferred knowledge to the target problem by fine-tuning or re-configuring it to better suit the characteristics of the target domain.\n\u2022 ETL aims to leverage knowledge from related domains to improve performance in a different domain, but focuses on improving the learning process of the model using EC through transfer learning based on models, features and/or instances. ETL has been widely used in NAS, with works like XferNAS, in which a process of transfer learning is applied to initiate architectures for a new task, so the knowledge in other related problems is transferred to create a warm-started network [100]. Another work elucidating the potential of ETL is presented in [107], where the process of knowledge transfer between tasks in similar domains is expedited by pruning the unnecessary components of the neural network using EAs, so the GPAIS can autonomously generalize to other tasks while retaining knowledge from the learned source tasks.\n\u2022 Differently, EMO can be used in scenarios where the parameters of the GPAIS are evolved jointly within a sin- gle evolutionary search based on several objective functions, as has been done in [92] in the context of multitask reinforcement learning. In this case, a unified search space can effectively encode parameters shared by all tasks. Evolutionary operators suited to exchange genotypic information across optimization tasks (such as those defined in multifactorial optimization [108]) effectively implement knowledge transfer across tasks.\nIn these three research areas, the transfer of parameters, backbones, or other architectural elements may expedite the learning process and boost even further the performance of the model. When dealing with GPAIS in these areas, the large search space of decision variables evolved by the EA can be a challenge. Recently, multitask learning approaches have been using multi-objective EAs and large-scale global optimization to ensure scalability for realistic model sizes [109]. The assessment of knowledge transfer between tasks often requires training on the target task(s), resulting in significant delays. To reduce this cost, surrogates have been used in evolutionary DL [110]. In multitask GPAIS, models for different tasks may have different processing latencies, and surrogates may benefit from those learned for the fastest evaluated tasks [111]."}, {"title": "4.3 EC-GPAIS by continuously creating new knowledge", "content": "Why is this strategy important for GPAIS?\nGPAIS need adaptability to new tasks with minimal data, therefore requiring the continuous creation of diverse knowl- edge to accelerate responsiveness and facilitate task integration. When GPAIS have good performance over a given set of tasks (closed-world scenario), their transition to an open-world setting is delivered by using mechanisms of knowl- edge diversification, which can be realized through ensemble modeling, the randomization of the GPAIS parameters or the generation of synthetic data, among other strategies."}, {"title": "How can EC support this strategy?", "content": "EC can be leveraged for the generation of diverse models, for handling problems in different search spaces, and for the generation of synthetic data [112]. The conundrum when implementing this strategy with EC is twofold: 1) how to formulate objective functions that properly model the diversity in open-world settings, even in the absence of data from the new task(s); 2) how to retain solutions during the evolutionary search that are both good for the source tasks and diverse to potentially be useful to address new tasks."}, {"title": "Which EC research areas are useful for this strategy?", "content": "Different EC research areas have paid attention to the discovery and retention of diverse solutions during the search. Furthermore, EC can also be useful to induce this diversity by synthesizing data that produce diverse model behaviors through training. We will now revisit some of these areas:\n\u2022 Open-ended evolution aligns with this strategy closely, as it pursues the continuous generation of knowledge without reaching an optimal state. The integration of open-ended evolution with EC makes it feasible to create systems that can continuously adapt [113], where the EC algorithm will help to create more data or working environments [114]. This is the approach followed in POET [50] or Minimal Criterion Coevolution [115].\n\u2022 Quality-diversity optimization aims at developing EC algorithms that prioritize generating diverse, high-quality solu- tions. In addition to evaluating solution quality, diversity metrics measure how distinct each solution is from others. Promoting diversity ensures that solutions are both good and varied across possible behaviors. The EC designed under this paradigm is well-suited for evolving GPAIS in open-world settings. By evolving their parameters and/or configuration and devising measures to quantify the diversity of the evolved GPAIS, quality-diversity optimization can provide a pool of diverse, well-performing models. This can offer better guarantees of adaptability to new tasks, even in the presence of conflicting objectives [116].\n\u2022 Multimodal optimization also seeks to retain diverse, high-quality solutions during the evolutionary search [117]. However, differently from quality-diversity optimization, the diversity is defined over the genotype of the solutions evolved by the algorithm rather than on a behavioral space. In any case, multimodal optimization can also be a promising EC research area to produce diverse models, particularly over combinatorial search spaces representing model configurations [118].\n\u2022 Novelty Search involves driving the search not based on an objective function, but rather by a measure of the novelty of solutions in the search space [114,119]. This ensures the discovery of new solutions and the accumulation of more knowledge about the problem at hand. Although both novelty search and quality-diversity optimization strive to re- tain diverse solutions, novelty search centers on promoting novelty as a measure of diversity, while quality-diversity optimization aims for a balance between high-quality solutions and diversity, encompassing multiple aspects be- yond novelty alone. These techniques can complement each other towards creating new knowledge for open-world GPAIS, and aspects of novelty search can contribute to diversity within the framework of quality-diversity opti- mization. Novelty search with EC has been explored in robotics and autonomous driving systems to ensure that the created knowledge avoids system malfunctions, showcasing the possibilities brought by this EC research area for complex systems to accommodate unknown circumstances [120].\n\u2022 Evolutionary ensemble learning studies the optimization of the learners within an ML ensemble based on different objectives. Most literature focuses on the ensemble's performance in modeling tasks, but some works address different objectives like fairness [121], explainability [122] and dealing with conflicting rewards in multi-agent reinforcement learning systems [123]. We envision that the rich background on methodologies to model diversity in evolutionary ensemble learning (recently revisited in [33]) can open up complementary ways to realize open-world GPAIS based on ML ensembles."}, {"title": "4.4 EC-GPAIS by constructing new models from scratch", "content": "Why is this strategy important for GPAIS?\nBuilding new models from scratch is another strategy used in GPAIS. In such cases, the construction of these models is typically associated with closed-world GPAIS, as it requires the assumption of having quality data to formulate an objective function that guides the search for the optimal model for the task(s) at hand. When new task(s) arrive, the model may not be optimal any longer, so it must be optimized again for such new task(s). To address this as an open-world problem, it is crucial to modify each part of the model for the new task. However, the lack of sufficient high-quality data makes this adaptation challenging. There might not be enough prior knowledge of the new tasks to accurately assess the generalization of the developed algorithm. Consequently, the difficulty lies in adapting this approach to open-world scenarios."}, {"title": "How can EC support this strategy?", "content": "EC can optimize various stages of an ML pipeline, making them suitable for GPAIS. ML areas like AutoML, hyper- parameter optimization, and feature engineering have been effectively addressed via EAs [26"}]}