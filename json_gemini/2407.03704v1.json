{"title": "Neural Probabilistic Logic Learning for Knowledge\nGraph Reasoning", "authors": ["Fengsong Sun", "Jinyu Wang", "Zhiqing Wei", "Xianchao Zhang"], "abstract": "Knowledge graph (KG) reasoning is a task that aims to predict unknown facts based\non known factual samples. Reasoning methods can be divided into two categories:\nrule-based methods and KG-embedding based methods. The former possesses\nprecise reasoning capabilities but finds it challenging to reason efficiently over\nlarge-scale knowledge graphs. While gaining the ability to reason over large-scale\nknowledge graphs, the latter sacrifices reasoning accuracy. This paper aims to\ndesign a reasoning framework called Neural Probabilistic Logic Learning(NPLL)\nthat achieves accurate reasoning on knowledge graphs. Our approach introduces\na scoring module that effectively enhances the expressive power of embedding\nnetworks, striking a balance between model simplicity and reasoning capabilities.\nWe improve the interpretability of the model by incorporating a Markov Logic\nNetwork based on variational inference. We empirically evaluate our approach on\nseveral benchmark datasets, and the experimental results validate that our method\nsubstantially enhances the accuracy and quality of the reasoning results.", "sections": [{"title": "1 Introduction", "content": "Knowledge representation has long been a fundamental challenge in artificial intelligence. Knowledge\ngraphs, a form of structured knowledge representation, have gained significant traction in recent years\ndue to their ability to capture rich semantics and facilitate reasoning over large-scale data. Compared\nto conventional approaches such as semantic networks and production rules, knowledge graphs offer\na more expressive and scalable representation of entities and their relationships in a graph-based\nformalism. This structured representation not only assists human comprehension and reasoning but\nalso enables seamless integration with machine learning techniques, facilitating a wide range of\ndownstream applications.\nOne prominent line of research in knowledge graph reasoning revolves around embedding-based\nmethods. These techniques aim to map the elements of a knowledge graph into a low-dimensional\nvector space, capturing the underlying associations between entities and relations through numerical\nrepresentations. While this approach has demonstrated promising results, it suffers from inherent lim-\nitations, including low interpretability, suboptimal performance on long-tail relations, and challenges\nin capturing complex semantic information and logical relationships.\nAlternatively, rule-based knowledge reasoning methods operate by extracting logical rules from\nthe knowledge graph, typically in the form of first-order predicate logic, and performing inference"}, {"title": null, "content": "based on these rules. However, these methods often face challenges stemming from the vast search\nspace and limited coverage of the extracted rules. Markov Logic Networks (MLNs) [1] have been\nproposed as a principled framework for combining probabilistic graphical models with first-order\nlogic, enabling the effective integration of rules and embedding methods for more accurate reasoning.\nIn this paper, we seek to develop a method that can better leverage the outputs of embedding networks\nto support knowledge graph reasoning. To this end, we propose a novel reasoning framework called\nNeural Probabilistic Logic Learning (NPLL). NPLL introduces a scoring module that efficiently\nutilizes knowledge graph embedding data, enhancing the training process of the entire framework.\nOur method, illustrated in Figure 1, makes the following key contributions:\nEfficient Reasoning and Learning: NPLL can be viewed as an inference network for MLNs,\nextending MLN inference to larger-scale knowledge graph problems.\nTight Integration of Logical Rules and Data Supervision: NPLL can leverage prior knowledge\nfrom logical rules and supervision from structured graph data.\nBalance between Model Size and Reasoning Capability: Despite its compact architecture and\nrelatively fewer parameters, NPLL demonstrates remarkable reasoning capabilities, sufficient to\ncapture the intricate relationships and semantics within knowledge graphs. Even in data-scarce\nscenarios where the available dataset size is relatively small, NPLL can achieve a high level of\nreasoning performance, making it well-suited for practical applications with limited labeled data.\nZero-shot Learning Ability: Our proposed NPLL framework exhibits strong zero-shot learning\ncapabilities, enabling it to handle reasoning tasks involving target predicates with very few or no\nlabeled instances. This flexibility and generalization power are particularly valuable in real-world\nsettings where exhaustive labeling of all possible relationships is often impractical."}, {"title": "2 Related Work", "content": "One prominent category of methods for knowledge graph reasoning is rule-based approaches. These\nmethods leverage logical rules, generally defined as B\u2192A, where A is the target fact, and B can\nbe considered a set of condition facts. Facts are composed of predicates and variables. To better\nutilize these symbolic features, methods like AMIE[2], RuleN[3], and RLvLR[4] employ rule mining\ntools to extract logical rules from knowledge graphs for reasoning. Approaches like KALE[5],\nRUGE[6], and IterE[7] started combining logical rules with embedding learning to construct joint\nknowledge graph reasoning models. Additionally, NeuralLP[8] proposed an end-to-end differentiable\nmethod to effectively learn the parameters and structures of logical rules in knowledge graphs.\nNeuralLP-num-lp[9] combined summation operations and dynamic programming with NeuralLP,\nwhich can be used to learn numerical regulations better. Simultaneously, DRUM[10] introduced a\nrule-based end-to-end differentiable model. Then, pLogicNet designed a probabilistic logic neural\nnetwork [11], demonstrating exemplary reasoning performance. Building on this, ExpressGNN[12]\nachieved more efficient reasoning by fine-tuning the GNN model. DiffLgic[13] designed a differential\nframework to improve reasoning efficiency and accuracy for large knowledge graphs. NCRL[14]\nproposed an end-to-end neural method that recursively leverages the compositionality of logical rules"}, {"title": null, "content": "to enhance systematic generalization. In contrast to these approaches, our proposed NPLL framework\nis significantly more effective for knowledge graph reasoning.\nAnother category of approaches for knowledge graph reasoning is embedding-based methods.\nThese techniques primarily represent entities and relations using vector embeddings. Knowledge\ngraph reasoning is achieved by defining various scoring functions to model different reasoning pro-\ncesses. For instance, methods like TransE[15], TransH[16], TransR[17], TransD[18], TranSparse[19],\nTransRHS[20], RotatE[21] project entities and relations into vector spaces, transforming computa-\ntions between facts into vector operations. The essential scoring function is the difference between\nthe head entity-relation vector and the tail entity vector. Rescal[22], DistMult[23], ComplEx[24],\nHolE[25], analog[26], SimplE[27], QuatE[28], DualE[29], HopfE[30], LowFER[31], QuatRE[32]\nrepresent each fact in the knowledge graph as a three-dimensional tensor, decomposed into a com-\nbination of low-dimensional entity and relation vectors. They use vector matrices to represent the\nlatent semantics of each entity and relation. The primary scoring function is the product of the\nhead entity, relation, and tail entity. Methods like SME[33], NTN[34], and NAM[35] employ neural\nnetworks to encode entities and relations into high-dimensional spaces. ConvE[36] first introduced\n2D convolutional layers for reasoning. RGCN[37], NBF-net[38], and RED-GNN[39] use graph\nneural networks to aggregate neighbor node information and decoders as scoring functions. However,\nthese embedding-based methods often sacrifice interpretability and prediction quality. In contrast,\nour proposed NPLL framework significantly improves the quality of reasoning results while more\nproperly handling reasoning problems through the principled integration of logical rules."}, {"title": "3 Preliminary", "content": "A knowledge graph is a graph-structured model composed of triplets, where the entities in the\ntriplets are nodes and the relations are edges. Given a known knowledge graph K = (E, L, F),\nwhereE = {1, 2, ..., em } represents a set of M entities, with entities typically referring to person\nnames, objects, locations and proper nouns;L {l1,l2,...,lN} represents a set of N relations;\nF = {f1, f2, ..., fs} represents a set of known facts involving entities from E and relations from L,\nwhere fi can be described as fi = {eh,l, et}, eh, et \u2208 E,l \u2208 L, indicating en has a relation 1 with et,\nor can be written as l(en, et), where l is treated as a predicate and eh and et as constants.\nWe now introduce the predicate logic representation, where each relation in the relation set is\nrepresented as a function l(x, y), with x and y having the domain E, and l(x, y) being directed, so\nl(x, y) and l(y, x) are different. For example, l(x,y) := S(Tom,basketball) (S denotes proficient\nsport), indicates that Tom's proficient sport is basketball, which clearly cannot be expressed as\nS(basketball, Tom).\nUsing the predicate logic representation, new facts can be inferred through logical deduction, e.g.,\nS (Tom, basketball) ^ F (Tom, John) \u21d2 S(John, basketball)(F denotes being friends). If\nvariables replace the constant entities in the above formula, it is called a rule, generally represented as:\n$Pred_{1}(x, y_{1}) \\land Pred_{2}(y_{1}, y_{2}) \\land  Pred_{n}(y_{n-1},z) \\Rightarrow Pred(x,z) \\quad n \\geq 1$,where x,yi',z are\nall variables. Pred(A, B) is called an atom, with A and B being the subject and object or the head\nand tail entities in the triplet. Pred(x, z) is the head atom; the rest are body atoms. After substituting\nvariables with constants, e.g. let C1, C2, C3 be constants, $Pred_{1}(C_{1}, C_{2}) \\land Pred_{2}(C_{2}, C_{3}) \\Rightarrow$\n$Pred_{3}(C_{1}, C_{3})$, which is called ground rule, and each atom with variables replaced by constants is\ncalled a ground predicate, whose value is a binary truth value. For example, $Pred_{1}(C_{1}, C_{2}) = \\{0,1\\}$.\nIf $Pred_{1}(C_{1}, C_{2}) \\in F$, then $Pred_{1}(C_{1}, C_{2}) = 1$. Therefore, the goal of knowledge reasoning is to\ninfer unknown facts $U = \\{U_{i}\\}$ from the known facts $F = \\{f_{i} = 1\\}_{i=1,2,...}$\nInferring unknown facts from known facts is a generative problem, which requires building a joint\nprobability distribution model and maximizing the generation probability to obtain the unknown\nfacts. Hence, we must construct a suitable joint probability distribution model for the reasoning task.\nConsidering the above conditions, the knowledge graph can be modeled as a MLN, which combines\nfirst-order predicate logic and probabilistic graphical models. Traditional methods employ first-order\npredicate logic for deductive reasoning in a black-and-white manner. However, as the example\n$S (Tom, basketball) \\land F (Tom, John) \\Rightarrow S(John,basketball)$ shows, it is not necessarily\nalways true. MLN assign a weight w to each rule, representing the probability of the event occurring,\nthus transforming the hard conditions of predicate logic into probabilistic conditions. The rule"}, {"title": null, "content": "representation form in first-order predicate logic is converted to Conjunctive Normal Form (CNF) for\ncomputational convenience.\n$S(A, B) \\land F(A,C) \\Rightarrow S(C, B) \\Leftrightarrow \\neg S(A, B) \\lor \\neg F(A,C) \\lor S(C, B)$\nTherefore, to construct a MLN from a knowledge graph, each ontology rule needs to be defined as a\nnetwork in the MLN, each having a weight w. The probability calculation formula for MLN is:\n$P(F,U|w) = \\frac{1}{Z(w)} \\prod_{r \\in R} exp (w_{r}N (F,U)),$ (1)\nwhere F is the set of known facts, U is the set of unknown facts, R = {r} is the set of rules, $w_{r}$ is the\nweight of rule r, and N(F, U) is the number of ground rules satisfying rule r. Z(w) is the partition\nfunction, which is the sum of all possible ground rule cases for normalization:\n$Z(w) = \\sum_{F,U} \\prod_{r \\in R} exp(w_{r}N(F,U)).$ (2)\nAll ground rules of each rule form a clique in MLN, exp $(w_{r}N (F, U))$ is the potential function of\nrule r, and each potential function expresses the situation of a clique. Generally, all ground rules\nof one rule form a clique, where each primary node, i.e., fact, is treated as a basic atom. Each state\nof MLN assigns different occurrence possibilities to all facts, representing a possible open world.\nEach set of possible worlds combines {F, U, R} relations, jointly determining the truth values of\nall basic atoms. After establishing the joint probability distribution, we infer the unknown facts U\nfrom the known facts F by solving the posterior distribution P(U|F, w), which can be viewed as an\napproximate inference problem.\nUnlike rule-based reasoning methods that evaluate rules holistically, knowledge embedding methods\nmainly score facts, assigning higher scores to correct facts and lower scores to incorrect ones,\nobtaining embedding vectors for different entities, and enabling inference of unknown facts."}, {"title": "4 Model", "content": "This section introduces a knowledge reasoning method that combines MLNs with embedding learning.\nBy utilizing MLN, which is trained with EM algorithm[40], to establish a joint probability distribution\nmodel of known facts and unknown facts, we decompose P(F|w) to obtain the following equation:\n$logP(F|W) = log [\\frac{P(F, U|w)}{Q(U)}] = log[\\frac{P(U|F,w)}{Q(U)}].$ (3)\nin equation 3, P(F, U|w) is the joint probability distribution of known facts and unknown facts. In\ncontrast, P(U|F, w) is the posterior distribution, and Q(U) is the approximate posterior distribution.\nTaking the expectation of both sides of 3 with respect to Q(U), we can define logP(F|w) as the sum\nof the evidence lower bound(ELBO) and the Kullback-Leibler(KL) divergence:\n$logP(F|w) = ELBO + KL(q||p),$ (4)\nwhere $ELBO = \\sum_{U}Q(U)log (\\frac{P(F,U|w)}{Q(U)}), KL(q||p) = - \\sum_{U}Q (U) log (\\frac{P(U|F,w)}{Q(U)}).$\nWhen the approximate posterior distribution Q(U) is the same as the true posterior distribution, we\nobtain the optimal result, at which point KL(q||p) is 0 and ELBO is maximized. Therefore, our\noptimization objective becomes maximizing the ELBO value:\n$dELBO (Q, P) = \\sum_{U}Q(U)logP(F, U|w) \u2013 \\sum_{U}Q(U)logQ(U),$ (5)\nthe approximate posterior distribution Q(U) is the probability distribution of unknown facts based on\nknown facts.\nSpecifically, in the t-th iteration, the first step is to fix the rule weight w as wt, which is a constant.\nWe then update the probability set of each factor in all ground rules through the reasoning method"}, {"title": null, "content": "proposed in this paper and obtain the current approximate posterior probability distribution Q(U).\nThe second step substitutes the approximate posterior distribution into ELBO and updates w by\nmaximizing ELBO:\n$w = argmax_{w} \\sum_{U}(Q(U)logP(F,U|w) \u2013 Q(U)logP(U, F|w+)),$ (6)\nin equation 6, the second term is independent of w and can be treated as a constant. Therefore,\nto reduce computation, we simplify the first step to fixing w and computing the expectation of\nlogP(F,U|w) concerning Q(U). The second step fixes the posterior distribution and updates w,\nobtaining $w^{t+1} = argmax_{w} \\sum_{U} Q(U)logP(F,U|\u03c9)$.\n4.1 Scoring Module\nThe most crucial part of the entire reasoning architecture is generating the approximate posterior\nprobability. We design a scoring module to generate evaluation scores for facts. The generated\nevaluation scores can be the approximate posterior probability to compute the KL divergence from\nthe actual posterior distribution. Additionally, they must satisfy the constraint that the loss for correct\nfacts is minimized while the loss for incorrect facts is maximized. Therefore, we use vectors en and\net to represent the head and tail entity features in a fact while representing the relation using three\nweight matrices.\nOur scoring module consists of three parts. First, an embedding network initializes the vector features\nfor each entity. Then, a scoring function g(l, en, et) computes the evaluation score for each fact.\nFinally, the evaluation scores are processed to form the approximate posterior probability. For the\nscoring function, the model computes the following function to represent the possibility of the head\nand tail entities forming a valid fact under a given relation:\n$g(l, e_{h}, e_{t}) = u_{R}f(e_{h}^{T}W_{R}e_{t} + V_{R} [e_{h};e_{t}] + b_{R}),$ (7)\nwhere f is a non-linear activation function. $W_{R}$ is a $d* d * k$ dimensional tensor, and $e_{h}^{T}W_{R}e_{t}$ results\nfrom a bilinear tensor product, yielding a k-dimensional vector. $V_{R}$ is a $k * 2d$ dimensional tensor,\nand $V_{R} [e_{h};e_{t}]$ is the result of a linear tensor product, also a k-dimensional vector. $u_{R}$ and $b_{R}$ are\nalso k-dimensional, so the final result is a scalar. We design the each parts as follows:\nWe set up initial vectors for entities in the knowledge graph separately. We then build a neural\nnetwork to update the vector features for all entities. The output of this part is the updated head and\ntail entity vectors {en, et} with dimension d.\nWe initialize a bilinear neural network layer WR and two linear neural network layers VR, UR. Taking\nthe head and tail entity vector features as input, we pass them through the scoring function g(l, eh, et)\nto output the result and compute the evaluation scores for all known facts, unknown facts, and negative\nsample facts.\nWe define the obtained evaluation scores as the approximate posterior probability for known and\nunknown facts. Specifically, we process the evaluation scores using the sigmoid function to bound\nthem between 0 and 1, i.e., $p = sigmod (g (l, e_{h}, e_{t}))$, where $sigmod (.) = \\frac{1}{1+exp (.)}$.\n4.2 E-step\nIn the expectation step, to solve for the unknown facts in the knowledge graph based on the known\nfacts, we need to obtain the posterior distribution P(U|F,w) of the unknown facts. This can be\nachieved by minimizing the KL divergence between the approximate and true posterior distribu-\ntions. However, directly solving the joint probability distribution model established by MLN is\nhighly complex. Therefore, this paper randomly samples batches of ground rules to form datasets,\nwherein the ground rules are approximately independent of each batch. By applying the mean-field\ntheorem[40], we define the approximate posterior distribution as the product of the probability\ndistributions of the individual ground rules. The truth value of a ground rule is 1 when it holds\nand 0 when it does not, and the truth value of each ground rule is jointly determined by the truth\nvalues of its constituent facts. Therefore, we set the probability distribution of a ground rule as\nthe product of the probability distributions of its constituent facts. For example, for the ground\nrule:R\u2081 = \u00acS(Tom, basketball) |\u2228 \u00ac F(Tom, John) | S(John, basketball)."}, {"title": null, "content": "The truth value of the ground rule R1 is determined by its three constituent facts. Thus, we define:\n$Q(U) = \\prod_{u_{g} \\in U} q (u_{g}) = \\prod_{u_{g} \\in U} \\prod_{u_{k} \\in u_{g}} f_{k}(U_{k}),$ (8)\nin equation 8, uk represents fact uk, ug represents all facts in a ground rule g, and U is the set of\nunknown facts. Each fact probability distribution fk (uk) follows a Bernoulli distribution, where the\ntruth value is 1 when the fact occurs and 0 when it does not, i.e., $f_{k}(u_{k}) = p_{k}^{u_{k}} (1 \u2013 P_{k})^{(1-u_{k})}$. The\nprobability pk of the fact occurring is obtained from the scoring module.\nThe truth value of each ground rule is jointly determined by the truth values of its constituent facts.\nTherefore, the number of ground rules is represented as:\n$N(F,U) = \\sum_{U_{g} \\in U} \\prod_{u_{g} \\in u_{r}} \\sum \\prod U_{k},$ (9)\nwhere ur represents the set of facts belonging to rule r. Thus, equation 1 can be defined as:\n$P(w/F,U) = \\frac{1}{Z(w)} \\prod_{r \\in R} exp \\Bigg[w_{r} \\Big[\\sum \\prod \\Big[ \\sum \\prod u_{k} \\Big]\\Big]\\Bigg].$ (10)\nSubstituting equations (8) and (10) into the optimization function (5), the term Z(w) can be treated\nas a constant, leading to:\n$\\sum_{r \\in R}\\Big[ w_{r} \\sum_{U_{g} \\in U} \\prod u_{k} - \\sum \\sum \\sum \\big[ (1- p_{k}) log (1 - p_{k}) + p_{k}logp_{k} \\big] \\Big] .$ (11)\nThis paper constructs the score dfact of the known fact set F to add constraints. We want the score\ndfact of the positive sample to be as small as possible. The final objective function is defined as:\n$\\sum_{r \\in R}\\Big[ w_{r} \\sum_{U_{g} \\in U} \\prod u_{k} - \\sum \\sum \\big[ (1 - p_{k}) log (1 - p_{k}) + p_{k}logp_{k} \\big] + d_{fact} \\Big] $ (12)\n4.3 M-step\nIn the M-step, we fix Q(U) and then update the weights wr of the rule set R. At this point, the\npartition function in equation 2 from the E-step is no longer a constant. Therefore, in the M-step,\nwe optimize the rule weights by minimizing the negative of the ELBO. However, when dealing with\nlarge-scale knowledge graphs, the number of facts also becomes enormous, making it difficult to\noptimize the ELBO directly. Consequently, we adopt the widely used pseudo-log-likelihood [39] as\nan alternative optimization objective, defined as:\n$P (U|w) := \\sum_{U_{k} \\in U} q(U) \\sum_{U_{k} \\in U} logP(u_{k}\u03c9, MB_{k})$ (13)\nMBk represents the Markov Blanket of an individual fact k in a ground rule. Therefore, following\nexisting studies [11][12], for each grounding formula k connecting the base predicate with its Markov\nBlanket, we optimize the weights using the gradient descent formula:\n$\\bigtriangledown_{w_{k}} \\sum_{U_{k} \\in U} (logP (u_{k}|\u03c9, MB_{k})).$ (14)"}, {"title": "5 Experiment", "content": "5.1 Experiment Settings\nWe evaluate the NPLL method on four benchmark datasets through the knowledge base completion\ntask and compare it with other state-of-the-art knowledge base completion methods. We will release\nthe code after the publication of the paper."}, {"title": "6 Conclution", "content": "In this paper, we study knowledge graph reasoning and propose a method called Neural Probabilistic\nLogic Learning (NPLL), which effectively integrates logical rules with data embeddings. NPLL"}]}