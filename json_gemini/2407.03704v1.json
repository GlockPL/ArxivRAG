{"title": "Neural Probabilistic Logic Learning for Knowledge Graph Reasoning", "authors": ["Fengsong Sun", "Jinyu Wang", "Zhiqing Wei", "Xianchao Zhang"], "abstract": "Knowledge graph (KG) reasoning is a task that aims to predict unknown facts based on known factual samples. Reasoning methods can be divided into two categories: rule-based methods and KG-embedding based methods. The former possesses precise reasoning capabilities but finds it challenging to reason efficiently over large-scale knowledge graphs. While gaining the ability to reason over large-scale knowledge graphs, the latter sacrifices reasoning accuracy. This paper aims to design a reasoning framework called Neural Probabilistic Logic Learning(NPLL) that achieves accurate reasoning on knowledge graphs. Our approach introduces a scoring module that effectively enhances the expressive power of embedding networks, striking a balance between model simplicity and reasoning capabilities. We improve the interpretability of the model by incorporating a Markov Logic Network based on variational inference. We empirically evaluate our approach on several benchmark datasets, and the experimental results validate that our method substantially enhances the accuracy and quality of the reasoning results.", "sections": [{"title": "1 Introduction", "content": "Knowledge representation has long been a fundamental challenge in artificial intelligence. Knowledge graphs, a form of structured knowledge representation, have gained significant traction in recent years due to their ability to capture rich semantics and facilitate reasoning over large-scale data. Compared to conventional approaches such as semantic networks and production rules, knowledge graphs offer a more expressive and scalable representation of entities and their relationships in a graph-based formalism. This structured representation not only assists human comprehension and reasoning but also enables seamless integration with machine learning techniques, facilitating a wide range of downstream applications.\nOne prominent line of research in knowledge graph reasoning revolves around embedding-based methods. These techniques aim to map the elements of a knowledge graph into a low-dimensional vector space, capturing the underlying associations between entities and relations through numerical representations. While this approach has demonstrated promising results, it suffers from inherent lim-itations, including low interpretability, suboptimal performance on long-tail relations, and challenges in capturing complex semantic information and logical relationships.\nAlternatively, rule-based knowledge reasoning methods operate by extracting logical rules from the knowledge graph, typically in the form of first-order predicate logic, and performing inference"}, {"title": "2 Related Work", "content": "One prominent category of methods for knowledge graph reasoning is rule-based approaches. These methods leverage logical rules, generally defined as B\u2192A, where A is the target fact, and B can be considered a set of condition facts. Facts are composed of predicates and variables. To better utilize these symbolic features, methods like AMIE[2], RuleN[3], and RLvLR[4] employ rule mining tools to extract logical rules from knowledge graphs for reasoning. Approaches like KALE[5], RUGE[6], and IterE[7] started combining logical rules with embedding learning to construct joint knowledge graph reasoning models. Additionally, NeuralLP[8] proposed an end-to-end differentiable method to effectively learn the parameters and structures of logical rules in knowledge graphs.\nNeuralLP-num-lp[9] combined summation operations and dynamic programming with NeuralLP, which can be used to learn numerical regulations better. Simultaneously, DRUM[10] introduced a rule-based end-to-end differentiable model. Then, pLogicNet designed a probabilistic logic neural network [11], demonstrating exemplary reasoning performance. Building on this, ExpressGNN[12] achieved more efficient reasoning by fine-tuning the GNN model. DiffLgic[13] designed a differential framework to improve reasoning efficiency and accuracy for large knowledge graphs. NCRL[14] proposed an end-to-end neural method that recursively leverages the compositionality of logical rules\nto enhance systematic generalization. In contrast to these approaches, our proposed NPLL framework\nis significantly more effective for knowledge graph reasoning.\nAnother category of approaches for knowledge graph reasoning is embedding-based methods.\nThese techniques primarily represent entities and relations using vector embeddings. Knowledge\ngraph reasoning is achieved by defining various scoring functions to model different reasoning pro-\ncesses. For instance, methods like TransE[15], TransH[16], TransR[17], TransD[18], TranSparse[19],\nTransRHS[20], RotatE[21] project entities and relations into vector spaces, transforming computa-\ntions between facts into vector operations. The essential scoring function is the difference between\nthe head entity-relation vector and the tail entity vector. Rescal[22], DistMult[23], ComplEx[24],\nHolE[25], analog[26], SimplE[27], QuatE[28], DualE[29], HopfE[30], LowFER[31], QuatRE[32]\nrepresent each fact in the knowledge graph as a three-dimensional tensor, decomposed into a com-\nbination of low-dimensional entity and relation vectors. They use vector matrices to represent the\nlatent semantics of each entity and relation. The primary scoring function is the product of the\nhead entity, relation, and tail entity. Methods like SME[33], NTN[34], and NAM[35] employ neural\nnetworks to encode entities and relations into high-dimensional spaces. ConvE[36] first introduced\n2D convolutional layers for reasoning. RGCN[37], NBF-net[38], and RED-GNN[39] use graph\nneural networks to aggregate neighbor node information and decoders as scoring functions. However,\nthese embedding-based methods often sacrifice interpretability and prediction quality. In contrast,\nour proposed NPLL framework significantly improves the quality of reasoning results while more\nproperly handling reasoning problems through the principled integration of logical rules."}, {"title": "3 Preliminary", "content": "A knowledge graph is a graph-structured model composed of triplets, where the entities in the\ntriplets are nodes and the relations are edges. Given a known knowledge graph K = (E, L, F),\nwhereE = {e1, e2, ..., em} represents a set of M entities, with entities typically referring to person\nnames, objects, locations and proper nouns;L \u2208 {l1,l2,...,lN} represents a set of N relations;\nF = {f1, f2, ..., fs} represents a set of known facts involving entities from E and relations from L,\nwhere fi can be described as fi = {eh,l, et}, eh, et \u2208 E,l \u2208 L, indicating en has a relation 1 with et,\nor can be written as l(eh, et), where l is treated as a predicate and eh and et as constants.\nWe now introduce the predicate logic representation, where each relation in the relation set is\nrepresented as a function l(x, y), with x and y having the domain E, and l(x, y) being directed, so\nl(x, y) and l(y, x) are different. For example, l(x,y) := S(Tom,basketball) (S denotes proficient\nsport), indicates that Tom's proficient sport is basketball, which clearly cannot be expressed as\nS(basketball, Tom).\nUsing the predicate logic representation, new facts can be inferred through logical deduction, e.g.,\nS (Tom, basketball) \u2227 F (Tom, John) \u21d2 S(John, basketball)(F denotes being friends). If\nvariables replace the constant entities in the above formula, it is called a rule, generally represented as:\nPred\u2081 (x, y\u2081) \u2227 Pred\u2082 (y\u2081, Y\u2082) \u041b ... \u039b Predn (yn-1,z) \u21d2 Pred(x,z) n \u2265 1,where x,y\u1d62',z are\nall variables. Pred(A, B) is called an atom, with A and B being the subject and object or the head\nand tail entities in the triplet. Pred(x, z) is the head atom; the rest are body atoms. After substituting\nvariables with constants, e.g. let C1, C2, C3 be constants, Pred\u2081 (C1, C2) \u2227 Pred\u2082 (C2, C3) \u21d2\nPred3 (C1, C3), which is called ground rule, and each atom with variables replaced by constants is\ncalled a ground predicate, whose value is a binary truth value. For example, Pred\u2081(C1, C2) = {0,1}.\nIf Pred\u2081(C1, C2) \u2208 F, then Pred\u2081(C1, C2) = 1. Therefore, the goal of knowledge reasoning is to\ninfer unknown facts U = {U\u1d62} from the known facts F = {fi = 1}i=1,2,...\nInferring unknown facts from known facts is a generative problem, which requires building a joint\nprobability distribution model and maximizing the generation probability to obtain the unknown\nfacts. Hence, we must construct a suitable joint probability distribution model for the reasoning task.\nConsidering the above conditions, the knowledge graph can be modeled as a MLN, which combines\nfirst-order predicate logic and probabilistic graphical models. Traditional methods employ first-order\npredicate logic for deductive reasoning in a black-and-white manner. However, as the example\nS (Tom, basketball) \u2227 F (Tom, John) \u21d2 S(John,basketball) shows, it is not necessarily\nalways true. MLN assign a weight w to each rule, representing the probability of the event occurring,\nthus transforming the hard conditions of predicate logic into probabilistic conditions. The rule"}, {"title": "4 Model", "content": "This section introduces a knowledge reasoning method that combines MLNs with embedding learning.\nBy utilizing MLN, which is trained with EM algorithm[40], to establish a joint probability distribution\nmodel of known facts and unknown facts, we decompose P(F|w) to obtain the following equation:\n$\\log P(F|W) = \\log \\left[\\frac{P(F, U|w)}{Q(U)}\\right] - \\log \\left[\\frac{P(U|F,w)}{Q(U)}\\right],$\n\nin equation 3, P(F, U|w) is the joint probability distribution of known facts and unknown facts. In\ncontrast, P(U|F, w) is the posterior distribution, and Q(U) is the approximate posterior distribution.\nTaking the expectation of both sides of 3 with respect to Q(U), we can define logP(F|w) as the sum\nof the evidence lower bound(ELBO) and the Kullback-Leibler(KL) divergence:\n$\\log P(F|w) = ELBO + KL(q||p),$\nwhere $ELBO = \\sum_U Q(U) \\log \\left(\\frac{P(F,U|w)}{Q(U)}\\right)$, $KL(q||p) = - \\sum_U Q (U) \\log \\left(\\frac{P(U|F,w)}{Q(U)}\\right)$.\nWhen the approximate posterior distribution Q(U) is the same as the true posterior distribution, we\nobtain the optimal result, at which point KL(q||p) is 0 and ELBO is maximized. Therefore, our\noptimization objective becomes maximizing the ELBO value:\n$\\delta_{ELBO} (Q, P) = \\sum_U Q(U) \\log P(F, U|w) - \\sum_U Q(U) \\log Q(U),$\nthe approximate posterior distribution Q(U) is the probability distribution of unknown facts based on\nknown facts.\nSpecifically, in the t-th iteration, the first step is to fix the rule weight w as wt, which is a constant.\nWe then update the probability set of each factor in all ground rules through the reasoning method"}, {"title": "4.1 Scoring Module", "content": "The most crucial part of the entire reasoning architecture is generating the approximate posterior\nprobability. We design a scoring module to generate evaluation scores for facts. The generated\nevaluation scores can be the approximate posterior probability to compute the KL divergence from\nthe actual posterior distribution. Additionally, they must satisfy the constraint that the loss for correct\nfacts is minimized while the loss for incorrect facts is maximized. Therefore, we use vectors eh and\net to represent the head and tail entity features in a fact while representing the relation using three\nweight matrices.\nOur scoring module consists of three parts. First, an embedding network initializes the vector features\nfor each entity. Then, a scoring function g(l, eh, et) computes the evaluation score for each fact.\nFinally, the evaluation scores are processed to form the approximate posterior probability. For the\nscoring function, the model computes the following function to represent the possibility of the head\nand tail entities forming a valid fact under a given relation:\n$g(l, e_h, e_t) = u_r^T f(e_h W_R e_t + V_R [e_h;e_t] + b_R),$\nwhere f is a non-linear activation function. WR is a d * d * k dimensional tensor, and $e_h W_R e_t$ results\nfrom a bilinear tensor product, yielding a k-dimensional vector. VR is a k * 2d dimensional tensor,\nand VR [eh;et] is the result of a linear tensor product, also a k-dimensional vector. UR and bR are\nalso k-dimensional, so the final result is a scalar. We design the each parts as follows:\nWe set up initial vectors for entities in the knowledge graph separately. We then build a neural\nnetwork to update the vector features for all entities. The output of this part is the updated head and\ntail entity vectors {eh, et} with dimension d.\nWe initialize a bilinear neural network layer WR and two linear neural network layers VR, UR. Taking\nthe head and tail entity vector features as input, we pass them through the scoring function g(l, eh, et)\nto output the result and compute the evaluation scores for all known facts, unknown facts, and negative\nsample facts.\nWe define the obtained evaluation scores as the approximate posterior probability for known and\nunknown facts. Specifically, we process the evaluation scores using the sigmoid function to bound\nthem between 0 and 1, i.e., p = sigmod (g (l, eh, et)), where $sigmod (.) = \\frac{1}{1+exp (.)}$."}, {"title": "4.2 E-step", "content": "In the expectation step, to solve for the unknown facts in the knowledge graph based on the known\nfacts, we need to obtain the posterior distribution P(U|F,w) of the unknown facts. This can be\nachieved by minimizing the KL divergence between the approximate and true posterior distribu-\ntions. However, directly solving the joint probability distribution model established by MLN is\nhighly complex. Therefore, this paper randomly samples batches of ground rules to form datasets,\nwherein the ground rules are approximately independent of each batch. By applying the mean-field\ntheorem[40], we define the approximate posterior distribution as the product of the probability\ndistributions of the individual ground rules. The truth value of a ground rule is 1 when it holds\nand 0 when it does not, and the truth value of each ground rule is jointly determined by the truth\nvalues of its constituent facts. Therefore, we set the probability distribution of a ground rule as\nthe product of the probability distributions of its constituent facts. For example, for the ground\nrule:R\u2081 = \u00abS(Tom, basketball) \u2227 \u00ac F(Tom, John) \u2227 S(John, basketball)."}, {"title": "5 Experiment", "content": "In this paper, we study knowledge graph reasoning and propose a method called Neural Probabilistic\nLogic Learning (NPLL), which effectively integrates logical rules with data embeddings. NPLL"}]}