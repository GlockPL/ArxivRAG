{"title": "Towards a Transformer-Based Pre-trained Model for IoT Traffic Classification", "authors": ["Bruna Bazaluk", "Mosab Hamdan", "Mustafa Ghaleb", "Mohammed S. M. Gismalla", "Flavio S. Correa da Silva", "Daniel Mac\u00eado Batista"], "abstract": "The classification of IoT traffic is important to improve the efficiency and security of IoT-based networks. As the state-of-the-art classification methods are based on Deep Learning, most of the current results require a large amount of data to be trained. Thereby, in real-life situations, where there is a scarce amount of IoT traffic data, the models would not perform so well. Consequently, these models underperform outside their initial training conditions and fail to capture the complex characteristics of network traffic, rendering them inefficient and unreliable in real-world applications. In this paper, we propose IoT Traffic Classification Transformer (ITCT), a novel approach that utilizes the state-of-the-art transformer-based model named TabTransformer. ITCT, which is pre-trained on a large labeled MQTT-based IoT traffic dataset and may be fine-tuned with a small set of labeled data, showed promising results in various traffic classification tasks. Our experiments demonstrated that the ITCT model significantly outperforms existing models, achieving an overall accuracy of 82%. To support reproducibility and collaborative development, all associated code has been made publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "IoT traffic classification stands as a crucial pillar in the realm of network management. It is an effective tool to improve the efficiency and security of the Internet of Things (IoT) ecosystem. The ability to classify IoT traffic accurately empowers Internet Service Providers (ISPs) to furnish high-quality services to network users, thereby ensuring optimal performance, security, and resource allocation [1].\nConventional traffic classification techniques, which distinguish various network services based on basic traffic characteristics like communication protocol and port number, are increasingly inadequate due to the complexity and change-ability of contemporary traffic [2], [3]. To overcome this challenge, numerous studies have employed machine learning (ML) algorithms for traffic classification using statistical features [4]\u2013[6]. Nonetheless, these methods depend heavily on expert judgment for selecting specific features, and even seemingly minor statistical features can significantly influence the effectiveness of the analysis.\nCurrent methods for IoT traffic classification predominantly lean on deep learning (DL) algorithms [7]\u2013[9]. One limitation is that these approaches hinge upon the availability of substantial volumes of labeled traffic data to construct traffic-level fingerprinting models. Thus, a significant challenge surfaces when handling specific classes of IoT devices that, during their operation, generate only limited labeled traffic.\nMany ML and DL models nowadays use the transformers architecture [10]. Even though transformers were initially built to tackle problems in Natural Language Processing (NLP), the successful utilisation of this architecture has become widespread, including problems related to e.g. image classification and tabular data analysis. This way, working with transformers can be a good and innovative idea to solve the IoT traffic classification problem. In fact, there are some works that already used transformers to classify IoT traffic, but some do not use specific IoT traffic datasets [11] and others focus on specific type of networks [12].\nThis work introduces a novel IoT Traffic Classification Transformer (ITCT) model, based on TabTransformer [13], for classifying IoT traffic. The ITCT model harnesses the benefits of Transformers, such as efficient learning through an attention mechanism, the ability to generalize using extensive datasets, and proficiency in sequence learning problems, which is pertinent since network packet data inherently forms a sequence. Our model can learn network dynamics from packet traces, and the obtained results from the experiments are promising: ITCT demonstrates a remarkable ability to generalize across various prediction tasks and environments. Having been evaluated with generic datasets, the model employs Transformers to learn contextual embeddings of categorical features effectively. We have pre-trained this model using an MQTT-based IoT traffic dataset [14], enabling others to further fine-tune it with their own data, regardless of the dataset's size. The results indicate the ITCT transformer's potential to achieve commendable evaluation metrics. For example, one of our proposed models attained an overall accuracy of 82%, a similar performance to other classifiers tested. Furthermore, to promote transparency and reproducibility, all developed code is made publicly available\u00b9.\nThis paper is organized as follows: we discuss some related works in Section II. Section III explain out the methodology and experiments. Finally, Section IV and Section V present"}, {"title": "II. RELATED WORK", "content": "There are some studies regarding IoT traffic classification that try to solve the problem of the small quantity of data. One of the most recent uses feature comparison to classify the packets [8]. The model consists of 10 different feature comparators, each being a neural network with a different combination of the following layers: Convolutional, batch normalization, ReLU, and Max Pooling. The Precision values vary between 80.34% and 99.04%, the Recall values fluctuate between 80.69% and 99.01%, and the F1-measure values span from 80.47% to 99.01% in two different datasets. However, this work suffers from computational costs.\nOne of the most recent studies regarding IoT traffic classification obtained great results [9]. In the paper, the authors describe MAFFIT (multi-perspective feature approach to few-shot classification of IoT traffic), a model based on BiLSTMs (Bidirectional Long Short-Term Memory Recurrent Neural Networks). The main modules of this model is the feature encoder, feature comparator and finetune optimizer. The purpose of feature encoder is to further extract the high-dimensional features of the packet length sequences and the packet byte sequence. The feature comparator does the final predictions and the finetune optimizer finetunes the parameters of the training phase in order to achieve better results. This model achieved the best results in the area so far, in terms of accuracy, using IoT traffic datasets. Another work has a slightly different approach [15]. The main idea is to transform the information in the packets into images and use them as input to a meta-learning model based on Convolutional Neural Networks. The authors used five different network traffic datasets, which were general and not necessarily from IoT traffic, and selected a few samples from each. The dataset called FSIDS-IoT was made to be used on few-shot models. The only measure described in the paper is the accuracy of their model, which ranged from 73.81% to 92.19%. However, this proposal also suffers from computational costs.\nIn [12] the authors proposed securing a smart home with a Transformer-Based IoT intrusion detection system. They created a model to classify the traffic combining network traffic and telemetry from the house's sensors data and training a transformer-based model with these combined data. This model achieved an accuracy of 98.39%. However, this work may not perform well in general cases.\nOur work focuses on using transformers to classify IoT traffic. Our approach is using TabTransformer [13]. This architecture was proposed by the end of 2020. It uses the encoder part of the transformer architecture [10] to process the categorical features and concatenates them with a normalization of the continuous features; this concatenation is then passed through a multi-layer perceptron to predict the specified class."}, {"title": "III. A NEW TRANSFORMER MODEL FOR IOT TRAFFIC CLASSIFICATION", "content": "Our proposed IoT Traffic Classification Transformer architecture, referred to as ITCT Transformer, is based on TabTransformer [13], which comprises a column embedding layer, a stack of N Transformer layers, and a multilayer perceptron (MLP). Each Transformer layer, as introduced by [13], [16], consists of a multi-head self-attention layer followed by a position-wise feed-forward layer. The architecture of the IoT Traffic Classification Transformer is depicted in Figure 1.\n*A. TabTransformer [13]*\nGiven a feature-target pair (x, y), where x = {xcat, Xcont}, Xcat represents all categorical features and cont \u2208 R denotes all of the continuous features. Each categorical feature Xi in Xcat = {X1,X2,...,xm} is embedded into a parametric embedding of dimension d through Column Embedding. The set of embeddings for all categorical features is denoted as E(Xcat) = {$1 (X1),..., \u20ac\u00a2m (Xm)}. These embeddings are then fed into the first layer of the Transformer. The output from the final Transformer layer, fe, transforms these parametric embeddings into contextual embeddings {h1,...,hm} where hi \u2208 Rd. These contextual embeddings, concatenated with continuous features Xcont, form a vector of dimension (dxm+c), which is then inputted into an MLP, gp, to predict the target y. The loss function L(x, y) is defined as\n$L(x,y) = H(gy(fo(Ep(xcat)), Xcont), y),                                  (1)$\nwhere H is the cross-entropy for classification tasks and mean square error for regression tasks. This function aims to minimize the prediction error and learn all the Transformer parameters in an end-to-end manner.\nFor the Transformer, a multi-head self-attention mechanism is used, involving parametric matrices Key (K), Query (Q), and Value (V). The attention head transforms each input embedding into a contextual one through the attention mechanism defined as\n$Attention(K, Q, V) = A \u2022 V,                                   (2)$\nwhere A = softmax((QKT)/\u221aK). For column embedding, each categorical feature i has an embedding lookup table \u0435\u0444\u2081 (\u00b7), with ep; (j) = [c\u00f3\u017c, W\u00a2ij] where c\u00f3; \u2208 R, Wpij \u2208 Rd-1. Unique identifiers C\u00f3, \u2208 R distinguish classes in column i from those in other columns. The embeddings are pre-trained in a supervised manner using labeled examples and further refined through fine-tuning with labeled data, with the loss defined in Equation (1). For scenarios with limited labeled examples, pre-training procedures like Masked Language Modeling (MLM) and Replaced Token Detection (RTD) are employed.\n*B. ITCT (IoT Traffic Classification Transformer)*\nAs detailed in Algorithm 1, the Transformer for IoT Traffic Classification model employs a systematic approach to categorize MQTT-IoT traffic data effectively. The process commences with a thorough data preprocessing phase, where"}, {"title": "IV. EXPERIMENTS AND DISCUSSION", "content": "To verify the effectiveness of ITCT, our proposed model, we implemented a set of experiments detailed as follows.\n*A. Dataset*\nWe chose an open IoT traffic dataset, MQTT Inter-net of Things Intrusion Detection Dataset (MQTT-IoT-IDS2020) [14]. The two main reasons for our choice are: 1) This dataset focuses on the MQTT protocol, which is one of the most used protocols in IoT communication; 2) this is a large dataset with more than 1GB, and as our idea is to develop a pre-trained model that can be fine-tuned with only a few samples, a large dataset is needed. We use the .csv version of the dataset, which is already tabular. This dataset was built under a simulated MQTT network. Its architecture contains twelve sensors, a broker, a simulated camera, and an attacker. The five captured scenarios are: aggressive scan, UDP scan, Sparta SSH brute-force, MQTT brute-force attack, and normal operation, as shown in Table I.\nnumerical features are normalized and categorical features are suitably encoded. A critical step in this phase is the application of feature selection techniques, ensuring that the model focuses on the most pivotal attributes of the data. By the way, the pre-processing was made for each file separately and then they were concatenated in one dataset. The pipeline to be followed is to preprocess the dataset, make the feature selection, and train and test ITCT Transformer model with the selected features on the preprocessed dataset. Subsequently, the algorithm proceeds to initialize a Transformer model, specifically designed and tuned to align with the nuanced characteristics of the MQTT-IoT dataset. Training this model involves carefully optimizing various parameters, guided by a predetermined loss function, optimizer, and critical performance metrics. Post-training, the model undergoes a rigorous evaluation on an independent test dataset, a crucial step to verify its robustness and effectiveness. In essence, the Transformer for IoT Traffic Classification stands as a comprehensive and potent tool, able to dissect and classify MQTT-IoT traffic data, enhancing performance and ensuring its practical viability in many real-world applications.\n*B. IoT Traffic Data Preparation and Preprocessing*\nAfter the preprocessing, we make the feature selection using Sci-kit Learn's Tree-based estimator [17] in order to separate the columns to be considered in the training process. From a total of 25 features, only 5 were selected: ttl, ip_len, tcp_flag_push, mqtt_message_type, and mqtt_message_length. Additionally, we include one more feature, namely, protocol.\nWe separate 80% of the samples for the training, 10% for validation and 10% for testing. Then, we use TensorFlow's keras\u00b2 to build the ITCT model. The first step is to define the vocabulary, which was defined using the tokens found on the categorical features of the data, tokenize it and pass it trough the embedding layer. Then, we define the optimizer to be used, in this case we use AdamW [18], which is the default keras' optimizer and the most recommended for transformers-based networks, and the loss function, which is Binary Categorical Entropy, because we are working with only 2 classes. After setting the layers and parameters, the model is compiled.\nMost of the parameters were the same as keras' TabTrans-former webpage suggested, others were changed to fit our specific problem. This model is then trained with the selected dataset. To avoid overfitting, we use a validation set during the training process, that helps the model to know when to stop the training. The last 10% of the data is used for testing. In the test, we use the trained model to classify the samples. The metrics we use are accuracy, precision, F1-Score, and recall.\n*C. Experimental Settings*\n*1) Parameters of the proposed ITCT model*: In the implementation of our model, careful consideration is given to the selection and tuning of hyperparameters, ensuring optimal performance and efficient learning.\n*2) Implementation*: The analytical procedures were implemented using Python version 3.7.10, with Keras and Tensor-Flow serving as the primary frameworks for deep learning tasks. For the purpose of replicability and to facilitate further research, we have provided all the experiments detailed in this section in a publicly accessible Python notebook 3.\n*D. Experimental Results and Analysis*\nIn the comparison of ITCT Transformer configurations for IoT Traffic Classification model, distinct variations in performance are evident."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this study, we introduce a novel IoT Traffic Classification model (ITCT), an approach based on the TabTransformer model, specifically designed for the classification of IoT traffic. The ITCT model showcases a good performance in traffic classification tasks compared to other existing IoT traffic classifiers [7] [9] [15], by integrating a traffic representation matrix and packet-level attention mechanisms. Notably, our model achieves great accuracy and precision, particularly in configurations without feature extraction and callbacks, demonstrating its effectiveness in correctly classifying IoT traffic. However, our findings also emphasize the importance of maintaining a balance between model complexity and performance, highlighting that simplified models can achieve substantial computational efficiency without significantly compromising predictive capabilities.\nAn interesting advantage our model has over the before cited methods is that we already trained ITCT with a large dataset [14], so, to use it in real life, users may fine-tune it with their own data to get better results for their specific environment.\nIn future work, we plan to make our model available at HuggingFace, so that it will be easier to be fine-tuned by users. We also aim to improve the model's adaptability to various IoT scenarios and network environments, training it with different datasets."}]}