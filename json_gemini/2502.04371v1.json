{"title": "PerPO: Perceptual Preference Optimization via Discriminative Rewarding", "authors": ["Zining Zhu", "Liang Zhao", "Kangheng Lin", "Jinze Yang", "En Yu", "Chenglong Liu", "Haoran Wei", "Jianjian Sun", "Zheng Ge", "Xiangyu Zhang"], "abstract": "This paper presents Perceptual Preference Optimization (PerPO), a perception alignment method aimed at addressing the visual discrimination challenges in generative pre-trained multimodal large language models (MLLMs). \u03a4\u03bf align MLLMs with human visual perception process, PerPO employs discriminative rewarding to gather diverse negative samples, followed by list-wise preference optimization to rank them. By utilizing the reward as a quantitative margin for ranking, our method effectively bridges generative preference optimization and discriminative empirical risk minimization. PerPO significantly enhances MLLMs' visual discrimination capabilities while maintaining their generative strengths, mitigates image-unconditional reward hacking, and ensures consistent performance across visual tasks. This work marks a crucial step towards more perceptually aligned and versatile MLLMs. We also hope that PerPO will encourage the community to rethink MLLM alignment strategies.", "sections": [{"title": "1. Introduction", "content": "The success of next token generation (Radford, 2018; Radford et al., 2019) has reignited the pursuit of artificial general intelligence (AGI). Representative methods (Brown, 2020; Anthropic., 2024) have achieved non-trivial advancements in both creative generation (Zhao et al., 2024b; Azaiz et al., 2024) and logical reasoning (Yang et al., 2023a; Frieder et al., 2023). Recently, they have also demonstrated exceptional multimodal capabilities (Achiam et al., 2023; OpenAI., 2024), achieving remarkable results in various generative visual tasks (Yang et al., 2023b; Wen et al., 2024).\nHowever, visual discrimination tasks have emerged as the Achilles' heel of these multimodal large language models (MLLMs) (Li et al., 2024b; Wei et al., 2024a;b). These tasks, which require minimal reasoning and yield deterministic answers-such as \u201cprovide the position of the person\", as illustrated in Figure 1a often leave these powerful models quite \"nearsighted\", or even \"blind\". Could it be that generative models fundamentally struggle with visual discrimination tasks that are simple for a child?\nDespite efforts (Yu et al., 2023; Wei et al., 2023) to address this issue by incorporating discriminative tasks into generative pre-training, results often remain suboptimal, compromising core linguistic abilities. This paper approaches the problem from an alignment perspective. We argue that performance deficiencies in pre-trained models with basic competencies stem primarily from misalignment. In practice, existing MLLMs lack alignment with perceptual objectives a fundamental expectation for such models. Recent methods (Sun et al., 2023; Zhao et al., 2023) using Direct Preference Optimization (DPO) (Rafailov et al., 2024) aim for low-hallucination, high-accuracy outputs but often fall into image-unconditional reward hacking (Skalse et al., 2022), a phenomenon where text preferences are optimized without engaging with visual input. Consequently, a truly perception-oriented alignment becomes necessary.\nIn this paper, we propose a simple yet effective approach: Perceptual Preference Optimization (PerPO) via discriminative rewards. Our method aims to align with the human coarse-to-fine visual perception process, which starts broadly and then refines: generating multiple hypotheses around the objective truth, and gradually narrowing down to the best hypothesis as rewards increase (Hegd\u00e9, 2008). To simulate this process, PerPO builds on empirical risk minimization (P\u00e9rez-Cruz et al., 2003; Golubev, 2004), initially defining the reward as the error between model predictions and the ground truth. Meanwhile, through Best-of-N (Charniak & Johnson, 2005) validation in Figure 1b, we observe the remarkable consistency between this reward and visual discriminative ability, also revealing the untapped discriminative potential within MLLMs.\nCentered on such discriminative rewards, PerPO first enables the acquisition of more negative samples, which refer to model-generated responses that deviate from the ground truth, with \"negative\" defined relative to the discriminative truth. This strategy aims to fully exploit the inherent scalability of discriminative rewards, facilitating efficient learning from diverse negative samples without human annotation. By employing a listwise preference optimization, PerPO is better suited to learning the relative relationships among different samples-i.e., ranking sequences of samples, which more effectively captures image-conditioned preference patterns. As Figure 1c confirms, PerPO significantly suppresses optimization toward image-unconditioned reward hacking. Meanwhile, to compensate for the uncertainty introduced by preference ranking, we treat the reward itself as a quantitative margin for anchoring the ranking. We demonstrate both theoretically and empirically that PerPO effectively combines generative preference optimization with discriminative empirical risk minimization. This ultimately ensures consistent modeling across visual generation and discrimination tasks.\nOur contributions are summarized as follows:\n\u2022 We highlight, for the first time, the capability dilemma of generative MLLMs in visual discrimination tasks. To address this, we propose PerPO to align with the human perception process, enhancing both visual discrimination and generation abilities.\n\u2022 Technically, we first introduce a scalable discriminative reward, which is consistent with the theoretical basis of ERM. Using the reward, we can obtain more negative samples for effective penalization.\n\u2022 Building on this, a listwise approach to preference optimization facilitates learning the ranking relationships of diverse negative samples and mitigates image-unconditional reward hacking.\n\u2022 Further, using the reward itself as a margin to anchor uncertainty in ranking is theoretically and experimentally proven to harmonize visual perception and generation performance.\""}, {"title": "2. Preliminaries", "content": "Best-of-N sampling (Charniak & Johnson, 2005; Nakano et al., 2021), also known as rejection sampling, involves generating N candidate solutions and selecting the one that scores highest according to a proxy reward. This method leverages the natural variability (Renze & Guven, 2024) in LLM responses, effectively finding the best output from a pool of possibilities. By picking the top-scoring candidate, Best-of-N increases the likelihood of identifying the correct answer, enhancing the problem-solving capabilities (Guo et al., 2024) of LLMs and making them more reliable and accurate (Bai et al., 2022).\nDirect Preference Optimization (DPO) (Rafailov et al., 2024) surpasses Best-of-N by utilizing an implicit reward derived from reinforcement learning objectives. DPO employs the LLM for both reward learning and proposal generation, fine-tuning the model to better align with human preferences. This integration improves the model's relevance and quality, pushing the boundaries of LLM performance. Formally, given pairwise preference data (x, y+, y\u00af), where y+ is preferred over y with respect to prompt x, the reward objective is defined as:\n$r(x, y) = \\beta log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)} + Z(q)$"}, {"title": "3. PerPO: Perceptual Preference Optimization", "content": "Motivated by the contrast between MLLMs' prowess in generative tasks (Yang et al., 2023b; Wen et al., 2024) and their struggles in visual discrimination (Li et al., 2024b; Qu et al., 2024), we aim to bridge this gap. We posit that this issue could be alleviated by an explicit perception alignment. Therefore, we employ preference optimization to simulate the human innate, coarse-to-fine visual perception process (Hegd\u00e9, 2008). As we will detail, we utilize the value of the model's prediction error relative to the visual ground truth as a reward signal. By maximizing the exploitation of this reward, we can effectively activate the model's inherent visual discrimination capability.\nA simple reward aligns well with visual discrimination. Visual models, when trained effectively with supervised learning, tend to produce generalizable and reliable predictions. In visual discrimination tasks, the quality of generated responses can often be measured using specific evaluation criteria. This indicates that the discrepancy between model predictions and ground truths can serve as a highly accurate and effective reward in visual discrimination tasks.\nTo substantiate this, Figure 1b shows the effects of Best-of-N (Charniak & Johnson, 2005; Nakano et al., 2021), SFT, DPO (Rafailov et al., 2024), and PerPO on RefCOCOg (Mao et al., 2016). Among them, Best-of-N selects the answer with the highest reward, SFT uses the ground truth, DPO chooses the pair answers with the largest reward discrepancy, and PerPO incorporates all answers. Notably, Best-of-N grows with N, achieving 50% improvement at N 20, demonstrating consistency between discriminative reward and model performance. In addition, DPO, trained on largest-margin pairs, surpasses SFT at N = 8, indicating the reward's efficacy in sample selection.\nMore negative samples and listwise optimization boost visual perception. Methods like PPO (Schulman et al., 2017; Ouyang et al., 2022) and LiPO (Liu et al., 2024d) highlight the importance of diverse preference sample sequences in RL optimization. Generally, a sufficiently varied and systematically ordered set of negative samples helps the model rectify deficiencies incrementally and learn true preferences from rankings. Discriminative rewards, which require no human annotation, scale efficiently and enhance the impact of diverse negative samples for MLLMs. This is corroborated by Figure 1b, where PerPO's performance improves with increasing N. Table 3 further compares PerPO and DPO performance as N increases, validating the superiority of listwise over pairwise optimization.\nMeanwhile, recent studies show that human alignment in MLLMs doesn't effectively extend to visual conditions (Wang et al., 2024a), suggesting a form of image-unconditional reward hacking (Skalse et al., 2022). Our comparative analysis of DPO and PerPO, with and without image input (Figure 1c), reveals that PerPO exhibits superior gains with visual information. This indicates PerPO's optimization is more dependent on visual conditions. We attribute this robustness to the precision of discriminative reward and the strength of listwise optimization. For MLLMs, this implies that visual input engagement is crucial for accurate pattern identification.\nYour reward is secretly the perfect margin. Rewards often lack absolute values or have unclear magnitudes. Previous methods have addressed this by adding margins (Meng et al., 2024) or constructing imbalanced rankings (Song et al., 2024), with their success arising from non-uniform objectives that create smoother optimization spaces (Burges et al., 2006). As mentioned earlier, discriminative rewards provide a measured deterministic signal, we can directly use them as margins to align with the expected target.\nConcretely, we use the absolute value of the reward itself as the weight for the sequence. Formally, we define {$R_1$, ..., $R_n$} = {f(x,$y_1$), ..., f (x, $y_n$)} to denote discriminative reward scores, where $R_i$ is derived by evaluating the"}, {"title": "4. Experiments", "content": "We conduct a comprehensive assessment of PerPO across various multimodal benchmarks.\n4.1. Implemental Details\nData construction.We construct listwise preference data for two visual discriminative tasks: object grounding and dense OCR. Discriminative rewards are calculated using Intersection over Union (IoU) for object grounding and edit distance for dense OCR. For object grounding, we derive the corpus from RefCOCO (Yu et al., 2016), RefCOCO+ (Yu et al., 2016), and RefCOCOg (Mao et al., 2016). We sample an equal amount of data from each dataset and perform 20 samplings per instruction using the model at a temperature of 0.5. The resulting preference data are then filtered based on the data margin, defined as the difference between the maximum and minimum discriminative rewards within a list of responses. By setting the margin to 0.8, we retain 3,000 high-quality samples. For dense OCR, we use page-level OCR data from Fox (Liu et al., 2024a), employing edit distance instead of IoU for rewarding. Setting the margin to 0.04 yields a dataset of 1,800 samples.\nModels and training settings. We adopt LLaVA-v1.5-7B (Liu et al., 2023a) as the base model, integrating CLIP-ViT-L-336px (Radford et al., 2021) and Vicuna-7B-v1.5 (Chiang et al., 2023; Liu et al., 2023b). All experiments are conducted using DeepSpeed ZeRO stage-3, applying LORA (Hu et al., 2022) for fine-tuning. The training setup includes a batch size of 8 and a learning rate of 5e-6 with the AdamW optimizer. Training is completed on 8 GPUs in approximately 1.5 hours. To further validate our approach, we utilize LLaVA-Next-7B (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024a) for object grounding task. However, these models demonstrate limited efficacy in the dense OCR task, due to a lack of sufficient training data. To address this, we construct page OCR datasets of varying sizes (25k, 50k), combining them with the original 780k instruction tuning data to train LLaVA-Next-*k-7B. The models employ SigLIP-400M (Zhai et al., 2023) as the visual encoder and Qwen2-7B (Yang et al., 2024) as the language model.\nEvaluation benchmarks.", "description": "We conduct a comprehensive assessment of PerPO across various multimodal benchmarks."}, {"title": "4.3. Ablation Study", "content": "Training data statistical analysis. Training data plays a crucial role in preference optimization. We conduct a comprehensive statistical analysis, focusing on data quality and quantity. Quality is assessed by the margin, defined as the difference between the highest and lowest discriminative scores within a list. As shown in Figure 2a, the experimental results are influenced by the margin. A balanced performance for both LLaVAW and RefCOCO+ is achieved with the margin of 0.8 to 1.0. Figure 2b indicates that RefCOCO+ improves with larger data size, while LLaVAW declines. Optimal performance occurs at 3k samples.\nHyperparameter \u03b2 in PerPO loss. DPO loss includes a hyperparameter \u03b2, which controls the model's sensitivity to differences between candidate responses. A higher \u03b2 increases the model's focus on subtle distinctions in outputs, while a lower \u03b2 allows for greater tolerance of minor deviations. During training, \u03b2 also affects the model's rate of assimilating human preferences, with an optimal value ensuring stable learning progression. This parameter, also applied in our PerPO method, underwent several experimental iterations. As shown in Figure 2c, the best performance was achieved with \u03b2 set to 0.1."}, {"title": "5. In-depth Analysis", "content": "5.1. Impact of Discriminative Reward in PerPO\nDiscriminative reward aligns well with perception. We conducted a comparative analysis of Best-of-N, SFT, DPO, and PerPO on object grounding task, using IoU as discriminative reward. To explore upper-bound performance, we calculated Best-of-N using test set ground truth, while other methods utilized the train set. Sampling was performed at temperature 0.5 from a moderately capable model. As shown in Figure 1a, Best-of-N's logarithmic performance trend with increasing samples validates the reward's effectiveness in aligning with perception performance in an oracle scenario. Meanwhile, the enhanced gains of DPO and PerPO at higher N values confirm the accuracy of reward-based sample selection or ranking, highlighting the potential of reward-guided approaches for model improvement.\nDiscriminative reward also aligns well with human.", "description": "To evaluate the impact of discriminative reward on the model performance."}, {"title": "5.2. Impact of More Negative Samples", "content": "More negative supervisions help discrimination. Figure 1b illustrates the asymptotic growth of DPO and PerPO under increased sampling, preliminarily validating the value of negative samples. We further conduct a comprehensive comparison between PerPO and DPO across multiple benchmarks including RefCOCO+, RefCOCOg, LLaVAW, and POPE, examining performance disparities at varying sample sizes 2, 4, 8, 12, 20. In Table 3, observations reveal that increased sampling consistently led to improved performance across diverse metrics. Moreover, PerPO demonstrated more pronounced absolute performance and performance gains relative to DPO. This confirms the role of negative sample supervision in visual preference optimization. Notably, as sampling size N increases, performance gains saturate, indicating a loss of negative sample diversity. Thus, mining more diverse negative samples is critical and will be pursued in future work."}, {"title": "5.3. Impact of Discriminative Margin.", "content": "Reward itself serves as the perfect margin. As shown in Eq 6, we introduce a coefficient \u03b3 to finely modulate the influence of the differential discriminative rewards on the corresponding sample pairs. It can be seen that when \u03b3 = 0, PerPO simplifies to LiPO. When \u03b3 \u2260 0, unlike LiPO balanced ranking, PerPO can emphasize inter-sample distinctions, facilitating more targeted optimization."}, {"title": "5.4. Further Analysis", "content": "PerPO aims to unlock the model's full potential. PerPO's effectiveness seems to depend on the capability level of the model. Comparing SFT and PerPO performance on models trained with varying amounts of OCR data (0k, 25k, 50k), we found that PerPO's advantage emerges only as the model's capabilities mature. Figure 5 shows that with weak or no dense OCR capabilities, PerPO and SFT perform similarly. However, as the model approaches capability saturation, the area of the light blue region increases significantly, indicating that PerPO outperforms SFT. To sum up, SFT is crucial for imparting basic capabilities, whereas PerPO is key to unlocking the model's full potential in later stages.\nQualitative analysis."}, {"title": "6. Related Work", "content": "Reinforcement Learning from Human Feedback (RLHF). RLHF (Christiano et al., 2017; Stiennon et al., 2020) is a crucial technique for aligning Large Language Models (LLMs) with human preferences, comprising both reward model-based and model-free methods. In PPO (Schulman et al., 2017; Ouyang et al., 2022), an auxiliary reward model is cultivated first and then used to optimize the policy. Conversely, DPO (Rafailov et al., 2024) directly leverages preference data for policy optimization, offering a streamlined yet effective pathway for alignment. To mitigate overfitting, IPO (Azar et al., 2024) incorporates a regularization term. KTO (Ethayarajh et al., 2024) and DPOP (Pal et al., 2024) optimize the relative gain of outputs, bypassing the need for pairwise data. sDPO (Kim et al., 2024) uses multi-stage training for better alignment. ORPO (Hong et al.) and SimPO (Meng et al., 2024) adopt reference-free reward formulations to simplify alignment. Despite impressive results, these methods rely on labeled perference data, limiting their generalizability. In contrast, PerPO uses a discriminative reward mechanism, allowing data scaling without extra costs and enhancing model performance across diverse domains.\nMultimodal Large Language Models (MLLMs). MLLMs (Liu et al., 2024c; Yu et al., 2023; Zhu et al., 2024; Dong et al., 2024; Ghosal et al., 2023; Lin et al., 2023) integrate various data modalities into a unified framework, enabling more sophisticated content understanding and generation. Vision-Language Models (VLMs) are a prominent example, aligning visual encoders with LLMs to connect different modal information. Recently, MLLMs have been evolving to enhance reliability and incorporate ethical considerations, aiming to align their outputs with human values (Amirloo et al., 2024; Yu et al., 2024a; Xu et al., 2024). LLaVA-RLHF (Sun et al., 2023) leverages supplementary factual information to enhance the reward model, mitigating vulnerabilities like reward hacking. HA-DPO (Zhao et al., 2023) reframes hallucination as a preference task, introducing an efficient pipeline for generating high-quality, consistent sample pairs. Additionally, mDPO (Wang et al., 2024a) balances language and image preferences, reducing the over-emphasis on textual inputs. Nevertheless, these models focus on reasoning and reducing hallucinations, they often struggle with discriminative tasks requiring minimal analysis and concise answers. PerPO, however, can enhance models' visual comprehension abilities through discriminative rewards.\nGenerative and Discriminative."}, {"title": "7. Conclusion", "content": "In this paper, we highlight the limitations of Multimodal Large Language Models (MLLMs) in visual discrimination tasks, such as object recognition and dense OCR. We propose Perceptual Preference Optimization (PerPO), a framework that enhances MLLMs' visual discrimination capabilities through discriminative rewarding. PerPO constructs perceptual ordered preferences based on prediction deviations, optimizing performance without extensive human annotations. Experiments demonstrate significant improvements in MLLMs' performance and output robustness. Our method bridges generative and discriminative learning, advancing towards more comprehensive AI systems."}, {"title": "A. Comprehensive Assessment of PerPO", "content": "A.1. General Visual Capacity Assessment\nOur method enhances model perception by employing discriminative rewards in specific tasks like object grounding and dense OCR. To thoroughly evaluate PerPO's capabilities on general visual tasks, we included diverse benchmarks in Table 4, such as MM-Vet (Yu et al., 2024b), MM-Bench (Liu et al., 2024e), MMMU (Yue et al., 2024), VQAv2 (Goyal et al., 2017), and LLaVAW (Liu et al., 2023a). The results clearly demonstrate a significant advantage over SFT and DPO, confirming PerPO's superior efficacy.\nMM-Vet stands as a preeminent multimodal evaluation metric, critically assessing models across six dimensions: recognition, OCR, knowledge, language generation, spatial reasoning, and mathematical computation. Detailed evaluation results within MM-Vet are presented in Table 5. Obviously, our method excels across multiple tasks, indirectly suggesting an enhancement in the model's perceptual capabilities.\nMM-Bench is designed to systematically evaluate multimodal models on a range of vision-language tasks with emphasis on robustness, reasoning, and generalization. It often focuses on benchmarks that highlight deficiencies in current vision-language systems. Detailed evaluation criteria and associated tasks span domains like captioning, VQA, and multimodal reasoning.\nMMMU stands for multimodal multitask understanding, encompassing datasets and benchmarks tailored to models capable of performing multiple tasks. It is a concept designed to focus on advanced perception and reasoning with domain-specific knowledge, emphasizing flexibility and comprehension across various visual and linguistic scenarios.\nVQAv2 is a dataset for visual question answering, addressing issues like biases in earlier datasets. It contains pairs of images and questions with answers verified by human annotators, ensuring higher reliability and reducing the tendency of models to exploit statistical patterns in the dataset.\nLLaVAW evaluates multimodal large language models on real-world, unstructured inputs like everyday photos and screenshots. It focuses on tasks such as visual question answering, reasoning, and conversational understanding, using human and AI feedback to assess accuracy and relevance. This benchmark emphasizes practical robustness in diverse, open-world applications."}]}