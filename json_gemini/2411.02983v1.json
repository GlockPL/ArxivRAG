{"title": "Autonomous Decision Making for UAV Cooperative Pursuit-Evasion Game with Reinforcement Learning", "authors": ["Yang Zhao", "Zidong Nie", "Kangsheng Dong", "Qinghua Huang", "Xuelong Li"], "abstract": "The application of intelligent decision-making in unmanned aerial vehicle (UAV) is increasing, and with the development of UAV 1v1 pursuit-evasion game, multi-UAV cooperative game has emerged as a new challenge. This paper proposes a deep reinforcement learning-based model for decision-making in multi-role UAV cooperative pursuit-evasion game, to address the challenge of enabling UAV to autonomously make decisions in complex game environments. In order to enhance the training efficiency of the reinforcement learning algorithm in UAV pursuit-evasion game environment that has high-dimensional state-action space, this paper proposes multi-environment asynchronous double deep Q-network with priority experience replay algorithm to effectively train the UAV's game policy. Furthermore, aiming to improve cooperation ability and task completion efficiency, as well as minimize the cost of UAVs in the pursuit-evasion game, this paper focuses on the allocation of roles and targets within multi-UAV environment. The cooperative game decision model with varying numbers of UAVs are obtained by assigning diverse tasks and roles to the UAVs in different scenarios. The simulation results demonstrate that the proposed method enables autonomous decision-making of the UAVs in pursuit-evasion game scenarios and exhibits significant capabilities in cooperation.", "sections": [{"title": "I. INTRODUCTION", "content": "The capability of unmanned aerial vehicle (UAV) continues to enhance through the utilization of advanced flight control, payload, power, and other technologies. This progress serves as a new driving force for its technological development and facilitates the rapid generation of UAV's game capability in highly challenging environments. The development of UAV equipment technology exhibits characteristics such as networking, decentralization, cost-effectiveness, and intelligence. With advancements in sensor technology, airborne computing power, and communication capabilities of weapons and equipment, the performance of UAV will witness further enhancements. Consequently, these low-cost and mass-produced UAV will find wider application across various scenarios. Equipped with autonomous decision-making capabilities, UAV can significantly contribute to areas including reconnaissance missions, manned-unmanned cooperation, as well as pursuit-evasion game.\nAt present, the research on UAV pursuit-evasion game primarily concentrates on 1v1 UAV game and multi-UAV cooperative game. In the field of 1v1 UAV pursuit-evasion game, there are three traditional method: game theory for modeling and solving pursuit-evasion game scenarios [1]\u2013[4], optimization theory to model pursuit-evasion game as a multi-objective decision optimization problem [3], [4], and utilizing artificial intelligence decision technology with self-learning capabilities [5]. The game theory-based approach is limited by its myopic focus on short-term advantages in UAV game, and the difficulty of accurately modeling complex pursuit-evasion game scenarios. The computational performance of the pursuit-evasion game decision method, based on optimization theory, often fails to satisfy the real-time requirements of pursuit-evasion game decision-making and is primarily employed for offline research aimed at optimizing pursuit-evasion game policies. While the pursuit-evasion game situation exhibits significant diversity and the artificially generated rules are incapable of encompassing all conceivable scenarios. Consequently, while the method may appear straightforward, it necessitates a substantial workload and falls short in terms of both robustness and accuracy. The emergence of deep learning technology has led to significant advancements in various domains [6]\u2013[9]. Reinforcement learning (RL), an artificial intelligence technique for intelligent decision-making, has merged with deep learning. In recent years, deep reinforcement learning has emerged as one of the most successful methodologies in the field of artificial intelligence, with widespread applications in intelligent decision-making, control and so on [10]\u2013[15], and also plays a crucial role in intelligent pursuit-evasion game. By establishing a decision-making framework to govern the agent-environment interaction and formulating a rational reward function, deep reinforcement learning empowers the UAV agent to effectively acquire knowledge and make informed decisions in pursuit-evasion game scenarios. This not only enhances confront effectiveness but also bolsters survival capabilities, thereby attracting considerable attention from researchers in the field of intelligent pursuit-evasion game. In order to enhance the efficiency of reinforcement learning algorithms in exploring the policy space, Zhang et al. [16] proposed a heuristic Q-network approach that incorporates expert knowledge to guide the search process. This method utilizes the heuristic Q-network technique to train neural network models for solving the over-the-horizon pursuit-evasion game maneuver decision problem. Yang et al. [17] Proposed a approach that presents a decision-making method for autonomous maneuvers of UAV in pursuit-evasion game, utilizing the DDPG algorithm. This method effectively filters out numerous invalid action values using optimized pursuit-evasion game maneuver action values generated by the optimization algorithm. Furthermore, it incorporates the optimized action into the replay buffer as an initial sample, thereby enhancing both game effectiveness and survivability of the DDPG algorithm during UAV pursuit-evasion game.\nThe increasing complexity of the UAV application environment and the growing diversity of tasks have posed challenges for a single UAV to effectively handle various application scenarios. With the development of deep reinforcement learning technology in the multi-agent field [18]\u2013[20], the cooperative technology of multiple UAVs has emerged as an imperative solution and a significant developmental trend. Based on the 1v1 UAV game, researchers have devoted their efforts to studying multi-UAV cooperative pursuit-evasion game. The decision-making problem of cooperative multi-target attack in pursuit-evasion game was investigated by Luo et al. [21], who proposed a heuristic adaptive genetic algorithm to effectively explore the optimal solution for missile target assignment. The proposed approach by Wang et al. [22] employed the clonal selection algorithm to establish a multi-step UAV dynamic weapon-target assignment game model, based on the double matrix game Nash equilibrium point solution method, resulting in a more precise Nash equilibrium solution. Furthermore, the technology of deep reinforcement learning also finds extensive applications in the domain of multi-agent systems. Zhang et al. [23] successfully implemented communication between UAVs through bidirectional recurrent neural networks, integrating target allocation and pursuit-evasion game situation assessment to generate cooperative tactical maneuver strategies that merge formation tactical objectives with each UAV's reinforcement learning objective. Li et al. [24] proposed a multi-agent double soft actor-critic algorithm, which employs a distributed execution framework based on decentralized partially observable Markov decision process and centralized training. It considers the multi-UAV cooperative pursuit-evasion game problem as a complete cooperative game in order to achieve effective collaboration among multiple UAVs. The aforementioned methods consider the communication and collaboration among multiple UAVs to effectively accomplish cooperative pursuit-evasion game missions. However, they regard the UAV formation as a whole, with only cooperation rather than detailed division of labor, focusing on winning the game while overlooking the cost of UAV's during such games. These approaches may lead to even if the UAV formation gain the eventual triumph, but individual UAVs may be exposed to potential encirclement, causing losses.\nThis paper proposes a deep reinforcement learning-based cooperative game method for multi-role formation of UAVs to effectively address this issue, wherein each UAV is assigned distinct roles in the pursuit-evasion game to optimize victory rate and minimize the cost of game. The main contributions of this paper are as follows:\ni) The proposed algorithm, MEADDQN, enhances the efficiency of data collection for interactive training in reinforcement learning and improves sample efficiency through PER;\nii) We designed reward shaping for two different UAV roles and conducted training, enabling them to proficiently perform pursuit and bait tasks respectively;\niii) We established multi-role UAV cooperative pursuit-evasion game framework and validated its effectiveness in scenarios involving 2v1, 2v2, and 3v2, yielding favorable outcomes.\nThe subsequent sections of the paper are structured in the following manner. Section 2 presents the UAV dynamics model and offers a comprehensive exposition of the pursuit-evasion game system. Section 3 presents the maneuvering decision algorithms employed by the opposing sides. Section 4 presents the components involved in constructing reinforcement learning models. Section 5 presents the training and testing of the model, which are demonstrated through simulation analysis. And the paper concludes with Section 6, presenting a comprehensive summary encompassing the entirety of the study."}, {"title": "II. DESCRIPTION OF UAV PURSUIT-EVASION GAME SYSTEM", "content": "The UAV dynamics model serves as the fundamental basis for comprehending the pursuit-evasion environment of UAV. This study aims to investigate the intelligent decision-making capabilities of UAV in such environments. Consequently, when establishing the UAV model, it is abstracted as a particle model and employs a 3 degree of freedom (3-DOF) control mode [25].\nIn the inertial coordinate system, the state variables of the 3-DOF equation for UAV consist of [x, y, z, v, \u03b3, \u03c8], where (x, y, z) represents the positional information of UAV in the inertial coordinate system, v is a scalar denoting the velocity of UAV, \u03b3 and \u03c8 are respectively indicative of the pitch angle and yaw angle of UAV, signifying its direction of motion. Where, \u03b3 is defined as the angle between \\vec{v}, the velocity vector of the UAV, and the x-o-y plane of the inertial coordinate system. \u03c8 is defined as the angle between \\vec{v} and the y-axis, while \\vec{v}' is the projection of \\vec{v} onto the x-o-y plane of the inertial coordinate system.\nThe control variable of UAV can be represented by three parameters:[nx, nz, \\dot{\u03c8}]. Where, nx represents the overload in the direction of UAV velocity, which is used to control the acceleration and deceleration. The variable nz represents the vertical axis overload of the UAV body, while \\dot{\u03c8} denotes the roll angle of the velocity vector, and they control the change of velocity direction collectively. The intelligent algorithm utilizes these three control variables to determine the maneuvering mode of the UAV, thereby enabling it to execute intricate aerial maneuvers and accomplish the pursuit-evasion game missions. Affected by these three control parameters, the changes in UAV's speed, roll angle and yaw angle are as follows:\n\\begin{equation}\n\\begin{cases}\n\\dot{v} = g(n_x - \\sin \\gamma)\\\\\n\\ddot{\\gamma} = \\frac{g}{v}(n_z \\cos \\phi)\\\\\n\\dot{\\psi} = \\frac{g n_z \\sin \\phi}{v \\cos \\gamma}\n\\end{cases}\n\\end{equation}\nFurthermore, in the inertial coordinate system, the UAV coordinates exhibit the following variations:\n\\begin{equation}\n\\begin{cases}\n\\dot{x} = v \\cos \\gamma \\sin \\psi\\\\\n\\dot{y} = v \\cos \\gamma \\cos \\psi\\\\\n\\dot{z} = v \\sin \\gamma\n\\end{cases}\n\\end{equation}"}, {"title": "B. Judgment Standard of Interception in Pursuit-Evasion Game", "content": "The advantages and disadvantages of the confrontation in the pursuit-evasion game environment are conveyed via the relative situational information of the UAVs. It is expected that UAV can achieve intelligent decision-making to secure more advantageous firing positions during pursuit-evasion game. The coverage of a UAV's firepower typically forms a frontal cone, thereby enabling the determination of UAV's advantages and disadvantages in a pursuit-evasion game environment based on the UAV's orientation. The pursuit-evasion game environment discussed in this paper does not encompass the simulation of UAV firepower. Therefore, in order to effectively neutralize enemy aircraft, the UAV must autonomously maneuver and strategically position itself behind the target UAV, ensuring a tail chase to target is executed within UAV's firepower range. This study imposes a strict numerical constraint on tracking interception in pursuit-evasion games.\nAs shown in Fig. 2, the vector \\vec{P} represents the relative position of the UAV and the target, the antenna train angle \u03b1u corresponds to the angle between the velocity vector \\vec{v}_u of the UAV and the relative position vector \\vec{P}, and the aspect angle \u03b1\u03c4 refers to the angle between the velocity vector \\vec{v}_\u03c4 of the target and the relative position vector \\vec{P}. The UAV is judged to successfully intercept the target when \u03b1u < 5\u00b0, \u03b1\u03c4 < 90\u00b0, and d = \\frac{\\Vert \\vec{P} \\Vert}{\\Vert \\vec{P} \\Vert} < 800m."}, {"title": "III. MANEUVER POLICY MODELING FOR UAV PURSUIT-EVASION GAME", "content": "In the UAV pursuit-evasion game scenario described in this study, the UAV formations representing opposing factions are denoted as red and blue correspondingly. The red team policy network model in this paper is trained using the deep reinforcement learning algorithm to guide the red UAVs in making maneuver decisions during cooperative pursuit-evasion game tasks within the red-blue formation. The blue UAVs realize their maneuver decision through the matrix game algorithm, serving as an adversary to evaluate the efficacy of the deep reinforcement learning algorithm."}, {"title": "A. Red Team - Multi-Environment Asynchronous Double Deep Q-network Algorithm with Prioritized Experience Replay", "content": "The field of reinforcement learning is dedicated to maximizing agent's cumulative reward within a complex and uncertain environment. The agent improves its action selection by perceiving the environmental state and receiving rewarding feedback, thus obtaining the maximum return. Reinforcement learning problems are usually modeled utilizing Markov decision processes (MDP). MDP is a mathematical framework that models the decision-making process of an agent in an uncertain environment. It captures the notion that future states are determined solely by the current state and actions taken, without any dependence on past states. MDP can be represented using a quad-tuple:(S, A, P, R). The state space is denoted as S, the action space as A, the environment state transition probability as P, and the reward function S\u00d7A \u2192 R quantifies the amount of feedback that agent can receive for executing an action in the current state.\nR(s, a) = E [rt+1 | St = s, at = a]\n(3)\nThe primary objective of reinforcement learning algorithms is to optimize strategies through interactive trial and error, with the ultimate goal of maximizing the returns.\n\\begin{equation}\n\\operatorname{maximize} G_t = \\sum_{k=0}^{T-t} \\gamma^k R_{t+k}\n\\end{equation}\nWhere, Gt is the returns, which is the cumulative discount reward after time t, and \u03b3 is a discount factor that satisfies 0 \u2264 \u03b3 \u2264 1. The aforementioned definition can be intuitively comprehended as an agent focusing more on near-term rewards than on rewards that are further away.\nReinforcement learning algorithm optimizes a policy \u03c0, \u03c0:S \u2192 A is the mapping function of the agent from state to action. Via maintaining an action value function Q\u03c0(s, \u03b1), the value-based reinforcement learning algorithms evaluate the benefit of selecting action a in state s when agent's policy \u03c0 is determined.\n\\begin{equation}\nQ^{\\pi}(s, a) = E_{\\pi} \\left[ \\sum_{k=0}^{\\infty} R_{t+k+1} | S_t = s, A_t = a \\right]\n\\end{equation}\nThe action-value function is updated iteratively according to the Bellman Equation (6), and Q*(s, a) = \\max Q\u3160(s, a) is obtained through constantly approximation. Based on this, agent can obtain the optimal policy \u03c0*(als) = arg maxa\u2208A(s) Q*(s, a).\nQ\u03c0(\u03c2, \u03b1) = \u0395\u03c0[Rt+1 + \u03b3Q\u3160(St+1, At+1)|St = s, At = a]\n(6)\nMnih et al. [11] utilized Q(s, a; 0) function to approximate the optimal Q*(s, a) function and employed a deep neural network to solve for Q(s, a; 0), which forms the fundamental concept of the Deep Q-Network (DQN) algorithm [26], [27]. In order to improve the efficiency and stability of the algorithm, a target network Q(s, a; \u03b8') is added, which participates in the training of the policy network and replicates the current parameters of the policy network at regular intervals. The target network in the training process introduces a certain time delay to decouple the value estimation of adjacent moments, thereby mitigating the impact of unstable fluctuations in data transmission during each iteration. In Double DQN [28] algorithm, the loss function of the neural network is\n\\begin{equation}\nL(\\theta) = \\mathbb{E}\\left[ Q(s_t, a_t; \\theta) - r_t + \\gamma Q(s_{t+1}, a_{\\max}; \\theta') \\right]^2\n\\end{equation}\nThe solution can be obtained by gradient descent method. Double DQN is an off-policy reinforcement learning algorithm that can utilize a distinct policy for data acquisition, which differs from the current update policy. The interactive data will be stored in the replay buffer as the form of transition transition (st, at,rt, St+1) and trained using the time series difference method, thereby effectively enhancing data utilization. Prioritized experience replay (PER) [29] is a method for sampling interactive data. When storing each transition in the replay buffer, PER assigns different priorities to each transition based on the absolute value of its TD-Error |dt| (8), and selects the transition with higher priority for training with a higher probability during sampling.\n\\begin{equation}\nd_t = R_t + \\gamma V(S_{t+1}) - V(S_t)\n\\end{equation}\nThis section proposes a Multi-Environment Asynchronous Double Deep Q-Network (MEADDQN) algorithm, which serves as a further optimization of the aforementioned algorithm through the introduction of multi-environment asynchronous experience collection, with the objective of expediting the training process and enhancing the efficiency of acquiring pursuit-evasion game interaction data. As shown in Fig. 3, MEADDQN concurrently generates multiple pursuit-evasion game environments with identical tasks in parallel threads, each environment being initialized differently. Agents operate asynchronously and interact with distinct environments while adhering to the same policy network. All interaction data collected from these environments is consolidated into a unified replay buffer that supports PER, enabling sampling of data from the buffer during training. To enhance the algorithm's robustness and explore strategic possibilities, this paper introduces different action noise to the UAV agents in diverse interactive environments. In environments with higher levels of action noise, agents will engage in more audacious exploration, whereas in environments with lower levels of action noise, agents will leverage their acquired experience to identify the most rewarding decision within the current policy."}, {"title": "B. Blue Team - Matrix Game Algorithm", "content": "This paper improves the UAV matrix game algorithm [30] for multi-UAV. In the multi-UAV matrix game algorithm, the blue UAV constructs a b\u00d7r-dimensional matrix G for each red UAV, where b represents the number of available actions for the blue UAV and r represents the number of available actions for the red UAV. The value of Gij in this matrix represents the reward score for the blue side, based on the current pursuit-evasion game situation, assuming that the blue side take action i and the red side take action j, following a similar approach as reinforcement learning's reward function to ensure fairness in subsequent adversarial games. After obtaining these payoff matrices, further processing is carried out:\ni) Find the row minimum of each row of each payoff matrix and sum according to the row index i;\nii) Select the maximum value from these minimum value sums;\niii) The action a\u017c corresponding to the maximum value's row index is the optimal maneuver of the blue side."}, {"title": "IV. REINFORCEMENT LEARNING ELEMENT IN MULTI-ROLE UAV PURSUIT-EVASION GAME", "content": "The reinforcement learning problem is typically described using the MDP 4-tuple model (S, A, P, R), where the state transition probability P is determined by the environment itself and does not necessitate explicit modeling in model-free reinforcement learning algorithms. Therefore, this section will provide a detailed description of modeling the state space, action space, and reward function for the reinforcement learning task of multi-role UAVs cooperation."}, {"title": "A. State Space", "content": "The state space in this paper is designed to encompass all the state information of both UAVs, as well as variables that can express the relative information between the two opposing sides. This comprehensive representation serves as input for the policy network, enabling it to make informed decisions in the current confrontational scenario. UAV's state information can be characterized by its position, pitch angle, and yaw angle. Furthermore, the variables illustrated in Fig. 2 can also depict the relative information during pursuit-evasion game. The efficacy of reinforcement learning training is ensured by representing the state of UAV in this paper as a 13-dimensional variable:\n(zu, vu, \u03b3\u03c5, \u03c8u,ZT, UT, \u1f59T, \u03c8\u03c4, \u03b1\u03c5, \u03b1\u03c4, d, \u03b3\u03c1, \u03c8\u03c1)\n(9)\nThe first four quantities represent the attributes of the UAV itself: zu denotes the altitude, vu is a scalar that represents speed, Yu signifies the pitch angle, and \u03c8\u03c5 indicates the yaw angle. The subsequent four quantities depict the characteristics of the target UAV: z refers to its altitude, vr denotes its speed, \u04af\u0442 represents its pitch angle, and T indicates its yaw angle. The remaining five quantities are employed to represent the relative information between the two drones, where \u03b1\u03c5, \u03b1r and d are introduced in Section II as indicators for antenna train angle, aspect angle and distance respectively. Additionally, as for the relative position vector \\vec{P}, its numerical magnitude is indicated by d, and the orientation of \\vec{P} can be represented in a similar manner to the pitch and yaw angles of the UAV point model."}, {"title": "B. Action Space", "content": "The present study utilizes the body axis direction overload, longitudinal overload, and roll angle as control variables to establish a 3-DOF UAV particle control model. This model facilitates more accurate simulation of realistic flight trajectories. The present section introduces a 15-dimensional discrete action space specifically tailored for the DDQN algorithm in the context of reinforcement learning with discrete control [31]. This customized discrete action space aims to accommodate the three control variables (nx, nz, \u03c6), which is shown as Fig. 4."}, {"title": "C. Reward Function for Multi-role UAV", "content": "The reward function design in the task of UAV cooperative pursuit-evasion game aims to strategically guide the victory of the pursuit-evasion game. Thus the final outcome rfinal which signifies the victory or defeat of pursuit-evasion game, can be utilized directly as a reward signal. However, the agent is only provided with rfinal at the end of each episode, necessitating a prolonged waiting period to ascertain the correctness of its actions. Moreover, identifying advantageous action paths in environments with sparse reward poses a formidable challenge for the agent. This section enhances the efficiency of reinforcement learning algorithms through the utilization of dense reward shaping for the pursuit-evasion game task. The effective allocation of tasks is critical strategies for enhancing the win rate of pursuit-evasion game and minimizing operational losses. The UAV entities in the pursuit-evasion game environment are assigned the following roles: one type is designated for target attack and pursuit, called pursuit UAV, while the other type, called bait UAV, functions as a bait to draw enemy fire and create one-on-one or even multi-on-one scenarios for other UAVs. This section tailors the dense rewards in distinct manners for these two categories of UAVs, ensuring their ability to successfully accomplish their respective tasks."}, {"title": "1) Pursuit UAV Reward Shaping:", "content": "Angle advantage reward:\n\\begin{equation}\nr_p = 1 - \\frac{\\alpha_U + \\alpha_T}{2\\pi}\n\\end{equation}\nThe angle advantage reward aims for that \u03b1\u03c5 and \u03b1\u03c4 should be minimized, which aligns with the angle requirements of the judgment standard of interception.\nDistance advantage reward:\nrd = exp \\left(\\frac{abs(||P|| - d_{opt})}{d_o}\\right)\n(11)\nThe distance advantage reward is designed to guide the UAV to reach the objective distance to the target. dopt in (11) represents the objective distance, set to dopt = 800m, and do is a distance constant parameter.\nVelocity advantage reward:\nrv = \\frac{\\vec{v}_U \\cdot \\vec{P}}{v_{max} ||P||}\n(12)\nThe velocity advantage reward is directly proportional to the projection of the UAV's velocity vector v\u03c5 onto the relative position vector P. The range of the velocity advantage reward is constrained to [-1,1] by \\frac{v}{v_{max}}, which aligns with the range of the other two rewards.\nTo sum up, combining the collision and out-of-bound penalty term r_{punish}, the pursuit UAV's dense reward rt design is as follows:\n\\begin{equation}\nr_t = \\begin{cases}\nr_{final}, & \\text{intercept or be intercepted} \\\\\nr_{punish}, & \\text{collision or out of bound} \\\\\nw_1 r_p + w_2 r_d + w_3 r_v, & \\text{otherwise}\n\\end{cases}\n\\end{equation}\nw1, w2, and w3 are the weights of each reward."}, {"title": "2) Bait UAV Reward Shaping:", "content": "Angle advantage reward:\n\\begin{equation}\nr_p = 2 * exp \\left( \\frac{abs(\\alpha_T - \\alpha_{opt})}{a_o} \\right) - 1\n\\end{equation}\n(14) represents that bait UAV needs to maintain an aspect angle to have a sufficient decoy effect on the target UAV, where Xopt is bait UAV's objective aspect angle, and ao is an angle constant parameter.\nDistance advantage reward:\n\\begin{equation}\nr_d = exp \\left(\\frac{abs(||P|| - d_{opt})}{d_o}\\right)\n\\end{equation}\n(15)\nThe distance advantage reward utilizes the identical calculation methodology as the pursuit UAV, wherein the objective distance is designated as dopt = 1500m. This ensures the safety of UAV while concurrently generating a decoy effect against the target.\nThe bait UAV does not necessarily require a consistent velocity advantage, but rather should be strategically positioned to allure the target. Therefore, considering the penalty term rpunish for collision and out-of-bound situations, the dense reward rt for the bait UAV is designed as follows:\n\\begin{equation}\nr_t = \\begin{cases}\nr_{final}, & \\text{be intercepted} \\\\\nr'_{punish}, & \\text{collision or out of bound} \\\\\nw_1 r_p + w_2 r_d, & \\text{otherwise}\n\\end{cases}\n\\end{equation}"}, {"title": "V. EXPERIMENT", "content": "In this study, pursuit UAV and bait UAV were trained independently and subsequently integrated within the multi-UAV environment. Both agent types use the same policy network architecture comprising three hidden layers with 512, 1024, and 512 neurons respectively, facilitating the policy transformation in a multi-UAV environment. In reinforcement learning training, the discount factor is \u03b3 = 0.95, the replay buffer capacity is 100000, the batch size is 1024, and the activation function is ReLU.\nTo faithfully replicate actual UAV flight conditions, no horizontal constraints are enforced within the airspace where pursuit-evasion game takes place while only the z-axis boundary is set as 1000m < z < 13000m in the inertial coordinate system. In the designated airspace, a specified number of red and blue UAVs are deployed for pursuit-evasion game by their own maneuver policy. The outcome is determined based on the cost of the game between the two sides' UAVs. Once a UAV is successfully intercepted by the opponent, we consider it destroyed and remove it from the pursuit-evasion game environment. The team that successfully intercepts all of the opponent first shall emerge as the victor. The initialization process of the pursuit-evasion game environment involves fixing the position of the red UAVs and establishing a spatial coordinate system with a red UAV as its origin. Then the blue UAVs should be initialized randomly within a rectangular space centered on the red UAV, which is limited 20000m in length, 20000m in width, and 6000m in height, as shown in Fig. 5, to ensure diversity in the initial position. To avoid the issue of decision steps becoming excessively short in subsequent rounds due to initial distances being too close, this study implemented an initial vacuum zone within a rectangular region measuring 4000 meters in length, 4000 meters in width, and 6000 meters in height, as shown in Fig. 5. The blue UAVs will not be initialized in this area, thereby ensuring a certain distance is maintained between both sides during the initial phase of the confrontation. As for the initialization of the flight attitude of UAVs, this study assumes that both UAVs start with a horizontal flight position. The red UAVs' initial yaw angle is set to \u03c8\u03c5 = 0, while the yaw angle of the blue team drone is randomly initialized. In this way, the simulated confrontation allows for various initial postures between the two side UAVs, thereby simulating states of advantage and disadvantage."}, {"title": "A. Basic Training of Pursuit UAV and Bait UAV.", "content": "In the initial stages of training, the policy network of the red team is initialized randomly. However, this can lead to consistent failures for the red team and even difficulties in obtaining positive reward signals when directly confronted with a blue team that possesses a higher level of intelligent decision-making capability. Consequently, not only does this situation impact training efficiency but it also results in an exceedingly slow convergence speed. To solve this problem, this study proposes three basic training methods for the red team policy network. These basic training methods are intended to enable the red team's policy network to learn the maneuver model of the drone and the basic logic of pursuit-evasion game. Through a progressive sequence from simplicity to complexity, these three training methods enable the red team's policy network to gradually assimilate knowledge pertaining to UAV pursuit-evasion game. This not only enhances the intelligence level of the UAV agent but also establishes a solid groundwork for competing against highly intelligent decision-making adversaries. The three basic training scenarios are as follows: \u2460 against straight-line maneuver target; \u2461 against circling maneuver target; \u2462 against random maneuver target.\nFirst, pursuit UAV's policy network undergoes sequential basic training using MEADDQN with PER for three basic sessions, each consisting of 200000 steps. Finally, all these basic trainings are amalgamated, with one randomly initialized in each round for a total training duration of 600000 steps."}, {"title": "B. Against Matrix Game Algorithm Training.", "content": "After successfully completing the three basic training sessions, it can be inferred that the red UAV's policy network possesses a rudimentary comprehension of the 3DOF particle model of drones and the fundamental principles of pursuit-evasion game, and exhibits certain game capabilities. Consequently, it can game with blue UAV that employs matrix game algorithm and possesses intelligent decision-making ability. Fig. 8 shows the training reward curves of the pursuit UAV and bait UAV in a pursuit-evasion game against UAV controlled by matrix game algorithm.\nAdditionally, Fig. 9 presents a comparison of MEADDQN with PER to other reinforcement learning algorithms using the training of the pursuit UAV as an example.\nFig. 10 demonstrates the track simulation results of the pursuit-evasion game of two different roles against matrix game algorithm, and gives real-time reward curves. As Fig. 10(a) shows, the Pursuit UAV can perform the pursuit-evasion game task very well, consistently maintaining a dominant position throughout the pursuit process. In the dogfight where both sides gain similar rewards, the pursuit UAV can adjust to secure the dominant position, thereby widening the reward gap and ultimately intercepting the blue UAV. In the test, the interception reward is set to rfinal = 2. The target of the bait UAV consistently receives positive reward feedback in Fig. 10(b), because the bait UAV in the trajectory simulation always stays in a position that appears to be advantageous to its target, Moreover, the special reward calculation method for the bait UAV enables it to receive a high reward under these circumstances."}, {"title": "C. Multi-Role UAV Cooperative Pursuit-Evasion Game.", "content": "After training pursuit UAV and bait UAV, two different roles of UAV, this study proposed a Multi-role UAV cooperative pursuit-escape game method, which also uses matrix game algorithm as the opponent for pursuit-escape game. According to the Multi-UAV pursuit-escape game environment introduced in Section II, in the Multi-role UAV cooperative pursuit-escape game, the red UAVs are initialized with different roles and execute different strategies, while the blue UAVs are controlled by matrix game algorithm. Once the red UAV has determined its own role, it must also establish a clear target, whether it be pursuit or bait. The roles and targets of each UAV can be assigned based on the current game situation. In order to enhance the efficiency of the game, this study prioritizes allocation methods that can quickly achieve tail-biting scenarios, enabling pursuit UAV to quickly eliminate Blue Team members. Bait UAV are responsible for creating one-on-one or multi-on-one scenarios to enable pursuit UAV to avoid potential encirclement.\nFig. 11 shows the training and testing procedure of multi-role UAV cooperative pursuit-evasion game. In training procedure, the pursuit and bait policies, which have been trained in the 1v1 UAV game, are concurrently applied in the multi-UAV environment and iterative trained through independent reinforcement learning. In each iteration, only one agent in the environment is designated as the training status, while the policies of the remaining agents are held constant. This approach effectively mitigates environmental fluctuations and enhances training stability. And as MEADDQN is an offline reinforcement learning algorithm, the interaction data from agents using the same policy as the agent being trained can also be utilized for training and updating the policy network, thereby enhancing sample efficiency. During the training process, only the UAVs' target are allocated, while their roles remain unchanged after initialization. In the test, the roles and targets of the UAVs are allocated by the role and target allocation system every 10 decision steps or after a UAV is intercepted. The study conducted experiments in 2v1, 2v2, and 3v2 settings, implementing corresponding methods for role and target allocation as presented in Table I. the simulation results are illustrated in Fig. 12.\nFig. 12(a) shows that two red UAVs play the role of pursuit UAV, successfully tracking and interception the target about 70 seconds after the 2v1 game begin. Fig. 12(b) depicts a game scenario involving two red UAVs, wherein one is designated as the pursuit UAV and the other as the bait UAV. The bait UAV sacrifices itself to strategically create a favorable situation for the pursuit UAV.(The reward curves of simulation tests in Fig. 12 use the reward calculation methodology of the pursuit UAV to show the UAVs' advantage and disadvantage in a pursuit-evasion game.) Fig. 12(c) illustrates that at the beginning of the 3v2 game, two red UAVs function as pursuit UAVs while one red UAV serves as a bait UAV. The target has been successfully intercepted by the pursuit group approximately 30 seconds into the game. At this moment, the bait UAV transitions into pursuit mode and efficiently accomplishes the interception of the target."}, {"title": "VI. CONCLUSION", "content": "This paper proposes the MEADDQN with PER algorithm to address the problem of multi-UAV pursuit-evasion game. By assigning distinct tasks to UAVs in the pursuit-evasion game environment, We trained two types of UAVs with different policies, enabling them to collaboratively solve the multi-drone pursuit-evasion game problem through collaboration and task allocation. The method proposed in this paper enhances the mission efficiency and cooperation ability of multi-UAV pursuit-evasion game. The future will witness further exploration of multi-UAV cooperation scenarios, with the aim of proposing universally applicable methodology for role and target allocation. In addition, the challenge of autonomous decision-making in the presence of incomplete information, particularly when the UAV lacks a comprehensive perception or encounters communication obstacles, constitutes a primary focus for our forthcoming studies."}]}