{"title": "Exploratory Models of Human-AI Teams: Leveraging Human Digital Twins to Investigate Trust Development", "authors": ["Daniel Nguyen", "Myke C. Cohen", "Hsien-Te Kao", "Grant Engberson", "Louis Penafiel", "Spencer Lynch", "Svitlana Volkova"], "abstract": "As human-agent teaming (HAT) research continues to grow, computational methods for modeling HAT behaviors and measuring HAT effectiveness also continue to develop. One rising method involves the use of human digital twins (HDT) to approximate human behaviors and socio-emotional-cognitive reactions to AI-driven agent team members (Barricelli & Fogli, 2024). In this paper, we address three research questions relating to the use of digital twins for modeling trust in HATs. First, to address the question of how we can appropriately model and operationalize HAT trust through HDT HAT experiments, we conducted causal analytics of team communication data to understand the impact of empathy, socio-cognitive, and emotional constructs on trust formation. Additionally, we reflect on the current state of the HAT trust science to discuss characteristics of HAT trust that must be replicable by a HDT such as individual differences in trust tendencies (e.g., propensity to trust, Jessup et al., 2019), emergent trust patterns (e.g., trust violation and repair, Wildman et al., 2024), and appropriate measurement of these characteristics (e.g., growth modeling, Abramov et al., 2020). Second, to address the question of how valid measures of HDT trust are for approximating human trust in HATs, we discuss the properties of HDT trust: self-report measures, interaction-based measures, and compliance type behavioral measures. Additionally, we share results of preliminary simulations comparing different LLM models for generating HDT communications and analyze their ability to replicate human-like trust dynamics. Third, to address how HAT experimental manipulations will extend to human digital twin studies, we share experimental design focusing on propensity to trust for HDTs vs. transparency and competency-based trust for AI agents.", "sections": [{"title": "1 Introduction", "content": "The integration of artificial intelligence (AI) into operational environments has become increasingly vital across diverse domains, fundamentally transforming how humans and machines collaborate to achieve shared objectives. Over the past decade, Human-AI Teaming (HAT) research has emerged as a critical field, with scholars applying cognitive science principles to understand the complexities of these novel partnerships. This growing body of research reflects the urgency of understanding how humans and AI can work together effectively, safely, and productively.\nAs the HAT literature expands and AI capabilities advance, innovative methodologies for studying these interactions have emerged. One promising approach involves the use of human digital twins (HDTs) - computational models designed to replicate human responses and behaviors within HAT contexts. These HDTs can be configured to simulate both state-based responses (such as transient cognitive and affective reactions) and trait-based characteristics (including dispositional individual differences), offering a versatile alternative to traditional human-subjects research [1, 2].\nThe advantages of HDTs in HAT research are significant. Beyond addressing the practical constraints of human-subjects studies, such as cost and recruitment challenges, HDTs provide unprecedented control over experimental variables and the ability to rapidly test multiple scenarios. This capability is particularly valuable for investigating complex team phenomena, with trust emerging as a critical area of focus. Trust - the willingness to be vulnerable to another agent's actions serves as a fundamental determinant of HAT effectiveness and success.\nHowever, the validity of using HDTs to study trust dynamics in HATs requires careful examination. This paper addresses three crucial questions regarding the implementation of HDTs in trust research: (1) How can we effectively model and measure HAT trust using HDT-based approaches? (2) What are the essential characteristics of HAT trust that must be operationalized in HDT trust models? (3) How do experimental manipulations from traditional HAT studies translate to HDT-based research?\nBy examining these questions, we aim to establish a framework for validating and implementing HDTs in HAT trust research, ultimately advancing our understanding of human-AI collaboration and trust development."}, {"title": "2 Research Question 1: How can we model and measure HAT trust dynamics in HDTs?", "content": null}, {"title": "2.1 Theoretical Considerations for Modeling HAT Trust in HDTATS", "content": null}, {"title": "2.1.1 Evolution of Trust Definitions in Human-AI Contexts", "content": "Trust has emerged as a multifaceted construct whose definition has evolved alongside technological advancement. In organizational contexts where HATs operate, Mayer et al. [3] established a foundational definition of trust as an individual's willingness to be vulnerable to another agent's actions during goal-oriented, risky situations. This conceptualization gained particular relevance in AI-driven systems, where users must navigate the inherent risks of relying on imperfect automation [4]. Building on this foundation, Lee and See [5] specifically addressed trust in automation, defining it as an attitude regarding an automated agent's reliability in risk-characterized situations. While this definition remains influential in HAT research, it primarily reflects traditional human-automation relationships characterized by clear supervisory hierarchies [6]. Modern AI systems, however, engage in more sophisticated interactions that mirror human teamwork dynamics, suggesting trust should be reconceptualized as a dynamic, relational construct [7]."}, {"title": "2.1.2 Trust Formation and Reliance in Human-AI Teams", "content": "Lee and See's [5] theoretical model suggests that trust in automation develops through people's understanding of three key elements: the system's intended functions, expected performance levels, and underlying processes. This framework suggests that optimal trust and consequently, appropriate reliance emerges when users' perceptions align with the AI's actual capabilities [4, 8]. This alignment becomes particularly crucial in safety-critical domains like search and rescue operations or disaster response, where HAT failures can have severe consequences [9]. The wealth of empirical literature on trust in AI makes it an ideal construct for exploring the utility of HDTAT experimental paradigms.\nHowever, recent meta-analyses challenge the assumed relationship between trust attitudes and behavioral reliance [10, 11, 12]. Users may maintain general trust in an AI system while choosing not to rely on it in specific instances, highlighting the distinction between attitudinal and behavioral trust. This observation aligns with the argument that trust in automation is indisputable only when actual reliance is observed [13]. Consequently, HDTAT experiments must model both perceptual trust, which encompasses attitudes and perceptions about an AI teammate's trustworthiness, and behavioral trust, which manifests through observable decisions reflecting trust through actions."}, {"title": "2.1.3 Beyond Capability-Based Trust", "content": "Contemporary research reveals that trust in HATs extends beyond perceptions of AI capabilities. Jessup et al. [14] emphasize the role of automation trust propensity an individual's baseline tendency to trust automated systems. Hoff and Bashir [15] characterize this as dispositional trust, shaped by demographic and personality factors including age, gender, culture, and individual traits. While these variables prove difficult to control in traditional laboratory studies, HDTATs offer unique opportunities to explore their effects systematically, particularly regarding emotional and moral dimensions of trust that remain understudied [16]."}, {"title": "2.1.4 Trust as a Dynamic Social Construct", "content": "The validation of HDTAT trust models must acknowledge trust's dynamic and social nature. Research has identified temporal patterns in human-automation trust, including the \"perfect automation schema\" where users initially overestimate unfamiliar automation capabilities [17], and a tendency to undervalue AI reliability following errors [18]. These patterns align with Lee and See's [5] model stating that people initially trust automation using faith-based assumptions that it is reliable and eventually progress to more situational assessments of its dependability.\nIn HAT contexts, trust dynamics increasingly resemble interpersonal relationships [19], functioning as a continuous activity with varying manifestations across different timescales [7]. While demographic influences on dispositional trust remain relatively stable [14], perceptual and behavioral trust fluctuate based on observed AI reliability [20, 21]. Successful HDTAT models must capture these nuanced trust dynamics, reflecting the sophisticated measures developed through decades of HAT research."}, {"title": "2.2 Methods for Measuring HAT Trust", "content": "Kohn et al. [22] identified three main techniques for measuring trust in automated agents, corresponding to how they are obtained from human trustors: 1) self-reported trust, 2) behavioral or observational trust, and 3) physiological trust. Note our HDTAT experiments investigate self-reported and behavioral trust measurement techniques."}, {"title": "2.2.1 Self-reported Trust", "content": "Self-reported trust measures are gathered through questionnaires administered at some point during a HAT experiment and are the most widely used trust measure in the literature [22]. Trust questionnaires are often developed as psychometric tests tested through experimental setups (e.g., [23]) or construct-based reconstructions of existing surveys (e.g., Chancey et al.'s [8] adaptation of Madsen and Gregor's Human-Computer Trust scale [24]). As such, self-reported techniques typically measure dispositional trust when administered at the beginning of an experiment, and perceptual trust when administered after a period of interaction with an AI counterpart [25, 15].\nWhile widely used, self-report measures face some validity challenges, including potential biases in how people assess and report their own trust levels. Individuals may struggle to accurately introspect about their trust, or social desirability biases could influence their responses. Some validity issues also arise from the rigorous demands of psychometric test development and the highly contextual nature of HAT experimentation. First, self-reported questionnaires are often administered as a repeated measure of perceptual trust throughout an experiment; however, the frequency and length of repeated surveys risk internal validity problems linked to survey fatigue and response anchoring effects [26, ch. 2]. However, this particular issue may be less of a problem in HDTATs as HDTs may be more consistent in answering repeatedly administered measures.\nA more pressing validity issue is that many HAT experiments use extensively modified versions of validated trust questionnaires, resulting in context-specific offshoot scales that do not measure trust in the way that their parent scales define it as a construct [27]. Many of these modified scales assume that survey respondents have similar internal definitions of trust and other trust-related terminology, which is problematic; an additional problem is that it is uncertain how to approximate individual differences in understanding survey questionnaires within HDTs. This could be due to the inherent difficulty LLMs have with quantitative reasoning [28] or an issue of some of these sociological survey scales being underrepresented in the training data. Researchers have found success when asking HDTs to rate their experiences in terms of importance on a scale from 1 to 10 as a means of enhancing memory synthesis through RAG [29] which could be emulated as a means of generating reasonable survey response data while avoiding issues of contextual invariance caused by more obscure response scales. This will cause a discontinuity between real survey results that rely on a Likert scale or a 1 to 5 scale for example, requiring HDT responses to be rescaled. But this is imperfect given that human survey responses are also known to be affected by the way in which survey scales are presented [30, 31]."}, {"title": "2.2.2 Behavioral Trust", "content": "Behavioral trust measures are gathered through observable human behaviors within a HAT context. Though also widely used, the unique contexts of individual HAT experiments can result in arbitrary definitions of observation-based techniques for measuring trust [22], requiring careful consideration when used in HDTAT trust studies.\nConsider, for instance, compliance-based measures, which describe humans' adherence to AI recommendations or commands as an indicator of trust. The rate at which people follow an AI agent's suggestions in decision-making scenarios can serve as more objective indicators of trust relative to self-reported data. But depending on how researchers interpret decision-making with automation, compliance can be easily conflated with reliance (i.e., delegation of a decision to an AI counterpart without seeing its recommendation; [13]). People have different tendencies to erroneously comply and rely on automation and, in many cases, these mistakes also have inequal consequences [8]. The distinctions between different forms of behavioral trust can be blurrier in abstract HDTAT experimental settings, posing a trade-off between the range of possible HDT-AI decision-making outcomes within them and the fidelity of modeling real human decision-making biases with automation.\nThe dynamic and observable nature of interactions is another reason why behavioral techniques are widely used measures trust. Rather than asking about trust directly, interaction patterns can be observed and processed into quantifiable measures that serve as proxies for trust. For example, highly stable human communication patterns regarding sensitive task information with an Al counterpart could indicate higher levels of trust [32]. Changes in the frequency and depth of interactions may also serve as behavioral indicators of trust development over time. As such, many researchers suggest tracking human-AI interactions over time to map fluctuations in trust and intervene where necessary [21]. But changes in interactions may not necessarily reflect changes in trust; under increased workload, people may accept machine counterpart's suggestions more frequently despite not necessarily trusting it more [33] (cf., [34]). In HDTATs, this vagueness is complicated by the difficulty to control for the impacts of time on trust dynamics; without human input, many current HDT models are restricted to ordinal approximations of temporal interaction sequences that shape HMT trust.\nAs previously noted, behavioral trust measures may offer conflicting narratives with self-reported trust measures about how people are trusting an automated counterpart (Hancock et al., 2011). It is important to consider whether such conflicts in HDTATs arise from validity issues, considering how trust measures are gathered within HDT settings."}, {"title": "2.2.3 Emerging Techniques for Measuring Trust", "content": "In addition to these established measures, we recommend exploring several other potential types of HAT trust measures. Physiological measures could provide insight into subconscious trust reactions by tracking metrics like heart rate variability, skin conductance, or eye movements during human-AI interactions. These may capture subtle trust signals that people are not consciously aware of. Multi-modal trust measures that combine data across self-report, behavioral, physiological, and other channels may provide a more holistic view of trust as a complex, multi-faceted construct. Integrating diverse measure types could help overcome limitations of any single approach. Longitudinal trust measures that track trust development over extended periods of human-AI collaboration are also needed. These could reveal how trust forms, stabilizes, or degrades across the life cycle of human-AI relationships.\nOne promising emergent trust measure for HDTATs involves the use of linguistic analysis of human-AI communication, particularly word choice, sentiment, and other language patterns. Notably, aside from a few examples (e.g., [35, 36, 37]), research is scarce on the semantic and emotional facets of language that are key to conveying trust [38]. Whereas extant HAT trust research guides how HDT trust should be formed and measured, additional research is needed to unpack the language of trust into the constituent social, emotional, and cognitive lexicon used to convey trust. How humans describe and refer to AI agents in natural conversation may indicate their trust orientations."}, {"title": "2.3 Empirical Investigations into HAT Trust using Exploratory Causal Analysis", "content": "To guide our causal analysis, we rely on extant theories that elucidate how social, emotional, and cognitive processes (i.e., socio-emotional-cognitive constructs) unfold in HATs (e.g., [39, 40]). The interplay between socio-emotional-cognitive constructs and how they manifest in team member interactions can be well understood through the affective infusion model [40] and emotional contagion theory [39]. As team members interact with one another, the affective infusion model explains that human team members will experience various emotions as the actions of other team members may trigger emotional reactions and affectively charge one's thoughts regarding the team and its current state/actions [40]. Other team members cue in to these affective tones, and thus one team member's emotions may subsequently manifest in other team members through the emotional contagion process [39]. This process creates a team's affective tone, which has been shown to influence constructs that vital indicators of healthy team functioning such as trust [41] and performance [42].\nTaken in concert, these theories underscore the importance of identifying key socio-emotional-cognitive constructs encoded in team communications. To this end, we conducted exploratory causal analyses of team communication data using the \"Saturn+\" dataset from DARPA's Artificial Social Intelligence for Successful Teams (ASIST) program [43] to identify the impact of three key construct groups on trust: empathy, socio-cognitive, and emotional constructs (see Table 1). We share the results of our causal structure learning analytics applied to analyze these data, and how these indicators predict each of the four indicators of trust used in the ASIST study (e.g., trust that the AI improved performance, trust in the AI's recommendation, trust in the AI's explanation of its recommendation, and trust that the AI improved team coordination).\nOur goal is to identify key the strength and directionality between these latent socio-emotional-cognitive constructs and key indicators of team functioning (to inform higher fidelity HDT development and improve HAT trust over time. Casual structure learning involves identifying and modeling the relationships between variables in a way that captures the underlying causal mechanisms [44]. In this context, the interpretation of weights on causal relationships is crucial, as they often represent the strength and direction of influence between variables. Positive weights indicate a direct positive relationship, where an increase in one variable leads to an increase in another, while negative weights suggest an inverse relationship. However, interpreting these weights requires careful consideration of confounding factors and the potential for bias [45]. Properly understanding these weights not only enhances the reliability of causal inferences but also aids in making informed predictions and decisions based on the modeled relationships."}, {"title": "2.3.1 Causal Analysis Results: How Empathy Constructs Effect HAT Trust?", "content": "Empathy plays a critical role in fostering effective teamwork within collaborative environments and encompasses the intent to understand and the emotional resonance with team members. The ability to recognize and respond to the feelings and perspectives of others cultivates a supportive atmosphere, enhancing communication and trust among team members [5]. This emotional intelligence allows individuals to navigate conflicts with greater sensitivity, leading to more constructive resolutions [46]. Moreover, when team members demonstrate genuine empathy, they contribute to a collective sense of belonging and motivation, which can enhance group cohesion and performance [47]. Within our analyses, we used the presence of specific empathetic strategies (e.g., agreeing, suggesting, and neutral responding) as indicators of empathy. Figure 1 shows how empathy indicators predicted various indicators of trust in the AI.\nThe causal analysis of empathy revealed the relationships between various empathetic approaches and the outcomes related to AI trust and team coordination. Specifically, strategies focused on empathy intent such as agreeing, neutral responses, and suggesting-demonstrated varying levels of effectiveness in fostering an environment conducive to collaboration. In particular, the findings indicate that empathetic communication can enhance trust in AI recommendations when deployed strategically, suggesting that the manner in which empathy is expressed can directly influence team dynamics and AI reliance.\nFurthermore, the results suggest that while some empathetic responses pro-mote positive outcomes such as team coordination, others may inadvertently lead to negative perceptions about trust and performance. For example, suggesting that empathetic responses appeared to cause declines in trust and team coordination. This paradox highlights the complexity of empathetic interactions, emphasizing that not all empathetic expressions yield beneficial results. Thus, the context and delivery of empathetic communication must be carefully considered to avoid potential pitfalls that could undermine team effectiveness.\nFinally, the varying effects of emotional empathy, such as anticipatory and hopeful responses, underscore the importance of emotional intelligence in team settings. Emotional expressions that resonate with team members can increase trust in AI recommendations, thus improving overall performance and coordination. This analysis reinforces the notion that fostering an emotionally aware team culture, where empathetic communication is prioritized, can lead to improved AI trust and more effective teamwork. Ultimately, the nuanced interaction between empathy treatments and trust outcomes requires further exploration to refine communication strategies within collaborative environments."}, {"title": "2.3.2 Causal Analysis Results: How Socio-cognitive Constructs Effect HAT Trust", "content": "The relationship between socio-cognitive elements such as subjectivity, connotation, and moral foundations, and teamwork in collaborative environments is crucial to promote effective communication and cohesion among team members. Subjectivity influences how individuals interpret shared information and experiences, shaping their perspectives and responses within the team context [48, 49]. Connotation further complicates this dynamic, as the emotional and cultural associations tied to language can lead to misunderstandings or reinforce biases that affect collaboration. Additionally, moral foundations underpin team values and ethical considerations, guiding decision-making and conflict resolution [50]. In environments where these socio-cognitive factors are acknowledged and managed, teams are better equipped to leverage diverse viewpoints, enhance mutual respect, and achieve common goals, ultimately driving organizational success. Figure 2 depicts how indicators of connotation, moral foundation, and subjectivity predict various indicators of trust.\nThe causal analysis of socio-cognitive treatments reveals that certain interventions, particularly those centered around connotation and moral foundations, can influence key outcomes related to team performance and AI trust. The findings indicate that fostering a positive connotation within team interactions is associated with enhancements in performance improvement and team coordination. This suggests that the way team members frame their communication can create a more conducive environment for collaboration, leading to better overall outcomes.\nConversely, the exploration of moral foundations highlights a more nuanced impact on team dynamics. While treatments focused on the harm virtue exhibit positive effects on team coordination and trust in recommendations, those centered on general morality may lead to adverse outcomes. This indicates that not all moral framing is equally effective; in some cases, it may even hinder trust and communication within the team. This complexity underscores the importance of selecting appropriate moral cues when designing interventions aimed at improving team interactions.\nFinally, the relationship between these socio-cognitive treatments and AI trust outcomes is particularly noteworthy. The analysis reveals that effective communication strategies can bolster trust in AI recommendations, thereby enhancing overall team performance. However, the detrimental effects observed with certain moral framing approaches suggest that careful consideration is necessary when integrating ethical dimensions into team communication. This insight emphasizes the critical role that both connotation and moral foundations play in shaping trust dynamics within teams utilizing AI, highlighting the need for tailored strategies that align with the specific goals of team interactions."}, {"title": "2.3.3 Causal Analysis Results: How Emotion Constructs Effect HAT Trust", "content": "In teaming environments, the interplay between emotion-encompassing toxicity, sentiment, and specific emotional responses-plays a critical role in shaping team dynamics and overall effectiveness. Positive emotions, such as trust and enthusiasm, foster collaboration and innovation, encouraging open communication and shared problem-solving [51]. Conversely, negative emotions, particularly toxicity manifested through conflict or resentment, can undermine cohesion and hinder performance [52]. Understanding and managing these emotional undercurrents is essential for leaders and team members alike, as they directly influence decision-making, motivation, and interpersonal relationships [53]. Acknowledging the emotional landscape of a team not only enhances individual well-being but also cultivates a more resilient and productive collaborative culture.\nWithin our emotion-construct analyses, we examine sentiment, toxicity, and discrete emotions. Figure 3 depicts how indicators of emotion constructs predicted various indicators of trust in the AI.\nThe causal analysis reveals distinct patterns in the impact of emotion treatments on AI trust outcomes. Notably, interventions focused on promoting joy within the team setting are associated with improvements in performance and coordination. This suggests that fostering positive emotional states can enhance collaborative dynamics, leading to increased overall effectiveness. Teams that experience higher levels of joy are likely to engage more constructively, reinforcing the value of emotional well-being in achieving collective goals.\nConversely, the emotional treatment centered on sadness appears to have detrimental effects on performance improvement and team coordination. The findings indicate that negative emotional states can undermine communication, leading to decreased trust in AI recommendations. This highlights the importance of emotional climate within teams, as negative emotions can create barriers to effective collaboration and impede trust-building processes. The adverse impact of sadness underscores the need for proactive management of team emotions to maintain high levels of engagement and performance.\nLastly, the analysis identifies a nuanced relationship between emotional states and AI trust. While joy enhances trust in recommendations, sadness erodes it. This suggests that the emotional context in which teams operate plays a critical role in how AI outputs are perceived and accepted. Ensuring a positive emotional environment may not only facilitate better communication and coordination but also bolster AI trust, ultimately leading to more effective teamwork and decision-making. The findings emphasize the need for leaders to cultivate a supportive emotional atmosphere to maximize both human collaboration and AI reliance."}, {"title": "2.3.4 Causal Analysis Key Takeaways", "content": "Empathy plays a pivotal role in enhancing teamwork and fostering trust in AI recommendations within collaborative environments. The analysis reveals that empathetic communication strategies, particularly those emphasizing intent, can influence team dynamics. By promoting an environment where team members feel understood and valued, empathy facilitates open dialogue, which is crucial for effective collaboration. However, the variability in outcomes depending on the type of empathetic response underscores the need for strategic implementation. For instance, while neutral and agreeing responses may bolster trust, suggesting responses can inadvertently create tension. This complexity necessitates a careful calibration of empathetic expressions to align with team goals, thereby ensuring that AI trust remains robust, and that team cohesion is strengthened.\nThe socio-cognitive dimension emphasizes the importance of communication framing in influencing team performance and AI trust. By focusing on positive connotations and appropriate moral foundations, teams can create an environment conducive to collaboration and trust. The analysis highlights that positive framing not only enhances team coordination but also reinforces the reliability of AI recommendations. Conversely, poorly chosen moral cues can lead to misunderstandings and diminish trust, illustrating the critical need for thoughtful communication strategies. As teams increasingly rely on AI for decision-making, understanding the socio-cognitive underpinnings of communication can enhance interactions, improve trust dynamics, and ultimately elevate team performance.\nEmotional dynamics within teams are crucial in shaping AI trust and overall collaboration. The findings indicate that fostering positive emotional states, such as joy, enhances both team performance and trust in AI recommendations. This suggests that an emotionally supportive environment can promote constructive interactions and bolster team effectiveness. In contrast, negative emotions, particularly sadness, can erode trust and hinder communication, leading to detrimental effects on teamwork. These insights highlight the necessity for leaders to actively cultivate a positive emotional climate, as emotional well-being not only facilitates better collaboration but also strengthens reliance on AI outputs. By prioritizing emotional health within teams, organizations can harness the full potential of both human collaboration and AI integration, lead-ing to improved decision-making and performance outcomes."}, {"title": "2.4 Preliminary Simulations of HDT Trust", "content": "The causal relationships established by the previous finding allows for the use of these socio-cognitive-emotional measures as a proxy to judge a large language model's capacity to believably simulate AI trust in future human digital twin simulations. Here we present a preliminary HDT experiments with the goal of investigating the capacities across large language models (LLMs) for generating simulated team communications from an assortment of closed and open-source models.\nGiven the scope of this investigation, only team communications were modeled. Without progress realistically tracked, simulation runs were arbitrarily terminated after a preset number of generations (100 turns). Communication dynamics such as conversational entropy and dominance were simulated by asking the model which HDT would like to speak next, and sequence length termination followed default settings for each LLM.\nA tiered prompting approach was used to test how well each LLM could replicate the kind of language used by real players from ASIST experiments. The base prompt included an overview of the mission itself, and a persona to describe the nature (i.e. capabilities and responsibilities) of each HDT's role within the mission. The next condition was to also include example chat logs from a real mission where the players demonstrated above average success in completing objectives. Lastly, in the third condition, the outcomes of the mission were also included, which indicated that the provided logs were an exemplar and ought to be emulated for success. Note that explicit prescriptions of personality traits were not included in any experimental conditions.\nA total of 5 models of different sizes and level of public availability were tested (gpt-4o-mini, gemma2_9b, llama3.1_8b, mistral_7b, phi3_14b) across the three conditions, 20 times each, for 100 turns per simulation. All provided logs were of the same exemplar mission, from which the socio-cognitive-emotional baseline outcomes were extracted, as seen in the following plots.\nThe ability of each model to simulate realistic conversation dynamics was analyzed in terms of dominance, turn order entropy, and sequence length variance as shown in Figure 4. We observed that the open-source model Llama 3.1 8b is most prone to misunderstanding the prompt and creating outliers. In one such example, HDTs would attempt to recreate the entire simulation themselves within a single reply. All models demonstrated a deviation from baseline human sentiment that skewed towards excessive positivity as shown in Figure 5. OpenAI's GPT-4o-mini did the best at overcoming this bias when provided additional context on how the conversation actually unfolded. The use of subjective language across all models except GPT-4o-mini was much higher than the baseline, while the use of language indicating appeals against authority was relatively rare in all models as shown in Figure 6."}, {"title": "2.4.1 HDT Simulation Key Takeaways and Limitations", "content": "Our preliminary simulations revealed several important insights and limitations regarding the use of HDTs for modeling trust in human-AI teams. A fundamental challenge emerged in the form of baseline behavioral deviations from realistic human interactions. Despite using sophisticated language models and varied prompting approaches, the simulated interactions consistently demonstrated patterns that diverged from authentic human team communications. These deviations suggest that current HDT implementations, while promising, require more advanced prompting strategies and architectural considerations to achieve higher fidelity with human behavior patterns.\nThe simulation results highlighted a particularly interesting challenge regarding the representation of diverse personality types and interaction styles. Our findings suggest that current HDT implementations tend toward normalized, socially acceptable behaviors, making it difficult to model the full spectrum of human personality characteristics that might affect team dynamics. This limitation becomes especially relevant when considering the need to understand how different personality types including more challenging or disruptive personalities influence trust development and team performance in HAT contexts.\nA critical direction for future research lies in exploring how HDTs can more accurately model diverse personality types, including those that might be considered toxic or extreme. This avenue of investigation is particularly valuable because studying such personalities in real-world HAT scenarios presents significant logistical and ethical challenges. However, this research direction faces its own set of obstacles. Commercial LLM providers have implemented robust safety measures in response to public scrutiny and concerns about model misuse, making it challenging to simulate more extreme personality types, even in controlled research contexts.\nThe tension between safety constraints and research needs is further complicated by the sophisticated methods that have been developed to circumvent these safety features, as documented by [54]. While such bypass techniques exist, their use raises important ethical considerations about responsible AI research. This challenge points to a broader question about how to balance the need for comprehensive HAT research with responsible AI development practices."}, {"title": "3 Research Question 2: What characteristics of trust must be operationalized in HDT trust models?", "content": "To inform the most important components of HAT trust for HDTs to model, we begin by reviewing three major considerations of HAT trust that must be incorporated in concert to holistically model HAT trust. Key to understanding trust is its longitudinal nature, which is not only initially set, but also calibrated over time [7, 21]. That is, an initial trust intercept is set, from which trust increases and decreases in response to trust violation and repair. To this end, modeling HAT trust requires approximating 1) how initial trust values are set, 2) the rate of changes in trust, and subsequently 3) operationalizing the time-dependent aspects of trust accordingly."}, {"title": "3.1 Setting HDT Initial Trust in AI Teammates", "content": "First, regarding initial trust, several dispositional characteristics have been noted to be of key influence in setting a human's initial trust of their agents [55]. Individual differences in trust tendencies refer to the innate or baseline levels of trust that individuals exhibit toward others, including AI-agent teammates. These tendencies shape how trust is initially formed and can impact the dynamics of human-agent interactions within a team setting. Of note, propensity to trust is a widely recognized component of a person's general willingness to rely on others, including non-human entities. Jessup et al. [14] emphasize that individuals with a high propensity to trust are more likely to extend trust to AI agents, even in early stages of interaction. This tendency is often influenced by past experiences, cultural backgrounds, and personality traits [55]. In designing HDTs, it is crucial to incorporate these individual differences in trust tendencies to achieve a more accurate and personalized representation of initial trust. HDTs should be capable of simulating how a particular human might respond to various AI agent behaviors based on their innate propensity to trust. This could involve calibrating the HDT's initial trust levels to reflect the human's baseline trust tendencies and adjusting these levels as the simulated interactions progress. Such an approach enables the HDT to model how trust might evolve uniquely for different individuals, thereby providing more nuanced insights into human-agent teaming scenarios."}, {"title": "3.2 Changes in Trust Over Time", "content": "Second, regarding longitudinal changes in trust, both characteristics of the human and characteristics of the event influence the magnitude of trust increases and decreases. Emergent trust patterns refer to how trust develops, changes, and adapts over time within human-agent teams. Trust in this context is not static; it evolves as the human and AI agent interact, especially in response to specific events such as trust violations and subsequent repair efforts. Trust violation and repair is a critical aspect of emergent trust patterns, where trust is compromised due to a perceived failure or breach by the AI agent. Wildman et al. [56] describe trust violation as a dynamic process influenced by factors such as the severity of the breach, the frequency of violations, and the perceived intentions of the AI agent. The process of trust repair, in turn, involves behaviors such as offering explanations, apologies, or demonstrating improved performance, which can help restore trust over time [57].\nTo accurately model emergent trust patterns, HDTs need to incorporate mechanisms for simulating trust violation and repair processes which negatively effects AI and HAI safety [58]. This includes the ability to model a human's attitudinal and behavioral trust reactions to an AI agent's mistakes, and how that trust could potentially be rebuilt through subsequent interactions. An HDT that effectively captures these dynamics would be better equipped to predict how trust might evolve in real-world human-agent teams, providing valuable insights into designing AI systems capable of maintaining and restoring trust over time."}, {"title": "3.3 Computational Modeling of HDT Trust Over Time", "content": "Third, modeling longitudinal trust values must be measurable. That is, approximations of human trust via the use of a HDT requires that the values and form of these trust trajectories are correspondingly output and analyzed. The measurement of trust characteristics is essential for understanding how trust develops, fluctuates, and stabilizes within human-agent teaming environments. Various quantitative and qualitative approaches have been employed to measure trust levels and their evolution over time.\nGrowth modeling is one such method that offers insights into how trust changes throughout the lifespan of a human-agent collaboration. Abramov et al. [59] highlight the importance of using longitudinal growth modeling techniques to capture the trajectory of trust development, accounting for both linear and non-linear changes. By modeling trust as a dynamic construct, researchers can identify critical periods when trust is most susceptible to influence and intervention. When operationalizing trust within HDTs, it is vital to integrate growth modeling techniques that allow for tracking and predicting trust evolution across different stages of interaction. An HDT should be able to simulate not just a static level of trust but also how trust characteristics might shift in response to various situational factors, such as the AI agent's performance consistency, transparency, and adaptability. This ability to model trust growth and decay over time ensures that the HDT provides a realistic and comprehensive representation of human-agent trust dynamics."}, {"title": "4 Research Question 3: How do experimental manipulations from HAT studies translate to HDT studies?", "content": "Although modeling trust dynamics in HDTs offers a viable alternative to human-subject studies for approximating HAT trust, not all experimental manipulations effectively translate to HDTs. Manipulations from the HAT literature may not always bear direct one-to-one approximation for reasons related to an HDT's inability to simulate complex human phenomenon (e.g., emotions, physiology, sensation, and perception). In this section, we first expand upon these reasons that HAT trust manipulations may not maintain their validity in HDT studies, then share our initial foray into studying the HAT trust manipulations that do maintain their validity and provide meaningful insight into the validity of HDT trust."}, {"title": "4.1 Replicability of HAT Trust Manipulations", "content": "In order to even measure and capture a HDT's reactions to AI agents, the HDT must first possess the ability to appropriately perceive and react to any manipulations in question. While HDTs aim to replicate human behaviors and cognitive patterns, the nuances of human emotion, cognition, and social interaction may not always be perfectly mirrored in a digital twin. In addition to the measurement concerns discussed in the sections above, this difficulty in accurately capturing these nuances also poses issues for designing experimental paradigms involving HDTs.\nAt the core of this discussion lies a key question"}]}