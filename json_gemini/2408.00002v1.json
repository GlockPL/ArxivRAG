{"title": "Evaluating Transfer Learning in Deep Learning Models for Classification on a Custom Wildlife Dataset: Can YOLOv8 Surpass Other Architectures?", "authors": ["Subek Sharma", "Sisir Dhakal", "Mansi Bhavsar"], "abstract": "Biodiversity plays a crucial role in maintaining the balance of the ecosystem. However, poaching and unintentional human activities contribute to the decline in the population of many species. Hence, active monitoring is required to preserve these endangered species. Current human-led monitoring techniques are prone to errors and are labor-intensive. Therefore, we study the application of deep learning methods like Convolutional Neural Networks (CNNs) and transfer learning, which can aid in automating the process of monitoring endangered species. For this, we create our custom dataset utilizing trustworthy online databases like iNaturalist and ZooChat. To choose the best model for our use case, we compare the performance of different architectures like DenseNet, ResNet, VGGNet, and YOLOv8 on the custom wildlife dataset. Transfer learning reduces training time by freezing the pre-trained weights and replacing only the output layer with custom, fully connected layers designed for our dataset. Our results indicate that YOLOv8 performs better, achieving a training accuracy of 97.39% and an F1 score of 96.50%, surpassing other models. Our findings suggest that integrating YOLOv8 into conservation efforts could revolutionize wildlife monitoring with its high accuracy and efficiency, potentially transforming how endangered species are monitored and protected worldwide.", "sections": [{"title": "1 Introduction", "content": "Each living creature in our world plays a vital role in maintaining the balance of the ecosystem. However, activities like poaching and illegal trade, reduction of prey base, habitat loss and degradation, and human-wildlife conflict have led to a rapid decline in the number of some species, including critically endangered ones [11]. Currently, wildlife monitoring is done using camera traps that capture the images of animals, which are then analyzed and studied by humans. This is time-consuming, tedious, and prone to errors.\n\nThe introduction of various deep learning techniques and their use in computer vision shows optimistic results. Convolutional Neural Networks (CNNs), introduced by LeCun et al. in 1998 [14], have been a significant milestone in image classification tasks [22, 26, 12]. In these networks, images are fed through convolutional and pooling layers for feature extraction, followed by fully connected layers. Transfer learning, highlighted by Pan and Yang [18], is fine-tuning pre-trained models on large datasets for specified tasks that significantly reduce training time with increased performance.\n\nDensely connected convolutional networks, popularized by Huang et al. in 2017 [8], and residual networks, put forward by He et al. in 2016 [7], are deep and complex networks that facilitate effective feature extraction. However, VGGNet, proposed by Simonyan and Zisserman in 2014 [25], is somewhat computationally expensive, although its results are excellent in some cases.YOLOv8 is an evolution in Redmon et al.'s object detection and has shown remarkable results in various computer vision tasks [20].\n\nEl Abbadi et al. achieved a classification accuracy of 97.5% using a deep convolutional neural network model for the automated classification of vertebrate animals, thereby demonstrating the effectiveness of deep learning in animal recognition tasks [6]. Similarly, Villa et al. demonstrated in their study that very deep convolutional neural networks achieved 88.9% accuracy in a balanced dataset and 35.4% in an unbalanced one for automatic species identification in camera-trap images, marking a notable advancement in non-intrusive wildlife monitoring [27]. In 2020 Ibraheam et al. reported in their paper that their deep learning-based system\n\nachieved 99.8% accuracy in distinguishing between animals and humans, and 97.6% in identifying specific animal species, significantly improving safety in wildlife-human and wildlife-vehicle encounters [9].\n\nBrust et al. have illustrated that ResNet50 outperformed VGG16 and Inception v3 on a wildlife image dataset, with the highest accuracy of 90.3%. It provided insights into the strengths and weaknesses of different CNN architectures for wildlife classification [3]. Similarly, Beery et al. 2018 validated the same on a challenging dataset of wildlife images with occlusions, varying lighting conditions, and motion blur. They found the Faster R-CNN model achieved the highest accuracy of 88.7%, indicating the importance of model robustness in real-world applications [2]. Similarly, Yilmaz et al. in 2021 demonstrated that the YOLOv4 algorithm achieved a high classification accuracy of 92.85% for cattle breed detection [29], emphasizing its effectiveness in wildlife classification tasks. M. Kumar et al. found that YOLOv4 was the most effective of the several deep learning-based models, including SSD and YOLOv5, achieving an accuracy of 95.43% in their bird classification task [13]. Hung Nguyen et al. achieved 96.6% accuracy in detecting animal images and 90.4 % accuracy in species identification using deep learning, highlighting its potential for automatically monitoring wildlife [17].\n\nThis paper answers some of the burning questions regarding selecting deep learning models for practical wildlife conservation tasks, such as which models provide the best accuracy and efficiency, how YOLOv8 com-pares to DenseNet, ResNet, and VGGNet, and what challenges and limitations exist in applying these models to wildlife conservation. Our results show that YOLOv8 is best suited for automated wildlife monitoring with much better accuracy and efficiency than models such as DenseNet, ResNet, and VGGNet. This work will supply essential guidance to researchers and practitioners on the choice of appropriate models for endangered species conservation. Addressing these research questions is crucial as the current methodologies for wildlife monitoring are labor-intensive and error-prone. This work aims to fill this gap by systematically evaluating different deep learning models and providing essential guidance to researchers and practitioners on the choice of appropriate models for endangered species conservation.\n\nIn this paper, we begin by reviewing related works and current methodologies in wildlife conservation. In Section 2, we describe our dataset and the preprocessing steps involved and a detailed overview of the methodologies employed in our study. Following this, Section 3 elaborates on the performance and evaluation metrics used for our machine learning models. In Section 4, we present and discuss the experimental results. Finally, we conclude in Section 5 with a summary of our findings and suggestions for future work."}, {"title": "2 Methodology", "content": "2.1 Dataset Description\n\n2.1.1\n\nSpecies Coverage\n\nOur dataset includes 23 each carefully selected based on its conservation status and the need for monitoring. These species cover many endangered animals, including mammals, reptiles, and amphibians"}, {"title": "2.1.2 Data Collection", "content": "Our study began by collecting the data from different sources on the internet. Each class is represented by 50 filtered images, resulting in 1150 images in our dataset. We maintained a balanced dataset to reduce the bias towards any particular species and to perform the balanced model training. The internet houses a huge amount of data, but finding and gathering useful ones is still challenging. To collect images of various animal species, we utilized online repositories like iNaturalist and ZooChat [10, 30]. The good thing about using such sites is their authenticity and fair use policy. For the same reason, we did not use the images shown in regular Google searches or try to automate the process."}, {"title": "2.2 Data Preprocessing", "content": "We divided the preprocessing of the image data into the following steps.\n\n2.2.1 Aspect Ratio Standardization\n\nWe preprocessed all the images utilized for our study to have an aspect ratio of 1:1 and a resolution of 400 x 400. We added padding whenever required, ensuring we did not lose any information from the images during resizing.\n\n2.2.2 Data Normalization\n\nWe normalized every image to increase accuracy and speed up the model's convergence. It also reduced the variance in the training data.\n\n2.2.3 Splitting\n\nWe split the dataset into train (80%) and val (20%) sets. We used the split-folders Python package to split the dataset while maintaining the original distribution [4]. Babu et al.'s study examines the impact of image data splitting on the performance of machine learning models [1]."}, {"title": "2.2.4 Data Augmentation", "content": "For better generalization, we increased the diversity of data by applying different augmentation techniques, as suggested by Shorten and Khoshgoftaar in 2019 [24]. While their survey offered valuable insights into the importance of data augmentation for enhancing the model's performance, we customized the specific methods to our dataset and requirements through numerous experiments"}, {"title": "2.3 Convolutional Neural Network", "content": "Convolutional Neural Networks are a special kind of neural network designed to work with grid data like images. They learn by extracting the features from input data via convolution and pooling operations followed by fully connected layers [14]. CNNs have played a pivotal role in the advancement of computer vision and related tasks. They are especially effective in performing tasks such as image recognition, object detection, and classification [5, 16, 23]."}, {"title": "2.4 Transfer Learning", "content": "Transfer learning is an approach to deep learning that enables researchers and developers to use the previously trained model in a huge dataset and implement it in downstream tasks [22]. It is especially useful when we have limited data to train the model. Also, it significantly reduces the training time because the feature extraction part remains unchanged. However, it may fail when there is a significant mismatch between the target domain task and the source task.\n\nThese are some of the models we used to compare transfer learning with our dataset. Figure 3 shows the corresponding architecture of these models.\n\nDenseNet: DenseNet was introduced by Gao Huang and colleagues in 2017 [26]. It connects each layer with all other layers densely, meaning each layer receives input from the preceding layers. By efficiently cutting the parameters' requirements and boosting the network's ability by reusing the features extracted, it allows for a deeper network. However, the dense connectivity may lead to increased computational cost and memory consumption.\n\nResNet: ResNet was introduced by Kaiming He et al. in 2016 [12]. It utilizes residual blocks, which are shortcut connections that bypass one or multiple layers. It also allows the network to learn residual functions instead of direct mapping. This architecture supports very deep networks without degradation problems."}, {"title": "YOLOv8", "content": "YOLOv8 is built upon the object detection framework introduced by Joseph Redmon, with contributions from many researchers over successive versions. It enhances speed and accuracy through an advanced backbone architecture, refined loss functions, and anchor-free detections [21]"}, {"title": "2.5 Model Building", "content": "We loaded the pre-trained models(DenseNet, ResNet and VGG, ) with their respective weights, made these weights untrainable(frozen), and replaced the last layer with custom, fully connected layers corresponding to the number of classes in our dataset. We added a GlobalAveragePooling2D layer to reduce the feature maps\u2019 spatial dimensions and prevent overfitting. This layer is followed by a Dense layer with 128 neurons and activated by ReLU to introduce non-linearity and learn more complex features. Finally, we added a Dense layer with 23 units activated by softmax to match the number of classes in our dataset, enabling the model to output the class probabilities. We then trained the models using Adam as the optimizer and cross-entropy as the loss function for 100 epochs with a batch size of 32 images. Meanwhile, we used a validation set to monitor the progress to avoid plateauing and adjusting the learning rate dynamically."}, {"title": "2.6 Hyperparameters", "content": "We experimented with multiple sets of hyperparameters, including learning rates, optimizers, batch sizes, and schedulers, thereby selecting the most favorable settings of hyperparameters that showed optimal performance. \nCategorical Cross Entropy: Categorical Cross Entropy (CCE) is used for multi-class classification tasks. It measures the difference between the true class labels and the predicted class probabilities. The formula sums the negative log-likelihood of the true class's predicted probability across all classes. We have used CCE as our loss function.\nCategorical Cross Entropy = $\\sum_{\\text {neuron}=1}^{\\text {classes}} \\sum_\\text{{neuron}} Y_{\\text {true, neuron}} \\cdot \\ln(y_{\\text {pred, neuron}})$   (1)\nBinary Cross Entropy for One Neuron: Binary Cross Entropy (BCE) is used for binary classification tasks. The binary cross-entropy loss measures the dissimilarity between predicted probability distributions and the ground truth labels. The formula is the negative log-likelihood of the predicted probability if the true label is 1 and 1 minus the predicted probability if the true label is 0. The BCE is utilized in each output layer neuron and used in the YOLOv8 classification task.\nBCE1neuron = [Ytrue. In(ypredicted) + (1 \u2212 Ytrue) \u00b7 ln(1 \u2013 Ypredicted)]  (2)\nAdamW Optimizer: AdamW stands for Adaptive Moment Estimation with Weight Decay. It is an extension of the Adam optimizer, including weight decay, to improve generalization by preventing over-fitting. AdamW adjusts the learning rate based on the gradients' first and second moments and consists of a weight decay term to penalize large weights.\nAdamW: $\u0398_{t+1} = \u0398_{t} \u2212 \u03b1\u00b7\\frac{V_{t}}{\\sqrt{S_{t} + \u20ac}} + emt $   a weight_decay. \u0398t (3)\nWhere:\n\\Theta_t is the parameter at time step t\na is the learning rate\nmt is the biased first-moment estimate\nVt is the biased second raw moment estimate\ne is a small constant to prevent division by zero\nweight_decay is the weight decay term [24]."}, {"title": "3 Performance and Evaluation Metrics", "content": "The performance of our models is not examined by accuracy alone. In addition to accuracy, other metrics like f1-score, precision, recall, and loss were employed for the evaluation. Precision is an indicator of the accuracy of model predictions i.e., the ratio of the true positive predictions to the total number of positive predictions made by the model. The recall is an indicator of the model's ability to identify all the relevant classes i.e., ratio of true positive predictions to the total number of actual positive instances. The F1 score provides the single value for the evaluation of the model and is the harmonic mean of precision and recall. The loss gives insights into how well the model's prediction matches the true outcomes and is the difference between the predicted values and the actual values."}, {"title": "4 Results And Discussion", "content": "4.1 Results\n\nOur study evaluated multiple deep-learning models for identifying endangered animal species from wildlife images. The results show the varying performance across the models. YOLOv8 outperformed the other models, and the DenseNet and Resnet models also did well, as their results were close to those of YOLOv8. In contrast, the VGG and Vanilla CNN models faced significant challenges. \nDenseNet: DenseNet architectures showed strong performance across all metrics. DenseNet 169 achieved a training accuracy of 98.27% and a validation accuracy of 93.91%. Also, it had F1 scores of 95.22% in training and 93.95% in validation. Impressively, DenseNet 201 outperformed by a slightly better training accuracy of 98.80% and had F1-scores of 96.36% in training and 92.22% in validation, though its validation accuracy was somewhat lower at 92.17%.\n\nFigure 6 shows training and validation set loss curves decreasing rapidly, finally reaching a plateau, indicating good learning and model convergence. The relatively smooth curve showed a minimal gap in Figure 7, indicating good generalization and no significant overfitting.\n\nResNet: The ResNets architecture also showed promising results. Resnet 101 V2 reached a training accuracy of 98.74% and a validation accuracy of 92.17%. Its F1 scores were 97.36% in training and 92.09% in validation, showing robust performance. Resnet 152 V2 showed a training accuracy of 98.20% and a validation accuracy of 93.04%, with high F1-scores of 95.79% in training and 93.22% in validation.\n\nThe training and validation data loss plunged drastically initially and finally tended towards low values, similar to DenseNet169, as shown in Figure 8. The very close train and validation loss curves indicated that the model generalized well without overfitting a similar trend in the training and validation accuracy from Figure 9.\n\nVGG: In contrast, the VGG architectures underperformed significantly compared to DenseNet and ResNet models. VGG 16 and VGG 19 showed much lower training accuracies (46.00% and 32.67%, respectively) and validation accuracies (33.91% and 33.04%, respectively). Their F1 scores were also considerably lower, with VGG 16 at 42.89% for training and 28.65% for validation and VGG 19 at 29.82% for training and 31.19% for validation.\n\nThe training loss decreased steadily, while the validation loss followed a different pattern, decreasing even slower and with apparent variance, as shown in Figure 10. There was a visible gap between training and validation loss, showing the possible overfitting phenomenon, i.e., a model fitting much better with the training set than the validation set. Also, Figure 11 shows oscillations in the training and validation accuracies.\n\nYOLOv8: The YOLOV8 model achieved an accuracy of 97.39% in training and 99.13% in validation with a shallow loss of 0.01175, which showed that despite being known for detection purposes, it surprisingly worked well for our classification task. The F1 score was 96.5% in training and 99.12% in validation.\n\nFigure 12 shows that the loss decreased rapidly and plateaued early in training, which shows the model's effectiveness for fast convergence during transfer learning. Unlike the loss, the accuracy oscillated slightly before it finally plateaued near 50 epochs, as indicated in Figure 13."}, {"title": "4.2 Discussions", "content": "Our experimental analysis shows the metrics of various deep-learning models in identifying endangered animals on our custom endangered wildlife dataset, as shown in Figure 14. The dataset was carefully created from reputable online databases, ensuring the species' authenticity and relevance. Our motto was to train a model that could recognize vulnerable species and assist in their proper monitoring. Furthermore, the other side of our study involved finding the best available architecture for this task. We experimented with various such architectures and analyzed their performance under different metrics.\n\nWe found that newer version of all the models showed stronger performance. VGG was the oldest among our models, so it performed poorly. Its accuracy was way below the vanilla CNN. Conversely, DenseNet and ResNet offered significantly better performance, so we can easily use them for related tasks. Also, we noted that these networks' newer and deeper versions did not provide any impactful difference across various metrics.\n\nIn addition, we implemented transfer learning and did not train them from scratch, benchmarking their ease of use in downstream tasks like ours. We had to freeze the feature extraction layers and only trained the last few fully connected layers. Doing this saved the time and computing required without compromising their performance.\n\nAlongside standard CNN models like ResNet, DenseNet, and VGGNet, specially designed for classification tasks, we also experimented with YOLOv8. YOLOs are primarily designed and used as go-to models for detection and segmentation tasks. To our great surprise, the metrics surpassed other standard classification models. Thus, YOLO might work well for classification tasks for custom datasets in similar niches.\n\nWhy does YOLOv8 perform the best?\n\nYOLOv8's superior performance might be due to its advanced architecture, which combines efficient feature extraction with fast processing capabilities by integrating components like CSPNet (Cross Stage Partial Networks) and PANet (Path Aggregation Network) [28, 15]. CSPNet reduces the computational cost while maintaining accuracy. It divides the feature map into two parts and merges them through the cross-stage hierarchy. On the other hand, PANet enhances the information flow between various layers, which is excellent for object detection across multi-scales. But, this proved beneficial for classification tasks, too. Even more importantly, the multiscale capability of YOLOv8 allows handling images with object sizes that can have significant variations and differing resolutions. It also has advanced augmentations, such as mosaic augmentation, which places four training images into one with diverse contexts in one image and hence helps the model generalize better to varying lighting and environmental conditions in training. All these features, together, carry out fast processing and effective feature extraction."}, {"title": "5 Conclusion", "content": "To sum up, we performed experimental analysis on transfer learning of various deep-learning CNN architectures on our custom dataset containing images of endangered mammals from Nepal. Our findings featured the superior performance of YOLOv8 compared to other models like DenseNet, ResNetV2, and VGG. It demonstrated higher accuracy, precision, and recall, making it practical for our and similar classification tasks. Although lagging by a narrow margin, other models like ResNet and DenseNet also performed well and competed neck and neck with YOLOv8. Transfer learning proved beneficial, drastically reducing training time and data required while maintaining high performance, which is crucial for tasks with limited data availability.\n\nIn the future, we will explore the ensemble methods that combine the strengths of multiple CNN architectures potentially enhancing classification accuracy and robustness, especially in diverse and challenging environmental conditions. We will also incorporate real-time monitoring capabilities in the future to provide feedback for conservation and take timely action in preventing the loss of endangered species. Hence, this study shows the reliance and robustness of deep-learning models in monitoring wildlife."}, {"title": "6 Funding", "content": "This research received no external funding."}, {"title": "7 Conflicts of Interest", "content": "The authors declare that there is no conflict of interest regarding the publication of this paper."}, {"title": "8 Acknowledgment", "content": "The authors thank their supervisor, Dr. Mansi Bhavsar, for their invaluable guidance and support. Both authors contributed equally to this work."}]}