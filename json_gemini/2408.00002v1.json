{"title": "Evaluating Transfer Learning in Deep Learning Models for Classification on a Custom Wildlife Dataset: Can YOLOv8 Surpass Other Architectures?", "authors": ["Subek Sharma", "Sisir Dhakal", "Mansi Bhavsar"], "abstract": "Biodiversity plays a crucial role in maintaining the balance of the ecosystem. However, poaching and unintentional human activities contribute to the decline in the population of many species. Hence, active monitoring is required to preserve these endangered species. Current human-led monitoring techniques are prone to errors and are labor-intensive. Therefore, we study the application of deep learning methods like Convolutional Neural Networks (CNNs) and transfer learning, which can aid in automating the process of monitoring endangered species. For this, we create our custom dataset utilizing trustworthy online databases like iNaturalist and ZooChat. To choose the best model for our use case, we compare the performance of different architectures like DenseNet, ResNet, VGGNet, and YOLOv8 on the custom wildlife dataset. Transfer learning reduces training time by freezing the pre-trained weights and replacing only the output layer with custom, fully connected layers designed for our dataset. Our results indicate that YOLOv8 performs better, achieving a training accuracy of 97.39% and an F1 score of 96.50%, surpassing other models. Our findings suggest that integrating YOLOv8 into conservation efforts could revolutionize wildlife monitoring with its high accuracy and efficiency, potentially transforming how endangered species are monitored and protected worldwide.", "sections": [{"title": "1 Introduction", "content": "Each living creature in our world plays a vital role in maintaining the balance of the ecosystem. However, activities like poaching and illegal trade, reduction of prey base, habitat loss and degradation, and human- wildlife conflict have led to a rapid decline in the number of some species, including critically endangered ones [11]. Currently, wildlife monitoring is done using camera traps that capture the images of animals, which are then analyzed and studied by humans. This is time-consuming, tedious, and prone to errors.\n\nThe introduction of various deep learning techniques and their use in computer vision shows optimistic results. Convolutional Neural Networks (CNNs), introduced by LeCun et al. in 1998 [14], have been a significant milestone in image classification tasks [22, 26, 12]. In these networks, images are fed through convolutional and pooling layers for feature extraction, followed by fully connected layers. Transfer learning, highlighted by Pan and Yang [18], is fine-tuning pre-trained models on large datasets for specified tasks that significantly reduce training time with increased performance.\n\nDensely connected convolutional networks, popularized by Huang et al. in 2017 [8], and residual networks, put forward by He et al. in 2016 [7], are deep and complex networks that facilitate effective feature extraction. However, VGGNet, proposed by Simonyan and Zisserman in 2014 [25], is somewhat computationally expensive, although its results are excellent in some cases.YOLOv8 is an evolution in Redmon et al.'s object detection and has shown remarkable results in various computer vision tasks [20].\n\nEl Abbadi et al. achieved a classification accuracy of 97.5% using a deep convolutional neural network model for the automated classification of vertebrate animals, thereby demonstrating the effectiveness of deep learning in animal recognition tasks [6]. Similarly, Villa et al. demonstrated in their study that very deep convolutional neural networks achieved 88.9% accuracy in a balanced dataset and 35.4% in an unbalanced one for automatic species identification in camera-trap images, marking a notable advancement in non-intrusive wildlife monitoring [27]. In 2020 Ibraheam et al. reported in their paper that their deep learning-based system achieved 99.8% accuracy in distinguishing between animals and humans, and 97.6% in identifying specific animal species, significantly improving safety in wildlife-human and wildlife-vehicle encounters [9].\n\nBrust et al. have illustrated that ResNet50 outperformed VGG16 and Inception v3 on a wildlife image dataset, with the highest accuracy of 90.3%. It provided insights into the strengths and weaknesses of different CNN architectures for wildlife classification [3]. Similarly, Beery et al. 2018 validated the same on a challenging dataset of wildlife images with occlusions, varying lighting conditions, and motion blur. They found the Faster R-CNN model achieved the highest accuracy of 88.7%, indicating the importance of model robustness in real- world applications [2]. Similarly, Yilmaz et al. in 2021 demonstrated that the YOLOv4 algorithm achieved a high classification accuracy of 92.85% for cattle breed detection [29], emphasizing its effectiveness in wildlife classification tasks. M. Kumar et al. found that YOLOv4 was the most effective of the several deep learning- based models, including SSD and YOLOv5, achieving an accuracy of 95.43% in their bird classification task [13]. Hung Nguyen et al. achieved 96.6% accuracy in detecting animal images and 90.4 % accuracy in species identification using deep learning, highlighting its potential for automatically monitoring wildlife [17].\n\nThis paper answers some of the burning questions regarding selecting deep learning models for practical wildlife conservation tasks, such as which models provide the best accuracy and efficiency, how YOLOv8 com- pares to DenseNet, ResNet, and VGGNet, and what challenges and limitations exist in applying these models to wildlife conservation. Our results show that YOLOv8 is best suited for automated wildlife monitoring with much better accuracy and efficiency than models such as DenseNet, ResNet, and VGGNet. This work will supply essential guidance to researchers and practitioners on the choice of appropriate models for endangered species conservation. Addressing these research questions is crucial as the current methodologies for wildlife monitoring are labor-intensive and error-prone. This work aims to fill this gap by systematically evaluating different deep learning models and providing essential guidance to researchers and practitioners on the choice of appropriate models for endangered species conservation.\n\nIn this paper, we begin by reviewing related works and current methodologies in wildlife conservation. In Section 2, we describe our dataset and the preprocessing steps involved and a detailed overview of the methodologies employed in our study. Following this, Section 3 elaborates on the performance and evaluation metrics used for our machine learning models. In Section 4, we present and discuss the experimental results. Finally, we conclude in Section 5 with a summary of our findings and suggestions for future work."}, {"title": "2 Methodology", "content": null}, {"title": "2.1 Dataset Description", "content": null}, {"title": "2.1.1 Species Coverage", "content": "Our dataset includes 23 each carefully selected based on its conservation status and the need for monitoring. These species cover many endangered animals, including mammals, reptiles, and amphibians as shown in table 1."}, {"title": "2.1.2 Data Collection", "content": "Our study began by collecting the data from different sources on the internet. Each class is represented by 50 filtered images, resulting in 1150 images in our dataset. We maintained a balanced dataset to reduce the bias towards any particular species and to perform the balanced model training. The internet houses a huge amount of data, but finding and gathering useful ones is still challenging. To collect images of various animal species, we utilized online repositories like iNaturalist and ZooChat [10, 30]. The good thing about using such sites is their authenticity and fair use policy. For the same reason, we did not use the images shown in regular Google searches or try to automate the process."}, {"title": "2.2 Data Preprocessing", "content": "We divided the preprocessing of the image data into the following steps."}, {"title": "2.2.1 Aspect Ratio Standardization", "content": "We preprocessed all the images utilized for our study to have an aspect ratio of 1:1 and a resolution of 400 x 400. We added padding whenever required, ensuring we did not lose any information from the images during resizing."}, {"title": "2.2.2 Data Normalization", "content": "We normalized every image to increase accuracy and speed up the model's convergence. It also reduced the variance in the training data."}, {"title": "2.2.3 Splitting", "content": "We split the dataset into train (80%) and val (20%) sets. We used the split-folders Python package to split the dataset while maintaining the original distribution [4]. Babu et al.'s study examines the impact of image data splitting on the performance of machine learning models [1]."}, {"title": "2.2.4 Data Augmentation", "content": "For better generalization, we increased the diversity of data by applying different augmentation techniques, as suggested by Shorten and Khoshgoftaar in 2019 [24]. While their survey offered valuable insights into the importance of data augmentation for enhancing the model's performance, we customized the specific methods to our dataset and requirements through numerous experiments. Table 2 displays the final set of parameters for data augmentation."}, {"title": "2.3 Convolutional Neural Network", "content": "Convolutional Neural Networks are a special kind of neural network designed to work with grid data like images. They learn by extracting the features from input data via convolution and pooling operations followed by fully connected layers [14]. CNNs have played a pivotal role in the advancement of computer vision and related tasks. They are especially effective in performing tasks such as image recognition, object detection, and classification [5, 16, 23]."}, {"title": "2.4 Transfer Learning", "content": "Transfer learning is an approach to deep learning that enables researchers and developers to use the previously trained model in a huge dataset and implement it in downstream tasks [22]. It is especially useful when we have limited data to train the model. Also, it significantly reduces the training time because the feature extraction part remains unchanged. However, it may fail when there is a significant mismatch between the target domain task and the source task.\n\nThese are some of the models we used to compare transfer learning with our dataset. Figure 3 shows the corresponding architecture of these models.\n\nDenseNet: DenseNet was introduced by Gao Huang and colleagues in 2017 [26]. It connects each layer with all other layers densely, meaning each layer receives input from the preceding layers. By efficiently cutting the parameters' requirements and boosting the network's ability by reusing the features extracted, it allows for a deeper network. However, the dense connectivity may lead to increased computational cost and memory consumption.\n\nResNet: ResNet was introduced by Kaiming He et al. in 2016 [12]. It utilizes residual blocks, which are shortcut connections that bypass one or multiple layers. It also allows the network to learn residual functions instead of direct mapping. This architecture supports very deep networks without degradation problems."}, {"title": "2.5 Model Building", "content": "We loaded the pre-trained models(DenseNet, ResNet and VGG, ) with their respective weights, made these weights untrainable(frozen), and replaced the last layer with custom, fully connected layers corresponding to the number of classes in our dataset. We added a GlobalAveragePooling2D layer to reduce the feature maps\u2019 spatial dimensions and prevent overfitting. This layer is followed by a Dense layer with 128 neurons and activated by ReLU to introduce non-linearity and learn more complex features. Finally, we added a Dense layer with 23 units activated by softmax to match the number of classes in our dataset, enabling the model to output the class probabilities, as shown in Figure 5. We then trained the models using Adam as the optimizer and cross-entropy as the loss function for 100 epochs with a batch size of 32 images. Meanwhile, we used a validation set to monitor the progress to avoid plateauing and adjusting the learning rate dynamically."}, {"title": "2.6 Hyperparameters", "content": "We experimented with multiple sets of hyperparameters, including learning rates, optimizers, batch sizes, and schedulers, thereby selecting the most favorable settings of hyperparameters that showed optimal performance. Table 3 lists the final set of hyperparameters.\n\nCategorical Cross Entropy: Categorical Cross Entropy (CCE) is used for multi-class classification tasks. It measures the difference between the true class labels and the predicted class probabilities. The formula sums the negative log-likelihood of the true class's predicted probability across all classes. We have used CCE as our loss function.\n\nCategorical Cross Entropy = $ \\sum_{neuron=1}^{classes} \\sum Ytrue,_{neuron} In(ypred,_{neuron})$\n\nBinary Cross Entropy for One Neuron: Binary Cross Entropy (BCE) is used for binary classification tasks. The binary cross-entropy loss measures the dissimilarity between predicted probability distributions and the ground truth labels. The formula is the negative log-likelihood of the predicted probability if the true label is 1 and 1 minus the predicted probability if the true label is 0. The BCE is utilized in each output layer neuron and used in the YOLOv8 classification task.\n\nBCE1neuron = $[Ytrue. In(ypredicted) + (1 \u2212 Ytrue) \u00b7 ln(1 \u2013 Ypredicted)]$\n\nAdamW Optimizer: AdamW stands for Adaptive Moment Estimation with Weight Decay. It is an extension of the Adam optimizer, including weight decay, to improve generalization by preventing over- fitting. AdamW adjusts the learning rate based on the gradients' first and second moments and consists of a weight decay term to penalize large weights.\n\nAdamW: $Ot+1 = \u0398t \u2212 \u03b1\u00b7 \\frac{Vt}{\\sqrt{St} + \u20ac} + emt -a weight\\_decay . Ot$\n\nWhere:\n\n\u2022 Ot is the parameter at time step t\n\n\u2022 a is the learning rate\n\n\u2022 mt is the biased first-moment estimate\n\n\u2022 Vt is the biased second raw moment estimate\n\n\u2022 e is a small constant to prevent division by zero\n\n\u2022 weight_decay is the weight decay term [24]."}, {"title": "3 Performance and Evaluation Metrics", "content": "The performance of our models is not examined by accuracy alone. In addition to accuracy, other metrics like f1-score, precision, recall, and loss were employed for the evaluation. Precision is an indicator of the accuracy of model predictions i.e., the ratio of the true positive predictions to the total number of positive predictions made by the model. The recall is an indicator of the model's ability to identify all the relevant classes i.e., ratio of true positive predictions to the total number of actual positive instances. The F1 score provides the single value for the evaluation of the model and is the harmonic mean of precision and recall. The loss gives insights into how well the model's prediction matches the true outcomes and is the difference between the predicted values and the actual values."}, {"title": "4 Results And Discussion", "content": null}, {"title": "4.1 Results", "content": "Our study evaluated multiple deep-learning models for identifying endangered animal species from wildlife images. The results show the varying performance across the models. YOLOv8 outperformed the other models, and the DenseNet and Resnet models also did well, as their results were close to those of YOLOv8. In contrast, the VGG and Vanilla CNN models faced significant challenges. Table 4 shows the performances of different models.\n\nFigure 6 shows training and validation set loss curves decreasing rapidly, finally reaching a plateau, indicating good learning and model convergence. The relatively smooth curve showed a minimal gap in Figure 7, indicating good generalization and no significant overfitting."}, {"title": "4.2 Discussions", "content": "Our experimental analysis shows the metrics of various deep-learning models in identifying endangered animals on our custom endangered wildlife dataset, as shown in Figure 14. The dataset was carefully created from reputable online databases, ensuring the species' authenticity and relevance. Our motto was to train a model that could recognize vulnerable species and assist in their proper monitoring. Furthermore, the other side of our study involved finding the best available architecture for this task. We experimented with various such architectures and analyzed their performance under different metrics.\n\nWe found that newer version of all the models showed stronger performance. VGG was the oldest among our models, so it performed poorly. Its accuracy was way below the vanilla CNN. Conversely, DenseNet and ResNet offered significantly better performance, so we can easily use them for related tasks. Also, we noted that these networks' newer and deeper versions did not provide any impactful difference across various metrics.\n\nIn addition, we implemented transfer learning and did not train them from scratch, benchmarking their ease of use in downstream tasks like ours. We had to freeze the feature extraction layers and only trained the last few fully connected layers. Doing this saved the time and computing required without compromising their performance.\n\nAlongside standard CNN models like ResNet, DenseNet, and VGGNet, specially designed for classification tasks, we also experimented with YOLOv8. YOLOs are primarily designed and used as go-to models for detection and segmentation tasks. To our great surprise, the metrics surpassed other standard classifica- tion models. Thus, YOLO might work well for classification tasks for custom datasets in similar niches.\n\nWhy does YOLOv8 perform the best?\n\nYOLOv8's superior performance might be due to its advanced architecture, which combines efficient feature ex- traction with fast processing capabilities by integrating components like CSPNet (Cross Stage Partial Networks) and PANet (Path Aggregation Network) [28, 15]. CSPNet reduces the computational cost while maintaining accuracy. It divides the feature map into two parts and merges them through the cross-stage hierarchy. On the other hand, PANet enhances the information flow between various layers, which is excellent for object de- tection across multi-scales. But, this proved beneficial for classification tasks, too. Even more importantly, the multiscale capability of YOLOv8 allows handling images with object sizes that can have significant variations and differing resolutions. It also has advanced augmentations, such as mosaic augmentation, which places four training images into one with diverse contexts in one image and hence helps the model generalize better to varying lighting and environmental conditions in training. All these features, together, carry out fast processing and effective feature extraction."}, {"title": "5 Conclusion", "content": "To sum up, we performed experimental analysis on transfer learning of various deep-learning CNN architectures on our custom dataset containing images of endangered mammals from Nepal. Our findings featured the superior performance of YOLOv8 compared to other models like DenseNet, ResNetV2, and VGG. It demonstrated higher accuracy, precision, and recall, making it practical for our and similar classification tasks. Although lagging by a narrow margin, other models like ResNet and DenseNet also performed well and competed neck and neck with YOLOv8. Transfer learning proved beneficial, drastically reducing training time and data required while maintaining high performance, which is crucial for tasks with limited data availability.\n\nIn the future, we will explore the ensemble methods that combine the strengths of multiple CNN architectures potentially enhancing classification accuracy and robustness, especially in diverse and challenging environmental conditions. We will also incorporate real-time monitoring capabilities in the future to provide feedback for conservation and take timely action in preventing the loss of endangered species. Hence, this study shows the reliance and robustness of deep-learning models in monitoring wildlife."}]}