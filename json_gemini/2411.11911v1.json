{"title": "ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling", "authors": ["Zikang Zhou", "Hengjian Zhou", "Haibo Hu", "Zihao Wen", "Jianping Wang", "Yung-Hui Li", "Yu-Kai Huang"], "abstract": "Anticipating the multimodality of future events lays the foundation for safe autonomous driving. However, multimodal motion prediction for traffic agents has been clouded by the lack of multimodal ground truth. Existing works predominantly adopt the winner-take-all training strategy to tackle this challenge, yet still suffer from limited trajectory diversity and misaligned mode confidence. While some approaches address these limitations by generating excessive trajectory candidates, they necessitate a post-processing stage to identify the most representative modes, a process lacking universal principles and compromising trajectory accuracy. We are thus motivated to introduce ModeSeq, a new multimodal prediction paradigm that models modes as sequences. Unlike the common practice of decoding multiple plausible trajectories in one shot, ModeSeq requires motion decoders to infer the next mode step by step, thereby more explicitly capturing the correlation between modes and significantly enhancing the ability to reason about multimodality. Leveraging the inductive bias of sequential mode prediction, we also propose the Early-Match-Take-All (EMTA) training strategy to diversify the trajectories further. Without relying on dense mode prediction or rule-based trajectory selection, ModeSeq considerably improves the diversity of multimodal output while attaining satisfactory trajectory accuracy, resulting in balanced performance on motion prediction benchmarks. Moreover, ModeSeq naturally emerges with the capability of mode extrapolation, which supports forecasting more behavior modes when the future is highly uncertain.", "sections": [{"title": "1. Introduction", "content": "Handling the intricate uncertainty presented in the real world is one of the major hurdles in autonomous driving. One aspect of the uncertainty lies in the multimodal behavior of traffic agents, i.e., multiple instantiations of an agent's future may be compatible with a given observation of the past. Without characterizing the multimodal distribution of agent motions, autonomous vehicles may fail to interact with the surroundings in a safe and human-like manner. For this reason, advanced decision-making systems demand a motion predictor to forecast several plausible and representative trajectories of critical agents [8, 16].\nAlthough multimodality has long been the central topic studied in motion prediction, this problem has not been fundamentally solved owing to the unavailability of multimodal ground truth, i.e., only one possibility is observable in real-world driving data. To struggle with this dilemma, most existing works adopt the winner-take-all (WTA) strategy [15] for training [4, 6, 17, 27, 39, 46, 53, 54]. Under this strategy, only the best among all predicted trajectories will receive supervision signals for regression, while all the remaining will be masked in the training loss. Despite being the current standard practice in the research community, the WTA solution has been found to cause mode collapse easily and produce indistinguishable trajectories [24, 34, 45], further confusing the learning of mode scoring [18]. As a remedy, some recent research intends to cover the ground-truth mode by generating a massive number of trajectory candidates [27, 39, 46], from which the most representative ones are heuristically selected based on post-processing methods such as non-maximum suppression (NMS)."}, {"title": "2. Related Work", "content": "Multimodality has been a dark cloud in the field of motion prediction. Early works employ generative models to sample multimodal trajectories [11, 13, 14, 32, 44], but they are susceptible to mode collapse. Modern motion predictors [4, 6, 17, 27, 39, 46, 53, 54] mostly follow the paradigm of multiple choice learning [15], where multiple trajectory modes are produced directly from mixture density networks [2]. Due to the lack of multimodal ground truth, these methods adopt the WTA training strategy [15], which is unstable and fails to deal with mode collapse fundamentally [24, 34, 45]. To mitigate this issue, a line of research performs dense mode prediction, i.e., decoding excessive trajectory candidates for better mode coverage [27, 39, 46]. Among these works, some equip the decoder with anchors [4, 29, 39, 51] to achieve more stable training. However, dense mode prediction necessitates the rule-based selection of the most representative trajectories from a large set of candidates, risking the precision of trajectories and the generalization ability across a wide range of scenarios. This paper provides new insights into multimodal problems by introducing the paradigm of sequential mode modeling and the EMTA training strategy, pursuing an end-to-end solution that produces a sparse set of diverse, high-quality, and representative trajectories directly.\nSequential modeling has found many applications in motion prediction and traffic modeling. On the one hand, applying sequential modeling to the time dimension results in trajectory encoders and decoders based on recurrent networks [1, 5, 7, 11, 12, 14, 26, 31, 32, 35, 44, 46] or Transformers [10, 21, 27, 28, 30, 36, 47, 50, 53-55], which can facilitate the learning of temporal dynamics. In particular, recent advances in motion generation [30, 36, 55] have shown that factorizing the joint distribution of multi-agent time-series in a social autoregressive manner [1, 32, 44] can better characterize the evolution of traffic scenarios. On the other hand, some works utilize sequential modeling in the agent dimension for multi-agent motion prediction [33, 43]. For example, M2I [43] uses heuristic methods to label influencers and reactors from pairs of agents, followed by predicting the marginal distribution of the influencers and the conditional distribution of the reactors. FJMP [33] extends M2I to model the joint distribution of an arbitrary number of agents, where the joint future trajectories of agents are factorized using a directed acyclic graph. Our work is the first attempt that employs sequential prediction in the mode dimension, which enhances the understanding of multimodal behavior by capturing the correlation between modes."}, {"title": "3. Methodology", "content": "Denote \\(S\\) as the input of motion prediction models, which encompasses the map elements represented as \\(M\\) polygonal instances and the \\(T\\)-step historical trajectories of \\(A\\) traffic agents (e.g., vehicles, pedestrians, and cyclists) in the scene. The models are tasked with forecasting \\(K\\) plausible trajectory modes per agent of interest, each comprising \\(T\\) waypoints and an associated confidence score. These trajectories are desired to be representative, reflecting distinct behavior modes of agents and properly measuring the likelihood of each mode via the estimated confidence.\nTypical motion predictors employ the encoder-decoder architecture, where an encoder computes the embedding \\(\\Psi\\) from the scene input, based on which a decoder learns the embeddings \\( \\{m_{i,k}\\}_{k\\in\\{1,...,K\\}} \\) of the \\(i\\)-th agent's future modes. Without loss of generality, the following simplifies \\(m_{i,k}\\) as \\(m_k\\) to discuss the prediction for a single agent, which can be extended for multiple agents by repeating the same decoding process. Given \\(m_k\\), a prediction head then outputs a trajectory \\(\\hat{y}_k = [\\hat{y}^1,..., \\hat{y}^T]\\) and a confidence score \\(\\phi_k\\) via simple modules such as multi-layer perceptrons (MLPs). The whole pipeline can be summarized as\n\\[\\begin{aligned} \\Psi &= \\text{Encoder} (S), \\\\ \\{m_k\\}_{k\\in\\{1,...,K\\}} &= \\text{Decoder} (\\Psi), \\\\ \\hat{y}_k, \\phi_k &= \\text{Head} (m_k), \\quad k\\in \\{1, ..., K\\}. \\end{aligned}\\]"}, {"title": "3.2. Motivation", "content": "Prior works formulate multimodal decoding as a problem of set prediction [3, 48]. For instance, most cutting-edge methods employ DETR-like decoders [3] to produce the joint embeddings of multiple modes from learnable or anchor-based queries that are permutation-equivariant [27, 39, 46, 54]. Ideally, the joint mode embeddings should be supervised by the ground-truth multimodal distribution, akin to the application of set prediction in object detectors [3, 20, 25, 56] where each object query receives supervision signals via optimal bipartite matching. However, real-world driving data contains only one instantiation of scene evolution, which compels motion prediction solutions to adopt the winner-take-all (WTA) matching [15]. As a result, only the mode embeddings of the best-predicted trajectories get optimized, which will easily degenerate the models into learning multiple mode embeddings independently. The inability to jointly optimize all modes explains why DETR-like motion decoders fail to avoid duplicated trajectories without the use of non-maximum suppression (NMS), given that gathering multiple predicted trajectories around the most probable regions is expected to achieve lower training loss when only one ground-truth future is presented during training.\nTo facilitate reasoning about multimodality in the absence of multimodal ground truth, our ModeSeq framework requires the decoder to conduct chain-based factorization on the joint embeddings, which is demonstrated as follows:\n\\[m_t = \\text{Decoder} (\\Psi, \\{m_k\\}_{k\\in\\{1,...,t-1}\\}), \\quad t \\in \\{1, ..., K\\}.\\]\nWith such a factorization that converts the unordered set of modes into a sequence, the correlation between modes can be naturally strengthened, as the mode to be decoded depends on the ones that appear previously. Further equipping this framework with appropriate model implementation and training scheme has the potential to offer better mode coverage and scoring without severe sacrifice in trajectory accuracy, which we introduce in the following sections."}, {"title": "3.3. Scene Encoding", "content": "Since this work focuses on the decoding of multimodal trajectories, we simply adopt QCNet [54] as the scene encoder, which is one of the de facto best practices in industry and academia due to its symmetric modeling in space and time leveraging relative positional embeddings [37]. This encoder, on the one hand, exploits a hierarchical map encoding module based on map-map self-attention to produce the map embedding of shape [M, D], with D referring to the hidden size. On the other hand, the encoder consists of Transformer modules that factorize the space and time axes, including temporal self-attention, agent-map cross-attention, and agent-agent self-attention. These three types of attention are grouped and interleaved twice to yield the agent embedding of shape [A, T, D], which constitutes the final scene embeddings together with the map embedding. In principle, any scene encoding method can fit into our ModeSeq framework with reasonable efforts."}, {"title": "3.4. Single-Layer Mode Sequence", "content": "This section illustrates the detailed structure of a single ModeSeq layer, which consists of a Memory Transformer module and a Context Transformer module. Stacking multiple ModeSeq layers can further improve the performance via iterative refinement, which will be discussed in Sec. 3.5.\nRoutine. We introduce the decoding procedure of the \\(l\\)-th ModeSeq layer, with the right-hand side of Fig. 2 depicting the first layer (i.e., \\(l=1\\)). The layer is designed to recurrently output a sequence of mode embeddings \\([m_1^{(l)},...,m_K^{(l)}]\\), where we slightly complicate the notation of modes with a superscript that identifies the layer index. The input of the layer includes the scene embedding \\(\\Psi\\) yielded by the encoder and the mode embeddings produced by the \\((l - 1)\\)-th decoding layer, i.e., \\([m_1^{(l-1)},...,m_K^{(l-1)}]\\). Since the latter does not apply to the first layer, we introduce an embedding \\(e \\in \\mathbb{R}^D\\) to serve as the output of the \"0-th layer\", which is randomly initialized at the beginning of training. The same learnable \\(e\\) is shared across the \\(K\\) input embeddings of the first layer:\n\\[m_k^{(0)} = e, \\quad k \\in \\{1, ..., K\\}.\\]\nBefore starting the decoding, we create an empty sequence \\(\\Omega_0^{(l)} = []\\) with the subscript and superscript indicating the 0-th decoding step and the \\(l\\)-th layer, respectively. This sequence will be used to keep track of the mode embeddings produced at various steps. At the \\(t\\)-th decoding step, we employ a Memory Transformer module and a Context Transformer module to update \\(m_t^{(l-1)}\\) to become \\(m_t^{(l)}\\), leveraging the information in the memory bank \\(\\Omega_{t-1}^{(l)}\\) and the scene embedding \\(\\Psi\\). The output mode embedding \\(m_t^{(l)}\\) is then pushed to the end of the sequence \\(\\Omega_{t-1}^{(l)}\\) to obtain \\(\\Omega_t^{(l)} = [m_1^{(l)},..., m_t^{(l)}]\\), which will serve as the input at the \\((t+1)\\)-th decoding step. After going through \\(K\\) decoding steps, we use a prediction head to transform each of the mode embeddings stored in \\(\\Omega_K^{(l)}\\) into a specific trajectory and a corresponding confidence score via MLPs. The following paragraphs detail the modules constituting a ModeSeq layer and discuss the differences between our approach and other alternatives.\nMemory Transformer. The Memory Transformer takes charge of modeling the sequential dependencies of trajectory modes. At the t-th decoding step of the l-th layer, this module takes as input \\(\\Omega_{t-1}^{(l)}\\) and \\(m_t^{(l-1)}\\), the memory bank for the current layer and the \\(t\\)-th mode embedding derived from the last layer. Since we desire the generation of the t-th mode embedding \\(m_t^{(l)}\\) to be aware of the existence of the preceding modes, we treat \\(m_t^{(l-1)}\\) as the query of the Transformer module, which retrieves the memory bank in a cross-attention manner:\n\\[m_t^{(l)} = \\text{MemFormer} \\left( \\text{query}=m_t^{(l-1)}, \\text{key/value}=\\Omega_{t-1}^{(l)} \\right).\\]\nIn this way, the information in \\(\\Omega_{t-1}^{(l)}\\) is assimilated into \\(m_t^{(l-1)}\\) to produce \\(m_t^{(l)}\\), which is a query feature conditioned on the modes up to the \\((t - 1)\\)-th decoding step.\nContext Transformer. To derive scene-compliant modes, we must provide the query feature with a specific scene context. To this end, we use the Context Transformer module to refine the conditional query \\(m_t^{(l)}\\) with the scene embeddings output by the encoder. Specifically, the t-th mode embedding \\(m_t^{(l)}\\) is computed by enriching \\(m_t^{(l)}\\) with \\(\\Psi\\) using cross-attention:\n\\[m_t^{(l)} = \\text{CtxFormer} \\left( \\text{query} = m_t^{(l)}, \\text{key/value} = \\Psi \\right).\\]\nConsidering the high complexity of performing global attention, we decompose the Context Transformer into three separate modules in practice, including mode-time cross-attention, mode-map cross-attention, and mode-agent cross-attention, each of which takes as input only a subset of the embeddings contained in \\(\\Psi\\). First, the mode-time cross-attention fuses the query feature with the historical encoding belonging to the agent of interest, enabling the query to adapt to the specific agent. Second, we aggregate the map information surrounding the agent of interest into the query feature leveraging the mode-map cross-attention, which contributes to the map compliance of the forecasting results. Finally, utilizing the mode-agent cross-attention module to fuse the neighboring agents' latest embeddings promotes the model's social awareness. After going through these three modules, the conditional query \\(m_t^{(l)}\\) eventually becomes \\(m_t^{(l)}\\), which is now context-aware.\nPrediction Head. Given the conditional, context-aware mode embedding \\(m_t^{(l)}\\), we use an MLP head to output the t-th trajectory \\(\\hat{Y}_t^{(l)}\\) and another to estimate the corresponding confidence score \\(\\hat{\\phi}_t^{(l)}\\):\n\\[\\begin{aligned} \\hat{Y}_t^{(l)} &= \\text{MLP} \\left( m_t^{(l)} \\right), \\\\ \\hat{\\phi}_t^{(l)} &= \\text{MLP} \\left( m_t^{(l)} \\right). \\end{aligned}\\]\nComparison with DETR-Like Decoders. In contrast to motion decoders [27, 39, 46, 54] inspired by DETR [3], where the relationships between modes are completely neglected [27, 46] or weakly modeled by mode-mode self-attention [39, 54], modeling modes as a sequence strengthens mode-wise relational reasoning thanks to the conditional dependence in generating multimodal embeddings, which is beneficial to eliminating duplicated trajectories. Furthermore, it is worth noting that DETR-like approaches can only decode a fixed number of modes, as the number of learnable/static anchors cannot be changed once specified at the start of training. By contrast, our ModeSeq framework supports decoding more/less modes at test time, which can be simply achieved by changing the number of decoding steps. This characteristic can be helpful since the degree of uncertainty varies by scenario.\nComparison with Typical Recurrent Networks. The ModeSeq layer can be viewed as a sort of recurrent network [5, 12] due to its parameter sharing across all decoding steps, though its core components are modernized with Transformers to achieve impressive performance. While typical recurrent networks compress the memory into a single hidden state, which is often lossy, the Memory Transformer inside a ModeSeq layer allows for direct access to all prior mode embeddings, naturally scaling the capacity of the memory as the number of modes grows."}, {"title": "3.5. Multi-Layer Mode Sequences", "content": "Single-layer mode sequences may have limited capability of learning high-quality mode representations. In particular, if the layer happens to produce unrealistic or less likely modes at the first few decoding steps, the learning of the later modes may be unexpectedly disturbed. Inspired by DETR [3], we develop an iterative refinement framework by stacking multiple ModeSeq layers and applying training losses to the output of each layer. As shown in the left part of Fig. 2, all layers except for the first one take as input the mode embeddings output from the last round of decoding, refining the features with the scene context. Crucially, we introduce the operation of mode rearrangement in between layers, which corrects the order of the embeddings in the mode sequence to encourage decoding trajectory modes with monotonically decreasing confidence scores.\nMode Rearrangement. Before transitioning from the l-th to the (l + 1)-th ModeSeq layer, we sort the mode embeddings stored in the memory bank \\(\\Omega_K^{(l)}\\) according to the descending order of the confidence scores predicted from them. The sorted mode embeddings will then be sequentially input to the (l+1)-th ModeSeq layer for recurrent decoding. Through iterative refinement with mode rearrangement, the trajectories and the order of modes become more scene-compliant and more reasonable, respectively."}, {"title": "3.6. Early-Match-Take-All Training", "content": "The WTA training strategy [15] is blamed for producing overlapped trajectories and indistinguishable confidence scores [18, 24, 34, 45]. Fortunately, our approach has the opportunity to opt for a more advanced training method thanks to the paradigm of sequential mode modeling. In this section, we propose the EMTA loss, which leverages the order of modes to define the positive and negative samples toward better mode coverage and confidence scoring without significantly sacrificing trajectory accuracy.\nTypical WTA loss optimizes only the trajectory with the minimum displacement error with respect to the ground truth. In comparison, our EMTA loss optimizes the matched trajectory decoded at the earliest recurrent step. For example, if both the second and the third trajectories match the ground truth, only the second one will be optimized, regardless of which one has the minimum error. To this end, we search over the \\(K\\) predictions to acquire the collection of mode indices associated with matched trajectories:\n\\[\\mathcal{G}^{(l)} = \\{ k \\mid k \\in \\{1, ..., K\\} \\wedge 1\\{\\text{IsMatch} (\\hat{y}_k^{(l)}, y) \\} \\},\\\\]\nwhere \\(\\mathcal{G}^{(l)}\\) denotes the set of qualified mode indices in the \\(l\\)-th ModeSeq layer, \\(1\\{\\cdot\\}\\) represents the indicator function, and \\(\\text{IsMatch}(\\cdot, \\cdot)\\) defines the criterion for a match given the ground-truth trajectory \\(y\\). The implementation of \\(\\text{IsMatch}(\\cdot, \\cdot)\\) can be flexible, depending on the trajectory accuracy demanded by practitioners. For instance, on the Waymo Open Motion Dataset [9], we decide whether a predicted trajectory is a match based on the velocity-aware distance thresholds defined in the Miss Rate metric of the benchmark; while on the Argoverse 2 Motion Forecasting Dataset [49], a matched trajectory is expected to have less"}, {"title": "4. Experiments", "content": "Datasets. We conduct experiments on the Waymo Open Motion Dataset (WOMD) [9] and the Argoverse 2 Motion Forecasting Dataset [49]. The WOMD contains 486995/44097/44920 training/validation/testing samples, where the history of 1.1 seconds is provided as the context and the 8-second future trajectories of up to 8 agents are required to predict. The Argoverse 2 dataset comprises 199908/24988/24984 samples with 5-second observation windows and 6-second prediction horizons for training/validation/testing.\nMetrics. Following the standard of the benchmarks [9, 49], we constrain models to output at most \\(K = 6\\) trajectories. We use Miss Rate (\\(MR_K\\)) to measure mode coverage, which counts the fraction of cases in which the model fails to produce any trajectories that match the ground truth within the required thresholds. Built upon the definition of a match, \\(mAP_K\\) and Soft \\(mAP_K\\) assess the precision of the confidence scores by computing the P/R curves and averaging the precision values over various confidence thresholds. To further evaluate trajectory quality, we use minimum Average Displacement Error (\\(minADE_K\\)) and minimum Final Displacement Error (\\(minFDE_K\\)) as indicators, which calculate the distance between the ground truth and the best-predicted trajectories as an average over the whole horizon and at the final time step, respectively. Besides, the \\(b\\)-\\(minFDE_K\\) concerns the joint performance of trajectories and confidences by summing the \\(minFDE_K\\) and Brier scores of the best-predicted trajectories.\nImplementation Details. We develop models with a hidden size of 128. The decoder stacks 6 layers for iterative refinement, with each layer executing 6 steps to obtain exactly 6 modes as required by the benchmarks [9, 49]. On the WOMD [9], we use the AdamW optimizer [23] to train models for 30 epochs on the training set with a batch size of 32, a weight decay rate of 0.1, and a dropout rate of 0.1. On Argoverse 2 [49], we use a similar training configuration except that the number of epochs is extended to 64. The initial learning rate is set to \\(5 \\times 10^{-4}\\), which is decayed to 0 at the end of training following the cosine annealing schedule [22]. Unless specified, the ablation studies are based on experiments on the WOMD with 20% of the training data."}, {"title": "4.2. Comparison with State of the Art", "content": "We compare our approach with QCNet [54] and the MTR series [38-40], which are currently the most effective sparse and dense multimodal prediction solutions across the WOMD and the Argoverse 2 dataset. As demonstrated in Tab. 1, ModeSeq achieves the best scoring performance among the Lidar-free methods on the validation split of the WOMD, though it lags behind the \\(mAP_6\\) performance of MTR v3 [38], a model that augments the input information with raw sensor data. As a sparse mode predictor, ModeSeq undoubtedly outperforms MTR++ [40] in terms of \\(MR_6\\), \\(minADE_6\\), and \\(minFDE_6\\) by a large margin. Compared with QCNet [54], ModeSeq attains better Soft \\(mAP_6\\), \\(mAP_6\\), and \\(MR_6\\) at the cost of slight degradation on \\(minADE_6\\) and \\(minFDE_6\\), confirming that our approach can improve the mode coverage and confidence scoring of sparse predictors without significant sacrifice in trajectory accuracy. As of the time we submitted the results to the benchmark, the ensemble version of ModeSeq ranked first among Lidar-free approaches on the test set of the WOMD. Our approach also exhibits promising performance on the Argoverse 2 dataset, where our ensemble-free model surpasses QCNet and the MTR series on all critical metrics as shown in Tab. 2."}, {"title": "4.3. Ablation Study", "content": "Effects of Sequential Mode Modeling. In Tab. 3, we examine the effectiveness of sequential mode modeling by comparing ModeSeq with the sparse DETR-like decoder enhanced with iterative refinement [3], both employing the same QCNet encoder [54] for fair comparisons. The results demonstrate that ModeSeq outperforms the baseline on all metrics when using the same training strategy. Interestingly, ignoring the confidence loss of the suboptimal modes that match the ground truth can improve the performance of both methods under the WTA training. The reason behind this is that treating the other matched modes as negative samples will confuse the optimization process, given that the best and the other matches usually have similar mode representations while they are assigned as opposite samples.\nEffects of EMTA Training. We also investigate the role of EMTA training in Tab. 3. After replacing the WTA loss with our EMTA scheme, the results on Soft \\(mAP_6\\), \\(mAP_6\\), and \\(MR_6\\) are considerably improved, which demonstrates the benefits of EMTA training in terms of mode coverage and confidence scoring. On the other hand, the performance on \\(minADE_6\\) and \\(minFDE_6\\) slightly deteriorates since the EMTA loss has relaxed the requirement for trajectory accuracy, but the degree of deterioration falls within an acceptable extent, leading to more balanced performance taken overall. Moreover, contrary to the conclusion drawn from the WTA baselines, treating other matches as ignored samples is detrimental under the EMTA strategy. This is because the joint effects of sequential mode decoding and EMTA training have broken the symmetry of mode modeling and label assignment, allowing us to assign the other matches as negative samples to drive them away from the ground truth for covering other likely modes.\nEffects of Iterative Refinement. To understand the effects of iterative refinement under our framework, we take the output from different decoding layers for evaluation. As shown in Fig. 3, the performance on Soft \\(mAP_6\\) and \\(MR_6\\) is generally improved as the depth increases, totaling a substantial enhancement between the first and the last layer. One of the reasons why iterative refinement works well can be attributed to the operation of mode rearrangement in between layers, which we explain in the following.\nEffects of Mode Rearrangement. We study the effects of mode rearrangement in Tab. 4. Comparing the first and third rows of the table, we can see that reordering the mode embeddings before further refinement can remarkably promote the forecasting capability. To gain deeper insights into the results, we develop a variant of label assignment, where the modes decoded earlier than the first match are deemed ignored samples. We found this strategy to outperform the de"}, {"title": "4.4. Qualitative Results", "content": "The qualitative results produced by ModeSeq are presented in Fig. 4. Figure 4a demonstrates that our model can generate representative trajectories when being trained to decode only 3 modes. Comparing Fig. 4c with Fig. 4b, we can see that the 6-mode model successfully extrapolates diverse yet realistic modes when executing 24 decoding steps during inference, showcasing the extrapolation ability of ModeSeq."}, {"title": "5. Conclusion", "content": "This paper introduces ModeSeq, a modeling framework that achieves sparse multimodal motion prediction via sequential mode modeling. The framework comprises a mechanism of sequential multimodal decoding, an architecture of iterative refinement with mode rearrangement, and a training strategy of Early-Match-Take-All label assignment. As an alternative to the unordered multimodal decoding and the winner-take-all training strategy, ModeSeq achieves state-of-the-art results on motion prediction benchmarks and exhibits the characteristic of mode extrapolation, creating a new path to solving multimodal problems.\nLimitations. Our approach is still flawed in some respects. First, the sequential generation of modes is less efficient than one-shot predictions, which necessitates an improvement in efficiency. Second, although our approach supports multi-agent forecasting in parallel, we only explore marginal multi-agent prediction, lacking validation on joint prediction. Future research may involve extending Mode-Seq into a joint multi-agent model.\nAcknowledgement. This project is supported by a grant from Hong Kong Research Grant Council under GRF project 11216323 and CRF C1042-23G."}]}