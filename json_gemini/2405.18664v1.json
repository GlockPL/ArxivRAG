{"title": "Fast Explainability via Feasible Concept Sets Generator", "authors": ["Deng Pan", "Nuno Moniz", "Nitesh Chawla"], "abstract": "A long-standing dilemma prevents the broader application of explanation methods: general applicability and inference speed. On the one hand, existing model-agnostic explanation methods usually make minimal pre-assumptions about the prediction models to be explained. Still, they require additional queries to the model through propagation or back-propagation to approximate the models' behaviors, resulting in slow inference and hindering their use in time-sensitive tasks. On the other hand, various model-dependent explanations have been proposed that achieve low-cost, fast inference but at the expense of limiting their applicability to specific model structures. In this study, we bridge the gap between the universality of model-agnostic approaches and the efficiency of model-specific approaches by proposing a novel framework without assumptions on the prediction model's structures, achieving high efficiency during inference and allowing for real-time explanations. To achieve this, we first define explanations through a set of human-comprehensible concepts and propose a framework to elucidate model predictions via minimal feasible concept sets. Second, we show that a minimal feasible set generator can be learned as a companion explainer to the prediction model, generating explanations for predictions. Finally, we validate this framework by implementing a novel model-agnostic method that provides robust explanations while facilitating real-time inference. Our claims are substantiated by comprehensive experiments, highlighting the effectiveness and efficiency of our approach.", "sections": [{"title": "1 Introduction", "content": "The increasing complexity and opacity of deep machine learning models have created significant challenges in ensuring their broad application, particularly in high-stakes domains where model transparency and safety are paramount. Various research has been conducted to address these concerns [1, 2, 3]. While deep models demonstrate superior performance across various tasks, their \u201cblack-box\" nature often impedes their acceptance and deployment in critical areas such as healthcare, finance, and autonomous systems [4, 5, 6, 7, 8]. In these contexts, it is essential not only to achieve high predictive accuracy but also to provide clear, understandable explanations of the models' decisions."}, {"title": "2 Related Work", "content": "In this section, we review two types of explanation approaches: 1) model-agnostic approaches, which are independent of specific model structures, and 2) model-specific approaches, which are usually optimized for specific model structures for efficient inference.\nModel-agnostic approaches: Model-agnostic approaches are designed to be broadly applicable, making minimal or no assumptions about the prediction models to be explained. One common strategy involves using surrogates to approximate the local behavior of models, which is particularly useful for black-box models. For instance, LIME [12] fits a surrogate interpretable model (such as a linear model) to explain predictions locally by perturbing the input data and observing the changes in predictions. Similarly, SHAP [13] leverages Shapley values from game theory to ensure a unique surrogate solution with desirable properties such as local accuracy, missingness, and consistency. RISE [14] generates saliency maps by sampling randomized masks and evaluating their impact on the model's output, sharing the idea of surrogate fitting via random input perturbation like LIME and SHAP, but using saliency maps as local surrogates. These surrogate methods, however, are resource-intensive due to the random sampling and additional model queries required. For example, SHAP often necessitates numerous additional queries to estimate marginal distributions around a given input, resulting in significant computational overhead.\nAnother model-agnostic strategy involves utilizing the gradient information from white-box models. Rather than learning surrogates, these methods exploit locally smoothed gradients to approximate the local behavior of prediction models. Integrated Gradients [16], for example, smooths gradients by averaging those of interpolated samples between a baseline and a specific input. While this interpolation can be viewed as uniform sampling, AGI [17] proposes a sampling strategy that utilizes adversarial attacks to locate decision boundaries. NeFLAG [18], inspired by the divergence theorem, approximates smoothed gradients via the flux of gradients flowing over a hypersphere. Despite"}, {"title": "3 Proposed Method", "content": "In this section, we first introduce a definition of explainability in terms of comprehensible concepts and feasible concept sets. Then, we propose a general framework for the minimum feasible set generation method and implement a model agnostic method based on the framework with custom concept mapping functions."}, {"title": "3.1 Defining Explainability", "content": "Definition 1 (Comprehensible Concept and Concept Mapping Function) A concept c is manually designed following certain rules, which ensures that individuals with proper background knowledge can understand the concepts. A concept mapping function maps the sample x to a concept c via c = fmap(x), denoting that x has concept c.\nExplanations should be provided based on comprehensible concepts, which can vary depending on the context. Therefore, to generate explanations for a prediction, it is crucial first to define the concepts on which these explanations will be based.\nDefinition 2 (Concept Mapping Function Set) A set of concept mapping functions mapping the raw input features to a set of concepts, Fmap : X \u2192 C, i.e., given one input x \u2208 X, we obtain a set of concepts {ci = fmap(x)| fmap \u2208 Fmap} = C.\nDefining the concepts is equivalent to defining the set of concept mapping functions. Taking the image and text data as examples, for image data, when choosing image patches as concepts, we can express an image x as a sequence of M individual patches (p1, ..., pM). Similarly, for text data, we can express it as a token sequence x of length M. In both scenarios, a concept mapping function can be defined by fmap(x) = s \u2299 x, where s \u2208 {0, 1}M is a permutation of {1,0} of length M and the set of concept mapping functions can be written by Fmap = {fmap|s \u2208 {0,1}M}. The concept set then consists of all possible combinations of patches or tokens.\nDefinition 3 (Concept Elimination) Assume that fmap is a concept mapping function, let X denotes its null space, i.e., fmap(X) = 0, or equivalently (fmap)-1(0) = X, we define that concept ci is eliminated from the input x by its projection on the null space X. i.e.,\nx\\i = P(X)(x), (1)"}, {"title": "3.2 Generating Feasible Sets", "content": "Assume we have a prediction model y = f(x), and for any input \u00e6, we have a set Fmap of concept mapping functions, hence concept ci can be obtained by ci = fmap(x), where fmap \u2208 Fmap.\nLet's define a distribution p(Fmap, x), from which we can generate random concept mapping functions given Fmap and \u00e6. From this distribution, for a specific input x, we can generate a set of concepts Cge to be eliminated. The corresponding input projected onto the null space is denoted by x\\cg. As per Definition 4, if d(f(x), f(x\\cge)) < \u0454, then Cge is a feasible elimination set. To ensure any set draw from p(Fmap, x) is a feasible elimination set, it is intuitive to have the following condition\nECge~p(Fmap,x)d(f(x), f(x\\cge)) < \u0454 (4)\nIf, instead, we choose to generate a feasible set Cg other than Cge, to ensure its feasible, the condition becomes\nECg~p(Fmap,x)d(f(x), f(xc\u2084)) < \u0454 (5)"}, {"title": "3.3 Generating Minimum Feasible Sets", "content": "Referring to Definition 7, we need to constrain the cardinality of X, i.e., the rank of the null space of Cg, to obtain a minimum feasible set. This leads to a minimization objective\nmin Ecg~p(Fmap,w) Card(X) (6)\ns.t. Ecg~p(Fmap,x) (d(f(x), f(x\\c\u2084)) \u2013 \u0454) > 0"}, {"title": "3.4 An Implementation of the Framework", "content": "Note that given different choices of concept mapping functions, a few existing methods can be categorized as special cases of this framework. For instance,\nSpecial Case I: When defining the concept mapping functions as the feature map extractors in CNN, i.e., fmap(x) = A(i) where fmap \u2208 Fmap and A(i) is a feature map, by first-order approximation, we have f(x) = \u2211i dA(i)df(x) \u2022 A(i). Hence, we can approximate f(xc) by f(xc) \u2248 \u2211iec dA(i)df(x). A(i), and then define d(f(x), f(xc\u2084)) = \u2211ifc AA(s). Additionally, if we assume p(Fmap, x) is a multi-variate Bernoulli distribution with parameter p and define the cardinality to be the L2 norm of its means, i.e., ||P||2, the optimization objective becomes\nmin EC9~p(Fmap,\u00e6)||P||2 + 1. dA(i)df(x). A(2), (8)\nP\nifCg\na solution to the above minimization problem is Pix ReLU (3)A()), which is identical to an intermediate result of GradCAM[9], which substantiates that it is a special case of our framework.\nProof: Denote wi = dA(i)df(x) A(i), and calculate the expectation by summing over all possible permutations of Cg, Eq 8 can be rewritten as\nmin ||p||2 + \u03bb\u00b7 \u03c9 \u00b7 (1 \u2212 p) \u21d2 min ||P||2 + 1||w|| \u2013 \u03bb \u00b7 \u03c9 \u00b7 p (9)\np p\nTo minimize the above objective, p must have the same direction as w, but since p must be non-negative, hence pi x ReLU (f). A()), qed.\nWe can also choose the masking mechanism as the concept mapping function.\nMasking as Concept Mapping Function: The concept mapping function can be defined using any human comprehensible concept. In this study, we define the concepts and concept mapping sets described in our example setting, i.e., choosing the masked inputs as the collection of concepts. Let the concept mapping function be fmap(x) = s \u2299 x = ($(s1,x1), (s2, x2), ..., $(sL, xL)), where $(si, xi) represents the masking strategy and s is a permutation of {0,1}L sequence. When si = 1, xi shall be kept as is, i.e., (1, xi) = xi. As for the cases when si = 0, the masking strategy can vary depending on different scenarios. For instance, for image data, one can define (0, xi) = 0 or \u03c6(0,xi) ~ \u039d(\u03bc, \u03c3) to mask the selected parts using blank patches or Gaussian noise, respectively; for text data, one could choose (0, xi) = [MASK] to conveniently replace the corresponding tokens as the [MASK] token.\nMasking as Concept Elimination: Given a permutation s and its corresponding concept sx, the projected input onto the elimination set is obtained by (1 \u2013 s)x.\nDistance Function: For a classification task, assuming f(x) denotes the probability of the predicted label, we define d(f(x), f(x)) = max(f(x) \u2212 f(x), 0). The intuition is that we need to keep the predicted label intact and maintain confidence.\nSpecial Case II: Keep \u00e6 fixed, assume f(x) = 1, then d(f(x), f(x)) = 1 \u2212 f(x), and define the cardinality by ||p||2. the objective 7 can be written by\nmin ||P||2 + 1 -\ns~Bernoulli (p)\nf(x), (10)\nSimilar to our proof for Special Case I, we have ps x f(sx). This coincides with RISE [14], which generates heatmaps via a weighted sum of randomly generated masks, with weights being simply f(x)."}, {"title": "4 Experiments", "content": "We conduct experiments on both image and text classification tasks. For image classification, we use the ViT model [23] fine-tuned on the ImageNet dataset [24] as the prediction model. For text classification, we use the BERT model [25] fine-tuned on the SST2 dataset [26] for sentiment analysis. In our proposed method, g(x) is fine-tuned on both datasets, with X = 1 and \u03b4 = 0.3 in both settings."}, {"title": "4.1 Baselines", "content": "Our experiments include five baselines, comprising model-specific and model-agnostic methods.\nModel-Specific Methods: GradCAM [9] computes a weighted feature map for the final CNN layers of the prediction model and is noted as a special case of our framework (Special Case I). AttLRP [10] combines layer-wise relevance propagation with the attention weights of transformer models. In our experiments, we use the default settings from [10].\nModel-Agnostic Methods: IG (Integrated Gradients) [16] averages the gradients of interpolated samples between a baseline and a specific input. We use an all-zero input as the baseline and set the number of integral approximation steps to 200. IG is excluded from text classification experiments as it's not directly adaptable to text data. RISE [14] calculates the weighted average of random masks, with weights determined by the predictions of masked inputs. This method is another special case of our framework (Special Case II). We generate 500 random masks per sample in our experiments.\nAdditionally, we also include random saliency maps as a sanity check to ensure the robustness of our evaluation."}, {"title": "4.2 Metrics", "content": "We follow the evaluation strategies reported by [10]. For image classification, we use positive and negative perturbations to evaluate the AUC of accuracies by progressively masking inputs based on"}, {"title": "4.3 Results", "content": "Figure 1 and Figure 2 demonstrate the qualitative illustration of different explanation methods on both the image classification task and the sentiment classification task. We can observe that our approach can achieve comparable visualization quality to the model-specific methods AttLRP and GradCAM, and has much better visualization quality than model-agnostic methods IG and RISE."}, {"title": "5 Conclusion", "content": "In this study, we addressed the persistent challenge in the field of explainable artificial intelligence (XAI) of balancing general applicability with inference speed. We identified that while model-agnostic methods are broadly applicable, they suffer from slow inference times, and model-specific methods, though efficient, are restricted to particular model architectures. To overcome these limitations, we"}, {"title": "Broader Impact", "content": "Our framework enhances transparency and trust in AI, crucial for applications in sectors like health-care. It aids debugging and bias identification, supporting ethical AI use and regulatory compliance. However, risks include potential oversimplification of explanations and exposure of proprietary model details. Addressing these challenges is key to maximizing positive impact."}]}