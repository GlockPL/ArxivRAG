{"title": "EARN Fairness: Explaining, Asking, Reviewing and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders", "authors": ["LIN LUO", "YURI NAKAO", "MATHIEU CHOLLET", "HIROYA INAKOSHI", "SIMONE STUMPF"], "abstract": "Numerous fairness metrics have been proposed and employed by artificial intelligence (AI) experts to quantitatively measure bias and define fairness in AI models. Recognizing the need to accommodate stakeholders' diverse fairness understandings, efforts are underway to solicit their input. However, conveying Al fairness metrics to stakeholders without Al expertise, capturing their personal preferences, and seeking a collective consensus remain challenging and underexplored. To bridge this gap, we propose a new framework, EARN Fairness, which facilitates collective metric decisions among stakeholders without requiring AI expertise. The framework features an adaptable interactive system and a stakeholder-centered EARN Fairness process to Explain fairness metrics, Ask stakeholders' personal metric preferences, Review metrics collectively, and Negotiate a consensus on metric selection. To gather empirical results, we applied the framework to a credit rating scenario and conducted a user study involving 18 decision subjects without Al knowledge. We identify their personal metric preferences and their acceptable level of unfairness in individual sessions. Subsequently, we uncovered how they reached metric consensus in team sessions. Our work shows that the EARN Fairness framework enables stakeholders to express personal preferences and reach consensus, providing practical guidance for implementing human-centered AI fairness in high-risk contexts. Through this approach, we aim to harmonize fairness expectations of diverse stakeholders, fostering more equitable and inclusive Al fairness.", "sections": [{"title": "1 Introduction", "content": "Ensuring artificial intelligence (AI) fairness is crucial for ethical integrity, legal compliance, and human trust [36], particularly in high-stakes scenarios such as healthcare [14], finance [34], and university admission [42]. AI fairness experts have made significant efforts to define fairness quantitatively through fairness metrics [43, 53]. These metrics help assess the fairness of AI systems, audit their compliance with specific fairness requirements, and guide unfairness mitigation [26]. Currently, there are over 20 popular fairness metrics, categorized into three primary categories - group fairness, subgroup fairness, and individual fairness to measure whether AI systems treat groups, subgroups, and individuals fairly, respectively [12, 43, 53].\nHowever, there are three primary issues when applying fairness metrics to assess Al fairness. Firstly, choosing the right metrics becomes problematic due to the lack of clear, unified guidelines for metric selection, especially in the absence of a one-size-fits-all metric and the insufficiency of using a single metric [11, 22, 41, 43]. Secondly, Al fairness metric selection, often led by AI experts lacking stakeholder input, risks overly depending on these experts' perspectives and disregards fairness as a societal value with diverse interpretations among stakeholders [2, 26, 43, 53]. This could lead to Al models deemed technically \"fair\" by AI experts but not accepted in practice because they fail to reflect stakeholders' views on fairness [37, 56]. Lastly, even when personal fairness perspectives are collected from stakeholders,"}, {"title": "2 Related Work", "content": "First, we explore how AI fairness has been conceptualized and measured across different metrics. We then review existing approaches to solicit stakeholders' fairness metrics preferences, and we conclude this section by identifying research gaps."}, {"title": "2.1 Al Fairness Metrics", "content": "Researchers have proposed various metrics to quantify AI fairness, focusing on three main categories: group fairness and subgroup fairness, which address fairness at the group level, and individual fairness, focusing on fairness at the individual level [43, 53]. Each category has its distinct advantages and limitations.\nGroup fairness prioritizes equal treatment among groups, usually defined by protected characteristics enshrined by legislation, such as age or gender, or sensitive features based on contexts, e.g., foreign worker status. Group fairness is easily comprehensible to policymakers and the public, and it is also feasible to implement computationally [7, 50]. Some prominent group fairness metrics include [26, 43]:\n\u2022 Demographic Parity [35] (also known as Statistical Parity [17]): Both protected and unprotected group members have the same probability of being classified in the positive category.\n\u2022 Equal Opportunity [23]: Both protected and unprotected groups have the same probability that a subject in the positive class is predicted as a positive class.\n\u2022 Predictive Equality [15]: Both protected and unprotected groups have the same probability that a subject in the negative class is predicted as a positive class.\n\u2022 Equalized Odds [23]: Simultaneously satisfy both Equal Opportunity and Predictive Equality.\n\u2022 Outcome Test [49]: Both protected and unprotected groups have the same probability that a subject with a positive prediction genuinely belongs to the positive class.\n\u2022 Conditional Statistical Parity [15]: Both protected and unprotected groups are equally likely to be predicted as a positive class, under conditions controlled by a set of legitimate features L. Legitimate features are those directly relevant and considered appropriate for influencing decision-making outcomes. For instance, in a credit rating scenario, legitimate features are factors that affect an applicant's creditworthiness, such as the applicant's credit history and employment status [53].\nHowever, group fairness and its metrics have been criticized for potentially overlooking intersectional bias caused by the combination of multiple protected features, e.g. black women. Thus researchers have introduced subgroup fairness, applying group fairness metrics on multiple protected features to ensure fairness over subgroups [20, 33]. Challenges include dealing with the potentially vast number of feature combinations and addressing issues like data sparsity, where some subgroups have very few or no observations, making it difficult to assess fairness accurately [11, 31, 32]. Fairness at the group level can still mask individual injustices [7, 17]. Therefore, individual fairness is also gaining attention. Popular metrics include:\n\u2022 Consistency [61]: Similar individuals will be treated similarly."}, {"title": "2.2 Soliciting Stakeholders' Fairness Metric Preferences", "content": "Al fairness experts need to select from numerous metrics in a specific application context, yet there is no agreement on the most appropriate metrics in specific contexts [16, 26]. This has motivated numerous studies to solicit stakeholders' metric preferences, ensuring AI fairness aligns with societal values [13, 22, 27, 48, 57].\nHowever, explaining metrics to stakeholders without any AI background is challenging due to the complex math- ematical concepts of these metrics [46]. Many studies opt to design various forms of user input to indirectly map users' metric preferences. One popular method is model selection, where stakeholders' metric preferences are inferred from their chosen models [22, 24, 50]. For example, lay people have been asked to identify which of two models is more discriminatory based on their prediction labels [50], or to compare three hypothetical ML models that adhere to different fairness metrics to determine the preferred one [22]. There are also other forms of input designed to indirectly solicit stakeholders' metric preferences [48, 52, 63]. For example, in loan scenarios, researchers collected stakeholders' fund allocation decisions to match their preferred metrics [48], or allowed lay users to label AI outcomes as \"fair\" or \"unfair\" and to adjust feature weights used for model training, thereby calculating which metric has been optimized to uncover their preferences [52]. These indirect solicitation methods reduce people's cognitive demands, yet mask the underlying metrics, potentially resulting in choices that inadequately map stakeholders' true fairness perspectives. Moreover, the user input formats and metrics, along with their application context, are often highly interrelated and need careful design and alignment, thereby lacking the flexibility to adapt to different scenarios.\nTo avoid these, some approaches aim to directly explain fairness metrics to non-Al experts through visualizations. Previous work [13] has shown the value of communicating group fairness metrics in a simplified and concrete form that allows metric exploration and preference elicitation. Other work has developed a range of UI components to assist stakeholders, e.g., domain experts [13, 44] and end users [45, 51] in exploring fairness within datasets and AI models. However, due to the complexity of the concepts behind these metrics, existing works rely on simplified textual and graphical explanations, lacking interactive designs that would allow users to delve into the details of metrics [13, 46, 48]. This may hinder stakeholders' understanding of different fairness metrics, thereby limiting their ability to provide insights or critiques on metric selection."}, {"title": "2.3 Research Gaps", "content": "In summary, the current research has three clear gaps. First, the indirect methods used to elicit metric preferences might reduce transparency and hinder stakeholders' understanding of AI fairness. More detailed explanation work, like that conducted by Cheng et al. [13], which directly interprets metrics, is necessary and should be prioritized to make fairness more comprehensible to a wider audience. Second, due to the complexity of metric explanation, current research generally involves a limited number of metrics and primarily focuses on the popular group fairness category, with little investigation into subgroup and individual fairness. We believe these limitations may not accurately reflect users' true preferences in certain scenarios. Last, current approaches focus on soliciting personal preferences without considering how to address differences between individuals.\nOur work addresses these gaps by designing an adaptive interactive system that visually communicates three fairness categories, including eight metrics, to stakeholders without requiring prior Al knowledge. We aim to uncover stakeholders' personal preferences, including their fairness threshold settings, and explore how to reconcile differences to reach a metric consensus and what consensus can be achieved."}, {"title": "3 The EARN Fairness Framework", "content": "We present the EARN (Explain, Ask, Review, Negotiate) Fairness framework to uncover a collective metric selection from stakeholders without necessitating prior Al expertise. This framework consists of two components: a stakeholder- centered EARN Fairness process and an adaptable interactive system, the Fairness Explainer and Explorer (FEE). The stakeholder-centered EARN Fairness process comprises two phases: (1) individual sessions to Explain fairness metrics and Ask for stakeholders' personal preferences on metrics and reasons; followed by (2) team sessions to collectively Review metric definitions and Negotiate a consensus on fairness metrics. FEE supports both phases of the EARN Fairness process, allowing the exploration of fairness metrics."}, {"title": "3.1 The EARN Fairness Process", "content": "3.1.1 Individual Session to Explain and Ask. Our framework begins with individual sessions, which are designed to discover each stakeholder's personal preferences, with one session per stakeholder. The Explain part is woven throughout the entire session, where each stakeholder interacts with the Fairness Explainer and Explorer (FEE) to delve into fairness metrics and model fairness. The Ask part requires stakeholders to reflect on three questions:\n\u2022 Question 1: What are the three most preferred metrics, and in what order? Given that there is no one-size- fits-all metric and selecting only one metric is insufficient [43], we ask each stakeholder to choose and rank their top three fairness metrics, producing a Top 3 Metric List. If group fairness metrics are selected, We further ask whether subgroup fairness for addressing biases across multiple features is more important, and allow them to specify which protected features are of most concern.\n\u2022 Question 2: What are the fairness thresholds for different fairness categories? Fairness thresholds quantify the boundaries of stakeholders' tolerance for unfairness, which is key to aligning AI fairness evaluation with their expectations.\n\u2022 Question 3: What are the reasons behind their choices? Stakeholders are encouraged to provide and reflect on the reasons behind their top three metric selections and threshold settings. This process aims to reveal their thought process, allowing a better understanding of their preferences.\n3.1.2 Team Session to Review and Negotiate. Our framework then shifts to a collaborative team session, marrying personal metric preferences into a team consensus among stakeholders. This session involves two key tasks: team members first collectively Review fairness metric definitions using FEE to establish a shared understanding, and then Negotiate. The team needs to accomplish two negotiation objectives. The first objective is to reach a consensus on selecting or defining a fairness metric that aligns with all team members' fairness expectations. The second objective, when group metrics are adopted, is to determine whether to focus on group fairness for a single feature or subgroup fairness for addressing biases across multiple features."}, {"title": "3.2 The Fairness Explainer and Explorer (FEE)", "content": "We designed the Fairness Explainer and Explorer (FEE) to support the EARN Fairness process. The FEE is an adaptive interactive system that explains Al fairness metrics, enabling users to explore, understand, and identify their preferred metrics. We will detail its design goals, supported metrics, and UI components, and provide a comparison of FEE with previous fairness UIs.\n3.2.1 Design Goals. Our target users are individuals with no background in AI. To help users comprehend and compare different metrics, we draw inspiration from prior research [13, 44, 51], and propose four design goals: a.) Contextual Understanding: help users develop a contextual understanding of the model by analyzing AI model predictions case- by-case, b.) Model Transparency: explaining basic AI concepts, offering model explanations and model performance information to enhance users' comprehension on AI model, c.) Diverse Explanation and Comparison: elucidating metrics in diverse styles and plain language, indicating when a metric shows the model to be unfair, presenting fairness metric results at the case level and providing intuitive comparisons across metrics, and d.) Fairness Adjustability: allowing users to question biases in original data and AI model predictions, and dynamically tracking their impact on model fairness. Moreover, our UI design and metric explanation methods are fundamentally rooted in the underlying mathematical logic of the metrics, allowing them to be adaptive and applicable across different domains.\n3.2.2 Fairness Metrics Covered in FEE. We have expanded the range of fairness metrics beyond prior work in both metric quantity and fairness category [13, 48, 50, 57, 63], introducing eight metrics covering group, subgroup, and individual fairness. Offering a broader range of fairness metrics helps meet the varied expectations and needs of stakeholders and also provides Al experts with more diverse insights into metric selection. Further, given that broader stakeholders, especially those without AI backgrounds, might have divergent understandings of fairness compared to Al experts, our fairness metrics include both widely used metrics in AI fairness practice and relatively less popular ones [26, 43, 53]. See Section 2.1 for definitions and Appendix A.1 for how we implemented these fairness metrics. All metric results are presented as percentages.\nFor group fairness, we provide five prevalent metrics in AI fairness practice: Demographic Parity [17, 35], Equal Opportunity [23], Equalized Odds [23], Predictive Equality [15], and Outcome Test [49]. We also include one less common metric, Conditional Statistical Parity [15]. Moreover, we apply these metrics to subgroups to support subgroup fairness exploration. Consistent with prior research targeting laypeople [13, 22], we represent group and subgroup fairness metric results using the maximum difference between (sub)groups.\nFor individual fairness, we used two popular individual fairness metrics: Counterfactual Fairness [35] and Consistency [61]. Counterfactual fairness is quantified by the proportion of instances where decisions remain consistent between the actual and counterfactual worlds. The Consistency score is computed based on the similarity of AI predictions among the five nearest neighbors, as per IBM's AI Fairness 360 [6].\nOur framework also allows fairness threshold settings. Group-level metric results range from 0% to 100%, indicating increasing disparities among (sub)groups. Therefore, a threshold of 0% means absolute fairness and higher values signify higher unfairness tolerance. In contrast, individual fairness metrics range from 0% to 100%, indicating the degree of fairness in treating individuals. Thus, a threshold of 100% means absolute fairness and lower values signify less unfairness tolerance."}, {"title": "5.1 What are stakeholders' personal preferences on Al fairness metrics, and why? (RQ1)", "content": "5.1.1 Preferences for Fairness Metrics. Table 2 shows all the 18 participants' preferences for fairness metrics and fairness thresholds. We first investigated preferred fairness metric categories. Individual fairness emerged as the most favored fairness metric category, chosen by 9 participants in their Top-1 metric, closely followed by subgroup fairness, chosen by 7 participants. In contrast, despite group fairness being widely favored in industry and academia for its ease of implementation [7], it only garnered support from 2 participants in this study.\nThen we delved into specific fairness metrics. Figure 5 shows the ranking distribution of metrics chosen by participants. We observed that preferences diverged among the participants' Top-1 choices. Conditional Statistical Parity was the most preferred, chosen by 7 out of 18 participants, which indicates the importance stakeholders place on non-protected features (job, savings, employment, and credit history in our credit rating scenario) in fairness assessment. It was followed by Counterfactual Fairness (6 participants), and Consistency (5 participants), indicating preferences for fairness at the individual level. It is noteworthy that Demographic Parity, the most widely used metric in existing Al fairness research [26, 43, 53], was not chosen by any participant as their Top-1 choice. Equal Opportunity and Equalized Odds, which are commonly used fairness metrics, were each selected by only one participant.\nOverall, while there were subgroups of participants that seemed very similar in their preferences, there was a lack of consensus. For example, P10, P11, P16 and separately P6, P14 had identical selections of preferred metrics in all the top three ranks but did not agree with each other. Participants did not agree on fairness metric categories, let alone fairness metrics within them, making how to measure and audit fairness problematic.\nTherefore, a strategy for choosing fairness metrics could instead leverage stakeholders' ranked preferences, especially as we observed that each metric was included in the top three rankings. To investigate what metric was most highly ranked overall, we decided to weight these rankings to investigate overall patterns of preference, with weights of 3, 2, and 1 for the Top-1, Top-2, and Top-3 choices, respectively. We found that Conditional Statistical Parity (36 points) stands out far ahead of the other metrics in terms of ranked preference, with Consistency (23 points) and Counterfactual Fairness (21 points) following behind. Other metrics, such as Equalized Odds (13 points), Equal Opportunity (12 points), Predictive Equality (5 points), Outcome Test (5 points) and Demographic Parity (2 points) were ranked a lot lower.\nLastly, we also gathered user preferences for protected features and feature combinations when applying group and subgroup fairness. The majority of participants identified age as their primary concern (8 participants), followed by gender, which was selected by seven participants. Interestingly, only three participants, all of whom were foreign residents, selected foreign worker status as their primary concern.\nNote that 14 out of 18 participants preferred subgroup fairness over group fairness. Consequently, the combined consideration of age and gender was identified by a majority of participants (7 participants). Three participants advocated for an even broader inclusivity, urging the consideration of age, gender, and foreign worker status at the same time. Less commonly selected combinations were gender and foreign worker status (2 participants), or age and foreign worker status (1 participant).\nMain finding 1: In credit prediction scenarios, lay stakeholders favored the individual-level fairness metric category, and Conditional Statistical Parity stood out as the preferred metric. Overall, lay stakeholders' preferences for fairness metrics diverged significantly from the current popular metrics in industry and academia. There was strong support for focusing on age and gender attributes and their intersection."}, {"title": "5.2 How do stakeholders negotiate to resolve personal differences in metric preferences? (RQ2)", "content": "Because stakeholders have diverse views and preferences on metrics and fairness categories, reflecting these views in \"fairer\" Al models remains challenging. In this section, we investigate how team members negotiate to resolve differences in metric preferences, identifying three distinct consensus-reaching styles without researcher input. We will detail the negotiation strategies shaped by team decision-making and leadership, and the consensus each team ultimately reached.\nIn Team 1, participants adopted mainly a majority-based voting approach to resolve their personal metric preferences. In this approach, they narrowed down options based on the frequency of preferred metrics. Team members agreed to follow the majority and abandon any minority preferences. They started out by choosing to prioritize subgroup fairness over group fairness, following a majority vote suggested by P11. Next, they eliminated the three least chosen metrics, as suggested by P8. Further, as proposed by P4, they narrowed down the selection of the specific metric to employ to the Top-1 choice, either Conditional Statistical Parity or Counterfactual Fairness. At this point, P7, P11, and P12 favored Counterfactual Fairness due to their personal Top-1 preference for individual-level fairness, while P4, P5, and P8 supported Conditional Statistical Parity, driven by their Top-1 preference for group-level fairness. This then led to in-depth discussions between team members as to the reasons behind their Top-1 choices, resurfacing what they had talked about during their earlier one-on-one sessions (Section 5.1.2). We also found that the process of discussion at this stage stimulated the exploration of the core advantages and disadvantages of fairness and ways to measure it, thus deepening participants' understanding of the metrics. For example, P7 supported Counterfactual Fairness and pointed out the drawbacks of group fairness: \"The thing is the subgroups just level up other groups, right? So if you and I have the same credit score and we apply for a credit, [. . . ], as I said, I don't get the credit but you get it because the metrics is 50% of all males good credit, only 30% of the females good credits. So we have to give more credits to females. So we have exactly the same income, but [you]'ll be prioritized because you belong to a group [...] [What] group and subgroup [metrics] generally do is to try from a government perspective, equalized. But the question is, is it fair or not?\". On the other hand, P8, a supporter of Conditional Statistical Parity, expressed concerns about individual fairness, stressing the importance of group fairness in maintaining equity between protected and non-protected groups at the societal level: \"It's maybe unfair [...] [Unless] I'm selfish, I will go with individual fairness\". Meanwhile, P11 emphasized the decision not to use Consistency due to the complexity of similarity calculations, explaining, 'Instead of neighbor because you have to explain again what this neighbor means, it is that you have the same [information]. But if the counterfactual fairness, it's just like male [or] female [...] or something like that'. This nuance wasn't fully realized during P11's individual session, yet group discussions further deepened the understanding of this metric. Therefore, after this discussion, the team agreed on a \"hybrid\" solution: using Conditional Statistical Parity with a focus on subgroup fairness first, and then using Counterfactual Fairness as a backup for individual fairness. As P8 suggested: \"[Use] Conditional Statistical Parity first and then I when I see a complaint then I would go with the Counterfactual Fairness to decide to, to judge him a second time\".\nTeam 2, similar to Team 1, also tried to discuss the reasons for their choices and then vote on them, emphasizing collective agreement to reconcile their diverging preferences. However, this discussion led to entrenched positions and inhibited negotiation. For example, initially, P1 suggested that each team member share and justify their top 3 preferred metrics in turn. Then, they checked if any team member had changed their mind after hearing the others. However, all members stuck to their initial personal choices. Then, as suggested by P17, they tried to overcome that deadlock by voting on the team's top 3 metric list. However, the team only accepted unanimous decisions and therefore this approach failed. As a final way out, the team attempted a compromise solution. P17 suggested: \"I like the idea of grouping with the Conditional Statistical Parity and then choosing an individual fairness afterward. And within same condition groups that we want to make sure each individual can be treated fairly\". All members, except P15, favored individual fairness with Counterfactual Fairness, while P15 suggested a combination of Counterfactual Fairness and Consistency. For comprehensiveness, they chose to combine both individual fairness metrics. Team 2 further prioritized subgroup fairness for Conditional Statistical Parity using a majority-based approach. Therefore, similar to Team 1, Team 2 also agreed on a \"hybrid\" solution: using Conditional Statistical Parity with a focus on subgroup fairness first, and then using Counterfactual Fairness and Consistency as backups for individual fairness.\nIn Team 3, reasons for and against fairness metrics were also discussed but they used a debate style, encouraging the presentation of arguments for and against certain fairness metrics. In the early negotiation stages, some polarization emerged similar to Team 1, but was successfully overcome through argumentation. For example, P10, P13, and P14 supported Counterfactual Fairness, while P9 and P18 favored Conditional Statistical Parity. In this team, we observed that certain team members dominated the discussions, leading to significant variations in the levels of participation among the group members. P10 presented arguments for Counterfactual Fairness while P18 supported Conditional Statistical Parity, leading two sides. The discussion primarily revolved around P18's focus on group-level legitimate factors for fairness versus P10's approach of directly excluding the impact of protected attributes for AI decision-making fairness. P10 proposed fairness as follows: \"It's like oh no matter the value of age, gender, foreign worker is changed or not, Al prediction should not be changed [...] You eliminate the bias first and then you think about employment [and other individual qualifications]\". After this perspective was accepted, Team 3, different from Team 1 and Team 2, advocated for using Counterfactual Fairness for individual fairness first, and then using Conditional Statistical Parity with a focus on subgroup fairness. Additionally, there was unanimous agreement on the importance of including at least age and gender to secure subgroup fairness in the Conditional Statistical Parity application.\nMain finding 4: Teams used different strategies to come to a consensus. Voting was frequently employed but there were differences in how to decide the outcomes of voting. Reasons for or against metrics were discussed in all teams but this can also lead to entrenchments of opinions if not carefully handled. All teams chose to overcome conflict by combining metrics in some way, ending up with compromise solutions. The consensus showed that all teams considered both subgroup fairness and individual fairness, but with different priorities."}, {"title": "6 Discussion", "content": "In this section, we will first discuss the limitations of our work and future research. Further, we will discuss the implications of our findings in the credit rating/loan scenario and the generalization of applying the EARN framework to other scenarios."}, {"title": "6.1 Limitations and Future Work", "content": "We acknowledge four limitations of our work. First, the current sample of participants in the user study was relatively small and focused solely on lay users as decision subjects without involving other stakeholder types, such as policymakers or domain experts. Future work needs to investigate how our framework could be scaled up and include multiple stakeholder types. Second, we only implemented a subset of fairness metrics predefined by Al experts. Future work could expand the range of metrics or induce custom fairness metrics from users to better reflect their understanding and expectations of fairness. Third, although we attempted to provide a comprehensive UI based on best practices, there might have been some limitations to its usability. Participants' feedback on UI design was coded into themes (in bold), see Appendix A.3.3. Although FEE received many positive comments from participants, including features such as real-time updates to fairness results (8 participants) and intuitive visualization for fairness metric explanations (6 participants) to bridge lay users' Al knowledge gap, it was cognitively challenging (8 participants), as P5 stated: \"Mind-twisting when I compare them\". Future work could concentrate on explaining the practical implications of fairness metrics (6 participants) by clearly linking calculations to real-world scenarios. Additionally, adopting simplified data visualization & explanation (3 participants) to explain metric principles and model transparency (2 participants) is crucial for understanding decision-making reasons. Last, we only tested our framework and UI in the credit rating scenario, limiting the scope of our findings. Future work should evaluate and refine our framework in diverse settings like healthcare, employment, and education to ensure its broader applicability and effectiveness in addressing fairness."}, {"title": "6.2 Fairness in the Credit Rating Scenario", "content": "Our work focused on one application domain in financial services and thus allowed us to advance our understanding of how decision subjects view fairness in this realm.\nWhether based on weighted rankings of metric preferences or the frequency of Top-1 choices, stakeholders' top 3 preferences were Conditional Statistical Parity, Counterfactual Fairness, and Consistency. This echoes findings from Nakao et al.'s study on loan scenarios [44], where they observed loan officers tended to prioritize individual fairness. Similarly, Saxena et al. [48] surveyed ordinary people in loan scenarios and also revealed that fairness metrics must account for the similarity among individuals, and thus Counterfactual Fairness and Consistency should be included.\nOur results also emphasized the importance of considering non-protected features (e.g., employment) in Al fairness for credit rating. In this domain, employment status may, in the eyes of stakeholders, unfairly affect individuals from lower socioeconomic backgrounds. However, these must be carefully balanced with legally protected features (e.g., gender), to avoid discrimination. It could be the case that stakeholders focused on Conditional Statistical Parity because it served as an ideal balance between important non-protected features and protected features.\nIn this domain, stakeholders preferred subgroup fairness over simple group fairness. This means considering multiple legal requirements for protected attributes (e.g., gender and age) and, given the stakeholder views we gathered, also integrating non-protected features. However, this will bring with it issues with data scarcity in these groups, which will make comparisons difficult.\nInterestingly, when teams negotiated the preferred fairness metric, we observed a common pattern where all teams tended to merge diverse preferences to form a more comprehensive and inclusive definition of AI fairness to resolve differences, despite using different negotiation strategies. That is, all teams agreed on a a dual focus on the absence of prejudice or favoritism toward both individuals and groups in the credit rating scenario. Our findings underscore the importance of integrating varied fairness considerations in this domain."}, {"title": "6.3 Generalizing the EARN Fairness Framework", "content": "We argue that tackling AI fairness transcends the bounds of solely technical, Al expert-driven approaches, evolving into a broader, human-centered concern. Our work demonstrates the feasibility of our approach. We will discuss how the EARN Fairness framework could be generalized to diverse contexts and scaled up to larger stakeholder groups.\n6.3.1 Explaining Fairness Metrics. As mentioned in Section 2, most research indirectly explores metrics that resonate with users' fairness perceptions [45, 48, 50, 52]. Instead, our framework shares a similar philosophy with the work of Cheng et al. [13], using the FEE interactive system to reveal the underlying logic of metrics, and thus enabling users to decide which fairness metric aligns best with their expectations. Although our FEE tool only covered a subsection of metrics, this could be easily extended to cover more and different fairness metrics, for a range of features extending beyond protected and legitimate features. Our tool also allows researchers to extend it to different application scenarios by changing datasets or incorporating other Al models' fairness outcomes.\nExplaining fairness metrics through FEE also had the effect of engaging the participants in our study to think about fairness, how fairness is defined, and what fairness means to them. For example, participants excluded metrics that defined fairness within specific groups, such as Equal Opportunity, which only ensured fairness within the subgroup that had been labeled as \"Good Credit\". Furthermore, we observed that when participants made modifications to AI predictions or ground-truth labels to explore real-time updates in fairness results, this made them \"reflect or re-think how the fairness results changed\" (P14), enhancing their understanding of the metrics. Therefore, we believe that soliciting stakeholders' metric preferences based on exposing the underlying logic offers a robust approach.\n6.3.2 Eliciting Personal Metric Preferences and Mitigating Fairness Issues. From the perspective of personal preferences, we found that stakeholders without Al expertise preferred fairness metrics significantly different from those commonly used by Al experts in industry and academia, such as demographic parity, equal opportunity, and equalized odds [3, 59]. Similarly, Nakao et al. [44] found that loan officers' preferences diverged from data scientists'. This highlights the need to involve a broader base of stakeholders in fairness metric decisions. However, even within the same type of stakeholders, in this case, decision subjects, we observed diverse understandings of fairness, not only in metrics but also in fairness categories. This finding echoes previous research [13, 36, 37].\nThis leads to the question of what to do with personal preferences once we have elicited them, in order to mitigate fairness issues. One approach might be to incorporate some simple voting for metrics and features, similar to the weighting that we have used, with the \"winning\" metric and features being the ones implemented [62]. We might also consider optimizing all metrics chosen by stakeholders. While individual and group fairness might not inherently conflict as they reflect different aspects of consistent moral and political concerns [7], conflicts may arise in practice when optimizing specific metrics [43]. In our view, these approaches relate back to attempting to solve a socio-technical problem with more technical solutions, underscoring the necessity of group negotiations to resolve differing views.\n6.3.3 Team Negotiation Methods for Metric Consensus. In our framework, we emphasized collective metric consensus to foster mutual acceptance, and our findings demonstrate that stakeholders can reach a consensus without the interference and aid of researchers.\nThe varying strategies adopted for negotiating consensus within the teams of our study indicate that robust negotiating approaches need to be developed, codified, and supported by tools and processes. For example, all teams in our study used a voting mechanism, however, sometimes this led to a 'voting deadlock'. We suggest that majority-based voting or Borda voting is integrated into this process as a key stage of consensus building."}, {"title": "7 Conclusions", "content": "In this paper, we proposed the EARN Fairness framework to uncover fairness metric consensus among stakeholders without prior Al knowledge. We applied our framework in a credit rating scenario with 18 participants to identify decision subjects' personal metric preferences with their fairness threshold settings (i.e., the acceptable level of unfairness) and then, by studying three teams of 6 participants, to reveal their team consensus on metric selection. We found that:\n\u2022 For personal preferences, the top 3 metrics most favored by individuals are Conditional Statistical Parity (for subgroup fairness), Consistency, and Counterfactual Fairness.\n\u2022 Stakeholders have varying tolerance levels for unfairness and they set the most stringent standards for individual fairness.\n\u2022 For team consensus, teams typically choose to compromise by combining multiple metrics to address differences in personal preferences. Three teams uniformly show a preference for Conditional Statistical Parity at the subgroup level, yet their choices in individual fairness metrics vary.\nWe discussed the implications of choosing fairness metrics for credit and loan scenarios, and how to extend the EARN framework to other settings. Overall, our research presents a feasible approach for engaging stakeholders in the collective selection of Al fairness metrics."}, {"title": "A Appendix", "content": "A.1 Fairness Metrics and Explanations"}]}