{"title": "EARN Fairness: Explaining, Asking, Reviewing and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders", "authors": ["LIN LUO", "YURI NAKAO", "MATHIEU CHOLLET", "HIROYA INAKOSHI", "SIMONE STUMPF"], "abstract": "Numerous fairness metrics have been proposed and employed by artificial intelligence (AI) experts to quantitatively measure bias and define fairness in AI models. Recognizing the need to accommodate stakeholders' diverse fairness understandings, efforts are underway to solicit their input. However, conveying Al fairness metrics to stakeholders without Al expertise, capturing their personal preferences, and seeking a collective consensus remain challenging and underexplored. To bridge this gap, we propose a new framework, EARN Fairness, which facilitates collective metric decisions among stakeholders without requiring AI expertise. The framework features an adaptable interactive system and a stakeholder-centered EARN Fairness process to Explain fairness metrics, Ask stakeholders' personal metric preferences, Review metrics collectively, and Negotiate a consensus on metric selection. To gather empirical results, we applied the framework to a credit rating scenario and conducted a user study involving 18 decision subjects without Al knowledge. We identify their personal metric preferences and their acceptable level of unfairness in individual sessions. Subsequently, we uncovered how they reached metric consensus in team sessions. Our work shows that the EARN Fairness framework enables stakeholders to express personal preferences and reach consensus, providing practical guidance for implementing human-centered AI fairness in high-risk contexts. Through this approach, we aim to harmonize fairness expectations of diverse stakeholders, fostering more equitable and inclusive Al fairness.", "sections": [{"title": "1 Introduction", "content": "Ensuring artificial intelligence (AI) fairness is crucial for ethical integrity, legal compliance, and human trust [36], particularly in high-stakes scenarios such as healthcare [14], finance [34], and university admission [42]. AI fairness experts have made significant efforts to define fairness quantitatively through fairness metrics [43, 53]. These metrics help assess the fairness of AI systems, audit their compliance with specific fairness requirements, and guide unfairness mitigation [26]. Currently, there are over 20 popular fairness metrics, categorized into three primary categories - group fairness, subgroup fairness, and individual fairness to measure whether AI systems treat groups, subgroups, and individuals fairly, respectively [12, 43, 53].\nHowever, there are three primary issues when applying fairness metrics to assess Al fairness. Firstly, choosing the right metrics becomes problematic due to the lack of clear, unified guidelines for metric selection, especially in the absence of a one-size-fits-all metric and the insufficiency of using a single metric [11, 22, 41, 43]. Secondly, Al fairness metric selection, often led by AI experts lacking stakeholder input, risks overly depending on these experts' perspectives and disregards fairness as a societal value with diverse interpretations among stakeholders [2, 26, 43, 53]. This could lead to Al models deemed technically \"fair\" by AI experts but not accepted in practice because they fail to reflect stakeholders' views on fairness [37, 56]. Lastly, even when personal fairness perspectives are collected from stakeholders,\n\u2022 RQ1: What are stakeholders' personal preferences on AI fairness metrics, and why?\n\u2022 RQ2: How do stakeholders negotiate to resolve personal differences in metric preferences?\n\u2022 We provide a framework for Al practitioners and researchers to engage stakeholders in reaching a collective agreement on metric selection, without Al knowledge being required. This framework can address metric selection by shifting from a technical focus to a stakeholder-centered approach, promoting broader acceptance.\n\u2022 We offer AI fairness practitioners empirical results on choosing metrics for a credit rating context that reflect decision subjects' views, including personal metric preferences and team consensus.\n\u2022 We identify strategies that stakeholders employ to reach consensus, which could be used as a blueprint for scaling up the representation of stakeholder views.\n\u2022 We put forward design recommendations for user interfaces (UIs) and fairness explanation methods, enhancing the ability of stakeholders with no Al knowledge to explore Al fairness.\nThis paper is structured as follows. Section 2 reviews related work on AI fairness metrics, on soliciting stakeholders' metric preferences, and highlights the research gaps. Next, we introduce the EARN Fairness framework in Section 3."}, {"title": "2 Related Work", "content": "First, we explore how AI fairness has been conceptualized and measured across different metrics. We then review existing approaches to solicit stakeholders' fairness metrics preferences, and we conclude this section by identifying research gaps."}, {"title": "2.1 Al Fairness Metrics", "content": "Researchers have proposed various metrics to quantify AI fairness, focusing on three main categories: group fairness and subgroup fairness, which address fairness at the group level, and individual fairness, focusing on fairness at the individual level [43, 53]. Each category has its distinct advantages and limitations.\nGroup fairness prioritizes equal treatment among groups, usually defined by protected characteristics enshrined by legislation, such as age or gender, or sensitive features based on contexts, e.g., foreign worker status. Group fairness is easily comprehensible to policymakers and the public, and it is also feasible to implement computationally [7, 50]. Some prominent group fairness metrics include [26, 43]:\n\u2022 Demographic Parity [35] (also known as Statistical Parity [17]): Both protected and unprotected group members have the same probability of being classified in the positive category.\n\u2022 Equal Opportunity [23]: Both protected and unprotected groups have the same probability that a subject in the positive class is predicted as a positive class.\n\u2022 Predictive Equality [15]: Both protected and unprotected groups have the same probability that a subject in the negative class is predicted as a positive class.\n\u2022 Equalized Odds [23]: Simultaneously satisfy both Equal Opportunity and Predictive Equality.\n\u2022 Outcome Test [49]: Both protected and unprotected groups have the same probability that a subject with a positive prediction genuinely belongs to the positive class.\n\u2022 Conditional Statistical Parity [15]: Both protected and unprotected groups are equally likely to be predicted as a positive class, under conditions controlled by a set of legitimate features L. Legitimate features are those directly relevant and considered appropriate for influencing decision-making outcomes. For instance, in a credit rating scenario, legitimate features are factors that affect an applicant's creditworthiness, such as the applicant's credit history and employment status [53].\nHowever, group fairness and its metrics have been criticized for potentially overlooking intersectional bias caused by the combination of multiple protected features, e.g. black women. Thus researchers have introduced subgroup fairness, applying group fairness metrics on multiple protected features to ensure fairness over subgroups [20, 33]. Challenges include dealing with the potentially vast number of feature combinations and addressing issues like data sparsity, where some subgroups have very few or no observations, making it difficult to assess fairness accurately [11, 31, 32]. Fairness at the group level can still mask individual injustices [7, 17]. Therefore, individual fairness is also gaining attention. Popular metrics include:\n\u2022 Consistency [61]: Similar individuals will be treated similarly."}, {"title": "2.2 Soliciting Stakeholders' Fairness Metric Preferences", "content": "Al fairness experts need to select from numerous metrics in a specific application context, yet there is no agreement on the most appropriate metrics in specific contexts [16, 26]. This has motivated numerous studies to solicit stakeholders' metric preferences, ensuring AI fairness aligns with societal values [13, 22, 27, 48, 57].\nHowever, explaining metrics to stakeholders without any AI background is challenging due to the complex math-ematical concepts of these metrics [46]. Many studies opt to design various forms of user input to indirectly map users' metric preferences. One popular method is model selection, where stakeholders' metric preferences are inferred from their chosen models [22, 24, 50]. For example, lay people have been asked to identify which of two models is more discriminatory based on their prediction labels [50], or to compare three hypothetical ML models that adhere to different fairness metrics to determine the preferred one [22]. There are also other forms of input designed to indirectly solicit stakeholders' metric preferences [48, 52, 63]. For example, in loan scenarios, researchers collected stakeholders' fund allocation decisions to match their preferred metrics [48], or allowed lay users to label AI outcomes as \"fair\" or \"unfair\" and to adjust feature weights used for model training, thereby calculating which metric has been optimized to uncover their preferences [52]. These indirect solicitation methods reduce people's cognitive demands, yet mask the underlying metrics, potentially resulting in choices that inadequately map stakeholders' true fairness perspectives. Moreover, the user input formats and metrics, along with their application context, are often highly interrelated and need careful design and alignment, thereby lacking the flexibility to adapt to different scenarios.\nTo avoid these, some approaches aim to directly explain fairness metrics to non-Al experts through visualizations. Previous work [13] has shown the value of communicating group fairness metrics in a simplified and concrete form that allows metric exploration and preference elicitation. Other work has developed a range of UI components to assist stakeholders, e.g., domain experts [13, 44] and end users [45, 51] in exploring fairness within datasets and AI models. However, due to the complexity of the concepts behind these metrics, existing works rely on simplified textual and graphical explanations, lacking interactive designs that would allow users to delve into the details of metrics [13, 46, 48]. This may hinder stakeholders' understanding of different fairness metrics, thereby limiting their ability to provide insights or critiques on metric selection."}, {"title": "2.3 Research Gaps", "content": "In summary, the current research has three clear gaps. First, the indirect methods used to elicit metric preferences might reduce transparency and hinder stakeholders' understanding of AI fairness. More detailed explanation work, like that conducted by Cheng et al. [13], which directly interprets metrics, is necessary and should be prioritized to make fairness more comprehensible to a wider audience. Second, due to the complexity of metric explanation, current research generally involves a limited number of metrics and primarily focuses on the popular group fairness category, with little investigation into subgroup and individual fairness. We believe these limitations may not accurately reflect users' true preferences in certain scenarios. Last, current approaches focus on soliciting personal preferences without considering how to address differences between individuals.\nOur work addresses these gaps by designing an adaptive interactive system that visually communicates three fairness categories, including eight metrics, to stakeholders without requiring prior Al knowledge. We aim to uncover stakeholders' personal preferences, including their fairness threshold settings, and explore how to reconcile differences to reach a metric consensus and what consensus can be achieved."}, {"title": "3 The EARN Fairness Framework", "content": "We present the EARN (Explain, Ask, Review, Negotiate) Fairness framework to uncover a collective metric selection from stakeholders without necessitating prior Al expertise. This framework consists of two components: a stakeholder-centered EARN Fairness process and an adaptable interactive system, the Fairness Explainer and Explorer (FEE). The stakeholder-centered EARN Fairness process comprises two phases: (1) individual sessions to Explain fairness metrics and Ask for stakeholders' personal preferences on metrics and reasons; followed by (2) team sessions to collectively Review metric definitions and Negotiate a consensus on fairness metrics. FEE supports both phases of the EARN Fairness process, allowing the exploration of fairness metrics."}, {"title": "3.1 The EARN Fairness Process", "content": "Our framework begins with individual sessions, which are designed to discover each stakeholder's personal preferences, with one session per stakeholder. The Explain part is woven\n\u2022 Question 1: What are the three most preferred metrics, and in what order? Given that there is no one-size-fits-all metric and selecting only one metric is insufficient [43], we ask each stakeholder to choose and rank their top three fairness metrics, producing a Top 3 Metric List. If group fairness metrics are selected, We further ask whether subgroup fairness for addressing biases across multiple features is more important, and allow them to specify which protected features are of most concern.\n\u2022 Question 2: What are the fairness thresholds for different fairness categories? Fairness thresholds quantify the boundaries of stakeholders' tolerance for unfairness, which is key to aligning AI fairness evaluation with their expectations.\n\u2022 Question 3: What are the reasons behind their choices? Stakeholders are encouraged to provide and reflect on the reasons behind their top three metric selections and threshold settings. This process aims to reveal their thought process, allowing a better understanding of their preferences."}, {"title": "3.1.2 Team Session to Review and Negotiate", "content": "Our framework then shifts to a collaborative team session, marrying personal metric preferences into a team consensus among stakeholders. This session involves two key tasks: team members first collectively Review fairness metric definitions using FEE to establish a shared understanding, and then Negotiate. The team needs to accomplish two negotiation objectives. The first objective is to reach a consensus on selecting or defining a fairness metric that aligns with all team members' fairness expectations. The second objective, when group metrics are adopted, is to determine whether to focus on group fairness for a single feature or subgroup fairness for addressing biases across multiple features."}, {"title": "3.2 The Fairness Explainer and Explorer (FEE)", "content": "We designed the Fairness Explainer and Explorer (FEE) to support the EARN Fairness process. The FEE is an adaptive interactive system that explains Al fairness metrics, enabling users to explore, understand, and identify their preferred metrics. We will detail its design goals, supported metrics, and UI components, and provide a comparison of FEE with previous fairness UIs."}, {"title": "3.2.1 Design Goals", "content": "Our target users are individuals with no background in AI. To help users comprehend and compare different metrics, we draw inspiration from prior research [13, 44, 51], and propose four design goals: a.) Contextual Understanding: help users develop a contextual understanding of the model by analyzing AI model predictions case-by-case, b.) Model Transparency: explaining basic AI concepts, offering model explanations and model performance information to enhance users' comprehension on AI model, c.) Diverse Explanation and Comparison: elucidating metrics in diverse styles and plain language, indicating when a metric shows the model to be unfair, presenting fairness metric results at the case level and providing intuitive comparisons across metrics, and d.) Fairness Adjustability: allowing users to question biases in original data and AI model predictions, and dynamically tracking their impact on model fairness. Moreover, our UI design and metric explanation methods are fundamentally rooted in the underlying mathematical logic of the metrics, allowing them to be adaptive and applicable across different domains."}, {"title": "3.2.2 Fairness Metrics Covered in FEE", "content": "We have expanded the range of fairness metrics beyond prior work in both metric quantity and fairness category [13, 48, 50, 57, 63], introducing eight metrics covering group, subgroup, and individual fairness. Offering a broader range of fairness metrics helps meet the varied expectations and needs of"}, {"title": "4 User Study in a Credit Rating Scenario", "content": "We applied the EARN Fairness framework in a credit rating scenario, where 18 stakeholders without a background in Al participated in a user study."}, {"title": "4.1 Data and Model", "content": "We applied the German Credit Dataset [25] due to its popularity in AI fairness research and providing a familiar high-risk banking scenario. It comprises 1000 instances, each with 20 attributes (13 categorical and 7 numerical features) for classifying \"Good\" or \"Bad\" credit, with no missing data. Our study investigated fairness on three features: age and gender, recognized as protected characteristics under law regulations 1, and foreign worker status as a sensitive feature utilized by many fairness research [21, 30]. They were processed as follows 1) Age: we converted age into a binary feature, with age<25 as the protected group; 2) Gender: we extracted this from the \"personal status and gender\" attribute, and selected \"female\" as the protected group; 3) Foreign worker: we chose \"yes\" as the protected (sensitive) group. We provided four non-protected features as possible legitimate features in a credit rating scenario: job, savings, employment, and credit history [53] for measuring Conditional Statistical Parity.\nWe employed a logistic regression classifier. Logistic regression's simplicity and interpretability make it ideal for improving model understanding, particularly for non-experts [5]. Previous research has also effectively employed logistic regression to study fairness in this dataset [26]. We used all 1000 instances, training this model with 5-fold cross-validation to ensure sufficient test instances for subgroup fairness analysis. To balance model accuracy and fairness, our model achieves a 0.76 accuracy on 200 test instances."}, {"title": "4.2 Participants", "content": "We recruited 18 participants for the study through public and university social media channels, all of whom are decision subjects impacted by AI decisions and have no AI background. Participants were aged 18 to 47 (mean = 28), with diverse genders, professions, nationalities, and ethnicities (Appendix A.2). Each participant received a \u00a320 Amazon Voucher as"}, {"title": "4.3 Procedure", "content": "Each participant attended two one-hour sessions: an individual session and a team session, both facilitated by a researcher. During the individual session, each participant first filled out a pre-study questionnaire to gather demographic data and then watched a video tutorial on how to use FEE. They then followed the EARN Fairness framework to complete Explain and Ask. Throughout this session, participants were encouraged to think aloud their thoughts, including but not limited to their perspectives on fairness metrics, as well as provide feedback on our interactive system design and user experience.\nAfter these individual sessions with 18 participants, a researcher analyzed their Top 3 Metric Ranking Lists and formed three teams, each consisting of six participants. To ensure diversity and thus necessitate negotiations, we employed hierarchical sampling to assign participants with similar preferences into different teams. Each participant's metric preference was mapped to an 8-dimensional vector with each dimension representing a specific metric, enabling us to cluster participants by similar metric choices. The final teams were established as follows: Team 1 (P4, P5, P7, P8, P11,P12), Team 2 (P1,P2,P6,P15,P16,P17), and Team 3 (P3, P9,P10,P13,P14,P18) 2.\nFinally, we followed the framework to conduct team sessions for each team to complete Review and Negotiate. Members were provided with hard copies outlining negotiation objectives and engaged in free-form negotiation without interference from researchers. During the negotiation process, members were free to use our interactive system. The negotiation results should be unanimously approved by all team members. Researchers aimed to ensure a harmonious negotiation atmosphere and offered alternative collective decision-making methods when needed, such as the Borda voting method due to its simplicity and robustness [29, 40]."}, {"title": "4.4 Data Collection and Analysis", "content": "We collected participants' Top 3 Metric Ranking Lists and fairness thresholds, along with audio recordings and the researcher's observation notes. We used descriptive statistics to analyze the Top 3 Metric Ranking Lists, reporting the raw frequencies for each metric within the top 3 ranks, and to calculate the mean values and variances for fairness thresholds. Further, we transcribed approximately 20 hours of recordings from all 18 participants. We conducted a thematic analysis [8] to generate insights into participants' preferences on fairness metrics and thresholds, the reasons behind them, and user feedback. Our research team met weekly to iterate over the themes and coding, reviewing and validating them. After 6 rounds of iteration, we finalized the final codebook (Appendix A.3).\nFor team sessions, we collected negotiation results on metric preferences along with corresponding video and audio recordings. Approximately 4 hours of audio recordings were transcribed, aided by videos for participant ID identification during transcription. For all three teams' negotiation results, we analyzed each team's metric consensus and conducted a thematic analysis of the transcribed data with 4 rounds of iterations to identify the negotiation strategies employed."}, {"title": "5 Results", "content": "In this section, we tackle each of our research questions in turn by evidencing these from data gathered during our user study."}, {"title": "5.1 What are stakeholders' personal preferences on Al fairness metrics, and why? (RQ1)", "content": "We first investigated preferred fairness metric categories. Individual fairness emerged as the most favored fairness metric category, chosen by 9 participants in their Top-1 metric, closely followed by subgroup fairness, chosen by 7 participants. In contrast, despite group fairness being widely favored in industry and academia for its ease of implementation [7], it only garnered support from 2 participants in this study.\nThen we delved into specific fairness metrics. Figure 5 shows the ranking distribution of metrics chosen by participants. We observed that preferences diverged among the participants' Top-1 choices. Conditional Statistical Parity was the most preferred, chosen by 7 out of 18 participants, which indicates the importance stakeholders place on non-protected features (job, savings, employment, and credit history in our credit rating scenario) in fairness assessment. It was followed by Counterfactual Fairness (6 participants), and Consistency (5 participants), indicating preferences for fairness at the individual level. It is noteworthy that Demographic Parity, the most widely used metric in existing Al fairness research [26, 43, 53], was not chosen by any participant as their Top-1 choice. Equal Opportunity and Equalized Odds, which are commonly used fairness metrics, were each selected by only one participant.\nOverall, while there were subgroups of participants that seemed very similar in their preferences, there was a lack of consensus. For example, P10, P11, P16 and separately P6, P14 had identical selections of preferred metrics in all the top three ranks but did not agree with each other. Participants did not agree on fairness metric categories, let alone fairness metrics within them, making how to measure and audit fairness problematic.\nTherefore, a strategy for choosing fairness metrics could instead leverage stakeholders' ranked preferences, especially as we observed that each metric was included in the top three rankings. To investigate what metric was most highly ranked overall, we decided to weight these rankings to investigate overall patterns of preference, with weights of 3, 2, and 1 for the Top-1, Top-2, and Top-3 choices, respectively. We found that Conditional Statistical Parity (36 points) stands out far ahead of the other metrics in terms of ranked preference, with Consistency (23 points) and Counterfactual Fairness (21 points) following behind. Other metrics, such as Equalized Odds (13 points), Equal Opportunity (12 points), Predictive Equality (5 points), Outcome Test (5 points) and Demographic Parity (2 points) were ranked a lot lower.\nLastly, we also gathered user preferences for protected features and feature combinations when applying group and subgroup fairness. The majority of participants identified age as their primary concern (8 participants), followed by gender, which was selected by seven participants. Interestingly, only three participants, all of whom were foreign residents, selected foreign worker status as their primary concern.\nNote that 14 out of 18 participants preferred subgroup fairness over group fairness. Consequently, the combined consideration of age and gender was identified by a majority of participants (7 participants). Three participants advocated for an even broader inclusivity, urging the consideration of age, gender, and foreign worker status at the same time. Less commonly selected combinations were gender and foreign worker status (2 participants), or age and foreign worker status (1 participant)."}, {"title": "5.1.2 Reasons for Metric Preferences and Concerns", "content": "We analyzed participants' think-aloud data from individual sessions using a codebook (Appendix A.3.1) and highlight the code in this section in bold text. We expected these reasons to be reprised and discussed during group negotiations as arguments for a metric consensus."}, {"title": "5.1.3 Fairness Thresholds and Reasons", "content": "We show how participants set these thresholds in Table 2. We also analyzed the reasons for choosing these thresholds during individual sessions; our codebook can be found in Appendix A.3.2.\nWe observed that there is a wide variability of thresholds, from 0% to 30% in group fairness, 0% to 30% in subgroup fairness, and 80% to 100% in individual fairness. Further, no two participants set the exact same thresholds for the three fairness categories. In group and subgroup fairness settings, only two participants chose a 0% threshold, while four opted for a 100% threshold in individual fairness. One approach for agreeing on these could be to average them. The mean for group fairness was 9.28% (sd=7.51%), subgroup fairness was 13.00% (sd=9.96%), and individual fairness was 92.44% (sd=8.53%). We found that participants imposed the strictest thresholds for individual fairness; within the context of group-level fairness, we observed that thresholds for subgroup fairness were set more leniently than group fairness 3. We can note that on average participants set thresholds much higher than expected by law.\nMost participants were willing to leave some room for unfairness, as it was deemed impossible to achieve in practice. As P14 remarked, \"We don't need to be too absolute, ... Experts or Al are not 100% accurate, that absolute perfection in fairness and accuracy is unattainable\". Only a small number of participants pursued perfect fairness, reflecting a zero-tolerance attitude towards bias. One reason for the higher thresholds for subgroup fairness was that more features were being considered, thereby allowing for more leeway. As P10 stated, \"Since you're taking more factors into account when assessing fairness, like more individual factors, you can maybe increase this a bit more, like [...] more leeway\". Strangely, participants did not extend this reasoning to individual fairness, even though even more features are considered in this fairness metric. This reflects higher fairness expectations or lower unfairness tolerance due to the majority of participants preferring the individual fairness category."}, {"title": "5.2 How do stakeholders negotiate to resolve personal differences in metric preferences? (RQ2)", "content": "Because stakeholders have diverse views and preferences on metrics and fairness categories, reflecting these views in \"fairer\" Al models remains challenging. In this section, we investigate how team members negotiate to resolve differences in metric preferences, identifying three distinct consensus-reaching styles without researcher input. We will detail the negotiation strategies shaped by team decision-making and leadership, and the consensus each team ultimately reached.\nIn Team 1, participants adopted mainly a majority-based voting approach to resolve their personal metric preferences. In this approach, they narrowed down options based on the frequency of preferred metrics. Team members agreed to follow the majority and abandon any minority preferences. They started out by choosing to prioritize subgroup fairness over group fairness, following a majority vote suggested by P11. Next, they eliminated the three least chosen metrics, as suggested by P8. Further, as proposed by P4, they narrowed down the selection of the specific metric to employ to the Top-1 choice, either Conditional Statistical Parity or Counterfactual Fairness. At this point, P7, P11, and P12 favored Counterfactual Fairness due to their personal Top-1 preference for individual-level fairness, while P4, P5, and P8 supported Conditional Statistical Parity, driven by their Top-1 preference for group-level fairness. This then led to in-depth discussions between team members as to the reasons behind their Top-1 choices, resurfacing what they had talked about during their earlier one-on-one sessions (Section 5.1.2). We also found that the process of discussion at this stage stimulated the exploration of the core advantages and disadvantages of fairness and ways to measure it, thus deepening participants' understanding of the metrics. For example, P7 supported Counterfactual Fairness and pointed out the drawbacks of group fairness: \"The thing is the subgroups just level up other groups, right? So if you and I have the same credit score and we apply for a credit, [. . . ], as I said, I don't get the credit but you get it because the metrics is 50% of all males good credit, only 30% of the females good credits. So we have to give more credits to females. So we have exactly the same income, but [you]'ll be prioritized because you belong to a group [...] [What] group and subgroup [metrics] generally do is to try from a government perspective, equalized. But the question is, is it fair or not?\". On the other hand, P8, a supporter of Conditional Statistical Parity, expressed concerns about individual fairness, stressing the importance of group fairness in maintaining equity between protected and non-protected groups at the societal level: \"It's maybe unfair [...] [Unless] I'm selfish, I will go with individual fairness\". Meanwhile, P11 emphasized the decision not to use Consistency due to the complexity of similarity calculations, explaining, 'Instead of neighbor because you have to explain again what this neighbor means, it is that you have the same [information]. But if the counterfactual fairness, it's just like male [or] female [...] or something like that'. This nuance wasn't fully realized during P11's individual session, yet group discussions further deepened the understanding of this metric. Therefore, after this discussion, the team agreed on a \"hybrid\" solution: using Conditional Statistical Parity with a focus on subgroup fairness first, and then using Counterfactual Fairness as a backup for individual fairness. As P8 suggested: \"[Use] Conditional Statistical Parity first and then I when I see a complaint then I would go with the Counterfactual Fairness to decide to, to judge him a second time\".\nTeam 2, similar to Team 1, also tried to discuss the reasons for their choices and then vote on them, emphasizing collective agreement to reconcile their diverging preferences. However, this discussion led to entrenched positions and inhibited negotiation. For example, initially, P1 suggested that each team member share and justify their top 3"}, {"title": "6 Discussion", "content": "In this section, we will first discuss the limitations of our work and future research. Further, we will discuss the implications of our findings in the credit rating/loan scenario and the generalization of applying the EARN framework to other scenarios."}, {"title": "6.1 Limitations and Future Work", "content": "We acknowledge four limitations of our work. First, the current sample of participants in the user study was relatively small and focused solely on lay users as decision subjects without involving other stakeholder types, such as policymakers or domain experts. Future work needs to investigate how our framework could be scaled up and include multiple"}, {"title": "6.2 Fairness in the Credit Rating Scenario", "content": "Our work focused on one application domain in financial services and thus allowed us to advance our understanding of how decision subjects view fairness in this realm.\nWhether based on weighted rankings of metric preferences or the frequency of Top-1 choices, stakeholders' top 3 preferences were Conditional Statistical Parity, Counterfactual Fairness, and Consistency. This echoes findings from Nakao et al.'s study on loan scenarios [44], where they observed loan officers tended to prioritize individual fairness. Similarly, Saxena et al. [48] surveyed ordinary people in loan scenarios and also revealed that fairness metrics must account for the similarity among individuals, and thus Counterfactual Fairness and Consistency should be included.\nOur results also emphasized the importance of considering non-protected features (e.g., employment) in Al fairness for credit rating. In this domain, employment status may, in the eyes of stakeholders, unfairly affect individuals from lower socioeconomic backgrounds. However, these must be carefully balanced with legally protected features (e.g., gender), to avoid discrimination. It could be the case that stakeholders focused on Conditional Statistical Parity because it served as an ideal balance between important non-protected features and protected features.\nIn this domain, stakeholders preferred subgroup fairness over simple group fairness. This means considering multiple legal requirements for protected attributes (e.g., gender and age) and, given the stakeholder views we gathered, also integrating non-protected features. However, this will bring with it issues with data scarcity in these groups, which will make comparisons difficult.\nInterestingly, when teams negotiated the preferred fairness metric, we observed a common pattern where all teams tended to merge diverse preferences to form a more comprehensive and inclusive definition of AI fairness to resolve differences, despite using different negotiation strategies. That is, all teams agreed on a a dual focus on the absence of prejudice or favoritism toward both individuals and groups in the credit rating scenario. Our findings underscore the importance of integrating varied fairness considerations in this domain."}, {"title": "6.3 Generalizing the EARN Fairness Framework", "content": "We argue that tackling AI fairness transcends the bounds of solely technical, Al expert-driven approaches, evolving into a broader, human-centered concern. Our work demonstrates the feasibility of our approach. We will discuss how the EARN Fairness framework could be generalized to diverse contexts and scaled up to larger stakeholder groups."}, {"title": "6.3.1 Explaining Fairness Metrics", "content": "As mentioned in Section 2, most research indirectly explores metrics that resonate with users' fairness perceptions [45, 48, 50, 52]. Instead, our framework shares a similar philosophy with the work of Cheng et al. [13], using the FEE interactive system to reveal the underlying logic of metrics, and thus enabling users to decide which fairness metric aligns best with their expectations. Although our FEE tool only covered a subsection of metrics, this could be easily extended to cover more and different fairness metrics, for a range of features extending beyond protected and legitimate features. Our tool also allows researchers to extend it to different application scenarios by changing datasets or incorporating other Al models' fairness outcomes.\nExplaining fairness metrics through FEE also had the effect of engaging the participants in our study to think about fairness, how fairness is defined, and what fairness means to them. For example, participants excluded metrics that defined fairness within specific groups, such as Equal Opportunity, which only ensured fairness within the subgroup that had been labeled as \"Good Credit\". Furthermore, we observed that when participants made modifications to AI predictions or ground-truth labels to explore real-time updates in fairness results, this made them \"reflect or re-think how the fairness results changed\" (P14), enhancing their understanding of the metrics. Therefore, we believe that soliciting stakeholders' metric preferences based on exposing the underlying logic offers a robust approach."}, {"title": "6.3.2 Eliciting Personal Metric Preferences and Mitigating Fairness Issues", "content": "From the perspective of personal preferences, we found that stakeholders without Al expertise preferred fairness metrics significantly different from those commonly used by Al experts in industry and academia, such as demographic parity, equal opportunity, and equalized odds [3, 59]. Similarly, Nakao et al. [44] found that loan officers' preferences diverged from data scientists'. This highlights the need to involve a broader base of stakeholders in fairness metric decisions. However, even within the same type of stakeholders, in this case, decision subjects, we observed diverse understandings of fairness, not only in metrics but also in fairness categories. This finding echoes previous research [13, 36, 37].\nThis leads to the question of what to do with personal preferences once we have elicited them, in order to mitigate fairness issues. One approach might be to incorporate some simple voting for metrics and features, similar to the weighting that we have used, with the \"winning\" metric and features being the ones implemented [62]. We might also consider optimizing all metrics chosen by stakeholders. While individual and group fairness might not inherently conflict as they reflect different aspects of consistent moral and political concerns [7], conflicts may arise in practice when optimizing specific metrics [43]. In our view, these approaches relate back to attempting to solve a socio-technical problem with more technical solutions, underscoring the necessity of group negotiations to resolve differing views."}, {"title": "6.3.3 Team Negotiation Methods for Metric Consensus", "content": "In our framework, we emphasized collective metric consensus to foster mutual acceptance, and our findings demonstrate that stakeholders can reach a consensus without the interference and aid of researchers.\nThe varying strategies adopted for negotiating consensus within the teams of our study indicate that robust negotiating approaches need to be developed, codified, and supported by tools and processes. For example, all teams in our study used a voting mechanism, however, sometimes this led to a 'voting deadlock'. We suggest that majority-based voting or Borda voting is integrated into this process as a key stage of consensus building."}, {"title": "7 Conclusions", "content": "In this paper, we proposed the EARN Fairness framework to uncover fairness metric consensus among stakeholders without prior Al knowledge. We applied our framework"}]}