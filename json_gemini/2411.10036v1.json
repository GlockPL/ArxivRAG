{"title": "Rethinking Normalization Strategies and Convolutional Kernels for Multimodal Image Fusion", "authors": ["Dan He", "Guofen Wang", "Weisheng Li", "Yucheng Shu", "Wenbo Li", "Lijian Yang", "Yuping Huang", "Feiyan Li"], "abstract": "Multimodal image fusion (MMIF) aims to integrate information from different modalities to obtain a comprehensive image, aiding downstream tasks. However, existing methods tend to prioritize natural image fusion and focus on information complementary and network training strategies. They ignore the essential distinction between natural and medical image fusion and the influence of underlying components. This paper dissects the significant differences between the two tasks regarding fusion goals, statistical properties, and data distribution. Based on this, we rethink the suitability of the normalization strategy and convolutional kernels for end-to-end MMIF. Specifically, this paper proposes a mixture of instance normalization and group normalization to preserve sample independence and reinforce intrinsic feature correlation. This strategy promotes the potential of enriching feature maps, thus boosting fusion performance. To this end, we further introduce the large kernel convolution, effectively expanding receptive fields and enhancing the preservation of image detail. Moreover, the proposed multipath adaptive fusion module recalibrates the decoder input with features of various scales and receptive fields, ensuring the transmission of crucial information. Extensive experiments demonstrate that our method exhibits state-of-the-art performance in multiple fusion tasks and significantly improves downstream applications. The code is available at https://github.com/HeDan-11/LKC-FUNet.", "sections": [{"title": "1. Introduction", "content": "In reality, single-modal images capture limited information and each modality contains essentially different information [9]. Image fusion technology fully integrates the complementary information of different modalities to generate a comprehensive representation of the image [52].\nIt is widely used for scene information enhancement or restoration [23, 24, 51]. Moreover, object detection [59, 60] and semantic segmentation [25, 27] can also benefit from clearer representations of scenes and objects in the fused images [61]. Fusion tasks include infrared and visible image fusion (IVIF) [18, 47], medical image fusion (MIF) [29, 34], multi-exposure image fusion [19], and so on, where IVIF and MIF are very similar and challenging subcategories [40]. Specifically, in IVIF, fused images can avoid the drawbacks of visible (VIS) images that are sensitive to illumination conditions and infrared (IR) images that are noisy and low resolution [24, 63]. Similarly, MIF generates images that comprehensively reflect the information of tissues, organs, and metabolism to assist medical diagnosis and improve reliability [55].\nIn recent years, many deep learning methods [32, 38, 50, 56, 58, 64] have been developed to address challenges in image fusion. Common encoder-decoder models demonstrating promising results utilize convolutional neural networks (CNNs) [54, 57] and Transfomer [21, 33, 65] to extract features and reconstruct images. It has achieved relatively satisfactory convergence performance in IVIF. However, it has focused more on the improvement of objective indicators in MIF, without sufficiently considering the needs of MIF itself. In fact, there are the following significant differences between IVIF and MIF: 1) Fusion goals: IVIF emphasizes the overall structure and salient objects. Whereas in MIF, details and structural information must be fully retained, and tiny textures may indicate important lesion information. 2) Data distribution. Fig. 1a shows that IR and VIS images are Gaussian distributed, while medical images are highly sparsely distributed. 3) Statistical properties. Medical images possess more complex details, and their statistical values, such as average gradient (AG), spatial frequency (SF), and standard deviation (SD), are far higher than those of IR and VIS images. 4) Intra-task inter-sample differences. Fig. la shows that the pixel distributions of VIS images differ significantly between daytime and nighttime scenes. Meanwhile, computed tomography (CT) images have more high-brightness regions compared to magnetic resonance imaging (MRI) and positron emission tomography (PET) images, which might result in mutual interference during the fusion process.\nSome methods [5, 63] are trained to achieve only unified fusion on IR and VIS images. These methods attain excellent performance in IVIF but severe sacrifice of details in medical images, clearly overlooking differences 1-3. SDNet [54] and CDDFuse [61] are trained with different parameters to deal with the two tasks. Nevertheless, the emphasis continues to be on IVIF, while for MIF, only its model parameters are adjusted. This neglects the limited effective feature and the strict fusion requirements for detail retention in MIF and fails to solve the conflict between simultaneously retaining the high-brightness regions in CT/functional images and the detail information in MRI.\nWe aim to rethink the fitness of the underlying components in this discrepancy case and design suitable models to resolve fusion conflicts and reduce inter-sample interference during training.\nFirstly, early fusion frameworks [8, 57] are derived from advanced vision tasks, in which the employed batch normalization (BN) seeks to normalize the feature distribution across the entire batch; but this approach ignored the independence of samples, leading to data smoothing. This has less impact on IVIF which emphasizes structure preservation. However, for medical images that are highly sparsely distributed and require strict detail retention, the interaction between samples will also cause a conflict between regions of high brightness and detail retention. While some methods [5, 12, 14, 25] forgo normalization to preserve sample independence, they fail to account for the inherent properties of images and the intrinsic relationships between features, resulting in limited improvements in fusion performance. Secondly, large kernel convolution (LKC) can capture spatial information within a wider range and is crucial for preserving image structure and details. However, its exploration in image fusion is limited, which may be related to its performance bottleneck. As depicted in Fig. 1b, when BN is applied, the interaction among samples results in data smoothing, further reducing effective features. Here, the large receptive fields of LKC have a limited or even hindering effect on detail retention. Not using normalization fails to offer better image features and makes it hard to raise the upper limit of fusion performance. Finally, when using UNet [35], simple skip connections do not consider the relative importance of feature maps in different paths during the fusion process. This may result in crucial features being neglected.\nTo address the above issues, we focus on exploring an efficient UNet to cope with the challenges of substantial differences among tasks and limited feature extraction. We employ a mixture of IN and GN to fully consider the sample independence, image properties, and intrinsic connections among features. This strategy enhances the generation of rich feature maps while more effectively preserving the distinctive attributes of source image pairs, as illustrated in Fig. 1b. The strategy elevates the upper bound of fusion performance, at this point, the application of LKC enhances detail retention capabilities. Finally, the feature maps in the input decoder are recalibrated by combining spatial, channel attention, and bidirectional interactions. We fully consider four critical differences, thereby guaranteeing not only outstanding performance in MIF, but also full applicability to IVIF. Our contributions are summarized as follows:\n\u2022 A UNet with LKC is proposed to achieve multimodal image fusion (MMIF), namely LKC-FUNet, including IVIF and MIF.\n\u2022 Rethink the impact of normalization and LKC on image fusion. Verify the inappropriateness of BN in MMIF. Mixing IN and GN preserves image properties and is well suited for highly sparsely distributed fusion tasks. Under the above strategy, LKC enlarges the \"effective receptive field\" to better preserve the detailed information and significantly improve the fusion performance.\n\u2022 A multipath adaptive fusion module is designed for feature fusion across different receptive fields and at various scales. Spatial-channel dual-attention feature maps, bidirectional interactions, and recalibration are used to provide more comprehensive inputs to the decoder.\n\u2022 Our method significantly improves multiple metrics in both tasks and achieves breakthroughs in MIF visualization. It is also shown to facilitate downstream multimodal object detection and semantic segmentation."}, {"title": "2. Related work", "content": "Multimodal image fusion. Deep learning methods in MMIF fall into four categories: encoder-decoder models, adversarial models, task- and text-driven models, and other models. Early coder-decoder models [17, 57] utilize pretrained autoencoders for feature extraction and image reconstruction, emphasizing the design of fusion rules. Later on, it develops into an end-to-end fusion model to get rid of the limitation of manual fusion rules. In this case, it mainly combines CNN and Transformer [33, 41] to enrich the feature representations, and facilitates the modal information interaction through the feature fusion module. Generative Adversarial Network (GAN)-based methods [32, 60] define image fusion as a game between a generator and a discriminator, where texture details are preserved and salient targets are highlighted in the confrontation. However, the instability of training is not effectively addressed. In recent years, studies have focused on the adaptability of fused images to downstream tasks. CDDFuse [61] and EMMA [63] measure the fusion performance in terms of the accuracy of object detection and image segmentation. And some studies [25, 45, 55, 59, 60] enhance the performance by guiding the fusion with semantic information of the downstream tasks. Currently, contrast learning [24, 58] and diffusion model [38, 53, 62] are also introduced into fusion field. In addition, researchers have designed various loss functions [9, 50] based on image fusion characteristics, which provide substantial guidance for network training. Although deep learning-based unified methods have demonstrated strong fusion performance on IVIF, they are often simply migrated to MIF.\nNormalization. The normalization technique unifies the data scale by scaling and panning to avoid the feature value magnitude difference affecting the model training, which makes the neural network have better generalizability [7, 36]. Common BN [13] accelerates training and improves stability. IN [44] and its variant [11] are suitable for image style conversion and synthesis, preserving details of the original image. GN [49] combines the advantages of BN and IN to perform well in image-intensive tasks. BCN [15] adaptively combines channel and batch information to improve classification performance. Although normalization has a significant role in advanced vision tasks, it is rarely applied in image fusion. The fusion results are mainly dependent on the source image pairs and the use of IN and GN is more helpful in preserving the image properties.\nLarge kernel convolution. LKC obtains more context-rich convolutional features by expanding the receptive field, thus excelling in image classification [22] and segmentation [26]. Among them, LR-Net [10] and ConvNeXt [30] can benefit from 7\u00d77 kernel. However, LR-Net suffers from performance degradation at 9\u00d79. Performance bottlenecks and parameter spikes limit the kernel to smaller values. To address this challenge, RepLKNet [6] achieves advanced performance by reparameterizing the 31\u00d731 deep convolution. And SLaK [28] extends the kernel size to 51\u00d751 by kernel decomposition and sparse grouping. Although LKC has facilitated performance improvements in many areas, its importance for image fusion remains elusive."}, {"title": "3. Method", "content": "3.1. Overview\nThe proposed method is a UNet architecture as shown in Fig. 2. We splice the source image pairs into the input model to completely avoid the design of inter-modal fusion rules. The basic modules are the initial block (InitBolck), LKCBlock, and Large kernel deep convolutional block (LKDCBlock). They are designed according to the following principles: enhance the image properties in the early stage, strengthen the correlation between features in the late stage, and maintain the independence between samples throughout. Finally, a multipath adaptive fusion module (MPAFM) is used to recalibrate the decoder inputs.\n3.2. Basic modules\nInitBolck. We use IN and LKC with 15\u00d715 as the initial block. Facing two modal image splicing inputs with very different statistical properties, IN targetedly normalizes the properties of each image to significantly reduce the intrinsic differences between modalities, while preserving more detailed features. On the other hand, LKC is more capable of capturing a wide range of spatial information, strengthening the preservation of image structure and contrast. Considering the computational efficiency and the intrinsic relationship between features, GN is adopted for the other modules.\nDuring the fusion process, different regions have different feature requirements, e.g., features should be consistent in the background, while regions with complex structures and details should be as unique as possible. Therefore, we compute the feature map consistency in chunks. This reflects the redundancy within the region and corresponds to the structure of the original map. From Fig. 3, it is reasonable that the consistency of the background regions under BN and IN is high. Moreover, the redundancy of complex regions in IN is significantly lower. This indicates that IN combined with LKC generates a richer feature map, which is beneficial for maintaining the image structure and details.\nLKCBlock. Consists of two Blocks containing GN, ReLU, optional Dropout, and convolutional layers. The convolution kernels at different stages are set to 15, 7, 5, and 5 to accommodate the feature extraction needs at different resolutions. Larger convolutional kernels are used for global information capture at high resolution, which helps in understanding the overall structure of the image. While smaller convolutional kernels are used for attention to details at low resolution, enhancing the model's perception of local features. The final residual connection provides a stable gradient flow and preserves the necessary information.\nLKDCBlock. Considering the efficiency of the model, we use LKDCBlock [6] combined with LKCBlock. It is worth noting that the grouping of GN in LKDCBlock is set to 1. This change is based on the property of deepwise convolution, in which convolution kernels independently operate on corresponding feature maps. At this point, grouping the GN may result in weak connections between features and amplification of certain features.\n3.3. Multipath adaptive fusion module\nSimple skip connections in UNet do not consider the relative importance of feature maps in different paths during the fusion process, which may result in some important features being overlooked [2, 16, 46]. Therefore, this section proposes the MPAFM to process the feature maps of the corresponding layers of the encoder and decoder, as shown in Fig. 4. The aim is to suppress unimportant features to minimize artifacts while progressively capturing detailed and structural information for the image at different scales and different receptive fields.\nAttention feature map. Obtain an attention feature map that integrates spatial and channel information. First, channel attention $C$ is used to process the encoder's feature $e$ to reflect the importance of each channel. Spatial attention $S$ is used to process the decoder's feature $d$ to capture the important regions. And the weight map that combines the two provides more comprehensive information guidance. Further, the weight map is refined into $A$ and $B$ using the weight refinement block $W$. They act on different input feature maps to enhance important features more precisely. The formula is as follows:\n$A, B = Split(W(C(e) + S(d)))$\n$Catt = A \\odot e$\n$datt = B \\odot d$\nwhere $\\odot$ denotes the dot product operation and $Split$ indicates the division operation.\nBidirectional interaction. Realizing the information interaction of the coder-decoder's attentional feature maps to facilitate complementary feature fusion at different receptive fields and different scales.\n$f = \\tau(e_{att}) \\oplus d_{att} + \\sigma(d_{att}) \\odot e_{att}$\nwhere $\\sigma$ is the sigmoid function.\nRecalibration block R. To generate a pixel attention map that is consistent with the dimensionality of the input image and recalibrate the features of the input decoder. It is ensured that the input feature maps are effectively augmented with details, context-aware information, etc.\n$X = R(f)$\n3.4. Loss function\nThe loss function of the fusion network consists of a mixture of structure level, intensity level, and gradient level defined as follows:\n$L_{total} = L_{ssim} + L_{int} + L_{grad}$\nwhere $L_{ssim} = 1-SSIM(I_A, I_X, I_F)$. SSIM is the structural similarity [48]. In the intensity loss, we want to focus on high-brightness regions, calculated as follows:\n$L_{int} = \\frac{1}{H W} ||I_F - max(I_A, I_B)||_1$\nwhere $I_A$ and $I_B$ are the source images, $I$ is the luminance channel of $I_B$, and $I_F$ is the fused image. $|| ||_1$ denotes the $L_1$ norm and $max(\\cdot)$ represents the element-wise maximum operation. In the gradient loss, the fused image should be consistent with the strongest details in the source image, i.e:\n$L_{grad} = \\frac{1}{H W} || |\\nabla I_F| - max (|\\nabla I_A|, |\\nabla I_B|) ||_1$\nwhere $\\nabla$ indicates the gradient operator used for texture information measurement within an image."}, {"title": "4. Experiment", "content": "4.1. Medical image fusion\n4.1.1. Setup\nDatasets. We selected three medical image combinations from the Harvard Medical School website [1] for MIF experiments, including MRI-CT, MRI-PET, and MRI-single-photon emission computed tomography (SPECT) image pairs. We trained on 40 pairs of datasets containing all modalities and then tested on 50 pairs of MRI-CT, MRI-PET, and MRI-SPECT images respectively. All images are aligned and have a size of 256\u00d7256 pixels.\nMetrics. We quantitatively analyzed the fusion results using six metrics: SD, AG, SF, sum of the correlations of differences (SCD), visual information fidelity for fusion (VIFF), and structural similarity index measure (SSIM). Higher metrics indicate better fusion performance. Detailed information on these metrics can be found in [31].\nComparison methods. We compare the fusion results with five state-of-the-art (SOTA) methods: CDDFuse [61], \u0395\u039c\u039c\u0391 [63], MMDRFuse [5], GeSeNet [20] and FATFusion [39]. It is worth noting that GeSeNet and FATFusion are only applied to MIF.\nImplementation details. The experimental equipment is a server equipped with four NVIDIA GeForce RTX 3090 GPUs under the PyTorch framework. The images are randomly cropped into 64\u00d764 patches with a batch size of 32 during training. We train the network for 1000 epochs using the Adam optimizer with an initial learning rate of 1e-4.\n4.1.2. Comparison with SOTA methods\nQualitative comparison. Visual comparisons are shown in Fig. 5, containing five representative pairs of medical images. It is obvious that severe detail weakening occurs in CDDFuse, EMMA, MMDRFuse, and GeseNet, while FATFusion completely neglects the preservation of high-brightness regions. Our method not only preserves the details in MRI completely, but also restores the cranial and calcified regions in CT images, and the radiotracer distribution in functional images, which can provide more comprehensive diagnostic information.\nQuantitative Comparison. Tab. 1 shows objective results for six metrics. Our method demonstrated excellent performance in almost all metrics. The slightly lower AG and SF resulting from MRI-CT fusion preserving a more complete cranial region is acceptable. In summary, the fusion results obtained by our method are more in line with the needs of MIF.\n4.1.3. Ablation experiment\nWe conducted ablation studies on three medical datasets to verify the proposed method, and the results are presented in Fig. 1b and Tab. 2. Obviously, the performance of models with BN is limited and even shows a regressive tendency\nwhen combined with LKC. IN in the InitBlock is essential for maintaining image properties and then significantly enhancing performance when combined with LKC. MPAFM effectively transmits key features by recalibrating decoder inputs. In summary, IN and GN enhance the performance potential, and LKC and MPAFM facilitate the model to reach the upper limit.\n4.2. Infrared and visible image fusion\n4.2.1. Setup\nDatasets. We perform comparative experiments on three mainstream datasets: MSRS [37], M\u00b3FD[60] and TNO[42]. We trained the model on 1083 pairs of images from MSRS and then evaluated the performance on 361 pairs of test data. In addition, the models are applied to the M\u00b3FD and TNO datasets without fine-tuning to verify the generalization performance.\nSOTA methods. We compare the fusion results with seven SOAT methods: CDDFuse [61], EMMA [63], MMDRFuse [5], LRRFNet [18], SegMIF [25], DCINN [47] and SHIP [64]. It is worth noting that the latter four methods are only applied to IVIF. The training strategy and evaluation metrics are the same as for MIF.\n4.2.2. Comparison with SOTA methods\nThe qualitative comparisons are presented in Fig. 6, again containing five representative pairs of images. It is evident that our method not only clearly highlights objects in dark areas, but also objects in high-intensity areas. This uniqueness enhances our understanding of the depicted scene. Moreover, as shown in Tab. 3, our method attains the most optimal evaluation metrics in all datasets, confirming its applicability to various environmental conditions and object classes. In summary, the full experimental results demonstrate the ability of LKC-FUNet to simultaneously preserve the integrity of source image features and generate information-rich fused images.\n4.2.3. Downstream IVIF applications\nIn this section, IR images, VIS images, and fused images of SOTA methods are applied to object detection and semantic segmentation to examine the benefits of information fusion for downstream tasks.\nObject detection. We conduct experiments on the M\u00b3FD dataset [60], which contains 4,200 pairs of IR/VIS images and is divided into training/validation/testing sets in a ratio of 8:1:1. The six categories are people, cars, buses, motorcycles, trucks, and lights. We chose yolov5 [43] as the detector and mAP@0.5 to evaluate the detection performance. During training, the epoch, batch size, optimizer, and initial learning rate are set to 1,000, 128, SGD, and 1e-2 respectively. As shown in Tab. 4, LKC-FUNet has the best detection performance, which indicates that fusing VIS information and highlighting hard-to-observe objects can improve detection accuracy.\nSemantic segmentation. We perform semantic segmentation on the MSRS dataset [37], and the division follows [37]. Nine labels are background, bumps, color cones, guardrails, guardrails, curves, bicycles, people, car parking, and cars. We choose DeeplabV3 [4] as the backbone, and the intersection-union ratio (IoU) as the evaluation metric. During training, the epoch, batch size, optimizer, and initial learning rate during training are set to 500, 4, SGD, and 1e-3, respectively. The segmentation results are shown in Tab. 5. LKC-UNet better integrates the edge and contour information in the source image, which enhances the model's ability to perceive object boundaries and makes the segmentation more accurate.\n4.3. Fusion efficiency\nThis section compares the inference time of SOTA methods on two datasets with different resolutions: 256\u00d7256 for MRI and CT images, and 640\u00d7480 for IR and VIS images in MSRS. Due to the differences in data processing and testing strategies of the different methods, we only calculate the time from calling the model to obtaining the fusion results to ensure a fair measurement. In addition, the rankings of the six metrics are presented in the form of histograms to comprehensively analyze the fusion performance and computational efficiency.\nAs shown in Fig. 7, our approach is slightly behind MMDRFuse and LRRNet in inference time, but they underperform in performance. In contrast, our method achieves a better trade-off between the two. It is worth noting that CDDFuse resolution decreases but inference time increases due to its two-stage testing process on medical images.\n4.4. Discussion\nWe briefly show the visualization of the partial IVIF method on two tasks. Fig. 8 shows that training only on IR-VIS images makes it difficult to retain the complex details of medical images. Whereas training on medical images only enhances the edges and textures of the fused images, there may be problems such as retaining too much IR image noise. Therefore, it is difficult to achieve unified multimodal image fusion by considering the data training model for only one task. Follow-up work will further consider how to balance the data differences between the two tasks to achieve a unified fusion method."}, {"title": "5. Conclusion", "content": "In this paper, we have analyzed the differences between medical and natural image fusion, revealing the specific fusion functions of normalization methods and demonstrating the effectiveness of large convolutional kernels. Accordingly, we can significantly enhance fusion performance using the proposed LKC-FUNet, whose basis modules comprise of merely two straightforward operations. Experiments show that our method achieves SOTA performance in both visual and objective evaluation metrics, and can boost the effectiveness of downstream tasks."}]}