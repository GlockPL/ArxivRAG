{"title": "Ensured: Explanations for Decreasing the Epistemic Uncertainty in Predictions", "authors": ["Helena L\u00f6fstr\u00f6m", "Tuwe L\u00f6fstr\u00f6m", "Johan Hallberg Szabadvarya"], "abstract": "This paper addresses a significant gap in explainable AI: the necessity of interpreting epistemic uncertainty in model explanations. Although current methods mainly focus on explaining predictions, with some including uncertainty, they fail to provide guidance on how to reduce the inherent uncertainty in these predictions. To overcome this challenge, we introduce new types of explanations that specifically target epistemic uncertainty. These include ensured explanations, which highlight feature modifications that can reduce uncertainty, and categorisation of uncertain explanations counter-potential, semi-potential, and super-potential which explore alternative scenarios. Our work emphasises that epistemic uncertainty adds a crucial dimension to explanation quality, demanding evaluation based not only on prediction probability but also on uncertainty reduction. We introduce a new metric, ensured ranking, designed to help users identify the most reliable explanations by balancing trade-offs between uncertainty, probability, and competing alternative explanations. Furthermore, we extend the Calibrated Explanations method, incorporating tools that visualise how changes in feature values impact epistemic uncertainty. This enhancement provides deeper insights into model behaviour, promoting increased interpretability and appropriate trust in scenarios involving uncertain predictions.", "sections": [{"title": "1. Introduction", "content": "Decisions in critical contexts, such as criminal justice [1] or medicine [2], require a well-founded basis for decision-making. Today, this is often achieved by incorporating Artificial Intelligence (AI) using Machine Learning (ML) models trained on extensive historical data. Although highly accurate, these models are not inherently objective or infallible. They are a result of identified patterns in the input data and, therefore, dependent on both the input and the algorithm itself.\nThe output of ML models often consists of a single output from the trained model, indicating, e.g., a likely house price or the probable diagnosis of a patient. To understand, critically inspect, and get support for decisions, it is essential that those affected can understand the reasons behind the output. In other words, human users require and have the right to demand explanations behind the decisions of these models. Several countries have also highlighted the critical aspects of transparency and explanations in such a system [3, 4].\nExplanations can take various forms when presented to users [5], such as based on examples similar to the instance to be explained [6], contributions of features [7, 8, 9],\npixels in images [10], or words in texts [11]. The explanations are generated to answer the questions of why, how, and what if [12], to help the user identify when to trust and not trust the model, and to provide the possibilities to make high-quality decisions\n[13, 14, 15, 16].\nExplanations can also be of different types, depending on what they are intended\nto explain [17, 18]. Factual explanations try to answer the question of why the model predicts a certain outcome, whereas counter-factual explanations reveal the minimal\nchanges to the feature values which cause a change in the prediction [19, 20, 21],\ne.g., what to change to get a loan when the model recommends a reject. Other types of explanations include semi-factuals [22], representing maximal changes that can be\nmade without changing prediction, and super-factuals [23], representing changes that increase belief in the predicted outcome. Counter-, semi-, and super-factuals all explore alternative outcomes when changing feature values in the data. The common denominator of the explanation types is their focus on explaining the outcome of the\nprediction. In other words, existing explanation types present how (changes to) feature values affect the prediction or outcome.\nHowever, recently [24, 25, 26, 27, 28, 29, 30, 31] uncertainty has been highlighted as critical in explaining predictions. A model may present a prediction with a probability of 85%, and simultaneously be highly uncertain that this is the actual value. Only presenting the probability to the user can in such a situation give the impression of a trustworthy prediction, which may have caused another decision if the user knew about\nthe level of uncertainty. [32] showed that users rank the prediction uncertainty as more important than the actual outcome in an AI model, and [33] highlights user uncertainty as one of four (information overload, time pressure, and complexity) stress factors that affect decision quality, while [25, 34] point to uncertainty as a form of transparency.\nDue to this change in focus, new explanation methods have recently been developed\nthat reveal the uncertainty in the predictions. In SkiNet [35] and [36] the authors obtain Bayesian versions of LIME and KernelSHAP, called BayesLIME and BayesSHAP that include uncertainty in the form of credible intervals. In both ConformaSight [37] and Calibrated Explanations [27] the authors use the Conformal Prediction framework to gain statistically guaranteed confidence sets to estimate the level of uncertainty.\nAlthough showing the uncertainty in the predictions, today's explanations still focus on the prediction probability and outcome.\nWith uncertainty, predictions achieve an additional dimension to explain; possible increase and decrease of uncertainty. There is no explanation type to catch these situations. Existing explanation methods could be said to explain a factual situation of the included uncertainty, in the sense that they show feature values that cause the actual level of uncertainty. Left to explain is, similar to the counterfactual situation, when users want to understand what would cause a move from the current level of uncertainty, i.e., to get ensured explanations.\nDiversity (to generate several distinct explanations) and proximity (closeness to the original datapoint) are two essential characteristics for alternative explanations [21, 19,\n38, 39] which are also highly relevant to ensured explanations. With the possibility to generate a large amount of explanations, it is essential to be able to identify the\nmost efficient ones. This paper explores several promising metrics and methods to"}, {"title": "2. Background", "content": "Within eXplainable Artificial Intelligence (XAI), there are generally two approaches:\neither developing models that are inherently interpretable and transparent or employing\npost-hoc methods to explain black box models. In post-hoc explanation techniques,\nsimplified and interpretable models are constructed to uncover the relationships between feature values and the model's predictions. The explanations are either local,"}, {"title": "2.1. Post-Hoc Explanation Methods", "content": "focusing on explaining a single prediction, or global, focusing on explaining the behaviour of an entire model [17, 18].\nThere are two distinct strategies for explaining model predictions (see Figure 1),\nfactual explanations, where a feature value directly influences the prediction outcome,\nand alternative explanations (such as, e.g., counter-factuals), exploring the potential impact on predictions when altering the values of a feature [19, 20, 21]. Importantly, alternative explanations are intrinsically local. They are particularly human-friendly,\nmirroring how human reasoning operates [17].\nWhen considering probabilistic explanations, there exist different types (also called modal narratives [23]) of explanations focusing on the prediction probability:\n\u2022 Factual: The explanation is factual, as it explains the current situation, showing\nhow each feature value directly influences the prediction outcome [40].\n\u2022 Counter-factual: In a counter-factual explanation, the minimal modifications that cause a change in prediction of features are highlighted, e.g. going from a probability above 0.5 for the actual prediction to a probability below 0.5, effectively predicting a different outcome [19, 20, 21].\n\u2022 Super-factual: The super-factual explanation focuses on the constraining factors to prevent the counter-factual explanations could happen [41, 23]. In other\nwords, super-factual explanations help identify changes that increase the probability of the predicted class.\n\u2022 Semi-factual: The semi-factual explanation shows the maximum feature modification that does not cause a change of the prediction, i.e. the possible maximum feature changes that do not alter the prediction [42, 43, 22, 44].\nFactual explanations are directly applicable to regression as well, whereas counter- factual, super-factual and semi-factual do not have any clear-cut definition in a regression context. Assuming higher predictions are better, a counter-factual could be defined as any explanation resulting in a lower prediction, and a super-factual would then be any explanation resulting in a higher prediction. If lower is better, the reasoning can simply be reversed, with counter-factuals for higher predictions and super-factuals for lower. Semi-factuals would not fill any meaningful use in this scenario. However, taking this reasoning one step further, we can assume a threshold (similar to 0.5 in a probabilistic setting) and define counter-factuals, super-factuals, and semi-factuals analogously as for probabilities above. Semi-factuals would represent any explanation indicating outcomes between the original prediction and the threshold, counter-factuals would represent any explanation beyond the threshold and super-factuals would represent any explanation above (or below) the prediction moving further away from the threshold.\nIn the remainder of this paper, the main focus will be on explaining probabilistic predictions, even if standard regression may be touched upon occasionally."}, {"title": "2.2. Calibration and Uncertainty Quantification", "content": "Making decisions based on accurate information is essential for effective decision-making, emphasising the requirements of well-calibrated predictive models with guarantees.\nConformal Prediction (CP) [45] is a model-agnostic framework that generates prediction regions with guaranteed coverage. Errors occur when the true value lies out-\nside these regions. Somewhat simplified, conformal predictors remain valid under the assumption of exchangeability, maintaining a long-term error rate of e. Conformal regression (CR) offers prediction intervals with user-specified coverage guarantees, while conformal predictive systems (CPS) [46] generate a cumulative conformal predictive distribution (CPD). These CPDs allow querying for intervals with guaranteed coverage, similar to CR but with more flexibility. Intervals are determined by percentiles in the distribution; for example, a symmetric interval with 90% coverage can be obtained using the 5th and 95th percentiles. In addition, CPDs can be queried to determine the\nprobability that the actual value falls below a user-defined threshold, which corresponds\nto the percentile of that threshold within the distribution.\nIn classification, the emphasis typically shifts to ensuring that the probability estimates provided by the classifier are well-calibrated, defined as follows:\n$P(c\\ |\\ P_c) \\approx P_c,$\nwhere \\(P_c\\) denotes the probability estimate for the class label c. A well-calibrated model ensures that the predicted probabilities align closely with the actual observed predictions. For example, when a model assigns a probability estimate of 0.9 to an instance, the true accuracy of all such instances should be approximately 90%.\nIt is widely recognised that many predictive models generate poorly calibrated probability estimates [47]. To correct this, an external calibration technique can be\napplied using a separate set of the labelled dataset, the calibration set, to revise the\npredicted probabilities and improve the calibration.\nWithin the Conformal Prediction framework, Venn [48] and Venn-Abers (VA) [49]\npredictors are used to generate multiprobabilistic predictors as confidence-based probability intervals. Venn prediction works by employing a Venn taxonomy, which groups calibration data to estimate probabilities. The probability estimate for a test instance is determined by the relative frequency of each class among the calibration instances within the same category (including the test instance). Designing an appropriate Venn taxonomy can be complex, which is where VA predictors offer an advantage.\nVenn-Abers Calibration automates taxonomy optimisation by leveraging isotonic\nregression, producing dynamic probability intervals for binary classification tasks. VA"}, {"title": "2.3. Uncertainty Estimation in Explanations", "content": "outputs one probability estimate for each of the possible class labels and one of the\nprobabilities is a perfectly calibrated probability estimate. Since the instance must\nbelong to one of these, the true probability must be either one or the other. Although\nthe true class label is unknown, the width and placement of the interval provide valuable insights. A narrower interval implies greater confidence in the prediction, while a wider interval reflects more uncertainty. To make the probability estimate more practical, especially for the positive class, regularising the interval is a common approach used to get a single probability estimate.\nTo construct a VA predictor for a test object \\(x_{n+1}\\), we define the training set as\n\\(Z = \\{z_1,..., z_n\\}\\), where n = l + q. Each instance \\(z_i = (x_i, y_i)\\) includes an object \\(x_i\\)\n(with feature set F) and its corresponding label \\(y_i\\). Typically, a separate calibration set\nis needed, which is why the training set is divided into a proper training set \\(Z_l\\) with l\ninstances, and a calibration set \\(Z_q = \\{z_1, . . ., z_q\\}\\). A scoring classifier is then trained on\n\\(Z_l\\) to calculate the scores s for \\( \\{x_1,..., x_q, x_{n+1}\\} \\). The score s is derived as the positive\nclass probability estimate from a classifier h. The steps of inductive VA prediction are\nas follows:\n1. Use \\(\\{(s_1, y_1), ..., (s_q, y_q), (s_{n+1}, y_{n+1} = 0)\\} \\) to derive the isotonic calibrator \\(g_0\\) and use \\(\\{(s_1, y_1), ..., (s_q, y_q), (s_{n+1}, y_{n+1} = 1)\\} \\) to derive the isotonic calibrator \\(g_1\\).\n2. The probability interval for \\(y_{n+1} = 1\\) is defined as \\([g_0(s_{n+1}), g_1(s_{n+1})]\\) (hereafter\nreferred to as \\([P_{low}, P_{high}]\\), representing the lower and upper bounds of the inter-\nval).\n3. The regularised probability estimate for \\(y_{n+1} = 1\\), minimising the log loss [49],\ncan be defined as:\n\\(P = \\frac{P_{high}}{1 - P_{low} + P_{high}}\\)\nTo summarise: VA provides a calibrated (regularised) probability estimate P, along\nwith a probability interval defined by its lower and upper bounds, \\([P_{low}, P_{high}]\\).\nConformal Predictive Systems (CPS) generate Conformal Predictive Distribu-"}, {"title": "2.4. Calibrated Explanations", "content": "Uncertainty is an inherent aspect of all decisions and a key component of machine\nlearning methodology. In machine learning, uncertainty can be found both in the data\nand in the model, resulting in predictions attached to a varying level of uncertainty.\nEstimates of these different types of uncertainty can offer critical insights into both the reliability of the data, the model, and its predictions [50, 51]:\n\u2022 Aleatoric (statistical) uncertainty represents the noise inherent in the data. It\naffects the spread of probability distributions (for probabilistic outcomes) and\npredictions (for regression). This uncertainty is irreducible because it reflects\nlimitations in the data generation process. Incorporating calibration ensures accurate aleatoric uncertainty.\n\u2022 Epistemic (systematic) uncertainty arises from the model's lack of knowledge\ndue to limited training data or insufficient complexity. It affects the confidence\nof the model in its output when it encounters unfamiliar or out-of-distribution\ndata. Unlike aleatoric uncertainty, epistemic uncertainty is reducible - it can be\nminimised by gathering more data, improving the model architecture, or refining features.\nThere are different approaches to quantifying uncertainty in models. The possibilities of producing probability intervals in VA for each prediction can be used to estimate the model uncertainty. The width of the intervals can be translated into the level of uncertainty. Another approach is to use accuracy-rejection curves, which illustrate the accuracy of a predictor based on the percentage of rejections and the variance to represent uncertainty [52].\nIn [53], different techniques for calibrating uncertainty information are compared\nand VA is identified as the preferred method for complementing predictions with a measure of uncertainty. The authors point out the simplicity of the VA approach, making it preferable to the other calibration methods discussed in the study. Uncertainty is also\naddressed in [36], where the authors develop a new method based on Naive Bayes. The authors in [54] use Venn predictors to quantify the uncertainty of rule-based explanations and highlight uncertainty quantification for additive feature importance methods as an attractive focus for research."}, {"title": "3. Ensured Explanations - Conceptual Framework", "content": "Calibrated Explanations is a recently released local explanation method for classi- fication [27] and regression [55] designed to enhance both the interpretability of model predictions and the quantification of uncertainty. The method provides calibrated explanations for both predictions and feature importance by quantifying aleatoric and epistemic uncertainty.\nBy providing estimates for both aleatoric and epistemic uncertainty, Calibrated Ex- planations offers a comprehensive understanding of predictions, both in terms of accuracy and confidence. This is particularly valuable in high-stakes environments where model reliability and interpretability are essential, such as in healthcare, finance, and autonomous systems.\nCalibrated Explanations produce instance-based (local) explanations, and a factual explanation is composed of a calibrated prediction from the underlying model accompanied by an uncertainty interval and a collection of factual feature rules, each\ncomposed of a feature weight with an uncertainty interval and a factual condition, cov-"}, {"title": "3.1. Probabilities and Uncertainty", "content": "ering that feature's instance value. It also enables exploring alternative explanations to provide insights about how changes to one or several features affect the calibrated\nprediction and uncertainty interval. Alternative explanations only contain a collection of alternative feature rules, each composed of a prediction estimate with an uncertainty interval and an alternative condition, covering alternative instance values for\nthe feature. For classification, the explanation explains the calibrated probability estimate (and its level of uncertainty) for the positive class. For regression, there are two alternative use cases:\n1. The standard regression explanation explains a calibrated estimate of the prediction from the regressor, with a confidence interval covering the true target with a user-assigned level of confidence.\n2. The thresholded explanation explains the calibrated probability estimate (and its\nlevel of uncertainty) for the calibrated estimate of the prediction being below a user-given threshold.\nCalibrated Explanations assume the existence of a predictive model h, trained using the proper training set \\(Z_l\\), outputting a numeric value when predicting an object h(\\(x_i\\)). For classification, the model is a scoring classifier, producing probability estimates for the positive class. For regression, it is an ordinary regressor predicting the expected value.\nThe core of Calibrated Explanations relies on a numeric estimate and a lower and an upper bound defining an uncertainty interval for the numeric estimate. As a consequence, the algorithm is agnostic to whether it is a classification or regression problem, as long as the numeric estimate and the lower and upper bound can be defined.\nSome of the fundamental aspects when explaining the outcome of a model is to be able to answer the questions of why the model has come to its conclusions and why not another outcome. When adding uncertainty to the explanations, new questions arise, such as how certain are you?. These question are answered in existing explanation methods that include uncertainty (see, e.g. [25, 36, 27]). However, none of these approaches addresses the question of how to reduce the uncertainty in a prediction, how to get more certain or for short how to ensure?\nMost existing explanation frameworks operate in the probability space (classification) or the numeric prediction space (regression) and are evaluated in that dimension. However, when uncertainty is introduced, another dimension needs to be considered. This raises the question of how to incorporate uncertainty when ranking explanations\nor feature weights. In the following, uncertainty U is defined as the difference between the upper and lower bounds (U = \\(\u00a2_{high}\\) - \\(\u00a2_{low}\\)). First, we will examine the general case\nof predictions and uncertainty, before we consider how this impacts explanations.\nProbabilistic predictions refer to classification but also to regression with thresholds (P(y < t)). Probability \\(P\\) is assumed to refer to the predicted class (which for thresholded regression is \\(P\\) = max(P(y < t),P(y \u2265 t))) if nothing else is mentioned. When considering predictions and uncertainty in probability space, both the probability and uncertainty is bounded to [0, 1]. In a simplistic view of traditional ML, where uncertainty is not considered, a construed uncertainty interval would consist of a lower and upper bound equal to the prediction, resulting in zero uncertainty. When adding an uncertainty interval for the predicted probability, the maximum uncertainty covers the entire probability range, resulting in an uncertainty of 1. Since the probability is the\nmean (or regularised mean) of the lower and upper bound, maximum uncertainty would\nresult in a probability of 0.5. Furthermore, when both the lower and upper bound are\neither below or above 0.5, the prediction is certain to be the positive class (if above 0.5)\nor the negative class (if below 0.5). An interval covering e.g. [0,0.5] would have an"}, {"title": "3.2. Explanations and Uncertainty", "content": "uncertainty of 0.5 and a mean probability of 0.25. However, when the interval covers\n0.5, it is not clear which class to predict, since both classes are possible.\nWhen considering explanations, we notice that there is some common ground be- tween explanations of probabilistic predictions and numeric predictions (regression). When discussing the common ground between explanations of both numeric and probabilistic predictions, Explanations of outcome will be used. Figure 3 shows the com- mon ground between all outputs when adding an uncertainty component. The basic\nform of explanations can be divided into factual explanations, providing insights into\nwhy an instance is predicted as it is, and exploring alternative explanations, providing"}, {"title": "3.3. Exploring Alternative Explanations in Probability Space", "content": "insights into what would have happened with alternative inputs. In both these forms of explanations, an uncertainty component can be added, which Calibrated Explanations provide an example of.\nFor factual explanations, there is no need to delve deeper, since it provides an explanation of the prediction. Thus, the predictions of individual instances can be mapped\nto the framework described in Section 3.1 for probabilistic explanations.\nThe existing explanation types (such as counter-, semi-, and super-factuals) only\nhandle the prediction outcome. For example, if a doctor is trying to decide if a patient\nhas cancer or not and uses an AI model, it is critical that the doctor can get an answer\nif and how it is possible to decrease the uncertainty. In other words, a new type of\nexplanation is needed that outlines how to achieve predictions with a higher level of\ncertainty. When looking solely at possible changes in uncertainty, the factual explanation (see Figure 3) covers an explanation answering the question of the reasons behind the existing outcome (as it is) including the factual uncertainty. Left to explore are explanations that include decreasement (Ensured) and increasement (Unsured) of un- certainty. With uncertainty, an explanation can not only change toward either a higher\nor lower outcome. An explanation can also move towards ensured (lower uncertainty)\nor unsured (higher uncertainty), where ensured explanations are generally desirable. While explanation types offer a clear understanding of the movements along the out-\ncome axis, they do not regard how to change the uncertainty.\nWhen considering the exploration of alternative explanations, probabilistic explanations open up some further considerations, whereas numeric predictions do not. The reasons are the same as given in Section 3.1. When taking the uncertainty into account\nfor probabilistic explanations, Figure 2 provides a key to a categorisation of possible\nexplanations. In Figure 4, the different possible kinds of explanations are shown."}, {"title": "3.4. Ranking Alternative Explanations", "content": "The exploration of alternatives may result in a substantial number of generated explanations, especially if conjunctive rules are explored. Thus, having efficient ways of filtering out the most promising alternative explanations, taking both aleatoric and epistemic uncertainty into account, is necessary. Or in other words, we need efficient ways of exploring ensured explanations.\nThere are two complementary approaches to ensured explanations. The first ap-\nproach relies on an explicit filtering using the categories in Figure 4. Using this ap-\nproach makes it possible to get e.g. all counter-explanations (i.e., all explanations located in any of the red areas in Figure 5) or only the counter-factual explanations, excluding any counter-potential explanations (i.e., only including the solid red areas in Figure 5). This is a crude but often effective way of filtering according to a well es- tablished and well-understood logic. However, it does not always make sense to make\nsuch an explicit division, since this categorisation is based primarily on the aleatoric\nscale, i.e., along the probability (or prediction) scale. Furthermore, even when using\nsuch a division, we may still end up with too many alternatives to explore. Another\napproach relies on ranking the explanations based on both aleatoric and epistemic un-\ncertainty, and this is where the notion of an ensured alternative comes into picture.\nThe goal of ensured alternatives is to identify alternatives that decrease the level\nof epistemic uncertainty. This can be done explicitly, filtering out any alternative with\nhigher epistemic uncertainty. However, it may often be desirable to identify alternatives\nin the lower right area in Figure 5 (if super-factual explanations are sought), decreasing\nthe epistemic uncertainty while at the same time increasing probability. Consequently,\nthere is a trade-off between the calibrated probability for the predicted class (P) and\nuncertainty (U) when choosing the most suitable ensured explanation.\nIn order to handle the trade-off while at the same time allow the user to choose\nbetween counter- and super-explanations, the following ranking metric is suggested:\nrank = (1 - |w|)(| - |w|) \u00b7 (1 \u2013 U) + |w|\n{|\n- if w < 0,\notherwise,\nwhere w is the absolute value of the ranking weight w, which is bounded by -1 <\nw \u2264 1. A ranking weight w = 0 means that alternative explanations are only ranked\nbased on uncertainty, with less uncertain explanations ranked higher. Both ranking\nweight w = -1 and w = 1 are ranked along P alone, with positive weights ranking\nexplanations with increased belief in the predicted class higher and vice versa.\nThe figure shown in Figure 6 demonstrates how the ranking metric functions with\ndifferent weights. In the plot, dark blue represents lower ranks and yellow represents\nhigher ranks. The subplot in the middle illustrates how the ranking effectively pe-\nnalises high epistemic uncertainty and treats instances along the probability axis with\nequal emphasis when w = 0. Conversely, the subplot on the right (and left) exclusively penalises low (high) probability for the predicted class. In the second (fourth) subplot, with a weight of w = -0.5 (w = 0.5), an increase (decrease) in both epistemic uncer- tainty and probability for the predicted class is penalised. Based on the assumption that ensured explanations tend towards super-explanations, we recommend using a weight of 0.5 to focus on both low epistemic uncertainty and high probability for the predicted\nclass."}, {"title": "4. Experimental Setup", "content": "Two experiments were conducted to exemplify the number of alternative explanations and how ensured explanations with the ranking metric can be used for decision- making. Two datasets were used in the experiments, Wine and California housing. The datasets are presented in Table 1, where #Attr. represents the number of attributes, #Inst. the number of instances in the dataset, and Type the type of problem. The al- ternative explanations were generated through experiments in which training and calibration sets were randomly re-sampled before a random forest model was trained and\nexplained. The size of the calibration set was chosen to consist of 100 or 500 instances.\nTo catch variated situations, the experiments were either run for single feature expla- nations or conjunctive explanations."}, {"title": "5. Results", "content": "This section presents the experimental results, including the suggested ranking met- ric for alternative explanations and the plots for ensured explanations in Calibrated Explanations and how they can be used in decision-making."}, {"title": "5.1. Experimental Results", "content": "Tables 2 and 3 show the number of alternative explanations divided into different categories. The rows represent the number of calibration instances and whether it is alternative explanations for single feature explanations (s) or for conjunctive explanations (c). The columns represent the total number of explanations, which can be split up in counter-factual (CoFa), counter-potential (CoPo), semi-factual (SeFa), semi-potential\n(SePo), super-factual (SuFa), and super-potential (SuPo) explanations. Furthermore, ensured (Ens) explanations are the proportion of explanations having lower uncertainty. The purpose of evaluating using differently many calibration instances is to show how the epistemic uncertainty is affected by the calibration size.\nThe number of alternative explanations is generally high, with a notable increase\nfor conjunctive explanations due to the number of possible combinations of features.\nIn addition, the number of alternative explanations increases slightly with an increment\nin the number of instances in the calibration set. As expected, the number of uncertain predictions is clearly affected by the calibration size, with a drastic increase in\npotential-explanations for the smaller calibration set."}, {"title": "5.2. Ensured Explanations in Calibrated Explanations", "content": "In this section, a number of plots area presented, highlighting the challenge of a multitude of alternative explanations and how our proposed approach for filtering out\nensured explanations can be used and uncerstood.\nLet us first look at Figure 7a, which shows the entire test set for the wine data set.\nIt shows the position of each instance when both probability and epistemic uncertainty\nare taken into account. The blue dots signify a prediction of class 0 and the red crosses\na prediction of class 1. This type of plot offers a direct global understanding of the\nepistemic uncertainty variations in the underlying model and if there are some instances\nthat should be taken into consideration for further inspection. The instances show a\nrelatively low level of uncertainty (often below 0.10), although there are a few instances\nwith a slightly higher level of uncertainty.\nFigure 7b presents the same type of global plot as Figure 7a from the California housing dataset. Since the target is numerical, a threshold value is set to be higher than or below 500, which was found to be a good approximation of the median value. The\nred crosses indicate that the predicted value is below 500 and the blue dots indicate that\nit is equal to or above 500. Variations in epistemic uncertainty are more pronounced,\nand some of the instances are around or above 0.20.\nFigure 8a shows an instance (red dot) in the Wine data set with a large number of alternative explanations (blue dots), making it a challenge to distinguish and choose the\nbest explanations. The plot clearly shows that almost all alternative explanations are\nensured, i.e. they reduce uncertainty. We use the ranking metric to find the ten explana- tions with a focus (w = 1) on maximising the probability for the predicted class, which results in Figure 8b. Figure 8c shows a bar plot with the rule conditions on the right and the feature values to the left for the alternative explanations filtered in Figure 8b. The lighter red area in the background is the original prediction for reference, where the width indicates the uncertainty. The red bars correspond to the uncertainty interval\nfor the alternative prediction resulting from the alternative condition proposed to the left.\nLooking at both figures, it is possible to see that the metric has succeeded in ef- fectively selecting the ten explanations that increase the likelihood of the prediction\nthe most. An interesting aspect is the blend of both conjunctive and single rules in the\nfiltering of explanations. One feature that stand out as especially crucial to decreasing\nepistemic uncertainty in prediction is chlorides with a value below 0.04. The feature\nis primarily singled out at the top of the ranked explanations, existing in four out of\nthe ten most influential explanations. Moreover, the feature exists with the same rule\nin three conjunctive rules, which further hints at its significance for the prediction's\nuncertainty. The answer on how to ensure the prediction of this instance is to decrease\nthe chlorides from 0.06 to below 0.04. By decreasing the chlorides, the probability will\nincrease simultaneously as the uncertainty decreases, resulting in a sharp prediction\nwith high probability and low epistemic uncertainty.\nFigure 9 shows an instance from the California housing dataset with its alternative\nexplanations. The threshold in this example is set to 500, which means that the proba-\nbility indicates whether the true target is below 500. An increase in probability in this\ncase means that a higher price is even less likely.\nFor this particular instance, with an already low uncertainty, the five explanations\nthat we filter out using the weight w = 0.5 take into account both the probability and\nepistemic uncertainty. The top ranked explanations dramatically increase the probabil-\nity. However, they slightly increase the uncertainty. The barplot shows that the third\nexplanation could be chosen if the lowered degree of epistemic uncertainty is preferred.\nFigure 10 shows the results of an instance with an uncertainty of approximately\n0.18 in the California housing dataset and the results when applying the suggested\nmetric with different weights to rank alternative explanations of interest. There are a\nnotable number of alternative explanations for this case, where only one indicates a\nhigher uncertainty. We first apply a weight of 0 to rank the five alternatives with the\nlowest epistemic uncertainty, resulting in 10b. Although it looks like there are only two\nexplanations, there are four explanations resulting in almost identical probability and\nepistemic uncertainty. In this situation, primarily penalising the epistemic uncertainty,\nthe probability increases simultaneously, resulting in suggestions of sharp explanations\nwith high probability. A slightly different situation is seen in 10c, where the weight\nof 1 only penalises low probability. Here, the result also includes those with a higher\nuncertainty. Choosing a weight with 0.5 would probably be very similar to choosing\na weight of 0. In 10d, the weight of -0.5 is chosen, penalising a high epistemic un-\ncertainty and high probability. The result is ensured explanations with low probability,\nsimilar to ensured counter-factuals.\nFigure 11 shows the same instance but using the filtering methods counter \n_explanations, semi explanations, and super_explanations, showing all such\nalternative explanations. As mentioned earlier, when choosing a weight of -0.5 (seen\nin 10d, in Figure 10) the result is similar to counter-factual explanations as seen in 11a,\nin Figure 11. In 11b, all super-factual explanations are chosen. Although helpful, there\nare a considerable number of alternative explanations. When filtering is combined\nwith ranking, an efficient and dynamic tool is provided, enabling precise selection of\nsubsets of explanations. An example of this is shown in Figure 11d, showing the subset\nof semi-explanations (shown in Figure 11c) closest to 0.5.\nIn summary, the experimental findings presented in this study demonstrate that\nmultiple explanations frequently arise when exploring alternative explanations in order\nto reduce epistemic uncertainty. This phenomenon was particularly pronounced when\nallowing conjunctive explanations, due to the wide range of possible feature combi- nations. These results highlight the necessity of an effective filtering mechanism to\nidentify the most optimal alternatives. To simplify filtering and offer the possibility to\nrank the most effective ensured explanations, we introduce a metric designed to priori-\ntise explanations based on epistemic uncertainty, probability, or a"}]}