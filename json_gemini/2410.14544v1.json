{"title": "Computational Grounding of Responsibility Attribution and Anticipation in LTL f", "authors": ["Giuseppe De Giacomo", "Emiliano Lorini", "Timothy Parker", "Gianmarco Parretti"], "abstract": "Responsibility is one of the key notions in machine ethics\nand in the area of autonomous systems. It is a multi-faceted\nnotion involving counterfactual reasoning about actions and\nstrategies. In this paper, we study different variants of respon-\nsibility in a strategic setting based on LTLf. We show a con-\nnection with notions in reactive synthesis, including synthesis\nof winning, dominant, and best-effort strategies. This connec-\ntion provides the building blocks for a computational ground-\ning of responsibility including complexity characterizations\nand sound, complete, and optimal algorithms for attributing\nand anticipating responsibility.", "sections": [{"title": "Introduction", "content": "Responsibility is a key notion in the areas of machine\nethics and multi-agent systems. In recent times, there have\nbeen several and diverse attempts to formalize it, using\nvarious approaches such as game-theoretic tools (Baier,\nFunke, and Majumdar 2021; Braham and van Hees 2012;\nLorini and M\u00fchlenbernd 2018) and logical tools includ-\ning STIT logic (Lorini, Longin, and Mayor 2014; Abarca\nand Broersen 2022; Baltag, Canavotto, and Smets 2021;\nLorini and Schwarzentruber 2011), LTLf (Parker, Grandi,\nand Lorini 2023), ATL (Yazdanpanah et al. 2019; Bulling\nand Dastani 2013), logics of strategic and extensive games\n(Shi 2024; Naumov and Tao 2021, 2023), structural equation\nmodels (Chockler and Halpern 2004). However, the over-\nall picture on the conceptual and computational aspects of\nresponsibility remains rather fragmented. This is due to its\npolysemic nature and to its many dimensions (e.g., forward-\nlooking vs backward-looking, active vs passive, direct vs in-\ndirect, causal vs moral, attributed vs anticipated).\nIn this paper we provide (i) a comprehensive analysis of\nthe complexity of reasoning about strategic responsibility,\nnamely, the responsibility of an agent due to its choice of\na given strategy, and (ii) a number of algorithms that can\nbe used to automate reasoning about strategic responsibility.\nAny ethical artificial agent should be endowed with this kind\nof reasoning. First of all, the agent should be able to attribute\nresponsibility to itself and to other agents in order to put in\nplace reparatory actions, e.g., in case of responsibility for a\nnorm violation or for a damage. Secondly, it should be able\nto anticipate its own responsibility and the responsibility of\nothers in order to refrain from exposing itself and others to\npossible blame and sanctions.\nTo illustrate the relevance of reasoning about strategic re-\nsponsibility in ethical AI applications, as well as to aid in the\nexplanation of our model, we will use the running example\nof a robot who must take care of a plant. In the morning and\nafternoon, the robot has to decide whether or not to water\nthe plant, knowing that it may also rain during that time. For\nsimplicity, we will analyse only a single day.\nSuppose that both the robot and the environment decide\nnot to water the plant under any condition, meaning that the\nplant will die since it was not watered. However, is the robot\ncausally responsible for the death of the plant? Given the en-\nvironment's choice, the robot could have prevented the plant\nfrom dying by watering it, so the robot is responsible for the\ndeath of the plant at least in a minimal sense.\nThe counterfactual requirement that one could have pre-\nvented w by choosing differently is called passive responsi-\nbility (Lorini, Longin, and Mayor 2014). It is put in oppo-\nsition to active responsibility, also called deliberative \"see-\ning to it that\" in the STIT tradition (Belnap, Perloff, and Xu\n2001), which consists in an agent's choice forcing a cer-\ntain fact to be true regardless of the choices of the other\nagents. Passive responsibility is a notion of responsibility in\na weak sense, as the previous example highlights. As pointed\nout in (Braham and van Hees 2012), to strengthen it one\nshould add the requirement that the alternative option the\nagent could have chosen was recommended or admissible\nby some rationality requirement, so that it has no excuse for\nwhat it chose. Two simple and universally accepted ratio-\nnality requirements are dominance and the related notion of\nbest-effort: there was an alternative recommended option in\nthe sense that the agent had an alternative choice that domi-\nnates its actual choice, which is equivalent to saying that the\nagent's actual choice is not best-effort. It comes as a wel-\ncome surprise that such notions of dominance and best-effort\nhave been independently developed in the field of strategy\nsynthesis. So, it becomes natural to combine the theory of\nresponsibility with existing work on best-effort synthesis.\nThis is the focus of the present paper. What we gain by doing\nthis is a better understanding of the complexity of reasoning\nabout strategic responsibility and novel algorithms for it."}, {"title": "Preliminaries", "content": "A trace over an alphabet of symbols \u2211 is a finite or infinite\nsequence of elements from \u03a3. The empty trace is \u039b. The\nlength of a trace is |\u03c0|. Traces are indexed starting at zero,\nand we write \u03c0 = \u03c00\u03c01 .... For a finite trace \u03c0, we denote\nby lst(\u03c0) the index of its last element, i.e., |\u03c0| \u22121. We denote\nby \u03c0k = \u03c00 \u00b7 \u03c0\u03b5 the prefix of \u03c0 up to the k-th index.\nLinear Temporal Logic on finite traces (LTLf) is a spec-\nification language for expressing temporal properties over\nfinitetraces (De Giacomo and Vardi 2013). LTLf has the\nsame syntax as LTL, which is instead interpreted over infinite\ntraces (Pnueli 1977). Given a set AP of atomic propositions\n(aka atoms), the LTLf formulas over AP are:\n\u03c9 ::= \u03b1 | \u00ab\u03c9 | \u03c9\u03c0\u03c9 | \u03bf\u03c9 | \u03c9U\u03c9\nWhere a \u2208 AP, and o (Next) and U (Until) are tempo-\nral operators. We use standard Boolean abbreviations V (or)\n\u2283 (implies), true, and false. We also use the following ab-\nbreviations: \u2022w \u2261 \u00acO\u00acw (Weak Next); \u25c7w \u2261 trueUw\n(Eventually); and \u25a1w \u2261 \u00ab\u25c7\u00acw (Always). The size of w,\nwritten |\u03c9|, is the number of its subformulas.\nLTL \u0192 formulas are interpreted over finite traces \u03c0 over the\nalphabet \u2211 = 2AP, i.e., consisting of propositional interpre-\ntations of atoms. For i < lst(\u03c0), we have that \u03c0i \u2208 2AP is\nthe i-th interpretation of \u03c0. That an LTLf formula w holds\nat instant i < Ist(\u03c0), written \u03c0,\u03af |= \u03c9, is defined induc-\ntively: 1. \u03c0,\u03af |= a iff a \u2208 \u03c0\u03af (for a \u2208 \u0391\u03a1); 2. \u03c0,\u03af |= \u00ab\u03c9 iff\n\u03c0, \u03b9 \u2260 \u03c9; 3. \u03c0, \u03af |= \u03c91 \u039b\u03c92 iff \u03c0, \u03af |= \u03c91 and \u03c0,\u03af |=\n\u03c92; 4. \u03c0,\u03af |= Ow iff i < Ist(\u03c0) and \u03c0,\u03af + 1 |= w; and\n5. \u03c0,\u03af |= \u03c91Uw2 iff \u2203j such that i < j < Ist(\u03c0) and\n\u03c0,j |= w2, and \u2200k, i \u2264 k < j we have that \u03c0,\u03ba |= \u03c91.\nWe say that \u03c0 satisfies w, written \u03c0 |= w, if \u03c0, 0 |= \u03c9.\nLTLf reactive synthesis under environment specifications\n(De Giacomo and Vardi 2015; Aminof et al. 2019) concerns\nfinding a strategy to satisfy an LTLf formula while interact-\ning with an environment. We consider LTLf formulas over\nAP = YUX, where Y and X are disjoint sets of atoms\nunder control of agent and environment, respectively. Traces\nover \u2211 = 2VUX will be denoted \u03c0 = (YoU Xo)(Y1 \u222a X1)\u2026\u2026\nwhere Yi \u2286 Y and Xi \u2286 X for every i \u2265 0.\nAn agent strategy is a function \u03c3ag : (2X)* \u2192 2\u02c7 map-\nping sequences of environment choices to an agent choice.\nWe require dag to be stopping (De Giacomo et al. 2021), i.e.,\nthe agent stops the execution of any action at some point of"}, {"title": "Responsibility", "content": "The paper studies the notion of causal responsibility (Vin-\ncent 2011), which specifies whether or not an agent caused\na certain state of affairs to occur by making a certain choice.\nIt is a general notion of responsibility that is a necessary con-\ndition for both legal (Jansen 2014) and moral (Talbert 2023)\nresponsibility. We will focus on two forms of responsibility\nthat were considered in the literature, active responsibility\nand passive responsibility. Active responsibility captures the\nnotion of an agent making w happen, while passive responsi-\nbility consists in the agent merely letting w happen. The dis-\ntinction between the two notions was proposed in (Lorini,\nLongin, and Mayor 2014) in an action-based setting. Ac-\ntive responsibility corresponds to the notion of deliberative\nstit studied in STIT logic while passive responsibility cor-\nresponds to the counterfactual notion of (something) could\nhave been prevented (CHP), a fundamental component of\nthe notion of regret as highlighted in (Lorini and Schwarzen-\ntruber 2011). More recently, a plan-based analysis of active\nand passive responsibility was proposed in (Parker, Grandi,\nand Lorini 2023). In line with their work we distinguish at-\ntribution from anticipation for both active and passive re-\nsponsibility. Responsibility attribution is an ex post notion:\nit is ascribed to a given agent after the agent and the environ-\nment have made their choices and the result of their choices\nhas been revealed. Responsibility anticipation is an ex ante\nnotion: it is the responsibility that an agent could incur by\nmaking a certain choice.\nIn this section, we present a novel strategy-based analysis\nof active and passive responsibility, from the point of view of\nboth attribution and anticipation. When moving from actions\nand linear plans to strategies some conceptual issues arise.\nIn particular, when attributing passive responsibility for a\nfact w to an agent, one has to fix the choice made by the\nenvironment and counterfactually check whether the agent\ncould have prevented w from being true by making a differ-\nent choice. While the action or plan chosen by the environ-\nment can be easily extracted from the actual history which\nwe assume to be fully observable, the same cannot be done\nfor strategies. From the actual history, one can only infer the\nset of strategies of the environment that are consistent with\nit and this set is not necessarily a singleton. So, in a strategy-\nbased setting we must distinguish responsibility attribution\nagainst agent strategies (supposing the environment strategy\nis fully observable) from responsibility attribution on histo-\nries (when the environment strategy is not fully known).\nAnother novel aspect of our analysis, in addition to the fo-\ncus on strategies and the consequent distinction between re-\nsponsibility attribution against environment strategies or on\nhistories, is a refinement of the notion of passive responsibil-\nity. Passive responsibility, as defined in (Lorini, Longin, and\nMayor 2014; Parker, Grandi, and Lorini 2023), requires to\nexistentially quantify over the agent's possible choices. We\nconsider a variant of passive responsibility attribution and\nanticipation in which existential quantification is restricted\nto the set of possible choices that dominates the agent's ac-\ntual choice with respect to the avoidance of w. We call this\ninexcusable passive responsibility. Specifically, an agent is\nheld responsible for letting w be true with no excuse, if it\ncould have prevented w from being true by choosing a differ-\nent strategy that dominates its actual choice with respect to\nthe avoidance of w. In other words (as we will prove later),\nthe agent chose a strategy which is not best-effort for the\navoidance of w. We say that the agent has no excuse since\nit cannot appeal to not knowing the choice of the environ-\nment at the moment of its choice, since it had an alternative\nchoice available that would have guaranteed the avoidance\nof w more than the choice it actually made, irrespective of\nthe choice of the environment.\nIn what follows we denote LTLf temporal properties as\nw, LTL f environment specifications as E, agent strategies as\n\u03c3ag, environment strategies as genv, and histories as h."}, {"title": "Attribution against Environment Strategies", "content": "We first\ndefine responsibility attribution in a perfect information set-\nting with full knowledge of the environment strategy. While\nconvenient, this is uncommon in a strategic setting where the\nenvironment is the model of the world in which the agent\noperates. Nonetheless, these notions are useful for defining\nfurther refinements of responsibility.\nDefinition 1 (Passive Responsibility Attribution, PRATTR).\nThe agent is attributed passive responsibility for w under\n\u03c3ag and \u03c3env \u2208 \u03a3\u03b5 if Play(\u03c3ag, \u03c3env) |= w and there exists\nan agent strategy \u03c3'ag such that Play(\u03c3'ag, 0env) |= \u00ab\u03c9.\nDefinition 2 (Inexcusable Passive Responsibility Attribu-\ntion, IPRATTR). The agent is attributed inexcusable pas-\nsive responsibility for w under \u03c3\u03b1\u03c2 and \u03c3env \u2208 \u03a3\u03b5 if\nPlay(\u03c3\u03b1\u03c2, \u03c3\u03b5\u03b7\u03c5) |= w and there exists an agent strategy \u03c3'\u03b1\nsuch that d'ag >\u00acw|\u03b5 \u03c3ag and Play(o'ag, env) |= \u00acw.\nWe assume that at the moment of its choice the agent\nhas no information about the possible choices of the envi-\nronment beyond the environment specification. This is be-\ncause in our model the environment is not necessarily ratio-\nnal and the agent knows this. Consequently, according to the\nagent, all environment strategies enforcing the environment\nspecification are possible, regardless of apparent rationality.\nThis also justifies the use of (weak) dominance and best-\neffort for characterizing the notion of 'inexcusable' in the\nprevious definition. Indeed, if there exists og for the agent\nwhich dominates the strategy gag with respect to \u00acw, the\nchoice of rag is inexcusable since it has certainly envisaged\nan environment strategy for which the alternative strategy\n\u03c3'ag would have been strictly better than the strategy gag.\nAttribution on Histories. We now consider the imperfect\ninformation setting where we have access only to the history,\nwhich is far less problematic than access to the full environ-\nment strategy. In the following definitions, we assume that\nthe history h is consistent with the strategy \u03c3\u03b1\u03c2.\nDefinition 3 ((Inexcusable) Passive Responsibility Attribu-\ntion on History, IPRATTR(h)/PRATTR(h)). The agent is\nattributed (inexcusable) passive responsibility for w under\n\u03c3\u03b1g, E, and h if there is an environment strategy genv \u2208 \u03a3\u03b5\ns.t h is consistent with \u03c3\u03c4\u03b7v and the agent is attributed (in-\nexcusable) passive responsibility for w under oag and genv\u00b7\nResponsibility anticipation is ex ante, meaning that it rea-\nsons in terms of potential responsibility that an agent could\nincur by making a certain choice. This means that is quan-\ntifies over the set of possible environment strategies rather\nthan any single environment strategy, meaning it only re-\nquires access to E. Therefore there is no difference between\nresponsibility anticipation in a perfect information setting\nand an imperfect information setting.\nDefinition 4 ((Inexcusable) Passive Responsibility Antici-\npation, IPRANT/PRANT). The agent anticipates (inexcus-\nable) passive responsibility for w under oag and E if there\nis an environment strategy \u03c3\u03c4\u03b7\u03bd \u2208 \u03a3\u03b5 s.t. the agent is at-\ntributed (inexcusable) passive responsibility for w under \u03c3\u03b1\u03c2\nand \u03c3\u03c4\u03b7\u03bd.\nThe agent anticipates responsibility for w iff there is some\npossible outcome where it is attributed responsibility for w.\nIt follows that if the agent adopts a strategy that does not"}, {"title": "Responsibility and Strategy Properties", "content": "Based on their definitions, active, passive and inexcusable\npassive responsibility are strongly connected with winning,\ndominant and best-effort strategies, respectively.\nTheorem 1.\n\u2022 The agent anticipates passive (resp. inexcusable passive)\nresponsibility for w under \u03c3\u03b1g and E iff oag is not domi-\nnant (resp. is not best-effort) for \u00abw under E.\n\u2022 The agent anticipates active responsibility for w under\n\u03c3\u03b1g and & iff gag is winning for w under E and there\nexists some d'ag that is weak for \u00acw under E;"}, {"title": "Checking Techniques", "content": "The rest of the paper is devoted to presenting the algorithms\nfor responsibility attribution and anticipation shown in Ta-\nble 2. The building blocks of these algorithms are finite-state\nautomata and DFA games, which we review below.\nA nondeterministic finite automaton (NFA) is a tuple N =\n(\u03a3, S, s0, \u03b4, F), where: \u2211 is a finite input alphabet; S is a fi-\nnite set of states; s0 \u2208 S is the initial state; \u03b4 : S \u00d7 \u03a3 \u2192 2S\nis the transition function; and F \u2286 S is the set of final states.\nThe size of N is |S|. Given a word \u03c0 = \u03c00 \u00b7\u00b7\u00b7 \u03c0n \u2208 \u03a3*, a\nrun of N in \u03c0is a sequence of states s0 \u00b7\u00b7\u00b7 Sn+1 starting in\nthe initial state of N and such that si+1 \u2208 \u03b4(si, \u03c0i) for every\ni \u2265 0. A word \u03c0 is accepted by N if it has a run whose last\nreached state is final. The language of N, written L(N), is\nthe set of words accepted by N. An automaton N is a de-\nterministic finite automaton (DFA) if |d(s, a)| \u2264 1 for every\n(s, a) \u2208 S \u00d7 \u03a3. Checking non-emptiness of L(N), written\nNONEMPTY(N), can be done by checking the existence of\na path from the initial state of N to some final state. Given\nthe NFAS N1 and N2 with languages L(N1) and L(N2), re-\nspectively, we can build in polynomial time the product NFA\nN = N1 \u00d7 N2 such that L(N) = L(N1) \u2229 L(N2). We\ndenote nondeterministic and deterministic automata by N\nand A, respectively. Every LTLf formula w can be trans-\nformed into an NFA N = TONFA(w) (resp. DFA Aw =\nTODFA(w)) with size at most exponential (resp. doubly-\nexponential) in |w| and whose language is exactly the set\nof traces satisfying w (De Giacomo and Vardi 2015).\nA DFA game is a DFA G with input alphabet 2DUX, where\nY and X are two disjoint sets under control of agent and\nenvironment, respectively. The notions of plays and strate-\ngies in Preliminaries also apply to DFA games. An agent\nstrategy is winning if Play(\u03c3ag, denv) is accepted by G\nfor every environment strategy genv. An agent strategy is\nweak if Play(\u03c3ag,denv) is accepted by G for some envi-\nronment strategy genv. Conversely, an environment strat-\negy is winning if Play(\u03c3ag, genv) is not accepted by G for\nevery agent strategy gag. The agent winning (resp. weakly\nwinning) region is the set of states s\u2208 S for which the\nagent has a winning (resp. weak) strategy in the game G' =\n(2\u222ax, S, s, \u03b4, F), i.e., the same game as G, but with initial\nstate s. The environment winning region is defined analo-\ngously. Solving a DFA game is the problem of computing\nthe winning (resp. weakly winning) region, written W =\nWINREGION(G) (resp. W' = WEAKREGION(G)). Games\nplayed over DFAS are determined, meaning that the agent\nwinning region and the environment winning region parti-\ntion the state space (Gale and Stewart 1953). The environ-"}, {"title": "Supplementary Material", "content": "We first give hardness results for the problems of checking\nwinning, dominant, and best-effort strategies and the exis-\ntence of weak strategies. That completes the proofs of The-\norems 4, 5, 6, and 7. Subsequently, we give complete proofs\nfor the results showing the connection between active, pas-\nsive, and inexcusable passive responsibility with winning,\ndominant, and best-effort strategies, respectively, i.e., The-\norems 1 and 2. We conclude by giving complete proofs for\nthe results of our computational grounding, i.e., Theorem 3.\nTheorem 4. LTLf weak synthesis for w under E is:\n\u2022 PSPACE-complete in the size of w;\n\u2022 2EXPTIME-complete in the size of E.\nTheorem 5. Checking if \u03c3ag is winning for w under E is:\n\u2022 PSPACE-complete in the size of w;\n\u2022 2EXPTIME-complete in the size of E;\n\u2022 Polynomial in the size of \u03c3ag.\nTheorem 6. Checking if \u03c3ag is dominant for w under E:\n\u2022 PSPACE-complete in the size of w;\n\u2022 2EXPTIME-complete in the size of E;\n\u2022 Polynomial in the size of \u03c3\u03b1\nTheorem 7. Checking if \u03c3ag is best-effort for w under E\nis: 1. 2EXPTIME-complete in the sizes of w and E; and 2."}]}