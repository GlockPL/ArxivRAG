{"title": "Atom of Thoughts for Markov LLM Test-Time Scaling", "authors": ["Fengwei Teng", "Zhaoyang Yu", "Quan Shi", "Jiayi Zhang", "Chenglin Wu", "Yuyu Luo"], "abstract": "Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AOT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AOT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AOT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AOT achieves an 80.6% F1 score, surpassing 03-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) demonstrate significant scaling effects, with their capabilities showing predictable improvements as model parameters and training data increase, leading to enhanced performance across diverse domains (Kaplan et al., 2020). While this scaling law faces bottlenecks in high-quality data availability, test-time scaling offers an alternative solution by forcing LLMs to engage in effective logical reasoning during inference to improve performance on diverse tasks (Snell et al., 2024; Muennighoff et al., 2025; Hou et al., 2025; Zhang et al., 2024a).\nHowever, existing test-time scaling methods excessively maintain historical information during reasoning, as they rely heavily on complex structural dependencies throughout the reasoning process. Chain-based methods must preserve the entire reasoning history to generate each subsequent step (Wei et al., 2022; Zhang et al., 2023), while tree-based approaches require tracking both ancestor and sibling relationships for branch selection (Yao et al., 2023; Zhou et al., 2024; Ding et al., 2024). Graph-based structures further compound these challenges through arbitrary node dependencies. As the scale of reasoning increases, the accumulation of historical dependencies not only wastes substantial computational resources but also interferes with the model's ability to reason effectively, as illustrated in Figure 1.\nHuman reasoning often progresses through solving a sequence of independent subquestions, a fundamental principle established in cognitive science (Simon, 1962) and problem-solving theory (Polya, 1945). When solving a complex problem, we naturally identify and resolve self-evident subquestions first, then seamlessly incorporate these solutions to reformulate a simplified problem state, rather than maintaining detailed reasoning processes for resolved components. This progression closely resembles a Markov process (Markov, 1906), where each state represents a question, and state transitions occur through resolving partial problems to form new, independent questions.\nInspired by this Markov nature of human reasoning, we propose Atom of Thoughts (AOT), a framework that realizes the Markov-style reasoning process. Our key insight is that each reasoning state can be defined as a simplified problem equivalent to the original one, where partial reasoning steps are either transformed into known conditions or excluded as incorrect explorations. This definition is achieved through a two-phase state transition mechanism: first decomposing the current question into a dependency-based directed acyclic graph (DAG) to capture rich structural information, then contracting subquestions into a new independent question. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, ensuring each state transition depends only on the current state while progressively reducing problem complexity.\nThis design endows AOT with two key advantages. First, AOT eliminates the need for maintaining and computing historical information when scaling computational resources. Second, these atomic questions can be seamlessly integrated into existing test-time scaling frameworks, allowing AOT to function as either a standalone framework or a plug-in enhancement for improving the overall reasoning capabilities.\nIn summary, our contributions are as follows:\n\u2022 Atom of Thoughts. We introduce AOT, a novel reasoning framework with Markov property that progressively decomposes problems into atomic units. This approach significantly reduces computational resources wasted on historical information processing, allowing the model to focus on effective reasoning during test-time scaling.\n\u2022 Plug-In Enhancement. The atomic questions derived by AOT can be directly integrated into existing test-time scaling methods (Bi et al., 2024; Wang et al., 2023b), enhancing both their performance and cost efficiency.\n\u2022 Extensive Evaluation. Experiments across six benchmarks demonstrate the effectiveness of AOT both as a standalone framework and as a plug-in enhancement. AOT outperforms all baselines, and notably on HotpotQA dataset, enables gpt-40-mini to surpass reasoning models: 03-mini by 3.4% and DeepSeek-R1 by 10.6%."}, {"title": "2 Related Work", "content": "2.1 Reasoning Framework\nChain-of-Thought (Wei et al., 2022) prompting has emerged as a fundamental technique for enhancing LLMs' reasoning. Decomposition methods like Least-to-Most (Zhou et al., 2023) and Plan-and-Solve (Wang et al., 2023a) prompting parse complex problems into sequential subtasks. Iterative optimization approaches like Self-Refine (Madaan et al., 2023), Step-Back (Zheng et al., 2024) prompting and Progressive-Hint Prompting (Zheng et al., 2023) refine solutions through cyclic feedback or abstraction. Multi-path aggregation techniques like Self-Consistency CoT (Wang et al., 2023b) and LLM-Blender (Jiang et al., 2023) further improve reasoning reliability by multi-trajectory consensus.\nMore sophisticated frameworks structure the representation of reasoning space through dedicated formalisms: Tree of Thoughts (Yao et al., 2023) enables systematic exploration of multiple reasoning paths, while Graph of Thoughts (Besta et al., 2024) represents reasoning processes as dynamic graphs with backtracking mechanisms. Addressing fundamental limitations in resampling-based paradigms, Thought Space Explorer (Zhang and Liu, 2024) strategically explores under-sampled regions of the solution space. These frameworks serve as universal augmentation of LLMs reasoning, enhancing their capacity across various domains, with their principles being widely adopted in agentic workflows for code generation, question answering, and data science applications (Hong et al., 2024b; Zhang et al., 2024a; Hong et al., 2024a; Zhang et al., 2025; Xiang et al., 2025; Zhang et al., 2024b).\n2.2 Test-time Scaling\nTest-time scaling approaches have demonstrated the value of extended computation during inference. Supervised fine-tuning on long chain-of-thought traces has proven effective at enhancing models' capabilities to conduct extended reasoning (Yeo et al., 2025; Yao et al., 2025). Building on this foundation, reinforcement learning methods have enabled models to automatically learn optimal inference expansion strategies, allowing for adaptive scaling of the reasoning process (Kimi et al., 2025; Zeng et al., 2025; DeepSeek-AI, 2025). Framework-based approaches have further expanded these capabilities by extending inference through external systems, incorporating techniques like verification, budget forcing, and ensemble methods (Zhang et al., 2024a; Saad-Falcon et al., 2024; Chen et al., 2024). These complementary approaches demonstrate how strategic use of additional computation during inference through learned behaviors, automated scaling, and system-level interventions can substantially enhance model performance.\nHowever, these approaches universally maintain extensive historical information throughout the reasoning process, leading to computational inefficiency and potential interference with effective reasoning. In contrast, AOT introduces a Markovian perspective that eliminates the need for historical dependency tracking, enabling more efficient resource allocation while maintaining compatibility with existing test-time scaling methods."}, {"title": "3 An Overview of AOT", "content": "This section presents an overview of AOT from a probabilistic modeling perspective. We first examine how traditional reasoning chains work and then introduce our dependency-based graph structures and their contraction mechanisms to enhance the modeling capability of reasoning processes.\n3.1 Reasoning Chain\nChain-of-Thought (CoT) prompting enables LLMs to progressively propose intermediate thoughts $T_i$ when solving a problem. As discussed earlier, this approach requires maintaining a complete reasoning history, which can be formalized as a proba-"}, {"title": "bilistic sampling procedure:", "content": "$A \\sim p(A|T, Q_0) \\prod_{i=0}^{N} p(T_i|T_{<i}, Q_0)$     (1)\nHere, $T = {T_0, T_1, ..., T_N}$ represents the sequence of intermediate thoughts generated by the LLM. Each thought $T_i$ depends on the previous thoughts $T_{i}$ and the initial question $Q_0$.\nTo explore chain-based methods with different node definitions, Least-to-Most (Zhou et al., 2023) replaces the intermediate thoughts $T_i$ with subquestions $Q_i$, resulting in a different formulation of the reasoning chain:\n$A \\sim p(A|Q) \\prod_{i=0}^{N} p(Q_i|Q_{<i})$  (2)\nwhere $Q = {Q_0, Q_1, ..., Q_N}$ is the sequence of subquestions.\nIn an ideal scenario where the reasoning chain Q exhibits the Markov property, each subquestion $Q_{i+1}$ would only depend on its immediate predecessor $Q_i$, similar to how humans naturally solve complex problems by resolving independent subquestions and reformulating simplified states. This leads to:\n$A \\sim p(A|Q_N) \\prod_{i=0}^{N} p(Q_{i+1}|Q_i)$  (3)\nHowever, achieving true Markov property in real-world reasoning tasks is challenging. We adopt the subquestion-based node structure from reasoning chains as states while exploring a two-phase state transition mechanism consisting of decomposition and contraction to address this challenge."}, {"title": "3.2 Dependency Directed Acyclic Graph", "content": "AOT utilizes temporary DAG structures to decompose the current question, unlike existing methods that maintain complex dependencies throughout the reasoning process. This DAG structure serves as a scaffold during state transitions, providing rich structural information to guide the complete state transition process, specifically functioning as the decomposition phase to facilitate the subsequent contraction phase.\nThe DAG G is defined as:\n$G = (Q, E), Q = {Q_i}_{i=1}^n, E \\subseteq Q \\times Q$  (4)\nIn our DAG definition, nodes represent subquestions $Q_i$, and edges $(Q_j, Q_i)$ indicate that $Q_j$ contains necessary information for solving $Q_i$. A major challenge in constructing Markov processes stems from the dependencies of various information in complex reasoning scenarios, and this definition provides structural information for identifying dependencies through rule-based determination.\nBased on their dependency relationships, all subquestion nodes can be categorized into two types:\nIndependent subquestions $Q_{ind}$ (nodes without incoming edges):\n$Q_{ind} = {Q_i \\in Q | \\nexists Q_j \\in Q, (Q_j, Q_i) \\in E}$  (5)\nDependent subquestions $Q_{dep}$ (nodes with incoming edges):\n$Q_{dep} = {Q_i \\in Q | \\exists Q_j \\in Q, (Q_j, Q_i) \\in E}$   (6)\nThe key assumption of acyclicity in our DAG is guaranteed by this edge definition: since subquestions are generated following natural language order, any subquestion $Q_i$ can only depend on previously generated subquestions $Q_{<i}$. Even in the maximally connected case where each subquestion links to all its predecessors, acyclicity is maintained, as any additional edges would create cycles by connecting to future nodes while violating the natural language order."}, {"title": "3.3 Contraction", "content": "The contraction phase transforms the temporary DAG structure into the next atomic state while preserving the Markov property. To ensure this Markov process is meaningful, we must maintain state atomicity while ensuring progress in the reasoning process. As the reasoning progresses, new conclusions and information are continuously derived, necessitating the selective discarding of information to maintain atomic states. AOT addresses this by treating results from $Q_{ind}$ as either given conditions or eliminated process information, while contracting $Q_{dep}$ into an independent question as the next state. This contracted question maintains solution equivalence to $Q_i$, ensuring the reasoning process stays on track.\nThe reasoning process is formally described in Algorithm 1, which shows how AOT iterates through decomposition and contraction steps. This iterative process continues until it reaches a maximum number D, which is assigned by the depth of the first generated graph $G_0$ to prevent infinite decomposition. The process can be formalized as:\n$A \\sim p(A|Q_D) \\prod_{i=0}^{D} p(Q_{i+1}|G_i) p(G_i|Q_i)$  (7)"}, {"title": "4 The Design Details of AOT", "content": "This section details the implementation of AOT's core components: decomposition and contraction, which together form one iteration of state transition in the Markov reasoning process, as illustrated in Figure 2. Through structured decomposition and principled contraction, our approach establishes a foundation for iterative reasoning that can flexibly integrate with other methods while balancing computational efficiency and reasoning depth.\n4.1 Decomposition\nDependency Directed Acyclic Graph. Addressing the challenge of excessive historical information maintenance, our decomposition phase introduces an efficient dependency extraction mechanism that only temporarily captures rich structural information, which provides the foundation for subsequent simplification. This process starts with decomposing the current question into granular subquestions, then leverages LLMs' zero-shot capabilities to efficiently identify inter-question dependencies. The dependency extraction is achieved through a JSON-formatted LLM invocation that progressively labels each subquestion's dependencies by indexing its upstream questions (see Appendix B.2 for annotation prompt templates).\n4.2 Contraction\nSubquestions Contracting. Based on the dependency relationships identified in DAG structure, AOT performs contraction through a single LLM invocation. This process constructs an independent contracted question by selectively integrating information from independent subquestions as known conditions and incorporating the descriptions of current dependent subquestions into the main body. This process maintains answer equivalence throughout the Markov chain while continuously eliminating the test-time of solved independent subquestions in past iterations when solving the contracted question independently. The elimination of the dependency relationships from independent subquestions and the generated contracted question facilitates the transmission of key information that causes dependency. (see Appendix B.3 for contraction prompt templates).\nMarkov Property Maintenance. Through this contraction process, AOT effectively eliminates redundant information in historical reasoning steps to reduce the test-time required for solving questions in subsequent states. The contraction mechanism ensures that each state generated in the process depends only on its immediate predecessor, preserving the Markov property while progressively simplifying inherent complexity of the question in the current state.\n4.3 Integration\nIterative Process. The pipeline of AOT operates through an iterative process where each state transition step involves question decomposition followed by contraction. The contracted question from each iteration serves as the input for the next decomposition phase. As the number of iterations increases, the test-time scales up in an attempt to achieve more robust and effective reasoning.\nTermination Mechanism. To optimize test-time efficiency, AOT incorporates an automated termination mechanism that uses LLM evaluation to assess solution quality through answer comparison. After each contraction step, an LLM examines three key elements: the execution results of the original question $Q_i$, the decomposed DAG structure $G_i$, and the independent execution results of contracted question $Q_{i+1}$. The LLM synthesizes these elements to generate a comprehensive answer for $Q_i$. If this synthesized answer demonstrates consistency with the answer produced by $Q_{i+1}$, the iterative process continues. Upon termination, AOT combines the current contracted question with the union of independent subquestions $Q_{dep} = \\cup_{i=1}^{n} Q_{dep_i}$, accumulated from all previous iterations to form a complete solution to the initial question $Q_0$. This structure provides a solution composed entirely of independent questions, maintaining semantic independence of each subquestion while ensuring completeness of whole solution.\nIntegration Through Configurable Termination. Building upon this termination mechanism, AOT enables seamless integration with existing test-time scaling methods by allowing any intermediate state to serve as an entry point. This flexibility comes from AOT's configurable termination strategy - often just a single decomposition-contraction cycle before passing this simplified question to other methods (refer to the right portion of Figure 2). This approach leverages AOT's structural optimization capabilities as a preprocessing step while allowing other methods to operate on a more manageable question. The contracted question passed to subsequent methods maintains answer equivalence with the original question while AOT's initial structural simplification helps redirect computational resources towards more direct and effective reasoning. The seamless transition between methods is facilitated by the atomic state representation in our Markov process, ensuring that essential question characteristics are preserved while unnecessary historical information is eliminated."}, {"title": "5 Experiments", "content": "We conduct comprehensive experiments to examine AOT through extensive benchmark evaluation on six standard datasets, reasoning models comparison, test-time optimization experiments, and ablation studies. Our main results demonstrate consistent improvements across different reasoning tasks, with significant gains especially in multi-hop reasoning. Through comparison with state-of-the-art reasoning models, we show AOT's effectiveness as a general framework. Our test-time optimization experiments further validate AOT's adaptability and efficiency. Finally, ablation studies on key components like DAG structure and decomposition mechanism confirm the essentiality of our design choices.\n5.1 Experimental Setup\nDatasets. We evaluate AOT using gpt-40-mini-0718 as the backbone model, chosen for its strong performance-efficiency trade-off. Our evaluation covers four categories of reasoning tasks: mathematical reasoning (MATH (Hendrycks et al., 2021) with numerical answers and GSM8K (Cobbe et al., 2021)), knowledge-intensive reasoning (MMLU-CF (Zhao et al., 2024)), logical reasoning (multiple-choice subsets of BBH (Suzgun et al., 2023), see Appendix D.1 for details), and multi-hop reasoning (HotpotQA (Yang et al., 2018) and Long-Bench (Bai et al., 2024) which test models' ability to connect information across multiple contexts). We use the first 1,000 examples from each dataset's test set, except for GSM8K where we use its complete test set (1,319 examples) and LongBench where we use the combined MuSiQue (Trivedi et al., 2022) and 2WikiMultiHopQA (Ho et al., 2020) subsets (400 examples).\nBaselines. Our baselines include classical prompting methods (Chain-of-Thought (CoT), CoT with Self-Consistency (CoT-SC, n = 5), Self-Refine, and Analogical Reasoning (Yasunaga et al., 2024)) and advanced reasoning frameworks (agentic workflow AFlow (Zhang et al., 2024a) and Forest of Thought (FoT)). For FoT, we implement it using Tree of Thoughts with branch number b = 3, chosen for its generalizability across diverse tasks. All experiments are averaged over three runs, with detailed reproduction settings in Appendix D.\n5.2 Experimental Results and Analysis.\nMain Results As shown in Table 1, AOT demonstrates consistent improvements across different reasoning tasks. AOT achieves strong performance on mathematics tasks, with AOT * reaching 84.9% on MATH and 95.1% on GSM8K (+1.9% over AFlow on MATH, +1.1% over FoT(n=8) on GSM8K). The most notable improvements are in multi-hop QA tasks, where our base version achieves 80.6% F1 score on HotpotQA (+7.1% over AFlow). Similar improvements on Long-Bench (68.8%, +7.5% over AFlow) further demonstrate the effectiveness of AOT's atomic state representation in long context scenarios."}, {"title": "Reasoning Models Comparison Results", "content": "We compare AOT with several reasoning models, including QwQ-32B-Preview (Qwen-Team, 2024), DeepSeek-R1 (DeepSeek-AI, 2025), and 03-mini-2025-01-31(OpenAI, 2025). Notably, 03-mini demonstrates remarkable raw performance with a 77.2% F1 score on HotpotQA, surpassing our previous best baseline AFlow (73.5% F1) in the main experiments, highlighting its strength as a foundation model. When integrated into our framework, even a relatively modest model like gpt-40-mini achieves an impressive 80.6% F1 score. Furthermore, employing o3-mini as the backbone of AOT leads to exceptional results: the F1 score increases to 81.4% and the Hit rate reaches 91.4% on HotpotQA. On the LongBench subset, our framework with o3-mini achieves a 63.3% F1 score and 72.1% Hit rate, establishing new state-of-the-art performance across all metrics. Due to the computational constraints and stability considerations, we evaluated on the first 100 examples from the Musique subset of LongBench, which may result in slightly higher scores compared to our main experiments in Table 1."}, {"title": "Test-Time Optimization Results", "content": "We investigate the test-time scaling behavior of AOT through two sets of experiments. First, as shown in Figure 3, we analyze the performance scaling of AOT on MATH dataset. Unlike the dynamic iteration limit determined by problem-specific graph structures described in Section 3, here we set a uniform maximum of 5 iterations to explicitly examine the depth-wise scaling behavior. Since each iteration produces an evaluable solution, we can track performance across different iteration depths. All 1000 test samples naturally generate solutions at depth 1, while fewer samples proceed to deeper iterations (dropping to 207 at depth 5), as many problems achieve satisfactory solutions at earlier depths. The results demonstrate that AOT exhibits consistent accuracy improvements from 83.2% to 92.7% as the iteration depth increases, with the performance gains gradually tapering. This pattern suggests that while deeper iterations continue to benefit overall performance, many problems can be effectively solved with fewer iterations, providing a natural trade-off between computational cost and solution quality."}, {"title": "Cost Analysis", "content": "Through analyzing computational efficiency as shown in Figure 4, our AoT achieves superior efficiency by reaching competitive performance at significantly lower computational costs compared to existing methods. This enhanced efficiency can be attributed to our atomic state representation that preserves only necessary information while eliminating redundant computations. Notably, AOT demonstrates the steepest performance-to-cost ratio among all compared methods, indicating it achieves the highest marginal improvement in accuracy per unit of computational investment."}, {"title": "Ablation Study", "content": "We conduct ablation studies to analyze the contribution of key components in AOT. As shown in Table 3, removing the decomposition phase (i.e., no extracted independent or dependent sub-problems as guidance) causes notable performance drops, while removing the DAG structure but keeping the decomposition phase (i.e., only extracting the first semantically independent sub-problem as guidance) leads to even larger degradation. Without decomposition structure, the LLM struggles to capture crucial dependencies between subquestions in the contraction phase, resulting in contracted questions that often contain redundant information. Moreover, providing single sub-problem guidance without proper structural information disrupts the parallel relationships between sub-problems. This reveals a critical insight: imperfect structural guidance can be more detrimental than no guidance at all (see Appendix C.1 for examples)."}, {"title": "6 Conclusion", "content": "In this paper, we introduced Atom of Thoughts (AOT), a novel framework that transforms complex reasoning processes into a Markov process of atomic questions. By implementing a two-phase transition mechanism of decomposition and contraction, AOT eliminates the need to maintain historical dependencies during reasoning, allowing models to focus computational resources on the current question state. Our extensive evaluation across diverse benchmarks demonstrates that AOT serves effectively both as a standalone framework and as a plug-in enhancement for existing test-time scaling methods. These results validate AOT's ability to enhance LLMs' reasoning capabilities while optimizing computational efficiency through its Markov-style approach to question decomposition and atomic state transitions."}, {"title": "7 Limitations", "content": "A key limitation of AOT lies in its Markov state transition process without a well-designed reflection mechanism. When the initial DAG decomposition fails to properly model parallel relationships between subquestions or captures unnecessary dependencies, it can negatively impact subsequent contraction and reasoning process, a scenario that occurs frequently in practice. The framework currently lacks the ability to detect and rectify such poor decompositions, potentially leading to compounded errors in the atomic state transitions. This limitation suggests the need for future research into incorporating effective reflection and adjustment mechanisms to improve the robustness of DAG-based decomposition."}, {"title": "8 Ethics Statement", "content": "While this work advances the computational efficiency and test-time scaling capabilities of language models through the AOT framework, we acknowledge that these models process information and conduct reasoning in ways fundamentally different from human cognition. Making direct comparisons between our Markov reasoning process and human thought patterns could be misleading and potentially harmful. The atomic state representation and dependency-based decomposition proposed in this research are computational constructs designed to optimize machine reasoning, rather than models of human cognitive processes. Our work merely aims to explore more efficient ways of structuring machine reasoning through reduced computational resources and simplified state transitions, while recognizing the distinct nature of artificial and human intelligence. We encourage users of this technology to be mindful of these limitations and to implement appropriate safeguards when deploying systems based on our framework."}, {"title": "B The prompt used in AOT", "content": "In this section, we mainly present the basic prompts in mathematical scenarios.\nB.1 Direct Solver"}, {"title": "B.2 Dependency Annotation", "content": ""}, {"title": "B.3 Subquestions Contracting", "content": ""}, {"title": "C Case study", "content": "C.1 The illusion phenomenon when contracting subquestions\nDestruction of Parallelism\nWhen solving complex questions through decomposition, parallel subquestions should maintain their independence. However, parallelism can be destroyed when merging results, as illustrated by this example: Original decomposition:\nDestruction of Independence\nWhen subquestions have dependencies, maintaining independence in the analysis chain is crucial. Loss of independence occurs when the relationship between dependent subquestions is not properly maintained during contraction, as shown in this example: Original decomposition:"}, {"title": "C.2 Example of AOT Reasoning Process", "content": "Question Statement\nFor a given constant $b > 10$, there are two possible triangles $ABC$ satisfying $AB = 10, AC = b$, and $\\sin B = \\frac{3}{5}$. Find the positive difference between the lengths of side $BC$ in these two triangles.\nGround Truth\nWe have that $\\cos^2 B = 1 - \\sin^2 B$, so $\\cos^2 B = 1 - \\frac{9}{25} = \\frac{16}{25}$, so $\\cos B = \\pm \\frac{4}{5}$. For $\\cos B = \\frac{4}{5}$, let $a_1 = BC$. Then by the Law of Cosines, $b^2 = a_1^2 + 100 - 200 a_1 \\frac{4}{10} = a_1^2 - 16a_1 + 100$. For $\\cos B = - \\frac{4}{5}$, let $a_2 = BC$. Then by the Law of Cosines, $b^2 = a_2^2 + 100 - 200 a_2 (-\\frac{4}{10}) = a_2^2 + 16a_2 + 100$. Subtracting these equations, we get $a_2^2 - a_1^2 + 16a_2 + 16a_1 = 0$. We can factor as $(a_2 - a_1)(a_2 + a_1) + 16(a_2 + a_1) = 0$. Since $a_1 + a_2$ is positive, we can safely divide both sides by $a_1 + a_2$, to get $a_2 - a_1 + 16 = 0$. Hence, $a_1 - a_2 = 16$.\nLLM Input-Output Solution\nTo find the lengths of side $BC$ in the two possible triangles $ABC$ given $AB = 10$, $AC = b$, and $\\sin B = \\frac{3}{5}$, we will employ the Law of Sines."}, {"title": "From the Law of Sines, we know:", "content": "$\\frac{a}{\\sin A} = \\frac{b}{\\sin B} = \\frac{c}{\\sin C}$\nwhere a, b, and c are the sides opposite angles A, B, and C, respectively.\nIn our triangle, we assign $AB = c = 10$, $AC = b$, $BC = a$, and $\\sin B = \\frac{3}{5}$.\nThe Law of Sines gives us:\n$\\frac{a}{\\sin A} = \\frac{10}{\\sin B}$\nFrom which we can write:\n$\\frac{a}{\\sin A} = \\frac{10}{\\sin B}$\nSince $\\sin B = \\frac{3}{5}$, we can substitute:\n$a = \\frac{10 \\sin A}{\\sin B} = \\frac{10 \\cdot \\frac{3}{5} \\sin A}{\\frac{3}{5}} = \\frac{50 \\sin A}{3}$\nUsing the Law of Sines again for side b:\n$\\frac{b}{\\sin B} = \\frac{a}{\\sin A}$\nWe have:\n$\\frac{b}{\\frac{3}{5}} = \\frac{a}{\\sin A}$\nRearranging gives:\n$b \\cdot \\sin A = a \\frac{3}{5}$\nFrom the triangle's cosine rule:\n$a^2 = b^2 + 10^2 - 2 \\cdot b \\cdot 10 \\cdot \\cos B$\nUsing the sine identity $\\sin^2 B + \\cos^2 B = 1$, we get:\n$\\cos B = \\sqrt{1-\\sin^2 B} = \\sqrt{1-(\\frac{3}{5})^2} = \\frac{4}{5}$\nSubstituting into the cosine rule:\n$\\frac{4}{5} = b^2 + 100 - 2 \\cdot b \\cdot 10 \\cdot \\frac{4}{5}$\nCalculating 2 \u00b7 b \u00b7 10 \u00b7 $\\frac{4}{5}$ = 16b, we get:\na^2 = b^2 + 100 - 16b = b^2 - 16b + 100\nWe have the equation a = $\\frac{50 \\sin A}{3}$. To find $\\sin A$, we notice $b^2 - 16b + 100$ can yield different values for two different angles. Let $A_1$ and $A_2$ be the angles corresponding to the two triangles possible due to the ambivalence in triangle formation.\nWhen considering these angles, the two possible values arise from:\n$\\sin A_1 = \\frac{3A_1}{5}$\nand similarly for the alternate $A_2$.\nThe key observation is that both A values yield a values symmetric around 10. The two sides will be exactly equal distances away from a midpoint. Using numeric approximations where b = 11, b = 12, etc., leads to side length differences. Final calculation yields:\n$|a_2 - a_1| = |C_2 + C_1| = 6$\nTherefore, the positive difference between the lengths of side BC in these two triangles is 6."}, {"title": "Decomposition-Only Solution", "content": "Q1: What are the known values and definitions for triangle ABC?\nA: In triangle ABC: AB = 10, AC = b (where b > 10), sin B = $\\frac{3}{5}$.\nFor the derivation of cos B: Since sin B = $\\frac{3}{5}$, using the Pythagorean identity:\n$\\sin^2 B + \\cos^2 B = 1$\n$(\\frac{3}{5})^2 + \\cos^2 B = 1$\n$\\frac{9}{25} + \\cos^2 B = 1$\n$\\cos^2 B = 1 - \\frac{9}{25} = \\frac{16}{25}$\n$\\cos B = \\pm \\frac{4}{5}$\nQ2: How to express sin A in terms of b using the Law of Sines?\nA: $\\sin A = \\frac{3b}{50}$.\nQ3: How do we determine the lengths of side BC using the Law of Sines?\nA: $BC_1 = \\frac{100}{b}$.\nQ4: Are there different cases for angle A that affect the length of BC?\nA: No, both acute and obtuse angles give the same result due to the symmetry of sine.\nQ5: What is the positive difference between the lengths of side BC in the two triangles?\nA: 0.\nFinal Answer: 0"}, {"title": "Error Analysis", "content": "In the Direct Solution", "the same result due to the symmetry of sine\" when in fact the Law of Cosines with different cos B values leads to distinct triangle configurations whose BC lengths differ by 16.\nAOT Reasoning Process\nFirst initialize the origin question as Qo.\nDecomposition of Qo": "n\u2022 Q: What are the values of the known sides triangle ABC? A: \u0410\u0412 = 10.\n\u2022 Q: What boundary conditions are known? A: AC = b > 10.\n\u2022 Q: It is known that sin B = $\\frac{3"}, {"A": "Use the Pythagorean identity", "Q1": "Given two triangles ABC satisfying AB = 10"}, {"Q1": "n\u2022"}]}