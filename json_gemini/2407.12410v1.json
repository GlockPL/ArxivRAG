{"title": "Proximity-based Self-Federated Learning", "authors": ["Davide Domini", "Gianluca Aguzzi", "Nicolas Farabegoli", "Mirko Viroli", "Lukas Esterle"], "abstract": "In recent advancements in machine learning, federated learning allows a network of distributed clients to collaboratively develop a global model without needing to share their local data. This technique aims to safeguard privacy, countering the vulnerabilities of conventional centralized learning methods. Traditional federated learning approaches often rely on a central server to coordinate model training across clients, aiming to replicate the same model uniformly across all nodes. However, these methods overlook the significance of geographical and local data variances in vast networks, potentially affecting model effectiveness and applicability. Moreover, relying on a central server might become a bottleneck in large networks, such as the ones promoted by edge computing. Our paper introduces a novel, fully-distributed federated learning strategy called proximity-based self-federated learning that enables the self-organised creation of multiple federations of clients based on their geographic proximity and data distribution without exchanging raw data. Indeed, unlike traditional algorithms, our approach encourages clients to share and adjust their models with neighbouring nodes based on geographic proximity and model accuracy. This method not only addresses the limitations posed by diverse data distributions but also enhances the model's adaptability to different regional characteristics creating specialized models for each federation. We demonstrate the efficacy of our approach through simulations on well-known datasets, showcasing its effectiveness over the conventional centralized federated learning framework.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated learning (FL) [1], [2] has dramatically trans-formed the landscape of machine learning, enabling collaborative model training across geographically distributed datasets while upholding data privacy. This paradigm is particularly crucial in scenarios where data contains sensitive information, such as healthcare or personal mobile device usage [3], [4], which are legally or ethically restricted from central aggregation.\nTraditional FL employs a central server to coordinate the aggregation of model updates across a network, which, while effective in smaller, controlled environments, scales poorly with the expansion of network size and geographic distribution. This central reliance becomes a bottleneck, introducing significant delays and increased communication overhead, which are further exacerbated by the growth in edge computing and IoT devices [5]. Recent FL adaptations, such as hierarchical and distributed approaches [6], [7], reduce the reliance of single central components to manage the federation process. However, these approaches still integrate all models equally and therefore fail to account for the variations in data distribution across different regions [8]. This leads to poor generalization across different geographic regions and underlying operational contexts and can adversely affect the model's effectiveness and accuracy.\nAddressing these limitations, this paper introduces a novel concept in federated learning called proximity-based selffederated learning (PSFL). Our methodology diverges from traditional federated learning by promoting a decentralized federation of models where nodes form clusters or federations based on both geographic proximity [9] and data similarity. By leveraging principles from self-organizing systems and inspired by novel macroprogramming paradigms (i.e., programming paradigms which have a collective abstraction as a first-class citizen [10]), like aggregate computing [11] (i.e., a top-down global-to-local programming model which express collective computation as a composition of computational fields), PSFL allows nodes within close physical or network proximity to collaborate more closely on model training, dynamically causing the systems to partition into federations where a single shared model emerges, thus enhancing the relevance and effectiveness of the federation-wise learned models.\nThe main contributions of this paper are:\n\u2022We propose a new decentralized federated learning framework that enables dynamic federation formation based on geographic and data-driven criteria based on space-fluid computing [12] (i.e., a modern approach to sample a spatially distributed phenomena) without the need of sharing data and a central coordinating node.\n\u2022Through simulations on well-known datasets, we demonstrate that PSFL significantly outperforms traditional centralized federated learning approaches, particularly in largely heterogeneous environments.\nThe rest of the paper is structured as follows: Section II provides the context in which our work is situated, Section III details the proposed PSFL approach and its implementation, Section IV presents the evaluation results, and Section V concludes the paper with a discussion of future work."}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": "This paper addresses the challenges posed by performing distributed cooperative learning in modern highly distributed systems including pervasive computing, collective adaptive systems, the internet of things, cyber-physical systems, and edge computing. These systems exhibit distinctive characteristics that complicate their management like distribution, situatedness, and scale, and they pose significant challenges in coordinating these systems [13], [14], including:\n1)Locality principle: operational efficiency and cost are influenced by the principle of locality, with dependencies on the proximity of sources, processes, and users.\n2)Control and decision-making: a balance between cen-tralized and decentralized control is essential, as ex-tremes are neither feasible nor desirable.\n3)Dynamic environments: constant changes in the envi-ronment and system structure, driven by factors such as mobility and component failures, demand adaptive responses.\nA recurrent solution in such scenarios consists in dynamically partitioning large network in contiguous regions such that the nodes in each region can efficiently cooperate one another. This approach is particularly effective in scenarios where the data distribution is highly dynamic and can change rapidly over time. These regions can be used for several purposes, from decentralized services orchestration to enhance scalability and performance [15], WSN middleware for organized communication and power management [16], distributed data processing in IoT networks [17], and dynamic control in robot swarms [18].\nAn example solution to achieve such partitioning without global supervisioning is the self-organizing coordination region (SCR) [14] pattern, which works as follows: a system-wide multi-leader election process determines a set of leaders among a set of leader candidates; the system (or its environment) forms a self-organising set of regions, each one regulated by a single leader; within each region, a feedback loop is established, whereby the leader receives upstream information flows from the members of the region (possibly leveraging intermediaries) and emits a control information flow downstream.\nInformation within these regions is disseminated primar-ily through gossip protocols and gradient-based information casts [19]. Gossip protocols are effective for disseminating information with the cost of a higher number of messages, while gradient-based approaches propagate information across increasing distances, useful for defining regional boundaries and leader-user communication.\nAccumulating information, especially in larger networks, poses greater challenges. While gossip methods can suffice in small regions, scalable solutions like spanning tree and multipath techniques are necessary for larger areas.\nSCR can be implemented using various programming paradigms, but it is particularly effective when using aggregate computing [11], which provides tools for expressing self-organising computations via functional composition of smaller \u201cbricks\u201d enhancing modularity and reusability. These \u201cbricks\u201d are:\n\u2022S (for Sparse-choice i.e., a scattered selection from the set of participating devices) and its variants, like the one based on space-fluid sampling [12] (i.e., the leader are selected based on a perception of a spatial distributed phenomena),\n\u2022G (for Gradient-cast i.e., a multicast diffusing information along a gradient),\n\u2022C (for Converge-cast\u2014i.e., a multicast aggregating infor-mation to a sink device).\nMoreover, this paradigm seems to be particularly effective in the context of distributed machine learning, as already demonstrated in related works [13], [20]\u2013[22]."}, {"title": "A. Self-organising Coordination Region", "content": "This paper addresses the challenges posed by performing distributed cooperative learning in modern highly distributed"}, {"title": "B. Federated Learning", "content": "Federated learning [1], [2] is a machine learning framework that aims to collaboratively train a unique global model from distributed datasets. This technique has been introduced to enable training models without collecting and merging various datasets into a single central server, thus making it possible to work in contexts with strong privacy concerns (e.g., hospital or banking data). Furthermore, in highly distributed systems, given the large amount of data that can be generated by various clients, it becomes infeasible to move all data to a single central server [23]. In this context, FL becomes crucial as it allows leveraging the computational capabilities of the various clients.\nEven though various versions of FL exist [24], they all share a common learning flow. Traditionally (as shown in Figure 1), FL is based on a client-server architecture and comprises the following steps:\n1)Model initialization: the central server initializes a com-mon base model that is shared with each client;\n2)Local learning: each client performs one or more steps of local learning on its own dataset;\n3)Local models sharing: each client sends back to the central server the new model trained on its own data (i.e. the local model);\n4)Local models aggregation: the local models collected by the central server are aggregated to obtain the new global model.\nThis process is carried out iteratively for a predefined number of global rounds.\nThe classical client-server architecture presents some limi-tations, namely: i) in federations with many clients, the server may be a bottleneck; ii) the server is a single point of failure, and if it fails to communicate with the clients, the entire the learning process is interrupted; and iii) the server must be a trusted entity, which could be a challenging constraint in some scenarios. For these reasons, various distributed federated learning approaches have been introduced [25]\u2013[27], based on peer-to-peer or semi-centralized architectures."}, {"title": "C. Data heterogeneity", "content": "Federated learning algorithms achieve remarkable perfor-mance when data are homogeneously distributed among clients. However, in real-life datasets, data are often heteroge-neous, namely non-independently and identically distributed (Non-IID). This heterogeneity can be caused by various factors [28]. For instance, consider a scenario in which we have a huge amount of stations for PM10 sampling distributed across Europe. In this case, data heterogeneity emerges since stations grouped in a certain geographical area have similar data distributions among themselves but different from those in other areas. Data heterogeneity can be categorized in various ways based on how the data are distributed among the clients [29], [30]. The main categories include: i) feature skew: all clients have the same labels but different feature distributions (e.g., in a handwritten text classification task, we may have the same letters written in different calligraphic styles); ii) label skew: each client has only a subset of the classes; and iii) quantity skew: each client has a significantly different amount of data compared to others.\nNon-IID data is a critical aspect because in such scenarios, individual clients have local objectives that diverge from the global one [29]. As a result, after the local training, the clients produce vastly different updates (i.e., local update drift) that, once aggregated, lead to a reduction in the accuracy of the global model or convergence issues [30]. For these reasons, FL with non-IID data is a growing research field. In the literature, there are various families of approaches aiming to enhance learning in these scenarios. One approach involves attempting to train a single global model by improving base algorithms (e.g., FedAvg [31]) through the addition of regularization terms to constrain the local updates or considering that each device may perform a different number of local training steps. This is implemented by various algorithms, including: FedProx [32], Scaffold [33], and FedNova [34]. Another possible approach \u2013 the one we focus on \u2013 involves dividing the devices into clusters and having multiple global models, one for each cluster, in order to create specialized models."}, {"title": "D. Clustered Federated Learning", "content": "Clustered federated learning is an approach to tackle data heterogeneity. It is a technique of personalized FL in which, instead of a single global model, multiple models are trained to target various local distributions. It is based on the assumption that clients can be divided into subgroups (i.e., clusters), and that within each cluster, clients have data that follow similar distributions (i.e., IID data). Moreover, studies [35], [36] show that aggregating models among clients within the same cluster leads to improved performance and personalization; while aggregating models from different clusters results in a degradation of the accuracy of the global model. A key aspect of these approaches is how to measure the similarity among various clients. Generally, there are three methods [37], namely: i) loss-based: clients measure the losses of the models from various clusters on their own data, with the model having the lowest loss considered most similar [35]; ii) gradient-based: clients measure similarity based on the distance between gradient updates [36], [38], [39]; and iii) weight-based: clients measure similarity based on the weights of their models [36], [40].\nIn literature, several algorithms based on client clustering have been proposed, such as: PANM [37], FedSKA [41], IFCA [35]. However, all these algorithms are based on centralized architectures and require prior knowledge of the number of clusters. This significantly impacts the final accuracy, as an incorrect choice of this hyperparameter can lead to a considerable degradation in performance."}, {"title": "III. PROXIMITY-BASED SELF-FEDERATED LEARNING", "content": null}, {"title": "A. Problem Statement", "content": "As depicted in Figure 2, we consider $A = \\{a_1,a_2,...,a_k\\}$ the spatial area divided into k distinct continuous areas. Each area $a_j$ has a unique local data distribution $\\Theta_j$; and provides specific localized information. This means that, giving two area i,j for two data distributions $\\Theta_i$ and $\\Theta_j$, a sample $d'$ from $\\Theta_i$ is distinctively dissimilar from a sample $d''$ from $\\Theta_j$. This dissimilarity can be quantified using a specific distance m(d', d''), which determines the disparity between two distributions. Formally, giving an error bound $\\delta$, the dissimilarity intra-area and inter-area can be quantified as:\n$\\forall i \\neq j,\\forall d, d' \\in \\Theta_i, \\forall d'' \\in \\Theta_j : m(d,d') \\leq \\delta < m(d,d'')$\n(1)\nIn A, a set of sensor nodes $S = \\{s_1,s_2,...s_n\\}$ (n > A) are deployed, each capable of processing data and participating FL. Sensors are assumed to be randomly uniformly distributed, and we let the position of a sensor $s_i$ be $p_i = (x_i, y_i)$: each sensor will be located in a specific area $a_j$, though this information is not available to sensors. Moreover, each node has a certain communication range $r_c$ and hence can communicate only with nodes within that range: this form a neighbourhood $N_i$ for each node i. Locally, each node i creates a dataset $D_i$ from samples perceived from the data distribution $\\Theta_j$. In this work, we consider a general classification task where each sample $d_i$ in the data distribution $\\Theta_j$ consists of a feature vector $x_i$ and a label $y_i$. Therefore, the complete local dataset $D_i$ is represented as $D_i = \\{(x_1,y_1), (x_2,y_2), ..., (x_m, y_m)\\}$. This dataset is used to train a local model $nn_i^t$ where t is the time step of the learning process. The $nn_i^t$ is initialized with a common neural network model $nn_i^0$ shared among all nodes. For sake of simplicity, we limited the learning process in T time steps, where at each time step t, each node $s_i$:\n\u2022perform local training on its local dataset $D_i$ to get a local model $nn_i^t$;\n\u2022can share its model to one (or all) of its neighbours in $N_i$\nFor each time step t, the nodes should devise a set of feder-ations $F_t = \\{f_1, f_2,..., f_m\\}$, where each $f_j$ is characterized by a leader node $l_j$ that is responsible for the federation and a group of nodes that are part of the federation. Giving T the number of time steps, the goal of PSFL is to create a set $F_T$ which approximates the areas A and finds a federation-wise model $nn_{f_j}^T$ for each federation $f_j$ (i.e., the one created by the leaders). Let $nn^T = \\{nn_{f_1}^T, nn_{f_2}^T,..., nn_{f_m}^T\\}$ be the set of all federation-wise models.\nGiven a correct federations partitioning, we want to find a set of federation-wise models that minimize the error across all areas, which can be formally described as:\n$\\min_{nn^T} \\sum_{i \\in A} \\sum_{nn_j^T \\in nn^T} L(\\Theta_i, nn_j^T)$\n(2)\nWhere L represent the average error computed by a loss function from each sample $x_i$ and label $y_i$ in the dataset $\\Theta_i$ using the model $nn_j^T$."}, {"title": "B. Proposed solution", "content": "Our goal is to effectively organize the sensor nodes into federations that enhance learning efficacy by exploiting federation-wise data similarities. To achieve this, we utilize a distributed federated learning approach structured with SCR [14] (see Section II-A) in which the regions are the federations of the learning process.\nThe PSFL process can be roughly described in four steps:\n1)Federation Creation: Nodes form a federation via a distributed multi-leader election process. The elected leaders are responsible for the coordination and management of federation activities.\n2)Models Collection: The leaders collect models from each node within the federation.\n3)Model Aggregation: Based on the aggregated models, the leaders synthesize a unified federation-wise model that reflects the collective learning outcomes of the federation.\n4)Model Distribution: The synthesized model is then disseminated by the leaders to all nodes in the federation, ensuring that each node operates with the most updated and accurate model reflective of their shared data environment."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "To autonomously establish federations (Step 1 of the FL process) aligning with local data distributions, we utilize a decentralized, multi-leader approach inspired by the \"space-fluid\" sparse choice algorithm [12]. In this method, federations form around a nominated leader, based on the leader's proximity to the nodes and the dissimilarity of their data distributions. Indeed, federations expand and contract dynamically based on the accumulated dissimilarity of their data distributions, computed from the nodes' models. Specifically, we consider a loss-based dissimilarity measure, denoted as ds(i, j), calculated by $L_{i,j} + L_{j,i}$. Here, $L_{i,j}$ and $L_{j,i}$ represent the average loss function using local dataset $D_i$ and the model from the adjacent node $nn_j^t$, respectively. Formally:\n$L_{i,j} = L(D_i, nn_j^t) = \\frac{1}{\\vert D_i \\vert} \\sum_{(x,y)\\in D_i} l(f(x;nn_j^t),y)$\n(3)\nf(x; nnj) is the prediction of the model nnj on the input x, and l one of the standard loss functions (e.g., cross-entropy, mean squared error).\nWe introduce a gradient field $G : N \\rightarrow R$ [19], constructed based on the dissimilarity metric ds(i, j) and using the leaders of the federations as source zones. This constructs a potential field where, for each node, the shortest accumulated error to the leader is defined. For example, consider a simple ring topology composed of three nodes, a \u2192 b \u2192 c, where a is the leader of the federation. The gradient field G(c) is then computed as the sum of the errors from c to b and from b to a, namely: G(c) = ds(c,b)+ds(b, a). Thus, a node can decide to join a federation based on the error relative to the leader, effectively creating a federation bounded by a maximum path error \u03c3. In this approach, this gradient field is continuously updated, ensuring that the federations align closely with data distributions.\nStarting from this notion of accumulated error, we can better illustrate the federation creation process, consider the following steps:\n1)Each node proclaims its candidacy for becoming a leader within its local neighbourhood.\n2)Nodes propagate the leadership candidacy information to their neighbours, including the candidate's model and the error metric computed from the leader to the candidates (i.e., computing G).\n3)Nodes evaluate received candidacies and disregard those where the path error from the leader exceeds a predefined threshold \u03c3.\n4)In scenarios where multiple valid candidacies are recognized, nodes employ a competition policy to select the most suitable leader, ensuring optimal federation configuration based on local data characteristics.\nThe competition policy can be based on various criteria, such as the quality of the model, the error metric, or just the node identifier. In this solution, we both use the accumulated error metric and the node identifier to break ties. Leveraging this space-fluid sparse choice approach, the following statement holds: for every node i in federation $f_i$, the condition $G(n_i) < \\sigma$ is satisfied. By choosing an appropriate \u03c3, we ensure that the federations are coherent with the perceived data distributions. Indeed, even if it is not possible to guarantee a direct relation between \u03c3 and \u03b4, the latter can be used as a reference to determine the former: higher \u03b4 values require higher \u03c3 values to ensure that a federation j aligns with the data distribution $\\Theta_i$ of the underlying area i.\nThe application of this pattern is structured to promote federated learning and adaptation:\n\u2022Continuous Model Exchange: At each time step, nodes exchange their models with neighbours. This exchange includes the integration of newly received models into each node's local training processes, thereby enhancing the model's accuracy and relevance.\n\u2022Specialized training: The process of repeatedly exchanging and updating the model creates a continuous loop that improves the model's training across a specific network. This loop gradually enhances the accuracy and efficiency of the model, allowing it to better represent areas with similar data characteristics and to create smaller, more focused models compared to the overall global model. Additionally, it increases the model's ability to make predictions in these areas by continuously incorporating new insights from similar data experiences.\nBy leveraging this structured approach to federated learning, we ensure that each federation not only optimizes its internal operations but also contributes effectively to the overarching goal of achieving high-accuracy, federation-specific models within the distributed learning network."}, {"title": "A. Learning setup", "content": "To evaluate the effectiveness of our approach, we performed learning on a well-known dataset in computer vision, namely: Extended MNIST [42]. Particularly, we used the version with handwritten letters, which contains 124800 train samples and 20800 test samples from 26 classes. Data have been synthetically partitioned to obtain multiple non-IID distributions and simulate labels skewness (i.e., a device has only a subset of the labels). More specifically, we conducted several simulations with a variable number of areas $|A| \\in \\{3,5,9\\}$. We performed a synthetic partitioning of the data as done in other works in the literature such as [29], [43] \u2013 rather than using real-world non-IID datasets, to be able to control the degree of label skewness and to simulate various situations with different numbers of areas, enabling a more extensive evaluation.\nThe learning process was conducted using both the algorithm proposed by us and the classic centralized FedAVG (utilized as a baseline for comparisons). The same hyperparameters were employed for both approaches. A simple MLP neural network with 128 neurons in the hidden layer was trained for a total of 60 global rounds. At each global round, every device performed 2 epochs of local learning using a batch size of 64, the ADAM optimizer with a learning rate of 0.001, and a weight decay of 0.0001. Additionally, in the PSFL algorithm proposed, FedAVG was used to aggregate the models within the various federations.\nAll simulations\u00b9 were conducted using the Alchemist simulator [44], which facilitated the division of data across various areas, spatial distribution of devices, and their neighbourhood relationships. The PSFL was implemented using ScaFi [45]\u2014\u0430 macroprogramming language for aggregate computing combined with PyTorch for the neural network training. For the baseline algorithm, we conducted 20 simulations with varying seeds. Conversely, for the PSFL, we conducted 180 simulations varying: i) the seed (from 0 to 19); ii) the number of areas (3, 5, and 9); and iii) the loss threshold \u03c3 (20, 40, and 80) used to determine the maximum expansion of a federation. Moreover, we conducted 10 additional simulations with nine areas in which we explored the effects of introducing a new node into an already stable system. The primary goal was to evaluate the system's ability to self-adapt without disrupting the existing federations. A special node, denoted as \u2605, was introduced to traverse all nine areas in sequence. The movement pattern of \u2605 includes systematically moving from one area to the next, covering the entire set of areas. Upon entering each new area, \u2605 remains there for a fixed period of 5 time steps to simulate interaction and impact on the area. The total duration of the simulation is 140 time steps."}, {"title": "B. Metrics", "content": "Regarding the learning process, we have extrapolated the loss and the accuracy for each node. The loss used in this case is a negative log-likelihood loss which is defined as:\n$NLL(D, nn) = \\frac{1}{N} \\sum_{i=1}^N y_i log(nn(x_i)), where x_i, y_i \\in D$\n(4)\nwhere nn(xi) is the predicted probability of the true label yi.\nThe accuracy was calculated as:\n$Accuracy = \\frac{Number of correct predictions}{Total number of predictions}$\n(5)"}, {"title": "C. Results", "content": "The results of our study were systematically collected during two distinct phases:\n1)training phase: this phase involved monitoring the vari-ations in model performance over time and assessing the accuracy of federations (see Figure 5);\n2)testing phase: stable federations were tested by intro-ducing new test data to evaluate the overall effectiveness (Figure 6).\nAnalysis shows that the threshold parameter is crucial in the formation of federations. A lower threshold increases the number of federations due to a more restrictive error metric, particularly noticeable in scenarios with fewer areas (e.g., 3). This is because the areas cover a larger portion of space and the federations depend on the error accumulated by the leader. The greater the spatial coverage, the longer the path that the leader and the nodes will cover, resulting in a larger G. Conversely, higher threshold values reduce the number of federations but improve their accuracy. This trend is more pronounced with a larger number of areas (e.g., 9). This is because, this time the areas are smaller and the federations, with a very large error, manage to incorporate others.\nThroughout the learning process, there is generally a reduction in loss and an improvement in accuracy. However, under conditions of high threshold values and many areas, the performance is more unstable as federations are inaccurately formed, leading to ineffective learning processes and inferior models. Despite these issues, our results consistently outperform the baseline across all conditions (see Figure 4 for a visual representation of the federations formed), notably in configurations with more areas. This is expected because, as illustrated in Section II, averaging models trained on nonIID datasets can lead to a significant reduction in overall performance. In this case, as the number of areas grows, the skewness of the data also increases, thereby deteriorating the performance of the model trained with the centralized algorithm.\nChart analysis of node movement (Figure 7) indicates that federations remain stable even as new nodes enter the system. After an initial period of instability, the system reorganizes, forming new federations as coherent as the previous ones. The behaviour of the system is evident when observing two key metrics: DL and NLL - Validation. Immediately following the node's movement phase, the DL metric stabilizes near zero. Conversely, the NLL \u2013 Validation initially peaks but subsequently stabilizes at a low value. Interestingly, when the node transitions to a new area, it attempts to establish a new federation but is prevented by a high error metric. Eventually, the node integrates into an existing federation, aiding in the stabilization of the system as the number of federations, denoted by |F|, returns to the optimal count of nine."}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "Our study introduce PSFL, a novel self-organising federated learning framework that leverages decentralized federation of models based on geographic proximity and data similarity. This approach addresses the challenges posed by data heterogeneity in traditional federated learning systems by allowing for dynamic federation formation without a central coordinating node. The results show that PSFL outperforms the baseline FedAVG algorithm across various scenarios, particularly in configurations with a larger number of areas. This highlights the potential of PSFL to enhance federated learning processes in real-world applications, where data distribution is inherently non-IID and when the aggregators nodes are not known a priori.\nFuture research may expand the evaluation of our approach to other well-known datasets, employing new partitioning methodologies to simulate diverse distributions and further increasing the number of areas and nodes to encompass scenarios of varying complexities. Additionally, it would be interesting to evaluate the system's performance when multiple nodes move simultaneously. This investigation could determine whether such movement is beneficial, perhaps due to cross-fertilization among federations, or detrimental due to the instability it causes in federations. Building on this point, and considering the dynamic nature of the systems we are studying, another area for future work could involve integrating continuous learning techniques to adapt models to data that evolves over time."}]}