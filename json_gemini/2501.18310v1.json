{"title": "Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis", "authors": ["Haoxiong Liu", "Jiacheng Sun", "Zhenguo Li", "Andrew C Yao"], "abstract": "The synergy between deep learning models and traditional automation tools plays a pivotal role in developing robust neural theorem provers (NTPs). However, for proof synthesis with LLMs, pre-vious work applies automation tools either only when the model explicitly calls the method, or only at a single granularity level, failing to fully exploit the power of built-in tactics and off-the-shelf automated theorem provers. In this work, we propose ProofAug, a novel theorem proving method that enjoys superior sample efficiency through equipping proof-generation LLMs with automation methods in different granularities via fine-grained structure analysis of model-generated proof proposals. Furthermore, ProofAug serves as a versatile plug-and-play module that seamlessly integrates with any tree-search algorithm, enabling our construction of an efficient recursive proving (ERP) module to further enhance performance. The superiority of our method is validated on the miniF2F-test benchmark using the open-source deepseek-math-7b-base model and the Isabelle proof assistant. Notably, by additionally employing a mixed prompting strategy, we achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9% for the original version), setting a new SOTA across all proof languages with a total sample budget of only 2100. Our code is available at https://github.com/haoxiongliu/ProofAug.", "sections": [{"title": "1. Introduction", "content": "Automated theorem proving is a field that not only appeals to mathematicians seeking efficient formalization and proof automation for mathematical theorems, but also finds sig-nificant applications in real-world industries such as inte-"}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. A unified view of theorem proving with generative language models", "content": "We first formulate the task of theorem proving in the context of language modeling and introduce our notations. We assume there is a global alphabet \\(\u03a3\\). An ITP is formulated as a triple (A, S, T), where:\n\\(A \u2282 \u03a3^*\\) is the set of proof steps. There is also a parser \\(Parse: \u03a3^* \u2192 A^*\\) that transforms a string into a sequence of proof steps.\nS is the set of states. A state \\(s \u2208 S\\) represents our ab-straction of the proof object underlying the ITP, which contains all information the ITP needs to proceed with the proof, including s.state, s.error, s.finish, etc.,"}, {"title": "3. Method", "content": "Algorithm 2 describes our method, ProofAug. It mainly consists of three parts: finding the Maximal Compatible Semi-Proof (MCSP), proof augmentation, and an optional Efficient Recursive Proving (ERP) module. In this section, we state each of them in details. For an illustrative walk-through example of ProofAug, refer to Figure 1."}, {"title": "3.1. Find the Maximal Compatible Semi-Proof", "content": "Given a proof proposal \\(y\\) sampled from \\(\u03c0(\u00b7|p(xi||yi, xf))\\) (where \\(p(\u00b7, \u00b7)\\) is a few-shot/zero-shot prompter that ap-propriately combines its arguments), the first procedure of ProofAug is to find the Maximal Compatible Semi-Proof (MCSP) of \\(yf\\)."}, {"title": "3.2. Proof Augmentation", "content": "The goal of ProofAug is to determine whether there exists a compatible semi-proof that can be completed into a valid proof using ATPs or built-in heuristic proof methods to fill the sorry gaps. Instead of first finding out all compatible semi-proofs and trying them one-by-one, we should design a more efficient way that makes use of the overlap among these semi-proofs to avoid waste of computation resources spent on calling ATPs. The resulted proof augmentation algorithm is shown in Algorithm 2 (Ignore the ERP module for now)."}, {"title": "3.3. Efficient Recursive Proving", "content": "Solving a complicated theorem from a single proof proposal generated by the language model \\(\u03c0(\u00b7|\u00b7)\\) is challenging even with our ProofAug procedure. By simply sampling more proof proposals, we can probably get better results as long as there is some randomness underlying the decoding strategy of \\(\u03c0\\). We ask the question: how can we outperform this naive retrying strategy, given a fixed sample budget?"}, {"title": "4. Experiments", "content": "A summary of our experimental results and comparison with previous methods is shown in Table 2. Below, we state the experimental setup and describe how different parts of the results are obtained and provide additional ablation study results."}, {"title": "4.1. Experimental Setup", "content": "We evaluate our methods on miniF2F (Zheng et al., 2021), a benchmark for formal Olympiad-level mathematics prob-lems in four different formal languages. Specifically, we use the Isabelle/Isar part of the miniF2F-test dataset pro-vided by Jiang et al. (2023), which provides an additional informal statement and informal draft for each problem. We use the PISA environment (Jiang et al., 2021) to inter-act with Isabelle 2022. Our <ATP> method is the com-bination of 8 Isabelle proof methods (auto, simp, auto, blast, fastforce, eval, sos, arith, simp:field_simps, simp add:mod_simps) and Sledgehammer. Following Jiang et al. (2023), we set the timeout for any proof step and Sledge-hammer as 10s and 120s, respectively. During verifica-tion, we run 12 PISA instances in parallel on one In-tel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz."}, {"title": "4.2. Results of ProofAug", "content": "We do ablation study to validate the effectiveness of ProofAug (without the ERP module). We first get the DSP baseline results in our setting. Compared to the perfor-mance of the original DSP implementation on miniF2F-test, our DSP baseline is 9.9% higher (49.2% v.s 39.3% in 100 attempts) due to differences on proving environments6, choice of prompting method and model difference. Then we compare ProofAug results with our DSP baselines. Under sample budgets of 1, 10 and 100, ProofAug achieves per-formance of 36.5%, 44.7% and 52.5%, significantly outper-forming the baseline by 7.8%, 4.1% and 3.3%, respectively."}, {"title": "4.3. Results of the ERP module", "content": "We try 500 attempts of proof for each problem, using zero-shot prompting for ProofAug w/ and w/o ERP module. When then ERP module is turned on, we record the cumula-tive number of queries and stop verifying when it reaches 500 for a fair comparison."}, {"title": "4.4. Cumulative Results of Mixed Strategy Experiments", "content": "During the completion of this work, we have tried various setups to search for the best recipe. While some setups fail to achieve the best performance on the benchmark compared to results we present above, diverse ways of generating proofs help prove more theorems. The setups include: 1) Three versions of demonstration examples. for few-shot prompting generation and two versions of zero-shot prompt are used in our experiments. 2) Whether to include Informal drafts. The informal drafts could mislead LLMs. Although ProofAug can sometimes be a rescue, it is not always the case. Besides, we find some incorrect theorem statements in the miniF2F-test dataset during our experiments, so we build a curated dataset and part of the experiments are done on the curated version."}, {"title": "5. Discussion and Future Directions", "content": "Although this work demonstrates that our ProofAug method significantly improves the performance of the neural the-orem prover on the miniF2F benchmark, there remains a notable gap in tackling IMO-level problems or research-level challenges within formal systems. How to incorpo-rate ProofAug into the expert iteration methods (Xin et al., 2024a) or online learning methods (Lample et al., 2022) to further improve the performance is one direction of the future work."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Other Related Work", "content": "In the main body, Table 1 and Table 2 only include representative neural theorem proving methods that are most related to this work: they all target on solving Olympic-level mathematical problems with generative language models. Below we introduce other related work."}, {"title": "B. Examples of the DSP Issues", "content": "Figure 4 and Figure 5 show examples of the 'Hard Conjecture' issue and the 'Complicated Draft' issuse described in Section 1."}, {"title": "C. Discussion of Implementing ProofAug for Other Proof Assistants", "content": "Lean. Although Lean proofs can be completely tactic-based, block-structures starting with by are very common as well, especially for the proofs of Olympics-level math problems (refer to Xin et al. (2024b) for the example proofs they find). As a result, Lean proofs are also amendable for proof structure analysis with some modifications. As to the substitution of the <ATP> method, firstly, there are built-in tactics such as simp, ring, nlinarith similar to the heuristics proof methods in Isabelle that have a predefined set of rules (facts) to use. Besides, there have been projects that can provide Lean"}, {"title": "D. Theorem Proving Environment Setup Details and Discussion on the Reproducibility", "content": "Our theorem proving environment setup mainly follows that described in Jiang et al. (2023), but there are some subtle differences. For the setup of PISA, we set the candidate provers of Sledgehammer to CVC4 vampire verit e spass z3 zipperposition, with a 30s timeout for each and 120s in total, as in Jiang et al. (2023). However, we find that in their PISA source code, they use cvc5 instead of CVC4 and the actual total timeout for Sledgehammer is around 35s instead of the intended 120s, seemingly due to an implementation bug. We fix this bug and still use CVC4 since cvc5"}, {"title": "E. Prompt Construction", "content": "The prompts in our experiments are all constructed from three components: prompt template, few-shot demonstration examples. and a prompter. The prompt template is a JSON file that contains different roles as names and corresponding templates as values. Each example is also an JSON object whose names occur in the templates. A prompter mainly consists of a chat template that maps the sampled examples to a conversation consisted by messages according to the prompt template."}, {"title": "F. Breakdown of the Mixture of Strategies", "content": "Table 3 shows the breakdown of the mixture of strategies in Section 4.3. Below are some explanations of the options appearing in Table 3."}, {"title": "H. An Example of How ProofAug Induce a Proof", "content": "Figure 9 shows an example of ProofAug inducing a proof from a failed proof proposal generated by the model. It can be seen that the initial proof fails inside the proof...qed block of proving \\(a(2 \u2013 a) \u2264 1\\) due to the wrong intermediate claim \\(2 * a - a\u00b2 = (a - 1)\u00b2 + 1 \u2013 a\u00b2\\). ProofAug finds that \\(a(2 \u2013 a) \u2264 1\\) can be directly proved with the proof method sos given the previous proved fact \\((a \u2013 1)\u00b2 \u2265 0\\)."}]}