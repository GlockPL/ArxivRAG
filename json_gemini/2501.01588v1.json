{"title": "Fine-Tuning PHI-3 for Multiple-Choice Question Answering: Methodology, Results, and Challenges", "authors": ["Mohamed Hisham"], "abstract": "Abstract-Large Language Models (LLMs) have become essential tools across various domains due to their impressive capabilities in understanding and generating human-like text. The ability to accurately answer multiple-choice questions (MCQs) holds significant value in education, particularly in automated tutoring systems and assessment platforms. However, adapting LLMs to handle MCQ tasks effectively remains challenging due to the hallucinations and unclear prompts. This work explores the potential of Microsoft's PHI-3[1], a compact yet efficient LLM, for MCQ answering. Our contributions include fine-tuning the model on the TruthfulQA dataset, designing optimized prompts to enhance model performance, and evaluating using perplexity and traditional metrics like accuracy and F1 score. Results show a remarkable improvement in PHI-3.5's MCQ handling post-fine-tuning, with perplexity decreasing from 4.68 to 2.27, and accuracy rising from 62% to 90.8%. This research underlines the importance of efficient models in adaptive learning systems and educational assessments, paving the way for broader integration into the classroom, particularly in fields like test preparation, student feedback, and personalized learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have evolved to become a cornerstone of natural language processing (NLP) tasks, including text generation, translation, summarization, and question-answering, as it is clear in I. Their capacity to understand and generate human-like text has led to impressive breakthroughs in various applications. However, despite their success in generating coherent and contextually relevant text, less attention has been directed toward their performance in more structured and specialized tasks, such as answering multiple-choice questions (MCQs).\nMCQ answering presents unique challenges for LLMs as it demands more than text generation. It requires a deep comprehension of the question's context, reasoning through potential answers, and the ability to discern and select the correct answer from multiple provided options. These tasks are critical in educational contexts, where automated systems are increasingly used for assessments, tutoring, and adaptive learning environments. The ability to accurately answer MCQs can directly impact the effectiveness of educational platforms, test preparation services, and personalized learning tools.\nThis paper investigates how Microsoft's PHI-3, a compact and resource-efficient LLM designed initially for general text generation, can be fine-tuned and adapted to handle MCQ answering tasks with high accuracy. While large models like GPT-4 and PaLM have demonstrated strong performance across many NLP tasks, we focus on the benefits of smaller models like PHI-3, which offer practical advantages in terms of deployment in constrained environments, such as on educational platforms with limited computational resources.\nOur main contributions are as follows:\n\u2022 A comprehensive exploration of fine-tuning PHI-3 for MCQ answering, leveraging the TruthfulQA dataset for training and evaluation.\n\u2022 A novel approach to prompt design significantly improves the model's performance by reducing common issues such as hallucinations and irrelevant responses.\n\u2022 An in-depth evaluation of the fine-tuned model using a range of metrics, including perplexity, accuracy, F1 score, and recall, to provide a holistic view of its capabilities.\nThe ability to adapt smaller, resource-efficient models like PHI-3 for specialized tasks such as MCQ answering offers excellent potential for educational applications. From automated testing and student assessments to adaptive learning tools, such models can be pivotal in modernizing education systems. This work demonstrates the viability of adapting LLMs for these tasks and highlights the importance of fine-tuning and prompt engineering in achieving high performance.\nThe structure of this paper is as follows:\nSection II provides a comprehensive review of related work, including an overview of LLMs and their applications in education.\nSection III presents the methodology used to fine-tune PHI-3 for MCQ answering, including dataset details and the"}, {"title": "II. RELATED WORK", "content": "The application of Large Language Models (LLMs) in question-answering (QA) tasks has garnered significant attention, with various models and datasets explored across different domains. One prominent approach is using multiple-choice questions (MCQs) as a robust and efficient evaluation method for LLMs. Studies have demonstrated that, although traditional models like BERT and GPT have shown strong performance in QA tasks, they typically require substantial computational resources for fine-tuning and deployment. This computational cost has driven interest in smaller, more resource-efficient models, such as Microsoft's PHI-3, initially designed for text generation. Still, it has shown promise in other areas when appropriately fine-tuned.\n\nThe use of MCQs as evaluators for LLMs has been explored in various works. In [2], MCQs were demonstrated to be effective and robust evaluators for assessing LLM capabilities, presenting a structured environment where models could showcase reasoning, comprehension, and decision-making skills. Building on this, [3] explored generating MCQS from textbooks, pushing the boundaries of automatic question generation for educational purposes. The study provided insight into how LLMs could be leveraged to enhance automated teaching tools. Additionally, [4] examined whether LLMs could replace human evaluators in MCQ-based assessments, suggesting that while promising, these models still faced challenges in reliably replacing human judgment, particularly in subjective tasks.\n\nThe usefulness of MCQs in detecting the reasoning abilities of LLMs was further discussed in [5], where the researchers examined how well these models could reason through structured formats. These studies highlighted that while LLMs excel in sentence completion tasks, as seen in the work by [6], they often struggle with more nuanced forms of reasoning, such as understanding common sense, as explored by [7]. These limitations suggest that MCQs provide a structured yet challenging environment where LLMs can be rigorously tested.\n\nIn more domain-specific settings, studies like [8] have applied LLMs to solve mathematical word problems, highlighting the potential of these models in specialized fields. Similarly, the MMLU (Massive Multitask Language Understanding) benchmark introduced by [9] provided a comprehensive dataset for evaluating LLMs across multiple academic subjects, including the sciences and humanities, through the lens of MCQs. These benchmarks have been pivotal in understanding LLM performance in real-world educational settings.\nFurthermore, transforming MCQs into open-ended questions, as demonstrated by [10], opens new possibilities for adapting structured QA formats into more flexible and less constrained question styles. The AI2 Reasoning Challenge by [11] further expanded on this, offering a dataset that pushes LLMs to demonstrate reasoning abilities akin to human-level problem-solving.\n\nRecent efforts in improving factual accuracy have led to the introduction of the TruthfulQA dataset, designed as a challenging benchmark for LLMs to answer factual questions without hallucinations [1]. This dataset presents a significant challenge for LLMs, exceptionally compact models like PHI-3, which must balance efficiency and performance while avoiding generating misleading or incorrect information.\n\nWhile these studies have made considerable strides in evaluating LLMs, there remain gaps in how smaller models, such as PHI-3, can be adapted for specialized tasks like MCQ answering. Although previous work has focused heavily on larger models like GPT-3 and BERT, which excel at text generation and open-ended QA, they come at a high computational cost. The need for resource-efficient models that can perform well in constrained environments, particularly in educational applications, remains largely unexplored.\nThis work addresses these limitations by adapting Microsoft's PHI-3 model for MCQ answering, focusing on fine-tuning and prompt design to improve model performance. Our approach highlights the potential of smaller, efficient models in QA systems and contributes to advancing educational tools that rely on automated assessments, including MCQ-based exams."}, {"title": "III. METHODOLOGY", "content": "Our methodology follows a well-defined pipeline of data preprocessing, prompt design, model fine-tuning, and evaluation. The key steps in this pipeline are outlined below:\n1) Dataset Preprocessing: We use the TruthfulQA dataset, which contains 1,000 MCQs across various categories. One challenge with this dataset is the inconsistent number of options per question. To standardize the input, we limited the number of wrong answers and retained the best correct answer for each question. This preprocessing step ensured the model"}, {"title": "IV. EXPERIMENTAL DESIGN", "content": "We utilized the TruthfulQA dataset for this study, which consists of factual MCQs across various categories like science, history, and general knowledge. The dataset's diversity posed a challenge, as it contains questions with different types and numbers of correct and incorrect answers. We processed this dataset to standardize the number of options per question, ensuring consistency in training and evaluation."}, {"title": "V. RESULTS AND DISCUSSION", "content": "Our experiments show that fine-tuning PHI-3.5 significantly improved its performance in answering MCQs.\nWe observed a sharp decrease in perplexity from 4.68 to 2.27 post-fine-tuning, indicating more confident predictions. Accuracy improved from 62% to 90.8%, and F1 score increased from 66 to 90.6. The results indicate that prompt design and fine-tuning significantly improved the model's MCQ answering capability.\nDespite the promising results, PHI-3.5 has limitations. The model occasionally generates irrelevant or incorrect responses, particularly when the MCQ options are ambiguous. Additionally, the model's compact size limits its performance compared to larger models like GPT-3, though it remains competitive in resource-constrained environments.\nOne of the key limitations encountered was overfitting during prompt-based fine-tuning, particularly with the initial prompt format. The model consistently selected the last option,"}, {"title": "VI. CONCLUSION", "content": "This study demonstrates the potential of PHI-3 for answering MCQs after fine-tuning. We improved the model's accuracy, F1 score, and perplexity through prompt engineering and dataset preprocessing, making it a viable option for applications requiring efficient models. Future work will focus on further improving the prompt design and addressing the limitations identified in this study.[12]"}]}