{"title": "PEnG: Pose-Enhanced Geo-Localisation", "authors": ["Tavis Shore", "Oscar Mendez", "Simon Hadfield"], "abstract": "Cross-view Geo-localisation is typically performed at a coarse granularity, because densely sampled satellite image patches overlap heavily. This heavy overlap would make disambiguating patches very challenging. However, by opting for sparsely sampled patches, prior work has placed an artificial upper bound on the localisation accuracy that is possible. Even a perfect oracle system cannot achieve accuracy greater than the average separation of the tiles. To solve this limitation, we propose combining cross-view geo-localisation and relative pose estimation to increase precision to a level practical for real-world application. We develop PEnG, a 2-stage system which first predicts the most likely edges from a city-scale graph representation upon which a query image lies. It then performs relative pose estimation within these edges to determine a precise position. PEnG presents the first technique to utilise both viewpoints available within cross-view geo-localisation datasets to enhance precision to a sub-metre level, with some examples achieving centimetre level accuracy. Our proposed ensemble achieves state-of-the-art precision with relative Top-5m retrieval improvements on previous works of 213%. Decreasing the median euclidean distance error by 96.90% from the previous best of 734m down to 22.77m, when evaluating with 90\u00b0 horizontal FOV images. Code will be made available: tavisshore.co.uk/PEnG.", "sections": [{"title": "I. INTRODUCTION", "content": "Localisation is vital in the majority of mobile robotics applications. Common techniques such as Global Navigation Satellite Systems (GNSS) provide absolute positioning data to clients. These are prone to failure in certain environments. One example are dense urban canyons such as New York City where tall buildings cause signal occlusions & reflections, preventing successful satellite communication. Another example are regions of conflict where malicious actors purposefully disrupt positioning by spoofing signals, inserting erroneous information.\nImage localisation may provide a solution as agents can fully self-localise using onboard sensors, removing requirements for external communication. These techniques aim to relate an agent's query image with previously seen geo-tagged images, determining an updated position according to feature and positional similarities with these references. A large proportion of mobile robots are already equipped with cameras, increasing the viability of image localisation. Cross-View Geo-localisation (CVGL) is an increasingly popular branch of image localisation research, offering a viable form of generalisable wide-scale image localisation. The objective is to relate a street-level query image to a database of reference satellite images - returning the geographic coordinates of the highest correlating known satellite image.\nPose estimation is a related field aiming to determine a camera's pose within a scene. These techniques generally operate at a smaller scale than CVGL, localising within a few metres, instead of whole cities. They generally operate as continuous prediction, rather than retrieval problems, and operate in N-Degrees of Freedom (DoF) as opposed to simple geographic coordinates. Pose estimation has two primary sub-fields - Absolute Pose Estimation (APE) and Relative Pose Estimation (RPE). APE aims to determine a camera's position and orientation within a 3D world coordinate frame. RPE aims to compute the same, but with respect to a reference camera."}, {"title": "II. RELATED WORKS", "content": "RPE can be divided into two categories: feature matching, and pose regression. More traditional camera localisation techniques often utilise structure-based methods, representing a scene with an explicit SfM or SLAM reconstruction [2], [3], [4]. This often requires a large number of images to have already been captured within a scene, limiting generalisation.\nShotton et al. [5] introduce a novel method called Scene Coordinate Regression Forest (SCoRe Forest) for inferring the pose of an RGB-D camera relative to a known 3D scene using a single image with decision forests. Kendall et al. propose PoseNet [6], the first CNN designed for end-to-end 6-DOF camera pose localisation, evaluating the network thoroughly to prove the viability of deep learning for the field. In their following paper [7], they apply a principled loss function based on the scene's geometry to learn camera pose without any hyper-parameters, achieving state of the art (SOTA) results, reducing the performance gap to traditional methods. Sattler et al. [4] propose using a prioritised matching approach, considering features more likely to yield 2D-to-3D matches, terminating searches once sufficient matches have been found. Brachmann et al. [8] propose DSAC, a differentiable counterpart to RANSAC, replacing the deterministic hypothesis selection with a probabilistic selection, deriving the expected loss with respect to all learnable parameters. Applying this to image localisation achieved higher accuracies than previous deep learning based methods. Clark et al. [9] propose extending to sequential camera pose estimation, designing an RNN which achieves smoothed poses and greatly reduced localisation error. Sarlin et al [10] propose HFNet - performing coarse-to-fine image localisation by predicting local features and global descriptors for 6-DoF localisation simultaneously. Map-free Relocalisation [11] introduces using a single photo from a scene for metric scaled re-localisation, negating the requirement to construct a scaled map of the scene. Rockwell et al. [12] propose FAR, combining correspondence estimation and pose regression techniques to utilise the benefits from both to provide precision and generalisation. Wang et al. [13] and Leroy et al. in the follow-up paper [14] propose Dust3r and Mast3r respectively. Both are techniques for dense unconstrained stereo 3D reconstruction of arbitrary image collections, with no prior information. Mast3r achieves SOTA performance in various fields including camera calibration and dense 3D reconstruction. Moreaul et al. [15] propose CROSSFIRE - using NeRFs as implicit scene maps and propose a camera re-localisation algorithm for this representation. CROSSFIRE achieves SOTA accuracy and is capable of operating in dynamic outdoor environments.\nSimilar to how FAR proposed combining multiple pose estimation paradigms to achieve SOTA performance in that particular sub-field, we propose combining multiple image localisation techniques to achieve high precision localisation in large scale regions with different input modalities."}, {"title": "B. Cross-View Geo-Localisation", "content": "Current CVGL techniques primarily focus on embedding retrieval extracting reduced dimensionality representations of reference satellite images, aiming to return geo-coordinates from those most similar to query images. Techniques are being increasingly proposed to improve performance by manipulating extracted features, [16], [17], [18].\nWorkman and Jacobs [19] first propose CNNs for learning feature relationships across viewpoints. This was extended by Lin et al. [20], treating each query uniquely, utilising euclidean similarities for retrieval. Vo and Hays [21] add rotation information through an auxiliary loss, evaluating misalignment impact. CVM-Net [22] add NetVLAD [23] to the CNN, aggregating local feature residuals to cluster centroids. Liu and Li [24] increase access to orientation information, improving the latent space robustness. Shi et al. [25] developed a spatial attention mechanism, improving feature alignment between views. In [26] they increase the cross-view feature similarity, by applying the techniques to limited-Field-of-View (FOV) data. This was important due to the ubiquity of monocular cameras compared with panoramic cameras, increasing feasibility. [27] computes feature correlation between ground-level images and polar-transformed aerial images, shifting and cropping at the strongest alignment before performing image retrieval. Toker et al. [28] synthesised streetview images from aerial image queries before performing image retrieval. L2LTR [29] developed a CNN+Transformer network, combining a ResNet backbone with a vanilla ViT encoder to increase performance over SOTA. TransGeo [16] proposed a transformer that uses an attention-guided non-uniform cropping strategy to remove uninformative areas.\nIn GeoDTR [30], [31], Zhang et al. separate geometric information from the raw features, learning spatial correlations within visual features to enhance performance. Zhu et al. introduced SAIG [17], an attention-based CVGL backbone, representing long-range interactions among patches and cross-view associations with multi-head self-attention layers. BEV-CV [18] introduces Birds-Eye-View (BEV) transforms to the field, reducing representational differences between viewpoints to create more similar embeddings. Sample4Geo [32] propose two CVGL sampling strategies, geographically sampling for optimal training initialisation, mining hard-negatives according to feature similarities between viewpoints. SpaGBOL [33] propose progressing the CVGL field from single and sequential representations to graph-based representation, allowing for more geo-spatially strong embeddings.\nTo date all of the above CVGL approaches have followed a retrieval paradigm where the accuracy of results is limited by the granularity of the geo-referenced database. Sparsely sampled data can lead to higher retrieval rates due to greater feature dissimilarities, while densely sampled data may enhance localisation precision but decrease performance, as overlapping satellite image patches increase the likelihood of incorrect retrievals"}, {"title": "III. METHODOLOGY", "content": "We propose leveraging the advantages of both techniques in a single two-stage system to achieve high-precision city-scale localisation, shown in top-down order in Figure 1. Taking as input a street-level image - the first stage performs city-wide CVGL, predicting the most recently observed road junction. Operating the CVGL stage at the scale of road junctions, helps to keep the reference set lean and discriminative, ensuring efficient and accurate retrieval results of coarse location. The second stage takes the CVGL sub-region predictions and performs RPE along neighbouring roads, merging likelihoods from both stages to determine a final 3-DoF pose. This novel combination of learned computer vision techniques achieves a reduction in the median localisation error from 734m to 22.77m, evaluating with 90\u00b0 crops of the StreetLearn dataset [1].\nIn summary, our research contributions are:\n\u2022\tIntroduce the first technique for performing precise image localisation in a city-scale by utilising information from both image viewpoints in CVGL datasets.\n\u2022\tIntroduce emulating a simple compass, filtering reference embeddings according to a configurable yaw threshold, greatly increasing localisation precision.\n\u2022\tDemonstrate strong generalisation to cities not seen in training - localising with a median error of 22.77m within the large dense region of Manhattan, considering a region area of 36.1km\u00b2."}, {"title": "A. City-Scale Geo-Localisation Data Representation", "content": "We frame CVGL as a graph comparison problem, similar to the technique demonstrated in SpaGBOL [33]. Where SpaGBOL established a lower bound on localisation precision by only applying graph nodes at road junctions, we incorporate orders of magnitude more nodes by placing secondary nodes along existing edges, enhancing the density of data. These graphs now have two classes of nodes, denoted primary nodes N representing road junctions, and secondary nodes Q captured along roads at varying intervals. This significant increase in data density greatly increases the precision upper bound. Each node in both classes has attributes - {Isat, Istreet, L, \u03a8, B}, containing a panoramic streetview image and a satellite image both RGB: Ii \u2208 R3\u00d7Wi\u00d7Hij \u2208 {street, sat}, location L = {, } consists of geographical latitude and longitude coordinates, \u03a8 \u2208 R : {\u2212180\u00b0 < \u03a8 \u2264 180\u00b0} is the north-aligned camera yaw, and B = {\u03b21, ..., \u03b2\u03ba} are north-aligned bearings to K neighbouring nodes where \u03b2\u2208R: {-180\u00b0 < \u03b2 \u2264 180\u00b0}.\nWe limit the streetview image's (Istreet) FOV to increase the technique's feasibility as a large proportion of existing vehicles possess monocular cameras. Cameras are assumed to be fixed to the vehicle in a forward-facing configuration. We experiment with FOVs, \u0398\u2208 {70\u00b0, 90\u00b0, 120\u00b0}."}, {"title": "B. PEnG Procedure", "content": "Our proposed technique, PEnG, operates in two stages, initially estimating candidate primary nodes with graph-based CVGL before performing RPE relative to the secondary nodes present along each candidate edge until a threshold is met, or all candidate edges have been processed.\nThe main purpose of the first stage is to reduce the number of reference images when performing relative pose estimation. This enables city-scale pose estimation as without it, pose estimation takes orders of magnitude longer.\n1) Graph-Based Cross-View Geo-Localisation: We perform CVGL following the standard procedure as used within previous works [18], [22], [27]. We implement a siamese-like network of CNN feature extractors, with no weight sharing, to produce similar embeddings nt from corresponding streetview-satellite image pairs. Creating a database of reference embeddings offline, querying this database for retrievals during online operation.\nnt = CNN (It|wt), t \u2208 {street, sat} (1)\nIn the first stage, CVGL retrievals are only performed on primary nodes Ni to provide efficient and accurate initial filtering. Retrieved reference embeddings are ordered by descending similarity with the query, and are then min-max normalised to between 0 & 1 giving a confidence score ci for each candidate node - concluding this stage. Top candidate nodes, Ck, are passed to the second stage depending on the minimum confidence threshold \u03b8c and maximum number of candidates k.\nci =\nscale\n(\nquery\npref\n,0,1) (2)\nCk = {Ci|Ci > \u03b8c and i < k} (3)\n2) Pose Refinement: For each candidate node, c, we select that candidate's connected edges, Ec = {ei,j|i = cor j = c}. We then filter these edges by matching the compass heading and the edge's yaw within the graph. For every remaining candidate edge, we then perform RPE in two stages: first estimating a coarse position of the query image along an edge before refining this relative to the two neighbouring reference secondary nodes.  For each image pair along an edge I\u00b9 & I2, we determine the set of cross-image pixel correspondences. We then use a transformer-based network to predict 3D pointmaps, X1,1, X1,2, from 2D points x\u00b2 between these images, expressed in the coordinate frame of I\u00b9. The pointmaps are then compared X1,1 \u2190\u2192 X1,2, computing the relative poses with RANSAC & PnP [34].\nThe objective of PnP is to minimise the reprojection error between the 3D points and their corresponding 2D image projections:\nx\u00b2 = K(RX\u00b2 + t) (4)\nWhere x is the projected 2D point, X\u00b2 is the 3D world point, K is the estimated camera intrinsic matrix, R & t are the rotation and translation matrices. RANSAC randomly samples 4 points for PnP, optimising the objective to estimate R and t.\nWe compute the reprojection error as ei = ||xi-K(RXi+\nT)||, rejecting outliers based on a predefined threshold \u20ac. We then maximise the number of inliers e\u00a1 <e to achieve the best pose estimate (R*,t*):\n(R*, t*) = argmax 1(ei \u2264 \u20ac) (5)\nR,t\nwhere (ei \u2264 e) is the indicator function - equals 1 if ei is less than or equal to a predefined threshold \u20ac, 0 otherwise.\nAll reference poses, Pr, are estimated prior to system operation, calculating a median 3-DoF rotational matrix for each edge F. As this is a preprocessing step, a larger number of iterations are used compared to during inference. These pre-determined poses then initialise optimisation processes during operation, reducing the required number of iterations - leading to lower operating times without effecting performance.\n until thresholds such as Maximum Rotational Error Ore or No. Candidate Nodes on are achieved. Rotational error Rerr is the 3-DoF summed euclidean distance between the query rotation RQ and the median edge rotation . This is calculated with an [X, Y, Z] axis weighting of [1, 0.25, 1] as roll has a smaller impact on performance. Where a query has multiple pose estimations and an L2 distance threshold has not been met, each pose is given a confidence score - rotational errors are summed and min-max scaled to between 0 & 1. Confidence scores from both stages are considered to determine a final pose estimation, calculated by scaling the relative poses to between the edge's ground truth limits."}, {"title": "IV. RESULTS", "content": "To verify the contribution of each constituent in the proposed system, we display an ablation study in Table I. CVGL shows the performance of the simple ConvNeXt-T feature extractor, evaluated in the same method as previous works - filtering by primary nodes initially to reduce the reference set. 1 Pose performs pose estimation against an entire edge's reference images, determining a relative 2-DoF pose between primary nodes. 2 Pose follows 1 Pose with a refined pose estimation relative to the 2 adjacent reference secondary nodes, determined in the first pose estimation step this enables a high precision final estimate. Pose Priors is the addition of estimating the pose of all secondary nodes prior to querying, increasing the accuracy of reference poses and offloading a portion of computation to an offline stage.\nThe ablation shows the vast decrease in median distance error achieved by combining these two localisation techniques, the median error decreases by an order of magnitude. Having a pose refinement stage after the initial position estimation further decreases median error by \u2248 3m. Finally, estimating reference poses prior to operation increased accuracy relatively by \u2248 10%.\nWe evaluate with distance-based Top-K recall accuracy, displaying euclidean distance errors in Cumulative Distribution Function (CDF) plots - displayed in Figures 6.  We evaluate how PEnG performs with images of varying FOV, with higher-FOV cameras tending to be more expensive but able to capture more information. All comparisons follow the 2-stage process: first predicting the closest primary node, then estimating the closest position within the reduced subset of connected secondary nodes. To demonstrate the generality of the PEnG approach we present results with both a traditional retrieval first stage, PEnG, and a graph-based first stage, PEnG*.\nTo increase fairness in comparison against traditional single-stage CVGL works, we augment these baselines with a secondary refinement stage where the same technique is run again, but only required to match against the ground-truth satellite images of the corresponding secondary nodes. In a real-world use case this is infeasible, as the reference set cannot contain precisely geographically aligned ground truth satellite images. However, it serves to provide a stronger baseline for comparison.\nThe evaluation shows that our proposal achieves significant improvements over current SOTA. With 90\u00b0 images, we achieve a 96.90%% reduction in median error, and an approximate 213%% increase in Top-5m accuracy. We note that using 90\u00b0 FOV images achieves a relative decrease in the median error of \u2248 4m compared to 70\u00b0. This is due to the increase in information available to each stage. However, further increasing the FOV to 120\u00b0 yields a decrease in localisation precision. This may be caused by the input image dimensionality limitation of our model - due to the backbone pre-training, the maximum image resolution for the system is 512 \u00d7 384, placing an upper bound on how much information can pass through the system. Another hindrance is experienced from extracting perspective images from a 360\u00b0 panorama. When increasing the horizontal FOV beyond 90\u00b0, these crops begin to display visibly distortion.\nWithin the discretised Top-Km metrics, PEnG performs slightly worse than previous works where K < 5 due to the inherent zero error bias in existing CVGL works. As K reaches 25m, performance is significantly higher across Fields-of-View (FOVs). As precisely centred ground-truth corresponding satellite images are known for each query streetview image in CVGL, they tend to perform unrealistically well with these Top-K metrics. This peculiarity of previous evaluation protocols is visible in Figure 6 where at x = 0, previous works start from a non-zero values."}, {"title": "V. CONCLUSION & FUTURE WORK", "content": "We successfully propose and demonstrate the utility of combining graph-representations, CVGL, and relative pose estimation techniques. This ensemble is proven to be a viable strategy for progressing CVGL within a large city-scale environment towards practicality, reducing median distance errors from hundreds of metres down to often centimetre level accuracy. PEnG achieves SOTA localisation precision when evaluated within the Manhattan region of 36.1km\u00b2, reducing the median error from Sample4Geo's previous best of 734m down to 22.77m when operating with 90\u00b0 FOV. In our ablation studies, we thoroughly demonstrate the significance of each portion of the 2-stage architecture, validating that the combination results in the maximum precision possible for PEnG. We release code for converting the StreetLearn dataset into the graph representation outlined above, along with PEnG technique's code and corresponding pretrained weights, enabling future works to build upon the technique and further evaluate this ensemble.\nSeveral aspects of this work will be the target for optimisation in order to further progress the field towards real-world application. Due to the vast disparity in viewpoint within CVGL, performance from the first stage limits the potential precision achieved in the second stage. A more probabilistic fusion technique could mitigate this. Furthermore, the second stage of PEnG, RPE, can be computationally costly compared to the first stage. There is a trade-off between accuracy and complexity, based on the number of iterations performed with RANSAC+PnP. Future work could explore sequential extensions of the technique, introducing temporal priors into the position estimation, to further filter the reference set and reduce the number of iterations required."}]}