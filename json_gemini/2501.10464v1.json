{"title": "Adapting Beyond the Depth Limit: Counter Strategies in Large Imperfect Information Games", "authors": ["David Milec", "Vojt\u011bch Kova\u0159\u00edk", "Viliam Lis\u00fd"], "abstract": "We study the problem of adapting to a known sub-rational opponent during online play while remaining robust to rational opponents. We focus on large imperfect-information (zero-sum) games, which makes it impossible to inspect the whole game tree at once and necessitates the use of depth-limited search. However, all existing methods assume rational play beyond the depth-limit, which only allows them to adapt a very limited portion of the opponent's behaviour. We propose an algorithm Adapting Beyond Depth-limit (ABD) that uses a strategy-portfolio approach - which we refer to as matrix-valued states for depth-limited search. This allows the algorithm to fully utilise all information about the opponent model, making it the first robust-adaptation method to be able to do so in large imperfect-information games. As an additional benefit, the use of matrix-valued states makes the algorithm simpler than traditional methods based on optimal value functions. Our experimental results in poker and battleship show that ABD yields more than a twofold increase in utility when facing opponents who make mistakes beyond the depth limit and also delivers significant improvements in utility and safety against randomly generated opponents.", "sections": [{"title": "1 INTRODUCTION", "content": "There has been a lot of progress on solving large imperfect information games, recently often enabled by depth-limited methods [3, 4, 23]. The high-level idea behind these methods is to avoid searching through the full game tree, which would be intractable. Instead, depth-limited search only looks a few steps ahead and uses a value function - typically a trained neural network - to approximate the events that occur beyond the depth limit. However, compared to perfect-information games such as chess and Go, the presence of imperfect information complicates depth-limited search in a number of ways. For example, being able to start the search from the current state of the game, as well as determining which value function to use, both become conceptually (and computationally) difficult [6].\nWhile most existing work is concerned with fully rational opponents, this assumption will often be false in practice, for example because the opponent is cognitively or computationally limited [1, 25]. Several prior works investigate depth-limited methods for dealing with subrational opponents [12, 19, 21]. However, these methods are conceptually quite challenging and the research on this topic seems far from complete. In particular, one limitation of the existing methods is their inability to take advantage of the opponent's mistakes that only occur beyond the current depth limit. (As a contrived example, imagine that saying \"please let me win\u201d would always cause the opponent to give up in d + 1 steps from now. If a method only looks d steps ahead and assumes that the opponent plays optimally afterward, it will be unable to exploit this vulnerability.)\nIn this paper, we study the problem of robust adaptation that is, performing well against a known opponent that is not fully rational (represented by some fixed stochastic strategy) while not being overly vulnerable to unexpected opponents. We are interested in solving this problem for large (two-player) imperfect-information games. Since such games are typically too complex to compute the full strategy at once, we focus on the online play setting (where the game is being played in real time). We operationalize the robust adaptation problem by making the conservative assumption that (rather than facing the fixed opponent all the time) there is some small probability p that we are instead facing a worst-case opponent (i.e., one that attempts to minimize our utility). Importantly, this formulation of the problem does not require the knowledge of the opponent's preferences \u2013 in other words, despite also being interested in general-sum games, it suffices to consider the problem for zero-sum games.\nDisregarding the issue of the prohibitive size of the game, robust adaptation has a straightforward solution [14]: Consider an imperfect-information game that consists of two copies of the original game and an extra chance decision which secretly determines whether we face a fixed or fully rational opponent, then find a Nash equilibrium (NE) of this game (Figure 1; for more details, see Section 2). Note that the fully rational opponent is aware that we are attempting to find a robust response to the fixed opponent, so their strategy will generally be different from the NE of the original game. This makes this problem more difficult than simply finding a best response to a convex combination of the fixed opponent and Nash equilibrium.\nHowever, this approach would be intractable in large games, which means that the main difficulty lies in combining this method with depth-limited search. To find a more scalable solution to the robust adaptation problem, we propose a novel approach we call Adapting Beyond Depth-limit (ABD). Our point of departure is that the traditional approach to depth-limited solving is to call an optimal value function upon reaching the depth limit [17, 23]. In contrast, ABD uses what we refer to as matrix-valued states: upon reaching the depth limit, each player is given a choice of one strategy in their portfolio, after which both players receive the utility that corresponds to their joint choice [5, 17]. The key insight behind ABD is that by leveraging the matrix-valued state representation, we"}, {"title": "2 BACKGROUND", "content": "To model imperfect information in games, we adopt the standard extensive-form model.\nDefinition 1 (EFG). A two-player zero-sum extensive form game (EFG) G = (A,H, pl, \\pi_c, I, u) is a tuple for which:\n\u2022 A = A1 \u222a A2 are the sets of actions of each player\n\u2022 H is the set of legal histories in G. Formally, H is a finite tree on A (i.e., it consists of sequences of elements of A and is closed under initial segments). Leaves of H are called terminal states and denoted z \u2208 Z.\n\u2022 pl : H \\setminus Z \u2192 {1, 2, c} determines which player acts at h.\n\u2022 A(h) := {a \u2208 A | ha \u2208 H} denote the set of actions that are legal at the given state.\n\u2022 For pl(h) = c, \u03c3c(h) \u2208 \u2206(A(h)) is the chance strategy at h.1\n\u2022 u: Z \u2192 R\u00b2 is a zero-sum utility function s.t. u2 = -\u04381.\n\u2022 I = (I1, I2) represent the information sets (infosets) of each player. Formally, each Ii is partition of H that\u00b2 conveys enough information to identify player i's legal actions. We assume that each player has perfect recall.\nWe will abbreviate \"player one\" (resp. two) as P1 and P2.\nEach player plays the game using a behavioural strategy \u03c3i. Formally, \u03c3i is a mapping \u03c3i : {h\u2208H |pl(h) = i} \u2194 \u03c3i(h) \u2208 \u2206(A(h)) which determines how to randomise over legal actions at every information set. \u03a3i is the set of all strategies of player i. A strategy \u03c3i is said to be pure when each of the distributions \u03c3i(h) is deterministic. Since each pair of strategies (\u03c31, \u03c32) induces\n\u00b9Where \u2206(X) denotes the set of all probability distribution over a finite set X.\n2In other words, we assume - for convenience - that the notion of player's information is defined everywhere in the game, rather than only when the player is acting."}, {"title": "2.1 Robust Adaptation: Restricted Nash Response", "content": "Consider a situation where we believe that with probability p\u2208 [0, 1], our opponent in a game G will use some fixed strategy \u03c3fix \u2208 \u03a32, but with the remaining 1\u2212p probability, they will play rationally (given that they know about our beliefs). This situation can be formalised as looking for a restricted Nash equilibrium [14], which is defined as a pair of strategies (\u03c31, \u03c32) such that \u03c31 is a best response to \u03c32 and \u03c32 is a best response to p \u00b7 \u03c3fix + (1 \u2212 p) \u00b7 \u03c3\\'_2. The \u03c3\\'_1 part of this pair is called p-restricted Nash response to \u03c3fix. It can be obtained by finding the (standard) Nash equilibrium of what we will refer to as the robust adaptation game RbAdapt(G, p, \u03c3fix):\nStructurally, RbAdapt(G, p, \u03c3fix) consists of a chance node and two copies of G (Figure 1). The chance node serves as the root of the game and its outcome is only observable to player two. With probability p, the chance node leads to a copy of G where P2's decisions are replaced by chance nodes which select actions according to \u03c3fix. With probability 1 - p, the chance node leads to a copy of the game that works identically to the original game G (except that P1 does not know which copy got selected)."}, {"title": "2.2 Online Play in Large Games: Continual Resolving", "content": "A common goal in studying large games is to find their Nash equilibrium, in the sense of being able to write down the whole strategy at once. However, some games are too large for this to be tractable e.g., too large to even fit the full strategy into memory. A more tractable alternative to this approach is online play, which only aims to allow the user to play the game according to some strategy. Formally, it aims to provide a mapping which takes an information set I \u2208 I1 as an input and outputs a probability distribution \u03c31(I) \u2208 \u2206(A1(I)) for some \u03c3\u2217 \u2208 NE(G) that is consistent across all I. While such methods might require costly pre-training, the eventual computation of individual probability distributions \u03c31(I)"}, {"title": "2.2.1 Restarting Search Mid-Game: Re-solving Gadget", "content": "The high-level idea behind re-solving is simply to replace the full game by a sub-game rooted at the current state. While this is trivial in perfect-information games, extending this approach to imperfect information games requires some care. First, to preserve the possibility of reasoning about uncertainty, the \"root\" of the game must include all histories from the current public state S [16]. (Where public states are defined as a partition of histories that is closed under the membership to both players' information sets.) Second, the naive approach (called unsafe re-solving) \u2013 of replacing the initial part of the game by a single chance node, with the probability of each h\u2208 S being proportional to its reach probability under some Nash equilibrium - unfortunately generally results in an exploitable strategy [6]. (Intuitively, this is because P1 might make mistakes that can only be punished by taking non-equilibrium actions in the part of the game that already took place.) This issue can be avoided by augmenting the root of the game with a re-solving gadget [6, 22]"}, {"title": "2.2.2 Depth-Limited Solving via Value Functions", "content": "A common approach to depth-limited solving is to only consider a limited look-ahead tree, in which every history h at the depth limit d \u2208 N is turned into a terminal state. The utilities of these new leaves are then given by some optimal value function v which assumes that, beyond the depth limit, both players play according to some Nash equilibrium \u03c3\u2217. In practice, the value function is typically implemented by a neural network trained to approximate the exact values [23]. As a complicating factor in imperfect-information games, the values of optimal values functions depend not only on the current history h, but also on the strategy that the players used prior to reaching h [17]. This makes such value functions more costly to both train and use during deployment."}, {"title": "2.3 Depth-Limited Solving via Matrix-Valued States", "content": "An alternative approach to depth limited solving (first described by Brown et al. [5], used by Brown and Sandholm [4], and further studied by Kova\u0159\u00edk et al. [17]) relies on a construction that resembles the normal-form representation of EFGs. The idea behind the classical normal-form representation of an EFG is that instead of acting sequentially, the players specify their full strategy ahead of time [27]. However, one can also consider a depth-limited analogy of this concept, where the agents act sequentially for the first d steps of the game, and then only specify their full strategy for the remainder of the game. In the idealised version of this approach, each player would be able to chose from all of their pure strategies. In practice, this would be intractable, so each player is only allowed to chose from a more limited portfolio Pi \u2282 \u03a3i. Depending on the quality (i.e., representativeness) of the portfolios, this method can recover an exact - or at least approximate - NE of the game [5, 17].\nDefinition 2 (Matrix-valued depth-limited game). Let G be a two-player EFG, d \u2208 N and Pi \u2282 \u03a3i. The corresponding matrix-valued depth-limited game Gd (P1, P2) is defined as the EFG that works as follows: For histories of length < d, Gd (P1, P2) works identically to G. For histories of length d, the information sets of each player are the same as information sets in G. However, unlike in G, both players are active in these histories and their legal actions correspond to selecting \u03c01 \u2208 P1, resp. \u03c02 \u2208 P2. This yields payoffs uGd (P1,P2) (h, \u03c01, \u03c02) := E[ui(z) | z \u223c (\u03c01, \u03c02), z extends h] and terminates the game.\nUsing this terminology, the existing methods [4, 5, 17] can be understood as finding an NE of particular games Gd (P1, P2). The name \"matrix-valued states\" is a reference to the multi-valued states used by Brown et al. [5] and comes from the fact that in Gd (P1, P2), the histories at depth d can be viewed as matrices of size |P1| \u00d7 |P2|."}, {"title": "3 RELATED WORK", "content": "Although opponent modeling in games has been extensively studied [24], this paper takes a different approach. We assume that an"}, {"title": "3.1 Adapting to Opponents in Smaller Games", "content": "A key feature of smaller games is that we can always compute an exact best response. Consequently, there are many works which focus on on creating the opponent model and then adapting to it by shifting to the best response [2, 8, 11, 28].\nThere is also a number of other approaches. Johanson et al. [14] show that by using restricted Nash response against a fixed strategy \u03c3fix, they can reconstruct a Pareto optimal set with respect to gain and exploitability. Milec et al. [21] also show an explicit bound on exploitability as a function of the parameter p. Another approach that is is similar to restricted Nash response is to focus on using limited data to exploit data-biased response [13]. Data-biased responses use a parameter for each the opponent's decisions and can differ depending on our confidence in the data. Finally, Ganzfried and Sandholm [10] focus on safe adaptation in repeated game, where they show that one can safely improve their utility by tracking the profit gained from opponent mistakes and risking at most that amount."}, {"title": "3.2 Adapting to Opponents in Perfect Information Games", "content": "Early work on adaptation in perfect-information games started by creating simple opponent models returning values for actions and using search to adapt using those values [7]. Later works focused on analyzing moves that are still part of the optimal strategy but adapt to the opponent by winning faster against moves that might be losing against a perfect player, hinting at the notion of safety [26]. This line of work concluded with the authors of [20] creating a method that not only computes where one can adapt to the opponent but also keeps a self-weakness model and tries to search for actions where the opponent is weak, but the method will be strong."}, {"title": "3.3 Adaptation in Large Imperfect Information Games", "content": "In large games with imperfect information, Timbers et al. [30] propose an optimised reinforcement learning that allows us to compute an exact response. However, this approach requires a significant investment time for each opponent, making it inapplicable when we need to respond to a new opponent in real time. Wu et al. [31] propose a method that can adapt online by evolving a neural network that can quickly adapt during play. However, this method does not provide any safety guarantees. Another recent contribution is the algorithm Cicero, developed for the large multi-player game Diplomacy, which involves adapting to the assumed intent of the opponents as its crucial component [9]. However, because most actions (i.e., every except communication) in Diplomacy are public, the direct applicability of this algorithm to general imperfect-information games is limited."}, {"title": "3.3.1 Safe Online Adaptation", "content": "An important next step in adaptation to opponents is the ability to adapt in online play while having safety guarantees. Several works tackle this issue by combining restricted Nash response with continual re-solving [23], with main differences being in the choice of the re-solving gadget (cf. Section 2.2.1) [12, 19, 21]. Liu et al. [19] use the max-margin gadget and only use the opponent model as it reaches the subgame. Ge et al. [12] use the re-solving gadget of Burch et al. [6]. Since neither of these works uses the opponent strategy in the re-solving subgame, they have strong safety guarantees but have a lower ability to adapt to the opponent."}, {"title": "3.3.2 Continual Depth-limited Responses", "content": "The current state-of-the-art approach for depth-limited opponent-robust exploitation is the continual depth-limited restricted Nash response (CDRNR) [21] and its special case continual depth-limited best response (CDBR) (which corresponds to setting CDRNR's \"robustness\u201d to zero). On the high level, CDRNR combines a restricted Nash re-sponse with depth-limited solving by using a neural network to approximate the optimal play after the depth limit (i.e., disregarding the opponent model there). The authors highlight limitations of existing re-solving gadgets, demonstrating why restricted Nash response guarantees fail with them. To address this, they introduce a full gadget that preserves the path from the root to the current subgame, fixing the resolving player and allowing opponent deviations."}, {"title": "4 MOTIVATION", "content": "When playing against a subrational opponent, we can follow the optimal strategy and limit our gain by a lot. We can adapt to some opponent mistakes that do not force us to deviate from the optimal strategy, and finally, we can adapt to opponent mistakes in cases where we need to deviate from the optimal strategy.\nThe concepts smoothly overlap in imperfect-information games, and it is a scale of how much we want to adapt and how much we would leave ourselves to be exploited. In perfect information games without chance, it is discrete, and we show an example of a tic-tac-toe to explain the concept.\nExample 3 (Tic-Tac-Toe). Figure 4 shows different situations in tic-tac-toe. Assume that the optimal strategy player 1 is playing always starts in the middle. In the left column, we see what happens when he starts in the middle. The opponent reacts by playing in the corner, and then we will have a game where he reacts optimally, ending in a draw. However, if player 1 starts in the corner, the opponent would follow as is shown in the middle column of Figure 4 and would play a suboptimal move, resulting in his loss (player 1 will win by playing any corner, forcing block and following with the other corner setting up two possible threes and the opponent can only block one). This deviation for player 1 is still part of another optimal strategy, and if played optimally by both players, the game would end in a draw. Finally, in the right column of Figure 4, we have a situation where the opponent starts and plays in the middle."}, {"title": "5 OPPONENT EXPLOITATION BEYOND THE DEPTH-LIMIT", "content": "In this section, we describe an online game-playing method, adapting beyond depth-limit (ABD), that is able to strike a balance between playing well against a specific opponent and remaining un-exploitable. Unlike previous methods that rely on depth-limited solving, ABD will be able to take advantage of all of the opponent's mistakes, irrespective of whether they occur before or beyond the depth limit. In Section 5.1, we describe the idealised version of the method that assumes access to a value function that captures the behaviour of the specific opponent. In Section 6, we describe two practical methods for approximating the idealised value function described in Section 5.1. We then conclude in Section 5.3 by showing how the algorithm can be simplified when we are certain about the opponent's identity and do not need to worry about remaining unexploitable."}, {"title": "5.1 Opponent Exploitation in Online Play via Matrix-Valued States", "content": "If we disregard efficiency, the idealised version of the adapting beyond depth-limit algorithm can be described as follows:\n\u2022 Let G be a game and \u2205 \u2260 Pi \u2282 \u03a3i portfolios for each player.\n\u2022 The parameters of the algorithm are a depth limit d \u2208 N, a fixed opponent strategy \u03c3fix \u2208 \u03a32, and p \u2208 [0, 1] which controls the adaptation-exploitability tradeoff.\n\u2022 The input to the ABD algorithm is an infoset I \u2208 I1.\n\u2022 The output, obtained in a way described below, is a probability distribution \u03c31(I) \u2208 \u2206(A1(I)).\n\u2022 The algorithm considers the game G\u00b3, which can be obtained as follows:\n(1) Let G\u00b9 be the robust adaptation game corresponding to G, \u03c3fix, and p.\n(2) Let G\u00b2 be the re-solving subgame [21] corresponding to P1 starting from I in G1.3\n(3) Let G\u00b3 := Gd (P1, P2) be the matrix-valued depth-limited variant of G\u00b2, corresponding to looking d steps ahead from I, after which each player is limited to only using strategies from their portfolio Pi. (Note that, as in G\u00b9 and G\u00b2, P2 is still limited to using \u03c3fix in the half of the game tree where their strategy is meant to be fixed.)\n\u2022 To compute \u03c31(I), the idealised algorithm finds a Nash equilibrium \u03c3\u2217 of G\u00b3 and sets \u03c31(I) := \u03c3\u2217(I).\nThis algorithm is idealised in three ways: First, it assumes we have access to a method that can compute an exact Nash equilibrium of G\u00b3. This can, in theory, be achieved by methods such as linear programming [15]. However, this would often be impractically slow in large games. As a result, a practical implementation of ABD will use an approximate iterative algorithm such as counterfactual regret minimisation [32]."}, {"title": "5.2 Computing Opponent-Specific Values", "content": "In larger games, the idealised form of the ABD algorithm would be intractable for two reasons. First, if we wanted to rely on Proposition 5, we would need to use a portfolio which consists of all pure undominated strategies in G. In practice, many of the strategies are redundant - when using a portfolio strategy to evaluate a particular state, the only relevant part of the strategy is its restriction to the plausible future states (not its past actions or actions in states that are no longer possible). While this greatly reduces the size of the number of strategies, even the reduced portfolios would be intractably large. (For example, in a mid-sized Battleships game with a 5x5 board, two 2x1 ships, and depth limit d = 1, the portfolios would contain roughly 1025 strategies.) The solution to this problem is to use a limited portfolio of different strategies (some of which might not necessarily be pure). Kubicek et al. [18] propose a method to learn such portfolios during training.\nSecond, even with a manageably-sized portfolio P1, the future part of the game tree might be large, making it prohibitively expensive to obtain the exact values of (\u03c01, \u03c3fix) and (\u03c01, \u03c02), \u03c0i \u2208 Pi. For (\u03c01, \u03c02), these values might be obtained prior to learning \u03c3fix or even come as a side-product of constructing Pi. However, (\u03c01, \u03c3fix) cannot be pre-computed in such matter. As a baseline variant, we thus compute the values E[ui(z) | z \u223c (\u03c01, \u03c3fix), z extends h] by sampling trajectories z from using \u03c3fix (and \u03c01 \u2208 P1). More specifically, we will additionally parametrize the algorithm by some Nsample and replace each of the values above with the corresponding empirical mean.\nOur approach significantly reduces computational costs compared to the previously used neural network-based value functions. Instead of repeatedly querying a neural network during each iteration of the solving process, we sample the values only at the start of the solving phase and then apply the solving algorithm on the well-defined extensive form game. In contrast, neural network methods must invoke the value function at each iteration for depth-limited leaves, which can become computationally expensive, especially with large network architectures."}, {"title": "5.3 Special Case: Efficient Best-Response Computation", "content": "When we are certain that the opponent will play the fixed strategy \u03c3fix or we want to evaluate the exploitability of \u03c3fix we will have the p = 1.4 This allows us to simplify the computation in two ways:\nObservation 1: finding a best response in a single pass. When the opponent is fixed both before and beyond the depth limit, we obtain an EFG where one player is fixed everywhere. This means that we no longer need to rely on iterative approaches to game solving (e.g., the CFR algorithm [32] used in CDBR (cf. Section 3.3.2) necessary to accommodate the value function represented by a neural network). Instead, we are able to compute a best response using backward induction, which only requires a single pass of the look-ahead tree and thereby saves significant computation time.\nObservation 2: searching a smaller part of the look-ahead tree. When we perform depth-limited search later in the game, the presence of imperfect information requires us to consider more states than just the (unknown) true state of the game. In particular, approaches such as CDBR must start the search from all histories in the current public state [16]. In contrast, the use of a fixed opponent even after the depth-limit allows ABD to only start the search from the histories in the current information set of player one. (E.G., in poker, this would mean the difference between considering all possible private hands for both players vs only considering all possible hands for the opponent.)"}, {"title": "6 EXPERIMENTS", "content": "We conduct experiments in both small and large imperfect information games to highlight the failure modes of CDBR (Section 3.3.2) and demonstrate the adaptability of our method against subrational strategies."}, {"title": "6.1 Failure of Previous Methods", "content": "The first experiment aims to illustrate the failure modes of the previous method, specifically CDRNR (CDBR). We utilize a 2x2 Battleships game featuring one 1x1 ship, as outlined in Example 1. The opponent adopts a uniform shooting strategy, except for the top-left corner, which they target only as a last move. In Table 1, we present the expected value of CDBR at increasing depths. With a game value of 0.25, it is evident that CDBR fails to exploit the opponent's shooting pattern when only the first move of ship placement is observed. When CDBR observes the first shooting move, it is still"}, {"title": "6.2 Varying p in Battleships", "content": "The next experiment explores the performance of ABD with a fixed depth of 2, varying the parameter p to examine its impact on the trade-off between exploitability and gain. We use the same 2x2 Battleships game described in Example 1 with the same portfolio, and we compute the value exactly again. CDRNR uses an optimal value function computed by a linear program. Unlike the previous method, which struggles to exploit this opponent, we show the tradeoff between exploitability and gain in Figure 5. We report the robust results this way since it is not that important which p corresponds to which points but how close we are to the Pareto optimal set considering gain and exploitability, which the RNR shows. Unsurprisingly, we see that CDRNR cannot achieve any gain and adapting beyond depth-limit is mimicking RNR exactly in this simple game."}, {"title": "6.3 Varying p in Leduc Hold'em", "content": "In this experiment, we evaluate the performance of ABD in a more realistic setting using the Leduc Hold'em poker game. We test two types of opponents: a uniform opponent, who plays without any particular strategic bias, and an exploitable opponent, who exhibits predictable tendencies. Specifically, the exploitable opponent only checks or folds in the first round and only bets or calls in the second round. We will denote the opponent's strategy S1.\nPlayer 1 uses a portfolio of strategies consisting only of a tight passive strategy (which only involves checking and folding) and a loose aggressive strategy (which consists of betting and calling when a bet is impossible). Values at the depth limit are sampled using 10 samples per history.\nOur experiment aims to assess how ABD adapts to these distinct opponent behaviors across various settings of p. Figure 6 shows the exploitability and the gain achieved by player 1 to compare performance against both the uniform and the exploitable opponent. We see that adapting beyond depth-limit cannot reconstruct an unexploitable strategy because of the limited portfolio. However, against both players, the adapting beyond depth-limit is able to recover strategies overall closer to the optimal RNR set.\nWe see that the main difference is in the ability to exploit the S1. This is expected since we are now using all the information about the opponent and not only the information available before the current depth-limit. Since this is the most interesting part of the comparison, our next experiments will be focused on those extreme cases where p = 1."}, {"title": "6.4 More Leduc Strategies", "content": "Next, the Leduc experiment uses the same setup and strategies; S1 is the same as in the previous experiment. Also, we add strategies: S2, which operates in a reverse manner, always bets or calls in the first round and always folds or checks in the second round; S3, which is based on S1, but folds in the second round when only folding or calling is available; and S4, which mirrors S2 with the same folding modification.\nWe evaluate adapting beyond depth-limit against the four opponent strategies alongside CDBR and show the results in Table 2. Additionally, we conducted experiments against 1000 randomly"}, {"title": "6.5 Large Game", "content": "Our final experiment examines 5x5 Battleships with two 2x2 ships, which have over 1050 possible histories. The opponent places ships uniformly and shoots uniformly, with the exception of the top-left corner, which they target only as a last move. We chose a uniform shooting strategy to avoid giving our method an unfair advantage that might arise from a more deterministic opponent. This configuration allows us to exploit the opponent's behavior by placing ships in the top left corner, which they will never target.\nOur approach utilizes a portfolio of three strategies: one that shoots uniformly across the grid and two that target even and odd cells, respectively. We use 100 samples per history to evaluate leaves in the depth-limited tree and compute the strategy with depth limit 2. Adapting beyond depth-limit correctly identifies placing the ship in the top left corner as the best response."}, {"title": "7 CONCLUSION", "content": "This paper addressed the challenge of safely adapting to subrational opponents in large, imperfect information games where computational constraints necessitate depth-limited solving. Previous approaches rely on simplified assumptions about the opponent's strategy beyond the depth limit, which can hinder the ability to respond effectively. To overcome these limitations, we introduced a novel framework that leverages matrix-valued states to represent the opponent's strategy beyond the depth limit.\nWe showed that in terms of robust adaptation to random opponents, our method slightly outperforms the previous state of the art method, CDRNR. However, when facing opponents who make mistakes in later parts of the game, our method significantly outperforms CDRNR, with a twofold increase in achieved gain.\nTheoretical results show that the idealized setting can recover the theoretical restricted Nash response - the theoretical optimum - for the given opponent. We also identified two important simplifications when p - the confidence in the opponent model - is set to one, allowing for a significant speed up in computation."}]}