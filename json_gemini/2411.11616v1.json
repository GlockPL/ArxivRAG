{"title": "Signaling and Social Learning in Swarms of Robots", "authors": ["Leo Cazenille", "Maxime Toquebiau", "Nicolas Lobato-Dauzier", "Alessia Loi", "Loona Macabre", "Nathana\u00ebl Aubert-Kato", "Anthony Genot", "Nicolas Bredeche"], "abstract": "This paper investigates the role of communication in improving coordination within robot swarms, focusing on a paradigm where learning and execution occur simultaneously in a decentralized manner. We highlight the role communication can play in addressing the credit assignment problem (individual contribution to the overall performance), and how it can be influenced by it. We propose a taxonomy of existing and future works on communication, focusing on information selection and physical abstraction as principal axes for classification: from low-level lossless compression with raw signal extraction and processing to high-level lossy compression with structured communication models. The paper reviews current research from evolutionary robotics, multi-agent (deep) reinforcement learning, language models, and biophysics models to outline the challenges and opportunities of communication in a collective of robots that continuously learn from one another through local message exchanges, illustrating a form of social learning.", "sections": [{"title": "1. Introduction", "content": "A general and well-accepted definition of swarm robotics highlights the deployment of a possibly large collective of robots each with limited computation and communication capabilities working together as a result of multiple local interactions to achieve a common cause [1, 2, 3, 4, 5, 6]. It is important to note that \"limited\u201d does not mean \u201csimple\": a hypothetical collective of idealized self-aware language-capable robots could still be considered a swarm if decentralized coordination is required due to the inherent delay in communication, even if the environment is static. The limited capabilities of each robot are to be understood as a relative property that puts into relation two conceptual levels: (1) at the individual level, the capabilities of one individual component of the swarm, which encompass both its physical (sensors and actuators) and algorithmic (memory and computing power) capabilities and (2) at the global level, the swarm complexity in terms of its size and spatial configuration, which define the possibilities of interactions between its components. While the hardware capabilities of the robots limit the goals that can be achieved, the limitation in software capabilities is the key factor. Whenever memory or computation is lacking at the individual level, collective action requires decentralized coordination, as each robot can only sense and act in its immediate surroundings. In addition, it is important to consider the time component of computational complexity, which depends on either or both a time-constrained task and an inherently dynamic environment. This implies that the swarm's response time should be short enough for its actions to be relevant.\nIn this paper, we do not impose limitations over the actual capabilities of the robots or on the swarm structure (e.g., a heterogeneous swarm of unconventional robots is possible) and allow for different interpretations of what cooperation means (e.g., from just avoiding each other to displaying complex coordinated strategies). This is covered by this slightly different and more accurate definition: swarm robotics involves deploying robotic agents that coordinate in a decentralized manner to achieve a common goal, with each robot limited to sensing and acting within its immediate environment\u00b9.\nThat definition opens new venues for thinking about the future of the field, including bridges toward other fields with similar concerns, as we will see later.\nThe design of efficient individual policies within a swarm of robots usually relies either on carefully crafting (possibly bio-inspired) behavioral rules or on using learning and/or evolutionary optimization algorithms. The robots' policies, which are generally similar across a given swarm, do not change after deployment. While this approach is enough in many cases, it is a limitation whenever the target environment is unknown before deployment or changes over time. This is why an important effort, originally stemming from evolutionary robotics, has been made since the advent of the 21st century to develop decentralized online evolutionary learning algorithms. This family of algorithms aims at enabling a robot swarm to adapt continuously while already deployed in the real world, as illustrated in Figure 1-A, and has been referred to as either embodied evolutionary (EE) robotics [7] or social learning for swarm robotics (SLSR) [8, 9]. They have achieved remarkable success in terms of the number of implementations on real robots when compared to other fields working with learning multi-robot systems (see [10] for a review).\nIn this paper, we posit that the class of problems addressed when using such social learning or embodied evolution algorithms is covered by the umbrella term of decentralized learning and execution (DLE), which designates a paradigm"}, {"title": "2. Dynamics of Decentralized Learning and Execution", "content": "As stated in the Introduction, natural evolution and social learning are good examples of processes working under the DLE paradigm. Individuals compete with one another to gain a selective advantage. Combined with random variations and inheritable traits, the traits of successful individuals will become more frequent over time. Of course, there is a stark contrast between natural systems and swarm robotics systems: we engineer the robot swarm to address a particular problem defined before deployment which may require coordination to be addressed (foraging, exploration, patrolling, transporting, construction, or monitoring to give a few examples [15]). While the desired outcome may be relatively easy to define, the challenge is to endow each robot with the capability to assess how much it contributes to solving the task, i.e., self-assessing the robot's contribution to the global welfare of the collective, which is itself determined by how efficiently the task is solved.\nIn a collective, devising the contribution of each individual is referred to as the credit assignment problem, which is well known in the multi-agent and cooperative game theory communities [16, 17]. If a complete alignment of the individual's interest with the global welfare of the collective is possible, the best actions from the robot's viewpoint will also be the best for the collective. In a setup where individual policies are learned, this corresponds to converging towards a Nash Equilibrium that is also a social optimum, meaning none of the robots has the incentive to deviate from its current behavioral strategy as it is already the best the robot can do reward-wise (see [18, 19, 20] for theoretical considerations in distributed robotic systems, and [21] for a practical example with evolutionary learning in a swarm of robots where there is a mismatch between evolutionary stable strategies and social optimal strategies).\nA direct way to make individual interests coincide with that of the team would be to provide each individual with a measure of their contribution to the global performance. However, estimating the marginal contribution of each robot to the performance of the collective is intractable in the general case. Even in an idealistic setting, when a scenario can be replayed an indefinite number of times and robots can be removed or added at will, computation time for estimating the marginal contributions for each individual grows exponentially with the population size as all subsets of individuals must be considered [22, 23, 24]. It is also interesting to note that the more classic reinforcement learning methods using centralized learning do not yield optimal results, as the robots' marginal contributions are often partially or badly estimated even by the centralized critic used in multi-agent (deep) reinforcement learning [12]. One efficient simplifying hypothesis used in the field of evolutionary collective robotics is to consider a swarm of clones [25, 26], turning what originally looks like a collective decision-making problem into an optimization problem as a single control parameter set is used for the entire swarm and optimized in a centralized fashion. This method is however not applicable under the DLE paradigm as it requires a centralized coordinator for learning.\nApproximation methods to estimate on the fly the marginal contribution of robots in a collective exist, of course, and trade tractability against a lack of optimality or assume simplifying hypotheses on the class of problems to be addressed (see in particular [27, 28, 29]). A straightforward method is for the human supervisor to define a priori an explicit evaluation function embedded in each robot whose goal is to evaluate locally the performance of said robot. This is the case with most works in embodied evolution and social learning in swarm robotics, where each robot computes an estimate of its performance based solely on directly available information and self-assessment [10]. This is also the case in cooperative multi-agent learning whenever each agent is an independent learner, i.e., considering others as part of a nonstationary environment [30]. In both cases, the global performance will depend on the ability of the human engineer to design a function that provides a reliable estimate of the performance of a robot, aligning local motivation with the desired global outcome. Obviously, this can quickly become challenging as the task and/or the environment grow in complexity e.g., foraging in a field without obstacles can be very different from foraging in a complex environment where the division of labor offers a significant advantage.\nUnfortunately, the slightest misalignment between the individual's interests and that of the collective can lead to a suboptimal group-wise performance. In that case, the whole swarm will eventually converge towards a Nash Equilibrium that does not guarantee social optimality. This is explained by the nature of the evolutionary dynamics at work behind social learning in a swarm: elements that play a part in the robots' behavioral strategies are competing among themselves to invade the population of robots. If the metric used to compare those elements is aligned (resp. not aligned) with the global task, then competition will end up with individual strategies that are optimal (resp. sub-optimal) w.r.t. the task. This can be explained by using the famous selfish gene metaphor popularized by Richard Dawkins [31]: robots are merely vehicles for competing units (e.g., genes or group of genes, neural network parameters, symbols from an emerging language, elements of an artificial culture, etc.) facing selective pressure.\nSuch evolutionary dynamics can then have a direct impact on the long-term behavioral strategies of neighboring robots, with sometimes surprising outcomes such as mutualistic cooperation (i.e., cooperation that benefits each involved party) and altruistic behavior (i.e., cooperation that involves a net loss at the individual level, but which indirectly benefits the survival of related individuals) [32]. In particular, each individual's strategy is shaped by its inclusive fitness that captures both its ability to survive and its ability to help related individuals (a relation that is generally, but not always, defined at the genotypic level) [33]. Kin selection, the process by which an individual favors their relatives, is also relevant for the development of cultural adaptation [34] and language [35]. This has been shown previously to also be the case with social learning algorithms for swarm robotics [36]: robots can lose part of their survival chances to help robots with whom they share information."}, {"title": "3. A Taxonomy of Signaling Methods in Swarm Robotics", "content": "First, let us start by narrowing the scope regarding the nature of communication we are interested in by distinguishing cues from signals. Cues provide information to the focal individual, extracted from the environment through direct observations (e.g., the relative alignment of nearby conspecifics [43, 44]) or identification of body markers (e.g., a conspecific's phenotypic trait). They do not require an identified interlocutor and, if another individual is involved, they are not produced intentionally. Signals involve an emitter and at least one receiver. They are produced intentionally by the emitter through one or several available modalities (auditory, visual, olfactory, etc.), and can vary greatly in complexity, from the production of a chemical compound to human language. The interested reader can refer to [45] for a comprehensive introduction to cues and signaling in nature.\nFigure 1-B and -C provides an illustration from a robot swarm perspective. Each robot may experience both cues, observed in the physical world, and signals, originating from other robots and received through dedicated channels such as short-range proximity communication devices (e.g., infrared, visible light, radio, etc.). We explicitly limit our scope to the moment when information from the signal is readily available to the robot, leaving any pre-processing transparent (signals can be initially extracted from another modality such as speech and sign language, as is the case in robot-human communication [46]).\nCommunication strategies in swarm robotics cover both stigmergic communication and direct communication. Stigmergic communication works by leaving a trace in the environment [47, 48], such as a virtual pheromone trail for other robots to consider [49, 50]). Direct communication involves explicit exchanges of information among robots, either through pre-defined or emergent signaling strategies. In particular, emergent communication strategies evolve naturally from the interactions and the optimization processes at work within the swarm, enabling robots to converge towards efficient adaptive behaviors without centralized control.\nIn addition to whether signaling strategies are learned or pre-defined, the nature of the signals can vary greatly taking, for instance, discrete and continuous forms. Low-level communication methods often mimic natural processes like diffusion, reaction, and advection, enabling robots to share information about their local environment. High-level methods involve more abstract forms of communication, such as emergent or structured language models, allowing for sophisticated interactions and decision-making.\nSignaling also necessarily incurs some form of restriction over the nature and the amount of information that will be shared, driven by the necessity to transfer relevant information only. This process may be lossless (e.g., suppressing redundant information, compressing information without loss, or changing the way information is represented) or lossy (e.g., ignoring irrelevant information, compression with loss). In practice, as the complexity of the environment increases, so does the need for sharing only what is relevant for the task at hand (e.g., selection attention in humans [51], or methods used to avoid the curse of dimensionality in machine learning [52]).\nWe propose two axes for classification using the degree of information selection and the degree of physical abstraction. On the one hand, information selection aims at reducing the quantity of information shared by loosing information that is not deemed relevant. On the other hand, physical abstraction aims at changing the way information is represented without loss of information in order to reveal what is already present. This is illustrated in Figure 3. The left part of the figure provides an analogy with algebra to provide an intuition using a mathematical metaphor. The right part maps well-known approaches used in swarm and collective robotics, which will be explored further in the later Sections.\nIn the region considering a low degree of both information selection and physical abstraction, communication processes are closely tied to raw physical phenomena, such as reaction, diffusion, and advection. These methods mimic natural processes to transfer information, focusing on detailed, low-level interactions. Increasing the level of physical abstraction (x-axis) enables the extraction of hidden but highly relevant information from raw information such as using spectrum analysis or Fourier transforms, e.g., to capture geometric information of collective spatial configurations [53]. Similarly, a second axis (y-axis) explores how increasing the degree of information selection can extract relevant information in different forms. For example, sharing parameters of an artificial neural network controller that maps sensory inputs to motor outputs, as is common in adaptive swarm robotics [10], can be seen as a highly compressed (and biased) instance of a reaction-diffusion process. Finally, the extreme of both axes points towards signaling strategies with high degrees of information selection and physical abstraction, where we can find, e.g., the use of a communication apparatus that is based on a large language model, enabling human-level structured perception and signaling."}, {"title": "4. Current Trends in Signaling for Swarm Robotics", "content": "In this Section, we provide a review of relevant methods from swarm robotics as well as other domains, to reveal what the future states of signaling could be, considering both ad hoc and emerging signaling methods. The Section follows the structure provided earlier: we first describe signaling methods with a low degree of information selection in Sub-section 4.1, then move up to those with a high degree of information selection in Sub-section 4.2. In each Sub-section, strategies with different degrees of physical abstraction are described, also drawing from domains beyond that of swarm robotics. We make significant room for signaling methods used in multi-agent reinforcement learning as well as in the currently popular domain of large language models (LLMs). As mentioned in the Introduction, we stress that while existing swarm robotics hardware is still technically limited, this state of affairs may change in the near future. As a consequence, we expect that collective systems that can be identified under the umbrella of robot swarms will feature embedded computation capabilities powerful enough to run, and possibly train in real-time, LLMs (e.g., LLMs can already run on limited hardware [54])."}, {"title": "4.1 Low Degree of Information Selection", "content": "Summary: In this section, we investigate communication in multi-agent systems as information exchanges with minimal simplification of the baseline observable data from local agents. This involves two types of signaling schemes. Scheme (1) involves signals that reflect local observations directly. Biological examples include social insect communication and autoinducers exchanges in bacteria. In swarm robotics, these principles are applied through algorithms mimicking biological behaviors through local interaction and communication rules. Scheme (2) includes signals with a high level of physical abstraction and structure, such as the use of Fourier transforms and wavelets to analyze and share periodic patterns and multi-scale features in data. In swarm robotics, agents might share with immediate neighbors their computed gradients or neural network weights, or perform Fourier transforms or eigenspectrum analysis to understand and communicate the underlying structure of complex data.\nMulti-agent communication with low information selection (lower part of Figure 3) involves the direct and explicit exchange of observable information from local agents [55]. The signals are transmitted in a form that retains most of the original observations, without any extensive selection mechanism removing parts of the baseline observation data. In Figure 1C, communication with low information selection involves minimal loss of information between the cues from the environment and their packaging into signals sent to other agents. This approach is relevant either 1) in cases where the observations already have low dimensionality, 2) in cases where most of the observations contribute to the collective dynamics of the group, or 3) in cases where knowing in advance which parts of the observations are useful to communicate is difficult to achieve.\n(1) Low physical abstraction: In the case with both low information selection and low physical abstraction (lower left quadrant in Figure 3), signals represent direct and tangible information about the environment, with minimal transformations from the observations of local agents. This type of signaling is exemplified by the following biological systems: autoinducers exchanges among bacteria [56], auditory and tactile signals in Drosophila [57], chemical alarms released from certain fish species to alert conspecifics of the presence of a predator [58], the bioluminescence mechanisms of fireflies for mate attraction [59], electric signals in certain fish [60], or birds songs to attract mates [61] or to signal aggressive intent [62].\nSwarm robotics algorithms deployed on small robots or with self-organization capabilities also fit in this quadrant, because they rely on simple ad hoc signaling rules based directly on local states and observations, without significant loss of information or transformations. For instance, in [42] a signaling behavior is optimized so that robots emit specific signals when they are close to an object or zone of interest. In [63] robots share all their local sensory information with their neighbors during a predator-prey task. The relative position of each robot or site of interest is locally broadcasted in [64, 65]. In [66], robots can probabilistically broadcast information from one to another to assess the dynamics of information propagation.\nMulti-agent systems inspired by physical dynamics can also be classified in this category: e.g., reaction-diffusion [67], chemical oscillations [68] and morphogenesis [69] can be seen as multi-agent systems where agents are spatial discretization points and global dynamics emerge from local interactions (communication without information loss). Diffusion is a fundamental physical process where particles spread from areas of higher density to areas of lower density. In multi-agent systems, diffusion can serve as a means of communication. For example, chemical signaling in cells relies on molecular diffusion to guide movement, growth, and specialization. Reaction-diffusion systems involve the creation, transformation, or destruction of diffusive elements through local interactions to create complex patterns. In multi-agent systems, reaction-diffusion can explain how agents interact with their environment and each other via chemical signals [70].\nMoreover, making robots out of molecules allows the creation of massive swarms of millions of robots. In the last decades, researchers have used artificial DNA as computing and building blocks to develop molecular robotics [71]. Such robots can take the form of DNA origami that self-assemble into complex 3D nanostructures, able to connect to each other or change configuration depending on biochemical cues [72, 73, 74, 75, 76, 77, 78, 79]. Simpler structures can also be programmed to move on tracks [80] and sort cargoes at the nanoscale [81]. Coating beads with DNA allows to create micro-robots with higher computing capabilities, with reaction-diffusion serving to form both controllers and signals [82, 83, 84]. Another emerging field is controllable active matter, where self-propelled agents process chemical signals locally, leading to self-organization [85, 86, 87, 88, 89, 90, 91]. A final example is the Turing model of morphogenesis, which explains how patterns like animal stripes and spots emerge from the interaction of diffusing chemicals, inspiring a swarm robotics implementation where local communication mimics a reaction-diffusion system to achieve shape formation [92].\n(2) High physical abstraction: The lower right quadrant of Figure 3 represents communication methods involving abstract and structured information, often detached from direct physical processes, with minimal information loss from observations.\nThis includes methods like broadcasting gradients where agents locally exchange mathematical abstractions rather than direct physical signals. Gradients represent the internal state of each agent's model, rather than a direct physical quantity. For instance, gradient propagation can compute a geodesic distance to a source robot by incrementally communicating values through neighboring agents [93]. Gradient broadcasting can occur through microscopic rules derived from local observations [94, 95, 93], or via multi-agent reinforcement learning where the gradients of the loss function are broadcasted from agents to agents [96]. Having differentiation capabilities, i.e., access to the gradient of local states and/or messages, allows the training process to directly use this information (e.g., via gradient descent algorithms), accelerating convergence.\nEigenspectrum analysis [97] also fits this quadrant, examining eigenvalues and eigenvectors to reveal the underlying structure of data. Eigenspectrum analysis is widely used in a variety of fields, ranging from signal processing and machine learning to network analysis. This process can involve similar dynamics as those obtained in the lower left quadrant however, it will also use mathematical tools to change the representation of information without loss of information. For instance, in [53] a swarm of Kilobot robots estimates, in a decentralized way, the eigenspectrum of the communication graph between robots. Such properties are then used to reach a global consensus on the shape of the swarm, achieving arena shape recognition. This process is achieved by relying on a physics-inspired communication scheme based on the diffusion of heat across the swarm and mathematical tools to locally extract the second eigenvalue $\\lambda 2$ of the graph Laplacian, a direct fingerprint of the arena shape containing the swarm."}, {"title": "4.2 High Degree of Information Selection", "content": "Summary: In this section, we explore decentralized communication as viewed from the prisms of the information bottleneck, language evolution, and multi-agent reinforcement learning in situated environments. Reinforcement learning approaches to emergent communication are examined, highlighting both benefits and challenges. We emphasize the opportunities provided by LLMs for advancing communication in swarm robotics, noting their strengths in generating human-like language and reasoning, and challenges such as biases, hallucinations, embodiment, and efficient deployment on robots. Overall, we present a range of approaches, with different degrees of physical abstraction, that enable decentralized agents to learn communication.\nIn realistic decentralized environments, all observed information is not relevant to transmit to partners. Thus, a higher degree of information selection (upper part of Figure 3) is required to allow efficient transmission of the relevant information. This task can be decomposed into two complementary sub-tasks: (1) selecting relevant information and (2) transmitting this information. The selection task involves extracting parts of the observed information that are relevant to other agents. The transmission task requires coding this information so other agents understand it while ensuring that bandwidth constraints are respected. Both tasks are highly interconnected. The selected information requires adequate means of coding to be transmitted without (or with minimal) loss. The transmission means, in turn, influence the information selection by dictating what information can be transmitted efficiently [99]. This is a form of information bottleneck, where agents need to generate a compressed mapping of their observations, that contains as much information as possible related to the task at hand [100]. As communication comes necessarily at a cost, languages operate a trade-off between meaning and compression [101, 102], maximizing expressiveness while minimizing communication costs.\nStudying language games shows how languages emerge from this information bottleneck, and from various ecological constraints. In his seminal work, Luc Steels [103] demonstrated that having a dynamic population of embodied agents, whose reasoning is unknown to one another, motivates the emergence of a shared compositional language. In the iterated learning framework [104, 105], the emphasis is put on a transmission bottleneck that occurs when language is transmitted between successive generations of agents, driving languages to adopt simple and compositional structures. Following works have shown that emergent languages are also shaped by environmental [106, 107] and physiological [108] constraints. These experiments highlight the different requirements for languages to originate in populations of independent agents, and demonstrate the emergence of efficient naming and grammatical conventions [109, 110, 102, 111].\nHowever, these language games still heavily simplify the context of communication interactions, by making the agents, their observations, and their actions, solely defined by the communication game. Previous works have classified this kind of setting as non-situated [112], as opposed to situated agents that have a localized existence and can physically interact with their surroundings. In situated environments, communication is one of many interfacing processes. It can be used for communicating not only about observations, but also about intents, or even about task-agnostic and abstract concepts. It may involve non-cooperative agents. It might not even be required at all times. In such realistic settings, choosing which information is relevant to communicate is a much more complex task that involves reasoning about the current state of the environment, the agent's objective, and the current knowledge and reasoning of other agents. In that sense, learning to communicate is inherently a multi-agent problem of learning how to behave in a dynamic, partially observable environment.\nRecently, research in multi-agent reinforcement learning has tackled such situated environments, where performance depends on a combination of physical and communication behavior [113]. In this context, multi-agent systems learn, often with centralized training and decentralized execution, to generate messages that participate in maximizing future returns. Here, messages are continuous vectors generated by neural networks inside the agents' system. This makes communication a differentiable sub-step of the action selection process, which can be learned fully end-to-end as a tool for maximizing returns [114, 115, 116, 117]. Because the message generation is differentiable, gradients can flow between agents. Thus, messages are explicitly trained to help other agents maximize their rewards. This approach has been extended in various ways for more targeted information sharing [118, 119, 120] or to limit bandwidth usage [121, 122, 123, 124]. Similar approaches have been developed using discrete symbols for communication [125, 126, 127, 128, 129]. In those, agents have to reach a consensus on the meaning of each symbol through trial and error. The compression constraint depends both on the size of the vocabulary and the size of the sequences. Previous works have shown that imposing constraints on both of these attributes induces emergent languages to develop common characteristics of natural languages such as compositionality [130, 131] and abbreviation of frequent words [132].\nHowever, learning emergent communication through this task-oriented reinforcement learning process has many important limitations. As already mentioned, it often requires a centralized learning algorithm to allow reinforcement learning tools to reliably converge to adequate solutions. As with all gradient-based learning methods, it acts as a black box that lacks practical ways of interpreting and measuring its efficiency [133, 134]. More importantly, differentiable emergent communication gives no guarantee of learning to communicate about concepts from the environment. Rather, guided by return maximization, agents converge to a consensus that may seem random to the human eye [135]. It lacks ways of anchoring its concepts in environmental, task-agnostic modalities. This is akin to the problem of symbol grounding [136]. Having emergent communication grounded in meanings from the environment would allow decentralized agents to learn to communicate about concepts that are shared with other agents, making learning easier and communication more efficient [103, 137, 106].\nFollowing this idea, grounding approaches have been explored in multi-agent reinforcement learning. By linking communication with visual data [138, 139], natural language [140, 141, 142, 143], or both [144, 145], agents learn to generate messages using task-agnostic concepts. In other words, they learn to use concepts dictated by external modalities to transmit information efficiently, instead of searching for a consensus on their own, starting from scratch and guided only by rewards. Agents may acquire \"grounded\" knowledge through a variety of techniques: pre-training on a supervised task [140, 144, 145, 146], alternating between supervision and self-play [147, 146], optimizing the supervised and RL objective at the same time [145, 139, 142, 143, 148], or constructing additional rewards based on supervised models [144]. The nature of the subsidiary tasks depends on the desired type of grounding. An autoencoding task can be added to ensure agents communicate about their observations [139, 148]. To ground communication in natural language, agents can be shown examples of human-generated sentences [140, 138, 142, 143] or learn to generate similar outputs as pre-trained language models [141, 145]. A challenge when learning to use natural language is to avoid language drift [145], requiring constant supervision to prevent RL agents from forgetting the intended use of the given language [144, 145, 146]. Natural language offers an efficient solution to the information bottleneck problem while allowing effortless interpretation and teaming with unknown agents (human or artificial).\nWhen using natural language, an obvious solution is to turn to LLMs. In addition to being extremely good for generating human-like sentences, they can also be grounded in visual and behavioral modalities [149]. Their context window can be exploited in various ways to insert factual information or state objectives to achieve and particular behaviors to adopt [150]. This is thanks to two important aspects of training the LLMs. First, the language-modeling pre-training phase shows the model of how humans formulate their reasoning in natural language. Second, the explicit instruction-following task optimized with reinforcement learning from human feedback [151] trains the LLM to pay close attention to what has been requested and how it should be answered. Consequently, LLMs can be used as a basis for modeling interacting agents [150, 149, 152]. Such agent-based LLMs are given information about the environment, the task, their identity, and their role in the environment, all inside an initial prompt. Following this initialization, they observe and act in the environment through visual, textual, and physical inputs and outputs [150, 152]. Thanks to their reasoning and conversing skills, LLM agents can discuss their knowledge and intents with partners before selecting an action [153]. This can even be pushed further with personas assigned to each LLM agent, allowing a large diversity of different behaviors and offering the advantages of collective reasoning [154, 155, 156, 157, 158].\nLLMs offer a nice playground for multi-agent interactions. They efficiently emulate human reasoning and communication. Their built-in interactivity provides a great tool for interpretation [159] and human-agent interactions [153, 160, 161]. However, several issues with LLMs remain and need addressing. First, embodying an LLM is a challenge requiring links to be made between language and environmental modalities (visual and behavioral). The current development of multimodal LLMs is a step towards solving this challenge [149]. But, these approaches often require a costly fine-tuning phase to adapt the model to its new modalities. A subsequent problem is the deployment of LLM-based agents on small robotic platforms, which requires engineering work to adapt to the constraints of such platforms. This is especially true for decentralized robots that must be self-sufficient and are often limited in memory and computing power. Furthermore, we need ways of countering the intrinsic biases present in human-generated data that LLMs inevitably reproduce [162]. Lastly, the problem of hallucinations remains an important obstacle. LLMs are known for inventing information and being reluctant to admit when they are wrong [163]. This can lead to issues ranging from deception to breaking the simulation, which requires more work on methods for detecting, measuring, and avoiding these hallucinations. While these issues can, and will certainly be addressed, this reminds us that other solutions using smaller models also work and might be preferable in many situations.\nTo conclude, we see that many approaches exist for teaching decentralized agents to communicate about high-dimensional environmental features. They rely on languages that select information to communicate more efficiently. These languages abstract physical elements of the world by grounding symbols in environmental features, allowing the establishment of conventions on how information should be transmitted. Different degrees of physical abstraction may serve different purposes. A group of agents specialized in a single task may be content with low physically-abstracted differentiable emergent communication learned from task reward. On the other hand, established concepts and grammatical rules provide the tools to generalize acquired knowledge, compose new ideas from fundamental language blocks, and communicate with unknown partners. Thus, higher physical abstraction, found in natural languages, is better fit to handle more general settings."}, {"title": "5. Conclusion", "content": "We explored how communication through signaling can be crucial in enhancing coordination within robot swarms operating under the Decentralized Learning and Execution paradigm. We proposed a structured framework to classify existing and future signaling methods", "occurs": "robots are mere resources for which policy parameters are competing", "164": "collective decision-making [16", "165": "sociophysics [166", "167": "physics of active matter [168", "169": "and machine learning [170", "revolution": "n\u2022 Researchers in Swarm Robotics: simple robots are not inherently \"simple\". What matters is the emergence of complex behaviors from microscopic interactions. Whether you work with large or small robots", "Learning": "this is all about embodiment. Swarm robotics introduces a unique category of machine learning problems with elements of \"social\" learning across physically embodied agents. Anchoring language models in physical systems brings new challenges and capabilities in distributed", "Systems": "swarm robotics provides a controllable model for exploring active matter, sociophysics"}]}