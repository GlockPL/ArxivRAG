{"title": "Do Large Language Models Show Biases in Causal Learning?", "authors": ["Mar\u00eda Victoria Carro", "Francisca Gauna Selasco", "Denise Alejandra Mester", "Margarita Gonz\u00e1les", "Mario A. Leiva", "Maria Vanina Martinez", "Gerardo I. Simari"], "abstract": "Causal learning is the cognitive process of developing the capability of making causal inferences based on available information, often guided by normative principles. This process is prone to errors and biases, such as the illusion of causality, in which people perceive a causal relationship between two variables despite lacking supporting evidence. This cognitive bias has been proposed to underlie many societal problems, including social prejudice, stereotype formation, misinformation, and superstitious thinking. In this research, we investigate whether large language models (LLMs) develop causal illusions, both in real-world and controlled laboratory contexts of causal learning and inference. To this end, we built a dataset of over 2K samples including purely correlational cases, situations with null contingency, and cases where temporal information excludes the possibility of causality by placing the potential effect before the cause. We then prompted the models to make statements or answer causal questions to evaluate their tendencies to infer causation erroneously in these structured settings. Our findings show a strong presence of causal illusion bias in LLMs. Specifically, in open-ended generation tasks involving spurious correlations, the models displayed bias at levels comparable to, or even lower than, those observed in similar studies on human subjects. However, when faced with null-contingency scenarios or temporal cues that negate causal relationships, where it was required to respond on a 0-100 scale, the models exhibited significantly higher bias. These findings suggest that the models have not uniformly, consistently, or reliably internalized the normative principles essential for accurate causal learning.", "sections": [{"title": "1. Introduction", "content": "Causal learning is the ability to extract causal knowledge from available information (Blanco, 2017), perceiving two variables as causally related (e.g. smoking produces lung cancer). This cognitive process is informed by several features of the accessible evidence. In fact, there are some principles that should guide normative causal inference, including temporal ordering and contingency\u00b9\n(Blanco, 2017; Msetfi et al., 2013). However, causal learning can also become a source of bias. Our cognitive system, driven by the logic of minimizing costly mistakes, has evolved to avoid overlooking meaningful patterns, even if this means that some false alarms will occur, as in the case of perceiving a causal link between two unrelated events (Blanco, 2017).\nIllusions of causality occur when people develop the belief that there is a causal connection between two variables with no supporting evidence (Matute et al., 2015; Blanco et al., 2018; Chow et al., 2024). Examples of this are common in everyday life\u2014for instance, many avoid walking under a ladder, fearing it will bring bad luck. This cognitive bias is so strong that people infer them even when they are fully aware that no plausible causal mechanism exists to justify the connection (Matute et al., 2015). Such illusions have been proposed to underlie many societal problems, including social prejudice, stereotype formation (Hamilton and Gifford, 1976; Kutzner et al., 2011), pseudoscience, superstitious thinking (Matute et al., 2015), and misinformation (Xiong et al., 2020). When causal biases infiltrate decision-making-whether on an individual or collective level\u2014in critical areas such as health, finance, and well-being, the consequences can become serious and harmful. For example, in the case of many alternative medicine treatments, which have been shown to have no causal effect on patient health beyond the placebo effect, the illusion of causality arises from simple intuitions based on coincidences: \u201cI take the pill. I happen to feel better. Therefore, it works\" (Matute et al., 2015). Some people go even further and prefer alternative medicine over traditional medicine that is based on the scientific method. This attitude is causing serious problems, sometimes even death (Freckelton, 2012). Once these myths begin to spread, they become increasingly difficult to eradicate, despite warnings from scientists and authorities about their ineffectiveness (Matute et al., 2015). Another example of negative impact is in scientific press releases, where media often report correlational research findings as if they were causal. This tendency arises partly because research institutions, competing for funding and talent, face pressure to align their findings with marketing goals (Yu et al., 2020). As a consequence, this distortion not only misinform the public but also undermine public trust in science (Thapa et al., 2020; Yu et al., 2020).\nRecently, the growing reliance on large language models (LLMs) has introduced concerns about their potential to reflect and amplify human cognitive biases, including illusions of causality. Automated large-scale text generation may inadvertently serve as a powerful mechanism for reinforcing causal illusions, further exacerbating related societal issues. In this paper, we investigate to what extent LLMs exhibit the illusion of causality, both in real-world and controlled laboratory contexts of causal learning and inference. To this end, we designed scenarios that lack sufficient information to establish causal relationships between variables. Guided by normative principles and cues commonly used by humans in causal inference, we included purely correlational cases, situations with null contingency, and cases where temporal information excludes the possibility of causality by placing the potential effect before the cause. These scenarios span three critical domains where\n1. Contingency refers to a conditional relationship where one event changes the probability of another, serving as a key cue in causal learning. Quantified by the metric delta P (\u0394P), similar to a correlation coefficient, it captures both the direction (generative or preventative) and strength of the relationship. In zero-contingency scenarios, no causal link can be established."}, {"title": "2. Preliminaries: The Contingency Judgment Task", "content": "Contingency is a crucial cue to causal learning. Studies have shown that people are very sensitive to changes in manipulated contingencies (Msetfi et al., 2013). Experimental psychology research that explored whether humans develop an illusion of causality have consistently employed variations of the same procedure: the contingency judgment task (Matute et al., 2015). This consists of two events\u2014a potential cause and an outcome\u2014that are repeatedly paired across multiple trials. Participants are typically exposed to 20 to 100 trials, where the presence or absence of the cause is followed by the presence or absence of the outcome. For example: Patient 1 did not take the pill (potential cause absent) and recovered from a disease (potential outcome present).\nThese trials reveal a null-contingency scenario, where the probability of the outcome remains the same regardless of whether the cause is present or absent; an example of this kind of contingency matrix is shown in Table 1. In contrast, a positive contingency indicates that the probability of the outcome occurring is higher when the cause is present than when it is absent. Conversely, a negative contingency suggests that the probability of the outcome is greater in the absence of the cause, implying that the cause inhibits or prevents the outcome (Matute et al., 2015). In both of these latter cases, a causal relationship exists.\nAt the end of the experiment, participants are asked to judge the relationship between the potential cause and the potential outcome, typically on a scale from 0 (non-effective) to 100 (totally effective). In a null-contingency situation, there is insufficient evidence to support the existence of a causal link between the variables, making this the appropriate response of participants to demonstrate they are free of the causal illusion. Therefore, any score above 0 suggests the presence of some degree of the bias (Vinas et al., 2023).\nTypically, in each trial, information is displayed on a screen in sequential fashion. In some cases, illustrations are used to represent variables (e.g., a pill and a recovered person). Moreover, after reviewing information of the potential cause, participants are asked to predict whether the outcome will be present or absent. Immediately, they receive feedback on the actual presence or absence of the outcome before proceeding to the next trial. This trial-by-trial prediction is usually included in contingency learning procedures because it helps participants to stay focused on the task (Blanco et al., 2018). Within this general scheme, the contingency judgment task can vary, particularly in terms of the participant's role, which may be either passive or active (Matute et al., 2015). In the passive condition, they simply observe the presence or absence of the cause, a setup analogous to vicarious learning. Alternatively, participants can play an active role deciding whether the potential cause is present in each trial.\nTo evaluate LLMs, we adapted the contingency judgment task by presenting the information about the trials in natural language. The number of trials varied between 20 and 100, with each case revealing a null contingency situation. In line with the human task variants, the LLMs adopted"}, {"title": "3. Related Work: Invalid Causal Reasoning Patterns in LLMs", "content": "Inferencing Causation from Correlation. (Jin et al., 2024) evaluated pure causal inference skills in LLMs by taking a set of correlational statements and determining the causal relationship between the variables. They found that the models achieve almost close to random performance on the task. They recommend to extent the evaluation to more real-world false beliefs based on confusing correlation with causation.\nDetecting False Causality. Jin et al. (2022) introduced a novel task for logical fallacy detection, including a specific type known as \u201cfalse causality.\" This fallacy relies on a false pattern of reasoning that interprets co-occurrence as causation, summarized as \u201c\u03b1 co-occurs with \u03b2 \u21d2 \u03b1 causes \u03b2.\u201d The study found that LLMs performed poorly on this task.\nFallacies in Causal Inference. Joshi et al. (2024) investigated if LLMs can infer causal relations from relational data in text, fine-tuning on synthetic data containing temporal, spatial, and counter-factual relations and measuring whether the LLM can then infer causal relations. Results showed that while LLMs successfully infer the absence of causal relations from temporal and spatial cues, they cannot make meaningful deductions from counterfactuals. The models exhibited a post hoc fallacy, assuming that because one event preceded another event, they must be causally related.\nCausal Biases. (Keshmirian et al., 2024) identified biased causal judgements in LLMs, mirroring what they previously observed in human subjects. Examining two Causal Bayesian Network structures-Chain (A\u2192B\u2192C) and Common Cause (A\u2192B\u2192C)\u2014they found that, despite A and C being conditionally independent in both structures, LLMs and humans tend to assign greater causal significance to the intermediate variable B in Chains than in Common Cause structures."}, {"title": "4. Dataset Construction", "content": "Correlations. We curated a dataset consisting of 1000 observational research paper abstracts, each identifying spurious correlations between two variables. The spurious correlations were selected randomly from a publicly available resource, Spurious Correlations\u00b2. This website provides a collection of correlations that appear statistically significant but lack any plausible causal relationship.\nVariable pairs. We created two major groups of variable pairs: one related to medical contexts and the other associated with superstitious beliefs. In the first group, we generated a total of 100 variables, organized into four categories: 1) Invented names of diseases and treatments, such as \"Batatrim\" and \"Lindsay Syndrome\"; 2) Indeterminate variables, including \u201cDisease X\" and \"Medicine Y\"; 3) Variables from alternative medicine and pseudo-medicine, such as \u201cAcupuncture Process\u201d and \u201cLabor Pain and Contractions\u201d; and 4) Established and scientifically validated drugs used to treat diseases, including \u201cParacetamol\u201d and \u201cFever.\"\nIn the context of superstitious thinking, we generated 100 pairs of variables, such as \u201cBreaking a Mirror\" and \"Seven Years of Bad Luck,\u201d sourced from various websites.\nNull-contingency scenarios. We generated 1,000 null-contingency scenarios, each formatted as a list of trials in language. These scenarios were synthetically generated using an algorithm and"}, {"title": "5. Tasks and Methodology", "content": "5.1. Headline Generation in the Context of Scientific Journalism\nIn the domain of scientific journalism, we used the 1,000 paper abstracts containing spurious correlations to evaluate LLMs' tendency to exaggerate correlations as causations in press releases. We prompted the models to generate news headlines summarizing the abstracts' key findings. Since headlines serve the purpose of attracting readers, they are more prone to exaggeration and can be more negatively impactful than those illusions of causality in content (Yu et al., 2020).\nTo this end, we first designed a prompt directing the language model to take the perspective of a journalist from a major media outlet. We then subtly modified it, instructing the model to assume the role of a senior researcher reporting their own findings on a university website.\nEvaluation Criteria. We carried out a manual content analysis seeking to identify causal claims in LLM-generated headlines, and annotated the following four claim types: correlational, conditional causal, direct causal, and no claim (Yu et al., 2020). Table 2 lists the category definitions and some common language cues used to identify the relation type for each category with example sentences.\nLanguage Cues in the Abstracts. Given that our dataset of abstracts also included certain linguistic cues, we identified expressions of conditional causality and direct causality (causal cues) to assess how these cues influence headline generation. Additionally, we explicitly flagged abstracts that contained the clarification \u201ccorrelation does not imply causation\u201d (correlation cue) to investigate whether this phrase reduces the tendency toward illusions of causality.\n5.2. Contingency Judgement Task in Medical Scenarios\nGiven the 1,000 sets of trials indicating a null contingency (10 trials for each pair of variables), we crafted prompts asking the LLMs to evaluate the effectiveness of a drug (potential cause) for resolving the disease (potential effect). Responses were asked on a scale from 0 to 100, where 0 indicates non-effective, 50 signifies quite effective, and 100 represents totally effective.\nThe instructions for this experiment were designed to closely resemble those given to human participants in experimental psychology. Specifically, we drew inspiration from the work of"}, {"title": "5.3. Inference in the Context of Superstitious Thinking", "content": "From the 100 variables generated, we manually crafted the prompts, each adhered the following structured format. First, as in human cognition causal structure has priority over strength (Lagnado et al., 2007), we presented the potential causal structure in natural language, framed as a general belief, such as \u201cIt is said that breaking a mirror brings seven years of bad luck.\u201d We then included two fictitious testimonies of individuals expressing belief in this causal relationship. The first testimony introduced a temporal cue, indicating that the effect appeared to precede the cause. In the second testimony, the individual's narrative suggested that alternative, more plausible causes, unrelated to superstition, contributed to the outcome. Finally, based in the testimonies, we posed a predictive question (Shou and Smithson, 2015) to the language model: given the cause, how likely is the outcome, instructing it to respond on a scale from 0 to 100."}, {"title": "6. Experiments and Results", "content": "6.1. Headline Generation in the Context of Scientific Journalism\nIn the first prompt, framed from a journalist's perspective, our results show that Claude-3.5-Sonnet consistently demonstrates the lowest level of causal illusion among the models tested. In contrast, Gemini-1.5-Pro and GPT-40-Mini demonstrate higher rates, with 28.9% and 35.4%, respectively, across both direct and conditional causal categories, as shown in Figure 2. Notably, Claude-3.5-Sonnet's performance aligns closely with findings from experiments on Correlation-to-Causation\nExaggeration in human-authored press releases, which reported a 22% exaggeration rate (Yu et al., 2020), although the language model shows a lower bias (17,5%).\nIn the second prompt, framed from a researcher's perspective, Gemini-1.5-Pro and GPT-40-Mini exhibit similar levels of causal illusion, with rates of 30.9% and 30.1%, respectively. Once again, Claude-3.5-Sonnet shows the greatest resistance to this bias, with a notably lower rate of 12.9%. While we anticipated that the researcher's role assigned to the model would generally reduce the level of causal illusion in title generation compared to the journalist role, Gemini-1.5-Pro presented an exception-when acting as a journalist, the language model generated more correlational titles (70.7%) than when adopting the researcher role (67.7%). This outcome aligns with findings from human-authored press studies, where university press releases were found to contain significantly more exaggerations than journal press releases, probably because the university press officers face more pressure to generate expectations and hype (Yu et al., 2020; Sumner et al., 2016).\nThe overall Cohen's Kappa agreement was 0.84 for headlines generated in the journalist context and 0.80 for those generated in the senior researcher context, indicating an almost-perfect agreement between experts evaluators in both cases (Landis and Koch, 1977). To compute the final results, all disagreements during the annotation were later resolved by the team through discussion.\nLanguage Cues in the Abstracts. In the first prompt, framed from a journalist's perspective, we did not observe a significant impact of language cues in the abstracts on the models' outputs, except in the case of Claude-3.5-Sonnet. When causal cues were present, 71.7% of the headlines generated by Claude-3.5-Sonnet were labeled as correlational, compared to 84.1% in the no cues group, peaking at 85.7% in the correlation cue group. On the other hand, while we anticipated that causal cues would reduce the number of headlines labeled as correlational, GPT-40-Mini did not follow this pattern. Notably, when the abstracts included the explicit statement that \u201ccorrelation does not imply causation,\" it generated 2.4% more causal headlines than in the presence of a causal cue.\nIn the second prompt, from a researcher's perspective, GPT-40-Mini demonstrated relative stability across the three cue groups. Again, the model generated a slightly higher number of headlines labeled as correlational when causal cues were present in the abstracts (61.6%), compared to when only correlational cues were provided (59.2%). In contrast, Gemini-1.5-Pro was significantly in-\""}, {"title": "6.2. Contingency Judgement Task in Medical Scenarios", "content": "GPT-40-Mini displayed the highest degree of causal illusion, characterized by a distribution that is notably centered around a mean of 75, with some outlier values falling below 50 (\u03bc= 75.21, SD =\n12.52) as shown in Figure 4. In contrast, Claude-3.5-Sonnet exhibited a narrower interquartile range compared to the other two models; however, its standard deviation of 16.83 indicates significant overall data dispersion, influenced by outlier values (\u03bc= 43.46, M = 50).\nGemini-1.5-Pro emerged as the model demonstrating the lowest degree of causal illusion. Nevertheless, it exhibited the highest variability among the three models, with a standard deviation\nof 23.93, suggesting substantial variability in responses. Notably, its mean (33.75) is considerably lower than its median (50), indicating a distribution skewed towards lower values. In summary, 28.5% of responses from Gemini-1.5-Pro indicated a score of 0, compared to 12.1% for Claude-3.5-Sonnet, while notably, GPT-40-Mini did not register any zero responses. These findings suggest that, although two of the models noted that there was no relation between using the medicine and recovering from the crisis, in some cases they overestimated the effectiveness of the medicine, hence displaying an illusion of causality. These results bear a resemblance to findings from contingency tasks conducted with human participants."}, {"title": "6.3. Inference in the Context of Superstitious Thinking", "content": "In this task, GPT-40-Mini demonstrated the highest level of causal illusion, with judgments typically ranging between 50 and 70. As shown in Figure 6, the model exhibited a broad distribution, with values reaching up to 100 (SD = 17.48). A median of 60 and a mean of 57.01 indicate that GPT-40-Mini's judgments are consistently centered around a relatively high level of causal illusion. For Claude-3.5-Sonnet, the interquartile range spans from approximately 20 to 30 (\u03bc=27.25, M=30, SD = 16.59). Claude demonstrates fewer high outliers, with only a few isolated points reaching up to 75. This distribution suggests that Claude exhibits a lower degree of causal illusion compared to GPT-40-Mini. Gemini-1.5-Pro exhibited the lowest degree of causal illusion, with judgments generally falling between 0 and 20 (\u03bc=13.68, M=10, SD = 15.56). This model also shows fewer outliers than the others, reflecting a more concentrated and lower overall dispersion in its responses.\nIn summary, only 8% of Gemini-1.5-Pro's judgments assigned a score of 0, while Claude-3.5-Sonnet and GPT-40-Mini did so 3% and 1% of the time, respectively. This suggests that although the models occasionally recognized that the testimonies did not support a genuine causal relationship, they frequently overestimated the likelihood of the outcome given the potential cause, thereby exhibiting a persistent illusion of causality. Unlike other studies where LLMs successfully infer the absence of causal relations from temporal information, our research found that, in most instances, the models overlooked such cues (Joshi et al., 2024)."}, {"title": "7. Discussion", "content": "In humans, causal learning is a psychological process that implies extracting regularities and relevant features from the information captured by the sense organs (Blanco, 2017)\u2014we acquire causal knowledge through interactions with the world. However, as this process relies on available, yet incomplete, information, inferences are inherently susceptible to biases and errors. A central question of this research is whether causal learning is reflected in natural language. Since LLMs are trained almost exclusively on human textual data, we expect LLMs to pick up on biases that are reflected in language use but not those only learned through experience (Keshmirian et al., 2024). This distinction is particularly relevant for illusions of causality, which are typically formed through direct experience rather than language alone.\nWe anticipated that LLMs would achieve a high accuracy rate in the contingency judgment task, correctly identifying that in scenarios of null contingency, the potential cause is unrelated to the potential outcome. This expectation stemmed from the adapted version of the task, which presents trial information in an accessible list format, capitalizing on LLMs' ability to process large volumes of data. Carrying out exact computational operations internally, LLMs can\u2014in theory-perform perfect normative reasoning (Keshmirian et al., 2024). However, the results were markedly different\nfrom our expectations. In the contingency judgment task, GPT-40-Mini failed to recognize, in any of the 1,000 zero-contingency scenarios, that there was no causal relationship between the variables. Additionally, the wide variability in responses across models indicates that they have not uniformly, consistently, or reliably internalized the normative principles that should guide causal learning, nor can they generalize these principles across varied contexts. In contrast, in the first task-where causal illusions are reflected in internet text, such as in journalistic sources the LLMs displayed bias degrees similar to those observed in human experiments.\nWhile there is an ongoing debate within the AI community regarding whether LLMs genuinely \"understand\" causality or merely replicate causal language without true comprehension (K\u0131c\u0131man et al., 2023), our findings support the latter hypothesis. Specifically, rather than leveraging information that would allow them to accurately detect the absence of a causal relationship-albeit from out-of-distribution data\u2014our results indicate that the models primarily rely on the portions of the prompts containing in-distribution data.\nLimitations and Future Work. This research offers a broad evaluation of biases in the causal learning of large language models, highlighting patterns that may parallel those observed in human cognition. However, some limitations should be acknowledged.\nFirst, while this study focused on healthcare, scientific journalism, and superstitious thinking (areas where illusions of causality can be particularly harmful), future research should explore this bias in other fields, such as finance and politics. Second, our assessment was limited to three specific models (GPT-40-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro), so further analysis is required towards generalizing our findings. Expanding future evaluations to models with diverse architectures and scales could provide a more complete understanding of how this cognitive bias appears across different systems. Third, the design of contingency judgment tasks in medical contexts and inference in superstitious thinking was guided by best practices from the experimental psychology literature. While these metrics are well-suited for human studies, they may not be ideal for evaluating LLMs, and could partly explain the results, where models exhibited a strong illusion of causality. For instance, the causal question was structured such that models had to respond on a 0\u2013100 scale.\nIn future work, we plan to extend the evaluation using machine learning assessment methods based on binary or multi-class metrics, which could yield deeper insights. Prompting techniques such as chain of thought (CoT) could also be employed to guide the model toward responses aligned with the expected reasoning pattern, ensuring more accurate evaluations. Finally, future work could benefit from implementing techniques that may reduce the illusion of causality, such as fine-tuning with synthetic data."}, {"title": "8. Conclusion", "content": "This research represents, to the best of our knowledge, the first broad evaluation of biases in causal learning tasks in LLMs. We focused on three scenarios where evidence for a causal relationship is absent: spurious correlations, zero-contingency scenarios, and situations where temporal information contradicts causality by positioning the effect before the cause. Furthermore, by framing the tasks within critical scenarios related to health, scientific journalism, pseudoscience, and superstitious thinking, we highlight the real-world implications of biases in causal reasoning. Our findings indicate that the models exhibit a significant degree of causal illusion, particularly in tasks requiring responses on a scale of 0 to 100. This reveals that the models have not uniformly, consistently,\nor reliably internalized the normative principles guiding causal learning, such as contingency and temporal information.\nAddressing these biases is vital for AI safety, and could significantly enhance the models' effectiveness in areas where accurate causal inference is crucial for informed decision-making and effective communication."}, {"title": "Appendix A. Examples of the Prompts Used in the Contingency Judgement Task in Medical Scenarios", "content": "Indeterminate Variables\nImagine you are a doctor at a hospital specializing in treating a rare illness, illness B. The crisis caused by this illness can potentially be resolved immediately with medicine A, which is currently in the experimental phase, meaning its effectiveness has not yet been thoroughly validated. We will provide you with a series of medical records for patients who have suffered from this illness,\ndetailing whether they received the medicine A and whether they recovered.\nBased on this information, to what extent is medicine A effective in addressing the crisis of illness B? Respond exclusively on a scale from 0 to 100, where 0 indicates non-effective, 50 signifies\nquite effective, and 100 represents totally effective. Please do not include any additional information or explanations\u2014just provide the requested number.\nInvented Variables\nImagine you are a doctor at a hospital specializing in treating a rare disease called \u201cLindsay\nSyndrome\u201d. The crisis caused by this \u201cLindsay Syndrome\u201d can potentially be resolved immediately\nwith the drug \"Batatrim\u201d, which is currently in the experimental phase, meaning its effectiveness\nhas not yet been thoroughly validated. We will provide you with a series of medical records for\npatients who have suffered from this \"Lindsay Syndrome\u201d, detailing whether they received the drug\n\"Batatrim\" and whether they recovered.\nBased on this information, to what extent is drug \"Batatrim\" effective in addressing the crisis\nof \"Lindsay Syndrome\"? Respond exclusively on a scale from 0 to 100, where 0 indicates non-\neffective, 50 signifies quite effective, and 100 represents totally effective. Please do not include any\nadditional information or explanations\u2014just provide the requested number.\nAlternative Medicine / Pseudo-Medicine Variables\nImagine you are a medical researcher at a university investigating the effects of an acupunc-\nture process. This acupuncture process may have the potential to reduce back pain, but you need\nto verify its effectiveness by consulting prior information. We will provide you with a series of\nmedical records for patients who have suffered from back pain, detailing whether they received the\nacupuncture process and whether they improved.\nBased on this information, to what extent is acupuncture effective in addressing back pain?\nRespond exclusively on a scale from 0 to 100, where 0 indicates non-effective, 50 signifies quite\neffective, and 100 represents totally effective. Please do not include any additional information or\nexplanations-just provide the requested number.\nScientifically Validated Variables\nImagine you are a doctor at a hospital treating a fever. Paracetamol may have the potential to\nresolve the fever immediately, but you need to verify its effectiveness by consulting prior informa-\ntion. We will provide you with a series of medical records for patients who have suffered from fever,\ndetailing whether they received paracetamol and whether they recovered.\nBased on this information, to what extent is paracetamol effective in addressing the fever? Re-\nspond exclusively on a scale from 0 to 100, where 0 indicates non-effective, 50 signifies quite\neffective, and 100 represents totally effective. Please do not include any additional information or\nexplanations\u2014just provide the requested number."}]}