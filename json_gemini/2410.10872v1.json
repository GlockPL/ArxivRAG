{"title": "TOOLBRIDGE: AN OPEN-SOURCE DATASET TO EQUIP LLMS WITH EXTERNAL TOOL CAPABILITIES", "authors": ["Zhenchao Jin", "Mengchen Liu", "Dongdong Chen", "Lingting Zhu", "Yunsheng Li", "Lequan Yu"], "abstract": "Through the integration of external tools, large language models (LLMs) such as GPT-40 and Llama 3.1 significantly expand their functional capabilities, evolving from elementary conversational agents to general-purpose assistants. We argue that the primary drivers of these advancements are the quality and diversity of the training data. However, the existing LLMs with external tool integration provide only limited transparency regarding their datasets and data collection methods, which has led to the initiation of this research. Specifically, in this paper, our objective is to elucidate the detailed process involved in constructing datasets that empower LLMs to effectively learn how to utilize external tools and make this information available to the public through the introduction of ToolBridge. ToolBridge proposes to employ a collection of general open-access datasets as its raw dataset pool and applies a series of strategies to identify appropriate data entries from the pool for external tool API insertions. By supervised fine-tuning on these curated data entries, LLMs can invoke external tools in appropriate contexts to boost their predictive accuracy, particularly for basic functions including data processing, numerical computation, and factual retrieval. Our experiments rigorously isolates model architectures and training configurations, focusing exclusively on the role of data. The experimental results indicate that LLMs trained on ToolBridge demonstrate consistent performance improvements on both standard benchmarks and custom evaluation datasets. All the associated code and data will be open-source at https://github.com/CharlesPikachu/ToolBridge, promoting transparency and facilitating the broader community to explore approaches for equipping LLMs with external tools capabilities.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have revolutionized natural language processing, excelling in tasks including question answering, summarization, and text generation [1, 2, 3]. Despite the impressive achievements of LLMs, they persistently underperform in fundamental areas, such as arithmetic or factual lookup, where introducing external tools like calculators and search engines can effectively provide solutions [4, 5, 6, 7, 8, 9, 10].\nConsequently, there has been a rise in research efforts aimed at equipping LLMs with the ability to leverage external tools. For example, Llama 3.1 [11] is trained to utilize Brave Search, Python interpreter and Wolfram Alpha API by using synthetic data combined with human-annotated data. Toolformer [6] adopts a self-supervised training strategy to equip LLMs with the capability to leverage diverse tools effectively. However, these existing LLMs tend to either only open-source their trained models and code or restrict access to the models through usage interfaces [9, 11, 2]. The data underlying the training of these models is commonly withheld from public disclosure, enveloped in secrecy.\nThe widespread availability of pre-trained models stands in stark contrast to the limited transparency surrounding their training datasets, which impedes the community's progress in advancing discoveries in this area. This emphasizes the critical need to explore ways to create high-quality, publicly available benchmark datasets for training LLMs to utilize external tools to facilitate their reasoning process.\nIn response to the challenges discussed, this paper proposes a pipeline for the large-scale creation of datasets designed to train LLMs in leveraging external tools. Specifically, we begin by aggregating a substantial collection of open-source datasets used for LLM supervised fine-tuning (SFT) from the community, which circumvents proprietary concerns (e.g.,"}, {"title": "2 Related Work", "content": "Tool Use for LLMs. Enabling LLMs to use tools like search engines and code interpreters significantly broadens the range of tasks they can address and improves their accuracy in tasks involving data processing, numerical computation"}, {"title": "3 ToolBridge", "content": "Previous models like GPT-40 and Llama 3.1 only provide limited information on how they curate the data to train LLMs to leverage external tools. To address the lack of transparency in training data, we propose a generic pipeline for constructing large-scale datasets from public sources to enable LLMs to use external tools. As illustrated in Figure 2, the whole pipeline follows three main steps: valuable data entries selection, conversion and filtering."}, {"title": "3.1 Dataset Pool Construction", "content": "Our research starts with a comprehensive review of the data accessible in the community for the purpose of supervised fine-tuning (SFT). Table 1 summarizes the results. Owing to the diverse range of teams contributing the SFT datasets, there is significant heterogeneity in their formats, which poses challenges for effective model training. Hence, we first reformat all candidate datasets into a standardized ChatML format for further processing:\nentry = [{\n    \"role\": \"user\", \"content\": \"...\"},\n    {\"role\": \"assistant\", \"content\": \"...\"},\n    ...]."}, {"title": "3.2 Valuable Data Entries Selection", "content": "Upon constructing the dataset pool P, due to the large scale of candidate data entries, we propose to select 10 million data entries from P for further processing according to the dataset attributes Wi and Qi. Particularly, we first arrange Di in descending order based on the value of Qi \u00d7 Wi. Then, the data entries are selected from the top-ranked datasets sequentially until the overall volume reaches 10 million entries.\nSubsequently, Llama3-70B with the prompt detailed in Appendix A.1 is applied to ascertain whether each entry within the 10 million samples is appropriate for LLMs to enhance reasoning via introducing external tools. And we represent the collection of these appropriate data entries with V (i.e., valuable data entries)."}, {"title": "3.3 Valuable Data Entries Conversion", "content": "Following valuable data entries selection, we further convert the selected data entries, allowing the language models to learn how to invoke external tools effectively within the proper context to support their reasoning process. In particular, we draw on previous approaches [6, 11, 5] by embedding special characters in each selected entry to enable the external tool invocation, and LLMs are employed to pinpoint the appropriate context for calling external tool APIs.\nAs illustrated in the examples in Figure 1, <python> and </python> are represented as a pair of special tokens. While the content enclosed by the special tokens specifies the Python invocation for external tool use. During the construction of ToolBridge, we predominantly utilize GPT-40-mini to insert the special tokens in the correct context within each data entry identified in Section 3.2, as well as to create the associated code for invoking the tools. To facilitate the return of tool execution results, we examine the code format utilized by GPT-40 when calling Python API as part of its reasoning process while answering user queries. It is observed that the final result is always printed at the end of the code. In line with the approach of GPT-40, we include a directive in the prompt for GPT-40-mini to print the final result of the tool execution as the concluding line of the code (refer to Appendix A.2).\nFollowing GPT-40-mini's processing of V, we extract the code segments between <python> and </python>, execute them, and finally insert the captured output after corresponding </python>, where the output will be wrapped within another pair of special tokens, denoted as <result> and </result>. To summarize, the tool invocation in ToolBridge can be formatted as,\n<python>generated code</python><result>captured output</result>.\nDuring the reasoning process of the models, it is sufficient to check for the presence of the pre-defined special tokens <python> and </python> during generation. If identified, Python interpreter will be adopted to execute the enclosed code between the special tokens, and the executed result is wrapped in <result> and </result> to condition further text generation. Algorithm 1 describes the primary inference process of LLMs post-SFT on the ToolBridge dataset."}, {"title": "3.4 Data Entries Filtering by Consistency Validation", "content": "In practice, we observe that LLMs trained on C do not always base their subsequent content on the results produced by the yielded Python code during inference. So, we conduct a re-evaluation of the data entries within C and observe that the execution results from the code generated by GPT-40-mini also does not always align with the ensuing text, which explains LLMs' sporadic inconsistencies between tool execution results and further contents during inference.\nTo alleviate the issues above, we propose to filter the data entries in C where the tool execution results are inconsistent with the following text, which is achieved by checking if the execution results are included in the subsequent text in our implementation. Upon completing filtering, the open-source dataset ToolBridge is finally constructed. In Appendix A.5, we compare the generated text of Llama3-8B after SFT on C and ToolBridge, which demonstrates the necessity for the data entries filtering by consistency validation.\nThe data sources that make up ToolBridge, totaling 178,023 entries, are outlined in Table 4, which represents 48.8% of the total data entries in C."}, {"title": "4 EXPERIMENTS", "content": "In this section, we first present the statistics of ToolBridge and then investigate whether LLMs with SFT on ToolBridge can use external tools to facilitate their reasoning process. The validation includes three parts: (1) by comparing LLMs' performance on standard benchmarks before and after SFT on ToolBridge (Section 4.3); (2) by evaluating the accuracy of the models on the custom datasets (Section 4.4); (3) by comparing the qualitative results of LLMs (Section 4.5)."}, {"title": "4.1 Dataset Statistics of ToolBridge", "content": "In Table 4, we have exhibited 15 source datasets involved in the data entries of ToolBridge, along with their respective composition ratios. To prevent any confusion for these datasets (e.g., other datasets with the same name and the same datasets with different versions), we also provide download links for these datasets in Appendix A.6."}, {"title": "4.2 Experimental Setup", "content": "Baseline Models. Our experiments involve four baseline models: the base model of Llama2-7B and Llama3-8B. Also, we remove all the external tool invocation sections in each data entry in ToolBridge (denote as ToolBridges) and report the results of Llama2-7B and Llma3-8B SFT on ToolBridges as two additional baseline models.\nBenchmark Datasets. The standard benchmark datasets used in our experiments include GSM 8k [4], GSM Plus [24], MathBench [25] and Stanford WebQA [26]. For GSM 8k and GSM Plus, we conduct performance evaluation on their respective test sets under few-shot setting, where we leverage a fixed CoT-n-shot prompt template, as outlined in [24]. For MathBench, we report results on MathBench-A, where we transform the multiple-choice questions in the College, High and Middle categories into a question-and-answer format for CoT-n-shot evaluation. To differentiate from original MathBench, we refer to this adjusted dataset as MathBench*.\nWe also design two custom datasets, named RandomQA and FACT, to evaluate the capabilities of the language models in data processing, numerical computation and factual retrieval. Section 4.4 elaborates on the specifics of both datasets.\nSFT Settings. All the models in our experiments are trained with the open-source TRL library from Hugging Face [27]. The LoRA module [28] is used to perform SFT on the base model of Llama2-7B and Llama3-8B utilizing ToolBridges or ToolBridge, with a LoRA rank of 16. Model training is conducted on 64 \u00d7 MI200 64GB GPUs, with each processing a batch size of 2 (i.e., total batch size is 128). AdamW is employed to optimize the parameters of LoRA, with a cosine learning rate scheduler, configuring the initial lr at 3e-5 and the total training epoch at 3.\nInference Settings. The primary process of model inference is described in Algorithm 1. Moreover, to handle potential tool call failures during inference, we propose to eliminate failed tool calls from the current output before conditioning the generation of further text. All trained models are evaluated on 16 \u00d7 MI200 64 GB GPUs, with the max new tokens set to 512 and the temperature set to zero."}, {"title": "4.3 Results on Standard Benchmarks", "content": "In this section, we conduct ablation studies on standard benchmark datasets, including GSM 8k, GSM Plus, MathBench and Stanford WebQA, where GSM 8k, GSM Plus and MathBench are primarily responsible for evaluating the capability of LLMs in numerical reasoning and computation, and Stanford WebQA is primarily used to assess the ability of LLMs in factual retrieval."}, {"title": "4.4 Results on Custom Benchmarks", "content": "To further assess whether SFT on the ToolBridge dataset can equip LLMs with the ability to leverage external tools for aiding its reasoning process, we propose to design two custom datasets to evaluate LLMs' performance before and after SFT on the ToolBridge dataset, namely, the RandomQA dataset and the FACT dataset.\nRandomQA. To assess the improvements in LLMs' data processing and numerical computation capabilities after SFT on ToolBridge, we propose to design 50 templates capable of generating question-answer pairs to validate the abilities of the large language models in these areas. Here are some examples,\n# Template1: Generate the smallest prime number greater than x\nnum = random.randint (2000, 100000)\nquestion = f\"Generate the smallest prime number greater than {num}.\"\nanswer = nextprime (num)\n# Template2: Calculate time difference between two time zones\ntz1, tz2 = random.sample(pytz.all_timezones, 2)\nnow = datetime.datetime.now()\ntime1 = pytz.timezone (tz1).localize(now)\ntime2 = pytz.timezone (tz2).localize(now)\nquestion = f'Calculate time difference between {tz1} and {tz2} in seconds.'\ntime_difference = abs((time1 time2).total_seconds())\n# Template3: Find the sum of all elements above the main diagonal of a matrix\nmatrix_len = random.randint(2, 10)\nmatrix = [[random.randint(1000, 1000000) for in range(matrix_len)] for in range (matrix_len)]\nquestion = f\"Find the sum of all elements above the main diagonal of the matrix {matrix}.\"\nanswer = sum(matrix [i] [j] for i in range(matrix_len) for j in range(i + 1, matrix_len))\n# Template4: Sum all odd numbers in a list\narray = [random.randint(1000, 1000000) for in range(random.randint(5, 15))]\nquestion = f\"Sum all the odd numbers in the list {array}.\"\nanswer = sum(x for x in array if x % 2 != 0)\nFACT. To determine if the factual retrieval skills of LLMs can be improved by SFT on ToolBridge, we propose to build the FACT dataset. Specifically, we begin by prompting GPT-40 to produce thousands of question-answer pairs focused on factual retrieval. Some examples are as shown following,\nGenerate 100 Q&A pairs for LLM factual retrieval testing. The question topic should be related with Geography. Return them as a Python dictionary, with concise answers (3-5 words).\nGenerate 100 Q&A pairs for LLM factual retrieval testing. The question topic should be related with History. Return them as a Python dictionary, with concise answers (3-5 words).\nAppendix A.8 contains the entire set of the leveraged prompts for constructing FACT. Upon obtaining these candidate question-answer pairs, we continuously draw random data entries from them and manually check their correctness until 200 correct data entries are verified."}, {"title": "4.5 Qualitative results", "content": "Finally, we provide a comparison of the qualitative results from Llama3-8B against other LLMs, both those capable of utilizing external tools to aid its reasoning process (e.g., Llama3.1 and GPT-40) and those that are not (e.g., Gemma2).\nAs shown in Figure 3, we can observe that, after SFT on ToolBridge, Llama3-8B demonstrates significant advantages in areas such as numerical computation, owing to its ability to utilize external tools to facilitate its reasoning process. For example, when calculating trigonometric functions, Llama3-8B SFT on ToolBridge can obtain accurate trigonometric values for each angle by invoking the Python math library, while LLMs that are unable to invoke external tools can only provide an answer with significant errors."}, {"title": "5 Conclusion", "content": "In this paper, we seek to shed light on the process of constructing datasets for training LLMs in the use of external tools. Specifically, we propose a pipeline to build the ToolBridge dataset, where the foundation of ToolBridge lies in the raw data entries for SFT, sourced from the community. The selection, conversion, and filtering of these raw data entries are crucial steps in developing high-quality training data for equipping LLMs with external tool abilities. Our experiments demonstrate that SFT on ToolBridge enables LLMs to effectively learn how to utilize external tools to aid in reasoning process, particularly in the areas including data processing, numerical computation and factual retrieval. We will make the data generation pipeline for training LLMs in tool use publicly available, along with ToolBridge, for the benefit of the community in advancing research on enabling LLMs to learn how to use external tools."}]}