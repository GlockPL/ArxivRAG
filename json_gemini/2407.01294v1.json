{"title": "A Collaborative, Human-Centred Taxonomy of Al, Algorithmic, and Automation Harms", "authors": ["GAVIN ABERCROMBIE", "DJALEL BENBOUZID", "PAOLO GIUDICI", "DELARAM GOLPAYEGANI", "JULIO HERNANDEZ", "PIERRE NORO", "HARSHVARDHAN PANDIT", "EVA PARASCHOU", "CHARLIE POWNALL", "JYOTI PRAJAPATI", "MARK A. SAYRE", "USHNISH SENGUPTA", "ARTHIT SURIYAWONGFUL", "RUBY THELOT", "SOFIA VEI", "LAURA WALTERSDORFER"], "abstract": "This paper introduces a collaborative, human-centered taxonomy of AI, algorithmic and automation harms. We argue that existing taxonomies, while valuable, can be narrow, unclear, typically cater to practitioners and government, and often overlook the needs of the wider public. Drawing on existing taxonomies and a large repository of documented incidents, we propose a taxonomy that is clear and understandable to a broad set of audiences, as well as being flexible, extensible, and interoperable. Through iterative refinement with topic experts and crowdsourced annotation testing, we propose a taxonomy that can serve as a powerful tool for civil society organisations, educators, policymakers, product teams and the general public. By fostering a greater understanding of the real-world harms of AI and related technologies, we aim to increase understanding, empower NGOs and individuals to identify and report violations, inform policy discussions, and encourage responsible technology development and deployment.", "sections": [{"title": "INTRODUCTION", "content": "Much has been said about the benefits, opportunities and risks of artificial intelligence (AI), algorithms and automation, notably by industry, government and advisors. Conversely, researchers and others have spent significantly less time exploring and assessing the near-term harms of these technologies, even though these are real, can be highly damaging, and affect individuals, groups, communities, organisations, societies and environmental ecosystems.\nOn the surface, this seems surprising. The harms of these systems are wide-ranging, and significant. They have been known to result in the erosion and loss of fundamental liberties, such as through wrongful arrests enabled by flawed facial recognition technology\u00b9, and infringe on the well-being and livelihoods of artists and other creators, whose works are used without acknowledgment or consent to train large language models\u00b2."}, {"title": "", "content": "Furthermore, they create and exacerbate social problems like teenage addiction to social media platforms fueled by attention-seeking algorithms\u00b3, disrupt democratic political elections in the form of deepfakes\u2074, and place immense strain on local communities by depleting essential resources such as energy and water through the operation of data centres\u2075. These are just a few examples of the diverse and far-reaching harms that AI and AI-adjacent technologies are known to inflict.\nNonetheless, the identification and evaluation of negative impacts can be challenging. The workings of some technological systems, notably deep neural networks, remain opaque, partly due to inadequate explainability. More broadly, in the absence of legislation and reporting standards, system developers and deployers have little incentive to publicly acknowledge or report upstream or downstream issues, incidents or harms lest they increase their strategic, reputational, operational, financial or legal exposure.\nHarms can also be hard to grasp and quantify; differentiating between the primary, secondary and tertiary impacts of particular technologies or applications, or their direct and indirect, and tangible and intangible impacts, is trickier than first meets the eye [18]. In addition, the harms of new technologies such as generative AI and emotion recognition are unclear and will evolve as new uses emerge, bad actors find novel ways to misuse them, and societal attitudes and behaviours evolve. Further complicating this picture is the often unregulated marketing of poorly designed products [26] that achieve mainstream adoption with low barriers to entry.\nThis information vacuum means most consumers and citizens, and many others, while aware of AI and related technologies, remain in the dark about the actual damage caused by their use and misuse-a lack of knowledge and understanding that has likely resulted in widespread and growing public scepticism about the use of these technologies in daily life [32].\nHarms taxonomies can provide a solid foundation for informing policy, educating citizens, capturing and tracking violations, and managing product risks. A number of taxonomies of AI-related harms exist, but these tend to reflect the goals and biases of their creators, suffer from inconsistent nomenclature and definitions, mean different things to different people, and are incomplete and not maintained.\nThe UN AI Advisory Board has described developing \u201ca comprehensive list of AI risks for all time' as a 'fool's errand\" [33]. It may be challenging, but the development of a clear, horizontal harms taxonomy is a worthwhile, indeed necessary, endeavour. It should help solidify the harms-and risks- and thereby:\n\u2022 improve general public and other stakeholders' literacy;\n\u2022 empower citizens and NGOs to report violations;\n\u2022 strengthen the armoury of journalists, investigators and auditors [25];\n\u2022 put more power in the hands of civil society organisations;\n\u26ab enable more effective risk management, and\n\u2022 foster a greater sense of ethics and responsibility amongst developers, deployers, and others.\nPrior works to develop taxonomies of AI harms [24, 29] are primarily designed to enable policymakers to inform legislative proposals, and develop risk assessment and mitigation strategies. These works may be beneficial and contribute to a better understanding and systematic analysis of harms, but they are often narrow, unclear in terms of categorisation, naming and definitions, and become quickly out-of-date.\nConsequently, the great majority of consumers and citizens, as well as many students, journalists, policymakers, and others remain woefully uninformed about the real dangers and actual harms of AI and related technologies. Little surprise that the general public often fails to report harms when they occur, and is generally excluded from the risks and harms identification, classification and reporting process, and from the governance debate that should rightfully follow [30]."}, {"title": "", "content": "With the focus of policy-makers and regulators shifting from risks to violations, and opinion against AI increasing in many countries as exposure increases, we believe it is an opportune time to introduce such a classification.\nThis paper introduces a collaborative and human-centered harms taxonomy aimed to serve as a powerful tool for civil society organisations, educators, policymakers, and product teams. We draw on other taxonomies, topic experts, and the 1, 500+ incidents and issues captured in the AIAAIC Repository\u201d to set out a taxonomy of harms experienced across a wide variety of domains, countries, and cultures due to the inappropriate use and misuse of AI and adjacent technologies.\nClustered into nine top-level and sixty-nine sub-categories, our taxonomy has benefited from several iterations having been reviewed by experts from multiple disciplines and members of the general public from diverse backgrounds, genders, ages, and interests. It has also been tested multiple times using a custom-built annotation tool.\nOur work is not intended to disrupt prior work, but to build on it in a way that makes the harms of AI and related systems clearer and more relevant and actionable to a broader set of audiences. As an ongoing process, we hope to add to the public discussion and technology ecosystem by building an understandable, practical, maintainable, open resource for humankind.\nThis paper is structured as follows:\n\u2022 Section 2 assesses the features and limitations of existing harms taxonomies\n\u2022 Section 3 sets out the process by which the new taxonomy was developed and refined\n\u2022 Section 4 describes the proposed new taxonomy\n\u2022 Section 5 sets out ways the new taxonomy can be used; and\n\u2022 Section 6 discusses the benefits and limitations of the new taxonomy and sets out potential next steps for its development."}, {"title": "RELATED WORK", "content": "Risk and harm taxonomies are at the heart of risk management practices in industries such as aerospace, defense, and healthcare. They constitute the basis for risk modelling and mitigation methods, e.g.: Failure Mode and Effects Analysis (FMEA) [20, 27]. Centuries of engineering convey the lesson that having a common understanding and vocabulary of any technological harm is paramount for its identification and evaluation, and help inform inform policy-making and regulatory compliance.\nVarious works have attempted to characterise the risks and harms of AI and related systems. Some are generic in scope, not limited to a particular domain or technology, while others focus on specific sectors or to particular technology lifecycle stages."}, {"title": "Generic Taxonomies", "content": "Mostly published by inter-governmental organisations of different kinds, generic taxonomies tend to focus on incident analysis and reporting, risk management, and ethical design.\nIncident Analysis & Reporting: An AI classification system [24] developed by The Organisation for Economic Co-operation and Development (OECD) categorises AI systems across five dimensions: people & planet, economic context, data & input, AI model, and task & output. Aimed primarily at policymakers, the framework aims at helps understand the implications of AI systems and their alignment with the OECD AI Principles."}, {"title": "Specific Taxonomies", "content": "The surveyed specific or focused taxonomies are mostly research articles and pre-prints, which can be differenti-ated in harm types, model types, and specific industries.\nSpecific harm type: Other taxonomies focus on specific types of harms, include one focusing on the socio-technical harms of algorithmic systems [28] based on surveying 172 papers. Raji et al. [26] analysed incidents from AIAAIC to propose an AI system failure taxonomy to connect them with actual harms. TASRA is scoped around societal-scale risks and accountability [8]. The authors based it on a fault tree analysis, resulting in a complex 6-level taxonomy.\nSpecific model types: Other taxonomies focus on specific model types such as large language models [34], calling for extensions through case studies or interviews. [4] also focuses on large language models (or foundational models as they coin them) and provide a very detailed survey on related work but do not attempt a comprehensive risk or harm taxonomy. Fergusson et al. [10] provides an overview of the risks of generative AI with illustrative case study examples without claiming comprehensiveness. Another taxonomy by researchers from Sony focuses on generative AI focused on speech [19], differentiating between affected and responsible entities.\nIndustry-specific: Giudici and Raffinetti [11] provides an overview of the risks of AI in line with recent regula-tions and frameworks, such as the EU AI Act, and proposes a related risk measurement model, focused on the financial domain. Babaei et al. [2] extends the model to further domains, providing reproducible Python code. Golpayegani et al. [13] also developed a taxonomy of AI risks in the health domain. In the context of EU AI Act, the vocabulary of AI risks (VAIR) Golpayegani et al. [14] provides a formal taxonomy of concepts related to AI systems and their risks. Finally, Burema et al. [5] analyses 125 incidents involving AI systems across five sectors (police, education/academia, politics, healthcare, automotive) to understand how ethical principles are breached in sector-specific contexts, finding that while some ethical issues span across sectors, others are sector-specific and relate to pre-existing structures and activities within those sectors.\nWe conclude that most existing taxonomies have different foci in terms of target audience and features, and accordingly mean different things to different people.\nFurther, only a few take an 'outside-in', holistic view of the actual, real-world impacts of on individuals, communities, businesses, society the environment, and other entities."}, {"title": "METHODOLOGY", "content": "The proposed harms taxonomy was developed using an open, collaborative and structured process. Its design was informed by more than 1,500 incidents and issues documented in the AIAAIC database. These real-world cases are based on an estimated 10,000+ media, research, legal and other reports from all around the globe, making the"}, {"title": "Elaboration, Testing And Improvement Process", "content": "The harms taxonomy was developed and refined using a structured, iterative methodology that combines a variety of approaches."}, {"title": "Use case analysis:", "content": "The working group first mapped the needs and set the initial goals of the taxonomy through an audit of AIAAIC Repository users. Approximately 50 users, comprising working group members and others identified as having taken part in the AIAAIC's 2023 user survey were surveyed by email on their use of the Repository, what they saw as its limitations and opportunities, and their requirements. Participants were individual users participating in a personal capacity from organisations including Commonwealth Scientific and Industrial Research Organisation (CSIRO), Mozilla, University College London, and the University of Science and Technology of China."}, {"title": "Literature review:", "content": "Relevant existing harms taxonomies were assessed to identify similarities, gaps, strengths and limitations. These previous works are mostly mentioned above and an overview of their respective foci, target audiences and features are outlined in Table 1.\nThis initial mapping, along with a review of current literature exploring the risks and harms of AI and related technologies [3, 7, 17, 31], informed the elaboration of a first version of the taxonomy, deliberately focused on near-term risks and documented harms of AI technologies. Key human rights and civil liberties charters, including"}, {"title": "Expert outreach:", "content": "This initial \u201cwork-in-progress\" version of the taxonomy was presented to experts at NGOs, universities, news publishers and other organisations across the world to collect feedback on its thoroughness and practicality, especially on topics including Economics, Human rights and civil liberties, Misinformation and disinformation, Law, Politics, and Sustainability. The experts who provided feedback are acknowledged and thanked at the end of this paper."}, {"title": "Annotation testing:", "content": "By selecting and annotating entries on the AIAAIC Repository, either randomly or choosing cases evidently different, and annotating them, the working group tested and refined the initial taxonomy to ensure it was clear, easy to use, and extensible. Individual annotations were then compared to identify cases with diverging classifications, collect feedback on the definitions and adapt incrementally the taxonomy, week after week, to maximise consensus and efficiency.\nTo facilitate this process, a dedicated open-source tool was developed by the working group. This user-friendly interface enables quick annotation through drop-down menus with reminders of the definitions, streamlined comment submission, faster human review and consensus assessment. This tool also generates, for each annotated incident, a Sankey diagram, visually depicting the annotators' answers for each incident and computes its"}, {"title": "Validation through broader community review.", "content": "This methodology of continuous incremental improvement and adaptation through multiple, parallel approaches is set to continue as an ongoing process. The methodology and tools set up by the working group are meant to scale and support a roadmap towards a more open and inclusive contribution process, involving a larger community of contributors, both for general or specialised reviews of the taxonomy itself, the annotation of historic cases in existing databases, or the \"live\" classification of new incidents upon reporting them or documenting them in harms and risks databases."}, {"title": "OVERVIEW OF THE HARMS TAXONOMY", "content": "The result of the research, annotation and external reviews outlined in the previous section is presented here. Focusing on harms, as distinguished from causes or risks, supports the project's aim to develop a taxonomy that is understandable and usable by a wide range of stakeholders. It also ensures that the taxonomy is not dependent on technology or context of use and can be applied to a wide range of technologies and applications.\nIt is important to note that the taxonomy presented here is not intended to be static but is dynamic and will evolve through continued annotation efforts, external reviews, and analysis. We therefore share this first version of the taxonomy to communicate our findings to a wider audience and start the process of soliciting additional feedback for further improvements.\nFor the purposes of this taxonomy, a harm is defined as a \"physical, pyschological or other form of damage to third-parties\" that results from the use of an AI, algorithmic or automation system. The taxonomy organises harms at two different levels of granularity. The first level is the \"harm type\u201d, which describes the general category"}, {"title": "BENEFITS AND INTENDED USES", "content": "The taxonomy proposed in this paper is intended to make the harms of AI, algorithmic and automation systems understandable and usable to a wide range of audiences, from experts to the general public.\nWe believe the taxonomy can be used for a wide variety of purposes.\nLiteracy and education: Research studies regularly show a significant gap between awareness and understanding of AI systems-a gap that is reportedly widening, leading to growing concerns about the use of the technology in daily life in the US, UK, and elsewhere.\nDespite these concerns, and the opaque and high-risk nature of some algorithmic systems, governments, educators and others recognise that these technologies play an increasingly central role in the everyday lives of citizens and consumers, government, civil society and business. As such, it is important that they are demystified and made understandable to everyone.\nThis realisation is evident in the use of the AIAAIC Repository by teachers, academics, professional associations, businesses and other entities to educate decision-makers, students, employees, suppliers and others on the nature, risks and harms of AI, algorithmic and automation systems [6]."}, {"title": "", "content": "The harms taxonomy should help make the full range of AI and algorithmic harms of clearer and easier to understand by a broader set of audiences.\nJournalism: AI and algorithmic decision-making for public or commercial purposes are of significant interest to journalists and other civil opinion-formers. However, it can be difficult to untangle actual impacts from official and unofficial sources, each of which may be partial and skewed.\nBy providing evidence-based, impartial data on the use and misuse of AI, algorithmic and automation systems, incident databases incorporating the ability to sort a wide range of actual and potential harms in an easy to understand and use manner can be of real value to technology, business, health and other journalistic beats, as well as to investigative journalists.\nStructured harms data can be used to inform journalistic enquiries and make the case to editors, in addition to supporting and strengthening news, commentary and analysis, and to create visual storytelling or 'data journalism'.\nAdvocacy: Non-governmental civil society organisations (NGOs) making the case for human rights and civil liberties, consumers, patients and other constituencies often expose abuses and violations in order to put pressure on perceived culprits-typically system developers and/or operators, but also malicious bad actors-and to make the case for stronger or better enforced legislation.\nSometimes, an NGO's findings and/or third-party accusations are recorded as a public resource-an example being the Business & Human Rights Resource Centre's news database.\nHowever, the majority of civil society organisations have few resources and struggle to record their findings in a structured manner, or maintain databases. The harms taxonomy can help NGOs identify common harms across different systems, countries and cultures, strengthen existing databases, and create new systems to track harms and related information.\nCitizen reporting: The ability for citizens and other users to hold the developers and deployers of AI and algorithmic technology systems are currently hampered by complaint reporting systems that are often challenging to find, cumbersome to use, and which can deliver slow and partial responses.\nFurthermore, it is often not in the interests of these entities to disclose incidents reported by citizens and other users publicly, lest they be exposed to greater legal, financial, operational, and reputational risk.\nThe AIAAIC Repository and the AI Incident Database (AIID) are considered to be the only two tools that enable individuals and organisations to report incidents based on media reports and, in the case of AIAAIC, legal dockets, research reports and other materials. However, the two entities use different taxonomies and definitions for defining harms, resulting in inconsistent outputs.\nImpending legislation, notably in Brazil, Canada and the European Union, provides citizens with the right to submit complaints about high-risk AI systems and to receive explanations about how these systems work.\nGovernments adopting these proposals will need to develop reporting systems that clearly spell out the harms associated with these systems so that citizens, consumers, patients, etc, can report incidents and issues in a clear, user-friendly and consistent manner.\nThe harms taxonomy can help governments and government agencies develop user-friendly, effective reporting systems that align user needs and expectations with their own obligations and requirements."}, {"title": "", "content": "Furthermore, the taxonomy may prove useful in developing a federated and standardised hybrid reporting framework of the kind proposed by the Center for Security and Emerging Technology 23.\nIn addition, civil society organisations such as advocacy groups, news publishers and other third-party watchdogs can use the taxonomy to inform reports and investigations, and to develop databases and systems tracking human rights, political, environmental and other misuses and violations.\nPolicy making and enforcement: Databases such as the AIAAIC Repository and the OECD AI Incident Monitor 24 provide policymakers and regulators with insights into the risks and harms of technological systems, enabling them to advise politicians and governments using data and examples, and to help set priorities for regulation and enforcement.\nIt may also help regulators develop taxonomies and systems that enable them to identify and track violations relevant to their remit.\nRisk management: The taxonomy proposed in this paper can also be used for risk management, auditing and impact assessment purposes. Several regulations and standards on AI systems underline the need to develop effective risk management models for AI systems (e.g. EU AI Act [9], US NIST risk management framework [29]). Such models should be based on both the likelihood and the expected harm of an incident. While the former can be measured on the basis of statistical metrics that measure the compliance of AI to a set of Safe and trustworthy requirements (see e.g. [2] for a consistent set of such metrics), the latter require an assessment of all possible harms that non-compliance can lead to. Traditional taxonomies focus on the harms caused to a private or public organisation, or to a national entity; however, our taxonomy is able to capture a wide spectrum of harms, occurring to individuals, organisations, society and the environment, across the world, and is therefore better equipped to estimate risks, and prioritise consequent mitigation actions.\nThe proposed taxonomy acts as an open and accessible guiding resource for compliance with risk management and fundamental rights impact assessment (FRIA) obligations of the EU AI Act. Given the cost burden of legal compliance, this is particularly useful for small and medium-sized businesses and less well resourced organisations providing or deploying AI and algorithmic systems.\nGiven the taxonomy will be regularly updated to reflect emerging harms, it should also serve as a valuable reference in shaping governance frameworks by providing insights on the current landscape of risks for policy-makers, regulators, and standardisation bodies."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "Limitations: The taxonomy is designed to be an ever-evolving work, and is still in a relatively early stage of development. Accordingly, it currently has a number of limitations that will be worked on and developed.\nThe creation of the taxonomy has been a labour-intensive endeavour, involving dozens of people with expertise over a wide variety of fields and subject areas, and has, to this point consumed hundreds of person-hours of work. This may not be sustainable in the long term, and yet it is important to ensure that relevance and coverage is maintained and that the taxonomy remains usable and useful to a broad range of users.\nIn addition, whilst the taxonomy has undergone multiple rounds of internal testing (see Methodology), it has not yet been tested with people outside the internal working group, and it remains to be seen how usable it is for people less familiar with its development.\nOne of the objectives of the development methodology for the taxonomy has been clarity. The elements of the taxonomy including definitions has been provided to the annotators. A visual hierarchical diagram of the taxonomy is always visible to annotators completing the task of annotation increasing clarity of the overall"}, {"title": "", "content": "structure. Post-annotation, the resulting differences have been explained through visual rendering of choices made by each individual annotator. This visual element of pre-annotation structure and post-annotation choice graphs has resulted in greater clarity of the taxonomy and its use, but this can always be improved and made more relevant to different cultures and countries.\nAnother objective of the taxonomy is flexibility. As the AI and algorithmic applications included in the database increase in scope and variety, the required classification system for harms i.e. the taxonomy must necessarily evolve. In some cases, there will be new categories of harm that are not covered adequately by the current taxonomy. In other cases the definitions and grouping of categories will need to be modified to capture emerging and growing areas of harm. The taxonomy therefore needs to be flexible enough to cover both the breadth and depth of an evolving set of harms from new technology. Whereas stability of a taxonomy is a desirable feature for some taxonomies, e,g, the base taxonomies for naming flora and fauna, in the area of classifying harm related to new and emerging technology flexibility is a required feature.\nBenefits: The development of this taxonomy is human centered and volunteer-based. The use of taxonomies to classify content, and in particular annotation has a dark side. There are examples of large technology companies exploiting gig workers for the completion of annotation tasks, including the classification of harmful content [35]. The taxonomy developed here, and to be used for classifying and annotating events, is human centered in the sense not just that it is intended to be understood and used by end users as well as experts, but also in the sense that it is based on volunteer input as opposed to an exploitative employment contract.\nDriven by news, legal, research and other negative reports of AI and algorithmic events, the taxonomy has an outside-in focus. By contrast, an inside-out taxonomy would develop a safety/vulnerability-focused taxonomy and then apply it from the first reported event onwards. The shape of the outside-in taxonomy described here enabled the collection of a critical mass of externally reported events first, and then developed an internal taxonomy based on that database of events. The taxonomy will be modified based on significant changes to the underlying database of events.\nThe taxonomy is developed by a multidisciplinary group of volunteers, including journalists, law, sociology, user experience, computer science and other experts from a number of different countries and levels of expertise and seniority.\nThe development of this taxonomy has been independent of financial support form companies or governments. As described by Abdalla and Abdalla [1], projects funded by big tech can be significantly oriented towards positive evaluations of the technology projects being researched, and minimisation of risks of the technology. Goldenfein and Mann [12] indicates that a similar conflict of interest issue potentially exists among digital rights civil society organisations, which are disproportionately funded by big tech. The taxonomy development process presented here has not received any finding from big technology companies, and is therefore relatively independent of these potential conflict of interest influences.\nThe proposed taxonomy has also benefited from its data-driven design, which incorporates information from a database comprising 1,500+ entries driven by and relating to AI, algorithmic and automation incidents and issues across the world. The diversity in the dataset substantially enhances the comprehensiveness and exhaustiveness of the categories and sub-categories delineated.\nFuture work: The proposed taxonomy will be strengthened to uncover potential deficiencies or oversights, enhance openness, transparency and accessibility, and account for emerging technologies and harms. Proposed milestones for the working group include:\nThe current taxonomy is mostly descriptive and focused on harms; it could be expanded to categorise associated causes and risks. Such an extension would have the potential to support advocacy, policy-making, risk management and governance but it would imply a more analytical and interpretative dimension. This requires further research."}, {"title": "", "content": "The taxonomy and annotation tool are intended to act as an open citizen science initiative. This will involve various stakeholders in assigning specific harms to incidents and issues documented in the AIAAIC Reposi-tory and/or similar databases. Through such an initiative, a broader set of stakeholders will be empowered to democratise important knowledge relating to AI, algorithmic and automation harms."}, {"title": "ETHICAL STATEMENT", "content": "The taxonomy presented here is designed to be an ever-evolving work in progress, and is still in a relatively early stage of development. At the same time, there are ethical considerations for the ongoing use of the taxonomy.\nOversimplification: The exercise of classifying harms is in itself an exercise in simplification. Although this exercise of classification has many benefits as outlined earlier, there is a risk of oversimplification in the form of a lack of breadth in the taxonomy. For example the number of categories and structure of the taxonomy can oversimplify a complex phenomenon of technology-based, real-world harms. Using an oversimplified taxonomy can result in misunderstandings about the nature and severity of harms involved, resulting in inadequate responses or interventions.\nNormalisation: Categorising an event as having a particular type of harm can minimise or reduce its importance. Harms are often specific to a group or an individual, and can have highly negative consequences for those particular groups or individuals that is not experienced by broader groups. Normalisation is a process of categorising an event in accordance with comparisons of harm from other events. The depth of the taxonomy determines the relative classification and categorisation of harms. Using a taxonomy that is not appropriately deep, is inappropriately scaled, and can result in mis-classification, resulting in inadequate scale of responses or interventions.\nIneffective Risk Mitigation: Taxonomies are political projects and taxonomies can be used in ways that do not effect actual reduction in the risks and harms identified through classification. There is a risk of ineffectiveness if the time and effort invested in developing and applying a taxonomy does not result in actual reduction in risks and harms. For example the number of categories and structure of the taxonomy can be mismatched to the way in which the technology systems are designed, and therefore taxonomy can be of limited use to developers and designers. Similarly, the taxonomy can be mismatched to the process by which civil society organisations advocate for the reduction of harms for the groups they represent. Using an ineffective taxonomy can result in a classification of events, but no consequent response, intervention or preventive process implementation."}]}