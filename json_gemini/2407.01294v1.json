{"title": "A Collaborative, Human-Centred Taxonomy of AI, Algorithmic, and Automation Harms", "authors": ["GAVIN ABERCROMBIE", "DJALEL BENBOUZID", "PAOLO GIUDICI", "DELARAM GOLPAYEGANI", "JULIO HERNANDEZ", "PIERRE NORO", "HARSHVARDHAN PANDIT", "EVA PARASCHOU", "CHARLIE POWNALL", "JYOTI PRAJAPATI", "MARK A. SAYRE", "USHNISH SENGUPTA", "ARTHIT SURIYAWONGFUL", "RUBY THELOT", "SOFIA VEI", "LAURA WALTERSDORFER"], "abstract": "This paper introduces a collaborative, human-centered taxonomy of AI, algorithmic and automation harms. We argue that existing taxonomies, while valuable, can be narrow, unclear, typically cater to practitioners and government, and often overlook the needs of the wider public. Drawing on existing taxonomies and a large repository of documented incidents, we propose a taxonomy that is clear and understandable to a broad set of audiences, as well as being flexible, extensible, and interoperable. Through iterative refinement with topic experts and crowdsourced annotation testing, we propose a taxonomy that can serve as a powerful tool for civil society organisations, educators, policymakers, product teams and the general public. By fostering a greater understanding of the real-world harms of AI and related technologies, we aim to increase understanding, empower NGOs and individuals to identify and report violations, inform policy discussions, and encourage responsible technology development and deployment.", "sections": [{"title": "1 INTRODUCTION", "content": "Much has been said about the benefits, opportunities and risks of artificial intelligence (AI), algorithms and automation, notably by industry, government and advisors. Conversely, researchers and others have spent significantly less time exploring and assessing the near-term harms of these technologies, even though these are real, can be highly damaging, and affect individuals, groups, communities, organisations, societies and environmental ecosystems [16].\nOn the surface, this seems surprising. The harms of these systems are wide-ranging, and significant. They have been known to result in the erosion and loss of fundamental liberties, such as through wrongful arrests enabled by flawed facial recognition technology\u00b9, and infringe on the well-being and livelihoods of artists and other creators, whose works are used without acknowledgment or consent to train large language models\u00b2.\nNonetheless, the identification and evaluation of negative impacts can be challenging. The workings of some technological systems, notably deep neural networks, remain opaque, partly due to inadequate explainability. More broadly, in the absence of legislation and reporting standards, system developers and deployers have little incentive to publicly acknowledge or report upstream or downstream issues, incidents or harms lest they increase their strategic, reputational, operational, financial or legal exposure.\nHarms can also be hard to grasp and quantify; differentiating between the primary, secondary and tertiary impacts of particular technologies or applications, or their direct and indirect, and tangible and intangible impacts, is trickier than first meets the eye [18]. In addition, the harms of new technologies such as generative AI and emotion recognition are unclear and will evolve as new uses emerge, bad actors find novel ways to misuse them, and societal attitudes and behaviours evolve. Further complicating this picture is the often unregulated marketing of poorly designed products [26] that achieve mainstream adoption with low barriers to entry.\nThis information vacuum means most consumers and citizens, and many others, while aware of AI and related technologies, remain in the dark about the actual damage caused by their use and misuse-a lack of knowledge and understanding that has likely resulted in widespread and growing public scepticism about the use of these technologies in daily life [32].\nHarms taxonomies can provide a solid foundation for informing policy, educating citizens, capturing and tracking violations, and managing product risks. A number of taxonomies of AI-related harms exist, but these tend to reflect the goals and biases of their creators, suffer from inconsistent nomenclature and definitions, mean different things to different people, and are incomplete and not maintained.\nThe UN AI Advisory Board has described developing \u201ca comprehensive list of AI risks for all time' as a 'fool's errand\" [33]. It may be challenging, but the development of a clear, horizontal harms taxonomy is a worthwhile, indeed necessary, endeavour. It should help solidify the harms-and risks- and thereby:\n\u2022 improve general public and other stakeholders' literacy;\n\u2022 empower citizens and NGOs to report violations;\n\u2022 strengthen the armoury of journalists, investigators and auditors [25];\n\u2022 put more power in the hands of civil society organisations;\n\u26ab enable more effective risk management, and\n\u2022 foster a greater sense of ethics and responsibility amongst developers, deployers, and others.\nPrior works to develop taxonomies of AI harms [24, 29] are primarily designed to enable policymakers to inform legislative proposals, and develop risk assessment and mitigation strategies. These works may be beneficial and contribute to a better understanding and systematic analysis of harms, but they are often narrow, unclear in terms of categorisation, naming and definitions, and become quickly out-of-date.\nConsequently, the great majority of consumers and citizens, as well as many students, journalists, policymakers, and others remain woefully uninformed about the real dangers and actual harms of AI and related technologies. Little surprise that the general public often fails to report harms when they occur, and is generally excluded from the risks and harms identification, classification and reporting process, and from the governance debate that should rightfully follow [30]."}, {"title": "3 METHODOLOGY", "content": "3.1 Working Group Setup And Core Aims\nThe proposed harms taxonomy was developed using an open, collaborative and structured process. Its design was informed by more than 1,500 incidents and issues documented in the AIAAIC database. These real-world cases are based on an estimated 10,000+ media, research, legal and other reports from all around the globe, making the\nAIAAIC one of the largest public records of harms driven by AI and algorithmic systems, but one which was becoming difficult to manage due to limitations of its current taxonomy\u00b9\u00b2 and the emergence of new risks and harms due to generative AI, emotion recognition and other technologies and applications.\nThis taxonomy is the result of the collective effort of an independent working group. Since June 2023, 25+ volunteers from various backgrounds, nationalities, genders and levels of seniority, replied to an open call for participation, all of whom taking part in a personal capacity to develop a general purpose, outside-in, open taxonomy of near-term risks and harms of AI, algorithms, and automation13.\nUnlike many of the taxonomies previously mentioned, the size, variety and openness of the working group ensured a maximal level of transparency, independence and diversity, both in the elaboration of the methodology and the taxonomy. The working group gathered at least once every two weeks, took decisions based on rough consensus\u00b94, and settled on a set of core objectives for the taxonomy, notably:\n\u2022 Human-centered, based on actual incidents and their impact on different existing stakeholders\n\u2022 Clear and understandable, both to domain experts, policy-makers and the general public\n\u2022 Practical and accessible to civil society organisations, including news publishers, policy-makers, activists, businesses and the general public\n\u2022 Comprehensive and applicable to harms stemming from the use of a wide variety of AI, algorithmic or algorithmic technologies and applications\n\u2022 Inclusive of various differing social, geographical, and cultural contexts\n\u2022 Flexible and extensible, to be easily updated to account for technological developments, adoption, and the emergence of new use cases\n\u2022 Machine-readable to facilitate further development and encourage adoption, including for compliance purposes 15.\nThese core aims are also reflected in the working methods of the working group.\n3.2 Elaboration, Testing And Improvement Process\nThe harms taxonomy was developed and refined using a structured, iterative methodology that combines a variety of approaches.\n3.2.1 Use case analysis: The working group first mapped the needs and set the initial goals of the taxonomy through an audit of AIAAIC Repository users. Approximately 50 users, comprising working group members and others identified as having taken part in the AIAAIC's 2023 user survey were surveyed by email on their use of the Repository, what they saw as its limitations and opportunities, and their requirements. Participants were individual users participating in a personal capacity from organisations including Commonwealth Scientific and Industrial Research Organisation (CSIRO), Mozilla, University College London, and the University of Science and Technology of China.\n3.2.2 Literature review: Relevant existing harms taxonomies were assessed to identify similarities, gaps, strengths and limitations. These previous works are mostly mentioned above and an overview of their respective foci, target audiences and features are outlined in Table 1.\nThis initial mapping, along with a review of current literature exploring the risks and harms of AI and related technologies [3, 7, 17, 31], informed the elaboration of a first version of the taxonomy, deliberately focused on near-term risks and documented harms of AI technologies. Key human rights and civil liberties charters, including\n3.2.3 Expert outreach: This initial \u201cwork-in-progress\" version of the taxonomy was presented to experts at NGOs, universities, news publishers and other organisations across the world to collect feedback on its thoroughness and practicality, especially on topics including Economics, Human rights and civil liberties, Misinformation and disinformation, Law, Politics, and Sustainability. The experts who provided feedback are acknowledged and thanked at the end of this paper.\n3.2.4 Annotation testing: By selecting and annotating entries on the AIAAIC Repository, either randomly or choosing cases evidently different, and annotating them, the working group tested and refined the initial taxonomy to ensure it was clear, easy to use, and extensible. Individual annotations were then compared to identify cases with diverging classifications, collect feedback on the definitions and adapt incrementally the taxonomy, week after week, to maximise consensus and efficiency.\nTo facilitate this process, a dedicated open-source tool was developed by the working group. This user-friendly interface enables quick annotation through drop-down menus with reminders of the definitions, streamlined comment submission, faster human review and consensus assessment. This tool also generates, for each annotated incident, a Sankey diagram, visually depicting the annotators' answers for each incident and computes its\n3.2.5 Validation through broader community review. This methodology of continuous incremental improvement and adaptation through multiple, parallel approaches is set to continue as an ongoing process. The methodology and tools set up by the working group are meant to scale and support a roadmap towards a more open and inclusive contribution process, involving a larger community of contributors, both for general or specialised reviews of the taxonomy itself, the annotation of historic cases in existing databases, or the \"live\" classification of new incidents upon reporting them or documenting them in harms and risks databases."}, {"title": "4 OVERVIEW OF THE HARMS TAXONOMY", "content": "The result of the research, annotation and external reviews outlined in the previous section is presented here. Focusing on harms, as distinguished from causes or risks, supports the project's aim to develop a taxonomy that is understandable and usable by a wide range of stakeholders. It also ensures that the taxonomy is not dependent on technology or context of use and can be applied to a wide range of technologies and applications.\nIt is important to note that the taxonomy presented here is not intended to be static but is dynamic and will evolve through continued annotation efforts, external reviews, and analysis. We therefore share this first version of the taxonomy to communicate our findings to a wider audience and start the process of soliciting additional feedback for further improvements.\nFor the purposes of this taxonomy, a harm is defined as a \"physical, pyschological or other form of damage to third-parties\" that results from the use of an AI, algorithmic or automation system. The taxonomy organises harms at two different levels of granularity. The first level is the \"harm type\u201d, which describes the general category\nof harm. The second level divides harm types into sub-categories, referred to as \u201cspecific harms\u201d. Definitions are provided for each harm category and specific harm.\nThe taxonomy outlines nine harm types:\n\u2022 Autonomy - Loss of or restrictions to the ability or rights of an individual, group or entity to make decisions and control their identity and/or output.\n\u2022 Physical - Physical injury to an individual or group, or damage to physical property.\n\u2022 Psychological - Direct or indirect impairment of the emotional and psychological mental health of an individual, organisation, or society.\n\u2022 Reputational - Damage to the reputation of an individual, group or organisation.\n\u2022 Financial and Business - Use or misuse of a technology system in a manner that damages the financial interests of an individual or group, or which causes strategic, operational, legal or financial harm to a business or other organisation.\n\u2022 Human Rights and Civil Liberties - Use or misuse of a technology system in a manner that compromises fundamental human rights and freedoms.\n\u2022 Societal and Cultural - Harms affecting the functioning of societies, communities and economies caused directly or indirectly by the use or misuse technology systems.\n\u2022 Political and Economic - Manipulation of political beliefs, damage to political institutions and the effective delivery of government services.\n\u2022 Environmental - Damage to the environment directly or indirectly caused by a technology system or set of systems.\nA diagram of specific harms, organised by harm type, is shown in Figure 1. The definitions for specific harms can be found in Appendix A.\nThough the harm types presented here are intended to be reasonably comprehensive, we recognise that new harm types will almost certainly have to be added in the future. Nonetheless, we expect that future additions to the taxonomy are more likely to occur at the specific harms level rather than the harm types level.\nThe specific harms are not intended to be mutually exclusive. Similar specific harms may exist within two or more harm types as necessary. For example, personal health deterioration is a specific harm within the Physical harm type, while damage to public health is a specific harm within the Societal and Cultural harm type. Where a similar harm exists in multiple harm types, they are intended to represent distinct concepts (e.g., personal health deterioration is focused on the harm to individual health while damage to public health is focused on harm to\ngroups or communities). However, specific harms may be related; when personal health deterioration affects a large number of individuals, damage to public health is also likely to occur.\nAdditionally, we do not envision restricting the relationship between a particular incident and the harm to be one-to-one. A single incident may result in multiple harms, including multiple different specific harms within the same harm category. Within the custom tool used for annotation testing, annotators are allowed to list as many harms as they have identified for a particular incident.\nFinally, the taxonomy envisions that annotators will characterise each identified harm as either \u201cactual\u201d or \u201cpotential\u201d with respect to the particular incident being annotated. A harm is considered actual when the articles, reports and other public sources of information about the incident explicitly state that the harm has occurred. A harm is potential when the articles, reports and other sources of information about the incident indicate or suggest that a harm is reasonably likely to occur.\nTo facilitate a comparative understanding of our proposed taxonomy, we have selected other prominent taxonomies based on their scope, purpose, and intended audience, ensuring a relevant and meaningful comparison. The comparison, outlined in Table 2, features frameworks developed by recognised institutions such as the OECD, EPIC, CSET, the Alan Turing Institute, Microsoft Azure, Sony, and SHAS [27]. These taxonomies were chosen because they similarly address a broad range of AI-related harms and are designed for diverse uses including policy guidance, risk management, and public education, and are intended for a wide audience encompassing policymakers, industry stakeholders, and the general public. Each column in the table represents a different taxonomy and indicates whether specific types of harm are addressed within their frameworks, highlighting how the AI harms taxonomy aligns with or differs from established approaches in the field. Among the taxonomies we compared, those developed by the OECD, CSET, and SHAS are most similar to ours due to their comprehensive scopes and focus on both policy-oriented applications and broad societal impacts. These frameworks emphasise a wide coverage of harms, both tangible and intangible, aligning closely with our objective to inform risk management and policy guidance comprehensively. The SHAS taxonomy, in particular, shares our emphasis on sociotechnical aspects, making it highly relevant for interdisciplinary approaches to understanding and mitigating AI-related harms."}, {"title": "5 BENEFITS AND INTENDED USES", "content": "The taxonomy proposed in this paper is intended to make the harms of AI, algorithmic and automation systems understandable and usable to a wide range of audiences, from experts to the general public.\nWe believe the taxonomy can be used for a wide variety of purposes.\nLiteracy and education: Research studies regularly show a significant gap between awareness and understanding of AI systems-a gap that is reportedly widening, leading to growing concerns about the use of the technology in daily life in the US,17 UK,18 and elsewhere.\nDespite these concerns, and the opaque and high-risk nature of some algorithmic systems, governments, educators and others recognise that these technologies play an increasingly central role in the everyday lives of citizens and consumers, government, civil society and business. As such, it is important that they are demystified and made understandable to everyone.\nThis realisation is evident in the use of the AIAAIC Repository by teachers, academics, professional associations, businesses and other entities to educate decision-makers, students, employees, suppliers and others on the nature, risks and harms of AI, algorithmic and automation systems [6].\nThe harms taxonomy should help make the full range of AI and algorithmic harms of clearer and easier to understand by a broader set of audiences.\nJournalism: AI and algorithmic decision-making for public or commercial purposes are of significant interest to journalists and other civil opinion-formers. However, it can be difficult to untangle actual impacts from official and unofficial sources, each of which may be partial and skewed.\nBy providing evidence-based, impartial data on the use and misuse of AI, algorithmic and automation systems, incident databases incorporating the ability to sort a wide range of actual and potential harms in an easy to understand and use manner can be of real value to technology, business, health and other journalistic beats, as well as to investigative journalists.\nStructured harms data can be used to inform journalistic enquiries and make the case to editors, in addition to supporting and strengthening news, commentary and analysis, and to create visual storytelling or 'data journalism'.\nAdvocacy: Non-governmental civil society organisations (NGOs) making the case for human rights and civil liberties, consumers, patients and other constituencies often expose abuses and violations in order to put pressure on perceived culprits-typically system developers and/or operators, but also malicious bad actors-and to make the case for stronger or better enforced legislation.\nSometimes, an NGO's findings and/or third-party accusations are recorded as a public resource-an example being the Business & Human Rights Resource Centre's news database.19\nHowever, the majority of civil society organisations have few resources and struggle to record their findings in a structured manner, or maintain databases. The harms taxonomy can help NGOs identify common harms across different systems, countries and cultures, strengthen existing databases, and create new systems to track harms and related information.\nCitizen reporting: The ability for citizens and other users to hold the developers and deployers of AI and algorithmic technology systems are currently hampered by complaint reporting systems that are often challenging to find, cumbersome to use, and which can deliver slow and partial responses.\nFurthermore, it is often not in the interests of these entities to disclose incidents reported by citizens and other users publicly, lest they be exposed to greater legal, financial, operational, and reputational risk.\nThe AIAAIC Repository and the AI Incident Database (AIID) 20 are considered to be the only two tools that enable individuals and organisations to report incidents based on media reports and, in the case of AIAAIC, legal dockets, research reports and other materials. However, the two entities use different taxonomies and definitions for defining harms, resulting in inconsistent outputs21.\nImpending legislation, notably in Brazil, Canada and the European Union22, provides citizens with the right to submit complaints about high-risk AI systems and to receive explanations about how these systems work.\nGovernments adopting these proposals will need to develop reporting systems that clearly spell out the harms associated with these systems so that citizens, consumers, patients, etc, can report incidents and issues in a clear, user-friendly and consistent manner.\nThe harms taxonomy can help governments and government agencies develop user-friendly, effective reporting systems that align user needs and expectations with their own obligations and requirements."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "Limitations: The taxonomy is designed to be an ever-evolving work, and is still in a relatively early stage of development. Accordingly, it currently has a number of limitations that will be worked on and developed.\nThe creation of the taxonomy has been a labour-intensive endeavour, involving dozens of people with expertise over a wide variety of fields and subject areas, and has, to this point consumed hundreds of person-hours of work. This may not be sustainable in the long term, and yet it is important to ensure that relevance and coverage is maintained and that the taxonomy remains usable and useful to a broad range of users.\nIn addition, whilst the taxonomy has undergone multiple rounds of internal testing (see Methodology), it has not yet been tested with people outside the internal working group, and it remains to be seen how usable it is for people less familiar with its development.\nOne of the objectives of the development methodology for the taxonomy has been clarity. The elements of the taxonomy including definitions has been provided to the annotators. A visual hierarchical diagram of the taxonomy is always visible to annotators completing the task of annotation increasing clarity of the overall\nstructure. Post-annotation, the resulting differences have been explained through visual rendering of choices made by each individual annotator. This visual element of pre-annotation structure and post-annotation choice graphs has resulted in greater clarity of the taxonomy and its use, but this can always be improved and made more relevant to different cultures and countries.\nAnother objective of the taxonomy is flexibility. As the AI and algorithmic applications included in the database increase in scope and variety, the required classification system for harms i.e. the taxonomy must necessarily evolve. In some cases, there will be new categories of harm that are not covered adequately by the current taxonomy. In other cases the definitions and grouping of categories will need to be modified to capture emerging and growing areas of harm. The taxonomy therefore needs to be flexible enough to cover both the breadth and depth of an evolving set of harms from new technology. Whereas stability of a taxonomy is a desirable feature for some taxonomies, e,g, the base taxonomies for naming flora and fauna, in the area of classifying harm related to new and emerging technology flexibility is a required feature.\nBenefits: The development of this taxonomy is human centered and volunteer-based. The use of taxonomies to classify content, and in particular annotation has a dark side. There are examples of large technology companies exploiting gig workers for the completion of annotation tasks, including the classification of harmful content [35]. The taxonomy developed here, and to be used for classifying and annotating events, is human centered in the sense not just that it is intended to be understood and used by end users as well as experts, but also in the sense that it is based on volunteer input as opposed to an exploitative employment contract.\nDriven by news, legal, research and other negative reports of AI and algorithmic events, the taxonomy has an outside-in focus. By contrast, an inside-out taxonomy would develop a safety/vulnerability-focused taxonomy and then apply it from the first reported event onwards. The shape of the outside-in taxonomy described here enabled the collection of a critical mass of externally reported events first, and then developed an internal taxonomy based on that database of events. The taxonomy will be modified based on significant changes to the underlying database of events.\nThe taxonomy is developed by a multidisciplinary group of volunteers, including journalists, law, sociology, user experience, computer science and other experts from a number of different countries and levels of expertise and seniority.\nThe development of this taxonomy has been independent of financial support form companies or governments. As described by Abdalla and Abdalla [1], projects funded by big tech can be significantly oriented towards positive evaluations of the technology projects being researched, and minimisation of risks of the technology. Goldenfein and Mann [12] indicates that a similar conflict of interest issue potentially exists among digital rights civil society organisations, which are disproportionately funded by big tech. The taxonomy development process presented here has not received any finding from big technology companies, and is therefore relatively independent of these potential conflict of interest influences.\nThe proposed taxonomy has also benefited from its data-driven design, which incorporates information from a database comprising 1,500+ entries driven by and relating to AI, algorithmic and automation incidents and issues across the world. The diversity in the dataset substantially enhances the comprehensiveness and exhaustiveness of the categories and sub-categories delineated.\nFuture work: The proposed taxonomy will be strengthened to uncover potential deficiencies or oversights, enhance openness, transparency and accessibility, and account for emerging technologies and harms. Proposed milestones for the working group include:\nThe current taxonomy is mostly descriptive and focused on harms; it could be expanded to categorise associated causes and risks. Such an extension would have the potential to support advocacy, policy-making, risk management and governance but it would imply a more analytical and interpretative dimension. This requires further research."}, {"title": "7 ETHICAL STATEMENT", "content": "The taxonomy presented here is designed to be an ever-evolving work in progress, and is still in a relatively early stage of development. At the same time, there are ethical considerations for the ongoing use of the taxonomy.\nOversimplification: The exercise of classifying harms is in itself an exercise in simplification. Although this exercise of classification has many benefits as outlined earlier, there is a risk of oversimplification in the form of a lack of breadth in the taxonomy. For example the number of categories and structure of the taxonomy can oversimplify a complex phenomenon of technology-based, real-world harms. Using an oversimplified taxonomy can result in misunderstandings about the nature and severity of harms involved, resulting in inadequate responses or interventions.\nNormalisation: Categorising an event as having a particular type of harm can minimise or reduce its importance. Harms are often specific to a group or an individual, and can have highly negative consequences for those particular groups or individuals that is not experienced by broader groups. Normalisation is a process of categorising an event in accordance with comparisons of harm from other events. The depth of the taxonomy determines the relative classification and categorisation of harms. Using a taxonomy that is not appropriately deep, is inappropriately scaled, and can result in mis-classification, resulting in inadequate scale of responses or interventions.\nIneffective Risk Mitigation: Taxonomies are political projects and taxonomies can be used in ways that do not effect actual reduction in the risks and harms identified through classification. There is a risk of ineffectiveness if the time and effort invested in developing and applying a taxonomy does not result in actual reduction in risks and harms. For example the number of categories and structure of the taxonomy can be mismatched to the way in which the technology systems are designed, and therefore taxonomy can be of limited use to developers and designers. Similarly, the taxonomy can be mismatched to the process by which civil society organisations advocate for the reduction of harms for the groups they represent. Using an ineffective taxonomy can result in a classification of events, but no consequent response, intervention or preventive process implementation."}, {"title": "A.1 Autonomy", "content": "Autonomy/agency loss - Loss of an individual, group or organisation's ability to make informed decisions\nor pursue goals.\nImpersonation/identity theft - Theft of an individual, group or organisation's identity by a third-party\nin order to defraud, mock or otherwise harm them.\nIP/copyright loss - Misuse or abuse of an individual or organisation's intellectual property, including\ncopyright, trademarks, and patents.\nPersonality rights loss - Loss of or restrictions to the rights of an individual to control the commercial\nuse of their identity, such as name, image, likeness, or other unequivocal identifiers."}, {"title": "A.2 Physical", "content": "Bodily injury - Physical pain, injury, illness, or disease suffered by an individual or group due to the\nmalfunction, use or misuse of a technology system.\nLoss of life - Accidental or deliberate loss of life, including suicide, extinction or cessation, due to the use\nor misuse of a technology system.\nPersonal health deterioration - Physical deterioration of an individual or animal over time, increasing\ntheir risk of disease, organ failure, prolonged hospital stay or death, etc.\nProperty damage - Action(s) that lead directly or indirectly to the damage or destruction of tangible\nproperty eg. buildings, possessions, vehicles, robots."}, {"title": "A.3 Psychological", "content": "Addiction - Emotional or material dependence on technology or a technology system.\nAlienation/isolation - An individual's or group's feeling of lack of connection with those around as a\nresult of technology use or misuse.\nAnxiety/depression - Mental health decline due to addiction, negative social interactions such as humilia-\ntion and shaming and traumatic distressing events such as online violence or rape.\nCoercion/manipulation - Use of a technology system to covertly alter user beliefs and behaviour using\nnudging, dark patterns and/or other opaque techniques, resulting in potential erosion of privacy, addiction,\nanxiety/distress, etc.\nDehumanisation/objectification - Use or misuse of a technology system to depict and/or treat people as\nnot human, less than human, or as objects.\nHarassment/abuse/intimidation - Online behaviour, including sexual harassment, that makes an individ-\nual or group feel alarmed or threatened.\nOver-reliance - Unfettered and/or obsessive belief in the accuracy or other quality of a technology system,\nresulting in addiction, anxiety, introversion, sentience, complacency, lack of critical thinking and other\nactual or potential negative impacts.\nRadicalisation - Adoption of extreme political, social, or religious ideals and aspirations due to the nature\nor misuse of an algorithmic system, potentially resulting in abuse, violence, or terrorism.\nSelf-harm - Intentional seeking out or sharing of hurtful content about oneself that leads to, supports, or\nexacerbates low self-esteem and self-harm.\nSexualisation - Sexual interest in a technology or application.\nTrauma - Severe and lasting emotional shock and pain caused by an extremely upsetting experience."}, {"title": "A.4 Reputational", "content": "Defamation/libel/slander - Use of a technology system to create, facilitate or amplify false perception(s)\nabout an individual, group, or organisation.\nLoss of confidence/trust - Misleading or unfair change(s) in how an individual, group, or organisation is\nviewed, leading to loss of ability to conduct relationships, raise capital, recruit people, etc."}, {"title": "A.5 Financial and Business", "content": "Business operations/infrastructure damage - Damage, disruption, or destruction of a business system\nand/or its components due to malfunction, cyberattacks, etc.\nConfidentiality loss - Unauthorised sharing of sensitive, confidential information and documents such as\ncorporate strategy and financial plans with third-parties.\nFinancial/earnings loss - Loss of money, income or value due to the use or misuse of a technology system.\nLivelihood loss - An individual or group's loss of ability to support themselves financially or vocationally\ndue to natural disasters, lack of demand for products/services, cost increases, etc, resulting in inability to\nprocure food, reduced employment prospects, bankruptcy, foreclosure, homelessness, etc.\nIncreased competition - The inappropriate or unethical use of technology to gain market share.\nMonopolisation - Abuse of market power through the control of prices, thereby limiting competition and\ncreating unfair barriers to entry.\nOpportunity loss - Loss of ability to take advantage of a financial or other opportunity, such as education,\nemployability/securing a job."}, {"title": "A.6 Human Rights and Civil Liberties", "content": "Benefits/entitlements loss - Denial or or loss of access to welfare benefits, pensions, housing, etc due to\nthe malfunction, use or abuse of a technology system.\nDignity loss - Perceived loss of value experienced by or disrespect shown to an individual or group, resulting\nin self-sheltering, loss of connections and relationships, and public stigmatisation.\nDiscrimination - Unfair or inadequate treatment or arbitrary distinction based on a person's race, ethnicity,\nage, gender, sexual preference, religion, national origin, marital status, disability, language, or other\nprotected groups.\nLoss of freedom of speech/expression - Restrictions to or loss of people's right to articulate their opin-\nions and ideas without fear of retaliation, censorship, or legal sanction.\nLoss of freedom of assembly/association - Restrictions to or loss of people's right to come together\nand collectively express, promote, pursue, and defend their collective or shared ideas, and/or to join an\nassociation.\nLoss of social rights and access to public services - Restrictions to or loss of rights to work, social secu-\nrity, and adequate standard of living, housing, health and education.\nLoss of right to information - Restrictions to or loss of people's right to seek, receive and impart infor-\nmation held by public bodies.\nLoss of right to free elections - Restrictions to or loss of people's right to participate in free elections at\nreasonable intervals by secret ballot.\nLoss of right to liberty and security - Restrictions to or loss of liberty as a result of illegal or arbitrary\narrest or false imprisonment.\nLoss of right to due process - Restrictions to or loss of right to be treated fairly, efficiently and effectively\nby the administration of justice."}, {"title": "A.7 Societal and Cultural", "content": "Breach of ethics/values/norms - An actual or perceived violation or deviation from the established societal\nvalues, norms or ethical standards or principles.\nCheating/plagiarism - Use of another person's or group's words or ideas without consent and/or acknowl-\nedgement.\nChilling effect - The creation of a climate of self-censorship that deters democratic actors such as journalists,\nadvocates and judges from speaking out.\nCultural dispossession - Intentional and/or unintentional erasure of cultural goods and values, such as\nways of speaking, expressing humour, or sounds and voices that contribute to a cultural identity, or their\ninappropriate re-use in other cultures.\nDamage to public health - Adverse impacts on the health of groups, communities or societies, including\nmalnutrition, disease and infection conditions.\nHistorical revisionism - Deliberate or unintentional reinterpretation of established/orthodox historical\nevents or accounts held by societies, communities, academics.\nInformation degradation - Creation or spread of false, hallucinatory, low-quality, misleading, or inaccurate\ninformation that degrades the information ecosystem and causes people to develop false or inaccurate\nperceptions, decisions and beliefs; or to lose trust in accurate information.\nJob loss/losses - Replacement/displacement of human jobs by a technology system, leading to increased\nunemployment, inequality, reduced consumer spending, and social friction.\nLabour exploitation - Use of under-paid and/or offshore labour to develop, manage or optimise a technol-\nogy system.\nLoss of creativity/critical thinking - Devaluation and/or deterioration of human creativity, artistic ex-\npression, imagination, critical thinking or problem-solving skills.\nStereotyping - Derogatory or otherwise harmful stereotyping or homogenisation of individuals, groups,\nsocieties or cultures due to the mis-representation, over-representation, under-representation, or non-\nrepresentation of specific identities, groups, or perspectives.\nPublic service delivery deterioration - Poor performance of a public technology system due to malfunc-\ntion, over-use, under-staffing etc, resulting in individuals, groups, or organisations unable to use it in a\nmanner they can reasonably expect.\nSocietal destabilisation - Societal instability in the form of strikes, demonstrations and other types of civil\nunrest caused by loss of jobs to technology, unfair algorithmic outcomes, disinformation, etc.\nSocietal inequality - Increased difference in social status or wealth between individuals or groups caused\nor amplified by a technology system, leading to the loss of social and community wellbeing/cohesion and\ndestabilisation.\nViolence/armed conflict - Use or misuse of a technology system to incite, facilitate or conduct cyber\nattacks, security breaches, lethal, biological and chemical weapons development, resulting in violence\nand armed conflict."}, {"title": "A.8 Political and Economic", "content": "Critical infrastructure damage - Damage, disruption to or destruction of systems essential to the func-\ntioning and safety of a nation or state, including energy, transport, health, finance, and communication\nsystems."}, {"title": "A.9 Environmental", "content": "Biodiversity loss - Over-expansion of technology infrastructure, or inadequate alignment of technology\nwith sustainable practices, leading to deforestation, habitat destruction, and fragmentation and loss of\nbiodiversity.\nCarbon emissions - Release of carbon dioxide, nitric oxide and other gases, increasing carbon emissions,\nexacerbating climate change, and negatively impacting local communities.\nElectronic waste - Electrical or electronic equipment that is waste, including all components, sub-assemblies\nand consumables that are part of the equipment at the time the equipment becomes waste\nExcessive energy"}]}