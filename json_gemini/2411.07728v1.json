{"title": "No-Reference Point Cloud Quality Assessment via Graph Convolutional Network", "authors": ["Wu Chen", "Qiuping Jiang", "Wei Zhou", "Feng Shao", "Guangtao Zhai", "Weisi Lin"], "abstract": "Three-dimensional (3D) point cloud, as an emerging visual media format, is increasingly favored by consumers as it can provide more realistic visual information than two-dimensional (2D) data. Similar to 2D plane images and videos, point clouds inevitably suffer from quality degradation and information loss through multimedia communication systems. Therefore, automatic point cloud quality assessment (PCQA) is of critical importance. In this work, we propose a novel no-reference PCQA method by using a graph convolutional network (GCN) to characterize the mutual dependencies of multi-view 2D projected image contents. The proposed GCN-based PCQA (GC-PCQA) method contains three modules, i.e., multi-view projection, graph construction, and GCN-based quality prediction. First, multi-view projection is performed on the test point cloud to obtain a set of horizontally and vertically projected images. Then, a perception-consistent graph is constructed based on the spatial relations among different projected images. Finally, reasoning on the constructed graph is performed by GCN to characterize the mutual dependencies and interactions between different projected images, and aggregate feature information of multi-view projected images for final quality prediction. Experimental results on two publicly available benchmark databases show that our proposed GC-PCQA can achieve superior performance than state-of-the-art quality assessment metrics. The code will be available at: https://github.com/chenwuwq/GC-PCQA.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the development of three-dimensional (3D) visual information acquisition technology makes point clouds easier to obtain and gradually becomes a popular type of visual data. A 3D Point cloud is mainly used to describe a complete 3D scene or object, including geometric attributes (position of each point in 3D space), color attributes (RGB attributes of each point), and others (normal vector, opacity,\nreflectivity, time, etc.) [1]. Point clouds have been widely studied and used in a wide range of application scenarios such as 3D reconstruction [2], [3], classification and segmenta- tion [4], [5], facial expression representation [6], autonomous driving [7], [8], and virtual reality [9], etc. Although point cloud can realistically record 3D objects through a large set of points, it also consumes a lot of memory, and it is difficult to achieve data transmission under limited network bandwidth [10], [11]. This new and effective data representation presents a challenge to the current hardware storage and network transmission. Therefore, in order to achieve efficient storage and transmission, compression of point clouds is necessary [12]\u2013[15]. However, point cloud compression may introduce artifacts, resulting in the degradation of point cloud visual quality. Point cloud visual quality is an important way to compare the performance of various point cloud processing algorithms. Effective point cloud quality assessment (PCQA) methods can not only help people evaluate the distortion degree of point clouds and the performance of compression algorithms but also be beneficial to optimize the visual quality of distorted point clouds. Thus, how to accurately assess the perceptual quality of point clouds has become a critical issue.\nSimilar to image quality assessment (IQA), PCQA can also be divided into subjective and objective methods. The subjective method is mainly based on the perception of the human visual system (HVS). It is difficult to be widely applied because this kind of assessment requires a large number of participants to ensure the rationality and accuracy of the assessment results in a statistical sense. Currently, the results obtained from subjective assessment experiments are generally served as the ground-truth data for benchmarking different objective methods [16]. According to the participation of original point clouds, objective PCQA methods can have three categories: full reference (FR), reduced reference (RR), and no reference (NR). Since the original point clouds are not always available, NR-PCQA methods that do not rely on any original information as a reference are more suitable in practical applications.\nThe traditional NR-PCQA methods [17], [18] generally predict the quality score by extracting quality-aware fea- tures based on the analysis of point cloud attributes such as geometry and color. Recently, the great success of deep learning in the field of NR-IQA has promoted the develop- ment of deep learning-based NR-PCQA metrics [19]\u2013[24]. The common practice of these deep NR-PCQA metrics is to directly apply the ordinary convolution operation on the point cloud for automatic feature learning in a data-driven\nmanner. Nonetheless, point cloud is a typical kind of non- Euclidean data which is sparsely distributed over the 3D space, and a large number of useless pixels are also involved with pixel-by-pixel convolution, thus resulting in a huge waste of resources and inefficient data processing. In order to solve this problem, some related works try to represent non-Euclidean data with the graph which includes node information and complex adjacency relations between nodes. With the graph- based non-Euclidean data as input, the current works then introduce to use graph convolutional network (GCN) rather than traditional convolutional neural network (CNN) for more effective feature representation learning [25]. For instance, Thomas et al. [26] proposed to convert non-Euclidean data into a graph based on which the GCN is used to realize graph feature extraction. As a typical kind of non-Euclidean data, GCN has also been applied to many point cloud-based vision tasks, such as point cloud classification [27], [28], point cloud segmentation [29], point cloud data analysis [30], action recognition [31], etc. Moreover, it has also been applied to infer the perceptual quality of various multimedia data, e.g., traditional 2D images [32], [33], 360-degree images [34], [35], and meshes [36], [37].\nDue to the strong capability of GCN in handling non- Euclidean data including 3D point cloud, this paper presents a novel GCN-based NR-PCQA method (GC-PCQA). One of the most critical issues is to effectively create a graph of the point cloud so that the GCN can be applied for feature learning. Since the goal of PCQA is to predict the quality of the test point cloud consistent with human perception, how to construct a highly perception-consistent graph of point clouds is the key to its success. It is known that the HVS reconstructs 3D objects in their mind based on multiple two-dimensional (2D) plane images observed from different viewpoints. In order to imitate the process of the HVS to perceive 3D objects, it is natural to perform multi-view projection on the point cloud to obtain a set of projected images with each corresponding to a specific viewpoint. Although these projected images are independent individuals, there is a certain extent of correlation between each other. Therefore, we regard all projected images as a set of non-Euclidean data and then establish a graph according to the dependencies between each individual projected image. Finally, GCN is applied to realize feature extraction from the constructed graph for quality prediction. Experimental results demonstrate that our proposed GC-PCQA method outperforms state-of-the-art reference and non-reference PCQA methods on two public PCQA databases. Overall, the main contributions of this paper are as follows:\n1) We perform multi-view projection on the point cloud to obtain a set of projected images based on which a highly perception-consistent graph is constructed to model the mutual dependencies of multi-view projected images. The graph nodes are defined with the projected images and connected by spatial relations between each other.\n2) We perform GCN on the proposed graph to characterize the interactions between different projected images and aggregate the feature information of multi-view pro- jected images for final quality prediction. The ablation study validates the effectiveness of the GCN architecture and the source code is available for public research usage.\n3) We fuse the horizontally and vertically projected image features extracted by two GCNs that do not share weights to boost the performance. Experimental results show that the proposed GC-PCQA can predict subjective scores more accurately than the existing state-of-the-art PCQA metrics.\nThe rest of this paper is organized as follows. In Section II, we introduce the related works. In Section III, we illustrate the proposed GC-PCQA with technical details. We conduct experiments and analyze the results in Section IV, and finally draw conclusions in Section V."}, {"title": "II. RELATED WORK", "content": "PCQA metrics have developed rapidly and can be mainly divided into PC-based metrics and projection-based metrics. PC-based metrics evaluate the quality score through the char- acteristic information of each point in the point cloud. While projection-based metrics use the projected images of the point cloud instead of the point cloud itself.\nA. PC-based Metrics\nAs one of the important evaluation methods, the FR method has been widely investigated in PC-based metrics. The ini- tial methods calculate quality scores based on geometric information of point clouds, such as $PSNR_{MSE,p2po}$ and $PSNR_{HF,p2po}$ [38], $PSNR_{MSE,p2pl}$ and $PSNR_{HF,p2pl}$ [39]. Among them, the point-to-point methods (p2point) com- pute the L2 norm of the nearest point pair as the distortion measure of the point, while point-to-plane methods (p2plane) increase the normal vector of the plane. Besides, Alexiou et al. [40] captured the distortion point cloud degradation through the angular similarity between the corresponding points. Javaheri et al. [41] used the generalized Hausdorff"}, {"title": "III. PROPOSED METHOD", "content": "We first give a brief overview of the proposed GC-PCQA method. Then, the details of each module in our GC-PCQA method will be illustrated. Finally, we describe how the network is trained.\nA. Overview\nThe framework of our proposed GC-PCQA method is shown in Fig. 2. It is mainly composed of three parts: multi-view projection, graph construction, and GCN-based quality prediction. Firstly, considering the behavior of the HVS when observing 3D point clouds, multi-view projection is performed on the point cloud to obtain a set of horizontally and vertically projected 2D images. The horizontally (vertically) projected 2D image set covers the visual contents that can be perceived within the horizontal (vertical) visual field by an observer. All these projected images are fed into a pre-trained back- bone and an attention block for attentive feature extraction. Secondly, a multi-level fusion of attentive feature maps is carried out through the multi-level conversion module, and graph construction is performed according to spatial relations"}, {"title": "B. Multi-view Projection", "content": "Point cloud consists of huge point sets, which is mainly used to describe 3D objects in detail, resulting in a large volume of point cloud and it is difficult to directly input point cloud data into the network. In order to reduce the cost of processing large-scale point clouds, many point cloud resampling strategies [61]\u2013[63] and point cloud projection methods [21]\u2013[24] have been proposed to simplify point cloud data. Since deep learning is very effective in the field of image processing, we choose to use the multi-view projection method to convert the point cloud into an image, so as to take advantage of deep learning for PCQA.\nThe visual field of the human eye is divided into horizontal visual field and vertical visual field. The range of view in different directions is limited, i.e., the horizontal view limit is approximately 190 degrees while the vertical view limit is 135 degrees. When human eyes observe 3D objects such as 3D point clouds, it is difficult to directly observe all contents within 360 degrees [34]. In general, human reconstructs 3D objects in the brain by observing from different viewpoints, so as to have a clearer perception of 3D objects. In order to imitate the perceptual process of the HVS, we perform a multi- view projection operation on the point cloud based on rotation stride (RS). Just as the human eye perceives 3D objects, we rotate and project the point cloud onto the 2D space in both horizontal and vertical directions. Finally, for each direction, we obtain a multi-view projected image group P containing N projected images:\n$P = [Y_1, Y_2, Y_3,\\cdots, Y_N] \\in \\mathbb{R}^{N\\times3\\times H\\times W}$,\nwhere $Y_i$ represents the i-th projected image, and N is the total number of projected images. H and W indicate the height and width of each individual projected image, respectively. We use $P_H$ to denote the horizontally projected image group and $P_V$ to denote the vertically projected image group."}, {"title": "C. Graph Construction", "content": "The graph construction module aims to construct a perception-consistent graph based on the features extracted from multi-view projected images. It is mainly composed of a pre-trained backbone network, an attention block, and a multi- level conversion module."}, {"title": "1) Feature Extraction:", "content": "We adopt the pre-trained ResNet101 [64] as the backbone for feature extraction. The input image size of the backbone network is 224x224. However, due to the presence of a large amount of quality-unrelated white regions in the projected images, there may be adverse effects or an inability to learn quality-related features. To address this, an additional image pre-processing step is performed. The projected image is cropped to effectively remove the non-informative white background regions. Subsequently, the cropped image is resized to 224\u00d7224 to create an informative image patch, which is then used as input to the network."}, {"title": "2) Attention Block:", "content": "The attention mechanism comes from the research on human vision and draws lessons from the attention thinking of the human vision, which can make the feature extractor focus more on those significant areas of the target while suppressing the most unimportant information to improve the performance of DNNs. At present, a variety of attention modules have been proposed such as SE attention [65], CBAM attention [66], scSE attention [67], etc. Therefore, in order to further improve the feature representation capability of the network, we devise an attention block to impose appropriate attention weights to the feature maps obtained by the pre-trained backbone.\nWe first introduce the upper flow, i.e., the spatial attention. The input feature maps from the pre-trained backbone can be represented as $F^{(i)} \\in \\mathbb{R}^{C\\times H\\times W}$, where i represents the output of the ith layer of the backbone. Firstly, 2D convolution with a convolution kernel size of 1 is used to reduce the number of channels to 1. Then, a sigmoid activation function is applied to map the range of feature values into [0,1]. Finally, the spatial attention map $F_s \\in \\mathbb{R}^{H\\times W}$ is expressed as follows:\n$F_s = \\delta(\\varphi(F)) = \\begin{bmatrix}\n    \\delta(S_{11}) & \\delta(S_{12}) & \\dots & \\delta(S_{1W}) \\\\\n    \\delta(S_{21}) & \\delta(S_{22}) & \\dots & \\delta(S_{2W}) \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\delta(S_{H1}) & \\delta(S_{H2}) & \\dots & \\delta(S_{HW}) \\\\\n\\end{bmatrix}$,\nwhere $\\varphi$ represents the 2D convolution with a convolution kernel size of 1, $\\delta(.)$ denotes the sigmoid function, and $S_{ij}$ (i \u2208 {1,2,...,H},j \u2208 {1,2,...,W}) indicates the relative importance of the eigenvalues at position (i, j).\nFor the lower flow, global average pooling is first applied on the input feature maps $F \\in \\mathbb{R}^{C\\times H\\times W}$. Then, the channel dimension of the feature maps is reduced and then increased by two 2D convolution layers with a convolution kernel size of 1. The final channel attention map $F_c \\in \\mathbb{R}^{C\\times1\\times1}$ is generated by attaching a sigmoid activation function $\\delta(\u00b7)$ in the end, which can be expressed as follows:\n$F_c = \\delta(\\varphi(avg(F))) = [\\delta(U_1), \\delta(U_1),\\cdots, \\delta(U_C)]$,\nwhere $U_i(i \\in \\{1,2,...,C\\})$ denotes the relative importance of the ith channel among all channels, avg denotes the global average pooling. After obtaining two attentions, the spatial attention map and the channel attention map are multiplied to obtain a mixed attention map $F_{sc} \\in \\mathbb{R}^{C\\times H\\times W}$, which makes the information important in both space and channel dimensions more prominent and will encourage the network to learn more meaningful features. Then, the skip connection is used to multiply the original feature map and the mixed attention map pixel-by-pixel to complete information calibra- tion. Finally, the residual connection is used to alleviate the gradient disappearance problem caused by increasing depth in the DNN. Mathematically, the final attentive feature map $F$ is generated as follows:\n$F = (F_s \\times F_c) \\bigodot F + F$,\nwhere $\\bigodot$ denotes pixel-by-pixel multiplication."}, {"title": "3) Multi-level Conversion Module:", "content": "Feature fusion [68]- [70] is an important way to make full use of the information from each individual feature input. Generally, the low-level features in shallow layers have higher resolution and contain more detailed information, while the high-level features in deep layers have lower resolution and stronger semantic repre- sentation ability. In addition, it has been demonstrated that the HVS tends to perform multi-level feature fusion in perceiving image quality [57]. In our method, we design a multi-level conversion module to fuse the low-level and high-level features from the 1st layer and the 4th layer of the backbone network to exploit the complementarity between them.\nThe multi-view projected images are obtained from the point cloud under different viewpoints. Therefore, there is a certain inter-dependency between each other. To capture and exploit such a kind of dependency, we build a graph based on the correlation between those multi-view projected images by taking each projected image feature representation as a node in the graph. As a consequence, the obtained multi-level attentive feature maps need to be converted into the feature vector $h_{v_i} \\in \\mathbb{R}^{D}$ of the graph node $v_i (i \\in \\{1,2, . . ., N \\})$ by the multi-level conversion module, where D is the feature dimension. Formally, the conversion process of a feature vector can be expressed as\n$h_i = avg(F^{(1)}) \\oplus avg(F^{(4)})$,\nwhere $F^{(1)}$ and $F^{(4)}$ represent the attentive feature maps output by the attention block, and the input of the attention block comes from the 1st and 4th layers of the pre-trained backbone, respectively, $\\oplus$ represents the concatenation of the channels of the feature maps. Finally, we create a set of nodes $V = [h_{v_1}, h_{v_2},\u2026\u2026, h_{v_N}]^T$ based on all the feature vectors,"}, {"title": "D. GCN-based Quality Prediction", "content": "After graph construction, we use GCN to model the inter- action between the contents of different projected 2D images according to the graph G, thus completing the quality predic- tion.\n1) Graph Convolutional Network: We take the two graphs corresponding to the horizontal and vertical projection im- age feature groups into two GCNs without weight sharing, respectively, and update the new node representations by constantly exchanging neighborhood information based on the \u00c2. The GCN consists of four graph convolutional blocks, and the number of output channels of these blocks are [512, 128, 32, 1]. The graph convolutional block include a graph convolutional layer, a softplus activation function, and a batch normalization layer. The process of GCN can be described as\n$(H^{(1)}, H^{(2)}, H^{(3)}, H^{(4)}) = MG(G, \\hat{A}; W_G)$,\nwhere $H^{(l)}$ is the feature matrix after activation of the lth layer of GCN, $H^{(0)} = V$, $MG$ denotes the GCN, $w_G$ is the network parameter that is constantly updated during training. The layer-wise propagation rule of GCN is defined as follows\n$H^{(l+1)} = \\sigma(BN(\\hat{A}H^{(l)}W_l))$,\nwhere $\\sigma$ represents the softplus activation function, $BN$ represents batch normalization, $W_l$ is the trainable weight matrix of the lth layer, and the size of the matrix is related to the number of input and output channels. Based on the above propagation rules, the GCN continuously learns the dependencies between nodes and uses the information in the adjacency matrix to aggregate the features of itself and its neighbors to extract richer features."}, {"title": "2) Quality Prediction:", "content": "Here, we fuse the multi-level fea- ture matrices $H^{(l)}(l \\in \\{1,2,3,4\\})$ output by GCN. Above all, the first dimension of the first three feature matrices is averaged pooling, so as to imitate the HVS to aggregate the feature information of different projection images. Then, the 1-dimensional perceptual feature matrix is obtained through the dimension reduction of the fully connected layer. Since the number of channels of the feature matrix output by the last layer is already 1, the average pooling layer and the fully connected layer are used for the feature information of the first dimension of the matrix, so as to enhance the diversity of features. The final obtained multi-level fusion feature matrix $H$ is described as follows\n$H =L(\u03b1(H^{(1)})) \\oplus L(\u03b1(H^{(2)}))\\oplus\nL(\u03b1(H^{(3)}))\\oplus L(H^{(4)}) \\oplus \u03b1(H^{(4)})$,\nin which \u03b1(\u00b7) represents average pooling, L(\u00b7) represents the fully connected layer. From this, we extract the multi-level fusion feature matrix of horizontal and vertical projection image groups respectively, which are denoted as $\\bar{H}_H$ and $\\bar{H}_V$. The two groups of features from different projection directions have different detail information and can complement each other. Finally, after fusing the two feature matrices, we use the fully connected layer to automatically assign weights to $\\bar{H}_H$ and $\\bar{H}_V$ to predict the perceptual quality score of the point cloud."}, {"title": "E. Network Training", "content": "For the whole network, we simultaneously input 20 images from the two projected image groups to jointly optimize the two branches. The loss function used to optimize the model is l\u2081, which can be defined as\n$l_1 = \\frac{1}{n} \\sum_{i=0}^{n} |y_i - \\hat{y}_i|,\n\\hat{Y}_i = Q(P, \\hat{A}; W_F)$,\nwhere n indicates batch size, yi and $\\hat{Y}_i$ indicates the ith subjective quality score and objective prediction score in the batch respectively. $\\hat{Y}_i$ is extracted through the GC-PCQA network Q. WF is the trainable parameters of the network, which are updated by minimizing l\u2081."}, {"title": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In this section, we first introduce the adopted subject-rated databases and performance measures. Then, the imple- mentation details are provided. Finally, we conduct extensive experiments and analyze the results to verify our proposed method, including both performance comparison and ablation study."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a new GCN-based NR-PCQA method called GC-PCQA. Our main inspiration comes from the fact that the HVS depends on a set of projected images from multiple viewpoints when perceiving 3D objects and the mutual dependencies among different projected images can be well modeled by GCN. Therefore, we first project the point cloud in the horizontal and vertical directions to obtain the projected image group under multiple views. Then, graph construction is performed on the projected image group and GCN is used to model the mutual dependencies between different projected 2D image contents to aggregate the feature information of images under different viewpoints, so as to imitate the viewing behavior of HVS and better measure the quality of point cloud. The experimental results on two benchmark datasets show that our proposed method has better performance than other state-of-the-art methods."}]}