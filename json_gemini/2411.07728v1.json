[{"title": "No-Reference Point Cloud Quality Assessment via Graph Convolutional Network", "authors": ["Wu Chen", "Qiuping Jiang", "Wei Zhou", "Feng Shao", "Guangtao Zhai", "Weisi Lin"], "abstract": "Three-dimensional (3D) point cloud, as an emerging visual media format, is increasingly favored by consumers as it can provide more realistic visual information than two-dimensional (2D) data. Similar to 2D plane images and videos, point clouds inevitably suffer from quality degradation and information loss through multimedia communication systems. Therefore, automatic point cloud quality assessment (PCQA) is of critical importance. In this work, we propose a novel no-reference PCQA method by using a graph convolutional network (GCN) to characterize the mutual dependencies of multi-view 2D projected image contents. The proposed GCN-based PCQA (GC-PCQA) method contains three modules, i.e., multi-view projection, graph construction, and GCN-based quality prediction. First, multi-view projection is performed on the test point cloud to obtain a set of horizontally and vertically projected images. Then, a perception-consistent graph is constructed based on the spatial relations among different projected images. Finally, reasoning on the constructed graph is performed by GCN to characterize the mutual dependencies and interactions between different projected images, and aggregate feature information of multi-view projected images for final quality prediction. Experimental results on two publicly available benchmark databases show that our proposed GC-PCQA can achieve superior performance than state-of-the-art quality assessment metrics. The code will be available at: https://github.com/chenwuwq/GC-PCQA.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the development of three-dimensional (3D) visual information acquisition technology makes point clouds easier to obtain and gradually becomes a popular type of visual data. A 3D Point cloud is mainly used to describe a complete 3D scene or object, including geometric attributes (position of each point in 3D space), color attributes (RGB attributes of each point), and others (normal vector, opacity, reflectivity, time, etc.) [1]. Point clouds have been widely studied and used in a wide range of application scenarios such as 3D reconstruction [2], [3], classification and segmentation [4], [5], facial expression representation [6], autonomous driving [7], [8], and virtual reality [9], etc. Although point cloud can realistically record 3D objects through a large set of points, it also consumes a lot of memory, and it is difficult to achieve data transmission under limited network bandwidth [10], [11]. This new and effective data representation presents a challenge to the current hardware storage and network transmission. Therefore, in order to achieve efficient storage and transmission, compression of point clouds is necessary [12]\u2013[15]. However, point cloud compression may introduce artifacts, resulting in the degradation of point cloud visual quality. Point cloud visual quality is an important way to compare the performance of various point cloud processing algorithms. Effective point cloud quality assessment (PCQA) methods can not only help people evaluate the distortion degree of point clouds and the performance of compression algorithms but also be beneficial to optimize the visual quality of distorted point clouds. Thus, how to accurately assess the perceptual quality of point clouds has become a critical issue.\nSimilar to image quality assessment (IQA), PCQA can also be divided into subjective and objective methods. The subjective method is mainly based on the perception of the human visual system (HVS). It is difficult to be widely applied because this kind of assessment requires a large number of participants to ensure the rationality and accuracy of the assessment results in a statistical sense. Currently, the results obtained from subjective assessment experiments are generally served as the ground-truth data for benchmarking different objective methods [16]. According to the participation of original point clouds, objective PCQA methods can have three categories: full reference (FR), reduced reference (RR), and no reference (NR). Since the original point clouds are not always available, NR-PCQA methods that do not rely on any original information as a reference are more suitable in practical applications.\nThe traditional NR-PCQA methods [17], [18] generally predict the quality score by extracting quality-aware fea-tures based on the analysis of point cloud attributes such as geometry and color. Recently, the great success of deep learning in the field of NR-IQA has promoted the develop-ment of deep learning-based NR-PCQA metrics [19]\u2013[24]. The common practice of these deep NR-PCQA metrics is to directly apply the ordinary convolution operation on the point cloud for automatic feature learning in a data-driven manner. Nonetheless, point cloud is a typical kind of non-Euclidean data which is sparsely distributed over the 3D space, and a large number of useless pixels are also involved with pixel-by-pixel convolution, thus resulting in a huge waste of resources and inefficient data processing. In order to solve this problem, some related works try to represent non-Euclidean data with the graph which includes node information and complex adjacency relations between nodes. With the graph-based non-Euclidean data as input, the current works then introduce to use graph convolutional network (GCN) rather than traditional convolutional neural network (CNN) for more effective feature representation learning [25]. For instance, Thomas et al. [26] proposed to convert non-Euclidean data into a graph based on which the GCN is used to realize graph feature extraction. As a typical kind of non-Euclidean data, GCN has also been applied to many point cloud-based vision tasks, such as point cloud classification [27], [28], point cloud segmentation [29], point cloud data analysis [30], action recognition [31], etc. Moreover, it has also been applied to infer the perceptual quality of various multimedia data, e.g., traditional 2D images [32], [33], 360-degree images [34], [35], and meshes [36], [37].\nDue to the strong capability of GCN in handling non-Euclidean data including 3D point cloud, this paper presents a novel GCN-based NR-PCQA method (GC-PCQA). One of the most critical issues is to effectively create a graph of the point cloud so that the GCN can be applied for feature learning. Since the goal of PCQA is to predict the quality of the test point cloud consistent with human perception, how to construct a highly perception-consistent graph of point clouds is the key to its success. It is known that the HVS reconstructs 3D objects in their mind based on multiple two-dimensional (2D) plane images observed from different viewpoints. In order to imitate the process of the HVS to perceive 3D objects, it is natural to perform multi-view projection on the point cloud to obtain a set of projected images with each corresponding to a specific viewpoint. Although these projected images are independent individuals, there is a certain extent of correlation between each other. Therefore, we regard all projected images as a set of non-Euclidean data and then establish a graph according to the dependencies between each individual projected image. Finally, GCN is applied to realize feature extraction from the constructed graph for quality prediction. Experimental results demonstrate that our proposed GC-PCQA method outperforms state-of-the-art reference and non-reference PCQA methods on two public PCQA databases. Overall, the main contributions of this paper are as follows:\n1) We perform multi-view projection on the point cloud to obtain a set of projected images based on which a highly perception-consistent graph is constructed to model the mutual dependencies of multi-view projected images. The graph nodes are defined with the projected images and connected by spatial relations between each other.\n2) We perform GCN on the proposed graph to characterize the interactions between different projected images and aggregate the feature information of multi-view pro-jected images for final quality prediction. The ablation study validates the effectiveness of the GCN architecture and the source code is available for public research usage.\n3) We fuse the horizontally and vertically projected image features extracted by two GCNs that do not share weights to boost the performance. Experimental results show that the proposed GC-PCQA can predict subjective scores more accurately than the existing state-of-the-art PCQA metrics.\nThe rest of this paper is organized as follows. In Section II, we introduce the related works. In Section III, we illustrate the proposed GC-PCQA with technical details. We conduct experiments and analyze the results in Section IV, and finally draw conclusions in Section V."}, {"title": "II. RELATED WORK", "content": "PCQA metrics have developed rapidly and can be mainly divided into PC-based metrics and projection-based metrics. PC-based metrics evaluate the quality score through the char-acteristic information of each point in the point cloud. While projection-based metrics use the projected images of the point cloud instead of the point cloud itself."}, {"title": "A. PC-based Metrics", "content": "As one of the important evaluation methods, the FR method has been widely investigated in PC-based metrics. The ini-tial methods calculate quality scores based on geometric information of point clouds, such as $PSNR_{MSE,p2po}$ and $PSNR_{HF,p2po}$ [38], $PSNR_{MSE,p2pl}$ and $PSNR_{HF,p2pl}$ [39]. Among them, the point-to-point methods (p2point) com-pute the L2 norm of the nearest point pair as the distortion measure of the point, while point-to-plane methods (p2plane) increase the normal vector of the plane. Besides, Alexiou et al. [40] captured the distortion point cloud degradation through the angular similarity between the corresponding points. Javaheri et al. [41] used the generalized Hausdorff distance for PCQA. In addition to geometric properties, the color properties of point clouds can also be used as one of the important features for quality assessment. PSNRY [42] evaluates texture distortion of colored point clouds based on point-to-point color components. Viola et al. [43] used global color statistics, such as color histograms and correlograms, to evaluate the degree of distortion of a point cloud. On the basis of PC-MSDM [44], Meynet et al. [45] proposed a linear model PCQM based on curvature and color attributes to predict 3D point cloud visual quality. Inspired by the idea of similarity, Alexiou et al. [46] used the structural similarity index based on geometric and color features for evaluation. Diniz et al. [47]\u2013[49] extracted statistical information of point clouds based on local binary pattern descriptors and local luminance pattern descriptors, which are assessed by distance metrics. Yang et al. [50] proposed to construct the local graph representation of the reference point cloud and the distorted point cloud respectively with the key point as the center and calculate the similarity feature by extracting three color gradient moments between the central key point and all other points, so as to estimate the quality score of the distorted point cloud. They also believed that point clouds have potential energy, and used multiscale potential energy discrepancy (MPED) [51] to quantify point cloud distortion. Furthermore, Viola et al. [52] extracted part of the geometric, color, and normal features from the point clouds, and then evaluated the distorted point cloud by finding the best combination of features through a linear optimization algorithm. Liu et al. [53] proposed an analytical model with only three parameters to accurately predict the MOS of V-PCC compressed point clouds from geometric and color features. All of these methods use reference point clouds, and despite the advanced performance achieved, these methods may not be useful in practical applications. Therefore, it is meaningful to research NR methods to overcome the problem of missing reference point clouds. Zhang et al. [18] used 3D natural scene statistics (3D-NSS) and entropy to extract geometric and color features related to quality, and then predicted quality scores through a support vector regression (SVR) model. With structure-guided resampling, Zhou et al. [54] estimated point cloud quality based on geometry density, color naturalness, and angular consistency. The success of deep learning in various research fields has prompted researchers to introduce it into PCQA. Chetouani et al. [19] used deep neural networks (DNNs) to learn the mapping of low-level features such as geometric distance, local curvature, and luminance values to quality scores. Liu et al. [20] constructed a large-scale PCQA dataset named LS-PCQA, which contains more than 22,000 distortion samples, and then proposed an NR metric based on sparse CNN."}, {"title": "B. Projection-based Metrics", "content": "In addition to the above methods that directly use point clouds for quality evaluation, projection-based metrics also play an important role in PCQA. The projection-based PCQA metrics project the point cloud from 3D space to 2D plane, so as to transform PCQA into IQA which has been relatively mature. Therefore, the existing IQA methods can be directly used to evaluate the quality of 2D projection images, such as PSNR [55], SSIM [56], MS-SSIM [57], IW-SSIM [58], VIFP [59], etc. Freitas et al. [60] used a multi-scale rotation invariant texture descriptor called Dominant Rotated Local Binary Pattern (DRLBP) to extract statistical features from these texture maps and calculate texture similarity. Finally, texture features and similarity features were fused to predict the visual quality of point clouds. Hua et al. [17] proposed a blind quality evaluator of colored point cloud based on visual perception, which reduces the influence of visual masking effect by projecting the point cloud onto a plane to extract geometric, color and joint features. Tao et al. [21] projected the color point cloud in 3D space into a 2D color and geometric projection map, and then weighted the quality scores of local blocks in the map based on a multi-scale feature fusion network. Liu et al. [22] projected the six planes of the 3D point cloud, and then extracted multi-view features through a DNN to classify the distortion types of the point cloud. Finally, the final quality score was obtained by multiplying the probability vector and the quality vector. Tu et al. [23] designed a two-stream CNN to extract the features of texture projection maps and geometric projection maps. Yang et al. [24] used natural images as the source domain and point clouds as the target domain, and predicted point cloud quality through unsupervised adversarial domain adaptation.\nBased on the above statements, PC-based metrics and projection-based metrics have achieved certain results. How-ever, most of the existing projection-based PCQA metrics are evaluated based on six projection planes, which do not take into account the quality perception of HVS for 3D point clouds from multiple views, and do not make full use of the correlation between different projected images for modeling. Thus, we propose a novel non-reference PCQA method by using GCN to characterize the mutual dependencies of multi-view 2D projected image contents."}, {"title": "III. PROPOSED METHOD", "content": "We first give a brief overview of the proposed GC-PCQA method. Then, the details of each module in our GC-PCQA method will be illustrated. Finally, we describe how the network is trained."}, {"title": "A. Overview", "content": "The framework of our proposed GC-PCQA method is shown in Fig. 2. It is mainly composed of three parts: multi-view projection, graph construction, and GCN-based quality prediction. Firstly, considering the behavior of the HVS when observing 3D point clouds, multi-view projection is performed on the point cloud to obtain a set of horizontally and vertically projected 2D images. The horizontally (vertically) projected 2D image set covers the visual contents that can be perceived within the horizontal (vertical) visual field by an observer. All these projected images are fed into a pre-trained backbone and an attention block for attentive feature extraction. Secondly, a multi-level fusion of attentive feature maps is carried out through the multi-level conversion module, and graph construction is performed according to spatial relations among different projected images. Thirdly, reasoning on the constructed graph is performed by GCN to model the mutual dependencies between nodes and generate more effective feature representations. Finally, multi-level feature fusion is carried out on the feature representations obtained by two GCNs (corresponding to the horizontal visual field and the vertical visual field, respectively) to predict the final quality score."}, {"title": "B. Multi-view Projection", "content": "Point cloud consists of huge point sets, which is mainly used to describe 3D objects in detail, resulting in a large volume of point cloud and it is difficult to directly input point cloud data into the network. In order to reduce the cost of processing large-scale point clouds, many point cloud resampling strategies [61]\u2013[63] and point cloud projection methods [21]\u2013[24] have been proposed to simplify point cloud data. Since deep learning is very effective in the field of image processing, we choose to use the multi-view projection method to convert the point cloud into an image, so as to take advantage of deep learning for PCQA.\nThe visual field of the human eye is divided into horizontal visual field and vertical visual field. The range of view in different directions is limited, i.e., the horizontal view limit is approximately 190 degrees while the vertical view limit is 135 degrees. When human eyes observe 3D objects such as 3D point clouds, it is difficult to directly observe all contents within 360 degrees [34]. In general, human reconstructs 3D objects in the brain by observing from different viewpoints, so as to have a clearer perception of 3D objects. In order to imitate the perceptual process of the HVS, we perform a multi-view projection operation on the point cloud based on rotation stride (RS). Just as the human eye perceives 3D objects, we rotate and project the point cloud onto the 2D space in both horizontal and vertical directions. Finally, for each direction, we obtain a multi-view projected image group P containing N projected images:\n$P = [Y_1, Y_2, Y_3,\u00b7\u00b7\u00b7, Y_N] \\in R^{N\\times3\\times H\\times W}$,\nwhere $Y_i$ represents the i-th projected image, and N is the total number of projected images. H and W indicate the height and width of each individual projected image, respectively. We use $P_H$ to denote the horizontally projected image group and $P_V$ to denote the vertically projected image group."}, {"title": "C. Graph Construction", "content": "The graph construction module aims to construct a perception-consistent graph based on the features extracted from multi-view projected images. It is mainly composed of a pre-trained backbone network, an attention block, and a multi-level conversion module."}, {"title": "1) Feature Extraction", "content": "We adopt the pre-trained ResNet101 [64] as the backbone for feature extraction. The input image size of the backbone network is 224x224. However, due to the presence of a large amount of quality-unrelated white regions in the projected images, there may be adverse effects or an inability to learn quality-related features. To address this, an additional image pre-processing step is performed. The projected image is cropped to effectively remove the non-informative white background regions. Subsequently, the cropped image is resized to 224\u00d7224 to create an informative image patch, which is then used as input to the network."}, {"title": "2) Attention Block", "content": "The attention mechanism comes from the research on human vision and draws lessons from the attention thinking of the human vision, which can make the feature extractor focus more on those significant areas of the target while suppressing the most unimportant information to improve the performance of DNNs. At present, a variety of attention modules have been proposed such as SE attention [65], CBAM attention [66], scSE attention [67], etc. Therefore, in order to further improve the feature representation capability of the network, we devise an attention block to impose appropriate attention weights to the feature maps obtained by the pre-trained backbone.\nWe first introduce the upper flow, i.e., the spatial attention. The input feature maps from the pre-trained backbone can be represented as $F^{(i)} \\in R^{C\\times H\\times W}$, where i represents the output of the ith layer of the backbone. Firstly, 2D convolution with a convolution kernel size of 1 is used to reduce the number of channels to 1. Then, a sigmoid activation function is applied to map the range of feature values into [0,1]. Finally, the spatial attention map $F_s \\in R^{H\\times W}$ is expressed as follows:\n$F_s = \\delta(\\varphi(F)) =\\begin{bmatrix}\\delta(S_{11}) & \\delta(S_{12}) & \\cdots & \\delta(S_{1W}) \\\\\\delta(S_{21}) & \\delta(S_{22}) & \\cdots & \\delta(S_{2W}) \\\\\\vdots & \\vdots & \\ddots & \\vdots \\\\\\delta(S_{H1}) & \\delta(S_{H2}) & \\cdots & \\delta(S_{HW})\\end{bmatrix}$,\nwhere $\\varphi$ represents the 2D convolution with a convolution kernel size of 1, $\\delta(.)$ denotes the sigmoid function, and $S_{ij}(i \\in {1,2,...,H},j \\in {1,2,...,W})$ indicates the relative importance of the eigenvalues at position (i, j).\nFor the lower flow, global average pooling is first applied on the input feature maps $F \\in R^{C\\times H\\times W}$. Then, the channel dimension of the feature maps is reduced and then increased by two 2D convolution layers with a convolution kernel size of 1. The final channel attention map $F_c \\in R^{C\\times1\\times1}$ is generated by attaching a sigmoid activation function $\\delta(.)$ in the end, which can be expressed as follows:\n$F_c = \\delta(\\varphi(avg(F))) = [\\delta(U_1), \\delta(U_1),\u00b7\u00b7\u00b7, \\delta(U_C)]$,\nwhere $U_i(i \\in {1,2,...,C})$ denotes the relative importance of the ith channel among all channels, avg denotes the global average pooling. After obtaining two attentions, the spatial attention map and the channel attention map are multiplied to obtain a mixed attention map $F_{sc} \\in R^{C\\times H\\times W}$, which makes the information important in both space and channel dimensions more prominent and will encourage the network to learn more meaningful features. Then, the skip connection is used to multiply the original feature map and the mixed attention map pixel-by-pixel to complete information calibra-tion. Finally, the residual connection is used to alleviate the gradient disappearance problem caused by increasing depth in the DNN. Mathematically, the final attentive feature map F is generated as follows:\n$F = (F_s \\times F_c) \\bigodot F + F$,\nwhere $\\bigodot$ denotes pixel-by-pixel multiplication."}, {"title": "3) Multi-level Conversion Module", "content": "Feature fusion [68]\u2013[70] is an important way to make full use of the information from each individual feature input. Generally, the low-level features in shallow layers have higher resolution and contain more detailed information, while the high-level features in deep layers have lower resolution and stronger semantic repre-sentation ability. In addition, it has been demonstrated that the HVS tends to perform multi-level feature fusion in perceiving image quality [57]. In our method, we design a multi-level conversion module to fuse the low-level and high-level features from the 1st layer and the 4th layer of the backbone network to exploit the complementarity between them.\nThe multi-view projected images are obtained from the point cloud under different viewpoints. Therefore, there is a certain inter-dependency between each other. To capture and exploit such a kind of dependency, we build a graph based on the correlation between those multi-view projected images by taking each projected image feature representation as a node in the graph. As a consequence, the obtained multi-level attentive feature maps need to be converted into the feature vector $h_{v_i} \\in R^D$ of the graph node $v_i (i \\in {1,2, . . ., N })$ by the multi-level conversion module, where D is the feature dimension. Formally, the conversion process of a feature vector can be expressed as\n$h_{v_i} = avg(F^{(1)}) \\bigoplus avg(F^{(4)})$,   where $F^{(1)}$ and $F^{(4)}$ represent the attentive feature maps output by the attention block, and the input of the attention block comes from the 1st and 4th layers of the pre-trained backbone, respectively, $\\bigoplus$ represents the concatenation of the channels of the feature maps. Finally, we create a set of nodes $V = [h_{v_1}, h_{v_2},\u2026\u2026, h_{v_N}]^T$ based on all the feature vectors,"}, {"title": "Meanwhile, the adjacency relation between any two nodes $v_i$, $v_j$ can be expressed as", "content": "$A(v_i, v_j) =\\begin{cases}1, & \\text{if } RSDist(v_i, v_j) \\leq \\Theta \\\\0, & \\text{otherwise}\\end{cases}$,\nwhere $A \\in R^{N\\times N}$ is the adjacency matrix representing the correlation between nodes of the graph, $RSDist(.)$ computes the total RS of the projected images corresponding to the two nodes, and the RS threshold $\\Theta$ is set to 36\u00b0 with experiments. Specifically, if the RS is less than or equal to 0, the two projected images are considered to be spatially connected and share some common information, otherwise they are not adjacent. Then, by multiplying both sides of the adjacency matrix A by the square root of the degree matrix for normal-ization [26], [71], the graph nodes with many neighbor nodes are avoided to have too much influence. The normalization formula is as follows\n$\\hat{A} = D^{-\\frac{1}{2}}(A + I)D^{\\frac{1}{2}}$,\nwhere \u00c2 is the normalized adjacency matrix. D is the degree matrix, which takes the degree of the corresponding node as the value only on the diagonal and is 0 in the rest of the posi-tions. Concretely, $D_{ii} = \\sum_{j=0}^{N} A(v_i, v_j) (i \\in {1,2,\u2026\u2026\u2026, N})$. I is the identity matrix and adds self-join to the adjacency matrix. Finally, we construct the graph $G = (V, \u00c2)$, so that the correlation between nodes $v_i$ and $v_j$ can be represented by the corresponding value $A(v_i, v_j)$ in the adjacency matrix."}, {"title": "D. GCN-based Quality Prediction", "content": "After graph construction, we use GCN to model the inter-action between the contents of different projected 2D images according to the graph G, thus completing the quality predic-tion."}, {"title": "1) Graph Convolutional Network", "content": "We take the two graphs corresponding to the horizontal and vertical projection im-age feature groups into two GCNs without weight sharing, respectively, and update the new node representations by constantly exchanging neighborhood information based on the A. The GCN consists of four graph convolutional blocks, and the number of output channels of these blocks are [512, 128, 32, 1]. The graph convolutional block include a graph convolutional layer, a softplus activation function, and a batch normalization layer. The process of GCN can be described as\n$(H^{(1)}, H^{(2)}, H^{(3)}, H^{(4)}) = MG(G, \\hat{A}; w_G)$,\nwhere $H^{(l)}$ is the feature matrix after activation of the $l^{th}$ layer of GCN, $H^{(0)} = V$, MG denotes the GCN, $w_G$ is the network parameter that is constantly updated during training. The layer-wise propagation rule of GCN is defined as follows\n$H^{(l+1)} = \\sigma(BN(\\hat{A}H^{(l)}w_l))$,\nwhere \u03c3 represents the softplus activation function, BN represents batch normalization, $w_l$ is the trainable weight matrix of the $l^{th}$ layer, and the size of the matrix is related to the number of input and output channels. Based on the above propagation rules, the GCN continuously learns the dependencies between nodes and uses the information in the adjacency matrix to aggregate the features of itself and its neighbors to extract richer features."}, {"title": "2) Quality Prediction", "content": "Here, we fuse the multi-level fea-ture matrices $H^{(l)}(l \\in {1,2,3,4})$ output by GCN. Above all, the first dimension of the first three feature matrices is averaged pooling, so as to imitate the HVS to aggregate the feature information of different projection images. Then, the 1-dimensional perceptual feature matrix is obtained through the dimension reduction of the fully connected layer. Since the number of channels of the feature matrix output by the last layer is already 1, the average pooling layer and the fully connected layer are used for the feature information of the first dimension of the matrix, so as to enhance the diversity of features. The final obtained multi-level fusion feature matrix H is described as follows\n$H =L(\u03b1(H^{(1)})) \\bigoplus L(\u03b1(H^{(2)})) \\bigoplus\nL(\u03b1(H^{(3)})) \\bigoplus L(H^{(4)}) \\bigoplus \u03b1(H^{(4)})$,   in which \u03b1(\u00b7) represents average pooling, L(\u00b7) represents the fully connected layer. From this, we extract the multi-level fusion feature matrix of horizontal and vertical projection image groups respectively, which are denoted as HH and Hv. The two groups of features from different projection directions have different detail information and can complement each other. Finally, after fusing the two feature matrices, we use the fully connected layer to automatically assign weights to HH and Hy to predict the perceptual quality score of the point cloud."}, {"title": "E. Network Training", "content": "For the whole network, we simultaneously input 20 images from the two projected image groups to jointly optimize the two branches. The loss function used to optimize the model is l1, which can be defined as\n$\\hat{l_1} = \\frac{1}{n} \\sum_{i=0}^{n} |y_i - \\hat{y_i}|$,   $\\hat{Y_i} = Q(P, \\hat{A}; w_F)$,   where n indicates batch size, $y_i$ and $\\hat{Y_i}$ indicates the ith subjective quality score and objective prediction score in the batch respectively. \u0177r is extracted through the GC-PCQA network Q. wF is the trainable parameters of the network, which are updated by minimizing l1."}, {"title": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In this section, we first introduce the adopted subject-rated databases and performance measures. Then, the imple-mentation details are provided. Finally, we conduct extensive experiments and analyze the results to verify our proposed method, including both performance comparison and ablation study."}, {"title": "A. Databases and Performance Measures", "content": "1) Databases: We perform experiments on two publicly available 3D point cloud databases which consist of SJTU [72] and WPC [73].\nThe SJTU database includes nine pristine and 378 distorted point clouds generated from seven distortion types. Each dis-tortion type corresponds to six distortion levels. The subjective testing protocol uses Absolute Category Rating (ACR), and the subjective scores are in the form of MOS values ranging from 1 to 10.\nThe WPC database has 20 original point clouds. For each reference point cloud, 37 distorted point clouds are created by simulating five distortion types (i.e., Downsample, Gaussian white noise, G-PCC(T), V-PCC, G-PCC(O)), leading to 740 distorted point clouds in total. Each distorted point cloud also relates to a MOS value. The subjective testing protocol uses the Double Stimulus Impairment Scale (DSIS), with MOS values ranging from 0 to 100.\n2) Performance Measures: We apply four measures to evaluate and compare different PCQA methods, including Spearman's Rank Correlation Coefficient (SRCC), Pearson's linear correlation coefficient (PLCC), Kendall Rank Cor-relation Coefficient (KRCC), and root mean squared error (RMSE). The SRCC and KRCC are used to measure the monotonicity, while PLCC and RMSE are used to evaluate the accuracy. Higher correlation coefficients and lower RMSE represent better performance. To resolve the scale differences between the predicted quality scores and the subjective scores, we use a five-parameter logistic function [74] after calculating the initial prediction results, which can be expressed as\n$y = \\beta_1(\\frac{1}{1 + exp(\\beta_2(x - \\beta_3))}}) + \\beta_4x + \\beta_5$,\nwhere x is the initial prediction result of the PCQA metric, y indicates the mapped objective quality score through the five-parameter logistic function, and \u03b2i (i \u2208 {1,2,\u2026\u2026,5}) are the fitting parameters."}, {"title": "B. Implementation Details", "content": "In our experiments, we employ PyTorch as the deep learning framework and the computer operating system is Ubuntu18.04. Moreover, the GPU is used to accelerate the training and testing procedures. The adaptive moment estimation optimizer (Adam) [75] is used for model training. We set batch size and initial learning rate as 32 and 1e-3, respectively. Additionally, the learning rate is reduced to 0.5 times of the original one every 10 epochs until the final convergence.\nThe network F is trained for 50 epochs and the training terminates early when there is no further optimized wF for 20 epochs. Meanwhile, we exploit data augmentation meth-ods such as horizontal and vertical flipping to enhance the generalization ability of the proposed network.\nIn addition, the k-fold cross-validation strategy is used for the performance test. For each point cloud quality database,  $\\frac{K-1}{K}$  distorted samples are randomly selected from the database as the train sets, and the rest point clouds are used as the test sets. Specifically, we choose K equalling to 9 and 5 for the SJTU and WPC databases, respectively. The final results can be obtained by averaging the performance values from K times."}, {"title": "C. Performance Comparison", "content": "We compare our proposed GC-PCQA with 20 state-of-the-art quality assessment methods. As mentioned in Section II", "types": "PC-based metrics and projection-based metrics. PC-based met-rics are directly evaluated from 3D point clouds", "38": "PSNR_{HF"}, {"38": "PSNR_{MSE", "39": "PSNR_{HF"}, {"39": "AS_{Mean"}, [40]], "40": "AS_{MSE"}, [40], "PSNRY [42", "PCQM [45", "PointSSIM [46", "GraphSIM [49", "PCMRR [52", 3, "D-NSS [18", "and ResSCNN [20", ".", "The projection-based metrics operate on the projected 2D images of point clouds, including SSIM [56", "MS-SSIM [57", "IW-SSIM [58", "VIFP [59", "PQANet [22", "and IT-PCQA [24", ".", "It is worth mentioning that the performance results of SSIM, MS-SSIM, IW-SSIM, and VIFP are the average values from six perpendicular projections [72", [76], ".", "nThe performance comparison results on SJTU and WPC databases are shown in Table I. From the table, we can draw several conclusions: (1) Compared to existing"]