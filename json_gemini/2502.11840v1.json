{"title": "ChordFormer: A Conformer-Based Architecture for\nLarge-Vocabulary Audio Chord Recognition", "authors": ["Muhammad Waseem Akram", "Stefano Dettori", "Valentina Colla", "Giorgio Carlo Buttazzo"], "abstract": "Abstract-Chord recognition serves as a critical task in music\ninformation retrieval due to the abstract and descriptive nature\nof chords in music analysis. While audio chord recognition\nsystems have achieved significant accuracy for small vocabularies\n(e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises\nfrom the inherent long-tail distribution of chords, where rare\nchord types are underrepresented in most datasets, leading to\ninsufficient training samples. Effective chord recognition requires\nleveraging contextual information from audio sequences, yet\nexisting models, such as combinations of convolutional neural\nnetworks, bidirectional long short-term memory networks, and\nbidirectional transformers, face limitations in capturing long-\nterm dependencies and exhibit suboptimal performance on\nlarge-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to\ntackle structural chord recognition (e.g., triads, bass, sevenths)\nfor large vocabularies. ChordFormer leverages conformer blocks\nthat integrate convolutional neural networks with transformers,\nthus enabling the model to capture both local patterns and\nglobal dependencies effectively. By addressing challenges such\nas class imbalance through a reweighted loss function and struc-\ntured chord representations, ChordFormer outperforms state-\nof-the-art models, achieving a 2% improvement in frame-wise\naccuracy and a 6% increase in class-wise accuracy on large-\nvocabulary chord datasets. Furthermore, ChordFormer excels\nin handling class imbalance, providing robust and balanced\nrecognition across chord types. This approach bridges the gap\nbetween theoretical music knowledge and practical applications,\nadvancing the field of large-vocabulary chord recognition.", "sections": [{"title": "I. INTRODUCTION", "content": "AUTOMATIC chord recognition (ACR) forms the foun-\ndation of music information retrieval, acting as a key\ntool for understanding and analyzing the harmonic structure\nof music. ACR systems aim at decoding audio recordings\ninto sequences of time-synchronized chord labels, enabling\nvarious applications such as automatic lead-sheet creation [1],\nmusic structure analysis [2], key classification [3], and cover\nsong identification [4]. These systems not only assist musi-\ncians, composers, and educators but also facilitate a deeper\nunderstanding of musical content, aiding in the preservation\nand exploration of diverse musical traditions. Despite its\ntransformative potential, ACR remains a challenging task due\nto the inherent complexities of musical chords, their temporal\nstructures [5], and the uneven distribution of chord types in\nreal-world music datasets [6], [7].\nChords, as fundamental elements of harmony, are more than\njust combinations of notes [8]. They carry rich musical seman-\ntics and temporal relationships that are rooted in centuries of\nmusic theory and practice. These relationships are far from\nrandom, often following well-defined harmonic progressions\nthat exhibit long-term dependencies. Modeling these depen-\ndencies is crucial for accurate chord recognition, but it is\nchallenging task [5]. Standard approaches, such as Hidden\nMarkov Models (HMMs) [9], [10], have historically relied on\ntransition matrices to capture these temporal dynamics, but\nthey struggle to extend beyond short-term dependencies due\nto their limited capacity to model complex and long-range\nrelationships. Similarly, n-gram models [11] and feedforward\nneural networks [12] have been explored, but their effective-\nness is limited by the relatively small harmonic context they\ncan capture.\nThe advent of deep learning has brought significant ad-\nvancements to ACR [13], [14], [12], enabling models to learn\nfrom large datasets and capture intricate patterns in musical\ndata. Architectures such as convolutional neural networks\n(CNNs) [6], [15], [16], [17] and recurrent neural networks\n(RNNs) [13], [18], [19], [20], [21] have been widely adopted,\noffering the ability to jointly model chord sequence consis-\ntency, chord duration, and other related features. However,\nthese architectures are not without limitations. CNNs, for\ninstance, are inherently designed to capture local patterns and\nstruggle with long-term dependencies, while RNNs often face\nissues such as vanishing gradient, which hinder their ability\nto remember distant temporal information. Such limitations\nhave spurred the exploration of attention mechanisms and\ntransformer-based architectures, which have revolutionized\nsequence modeling in fields like natural language process-\ning [22], [23], speech recognition [24], and music genera-\ntion [25].\nTransformers, characterized by their self-attention mecha-\nnisms, excel at capturing long-range dependencies and global\ncontext without relying on recurrence or convolution. Their\nability to dynamically weigh the importance of different parts\nof an input sequence has proven transformative in various\ndomains. In the context of ACR, such capabilities hold promise\nfor addressing long-term chord dependencies, a challenge that\nhas historically limited the performance of many systems [6],\n[13], [15], [16], [17], [18], [19], [20], [21]. In addition\nto the challenges of modeling temporal dependencies, ACR\nfaces significant hurdles related to chord vocabulary and class\nimbalance [6], [7].\nLarge-vocabulary chord recognition is inherently complex\ndue to the vast number of chord qualities and the subtle\ndifferences between them. For instance, extended chords such\nas ninths, elevenths, and thirteenths often share overlapping\nchromatic features, making them difficult to distinguish. More-\nover, the distribution of chord types in real-world datasets\nis highly imbalanced, with a small number of chord types\ndominating most of the musical content. This imbalance skews\nlearning processes, causing models to overfit common chord\ntypes and underperform on rarer ones. Addressing chord\nimbalance is critical for developing robust and generalizable\nACR systems.\nTo tackle these challenges, this paper introduces a new\nmodel, ChordFormer, for structural chord recognition based on\nconformer blocks [24]. The conformer architecture combines\nconvolutional layers with self-attention mechanisms and is\nparticularly well-suited for this task, as it effectively balances\nthe modeling of local and global dependencies. By leveraging\nsuch a hybrid design, our model achieves a nuanced under-\nstanding of chord sequences, overcoming the limitations of\nexisting approaches. ChordFormer incorporates a reweighted\nloss function [6], ensuring that underrepresented chord classes\nreceive an adequate attention during training. This approach\nmitigates the overemphasis on frequent chord types, promot-\ning a more balanced learning across the chord vocabulary.\nFurthermore, ChordFormer adopts a structured representation\nof chord labels inspired by chord structure decomposition\nproposed by Jiang et al. [6]. Unlike traditional flat classifica-\ntion methods, this representation captures the hierarchical and\nsemantic relationships between chord qualities, enabling the\nmodel to better differentiate between subtle variations in chord\nstructures. By integrating these innovations, our approach not\nonly improves recognition accuracy, but also provides a more\nmusically meaningful interpretation of chord labels.\nIn summary:\n1) A new Conformer-based architecture is proposed for a\nstructural chord recognition, leveraging its hybrid design\nto capture both local and long-term dependencies in\nchord sequences.\n2) The pervasive issue of class imbalance is addressed\nthrough a reweighted loss function, which enhances the\nmodel's performance on rare chord types.\n3) A structured representation of chord labels that aligns\nwith musical theory, is applied to offer a more in-\nterpretable and effective approach to large-vocabulary\nchord recognition.\nThe effectiveness of the proposed approach has been eval-\nuated by conducting extensive experiments on a chord recog-\nnition dataset comprising 1,217 songs compiled by Humphrey\nand Bello [17], [26]. The achieved results demonstrate that\nthe ChordFormer model outperforms state-of-the-art methods,\nincluding those employing large-vocabulary transcription and\nchord structure decomposition. It exhibits superior perfor-\nmance across various metrics, with notable improvements in\nrecognizing rare and complex chord types.\nThe remainder of this paper is organized as follows. Sec-\ntion II reviews related work in chord recognition. Section III\ndetails the design and implementation of the ChordFormer\nmodel, including its reweighted loss function and structured\nlabel representation. Section IV presents the experimental\nsetup and evaluation metrics, while the results are discussed in\nSection V. Finally, Section VI concludes the paper and outlines\nsome directions for future research in this domain."}, {"title": "II. RELATED WORK", "content": "Automatic Chord Recognition (ACR) has undergone sub-\nstantial evolution since Fujishima's groundbreaking work in\n1999 [27]. This pioneering effort introduced a real-time sys-\ntem utilizing 12-dimensional chroma features and Hidden\nMarkov Models (HMMs) to model chord sequences. Fu-\njishima's approach transitioned ACR from handcrafted feature-\nbased methodologies into data-driven paradigms powered by\nadvancements in machine learning and deep learning [28].\nInitial ACR methodologies focused on feature extraction\nand chord sequence decoding. Techniques such as chroma\nvectors and tonal centroid representations, including the Ton-\nnetz [29], were instrumental in capturing harmonic con-\ntent. Probabilistic models like Gaussian Mixture Models\n(GMMs) [30] and HMMs [10], [9], [11] served as the\nbackbone for sequence decoding. Despite their utility, these\napproaches faced limitations in modeling complex harmonic\nprogressions and handling large vocabularies. Incremental\nimprovements [31], [32], such as higher-order HMMs, alle-\nviated some issues but were restricted by the inadequacies of\nmanually designed features.\nThe introduction of deep learning marked a paradigm\nshift in ACR [12], [13], [14]. Convolutional neural net-\nworks (CNNs) and recurrent neural networks (RNNs) [6],\n[15], [17], [18], [19], [20], [21] became dominant archi-\ntectures, with CNNs excelling at capturing local harmonic\npatterns and RNNs addressing sequential dependencies. Hy-\nbrid models [16], [33] combining CNNs with Conditional\nRandom Fields or RNNs demonstrated significant performance\nimprovements by integrating learned auditory features with\nsequence modeling capabilities. Fully convolutional models\nand Bidirectional LSTMs [6], [18] further showcased the\npotential of deep learning to address ACR's complexities.\nTo address the limitations of CNNs and RNNs, attention\nmechanisms and Transformer-based architectures emerged as\nkey innovations [22]. Transformers, leveraging self-attention\nmechanisms, dynamically weigh sequence elements, enabling\nrobust modeling of global context and long-term harmonic\ndependencies [24]. Recent advancements, such as the Bi-\nDirectional Transformer for Chord Recognition [34], demon-\nstrated the effectiveness of these architectures in sequence\nsegmentation and chord classification.\nACR systems must also address challenges posed by large\nvocabularies and class imbalance [5], particularly the un-\nderrepresentation of rare chord qualities. Structured chord"}, {"title": "III. METHODOLOGY", "content": "This section presents the proposed ChordFormer architec-\nture for large-vocabulary chord recognition. The methodology\nintegrates several components, including a structured represen-\ntation of chord labels, pre-processing, feature extraction using\nconformer blocks, a decoding model based on Conditional\nRandom Fields, and a class re-weighting mechanism to address\nimbalance issues. Each component is detailed in the following\nsubsections.\nAn important insight from the literature [17], [31], [6] is\nthat the effectiveness of models in chord classification tasks\nheavily depends on the chord representation employed in terms\nof embeddings. A good chord representation not only enhances\nclassification performance but can also help in addressing\nthe challenge of class imbalance in large chord vocabularies.\nRepresenting a chord by its meaningful components, such\nas root, triad, bass, and extensions (7th, 9th, 11th, and 13th),\ncaptures the hierarchical and semantic relationships among the\nnotes that compose complex chords. It further accommodates\nall possible inversions through bass identification. Such a\nrepresentation not only mitigates the complexity associated\nwith large chord vocabularies, but also standardizes component\nlabels for a precise and interpretable recognition.\nThe chord representation utilizes the following components\nand labels:\n\u2022 Root + triad: where root \u2208{N, C, C#/D\u266d, ..., B} and triad\n\u2208{N, major, minor, sus4, sus2, diminished, augmented}\n\u2022 Bass: {N, C, C#/D\u266d, ..., B}\n\u2022 7th: {N, 7, b7, bb7}\n\u2022 9th: {N, 9, #9, b9}\n\u2022 11th: {N, 11, #11}\n\u2022 13th: {N, 13, 13}\nIn the notation presented above, it is worth observing that:\n\u2022 The \"N\" class included in each component denotes the\nabsence of that component.\n\u2022 The double flat seventh (bb7) denotes the sixth and it is\nused to maintain a theoretical consistency with certain\nnotations encountered in complex modulations.\n\u2022 The sixth and thirteenth extensions refer to the same\nabsolute note, but their naming depends on the harmonic\ncontext and chord function: while a sixth chord refers to\na major triad with an added sixth (e.g., C6 = C-E-G-A)\nwithout a seventh, a thirteenth chord normally includes\nthe dominant seventh (b7) and the ninth (9), extending\nbeyond the octave (e.g., C13 = C-E-G-B\u266d-D-A).\nThe proposed ChordFormer represents chords using a six-\ndimensional vector, with each dimension corresponding to a\nspecific chord component. In particular, the first dimension\nencodes the combination of the root and triad types (e.g.,\nC# augmented, D minor). The second to sixth dimensions\nrepresent the bass (capturing inversions), 7th, 9th, 11th, and 13th\nextensions, respectively. Each component label in the output is\nencoded using a one-hot representation, enabling the model to\nfocus on specific chord components. This structured approach,\nproposed by Jiang et al [6], has been selected for improving\nthe interpretability of the classification output and facilitating\nprecise recognition of chords, even in scenarios involving rare\nextensions or complex inversions. This approach also allows a\nmore consistent comparison of the results with respect to the\ncurrent state-of-the-art methods."}, {"title": "B. ChordFormer Architecture Overview", "content": "The ChordFormer architecture integrates three main com-\nponents: a pre-processing module, a conformer block, and\na decoding model. They work together to achieve efficient\nchord recognition. The pre-processing module converts au-\ndio into a spectrogram computed by the Constant Q Trans-\nform (CQT) [37], which is more suitable for music, since\nit provides a time-frequency representation where frequency\nbins are logarithmically spaced, having higher resolution for\nlower frequencies and lower resolution for higher frequencies.\nThe CQT spectrogram is then processed by the conformer\nblock that produces frame-wise activations that capture chord\ncomponent values. By leveraging contextual frames, the con-\nformer block enhances the accuracy of the predictions. To\naddress the context-dependent nature of chord recognition,\nthe ChordFormer incorporates a modified Transformer design\nwith conformer blocks [38], [39], which enable advanced\nfeature extraction. Finally, the decoding model interprets these\nactivations to generate the final chord sequence. An overview\nof the ChordFormer architecture is illustrated in Figure 1."}, {"title": "C. Feature extraction using conformer blocks", "content": "The Conformer block, introduced by Gulati et al. [24], has\nbecome a state-of-the-art architecture for speech recognition.\nIt combines Convolutional Neural Networks (CNNs) for cap-\nturing local patterns and self-attention mechanisms for long-\nrange dependencies, making it highly effective for sequence\nmodeling tasks. A Conformer block includes a feed-forward\nmodule, a self-attention module, a convolution module, and a\nfinal feed-forward module. Sections III-C1, III-C2, and III-C3\nintroduce the self-attention, convolution, and feed-forward\nmodules, respectively. Lastly, the process of combining these\nsub-blocks is explained in Section III-C4.\n1) Multi-Headed Self-Attention: Accurate chord recogni-\ntion requires integrating information from the target frame,\nits surrounding frames, and correlated frames. Traditional\narchitectures, such as CNNs and Recurrent Neural Networks\n(RNNs), are effective at leveraging contextual information.\nHowever, self-attention mechanisms offer distinct advantages,\nmaking them particularly well-suited for this task.\nOne significant strength of self-attention is its ability to\nselectively focus on relevant frames, resulting in an adaptive\nreceptive field. Unlike CNNs, which operate with a fixed\nkernel size, self-attention dynamically adjusts its focus based\non the input. In contrast, a self-attention mechanism can\ndynamically attend to relevant frames, effectively isolating the\ntarget irrespective of its position.\nAn additional benefit of self-attention is its effectiveness\nin accurately encapsulating long-term dependencies. While\nRNNs can process distant information, they lack direct access\nto the related frames, which reduces their efficiency. CNNs\npartially address this issue by increasing the number of layers\nor kernel size, but these approaches come with inherent\nlimitations: deeper CNNs lose weight-sharing efficiency, and\nRNNs often encounter information degradation over extended\nsequences. Self-attention resolves these challenges by enabling\nimmediate access to any frame within the sequence, regardless\nof its distance from the target frame with an additional\ncomputational cost.\nThe proposed approach leverages Multi-Headed Self-\nAttention (MHSA), enhanced by the relative sinusoidal po-\nsitional encoding method a significant improvement intro-\nduced in Transformer-XL [40]. This encoding scheme en-\nhances the self-attention mechanism's generalization capabil-\nity by explicitly modeling relative spatial and temporal correla-\ntions. Consequently, the encoder can effectively contextualize\ntemporal information, resulting in a more robust representation\nof the input sequence.\nIn MHSA, the input features for each time frame are divided\ninto nh segments, corresponding to individual attention heads.\nFor an input matrix Z, the MHSA mechanism is defined as:\nMultihead = Concat(h1,..., hnn)Wo   (1)\nwhere h\u2081 represents the output of the first attention head, and\nWo is the weight matrix of the final fully connected layer\nthat projects the concatenated outputs of all attention heads\ninto the desired dimensional space.\nThe query, key, and value matrices for the j-th attention\nhead are denoted as Qj = (ZWQ)j, Kj = (ZWK)j, and\nVj = (ZWv)j, respectively. These matrices are computed by\nprojecting the input matrix Z through fully connected layers\nWQ, WK, and Wy, ensuring that each head processes a unique\nrepresentation of the input features.\nThe attention function for each head hj is defined as:\nAttention(Q, K, V) = softmax (QKT/\u221adk) V   (2)\nIn this formulation, dk represents the dimensionality of\nthe key vectors. The scaling factor \u221adk is introduced to\nmitigate instability during training by preventing excessively\nlarge values in the dot-product operation.\nThe softmax activation function normalizes the attention\nscores into probabilities, ensuring that each key-value pair\ncontributes appropriately. After concatenating the outputs from\nall nh attention heads, the result is processed through a fully\nconnected layer Wo, projecting the combined output (of size\nnh \u00d7 dv;) into the desired dimension.\nTo further enhance training stability, pre-norm residual\nconnections [41], [42] and dropout are incorporated. These\nlayers help mitigate overfitting and improve gradient flow,\nparticularly in deeper architectures. Figure 2(ii) illustrates\nthe multi-head self-attention block's structure, highlighting its\nmodular design and data flow.\n2) Position-Wise Convolution: To efficiently capture and\nutilize neighboring feature information within each time frame,\na position-wise convolutional network is employed instead of\nthe fully connected feedforward network present in the original\nTransformer architecture. Building on prior research [43],\nthe convolutional module is designed to begin with a gat-\ning mechanism [44], incorporating a pointwise convolution\nfollowed by a gated linear unit (GLU). This initialization\nis subsequently enhanced with a one-dimensional depth-wise\nconvolutional layer, further refining the module's capacity for\nfeature extraction and contextual representation."}, {"title": "D. Loss function", "content": "The Conformer block outputs a hidden vector of Me com-\nponents, one for each time frame t. To transform this hidden\nstate into meaningful chord predictions, a linear unit is applied,\nprojecting the hidden state into six separate vectors, denoted\nas St, where j = 1,2,...,6. Each vector corresponds to\na distinct component of the chord sequence, as described in\nsection III-A, which includes components like root, triad, bass,\nand extensions (7th, 9th, 11th, and 13th).\nA softmax function is then applied to these vectors to\ncompute the activation probabilities for the corresponding\nchord components. The activation probability Bt) for the\nj-th component at time t is computed as:\n\u03b2(t,j)m = exp(S(t,j)m)\n\u03a3M\nm'=1exp (S(t,j)m\u2032)\n\u2200j = 1,..., 6,\n\u2200m = 1,..., Mj (7)\nwhere Bt) represents the activation probability of class m for\nthe j-th chord component at frame t, St is the corresponding\nscore for class m, and Mj is the vocabulary size of the j-\nth chord component. This structured approach ensures that\nthe predicted activations are normalized and interpretable,\nenabling effective modeling of complex harmonic relationships\nwithin chords.\nTo optimize the model, a weighted cross-entropy loss\nis employed to account for class imbalance. Let Z =\n{Z(1),..., Z(T)} be the ground-truth chord sequence, where\nZ(t) is the chord vector at time t, and letz(t) be the ground-\ntruth class index for the j-th component at time t. Then, the\nloss function is defined as:\nL = \u2212\u03a3T\nt=1 \u03a36j=1 \u03a3Mj\nm=1 w(j)mI[m = z(t)j ] log \u03b2(t,j)m (8)\nwhere w is the weight assigned to class m of the j-th\ncomponent to address class imbalance, I[m = zt)] is the\nindicator function that equals 1 if the predicted class matches\nthe ground-truth class and 0 otherwise, and Bt) is the\nactivation probability predicted for the respective class. This\nweighted loss ensures that rare chord classes are appropriately\nconsidered during training, enabling a balanced performance\nacross all components."}, {"title": "E. Addressing Data Imbalance through Class Re-weighting", "content": "During the training phase, the issue of class imbalance\nbecame evident, even though the model does not directly\nclassify individual chords. Certain chord extensions are poorly\nrepresented in the training set, resulting in an imbalance with\nan excess of majority class samples and a shortage of minority\nclass samples. This imbalance skews the training process,\ncausing the model to prioritize optimizing performance for\nclasses with larger sample sizes while disadvantaging under-\nrepresented classes.\nAn additional critical factor exacerbating bias in automatic\nchord recognition systems is the inherent ambiguity in audio\nchord annotation. Even among human experts, determining the\n\"ground truth\" chord label for a specific audio segment often\ninvolves subjective interpretation. This ambiguity can result\nin overlapping class boundaries, generating uncertainty and\nreducing classification accuracy. Such ambiguity dispropor-\ntionately impacts classes with lower prior probabilities, further\nlimiting the overall performance.\nTo overcome these limitations, we applied a class re-\nweighting strategy that assigns a class-specific weight factor\nw for each possible value of a chord component, using\napproach presented by Jiang et al. [6]. This re-weighting\napproach mitigates class imbalance by amplifying the contri-\nbution of underrepresented classes in the loss function during\nthe training phase. The weight factor is defined as follows:\nw(j)m = min((nmmaxm\u2032n(j)m\u2032, wmax) (9)\nwhere nm represents the total number of training samples\nassociated with class m in the j-th component. The balancing\nfactor \u03b3, constrained to the range 0 \u2264 \u03b3 \u2264 1, regulates the\ndegree of weighting adjustment, with smaller values resulting\nin more equitable weight distribution across classes. Addition-\nally, wmax serves as the clamping parameter, set to wmax \u2265 1,\nwhich prevents the weight values from reaching excessively\nlarge magnitudes.\nBy adopting this weighting strategy, underrepresented\nclasses with fewer samples are assigned larger weights, effec-\ntively mitigating bias in the training process. Lower values of\n\u03b3 yield a more uniform weight distribution, whereas higher\nvalues allow the weights to more closely reflect the sam-\nple distribution. This approach ensures that infrequent chord\nclasses are adequately emphasized during training, thereby\nreducing the adverse effects of data imbalance and anno-\ntation ambiguity. The proposed ChordFormer leverages this\nre-weighting strategy to establish a more balanced training\nprocedure, enhancing its robustness and accuracy across a\ndiverse range of chord classes."}, {"title": "F. Decoding Strategy for Chord Recognition", "content": "A straightforward approach to decoding the final chord\nsequence from the activation probabilities involves selecting\nthe class that has the highest probability for each compo-\nnent at every frame. However, this method often results in\nexcessive transitions between adjacent chords, as it lacks\na mechanism to impose penalties for immediate changes.\nAdditionally, this approach provides limited control over the\noutput chord vocabulary, which can lead to inconsistencies in\nthe predictions. To overcome these challenges, ChordFormer\nincorporates a linear Conditional Random Field (CRF) for\ndecoding the final chord sequence. This technique effectively\nbalances activation probabilities with temporal smoothness,\nensuring more coherent chord transitions. The CRF models\nthe probability of a chord sequence Z given the audio features\nX using the following formulation:"}, {"title": "IV. EXPERIMENTS", "content": "A set of experiments have been conducted to evaluate the\nperformance of the proposed ChordFormer model in com-\nparison to other baseline models from the literature. This\nsection explains how data are organized and preprocessed,\ndescribes the model's training setup and the evaluation metrics\nfor assessing models' performance, and presents the achieved\nresults, highlighting the differences with respect to the other\nmodels."}, {"title": "A. Dataset Description and Preprocessing", "content": "The evaluation of the proposed model has been carried out\nusing the dataset by Humphrey and Bello [17], [26] com-\nprising 1,217 songs obtained from the Isophonics, Billboard,\nand MARL collections. To ensure a rigorous and unbiased\nevaluation, we implemented a 5-fold cross-validation strategy,\nusing the same training, validation, and testing ratio defined\nin [26]. Specifically, 60% of the dataset has been reserved\nfor training, 20% for validation, and 20% for testing. Such\na partitioning is consistent with prior studies, thus enabling\na direct and reliable comparison of results while maintaining\nmethodological integrity.\nThe CQT spectrograms were generated from the audio\ndata using the Librosa library, configured with a sampling\nrate of 22,050 Hz and a hop length of 512. The frequency\nrange extends from note C1 (inclusive) to C8 (exclusive),\nutilizing 36 bins per octave, resulting in a total of 252 CQT\nbins. After that, the spectrogram was transformed into the\ndecibel scale using Librosa's amplitude_to_db function,\nwith normalization relative to the maximum amplitude. This\nstep enhances interpretability by representing the data on\na logarithmic scale, which aligns with the human auditory\nperception of sound intensity. To enhance the model's robust-\nness, data augmentation was applied to the training set using\npitch-shifting techniques. These pitch shifts range from -5 to\n+6 semitones, generating augmented features through direct\nmodifications of the CQT spectrograms. The corresponding\nannotated chord labels were adjusted to align with the altered\naudio attributes. This augmentation strategy ensures a more\ndiverse and representative training dataset while preserving\nprecise chord annotations. By incorporating this approach,\nthe model's ability to generalize effectively across varied\nmusical contexts was significantly improved, advancing its\nperformance in music chord recognition tasks."}, {"title": "B. Training ChordFormer and Hyperparameters", "content": "Table I summarizes the hyperparameters employed for the\nConformer block, following a 5-fold cross-validation process.\nThese hyperparameters were carefully tuned to achieve the\nbest results, ensuring both robustness and accuracy in the\nmodel's performance. The AdamW optimizer was used to\ntrain the neural network, incorporating a dynamic learning\nrate schedule to enhance optimization. The initial learning rate\nwas set at 1 \u00d7 10-3 and reduced by 90% if no improvement\nwas observed in the validation loss over five consecutive\nepochs. Training concludes when the learning rate drops below\n1 \u00d7 10-6, ensuring efficient computation and minimizing\noverfitting risks.\nEvery epoch involves the random extraction of a 1,000-\nframe segment (approximately 23.2 seconds) from every song\nto introduce temporal diversity. Mini-batches were formed by\ncombining 24 such segments, effectively balancing computa-\ntional efficiency with data variability to facilitate robust model\ntraining."}, {"title": "C. Evaluation metrics", "content": "To facilitate a thorough comparison with existing models,\nthe proposed ChordFormer was evaluated using the widely\nadopted mir_eval library [48], a standard tool in automatic\nchord recognition research. The assessment includes Root\nScores, Maj-Min Scores, Seventh Scores, Thirds Scores, Triad\nScores, Tetrads Scores, and MIREX Scores. These metrics\ncollectively provide a comprehensive evaluation of chord\nrecognition performance, capturing various aspects of chord the number of correctly predicted frames in the i-th track over\nprediction.\nA key method used to measure these evaluation metrics\nis the Weighted Chord Symbol Recall (WCSR), proposed by\nPauwels and Peeters [49], which quantifies the average frame-\nlevel accuracy of chord recognition. It is defined as:\nWCSR = \u03a3N\ni=1(ZiZi) \u00d7 100 (13)\nwhere N is the total number of songs in the evaluation\ndataset, Zi denotes the total duration of the i-th song, and\nZi corresponds to the duration of correctly predicted chord\nframes for the i-th song. This metric enables a quantitative\nunderstanding of how well a model performs across different\nmusical contexts, providing a robust measure of its chord\nrecognition capabilities.\nTo evaluate the model on a large vocabulary and address\nthe challenges posed by imbalanced class, we followed the\nsame approach proposed by Jiang et al. [6] and tested it on\na vocabulary V consisting of 301 distinct chords, including\nBasic triads, Inverted triads, Seventh chords, Extended chords,\nSuspended chords, Slash chords, and a Non-chord class (\u039d).\nIn particular, the evaluation incorporates two critical metrics:\nmean frame-wise accuracy (aCCframe) and mean class-wise\naccuracy (accclass). These metrics are designed to measure\nboth the overall performance and the fairness of chord recog-\nnition across different chord classes.\nThe mean frame-wise accuracy (accframe) is defined as:\nACCframe = \u03a3n\ni=1zi\u03a3n\ni=1Zi (14)\nwhere n represents the total number of tracks in the test set,\nZi is the total number of frames in the i-th track, and zi is\nthe chord vocabulary V.\nThe mean class-wise accuracy (accclass) is defined as:\nACCclass = 1|V| \u03a3n\ni=1 \u03a3v\u2208Vzi,vZi,v (15)\nwhere Z is the number of frames in the i-th track labeled\nwith the ground-truth chord v, and z is the number of frames\ncorrectly predicted as v in the i-th track. Here, |V| denotes the\ntotal number of unique chord classes in the vocabulary."}, {"title": "V. RESULTS AND DISCUSSION", "content": "To ensure a fair and consistent comparison, the proposed\nChordFormer model and its variant ChordFormer-R (with\nreweighted loss) were evaluated against several baseline mod-\nels using the same structural chord embeddings and training\nhyperparameters. The baseline models include\n\u2022 CNN: a convolutional neural network designed for auto-\nmatic chord recognition proposed by Korzeniowski and\nWidmer [16].\n\u2022 CNN+BLSTM: a convolutional neural network that incor-\nporates a bidirectional long short-term memory (BLSTM)\nlayer to enhance sequence modeling, as proposed by\nJiang et al. [6].\n\u2022 BTC+CNN: a model based on Bidirectional Self-\nAttention, evaluated by removing the position embed-\nding layer and integrating self-attention with CNN, as\nproposed by Rowe and Tzanetakis [7].\n\u2022 Transformer: the original transformer model proposed by\nVaswani [22], along with its combination with a CNN as\na feature extractor (Transformer+CNN).\nThe WCSR results are reported in Table II and clearly\ndemonstrate the superior performance of ChordFormer and\nChordFormer-R. In fact, ChordFormer achieves a Root ac-\ncuracy of 84.69%, MajMinor accuracy of 84.09%, and a"}, {"title": "VI. CONCLUSIONS", "content": "This work introduced ChordFormer, a novel Conformer-\nbased architecture for large-vocabulary automatic chord recog-\nnition. By integrating convolutional layers with self-attention\nmechanisms, ChordFormer effectively captures both local\nspectral features and long-range harmonic dependencies,\naddressing key challenges in music information retrieval.\nThe proposed model significantly outperforms state-of-the-art\nmethods, demonstrating superior performance in both frame-\nwise accuracy and class-wise accuracy, particularly in recog-\nnizing rare and complex chord types.\nTo mitigate the long-tail distribution of chord datasets, a\nre-weighted loss function was employed, ensuring balanced\nlearning across frequent and underrepresented chord classes.\nThe results confirm that this strategy enhances the model's\ngeneralizability without severely compromising frame-wise\naccuracy.\nFurthermore, the use of structured chord representation\naligns model's predictions with music theory principles,\nthereby improving interpretability and robustness. Extensive\nevaluations on a large-scale chord recognition dataset illus-\ntrate"}]}