{"title": "Towards Optimizing and Evaluating a Retrieval Augmented QA Chatbot using LLMs with Human-in-the-Loop", "authors": ["Anum Afzal", "Alexander Kowsik", "Rajna Fani", "Florian Matthes"], "abstract": "Large Language Models have found application in various mundane and repetitive tasks includ- ing Human Resource (HR) support. We worked with the domain experts of SAP SE to develop an HR support chatbot as an efficient and effec- tive tool for addressing employee inquiries. We inserted a human-in-the-loop in various parts of the development cycles such as dataset col- lection, prompt optimization, and evaluation of generated output. By enhancing the LLM- driven chatbot's response quality and explor- ing alternative retrieval methods, we have cre- ated an efficient, scalable, and flexible tool for HR professionals to address employee inquiries effectively. Our experiments and evaluation conclude that GPT-4 outperforms other mod- els and can overcome inconsistencies in data through internal reasoning capabilities. Addi- tionally, through expert analysis, we infer that reference-free evaluation metrics such as G- Eval and Prometheus demonstrate reliability closely aligned with that of human evaluation.", "sections": [{"title": "Introduction", "content": "In recent years, incorporating Artificial Intelligence (AI) into various sectors has led to significant im- provements in automated systems, particularly in customer service and support. Since the offset of Large Language Models (LLMs), more companies are now incorporating Natural Language Process- ing (NLP) techniques to minimize the need for hu- man support personnel, especially domain experts (Shuster et al., 2021). With a chatbot providing accurate and comprehensive responses promptly, domain experts can redirect their focus towards higher-value tasks, leading to potential cost savings and improved productivity within the HR depart- ment. Moreover, an effective chatbot can play a pivotal role in enhancing overall employee satis- faction and engagement by delivering timely and relevant assistance.\nTo this end, we worked with a SAP SE on de- veloping an HR chatbot to evaluate the potential of LLMs on industrial data. We used domain experts as a human-in-the-loop through various iterations of LLM-centric development such as dataset col- lection, prompt optimization, and most importantly the evaluation of model outputs.\nThe well-known Retrieval Augmented Genera- tion (RAG) (Lewis et al., 2021) approach is ideal for this use case as it allows the model to produce more grounded answers, hence reducing hallucina- tions. We optimized different modules of the stan- dard RAG pipeline such as the retriever and model prompts, while constantly incorporating feedback from the domain experts. While the retrieval ac- curacy of an LLM could still be assessed to a de- gree, the generative nature of LLMs makes eval- uation of the generated output quite challenging. To overcome this, we explored the effectiveness of both traditional reference-based and reference-free (LLM-based) automatic evaluation metrics while using human evaluation as a baseline.\nWe benchmark OpenAI's models in our experi- ments while using the open-source LongT5 (Guo et al., 2022) and BERT (Devlin et al., 2019) as a baseline. In essence, both the industry and the research community could benefit from our find- ings related to the retriever and the reliability of automatic evaluation metrics."}, {"title": "Corpus", "content": "The dataset used in the development of the HR chat- bot was compiled using SAP's internal HR policies with the help of domain experts. While each sam- ple forms a triplet consisting of a Question, Answer, and Context, additional metadata such as the user's region, company, employment status, and applica- ble company policies were also included. A snippet of such a sample is shown in Appendix A.4. The dataset was compiled using two separate sources to have a mix of a gold dataset (FAQ dataset) and a user-utterance dataset (UT dataset). Both datasets follow the same structure and differences exist in the distribution of the questions. We extracted all unique HR articles to form a knowledge base for answering new user questions. Additionally, an evaluation set of 6k samples was used to evaluate both the retriever and the chatbot as a whole."}, {"title": "Dataset Collection", "content": "FAQ Dataset (N\u224848k): This is a collection of potential questions, along with their corresponding articles and gold-standard answers. It is carefully created and curated by domain experts based on the company's internal policies.\nUT Dataset (N\u224841k): This is a collection of real user utterances (UT) gathered from previous itera- tions of the chatbot. Inspired by a semi-supervised learning approach, a simplistic text-matching ap- proach was implemented that mapped each user query to a question from the FAQ dataset. The chatbot logs from this development cycle were in- spected and corrected by the domain experts."}, {"title": "Dataset Statistics", "content": "shows that the majority of the articles in our dataset have under 4k tokens. Hence, they can easily fit into the context window of OpenAI models. As displayed in Table 1, the most asked questions in the dataset revolve around payslips, leave days of any kind, and questions regarding management."}, {"title": "Methodology", "content": "In general, the HR chatbot follows the standard RAG pipeline with optimizations done on individ- ual modules with the help of domain experts as shown in Figure 2. The methodology illustrates var- ious parts of the chatbot pipeline that are influenced by a human-in-the-loop and is further discussed in Appendix B."}, {"title": "Retriever", "content": "We compiled a comprehensive knowledge base of all possible HR articles occurring in the whole dataset as the basis for retrieval, resulting in roughly 50k unique articles. Given a user utterance, the goal of the retriever is to find the most relevant article from the collection. While the technical de- tails for each retriever may differ, in general, they are both embedding-based. Technical details of the Retriever module are discussed in Appendix D.1.\nMoreover, we developed extensive filter func- tionalities, ensuring that the vector search only con- siders articles relevant to the user, like their country, region, or employment status as shown in Table 4. For example, from the top retrieved articles, we filter them to only keep the ones that are applicable to the employee and then pick the article with the maximum similarity score from the filtered list."}, {"title": "Dense Passage Retriever (BERT)", "content": "Dense Passage Retriever (DPR) fine-tunes bert- base-uncased embedding to generate a model that given a user query, retrieves the most relevant ar- ticle from a set of documents. The dataset used for training was processed to contain questions paired with their respective gold answers, as well as positive and negative contexts for each question. A triplet loss function (Hoffer and Ailon, 2018) was used for training such that the relevant article served as the positive context, with two random ar- ticles from the entire dataset providing the negative contexts. This retriever is used in the framework with the fine-tuned LongT5 model and also serves as a baseline for evaluating the OpenAI retriever."}, {"title": "Vector Search (OpenAI)", "content": "The OpenAI Retriever is plain vector search, that utilizes the text-embedding-ada-002 embedding model by OpenAI to generate embeddings for each article, followed by using similarity search to find the relevant article. To further enhance retrieval accuracy, we implemented various Query Trans- formation techniques\u00b9 (Cormack et al., 2009a). These methods alter the user query into a different representation using LLMs before the embedding model computes the query vector. The following three query transformation methods were explored and evaluated:\n1) Intended Topics: Inspired by Ma et al. (2023), the user question is sent to an LLM with the in- struction to return a list of three intended topics of the question, which are then embedded instead of the user question.\nExample: How to request a parental leave? \u2192 parental leave, childcare leave, maternity leave\n2) HyDE (Hypothetical Document Embeddings):\nIn this method introduced by Gao et al. (2022), the user question is transformed by an LLM into three distinct excerpts from potential HR articles answering the original question. These parts are then embedded instead of the user question itself. This approach leads to query embeddings that are very close to the article embeddings, because of the very similar content.\nExample: How to request a parental leave? \u2192 To request parental leave, please submit..., If you wish to request...,\n3) Multi-Query: This method\u00b2 employs LLMs to generate multiple variations of a user's question varying in length and phrasing but maintaining the same meaning and intent as the original question. We then embed each of these variants individually. Along with the embedded original question, we perform a vector search for each query, combining the results using Reciprocal Rank Fusion (Cormack et al., 2009b). Additionally, we include queries from the Intended Topics and HyDE methods.\nExample: parental leave request? \u2192 How can I request a parental leave?, Where can I apply for parental leave?, ..."}, {"title": "NLG Module", "content": "We use the previously optimized DPRs with the top-1 article for our NLG Module consisting of ChatGPT, GPT-4 and fine-tuned LongT5 as shown in Figure 2. An overview of all evaluation scores highlighting model performance across several di-"}, {"title": "LongT5 (Fine-tuning driven)", "content": "We fine-tuned LongT5 (Guo et al., 2022), employ- ing the local-attention-based variant\u00b3, which con- sists of 296 million trainable parameters. This model was fine-tuned on a combination of the FAQ dataset and UT dataset for a generative question- answering task. To limit computational require- ments, we fine-tuned it on a context window of 7168 tokens, retaining approximately ~86K sam- ples from the original dataset to avoid truncation."}, {"title": "OpenAI Models (Prompt driven)", "content": "We used OpenAI's ChatGPT and GPT-4 to gener- ate the answer to the user's query by passing both the user query and the retrieved article via a mean- ingful prompt. We conducted extensive prompt engineering to tailor the responses of the LLMs to the company's requirements for an HR chat- bot. Prompt engineering was an iterative process that included our qualitative analysis and multi- ple small evaluations of 10-100 sample responses by the company's HR experts who served as the human-in-the-loop. We analyzed feedback from these evaluation runs and addressed the main issues in the next iteration of the process to produce the final prompt shown in Table 5."}, {"title": "Evaluation Framework", "content": "For our analysis we employ Reference-based eval- uation metrics such as BERTScore (Zhang et al., 2019), ROUGE (Lin, 2004), and BLEU (Papineni et al., 2002). We also explore the concept of using LLM as an evaluator, and finally, we assess the effectiveness of automated metrics by involving domain experts in a human-in-the-loop process."}, {"title": "Retriever Evaluation", "content": "Our primary evaluation metric for the retriever is accuracy, defined as the percentage of times the retriever returns the correct article for a given ques- tion."}, {"title": "Human Evaluation Setup", "content": "The domain experts who served as the human-in- the-loop brought a high level of precision and in- sight to the evaluation process. Apart from dataset curation, they also evaluated the performance of the retriever by verifying the correctness of the retrieved articles. After discussion with domain ex- perts, we found four dimensions across which the quality of the model's output could be evaluated on a score between 1 - 5 following a 5-point Lik- ert (Likert, 1932) scale. One domain expert eval- uated 100 samples across the fine-tuned LongT5, ChatGPT and GPT-4 across Readability, Relevance, Truthfulness, and Usability."}, {"title": "Reference-based Metrics", "content": "In evaluating the effectiveness of reference-based metrics, we examine two distinct categories: N- gram-based and embedding-based metrics metrics. N-gram based metrics: N-gram-based metrics, such as BLEU (Bilingual Evaluation Understudy) and ROUGE (Recall-Oriented Understudy for Gist- ing Evaluation), assess the similarity between the generated response and the ground truth answer by analyzing the overlap of n-grams.\nEmbedding-based metrics: Embedding-based metrics, such as BERTScore, leverage deep contex- tual embeddings from language models like BERT to assess the semantic similarity between generated and reference texts."}, {"title": "Reference-free Metrics", "content": "In the evolving landscape of Natural Language Gen- eration evaluation, LLM-based metrics emerge as a compelling alternative, offering insights into model performance without the constraints of pre-defined reference responses. Details regarding the prompts used for these Reference-free metrics are present in Appendix C.\nPrompt-based Evaluation: Prompt-based eval- uation is at the forefront of NLG advancements, particularly with the utilization of LLMs (Li et al., 2024). Inspired by G-Eval, we followed the ap- proach described by Liu et al. (2023) and tailored the prompts to be suitable for the evaluation of a question-answering task.\nTuning-based Evaluation: Nowadays, there is a significant shift toward leveraging open-source lan- guage models, such as LLaMA (Touvron et al., 2023), for fine-tuning purposes. We utilize Prometheus (Kim et al., 2023), which stands out for its fine-tuned evaluation capability, leveraging a large language model to perform nuanced anal- ysis based on customized score rubrics (Li et al., 2024). This unique approach enables Prometheus to evaluate text generation tasks comprehensively, considering factors such as creativity, relevance, and coherence without relying on reference texts."}, {"title": "Results and Discussion", "content": "We use the previously optimized DPRs with the top-1 article for our NLG Module consisting of ChatGPT, GPT-4 and fine-tuned LongT5 as shown in Figure 2. An overview of all evaluation scores highlighting model performance across several di-"}, {"title": "Dense Passage Retriever", "content": "As depicted in Table 2, surprisingly the BERT- based DPR significantly outperforms all new meth- ods with a top-1 accuracy of 22.24%, whereas the OpenAI-based retriever only reaches a top-1 accu- racy of 11.12%. Of the latter, the best performer is Multi-Query, with 10.92%, yet this still falls short of the Basic retriever (no query transforma- tion). These results resonate with the findings of Weller et al. (2024), confirming that query transfor-"}, {"title": "NLG Evaluation", "content": "mensions is summarized in Table 3.\nOverall, GPT-4 shows clear domination in terms of generation capabilities for an HR chatbot. N-gram- based evaluation scores such as ROUGE and BLEU are quite low due to the generative nature of the (L)LMs, as the answer may contain words differ- ent than the reference answers. Nonetheless, these results establish GPT-4 as the leading model, ef- fectively combining advanced language skills with the demands of content accuracy and user engage- ment. On the other hand, the fine-tuned LongT5's performance is observed to be inferior when bench- marked against the OpenAI models. This outcome is consistent with the anticipated advancements in LLMs, which are progressively outpacing the capa- bilities of fine-tuning-driven models. The perfor- mance of ChatGPT has been notably strong, trail- ing marginally behind GPT-4 in only a few scoring categories. Its close performance to GPT-4 raises important considerations for the trade-offs between computational efficiency and output quality."}, {"title": "Correlation Analysis", "content": "Inspired by Zhong et al. (2022), we assessed the reliability of the evaluation score using Spearman (Myers and Sirois, 2004) and Kendall (Abdi, 2007) correlation coefficients in Table 9.\nHuman Evaluation & Reference-based Metrics Due to its limited innovation, LongT5 typically produces text with fewer novel sentences, result- ing in more favorable scores from n-gram-based metrics like BLEU and ROUGE. The analysis of GPT-3.5 and GPT-4, in particular, illuminates a sig- nificant gap between automated metrics and human judgment. As these models generate more varied and longer sentences, their outputs increasingly di- verge from the patterns recognized by word-overlap metrics, such as BLEU and ROUGE. For instance, GPT-4's BLEU score correlation marks a clear dis- connect, indicating that as text generation becomes more complex, the less effective traditional metrics are in evaluating it. This discrepancy calls into question the reliance on current automated metrics for assessing the creativity and nuance of outputs from advanced language models, highlighting the need for more sophisticated evaluation frameworks that can better align with human judgment.\nHuman Evaluation & Reference-free Metrics Despite similar average scores between Reference- free metrics and Domain Expert evaluations shown in Table 3, their correlations are low. Since these methods measure linear and ordinal relationships, similar averages in evaluations do not imply a strong correlation as depicted in Table 9.\nOverall, while Prometheus and G-Eval both serve as proxies for human evaluation, their ef- fectiveness varies by model and evaluation criteria. While G-Eval excels in assessing truthfulness, its capability in evaluating readability and usability lags behind. Prometheus on the other hand, out- performs G-Eval in assessing usability across all models. However, G-Eval shows a steadier perfor- mance across different models, particularly with LongT5, suggesting its robustness in accurate eval- uations. Both metrics show weak alignment in assessing readability, reflecting the inherent chal- lenge of one LLM evaluating another's ability to produce easily understandable text.\nAdditionally, LLM-based metrics sometimes fail to align with human judgment, particularly when an- swers or instructions involve unfamiliar HR terms or sensitive information. Notably, OpenAI mod- els' novel answers exhibit lower human correla- tion compared to LongT5, which provides answers more similar to the golden response."}, {"title": "Related Work", "content": "Previously, domain-specific chatbots meant for a specific task were designed using conversational AI frameworks like RASA (Bocklisch et al., 2017). Latest advancements in NLP have shifted focus to- wards employing and optimizing LLM-based RAG (Gao et al., 2024b). Chen et al. (2023) experi- ment with ChatGPT and several other open-source models like Vicuna to benchmark their capabili- ties in RAG, and Wang et al. (2023) use a smaller secondary domain-specific model to assist a big- ger LLM on a domain-specific question answering task on industrial data. Recent studies have ex- plored various retrieval methods, including dense vector retrieval (Karpukhin et al., 2020a), sparse retrieval (Robertson et al., 2004, 2009), and hybrid approaches (Guu et al., 2020a), to improve the rel- evance and diversity of retrieved documents. Guu et al. (2020b) uses various RAG techniques to en- sure that chatbot responses are based on relevant HR policies, leading to accurate and helpful user support.\nGiven the diverse distribution of the text gener- ated by LLMs, conventional metrics are not suit- able for its evaluation (Wei et al., 2021; Belz and Reiter, 2006; Novikova et al., 2017). Consequently, a lot of follow-up research has come up in the area of NLG Evaluation (Gao et al., 2024a; Li et al., 2024). Specifically focusing on RAG, Es et al. (2024) released a Framework for the automatic evaluation of generated output using LLM-based metrics with a focus on faithfulness. A similar ap- proach is followed by Saad-Falcon et al. (2023) in their framework ARES which also evaluates the performance of RAG systems over relevance and faithfulness by fine-tuning a lightweight LM judge."}, {"title": "Conclusion", "content": "By optimizing retrieval techniques and benchmark- ing state-of-the-art LLMs with the help of domain experts, we show how LLM-based applications could benefit from a domain expert as human- in-the-loop within various iterations of the devel- opment. Even though our optimizations on the OpenAI-based retriever show minor improvements, the accuracy remains quite low due to the poor quality of the evaluation dataset. Nonetheless, both ChatGPT and GPT-4 show competence when ad- dressing the user query. This hints that the in- ternal reasoning capabilities and domain knowl- edge of these LLMs are strong enough to over- come the knowledge in the supposed incorrect ar- ticle. This also suggests that, given the nature of the dataset used, the accuracy metric used for the evaluation of the retriever is not a good measure of its performance. We employed and studied a range of evaluation metrics and concluded that in contrast to traditional evaluation approaches such ROUGE & BERTScore, LLM-based metrics such as Prometheus and G-Eval come very close to hu- man evaluation on average. Nonetheless, our find- ings reiterate the importance of human judgment, particularly in use cases that require an understand- ing of a specific domain."}, {"title": "Acknowledgements", "content": "The work outlined in this paper is part of a re- search project between the Technical University of Munich and SAP SE under SAP@TUM Col- laboration Lab. The authors would like to thank Patrick Heinze, Christopher Pielka, Albert Neu- mueller, Darwin Wijaya from the SAP IES as well as the Domain Experts from the Human Resource department for their continued support."}, {"title": "Limitations", "content": "In our experiments, we mostly worked with Ope- nAI models which are closed-source and hence raise concerns of privacy. Additionally, their large sizes inhibited fine-tuning as they required exten- sive hardware. Fine-tuning open source and smaller models tailored to HR-specific contexts could fur- ther improve response accuracy and relevance. Ad- ditionally, since we worked with only one domain expert for the evaluation of the generated answers, the human evaluation might be biased. Because of the data protection concerns with the associated dataset, we cannot make the dataset open source. We employed basic filtering techniques to include user-specific information and context, more ad- vanced approaches could be explored to include this information into the LLM prompt."}, {"title": "Ethics Statement", "content": "Throughout our experiments, we strictly adhere to the ACL Code of Ethics. The dataset used for our research was anonymized to not include any per- sonal information. We employed in-house domain experts, who receive a full salary for evaluation for generated summaries. They were informed about the task and usability of data in the research. Their annotations were stored in an anonymized fashion, mitigating any privacy concerns. Through our fine-tuning strategies, no additional bias was introduced into the models, other than what might already be part of the dataset. The goal of the research was to optimize an LLM-centric chatbot with the help of a human-in-the-loop. The results and discus- sions in this paper are meant to further promote research in LLM-based development, bridging the gap between academia and application."}, {"title": "Dataset", "content": "FAQ Dataset: The internal HR policies of the company consist of Wiki articles, where each ar- ticle contains a description text followed by some frequently asked questions. The FAQ dataset was constructed by the domain articles by compiling all the FAQ questions from all articles. Each FAQ question is in the form of a triplet where the con- text is the original Wiki article the question was de- rived from. UT Dataset: The user utterance (UT) dataset was compiled using the user utterances col- lected from the chatbot logs. To reduce the manual labeling effort, a simple text-matching approach was deployed that mapped each user query to one of the questions from the FAQ dataset. The respec- tive answers and context of the matched question were used to create the triplets that form the UT dataset."}, {"title": "Dataset Pre-processing", "content": "We cleaned the dataset using regular expressions and with the help of LLMs. This involved remov- ing unnecessary formatting like HTML tags, lead- ing or trailing white spaces and newline characters, and removing some wasteful markdown annota- tions without text. This process thus reduced the number of tokens in each document. Some of the documents were too long to fit into the LLM's context window, so we excluded them from our analysis."}, {"title": "Dataset Challenges", "content": "We discovered that our dataset contains multiple ar- ticles answering most questions. These articles dif- fer in a few characters, often in an unequal amount of whitespaces, or a few exchanged words, or even entire sections not present in other articles. This sit- uation leads to multiple slightly different versions of the same article present in the dataset, all linked to similar questions. Consequently, the retriever often retrieves very relevant articles that do not exactly match the gold standard article but are a slightly different version.\nTo address this, we implemented an evaluation method measuring the Levenshtein distance be- tween the retrieved article and the gold article. If this distance is below a threshold of 100, we con- sider it a successful retrieval. However, this ap- proach does not match articles with varying sec- tions, as the Levenshtein distance is much higher, and we didn't want to risk matching incorrect arti- cles by increasing the threshold. All of the results in Table 2 are using this evaluation method.\nAs the DPR is fine-tuned on the dataset, which likely has a strong imbalance in the counts of dif- ferent article versions, it tends to favor the most common version. This bias contributes to its higher accuracy, as the retriever fetches the correct article more often than not."}, {"title": "Dataset Example", "content": "shows an example sample from the FAQ dataset representing the training triplet along with all metadata."}, {"title": "Human-in-the-Loop", "content": "As shown in Figure 2, the domain experts are in- volved in various parts of the development cycle explained below:\nDataset Collection: The domain experts play a big role in the compilation and quality control of the datasets used in this paper\nPrompt Optimization: The domain experts eval- uated answers generated by models on various prompt versions. They also provided guidelines the chatbot should follow when addressing the user query which is reflected in the final prompt dis- played in Table 5.\nEvaluation: Domain experts also served as the human annotators for the answers generated by"}, {"title": "Technical Details", "content": "It is worth noting that we embed the whole arti- cle and do not perform chunking. As shown in Figure 1, these articles are quite long. To cater to the limited context window of the models, we opt for the top-1 article to be passed as context. This also makes sense for our use case as the dataset is designed such that the answer to any given HR question usually exists in only one article."}, {"title": "Dense Passage Retriver Training", "content": "Dense Passage Retriever (DPR) (Karpukhin et al., 2020b) powered by Haystack4 uses the bert-base- uncased embedding model by google-bert, openly available on HuggingFace. DPR training aims to generate a model that creates embeddings where the question embedding closely aligns with the rel- evant context embedding. During retrieval, the user query is processed through the previously trained retriever, producing a query vector in the same em- bedding space as the articles. This query vector is then compared to all article vectors within the vector store using cosine similarity. The top-k arti- cles belonging to the embeddings with the highest cosine similarities are returned."}, {"title": "LongT5 Fine-tuning", "content": "During fine-tuning of the LongT5 models, the train- ing process was configured with a learning rate of 1e-4 and a batch size of 8, spanning 5 epochs."}, {"title": "Results and Evaluation", "content": "Throughout our research, we encountered several challenges that warrant attention. The variability in retrieved articles due to slight differences in con- tent or formatting posed complexities in evaluating retrieval accuracy and ensuring consistency in re- sponse generation. Addressing this challenge may require further refinement of the retrieval mecha- nism or additional preprocessing steps to standard- ize the retrieved content."}, {"title": "Retriever", "content": "The accuracy of both DPR on the top-1, top-2, top- 3, and top-5 articles on both retrievers is shown in Table 8. As expected, the accuracy of the retriever module increases as the value of k is increased. However, we are limited to including only top-1"}, {"title": "Correlation between Automatic Evaluation and Domain Expert Evaluation", "content": "shows the individual across for correlation of each evaluation metric with human evaluation across LongT5, ChatGPT, and GPT-4. The low correlation coefficients are a consequence of the Spearman and Kendall methods, which analyze the linear and ordinal relationships between vari- ables by comparing each set of scores. When these methods detect divergent scores between two eval- uations, it leads to a reduced correlation coefficient, indicating a disproportion that is not apparent when considering the average scores alone."}]}