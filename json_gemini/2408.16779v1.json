{"title": "Inductive Learning of Logical Theories with LLMs: A Complexity-graded Analysis", "authors": ["Jo\u00e3o Pedro Gandarela", "Danilo S. Carvalho", "Andr\u00e9 Freitas"], "abstract": "This work presents a novel systematic methodology to analyse the capabilities and limitations of Large Language Models (LLMs) with feedback from a formal inference engine, on logic theory induction. The analysis is complexity-graded w.r.t. rule dependency structure, allowing quantification of specific inference challenges on LLM performance. Integrating LLMs with formal methods is a promising frontier in the Natural Language Processing field, as an important avenue for improving model inference control and explainability. In particular, inductive learning over complex sets of facts and rules, poses unique challenges for current autoregressive models, as they lack explicit symbolic grounding. While they can be complemented by formal systems, the properties delivered by LLMs regarding inductive learning, are not well understood and quantified. Empirical results indicate that the largest LLMs can achieve competitive results against a SOTA Inductive Logic Programming (ILP) system baseline, but also that tracking long predicate relationship chains is a more difficult obstacle than theory complexity for the LLMs.", "sections": [{"title": "Introduction", "content": "The integration of Large Language Models (LLMs) with formal methods stands out as a promising frontier in the field of Natural Language Processing. It is an avenue for improving model inference control and explainability, by both complementing the content flexibility of Large Language Models (LLMs) with the systematicity of symbolic/formal systems (Quan et al., 2024a,b) and by using well-defined formal settings to assess the underlying inference properties of the model.\nInductive Logic Programming (ILP) is a subfield of symbolic Al which focuses on methods that can derive (generalise) theories to explain observed facts and rules (Muggleton, 1991; Nienhuys-Cheng and de Wolf, 1997). Addressing inductive learning over complex sets of facts and rules, poses unique challenges for current autoregressive LLMs, as they do not operate over data symbolically, rather combining an extensive set of structural an semantic signals to approximate the most probable answer in a generative fashion.\nWhile such means of problem solving might lack explicit symbolic grounding, LLMs can leverage its large-scale internal representation to support inductive-style inference. Still, the properties delivered by LLMs w.r.t. inductive learning, in particular regarding logic rules and theory induction, are not well understood and quantified.\nThis paper presents a systematic methodology to evaluate the inductive learning properties (in the context of logic theory induction) of LLMs. It is aimed at answering the following research questions (RQs):\nRQ1. To what extent the combination of an LLM with feedback from a formal inference engine can compare to a SOTA inductive logic programming (ILP) system in logic theory induction, w.r.t. inference quality at different degrees of complexity?\nRQ2. How does the complexity of the target theories affect inference quality of LLMs for inductive reasoning?\nIn order to address these RQs, we propose a method for combining iterative theory refinement on stock LLMs (i.e., zero-shot inference), a formal ILP inference engine and a synthetic generator for inductive reasoning datasets, in order to perform systematic evaluations of the LLM induced theories, using state-of-the-art ILP solvers as a baseline. Moreover, to quantify the extent in which LLMs can address ILP tasks, the evaluation is graded wrt. the dependency complexity of the target rulesets.\nThis work's main contributions are as follows:\n1. (Methodological) A novel method for system-"}, {"title": "Inductive Learning, Complexity & Datasets", "content": "In this section we introduce the target task and the typology of inductive learning complexity classes, as well as the dataset generation process."}, {"title": "Inductive Logic Programming", "content": "Inductive Logic Programming main objective is to generate logical hypotheses or theories from the available background knowledge, such as facts, rules, and positive and negative examples. Unlike traditional machine learning methods, ILP works directly with logical predicates and rules, using a formal representation that is usually represented using first-order logic (FOL). For example, the fact \"parent(john, tom).\" means that John is the parent of Tom, and \u201cparent(john, anne).\u201d means that John is the parent of Anne. From that, we can create a rule (theory) \u201csibling(X, Y) :- parent(P, X), parent(P, Y), X \u2260 Y.\" . This rule states that if there exists a parent who has both X and Y as children, and X and Y are not identical, then X and Y are considered siblings. Deriving rules from a set of examples is a process known as theory induction. This task can be formally defined as follows:\nGiven: background knowledge (BK), a set of logical clauses that represent prior knowledge about the domain a set of positive examples ($\\mathcal{E}^+$), a set of ground facts (instances) which the learned theory should entail (i.e., these are examples that should be true according to the theory), a set of negative examples ($\\mathcal{E}^-$), a set of ground facts which the learned theory should not entail (i.e., these are examples that should be false according to the theory). Find a hypothesis H (a set of logical clauses) such that:\nCompleteness: For every example $e \\in \\mathcal{E}^+$, $H\\cup BK \\models e$, meaning the hypothesis H, together with the background knowledge BK, should entail all positive examples.\nConsistency: For every example $e \\in \\mathcal{E}^-$, $H\\cup BK \\not \\models e$, meaning the hypothesis H, together with the background knowledge BK, should not entail any negative examples.\nFormally, an ILP system seeks a hypothesis H that satisfies:\n$\\forall e \\in \\mathcal{E}^+, BK \\cup H \\models e$\n$\\forall e \\in \\mathcal{E}^-, BK \\cup H \\not \\models e$\nThe learned hypothesis H should thus be a logical theory that explains the positive examples and excludes the negative examples, based on the given background knowledge."}, {"title": "Theory complexity", "content": "Inductive learning can be organised according to different classes of complexity, which involves a typology of the structural complexity of the problem. Moreover, variables such as the amount and type of noise within the evidence set (such as the number of incorrect or missing facts or completeness levels) can be integrated within this setting. Following the typology of (Cornelio and Thost, 2021) four base categories of rules can be introduced based on their dependencies: Chain, Rooted Directed Graph (RDG), Disjunctive Rooted DG, and Mixed. Each category represents a hierarchical generalisation, with each step encompassing a broader range of structures. Starting with CHAIN, which represents a linear composition of predicates, RDG is a generalisation of CHAIN, meaning it includes all chain structures but also accommodates more complex dependencies. Moving up, DRDG generalises RDG by incorporating directed relationships, thus offering a more extensive representation of dependencies. Finally, Mixed contains connected components from CHAIN, RDG, and DRDG. Each progression from CHAIN to MIXED represents a step towards greater inclusivity and complexity in the types of structures captured."}, {"title": "Dataset synthesis", "content": "In order to generate datasets for rigorous analysis, this study employed the RuDaS tool (Cornelio and Thost, 2021) to systematically vary parameters such as noise, open-world degree, and missing data. By adjusting these factors in conjunction with the category parameter, it is possible to ensure comprehensive coverage of different structural configurations and complexity levels.\nFor each configuration, the following settings were applied:\n\u2022 The minimum and maximum number of Directed Acyclic Graphs (DAGs) were both set to 1 (mindags = 1, maxdags = 1).\n\u2022 Noise levels were systematically varied at intervals of 0.1, 0.2, and 0.3.\n\u2022 The percentage of missing data (missing) and open world degree (owa) were similarly varied across 0.1, 0.2, and 0.3.\n\u2022 The category parameter was set to cover all the complexity classes described in the previous section, and listed in Table 1. The distribution of each category in the synthesised dataset is an independent hyperparameter, discussed in Section 4.\nFurther details regarding the dataset generation can be found in Appendix E."}, {"title": "Proposed Approach", "content": "The proposed approach can be divided in two parts: iterative refinement and graded evaluation. They form a systematic evaluation loop, covering all the complexity classes described in the previous section, for a given set of LLMs and dataset synthesis parameters."}, {"title": "Iterative Refinement", "content": "Consists of an iterative refinement loop that alternates between the generation of a theory by a language model and the evaluation of said theory through a formal interpreter. It is comprised of the following components, as illustrated in Figure 1:\n(a) A language model capable of generating a theory H, based on background knowledge, positive and negative examples, and hypothesis search, provided as a prompt text in a logic program language. Typically an LLM with structured (e.g., code) generation capabilities. (b) A logic program interpreter. We use Prolog (Warren et al., 2023) as the logic program language. (c) A prompt generation component, that interleaves logical programming language with natural language queries designed to drive the theory induction responses. The logical programming language expresses background knowledge and the relevant outputs of the program interpreter. (d) An evaluation module, that uses the logic program interpreter to execute the generated theory H as logical rules, and computes a set of evaluation metrics.\nGiven: background knowledge (BK), positive examples ($\\mathcal{E}^+$), negative_examples ($\\mathcal{E}^-$), and assuming a language model LM which can find a hypothesis H (a set of logical clauses) such that it satisfies the conditions of completeness (for every example $e \\in \\mathcal{E}^+, H\\cup BK \\models e$) and consistency (For every example $e \\in \\mathcal{E}^-, H\\cup BK \\not \\models e$).\n1. Context Representation: Represent the input to the language model as a combination of background knowledge and examples: Context = encode(BK, $\\mathcal{E}^+$, $\\mathcal{E}^-$).\n2. Theory Generation: From a background knowledge set of clauses sampled from a knowledge base dataset, including positive and negative examples, a prompt is created for the LM to induce a theory as Prolog code, i.e. using the language model to generate a set of logical clauses (hypothesis H): H = LM(Theory Prompt + Context).\n3. Evaluation of Hypothesis: Checking for the completeness and consistency conditions:\nTrue Positives (TP): The number of positive examples correctly entailed by the hypothesis.\nTP = |{e \u2208 $\\mathcal{E}^+$ | BK \u222a H $\\models$ e}|\nFalse Positives (FP): The number of negative examples incorrectly entailed by the hypothesis.\nFP = |{e \u2208 $\\mathcal{E}^-$ | BK \u222a H $\\models$ e}|\nFalse Negatives (FN): The number of positive examples not entailed by the hypothesis.\nFN = |{e \u2208 $\\mathcal{E}^+$ | BK \u222a H $\\not \\models$ e}|\nTrue Negatives (TN): The number of negative examples correctly not entailed by the hypothesis.\nTN = |{e \u2208 $\\mathcal{E}^-$ | BK \u222a H $\\not \\models$ e}|\nfrom which precision (P) ($$\\frac{TP}{TP+FP}$$), recall (R) (Recall = $\\frac{TP}{TP+FN}$) and F1-score (F1) (F1 = $\\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision}+\\text{Recall}}$) can be generated.\n4. Theory refinement: Following an initial evaluation, the LM is tasked to refine the induced theory iteratively. Each refinement round involves adjusting the theory based on feedback from the Prolog interpreter validation. The refinement aims to improve the theory's performance by addressing misclassifications and enhancing its predictive capabilities. If H does not satisfy completeness and consistency, update the input representation based on feedback and generate a new hypothesis using the language model, given the Feedback Context \u2190 {FP, FN, P, R, F1} and the final prompt input Input \u2190 (Refinement Prompt + Context + Feedback Context): H \u2190 LM(Input).\nThe main loop of the algorithm continues until the evaluation metrics meet the defined thresholds or the maximum number of iterations is reached. In each iteration, the language model generates a theory based on the current prompt. This generated theory is then evaluated using the logic program interpreter, in our case Prolog, resulting in a validation set. Evaluation metrics are computed from these validation results and stored. Based on the feedback from the validation, a new prompt is generated by incorporating the initial knowledge base sample, the current theory, and the validation feedback. Our approach removes recursive rules from the LLM-induced theory before evaluation. The refinement loop is summarised in Algorithm 1. The process starts by sampling facts from the knowledge base dataset to create kb. An initial prompt is then generated using these sampled facts, denoted as prompt.\n5. Termination: The process continues iteratively until a maximum number of iterations is reached."}, {"title": "Graded evaluation", "content": "A synthetic data generator is used to control for the input parameters of the ruleset complexity, namely: categorical distribution (CHAIN, RDG, DRDG, etc.), background knowledge, positive examples and negative examples as well as the amount of noise introduced within the dataset.\n1. Categorised Learning Sets: Consisting of C: set of ruleset complexity categories (e.g., CHAIN, RDG, DRDG, etc.), N: set of noise levels, S: number of samples per combination of C and N. For each c\u2208 Cand n \u2208 N, generate S datasets {Dc,n,i | i = 1, ..., S} where each dataset Dc,n,i includes:\nDc,n,i = (BKc,n,i, $\\mathcal{E}_{c,n,i}^+$, $\\mathcal{E}_{c,n,i}^-$, noisec,n,ic,n,i)\n2. Hypothesis Generation and Evaluation: For each dataset Dc,n,i, use a learning algorithm to generate a hypothesis Hc,n,i, tracking the F1 score Fc,n,i and processing time Tc,n,i at each iteration and recording the best F1 score F1c,n,i and corresponding processing time Timec,n,i:\nF1c,n,i = max(Fc,n,i)\nTimec,n,i = time until max(Fc,n,i)\n3. Aggregation: The information is then aggregated by complexity category and noise level for all the samples, averaging times and F1 scores, to obtain the complete graded evaluation statistics. For each combination of c\u2208 C and n \u2208 N, compute the average F1 score and average processing time:\nF1c,n = $\\frac{1}{S}\\sum_{i=1}^{S} F1_{c,n,i}$\nTimec,n = $\\frac{1}{S}\\sum_{i=1}^{S} Time_{c,n,i}$"}, {"title": "Experiments", "content": "In order to answer the research questions, a set of experiments was elaborated to systematically analyse the theory inducing capabilities of a set of most popular open-source LLMs and two versions of GPT, with the proposed approach, having the state-of-the-art ILP system Popper (Cropper and Morel, 2021) as a baseline. The tests covered all data categories discussed on Section 2, allowing a graded analysis w.r.t. the expected level of induction complexity and tolerated noise."}, {"title": "Experimental Setup & Dataset", "content": "For each data category, five datasets were generated using RuDaS (Cornelio and Thost, 2021). The size of each dataset was set to XS (min = 50, max = 100, support = 3), and noise, missing, and open world were all set to 0.1, then all set to 0.2, and finally all set to 0.3. This resulted in 105 datasets in total, with 35 datasets for each rate. Subsequently, two methods are used to induce a theory for each dataset: (1) employing Popper, with NuWLS (Chu et al., 2023) and WMaxCDCL, varying its time limit parameter from 10 to 800 seconds; (2) applying the proposed iterative LM theory refinement method (Section 3), with parameters Maxiter = 4 and MTthresh = 1.0. Three different LLM models were used for (2): Open AI's model GPT-40, Mistral AI's Mixtral-8x7B (Jiang et al., 2023), and Google's Gemma (Team et al., 2023).\nTable 2 presents a comprehensive overview of statistical metrics pertaining to each category of data, in order of complexity (except MIXED).\nWe computed the average F1-score for each category, taking into account the level of noise, open"}, {"title": "Results & Discussion", "content": "Overall results for F1 are presented in Figure 2. We additionally report on processing times as a measure of practical interest in Figure 3. We present the values obtained in detail in the supplementary material (Appendix B, in Tables 4 and 5). Gemma-7B-It results are not included as it failed to generate valid theories. The results reveal significant insights into LLM capabilities and limitations w.r.t. theory induction, which are summarised as follows:\nLLMs can achieve competitive performance against the baseline, specially at higher noise levels. The larger scale models (GPT3.5, 4) demonstrate more resilience to noise and consistent F1 across the different categories, as indicated in Figure 2, with an average F1-score difference of \u00b10.25 against Popper. This answers the overall quality part of RQ1.\nInducing theories on long relation chains is the major obstacle for LLMs, rather than dependency complexity. With the CHAIN category being the least complex and one of the most consistently solved by Popper, but none of the tested LLMs was able to overcome the baseline performance at all noise levels (Figure 2 CHAIN category). This suggests that such models have a limited capacity of tracking long relationship chains of independent predicates. A part of RQ2 answer.\nIncreasing iteration limits does not monotonically improve results for LLMs. Upon increasing the iteration limits from 1 to 4, it was found that the metrics can either increase or decrease non-monotonically. Thus Maxiter was set to 4 and the iteration with the best accuracy is taken as the final result.\nPerformance is remarkably lower on complex rule sets at moderate noise levels. Responses for complex categories, such as RDG and DRDG display higher variance and LLM hallucination artifacts, such as valid rules containing predicates that do not exist in the rule set. We present an error analysis in Section 4.3. For instance, a comparison of the results for the RDG category generated by GPT-40 under noise levels set to 0.1 and 0.3 reveals a significant decline in performance, with the F1-score dropping from 0.75 to 0.22. A comparable pattern is observed with GPT-3.5 Turbo for RDG and DRDG and Mixtral RDG in the presence of elevated noise levels, with GPT-3.5 Turbo scores going from 0.56 to 0.0, 0.28 to 0.08, and Mixtral-8x7B going from 0.60 to 0.0.This complements the answer to RQ2. Further details are included in the supplementary material (Appendix B).\nInduction capability varies substantially across models. Using the same inputs resulted in vastly different responses from different models, suggesting critical influence from model size in parameters. Figure 2 illustrates this: When comparing GPT-40, Mixtral-8x7B and Llama3 at noise levels set to 0.1 and 0.3 respectively, the consistency in generating a valid theory correlates to their relative size.\nAt a noise level of 0.1, GPT-40's F1 score is almost twice that of the GPT-3.5-Turbo in average, and at a noise level of 0.3, the difference increases to a ratio of 4, indicating substantially higher noise resiliency. The performance gap is more pronounced when comparing with Llama3-8B, where GPT-40 F1 score is 21 times higher at the lowest noise setting.\nMixtral-8x7B-It-v0.1 performs similarly to GPT-3.5-Turbo at lower noise levels, scoring 13.4% higher in average at 0.1 noise. However, its performance becomes less stable at higher noise levels. It consistently outperforms Llama3-8B-it, at 0.1 noise, with a F1-score 11 times higher in average.\nModel size does not correlate to noise resilience Despite being able to achieve higher scores than GPT-3.5 and Mixtral-8x7B in some of the tests (e.g., RDG-R @noise = 0.1, CHAIN-R @noise = 0.2) and scoring higher on intermediate noise than on low noise, Llama3-8B did not consistently generate valid theories. On the other hand, Mixtral-8x7B, a much larger model, is particularly susceptible to noise, with an average F1-score drop of over 0.8 from noise = 0.1 to noise = 0.2 and a monotonic performance reduction with the increase of the noise level.\nRegarding the parameterisation of each method, some higher-level observations on the trade-off between time and inference quality can be summarised as follows:\nComputational scalability allow LLMs to operate at substantially lower times. While Popper is a serial algorithm, the parallel nature of transformer-based LLMs allows them to operate at times about 3 orders of magnitude lower, given enough computational resources. This can be observed in Figure 3, for all complexity classes and all tested noise intervals."}, {"title": "Error analysis", "content": "The errors found in the evaluation of the generated theories could be separated in two categories: syntactic and logical.\nSyntactic errors occur when the generated response does not match the logic programming language grammar. For example, the following response:\ntheory :-\np(X, Y), pos (p0(X, Y)) - positive.\np(X, Y), neg(p0(X, Y)) - negative.\n\\+ p(X, Y), pos(p0(X, Y)) false.\n\\+ p(X, Y), neg(p0(X, Y)) true.\nis not valid Prolog and will fail evaluation.\nLogical errors occur when the generated response has correct grammar, but cannot be induced from the examples. Consider the following Prolog theory:\ntheory :-\np(X, Y):- p1(X, Y); p3(X, Y);\np4(X, Y); p7(X, Y); p8(X, Y);\np0(X, Y),\nnot neg(p(X, Y)),\n(pos(p(X, Y)) true; fail).\nThe response contains the head of the clause \"theory,\" as well as the predicates \"p\" and \"pos\", which do not exist in the BK. Table 3 presents a distribution of error categories for the analysed models. A more detailed analysis of the models outputs is included in the supplementary material (Appendix G)."}, {"title": "Related Work", "content": "Neural symbolic computation combines neural networks with symbolic methods to enhance AI reasoning capabilities. (Yang et al., 2017) introduced in handling composition tasks. Their results show that, despite their strengths, transformers face significant challenges in dealing with compositionality, which involves understanding and generating complex structures from simpler components. This limitation highlights the need for innovative approaches, such as self-refining, to further enhance the capabilities of machine learning models.\nIn contrast, our work focuses on the still under-explored area of assessing and controlling inductive learning/inference capabilities of LLMs. These contributions integrate LLMs and formal logic for robust theory induction and allows a graded analysis of LLM capabilities, with respect to theory induction complexity."}, {"title": "Conclusion", "content": "In this study we thoroughly investigate the integration of state-of-the-art formal theory induction within the context of large language models (LLMs), aiming to elucidate the extent in which LLMs can systematically perform inductive learning for theories spanning across different complexity levels. At the heart of this exploration lies the recognition of relational data's inherent semantic depth, stemming from its symbolic representations. The empirical results presented here have indicated the ability of LLMs to address inductive learning tasks, with the largest LLMs achieving competitive results against the algorithmic SOTA with better tolerance to higher noise levels, which can be attributed to their semantic flexibility. This flexibility however has certain limitations, as we found that tested language models are more limited by their capacity of tracking long relationship chains of independent predicates than by the dependency complexity of the rule sets (disjunctiveness, recursivity).\nAs future work we plan to utilise larger datasets to test the scalability of the proposed approach, allowing researchers to assess its performance across a broader range of scenarios. Additionally, it is worth considering the integration of the LLM's output as an initial input for the ILP process, potentially leveraging the strengths of both approaches to overcome their respective limitations. Another avenue, is the use of ILP middle steps, such as the bottom clause, to help LLM induce a theory."}, {"title": "Limitations", "content": "While the proposed evaluation methodology aims to cover a wide range of logic theory induction complexity, it is limited in its resolution to the categories specified by (Cornelio and Thost, 2021), and does not quantify other ruleset characteristics, such as workspace size or unification rate in the case of Prolog (Dikovsky, 1993).\nThe methodology compares all models under the same inputs. Therefore it is not concerned with extracting maximum performance of any given model, but obtaining a relative assessment of their fundamental capabilities. This means that the empirical analysis reported scores should not be taken as a measure of SOTA performance.\nFurthermore, the time gains demonstrated in the experiments are presented as an achievable result, conditioned to the combination of software and hardware indicated in the paper and the services provided by third-parties (e.g., OpenAI). They should not be interpreted as a measure of computational efficiency."}, {"title": "Further theoretical background", "content": ""}, {"title": "Detailed Complexity Classes", "content": "Category Chain. In this category, every rule, except the root, deduces facts relevant to precisely one other rule. Essentially, each node has at most one parent, and each rule is associated with at most one other rule that might infer relevant facts. Recursive rules, where the predicate in the head also occurs in the body, are exceptions, as they are relevant both for themselves and one additional rule. For example:\np5(X, Y) :- p0(X, Z), P2(Y, W).\np0(X, Y) :- p3(X, Z), p4(W, Y).\np3(X, Y) :- p6(X, Z), p7(W, Y).\nFor example, according to Rule 1, p0(X, Z) is necessary for p5(X, Y). Therefore, satisfying p5(X, Y) requires p0(X, Z), which in turn requires p3(X, Z) and p4(W,Y). This creates a dependency chain where p5(X, Y) relies on p3(X, Z) and p4(W, Y).\nCategory Rooted DG (RDG). This category generalises the Chain category. Here, every rule can be relevant for several others, and each node can have multiple parent nodes. Furthermore, for each rule, there may be several other rules that might infer facts relevant for it. However, for each predicate occurring in the body of a rule, there must be at most one other rule with this predicate in the head. In other words, there are no alternative rules to derive facts relevant for a rule with respect to a specific body atom. For example:\np0(X0,X1):- p1(X1,X2),p3(X0,X1).\np3(X0,X1) :- p8(X0,X1),p6(\u03a70,X1).\np1(X1,X2) :- p7(X2,X1).\nIn the given example, each rule has at least one child node. For instance, p0(X0, X1) has two child nodes: p1(X1, X2) and p3(X0, X1). Each predicate in the body of a rule corresponds to at most one rule with that predicate in the head. There are no alternative rules for deriving facts related to a specific body atom. For example, p1(X1, X2) appears in the body of p0(X0, X1) and only one rule has p1(X1, X2) in the head: p1(X1, X2) :- p7(X2, X1). The same applies to p3.\nCategory Disjunctive Rooted DG (DRDG). Category DRDG generalises Category RDG by allowing for alternative rules represented as children of an \"OR\" node. For instance:\np7(X0,X1) :- p5(X0,X1).\np5(X0,X1) :- p0(X0,X1).\np5(X0,X1):- p8(X1,X0).\nIn the example, the first rule states p7(X0, X1) is true if p5(X0, X1) is true, indicating p7 depends on p5. The second rule states p5(X0, X1) is true if p0(X0, X1) is true, showing p5 depends on p0. The third rule states p5(X0, X1) is true if p8(X1, X0) is true, adding an alternative condition with swapped arguments. Thus, p5 acts as an \"OR\" condition in the first rule's body and the second and third rules' heads.\nCategory Mixed. A rule graph in this category contains connected components of different categories mentioned above. Additionally, recursion is allowed, meaning that the head of the rule may appear in the body as well."}, {"title": "Further empirical data & findings", "content": "GPT-40 shows stable performance with moderate to high accuracy but is sensitive to noise, especially in RDG and DRDG. For instance, its F1 score in RDG drops from 0.75 at noise level 0.1 to 0.25 at noise level 0.3. GPT-3.5-Turbo does not perform well with complex categories like RDG and DRDG under noise, with an F1 score of 0 at noise level 0.3 in RDG.\nMixtral-8x7B-Instruct-v0.1 shows high variability w.r.t. noise, performing reasonably well in RDG (0.64 F1 at noise level 0.1, dropping to 0.43 at noise level 0.3) but with significant time consumption, especially in DRDG (130.160 seconds at noise level 0.2). It does not perform well with complex rule sets like DRDG across all noise levels.\nLlama3-8B instruct has low accuracy across most categories, with slight improvement at higher noise levels but increased time consumption. At noise level 0.1, it achieves an F1 score above 0 only in RDG R. It often fails to produce valid theories or introduces new predicates incorrectly. For instance, the rule p(X, Y) - p2(X, Y); p0(X, Y); p4(X, Y); p9(X, Y). is valid, but the predicate p, in the head of the rule, does not exist in the BK, neither is the target predicate. Llama3-8B was the only model to exhibit this pattern.\nThe models generally present higher initial accuracy on recursive (R) categories, but are more sensitive to noise on them, leading to larger performance drops. For example, on DRDG-R, GPT-40's F1 score drops from 0.83 at noise level 0.1 to 0.120 at noise level 0.3. Non-recursive categories like CHAIN present more stable performance.\nTime variance has an inverse relation w.r.t. noise levels on LLMs when compared with Popper. As the noise increases, Popper may take less time to induce a theory based on the remaining relevant data, as indicated by the scattering pattern progression in Figure 4, while the LLMs are more likely to take longer to process it. Detailed values are presented in Tables 4 and 5."}, {"title": "Prompt templates", "content": "For the iterative LM theory refinement method, the following template was used for the initial prompt:\nPrompt p1:\nInduce a theory based on background knowledge, positive and negative examples. Write it in Prolog. Do not give an explanation. Answer only the theory.\nBK:\n{BK}\nExamples:\n{positive and negative examples}\nFor the refinement steps, the following prompt template was used:\nPrompt p2...Pn:\nThe theory scored:\naccuracy = {acc} precision = {precision}\nrecall = {recall} f1 = {f1}\nand got these examples wrongly:\n{examples that were misclassified}\nRefine the theory. Answer only the theory.\nThe prompt templates were designed to be objective and minimal, containing only the necessary instructions and data for solving the task. They were adjusted using a small sample of inputs, to minimise the syntax errors across all models. The same prompt templates were used across all language models."}, {"title": "Reproducibility", "content": "Upon acceptance of this paper, we will release all code and data associated with our study. The raw data, along with detailed instructions for data processing, are accessible via the provided repository link. Any proprietary tools or materials used in this study are either commercially available or provided under a reasonable request. By ensuring that all aspects of this research are openly accessible, we invite the scientific community to replicate our findings and build upon this work, fostering a collaborative and transparent scientific environment."}, {"title": "Dataset Generation Process", "content": "Below", "mindags": "nDefinition: Minimum number of generated DAGs. This parameter ensures that at least the specified number of DAGs is generated in the dataset.\nConstraint: Must be greater than 0.\n\u2022 maxdags:\nDefinition: Maximum number of gener-ated DAGs. This parameter sets an upper limit on the number of DAGs to be included in the dataset.\nConstraint: Must be greater than or equal to mindags.\n\u2022 noise:\nDefinition: Represents the percentage of noise in the datasets. Noise here refers to the random perturbations added to the data.\nConstraint: Must be a value in the range [0", "1": ".", "World)": "nDefinition: The open-world degree indicates how many of the consequences of an initial set of relevant facts", "data.\nConstraint": "Must be a value in the range [0"}, {"1": ".", "missing": "nDefinition: Specifies the percentage of missing data in the dataset.\nConstraint: Must be a value in the range [0"}, {"1": ".", "category": "nDefinition: Determines the type of the rule to be generated. The categories include different structural patterns and combinations.\nValues:\n* Chain\n* Rooted Directed Graph (DG)\n* Disjunctive Rooted DG\n* Mixed\n* All of them with recursion\nIllustrative Example: Family Knowledge Base\nTo illustrate the dataset's construction", "as": "X, Y (parent(X,Y) \u2192"}]}