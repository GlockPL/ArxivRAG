{"title": "Representation Learning for Time-Domain High-Energy Astrophysics: Discovery of Extragalactic Fast X-ray Transient XRT 200515", "authors": ["Steven Dillmann", "Rafael Mart\u00ednez-Galarza", "Roberto Soria", "Rosanne Di Stefano", "Vinay L. Kashyap"], "abstract": "We present a novel representation learning method for downstream tasks such as anomaly detection and unsupervised transient classification in high-energy datasets. This approach enabled the discovery of a new fast X-ray transient (FXT) in the Chandra archive, XRT 200515, a needle-in-the-haystack event and the first Chandra FXT of its kind. Recent serendipitous breakthroughs in X-ray astronomy, including FXTs from binary neutron star mergers and an extragalactic planetary transit candidate, highlight the need for systematic transient searches in X-ray archives. We introduce new event file representations, E t Maps and E - t - dt Cubes, designed to capture both temporal and spectral information, effectively addressing the challenges posed by variable-length event file time series in machine learning applications. Our pipeline extracts low-dimensional, informative features from these representations using principal component analysis or sparse autoencoders, followed by clustering in the embedding space with DBSCAN. New transients are identified within transient-dominant clusters or through nearest-neighbor searches around known transients, producing a catalog of 3,539 candidates (3,427 flares and 112 dips). XRT 200515 exhibits unique temporal and spectral variability, including an intense, hard <10 s initial burst followed by spectral softening in an ~800 s oscillating tail. We interpret XRT 200515 as either the first giant magnetar flare observed at low X-ray energies or the first extragalactic Type I X-ray burst from a faint LMXB in the LMC. Our method extends to datasets from other observatories such as XMM-Newton, Swift-XRT, eROSITA, Einstein Probe, and upcoming missions like AXIS.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent serendipitous discoveries, such as extragalactic fast X-ray transients (FXTs) linked to neutron star merger candidates as electro-magnetic counterparts to gravitational wave events (Lin et al. 2022) and an X-ray dip associated with the first extragalactic planet candi-date (Di Stefano et al. 2021), underscore the challenges of identify-ing such rare events within large X-ray catalogs. Beyond magnetar-powered FXTs as the aftermath of binary neutron star mergers (Dai et al. 2006; Metzger et al. 2008; Zhang 2013; Sun et al. 2017; Bauer et al. 2017; Xue et al. 2019), other interesting origins of extragalactic FXTs include supernova shock breakouts (SBOs) (Soderberg et al. 2008; Modjaz et al. 2009; Alp & Larsson 2020; Novara et al. 2020), tidal disruption events (TDEs) (Jonker et al. 2013) including quasi-periodic eruptions (QPEs) (Arcodia et al. 2021; Chakraborty et al. 2021), thermonuclear (Type I) X-ray bursts from accreting neutron stars (in't Zand et al. 2013), or binary self-lensing events (D'Orazio & Di Stefano 2018, 2020; Hu et al. 2020). Both because of their very stochastic nature, and because narrow field X-ray missions such as the Chandra X-ray Observatory (Chandra) (Weisskopf et al. 2000), XMM-Newton (Jansen et al. 2001) and Swift-XRT (Burrows et al. 2005) are not designed as wide time-domain surveys, X-ray transient discoveries are often serendipitous. They can be found in observa-tions that were originally proposed for a completely unrelated sci-ence objective and are rarely the target of the observation. In many cases serendipitously found X-ray sources do not get characterized or classified, since their transient nature is not immediately obvi-ous. Instead, observations with X-ray transients often get stored in large data archives and remain unnoticed. This raises the need for a systematic search for short-duration phenomena in high-energy catalogs. New missions such as eROSITA (Predehl et al. 2021), Ein-stein Probe (Yuan et al. 2022) and the upcoming AXIS Observatory (Reynolds et al. 2024) target X-ray transients more directly, thus the development of novel transient detection methods is becoming even more relevant. The temporary, unpredictable and 'unusual' nature of X-ray transients distinguishes them from 'normal' X-ray source emissions. From a data science perspective, they can be understood"}, {"title": "2", "content": "S. Dillmann et al.as 'anomalies' within a large dataset. Existing methods for identify-ing X-ray transients primarily rely on statistical tests of variability(Yang et al. 2019; Pastor-Marazuela et al. 2020; Quirola-V\u00e1squezet al. 2022; Quirola-V\u00e1squez et al. 2023). While effective withinspecific constraints, these approaches are inherently limited by theirunderlying assumptions, which may not capture the diverse natureof transient phenomena. In contrast, machine learning offers a moreflexible, expressive, and scalable framework, making it particularlywell-suited for anomaly detection in large, high-dimensional datasetswith diverse transient types. While optical time-domain surveys areat the forefront of leveraging extensive observational programs, likeZTF (Bellm et al. 2019) or the upcoming LSST survey (Ivezi\u0107 et al.2019), and neural network-based anomaly detection tools to identifyrare sources among countless ordinary objects (Villar et al. 2021;Muthukrishna et al. 2022), the X-ray astronomy community has onlyrecently begun exploring the potential of machine learning to clas-sify sources (Yang et al. 2022; P\u00e9rez-D\u00edaz et al. 2024) or to searchfor needle-in-a-haystack events in large X-ray datasets and archives(Kova\u010devi\u0107 et al. 2022; Dillmann & Mart\u00ednez-Galarza 2023). Theeffectiveness of machine learning methods largely depends on thealgorithm's ability to learn useful representations from the data.Representation learning (Bengio et al. 2013) is an increasinglypopular technique in astronomy used in supervised, semi-supervised,self-supervised and unsupervised frameworks (Naul et al. 2018;Hayat et al. 2021; Walmsley et al. 2022; Slijepcevic et al. 2024; Mo-hale & Lochner 2024). It involves creating or learning meaningfulrepresentations for specific modalities of scientific data, which canthen be used for downstream tasks such as regression, classification,or, as in this work, anomaly detection. The compressed representa-tions live in a low-dimensional embedding space, in which anomalousdata samples are well-separated from more ordinary ones.We propose a new unsupervised representation learning methodto perform a large-scale search for X-ray transients in the Chandraarchive. High-energy catalogs include individual X-ray source obser-vations in the form of event files. The variable length of these timeseries poses a challenge in creating consistent representations suit-able for transient searches with machine learning. Most deep learningalgorithms take a fixed-length input for all data samples. In order toeffectively represent event files over a broad range of lengths, we in-troduce novel fixed-length event file representations, which take intoaccount both their time-domain and energy-domain information. Ap-plying feature extraction and dimensionality reduction techniques, forexample with sparse autoencoders, we create a representation spacethat encodes scientifically meaningful information, such as the spec-tral and variability properties of the astrophysical sources. Previouslyidentified X-ray transients occupy distinct, well-isolated clusters inthe embedding space. Using clustering techniques and nearest neigh-bor searches allows us to effectively explore these transient-dominantclusters to discover new X-ray transients. We collect the identifiedX-ray flare and dip candidates in a publicly available catalog, servingas a fertile ground for new discoveries in time-domain high-energyastrophysics.Among these candidates, we identify an intriguing extragalacticFXT, XRT 200515, which exhibits unique temporal and spectralcharacteristics distinct from any previously reported Chandra FXTs.The transient's initial hard <10s burst shows a sharp rise exceeding4 orders of magnitude, followed by spectral softening in an ~800 socillating tail. This transient is likely related to either a giant mag-netar flare (GMF) from a distant soft gamma repeater (SGR) behindthe Large Magellanic Cloud (LMC) or an extragalactic Type I X-rayburst from a faint LMXB in the LMC. Each of these interpretationspresents its own set of challenges. Alternatively, XRT 200515 couldbe a new type of astronomical phenomenon found by our anomalydetection method using machine learning.Our method is the first representation learning approach foranomaly detection in high-energy astrophysics. It is applicable todatasets from high-energy catalogs like Chandra, XMM-Newton,Swift-XRT, EROSITA, and Einstein Probe. We created semanticallymeaningful representations that can be aligned with other data modal-ities, such as optical images or infrared spectra to design multi-modalmodels (Parker et al. 2024; Mishra-Sharma et al. 2024; Zhang et al.2024; Rizhko & Bloom 2024) using contrastive learning (Radfordet al. 2021), that can improve on current state-of-the-art algorithmsused to characterize the physics of the associated objects. Ulti-mately, this work and other representation and contrastive learningapproaches lay the groundwork for developing generalized founda-tion model in astronomy.The paper is organized as follows: In \u00a7 2, we provide informationon the dataset of Chandra event files used in this analysis. In \u00a7 3, wedescribe in detail the implementation of our novel transient detectionapproach leveraging representation learning. In \u00a7 4, we present anddiscuss the results in form of the semantically meaningful representa-tion space of the event files, the catalog of X-ray transient candidatesand the discovery of the new Chandra transient XRT 200515. Fi-nally, we highlight our contributions to time-domain high-energyastrophysics and outline potential directions for extending this workin the future in \u00a7 5.The relevant code, a demonstration of the pipeline, and aninteractive embedding selection, transient search and lightcurveplotting tool are available online at the GitHub repositoryhttps://github.com/StevenDillmann/ml-xraytransients-mnras."}, {"title": "2 DATASET", "content": "We use data from the Chandra Source Catalog (CSC) version 2.1(Evans et al. 2024), which includes all publicly available X-raysources detected by Chandra as of December 2021. For this study, wefocus specifically on observations from the Advanced CCD Imag-ing Spectrometer (ACIS). CSC 2.1 had not been fully released atthe time our analysis was performed, but catalog data was availablefor sources that had completed processing in the Current DatabaseView, a snapshot of which we took on 11 April 2023. CSC 2.1 per-forms source detection on stacked observations, and catalog proper-ties are provided both for these stack-level detections, and for each ofobservation-level detection that contribute to a stack detection. Be-cause we are interested in short-time variability that happens withina single observation of a source, we use the catalog products for theobservation-level detections in our analysis. For a given X-ray de-tection, two types of products are provided in the CSC: (i) databasetables with source properties, such as fluxes in the different X-rayenergy bands, hardness ratios, variability indices, etc., and (ii) file-based data products for each detection of a source, such as the de-tect regions, the Chandra PSF at that location, etc. The followingobservation-level catalog properties are relevant for our analysis:\u2022 var_prob_b: The probability that a source detection is variablein time for the broad energy band (0.5\u20137 keV), as estimated using theGregory-Loredo algorithm (Gregory & Loredo 1992). In this paperwe call this quantity $p_{var}$.\u2022 var_index_b: The variability index in the broad band, whichindicates the level of confidence for time variability. A variability"}, {"title": "3 METHODS", "content": "In this work, we introduce a novel representation learning basedanomaly detection method to systematically search for X-ray tran-sients in high-energy archives. We begin with an overview of themethod here and provide detailed explanations of each step in in-dividual subsections. The full pipeline is illustrated in Figure 1.Starting with the event files described in \u00a7 2, we (i) build twonovel and uniform event file representations by binning their arrivaltimes and energies into $E \\- t$ Maps (Event File Representation I) or$E \\- t \\- dt$ Cubes (Event File Representation II); (ii) use principalcomponent analysis (Feature Extraction I) or sparse autoencoders(Feature Extraction II) to extract informative features from the event"}, {"title": "3.1 Event File Representation", "content": "The different event files in the dataset are variable in length $N$ andduration $T$, as shown in Appendix A. The large variation in thenumber of events and duration highlights the challenge in producinguniform data representations that preserve relevant information ontime variability and spectral properties. While there exist machinelearning architectures that take variable length inputs, the significantdifferences in the number of events from object to object make stan-dardization of the inputs challenging, even when these architecturesare used (Mart\u00ednez-Galarza & Makinen 2022). As a first step in ouranalysis, we introduce 2-dimensional and 3-dimensional fixed-lengthrepresentations based on an informed binning strategy for the eventfiles, similar to the DMDT maps for optical lightcurves introducedby Mahabal et al. (2017).\n3.1.1 2D Histogram Representation ($E \\- t$ Maps)\nAssume an event file with $N$ photons and a photon arrival timecolumn $t$ with entries $\\{t_k\\}_{k=1}^N$ and energy column $E$ with entries$\\{E_k\\}_{k=1}^N$. The event file duration is given by $T = t_N - t_1$. The energycolumn entries take values in the broad energy band of Chandra'sACIS instrument, i.e. $E_k \\in [E_{min}, E_{max}]$, where $E_{min} = 0.5 \\mathrm{keV}$and $E_{max} = 7 \\mathrm{keV}$ comes from considering appropriate boundariesfor the energy response of Chandra's ACIS instrument. Beyond theseboundaries, the telescope's aperture effective area is low for themajority of detected sources. First, we obtain the normalized timecolumn, given by $\\tau = \\frac{t-t_1}{T}$, and the logarithm of the energy column,given by $e = \\log E$. The resulting boundaries for normalized timecolumn are $\\tau \\in [\\tau_{min}, \\tau_{max}]$, where $\\tau_{min} = 0$ and $\\tau_{max} = 1$."}, {"title": "3.1.2 3D Histogram Representation ($E \\- t \\- dt$ Cubes)", "content": "We now introduce the $E \\- t \\- dt$ Cubes, which extend the $E \\- t$ Mapsby a third dimension that serves as a proxy for the photon arrival rate.For an event file of length $N$, consider the array of time differencesbetween consecutive photon arrivals $\\Delta t$ with entries $\\Delta t_k = t_{k+1} - t_k$for $k = 1, 2, ..., N-1$. We again scale and normalize the obtainedvalues, so that they adopt values between 0 and 1, using in each casethe minimum value $\\Delta t_{min}$ and maximum value $\\Delta t_{max}$. This providesthe third dimension $\\delta \\tau$:\n$$\\delta \\tau = \\frac{\\Delta t - \\Delta t_{min}}{\\Delta t_{max} - \\Delta t_{min}}$$\nThe additional dimension is intended to better isolate short-durationfeatures in time variability by capturing high photon arrival rates,which are typical of flares, as well as very low photon arrival rates,which are typical of dips. The boundaries of our histogram represen-tations in this dimension are $\\delta \\tau \\in [\\delta \\tau_{min}, \\delta \\tau_{max}]$, where $\\delta \\tau_{min} = 0$and $\\delta \\tau_{max} = 1$. We determine the optimal number of bins in the $\\delta \\tau$dimension, $\\eta_{\\delta \\tau}$, again by computing the optimal bin width $b_{\\delta t}$ withthe Freedman-Diaconis rule and dividing the range for $\\delta \\tau$ by $b_{\\delta t}$:\n$$b_{\\delta \\tau} = 2 \\frac{IQR(\\delta \\tau)}{N^{1/3}}$$\n$$\\eta_{\\delta \\tau} = \\frac{\\delta \\tau_{max} - \\delta \\tau_{min}}{b_{\\delta \\tau}}$$"}, {"title": "3.1.3 Feature Notation", "content": "The event file representations can now be used as inputs for variousstatistical learning and machine learning algorithms. For the $i^{th}$ eventfile in the dataset of length $m = 95,473$, we denote the correspondingfeature vector as $x_i = [x_1, x_2, ..., x_n]_i$, where $n = \\eta_{\\tau} \\cdot \\eta_{\\epsilon} = 384$ forthe $E \\- t$ Maps and $n = \\eta_{\\tau} \\cdot \\eta_{\\epsilon} \\cdot \\eta_{\\delta \\tau} = 6,144$ for the $E \\- t \\- dt$ Cubes.The set of all feature vectors is denoted as $X = [x_1, x_2, ..., x_m]$with size $(m, n)$."}, {"title": "3.2 Feature Extraction I: Principal Component Analysis", "content": "We use Principal Component Analysis (PCA) (Pearson 1901) pro-vided by scikit-learn (Pedregosa et al. 2011) as our first featureextraction method. The extracted principal components should en-code relevant time-domain and spectral information of the event filethey represent. PCA involves transforming a dataset into a new co-ordinate system by finding the principal components of the data thatcapture most of the variance in the data. By projecting the datasetonto principal components, PCA reduces the dimensionality of thedata while retaining the most important information, which increasesthe interpretability of high-dimensional data."}, {"title": "3.2.1 PCA Algorithm", "content": "We start with the feature vector set $X$ of size $(m, n)$ representingour dataset with $m$ samples and $n$ dimensions. PCA aims to finda new coordinate system defined by a set of orthogonal axes, i.e.the principal components, that captures the maximum amount ofvariance in the data. The PCA result is a transformed dataset $X_{pc}$obtained by projecting $X$ onto the principal components:\n$$X_{pc} = XW,$$\nwhere $W$ is matrix of size $(n, n_{pc})$ containing the first $n_{pc}$ principalcomponents to be retained as its columns and $X_{pc}$ is of size $(m,n_{pc})$ with a reduced dimensionality of $n_{pc}$. For a more detailedexplanation of the algorithm, we refer the reader to Jolliffe (2002)."}, {"title": "3.2.2 Principal Components Retained", "content": "The main PCA hyperparameter is the number of principal compo-nents $n_{pc}$ to retain. Figure 5 shows two scree plots illustrating theamount of variance explained by each principal component in de-scending order and the cumulative proportion of variance explainedby the principal components for both $E \\- t$ Maps and $E \\- t \\- dt$ Cubes.A common approach to determine the optimal value of $n_{pc}$ is to findthe knee point in the cumulative scree plot of the principal compo-nents. This balances the objective of minimizing the dimensionalitywhile retaining as much information as possible. Defining the kneepoint as the point beyond which adding additional principal com-ponents increases the amount of variance by less than 0.1% gives$n_{pc} = 15$ for $E \\- t$ Maps and $n_{pc} = 22$ for $E \\- t \\- dt$ Cubes as in-dicated in Figure 5. These capture 94.1% and 89.9% of the variance respectively."}, {"title": "3.3 Feature Extraction II: Sparse Autoencoder Neural Network", "content": "As an alternative to PCA, we now build Autoencoder (Hinton &Salakhutdinov 2006) models with TensorFlow (Abadi et al. 2015)to learn a set of latent features from the $E \\- t$ Maps and $E \\- t \\- dt$ Cubesthat can be used to isolate transients and encode specific spectralproperties. An autoencoder is composed of two neural networks, anencoder and a decoder, which work together to learn a compressedrepresentation of the input data. The encoder network takes the inputdata and maps it to a lower-dimensional representation, often called'latent space' or 'bottleneck'. The number of neurons in the bot-tleneck determines the dimensionality of the learned representation.The decoder network then aims to reconstruct the original input fromthis compressed representation. The decoder is typically a mirroredversion of the encoder gradually upsampling the latent space until theoutput matches the dimensions of the original input. By minimizingthe reconstruction error between input and output during training,the model learns a low-dimensional representation of the input. Thebottleneck forces the encoder to capture the most important informa-tion necessary for accurate reconstruction, effectively compressingthe input and learning to extract informative features in an unsuper-vised manner. Once the autoencoder is trained, the encoder networkcan be used as a standalone feature extractor to obtain a compressedrepresentation of the input data, which can be used for downstreamtasks such as clustering or anomaly detection. As opposed to PCA,which is a linear technique that works well for linearly correlated databut fails to capture complex non-linear relationships, an autoencoderis able to learn complex non-linear relationships. We design two dif-ferent autoencoders to process the $E \\- t$ Maps and $E \\- t \\- dt$ Cubes."}, {"title": "3.3.1 Convolutional Autoencoder", "content": "In a convolutional autoencoder (Masci et al. 2011), both the en-coder and decoder network consist of convolutional layers (LeCunet al. 1998), which perform convolutions over the input using a filter.These filters are small matrix kernels with learnable weights thatslide across the input, allowing the network to capture high-levelfeatures while preserving important spatial hierarchies and relation-ships, which is why they are often used for image-like data. Thismakes this architecture particularly well-suited to recognize spatialpatterns such as dips or flares in our $E \\- t$ Maps. To gradually reduce the dimension of the input while it is being passed throughthe encoder network, we use stride convolution layers (Simonyan &Zisserman 2014) with a stride value of 2 for downsampling. Thismeans that the learnable filter jumps two pixels at a time as it slidesover the input. The output of the convolutional layers is a featuremap, which is then flattened to a feature vector and passed througha series of fully connected layers, where every neuron in the previ-ous layer is connected to every neuron in the next layer. These fullyconnected layers are responsible for mapping the learned features toa lower-dimensional latent representation in the bottleneck and perform non-linear transformations while downsampling throughthe use of non-linear activation functions. The final latent space has$n_{ae} = 12$ elements, representing the most essential features of theinput data, which can now be used for further downstream tasks."}, {"title": "3.3.2 Fully Connected Autoencoder", "content": "Our $E \\- t \\- dt$ Cubes introduce an additional dimension resulting insparse 3D input data. Convolutional layers assume regular grid-likedata, making them less effective for handling sparse data. Moreover,very expensive 3D convolutional operations would substantially increase complexity of the model. Therefore, we use a simple fullyconnected autoencoder for the $E \\- t \\- dt$ Cubes. Its encoder networkconsists of a series of fully connected layers, which gradually mapthe original input data to a latent space with $n_{ae} = 24$ elements."}, {"title": "3.3.3 Activation Functions", "content": "Neural networks are able to learn and represent complex non-linearrelationships due to the introduction of non-linear activation func-"}, {"title": "3.3.4 Loss Function and Sparsity Regularization", "content": "In order to encourage the autoencoder to generate reconstructionsclose to the original inputs, we use the mean squared error (MSE)as as a measure of the reconstruction quality given by:\n$$MSE = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\hat{x_i})^2,$$\nwhere $x_i$ is the $i^{th}$ element of the input vector and $\\hat{x_i}$ is the corre-sponding is reconstructed output. The MSE is a straightforward mea-sure of reconstruction error, and its differentiability allows efficientgradient computation for updating model weights via gradient-basedoptimization.Our neural networks are so called sparse autoencoders (Ng et al.2011), which promote sparsity in the learned representation, meaningonly a small subset of the neurons in the network are active at anygiven time. Sparse representations are valuable for our work becausethey help extract highly informative features from the input, whiledisregarding irrelevant or noisy information. To encourage sparsityin the latent space, we introduce a L1 regularization term in theobjective, resulting in the following loss function:\n$$L = MSE + \\lambda \\cdot \\sum_{j=1}^{n_w} |w_j| = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\hat{x_i})^2 + \\lambda \\cdot \\sum_{j=1}^{n_w} |w_j|,$$\nwhere $\\lambda = 0.1$ is the regularization strength and $w_j$ are the indi-vidual bottleneck weight values of which there are $n_w$ in total. L1regularization pushes small weights to zero and thus helps the modelprioritize the most significant features of the input data, leading to asemantically meaningful latent space."}, {"title": "3.3.5 Training", "content": "Starting with the original dataset with a $m = 95,473$ samples andusing a test split of 0.1 gives us a training and validation set of length85,925 and a test set of length 9,548. Further using a validation splitof 0.2, gives 68,740 samples for training and 17,185 for validation.We run the training process for a maximum of 200 epochs with abatch size of 1,024 samples. The initial learning rate was set to 0.01along with an on plateau learning rate scheduler, which dynamicallyreduces the learning rate by a factor of 0.1 if the validation lossplateaus for longer than 10 epochs. Reducing the learning rate whena plateau is detected can help escape local minima in the loss surfaceand converge to a more optimal solution in the parameter space. Thisscheduler is used in combination with the Adaptive Moment Estima-tion (Adam) optimizer (Kingma & Ba 2014), which is a stochasticgradient descent algorithm combining the benefits of both adaptivelearning rates (Duchi et al. 2011) and momentum-based optimizationtechniques (Sutskever et al. 2013). Finally, we use an early stoppingcallback to monitor the validation loss. It automatically interrupts thetraining process if the validation loss does not improve for 25 epochsand restores the weights of the model to the best observed weightsduring training. The training process for both autoencoder models isshown in Appendix B. Once the autoencoder is trained, we can usethe encoder to transform the original dataset $X$ to the feature vectorspace $X_{ae}$ of size $(m, n_{ae})$ with a reduced dimensionality of $n_{ae}$features."}, {"title": "3.4 Dimensionality Reduction", "content": "Using t-SNE (Maaten & Hinton 2008), short for t-DistributedStochastic Neighbor Embedding, we create two-dimensional em-beddings of the informative features previously extracted from theevent file representations using PCA or sparse autoencoders. Thet-SNE algorithm is a method used to map the input data onto alow-dimensional embedding space, and is particularly useful for thevisualization of clusters and patterns in high-dimensional datasets.Each high-dimensional sample is transformed into a low-dimensionalembedding in such a way that similar object are nearby points, whiledissimilar objects are distant points in the embedding space. Essen-tially, it aims to capture the local structure of the data by preservingthe pairwise similarities between objects while mapping them to alower-dimensional embedding space."}, {"title": "3.4.1 Algorithm", "content": "We use our informative features, $X_{if} = X_{pc}$ or $X_{if} = X_{ae}$, as input to the t-SNE algorithm to reduce the data to a two-dimensionalembedding, denoted as $Z$. First, t-SNE creates a probability dis-tribution $P$ for pairs of high-dimensional data points in $X_{if}$, assigninghigher probabilities to similar pairs and lower probabilities to dis-similar ones. This is done by modeling pairwise similarities using aGaussian kernel with a specific perplexity parameter, which controlsthe effective number of neighbors considered for each point. Next,t-SNE defines a similar probability distribution $Q$ for the pairwise"}, {"title": "3.5 Clustering", "content": "The next step is the identification of individual clusters in the em-bedding space using DBSCAN (Hartigan & Wong 1979), short forDensity-Based Spatial Clustering of Applications with Noise. Unliketraditional clustering algorithms such as k-means, DBSCAN does notrequire the number of clusters to be specified, as it identifies denseregions in the data space based on a density criterion."}, {"title": "3.5.1 Algorithm", "content": "We use our t-SNE embedding space $Z$ as input to the DBSCANalgorithm, which segments the embedding space into multiple clus-ters. The DBSCAN algorithm has two main hyperparameters. Theeps parameter defines the radius of the neighborhood surroundingeach point in the dataset, while the minPts parameter specifies theminimum number of points required within this neighborhood for adata point to be classified as a core point. A border point is defined asa point that is in the vicinity of at least one core point but has fewerthan minPts within its neighborhood. All other points are consideredto be noise points. Clusters are then created from the aggregation ofcore points and their associated border points, with noise points beingcategorized as outliers. Figure 7 visualizes the clustering method."}, {"title": "3.6 Previously Reported Transients", "content": "We highlight the embeddings of previously reported bona-fide tran-sients, listed in Table 6, in our low-dimensional representation spaceto identify transient-dominant clusters. The flares include extragalac-tic FXTs reported by Jonker et al. (2013), Glennie et al. (2015), Yanget al. (2019), Lin et al. (2021), Lin et al. (2022), Quirola-V\u00e1squezet al. (2022) and a set of stellar flares found in the dataset by man-ual inspection. The dips include the extragalactic planet candidate inM51 reported by Di Stefano et al. (2021), the ultraluminous X-raysource (ULX) 2E 1402.4+5440 in NGC 5457 (Colbert & Ptak 2002;Swartz et al. 2004) and the well-studied eclipsing and bursting low-mass X-ray binary (LMXB) EXO 0748-676 (Parmar et al. 1986;D'A\u00ec et al. 2014). These transients occupy well-isolated clusters. Ex-ploring transient-dominant clusters and performing nearest-neighborsearches around known transients allows us to find new transients."}, {"title": "3.7 Candidate Selection", "content": "New transients are identified in embedding clusters containing previ-ously reported transients. For well-isolated clusters containing knowndiscovered transients, we use the entire cluster to define new tran-sient candidates. The well-isolated transient-dominant clusters usedfor candidate selection are listed in Appendix E. However, in a fewcases known discovered transients reside within larger poorly sepa-rated clusters. Selecting the entire cluster would result in a high num-ber of false positives. To address this, we instead use the k-nearest"}, {"title": "3.8 Cross Matching", "content": "We use an existing cross-match table (Green et al. 2023) betweenCSC 2.1 and five other catalogs - Gaia DR3 (Gaia Collaboration et al.2021), DESI Legacy Survey DR10 (Dey et al. 2019), PanSTARRS-1(Chambers et al. 2016), 2MASS (Skrutskie et al. 2006), and the SDSSDR17 catalog - to complement the X-ray properties derived from theCSC with additional multi-wavelength observations. This includescatalog identifiers, positions, magnitudes, source type classificationsand other columns. We cross-matched our transient candidates withthe SIMBAD database (Wenger et al. 2000) by associating eachcandidate with the nearest SIMBAD object, provided the object islocated within a 5 arcsec radius of the candidate's coordinates listed"}, {"title": "4 RESULTS AND DISCUSSION", "content": "We now present the results of applying the methods in \u00a7 3 to the setof representations of X-ray event files in the dataset from \u00a7 2."}, {"title": "4.1 Representation Embedding Space and Clusters", "content": "Figure 8 shows the t-SNE embedding space for the 3D-PCA and 3D-AE cases color-coded by the hardness ratio HRhs. The embeddingspace for the other two cases, 2D-PCA and 2D-AE, are shown inAppendix D. The observed hardness ratio gradients in all embeddingspaces indicate that the learned representations effectively encode"}, {"title": "4.2 Catalog of X-ray Flare and Dip Candidates", "content": "We identify new transient candidates within clusters that are occu-pied by"}]}