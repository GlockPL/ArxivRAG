{"title": "CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature", "authors": ["Stefan Langer", "Fabian Neuhaus", "Andreas N\u00fcrnberger"], "abstract": "Ontologies are formal representations of knowledge in specific domains that provide a structured framework for organizing and understanding complex information. Creating ontologies, however, is a complex and time-consuming endeavor. ChEBI is a well-known ontology in the field of chemistry, which provides a comprehensive resource for defining chemical entities and their properties. However, it covers only a small fraction of the rapidly growing knowledge in chemistry and does not provide references to the scientific literature. To address this, we propose a methodology that involves augmenting existing annotated text corpora with knowledge from Chebi and fine-tuning a large language model (LLM) to recognize chemical entities and their roles in scientific text. Our experiments demonstrate the effectiveness of our approach. By combining ontological knowledge and the language understanding capabilities of LLMs, we achieve high precision and recall rates in identifying both the chemical entities and roles in scientific literature. Furthermore, we extract them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a knowledge graph (KG) of chemical entities and roles (CEAR), which provides complementary information to ChEBI, and can help to extend it.", "sections": [{"title": "1. Introduction", "content": "Chemistry is a large and diverse field of research with a rapidly growing number of publications available. While this is exciting and demonstrates rapid progress, the sheer volume of research texts makes it increasingly difficult to keep track of all the new discoveries and developments. Ontologies have been used to provide a structured framework for organizing this knowledge. However, manually incorporating knowledge into ontologies is a labor-intensive and time-consuming task, and therefore not feasible for all available research.\nIn recent years, Large Language Models (LLMs) have demonstrated exceptional performance in understanding natural language, excelling in tasks such as summarization and question answering. In this paper, we propose a novel approach that leverages the capabilities of these models to automatically create a knowledge graph (KG) of Chemical Entities And Roles (CEAR) from research publications and to extend existing ontological knowledge."}, {"title": "2. Related work", "content": "The SmartProSys research initiative aims to replace fossil raw materials in chemical production with renewable carbon sources, thus contributing to a carbon-neutral society. The transition to sustainable and circular production processes requires research into novel chemical reaction pathways that lead from renewable raw materials via energy-efficient and low-CO2 synthesis processes to green products. The task of identifying such pathways requires the collective chemical knowledge of the world to be searched and structured in a methodical, systematic and targeted manner. This knowledge is growing rapidly: the ChemRxiv platform, launched in 2017, already contains more than 20,000 research papers on chemistry. In addition, there are journals such as the International Journal of Molecular Sciences, which has published more than 20,000 scientific articles in 2022, of which about 30-35% are in the field of biochemistry [1].\n[2] emphasizes that the first step in designing an effective knowledge representation system, and vocabulary, is to perform an effective ontological analysis of the field, or domain and that ontologies enable knowledge sharing.\nChEBI is a database and ontology for chemical entities of biological interest. In its November 2012 release, it contained nearly 30,000 fully annotated entities, all of which were added by expert annotators [3]. In 2024, ChEBI contains almost 218,000 entities, of which more than 60,000 were fully annotated by ChEBI curators. However, the content of ChEBI is still very limited, when compared to data sources like PubChem with information on nearly 317 million substances and 118 million compounds\u00b9.\nKnowledge graphs, on the other hand, are a powerful tool for representing and querying complex, interrelated data. They are essentially a network of entities (nodes) and their interre-lations (edges). The relationship between ontologies and knowledge graphs is complementary. Ontologies provide a well-defined, interconnected vocabulary, while knowledge graphs populate this vocabulary with specific real-world data instances."}, {"title": "3. Methods", "content": "In our work, we create a KG for chemical entities and roles as defined in ChEBI. Chemical entities are atoms, substances, groups and molecules and are classified as such based on shared structural features, while roles are classified based on their activities in biological or chemical systems or their use in applications [11]. Figure 1 outlines the method we use to create the KG: First, we extract the full text from research papers and then fine-tune an LLM to identify chemical entities and roles. Candidate sentences containing both are collected and a different LLM is used to validate the relationship between the two. Finally, we de-duplicate and normalize both chemical entities and roles, link them to the ChEBI ontology and create the KG. The following subsections explain each step in detail.\nFigure 2 shows the different types of information provided by our approach. The information that is extracted from the papers has the form <chemical entity> has_Role <chemical Role>, together with additional information about the text location that supports this triple. Each text location consists of a specific paper, the page number in the paper, and the character position of the sentence relative to that page number. RDF is not ideal to model these relations because it does not allow to annotate a triple with its source without clumsy workarounds (e.g., reification of triples). Thus, we plan to release a KG built using RDF-star. The current RDF version does not include any text locations."}, {"title": "3.1. Text extraction from research papers", "content": "Research papers are a rich source of information. They contain author names, images, tables, citations, bibliographies, and more. To address the challenge of extracting the papers' full texts in an efficient way, we chose a very simple approach which involves using a Linux utility called pdftotext. While it cannot identify floating objects in plain text, such as image and table captions or footers and page numbers, it can reliably extract different formats, ranging from one-column to two-column styles.\nWe downloaded a set of 8,000 chemistry research papers from various categories of ChemRxiv and extracted their full text as JSON documents, including information about the page it was extracted from. Content-based checksums ensure that no duplicates are processed, even when crawling other sources for research papers. The checksums are also used as identifiers between the original PDF file and the associated JSON document."}, {"title": "3.2. Chemical entity and role recognition", "content": "Transformer-based Large Language Models (LLMs) have proven effective in understanding language patterns and thus in Natural Language Processing (NLP) tasks such as Named-Entity-Recognition (NER), which we use in order to identify chemical entities and roles. Approaches such as ROBERTa or BERT use masked language modeling (MLM), where some tokens in an input sequence are randomly masked and the model is trained to predict the original token [12]. Electra models use a pre-training task called replaced token detection or token discrimination, where instead of predicting a masked token, a discriminative model is trained to predict whether a token in the corrupted input sequence was replaced by a generator sample. We chose this approach because it is more sample-efficient [13], and fine-tuned a pre-trained Electra model on three different datasets:\n\u2022 The BC5CDR dataset consists of human annotations of chemicals, diseases and their interactions from 1,500 PubMed articles [14].\n\u2022 The NLM-Chem corpus contains 150 full-text articles on biomedical literature, carefully selected for containing chemical entities which are difficult to find for NER tools. Ten domain experts annotated the chemical entities in three annotation rounds [15].\n\u2022 The CRAFT corpus contains 97 full-text open access articles from the PubMed Central Open Access subset. It identifies all mentions of nearly all concepts from nine prominent biomedical ontologies, including ChEBI [16].\nA fourth manually annotated dataset, EnzChemRED [17], provides chemical entities and proteins, as well as conversions during chemical reactions. It is highly relevant to our NER task, but given its recent availability, it has not yet been used for fine-tuning.\nThe CRAFT corpus annotates all entities according to nine different ontologies from different areas of interest. Chemical annotations, including chemical entities and roles are provided along an extension of an older version of the ChEBI ontology. Although the NLM-Chem corpus and the BC5CDR dataset also annotate all chemicals in the provided full texts, and although BC5CDR annotates diseases, they do not include any chemical roles, such as ligand, acid, buffer, or catalyst. To overcome this limitation, we used a semi-supervised approach and automatically annotated all roles defined by their label and synonyms in ChEBI using a lexical approach. We ignore all role strings that are shorter than four characters to avoid mislabeling identical strings with different meanings (homonyms)."}, {"title": "3.3. Link Validation", "content": "We applied the fine-tuned Electra model to the extracted text of the chemistry research papers, collecting all sentences, that contained at least one chemical entity and at least one chemical role (Figure 3). For each sentence, we store the exact text location and the inferred chemical entities and roles.\nThe co-occurrence of chemical entities and roles within the same text block suggests that the chemical entity may have this specific role. However, this correlation alone is not sufficient to draw a definitive conclusion. To address this, we use another LLM to verify the role of a chemical entity based on the given contextual information. LLAMA 2 is a collection of pre-trained and fine-tuned LLMs ranging in size from 7 billion to 70 billion parameters. LLAMA 2-CHAT is specifically trained for conversational tasks using reinforcement learning with human feedback (RLHF) [18].\nIn this paper we used LLAMA-2-7b-CHAT and split the prompt into:\n\u2022 a system prompt, that defines the role of the LLM and makes sure that it simply confirms or rejects the relation between chemical entity and chemical role without any further explanations or other context that could complicate the parsing of the answer. For this paper we used:\n1 system_prompt = \"Do you agree with the provided question? Please answer with one word, either 'yes' or 'no'.\"\n2\n\u2022 a user prompt, that presents the context to the LLM along with the question whether, according to the given context, a specific chemical entity has a specific role. For this paper we used:\n1 user_prompt = f\"In the sentence '{sentence}': Is {chemical} explicitly described as {role}?\"\n2\nA temperature hyperparameter of 0.1 and a top-p of 0.95 ensure a somewhat deterministic behavior and reproducible results. All confirmed relations, as well as the associated sentence location, the chemical entity, and the role, are collected for the construction of the KG, while the remaining discarded relations are stored for analysis. Figure 4 shows how LLAMA-2 answers the questions whether trans-b-methylstyrene or NAOH is described as cofactor in the given sentence (see the first sentence in Figure 3 for a visualization of the same sentence with its chemical entities and roles rendered in red and blue)."}, {"title": "3.4. Knowledge graph creation", "content": "From the confirmed relationships, we group all chemical entities and roles using the labels and synonyms from ChEBI. If an entity is not part of ChEBI, we use its original appearance in the text. For this we only use chemical entities and roles with a character length of at least 2. For each pair of chemical entity and role, we count how many references to specific text locations exist. A higher frequency of occurrence of a relation increases our confidence in both, its correct identification in the research text and the correctness of its meaning. At the same time, it also reduces the novelty of the identified information. A hyperparameter minRef, which simply ignores relations with a low frequency, can be used to increase precision at the expense of recall or vice versa.\nThe knowledge graph consists of the described relations. It is stored using the Terse RDF Triple Language (Turtle). All chemical entities (obo:CHEBI_24431) and roles (obo: CHEBI_50906) that are known to ChEBI are defined using their ChEBI identifier.\nChemical entities or roles that are unknown to ChEBI are defined using the @prefix cear: <https://wwwiti.cs.uni-magdeburg.de/iti_dke/cear/> namespace. The obo: RO_0000087 is used in ChEBI to define roles of chemical entities.\nThe following listing shows an example for two chemical entities, ethylene glycol bis(2-aminoethyl)tetraacetate and PBS, both of which have the role buffer:\n1 @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n2 @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n3 @prefix obo: <http://purl.obolibrary.org/obo/>\n4 @prefix cear: <https://wwwiti.cs.uni-magdeburg.de/iti_dke/cear/> .\n5\n6 obo: CHEBI_35225 rdf: type obo: CHEBI_50906\n7 obo: CHEBI_35225 rdfs:label \"buffer\"\n8\n9 obo: CHEBI_30741 rdf:type obo: CHEBI_24431\n10 obo: CHEBI_30741 rdfs:label \"ethylene glycol bis(2-aminoethyl)tetraacetate\"\n11 obo: CHEBI_30741 obo:RO_0000087 obo: CHEBI_35225\n12\n13 cear:chem_4023 rdf: type obo: CHEBI_24431.\n14 cear:chem_4023 rdfs:label \"PBS\"\n15 cear:chem_4023 obo:RO_0000087 obo: CHEBI_35225"}, {"title": "4. Results", "content": "4.1. Chemical entity and role recognition\nIn section 3.2, we discussed how we used the BC5CDR corpus, the NLM-Chem corpus and the CRAFT corpus to fine-tune our Electra model for NER. As in [15], we counted a prediction as a true positive only if both the start and end locations of the characters of the complete entity exactly matched. This is a very strict definition, since the complexity of chemical entities makes it difficult to identify exact boundaries of entities or word tokens, for example: dipotassium 2-alkylbenzotriazolyl bis(trifuoroborate)s, 4,7-dibromo-2-octyl-2,1,3-benzotriazole[15].\nTable 1 shows the precision, recall and f-measure when fine-tuned on only one or multiple of the corpora. We have included cross-corpus evaluation data, and we can see that a model fine-tuned on the NLM-Chem or BC5CDR corpus performs very poorly when evaluated on the CRAFT corpus. Similarly, when a model fine-tuned using CRAFT is evaluated on NLM-Chem, the results are very poor. This indicates a lack of generalizability across datasets. Table 2 shows the reason by listing the ten most frequent misclassifications. All of the text corpora were manually curated to annotate all chemical entities contained in the texts. However, despite their common goal, they show discrepancies in annotation. For example, the chemical entities \"DNA\", \"RNA\" and \"mRNA\" are annotated in the CRAFT corpus, but not in the NLM-Chem corpus, hence the false negatives. The character \"b\", that appears as a false positive when a model fine-tuned on NLM-Chem is evaluated on CRAFT, is used in genetics to describe base pairs of DNA or RNA. Similarly, \"PBS\" is marked as a chemical entity in the NLM-chem corpus, but in CRAFT it is neglected. This illustrates how, depending on the context or background of the annotators, or depending on their research goals, there may be disagreement about which"}, {"title": "4.2. Link Validation and Knowledge Graph Construction", "content": "After applying the LLAMA-2 model for the validation of links between chemical entities and roles, and after grouping and applying the minRef hyperparameter as discussed in section 3.4, two representations of the resulting KG are available. An RDF representation and a graph"}, {"title": "5. Conclusion", "content": "In this paper, we have shown how to create a KG, which is linked to ChEBI, using the same vocabulary and extending it with knowledge from chemistry research papers. We see several applications for our approach:\nOur KG can assist in extending the ChEBI ontology by suggesting chemical entities and roles"}]}