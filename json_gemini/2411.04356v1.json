{"title": "GaGSL: Global-augmented Graph Structure Learning via Graph Information Bottleneck", "authors": ["Shuangjie Li", "Jiangqing Song", "Baoming Zhang", "Gaoli Ruan", "Junyuan Xie", "Chongjun Wang"], "abstract": "Abstract-Graph neural networks (GNNs) are prominent for their effectiveness in processing graph data for semi-supervised node classification tasks. Most works of GNNs assume that the observed structure accurately represents the underlying node relationships. However, the graph structure is inevitably noisy or incomplete in reality, which can degrade the quality of graph representations. Therefore, it is imperative to learn a clean graph structure that balances performance and robustness. In this paper, we propose a novel method named Global-augmented Graph Structure Learning (GaGSL), guided by the Graph Information Bottleneck (GIB) principle. The key idea behind GaGSL is to learn a compact and informative graph structure for node classification tasks. Specifically, to mitigate the bias caused by relying solely on the original structure, we first obtain augmented features and augmented structure through global feature augmentation and global structure augmentation. We then input the augmented features and augmented structure into a structure estimator with different parameters for optimization and re-definition of the graph structure, respectively. The redefined structures are combined to form the final graph structure. Finally, we employ GIB based on mutual information to guide the optimization of the graph structure to obtain the minimum sufficient graph structure. Comprehensive evaluations across a range of datasets reveal the outstanding performance and robustness of GaGSL compared with the state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "GRAPH data is pervasive in a variety of real-world scenarios, including power networks [1], social media [2], [3], and computer graphics [4]. In these scenarios, each node with attributes represents an entity, while each edge represents the relationship between the entity pairs. For example, in social networks, each node represents a user or individual, and they may have attributes such as personal information, interests, and professions. The edge that connects different nodes represents a friendship or following relationship, and it can be either directed or undirected. In recent years, graph neural networks (GNNs) have emerged as a powerful approach for working with graph data [5]\u2013[8], and have been widely adopted for diverse network analysis tasks, including node classification [9], [10], link prediction [11], [12], and graph classification [13], [14].\nMost existing GNNs rely on one basic assumption that the observed structure precisely represents the underlying node relationships. However, this assumption does not always hold in practice, as there are multiple factors that can lead to noisy graph structure: (1) Presence of noise and bias. Data can be collected and annotated from multiple sources. In the process of data collection and annotation, noisy connections and bias may be introduced by subjective human judgment or limitations in device precision. In some special cases (e.g., graph-enhanced applications [15] and visual navigation [16]), the data may lack inherent graph structure and require additional graph construction (e.g., kNN) for representation learning. (2) Adversarial attacks on graph structure. The majority of current methods for adversarial attacks on graph data, including poisoning attacks [17] on graph structure, concentrate on altering the graph structure, especially adding/deleting/rewiring edges [18], and the original structure can be severely damaged. For instance, in credit card fraud detection, a fraudster might generate numerous transactions involving multiple high-credit users to conceal their identity and avoid detection by GNNs.\nHow does the model's performance vary when the graph structure faces different levels of noise? And what is the impact on the graph structure when it is subjected to noise? To investigate the answer to this question, we simulated the noisy graph structure by introducing artificial edges into the graph at different proportions (i.e., 25%, 50%, and 75%) of the original number of edges (simulated noise), using the polblogs [19] dataset. Additionally, we calculate the probability matrices between communities for the original graph structure, as well as the graph structure with 75% additional edges, and draw them as heat maps. As illustrated in Fig. 1, the performance of GNNs models drops dramatically with the increase of edge addition rate, with SGC [20] exhibiting the most significant decline. This observation suggests that randomly adding edges has a detrimental effect on node classification performance. Further comparison of the middle and right plots reveals that randomly adding edges can result in connections between nodes from different communities. This inter-community connectivity decreases the distinguishing ability of nodes after GNN aggregates neighbor information, thereby leading to a decline in model performance. In summary, the effectiveness of GNNs is heavily dependent on the underlying graph structure, while real-world graphs often suffer from missing, meaningless, or even spurious edges. This structural noise may hinder message passing and limit the generalization ability of GNNs. Therefore, there is an urgent need to explore the optimal graph structure suitable for GNNs.\nIn recent years, various graph structure learning (GSL) methods [21] have emerged to handle the aforementioned problems, ensuring the performance and robustness of the models. Nevertheless, developing effective techniques to learn an optimal graph structure for GNNs represents a technically challenging task. (1) How can multifaceted information be introduced to provide a more comprehensive perspective? Relying on a single graph structure is inadequate to fully capture the complexity and diversity inherent in a graph [22]. Different perspectives of graph structure can shed light on various aspects and features within the graph. Therefore, it becomes imperative to integrate multi-perspective graph structure to acquire a more comprehensive and diverse graph structure. Most current methods for graph structure obtain the optimal graph structure from the single original structure [19], [23], [24]. There are also methods that obtain optimal graph structure based on multiple fundamental views [9], [22], [25], [26]. As an illustrative example, Chen et al. [25] constructed graph structure by incorporating both the normalized adjacency matrix and the node embedding similarity matrix. Nevertheless, it only considers feature similarity and fails to capture structural role information. (2) How to learn a clean graph structure for node classification tasks? In information theory, two key principles are Graph Information Bottleneck (GIB) [27]\u2013[29] and Principle of Relevant Information (PRI) [30]. GIB and PRI represent distinct approaches to redundancy reduction and information preservation. Specifically, GIB offers an essential guideline for GSL: an optimal graph structure should contain the minimum sufficient information required for the downstream prediction task. For example, Sun et al. [28] advanced the GIB principle in graph classification by jointly optimizing the graph structure and graph representation. PRI views the issue of reducing redundancy and retaining information as a balancing act. This balance is struck between reducing the entropy of the representation and its relative entropy to the original data. For instance, a structure containing the most relevant yet least redundant information, quantified using von Neumann entropy and Quantum Jensen-Shannon divergence, was developed by Sun et al. [31]. However, exploring the learning of the optimal graph structure that strike a balance between performance and robustness in node classification tasks, based on information theory principle, remains an ongoing challenge.\nTo address the aforementioned issues, in this paper, we propose a novel Global-augmented Graph Structure Learning (GaGSL) method to enhance the node classification performance and robustness based on the principle of GIB. GaGSL consists of global feature and structure augmentation, structure redefinition and GIB guidance. In global feature and structure augmentation, two different techniques used to obtain augmented features and augmented structure, respectively. Details are presented in subsection IV-B. In structure redefinition, we introduce a structure estimator to appropriately refine the graph structure. This involves reallocating weights to the graph adjacency matrix elements based on node similarity. Then, the redefined structures are integrated to form the final structure. More information is provided in subsection IV-C. In GIB guidance, we aim to maximize the mutual information (MI) between node labels and node embeddings Z* based on the final graph structure, while simultaneously imposing constraints on the MI between Z* and the node embeddings Zr1 or Zr2 based on the redefined structures. To effectively evaluate the MI, we employ an MI calculator based on the InfoNCE loss [32]. More elaboration is given in subsection IV-D. Finally, we employ a cyclic optimization scheme to iteratively update the model parameters. Details are presented in subsection IV-E. Our contributions can be summarized as follows:\n\u2022 We propose a Global-augmented GSL method, GaGSL, which is guided by GIB and aims at obtaining the most compact structure. This endeavor seeks to strike a finer balance between performance and robustness.\n\u2022 To alleviate the limitations of relying solely on a single graph structure, we integrate augmented features and augmented structure to obtain a more global and diverse graph structure.\n\u2022 The evaluation of our proposed GaGSL method is conducted on eight benchmark datasets, and the experimental findings convincingly showcase the effectiveness of GaGSL. Notably, GaGSL exhibits superior performance when contrasted to state-of-the-art GSL methods, and this advantage is particularly pronounced when the method is applied to datasets that have been subjected to attacks.\nThe rest of the paper is organized as follows. In section II we briefly introduce the related works. Before presenting"}, {"title": "II. RELATED WORKS", "content": "Aligned with the focus of our study, we provide a concise review of the two research areas most pertinent to our work: Graph Neural Networks and graph structure learning."}, {"title": "A. Graph Neural Networks", "content": "GNNs have emerged as a prominent approach due to their effectiveness in working with graph data. These GNN models can be broadly categorized into two main groups: spectral-based methods and spatial-based methods.\nSpectral-based methods aim to identify graph patterns in the frequency domain, leveraging the sound mathematical precepts of Graph Signal Processing (GSP) [33], [34]. Bruna et al. [35] first extended the convolution to general graphs using a Fourier basis, treating the filter as a set of learnable parameters and considering graph signals with multiple channels. but eigendecomposition requires O(n\u00b3) computational complexity. In order to reduce the computational complexity, Defferrard et al. [36] and Kipf et al. [10] made several approximations and simplifications. Defferrard et al. [36] defined fast localized convolutional filters on graphs based on Chebyshev polynomials. Kipf et al. [10] further simplified ChebNet via a localized first-order approximation of spectral graph convolutions. Despite being spectral-based, GCN can also be viewed through a spatial perspective. In this context, GCN operates by aggregating feature information from the local neighborhood of each node. Recent studies have progressively improved upon GCN [10] by exploring alternative symmetric matrices. For example, The adaptive Graph Convolution Network (AGCN) [37] proposed a Spectral Graph Convolution layer with graph Laplacian Learning (SGC-LL), which efficiently adapts the graph topology according to the data and learning task context. Dual Graph Convolutional Network (DGCN) [38] hamilton2017inductivedesigned a dual neural network structure to encode both local and global consistency. Other spectral graph convolutions have also been proposed. Graph Wavelet Neural Network (GWNN) [39] defines the convolution operator via wavelet transform, which avoids matrix eigendecomposition and provides good interpretability with its local and sparse graph wavelets. Simple Spectral Graph Convolution (S2GC) [40] derives a variant of GCN based on a modified Markov Diffusion Kernel. It achieves a balance between low- and high-pass filter bands to capture both global and local contexts of each node.\nConversely, spatial-based methods draw inspiration from the message passing mechanism employed in Recurrent Graph Neural Network (RecGNN) [41], [42]. They directly define graph convolution in the spatial domain as transforming and aggregating local information. The Neural Network for Graphs"}, {"title": "B. Graph Structure Learning", "content": "GSL attempts to approximate a better structure for the original graph, which is not a newly emerged topic and has roots in prior works in network science [46], [47]. GSL methods can be generally classified into three categories: metric learning approaches, probabilistic modeling approaches and direct optimization approaches.\nMetric learning methods polish the graph structure by learning a metric function that evaluates the similarity between pairs of node representations. For example, Zhang et al. [48] and Wang et al. [9] leveraged cosine similarity to model the edge weights. Zhang et al. [48] detected fake edges with different features and labels between nodes and mitigates their negative impact on prediction by removing these edges or reducing their weight in neural messaging. Wang et al. [9] employed a distinct method to derive the final node embeddings. It diffuses node features across the original graph and integrates the representations from both the generated feature graph and"}, {"title": "III. NOTATIONS AND BACKGROUNDS", "content": "In this section, we present the notations and backgrounds related to this paper."}, {"title": "A. Notions", "content": "Let G = (A,X) be a graph with adjacency matrix A \u2208 R|V|\u00d7|V| and node feature matrix X \u2208 R|V|\u00d7F, where V := {vi}i=1 denotes node set. Following the commonly adopted semi-supervised node classification setting, only a small portion of nodes, denoted as V\u2081 := {vi}i=1, have associated labels available, represented as Y := {yi}i=1, where Yi corresponds to the label of node vi. L = IN \u2212 D-1/2 AD-1/2 denotes the normalized graph Laplacian matrix, where IN represents the identity matrix, and D \u2208 R|V|\u00d7|V| is a diagnoal degree matrix with Di,i = \u2211j Ai,j.\nGiven an input graph G = (A, X) and partial node labels Y, the objective of GSL for GNNs is to jointly learn an optimal graph structure and the GNN model parameters, with the aim of enhancing the node classification performance for unlabeled nodes. The main notations used in this paper are summarized in Table I."}, {"title": "B. Graph Neural Network", "content": "Modern GNNs stack multiple graph convolution layers to learn high-level node representations. The convolution operation usually consists of two steps: aggregation and update, which are respectively represented as follows:\nm(l+1)i = AGGREGATE(l)(hlvj, \u2200vj \u2208 N(vi)) (1)\nh(l+1)i = UPDATE(l)(hlvi , m(l+1)i ) (2)\nwhere m(l+1)i and h(l)i are the message vector and the hidden embedding of node vi at the l-th layer, respectively. The set N(vi) consists of nodes that are adjacent to node vi. If l = 0, then h(0)i = xi. AGGREGATE(l)(\u00b7) and UPDATE(l)(\u00b7) are characterized by the specific model, respectively. In an L-layer network, the final embedding h(L)i is fed to a linear fully connected layer for the classification task."}, {"title": "C. Graph Information Bottleneck", "content": "Inspired by the Information Bottleneck (IB), Wu et al. [27] proposed GIB principle to optimize node-level representations Z to to capture the minimal sufficient information within the input graph data G = (A, X) required for predicting the target Y. The objective is defined as the following optimization:\narg min -I(Z; Y) + \u03b2I(Z;G) (3)\nIntuitively, the first term -I(Z; Y) encourages the representations Z to be maximally informative about the target (sufficient). The second term (Z; G) serves to prevent Z from obtaining extraneous information from the data that is not pertinent to predicting the target (minimal). The Lagrangian multiplier \u03b2 trading off sufficiency and minimality. Existing works based on GIB primarily focus on graph-level tasks [28], [29], [51]. In this paper, we specifically discuss GIB for node classification tasks."}, {"title": "IV. METHODOLOGY", "content": "In this section, we illustrate the proposed streamlined GSL model GaGSL guided by GIB. We begin with the overview of GaGSL in subsection IV-A. Subsequently, we will detail the 3 main parts of GaGSL (i.e., global feature and structure augmentation in subsection IV-B, structure redefinition in subsection IV-C, and GIB guidance in subsection IV-D). Finally, we detail the process of joint iterative optimization in subsection IV-E."}, {"title": "A. Overview", "content": "Most existing GNNs assume that the observed graph structure accurately reflects the true relationships between nodes. However, this assumption often fails in real-world scenarios where graphs are typically noisy. This pitfall in the observed graph can lead to a rapid decline in GNN performance. Thus, a natural and promising approach is to jointly optimize the graph structure and the GNN model parameters in an integrated manner. In this paper, the proposed GaGSL aims to learn compact and informative graph structure for node classification tasks guided by the principle of GIB. Fig. 2 provides an overview of the GaGSL model."}, {"title": "B. Global Feature and Structure Augmentation", "content": "Most of the current research in GSL is predominantly conducted from a single structure. However, this single-structure approach is susceptible to biases that can result in an incomplete understanding of the entire graph structure, ultimately limiting the performance and robustness of the model. To mitigate this concern, we employ a two-pronged approach that combines global feature augmentation and global structure augmentation. This aims to comprehensively understand the graph structure from multiple perspectives."}, {"title": "1) Global Feature Augmentation:", "content": "Nodes located in different regions of a graph may exhibit similar structural roles within their local network topology, and recognizing these roles is essential for understanding network organization. For example, nodes v\u2081 and v6 in Fig. 2 exhibit similar structural roles despite being far apart in the graph. Following previous works [31], [52], we use spectral graph wavelets and empirical characteristic function to generate structural embedding for every node.\nThe filter kernel gs is characterized by a scaling parameter s that controls the reach of the diffusion process, where greater s promotes wider-ranging diffusion.\nIn this paper, we employ the heat kernel gs(\u03bb) = e\u2212\u03bbs . The spectral graph wavelet \u03a8(vi) centered around node vi is given by an N-dimensional vector:\n\u03a8s(vi) = UDiag(gs(\u03bb1 ), gs(\u03bb2 ), ..., gs(\u03bbN ))U\u03b4i (4)\nwhere \u03bbi and U denote the eigenvalue and the eigenvector of the graph Laplacian L, respectively. \u03b4i is the one-hot vector of node vi.\nTo address the node mapping problem, we regard the wavelets as probability distributions and describe them using empirical characteristic functions. Following [28], the empirical characteristic function of vi is:\n\u03a6s (vi, t) = 1/N \u2211n=1N e\u2212i\u03a8s(vi)t (5)\nLastly, the structural embedding hs(vi) of node vi is obtained by sampling a 2-dimensional parametric function (as defined in Eq. (5)) at d different points {t1, t2,...,td}, and then concatenating the resulting values:\nhs(vi) = [Re(\u03a6s (vi, ti)), Im(\u03a6s (vi, ti))]t1,t2,...,td (6)"}, {"title": "2) Global Structure Augmentation:", "content": "In addition to structural role embeddings, we also augment structure from another perspective. Specifically, we employ the widely-used diffusion matrix to capture the global relationships between nodes. We utilize Personalized PageRank (PPR) [53], which provides a comprehensive representation of global structure. The PPR has a closed-form solution given as follows:\n\u00c2 = \u03b1(IN \u2212 (1 \u2212 \u03b1)D\u22121/2 AD\u22121/2)\u22121 (8)\nwhere, \u03b1 denotes the restart probability. Note that S could be dense, thus we just keep 5 edges for each node on some datasets corresponding to the top 5 most affinity nodes on some datasets."}, {"title": "C. Structure Redefinition", "content": "Given two graph Gaf = (A, X) and Gas = (\u00c2, X), to further capture the complex associations and semantic similarities between nodes, we perform a structure estimator for each graph. Specifically, for graph Gaf, we conduct a layer of GCN [10] followed by a MLP layer:\nH1 = GCN(A, X) (9)\nw = MLP([h||h]) (10)\nwhere wij denotes the weight between node vi and node vj, h and h are the embeddings of node vi and node vj, respectively. Here, to mitigate resource consumption in terms of both space and time, we only estimate the h-order neighbors of each node. Then, wij are normalized via sotfmax function to get the final weight:\nSij = exp(wij)/\u2211k exp(wk) (11)\nWe can construct a similarity matrix S1 by Eq. (11). The original graph structure A carries relatively rich information. Ideally, the learned graph structure S1 can complement the original graph structure A, creating an optimized graph for GNNs that enhances performance on the downstream task [25]. Thus, the matrix A is combined with the similarity matrix S1 to get the redefined structure Ar1:\nAr1 = A + \u03b31 \u2217 S1 (12)\nwhere \u03b31 \u2208 [0, 1] is combination coefficient. Similarly, we can obtain the redefined structure Ar2 for graph Gaf:\nAr2 = \u03bc \u2217 A + (1 \u2212 \u03bc) \u2217 \u00c2 + \u03b32 \u2217 S2 (13)\nwhere \u03b32, \u03bc\u2208 [0,1] are combination coefficients. Note that when using Eq. (11) based on the diffusion matrix, top-k neighbors are selected for each node based on the PPR values. After employing structure redefinition, we can obtain two corresponding graphs Gr1 = (Ar1, X) and Gr2 = (Ar2, X)."}, {"title": "D. GIB Guidance", "content": "In this section, the question we would like to answer is how to get the optimal structure A\u2217 for node classification tasks? And how to guide the training of A\u2217 so that it is the minimal sufficient? In order to avoid introducing new parameters and to simplify the model as much as possible, we obtain the final structure by applying the average function, with the inputs being two redefined structures:\nA\u2217 = 1/2 (Ar1 + Ar2) (14)\nPlease review that we aims to learn minimum sufficient graph structure for node classification tasks. In other words, we want the learned representations Z\u2217 based on G\u2217 = (A\u2217, X) to contain only labeled information, while filtering out label-irrelevant noise. To the end, we use GIB to guide the training of A\u2217 so that it is the minimal sufficient. We presume that no information is lost during this process, following the standard practice of MI estimation [54]. Therefore, we have I(G\u2217; Gr1) \u2248 I(Z\u2217; Gr1) and I(G\u2217;Y) \u2248 I(Z\u2217;Y). For the sake of convenience in the following, we replace I(Z\u2217, Y) with I(G\u2217;Y) and I(Z\u2217;Gr1) with I(G\u2217;Gr1). The objectives of GaGSL are as follows:\narg min -I(G\u2217; Y) + \u03b21I(G\u2217; Gr1) (15)\nG\u2217\narg min -I(G\u2217; Y) + \u03b22I(G\u2217; Gr2) (16)\nG\u2217\nBy adding Eq. (15) and Eq. (16), we obtain:\narg min -I(G\u2217; Y) + \u03b21I(G\u2217; Gr1) + \u03b22I(G\u2217; Gr2) (17)\nG\u2217\nWe can simplify Eq. (17) by letting \u03b21 = \u03b22 = \u03b2:\narg min -I(G\u2217; Y) + \u03b2(I(G\u2217; Gr1) + I(G\u2217; Gr2)) (18)\nG\u2217\nwhere the first term \u2212I(G\u2217; Y) is used to encourage G\u2217 to contain maximum information about the labels Y. The second term I(G\u2217; Gr1) + I(G\u2217;Gr2) encourages G\u2217 to contain as little irrelevant information from Gr1 and Gr2 as possible, which is label-irrelevant for predicting the target.\nThe non-Euclidean nature of graph data makes it challenging to estimate the MI in Eq. (18) accurately [55]. Therefore, we introduce a variational upper bound of \u2212I(G\u2217;Y), and use the InfoNCE [56], [57] approximation to calculate I(G\u2217; Gr1) and I(G\u2217; Gr2). First, we examine the prediction term -I(G\u2217; Y).\nProposition 4.1 (Upper bound of \u2212I(G\u2217; Y)). Given graph G with label Y and G\u2217 learned from G, we have\n\u2212I(G\u2217; Y) \u2264 \u2212\u222c p(Y, G\u2217)log(q\u0398(Y|G\u2217))dYdG\u2217 + H(Y) (19)\nwhere q\u0398 (Y|G\u2217) is the variational approximation of the p(Y|G\u2217). We have the following proof based on Sun et al. [28]:\n\u2212I(G\u2217; Y) = \u222c p(Y, G\u2217) log(p(Y,G\u2217)/p(Y)p(G\u2217)) dYdG\u2217\n= \u2212\u222c p(Y, G\u2217) log(p(Y|G\u2217)/p(Y)) dY dG\u2217 (20)"}, {"title": "E. Iterative Optimization", "content": "Optimizing the parameters \u0398 of the structure estimator, \u03a6 of the MI calculator, and \u03a9 of the classifier simultaneously is challenging. The interdependence among them further complicates this process. In this study, we employ an alternating optimization approach to iteratively update \u0398, \u03a6, and \u03a9 inspired by Wang et al. [22].\n1) Update \u03a6: The parameters involved in Eq. (24) and (25) are regarded as the parameters \u03a6 of MI calculator. To encourage G\u2217 to contain as little label-irrelevant information from Gr1 and Gr2 as possible, the objective function used to optimize the MI calculator is presented as follows:\nLMI = L(G\u2217, Gr1) + L(G\u2217, Gr2) (28)\n2) Update \u03a9: We can obtain the final learned structure A\u2217 by Eq. (14). Then we employ two-layer of GCN [10] to obtain node representations.\nZ\u2217 = GCN(A\u2217, X) (29)\nThe parameters involved in Eq. (29) are collectively considered as the classifier\u2019s parameters \u03a9, and the cross-entropy loss is utilized for optimization:\nLcls = \u2211 Cross-Entropy(Zv, Yv) (30)\n3) Update \u0398: After training the classifier and MI calculator, we proceed with the continuous optimization of the structure estimator parameters \u0398. Guided by GIB, the resulting loss function is as follows:\nL = Lcls \u2212 \u03b2LMI (31)\nwhere \u03b2 is a balance parameter trading off sufficiency and minimality. The first term Lcls is to motivate G\u2217 to contain maximal information about the labels Y in order to improve the performance on the predicted target. The intention of the second term LMI is to minimize the information in G\u2217 from Gr1 and Gr2 that is label-irrelevant for predicting the target."}, {"title": "V. EXPERIMENTS", "content": "In this section, we carry out a comprehensive evaluation to assess the effectiveness of the proposed GaGSL model. We first compare the performance of GaGSL against several state-of-the-art methods on the semi-supervised node classification task. Additionally, we perform an ablation study to verify the importance of each component within the GaGSL model. Then, we analyze the robustness of GaGSL. Finally, we present the graph structure visualization, hyper-parameter sensitivity and values in the learned structure."}, {"title": "A. Experiment Setup", "content": "1) Datasets: The eight datasets we employ consist of four academic networks (Cora, Citeseer, Wiki-CS, and MS Academic (MS)), three non-graph datasets (Wine, Breast Cancer (Cancer), and Digits) that are readily available in scikit-learn [60], and a blog graph dataset Polblogs. Table II provides a summary of the statistical information about these datasets. It is important to note that, for the non-graph datasets, we adopt the approach described in [25] and construct a kNN graph as the original adjacency matrix.\n2) Baselines: To demonstrate the effectiveness of our proposed method, we compare the proposed GaGSL with two categories of baselines: three classical GNN models (GCN [10], GAT [5], SGC [20]) and four GSL based methods (ProGNN [19], IDGL [25], GEN [22], PRI-GSL [31]). The details are given as follows.\na) GCN: It directly encodes the graph structure using a neural network, and trains on a supervised target for all labeled nodes. This neural network employs an efficient layer-wise propagation rule, which is derived from a first-order approximation of spectral graph convolutions.\nb) GAT: It introduces an attention-based mechanism for classifying nodes in graph-structured data. By stacking layers, it enables nodes to incorporate features from their neighbors and implicitly assigns different weights to different nodes in the neighborhood. Additionally, this model can be directly applied to inductive learning problems.\nc) SGC: It alleviates the excessive complexity of GCNs by iteratively eliminating nonlinearities between GCN layers and consolidating weights into a single matrix.\nd) Pro-GNN: To defend against adversarial attacks, it iteratively eliminates adversarial structure by preserving the graph low rank, sparsity, and feature smoothness, while maintaining the intrinsic graph structure.\ne) IDGL: Building on the principle that improved node embeddings lead to better graph structure, it introduces an end-to-end graph learning framework for the joint iterative learning of graph structure and embeddings. Additionally, it frames the graph learning challenge as a similarity metric learning problem and employs adaptive graph regularization to manage the quality of the learned graph.\nf) GEN: It is a graph structure estimation neural network composed of two main components: the structure model and the observation model. The structure model characterizes the underlying graph generation process, while the observation model incorporates multi-order neighborhood information to accurately infer the graph structure using Bayesian inference techniques.\ng) PRI-GSL: It is an information-theoretic framework for learning graph structure, grounded in the Principle of Relevant Information to manage structure quality. It incorporates a role-aware graph structure learner to develop a more effective graph that maintains the graph\u2019s self-organization.\n3) Implementation: For three classical GNN models (GCN, GAT, SGC), we use the corresponding Pytorch Geometric library implementations [62]. For four GSL based methods (Pro-GNN, IDGL, GEN, and PRI-GSL), we utilize the source codes provided by the authors and adhere to the settings outlined in their original papers, with careful tuning. For different datasets, we follow the original splits on training/validation/test. For the proposed GaGSL, we use Adam [63] optimizer and adopt 16 hidden dimensions. We set the learning rate for the classifier and the MI calculator to a fixed value of 0.01, while tuning it for the structure estimator across the values {0.1, 0.01, 0.001}. On the MS dataset we set combination coefficient \u00b5 to 0. On the other datasets we set \u00b5 to 1. We test the combination coefficients \u03b31 and \u03b32 in the range {0.1, 0.5}. The dropout for classifier is chosen form {0.3, 0.5, 0.7, 0.9}, and the dropout for MI calculator is turned amongst {0.2, 0.4, 0.6, 0.8}."}, {"title": "B. Node Classification", "content": "In this section, we assess the proposed GaGSL on semi-supervised node classification, with the results presented in"}, {"title": "C. Ablation Study", "content": "Here, we describe the results of the ablation study for the different modules in the model. We report F1-macro and standard deviation results over 5 independent trials using different random seeds. According to the ablation study results presented in Table IV, the model\u2019s performance exhibits a significant decline when the structure augmentation (SA) component is removed. The substantial drop in performance observed without the SA component suggests that the structure augmentation mechanism is highly important and plays a crucial role in enabling the model to effectively capture salient features. Notably, turning off any of the individual components results in a significant decrease in the model\u2019s performance across all evaluated datasets, which underscores the effectiveness and importance of these components. Furthermore, the results highlight the benefits of leveraging GIB to guide the model training process."}, {"title": "D. Defense Performance", "content": "In this section, we perform a careful evaluation of various methods, specifically focusing on comparing GSL models. These models exhibit the ability to adapt the original graph structure, making them more robust in comparison to other GNNs. To ensure a comprehensive evaluation, we conduct attacks on both edges and features, respectively.\n1) Attacks on edges: To attack edges, we generate synthetics dataset by deleting or adding edges on Cancer, Cora, and Citeseer following [25]. Specifically, for each graph in the dataset, we randomly remove 5%, 10%, 15% edges or randomly inject 25%, 50%, 75% edges. We select the poisoning attack [64] and first generate the attacked graphs and subsequently train the model using these graphs. The experimental results are displayed in Figs. 3 and 4.\nAs can be seen in Figs. 3 and 4, GaGSL consistently outperforms all other baselines as the perturbation rate increases. On the Cancer and Citeseer datasets, the performance of GaGSL fluctuates lightly under different perturbation rates. On the Cora dataset, although GaGSL\u2019s performance declines with increasing perturbation rates, it still outperforms other baselines. Specifically, our model improves over vanilla GCN by 12.6% and over other GSL methods by 0.8%-11% when 50% edges are added randomly on Cora. This suggests that our method is more effective against robust attacks. We also find that the GSL method (i.e. Pro-GNN [19], IDGL [25], GEN"}, {"title": "E. Graph Structure Visualization", "content": "Here, we visualize the probability matrices of the original graph structure, the perturbed graph structure, and the graph structure learned by GaGSL and draw them in Fig. 6. From the visualization, we can observe that in the perturbed graph structure, there exist noisy connections, as indicated by the higher probability of edges between different communities compared to the original graph structure. These noisy connections degrade the quality of the graph structure, thereby reducing the performance of GNNs. Additionally, we observed that, in contrast to the perturbed graph, the learned graph structure weakens the connections between communities and strengthens the connections within communities.\nThis observation is expected because GaGSL is optimized based on the GIB principle. The GIB optimization allows GaGSL to effectively capture information in the graph structure that contributes to accurate node classification while constraining information that is label-irrelevant. This optimization process ensures that the learned graph structure is robust against noisy connections and focuses on preserving the most informative aspects of the graph for the task at hand."}, {"title": "F. Hyper-parameter Sensitivity", "content": "In this subsection", "hyperparameters": "combination coefficient \u03b31 in Eq. (12), \u03b32 in Eq. (13), as well as the balance parameter \u03b2 in Eq. (31). More concretely, we vary the value of \u03b31, \u03b32 and \u03b2 to analyze their impact on the performance of our proposed model. We vary \u03b31 or \u03b32 from 0 to 1, and \u03b2 from 0 to 0.1. For clarity, we report the node classification results on Cora and Polblogs datasets, as similar trends are observed across other datasets. The results are presented in Figs. 7 and 8.\nAs can be observed from Fig. 7, by tuning the value of \u03b31 (or \u03b32), we can achieve better node classification"}]}