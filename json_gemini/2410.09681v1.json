{"title": "LoRD: Adapting Differentiable Driving Policies to Distribution Shifts", "authors": ["Christopher Diehl", "Peter Karkus", "Sushant Veer", "Marco Pavone", "Torsten Bertram"], "abstract": "Distribution shifts between operational domains can severely affect the performance of learned models in self-driving vehicles (SDVs). While this is a well-established problem, prior work has mostly explored naive solutions such as fine-tuning, focusing on the motion prediction task. In this work, we explore novel adaptation strategies for differentiable autonomy stacks consisting of prediction, planning, and control, perform evaluation in closed-loop, and investigate the often-overlooked issue of catastrophic forgetting. Specifically, we introduce two simple yet effective techniques: a low-rank residual decoder (LoRD) and multi-task fine-tuning. Through experiments across three models conducted on two real-world autonomous driving datasets (nuPlan, exiD), we demonstrate the effectiveness of our methods and highlight a significant performance gap between open-loop and closed-loop evaluation in prior approaches. Our approach improves forgetting by up to 23.33% and the closed-loop OOD driving score by 8.83% in comparison to standard fine-tuning.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine learning-based SDV architectures have demonstrated impressive performance across many tasks. However, they often struggle with distribution shifts between training and deployment scenarios. As visualized in the SDV example in Fig 1, distribution shifts can result from differences in traffic rules, social norms, weather conditions, traffic density, vehicle types, etc., and can cause severe degradation in model accuracy, leading to catastrophic failures [14]. For instance, a recent study on trajectory prediction [21] revealed significant performance degradation during open-loop (OL) evaluations.\nHow can we adapt learned driving policies to distribution shifts using only a small amount of out-of-distribution (OOD) data? Our analysis indicates that existing approaches suffer from catastrophic forgetting. To address this challenge, we introduce architectural and training procedure improvements specifically designed for structured policies.\nThe first generation of learning-based self-driving approaches used unstructured policies, that directly approximate the entire decision-making process using a neural network, mapping a sequence of observations to an action [6], [33], [41]. More recently, structured policies have emerged, that maintain a modular yet differentiable architecture [5], [8], [29], [38], that offer various advantages such as verifiability, interpretability and better generalization while addressing challenges like compounding errors and information bottlenecks [38]. For instance, planning [15], [17], [31], [38], [56] often employs differentiable optimization layers [2], [3] with learned cost functions.\nOne of our key insights is that structured policies can also be better adapted to distribution shifts by adding residual layers to specific components. Specifically, we propose to add a Low-Rank Residual Decoder (LORD) in the structured policy during fine-tuning. For example, to adapt to different traffic norms in a new country, we can add a LoRD to the cost estimation network. Further, to mitigate catastrophic forgetting [1] when fine-tuning on a small OOD dataset, we propose multi-task fine-tuning, where we train on a combination of in-distribution (ID) and OOD data.\nPrevious works have proposed adaptation strategies such as fine-tuning with a few samples from the OOD domain (e.g., [35], [42]). However, most of these efforts focus on adapting individual tasks like trajectory prediction (unstructured policies) while overlooking integrating these tasks into a more complex, differentiable stack (structured policies) that includes prediction, planning, and control modules for SDVs (see Fig. 2). Furthermore, there is a lack of research investigating the impact of these shifts on closed-loop (CL) performance, and evaluation is often performed exclusively in the OOD domain. We argue that evaluations should also be performed in the source (ID) domain to investigate forgetting and avoid the need for training a separate model for each domain.\nWe evaluate our proposed methods on two real-world autonomous driving datasets (nuPlan, exiD), comparing them to prior work in both open-loop and closed-loop with cross-domain evaluations (ID and OOD). Our analysis shows that our method improves open-loop OOD performance while reducing forgetting by 23.33% (ID) in comparison to fine-tuning (metric: bminSFDE) and improves the closed-loop OOD performance by 8.83 % (metric: CL-NR).\nOur specific contributions are as follows. First, we introduce a low-rank residual decoder (LoRD) that predicts residuals (e.g., cost parameters) for structured policies to adapt to the OOD domain. Second, we propose fine-tuning using a mix of ID and OOD data to mitigate catastrophic"}, {"title": "II. RELATED WORK", "content": "SDV Policy Architectures. Unstructured policies map from low-level sensor data [25], [41] or mid-level representations [33] to actions. These approaches benefit from the absence of information bottlenecks and scalability with data, but their downside is the lack of interpretability [38]. Trajectory prediction methods can also be viewed as unstructured policies as networks are trained via imitation learning [9].\nAnother research direction models policies in a more structured manner using differentiable algorithm networks [39]. These structured policies are also end-to-end trainable, while incorporating differentiable model-based components for perception [37], prediction [16], [17], [44], and planning and control [5], [15], [31], [38], [56]. However, no prior work has investigated the adaptation of structured policies to distribution shifts.\nAdaptation to Distribution Shifts. Distribution shifts can degrade model performance [21], leading to catastrophic failures. To mitigate this, [18] penalizes predicted OOD states using uncertainty estimates by ensembling learned models. The authors of [24], [52] propose using large language models to adapt parameters of non-differentiable planners at test time. Another line of transfer learning research focuses on fine-tuning networks with a few samples from the OOD domain. Various works [34], [35], [42], [45], [49], [54] have investigated adaptation in the trajectory prediction setting (open-loop), which could be considered as adapting unstructured policies whereas we adapt structured policies consisting of prediction, planning, and control and also evaluate in closed-loop. While it has been shown in the natural language processing (NLP) domain [36] that multi-task fine-tuning can improve performance, this has not yet been explored for structured SDV policies.\nResidual Learning. Residual learning has been widely applied in fields such as computer vision [26], [50] and NLP to enhance model performance and adaptation to distribution shifts. For instance, LoRA [28] is a commonly used parameter-efficient fine-tuning strategy for large language"}, {"title": "III. PROBLEM FORMULATION", "content": "Policy Representations. Define the policy \\(\\pi: o \\rightarrow a\\) that maps from a sequence with length \\(H \\in \\mathbb{R}+\\) of observations \\(o \\in \\mathbb{R}^{H\\times n_o}\\) to an action. In this work, \\(a\\) is a control or state trajectory over a future horizon \\(T\\in \\mathbb{R}+\\), with \\(a\\in \\mathbb{R}^{T\\times n_a}\\). The state and observation dimensions are denoted by \\(n_a\\) and \\(n_o\\), respectively. We can either choose an unstructured policy \\(a = \\pi_w(o)\\) (explicit function) or a structured policy [23] (implicit function) defined by: \\(a^* = \\arg \\min_a E_w(a, o)\\), where the subscript \\(w\\) denotes learnable parameters and \\(E_w: \\mathbb{R}^{T\\times n_a} \\times \\mathbb{R}^{H\\times n_o} \\rightarrow \\mathbb{R}\\) is a cost function. We choose a neural network to represent the policy. We can also generalize this to a broader class \\(a^* = A(f_w(o))\\)"}, {"title": "IV. METHODS", "content": "We now describe our contributions in terms of architecture (residual decoder) and data (multi-task fine-tuning).\nA. Low-Rank Residual Decoder (LoRD)\nThe main idea of our approach is that driving between different domains has similarities in various aspects (e.g., in terms of vehicle kinematics and agent behavior) but differs by a few features (e.g., left- vs. right-handed driving) [42]. For instance, most drivers try to avoid collisions while reaching a goal in a comfortable manner. We model this slight distribution shift using a residual decoder \\(f_{res}\\), with parameters \\(\\xi\\) that outputs a residual value \\(y_{res} = f_{res,\\xi}(z)\\), which is added to the output of the base decoder during adaptation:\n\\[\\hat{y} = y + y_{res}.\\]\nAnother hypothesized advantage of the residual is that the base network can maintain performance on the ID data, whereas adapting the weights of the residual decoder \\(f_{res, \\xi}\\), can account for the distribution shift. Lastly, residuals provide a direct path for gradient flow, which helps mitigate the vanishing gradient problem during fine-tuning [26].\nIn the case of a structured policy, the residual decoder of this work outputs residual cost parameters \\(w_{res}\\) to parameterize a residual cost \\(E_{res}\\). For an unstructured policy, the residual decoder outputs a residual action \\(a_{res}\\).\nWhile \\(f_{res}\\) can have various structures, this work uses a single linear layer without bias to minimize the number of additional parameters. Inspired by the fine-tuning of large language models [28] and applications in marginal trajectory prediction [42], we perform a low-rank matrix decomposition\n\\[f_{res,\\xi} = BA,\\]\nleading to a further parameter reduction and computational efficiency. \\(B \\in \\mathbb{R}^{n_B\\times r}\\) and \\(A \\in \\mathbb{R}^{r\\times n_a}\\) describe matrices with rank \\(r < \\min(n_a, n_B)\\). The matrices are initialized such that the residuals initially do not influence the original network, which stabilizes the training [28]. We also perform dropout on the input of \\(f_{res}\\), with probability \\(p_{drop}\\) for better generalization. This decoder is referred to as Low-Rank Residual Decoder.\nAlthough residuals can also be added in the encoder [42], [50], we opt to add them in the decoder because differentiable joint prediction and planning methods [17], [30] model interactions, occurring in multi-agent driving scenarios, explicitly in the decoders and differentiable optimization layers. This contrasts with applications of marginal trajectory predictions [42], where the decoder often consists of a simple MLP head, and predictions are batched over agents without explicit interaction modeling [12].\nB. An Energy-based Model Perspective\nNext, we highlight a connection of the proposed LORD to the compositional properties [19], [20] of energy-based models [43] in the case of structured policies.\nWe begin by interpreting the cost as an energy [22] and assuming a probabilistic policy \\(p_w(a|o)\\). One possible way to turn energies into probabilities is by using the Gibbs distribution \\(p_w(a|o) = \\frac{exp(-E_w(a,o))}{Z_w(o)}\\), with an underlying partition function \\(Z_w(o) = \\int exp(-E_w(a, o)) da\\). Prior work [11], [19], [20] has shown how to compose energies using a product of experts [27] [; \\(p_w (a, o) \\propto \\exp(-E_w (a,o)\\).\nIn the above described setting, LoRD predicts \\(w_{res}\\) used to parametrize a cost/energy \\(E_{w_{res}}\\) resulting in \\(p_{w,wres} (a, o) \\propto \\exp(- (E_w(a, o) + E_{w_{res}} (a, o))\\).\nHence, under our formulation in Sec. IV-A, we aim to find actions \\(a\\) that have high likelihood under a composition of the old distribution (ID) and a residual distribution, whereas the weighting of both is learned. For instance, a lane-keeping action that was highly probable on highways (ID) may also need to account for new residual factors such as narrower streets in an urban environment (residual distribution).\nC. Multi-Task Fine-Tuning\nFine-tuning with data from the OOD target domain can lead to catastrophic forgetting of the original ID domain [1]. To mitigate this issue, we propose using multi-task fine-tuning, which facilitates information sharing across various tasks and has demonstrated effectiveness in the natural language processing domain [36]. Specifically, during fine-tuning, we sample data from a distribution \\(P_{MT} = P_{ID} + \\alpha P_{OOD}\\), where \\(\\alpha\\) represents the ratio of ID to OOD data."}, {"title": "V. EMPIRICAL EVALUATION", "content": "This section investigates the following research questions:\nQ1: How do LoRD and multi-task fine-tuning (FT) impact the open-loop and closed-loop performance in ID and OOD scenarios compared to the state-of-the-art baselines?\nQ2: Are structured policies (SP) and unstructured policies (USP) similar impacted by adding a LoRD?\nOur key findings are the following: F1: LoRD performs similar or better than the baselines (OOD) and keeps the ID performance better by a large margin in open-loop. Multi-task fine-tuning improves the performance of all methods. F2: While the trend for SP is the same as in open-loop and LORD is the best adaptation method, we observe a large gap between open-loop and closed-loop performance, which is in line with prior work [13], [57]. Moreover, the fine-tuning data distribution has a high impact in closed-loop. F3: Unstructured policies benefit less using LoRD. F4: All design choices contribute to the performance.\nA. Experimental Setup\nDatasets and Evaluation Environments. exiD [48] is an interactive highway dataset captured in Germany that contains recordings of seven locations with no geographic overlap. We split the data such that the ID splits contain data from the same six maps but from different sequences. Recordings of the last map are preserved for the OOD split. One sample constitutes of 4s future with 2s of history sampled at \\(\\Delta t = 0.2\\) s. The ID sets contain 110431 (train) / 12425 (val) / 13884 (test) samples from different sequences. The fine-tuning OOD sets contain 12378 (train) / 3459 (val) / 3561 (test) samples. nuPlan [40] is a standard benchmark for motion planning in urban driving environments, enabling learning using large-scale data from different geographies. We use data from Boston and Pittsburgh (ID, right-handed traffic) to train a base model and data from Singapore (OOD, left-handed traffic) for fine-tuning. We evaluate models in both domains (ID, OOD), in open-loop and closed-loop. During closed-loop evaluation, the planned trajectory is executed. Data is sampled with \\(\\Delta t = 0.1\\)s, and the future trajectory and agent history length are 8s and 2s, respectively. The ID sets contain 155366 (train) / 21853 (val) / 22449 (test) samples split by sequences. The fine-tuning OOD set contains 32,901 (train) / 7067 (val) / 9182 (test) samples. We extract 80 unseen scenarios for closed-loop evaluation per domain (ID or OOD) accounting for balancing scenario types.\nWhile our approach can potentially be used for any kind of distribution shift, we test it for geographical shifts as these are easy to simulate with real-world data.\nModels. We evaluate our contributions on top of two different base models: On exiD, the model consists of EPO [17], a differentiable game-theoretic gradient-based optimization approach for joint multi-agent predictions and control (structured multi-agent policy). We apply the residual on all predicted game parameters (cost weights and agents' goals) and the initial joint control trajectories. The nuPlan experiments use DTPP [30], a structured policy. During learning the approach differentiates through sampling-based optimization. We use the integration in tu_garage [13] for closed-loop simulation provided by [24]. The residual is added to the predicted agents' trajectories used in the SDV cost function. We further explore extensions of DTPP, training with additional rewards, and history dropout (see F2), which are specifically useful for closed-loop control. Baselines. We compare to the following baselines:\n\u2022 FT: Fine-tuning of all parameters as in [35].\n\u2022 PFT: Partial model fine-tuning adapting the last layer of each decoder as in [35].\n\u2022 MOSA [42]: MOSA employs low-rank adaptation layers in the encoder while freezing the base networks. MOSA (F) applies adapters in the attention-based fusion layers from [17], and MOSA (A+L) adds adapters in the agent encoders.\n\u2022 PA [50]: Parallel adapter similar to MOSA but without freezing of weights.\nOn exiD (prediction) we compare extensively against prior methods, while for the planning and control experiments on nuPlan we focus on the open-loop to closed-loop gap and compare against the strongest baseline.\nHyperparameters. To ensure a fair comparison, we performed grid searches for the hyperparameters (learning rate, dropout probability, rank in Equ. (3)) of all our models, baselines, and ablations. For open-loop evaluation we use the checkpoint with the lowest validation metrics (exiD: minSADE, nuPlan: ADE). For closed-loop the last checkpoint is chosen as our preliminary experiments showed that this correlated better with closed-loop performance, which is in line with related work [32].\nMetrics. For closed-loop evaluation, we use the official CL-scores (1: best score, 0: worst score) of the nuPlan simulation, where CL-R denotes a reactive and CL-NR a non-reactive (log-replay) simulation of the other agents. The open-loop evaluation uses the following standard metrics. On nuPlan, we use the Average Displacement Error ADE and Final Displacement Error (FDE) of the SDV's trajectory. For joint predictions (exiD), the minSADE and minSFDE are the multi-modal scene-level equivalents [17]. The bminSFDE (official ranking metric of Argoverse 2 [55]) penalizes joint predictions with poorly calibrated probabilities. Lastly, we compute the Scene Miss Rate (SMR), which describes the percentage of samples where minSFDE \u2265 3.6 m. We report the performance in the OOD domain and the average performance over the ID and OOD domain denoted by ID+OOD.\nB. Results\nNext, we will describe our findings, which are visualised in Tab. II (exiD, open-loop) and III (nuPlan, open-loop and closed-loop). Moreover, Fig. 3 and Fig. 4 provide a dataset analysis and qualitative results for nuPlan. Lastly, Fig. 5 and Tab. IV present ablation studies."}, {"title": "VI. CONCLUSION, DISCUSSION, AND FUTURE WORK", "content": "This work presented an approach for adapting differentiable prediction, planning, and control. Our results showed improved performance on two datasets for structured policies in both open-loop and closed-loop settings. We also observed a significant gap between open-loop and closed-loop metrics, highlighting the need for more focus on closed-loop evaluation. One limitation is that the residual decoder introduced a small increase in parameters (nuPlan: 0.09%, exiD: 0.42%). Future work could explore merging LoRD weights with the base network to address this. Additionally, future research should investigate adapting to various shifts,"}]}