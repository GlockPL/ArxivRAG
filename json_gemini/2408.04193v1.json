{"title": "Uncertainty-Aware Crime Prediction With Spatial Temporal Multivariate Graph Neural Networks", "authors": ["Zepu Wang", "Xiaobo Ma", "Huajie Yang", "Weimin Lvu", "Peng Sun", "Sharath Chandra Guntuku"], "abstract": "Crime forecasting is a critical component of urban analysis and essential for stabilizing society today. Unlike other time series forecasting problems, crime incidents are sparse, particularly in small regions and within specific time periods. Traditional spatial-temporal deep learning models often struggle with this sparsity, as they typically cannot effectively handle the non-Gaussian nature of crime data, which is characterized by numerous zeros and over-dispersed patterns. To address these challenges, we introduce a novel approach termed Spatial Temporal Multivariate Zero-Inflated Negative Binomial Graph Neural Networks (STMGNN-ZINB). This framework leverages diffusion and convolution networks to analyze spatial, temporal, and multivariate correlations, enabling the parameterization of probabilistic distributions of crime incidents. By incorporating a Zero-Inflated Negative Binomial model, STMGNN-ZINB effectively manages the sparse nature of crime data, enhancing prediction accuracy and the precision of confidence intervals. Our evaluation on real-world datasets confirms that STMGNN-ZINB outperforms existing models, providing a more reliable tool for predicting and understanding crime dynamics.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurate crime forecasting can significantly enhance police deployment strategies and infrastructure allocation, substantially improving urban safety [8, 9, 22]. With advancements in deep learning [2, 12, 15, 16, 23], researchers are increasingly utilizing complex neural network architectures to model crime patterns. These include recurrent neural networks [9], convolutional neural networks [20], and graph neural networks [8, 21]. These models aim to capture both the spatial-temporal correlations of crime patterns and the interrelationships among different crime categories. Specifically, spatial-temporal graph neural networks are good at extracting spatial-temporal correlations from urban data [17, 19, 24].\nHowever, the deterministic models predominantly used in crime forecasting implicitly presuppose that the outputs follow a Gaussian distribution [1, 6, 7, 9, 29], significantly simplifying the variance structure [3, 5]. Despite their effectiveness, most existing spatial-temporal prediction methods face limitations when predicting urban crimes due to the sparsity of crime patterns. The data for each fine-grained urban region is extremely sparse [26, 27]. A plethora of zero values in crime data indicates that it is not appropriate to assume that crime data patterns follow a Gaussian distribution. Research demonstrates the effectiveness of the Zero-Inflated Negative Binomial (ZINB) distribution in analyzing sparse travel demand data [29] and sparse traffic risk forecasting [4]. Additionally, uncertainty quantification with ZINB has been conducted for many urban and transportation tasks [11, 28].\nIn this paper, we propose the Spatial Temporal Multivariate Zero-Inflated Negative Binomial Graph Neural Networks (STMGNN-ZINB)-a comprehensive framework designed for joint numeric prediction and uncertainty quantification of urban crime. Our main contributions are as follows:\n\u2022 We utilize the Zero-Inflated Negative Binomial (ZINB) distribution to model crime, effectively capturing zero-inflation and addressing data sparsity.\n\u2022 We integrate ZINB with spatial-temporal multivariate graph neural networks, enabling precise quantification of the sparse and discrete uncertainty in crime data."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Problem Definition", "content": "Formally, a graph G is defined as an ordered pair (V, E), where V represents the set of vertices (or nodes) and E represents the set of edges. Suppose the historical crime data is embedded in graph G with C multivariate crime features over T time intervals. The historical time series features can then be represented as $X \\in \\mathbb{R}^{N\\times T \\times C}$.\nThe objective of this task is to learn a mapping function f that uses the historical crime data X and the graph structure G as inputs to forecast future crime data for Q time intervals. Our goal is not only to predict the expected values of future crime but also to estimate the confidence intervals for these predictions. Consequently, we denote our output as $X \\in \\mathbb{R}^{N\\times Q \\times C \\times Z}$, where Z represents the parameters of the assumed distribution of crime data."}, {"title": "2.2 Zero-Inflated Negative Binomial Distribution", "content": "In our model, we assume that the distribution of crimes follows the Zero-Inflated Negative Binomial (ZINB) distribution [10, 29]. The probability mass function (PMF) of a random variable following the ZINB distribution is given by:\n$P(Y = y) = \\begin{cases}\n    \\pi + (1 - \\pi) (1-p)^r & \\text{if } y = 0 \\\\\n    (1 \u2013 \\pi) \\binom{y+r-1}{r-1} p^y (1 \u2013 p)^r & \\text{if } y = 1, 2, 3, . . .\n\\end{cases}$\t\t\\qquad(1)\nHere, \u03c0 represents the probability of an extra zero, indicating zero inflation, which in this context, signifies the likelihood of no occurrences of a specific type of crime. The parameters r and p are the shape parameters of the traditional negative binomial distribution, where r affects the dispersion and p is the probability of success in each experiment."}, {"title": "2.3 Diffusion Graph Convolution Networks (DGCNS)", "content": "To capture spatial correlations within a predefined graph structure, we utilize Diffusion Graph Convolutional Networks (DGCNs). These models employ the concept of diffusion processes on graphs to effectively capture the spread of information across the graph's topology.\nAt the heart of DGCNs is the diffusion convolution operation, which can be considered a generalization of the traditional convolutional operations adapted for graph data. The underlying principle is to simulate a diffusion process on the graph, allowing information to propagate from a node to its neighbors through multiple steps or layers. This dynamic can be mathematically articulated using the graph Laplacian and its exponentiations to represent various degrees of diffusion.\nThe diffusion convolution operation in a DGCN is mathematically defined as:\n$H^{(l+1)} = \\sigma (D^{-1} A H^{(l)} W^{(l)} + B^{(l)} H^{(l)})$\nwhere: - $H^{(l)}$ denotes the node features (or hidden states) at layer l, - A is the adjacency matrix of the graph, where $A_{ij}$ represents the edge weight between nodes i and j, with $A_{ij} = 0$ if no edge exists, - D is the diagonal degree matrix, with each diagonal element $D_{ii}$ being the sum of the weights of all edges connected to node i, - $W^{(l)}$ is the weight matrix for layer l, which is learned during the training process, - $B^{(l)}$ is the bias term for layer l, - $\u03c3(\u00b7)$ is a non-linear activation function, such as the Rectified Linear Unit (ReLU).\nThis formulation enables the network to effectively balance the influence of immediate neighbors (through $D^{-1} A H^{(l)}$) and the retention of the current node's features (via $B^{(l)} H^{(l)}$), thus integrating both local and global information in the learning process."}, {"title": "2.4 Multivariate-Temporal Convolutional Networks (MTCNs)", "content": "Traditional Temporal Convolutional Neural Networks (TCNs) are a specialized variant of convolutional neural networks designed to process sequence data effectively. The core principle of TCNs is to apply a shared gated 1D convolution across a specified width $w_i$ in the $l^{th}$ layer, allowing the integration of information from $w_i$ adjacent time points. In this study, we adapt traditional TCNs to simultaneously capture multivariate-temporal correlations. Specifically, we aim to incorporate information from all crime types at preceding time points for predicting a specific crime type at the current time point. This integration is facilitated by reshaping the multivariate and temporal dimensions together.\nEach TCN layer $H_l$ receives input from the preceding layer $H_{l-1}$ and updates as follows:\n$H^{(l+1)} = f(\\Gamma_l * H^{(l)} + b),$\\qquad(2)\nwhere \u0393 represents the convolution filter for the layer, * denotes the shared convolution operation, and b represents the bias. If the previous hidden layer follows $H_{l-1} \\in \\mathbb{R}^{B\\times |V|\\times (w_{l-1}C)}$, then the convolution filter is $\\Gamma_l \\in \\mathbb{R}^{(w_lC)\\times (w_{l-1}C)}$, ensuring that $H_l \\in \\mathbb{R}^{B\\times |V|\\times (w_lC)}$. Notably, if there is no padding in each TCN layer, it follows that $w_l < w_{l-1}$.\nThe primary motivation for capturing multivariate-temporal correlations is to leverage the streamlined architecture of traditional TCNs for rapid training. Moreover, given that the temporal and multivariate dimensions typically exhibit shorter lengths compared to the spatial dimension, they can be treated as a single combined dimension. This approach helps mitigate the risk of extracting spurious relationships from the training data, which can lead to overfitting-a common issue when neural networks attempt to model relationships between two dimensions with extensive lengths [25]."}, {"title": "2.5 General Structure", "content": "As illustrated in Figure 1, we utilize DGCNs to capture spatial dependencies, resulting in spatial embeddings \u03c01, P1, and r1. Concurrently, MTCNs are employed to capture multivariate temporal correlations, yielding multivariate temporal embeddings \u03c02, P2, and r2. These embeddings are then fused into combined parameters \u03c0, p, and r using the Hadamard product. This method of integration is inspired by recent advances in uncertainty quantification techniques [29].\nTo address the issue where zeros result in infinite values for the KL-divergence based variational lower bound [29], we opt to directly utilize the negative likelihood as our loss function. This approach allows for a more accurate fitting of the distribution to the data, circumventing the limitations posed by zeros in the calculation of the KL-divergence.\n$NLL = -\\begin{cases}\n    \\sum_{i=1}^{|M|}  log[\\pi_i + (1 - \\pi_i)\u00b7 (1 - p_i)^{r_i}]  &  \\text{if } y_i = 0 \\\\\n    \\sum_{i=1}^{|M|} log[(1 \u2013 \\pi_i) \\frac{\\Gamma(r_i+y_i)}{\\Gamma(r_i)\\Gamma(y_i+1)} p_i^{y_i} (1-p_i)^{r_i}] & \\text{if } y_i > 0\n\\end{cases}$\t\\qquad(3)"}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 Experiment Setup", "content": "3.1.1 Data. Following the methodology in [14], our experiments utilize two distinct crime datasets from New York City (NYC) and Chicago (CHI). These datasets cover various types of crime incidents, including Robbery and Larceny in NYC and Damage and Assault in CHI, across different locations within each city. For the purpose of our experiments, we divided each city into a spatial grid of 3 km \u00d7 3 km, resulting in 256 regions for NYC and 168 regions for CHI. Our prediction targets are set at a daily resolution.\nThe datasets were split into training and testing sets with a 7:1 ratio along the time dimension. Additionally, we used the last 30 days of the training set as a validation set to fine-tune the model parameters. Table 1 provides a summary of the dataset statistics.\n3.1.2 Evaluation Metrics. We evaluate the performance of different models from three perspectives: 1. Point Estimation: We use Mean Absolute Error (MAE) to measure the accuracy of the mean value of the predicted distributions. A lower MAE indicates better accuracy. 2. Uncertainty Quantification: This includes Mean Prediction Interval Width (MPIW) and Prediction Interval Coverage Probability (PICP) within the 10%-90% confidence interval. MPIW calculates the average width of the confidence intervals, reflecting the extent of uncertainty in the predictions. PICP quantifies the percentage of actual data points that fall within these intervals, aiming for a coverage as close to 90% as possible. Additionally, KL-Divergence is used to assess the similarity between the predicted and actual data distributions, with lower values indicating better model performance. 3. Discrete Metrics: We round our results to their closest integer to measure the true-zero rate and the F1-score. The true-zero rate evaluates the model's ability to accurately represent data sparsity, whereas the F1-score assesses the accuracy of discrete predictions. Higher values of the true-zero rate and F1-score signify superior performance.\n3.1.3 Baseline Methods. In order to explore the advantages of STMGNN-ZINB, we compare the STMGNN-ZINB results against three other models: (1) Historical Value (HA) serves as the statistical baseline. It uses historical input directly to represent prediction results. (2) Spatial-Temporal Graph Convolutional Networks (STGCN) 1 is the state-of-the-art deep learning model for spatial-temporal prediction but it only produces point estimates; (3) Models with probabilistic assumptions to replace ZINB: Negative Binomial (STMGNN-NB), Gaussian (STMGNN-G), and Truncated Normal (STMGNN-TN)."}, {"title": "3.2 Experimental Results and Analysis", "content": "As shown in Table 2, STMGNN-ZINB outperforms all baseline methods across the majority of evaluation metrics. Specifically, in point estimation, STMGNN-ZINB achieves the lowest Mean Absolute Error (MAE), indicating that its mean value accurately reflects the trends in real crime data. For uncertainty metrics, the"}, {"title": "3.3 Interpretation of Parameter \u03c0", "content": "The sparsity parameter \u03c0 in the Zero-Inflated Negative Binomial (ZINB) distribution quantifies the likelihood that a specific zone is devoid of any crime activities. Our STMGNN-ZINB model outputs \u03c0 for various types of crime activities across different spatial regions for successive future time steps.\nFigure 4 displays a heatmap of the overall crime activities (parameter \u03c0) for New York City and Chicago. This visualization offers valuable insights for government agencies and public security entities. Notably, it reveals the presence of spatial locality, where changes in crime activities gradually vary across the spatial dimension. It is uncommon to find isolated small regions with low crime rates surrounded by areas with high crime rates. Furthermore, consistent with mainstream criminological research, our findings indicate a higher concentration of crime activities in the downtown areas of both cities compared to the marginal regions (rural areas) [13, 18]. This pattern underscores the heightened crime rates typically observed in urban centers as opposed to rural settings.\nThus, the sparsity parameter a significantly enhances the interpretability of the STMGNN-ZINB model, offering a more nuanced understanding compared to models based on simpler distribution assumptions."}, {"title": "4 CONCLUSION", "content": "In this work, we introduce a novel combination of Spatial Temporal Multivariate Graph Neural Networks and Zero-Inflated Negative Binomial distribution to quantify uncertainty in the urban crime prediction problem. We validated our model's performance through extensive experiments across five representative scenarios, focusing particularly on point estimation and uncertainty measurement. Given that the parameter \u03c0 has a clear physical interpretation, our model could assist social security decision-makers in efficiently allocating limited social resources to different regions. This approach"}]}