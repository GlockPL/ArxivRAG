{"title": "Zero-Shot Commonsense Validation and Reasoning with Large Language Models: An Evaluation on SemEval-2020 Task 4 Dataset", "authors": ["Rawand Alfugaha", "Mohammad AL-Smadi"], "abstract": "This study evaluates the performance of Large Language Models (LLMs) on SemEval-2020 Task 4 dataset, focusing on commonsense validation and explanation. Our methodology involves evaluating multiple LLMs, including LLaMA3-70B, Gemma2-9B, and Mixtral-8x7B, using zero-shot prompting techniques. The models are tested on two tasks: Task A (Commonsense Validation), where models determine whether a statement aligns with commonsense knowledge, and Task B (Commonsense Explanation), where models identify the reasoning behind implausible statements. Performance is assessed based on accuracy, and results are compared to fine-tuned transformer-based models. The results indicate that larger models outperform previous models and perform closely to human evaluation for Task A, with LLaMA3-70B achieving the highest accuracy of 98.40% in Task A whereas, lagging behind previous models with 93.40% in Task B. However, while models effectively identify implausible statements, they face challenges in selecting the most relevant explanation, highlighting limitations in causal and inferential reasoning.", "sections": [{"title": "Introduction", "content": "Commonsense reasoning is a crucial aspect of Natural Language Processing (NLP) that enables models to understand and validate knowledge beyond explicit textual data. The motivation behind this research comes from the need to develop NLP models that can reason beyond surface-level text representations and apply real-world knowledge to language understanding tasks. Existing benchmarks, such as CommonGen (Lin et al., 2019), SemEval-2020 Task 4: Commonsense Validation and Explanation (Wang et al., 2020), CommonSenseQA 2.0 (Talmor et al., 2022), and COPEN (Peng et al., 2022), have highlighted various aspects of commonsense reasoning, including generative commonsense reasoning, multi-hop reasoning, and physical commonsense knowledge. However, these tasks still pose challenges in handling nuanced reasoning (El-Sayed and Pacholczyk, 2002), causal inference(Yao et al., 2021), and knowledge integration (Chen et al., 2020).\nThe SemEval-2020 Task 4: Commonsense Validation and Explanation (Wang et al., 2020) has served as a benchmark for evaluating various NLP models' capabilities in this domain. The task consistes of three sub-tasks, where in this research we will focus on the first two namely: Task A - Commonsense Validation: Determining whether a given statement aligns with commonsense knowledge, and Task B - Commonsense Explanation: Identifying the reasoning behind why a statement is against common sense. Table 1 provides examples on both tasks as they appear in the dataset.\nThis paper aims to explore how well large language models (LLMs) perform on commonsense reasoning tasks using zero-shot prompting. By evaluating multiple LLMs on SemEval-2020 Task 4, we investigate their ability to reason effectively without explicit fine-tuning. We present an overview of existing research, detail our methodology, and analyze experimental results to assess the strengths and limitations of current approaches."}, {"title": "Related Work", "content": "SemEval-2020 Task 4, which focuses on Commonsense Validation and Explanation, attracted considerable attention, with numerous teams participating in its three subtasks. This literature review highlights the best-performing models in Tasks A and B, showcasing their methodologies and contributions to the field.\nCN-HIT-IT.NLP (Zhang et al., 2020) emerged as the leading model in Subtask A, employing a variant of K-BERT (Liu et al., 2019a) as its encoder. This model stands out for its integration of knowledge graphs, specifically ConceptNet (Speer et al., 2017), which allows it to extract relevant triples that enhance the understanding of language representations. This approach underscores the importance of leveraging structured knowledge to improve commonsense reasoning capabilities.\nIn Subtask B, ECNU-SenseMaker (Zhao et al., 2020) achieved top performance by also utilizing K-BERT (Liu et al., 2019a). This model innovatively combines structured knowledge from ConceptNet (Speer et al., 2017) with unstructured text through a Knowledge-enhanced Graph Attention Network. This integration facilitates a deeper understanding of commonsense knowledge, demonstrating the effectiveness of combining different types of information to enhance model performance.\nAnother notable model, IIE-NLP-NUT (Xing et al., 2020), utilized RoBERTa as its encoder. This model's unique contribution lies in its second pretraining phase, which involved a textual corpus from the Open Mind Common Sense (OMCS) project (Singh et al., 2002). By exploring various prompt templates for input construction, this model illustrates the potential of tailored input strategies in improving commonsense validation tasks\nTeam Solomon (Srivastava et al., 2020) was ranked 5th and 4th in Subtasks A and B, respectively. Their approach, which also relied on ROBERTa, highlighted the capacity of large-scale pretrained language models to encapsulate commonsense knowledge effectively without external resources.\nAcross the two subtasks, the dominant trend was the use of large-scale pretrained language models such as K-BERT (Liu et al., 2019a), ROBERTa (Liu et al., 2019b), BERT (Devlin et al., 2018), and GPT-2 (Radford et al., 2019), often fine-tuned with additional commonsense knowledge sources. Additionally, models incorporating external structured knowledge sources (e.g., ConceptNet) generally outperformed purely language-model-based approaches."}, {"title": "Methodology", "content": "Our study aims at evaluating the performance of multiple Large Language Models (LLMs) for commonsense validation and reasoning using zero-shot prompting. This approach leverages pre-trained LLMs without task-specific fine-tuning, relying solely on their inherent reasoning capabilities. For this purposes, we utilize the SemEval-2020 Task 4 dataset (Wang et al., 2020), which comprises labeled statements designed for commonsense validation and explanation tasks. To ensure a fair comparison between explicitly fine-tuned models and those evaluated solely with zero-shot prompting, we use only the test set for evaluation. The test set contains 1,000 entries for each task (Task A and Task B), providing a standardized benchmark for assessing model performance. The dataset is publicly available and can be accessed at 1.\nAs depicted in Figure 1, the methodology consists of the following key stages:\n\u2022 Pre-processing: preparing the input test data templatic prompt to ensure compatibility with zero-shot prompting.\n\u2022 Model Calling: Applying zero-shot prompting to multiple LLMs, including LLaMA3, Gemma2, and Mixtral to assess their commonsense validation and reasoning abilities.\nLLMs are directly accessible through the Gro-qCloud 2 Models API endpoint using the model IDs\n\u2022 Performance Metrics: Evaluating model outputs based on accuracy to quantify their effectiveness.\n\u2022 Comparative Analysis: Benchmarking zero-shot LLMs performance against fine-tuned transformer models to examine the impact of training on commonsense validation and reasoning tasks."}, {"title": "Results and Discussion", "content": "Table 2 presents the performance of the models on the commonsense validation (Task A) and commonsense explanation (Task B) tasks from SemEval-2020 Task 4. The results for human performance and transformer-based models (CN-HIT-IT.NLP, ECNU-SenseMaker, IIE-NLP-NUT, and Solomon) are as reported in the original SemEval-2020 Task 4 paper (Wang et al., 2020). In contrast, the results for the LLMs (LLaMA3, Gemma2, and Mixtral) are obtained from our experiments with zero-shot prompting. Findings are reported in the following subsections."}, {"title": "Performance Analysis", "content": "Among the models evaluated in this study, L3-70B (LLaMA3-70B) demonstrated the highest performance in Task A, scoring 98.4%, with an evidence that large-scale LLMs can effectively validate commonsense knowledge with zero-shot prompting. However, its performance in Task B (93.4%) lags behind the transformer-based models reported as top 4 performing models in the Task paper. These models were explicitly fine-tuned for the task and some of them used external resources for the models training. This indicates that while LLMs are highly proficient in identifying implausible statements, they still struggle with selecting the most relevant explanation, demonstrating limitations in causal and inferential reasoning.\nSimilarly, the G2-9B (Gemma2-9B) model achieves strong performance in Task A (97.9%) but showing a more significant decline in Task B (91.0%). This further highlights the challenge of explanation selection, as these models may recognize implausibility without fully understanding the underlying causal mechanisms.\nA size-dependent trend is observed in the LLaMA3 models. The smaller L3-8B (LLaMA3-8B) demonstrates significantly weaker performance than its larger version, with 84.4% in Task A and 83.1% in Task B. Finaly, the M8x7B (Mixtral-8x7B) model exhibited the weakest performance, with 66.0% in Task A and 50.9% in Task B. Its near-random performance in explanation selection suggests that it struggles not only with causal inference but also with general commonsense understanding, likely due to limitations in its training data or architecture. It is important to note that this lower accuracy was not due to weak reasoning abilities but rather due to inconsistencies in the output format, where the model provided both classification and explanation instead of following the expected template for the output."}, {"title": "Implications for Zero-Shot Commonsense Reasoning", "content": "The results indicate that while LLMs often recognize implausible statements but fail to select the most relevant explanation, highlighting deficits in causal and inferential reasoning. This suggests that current zero-shot approaches may capture surface-level plausibility but lack deeper reasoning abilities necessary for explanation generation.\nFurthermore, the comparison between pre-trained LLMs and task-specific models from SemEval-2020 Task 4 suggests that explicit fine-tuning on commonsense explanation data remains beneficial. While larger models such as L3-70B outperform fine-tuned models in validation, they do not surpass them in explanation selection, reinforcing the need for additional adaptation to improve causal reasoning."}, {"title": "Common Misclassification Patterns", "content": "An analysis of misclassified instances provides insights into the reasoning patterns of different models. In Task A, some models failed to differentiate between subtle variations in sentence structure. For example, the model incorrectly classified the following pair:\nThe dog pounced on the rabbit. The cat pounced on the rabbit.\nThis type of error suggests that the models may rely on statistical patterns rather than deep semantic understanding.\nIn Task B, errors were primarily related to the selection of the most plausible explanation. A notable example is:\nFalse Statement: \"There are four years each season.\"\nCorrect Explanation: \"A year can be divided into four seasons.\"\nSome models selected incorrect explanations, indicating potential limitations in their ability to link cause-effect relationships effectively. It should be noted that sentence IDs 1388, 1444, and 1172 are not present in the common misclassified instances of Task A.\nDespite the overall strong performance, the results also highlight challenges in certain reasoning aspects. The models demonstrated difficulty in selecting the most appropriate explanation for an implausible statement in Task B, even though they performed well in identifying implausible statements in Task A. This suggests that while the models recognize commonsense inconsistencies, they may struggle to justify their choices accurately. One possible explanation for this challenge is that Task B requires models to establish causal or inferential relationships between a false statement and its explanation. While Task A is a binary classification task requiring identification of implausible statements, Task B introduces additional complexity by demanding a deeper understanding of reasoning patterns and cause-effect relationships. Selecting the correct explanation requires not only recognizing a logical inconsistency but also evaluating multiple plausible justifications and determining which one best aligns with human commonsense knowledge. This suggests that current LLMs, despite their powerful language modeling capabilities, may still struggle with selecting the most contextually relevant explanation among multiple plausible options, as this task requires a nuanced understanding of real-world implications and reasoning structures (Mondorf and Plank, 2024).\nAdditionally, the low measured performance of Mixtral-8x7B can be attributed to output inconsistencies. The model frequently produced both an answer and an explanation, which deviated from the required response format. This indicates that we cannot rely on the achieved results for this model to evaluate its performance on both tasks. More post-processing steps are required to ensure consistent output formatting when evaluating model performance."}, {"title": "Conclusion", "content": "This study demonstrates that large-scale LLMs, particularly LLaMA3-70B and Gemma2-9B, exhibit strong commonsense reasoning capabilities even in a zero-shot setting. These models outperform state-of-the-art fine-tuned transformer-based models, indicating that LLMs can generalize well across commonsense validation tasks. However, challenges remain in explanation selection and maintaining consistent output formats. Future research may include exploring Commonsense knowledge-graph LLMs (Li et al., 2022; Zhao et al., 2024; Toroghi et al., 2024), in addition to fine-tuning strategies, retrieval-augmented approaches, and structured prompting techniques to enhance the inferential reasoning capabilities of LLMs in zero-shot settings."}]}