{"title": "Generalizing Teacher Networks for Effective Knowledge Distillation Across Student Architectures", "authors": ["Kuluhan Binici", "Weiming Wu", "Tulika Mitra"], "abstract": "Knowledge distillation (KD) is a model compression method that entails training a compact student model to emulate the performance of a more complex teacher model. However, the architectural capacity gap between the two models limits the effectiveness of knowledge transfer. Addressing this issue, previous works focused on customizing teacher-student pairs to improve compatibility, a computationally expensive process that needs to be repeated every time either model changes. Hence, these methods are impractical when a teacher model has to be compressed into different student models for deployment on multiple hardware devices with distinct resource constraints. In this work, we propose Generic Teacher Network (GTN), a one-off KD-aware training to create a generic teacher capable of effectively transferring knowledge to any student model sampled from a given finite pool of architectures. To this end, we represent the student pool as a weight-sharing supernet and condition our generic teacher to align with the capacities of various student architectures sampled from this supernet. Experimental evaluation shows that our method both improves overall KD effectiveness and amortizes the minimal additional training cost of the generic teacher across students in the pool.", "sections": [{"title": "Introduction", "content": "In practical scenarios, such as deploying AI solutions in mobile devices, IoT devices, and embedded systems, there is a vast spectrum of computational capabilities and memory limitations. These differences necessitate the use of neural network models of varying sizes and complexities, tailored to the specific resources available on each device. Selecting the most appropriate model is challenging as high-performing models are often resource-demanding, while the resource-efficient ones often cannot achieve high performance. To break this tradeoff, a popular approach is reducing the resource footprint of large pre-trained models, without sacrificing significant performance through model compression. Knowledge distillation (KD) [10] is a prominent model compression technique that involves transferring the predictive behavior acquired by a large pre-trained model, known as the \"teacher\", to a compact \"student\" architecture. However, not every neural network effectively benefits from KD due to the potential capacity mismatch with the teacher model.\nAddressing this issue, some works explored methods for bridging such capacity gap at distillation time [24]. These typically involve controlling the influence of the teacher model throughout the training process of the student to prevent adverse impacts on accuracy. However, despite their user-friendly nature, as they do not require modifications to the teacher model, their effectiveness is constrained, as evidenced by our experiments. An alternative approach is specializing a given teacher architecture to be conducive for effective knowledge transfer to a particular reference student [16]. This is achieved by conditioning the training process of the given teacher, which involves optimizing a snapshot of the reference student model jointly with the teacher on the target dataset. Throughout this process, the teacher is additionally constrained to match the outputs of the student snapshot. This causes the teacher to converge to a function that the student can easily approximate within its capacity later during the KD stage. However, the specialization procedure has to be repeated between every teacher-student pair as teacher models tailored to a particular student might be ineffective or even detrimental for other students, as highlighted in our experiments. This need for iterative repetition upscales the time cost, rendering the method impractical when dealing with scenarios with multiple students requiring KD. This creates a tradeoff between performance and scalability.\nIn this work, we present a one-off KD-aware teacher training method that trains a generic teacher model with consideration to the capacities of all student models contained in a given finite pool of architectures, as illustrated in Figure 1. This eliminates the aforementioned performance vs. scalability tradeoff by limiting the time cost to the one-off training time of the generic teacher. Our method involves partially training reference student models sampled from the pool, to regularize the output predictions of the teacher. To avoid significant time costs, we allow parameter sharing among these reference student models. For this, we configure the pool as an over-parameterized supernet architecture [12], containing a diverse range of neural network blocks that can be connected in various ways to represent different neural network architectures. At each training iteration of the teacher model, the supernet model is reconfigured to represent a different reference student model by selecting a single NN block at each supernet layer. Therefore, parameters are shared across all students containing the same NN blocks within their architectures. By reconfiguring the supernet at each iteration, the teacher model is regularized based on the capacity of a different student model.\nExperimental evaluation of our method reveals consistent improvements in the KD accuracy of students sampled from the pool of architectures, resolving the limited effectiveness of specialized teachers in generalizing to different students. While providing such flexibility, our approach maintains a constant time overhead which is equivalent to training 2-3 teacher models specialized for different students. Furthermore, as an extended evaluation, we explore the use of students selected through Neural Architecture Search (NAS) rather than manual selection from the given pool. The results of these experiments indicate the viability of our approach in NAS scenarios to boost the performance of various student models customized for different deployment platforms."}, {"title": "Related Work", "content": ""}, {"title": "Knowledge Distillation (KD)", "content": "Knowledge Distillation (KD) [10] is a technique that involves training a compact neural network, referred to as the \"student\" model, to approximate the decision-making capabilities of a more complex one known as the \"teacher.\" Typically, the student model is trained to match the logit scores or softmax probabilities of the teacher model [19]. However, alternative approaches, such as employing activation maps or attention scores, have also been explored [22] for this purpose. Recently Zhao et al. [23] identified that the skewness of the predictive distribution of complex teachers towards target classes limits the transfer of useful knowledge from non-target classes. Introducing Decoupled Knowledge Distillation (DKD), they separate knowledge transfer from target and non-target classes. DKD dynamically adjusts the weighting of both learning objectives to ensure that the impact of non-target class-related information is not overlooked."}, {"title": "Teacher-Student Capacity Gap in KD", "content": "Research by Cho et al. [3], Mirzadeh et al. [15], and Menon et al. [14] challenges the notion that more accurate teachers always enhance student learning, highlighting that mismatches in model capacities can hinder KD. Liu et al. [13] further suggest that different student models perform better with different teachers, indicating that compatibility between teacher and student models affects KD effectiveness. Zhu et al. [24] introduced Student-customized Knowledge Distillation (SCKD) to mitigate issues arising from the capacity gap by adapting knowledge transfer based on gradient orientations, although it does not alter the teacher's teaching abilities.\nTo address the capacity gap more effectively, SFTN [16] customizes the teacher model for specific students, enhancing KD outcomes for those students but potentially harming others. This customization is not scalable when multiple students with different resource needs must be accommodated, leading to significant time costs."}, {"title": "Neural Architecture Search & Supernet Architectures", "content": "Neural Architecture Search (NAS) automates the design of neural architectures, often outperforming manual designs for various tasks [5, 8]. Initial approaches utilized genetic algorithms [18] or reinforcement learning [25], which are time-consuming due to the need to train and evaluate many models. To enhance efficiency, DARTS [12] introduced a differentiable search strategy within a weight-sharing \"supernet\", allowing gradient-based optimization to find optimal models. However, this method significantly increases memory consumption. ProxylessNAS [2] addresses this by binarizing paths in the supernet, reducing memory demands by loading only one path at a time during the search. Our method's supernet configuration is inspired by the design used in ProxylessNAS."}, {"title": "Method", "content": "To obtain our generic teacher we condition its training process with consideration to the capacities of various reference students drawn from the given set. For such conditioning, we use the SFTN algorithm. As this would involve training snapshots of each of these reference students from scratch, we allow weight-sharing among them to avoid excessive time costs. This is achieved by using a supernet architecture that contains all possible candidate operations required to build any student model from the pool. We name our method as Generic Teacher Networks (GTN). Technical details are discussed in the following sections."}, {"title": "Conditioning the Teacher Based on the Capacity of a Reference Student Architecture", "content": "This process aims to achieve two goals: (i) training a teacher model until convergence to a function that yields high accuracy, (ii) constraining the set of functions that the teacher can converge to, based on the reference students' capacity. The first target can be simply achieved by minimizing the cross-entropy between the predictions of the teacher and the ground truth labels. As for meeting the second objective, we make use of SFTN approach. First, the reference student model is partitioned into a number of blocks, and various combinations of these blocks are grafted on top of teacher blocks as in Figure 2 (a). Later during training, the outputs of the teacher are forced to match those of each of these grafted student branches. Therefore the final optimization objective is minimizing the loss function in Eq. 1.\n$\\begin{equation} L_{CT} = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_{gt} \\log(\\hat{y}_{s_i}) + \\alpha T^2 D_{KL} \\left( \\frac{z_t}{T} \\middle\\| \\frac{z_{s_i}}{T} \\right) + y_{gt} \\log(\\hat{y}_{t}) \\right] \\tag{1} \\end{equation}$\nIn Eq. 1 n denotes the number of student branches, while \u0177t and \u0177s; represent the predictive outputs of the teacher and student branches indexed with i respectively. a is a coefficient that adjusts the weighting of the LKL with respect to other terms. The loss term is composed of three sub-terms, namely LCES, LKL and LCET. The first and the last terms are the cross-entropy losses from the student and teacher branches calculated using the ground truth labels ygt. Moreover, the second term, LKL, is the KL-divergence between the re-scaled logits of the teacher and student (zt and zs,) by temperature T. Minimizing this term constrains the outputs of the teacher to match those of the student branches."}, {"title": "GTN Training Using a Supernet as Reference Architecture", "content": "The most straightforward way for conditioning the teacher using various reference students, would be extending the amount of grafted student branches with blocks from different architectures. However, this would increase the memory footprint significantly and render the training process infeasible on many hardware systems. Moreover, optimizing such a large number of student branches from scratch along with the teacher would consume a significant amount of time. Instead, we propose configuring a single reference architecture that embodies all student models in the given pool, and allows weight sharing among them. For this purpose we use a supernet architecture consisting of multiple alternate paths at every layer, with each path containing a different candidate operation that maps the inputs to the outputs, as shown in Figure 2 (b). Operations that are commonly shared among different student models at corresponding layers, use the same trainable parameters. This prevents excessive time costs due to re-training the same set of parameters for each student. To condition the teacher with this supernet as the reference architecture, we again partition it into blocks and create the student branches. Later during subsequent training, we employ path binarization to reduce the memory requirement, as done in Cai et al. [2]. This technique trains a supernet stochastically, by sampling and updating a single operation path from every layer at each iteration. Therefore, at any time, the memory occupation is limited to the size of a single sub-network contained within the supernet. In the context of our GTN framework, these sampled sub-networks correspond to individual student models from the given pool. As sampling operations randomly can cause the teacher model to be regularized by aribtrarily bad student models and damage the conditioning outcome, we instead sample them from a trainable probability distribution. For this, we use a multinomial probability distribution \u03a1\u03a6 = softmax(\u03a6) parameterized by a single trainable random variable \u03a6. To activate the sampled operation paths and deactivate the rest, we use binary gates. For k-many candidate operations, we use the same amount of binary gates, with each gate gi controlling whether the jth operation will be activated or not. The index of the active operation is denoted as jact.\n$\\begin{equation}  g^{j}= \\begin{cases}  1, & \\text{if } j = j^{act} \\sim P_{\\Phi} \\\\  0, & \\text{otherwise}  \\end{cases} \\tag{2} \\end{equation}$\nThe random variable \u03a6 has k-many possible outcomes (one for each operation) and is learned during the GTN training process by minimizing L\u00f8 loss given in Eq. 3.\n$\\begin{equation} L_{\\Phi} = L_{CES} - \\alpha L_{KL} \\tag{3} \\end{equation}$\nDifferent from LCT, LKL appears with a negative a coefficient in L\u00f8 This is because, ideally, the gate parameters should be updated in a way that would motivate an extensive exploration of candidate operations at each training step. While sampled student parameters 0 are updated to match the predictions of the teacher, those that are not sampled remain divergent from the teacher. This means that the unexplored operations would yield high KL divergence from the outputs of the teacher. Therefore, by minimizing the \u2212\u03b1LKL term, the \u03c6 variable is updated to encourage the sampling of operations that yield high KL-divergence loss typically, those that are yet to be explored.\nOnce the gate values are set for a certain iteration, the function fjact that maps the inputs of the Ith supernet layer to the outputs is computed based on Eq. 4. The denotes the trainable parameters of the candidate operations.\n$\\begin{equation} f_{\\theta}^{j^{act}}(x) = \\sum_{j} g_{i}^{j}f_{\\theta_{j}}^{j}(x) \\text{ where } g^{g^{j}}_{hi} \\in \\{0, 1\\} \\tag{4} \\end{equation}$\nUsing this definition, the forward process of each student branch si, containing Ls; number of layers, can be represented by the composite function given in Eq. 5.\n$\\begin{equation} \\hat{y}_{s_i} = f_{\\theta_{O_{s_i}}}^{O_{s_i}^{L_{s_i}^{act}}} \\circ ... \\circ f_{\\theta_{O_{s_i}}}^{O_{s_i}^{2^{act}}} \\circ f_{\\theta_{O_{s_i}}}^{O_{s_i}^{1^{act}}}(x) \\tag{5} \\end{equation}$\nIn training our GTN with the supernet architecture, we follow Algorithm 1. The parameters of the sampled operations in the supernet (0s) and the teacher (07) are updated together (line 12). The \u03c6 values, controlling the sampling probability, are updated alternately with \u03b8s and Or (lines 11-15). Initially, \u03c6 is fixed, and operations are sampled for each layer (lines 4-8), which helps regularize the teacher's optimization by minimizing Lcr. In the following iteration, while Os and Or are fixed, \u03c6 is updated to minimize L\u00f8 (line 14). This alternating process is repeated until the training concludes after the designated number of epochs."}, {"title": "Knowledge Distillation", "content": "After training the GTN, the auxiliary branches containing supernet blocks are discarded and the remaining teacher model can be used to enhance the accuracy of any student model from"}, {"title": "Experimental Evaluation", "content": "To assess the effectiveness of our method, we evaluate the accuracies of student models randomly selected from a pool of architectures. These students are trained using our method and compared against five different approaches: SFTN, SCKD, DKD, Vanilla KD [10], and supervised training (denoted as no-KD). Vanilla KD and DKD, which do not address the capacity gap, establish the lower performance bounds for our comparison. In contrast, SFTN and SCKD, which consider this gap, serve as the main baselines to assess our method's effectiveness in mitigating it. We also include students derived from NAS using Proxyless-NAS [2], comparing their performance on CIFAR-100 [11] and ImageNet-200 [4], to further validate our approach. Lastly, we display the memory sizes of these student models in conjunction with the on-chip memory availability of three edge devices to showcase a real-world scenario of tailoring student models for specific hardware platforms.\nImplementation Details: In our KD experiments, we employ three teacher architectures: ResNet-32, WRN40-2, and EfficientNet-b0 [9, 20, 21]. During teacher training, our GTN framework modifies the teacher architecture by grafting blocks of the reference supernet architecture, onto the teacher to form different student branches. Once the teacher is trained, these branches are discarded and the teacher model is used to train student models via KD. The supernet is constructed with ResNet layers varying in depth and filter size, equipped with identity and zero operations to allow flexible architecture sampling. For knowledge transfer, we use Vanilla KD and DKD methods, training student models over 240 epochs with learning rates of 0.05 and 0.1 for CIFAR-100 and ImageNet-200 datasets respectively. Cosine annealing is used for adjusting the learning rates. Further implementation details and training configurations are provided in the Appendix.\nKD with random students: We first compare our KD approach with baseline methods based on the performance of student models randomly drawn from the pool of architectures represented by the supernet. For each dataset, we randomly select seven student architectures. Each method except SFTN uses a single teacher model leading to seven teacher-student pairings for each dataset and teacher architecture combination (e.g. ResNet-32 & CIFAR-100). As for SFTN, we train four teachers, each specialized for a different student, leading to twenty-eight teacher-student pairings. The results are summarized in Table 1, with A indicating the relative improvements compared to the vanilla KD method. \u00b5\u2081 and\n$\\begin{equation} L_{KD} = L_{CEs} + \\alpha L_{KL} \\tag{6} \\end{equation}$"}, {"title": "KD with students obtained by NAS", "content": "Besides selecting the student models for different deployment scenarios manually, NAS can also be employed to sample students that exhibit close to optimal performance within the given resource budgets. To test the effectiveness of our method in improving KD accuracy for NAS-discovered student architectures, we search our pool of student architectures to obtain three distinct student architectures of varying sizes. Since we represent the architecture pool using a supernet, we employ a differentiable search strategy, that is ProxylessNAS [2]. To obtain each student, we limit the searched model size by a different number of layers. The largest architecture, denoted as s snas, is derived from the full-scale supernet consisting of 6 searchable layers without any constraints. For the medium (smas) and small (snas) architectures, we reduce the number of searchable layers to 5 and 3, respectively. Subsequently, to prepare the SFTN baseline for comparison, we train specialized teachers using these three architectures as reference students. We then evaluate these teachers based on their KD performance for all three NAS-discovered students. We again use DKD as the base distillation method in this evaluation. From Table 3 we can observe that GTN method again caused the largest improvement in student accuracies in almost all experiments. This means that when a new student is configured via NAS due to changes in the deployment platform, our GTN method is more likely to provide higher accuracy benefits through KD.\nIn Table 4, we display the memory sizes of seven student models we considered in CIFAR-100 experiments, detailing both the full-scale (32-bit) and the 8-bit weight quantized versions. The varied memory sizes of these student models, demonstrate the diversity of our student model pool, which contains a range of architectures with different capacities. Moreover, we provide the on-chip memory capacities of three edge devices\u2014Arm Ethos N77 [1], Edge TPU [7], and Raspberry Pi [17]\u2014, focusing on the feasibility of deploying each student model on these platforms. The green checkmarks and red Xs, show whether each 8-bit quantized model fits within the memory constraints of these devices."}, {"title": "Conclusion", "content": "In conclusion, this study tackles the teacher-student capacity gap problem, which undermines the effectiveness of Knowledge Distillation (KD) in optimizing neural networks. Previous methods have concentrated on customizing teacher-student pairs, a process that needs to be repeated for each student architecture requiring KD. This characteristic renders these approaches impractical, particularly in scenarios involving multiple deployment platforms, each demanding KD for distinct deployment-ready students. Our novel approach introduces a single generic teacher model capable of transferring knowledge effectively across a variety of student architectures. The proposed technique's advantage is that it both improves KD performance and maintains constant time cost, which is equivalent to that of a few specialized teachers. Moreover, the adaptability of our approach to NAS scenarios further highlights its practicality."}]}