{"title": "Generalizing Teacher Networks for Effective Knowledge Distillation Across Student Architectures", "authors": ["Kuluhan Binici", "Weiming Wu", "Tulika Mitra"], "abstract": "Knowledge distillation (KD) is a model compression method that entails training a compact student model to emulate the performance of a more complex teacher model. However, the architectural capacity gap between the two models limits the effectiveness of knowledge transfer. Addressing this issue, previous works focused on customizing teacher-student pairs to improve compatibility, a computationally expensive process that needs to be repeated every time either model changes. Hence, these methods are impractical when a teacher model has to be compressed into different student models for deployment on multiple hardware devices with distinct resource constraints. In this work, we propose Generic Teacher Network (GTN), a one-off KD-aware training to create a generic teacher capable of effectively transferring knowledge to any student model sampled from a given finite pool of architectures. To this end, we represent the student pool as a weight-sharing supernet and condition our generic teacher to align with the capacities of various student architectures sampled from this supernet. Experimental evaluation shows that our method both improves overall KD effectiveness and amortizes the minimal additional training cost of the generic teacher across students in the pool.", "sections": [{"title": "Introduction", "content": "In practical scenarios, such as deploying AI solutions in mobile devices, IoT devices, and embedded systems, there is a vast spectrum of computational capabilities and memory limitations. These differences necessitate the use of neural network models of varying sizes and complexities, tailored to the specific resources available on each device. Selecting the most appropriate model is challenging as high-performing models are often resource-demanding, while the resource-efficient ones often cannot achieve high performance. To break this trade-off, a popular approach is reducing the resource footprint of large pre-trained models, without sacrificing significant performance through model compression. Knowledge distillation (KD) [10] is a prominent model compression technique that involves transferring the predictive behavior acquired by a large pre-trained model, known as the \"teacher\", to a compact \"student\" architecture. However, not every neural network effectively benefits from KD due to the potential capacity mismatch with the teacher model.\nAddressing this issue, some works explored methods for bridging such capacity gap at distillation time [24]. These typically involve controlling the influence of the teacher model throughout the training process of the student to prevent adverse impacts on accuracy. However, despite their user-friendly nature, as they do not require modifications to the teacher model, their effectiveness is constrained, as evidenced by our experiments. An alternative approach is specializing a given teacher architecture to be conducive for effective knowledge transfer to a particular reference student [16]. This is achieved by conditioning the training process of the given teacher, which involves optimizing a snapshot of the reference student model jointly with the teacher on the target dataset. Throughout this process, the teacher is additionally constrained to match the outputs of the student snapshot. This causes the teacher to converge to a function that the student can easily approximate within its capacity later during the KD stage. However, the specialization procedure has to be repeated between every teacher-student pair as teacher models tailored to a particular student might be ineffective or even detrimental for other students, as highlighted in our experiments. This need for iterative repetition upscales the time cost, rendering the method impractical when dealing with scenarios with multiple students requiring KD. This creates a tradeoff between performance and scalability.\nIn this work, we present a one-off KD-aware teacher training method that trains a generic teacher model with consideration to the capacities of all student models contained in a given finite pool of architectures, as illustrated in Figure 1. This eliminates the aforementioned performance vs. scalability tradeoff by limiting the time cost to the one-off training time of the generic teacher. Our method involves partially training reference student models sampled from the pool, to regularize the output predictions of the teacher. To avoid significant time costs, we allow parameter sharing among these reference student models. For this, we configure the pool as an over-parameterized supernet architecture [12], containing a diverse range of neural network blocks that can be connected in various ways to represent different neural network architectures. At each training iteration of the teacher model, the supernet model is reconfigured to represent a different reference student model by selecting a single NN block at each supernet layer. Therefore, parameters are shared across all students containing the same NN blocks within their architectures. By reconfiguring the supernet at each"}, {"title": "Related Work", "content": "Knowledge Distillation (KD) [10] is a technique that involves training a compact neural network, referred to as the \"student\" model, to approximate the decision-making capabilities of a more complex one known as the \"teacher.\" Typically, the student model is trained to match the logit scores or softmax probabilities of the teacher model [19]. However, alternative approaches, such as employing activation maps or attention scores, have also been explored [22] for this purpose. Recently Zhao et al. [23] identified that the skewness of the predictive distribution of complex teachers towards target classes limits the transfer of useful knowledge from non-target classes. Introducing Decoupled Knowledge Distillation (DKD), they separate knowledge transfer from target and non-target classes. DKD dynamically adjusts the weighting of both learning objectives to ensure that the impact of non-target class-related information is not overlooked."}, {"title": "Knowledge Distillation (KD)", "content": "Knowledge Distillation (KD) [10] is a technique that involves training a compact neural network, referred to as the \"student\" model, to approximate the decision-making capabilities of a more complex one known as the \"teacher.\" Typically, the student model is trained to match the logit scores or softmax probabilities of the teacher model [19]. However, alternative approaches, such as employing activation maps or attention scores, have also been explored [22] for this purpose. Recently Zhao et al. [23] identified that the skewness of the predictive distribution of complex teachers towards target classes limits the transfer of useful knowledge from non-target classes. Introducing Decoupled Knowledge Distillation (DKD), they separate knowledge transfer from target and non-target classes. DKD dynamically adjusts the weighting of both learning objectives to ensure that the impact of non-target class-related information is not overlooked."}, {"title": "Teacher-Student Capacity Gap in KD", "content": "Research by Cho et al. [3], Mirzadeh et al. [15], and Menon et al. [14] challenges the notion that more accurate teachers always enhance student learning, highlighting that mismatches in model capacities can hinder KD. Liu et al. [13] further suggest that different student models perform better with different teachers, indicating that compatibility between teacher and student models affects KD effectiveness. Zhu et al. [24] introduced Student-customized Knowledge Distillation (SCKD) to mitigate issues arising from the capacity gap by adapting knowledge transfer based on gradient orientations, although it does not alter the teacher's teaching abilities.\nTo address the capacity gap more effectively, SFTN [16] customizes the teacher model for specific students, enhancing KD outcomes for those students but potentially harming others. This customization is not scalable when multiple students with different resource needs must be accommodated, leading to significant time costs."}, {"title": "Neural Architecture Search & Supernet Architectures", "content": "Neural Architecture Search (NAS) automates the design of neural architectures, often out-performing manual designs for various tasks [5, 8]. Initial approaches utilized genetic algorithms [18] or reinforcement learning [25], which are time-consuming due to the need to train and evaluate many models. To enhance efficiency, DARTS [12] introduced a differentiable search strategy within a weight-sharing \"supernet\", allowing gradient-based optimization to find optimal models. However, this method significantly increases memory"}, {"title": "Method", "content": "To obtain our generic teacher we condition its training process with consideration to the capacities of various reference students drawn from the given set. For such conditioning, we use the SFTN algorithm. As this would involve training snapshots of each of these reference students from scratch, we allow weight-sharing among them to avoid excessive time costs. This is achieved by using a supernet architecture that contains all possible candidate operations required to build any student model from the pool. We name our method as Generic Teacher Networks (GTN). Technical details are discussed in the following sections."}, {"title": "Conditioning the Teacher Based on the Capacity of a Reference Student Architecture", "content": "This process aims to achieve two goals: (i) training a teacher model until convergence to a function that yields high accuracy, (ii) constraining the set of functions that the teacher can converge to, based on the reference students' capacity. The first target can be simply achieved by minimizing the cross-entropy between the predictions of the teacher and the ground truth labels. As for meeting the second objective, we make use of SFTN approach. First, the reference student model is partitioned into a number of blocks, and various combinations of these blocks are grafted on top of teacher blocks as in Figure 2 (a). Later during training, the outputs of the teacher are forced to match those of each of these grafted student branches. Therefore the final optimization objective is minimizing the loss function in Eq. 1.\n$L_{CT} = \\frac{1}{n} \\sum_{i=1}^{n} \\bigg( y_{gt} \\log(\\hat{y}_{s_i}) + \\alpha T^2 D_{KL} \\Big( \\frac{z_t}{T} || \\frac{z_{s_i}}{T} \\Big) + y_{gt} \\log(\\hat{y}_t) \\bigg)$\nIn Eq. 1 n denotes the number of student branches, while $\\hat{y}_t$ and $\\hat{y}_{s_i}$ represent the predictive outputs of the teacher and student branches indexed with i respectively. $\\alpha$ is a coefficient that adjusts the weighting of the $L_{KL}$ with respect to other terms. The loss term is composed of three sub-terms, namely $L_{CES}$, $L_{KL}$ and $L_{CET}$. The first and the last terms are the cross-entropy losses from the student and teacher branches calculated using the ground truth labels $y_{gt}$. Moreover, the second term, $L_{KL}$, is the KL-divergence between the re-scaled logits of the teacher and student ($z_t$ and $z_{s_i}$) by temperature T. Minimizing this term constrains the outputs of the teacher to match those of the student branches."}, {"title": "GTN Training Using a Supernet as Reference Architecture", "content": "The most straightforward way for conditioning the teacher using various reference students, would be extending the amount of grafted student branches with blocks from different architectures. However, this would increase the memory footprint significantly and render the training process infeasible on many hardware systems. Moreover, optimizing such a large"}, {"title": "Knowledge Distillation", "content": "After training the GTN, the auxiliary branches containing supernet blocks are discarded and the remaining teacher model can be used to enhance the accuracy of any student model from"}, {"title": "Experimental Evaluation", "content": "To assess the effectiveness of our method, we evaluate the accuracies of student models randomly selected from a pool of architectures. These students are trained using our method and compared against five different approaches: SFTN, SCKD, DKD, Vanilla KD [10], and supervised training (denoted as no-KD). Vanilla KD and DKD, which do not address the capacity gap, establish the lower performance bounds for our comparison. In contrast, SFTN and SCKD, which consider this gap, serve as the main baselines to assess our method's effectiveness in mitigating it. We also include students derived from NAS using Proxyless-NAS [2], comparing their performance on CIFAR-100 [11] and ImageNet-200 [4], to further validate our approach. Lastly, we display the memory sizes of these student models in conjunction with the on-chip memory availability of three edge devices to showcase a real-world scenario of tailoring student models for specific hardware platforms.\nImplementation Details: In our KD experiments, we employ three teacher architectures: ResNet-32, WRN40-2, and EfficientNet-b0 [9, 20, 21]. During teacher training, our GTN framework modifies the teacher architecture by grafting blocks of the reference supernet architecture, onto the teacher to form different student branches. Once the teacher is trained, these branches are discarded and the teacher model is used to train student models via KD. The supernet is constructed with ResNet layers varying in depth and filter size, equipped with identity and zero operations to allow flexible architecture sampling. For knowledge transfer, we use Vanilla KD and DKD methods, training student models over 240 epochs with learning rates of 0.05 and 0.1 for CIFAR-100 and ImageNet-200 datasets respectively. Cosine annealing is used for adjusting the learning rates. Further implementation details and training configurations are provided in the Appendix.\nKD with random students: We first compare our KD approach with baseline methods based on the performance of student models randomly drawn from the pool of architectures represented by the supernet. For each dataset, we randomly select seven student architectures. Each method except SFTN uses a single teacher model leading to seven teacher-student pairings for each dataset and teacher architecture combination (e.g. ResNet-32 & CIFAR-100). As for SFTN, we train four teachers, each specialized for a different student, leading to twenty-eight teacher-student pairings. The results are summarized in Table 1, with $\\Delta$ indicating the relative improvements compared to the vanilla KD method. $\\mu_{\\Delta}$ and"}, {"title": "Conclusion", "content": "In conclusion, this study tackles the teacher-student capacity gap problem, which undermines the effectiveness of Knowledge Distillation (KD) in optimizing neural networks. Previous methods have concentrated on customizing teacher-student pairs, a process that needs to be repeated for each student architecture requiring KD. This characteristic renders these approaches impractical, particularly in scenarios involving multiple deployment platforms, each demanding KD for distinct deployment-ready students. Our novel approach introduces a single generic teacher model capable of transferring knowledge effectively across a variety of student architectures. The proposed technique's advantage is that it both improves KD performance and maintains constant time cost, which is equivalent to that of a few specialized teachers. Moreover, the adaptability of our approach to NAS scenarios further highlights its practicality."}]}