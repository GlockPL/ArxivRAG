{"title": "Less is more: Embracing sparsity and interpolation with Esiformer for time series forecasting", "authors": ["Yangyang Guo", "Yanjun Zhao", "Sizhe Dang", "Tian Zhou", "Liang Sun", "Yi Qian"], "abstract": "Time series forecasting has played a significant role in many practical fields. But time series data generated from real-world applications always exhibits high variance and lots of noise, which makes it difficult to capture the inherent periodic patterns of the data, hurting the prediction accuracy significantly. To address this issue, we propose the Esiformer, which apply interpolation on the original data, decreasing the overall variance of the data and alleviating the influence of noise. What's more, we enhanced the vanilla transformer with a robust Sparse FFN. It can enhance the representation ability of the model effectively, and maintain the excellent robustness, avoiding the risk of overfitting compared with the vanilla implementation. Through evaluations on challenging real-world datasets, our method outperforms leading model PatchTST, reducing MSE by 6.5% and MAE by 5.8% in multivariate time series forecasting.", "sections": [{"title": "I. INTRODUCTION", "content": "Lots of real-world scenarios require forecasting long sequence time-series, like weather forecasting, transportation planning, economics [1]\u2013[11]. Accurate long sequence time-series forecasting (LSTF) relies on a model's robust predictive ability, which includes accurately capturing the intricate periodic and trend patterns.\nHowever, time series data collected from real-world applications are often replete with lots of noise [12]\u2013[15], causing the high variance of the data, which severely impairs predictive performance. This is mainly because the data with high variance contains greater volatility and noise, which increases the degree of dispersion between data points, thereby masking the underlying trends and cyclical patterns. For forecasting models, this high volatility increases the difficulty of capturing the inherent patterns of the data, resulting in greater uncertainty in the forecast results.\nMany studies [16]\u2013[18] have suggested that interpolating data can reduce the data variance and thus enhance model performance. Similarly, we apply the interpolation technique to time series. Since all of the new interpolated data points are generated from the original data points, the content of information in data is preserved but the overall variance is reduced. Consequently, the model can capture temporal patterns more easily and obtain more accurate predictions."}, {"title": "II. METHOD", "content": "We consider the following problem: given a collection of multivariate time series samples with lookback window $X$ : $(X_1,...,X_L) \\in \\mathbb{R}^{L\\times M}$, we aim to forecast future values $Y$ : $(X_{L+1}, ..., X_{L+T}) \\in \\mathbb{R}^{T\\times M}$. For convenience, we denote $X_{t,:}$ as the simultaneously recorded time points in step $t$, and $X_{:,n}$ as the entire time series of each variate indexed by $n$.\nMotivated by the aforementioned considerations, we introduce Esiformer, depicted in Figure 2. We interpolate the data for each channel independently."}, {"title": "A. Model Structure", "content": "$X_{2L-1} = Interpolation(X_L)$,\n$H = Embedding(X_{2L-1})$,\n$H' = TransformerBlock(H)$,\n$\\hat{Y}_T = Projection(H')$."}, {"title": "B. Interpolation", "content": "In time series forecasting, time series with high variance tends to mask inherent patterns or trends in the data, making prediction more difficult. Previous work [16], [24], [25] in computer vision has shown that interpolation on images causes a decrease in variance. For example, through the bilinear interpolation, the variance of interpolated pixels is reduced to 1/4 of that of the original pixels.\nThus, we propose to interpolate the time series data to reduce the variance of data, as shown in Figure 2, and thereby facilitate prediction [26]. Here we show four interpolation methods."}, {"title": "1) TwoAver:", "content": "$X'_i = \\begin{cases}X\\_{i/2} & \\text{if i is odd}\\\\\\frac{X\\_{i/2}+X\\_{i/2+1}}{2} & \\text{otherwise}\\end{cases}$"}, {"title": "2) FourAver:", "content": "$X\\_{3i} = \\frac{X\\_{2i-1}+X\\_{2i}+X\\_{2i+1}+X\\_{2i+2}}{4}$\n$X\\_{3i-1} = X\\_{2i}$\n$X\\_{3i-2} = X\\_{2i-1}$"}, {"title": "3) Spline:", "content": "Spline aims to find a function $s(x)$ that is a cubic polynomial on each interval $[x_i, x_{i+1}]$ and satisfies the following conditions:\nInterpolation condition:\n$S(x_i) = y_i$ for all $i = 1, 2, ..., n$\nContinuity condition:\n$s_{i-1}(x_i) = s_i(x_i)$ for all $i = 2, 3, . . ., n$\nwhere $s_{i-1}(x)$ and $s_i(x)$ are cubic polynomials on the intervals $[x_{i-1}, x_i]$ and $[x_i, x_{i+1}]$, respectively.\nSmoothness condition:\n$s'_{i-1}(x_i) = s'_i(x_i)$ and $s''_{i-1}(x_i) = s''_i(x_i)$ for all $i$\nThe interpolated \"sequence\" is not actually a set of discrete points, but rather a smooth curve $s(x)$. So we calculate the value of $s(x)$ at a fixed interval between the original data points to generate a new, denser sequence."}, {"title": "4) Rbf:", "content": "RBF tries to find a function $s(x)$ that satisfies $s(x_i) = y_i$ for all $i = 1,2,...,n$, which can usually be expressed as:\n$s(x) = \\sum_{i=1}^n \\lambda_i \\varphi(||x - x_i||)$\nWherein, $\\varphi(||x-x_i||)$ is the radial basis function, which depends only on the Euclidean distance $||x - x_i||$ between $x$ and $x_i$, and $\\lambda_i$ is the coefficient to be determined. For the radial basis function $\\varphi$, we use Gaussian function, defined as:\n$\\varphi(r) = exp(-\\frac{r^2}{2\\sigma^2})$\nwhere $r = ||x - x_i||$ is the distance, $\\sigma$ is the parameter that controls the width of the function."}, {"title": "C. Sparse FFN", "content": "The standard linear regression model postulates a linear relationship between the dependent variable $y$ and the independent variables $X$, formulated as $y = X\\beta + \\epsilon$. Least squares optimization is used to find $\\beta$ to best fit observed $y$ values.\n$\\min_\\beta g(y - X\\beta),$\nwhere $y \\in \\mathbb{R}^m$ and $X \\in \\mathbb{R}^{m \\times n}$ are data and $g$ is some convex function, typically a norm.\nTo prevent overfitting, regularization(sparsity) is employed in model parameter estimation:\n$\\min_\\beta g(y - X\\beta) + h(\\beta),$\nRegularization via a convex penalty $h$ simplifies models, focuses on key features, and boosts generalization by reducing overfitting to noise.\nTherefore, after self-attention mechanism in EncoderBlock, we choose sparse linear network for temporal modeling to improve the generalization ability of the model.\nIn regression analysis, data is ideally noise-free. However, practical data often contains errors and adversarial noises, impacting the accuracy and reliability of models [27]\u2013[30]. Neglecting these factors may lead models to learn incorrect patterns. Compared to traditional regression, robust regression explicitly considers the noise in the data when modeling. This approach aims to improve the stability and robustness of the model in a noisy environment. Specifically, we define an uncertainty set to include possible noise or perturbations and minimize the worst-case loss on this uncertainty set. As in [31]\u2013[34], this approach may take the form:\n$\\min_\\beta \\max_{\\Delta \\in \\Psi} g(y - (X + \\Delta)\\beta),$\nwhere the set $\\Psi \\subseteq \\mathbb{R}^{m \\times n}$ characterizes the user's belief about uncertainty on the data matrix $X$.\nSparse regression focuses on the sparsity of model parameters, that is, most parameters are zero or close to zero, which helps the model's interpretability and generalization ability. Robust regression focuses on outliers or noise in the data, aiming to find a regression model that is insensitive to outliers. Although they deal with different problems in essence, sparse regression (2) and robust regression (3) are equivalent if $g$ is a semi-norm and $h$ is a norm [35]. Furthermore, [36] prove that they are equivalent when $g$ and $h$ are functions of semi-norms and norms. They also show the equivalence between robust estimation in linear mixed models and comprehensively penalized regularized parameter estimation. So sparse FFN can positively impact robustness of the model while simultaneously preventing overfitting. With interpolation, the model can better adapt to noisy environments."}, {"title": "III. EXPERIMENTS", "content": "We conduct comprehensive experiments on widely used datasets and choose the renowned PatchTST as our baseline and follow the same experimental setup. All forecasting results are run on GeForce GTX 1080 Ti GPU."}, {"title": "B. Long-term Forecasting", "content": "The multivariate time series forecasting results are listed in Table II, where the best methods are highlighted in bold. Our Esiformer shows superior forecasting performance compared to the recent SOTA methods. Remarkably, this model demonstrates notable performance advantages over PatchTST when handling the high-dimensional time series datasets. Quantitatively, Esiformer achieves an overall 10.0%"}, {"title": "C. Ablations", "content": "Interpolation We study the effects of interpolation in Table III. To verify the effectiveness of interpolation, we use different interpolation methods to perform univariate forecasting on the traffic dataset, and the input time series length is fixed to 96. And in order to eliminate the impact of the sparse mechanism, this part of the ablation experiment excludes it. The results show that interpolating the original time series can effectively reduce the variance and thus improve the forecasting accuracy. Compared with no interpolation, interpolation reduces MSE by 10.3% on average and 7.8% for MAE.\nIn practice, cubic spline interpolation and RBF interpolation tend to generate smoother curves, which is conducive to capturing long-term trends in the data and thus performing better in long-term predictions. However, in medium- and short-term predictions, they distort important local features and are therefore inferior to the other two simple interpolation methods. In addition, the effect of RBF interpolation depends on parameter selection (such as basis function shape and width). If the parameters are not selected properly, the interpolation effect may not be ideal. In contrast, simple interpolation methods are more reliable and easy to optimize in short-term predictions due to their directness and controllability.\nSparse FFN As mentioned above, when hidden_dimension increases, although the representation power of model improves, the risk of overfitting also rises, with these two effects balancing each other to some extent. So we want to introduce sparse FFN to alleviate overfitting. To validate the effectiveness of the sparse mechanism, we present detailed ablation study results, summarized in Table IV. Similarly, this part of the ablation experiment excludes the interpolation mechanism. The results show that incorporating the sparse mechanism led to significant improvements in MSE and MAE for both electricity and traffic datasets. The introduction of the sparse mechanism improves the MSE by 8.97% and the MAE by 5.49% in the electricity dataset prediction task. And for the traffic dataset, the MSE is improved by 6.65% and the MAE is improved by 6.60%.\nWe also combine our method with FEDformer [2] and Informer [37] as a plug-in to evaluate the generality of our Esiformer. As shown in Table V, our method can yield a boosting result and enhance the prediction accuracy."}, {"title": "IV. CONCLUSION", "content": "This paper introduces a Transformer-based time series forecasting model Esiformer. It interpolates the original data, reduces the overall variance of the data and mitigates the impact of noise. In addition, we use robust sparse FFN to enhance the vanilla Transformer. The sparsification strategy not only reduces the number of parameters and model complexity, but also enhances the locality of features through sparse connection patterns, which helps the model to better learn important features while reducing the interference of noise information. It can effectively enhance the representation ability of the model and avoid the risk of overfitting. Experiments show that our proposed model outperforms PatchTST on five mainstream time series forecasting datasets."}]}