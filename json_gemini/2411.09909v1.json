{"title": "AMXFP4: TAMING ACTIVATION OUTLIERS WITH ASYMMETRIC MICROSCALING FLOATING-POINT FOR 4-BIT LLM INFERENCE", "authors": ["Janghwan Lee", "Jiwoong Park", "Jinseok Kim", "Yongjik Kim", "Jungju Oh", "Jinwook Oh", "Jungwook Choi"], "abstract": "Scaling Large Language Models (LLMs) with extended context lengths has increased the need for efficient\nlow-bit quantization to manage their substantial computational demands. However, reducing precision to 4 bits\nfrequently degrades performance due to activation outliers. To address this, we propose Asymmetric Microscaling\n4-bit Floating-Point (AMXFP4) for efficient LLM inference. This novel data format leverages asymmetric\nshared scales to mitigate outliers while naturally capturing the asymmetry introduced by group-wise quantization.\nUnlike conventional 4-bit quantization methods that rely on data rotation and costly calibration, AMXFP4 uses\nasymmetric shared scales for direct 4-bit casting, achieving near-ideal quantization accuracy across various\nLLM tasks, including multi-turn conversations, long-context reasoning, and visual question answering. Our\nAMXFP4 format significantly outperforms MXFP4 and other leading quantization techniques, enabling robust,\ncalibration-free 4-bit inference. Our code is available at aiha-lab/MX-QLLM.git.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-modal Large Language Models (LLMs) are increas-\ningly utilized for advanced natural language processing\ntasks, supporting applications such as chatbots, long-\ndocument question-answering, and visual graph interpreta-\ntion (Touvron et al., 2023; Bai et al., 2023; Liu et al., 2023a).\nLLMs have been scaled significantly in parameter size to\nenhance their capabilities and trained to handle extended\ncontext lengths (Chung et al., 2022; Chowdhery et al., 2022;\nAI@Meta, 2024). For instance, LLaMA3 now encompasses\n405 billion parameters and supports context lengths of up\nto 128K tokens. As shown in Fig. 1(a), this scaling leads\nto computational demands in the peta-FLOP (floating-point\noperation) range just for the prefill phase, where the model\nprocesses the user's context in preparation for inference.\nThis surge in computational demand underscores the need\nfor innovation in computational platforms.\nLeading computing platforms have prioritized bit-precision\nscaling to meet LLM's high computational demands (An-\ndersch et al., 2022; Nvidia, 2024; AzureAI, 2024). As\ndemonstrated in (Horowitz, 2014), reducing operand bit-\nwidths enhances area and energy efficiency in arithmetic\nlogic operations, enabling higher computation density in\naccelerators. For instance, NVIDIA's Tensor Cores dou-\nbled its computation speed (FLOPS) by reducing multiply-"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 Bit-Precision Scaling for Accelerators\nReduced-precision data formats are vital for enhancing scal-\nability and computational efficiency in deep learning accel-\nerators, conserving area and energy in direct proportion to\nbit-width reduction (Horowitz, 2014). This scaling enables\nhigher floating-point operations per second (FLOPS) with\nlower power usage, thereby increasing accelerator through-\nput. For instance, NVIDIA's Tensor Cores have progressed\nfrom FP16 in Volta (Nvidia, 2017) to FP8 in Hopper (An-\ndersch et al., 2022) and FP4 in Blackwell (Nvidia, 2024),\nboosting computational speeds from 112 tera to 20 peta\nFLOPS. Similar advancements by other computing platform\ncompanies in scaling precision from 16-bit to 4-bit are cru-\ncial for managing the growing complexity of LLMs (AMD,\n2024; AzureAI, 2024).\nReduced-precision computation takes advantage of neural\nnetworks' resilience to small numerical errors, though ag-\ngressive scaling often results in accuracy loss. To address\nthis, various algorithmic enhancements have been developed.\nEarly methods introduced formats like BFloat16 (Burgess\net al., 2019) and DLFloat (Agrawal et al., 2019) to opti-\nmize the exponent and mantissa trade-off. More recent\nresearch has advanced 8-bit (Wang et al., 2018; Sun et al.,\n2019) and 4-bit (Sun et al., 2020) formats, although FP4\nhas shown noticeable accuracy degradation. To enhance\n2.2 Quantizing LLM's Activation and Weight\nRecent research highlights the difficulty quantifying LLM\nactivations due to outliers extending the activation dynamic\nrange, leading to increased quantization error (Xiao et al.,\n2022; Ashkboos et al., 2024). Prior studies propose rescal-\ning weights and activations to reshape their distributions for\nbetter quantization compatibility while preserving mathe-\nmatical equivalence (Xiao et al., 2022; Shao et al., 2024;\nLee et al., 2023). However, such methods often experience\nperformance degradation in 4-bit inference (Lin et al., 2024).\nData rotation strategies, including QuaRot (Ashkboos et al.,\n2024) and SpinQuant (Liu et al., 2024c), use orthogonal\nmatrices to redistribute concentrated channel information\n(represented as $R$ in Fig.2(a)). QuaRot applies a random-\nized Hadamard matrix, while SpinQuant uses learned rota-\ntion matrices. DuQuant further enhances this approach by\ncombining per-channel permutation and rotation, achieving\nstate-of-the-art performance in 4-bit inference (Lin et al.,"}, {"title": "3 MICROSCALING FOR TAMING OUTLIERS", "content": "In this section, we systematically analyze activation out-\nliers across various LLMs using representative statistical\nmeasures-kurtosis and mean to understand the effects\nof microscaling (i.e., reducing a quantization group to 32\nelements). Kurtosis, the fourth standardized moment, is\ncommonly used to assess the prevalence of outliers (Liu\net al., 2024c; Li et al., 2024), while the mean reflects asym-\n3.1 Analysis of LLM's Activation Outliers\nFig. 3(a) and (b) present the kurtosis box plots for the\nOPT (Zhang et al., 2022) and LLaMA-like models (LLaMA,\nQwen, Mistral (Touvron et al., 2023; AI@Meta, 2024; Bai\net al., 2023; Jiang et al., 2023)). In cases of row-wise group-\ning (typically $GS \\gg 1024$), the OPT models exhibit high\nkurtosis in FFN1 activations, indicating many outliers that\nchallenge quantization. Additionally, outlier prevalence in-\ncreases with model size, aligning with previous findings that\nlarger models are more affected by quantization (Dettmers\net al., 2022). Conversely, LLaMA-like models use the Gated\nLinear Unit (GLU) activation function, involving extra ma-\ntrix multiplication; thus, data passing through FFN1 un-\ndergoes element-wise multiplication before FFN2, further\namplifying outliers\u2014a phenomenon observed in recent stud-\nies (Yang et al., 2024; Fishman et al., 2024). Notably, outlier\ndominance is reduced as group size decreases in both model\ntypes. At $GS = 32$, kurtosis nearly disappears, suggesting\nthe activation dynamic range within groups becomes more\nsuitable for quantization. This observation helps explain the\npreliminary success of MXFP8 in direct-casting for selected\nLLMs (Rouhani et al., 2023b), but it does not explain the\ndisappointing performance of MXFP4.\nTo assess the trade-offs in the MX format's handling of\noutliers, we examine the box plots of group means, which\nreflect distribution asymmetry. Fig. 3(c) and (d) show the"}, {"title": "3.2 Data Rotation vs. Microscaling", "content": "We then examine how data rotation reduces outliers along-\nside microscaling and assess its effectiveness as group size\ndecreases. Fig. 3(e) shows the kurtosis before and after\napplying data rotation using a random Hadamard transform\nacross decreasing group sizes. When the group size spans an\nentire row, activation rotation substantially lowers kurtosis,\ndemonstrating its efficacy in 4-bit activation quantization.\nHowever, as group size decreases, the original activation's\nkurtosis also drops, reaching levels comparable to those\nachieved with rotation. Thus, the benefit of data rotation in\noutlier reduction diminishes with smaller group sizes.\nOn the other hand, Fig. 3(f) shows the group means of\nthe activation before and after applying data rotation. As\nwith the original activation, the group means scatter more\nas group sizes decrease, but this scattering is even more\npronounced with rotated activations. This indicates that\nrotation introduces an additional asymmetry in group dis-\ntributions, which complicates quantization with MXFP4's\nsymmetric representation (cf. Table 4). In other words, data\nrotation and microscaling lack synergy, as both focus on\noutlier suppression without addressing asymmetry. Given\ndata rotation's dependence on calibration, developing a new\nmicroscaling data format that can handle group distribution\nasymmetry becomes increasingly compelling."}, {"title": "3.3 Multi-modal LLM's Activation Outlier", "content": "To further understand activation outliers under microscal-\ning in multi-modal LLMs, we examine the popular vision-\nlanguage model LLaVA (Liu et al., 2023a). LLaVA com-\nbines a visual encoder and a language model backbone: an\nimage is processed by a vision transformer-based encoder\n(ViT, (Dosovitskiy et al., 2021)) to generate vision tokens,\nwhich are then input to the language model along with lan-\nguage tokens from the user prompt.\nAs shown in Fig. 4(a), both vision and language tokens\nexhibit outliers within the same hidden dimension of the ac-\ntivation, though their distributions differ. Language tokens\ntypically concentrate around larger magnitudes, while only\nsome vision tokens reach high magnitudes, a trend observed"}, {"title": "4 ASYMMETRIC MICROSCALING FORMAT", "content": "The findings from Sec. 3 motivate the development of a new\nmicroscaling format that inherently supports asymmetric\ndata representation. In this section, we explore the design\nspace of the microscaling data format ($P_i$ and $S$) along-\nside considerations for asymmetric quantization schemes to\nenable efficient MAC unit implementation.\n4.1 Asymmetric Quantization\nWe consider two popular asymmetric quantization schemes:\nasymmetric INT (AsymINT) and asymmetric FP (AsymFP).\nAsymmetric INT Quantization (AsymINT). In INT quan-\ntization, asymmetry is applied through a zero-point, shift-\ning the data range from zero-centered to span between the\nminimum and maximum values, represented uniformly in\nreduced-precision integers (Dettmers et al., 2022). Multiply-\ning two AsymINT numbers incurs computational overhead\ndue to cross-terms associated with the zero-point (see Ap-\npendix B.3 for details).\nAsymmetric FP Quantization (AsymFP). In FP quanti-"}, {"title": "4.2 Selecting Element-Wise Data Format", "content": "First, we explore the design space of the data format for\neach element $P_i$. To understand the benefits of asymmet-\nric data format, we compare the mean-square error (MSE)\non example activation data (sampled from LLaMA2-7B's\nQKV-Proj at layer 5) with other symmetric data format\n(FP4, NF4 (Dettmers et al., 2023), SF4 (Dotzel et al., 2024)).\nFig. 5(a) shows the characteristics of this activation in terms\nof group means (x-axis) and kurtosis (y-axis). As a refer-\nence, we cluster the groups based on similarity in mean and\nkurtosis, then conduct Lloyd-Max algorithm (Lloyd, 1982)\nfor close-to-ideal quantized number representation (100 it-\nerations; the number of clusters is set as 16, as no further\nMSE reduction observed from more clusters).\nFig. 5(b) presents the MSE of various element-wise data\nformats. Compared to Lloyd-Max quantization (used as a\nreference), all symmetric data formats show a significant\nMSE increase, with INT4 experiencing the most notable\ndegradation. In contrast, AsymINT4 and AsymFP4 achieve\nlower MSE, with AsymFP showing MSE closest to Lloyd-\nMax (a consistent trend across models and layers). This\nfinding supports the selection of AsymFP4 as the element-\nwise format, further validated empirically in Table 2."}, {"title": "4.3 Selecting Shared-Scale", "content": "With AsymFP4 selected as the preferred element-wise data\nrepresentation, we further examine options for shared pos-\nitive and negative scales (variations in the exponent and\nmantissa in Eq. 1) for efficient MAC unit implementation.\nPoT: When $M_p = M_n = 1$, the dynamic range for positive\nand negative values can be adjusted via exponent modifica-\ntion. For AMX, an additional shift operation is required in\nthe intra-level reduction in Fig. 1(d) to account for positive\nand negative scales. We found that the 5-bit PoT avoids\nsignificant clamping errors for quantizing large magnitude\nvalues (see Appendix A.2 for more details).\nFP8: While a 5-bit PoT scale prevents clamping errors, its\nlimited resolution still causes accuracy degradation. To ad-\ndress this, we propose using FP8 scales, which significantly\nreduce memory requirements and minimize resource over-\nhead for multiplication due to a shorter mantissa than FP16.\nWith a 5-bit exponent, E5M2 effectively mitigates accuracy\nloss from a limited dynamic range (see Table 16 for ablation\nstudies). FP8 scales require additional multiplication of the\nmantissas for positive and negative scales depending on the\noperand's sign; however, this incurs minimal resource over-\nhead due to the small mantissa size, and once calculated, the\noverhead is shared within a group. Our evaluation shows\nthat AMXFP4-FP8 incurs only a 10% overhead compared\nto MXFP4-FP8 (Fig. 1(d))."}, {"title": "5 EXPERIMENTS", "content": "5.1 Impact of Microscaling and Data Rotation\nSec. 3.2 demonstrates that data rotation effectively addresses\nactivation outliers in configurations with large group sizes\nbut shows limited compatibility with microscaling. We ex-\namine the impact of data rotation on Wikitext-2 (Merity\net al., 2016) perplexity across group sizes from an entire\nrow to 32 in Table 1. For the data rotation, we apply a ran-\ndomized Hadamard transform matrix (following (Ashkboos\net al., 2024)) and quantize the input operands for all matrix\nmultiplications within the decoder layer. When the group\nsize encompasses an entire row, MXFP8-PoT maintains\nperformance close to the baseline, with MXFP6-PoT experi-\nencing only minimal performance loss. For LLaMA2-13B,\nMXFP4-FP8 undergoes a notable increase in perplexity,\nwhile AMXFP4-FP8 reduces perplexity to approximately\n34. Importantly, data rotation substantially lowers perplexity\nto around 11 without the need for asymmetric representa-\ntion, highlighting its effectiveness in handling outliers with\nlarger group sizes. However, as the group size decreases,\ndata rotation significantly increases perplexity by 3.6 in\nLLaMA2-7B with MXFP4-FP8, whereas AMXFP4-FP8\nachieves a reduction of 0.3 in the same model. This result\naligns with our finding that outlier handling becomes less"}, {"title": "5.2 Comparison with Calibration-Based Methods", "content": "We investigate the influence of the calibration dataset on\noutlier handling techniques and examine the calibration over-\nhead of these methods. Additionally, we compare our ap-\nproach with state-of-the-art (SOTA) techniques for weight-\nactivation quantization. In this section, we omit quantization\nfor activations in cases where previous studies do not apply\nit (e.g. Softmax output), in order to evaluate whether our\nproposed MX format can directly replace existing outlier\nhandling techniques. Detailed experimental settings are\nprovided in Appendix B.2.\nRobustness to Calibration Set Distributions. We in-\nvestigate the sensitivity of QuaRot and SpinQuant to dif-\nferent calibration set distributions, as shown in Table 3.\nWe measure perplexity (PubMed (of the U.S. National Li-\nbrary of Medicine, 2023) and Enron Emails (Klimt & Yang,\n2004)) and accuracy (PIQA (Bisk et al., 2019) and Wino-\nGrande (Sakaguchi et al., 2019)) under conditions where\nthe calibration and evaluation datasets are the same and\nwhere they differ. Details on the composition of the calibra-\ntion dataset are provided in the Appendix B.2. QuaRot and\nSpinQuant significantly improve performance over random\nHadamard rotation but show a clear pattern of achieving\nbetter results on data observed during calibration. There is\nan exception that SpinQuant achieves high performance on\nboth PIQA and WinoGrande when PIQA data is used for cal-\nibration, though a 2-3% performance difference arises solely\nfrom varying calibration datasets, indicating that calibration-\nbased methods are highly dataset-sensitive. MXFP4-PoT is\nunaffected by the calibration set but exhibits severe perfor-\nmance degradation, whereas the proposed AMXFP4-FP8\nsignificantly enhances performance, surpassing traditional\ncalibration-based methods.\nCalibration Overhead and Performance Variation. Ta-\nble 4 displays the effects of varying calibration settings\n(dataset, sequence length, and number of samples) on\nWikitext-2 perplexity, ARC-Challenge (Clark et al., 2018)\nand WinoGrande accuracy for QuaRot and SpinQuant.\nWhen using QuaRot alone, CSQA accuracy drops by 10%.\nWhen combined QuaRot with GPTQ, results depend on cali-\nbration settings; using only 32 calibration samples leads to a\n2.4% reduction in WinoGrande accuracy compared to using\n128 samples. SpinQuant, which trains a rotation matrix,\nachieves higher accuracy than QuaRot alone but increases\ncalibration time by approximately 6\u00d7 and exhibits greater\nsensitivity to the calibration set. When calibrated with the\nPTB (Marcus et al., 1993) dataset instead of Wikitext-2,\nperplexity on Wikitext-2 rises by around 0.9. The proposed\nAMXFP4 shows minimal performance degradation com-\npared to the baseline and remains unaffected by calibration\nsettings.\nComparison with Other 4-bit Inference Methods. Ta-"}, {"title": "5.3 Enhancing MX Performance", "content": "We experiment with AMXFP4-FP8 across various bench-\nmarks, including chatbot, visual, and long-context tasks,\nto explore the practical advantages of our proposed data\nformat in real-world applications compared to MXFP4-PoT.\nExperimental details, including quantization settings and\nbenchmarks, are provided in Appendix B.1.\nMulti-Turn Chatbot Tasks. Quantization adversely affects\nthe conversational capabilities of chatbots (Lee et al., 2024);\ntherefore, we conduct an MT-Bench evaluation (Zheng et al.,\n2023) on LLaMA2-Chat-7B (Touvron et al., 2023) to de-\ntermine whether AMXFP4-FP8 significantly improves con-\nversational abilities compared to MXFP4-PoT. MT-Bench\nassigns scores ranging from 1 to 10, given by GPT-4 (Ope-\nnAI, 2023), to responses generated from an initial question\nand a subsequent follow-up question across 80 multi-turn\nconversations. Fig. 6 presents the normalized scores of\nMXFP4-POT, MXFP4-FP8, and AMXFP4-FP8, with the\n16-bit baseline score set to 1. While MXFP4-PoT inference\nshows severe performance degradation across all categories,\nAMXFP4-FP8 demonstrates recovery of conversational abil-\nities close to the baseline. Fig. 10 provides detailed ex-"}, {"title": "6 CONCLUSION", "content": "To meet the computational demands of large language mod-\nels (LLMs) with extended contexts, we introduce Asym-\nmetric Microscaling 4-bit Floating-Point (AMXFP4), which\nuses asymmetric shared scales to handle outliers and quan-\ntization asymmetry. Unlike traditional methods requiring\ncostly calibration, AMXFP4 provides direct 4-bit inference\nwith high accuracy, outperforming MXFP4 and other tech-\nniques for efficient, calibration-free inference."}]}