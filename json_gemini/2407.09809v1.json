{"title": "Preserving the Privacy of Reward Functions in MDPs through Deception", "authors": ["Shashank Reddy Chirra", "Pradeep Varakantham", "Praveen Paruchuri"], "abstract": "Preserving the privacy of preferences (or rewards) of a sequential decision-making agent when decisions are observable is crucial in many physical and cybersecurity domains. For instance, in wildlife monitoring, agents must allocate patrolling resources without revealing animal locations to poachers. This paper addresses privacy preservation in planning over a sequence of actions in MDPs, where the reward function represents the preference structure to be protected. Observers can use Inverse RL (IRL) to learn these preferences, making this a challenging task.\n\nCurrent research on differential privacy in reward functions fails to ensure guarantee on the minimum expected reward and offers theoretical guarantees that are inadequate against IRL-based observers. To bridge this gap, we propose a novel approach rooted in the theory of deception. Deception includes two models: dissimulation (hiding the truth) and simulation (showing the wrong). Our first contribution theoretically demonstrates significant privacy leaks in existing dissimulation-based methods. Our second contribution is a novel RL-based planning algorithm that uses simulation to effectively address these privacy concerns while ensuring a guarantee on the expected reward. Experiments on multiple benchmark problems show that our approach outperforms previous methods in preserving reward function privacy.", "sections": [{"title": "1 Introduction", "content": "In the realm of decision-making, particularly in situations involving resource allocation in the context of security, agents face the complex task of making choices that are potentially observable by external entities. These choices can carry substantial implications, revealing critical insights into the preferences (or significance) over different targets (or states in general); which can be strategically harnessed by observers in potentially harmful ways. The central challenge lies in optimizing these decisions while safeguarding the privacy of the agents' underlying preferences. Our specific focus is on addressing this challenge within the context of Reinforcement Learning (RL) based planners where the reward function represents the preferences that must be kept private. In such a case, it is crucial to recognize that a significant portion of the reward is embedded in the agents's decision-making policy. Therefore the agent must take actions that preserve the privacy of the reward while still achieving good performance. Take, for instance, green security games (GSGs) [9], where forest rangers patrol to monitor various animal populations. Poachers observing these patrols could exploit the information to locate and target animals. Thus, rangers must conduct effective surveillance while simultaneously deceiving the poachers. Similarly, in urban policing, cities are divided into regions, with each region assigned a reward based on factors such as crime rates, wealth, etc [6, 10]. It is important for law enforcement to keep this reward function private for enhanced security.\n\nThe potential of reverse-engineering the agent's reward forms the basis for the field of Inverse Reinforcement Learning (IRL) [23] which poses a substantial privacy risk. IRL has demonstrated the remarkable ability to reconstruct high-quality reward functions across various environments [17, 11]. This concern underscores the importance of developing robust mechanisms to shield the agent's reward function, ensuring the integrity of decision-making and preventing potential privacy breaches. The problem of privacy preservation of the reward function is illustrated in Figure 1. First, the user defines the reward function r, which encodes their preferences. Next, a private RL algorithm learns a policy that maximizes the reward while simultaneously keeping the reward function private. An observer can then use an IRL algorithm to recover a reward function 7 by observing demonstrations of the agent. If 7 is of a high quality it will have properties very similar to r making it feasible for an observer to estimate the preferences of the user. An ill-intentioned observer can use this information about the reward function to manipulate the agent to engage in undesired behaviours [12]. IRL covers the entirety of methods for recovering the reward function within our specific context, wherein the observer lacks additional information such as the nature of the reward function, domain knowledge, etc. However, a notable limitation of IRL lies in its assumption that demonstrations are not deceptive. To address this limitation, we introduce two modifications to IRL algorithms that"}, {"title": "2", "content": "account for deceptive demonstrations.\n\nExisting Work tackles the problem in two ways:\n(1) Through the use of Differential Privacy (DP) methods: DP-based methods [26, 34, 37] and the Deep Q-learning with Functional Noise (DQFN) algorithm [35] introduce noise to computations such as Q-functions, Value functions, and Policy Gradients. This addition of noise guarantees that reward functions within lo-neighbourhood of each other return the same policy, making it difficult for an observer to reconstruct the exact reward function. In the context of reward reconstruction, these guarantees are ill-suited as (a) there are infinitely many reward functions that can explain the observed behaviour, (b) the lo and lp norms are not good metrics to use when comparing reward functions as two reward functions in the same lo neighbourhood may possess several other properties that pose a privacy leak such as ordering of polices (explained in Section 3). This leads to a privacy leak in practice as highlighted in [26]. This is in addition to the fact that these approaches lack built-in reward constraints make it difficult for a user to balance the tradeoff between expected reward and privacy without resorting to time-consuming hyper parameter searches. As shown in Figure 2, it is difficult to achieve a good privacy/reward trade-off due to high variance in expected reward as noise is increased. As highlighted in [26], another drawback of these methods is that the quality of the reward function recovered by an observer is independent of the noise added, undermining their effectiveness as a private algorithm.\n(2) Through the use of deception: Deception involves the act of intentionally creating or upholding false beliefs in the minds of others [5]. There are two primary approaches to deception: (a) \"dissimulation\" that relates to \"hiding\" the truth, and (b) \"simulation\" which entails providing false information to \"mislead\" the observer into believing something that is not true. The Max Entropy Intentional Randomization (MEIR) [25] algorithm developed to preserve the privacy of the reward function is an existing \"dissimulation\" based deception algorithm (as shown later in this paper). Although the MEIR algo-rithm satisfies constraints on the expected rewards, we show that it leaks significant information about the reward function when faced with IRL-based observers. Other deception-based planning algorithms such as those discussed in [18, 19, 22, 24, 21, 20], are limited in their ability to tackle this problem. This limitation arises from their focus on optimizing deception for a singular trajectory, inadvertently disclosing information about the reward function across multiple trajectories."}, {"title": "Background", "content": "These methods also consider a different problem as discussed in Section G in the Appendix.\nContributions: Our contributions are as follows:\n\u2022 Theoretical Analysis of Privacy Leak for MEIR: We demonstrate theoretically and intuitively the significant privacy vulnerabilities of the MEIR algorithm when faced with an IRL observer.\n\u2022 Novel Max Misinformation Algorithm: We introduce the innovative Max Misinformation (MM) Algorithm, designed to address the shortcomings of MEIR and DP-based methods. A key element is the introduction of an anti-reward function, enabling a balanced tradeoff between the expected value and the ability to deceive the observer.\n\u2022 Effectiveness Against IRL algorithms: We provide insights into why MM can robustly counteract observers utilizing various IRL algorithms, demonstrating its superiority in preserving privacy of the reward function. In addition, we experiment against two additional algorithms based on IRL that an observer might use if they know they are being deceived, and demonstrate the robustness of the proposed algorithm in this case as well.\n\u2022 Comprehensive Evaluation: To gauge the effectiveness of our algorithm, we rigorously evaluate it against IRL-based observers across diverse benchmark environments. We measure the quality of the recovered reward functions in comparison to the original reward using the Rollout method [13], Pearson Correlation, and the Equivalent-Policy Invariant Comparison (EPIC) distance [13]. Our conclusive findings highlight that the MM algorithm outperforms existing deception-based and DP-based algorithms, firmly establishing its efficacy in maintaining the privacy of the reward.\nWe provide a brief overview of the relevant decision-making models (Markov Decision Process (MDP) and Deceptive RL), Max Casual Entropy Inverse Reinforcement Learning (MCE-IRL) and Max Entropy Reinforcement Learning (MERL) which is the backbone for MCE-IRL based algorithms as well as pre-existing Deceptive RL formulations.\nMarkov Decision Process We consider environments that can be expressed as Markov Decision Processes (MDP). An MDP M is defined by the tuple $(S, A, P, r, \\gamma, \\mu)$, where S is the set of states, A is the set of actions, $P(s'|s, a) \\in [0, 1]$ is the transition probability, $r(s, a) \\in \\mathbb{R}$ is the reward function, $\\gamma \\in [0, 1]$ is the discount factor and u is the initial state distribution. A policy $\\pi(.s)$ is a probability distribution over the set of valid actions for a given state. In this paper, we base our results on the assumption that both S and A are discrete, with every state reachable under \u00b5 and P. The cumulative \\gamma-discounted value, or expected value, of the reward obtained by following \u03c0 in M is denoted as $E_{\\pi}[r(s, a)] = E[\\Sigma_{\\tau=0}^{\\infty} r(s_t, a_t)]$. The occupancy measure $p_\\pi: S \\times A \\rightarrow \\mathbb{R}$ of a policy \u03c0 is defined as $p(s, a) = (1-\\gamma)\\pi(a|s) \\Sigma_{\\tau=0}^{\\infty} \\gamma^tP(s = s_t|\\pi)$. The expected reward can be expressed in terms of occupancy measures as $E_{\\pi}[r(s, a)] = \\Sigma_{s} \\Sigma_{a} p_\\pi(s, a)r(s, a)$. For brevity, we sometimes use p to denote the occupancy measure of a policy \u03c0. It is worth mentioning that there exists a one-one mapping between a policy and its corresponding occupancy measure [32]. For rest of this paper, we rely on this result to use and pinterchangeably.\nDeceptive Reinforcement Learning Let R be the set of all reward functions, then a deceptive reinforcement learning problem is defined by the tuple, (S, A, P, r, \\gamma, \\mu, LR), where S, A, P, r, \\gamma, \\mu are the same as defined for a regular MDP, and $LR(s, a)$ stands for"}, {"title": "3", "content": "deception-inducted reward function [24] which combines the objective of reward maximization with deception.\nA deceptive policy maximises the objective,\n$J_D = E[L_R(s, a)]$ (1)\nWe do not make any assumptions about the knowledge of the observer similar to [25]. In such a case, $L_R$ is a weighted mixture of the reward function and the deception level [21],\n$L_R(s, a) = wr(s, a) + dr'(s,a)$ (2)\nwhere $d'(s, a) \\in \\mathbb{R}$ is a measure of deception, and $w \\in \\mathbb{R}$ controls the trade-off between reward maximization and deception. For example, $d'(s, a)$ could be the entropy of the policy, i.e, \u2013 log (\u03c0(as)), leading to deception by dissimulation.\nMaximum Entropy RL (MERL) The objective of Maximum En-tropy Reinforcement Learning (MERL) [16] is to optimize both the value function and the entropy or uncertainty of the agent's policy. Formally,\n$RL(r) = \\underset{\\pi}{argmax} E_{\\pi} [r(s, a)] + H(\\pi)\\}$ (3)\nwhere r is the reward function, $H(\\pi)= E_{\\pi}[-log(\\pi(a|s))]$ is the \\gamma-discounted cumulative casual entropy. MERL is an important algorithm in this paper as the MCE-IRL model (discussed below) is built on MERL. In addition, we showcase in subsequent sections that the MEIR algorithm (discussed below) is also an instance of MERL making it a \"dissimulation\" based Deceptive RL algorithm.\nMaximum Entropy Intentional Randomization (MEIR) The MEIR algorithm is a private RL algorithm that is driven by the concept that maximizing the entropy of the policy H will keep the reward function private. MEIR solves the following optimization problem,\n$MEIR(r, E_{min}) = \\underset{\\pi}{argmax} H(\\pi)$\nsubject to $E_{\\pi}[r(s, a)] \\geq E_{min}$ (4)\nwhere $E_{min} \\in [\\hat{E}, E^*]$ is the reward threshold. $\\hat{E} = E_{\\hat{\\pi}}[r(s,a)]$ and $E^* = E_{\\pi^*}[r(s, a)]$ where $\\hat{\\pi}$ and $\\pi^*$ correspond to the uniform random and optimal deterministic policies, respectively. The reward threshold $E_{min}$ is used to control the privacy/reward tradeoff.\nMaximum Causal Entropy IRL (MCE-IRL) The Maximum Causal Entropy Inverse Reinforcement Learning [38] model has emerged as the most prominent method to infer an unknown reward function from demonstrations. Given the occupancy measure p of an agent (calculated from demonstrations) MCE IRL recovers a reward function based on the following formulation,\n$MCE\\text{-}IRL(\\rho) = \\underset{r}{argmax} \\underset{\\rho}{min} E_\\rho[r] - E_{\\rho}[r] - H(\\rho)$\nMCE-IRL is closely linked with MERL as the occupancy measures of the agent is obtained from the recovered reward function r as,\n$\\rho = RL(r)$ (5)"}, {"title": "Assessing Learnt Reward, r, from IRL", "content": "Before we describe our contributions, we describe mechanisms to evaluate whether the original preferences (reward) are captured by the reward learnt by an observer (using IRL). It is challenging to assess the quality of the recovered reward function due to the inherent ambiguity in the Inverse RL problem. The ambiguity is on account of multiple reward functions being able to explain the demonstrated behaviour. To evaluate the quality of the recovered reward function, we adopt the framework proposed in [31], which introduces three quality standards.\nThe first standard implies the preservation of the \"ordering\" of policies with respect to the true reward function, r in the recovered reward function, 7 as an indicator of the highest quality. A policy $\\pi_1$ is better ($\\succ$) than a policy $\\pi_2$ if the expected value with policy $\\pi_1$ is higher than the expected value of $\\pi_2$.\n$\\pi_1 \\succ \\pi_2$\n$E_{\\pi_1} [r(s, a)] > E_{\\pi_2} [r(s, a)] \\land E_{\\pi_1} [\\tilde{r}(s, a)] \\geq E_{\\pi_2}[\\tilde{r}(s, a)]$\nThe second standard implies that the recovered reward function, r shares the same set of optimal policies as the true reward function.\n$\\underset{\\pi}{argmax}E_{\\pi} [r(., .)] = \\underset{\\pi}{argmax}E_{\\pi}[\\tilde{r}(., .)]$\nLastly, the third criterion implies that the recovered reward function fails to preserve any of these desirable properties, indicating a lower level of quality in learning.\nBy applying these three quality standards, we can assess and com-pare the effectiveness of the recovered reward function. Matching optimal policies between the true and recovered reward functions sig-nifies valuable insights into the agent's optimal trajectories. If policy ordering is preserved, the observer not only gains trajectory insights but also learns their order, increasing the chances of unveiling the agent's encoded preferences in the reward function.\nFormally, Let R be the set of all possible reward functions $r:S\\times A \\rightarrow \\mathbb{R}$ with state space S and action space A, and $M < S, A, P, r, \\gamma, \\mu >$ be an MDP without a reward function. Let partitions $OPT_M$ and $ORD_M$ be defined on R as follows: given two reward functions $r_1$ an $r_2$, we say that $r_1 =_{OPT_M} r_2$ if $< S, A, P, r_1, \\gamma, \\mu >$ and $< S, A, P, r_2, \\gamma, \\mu >$ have the same set of optimal policies, and $r_1 \\equiv_{ORD_M} r_2$ if $< S, A, P, r_1, \\gamma, \\mu >$ and $< S, A, P, r_2, \\gamma, \\mu >$ have the same ordering over policies 3.\nIf two reward functions have the same ordering of policies, then they have the same set of optimal policies (Section 2.4 in [31])."}, {"title": "4 Privacy Leak in MEIR", "content": "We study the privacy leak of MEIR in two situations: (i) Observer has access to the agent's true occupancy measure; (ii) Observer obtains a few demonstrations instead of the true occupancy measure. For (i), we can theoretically prove that there exists a privacy leak. For (ii), we provide a bound on the quality of the recovered reward function 7 w.r.t r in terms of the distribution over policies they induce.\n4.1 True Occupancy Measure: MEIR = MERL\nWe show this by hypothesizing that any policy that is a solution for MEIR can be computed by solving the MERL problem, where the rewards are multiplied by a positive scalar.\nLemma 1. Any policy that is the solution of a Max Entropy In-tentional Randomization formulation $MEIR(r, E_{min})$ with a reward constraint $E_{min} \\in [\\hat{E}, E^*]$, can be expressed as the solution of the Maximum Entropy RL problem as,\n$\\pi = RL(\\lambda^*r)$ (6)"}, {"title": "4.2 Limited Demonstrations:", "content": "In section 4.1, we showed that the policy generated by the MEIR algorithm can be represented as a solution to the MERL objective, i.e, $MEIR(r, E_{min}) = RL(\\lambda^*r)$. The solution of the MERL objective can also be interpreted as a mixture policy over the set of all stochastic policies \u041f, with the weight given to each policy proportional it's value. Formally, a mixture policy $\u03c0_{mix}$ contains a set of policies {$\u03c0_1, ..., \u03c0_n$}, and a distribution w over these policies. Before each episode, a policy is sampled according to w and executed for the entire trajectory. The probability of a sampling policy \u03c0\u2208 \u03a0 is given by [38],\n$P_{MERL} (\\pi|r) \\propto E_{\\pi}[r]$ (7)\nThus, learning this distribution (or a reward function that induces this distribution over policies) will give an observer insight into the ordering over policies in r. During execution however, from Lemma 1, we know that the agent samples a policy according to the distribution,\n$P_{MERL} (\\pi|\\lambda^*r) \\propto E_{\\pi}[\\lambda^*r]$\nwhere x \u2265 0 which the observer learns via maximum likelihood (MCE-IRL). This distribution preserves the same ordering of policies in r and hence the accurate estimation of this distribution by an observer poses a significant privacy leak.\nThe quality of the distribution learned (Total Variation (TV) dis-tance from the true distribution) is highlighted in Proposition 1.\nProposition 1. For any MDP M, let \u03c0 = MEIR(r, $E_{min}$) for any reward constraint $E_{min} > \\hat{E}$, i.e, \u03c0 = RL(\u03bbr) for some \u03bb > 0 (Lemma 1) and let p be the empirical occupancy measure of n demonstrations obtained by executing \u03c0 in M. If r = MCE-IRL(p\u5143), then,\n$Pr(TV(P_{MERL} (\\pi|\\lambda^*r), P_{MERL} (\\pi|\\tilde{r})) > \\epsilon) \\leq \\delta$ (8)\nwhere, $\\delta = O(e^{-|\\Pi|-n\\epsilon^2)}$\nProposition 1 directly follows from [4] given that P(\u03c0\u2758r) is the maximum likelihood estimator of P(\u03c0\u03bbr)."}, {"title": "5 The Max Misinformation Algorithm", "content": "To address the privacy leak of MEIR, we introduce a novel algorithm referred to as the Max Misinformation (MM) algorithm. MM uses a deceptive measure called an anti-reward to incentivize the agent to stay away from the optimal trajectories. That is to say, MM intentionally leads the agent to take sub-optimal trajectories, in a bid to fool the observer into believing that these trajectories are highly rewarding. This makes MM a simulation based Deceptive RL algorithm.\nFormally, let $\\bar{r}(s, a)$ be an anti-reward that induces the agent to take sub-optimal trajectories, then the MM algorithm solves the following constrained optimization problem,\n$MM(r, \\bar{r}, E_{min}) = \\underset{\\pi}{argmax} E_{\\pi}[\\bar{r}(s, a)]$\ns.t. $E_{\\pi}[r(s, a)] \\geq E_{min}$ (9)\nwhere $E_{min} \\in [\\bar{E}, E^*]$ is the reward threshold that is used to control the privacy-expected reward tradeoff. $\\bar{E} = E_{\\bar{\\pi}}[r(s,a)]$ and $E^* = E_{\\pi^*}[r(s, a)]$ where $\\bar{\\pi}$ and $\\pi^*$ correspond to the optimal policies with respect to the anti-reward, r and actual reward, r. We describe mechanisms for computing anti-reward in Section 5.2.\nThe MM formulation is a linear program (Appendix B) and hence the primal optimum can be uniquely recovered from the dual optimum X* [3, Section 5.5.5] as,\n$\\pi = \\underset{\\pi}{argmax} E_{\\pi}[\\lambda^*r(s, a) + \\bar{r}(s, a)]$ (10)\nEquation 10 is a form of the Deceptive RL objective 2, where $d(s,a) = \\bar{r}(s, a)$ and the dual variable \u03bb* acts as a tempera-ture parameter controlling the trade-off between reward and deception maximization. As $ \\underset{\\lambda \\rightarrow 0}{lim}, the anti-reward dominates the reward resulting in $\\bar{\\pi}$, and as  $\\underset{\\lambda \\rightarrow \\infty}{lim}$, the reward dominates the anti-reward resulting in \u03c0* as the solution to Equation 10.\nThe linear program formulation of MM presented in Appendix B relies on known model dynamics. However, this assumption is often impractical in real-world scenarios. To address this limitation, we introduce Algorithm 1, which demonstrates how to solve Equation 9 using primal-dual descent without requiring explicit knowledge of the model dynamics. This formulation is particularly useful in the context of large MDPs with continuous state and (or) action spaces. In such scenarios, solving the primal problem to convergence (Line 4 of Algorithm 1) can be very time-consuming. Instead, one could take a few steps towards maximizing the objective function: Line 4 and then incrementally optimize A and so on.\nIn the case of discrete MDPs, where solving the primal problem is much faster, we can use binary search to speed up the optimization procedure significantly as highlighted in Appendix C.1."}, {"title": "5.1 Security of the Max Misinformation Algorithm", "content": "In Section 4, we highlighted that the privacy leakage in the MEIR algorithm was due to the fact that MEIR is an instance of MERL, which preserves the ordering of policies which can be learnt efficiently by the observer. The proposed MM algorithm addresses this privacy leakage as the addition of an anti-reward does not preserve the ordering over policies as it assigns a high value to sub-optimal trajectories. Consequently, it does not preserve the set of optimal policies either.\nThe difference between the occupancy measures of the MEIR and the MM algorithms are highlighted in Figure 3. Despite the MEIR policy exhibiting higher entropy, it readily reveals locations with high rewards. In contrast, the MM policy visits a nuanced mix of high and low reward states, making it more challenging to discern important locations."}, {"title": "5.2 Generating anti-reward functions", "content": "We now describe mechanisms for generating anti-reward functions that maximize deception by steering an agent away from the optimal trajectories.\nIdeally, we would like to maximize the distance between the true reward function and the recovered reward function, but this would make the deceptive policy specific to an IRL algorithm (as recovered reward function is dependent on the algorithm). Instead, to ensure robustness against the observer reward recovery methods (IRL or some other mechanism), we propose a mechanism that is agnostic to the specific algorithm utilized to recover the reward function.\nIntuitively, we compute an anti-reward that maximizes the distance between a distribution/statistic corresponding to the optimal policy for the original reward and optimal policy for the anti-reward. This will ensure that observer receives minimal information about the optimal policy for the original reward. Let o be a distribution/statistic that can be computed from the agent's reward function r. Two examples of o would be the policy computed or occupancy distribution corre-sponding to the reward function r. We can generate an anti-reward function r by maximizing the distance between o* and o, where o* is observed when behaving optimally according to r and o is observed when behaving optimally according to r. The algorithm for computing the anti-reward is provided in Algorithm 2. Let C be the function that maps from r to o. We iteratively do the following steps by starting from a randomly initialised o : (1) Set r as the anti-reward function that maximises the distance between o* and o (2) Compute the new o from the new value of r. We repeat this process for a set number of iterations. An intuition behind why this approach works is highlighted in Appendix D.3.\nWe will now outline the various forms of o (occupancy measures and trajectory distributions) and the corresponding distance measures, denoted as D, applied in each case.\nOccupancy Measures Since IRL algorithms try to match occupancy measures, one could try to directly maximize the distance between the occupancy measures of the optimal policy of r, i.e, p* and p. Hence, in this case, o = p. We use f-divergences and Integral Probability Metrics (IPMs) to measure the distance between p* and p.\nThe f-divergence between p and p* is defined using the convex conjugate f* as,\n$D_f(p^*||p\\overline{) = \\underset{g:D \\rightarrow R}{sup} E_{p^*} [g(s, a)] - E_{p^{-}} [f^* (g(s, a))]$\nsetting g = -r and $ \\phi(u) = -f^* (-u)$, so that $E_{p^{-}}$ is maximized,\n$D_f(p^*||p\\overline{) = \\underset{r^{-}:D\\rightarrow R}{sup} E_{p^{-}} [\\phi(r^{-}(s,a))] - E_{p^*} [r^{-}(s, a)]$\nIPMs that are parameterized by a family of functions F are defined,\n$\\mathcal{F}(p^{-},p^*) = \\underset{f \\in F}{sup} |E_{p^{-}}[f(s,a)] - E_{p^*}[f(s, a)]|$ (12)\nWe can see that in both IPMs (Equation 12) and f-divergences (Equation 11), the anti-reward function gives a high reward to the state-action pairs visited by \u03c0 (due to the sup and expectation over p being the first term) and a low reward to the ones visited by \u03c0*. In both these cases, the anti-reward function can be represented using a function approximator and solved using gradient ascent, or in the case of discrete environments, Equation 11 can be solved using the closed form solution (Table 2 in Appendix D.3)."}, {"title": "6", "content": "where x \u2265 0 which the observer learns via maximum likelihood\n4 If \u03c0* is deterministic $log(\\pi^* (a|s))$ is not defined when $ \\pi^* (als) = 0$. This\n(MCE-IRL). This distribution preserves the same ordering of policies can be avoided by setting \u03c0* as the a solution of MERL which ensures all actions have non-zero support.\nand (2) The quality of the recovered reward function is proportional to\nThe quantitative comparison between the Deep Q-learning with Func-\nThe effectiveness against observers who are aware of the use of decep-\nFigure 6 contains evaluations of MM against an observer that is aware\nMCE IRL based"}, {"title": "Experiments and Results", "content": "In this section, we intend to answer the following key questions: (1) Does the MEIR algorithm as described in Section 4 suffer a significant privacy leak in the case of limited demonstrations, and how does the MM algorithm perform in comparison? (2) How does the MM algorithm fare in comparison to the MEIR and DQFN algorithms in preserving the privacy of the reward function when the observer has access to the true occupancy measures of the agent? (3) How does MM perform against observers that know they are being deceived?\n6.1 Environments and Evaluations Metrics\nWe conduct our experiments in the following environments: Cyber Security which is based on Moving Target Defence [30], Frozen Lake [33], Four Rooms [7] and randomly generated MDPs. In the CyberSecurity [30] environment, the state space consists of network config-urations generated by the CyberBattleSim library [29] that emulates real-world active directory networks. The value associated with a network configuration corresponds to the ease with which malicious actors could compromise and gain control over the network. Agent's objective is to dynamically switch between network configurations to bolster security. The intention behind using Frozen Lake [33] and Four Rooms [7] from the standard Gym environment draws inspi-ration from tangible real-world security challenges such as Police Patrolling [6] and Green Security Games (GSGs) [9]. Randomly generated MDPs can be representative of a broad set of domains in general. A more detailed description is provided in Appendix F. The reward function in the above domains reflects the user's preferences, encompassing critical aspects such as the valuation of patrol locations, density of animals in various regions, and significance of distinct network configurations, all of which must be kept private.\nWe consider Inverse Reinforcement Learning (IRL) as the main method for reward function reconstruction due to two primary con-siderations: (a) Observer does not know that they are being deceived, which is the assumption made in the prior deception based methods as well [21, 20] (b) Recovering the reward function from deceptive demonstrations is not a trivial task. Consider the example in Figure 3 - the agent visits multiple diverse locations and deducing the high reward states from them is not easy for the observer. In addition, there does not exist any prior work in this space to benchmark our algo-rithms against. In such a case, irrespective of whether the observer knows that they are being deceived, learning a reward function that maximises the likelihood of the demonstrations is a strong strategy for the observer given that they are guaranteed at least Emin reward.In the IRL space, MCE-IRL based methods have demonstrated superior performance in recent times for reward reconstruction [17, 11] and hence we use MCE-IRL and IQ-learn [11] (a variant of MCE-IRL that performs well in the limited demonstrations setting) in our experiments. Furthermore, we introduce two additional baselines that try to account for observers that are aware that they are being deceived by the usage of the MM algorithm. Given that the observer knows that the agent is intentionally visiting sub-optimal states along with the optimal states, the observer can cluster the occupancy measures of the agent and selectively recover a reward function that matches just one (or more) of them. See Figure 3 and 7 - each cluster either contains the optimal states or the misleading states intentionally visited by the agent. Based on how we select the cluster(s), we split these methods into: (a) $IRL_{random}$ that picks a cluster at random and (b) $IRL_{max}$ that"}, {"title": "6.2 Analysis of Results", "content": "greedily picks the cluster that has the highest occupancy measure.\nWe use three metrics to evaluate the quality of the reward function learned by the IRL algorithms", "namely": 1, "Correlation": "high value indicates better learning; (2) EPIC distance [13]: low value indicates better learning); and (3) Evaluation of the optimal policy of the recovered reward function in the original reward function: higher expected reward implies better learning.\nPrivacy Leak in MEIR and the efficacy of MM Figures 4", "follows": "for a given private RL algorithm. First"}, {"title": "7 Conclusion and Future Work", "content": "RL-based planning algorithms have found applications in many domains including security related where protection of the reward function from potential observers becomes critical. Our study identifies vulnerabilities and limitations in existing methodologies and proposes the Max Misinformation (MM) algorithm as a solution. While our experiments are limited to discrete state/action spaces, MM can be used in continuous settings, making for future research endeavors. Further-"}, {"title": "A Proofs", "content": "A.1 Proof of Lemma 1\nLemma 1. Any policy that is the solution of a Max Intentional Entropy randomization problem $MEIR(r", "E^*": "can be expressed as the solution of the Maximum Entropy RL problem as", "32": "n$\\rho(s", "\u03c1_\u03c0$": "\u03c0\u2208\u03a0} be the set of all valid occupancy measures. There exists a bijection between and D and \u03a0 [32"}]}