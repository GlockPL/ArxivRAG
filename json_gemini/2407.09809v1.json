{"title": "Preserving the Privacy of Reward Functions in MDPs through Deception", "authors": ["Shashank Reddy Chirra", "Pradeep Varakantham", "Praveen Paruchuri"], "abstract": "Preserving the privacy of preferences (or rewards) of a sequential decision-making agent when decisions are observable is crucial in many physical and cybersecurity domains. For instance, in wildlife monitoring, agents must allocate patrolling resources without revealing animal locations to poachers. This paper addresses privacy preservation in planning over a sequence of actions in MDPs, where the reward function represents the preference structure to be protected. Observers can use Inverse RL (IRL) to learn these preferences, making this a challenging task.\n\nCurrent research on differential privacy in reward functions fails to ensure guarantee on the minimum expected reward and offers the-oretical guarantees that are inadequate against IRL-based observers. To bridge this gap, we propose a novel approach rooted in the theory of deception. Deception includes two models: dissimulation (hiding the truth) and simulation (showing the wrong). Our first contribution theoretically demonstrates significant privacy leaks in existing dissimulation-based methods. Our second contribution is a novel RL-based planning algorithm that uses simulation to effectively address these privacy concerns while ensuring a guarantee on the expected reward. Experiments on multiple benchmark problems show that our approach outperforms previous methods in preserving reward function privacy.", "sections": [{"title": "1 Introduction", "content": "In the realm of decision-making, particularly in situations involving resource allocation in the context of security, agents face the complex task of making choices that are potentially observable by external entities. These choices can carry substantial implications, revealing critical insights into the preferences (or significance) over different targets (or states in general); which can be strategically harnessed by observers in potentially harmful ways. The central challenge lies in optimizing these decisions while safeguarding the privacy of the agents' underlying preferences. Our specific focus is on addressing this challenge within the context of Reinforcement Learning (RL) based planners where the reward function represents the preferences that must be kept private. In such a case, it is crucial to recognize that a significant portion of the reward is embedded in the agents's decision-making policy. Therefore the agent must take actions that preserve the privacy of the reward while still achieving good perfor-mance. Take, for instance, green security games (GSGs) [9], where forest rangers patrol to monitor various animal populations. Poachers observing these patrols could exploit the information to locate and tar-get animals. Thus, rangers must conduct effective surveillance while simultaneously deceiving the poachers. Similarly, in urban policing, cities are divided into regions, with each region assigned a reward based on factors such as crime rates, wealth, etc [6, 10]. It is important for law enforcement to keep this reward function private for enhanced security."}, {"title": "2 Background", "content": "We provide a brief overview of the relevant decision-making models (Markov Decision Process (MDP) and Deceptive RL), Max Casual Entropy Inverse Reinforcement Learning (MCE-IRL) and Max En-tropy Reinforcement Learning (MERL) which is the backbone for MCE-IRL based algorithms as well as pre-existing Deceptive RL formulations.\n\nMarkov Decision Process We consider environments that can be expressed as Markov Decision Processes (MDP). An MDP M is defined by the tuple $(S, A, P, r, \\gamma, \\mu)$, where S is the set of states, A is the set of actions, $P(s'|s, a) \\in [0, 1]$ is the transition probability, $r(s, a) \\in R$ is the reward function, $\\gamma \\in [0, 1]$ is the discount factor and u is the initial state distribution. A policy $\\pi(.s)$ is a probability distribution over the set of valid actions for a given state. In this paper, we base our results on the assumption that both S and A are discrete, with every state reachable under \u00b5 and P. The cumulative y-discounted value, or expected value, of the reward obtained by following \u03c0 in M is denoted as $E_{\\pi}[r(s, a)] = E[\\Sigma_{\\tau=0}^{\\infty} r(s_t, a_t)]$. The occupancy measure $p_{\\pi} : S \\times A \\rarr R$ of a policy \u03c0 is defined as $p(s, a) = (1-\\gamma)\\pi(a|s) \\Sigma_{\\tau=0}^{\\infty} \\gamma^{\\tau}P(s = s_t|\\pi)$. The expected reward can be expressed in terms of occupancy measures as $E_{\\pi}[r(s, a)] = \\Sigma_s \\Sigma_a p_{\\pi} (s, a)r(s, a)$. For brevity, we sometimes use p to denote the occupancy measure of a policy \u03c0. It is worth mentioning that there exists a one-one mapping between a policy and its corresponding occupancy measure [32]. For rest of this paper, we rely on this result to use and pinterchangeably.\n\nDeceptive Reinforcement Learning Let R be the set of all re-ward functions, then a deceptive reinforcement learning problem is defined by the tuple, $(S, A, P, r, \\gamma, \\mu, L_R)$, where $S, A, P, r, \\gamma, \\mu$ are the same as defined for a regular MDP, and $L_R(s, a)$ stands for"}, {"title": "3 Assessing Learnt Reward, r, from IRL", "content": "Before we describe our contributions, we describe mechanisms to evaluate whether the original preferences (reward) are captured by the reward learnt by an observer (using IRL). It is challenging to assess the quality of the recovered reward function due to the inherent ambiguity in the Inverse RL problem. The ambiguity is on account of multiple reward functions being able to explain the demonstrated behaviour. To evaluate the quality of the recovered reward function, we adopt the framework proposed in [31], which introduces three quality standards.\n\nThe first standard implies the preservation of the \"ordering\" of policies with respect to the true reward function, r in the recovered reward function, 7 as an indicator of the highest quality. A policy \u03c0\u03b9 is better ($\\succ$) than a policy #2 if the expected value with policy \u03c0\u2081 is higher than the expected value of \u03c02.\n\n$\\pi_1 \\succ \\pi_2 \\\\ \\qquad E_{\\pi_1} [r(s, a)] > E_{\\pi_2} [r(s, a)] \\land E_{\\pi_1} [\\tilde{r}(s, a)] \\geq E_{\\pi_2}[\\tilde{r}(s, a)]$\n\nThe second standard implies that the recovered reward function, r shares the same set of optimal policies as the true reward function.\n\n$\\arg \\max_{\\pi} E_{\\pi}[r(., .)] = \\arg \\max_{\\pi} E_{\\pi}[\\tilde{r}(., .)]$\n\nLastly, the third criterion implies that the recovered reward function fails to preserve any of these desirable properties, indicating a lower level of quality in learning.\n\nBy applying these three quality standards, we can assess and com-pare the effectiveness of the recovered reward function. Matching optimal policies between the true and recovered reward functions sig-nifies valuable insights into the agent's optimal trajectories. If policy ordering is preserved, the observer not only gains trajectory insights but also learns their order, increasing the chances of unveiling the agent's encoded preferences in the reward function.\n\nFormally, Let R be the set of all possible reward functions $r:S \\times A \\rarr R$ with state space S and action space A, and $M < S, A, P, r, \\gamma, \\mu >$ be an MDP without a reward function. Let partitions $OPT_M$ and $ORD_M$ be defined on R as follows: given two reward functions r\u2081 an r2, we say that $r_1 \\equiv_{OPT_M} r_2$ if $< S, A, P, r_1, \\gamma, \\mu >$ and $< S, A, P, r_2, \\gamma, \\mu >$ have the same set of optimal policies, and $r_1 \\equiv_{ORD_M} r_2$ if $< S, A, P, r_1, \\gamma, \\mu >$ and $< S, A, P, r_2, \\gamma, \\mu >$ have the same ordering over policies 3.\n\nIf two reward functions have the same ordering of policies, then they have the same set of optimal policies (Section 2.4 in [31])."}, {"title": "4 Privacy Leak in MEIR", "content": "We study the privacy leak of MEIR in two situations: (i) Observer has access to the agent's true occupancy measure; (ii) Observer obtains a few demonstrations instead of the true occupancy measure. For (i), we can theoretically prove that there exists a privacy leak. For (ii), we provide a bound on the quality of the recovered reward function 7 w.r.t r in terms of the distribution over policies they induce."}, {"title": "4.1 True Occupancy Measure: MEIR = MERL", "content": "We show this by hypothesizing that any policy that is a solution for MEIR can be computed by solving the MERL problem, where the rewards are multiplied by a positive scalar.\n\nLemma 1. Any policy that is the solution of a Max Entropy In-tentional Randomization formulation MEIR(r, Emin) with a reward constraint $E_{min} \\in [\\hat{E}, E^*]$, can be expressed as the solution of the Maximum Entropy RL problem as,\n\n$\\pi = R_L(\\lambda^*r)$        (6)"}, {"title": "5 The Max Misinformation Algorithm", "content": "To address the privacy leak of MEIR, we introduce a novel algorithm referred to as the Max Misinformation (MM) algorithm. MM uses a deceptive measure called an anti-reward to incentivize the agent to stay away from the optimal trajectories. That is to say, MM intentionally leads the agent to take sub-optimal trajectories, in a bid to fool the observer into believing that these trajectories are highly rewarding. This makes MM a simulation based Deceptive RL algorithm.\n\nFormally, let $\\bar{r}(s, a)$ be an anti-reward that induces the agent to take sub-optimal trajectories, then the MM algorithm solves the following constrained optimization problem,\n\n$MM(r, \\bar{r}, E_{min}) = \\arg \\max_{\\pi} E_{\\pi}[\\bar{r}(s, a)]$\n\n$s.t. \\qquad E_{\\pi}[r(s, a)] \\geq E_{min}$\n\n                                                   (9)\n\nwhere $E_{min} \\in [\\bar{E}, E^*]$ is the reward threshold that is used to control the privacy-expected reward tradeoff. $\\bar{E} = E_{\\pi}[r(s, a)]$ and $E^* = E_{\\pi^*}[r(s, a)]$ where $\\pi$ and $\\pi^*$ correspond to the optimal policies with respect to the anti-reward, r and actual reward, r. We describe mechanisms for computing anti-reward in Section 5.2.\n\nThe MM formulation is a linear program (Appendix B) and hence the primal optimum can be uniquely recovered from the dual opti-mum $\\lambda^*$ [3, Section 5.5.5] as,\n\n$\\pi = \\arg \\max_{\\pi} E_{\\pi}[\\lambda^*r(s, a) + \\bar{r}(s, a)]$  (10)\n\nEquation 10 is a form of the Deceptive RL objective 2, where $d(s,a) = \\bar{r}(s, a)$ and the dual variable $\\lambda^*$ acts as a tempera-ture parameter controlling the trade-off between reward and deception maximization. As $\\lim_{\\lambda \\rarr 0}$, the anti-reward dominates the reward resulting in $\\pi^-$, and as $\\lim_{\\lambda \\rarr \\infty}$ the reward dominates the anti-reward resulting in $\\pi^*$ as the solution to Equation 10.\n\nThe linear program formulation of MM presented in Appendix B relies on known model dynamics. However, this assumption is often impractical in real-world scenarios. To address this limitation, we introduce Algorithm 1, which demonstrates how to solve Equation 9 using primal-dual descent without requiring explicit knowledge of the model dynamics. This formulation is particularly useful in the context of large MDPs with continuous state and (or) action spaces. In such scenarios, solving the primal problem to convergence (Line 4 of Algorithm 1) can be very time-consuming. Instead, one could take a few steps towards maximizing the objective function: Line 4 and then incrementally optimize A and so on.\n\nIn the case of discrete MDPs, where solving the primal problem is much faster, we can use binary search to speed up the optimization procedure significantly as highlighted in Appendix C.1."}, {"title": "5.1 Security of the Max Misinformation Algorithm", "content": "In Section 4, we highlighted that the privacy leakage in the MEIR algorithm was due to the fact that MEIR is an instance of MERL, which preserves the ordering of policies which can be learnt efficiently by the observer. The proposed MM algorithm addresses this privacy leakage as the addition of an anti-reward does not preserve the order-ing over policies as it assigns a high value to sub-optimal trajectories. Consequently, it does not preserve the set of optimal policies either.\n\nThe difference between the occupancy measures of the MEIR and the MM algorithms are highlighted in Figure 3. Despite the MEIR policy exhibiting higher entropy, it readily reveals locations with high rewards. In contrast, the MM policy visits a nuanced mix of high and low reward states, making it more challenging to discern important locations."}, {"title": "5.2 Generating anti-reward functions", "content": "We now describe mechanisms for generating anti-reward functions that maximize deception by steering an agent away from the optimal trajectories.\n\nIdeally, we would like to maximize the distance between the true reward function and the recovered reward function, but this would make the deceptive policy specific to an IRL algorithm (as recovered reward function is dependent on the algorithm). Instead, to ensure robustness against the observer reward recovery methods (IRL or some other mechanism), we propose a mechanism that is agnostic to the specific algorithm utilized to recover the reward function.\n\nIntuitively, we compute an anti-reward that maximizes the distance between a distribution/statistic corresponding to the optimal policy for the original reward and optimal policy for the anti-reward. This will ensure that observer receives minimal information about the optimal policy for the original reward. Let o be a distribution/statistic that can be computed from the agent's reward function r. Two examples of o would be the policy computed or occupancy distribution corre-sponding to the reward function r. We can generate an anti-reward function $\\bar{r}$ by maximizing the distance between o* and $\\bar{o}$, where o* is observed when behaving optimally according to r and $\\bar{o}$ is observed when behaving optimally according to $\\bar{r}$. The algorithm for computing the anti-reward is provided in Algorithm 2. Let C be the function that maps from r to o. We iteratively do the following steps by starting from a randomly initialised $\\bar{o}$: (1) Set $\\bar{r}$ as the anti-reward function that maximises the distance between o* and $\\bar{o}$ (2) Compute the new $\\bar{o}$ from the new value of r. We repeat this process for a set number of iterations. An intuition behind why this approach works is highlighted in Appendix D.3.\n\nWe will now outline the various forms of o (occupancy measures and trajectory distributions) and the corresponding distance measures, denoted as D, applied in each case.\n\nOccupancy Measures Since IRL algorithms try to match occupancy measures, one could try to directly maximize the distance between the occupancy measures of the optimal policy of r, i.e, $\\rho^*$ and $\\bar{\\rho}$. Hence, in this case, o = p. We use f-divergences and Integral Probability Metrics (IPMs) to measure the distance between $\\rho^*$ and $\\bar{\\rho}$.\n\nThe f-divergence between $\\rho^*$ and $\\bar{\\rho}$ is defined using the convex conjugate $f^*$ as,\n\n$D_f(\\rho^*||\\bar{\\rho}) = sup_g E_{\\rho^*} [g(s, a)] - E_{\\bar{\\rho}} [f^*(g(s, a))]$\n\nsetting $g = -\\bar{r}$ and $\\phi(u) = -f^* (-u)$, so that $E_{\\bar{p}}$ is maximized,\n\n$D_f(\\rho^*||\\bar{\\rho}) = sup_{\\bar{r} :D \\rightarrow R} E_{\\bar{\\rho}} [\\phi(\\bar{r}(s,a))]] - E_{\\rho^*} [\\bar{r}(s, a)]$\n\n\\qquad (11)\n\nIPMs that are parameterized by a family of functions F are defined,\n\n$F(\\bar{\\rho},\\rho^*) = sup_{f \\in F} |E_{\\bar{p}}[f(s,a)] - E_{\\rho^*} [f(s, a)]|$         (12)\n\nWe can see that in both IPMs (Equation 12) and f-divergences (Equation 11), the anti-reward function gives a high reward to the state-action pairs visited by $\\bar{\\pi}$ (due to the sup and expectation over $\\bar{\\rho}$ being the first term) and a low reward to the ones visited by $\\pi^*$. In both these cases, the anti-reward function can be represented using a function approximator and solved using gradient ascent, or in the case of discrete environments, Equation 11 can be solved using the closed form solution (Table 2 in Appendix D.3)."}, {"title": "6 Experiments and Results", "content": "In this section, we intend to answer the following key questions: (1) Does the MEIR algorithm as described in Section 4 suffer a significant privacy leak in the case of limited demonstrations, and how does the MM algorithm perform in comparison? (2) How does the MM algorithm fare in comparison to the MEIR and DQFN algorithms in preserving the privacy of the reward function when the observer has access to the true occupancy measures of the agent? (3) How does MM perform against observers that know they are being deceived?"}, {"title": "6.1 Environments and Evaluations Metrics", "content": "We conduct our experiments in the following environments: Cyber Security which is based on Moving Target Defence [30], Frozen Lake [33], Four Rooms [7] and randomly generated MDPs. In the Cyber"}, {"title": "A Proofs", "content": "A.1 Proof of Lemma 1\n\nLemma 1. Any policy that is the solution of a Max Intentional\nEntropy randomization problem MEIR(r, Emin) with a reward con-\nstraint Emin \u2208 [\u00ca, E*], can be expressed as the solution of the\nMaximum Entropy RL problem as,\n\n$\\pi = R_L(\\lambda^*r)$                                             (14)\n\nfor some \u03bb\u2217 \u2265 0.\n\nBefore we begin the proof, we first introduce some of the properties\nof occupancy measures. Any valid occupancy measure p must satisfy\na set of affine constraints known as the Bellman Flow Constraints\n:\n\n$\\rho(s, a) \\geq 0, \\forall(s, a) \\in S \\times A$, and\n\n$\\Sigma\\rho(s,a) = \\mu_s + \u03a5\\Sigma \\Sigma P(s'|s, a)\\rho(s', a), \\forall s \\in S$\n\nIf \u03a0 is the set of all stationary policies, then let D = {\u03c1\u03c0 : \u03c0\u2208\u03a0}\nbe the set of all valid occupancy measures. There exists a bijection\nbetween and D and \u03a0 [32]. Hence, \u03c0 can be uniquely recovered from\np as\n$\\pi(a|s) = \\frac{\\rho(s, a)}{\\Sigma_{a'} \\rho(s, a')}$\n\nThe cumulative y-discounted causal entropy H(\u03c0) can also be defined\nusing occupancy measures [17]:\n\n$H(\\rho) = \\Sigma\\Sigma \\rho(s, a) log (\\frac{\\rho(s, a)}{\\Sigma_{a'} \\rho(s, a')})$\n\nProof. The MEIR algorithm can be written in terms of occupancy\nmeasures as,\nMEIR(r, Emin) = argmax H(\u03c1) (15)\n\n                                        \u03c1\u2208D\nsubject to \u03c1(s, a)r(s, a) \u2265 Emin\n\nThe resulting Lagrangian can be expressed as,\nL(\u03c1, \u03bb) = H(\u03c1) + \u03bb \u03a3\u03c1(s, a)r(s, a) \u2013 Emin\n\n                                (16)\nThe feasible set D' = {p : p \u2208 D and pr > Emin } is a convex\nset (as it is defined by a set of affine constraints) and H is strictly\nconcave [17], hence strong duality holds, and the primal optimal\n\np\u2217 = argmax min L(\u03c1, \u03bb)\n\n                                      \u03c1\u2208D\ncan be uniquely recovered from the dual optimum\n\n\u03bb\u2217 = argmin max L(\u03c1, \u03bb), as\n\np\u2217 = argmax L(\u03c1, \u03bb\u2217) = RL(\u03bb\u2217r)\n\n(Section 5.5.5 in [3]). Hence, the randomized policy can be obtained\nas,\n\u03c0 = RL(\u03bb\u2217r) (17)"}, {"title": "A.2 Proof of Lemma 2", "content": "Lemma 2 is a corollary of Theorem 3.4 in [31]. Theorem 3.4 in [31]\ndefines all the functions g such that if g(r1) = RL (r2) for two reward\nfunctions r1, r2, then they share the same ordering over policies. We\nuse this Theorem to show that RL(\u03bb\u00b7 r\u2081) where \u03bb\u2208 R+ belongs to\nthis class of functions, i.e, if RL(\u03bb\u00b7r\u2081) = RL(r2), then r\u2081 has the\nsame ordering of policies as r2.\n\nPrior to presenting this theorem and proving Lemma 2, we intro-\nduce the necessary notation and definitions.\n\n1. Let R be the set of all possible reward functions r: S \u00d7 A \u2192 R.\nA reward object f: R \u2192 X is a mapping from R to an arbitrary\nset X. In this section we are only concerned with reward objects\nof the form R \u2192 \u03a0, i.e, the MERL, MEIR and MM algorithms.\n2. Given a partition P of R, we say that a reward object, f is P-\nadmissible if f (r1) = f(r2) \u21d2 r1 \u2261P r2\n3. Given a partition P of R, f is said to be P-robust to misspecifi-\ncation with g, if f is P-admissible, f \u0338= g and f(r1) = g(r2) \u21d2\nr1 \u2261P r2.\n\nLet \u03c8 : r \u2192 R+ represent a function for weighing the rewards.\nLet c\u03c8(r) RL(\u03c8(r) \u00b7 r) denote objective 3 with a weight of \u03c8(r)\ngiven to the reward term and C = {c\u03c8 : \u03c8 \u2208 r \u2192 R} be the set of\nall such functions.\nProposition 1. (Theorem 3.4 in [31]) Consider cy \u2208 C, then, cy is\nORDM -robust to misspecification with g if and only if g \u2208 C and\ng\u0338= cy.\nWe would kindly direct the reader to [31] (Section A.3) for the\nproof of Proposition 1.\nLemma 2. (Corollary of Proposition 1) For any two scalars, l1, l2 \u2208\nR+ and two reward functions r1,r2, if RL(\u03bb1r1) = RL(\u03bb2r2), then\nr1 \u2261ORDM r2.\nProof. RL(\u03bb1r1) and RL(\u03bb2r2) can be written as cy1 and cy2 re-\nspectively where \u03c81(r) = \u03bb1 and \u03c82(r) = \u03bb2 \u2200r \u2208 R. We break\nthe proof down into two cases:\nCase 1: \u03c81 \u0338= \u03c82\nProposition 1 states cy1 is ORDM-robust to misspecification with cy2,\nhence, from the definition P-robustness we get cy1(r1) = cy2(r2) \u21d2\nr1 \u2261ORDM r2.\nCase 2: \u03c81 = \u03c82\nFor cy to be ORDM-robust to misspecification, it must be ORDM-\nadmissible (Definition 3). From the definition of P-admissibility, we\nget cy1 (r1) = cy1 (r2) \u21d2 r1 \u2261ORDM r2."}, {"title": "B Linear Program Formulation of the Max Misinformation Algorithm", "content": "The Max Misinformation algorithm can be written as a linear program\nin terms of its occupancy measures,\n\nMM(R, Emin) = argmax  \u03a3\u03a3\u03c1(s, a)r(s, a)              (18)\nSubject to,\n\n\u03c1(s, a) \u2265 0 \u2200s \u2208 S, a \u2208 A\n\n\u03a3\u03c1(s, a) = \u00b5s +\u03a5 P(s |s, a)\u03c1(s , a) \u2200s \u2208 S, a \u2208 A\n\n\u03a3\u03b5\u03c1(s, a)r(s, a) \u2265 Emin"}, {"title": "C Solving the MM formulation", "content": "C.1 Binary Search solution for MM\n\nLet E be the expected reward obtained by a policy maximizing the\nobjective in Line 4 of Algorithm 1 for an arbitrary \u03bb \u2265 0. It should\nbe noted that as \u03bb increases (decreases), E increases (decreases). Due\nto this property, we can search for the optimal \u03bb\u2217 via binary search,\nwhich drastically improves runtime. This is presented in Algorithm 3.\n\nC.2 Solving the MEIR formulation\n\nThe original formulation of the MEIR algorithm, as presented in\n[25], relies on a model-based approach limited to discrete environ-\nments. To transform the MEIR algorithm into a model-free version,\none can employ primal-dual descent or binary search techniques by\nreplacing the contents of Lines 4 and 5 in Algorithm 1 and 3 with\n\u03c0 = Ex[dir(s, a) \u2013 log(\u03c0(a|s))]. The Soft-Q learning algorithm\ndescribed in [38] can be used to solve this expectation. In situations\ninvolving continuous state spaces, extensions like Soft Actor-Critic\noffer practical solutions. We use this method to obtain policies\ngenerated by MEIR in the experiments."}, {"title": "D Generating anti-reward functions", "content": "D.1 Closed form solutions for f-divergence between\ntwo distributions.\n\nDifferent f-divergences and their closed-form solutions (for equation\n11) are present in Table 2. This is adapted from the closed-form\nsolutions of f-divergences presented in [11]."}, {"title": "D.2 More on the use of IPMs", "content": "In our experiments, we only use one type of IPM namely the\nWasserstein-1 distance, which is obtained by constraining the family\nof functions F to be 1-Lipschitz continuous."}, {"title": "D.3 Idea behind Algorithm 2", "content": "The concept behind algorithm 2 becomes clearer through an example.\nSuppose we take the distribution o to represent the occupancy measure.\nInitially, we can establish p as the occupancy measure corresponding\nto a purely random policy . Consequently, the anti-reward function\nderived in Line 4 of Algorithm 2 would attribute a low reward to\nstate-action pairs visited by \u03c0\u2217, while granting higher rewards to\nother state-action pairs visited by \u03c0. To exemplify this, if we employ\nKL divergence to quantify the divergence between p and p\u2217, the\nanti-reward would adopt the structure of"}, {"title": "E.1 Mixed Policy method", "content": "It is important to note that there can be multiple solutions when\ngenerating an anti-reward using Algorithm 2. This is because, there\nare multiple states/trajectories that are far away from the optimal\nstates/trajectories, and each solution is an anti-reward that prefers just\none of the regions, as can be seen in Figure 7. To overcome this, we\ncan generate multiple anti-rewards ri (by setting a different seed\nwhen initializing the neural network) and for each of them generate\na deceptive policy with the same Emin value. The agent can then\nuse a mixed policy \u03c0mix = {{\u03c0i}, w} where w can be a uniform\ndistribution or manually chosen by the user based on their preference\nof each policy. We call this method MMmix. Note that \u03c0mix satisfies\na reward threshold of Emin as each of its constituent policies \u03c0i\nsatisfy the reward constraint.\nProposition 2. If a set of policies {\u03c0i} satisfy a constraint on the\nexpected reward, i.e, Er[r] > Emin \u2200i, then the mixed policy,\n\u03c0mix = {{\u03c0i}, w} satisfies the constraint on expected reward, i.e,\nExmix [r]\u2265 Emin.\nProof. Let xi be the occupancy measure of \u03c0i, then, the occupancy\nmeasure of the mixed policy Xmix(s, a) = \u03a3i Wixi(s, a)\nxi(s,a)r(s,a) \u2265 Emin \u2200i\n\u2211 wi\u03a3xi(s,a)r(s, a) \u2265 \u2211 wi emin\ns a  i\ns a    \n\u2211(\u2211wi xi(s, a))r(s, a) \u2265 (\u2211 wi) emins a s a  i\nxmix (s, a)r(s, a) \u2265 Emin"}, {"title": "E.2 Boltzmann Randomization", "content": "We can make the policy more stochastic by sampling actions ac-\ncording to the Boltzmann distribution [28] during inference. The\nBoltzmann distribution is defined as,\n\nBE(r, \u03b2) = BE(a|s) = e Q (s,a) \u03a3a e 2 (s,a)\nwhere Q\u2217 is the optimal Q-function and \u03b2 is a temperature parameter\nused to control the stochasticity of the policy distribution. Based on\nthis, we introduce the Max Misinformation with Boltzmann Explo-\nration algorithm as,\n\nMMBE(r, Emin, \u03b2) = BE(r, \u03b2) \u25ca MM(r, Emin) (20)\n\nwhere, r = A\u2217r + r. That is, we first run the MM algorithm and\nthen employ BE on top of the Q function (obtained using MM algo-\nrithm) to obtain a randomized policy. Most value methods, learn a\nQ-function when solving the objective in Lines 4 in 5 in Algorithm 1.\nIn case a Q-function is not available, it can be estimated using Monte\nCarlo rollouts. However, randomizing the policy in this way does not\nguarantee that the reward constraint is respected leading to a tradeoff\nbetween increasing the stochasticity of the policy and obeying the\nreward constraint."}, {"title": "6 Related Work", "content": "There exist two primary approaches towards reward function privacy:\nDifferential Privacy (DP) and Deception."}, {"title": "G.1 Differential Privacy based methods", "content": "Differential Privacy is a mathematically rigorous framework that\nensures that the output of a mechanism when applied to a dataset\nis robust to changes in individual data points of the dataset. This\nproperty makes it difficult for observers to reconstruct individual\ndata points from the output, thereby preserving privacy. The appli-\ncation of differential privacy in RL has been studied in prior works\n[26, 34, 37, 35]. The approach involves introducing Gaussian noise\nto Bellman updates [26, 35], gradients of TD-errors and surrogate\nadvantages [26] to ensure differential privacy. By doing so, the Value\nfunctions, and post processed functions such as policies/occupancy\nmeasures become resistant to observers attempting to recover the\nexact reward function by querying these functions. In the context of\nreward function privacy, DP based security guarantees are ill-suited\nas: (a) there are infinitely many reward functions that can represent the\nuser\u2019s preferences (e.g., shaped rewards), so preventing the observer\nfrom reconstructing the exact reward function does not fully address\nthe issue and (b) the lo and lp norms are not good metrics to use when\ncomparing reward functions as two reward functions in the same\nlo neighbourhood may possess several other properties that pose a\nprivacy leak such as ordering of polices, hence more nuanced metrics\nsuch as those presented in Section 3 are required. Apart from this,\nit has been demonstrated in [26] that policies learnt using DP-based"}, {"title": "G.2 Deception based methods", "content": "Deception can be categorized as the act of creating or maintaining\nfalse beliefs in the minds of others . Military strategists Bell and\nWhaley argue that the distortion of perceived reality can\nbe achieved through two methods: simulation (showing the false) and\ndissimulation (hiding the truth). Deceptive Reinforcement Learning\nintroduced in is a paradigm within RL that uses the concept of\ndeception to learn policies that preserve the privacy of the reward func-\ntion. Previous research has explored model-based deception-based\npath planning , whereas the algorithm presented in this"}]}