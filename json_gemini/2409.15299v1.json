{"title": "Irrelevant Alternatives Bias Large Language Model Hiring Decisions", "authors": ["Kremena Valkanova", "Pencho Yordanov"], "abstract": "We investigate whether LLMs display a well-known human cognitive bias, the attraction effect, in hiring decisions. The attraction effect occurs when the presence of an inferior candidate makes a superior candidate more appealing, increasing the likelihood of the superior candidate being chosen over a non-dominated competitor. Our study finds consistent and significant evidence of the attraction effect in GPT-3.5 and GPT-4 when they assume the role of a recruiter. Irrelevant attributes of the decoy, such as its gender, further amplify the observed bias. GPT-4 exhibits greater bias variation than GPT-3.5. Our findings remain robust even when warnings against the decoy effect are included and the recruiter role definition is varied.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are increasingly getting adopted in a wide range of industries to assist in decision-making for complex problems. Entrusting decision processes to LLMs requires a comprehensive understanding of potential biases inherent in these models and implementing rigorous measures to minimize them. This is especially important for high-risk applications in industries such as Human Resources (Act, 2021), where upholding essential human rights and ensuring fairness and accuracy in decision-making processes are crucial.\nThe complexity of decision-making problems often arises from the need to evaluate numerous alternatives simultaneously. Human judgements are known to be prone to various biases stemming from the composition of the choice set, known as context effects. One such well-documented and extensively studied cognitive bias is the attraction effect, also known as the asymmetric dominance effect (Huber et al., 1982). An alternative is asymmetrically dominated (ASD-ed) when it is inferior to one alternative (the target) in all attributes but only partially inferior to another alternative (the competitor). The attraction effect occurs when an ASD-ed decoy alternative increases the likelihood of choosing the target, over a non-dominated competitor.\nThe bias has been documented even if the decoy is not available for choice, a phenomenon known as the phantom decoy effect (Highhouse, 1996; David, 1999; Pettibone and Wedell, 2000). Adding a phantom decoy that is superior to the target and asymmetrically dominating (ASD-ing) leads biased decision-makers to select the target more often than the non-dominated competitor. The possible positions of the ASD-ed decoy and ASD-ing phantom decoy alternatives are illustrated in Figure 1 for two-dimensional alternatives."}, {"title": "2 Related Literature", "content": "This work contributes to three main strands of literature - cognitive biases of LLMs, decision-makers exhibiting the attraction effect, and biases in hiring decisions."}, {"title": "Cognitive biases of Large Language Models", "content": "The emerging literature on decision-making by LLMs has elucidated that these are also prone to various human cognitive biases (Hagendorff et al., 2023; Macmillan-Scott and Musolesi, 2024; Lin and Ng, 2023; Talboy and Fuller, 2023; Binz and Schulz, 2023; Dasgupta et al., 2023). To the best of our knowledge, Itzhak et al. (2023) is the most related paper to ours, since they find evidence for the attraction effect in LLMs, particularly in a product selection context on the basis of price and quality attributes. Their focus lies in testing the effect of alignment with human preferences on cognitive biases. In contrast, we study the attraction effect in AI-recruitment."}, {"title": "Decision-makers displaying the attraction effect", "content": "The attraction effect was first documented in consumer research (Huber et al., 1982), but has since then been observed in a variety of contexts including, but not limited to policy choices (Herne, 1997), risky choice (Mohr et al., 2017), and intertemporal choice (Marini and Paglieri, 2019) 4. This effect does not seem to be limited to human adults, but has been documented with other species such as primates (Parrish et al., 2015; Marini et al., 2024), frogs (Lea and Ryan, 2015), and amoeboid organisms (Latty and Beekman, 2010). The literature also includes some failures to replicate the attraction effect, as noted by Frederick et al. (2014) and Yang and Lynn (2014). However, the bias is consistently reproducible when the primary experimental design parameters are maintained (Huber et al., 2014). We contribute to this literature by showing that LLMs exhibit this bias."}, {"title": "Decoy effect in hiring decisions", "content": "Highhouse (1996) provides the first evidence of the attraction effect in hiring decisions, where participants"}, {"title": "3 Experimental Design", "content": "Overview and bias definition We adopt the general methodology of standard experiments on the attraction effect such as Huber et al. (1982), whereby for each job opening, we establish two conditions: a control condition, where a hiring decision is made between a target and a competitor candidate, and a treatment condition, where a decoy candidate is introduced to the choice set. Each candidate is defined by two relevant qualification values locating them in the two-dimensional alternatives space. More specifically, there is a trade-off between the target and the competitor in the sense that each of them is superior to the other with respect to one attribute as shown in Figure 1.\nWe then prompt an LLM to assume the role of a recruiter and task it with selecting the most qualified candidate.\nThe bias resulting from the attraction effect is quantified as the difference in the probability for an LLM to choose the target in both conditions.5"}, {"title": "4 Results", "content": "We test the attraction effect for a fixed asymmetrically dominated decoy location in the attribute space (see Table 2) across candidate selection tasks for six diverse occupations (see Table 1). The results are presented in Figure 4. Additionally to the aggregated results, target probabilities for each candidate order permutation can be found in Figure 10, illustrating a strong candidate order/identifier bias. First, we note that in the control condition, few target probabilities are not close to 0.5, despite our prompt design goal of establishing the equal importance of qualification attributes. Potential reasons for this outcome may be inadequate model accuracy, insufficient prompt engineering, or an unaddressed bias. For instance, we are not controlling for a possible bias in the order of qualification listing for each candidate.\nNext, we observe that the decoy effect is consistently present and that its magnitude is larger on average for GPT-4 compared to GPT-3.5. The difference between the target probability in the control and treatment conditions is positive and significant for all occupations (no significance test is applied for GPT-3.5 as choice probabilities are extracted directly from the model; for GPT-4, a \\(x^2\\) test yields p<.01). The only exception is GPT-4's choices for 'Mechanical engineer' (\\(x^2\\) test, p > .01), indicating no bias."}, {"title": "4.2 Exploration of the decoy space", "content": "Previous studies on the attraction effect in humans have highlighted the crucial role of decoy positioning within the attribute space. Specifically, suboptimal decoy locations may suggest that the attraction effect is negligible or even reversed (Kaptein et al., 2016; Dumbalska et al., 2020). To this end, we exhaustively explore the bi-dimensional job qualification attribute space, also extending our analysis beyond asymmetrically dominated decoy regions. We observe that, like human decision-makers, LLMs exhibit stronger bias depending on decoy location (see Figure 5). For example, we find no evidence for the attraction effect in our initial experiment for 'Mechanical engineer' with GPT-4 as presented"}, {"title": "4.3 Gender decoys", "content": "We assign gender to target, competitor, and decoy candidates using possessive pronouns (his/her) in their expositions to examine the influence of gender on the attraction effect. The results are presented in Figure 6. A two-sided paired t-test comparing the mean bias across occupations of female vs. male decoy conditions revealed a significant difference for GPT-3.5 (female target: t(5) = 4.89, p < .01; male target: t(5) = \u22124.69, p < .01). No significant difference was observed for GPT-4 (female target: t(5) = \u2212.17, p > .01; male target: t(5) = -.46, p > .01). However, the aggregate attraction effect might be offset by the existing job sub-groups which respond differently to varying the gender of the decoy candidate."}, {"title": "4.4 Robustness", "content": "LLM responses can be sensitive to even modest prompt variations (Loya et al., 2023; Sclar et al., 2023). Therefore, it is important to investigate if decision-making behaviour remains robust across different prompt phrasings and compositions. We alter prompt components that can directly impact bias. Specifically, we vary the recruiter role instruction and incorporate a warning against the (phantom) attraction effect. We keep all other parameters as in the baseline experiment."}, {"title": "Warning against the attraction effect", "content": "We devise a cautionary sub-prompt against succumbing to the attraction and phantom decoy effects and incorporate it just after the recruiter role definition. The sub-prompt includes a thorough explanation of the phenomenon, an illustrative example showing biased decision-making between candidates, and a set of recommendations aimed at avoiding such biases (see Figure 9).\nFigure 7 shows that including a warning about the attraction effect does not mitigate the bias. A two-sided paired t-test comparing the mean bias across occupations of the 'warning absent' versus 'warning present' conditions did not reveal a significant difference (GPT-3.5: t(5) = 1.42, p > .01, GPT-4: t(5) = .69, p > .01). Despite this, for GPT-4 we observe two distinct sub-groups \u2013 occupations for which the warning is effective in reducing and even reversing the attraction effect (see House cleaner and Social psychologist), and occupations for which the bias is slightly but consistently increased. Additionally, we again observe a larger variance of the bias for GPT-4 compared to GPT-3.5. Ultimately, the warning does not resolve the attraction effect, indicating the need for alternative approaches to mitigate bias."}, {"title": "Varying the recruiter role definition", "content": "We formulate four recruiter role definitions with varying lengths, levels of flattery, and instructions regarding unbiased decision-making (see Table 3). We examine the effect of these role instructions on bias in Figure 8. A one-way repeated measures ANOVA conducted across the six occupations indicates that the type of recruiter instruction sub-prompt used did not result in statistically significant differences in bias (GPT-3.5: F(3, 15) = .39, p > .01, GPT-4: F(3, 15) = 1.40, p > .01). Consistently with all previous experiments, GPT-4 presented much larger bias variation than GPT-3.5.\nOur results do not provide evidence that enriching recruiter role definitions reliably mitigates bias."}, {"title": "5 Conclusion", "content": "We find evidence that hiring decisions made by LLMs such as GPT-3.5 and GPT-4 are influenced by asymmetrically dominated alternatives, similarly to human recruiters. We explore the placement of a decoy in the complete two-dimensional attribute space and find consistent patterns aligned with the classical attraction effect. We also study the effect of decoy gender and observe that it is most effective when aligned with the target. In general, GPT-4 presented much larger bias variation than GPT-3.5. We show that our results are robust to including a warning against the decoy and varying the recruiter role definition."}, {"title": "6 Limitations", "content": "Our investigation is based on a minimal experimental setup featuring a stylized candidate selection task \u2013 two or three candidates compete for a job described by two required qualifications, whose values could be numerical or ordinal. This approach allows to: i) immediately compare results with existing literature, ii) more clearly isolate the attraction effect, and iii) mimic the final stages of candidate selection process when only a limited number of candidates remain. However, the underlying settings might affect the generalisability of our studies to real-world candidate selection or ranking tasks that involve job descriptions and candidate resumes. Such documents provide a much more complex picture of candidates and jobs, and contain multiple (not always easily comparable) qualifications and other relevant information.\nWe perform experiments on six carefully selected occupations. This small sample is not sufficient to rule out the existence of professions not affected by the attraction effect nor identify subgroups of professions exhibiting similarly biased behaviour.\nWe use two OpenAI models demonstrating distinct biased behaviours. The extent to which other LLMs respond to decoys in their decision-making can also vary greatly, particularly depending on the degree of their instruction tuning and human preference alignment as shown by Itzhak et al. (2023). Additionally, LLMs display limited reasoning abilities (Lee et al., 2024), which can be enhanced by more complex prompt engineering, such as Chain-of-Thought (Wei et al., 2022); however, this work does not explore such techniques.\nCandidate gender in the investigation of gender decoys is conveyed through possessive pronouns. It is unknown how other direct or indirect gender signals, such as personal names of explicit gender, influence the attraction effect.\nFinally, we tested robustness of the attraction effect by varying recruiter instructions and warning about the decoy effect. It remains uncertain how other types of variations might affect the results, including those unrelated to the candidate selection task, such as prompt formatting."}, {"title": "Ethics statement", "content": "This work involves LLM decision-making in the high-risk human resources context. If LLMs are used as tools for screening candidates, ethical concerns may arise due to biases, some of which are not well-understood and mitigated, as well as the models' limitations in reasoning.\nAdditionally, our work reveals, albeit through a set of stylised experiments, incentives for candidates to submit two CVs when applying for jobs. These results have the potential to lower the quality of candidate CV pools and increase the difficulty of screening processes.\nLastly, we use ChatGPT to refine our writing on a sentence level, without suggesting new content."}, {"title": "A Appendix", "content": "We present the ingredients used for the assembly of candidate selection prompts.\nThe entries from Table 1, which lists jobs and their corresponding required qualifications, are used to define candidate selection tasks."}]}