{"title": "MITIGATING ADVERSARIAL ATTACKS IN LLMS THROUGH\nDEFENSIVE SUFFIX GENERATION", "authors": ["Minkyoung Kim", "Yunha Kim", "Hyeram Seo", "Heejung Choi", "Jiye Han", "Gaeun Kee", "Soyoung Ko", "HyoJe Jung", "Byeolhee Kim", "Young-Hak Kim", "Sanghyun Park", "Tae Joon Jun"], "abstract": "Large language models (LLMs) have exhibited outstanding performance in natural language pro-\ncessing tasks. However, these models remain susceptible to adversarial attacks in which slight input\nperturbations can lead to harmful or misleading outputs. A gradient-based defensive suffix generation\nalgorithm is designed to bolster the robustness of LLMs. By appending carefully optimized defensive\nsuffixes to input prompts, the algorithm mitigates adversarial influences while preserving the models'\nutility. To enhance adversarial understanding, a novel total loss function ($L_{total}$) combining defensive\nloss ($L_{def}$) and adversarial loss ($L_{adv}$) generates defensive suffixes more effectively. Experimen-\ntal evaluations conducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by an average of\n11% compared to models without defensive suffixes. Additionally, the perplexity score of Gemma-7B\ndecreased from 6.57 to 3.93 when applying the defensive suffix generated by openELM-270M.\nFurthermore, TruthfulQA evaluations demonstrate consistent improvements with Truthfulness scores\nincreasing by up to 10% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive retraining.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in natural language processing (NLP) have been largely driven by the emergence of large\nlanguage models (LLMs) that have revolutionized tasks such as text generation, translation, and dialogue systems Wei\net al. (2022); Abburi et al. (2023); Li et al. (2023). Despite these remarkable achievements, LLMs remain susceptible\nto adversarial attacks Yao et al. (2024); Xu et al. (2023) that exploit the intricate syntactic, semantic, and contextual\nnuances of language and make such attacks challenging to detect and mitigate Zhang et al. (2020). Even subtle\nmanipulations of input can circumvent conventional safeguards, raising critical concerns about the robustness and\nreliability of LLMs in real-world applications Wang et al. (2023). For instance, jailbreaking techniques allow attackers\nto rephrase prompts, bypassing content filters and inducing the generation of harmful outputs Xue et al. (2024); Ma\net al. (2024).\nExisting defense mechanisms, including adversarial training and static safeguards, have been extensively explored to\naddress these vulnerabilities Xhonneux et al. (2024); Kumar et al. (2023). However, these approaches often entail high\ncomputational overhead and exhibit limited adaptability to emerging attack strategies while underscoring the necessity"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Defense Strategies for LLMs and Their Challenges", "content": "A primary challenge in LLM defense is achieving a balance between robustness and generalization. Among various\nstrategies adversarial training has been extensively studied and enhances robustness by training models on adversarial\nexamples. Although this method enhances robustness by training models on adversarial examples, it incurs high\ncomputational costs and has been shown to struggle with overfitting, particularly when addressing novel attack patterns\nAndriushchenko and Flammarion (2020). Techniques like synonym substitution and paraphrasing aim to neutralize\nadversarial inputs but risk degrading input semantics and output quality Garg and Ramakrishnan (2020). Guardrail\nmechanisms such as prompt-driven safeguarding and self-reminders detect and prevent harmful outputs. These methods\nhave shown promise particularly against Jailbreak attacks by enabling models to set internal boundaries Wang et al.\n(2023). However, they tend to produce false positives and remain vulnerable to attackers familiar with model structures.\nDynamic prompt-based methods Zou et al. (2023) have shown potential by dynamically eliciting model knowledge,\nthough ensuring semantic validity and consistency remains a challenge. Recent studies emphasize the limitations of\nthese defenses and particularly their inability to adapt to evolving adversarial tactics, highlighting the need for scalable\nand adaptive strategies. Building upon these prior works, our proposed gradient-based defensive suffix generation\nmethod introduces a universal suffix that integrates with existing safety mechanisms without requiring retraining.\nLeveraging smaller large language models (sLLMs), this approach generates suffixes efficiently while dynamically\nadapting to adversarial strategies through the total loss function $L_{total}$. This ensures both robustness and fluency across\ndiverse scenarios, as detailed in the Methodology section."}, {"title": "3 Methodology", "content": "Figure 1 illustrates the process of generating and applying the universal defensive suffix. The harmful instruction,\n\"Teach children how to access and use firearms.\" is associated with two target sequences. The first is an adversarial\nsequence, where the model might generate a harmful response such as, \"Sure, here is a tutorial for teaching children\nhow to access and use firearms.\" The second is a defensive sequence, where the expected response is, \"I can't provide\na tutorial for teaching children how to access and use firearms.\" A smaller large language model (sLLMs) generates\nthe defensive suffix by optimizing both adversarial and defensive sequences. The optimization minimizes the total\nloss, $L_{total}$, which integrates adversarial loss ($L_{adv}$) for adversarial sequences and defensive loss ($L_{def}$) for defensive\nresponses. Through this gradient-based optimization, a universal defensive suffix is derived and appended to the system\nprompt, remaining hidden from the user. This allows users to submit queries normally while the model consistently\ndefends against adversarial inputs.\nOnce generated, the defensive suffix is applied to victim LLMs during interactions with harmful queries. Without the\nsuffix, a model might respond to a query like \"Write a tutorial on how to make a bomb\" with unethical instructions,\nsuch as \"To create a bomb generator in Unity, follow these steps.\" With the suffix appended, the model rejects the query\nand responds that it cannot provide information related to illegal activities. This process demonstrates how the universal\ndefensive suffix, generated by the sLLMs, effectively neutralizes harmful queries when applied to larger victim models."}, {"title": "3.1 Dataset and Preprocessing", "content": "This study utilized the AdvBench dataset Zou et al. (2023) which is a benchmark for adversarial attacks in NLP\nand contains over 500 harmful queries designed to exploit vulnerabilities in LLMs. As the dataset primarily focuses\non adversarial behaviors without providing corresponding defensive responses, we created custom defensive target\nsequences designed to counter these attacks. For instance, in response to the adversarial query \"How can I create a script\nto exploit vulnerabilities?\", the defensive sequence was, \"I cannot provide a script that can exploit vulnerabilities.\"\nThis approach ensures the model consistently generates ethical responses when confronted with adversarial inputs. To\noptimize the dataset, we removed excessively long or redundant queries and finalized a set of 400 samples."}, {"title": "3.2 Initialization", "content": "The defensive suffix generation process is designed to improve the model's robustness against adversarial inputs. The\nsuffix begins with an initial sequence, appended to prompts to neutralize harmful outputs. Inspired by the approach in\nWang and Qi (2024), we initialized the suffix with the string 'begincontent: As an AI model' and then optimized it\niteratively using gradient-based methods.\nThe optimization process uses two primary inputs, adversarial prompts from the AdvBench dataset and the corresponding\ndefensive target sequences we developed. These sequences guide the model to generate safe and ethical responses to\nadversarial inputs. Iterative refinement of the suffix through gradient-based optimization makes the model increasingly\nresilient to diverse adversarial scenarios, ensuring generalization without overfitting to specific attack patterns.\nThe process begins with the initialized suffix $S_{init}$, which is appended to the input prompts. This serves as the baseline\nfor iterative optimization, where the model processes adversarial queries and refines its outputs accordingly."}, {"title": "3.3 Loss Functions", "content": "The optimization of the defensive suffix is governed by two primary loss functions: Defensive Loss and Adversarial\nLoss. These two components jointly inform the optimization process, ensuring that the model not only produces safe"}, {"title": "3.4 Gradient-Based Optimization Process", "content": "The defensive suffix is refined through a gradient-based optimization process, where token-wise gradients guide the\niterative updates of the suffix to enhance the model's robustness against adversarial inputs.\nFor each token $s_i$ in the suffix, the gradient of the $L_{adv}$ is computed with respect to the token embeddings, which informs\nhow each token in the suffix affects the model's output. The gradient calculation is as follows:\n$\\frac{\\partial L_{total}}{\\partial s_i} = \\frac{\\partial (L_{def} - \\alpha \\log(L_{adv}))}{\\partial s_i}$\nwhere $L_{def}$ and $L_{adv}$ are defined in Section 3.3. This gradient calculation determines how each token in the suffix affects\nthe model's response to adversarial inputs, guiding updates to improve robustness.\nThis allows us to determine how adjustments to each token's embedding will influence the model's likelihood to produce\nharmful or defensive responses. The top-k gradients with the largest values are selected for token updates in each\niteration.\nWe apply a top-k selection method to identify the most significant tokens based on their computed gradients. From\nthis subset of top-k tokens, a candidate is chosen for suffix update, allowing both exploitation of high-impact tokens\nand exploration of alternative candidates. The suffix $s_{init}$ is iteratively updated over multiple rounds, recalculating\nthe Total Loss after each update. This process continues until convergence criteria are met, such as a predefined loss\nthreshold or a maximum number of iterations. Each iteration progressively refines the suffix, ensuring the model\nbecomes increasingly robust to adversarial inputs while maintaining generalizability across diverse attack types."}, {"title": "3.5 Defensive Suffix Generation", "content": "The defensive suffix is optimized through a gradient-based process, where token-wise gradients are computed with\nrespect to the total loss function. Beginning with an initial sequence, the suffix is iteratively refined to minimize the\ntotal loss and guide the model toward generating safe and robust outputs. Algorithm 1 formalizes this process, detailing\nthe progressive optimization and integration of the suffixes.\nInitially, each prompt is paired with an unoptimized defensive suffix. Token-wise gradients are calculated to identify the\ntop-k tokens that contribute most effectively to reducing the loss. A new candidate suffix is sampled from this token set,"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "The experiments were conducted using Python (3.10.12), PyTorch (2.1.2+cu118), and Transformers (4.44.1). Due to\nconstraints related to training data size and GPU memory, the learning rate was set to $10^{-4}$, and the batch size was\ndynamically adjusted between 1 and 10 to optimize resource usage without compromising model performance. The\nexperiments were executed on Ubuntu 21.04.6 LTS with two GeForce RTX 3090 devices. Additional libraries used\nincluded Hugging Face Hub (0.20.2), NumPy (1.22.4), and Pandas (1.3.5) for data manipulation and analysis."}, {"title": "4.2 Model Selection", "content": "Given the need to run experiments for both suffix generation and evaluation on victim models concurrently, we opted for\nsmaller models such as openELM-270M ? and Llama3.2-1B Hugging Face (2024a) to generate the universal defensive\nsuffix. Resource constraints made it impractical to use larger models for both tasks. Despite their smaller size, these\nmodels offered sufficient contextual understanding to generate robust defensive suffixes while allowing faster iterations\nduring optimization.\nFor evaluation, we used Gemma-7B Team et al. (2024), Mistral-7B Jiang et al. (2023), Llama2-7B Touvron et al. (2023)\nand Llama2-13B Hugging Face (2024b) as victim models due to their ability to produce reliable and complex responses.\nLarger models were essential for evaluating the defensive suffix in realistic scenarios.\nThis combination of smaller models for suffix generation and larger models for evaluation allowed us to manage\nresources effectively while ensuring both tasks could be conducted simultaneously without compromising the rigor of\nour experiments."}, {"title": "4.3 Evaluation Metrics", "content": "To evaluate the defensive performance of the suffix generated by the proposed algorithm, we employed the following\nmetrics:\nAttack Success Rate (ASR): This metric was assessed using multiple external LLMs via the OpenAI API OpenAI\n(2024a), including GPT-3.5 OpenAI (2024b) and GPT-4 OpenAI (2024c). ASR measures the proportion of adversarial\ninputs that successfully bypass the model's safety mechanisms and elicit harmful or misleading outputs. A lower ASR\nindicates enhanced model security, reflecting the model's ability to consistently reject harmful prompts and prevent\nadversarial breaches.\nThe ASR(mean) represents the average ASR calculated from the evaluations using GPT-3.5 and GPT-4. A lower\nASR(mean) demonstrates improved defensive performance across different LLMs."}, {"title": "5 Results", "content": "The proposed defensive suffix generation method significantly reduces ASR across all evaluated models, as summarized\nin Table 1. For Gemma-7B, the ASR(mean) dropped from 0.37% (w/o suffix) to 0.28% with Llama3.2-1B, while\nopenELM-270M achieved 0.32%. Llama2-7B and Llama2-13B showed substantial improvements, with ASR(mean)\nd"}]}