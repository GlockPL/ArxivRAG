{"title": "TOAST Framework: A Multidimensional Approach to Ethical and Sustainable AI Integration in Organizations", "authors": ["Dian Tjondronegoro"], "abstract": "Artificial Intelligence (AI) has emerged as a transformative technology with the potential to revolutionize various sectors, from healthcare to finance, education, and beyond. However, successfully implementing AI systems remains a complex challenge, requiring a comprehensive and methodologically sound framework. This paper contributes to this challenge by introducing the Trustworthy, Optimized, Adaptable, and Socio-Technologically harmonious (TOAST) framework. It draws on insights from various disciplines to align technical strategy with ethical values, societal responsibilities, and innovation aspirations. The TOAST framework is a novel approach designed to guide the implementation of AI systems, focusing on reliability, accountability, technical advancement, adaptability, and socio-technical harmony. By grounding the TOAST framework in healthcare case studies, this paper provides a robust evaluation of its practicality and theoretical soundness in addressing operational, ethical, and regulatory challenges in high-stakes environments, demonstrating how adaptable Al systems can enhance institutional efficiency, mitigate risks like bias and data privacy, and offer a replicable model for other sectors requiring ethically aligned and efficient AI integration.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) significantly transforms industries and drives innovation in the digital age. Technological advancements and increased global investments have accelerated the adoption of AI in business operations. The adoption rates of AI range between 50-60% (Maslej et al., 2023), a significant increase compared to 2017, when only 20% of enterprises had incorporated AI into their workflows (Ransbotham, 2017). Organizations leverage AI to gain competitive advantages, create new market opportunities, and improve product quality and processes. Despite the increasing use of AI in various industries, many organizations face challenges when effectively integrating AI systems. Implementing AI without thorough planning and stakeholder alignment can lead to rushed decisions, wasted resources, and subpar outcomes (Li et al., 2023).\nThe rapid growth of AI automation brings forth issues such as job displacement, the need for reskilling, and increased responsibility for data privacy and security (Borges et al., 2021;\nEnholm et al., 2022). Additionally, concerns regarding regulatory compliance, ethical\nconsiderations, data quality, organizational readiness, and insufficient training and support\ncan hinder the effective implementation of AI (Ganapathi & Duggal, 2023). Therefore, taking\na strategic and responsible approach to AI implementation is crucial to ensure it aligns with\norganizational objectives and ethical standards (Sartori & Theodorou, 2022). Recent studies\nhighlight the crucial role of senior leaders in ensuring that AI implementation aligns with the\norganization's overall objectives. Strategic alignment and leadership are emphasized in this\nprocess (Abonamah & Abdelhamid, 2024), and a new diagnostic framework has been\nproposed to tackle the challenges of integrating AI. It emphasizes the significance of building\na network of partners and establishing an ecosystem for creating value (\u00c5ngstr\u00f6m et al.,\n2023). Trust is the essential foundation to enable the value creation and foster innovation,\ncreating a positive cycle where trust leads to innovation, and vice versa, through responsible\nAl practices.\nThe guiding question for our research is: \"How can the development of adaptive AI systems\nincrementally build trust and integrate effectively into organizational structures while\nensuring adherence to ethical standards, promoting a culture of innovation, and achieving\nsocio-technical integration?\". This paper contributes to the existing knowledge by proposing\nthe TOAST framework, designed to guide organizations through the complexities of AI\nintegration, ensuring that AI initiatives are ethically aligned, socially responsible, and\nconducive to continuous innovation. The TOAST framework emphasizes that successful AI\nimplementation requires a deep understanding of AI technologies, their strategic roles, and\ntheir impact on organizational performance and societal well-being. It addresses challenges\nfaced by internal and external stakeholders, including workforce implications, decision\nsupport, ethical considerations, and broader ethical, legal, and societal impacts.\nFocusing on healthcare case studies provides a compelling context for examining the TOAST\nframework's relevance and utility. Healthcare settings represent some of the most complex\nenvironments for AI adoption due to high-stakes decision-making, rigorous regulatory\nstandards, and deeply rooted organizational cultures. AI in healthcare demands precise\nintegration to enhance clinical decision-making, optimize operations, and manage sensitive\npatient data responsibly. Case studies offer valuable insights into these challenges,\nhighlighting the unique barriers and opportunities for implementing AI systems that are both\ntechnically robust and ethically sound. Moreover, the healthcare sector's rapid digital\ntransformation, accelerated by the COVID-19 pandemic, underscores the need for adaptive,\ntransparent, and trustworthy AI systems that align with organizational goals while upholding\npublic trust. By grounding the research in healthcare, this paper draws from real-world\ncomplexities to explore how the TOAST framework can enable AI solutions that are scalable,\nsocially responsible, and aligned with the high standards required in critical care settings.\nThe novelty of this paper lies in its interdisciplinary approach to AI integration, drawing from\nmanagement, computer science, ethics, and social sciences to create a robust, adaptable\nframework. The TOAST framework's emphasis on trust-building and adaptability within the\nsocio-technical environment positions it as a versatile model that can transcend healthcare to\nsupport AI integration across diverse industries. By prioritizing ethical governance, system\noptimization, and alignment with organizational culture, the framework establishes a standard\nfor sustainable Al implementation that addresses both technical and human-centric\nrequirements. This adaptability allows the TOAST framework to meet the distinct needs of\nsectors like finance, manufacturing, and education, where Al's integration similarly demands\ntransparency, ethical compliance, and alignment with specific organizational structures. As\nsuch, TOAST not only sets a new benchmark for trust and adaptability in healthcare but\noffers a replicable model for other industries, enabling a consistent approach to AI\ndeployment that balances innovation with responsible practices.\nThis paper is structured as follows. Firstly, we comprehensively analyze the socio-technical\nchallenges associated with AI deployment. Secondly, we propose the TOAST Framework as\na multidisciplinary guide that encompasses ethical, organizational, operational, and\nreputational dimensions of AI implementation, ensuring AI initiatives are ethically aligned,\nsocially responsible, and innovative. Thirdly, we demonstrate the TOAST framework's\npractical and theoretical soundness through healthcare case studies, showing its applicability\nin real-world scenarios."}, {"title": "2. Literature Review", "content": "Integrating Artificial Intelligence (AI) within organizational frameworks presents a\nmultifaceted challenge that necessitates a deep dive into social and technical dimensions.\nThis section will begin by discussing the challenges of AI implementation as identified in the\nliterature. It will then review the existing frameworks for AI implementation and identify the\ngaps the TOAST framework aims to fill. Finally, it will review the case studies selected from\nthe literature to illustrate the challenges of AI implementation and justify the need for a\ncomprehensive framework like TOAST."}, {"title": "2.1. AI Implementation Challenges", "content": "The increasing dependence on AI systems presents significant challenges that need thorough\nconsideration, especially in decision-making roles (Loureiro et al., 2021). The 'black box'\nnature of AI systems poses a challenge to trust development, leading to skepticism and\nreluctance, particularly when the decisions have substantial impacts. Therefore, there is a\ngrowing call for explainable AI to communicate its reasoning to users (Adadi & Berrada,\n2018). Enhancing transparency and explainability is fundamental for fostering responsible AI\nusage (Duan et al., 2019; Loureiro et al., 2021). Control over Al systems is closely related to\ntrust, as it involves the ability to comprehend, predict, and influence AI behavior (Hevner &\nStorey, 2023). The challenge is to create systems that balance autonomy with control,\nensuring AI operates under human supervision and aligns with organizational and ethical\nstandards (Sartori & Theodorou, 2022).\nIntegrating Al in organizations demands a holistic approach considering the socio-technical\nintegration between AI technology, organizational culture, workforce dynamics, and strategic\nalignment (Dwivedi et al., 2021; Loureiro et al., 2021). Visionary leaders are essential for\nintegrating AI into organizational culture and processes, leveraging its strengths, and\nminimizing risks. (Cao, 2023; Chowdhury et al., 2022; Makarius et al., 2020). The transition\nto AI-enhanced workplaces must be managed responsibly, positioning AI as a collaborator\nthat enhances human potential and addresses the need for reskilling and cultural readiness\n(Chowdhury et al., 2022). The success of AI adoption depends on aligning AI initiatives with\nstrategic goals, the organization's cultural preparedness, and integrating AI into existing\nworkflows (Uren & Edwards, 2023). It is essential to evaluate Al's impact on economic\nproductivity and the trade-offs of different policy approaches for its adoption to validate the\nproductivity advantages where AI supports the human workforce (Furman & Seamans, 2019).\nAI offers significant opportunities for innovation but also requires substantial investments in\nchange management and strategic alignment (Burstr\u00f6m et al., 2021). Innovative business\nmodels incorporating AI must adhere to ethical standards and sustainable competitive\npractices (Attard-Frost et al., 2023). A responsible approach to innovation, informed by an\nunderstanding of AI deployment's ethical and societal implications, is imperative for\nsustaining public trust and the acceptance of AI technologies (Alhashmi et al., 2020; Arias-\nP\u00e9rez & V\u00e9lez-Jaramillo, 2022)."}, {"title": "2.2. Existing AI Implementation Frameworks", "content": "A key principle that underpins implementation frameworks is Responsible AI, \u201cthe practice of\ndeveloping, using, and governing AI in a human-centered way to ensure that AI is worthy of\nbeing trusted and adheres to fundamental human values\" (Vassilakopoulou, 2020). It signifies\nthe development of intelligent systems that maintain fundamental human values to ensure\nhuman flourishing and well-being in a sustainable world (Dignum, 2019). AI systems should\nalso contribute toward global sustainability challenges by ensuring effective computational\nmodels through energy-aware solutions and greener data centers and promoting AI use to\nhelp achieve sustainability goals (Chatterjee & Rao, 2020). Business models for responsible\nAl can be developed by creating new or specific business models, using ethical principles for\nnavigating the potential conflicts between commercial and societal interests, and emphasizing\nthe criticality of social responsibility (Zimmer et al., 2022). The four pillars of Responsible\nAI are organizational, operational, technical, and reputational (Eitel-Porter et al., 2021).\nThese pillars schematize the distinct yet interconnected guiding principles that guide\norganizations through the various stages of Al maturity, ensuring a balanced progression that\naligns with ethical standards, operational integrity, and stakeholder trust.\nDynamic Trust Framework emphasizes that trust in AI is not static; it evolves with\ntechnology, user experiences, and societal perceptions (Adewuyi et al., 2019). The framework\nincludes mechanisms for the dynamic and incremental assessment of trust in AI systems\n(Cabiddu et al., 2022). Dynamic trust involves regular evaluation of user experiences,\ntransparency in AI operations, and responsiveness to stakeholder concerns (Robinson, 2020).\nAI Accountability Framework is pivotal in the governance of Al systems, supported through\nkey performance indicators and internal auditing that measure both ethical and operational\naspects of AI (T\u00f3th et al., 2022). These metrics provide a quantifiable means to assess\ncompliance with ethical standards, effectiveness, efficiency, and overall (end-to-end) impact\nof AI systems (Raji et al., 2020). The TOE Framework assesses technological, organizational,\nand environmental factors in AI adoption at different stages. It focuses on integrating\nadvanced and ethical AI technologies, establishing clear roles and governance structures,\nmaintaining regulatory compliance, leveraging external resources effectively, and upholding\nthe organization's reputation and trust in its AI technologies (Neumann et al., 2022).\nThe Ethical AI Framework prioritizes continuous ethical investigations to assess moral\nimplications across all aspects of Responsible AI, including AI decision-making, algorithm\nfairness, and user privacy and rights. (Prem, 2023). It ensures that AI systems align with\nsocietal values and ethical norms by embedding them within the organizational culture,\noperational processes, technical development, and reputation management (Floridi et al.,\n2021). AI Risk Framework focuses on identifying, analyzing, and mitigating potential risks\nassociated with AI systems. It involves a proactive approach toward recognizing potential\nthreats, from data privacy breaches to biased decision-making, and establishing protocols to\nprevent or minimize their impacts (Wirtz et al., 2022). It emphasizes risk mitigation and the\nmaximization of positive impacts, ensuring that AI systems contribute constructively to\norganizational goals and societal welfare (Tabassi, 2023)."}, {"title": "2.3. Case Studies in AI Integration into Organizations", "content": "Focusing on healthcare case studies for AI integration offers a critical contribution to the\ncurrent literature due to the distinct challenges and transformative potential AI holds within\nthis sector. Healthcare systems are complex, involving high-stakes decision-making,\nregulatory constraints, and deeply embedded workflows that necessitate specialized\napproaches to technology adoption. Case studies provide nuanced insights into how AI can\naddress these complexities\u2014enhancing clinical decision-making, streamlining operations,\nand refining diagnostics\u2014all while navigating ethical, data privacy, and interoperability\nconcerns. By examining real-world applications, healthcare case studies contribute a\ngrounded perspective on both the operational and ethical frameworks needed to implement\nAl responsibly, helping to bridge gaps between theoretical AI models and practical, scalable\nsolutions that enhance patient care and institutional efficiency.\nSeven healthcare-related case studies from the literature have been selected for our analysis.\nEach study reveals specific challenges, diverse perspectives, and findings, offering a\nmultidimensional view of AI implementation across this sector.\n(Ganapathi & Duggal, 2023): This study delves into the challenges and opportunities\nassociated with AI implementation in the UK's National Health Services (NHS) from\npractitioners' perspective. Through thematic analysis of eleven semi-structured interviews\nwith doctors, it was found that the main hurdles include the need for structured pathways into\nthe AI field, a steep learning curve, and greater comfort with uncertainty. The study\nemphasizes the importance of involving doctors in developing AI tools to leverage their\nclinical knowledge for better oversight and effective AI integration.\n(Morrison, 2021): Morrison's research applies innovation theory to explore solutions to AI\nadoption barriers within the NHS, focusing on radiology, pathology, and general practice.\nTwelve interviews with key informants, including NHS doctors, managers, and regulatory\npersonnel, revealed that IT infrastructure quality and financial pressures are significant\nbarriers. The study highlights the need for clear language around AI and establishing a gold\nstandard for Al deployment.\n(Petersson et al., 2022): This study investigates healthcare leaders' perspectives on AI\nimplementation in Swedish healthcare. Through qualitative content analysis of 26 interviews,\nit identifies the transformative impact of AI on healthcare professions. Challenges beyond the\nhealthcare system's direct control, such as strategic change management, are significant. The\nstudy emphasizes the need for systematic approaches and shared plans level to address these\nchallenges, as well as support from top leadership for successful AI adoption.\n(Li et al., 2023): This study explores how AI transforms healthcare's Human Resource\nManagement (HRM). Their interviews with HRM staff and analysis of archival data highlight\nthat AI facilitates co-creation processes and personalized care pathways and improves\nperformance by cutting costs and extending service offerings. The study underscores Al's\npotential to revolutionize HRM through targeted, individualized approaches to patient care.\n(Dumbach et al., 2021): This cross-national comparison examines AI adoption in healthcare\nSMEs in China and Germany, highlighting cultural and regulatory differences and their\nimpact on Al development. Fourteen semi-structured interviews with managers from both\ncountries reveal that AI is increasingly used to enhance innovation and product development,\nincluding medical image diagnosis and autonomous robotic surgery. The study suggests that\nSMEs adopt AI to co-create processes and build personalized care pathways, although\nregulatory and privacy issues need addressing.\n(Henry et al., 2022): This study explores clinicians' experiences with a deployed machine\nlearning (ML) system in a clinical setting, focusing on trust and human-machine teaming.\nThrough interviews, observations, and experimental studies with 20 clinicians using a\n\"targeted real-time warning\" system, the study finds that clinicians see ML systems as\nsupportive tools in diagnosis and treatment. However, concerns about potential over-reliance\non automated systems could degrade clinical skills over time.\n(Sun, 2021): Sun's research investigates the interplay of technology, power, and\norganizational behavior in AI adoption in Chinese hospitals. The multi-case study approach\nand 29 interviews examine how stakeholders use their social power to influence AI adoption.\nChallenges include insufficient data quality for training Al systems and the necessity for\ndeveloped Al systems to be introduced in hospitals.\nThe inclusion criteria for these healthcare case studies focused on capturing a comprehensive\nview of AI implementation challenges, aligned with the TOAST framework's core\ncomponents: trust, optimization, adaptability, and socio-technical integration. Each study,\nselected for its relevance to different healthcare applications and geographic contexts,\naddresses critical challenges such as regulatory barriers, IT infrastructure, data privacy, and\nimpacts on professional roles. Ganapathi & Duggal (2023) and Morrison (2021) provide\ninsights into structural and logistical challenges within an organization (the NHS), while\nPetersson et al. (2022) and Henry et al. (2022) explore the importance of clinician trust and\nacceptance in clinical AI deployment. Li et al. (2023) and Dumbach et al. (2021) expand the\nfocus to non-clinical applications and SMEs, emphasizing adaptability and regulatory\ncompliance across diverse settings. Sun (2021) highlights socio-political dynamics in Chinese\nhospitals, focusing on power relations and data quality issues. These selected case studies\noffer a multidimensional basis for evaluating the TOAST framework in real-world settings."}, {"title": "3. Methodology", "content": "Our research is based on a comprehensive literature review and case studies analysis on Al\nimplementation's ethical, socially responsible, and innovation-oriented impacts. We\nintegrated the existing multidisciplinary theories to develop the TOAST framework as a\nunifying view for responsible AI use and management. We then applied the framework on the\nselected case studies in the healthcare sector to evaluate the theoretical soundness and deepen\nour understanding of technology, ethics, and operational demands.\nThe literature review methodology focused on exploring and integrating multidisciplinary\ntheories and frameworks across different AI maturity levels within organizations,\nacknowledging each level's unique challenges and needs. We searched academic journals\nfrom databases such as Google Scholar, IEEE Xplore, ACM, and AIS eLibrary. The search\nkeywords include \"organizational AI implementation,\" \"multidisciplinary AI frameworks,\"\nand \"responsible AI.\" We have reviewed literature from the last ten years to ensure relevance\nand recency in the context of rapid technological advancements. Non-English studies and\nthose focused solely on the technical aspects of AI were excluded to maintain a clear\norganizational and implementation perspective.\nThrough a rigorous thematic analysis, we synthesized data from diverse studies, distilling\nessential concepts such as trust, control, socio-technical alignment, and innovation. This\nmethod enabled us to identify and integrate consistent patterns and principles across\ndisciplines, clarifying the shared challenges that effective AI implementation frameworks\nmust address. The TOAST framework is directly grounded in these findings, blending\ninsights from management, ethics, and technology fields to define the critical components of\na robust AI implementation strategy. By anchoring each of TOAST's core elements\u2014trust\nand accountability, optimization and control, adaptability and innovation, and socio-technical\nharmony in these cross-disciplinary insights, we crafted a framework that is both\ntheoretically rigorous. These components collectively address the complex needs outlined in\nthe literature, ensuring that TOAST offers a comprehensive, adaptable model for guiding AI\nintegration across various organizational environments.\nThe case study methodology was designed to ground the TOAST framework in real-world\nand practical cases, ensuring its theoretical soundness and practical applicability. In the initial\nstage, we carefully selected diverse case studies from the literature, focusing on those that\noffered insights into real-world AI adoption challenges, methodologies, and varying levels of\nAl maturity. Each case was chosen for its empirical depth, typically gathered through\ninterviews, providing practical perspectives on the complexities of integrating AI in\nhealthcare. Additionally, each study included a robust literature review and theoretical\nanalysis to anchor its findings in broader AI implementation theory, ensuring that our\nframework builds upon validated insights.\nThe second stage involved detailed data extraction from each case study, capturing a wide\narray of information, including specific AI implementation strategies, organizational\nchallenges, and the solutions and outcomes observed. This data provided a rich foundation\nfor understanding the situational factors influencing AI adoption, such as regulatory\nrequirements, technical constraints, and workforce considerations. By examining these\nfactors, we were able to assess how they impact each stage of AI maturity, offering a\ncomprehensive view of the practical challenges and strategies involved in AI integration.\nIn the third stage, we conducted a comparative analysis across the selected cases, identifying\ncommon themes, critical differences, and nuanced insights into Al adoption. By\nsystematically contrasting each case, we discerned patterns in how organizations address\ntrust, control, socio-technical integration, and innovation challenges. This comparative\napproach provided a cohesive understanding of how AI implementation strategies and\noutcomes vary across organizational contexts, contributing to a nuanced validation of the\nTOAST framework's principles and its adaptability across different stages of AI maturity in\nhealthcare."}, {"title": "4. Framework Design", "content": "This section explores the theoretical foundations and hypotheses essential for AI\nimplementation in organizational contexts. By examining various theoretical perspectives, we\ndesigned a robust framework to understand AI's multifaceted impact, emphasizing trust,\naccountability, optimization, adaptability, and socio-technical alignment. The framework\nbridges the gap between theory and practice, offering a structured approach for sustainable\nand effective AI integration in organizational settings."}, {"title": "4.1. Discussion of Key Theories", "content": "Trust theory, examined across disciplines like psychology, sociology, and computer science,\nunderscores the importance of transparency and ethical considerations in building user\nconfidence in AI systems. Al systems must be perceived as useful, reliable, and influenced by\nsocial factors to gain user acceptance (Cho et al., 2015; Dumbach et al., 2021; Gille et al.,\n2020; Omrani et al., 2022; Vereschak et al., 2021). Trust is dynamic and context-dependent,\nrequiring Al systems to be adaptable to various domains and user needs, ensuring ethical,\ntransparent operations aligned with human values (Araujo et al., 2020; Ferrario et al., 2020;\nGlikson & Woolley, 2020; Jacovi et al., 2021; Razmerita et al., 2022).\nControl theory, rooted in engineering and mathematics, leverages algorithms and feedback\nmechanisms to manage system behavior, allowing for self-adjustment and optimization in\nresponse to new data (Filieri et al., 2015; Gill et al., 2022; Hou & Wang, 2013). This theory's\napplication in Al advocates for a symbiotic relationship between human intuition and AI's\ncomputational autonomy, contributing to organizational learning and sustainability (Bankins\net al., 2023; Caldas et al., 2020; Khargonekar & Dahleh, 2018; Seok et al., 2012).\nSocio-technical theory advocates for culturally compatible AI ethically aligned with\nstakeholder values. This approach ensures that AI systems are accountable by considering\nprivacy, transparency, and fairness as integral components (Baxter & Sommerville, 2011;\nHoda, 2022; Holton & Boyd, 2021; Sartori & Theodorou, 2022; Toreini et al., 2022;\nVassilakopoulou, 2020).\nInnovation theory positions AI technologies as transformative agents that fundamentally shift\ninnovation trajectories across various domains and industries. It includes the principles for\ngrasping Al's role in instigating systemic changes in business models, organizational\nstrategies, and market dynamics (Haefner et al., 2021; Mariani et al., 2023), underscoring\norganizations' need to evolve to stay relevant and competitive (Burstr\u00f6m et al., 2021; Di Vaio\net al., 2020). AI cultivates innovative products, services, processes, and business practices\nthat often represent significant advancements, redefining industry benchmarks and consumer\nexpectations (Alsheibani et al., 2018; J\u00f6hnk et al., 2021; Verganti et al., 2020)."}, {"title": "4.2. Synthesis of Theories Linked to Hypotheses", "content": "Hypothesis 1 (H1): Development of Adaptive AI Systems Cultivates Incremental Trust\nEstablishing trust in AI systems is a gradual process that strengthens over time. Trust\ndevelops as users engage with the system and observe its consistent reliability and\neffectiveness. The context in which AI software is deployed\u2014including the user\nenvironment, application specificity, and the domain of use\u2014significantly influences trust\nformation (Ferrario et al., 2020). AI software engineering must address socio-technical\ncomplexities to build trust by integrating AI into existing systems and aligning it with user\nexpectations (Cho et al., 2015; Gille et al., 2020) while incorporating technical soundness,\nsocial implications, ethical guidelines, user experience, and societal impact (Baxter &\nSommerville, 2011; Hoda, 2022).\nAl systems must adapt dynamically by observing and adjusting their behavior to meet\nobjectives. Control theory has been used in software engineering to create self-adjusting\nsoftware systems. It is important to incorporate feedback control strategies and strong\nadaptation principles to improve AI systems' resilience in unpredictable conditions(Caldas et\nal., 2020; Filieri et al., 2015). Innovation theory is also relevant to AI development, from\nbusiness model innovation to practical AI implementation. A comprehensive approach to AI\nadoption is required, balancing technological components with human factors in the\norganization (Burstr\u00f6m et al., 2021; Mariani et al., 2023).\nHypothesis 2 (H2): Transparent AI Algorithms and Models Enhance Human-centered\nAccountability and AI Readiness\nThe cultivation of trust among users requires transparency in AI algorithms and models,\nalong with ethical considerations to ensure impartiality and absence of bias (Liu, 2021). The\ntype of tasks executed by AI algorithms profoundly influences trust levels. Different types of\ntasks influence trust levels, with technical tasks often garnering more trust than those\ninvolving social intelligence or ethical discernment (Glikson & Woolley, 2020). Control\ntheory is crucial for designing and refining AI algorithms, enabling real-time performance\nenhancement, system resilience (Hou & Wang, 2013) and system performance optimization,\nespecially in real-time settings (Mueller et al., 2019).\nSocio-technical theory emphasizes the need for accountable and transparent AI systems that\nprioritize human values, clarity, and equity while addressing issues related to bias, privacy,\nand ethical application (Herrmann & Pfeiffer, 2023; Vassilakopoulou, 2020). Innovation\ntheory highlights the importance of employees' awareness of AI and organizational readiness\nfor AI integration (Arias-P\u00e9rez & V\u00e9lez-Jaramillo, 2022; J\u00f6hnk et al., 2021). A holistic\napproach to AI adoption within organizations, including understanding employees'\nperspectives on AI and ensuring strategic preparedness, is essential for successful AI\nintegration (Brock & Von Wangenheim, 2019).\nHypothesis 3 (H3): Responsible AI Innovation Sustains Societal Trust in A\u0399\nEstablishing trust in artificial intelligence (AI) requires a responsible innovation strategy to\nensure equitable benefits. This involves accountability, privacy, fairness, explainability,\ntransparency, reproducibility, reliability, user interaction, data safeguarding, and security\nthreat mitigation (Chatila et al., 2021; Cho et al., 2015). Adapting to AI technology\nchallenges involves balancing data sharing for training while protecting privacy and\naddressing safety, privacy, and bias concerns. Al systems must also consider social,\neconomic, and environmental sustainability, as well as information security control and\nethical considerations in data sharing (Anderson et al., 2017; Seok et al., 2012).\nThe societal discourse and perceptions of AI are crucial in shaping its development and\npublic image. Engaging the public is important for establishing a positive relationship with\nAI (Sartori & Bocca, 2023; Sartori & Theodorou, 2022). Using non-explainable Al models\nrequires balancing performance advantages and associated risks (Asatiani et al., 2021).\nIncorporating AI algorithms into business operations can enhance efficiency and lead to\ninnovations in products and services, improving decision-making capabilities and business\nmodels(Burstr\u00f6m et al., 2021; Haefner et al., 2021). An organization's readiness for AI\ninnovation depends on aligning AI technologies with strategic objectives, resource\navailability, and fostering an innovation-friendly culture (Alsheibani et al., 2018; Arias-P\u00e9rez\n& V\u00e9lez-Jaramillo, 2022; J\u00f6hnk et al., 2021).\nHypothesis 4 (H4): Human-AI Symbiotic and Synergistic Collaborations Enable\nTransformational Impact\nTrust in AI is crucial for effective collaboration and decision-making in autonomous systems,\nespecially when Al's decisions have significant consequences for users (Jacovi et al., 2021).\nHuman-AI interactions are pivotal in cultivating trust within organizational contexts and are\ninfluenced by individual user characteristics, such as AI literacy, privacy concerns, and\ncultural diversity (Araujo et al., 2020). Trust is contingent upon the degree of autonomy\ngranted to AI and its effects on users (Liu, 2021). Human-AI symbiosis suggests that AI\nsystems should enhance human cognitive capabilities, and control theory emphasizes the\nneed for psychologically and technically proficient systems (Clemmensen, 2021). Human and\nAI interactions within organizations reveal differing perceptions of competencies, employee\nattitudes toward AI, and the broader implications of AI on the labor market (Bankins et al.,\n2023; Seok et al., 2012). AI's capability to process complex datasets fosters a symbiotic\nrelationship that enhances organizational decision-making (Jarrahi, 2018; Soma et al., 2022).\nProposals for advancing human-centered AI include a two-dimensional framework for high\nlevels of human control and automation, redefining AI systems as potent tools under human\ncommand, and a tripartite governance structure to aid software engineering teams in\ndeveloping reliable, safe, and trustworthy systems (Shneiderman, 2020).\nThe integration of social and technical systems is crucial for the trustworthy implementation\nof AI technologies. It emphasizes transparency, human oversight, and equitable deployment,\nshaping employment trends, job dynamics, and organizational roles (Chowdhury et al., 2022;\nSartori & Theodorou, 2022; Yu et al., 2022), Integrating Al requires knowledge\ndissemination, workplace AI socialization, management of evolving AI systems, and\nadjustments to decision-making frameworks (Herrmann & Pfeiffer, 2023; Sony & Naik,\n2020). Al innovation has substantial transformative potential, enhancing service provision,\ndecision-making, and operational efficiency. For example, AI has improved patient outcomes\nand reduced healthcare costs (Alhashmi et al., 2020). It also influences job satisfaction,\npsychological well-being, and organizational productivity (Yu et al., 2022). However, the\nintegration of AI requires careful consideration of data privacy, ethical imperatives, and\nhuman-centric services. The growing prevalence of autonomous systems demands a nuanced\ncomprehension of the dynamics between humans and AI (Verganti et al., 2020)."}, {"title": "4.3. The TOAST Framework as a Unifying View", "content": "By melding multidisciplinary theories", "Ethics)": "This component is the ethical compass\nof the framework", "Requirements)": "This component is the technical\nbackbone of our framework", "Collaboration)": "This component fosters\ncontinuous innovation and harnesses Al's transformative potential by promoting adaptable", "Readiness)": "Informed by socio-technical\ntheory, this component recognizes that AI is deeply interconnected with organizational\nstructures, culture, and human factors. Successful AI implementation requires aligning\ntechnology with users' skills, workflows, and values, creating systems that enhance rather\nthan replace human capabilities. By"}]}