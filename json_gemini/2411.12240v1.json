{"title": "EVALUATING TOKENIZER PERFORMANCE OF LARGE LANGUAGE MODELS ACROSS OFFICIAL INDIAN LANGUAGES", "authors": ["Sagar Tamang", "Dr. Dibya Jyoti Bora"], "abstract": "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer's superior handling of Indic languages, GPT-40's advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency.", "sections": [{"title": "Introduction", "content": ""}, {"title": "Background", "content": "In an ever-evolving landscape of Artificial Intelligence (AI), transformers-based generative Large Language Models (LLMs) are transforming an increasing number of fields with an ever-increasing number of applications in finance, medicine, education, and many more [1, 2]. Tokenization is an important step for LLMs, especially in pre-processing and fine-tuning stages [3].\nMost of the LLMs use either of two types of tokenization algorithms, namely WordPiece and Byte Pair Encoding (BPE). For example, OpenaAi's GPT-40 model and META's Llama 3, both employ a modified BPE tokenizer [4]. WordPiece was developed for models like BERT employing a greedy approach. It starts with the longest substring that matches a token in its vocabulary, allowing it to handle out-of-vocabulary words effectively by breaking them down into known subword units [5, 6]. BPE works by iteratively merging the most frequently occurring pairs of characters or subwords in a corpus to create a complete vocabulary [7, 8].\nAgnostic of the tokenization algorithms used, many techniques have been developed to compare tokenizers of LLMs. Subword fertility is one such technique which measures the average number of tokens used per word [9]. Normalized Sequence Length (NSL) is another such metric used to evaluate the efficiency of tokenizers [12]."}, {"title": "Tokenizers in Multilingual & Indic Models", "content": "Many multilingual models, including OpenAI's ChatGPT [14], Google's Gemini [16], Meta's Llama [17], and TWO AI's SUTRA [15], are designed to deliver coherent performance across a wide range of global languages, including Indian languages. Achieving this requires tokenizers in these LLMs to handle diverse languages efficiently. This study evaluates the performance of tokenizers in these multilingual models and Indic-specific models across all official languages of India.\nThe rest of the paper is organized in the following way: Section 2 highlights related works, Section 3 describes the methodology of this study, Section 4 is where the results are showcased and Section 5 is for discussions with future outlook."}, {"title": "Literature Review", "content": ""}, {"title": "Large Language Models and Tokenization", "content": "Ever since the release of transformer-based architecture, Large Language Models (LLMs) have demonstrated remarkable capabilities in Natual Language Processing (NLP) tasks and beyond [19]. However, some recent studies [18] have also proposed promising non-transformer LLMs. LLMs today exhibit a plethora of applications not limited to NLP tasks, but can also perform general tasks performing multi-step reasoning. Thus, LLMs are becoming the basic building block for developing general-purpose AI agents or Artificial General Intelligence (AGI) [18].\nTokenization refers to the process of converting a sequence of texts into smaller parts, known as tokens. In order to increase the coverage of dictionaries and also to deal with words that were unseen in training data, LLMs use sub-words based tokenizers like BPE or Wordpiece [4, 8, 7, 18]."}, {"title": "Indic-Specific Language Models", "content": "S. Bhat et al. evaluated the capabilities of generative models like ChatGPT, mT0, and BLOOMZ in generating Indic languages, and the findings revealed that these models have limited capabilities in generating text in Indic languages in a zero-shot setting. While they performed better on manual quality evaluations for English, their performance in Indic languages highlighted the need for further development and training specifically tailored to these languages [22]. Several studies have introduced benchmarks to evaluate the performance of LLMs across Indic languages, and their results emphasize the necessity for more focused research and development efforts in LLMs to handle the linguistic diversity of India [23, 24, 25].\nOne notable advancement is SUTRA (Scalable Multilingual Language Model Architecture), introduced by A. Bendale et al. SUTRA supports over 50 languages, including Indic ones, by decoupling conceptual understanding from language-"}, {"title": "Comparative Evaluation of Tokenizers", "content": "To compare the tokenizers, several metrics and methodologies have been introduced. Subword fertility is one such metric that measures the average number of tokens generated per word [12]. An ideal tokenizer in this case would have a fertility of 1.0, indicating that most words are represented as single tokens, while a higher fertility score signals that many words are split into multiple tokens [10]. Another metric that is employed is the proportion of continued words [10] which indicates the percentage of words that are split into multiple tokens. O is an ideal value for the proportion of continued words. A lower proportion indicates better performance, as it means fewer words are fragmented [10]. Another metric that is used to evaluate and compare tokenizers is the Normalized Sequence Length (NSL). NSL measures the average length of tokenized sequences produced by a tokenizer relative to a baseline tokenizer [12].\nExisting tokenizer studies are performed at a smaller scale than what is typical for modern LLMs or focus on multilingual tasks [12]. Some studies have tried to evaluate the performance of tokenizers but only in some select languages like Turkish [21], Arabic [26], or focusing on a few Indian languages [3, 20]."}, {"title": "Gaps in Existing Research", "content": "Thus, we can observe that there exists a research gap to evaluate the tokenizers in other Indian languages as well. Hence, in this study, we are conducting an overall evaluation of the tokenizers of LLMs in all 22 official languages of India as recognized by the eighth schedule of the Indian constitution [27]."}, {"title": "Methodology", "content": "Our evaluation step is summarized in Figure 1."}, {"title": "Example Texts", "content": "We compiled example texts in all 22 languages to evaluate the performance of the tokenizers. Each language's text was selected in its primary writing script to ensure an authentic assessment of the tokenizer's capability to process native scripts accurately. The curated example texts represent diverse linguistic structures and scripts, enabling a comprehensive analysis of tokenization performance.\nAll the example texts used during our study can be found in the Appendix A.2 or Figure 26 and 27. One such example text can be found in Figure 2."}, {"title": "Models", "content": "We chose 12 models including proprietary multilingual models, as well as Open-weights multilingual and Indic language models for our study. The list of models can be found in Table 1.\nWhile we acknowledge that some models are not specifically designed for all Indian languages\u2014such as MahaMarathi, which is tailored for Marathi, and MBZUAI's Nanda, which is optimized for Hindi and English-we have included them in this study for the sake of comprehensive evaluation."}, {"title": "Evaluation Metric", "content": "For our work is extending the previous works by [3], we have chosen to go with the NSL metric. Formally the NSL is defined by [12] $c_{\\alpha\\beta}$ as the ratio between the length of an encoded sequence from a tokenizer $T_x$ and a tokenizer $T_B$. For N examples taken from a dataset D:\n$\\qquad C_{A\\beta} = \\frac{\\sum_{i=1}^N length(T_x(D_i))}{\\sum_{i=1}^N length(T_B(D_i))}$"}, {"title": "Results", "content": "Average NSL Values Table 2 presents the average NSL values for all tokenizers across the 22 languages, calculated using the examples provided in Appendix A.2. The scores are reported to four decimal places for precision. The bold text indicates the lowest value or the best performance among all the other tokenizers. It can be observed that SUTRA tokenizer manages to excel among all the other tokenizers including ChatGPT's 4-0 or other Indic models.\nFigure 3 illustrates the number of languages in which each tokenizer achieved the highest NSL score. For instance, TWO AI's SUTRA outperformed all other tokenizers in 14 languages, while MBZUAI's Nanda excelled in 6, and OpenAI's GPT-40 in 5. Other notable performers include indic models like Tech Mahindra's Project Indus with 4, Sarvam Al's OpenHathi and MahaMarathi with 2 each, and Indic Gemma, Microsoft Phi, and Airavata with 1 each.\nNumber of Tokens Appendix A.1 provides individual row bar charts for each language, offering a detailed breakdown of the number of tokens generated in each language. Lower token counts indicate better outcomes."}, {"title": "Discussion", "content": "In this study, we evaluated the tokenizers from 12 LLMs in all 22 official languages of India and we found that the SUTRA tokenizer performed the best among all others, outperforming the 2nd best tokenizer by a large margin. This showcases the multilingual strength of the SUTRA tokenizer to handle the Indic languages.\nMicrosoft's Phi-3.5-MoE-instruct and Google's Indic Gemma Though both the models were developed for Indic languages, they did not perform up to the level securing best performances only in one language out of 22 languages.\nObservation between GPT-4 and GPT-40 Another interesting observation is that GPT-4, the predecessor of GPT-40, did not manage to secure the best tokenizer value in any of the 22 languages, a stark contrast to GPT-40. Perhaps, this highlights that an important difference between the two models is that the newer GPT-40 is well adept at Indian languages, increasing the multi-lingual capability."}, {"title": "Observation of Tech Mahindra's Project Indus", "content": "Both SUTRA and GPT-40 tokenizers manage to get a consistently low average NSL value (below 1.0) for all the languages but the Project Indus' tokenizer seems to be getting the same for only a few languages like (1) Bodo, (2) Dogri, (3) Hindi, (4) Konkani, (5) Maithili, (6) Marathi, (7) Nepali, and (8) Sanskrit. This is probably because all these 8 languages follow the same Devanagari script of writing, which the model's tokenizer was probably trained on. But for the rest of the languages, the tokenizer seems to be struggling, getting an average NSL score of above 1 (the higher the worse)."}, {"title": "Number of Tokens", "content": "According to the results and appendix A.1, tokenizers like SUTRA generate fewer tokens across the 22 Indian languages compared to others. Lower token counts suggest that the tokenizer is more efficient in"}, {"title": "Significance of tokenization in LLMs", "content": "Tokenization plays a vital role in LLMs by breaking down text into smaller units (tokens) that the model can process efficiently. A well-designed tokenizer enables the model to handle complex language structures, out-of-vocabulary words, and multi-language contexts effectively. It enhances the model's ability to understand and generate language with greater accuracy. Additionally, a good tokenizer leads to reduced computational costs and resource requirements by optimizing token generation. This results in faster training times, lower resource consumption, and overall improved performance, allowing the model to process diverse languages more effectively."}, {"title": "Real-World Applications and Future Directions", "content": "The insights from this study have important implications for the development of multilingual models across Indian languages. Future research could focus on enhancing tokenizers to better handle languages with complex scripts or languages with a high degree of dialectical variation, improving model performance for both high-resource and low-resource languages."}, {"title": "Appendix", "content": ""}, {"title": "Bar Charts of Token Counts for Each Language", "content": ""}, {"title": "Example Texts Used for Tokenizer Evaluation", "content": ""}]}