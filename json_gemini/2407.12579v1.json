{"title": "The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation", "authors": ["Yi Yao", "Chan-Feng Hsu", "Jhe-Hao Lin", "Hongxia Xie", "Terence Lin", "Yi-Ning Huang", "Hong-Han Shuai", "Wen-Huang Cheng"], "abstract": "In spite of recent advancements in text-to-image generation, limitations persist in handling complex and imaginative prompts due to the restricted diversity and complexity of training data. This work explores how diffusion models can generate images from prompts requiring artistic creativity or specialized knowledge. We introduce the Realistic-Fantasy Benchmark (RFBench), a novel evaluation framework blending realistic and fantastical scenarios. To address these challenges, we propose the Realistic-Fantasy Network (RFNet), a training-free approach integrating diffusion models with LLMs. Extensive human evaluations and GPT-based compositional assessments demonstrate our approach's superiority over state-of-the-art methods. Our code and dataset is available at https://leo81005.github.io/Reality-and-Fantasy/.", "sections": [{"title": "1 Introduction", "content": "Considerable advancements have been made in the field of text-to-image generation, especially with the introduction of diffusion models, e.g., Stable Diffusion [31], GLIDE [22], DALLE2 [30] and Imagen [32]. These models exhibit remarkable proficiency in generating diverse and high-fidelity images based on natural language prompts.\nHowever, despite their impressive capabilities, diffusion models occasionally face challenges in accurately interpreting complex prompts that demand a deep understanding or specialized knowledge [39,41,43]. This limitation becomes particularly apparent with creative and abstract prompts, which require a nuanced"}, {"title": "2 Related Work", "content": "Diffusion models, utilizing stochastic differential equations, have emerged as effective tools for generating realistic images [1,4,9, 21, 22, 30\u201332]. DALL-E 2 [30] pioneered the approach of converting textual descriptions into joint image-text embeddings with the aid of CLIP [29]. GLIDE [22] demonstrated that classifier-free guidance [11] is favored by human evaluators over CLIP guidance for gen-erating images based on text descriptions. Imagen [32] follows GLIDE but uses pretrained text encoder instead, further reducing negligible computation bur-den to the online training of the text-to-image diffusion prior, and can improve sample quality significantly by simply scaling the text encoder.\nAlthough text-to-image capabilities have seen significant development, there has been limited focus on generating images involving high levels of creativity,"}, {"title": "2.1 Text-guided diffusion models", "content": "Diffusion models, utilizing stochastic differential equations, have emerged as effective tools for generating realistic images [1,4,9, 21, 22, 30-32]. DALL-E 2 [30] pioneered the approach of converting textual descriptions into joint image-text embeddings with the aid of CLIP [29]. GLIDE [22] demonstrated that classifier-free guidance [11] is favored by human evaluators over CLIP guidance for gen-erating images based on text descriptions. Imagen [32] follows GLIDE but uses pretrained text encoder instead, further reducing negligible computation bur-den to the online training of the text-to-image diffusion prior, and can improve sample quality significantly by simply scaling the text encoder.\nAlthough text-to-image capabilities have seen significant development, there has been limited focus on generating images involving high levels of creativity,"}, {"title": "2.2 LLMs for image generation", "content": "Recently, researchers have explored using LLMs to provide guidance or auxil-iary information for text-to-image generation systems [7, 15, 19, 27, 28, 36, 38]. In LMD [15], foreground objects are identified using LLMs, and then images are generated based on the layout determined by the diffusion model. Phung et al. [25] proposes attention-refocusing losses to constrain the generated objects on their assigned boxes generated by LLMs. LVD [16] requires LLMs to gener-ate continuous spatial constraints to accomplish video generation. Besides using LLMs to generate spatial layout from user prompts, some studies [27,36,38] inves-tigate integrating LLMs directly into the image generation pipeline. SLD [36] in-tegrates open-vocabulary object detection with LLMs to enhance image editing. RPG [38] integrates LLMs in a closed-loop manner, allowing generated images to continuously improve through LLMs feedback, and uses Chain-of-Thought [35] to further improve generation quality.\nAs a result of these developments, LLMs can be incorporated into pipelines for the generation of images. In this work, we use LLMs to uncover and elaborate upon the complexities embedded within complex and abstract prompts."}, {"title": "3 Our Proposed Realistic-Fantasy Benchmark", "content": "In this study, we explore how diffusion models can effectively process and gen-erate imagery from prompts that pose significant challenges due to their re-liance on creative thinking or specialized knowledge. Recognizing the absence of a dedicated evaluation framework for such tasks, we introduce a new benchmark, the Realistic-Fantasy Benchmark (RFBench), which blends scenarios from both realistic and fantastical realms.\nBenchmark Collection. We focus on two main categories, each with dis-tinct subcategories, Realistic & Analytical and Creativity & Imagination, totaling nine subcategories. Each sub-category is meticulously crafted with around 25 text prompts, leading to an aggregate of 229 unique compositional text prompts designed to test the models against both conventional and unprece-dented creative challenges. The collection process, outlined in Fig. 2, employs"}, {"title": "4 Our Proposed Realistic-Fantasy Network", "content": "In this section, we propose a Realistic-Fantasy Network (RFNet) for the benchmark scenario we proposed in the previous section. To thoroughly interpret"}, {"title": "4.1 LLM-Driven Detail Synthesis", "content": "In the first stage of our methodology, we concentrate on utilizing LLMs to un-cover and elaborate on the intricacies embedded within the user's input prompt. This process involves specifying task requirements to more accurately define the task and incorporating in-context learning to enhance understanding for LLMs. The enriched response from the LLM encompasses additional information, such as layout, detailed descriptions, background scenes, and negative prompts 4. This step is crucial as it aims to mitigate the primary challenge we seek to overcome: the training data bias inherent in current diffusion models. By leveraging the pre-trained LLM for logical reasoning and conjecture, we aim to compensate for the gaps left by these biases, ensuring a more accurate and coherent image generation process."}, {"title": "4.2 Semantic Alignment Assessment", "content": "As we proceed with generating images using the diffusion model using the details generated by the previous step, there is a critical challenge: the description lists generated by LLMs for one object usually overlook the relationships among them. For example, interpretations of \"a lion\" could range from being \"unaware and asleep\" to \"frightened and trying to escape.\" Although both depictions are valid, descriptions such as \"unaware\" and \"trying to escape\" can lead to conflicting interpretations, thus complicating the image generation process.\nTo overcome this challenge, we introduce the Semantic Alignment Assess-ment (SAA) module. This module calculates the relevance between different object vectors, thereby selecting the candidate description that best fits the cur-rent scenario. By conducting the cosine similarity among different descriptions, we can navigate the complexities introduced by the LLM's output, selecting the most compatible details for the diffusion model. This step is crucial for maintain-ing the coherence and accuracy of the generated images, highlighting our novel approach to mitigating the risk of conflicting descriptions. Through this mod-ule, we ensure textual precision and compatibility, and provide clear, consistent instructions for the subsequent diffusion model to generate visually coherent representations."}, {"title": "4.3 Comprehensive Image Synthesis", "content": "In the second stage of our proposed RFNet, following LMD [15], we propose a two-step generation process for imaginative and abstract concepts. As shown"}, {"title": "5 Experiments", "content": "In this section, we propose a Realistic-Fantasy Network (RFNet) for the benchmark scenario we proposed in the previous section. To thoroughly interpret"}, {"title": "5.1 Implementation Details", "content": "Experimental Setup. In this work, we choose versions 1.4 and 2.1 of Stable Diffusion [31] as the text-to-image baseline model. The number of denoising steps is set as 50 with a fixed guidance scale of 7.5, and the synthetic images are in a resolution of 512 \u00d7 512. All experiments are conducted on the NVIDIA RTX 3090 GPU with 24 GB memory.\nEvaluation Metrics. We generate 32 images for each text prompt in RF-Bench for automatic evaluation. We selected the following two metrics: (1) GPT4-CLIP 5. By utilizing GPT4 for captioning and calculating CLIP text-text cosine similarity, GPT4-CLIP ensures a more precise reflection of the intended meanings between images and prompts. (2) GPT4Score. Inspired by [13,17], we adopt GPT4Score to evaluate image alignment with text prompts, where GPT4 rates images on a 0-100 scale based on their fidelity to the prompts, enabling precise assessment of model-generated visuals against specified criteria.\nComparison with Existing Methods. We benchmark our proposed RFNet against various open-source scene generation methods, including Stable Diffu-sion [31], Attend and Excite [3], LMD [15], BoxDiff [37], MultiDiffusion [2], and SDXL [26]. Notably, all methods, including ours, utilize Stable Diffusion 2.1 as the foundational model, ensuring a fair comparison."}, {"title": "5.2 Quantitative Evaluation", "content": "Evaluation on RFBench. As evidenced in Tab. 5, our approach significantly outperforms other methods for both Realistic & Analytical and Creativity & Imagination tasks. For Realistic & Analytical task, our method seamlessly inte-grates LLM-based insights, achieving a remarkable accuracy improvement. Un-like Attend-and-excite, which focuses on semantic guidance, our method ensures precise adherence to detailed and complex prompt requirements. For the Cre-ativity & Imagination, which demands high degrees of creativity and abstract conceptualization, our method outperforms others by not only adhering to the imaginative aspects of prompts but also maintaining coherent structure and contexts. For instance, SDXL, while adept at high-resolution image synthesis, occasionally lacks in capturing the nuanced creativity intended in prompts; our method fills this gap effectively. Similarly, LMD, though enhancing prompt un-derstanding through LLMs, sometimes struggles with the scientific reasoning required for Realistic& Analytical tasks.\nNotably, for Realistic & Analytical category, our approach shows a 61% per-formance increase over MultiDiffusion on GPT4Score. Meanwhile, in Creativity & Imagination task, we observe a substantial enhancement, outperforming Sta-ble Diffusion by over 43%. In light of the above, our method is unique in its ability to bridge the gap between realistic reasoning and imagination, creating a new benchmark for text-to-image generation.\nEvaluation on DrawBench. We also evaluate our method on Draw-Bench [32], a comprehensive and chal-lenging benchmark for text-to-image models. Similar to us, DrawBench also includes some Creativity & Imagina-tion prompts, and we evaluate our method with Imagen [32] on these prompts. As shown in Tab. 3, our ap-proach significantly outperforms Ima-gen on most prompt settings, demon-strating the generalization ability of our model."}, {"title": "5.3 Qualitative Evaluation", "content": "In the qualitative comparison of text-guided image generation, we select some ad-vanced baseline methods, including Attend-and-excite [3], BoxDiff [37], LMD [15], MultiDiffusion [2], and SDXL [26]. Attend-and-excite focuses on enhancing the semantic understanding of prompts through attention mechanisms, while BoxD-iff introduces a novel approach to text-to-image synthesis with box-constrained diffusion without the need for explicit training. MultiDiffusion proposes a method for fusing multiple diffusion paths to achieve greater control over the image gen-eration process, and SDXL aims at improving the capabilities of latent diffusion models for synthesizing high-resolution images. As shown in Fig. 5, our method, produces more precise editing results than the aforementioned methods. This is attributed to our In-Depth Object Generation and Seamless Background Integration strategy. It ensures outstanding fidelity in outcomes and flawlessly retains the semantic structure of the source image, highlighting our approach's superior capability in complex editing tasks."}, {"title": "5.4 User study", "content": "Through an extensive user study, we benchmarked our model against other methods to assess real human preferences for the generated images. Utilizing our newly proposed benchmark, the RFBench, we selected a diverse set of 27 prompts and generated six images per prompt to ensure a broad representation of the model's capabilities. Detailed feedbacks were collected from 120 partici-pants, evaluating each image for visual quality and text prompt fidelity 7. These criteria are critical, which measure the image's quality and correctness of seman-tics in the synthesized image. Participants rated images on a scale from {1, 2, 3, 4, 5}, with scores normalized by dividing by 5. We calculated the average score across all images and participants.\nAs illustrated in Fig. 6, participants uniformly favored our model's output, recognizing it as superior in both quality and alignment with the textual de-scriptions."}, {"title": "5.5 Ablation study", "content": "Impact of Various Constraints.\nTo validate the impact of guidance constraint and suppression constraint, we perform ablation studies on differ-ent combinations of constraints, and the results are listed in Tab. 4. As shown, the baseline model (Stable Dif-fusion) achieves a 0.295 in terms of GPT4Score without any constraints. As guidance constraint and suppression constraint work complementary to restrict the cross-attention of objects inside the conditional boxes, a higher GPT4Score"}, {"title": "6 Conclusion and Future Work", "content": "In this research, we present a novel challenge: generating scenes that blend real-ity and fantasy. We investigate the capacity of diffusion models to create visuals from prompts that demand high levels of creativity or specific knowledge. Not-ing the lack of a specific evaluation mechanism for such tasks, we establish the Realistic-Fantasy Benchmark (RFBench), combining elements of both realistic and imaginary scenarios. To address the task of generating realistic and fantas-tical scenes, we introduce a unique, training-free, two-tiered method, Realistic-Fantasy Network (RFNet), that combines diffusion models with large language models (LLMs). Our approach, evaluated through the RFBench using thorough human assessments and GPT-based compositional evaluations, has proven to be superior to existing cutting-edge techniques. Given the novelty of our task, future research could develop additional evaluation metrics beyond those used in this study, enhancing the assessment of generated scenes."}, {"title": "A LLM-Driven Detail Synthesis", "content": "In this work, as described in the Sec. 4.1 of the main paper, we emphasized that by leveraging LLMs, we have significantly enriched responses to encompass additional information, such as layout, detailed descriptions, background scenes, and negative prompts. To achieve this, we facilitated an interaction with a LLM as shown in Fig. 8. The input given to the LLM, depicted on the left side of the figure, includes detailed task specifications and in-context learning examples to enhance the LLM's comprehension. The response from the LLM, shown on the right, is rich with details extracted from the prompt. Notably, the descriptions are particularly crucial for our work, serving as indispensable information for the later image generation stage."}, {"title": "B Qualitative Comparison on RFBench", "content": "In Fig. 9 and Fig. 10, we present additional qualitative examples to showcase the exceptional outcomes of our work. Fig. 9 shows the results under the cate-gory Realistic and Analytical, while Fig. 10 shows the category Creativity and Imagination. Both figures demonstrate that our method achieves more accurate editing results compared to other approaches."}, {"title": "C GPT4Score", "content": "We follow the approach of T2I-Compbench, using Multimodal LLM (MLLM) to measure the similarity between generated images and input prompts. The key deviation lies in our observation that MiniGPT4, employed in T2I-Compbench, struggles to comprehend the surreal aspects of the images effectively. Therefore, we employ GPT4, a more powerful MLLM, as our new benchmarking model for evaluation, as mentioned in the Sec. 5.1 of the main paper.\nSpecifically, given a generated image and its prompt, we input both the image and prompt into GPT4. Subsequently, we pose two questions to the model:"}, {"title": "ALLM-Driven Detail Synthesis", "content": "In this work, as described in the Sec. 4.1 of the main paper, we emphasized that by leveraging LLMs, we have significantly enriched responses to encompass additional information, such as layout, detailed descriptions, background scenes, and negative prompts. To achieve this, we facilitated an interaction with a LLM as shown in Fig. 8. The input given to the LLM, depicted on the left side of the figure, includes detailed task specifications and in-context learning examples to enhance the LLM's comprehension. The response from the LLM, shown on the right, is rich with details extracted from the prompt. Notably, the descriptions are particularly crucial for our work, serving as indispensable information for the later image generation stage."}, {"title": "E Human Correlation of the Evaluation Metrics", "content": "We adopt the methodology from T2I-Compbench, calculating Kendall's tau (T) and Spearman's rho (p) to evaluate the ranking correlation between CLIPScore, GPT4Score, and human evaluation. For better comparison, the scores predicted by each evaluation metric are normalized to a 0-1 scale. The human correlation results are presented in Tab. 5. These results indicate that CLIP underperforms in both categories, as discussed in Section 5.1 of the main paper. This underper-formance may be due to CLIP's approach to image understanding, which is often too simplistic. Nevertheless, both metrics encounter challenges with Creativity and Imagination, highlighting that although GPT4Score offers a broader un-derstanding of images, accurately assessing creativity remains a difficult task for both."}, {"title": "F Visualization of Ablation Study", "content": "In addition to the quantitative results presented in our ablation study, we have also included visual examples to showcase the impact of different components in our work. As shown in Fig. 12, the removal of guidance constraint and sup-pression constraint both causes the diffusion model to become muddled when"}, {"title": "F.1 Effect of the hyperparameter \u1e9e of guidance constraint", "content": "In our paper, we emphasize the critical role of the guidance constraint in in-tegrating multiple objects into the background. To underscore its significance, we performed an additional ablation study focusing on the hyperparameter \u03b2, which influences the strength of guidance constraint. As shown in Fig. 13, we varied \u1e9e from 0.1 to 30 to observe the effects on the generated results. The find-ings reveal that an optimal \u1e9e value (e.g., setting it to 15) ensures objects are accurately aligned with the layout and are of high quality. However, extreme B values, such as 0.1 or 30, disrupt the layout and diminish the overall quality of the generated images."}]}