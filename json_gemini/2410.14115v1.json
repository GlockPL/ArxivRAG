{"title": "A Communication and Computation Efficient Fully First-order Method for Decentralized Bilevel Optimization", "authors": ["Ming Wen", "Chengchang Liu", "Ahmed M. Abdelmoniem", "Yipeng Zhou", "Yuedong Xu"], "abstract": "Bilevel optimization, crucial for hyperparameter tuning, meta-learning and reinforcement learning, remains less explored in the decentralized learning paradigm, such as decentralized federated learning (DFL). Typically, decentralized bilevel methods rely on both gradients and Hessian matrices to approximate hypergradients of upper-level models. However, acquiring and sharing the second-order oracle is compute and communication intensive. To overcome these challenges, this paper introduces a fully first-order decentralized method for decentralized Bilevel optimization, C2DFB which is both compute- and communicate- efficient. In C2DFB, each learning node optimizes a min-min-max problem to approximate hypergradient by exclusively using gradients information. To reduce the traffic load at the inner-loop of solving the lower-level problem, C2DFB incorporates a lightweight communication protocol for efficiently transmitting compressed residuals of local parameters. Rigorous theoretical analysis ensures its convergence of \u00d5(\u20ac\u00af\u00b9). Experiments on hyperparameter tuning and hyper-representation tasks validate the superiority of C2DFB across various typologies and heterogeneous data distributions.", "sections": [{"title": "Introduction", "content": "Bilevel optimization covers a significant category of hierarchical optimization problems. Solving such problems involves an upper-level problem that depends on the solution of a lower-level problem. This approach has a variety of important applications in machine learning, including hyperparameter optimization (Gao et al. 2022; Ji, Yang, and Liang 2021), meta-learning (Qin, Song, and Jiang 2023; Franceschi et al. 2018), and reinforcement learning (Hong et al. 2023). For instance, when tuning hyperparameters of machine learning models, the model training is considered the lower-level problem, while the selection of hyperparameters is the upper-level problem.\nHowever, solving bilevel optimization in the decentralized learning paradigm is both compute- and communication-intensive, whereas it receives little research attention from existing works. In decentralized environment, each learning node is required to compute the inverse of a Hessian matrix for the lower-level optimization per training round, incurring a complexity order of O(d\u00b3), where d is the model size. Instead of sending gradients, it needs to share its Hessian matrix, which has an order of O(d2) (Tarzanagh et al. 2022) for communication.\nAbove challenges for solving bilevel optimization are further exacerbated by privacy preservation constraints, which give rise to the emergence of decentralized federated learning (DFL) (Bonawitz et al. 2021). DFL is a serverless variant of federated learning (FL) for coordinating multiple nodes to co-train a global model. In DFL, learning nodes perform local model updates and share them directly with one another. Despite its flexibility, efficiently solving bilevel optimization is especially critical for DFL because (Shi et al. 2023): 1) the limited computational resources on learning nodes make solving complex bilevel problems challenging; and 2) the heterogeneous data distribution across nodes coupled with the lack of a central server slows down convergence, leading to increased computational and communication overhead.\nThis paper addresses the problem of m clients collaboratively solving a hyper-objective bilevel problem in a decentralized manner (Chen et al. 2023; Yang, Zhang, and Wang 2022), as formulated in equation (1):\n$\\begin{equation}\n\\begin{aligned}\n\\min_{x \\in \\mathbb{R}^{d_x}} \\Phi(x) &:= \\frac{1}{m} \\sum_{i=1}^{m} f_i(x, y^*(x)), \\\\\ny^*(x) &= \\arg \\min_{y \\in \\mathbb{R}^{d_y}} g(x, y) := \\frac{1}{m} \\sum_{i=1}^{m} g_i(x, y)\n\\end{aligned}\n\\tag{1}\n\\end{equation}$$\nwhere m is the number of nodes in the decentralized topology. In this context, each learning node deals with distinct upper-level (UL) and lower-level (LL) problems, fi and gi respectively, derived from their various local datasets.\nTo tackle the problem above in a resource-friendly manner, we proposes a Compressed Communication-efficient Decentralized First Order Bilvel Optimization method (C2DFB) by relying solely on first-order oracles and transmitting compressed residuals of local parameters. Specifically, our algorithm solves a reorganized min-min-max problem instead of the original bilevel problem, enabling us to use only first-order oracles, which significantly reduces the computational complexity of acquiring the hypergradient compared to the state-of-the-art baselines. To further enhance communication efficiency, we devise the reference"}, {"title": "Related work", "content": "It is non-trivial to approximately derive the stationary point in (1), particularly when the UL function f(x) is noncon-vex and may exhibit non-differentiability or discontinuity. A widely accepted approach in the literature (Lu et al. 2022; Ji et al. 2022) assumes strong convexity in the LL problem with respect to y. Under this condition, it is necessary to use the second-order methods due to the involvement of the Hessian matrix.\nHowever, the process of inverting this matrix introduces nonlinearity computation, making bilevel optimization challenging in decentralized environment because averaging the local hyper-gradients does not exactly equate to the global gradients. Consequently, there is an imperative need to compute and communicate the Hessian matrix of local objectives. Performing such computations often becomes impractical for resource-limited nodes commonly found in decentralized systems (Chen et al. 2023)."}, {"title": "Decentralized bilevel optimization", "content": "Recently, bilevel optimization in decentralized systems receives arising attention due to the emergence of FL, invoking exploration on federated bilevel optimization problems (Tarzanagh et al. 2022; Li, Huang, and Huang 2023; Yang, Xiao, and Ji 2023) .\nFedNest (Tarzanagh et al. 2022) incorporated variance reduction and inverse Hessian-gradient product approximation techniques to improve the applicability of federated algorithms in bilevel optimization. FedBiOAcc (Li, Huang, and Huang 2023) was proposed to efficiently evaluate the hyper-gradient in a distributed setting by formulating it as a quadratic federated optimization problem, which can be accelerated by utilizing momentum-based variance reduction techniques. SimFBO (Yang, Xiao, and Ji 2023) further addressed challenges posed by extensive matrix-vector product calculations in federated settings. This is achieved through updating the UL and LL models in a single loop iteration, aligning with an auxiliary vector for Hessian-inverse vector products via the gradient of a local quadratic function.\nDFL, as a serverless variant of FL, has attracted research efforts to improve bilevel optimization computational and communication efficiency. Chen et al. (2023) proposed to use the Hessian-Inverse-Gradient-Product (HIGP) oracle by leveraging a quadratic sub-solver to approximate the product of the inverse Hessian and gradient vectors without computing the full Hessian or Jacobian matrices. Yang, Zhang, and Wang (2022) introduced a method to estimate this product in a decentralized manner. Specifically, it approximates the Hessian-inverse by recursively computing the Neumann series. However, in some applications (Finn, Abbeel, and Levine 2017; Nichol, Achiam, and Schulman 2018), computing Jacobian/Hessian-inverse vector products heavily consumes more computational and memory resources than obtaining the gradient. This becomes particularly prohibitive in resource-limited decentralized settings.\nIt is worth mentioning that additional efforts have been dedicated to mitigating communication overhead by averting the inner-loop communications through single-loop algorithm design (Dong et al. 2024; Gao, Gu, and Thai 2023). Dong et al. (2024) proposed a single-loop method with an asymptotic rate of O(1/\u221aT). Gao, Gu, and Thai (2023) combined local Neumann-type approximations and gradient tracking to design a single-loop method, though it explicitly required the homogeneous data distribution across all devices. Besides, there is a lack of proof for non-asymptotic stage convergence for loopless algorithms, and the reliability remains unknown.\nBuilding on existing research, we enhance the computational and communication efficiency of bilevel optimization by computing and sharing only compressed first-order gra-"}, {"title": "Preliminary", "content": "The objective of centralized bilevel optimization, as outlined in (1), is to find an e-stationary point of f(x).\n$\\bf{Definition 1.}$ A point x is called an e-stationary point of a differentiable function \u03c8(x) if ||\u2207\u03c8(x)|| \u2264 \u0454.\nTypically, solving this problem queries the second-order information of the ULfunction g to obtain the hyper-gradient \u25bd(x), which however is computationally expensive. A practical approach is to approximate the hyper-gradient by only using gradients. To achieve this, Kwon et al. (2023) reformulates (1) as a constrained optimization problem:\n$\\begin{equation}\n\\begin{aligned}\n\\min_{x \\in \\mathbb{R}^{d_x}} \\Phi(x) &:= \\min_{y \\in \\mathbb{R}^{d_y}} f(x,y) + \\lambda \\left(g(x,y) - g^*(x)\\right), \\\\\ng^*(x) &= \\min_{z \\in \\mathbb{R}^{d_y}} g(x,z).\n\\end{aligned}\n\\tag{2}\n\\end{equation}$$\nIn this formulation, the Lagrangian includes a multiplier \u03bb, where \u03bb > 0. Here, \u2207\u03a6(x) approximates \u2207f(x) with\n$\\begin{equation}\n||\\nabla \\Phi_*(x) - \\nabla \\Phi(x)|| = O\\left(\\frac{\\epsilon}{\\lambda}\\right).\n\\tag{3}\n\\end{equation}$$\nChoosing \u03bb on the order of O(\u20ac\u00af\u00b9) ensures that the e-stationary point of \u03c8\u03bb(x) is also an e-stationary point of \u03a6(x). Denote y\u03bb(x) as\n$\\begin{equation}\ny_{\\lambda}(x) = \\arg \\min_{y \\in \\mathbb{R}^{d_y}} f(x, y) + \\lambda \\left(g(x,y) - g^*(x)\\right).\n\\end{equation}$$\nThen, the closed form of \u2207\u03a6x(x) can be derived using only the first-order gradients of f and g by:\n$\\begin{equation}\n\\begin{aligned}\n\\nabla \\Phi(x) &= \\nabla_x \\Phi_{\\lambda}(x, y_{\\lambda}(x)) + \\nabla_y y_{\\lambda}(x) \\nabla_y \\Phi_{\\lambda}(x, y_{\\lambda}(x)) \\\\\n&= \\nabla_x f(x,y_{\\lambda}(x)) + \\left(\\nabla_x g(x, y_{\\lambda}(x)) - \\nabla_x g(x, y^*(x))\\right) \\nabla_z g(x,y^*(x)).\n\\end{aligned}\n\\tag{4}\n\\end{equation}$$\nModel compression has been widely used to boost communication efficiency in decentralized learning such as sparsification and quantization, which can be customized to combine with our bilevel optimization solution. A general compression operator, known as the contractive compressor, is defined as below (Tang et al. 2018).\n$\\bf{Definition 2.}$ The contractive compression operator Q: Rm\u00d7d \u2192 Rm\u00d7d satisfies\n$\\begin{equation}\n\\mathbb{E} \\left[||Q(A) - A||^2\\right] \\le (1 - \\delta_c)||A||^2\n\\end{equation}$$\nfor all A \u2208 Rm\u00d7d, where \u03b4c \u2208 (0,1] is a constant determined by the compression operator Q."}, {"title": "Decentralized Billevel Optimization", "content": "To facilitate our discussion, we introduce the following notations. The parameters on node i are denoted by (xi)t, (yk)t, and (zh), for the k-th iteration of the inner loop and the t-th iteration of the outer loop.\nWe denote the average of the parameters as follows:\n$\\begin{equation}\n\\bar{x} = \\frac{1}{m} \\sum_{i=1}^{m} x_i, \\quad \\bar{y} = \\frac{1}{m} \\sum_{i=1}^{m} y_i, \\quad \\bar{z} = \\frac{1}{m} \\sum_{i=1}^{m} z_i.\n\\end{equation}$$\nA stacked version of the global parameters is defined as:\n$\\begin{equation}\nx = [X_1, X_2, \\dots, X_m]^T \\in \\mathbb{R}^{m \\times d_x}.\n\\end{equation}$$\nAdditionally, let 1 = [1,\u2026,1]T \u2208 Rm, and || \u00b7 || denote the 2-norm for vectors and the Frobenius norm for a matrix."}, {"title": "System modeling", "content": "We follow the method to reformulate the bilevel problem in (4) to make it feasible for resource-limited decentralized settings. Consider a decentralized graph G consisting of a set of nodes V = {1, 2, . . ., m} and an edge set E. The neighbors of node i are denoted by the set Ni. We define the mixing matrix W as follows and impose common assumptions on the decentralized problem:\n$\\bf{Assumption 1.}$ To align with networks in real-world scenarios, the graph G = (V,E) is connected and undirected, which can be represented by a mixing matrix W \u2208 Rmxm. Let wij be the element in the i\u2013th row and j\u2013th column of matrix W, the following properties hold:\n1) wij > 0 if (i, j) \u2208 E, and wij = 0 otherwise.\n2) W is doubly stochastic, i.e., WT = W\u2122, \u2211mj=1 Wij = 1, and \u2211mi=1 Wij = 1.\n3) The eigenvalues of W satisfy Am < ... < \u03bb2 < \u03bb1 = 1 and \u03bd = max{|\u03bb2|, |\u03bbm|} < 1.\nBased on W, we define the spectral gap, a well-known measure indicating how well nodes are connected.\n$\\bf{Definition 3.}$ For a gossip mixing matrix W that satisfies Assumption 1, we define the spectral gap as \u03c1 = 1 \u2212 \u03b4\u03c1, where \u03b4\u03c1 = max{|\u03bb2(W)|, |\u03bbm(W)|} is the second largest eigenvalue in magnitude.\nWe also make the following standard assumptions regarding the smoothness of the UL and LL problems (Chen, Ma, and Zhang 2023).\n$\\bf{Assumption 2.}$ Recall the definitions of fi and gi in Eq (1). We assume the following:"}, {"title": "Algorithm design", "content": "We embark to present the C2DFB (Compressed Communication-efficient Decentralized First Order Bilvel Optimization) method. As described in Algorithm 1 - 2, the algorithm operates in two main loops.\nIn each iteration of the Outer loop, the UL model xi is updated using the gradient tracker (si). This update incorporates a weighted averaging, also known as the mixing step, which blends parameters from neighboring nodes with the node's local parameters. Subsequent to this, the inner loop proceeds to update the LL model yi and zi. Following this, the estimation of UL gradients (u)+1 is computed to update the tracker (si)+1. For a better clarity, default hyperparameters such as Vin and Nin are omitted when invoking the IN function.\nWithin the inner loop, a three-step optimization is conducted below. First, each training step begins with updating model di(which may represent either Yi or zi) using the gradient tracker. A consensus mechanism refines the update by computing a weighted average of the models from neighboring nodes, controlled by the mixing step size \u03b3. \u03b3 increases the adaptability of the system to reach consensus while providing stability against aggressive compression strategies, particularly under the heterogeneity of local objectives.\nIn the second step, a local reference point d\u2081 is updated by incorporating a compressed residual. This residual compresses the deviation between the current model parameter di and the reference point di. As the training advances, d\u2081 increasingly aligns with di, evidenced by the diminishing magni-"}, {"title": "Value functions framework", "content": "Our convergence analysis is grounded on Lyapunov functions. Given that bilevel problem comprises two loops, we delibrately design Lyapunov functions for each loop. For the outer-loop, we design the following Lyapunov functions:\n$\\begin{equation}\nModel consensus error: \\Omega_1 := ||\\bar{x}^{t} - 1\\bar{x}^{t} ||^2\n\\end{equation}$$\n$\\begin{equation}\nTracker consensus error: \\Omega_2 := ||\\bar{s}^{t} - 1\\bar{s}^{t} ||^2\n\\end{equation}$$\n$\\begin{equation}\nValue function: \\Omega^{t} \\triangleq \\psi(x^{t}) + \\frac{1}{m}\\Omega_1 + \\frac{\\eta_{out}}{m}\\Omega_2\n\\tag{8}\n\\end{equation}$$\nAs for the inner-loop, our Lyapunov function are delibrately designed as:\n$\\begin{equation}\n\\begin{aligned}\n&Model compression error: \\Omega_3 := ||d^* - \\bar{d}^{k}||^2 \\\\\n&Model consensus error: \\Omega_4 := ||d^{k} - 1\\bar{d}^{k}||^2 \\\\\n&Model compression error: \\Omega_5 := ||\\bar{s} - \\hat{s}||^2\\\\\n&Tracker consensus error: \\Omega_6 := ||s^{k} - 1\\bar{s}^{k}||^2 \\\\\n&L_r \\kappa_r(d^{k}) + \\frac{\\lambda^2}{mL_r}\\Omega_3 + \\frac{\\delta_c}{\\eta \\nu L_r }\\Omega_4 + \\frac{1}{mL_r}\\Omega_5 + \\frac{\\eta}{mL_r^4}\\Omega_6,\n\\end{aligned}\n\\tag{9}\n\\end{equation}$$\nTo make our presentation concise, d can denote y or z. r may represents the objective function h and g. Lr is the corresponding Lipschitz parameter 2XLg and Lg respectively. The core idea behind these designed functions is that the errors recursively intertwine with each other. Through a value function, we can unify these complex relationships by progressively decreasing the value function, such that the algorithm converges to a stable state within a finite number of iterations. We will briefly outline the proof with key lemmas and highlight the final result."}, {"title": "Convergence Results", "content": "We first provide the following lemma which shows that 4x(x) is an effective proxy of f(x)."}, {"title": "Experiments", "content": "We evaluate our algorithm by processing a hyperparameter tuning task using the 20 Newsgroups dataset, following the methodology outlined in Liu et al. (2022); Kong et al. (2024). This dataset contains 101,631 features, making the computation and storage of the Jacobian/Hessian matrix impractical. In this task, UL and LL functions are defined as follows:\n$\\begin{equation}\n\\begin{aligned}\nf_i(x, y) &:= \\frac{1}{|\\mathcal{D}_{val}|} \\sum_{(a_i,b_i) \\in \\mathcal{D}_{val}} l((a_i, y), b_i), \\\\\ng_i(x, y) &:= \\frac{1}{|\\mathcal{D}_{tr}|} \\sum_{(a_i,b_i) \\in \\mathcal{D}_{tr}} l((a_i, y), b_i) + \\gamma \\text{diag}(\\exp(x)) y,\n\\end{aligned}\n\\end{equation}$$\nwhere l(,) denotes the cross-entropy loss. Dval and Dtr represent the validation and training datasets, respectively. In this context, we employ a linear classifier as the model, with the objective of optimizing its coefficients through bilevel optimization.\nFor this experiment, we set the learning rates for both the outer and inner loops to 1, the consensus step to 0.5, and \u03c3 to 10. The experiment was conducted on a network of 10 nodes using PyTorch multiprocessing. We evaluate three network topologies: a ring topology with each node linked to its two immediate neighbors; a 2-hop topology, connecting nodes to their neighbors' neighbors; and an Erdos-Renyi (ER) topology, randomly forming edges between nodes with a probability of p = 0.4. Our results, depicted in Figure 2, show that C2DFB achieves faster convergence than these baselines across all topologies. Additionally, we assess performance in a data-heterogeneous setting where 80%(h) of each class's data is allocated to a specific client, with the remaining distributed among others."}, {"title": "Hyper-Representation Learning", "content": "We further evaluate our algorithm on a hyper-representation learning task, as described in Tarzanagh et al. (2022). This task aims to enhance the model's feature representation and improve its performance on downstream tasks. The outer objective focuses on refining the model backbone, while the inner objective optimizes a smaller, task-specific head responsible for classification on the training data. We implement this task using a three-layer multilayer perceptron on the MNIST dataset, where the outer optimization targets the hidden units with 81,902 parameters, and the inner optimization focuses on the classification head, comprising approximately 640 parameters.\nTo demonstrate the impact of our reference point compression technique, we introduce a baseline variant, C2DFB(nc), which simply compresses the transmitted pa-"}, {"title": "Conclusion", "content": "This paper presents the C2DFB algorithm, a novel first-order gradient-based method developed for decentralized bilevel optimization. It addresses both computational and communication challenges by employing a reference point-based compression strategy that substantially lowers communication requirements. Our theoretical analysis validates the con-"}]}