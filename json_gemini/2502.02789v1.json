{"title": "Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation", "authors": ["Jingyu Liu", "Beidi Chen", "Ce Zhang"], "abstract": "Improving time-to-first-token (TTFT) is an essentially important objective in modern large language model (LLM) inference engines. Because optimizing TTFT directly results in higher maximal QPS and meets the requirements of many critical applications. However, boosting TTFT is notoriously challenging since it is purely compute-bounded and the performance bottleneck shifts from the self-attention to the MLP part. We present SPECPREFILL\u00b9, a training free framework that accelerates the inference TTFT for both long and medium context queries based on the following insight: LLMs are generalized enough to still preserve the quality given only a carefully chosen subset of prompt tokens. At its core, SPECPREFILL leverages a lightweight model to speculate locally important tokens based on the context. These tokens, along with the necessary positional information, are then sent to the main model for processing. We evaluate SPECPREFILL with a diverse set of tasks, followed by a comprehensive benchmarking of performance improvement both in a real end-to-end setting and ablation studies. SPECPREFILL manages to serve Llama-3.1-405B-Instruct-FP8 with up to 7\u00d7 maximal end-to-end QPS on real downstream tasks and 7.66\u00d7 TTFT improvement during benchmarking.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) represent a transformative innovation in artificial intelligence, enabling machines to understand and generate human-like languages (Bubeck et al., 2023; Wei et al., 2022; Feng et al., 2024). Many SOTA models have been developed, such as GPT-4 (OpenAI et al., 2024), the Llama family (Grattafiori et al., 2024), DeepSeek R1 (DeepSeek-AI et al., 2025), Mistral (Jiang et al., 2023a), Gemini (Team et al., 2024), and Qwen2 (Yang et al., 2024), to meet the increasing expectations of users. In order to broaden their real-world applications, one essential requirement is to build an efficient serving engine that can satisfy various requirements (Miao et al., 2023; Kwon et al., 2023; Zheng et al., 2024; Shoeybi et al., 2020).\nThere are several fundamental reasons why TTFT stands so pivotal: 1) many applications require a fast response time that directly influences how users perceive the responsiveness of the system and 2) more importantly, TTFT determines the scaling of maximal QPS an inference engine can support as shown in Figure 1. However, optimizing TTFT is an arduous task mostly because the prefill stage is largely compute-bounded and the computational bottleneck can change depending on the prompt length and batch size. For example, many works focus on improving the self-attention speed (Dao et al., 2022; Jiang et al., 2024a), but in reality, there is still a huge traffic of large-batch short to medium context queries where it is the MLP part that clogs the whole system. Despite achieving impressive results, prior works that target the prefill phase either require a post-training adaptation (Qiao et al., 2024; Horton et al., 2024) or scale less efficiently (Shi et al., 2024).\nInspired by those work, we found a key insight that LLMs can retain most of its performance when given only a carefully chosen subset of tokens from the prompt, and the model is able to adapt to that in a zero-shot manner. SPECPREFILL optimizes the TTFT by leveraging a secondary lightweight model to speculate locally important tokens. Only these tokens are sent later to the base model. It reduces the total FLOPS by a factor proportional to the percentage of token drop. SPECPREFILL does not require any fine-tuning and is ready to be deployed and scaled to larger models. We summarize our key contributions:\n\u2022 We present a conceptually simple, effective, and novel framework called SPECPREFILL that significantly"}, {"title": "2. Background", "content": "Improving LLM inference efficiency has been extensively studied in prior work (Miao et al., 2023; Yuan et al., 2024). We review works that focus on different aspects when dealing with real serving systems where the bottlenecks are quite different under various serving requirements (e.g. long context domains, latency sensitive applications, etc) (Kwon et al., 2023; Zheng et al., 2024).\nLLM inference can be roughly divided into two major procedures, namely the prefill phase where the model computes the KV necessary for producing the output based on the query and the decoding phase where the model predicts new token auto-regressively."}, {"title": "2.1. Inference Bottlenecks", "content": null}, {"title": "2.2. Decoding Acceleration", "content": "The decoding phase is mostly memory-bounded, and therefore reducing the amount of data to move around will effectively help improve the latency. As a result, explicitly manipulating the KV cache has been extremely successful with many strategies: H2O (Zhang et al., 2023) and StreamingLLM (Xiao et al., 2024b) idenfitied key insights to KV dynamics which is used to evict less essential KV caches during decoding. CacheGen (Liu et al., 2024), Q-Hitter (Zhang et al., 2024b), and ShadowKV (Sun et al., 2024) apply efficient techniques to compress/quantize, store, and transmit KV caches to reduce memory overhead. Speculative decoding relies on the insight that hides the memory latency by concurrently verifying several speculated tokens from either a draft model or itself (Leviathan et al., 2023; Zhang et al., 2024a; Xia et al., 2024).\nDespite being crucially important, decoding speed is not the only factor that influences the overall inference pipeline and we will review why sometimes the prefill time optimization is even more essential in many cases."}, {"title": "2.3. Prefill Acceleration", "content": "The time-to-first-token (TTFT) is crucially important both from a user experience but also the system serving perspective (Sec 4.6). Unlike the decoding phase, the prefill phase is usually compute-bounded and its bottleneck can shift between the attention module and the MLP based on the prompt sequence length. Moreover, the input token length can often eclipse that of the generation tokens (e.g. 10:1 ratio) in real traffic (Qiao et al., 2024).\nMany works have explored ways to make self-attention faster: Flash-attention series (Dao et al., 2022; Dao, 2023) computes the exact attention using carefully designed hardware-aware algorithms. Special attention masks are designed for sparse calculation such as LongFormer (Beltagy et al., 2020), MInference (Jiang et al., 2024a) and Duo-attention (Xiao et al., 2024a). However, when the sequence lengths become shorter, the MLP computation quickly becomes dominant and the gains from attention acceleration are deflating (Xiong et al., 2023).\nOrthogonal to techniques such as prompt compression/rewrite (Jiang et al., 2023b; 2024b), layer dropping (Elhoushi et al., 2024), and weight quantization methods (Lin et al., 2024), we explore selecting important prompt tokens to skip the full forward computation. GemFilter (Shi et al., 2024) uses an extra pass to get a model's own middle layer attention information that decides on what tokens to keep for the real forward. Contrast to this, we apply a separate and cheaper model to speculate locally important tokens via token transferability, which can scale more efficiently than GemFilter. Concurrent to ours, SwiftKV (Qiao et al., 2024) learns to skip later layers by reusing the past layers' KV, which achieves up to 50% TTFT reduction (SPECPREFILL can reach up to 87% TTFT reduction). Unlike our zero-shot requirement, they require extra light-weight fine-tuning due to modified model behavior. It is worth noting that our method approaches the problem in a different way, which makes them complimentary to each other. Finally, akin to our motivation, KV Prediction (Horton et al., 2024) proposes to adapt a cheaper system (i.e. a learned auxiliary network and a KV predictor) to predict the KV cache of the base model, thus bypassing the original KV computation. We show that SPECPREFILL can accomplish better TTFT reduction than theirs, without introducing extra overhead when coupled with speculative decoding (Leviathan et al., 2023) while maintaining competitive quality."}, {"title": "3. Speculative Prefill", "content": "In this section, we present SPECPREFILL by first describing its high-level algorithm, followed by several design choices that mitigate various biases, and a detailed implementation account. Finally, we touch a bit on how to integrate SPECPREFILL to speculative decoding, forming a full small-model-assisted inference paradigm."}, {"title": "3.1. Overall Architecture", "content": "SPECPREFILL follows a conceptually simple architecture where a usually less expensive model is chosen as the speculator model that predicts contextually important tokens given a prompt. The speculated tokens, alone with the original position information, are then fed to the main model for processing. In the following section, we will discuss two central design choices in more details, namely the token estimation algorithm and the selection strategy. Note that SPECPREFILL can be seamlessly integrated with speculative decoding in which the small model can work both in the prefill stage for token selection and the decoding stage for drafting proposals, making our approach almost free to integrate and deploy."}, {"title": "3.2. Token Importance Speculation", "content": "The goal here is to select which tokens are contextually important for a given query and send those along with necessary positional information for the main model. The procedure starts with calculating the attention scores from the speculator, which uses the last token's attention score w.r.t. the context as the surrogate for measuring token importance:\n$a_{ij} := Softmax(QM+jKT))_i, \u22000 < i < M, 0 \u2264 j < N$\nwhere M is the context length, N is the number of look-ahead steps, and aij is the attention score for the ith token in the prompt w.r.t. the jth decoded token, assuming we're looking at a particular layer.\nWe build on top of this by aggregating the scores over the whole speculator model (Section 3.2.2) with potential look-ahead (Section 3.2.1) and select tokens based on chunks (Section 3.2.3). The subset of chosen tokens with their original positional information (Section 3.2.4) will then be used for the main model's inference."}, {"title": "3.2.1. \u039c\u0399\u03a4IGATE POSITION BIAS VIA LOOK-AHEAD", "content": "Prior works have shown that there are many biases for attention scores, such as the sink phenomenon (Xiao et al., 2024b) (the first couple of tokens tend to have higher weights) and the proximity bias (tokens closer to the output tend to have higher weights (Lv et al., 2024)). To mitigate these issues, instead of relying on the attention score of the last token alone, we further decode the speculator by N steps and obtain the attention information from the new N tokens (Wan et al., 2024). N here serves as a trade-off between bias and budget, which can substantially increase the performance for shorter context queries."}, {"title": "3.2.2. AGGREGATED ATTENTION SCORE AS TOKEN IMPORTANCE", "content": "Given the full attention scores of the speculator, we decide to use a max-mean aggregation strategy to map to scalar token importance. Formally, given an attention score tensor"}, {"title": "3.2.3. DENOISE ATTENTION SCORES BY CHUNK SELECTION AND POOLING", "content": "It has also been observed in concurrent works (Lv et al., 2024) that tokens that are positioned nearby share similarity in importance. We take this insights to select tokens by chunks in order to reduce the variance of our token importance estimation. Specifically, we chunk the context contiguously and average the token score within each block, and then we select the Top-K blocks. In order to eliminate the artifacts of chunkation, we apply a 1D average pooling before this to smooth the cross block scores."}, {"title": "3.2.4. RESTORATION OF POSITION IDS", "content": "Finally, when we select the subset of tokens based on our compute budget and query compressibility, we also need to restore the position information which are also sent to the main model. Basically, instead of using a contiguous position ids as before, we send a potentially non-continuously increasing position ids which are obtained from tokens' positions in the original context. In addition to that, we also need to explicitly set the decoding token position to the context length in case we dropped tokens before the first decoding token. An example is shown below with ten prompt tokens and three decoding tokens (bold):\nOriginal Pos Ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nSpeculated Pos Ids: [0, 1, 3, 6, 7]\nDecoding Pos Ids: [0, 1, 3, 6, 7, 10, 11, 12, ...]\nwhere the bold indices are the decoding positions which are offset based on the original position information. We found this design choice to be crucially essential, especially for position-sensitive tasks such as synthetic tasks involving retrieval and counting."}, {"title": "3.3. Implementation Details", "content": "We describe both the high-level procedure and the implementation details of SPECPREFILL in this section. In Algorithm 1, we list the high-level steps of conducting SPECPREFILL. Our implementation is based on creating a monkey patch on top of vLLM (Li et al., 2024a) which only needs a few line of code along with a configuration file to enable SPECPREFILL. The KV cache is not necessary if we do not need to look-ahead for our speculator, which can save lots of memory allocation. However, we do need to explicitly store the queries of the decoded tokens (including the last token of the input query) which we later retrieve to compute the attention score. Note that a specific mapping (e.g. slot mapping in vLLM) might be kept track of to retrieve the right data. For batched look-ahead, we only consider tokens that are valid by checking afterwards whether they are equal to the EOS tokens. Finally, we want to mention that despite being being a sequential implementation, we can actually split the process of speculation into a separate procedure and decouple from the inference of the main model by adding a new layer of scheduling, which we leave as a future work."}, {"title": "3.4. Relation to Speculative Decoding", "content": "Speculative decoding has been proven to be extremely successful at accelerating the decoding TPS. SPECPREFILL can be seamlessly combined with speculative decoding by sharing the same draft model. Since speculative decoding itself requires a full forward pass of the context, SPECPREFILL will provide the necessary KV information required for subsequent decoding speculation, hence amortizing the overhead. This will open-up a huge space of possibilities, and leading to the first paradigm of an inference system that is fully aided by smaller speculators."}, {"title": "4. Experiments", "content": "In this section, we start with our experiment setup for reproducibility, followed by categorizing prompt compressibility of different queries. We evaluate SPECPREFILL on downstream long context, synthetic context probing, and standard short tasks. Finally, we conclude with a comprehensive efficiency measurement of our system under the real end-to-end setting."}, {"title": "4.1. Setup", "content": "We implement SPECPREFILL in vLLM that supports tensor parallelism with the same degree as the main model2. Due to its token dropping nature, we focus on evaluating generative tasks in this section and include a comprehensive range of benchmarks to fully present its applicability and potential pitfalls. We run all of experiments using a tensor parallelism of 8 for both the speculator and the base model across either 8 NVIDIA H100s or H200s (full system specification in Appendix D and guidance on reproducing results in Appendix C). We choose LLAMA-3.1-8B-INSTRUCT (Grattafiori et al., 2024) with BF16 precision as our speculator for a balance of efficiency and attention transferability and couple it with either Llama-3.1-70B-Instruct (BF16) or Llama-3.1-405B-Instruct-FP83 (fully quantized FP8) as the base model. In terms of token keep rate, we use a fixed percentage (i.e. the ratio of chunks when we do chunk selection) for a given task. In practice, we might devise more adaptive strategy for how many tokens to keep based on the query compressibility discussed next, or delegate the decision to users based on their needs. We leave all these possibilities for prospective applications."}, {"title": "4.2. Query Context Compressibility", "content": "We empirically found three types of queries during our evaluations based on the quality difference before and after applying SPECPREFILL:\n1. Information-dense queries: These queries usually are short and information dense, which naturally makes token dropping less effective because there is no redundancy in the prompt.\n2. Compressible queries: These queries are those that do not get degradation after removing a significant amount of tokens, often seen in long context tasks.\n3. Noisy queries: These queries, perhaps surprisingly, get better results after dropping some \"noisy\" tokens. We hypothesize the reason behind the improvement might be that SPECPREFILL helps remove noisy and distracting tokens in the prompt, hence projecting the prompt to the space where the main model performs better."}, {"title": "4.3. Real Long Context Tasks: LongBench", "content": "We start with long context tasks using LongBench (Bai et al., 2024), which consists of six different categories focusing on various aspects of long context modeling abilities. For this benchmark, we compare two versions of our techniques against three baselines to showcase the trade-offs between using more speculative computation and quality:\n\u2022 Baseline: the original Llama model we evaluate.\n\u2022 Sentence RAG: SPECPREFILL can be framed as a special case of retrieval-augmented (RAG) LLM with the granularity of tokens or blocks and the relevance metric controlled by the speculator's internal knowledge (Li et al., 2024b; Gao et al., 2024b; Lewis et al., 2021). Therefore, we implemented two simple sentence-level RAG baselines to compare SPECPREFILL against, which we call RAG-LLAMA LS and RAG-LLAMA EQ (more detailed description in Appendix B).\n\u2022 SPECPREFILL: SPECPREFILL with raw attention scores and ignoring the techniques we discussed in Section 3.2.1 and 3.2.3.\n\u2022 SPECPREFILL Full: SPECPREFILL with all techniques but no look-ahead.\n\u2022 SPECPREFILL Full LAH: SPECPREFILL with all techniques with 8-step look-ahead\u2074."}, {"title": "4.4. Synthetic Context Probing: RULER", "content": "In addition to LongBench, we also evaluate SPECPREFILL on a synthetic context probing task to see if SPECPREFILL can preserve effective context lengths. RULER (Hsieh et al., 2024) is a suite of synthetically created tasks with controllable lengths, which ranges from retrieval, multi-hop tracking, real QA datasets, and context aggregation tasks."}, {"title": "4.5. Standard Short Tasks", "content": "Unlike prior works on prefill token dropping techniques (Lv et al., 2024; Shi et al., 2024) that do not include regular short context task evaluation, we present a wide range of standard tasks to show the full spectrum of SPECPREFILL'S performance and potential caveats. We select tasks spanning general knowledge (Generative MMLU (Hendrycks et al., 2021) and Instruction Following Evaluation (Zhou et al., 2023)), math (GSM8K 8 Shots (Cobbe et al., 2021)), coding (HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)), and reasoning abilities (Arc Challenge (Clark et al., 2018) and GPQA 8 Shots (Rein et al., 2023)).\nIn Figure 5, we showcase the performance of Llama-3.1-70B-Instruct on these tasks. Non-surprisingly, prompts from standard tasks without few shot examples are very information dense, making SPECPREFILL less effective with low token keep rate. However, for certain tasks (e.g. MBPP and GPQA), we do observe"}, {"title": "4.6. Efficiency Benchmarking", "content": "SPECPREFILL offers great improvement to TTFT, a speedup almost proportional to the percentage of tokens we drop from the speculator, with almost ignorable overhead as we increase the base model size. In this section, we benchmark both the 70B and 405B models under two settings: 1) understanding the average query latency and QPS dynamics with real downstream datasets, and 2) evaluating TTFT with varying sequence lengths on synthetic data. We used one node consisting of eight NVIDIA H200s for all experiments (full system specification is listed in Table 3 from the Appendix D)."}, {"title": "4.6.1. AVERAGE QUERY LATENCY UNDER DIFFERENT QPS WITH REAL DOWNSTREAM DATASETS", "content": "We want to measure the real performance gain we can create in an end-to-end fashion. To do this, we launch a vLLM server with a given model, and an OpenAI API client 6 that sends asynchronous requests at a constant QPS with queries from datasets in LongBench. We measure the client-side per query latency that consists of the prefill stage with several decoding steps based on the maximum budget defined by the task. In Figure 1, we increase the QPS of our client and calculate the average query latency with a given fixed timeout to simulate real-world user needs. For each task category, we draw samples randomly from each subtask and shuffle them before starting the querying, making sure the same set of queries is used over all QPS. As we can observe, all models will follow a standard three-stage pattern: 1) the initial constant stage where latency remains almost unchanged as all queries can be finished before receiving new ones, 2) the middle linear stage where TTFT is small enough but the decoding step might not finish fast enough, and 3) the final timeout stage where the server can not even finish the prefill stage before new requests and all subsequent"}, {"title": "4.6.2. TTFT IMPROVEMENT OVER DIFFERENT BATCH SIZE X SEQUENCE LENGTH PRODUCTS", "content": "We try to understand the dynamics of SPECPREFILL under different batch-size-sequence-length products and keep percentage while isolating the advantage of TTFT. We use the official script from vLLM for latency benchmarking. In Figure 3 and 4.6.2 (with look-ahead), we highlight the TTFT speedup against the vanilla base model without SPECPREFILL, which we produce by setting the maximum decoding step to be 1. As we can see, for both the 70B and 405B models, not only do we see more direct effects of SPECPREFILL but also imply the increasing scaling along the sequence dimension. As the relative FLOPS ratio between the speculator and main model becomes larger, the overhead of speculation starts to become more negligible, which leads to more substantial improvement. In order to better understand the whole system, we include theoretical analysis of the overhead and the performance gains in Appendix E.\nIf we have finitely large timeout, we would expect SPECPREFILL to support around N times larger maximal QPS if we reduce TTFT by N times."}, {"title": "5. Limitation", "content": "One of the main limitations of SPECPREFILL, like all tokendropping based method, does not support explicit logit outputs for all input tokens, and hence we focus on generative evaluation. Given that we have the full knowledge of the speculator, we advertise for more principled method to estimate token importance beyond attention scores. Finally, we believe that a robust algorithm that determines how many tokens are required for a given prompt will be of great use for SPECPREFILL."}, {"title": "6. Impact Statement", "content": "This paper presents work whose goal is to accelerate large language model inference procedures. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "7. Conclusion", "content": "In this work, we introduce SPECPREFILL, a training-free framework for accelerating the LLM inference by speculating what tokens to drop with the help of a smaller speculator model. Leveraging the insight that models of different sizes within the same family can usually transfer token importance, SPECPREFILL not only achieves substantial improvement on TTFT, which leads to 7\u00d7 maximal supported QPS of an inference system, but also reduces the memory required. SPECPREFILL can also be readily combined with other techniques such as speculative decoding, the combination of which could result in the first unified small-model-assisted inference pipeline. With extensive evaluations, we believe that SPECPREFILL will be one of the practical answers to large-scale LLM inference systems."}, {"title": "A. Standard Short Task Performance of SPECPREFILL", "content": "In Figure 5, we report SPECPREFILL when applied to Llama-3.1-70B-Instruct on standard short tasks as discussed in Sec 4.5. It is worth noting that for shorter tasks, the queries are more likely to become information dense, rendering SPECPREFILL less effective especially for certain tasks."}, {"title": "B. Comparing SPECPREFILL with RAG Based Systems", "content": "In this section, we first detail the algorithm behind two of our RAG baselines and present results comparing SPECPREFILL against them. Both variants split the context of the prompt into sentences using nltk library (Loper & Bird, 2002). After splitting the context, all sentences are encoded using pretrained sentence embedding models (Reimers & Gurevych, 2019). A specially chosen query is used to select relevant sentences based on similarity scores without exceeding the predefined budget. Finally, the new context is re-assembled and fed to the main model. We highlight the key differences in various steps of the pipeline in the following table 2:"}, {"title": "C. Experiment Details", "content": "There are some details for our experiments that we wish to give some accounts for:\n1. For standard short task evaluation in Sec 4.5, we use LM-EVAL-HARNESS (Gao et al., 2024a) and EVAL-PLUS (Liu et al., 2023). For several tasks in LM-EVAL-HARNESS, we include task configuration files for Llama-3.1 based on its templates in our code base, which should be placed in the right place for reproducing experimental results.\n2. In the QPS experiment in Sec 4.6, we add an extra 5 seconds to the timeout in order to avoid potential system instability. The final results are reported with the original timeout. We set the number of samples for each category based on the maximum QPS we want to evaluate, which makes sure we have a constant QPS during the duration of querying.\n3. When running all experiments in vLLM (0.6.3.post1), we set enforce_eager=True and enable_chunked_prefill=False to avoid any unexpected behaviors."}, {"title": "D. System Specification", "content": "In Table 3, we list the detailed specification of the system on which we test the efficiency of models."}, {"title": "E. Overhead Analysis", "content": "SPECPREFILL uses a smaller model as a speculator to help accelerate the larger model. Although proven to be effective, there is no free lunch. In this section, we analyze and quantify the overhead incurred by the speculator so that practitioners are more informed when they choose to use it."}, {"title": "F. LongBench Task Length Distribution", "content": "We visualize the LongBench suite's average length for each task, which, when coupled with the token keep rate, provides a more clear picture of the model's efficiency gain."}]}