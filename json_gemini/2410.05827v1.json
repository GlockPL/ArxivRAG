{"title": "Towards an Operational Responsible Al Framework for Learning Analytics in Higher Education", "authors": ["ALBA MORALES TIRADO", "PAUL MULHOLLAND", "MIRIAM FERNANDEZ"], "abstract": "Universities are increasingly adopting data-driven strategies to enhance student success, with AI applications like Learning Analytics (LA) and Predictive Learning Analytics (PLA) playing a key role in identifying at-risk students, personalising learning, supporting teachers, and guiding educational decision-making. However, concerns are rising about potential harms these systems may pose, such as algorithmic biases leading to unequal support for minority students. While many have explored the need for Responsible AI in LA, existing works often lack practical guidance for how institutions can operationalise these principles. In this paper, we propose a novel Responsible Al framework tailored specifically to LA in Higher Education (HE). We started by mapping 11 established Responsible Al frameworks, including those by leading tech companies, to the context of LA in HE. This led to the identification of seven key principles such as transparency, fairness, and accountability. We then conducted a systematic review of the literature to understand how these principles have been applied in practice. Drawing from these findings, we present a novel framework that offers practical guidance to HE institutions and is designed to evolve with community input, ensuring its relevance as LA systems continue to develop.", "sections": [{"title": "1 Introduction", "content": "Learning Analytics (LA)\u00b9 are becoming increasingly central to higher education institutions worldwide. LA systems\nutilise data to identify at-risk students, support student development, provide personalised and timely feedback, support\nself-reflection, and enhance the quality of learning and teaching [47, 49].2\nHowever, the adoption of AI-powered systems within Higher Education (HE) brings with it a range of ethical\nconcerns. Issues like algorithmic bias, lack of transparency, and potential misuse can have serious implications. For\nexample, systems that automatically identify students at risk may suffer from algorithmic biases and disproportionally\nunder-detect students from certain minority groups, leading to those students not receiving equivalent support to the\nmajority group [7]. Other concerns include the need for these automated systems to be able to explain their decisions\nor the safe and transparent usage of student data. Addressing these issues is crucial to ensure that AI technologies are\ndeployed in a manner that is fair, equitable, and responsible.\nIn response to the need of addressing the ethical concerns of AI deployment, tech companies and other organisations\nhave developed Responsible Al frameworks to guide the design and development of AI. These frameworks provide\nguiding principles such as fairness, transparency, accountability, and data privacy to ensure that AI systems are\nbuilt in a way that minimises harm and maximises societal benefit. However, while Responsible AI frameworks are\nessential for guiding the design, development and deployment of AI technologies, these frameworks are frequently\ndesigned following high-level concepts and principles that can be applied to any AI application, without considering the\nspecificities of the technology or the environment in which the Al system will be deployed. Similarly, numerous works"}, {"title": "2 Motivation", "content": "Higher Education Institutions (HEIs) face several ethical challenges when integrating Artificial Intelligence (AI) into\ntheir operations, teaching, research, and administrative functions. These challenges stem from the complexity of\nAl technologies, the sensitivity of academic and student data, and the societal implications of widespread AI use in\neducation. The spectrum of technologies used is also broad [12], from Generative AI applications that help to generate\nnew curricula, to AI that can monitor attendance, to LA solutions that could identify students at-risk.\nLA systems in particular introduce numerous ethical challenges, especially given their growing use in HE to enhance\nstudent success and institutional efficiency. These challenges often stem from the use of vast amounts of student data\nand algorithmic predictions that can impact decision-making in educational settings. Below is a brief discussion of\nsome of the key ethical issues associated with the use of LA in HE. For a broader overview of the problem, the reader is\ndirected to the following literature [1, 18, 35, 37, 45, 51].\nOne key ethical issue in LA systems is bias. For instance, a predictive model might unfairly classify students from\ncertain demographics as at-risk or not, leading to unequal treatment and opportunities. If LA decisions guide resource\nallocation or interventions, some students may receive more support, while others are overlooked. LA systems often\ninherit bias from the training data [38], reinforcing social inequalities. Bias can also be introduced during data processing,\nresulting in different levels of support for students based on factors like race, gender, disability, or socioeconomic status"}, {"title": "3 Analysing Existing Responsible Al Frameworks", "content": "To address our first two research questions: (i) RQ1: To what extent do existing Responsible Al frameworks address the\nspecific needs and challenges of Learning Analytics in Higher Education? and (ii) RQ2: Which Responsible Al principles from\nexisting frameworks are applicable to the context of Learning Analytics in Higher Education?, we followed a comparative\nanalysis of eleven well-established Responsible Al frameworks.\nOur methodology began by clearly defining our research objectives and scope. To ensure a comprehensive and\ndiverse set of frameworks, we conducted an extensive search using multiple queries through Google's search engine.\nKey search terms included 'Responsible AI framework,' 'ethical AI adoption,' 'ethical AI in education,' and 'responsible\nAl in education.' For each query, we reviewed the top 30 results to capture a broad spectrum of frameworks across\nindustry, government, and the education sector. Additionally, we integrated findings from recent literature reviews of\nResponsible Al frameworks [6, 55], ensuring the inclusion of widely recognised frameworks from leading technology\ncompanies like Microsoft, Amazon, and Google.\nGiven that many Responsible Al frameworks are not published as traditional academic papers, but rather proposed\nby industry, government, or third-sector organisations, we opted not to limit our search to scholarly databases. This\napproach allowed us to capture the most relevant and practical frameworks beyond academic literature.\nFor inclusion in our analysis, we applied the following eligibility criteria:\n\u2022 Documents must be written in English.\n\u2022 Frameworks must address the ethical use of data or software development practices, considering the ethical,\nlegal, and social challenges related to AI design, development, and adoption.\n\u2022 Documents describing policies, guidelines, or codes of practice, rather than full frameworks, were included if:\n(i) they specifically targeted the education sector, or (ii) they were produced by leading technology companies.\nThis systematic process ensures a robust and diverse dataset, allowing for a thorough analysis of Responsible AI\nprinciples relevant to Learning Analytics in Higher Education. As detailed in Table 1 we identified eleven relevant\ninitiatives launched by organisations in different domains. The table details: (i) the originating organisation, (ii) the\ndocument's name and URL, (iii) the primary focus of the document (Artificial Intelligence, Machine Learning, Data,\nLearning Analytics, or Predictive Learning Analytics), (iv) the document type (framework, policy, principles, code\nof practice, or guidance), (v) the year of release, (vi) the number of Responsible Al principles discussed, and (vii) the\ncontext (domain, country, sector) in which the principles are applied. Although marginally relevant, we excluded the\nSHEILA Framework [55], as it did not fully meet our eligibility criteria, being primarily focused on strategic planning\nand policy processes for Learning Analytics."}, {"title": "4 Analysing LA works with respect to Responsible Al principles", "content": "We address in this section the third research question (RQ3): How have previous Learning Analytics studies incorporated\nResponsible Al principles in practice? To answer this, we conducted a systematic literature review of relevant studies.\n4.1 A Systematic Literature Review\nWe initiated our systematic literature review by identifying key terms derived from our research questions and the\nethical principles discussed (see Table 2). We compiled a list of synonyms to create a comprehensive search string using\nBoolean operators (AND, OR). The structured search query was formulated as follows: {domain of interest} + {area of\nimplementation} + {principles} + {focus}. The resulting search string was defined as ('learning analytics' OR 'predictive\nlearning analytics') AND ('higher education') AND (fairness OR transparency OR privacy OR accountability OR safety OR\nexplainability OR ethics OR 'responsible AI') AND (framework OR guideline OR policy OR 'code of practice' OR principles\nOR 'best practice' OR implications OR 'lessons learn').\nThis search string was applied across three digital libraries-ERIC https://eric.ed.gov/, SCOPUS https://www.scopus.\ncom, and ACM https://dl.acm.org -selected for their relevance to our study. Searches were conducted on titles, abstracts,\nand keywords. We obtained: ERIC (54), Scopus (70) and ACM (110) results from each library. Before selection, we\nremoved duplicate results. Subsequently, we established robust inclusion and exclusion criteria based on our research\nquestions (see Table 3). We focused on studies that explore the responsible adoption of LA. This included papers\ndetailing lessons learned from LA implementations, identifying challenges faced by Higher Education Institutions\n(HEIs), and proposing strategies to address these issues, including policy and guideline development.\nThe study selection followed a three-stage process: (a) reviewing the titles and abstracts, (b) reading the introductions\nand conclusions, and (c) evaluating the full text. At each stage, documents were categorised into three groups: 'important,"}, {"title": "4.2 Findings", "content": "The publication years of the selected papers range from 2013 to 2024. The selected studies include journal articles, book\nchapters, and conference papers. We first classified the selected works according to the seven principles identified\nin Section 3. The classification was done in two steps: (a) reading the titles and abstracts to identify the potential\nResponsible Al principle(s) addressed by each study, and (b) reviewing the full text, with a focus on the methodology,\nresults, and discussion sections, to determine the primary and secondary principles covered. As shown in Table 4, 40%\nof the analysed studies primarily focus on Privacy. The second most commonly addressed principle is Transparency. We\nalso created a 'Various principles' category for works focusing on more than two principles. In the following section,\nwe review the selected works, discussing how they have applied Responsible Al principles in practice, the challenges\nencountered, and the lessons learned.\n4.2.1 Accountability. The accountability principle mandates that institutions take responsibility for decisions generated\nby predictive analytics systems. All stakeholders-such as HEI directors, managers, and data scientists-must understand\ntheir roles throughout the lifecycle of Learning Analytics (LA) systems. For instance, [40] highlight the importance of\naccountability in the design phase, particularly concerning data management. Similarly, [3] emphasise that unclear\ngovernance policies can undermine trust in LA systems. [61] provide operational criteria for accountability, such as\ncreating clear documentation of roles for developers and users of LA dashboards. Compliance with GDPR also plays a\ncritical role, in defining key responsibilities among data controllers, processors, and subjects. The literature reveals two\ndimensions of accountability: forward-looking responsibility, which focuses on identifying stakeholders and their roles,\nand backwards-looking responsibility, which involves acknowledging the outcomes of LA systems. Overall, addressing"}, {"title": "4.2.2 Safety", "content": "The Alan Turing Institute's guidelines for safe Al systems [30] highlight accuracy, reliability, and\nrobustness as essential technical characteristics necessary to ensure Al functions safely and avoids harmful outcomes.\nIn the context of LA, components such as data, decision algorithms, and applications (e.g., dashboards, alert systems)\nmust be designed, deployed, and monitored to minimise errors, ensure consistent behaviour aligned with LA goals,\nand produce trustworthy predictions. To enhance safety it is crucial to provide end users-students, academic staff,\nand administrators-with clear documentation outlining the responsible use of LA systems, including guidance on\ninterpreting data and engaging with at-risk students [26]. Developers should also receive clear conceptual frameworks\nand usage guidelines for LA systems [54]. Reliability in decision algorithms is critical; thus, establishing measurable\ngoals for accuracy and expected model performance is vital. Specific considerations regarding acceptable error rates\nand performance metrics should be implemented. It is important to acknowledge that various factors-such as the\nchoice of machine learning algorithms, missing data, and data noise-can influence predictions. Therefore, setting\ncheckpoints for training and testing data is recommended [36]. Additionally, ensuring data accuracy is paramount;\nresearch by [41] underscores the need for HEI policies that guarantee access to up-to-date student data to prevent\nunreliable predictions. Despite these recommendations, there seems to still be a big gap in the literature on methods\nand actions that HEIs could put into practice to ensure that LA systems minimise harm to staff and students, whether\npsychological, emotional or academic."}, {"title": "4.2.3 Security", "content": "The security principle includes the implementation of technical, administrative and physical controls\nto mitigate risks and prevent information assets from being accidentally or deliberately compromised. Key aspects\ninclude ensuring confidentiality by controlling access to sensitive data about students and staff, maintaining integrity\nby preventing unauthorised data alteration or deletion, and guaranteeing availability for authorised users to access\nLA systems promptly [29]. Literature around Security indicates a dual focus on privacy and data security concerns\n[10, 17, 53]; and both principles are strongly interlinked. For instance, implementing strong security measures (like\ndata anonymisation) is crucial for protecting personal information, thus supporting privacy. Similarly, adhering to data\nprivacy regulations comprises technical measures and policies to restrict unauthorized access or disclosure of personal\ninformation. Given that educational institutions collect extensive socio-demographic and progress data from students,\nany breach of this information could have detrimental effects on individuals and institutions alike. Therefore, the\nacquisition, processing, storage, and disposal of personal and sensitive data must adhere to strict legal and regulatory\ncompliance standards [29]. In this context, [50] presents a compilation of ethical data governance considerations,\nwhich encompass data security aspects such as data process (public, sensitive, personal, high-risk), data storage (local\nvs. remote), and data audit plans. In the same line, the work by [16] compiles reference questions for managers and\ndecision-makers to consider when implementing LA data security. Other researchers, however, who have looked at\nsecurity in LA, have used the General Data Protection Regulation (GDPR) as the main guidance to protect personal data;"}, {"title": "4.2.4 Fairness and bias", "content": "The principles of fairness and bias have garnered considerable attention within the Learning\nAnalytics (LA) community, exemplified by dedicated workshops such as FairLAK7. Research has underscored the\nnecessity of evaluating LA algorithms for biases and implementing effective mitigation strategies [44]. Some studies\nfocus on fairness metrics to assess biases affecting specific groups, including minority ethnic students [7]. Advancing this\nwork, Deho and colleagues [15] shifted from merely detecting bias to actively mitigating it, conducting a comparative\nevaluation of selected bias mitigation approaches. Their findings reveal that fairness lacks a universal definition, making\nthe choice of definition a crucial first step in determining appropriate mitigation strategies. Furthermore, both studies\nindicate that enhancing fairness in LA systems may not need a compromise on predictive performance. [44] recommend\nthe de-weighting or removal of sensitive attributes (and potential proxies, such as socio-economic status) from the\ntraining data of LA algorithms. However, Deho et al. [13] clarify that the inclusion or exclusion of a protected attribute\nimpacts performance and fairness only if it is correlated with the target label and deemed significant. Importantly,\nLA models that demonstrate fairness based on historical data may not maintain this fairness when applied to current\nor future datasets. Deho and colleagues therefore advocate for ensuring robustness against dataset drifts prior to\ndeployment [14]. In a recent survey on biases in education, Li et al. [31] emphasised the importance of considering\nintersectionality-how multiple sensitive attributes like gender and ethnicity interact-when evaluating algorithmic\nbias. They cautioned that applying fairness metrics to inappropriate tasks could lead to false conclusions and potentially\nharmful decisions. On the social aspect of fairness, [57] utilised questionnaires to gather insights from students and staff\nregarding the implications of bias in decision-making processes. Students expressed concerns about bias perpetuation\nand the fear of unfair assessments, while staff highlighted apprehensions regarding decisions made about them based on\nLA, such as managers using LA for performance evaluations. While the existing identification and mitigation methods\nfor bias provide valuable insights for HEIs, there remains a significant gap. The lack of clear guidelines on which\ndefinitions of fairness should be adopted based on specific objectives, as well as which bias mitigation methods are\nmost suitable depending on the context (data, algorithm, etc.), leaves many open questions when operationalising this\nprinciple."}, {"title": "4.2.5 Transparency", "content": "The principle of transparency is crucial in ensuring that all stakeholders involved in a LA system\n(students, staff, and other relevant parties) are well-informed about its operations. Numerous studies highlight the\nnecessity of transparency in LA [8, 34, 59], yet many fall short of providing practical guidance on how to achieve it.\nDrachsler and Greller [16] note the inherent complexity in data collection and algorithmic processes, emphasising the\nchallenge of conveying this information to non-technical stakeholders, including learners, teachers, and education\nmanagers. They advocate for giving data subjects access to their analytics results, empowering them to decide whether to\nseek pedagogical support or interventions, thereby placing the learner in control. The work also stresses the importance\nof obtaining clear consent prior to data collection, including the need for straightforward yes/no questions and the\noption to opt-out without repercussions. Hakami [24] reviewed 37 LAK papers mentioning LA and Transparency."}, {"title": "4.2.6 Privacy", "content": "A wide range of studies have considered the privacy implications of LA. In a concept mapping exercise\nwith experts, privacy as well as transparency were identified as the most important elements of LA policy [48]. A key\nissue identified in prior studies is how and whether the student has agency over use of their data in the LA system.\nRecommendations include giving students the option to opt-out [55]. Alternatively, LA could be offered on an opt-in\nbasis [42] in which LA is presented in terms of how it can improve their learning experience [22]. To ensure consent is\ngenuinely informed [40, 55, 63] students need greater awareness of what data is used and how [42] and the expected\nimpact of granting or withdrawing consent [3]. Workshops or meetings with students may be used to ensure students\nhave appropriate knowledge of data literacy and data protection [58]. Consent-seeking procedures should be defined at\nan early stage [4] to help ensure initial or changing student preferences can be handled in the LA infrastructure [17].\nSimilarly, staff should be given better guidance on the appropriate use of data and also the consequences of misuse\n(e.g. loss of confidentiality, negative publicity, legal action) [20]. Such guidance could be informed by a Privacy Impact\nAssessment, covering impact on individuals, groups and wider society [9]. Privacy should be initially considered at\nthe point of the initial business case as part of the risk analysis for the initiative [9]. Data should be anonymised\nwherever possible [55, 58], for example when aggregated to inform curriculum improvements [22]. More broadly, data\ngovernance guidelines should inform data sharing and ownership [3] and be used to continually assess data access\nrights for different stakeholders [4], minimising access to student data [58]. Privacy enhancing technologies such as\nanonymisation, encryption and digital signatures should be considered to improve the security of personal data [42]\nand any stakeholders should have access to an independent complaints body if they have any grievance over how their\ndata has been accessed and used [9]."}, {"title": "4.2.7 Explainability", "content": "The principle of explainability highlights the need for LA systems to offer clear insights into\nhow their predictions and decisions are made. For example, [33] note that traditional machine learning methods, like\ndecision trees, provide higher interpretability compared to modern deep learning models, which often function as\n\"black boxes.\" As a result, despite their lower accuracy, these interpretable systems may be preferred in scenarios where\nunderstanding the decision-making process is critical. This observation is further supported by Gunasekara et al. [23],\nwho reviewed explainability research within Educational Data Mining (EDM) and Learning Analytics, underscoring\nthe importance of clarity in these systems. To address the explainability of more complex models, Li et al. [32] utilise"}, {"title": "5 Responsible Al Framework for Learning Analytics", "content": "In this section, we introduce our Responsible AI (RAI) framework tailored to Learning Analytics (LA) in Higher\nEducation (HE) - see Figure 1. The primary aim of this framework is to provide higher education institutions with\nactionable guidance on how to incorporate responsible Al principles effectively into their LA initiatives. Recognising\nthat institutions are at various stages of their LA adoption, we have structured the framework to follow the stages of\nthe software development lifecycle: Requirements and Data Collection, Design, Development, Testing, Release, and\nMonitoring. By aligning the framework with these stages, we address a key limitation of many existing resources,\nallowing HEIs to engage with the specific stage of development they are currently in. This approach enables a more\nflexible, actionable pathway for integrating responsible Al principles.\nWhile our ultimate goal is to provide both a list of actions HEIs can take to ensure their LA systems incorporate\nresponsible Al principles and how to implement these actions, our literature review reveals a significant lack of real-\nworld examples of how HEIs have operationalised these principles-if they have done so at all. We acknowledge that\nthis leaves our framework incomplete, particularly in offering specific, practical steps that have already been tested\nin the field. However, we see this as an opportunity for continued growth. Our ambition is to refine this resource in\ncollaboration with the wider academic and practitioner community, learning from best practices as they emerge.\nFor the purposes of this paper, a version of our proposed framework is summarised in Figure 1 and accessible via\nan anonymised URL, where we have presented the relevant elements through a PowerPoint presentation (due to the\nconstraints of the double-blind review process). The framework is hosted on a dedicated project website, where each\nstep is linked to available documentation and real-world case studies from HEIs. This evolving resource will allow\nthe community to contribute relevant materials, such as code libraries, consent forms, and other practical examples,\nfostering a collaborative environment where institutions can learn from one another.\nUltimately, we hope that this framework becomes a dynamic tool for HEIs seeking to responsibly implement LA\nsystems, enabling them to align their practices with responsible Al principles."}, {"title": "6 Discussion and Conclusions", "content": "In this paper, we have sought to address a pressing need within Higher Education Institutions (HEIs) for practical\nguidance on implementing Responsible AI principles within Learning Analytics solutions. Our aim is to establish a\ncomprehensive Responsible AI framework tailored specifically for LA applications in HE. This framework is rooted in"}]}