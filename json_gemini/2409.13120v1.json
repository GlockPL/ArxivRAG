{"title": "Are Large Language Models Good Essay Graders?", "authors": ["Anindita Kundu", "Denilson Barbosa"], "abstract": "We evaluate the effectiveness of Large Language Models (LLMs) in assessing essay quality, focusing on their alignment with human grading. More precisely, we evaluate ChatGPT and Llama in the Automated Essay Scoring (AES) task, a crucial natural language processing (NLP) application in Education. We consider both zero-shot and few-shot learning and different prompting approaches. We compare the numeric grade provided by the LLMs to human rater-provided scores utilizing the ASAP dataset, a well-known benchmark for the AES task. Our research reveals that both LLMs generally assign lower scores compared to those provided by the human raters; moreover, those scores do not correlate well with those provided by the humans. In particular, ChatGPT tends to be harsher and further misaligned with human evaluations than Llama. We also experiment with a number of essay features commonly used by previous AES methods, related to length, usage of connectives and transition words, and readability metrics, including the number of spelling and grammar mistakes. We find that, generally, none of these features correlates strongly with human or LLM scores. We note that some of these features are intricately related to grading rubrics, including those in the ASAP dataset, suggesting that human graders often overlook some aspects of the rubric while grading. Indeed, we observe that, generally speaking, longer essays that use more connectives tend to get higher grades regardless of spelling and grammar mistakes. On the other hand, both LLMs not only can reliably detect spelling and grammar mistakes but also seem to take those mistakes into account when computing their score, further distancing themselves from human grading. Finally, we report results on Llama-3, which are generally better across the board, as expected. Overall, while LLMs do not seem an adequate replacement for human grading, our results are somewhat encouraging for their use as a tool to assist humans in the grading of written essays in the future.", "sections": [{"title": "Introduction", "content": "Essay writing is a common component of student assessment, playing a pivotal role in education by providing insights into the text comprehension, critical thinking, and communication skills of the students [17]. Writing good essays requires students to articulate their thoughts clearly and coherently, demonstrating their under- standing of a subject matter and their ability to construct logical arguments [26]. Moreover, effective essay assessment not only measures the knowledge and the skills students have acquired but also encourages deeper learning and engagement with the material [45]. Traditionally, essay grading has been mostly done by human graders. However, this presents significant challenges in modern education settings, especially when it comes to distance education, which contributes"}, {"title": "Research Questions", "content": "We investigate this in-context learning approach, guided by a fundamental question: Can AES based on LLMs match human graders in judging the quality of essay writing? Related to that question, we revisit the role of annotated training data for the AES task and consider whether LLMs can act as zero/few-shot classifiers to assess essay quality and explain their understanding reasonably behind the score they provide. In particular, our study aims to address the following research questions:\n\u2022 RQ1: Do human scores align with the LLM scores?\n\u2022 RQ2: What are the possible reasons behind the similarity/difference in scores?\n\u2022 RQ3: Do LLMs offer explanations in a tone that reflects their scores?\n\u2022 RQ4: Can LLMs correctly identify spelling and grammatical errors and reflect those into their scoring?"}, {"title": "Outline", "content": "The rest of the paper is structured as follows. Section 2 provides a concise review of related research. In Section 3, we present detailed methodology about the dataset in subsection 3.1, along with the experimental setup in subsection 3.2. Section 4 is dedicated to the presentation and analysis of the results. Chapter 5 is an extension and additional experiment results with different prompts. Finally, in Section 6, we offer our conclusions and discuss potential avenues for future enhancements."}, {"title": "Related Work", "content": "Human raters possess diverse skills in evaluating assignments, including understanding the meaning of the text, assessing critical thinking, creativity, and content relevance [10]. They excel in evaluating logic, argument quality, and factual correctness, and can judge audience awareness [34]. However, maintaining consistency and eliminating subjectivity can be challenging [34]. Additionally, large-scale essay scoring can be labor-intensive and time-consuming [56], which in turn can lead to fatigue and inconsistencies by human scorers. With the increasing demand for personalized education and the growing shortage of teachers, Automated Essay Scoring (AES) systems are increasingly needed. AES can assist in managing large classes by providing consistent and efficient essay assessments. This capability can alleviate some of the workload from educators, allowing them to focus on more interactive and engaging aspects of teaching. The first automated essay scoring system was developed more than 50 years ago, in 1966 [47]. Since then, AES systems have become more advanced, offering more features than the early versions. There are many good surveys of AES systems, such as Ramesh and Sanampudi [44] and Ke and Ng [27]. Most AES research focuses on supervised learning of holistic scoring due to the availability of annotated corpora and their commercial value in automating standardized test grading. Following a similar evolution as other fields of AI, AES systems evolved from using hand-crafted rules in PEG (Project Essay Grade) [41] to features-based statistical machine learning such as E-raters [3], Intelligent Essay Assessor (IEA) [15] to using deep neural models such as Recurrent Neural Network (RNN), Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM) that learn own representations of the data [48, 13]. We also see approaches which learn from"}, {"title": "Methodology", "content": "We used the Automated Student Assessment Prize (ASAP) benchmark [20], well-known for the Automated Essay Scoring (AES) task. It consists of around 13000 essays written by students of grade levels from 7 to 10. There are eight sets of tasks in ASAP, each linked to different prompts and scoring ranges. Detailed statistical information about this dataset is shown in Table 1. Students wrote essays in response to three types of prompts. An argumentative essay is when students write to convince others about their opinion on a topic. They need to research the topic, find evidence, and explain their ideas clearly. A narrative essay is when students write a story. Here they can make up characters or use personal experience, and events to create an interesting tale. A source-dependent essay is when students need to read an article and then write an essay using the information from that article. In this dataset, two to three human graders annotate each essay. The final score of an essay is the sum of scores given by the individual human graders."}, {"title": "Experimental Setup", "content": "We consider two popular LLMs for response generation: ChatGPT and Llama. Specifically, we consider the versions gpt-3.5-turbo, and Llama-2-70B-chat for our experiment. From the ASAP dataset, we input the prompt given to students, their corresponding essays, rubric guidelines, score range, and any additional instructions into the ChatGPT and Llama. The rubric and guidelines are presented in the same format and detail as they would be for human raters, mirroring how they were described in the dataset. Appendix A includes examples of prompt and ChatGPT or Llama-generated response. Next, we ask LLMs for a numeric score in the appropriate range and an explanation for Task 1. Task 7 has four trait scores and a total score along with an explanation from ChatGPT, Llama. We make sure that the entire input (prompt + actual essay + output) never exceeds its token limit (4096 token as of December 2023). While generating responses, we employed default parameter settings to maintain balance and maximize the creativity and diversity of LLMs when solving a complex task like AES."}, {"title": "Extracting features from essays", "content": "We extract a wide range of features from essays to further analyze different aspects of how the grading process of LLMs coaligns with humans.\n\u2022 Essay Statistics: We have extracted several basic statistical features for each essay such as the number of sentences and the number of tokens found in the essay.\n\u2022 Readability: This assessment in our analysis aimed at understanding whether human graders implicitly consider readability and if LLMs are pre-trained to evaluate text difficulty. Readability refers to the ease with which a reader can understand a written text. It is often assessed using metrics that consider factors such as sentence length, word complexity, and sentence structure. To achieve this, we employed several well-known readability formulas, including the Flesch Reading Ease [29], Automated Readability Index [46], Coleman-Liau Index [11], Dale-Chall readability score [12], Flesch\u2013Kincaid Grade Level [29], Gunning Fog Index [19], Linsear Write Formula, and SMOG index [14]. These formulas help de- termine how challenging an essay is to read and comprehend. We have followed the implementation of text readability formulas from the paper by Martinc et al. [36]. This approach allows us to investigate how readability influences the grading process for both human graders and language models.\n\u2022 Linking Words: In our analysis, we extracted features from essays that include the use of the total and unique number of transition words\u00b2 and \u201cFANBOYS\u201d3 words. Transition words serve as a proxy for the number of arguments in the essay. Transition words and Fanboys (coordinating conjunctions) serve similar purposes in English writing by connecting ideas and improving the flow of text. Transition words, such as \u201chowever\u201d or \u201ctherefore,\u201d indicate relationships between sentences or paragraphs, aiding in smooth transitions and clarifying the logical progression of ideas. Fanboys, on the other hand, specif- ically join clauses within a sentence, emphasizing relationships like addition (\u201cand\u201d), contrast (\u201cbut\u201d), choice (\"or\"), or conclusion (\"so\"). Both types of words enhance coherence and cohesion in writing,"}, {"title": "Extracting features from LLM Explanations", "content": "We gathered various fundamental statistics from each LLM explanation, including the count of sentences and tokens. Additionally, we employed sentiment analysis to assess the tone of the explanation and implemented rule-based methods to extract specific information from the LLM explanations, as described below:\n\u2022 Length-related Statistics: We extracted length-related features from LLM-generated explanations, such as the number of sentences and the number of tokens.\n\u2022 Sentiment Score of the Explanation: To explore whether the tone and content of the explanations align with the LLM's scores, we calculated the sentiment score of the explanations using VADER (Valence Aware Dictionary and sEntiment Reasoner), a sentiment analysis tool specifically designed for social media content [24]. VADER combines a lexicon-based approach with rule-based techniques to assess sentiment polarity (positive, negative, or neutral) and intensity. VADER provides the following sentiment scores:\n1. Compound score: This overall sentiment score ranges from -1 (extremely negative) to 1 (extremely positive). It captures the overall sentiment of the text.\n2. Positive score: Indicates the proportion of positive words in the text.\n3. Negative score: Reflects the proportion of negative words.\n4. Neutral score: Represents the neutrality of the text.\nWe extracted all these four sentiment scores from VADER for each ChatGPT and Llama response. Then, we split each response into individual sentences and asked for a polarity score to determine the maximum sentence-wise compound sentiment score."}, {"title": "Mention of Language Mistakes", "content": "As mentioned before, we asked the LLMs to provide an explanation to justify the numerical score. We observed that, often, the explanation mentioned grading criteria v explicitly mentioned in the rubric. We extracted those mentions to measure the LLMs' self-consistency; more specifically, we sought to investigate whether the LLM's final score was lower when the explanations contained mentions to problems. As will be discussed later, we partitioned the essays into three groups based on the corresponding explanation provided by the LLM when grading them. The first group consists of essays for which the LLMs did not mention any problem (e.g., no grammar mistakes). The second group consists of essays where the LLMs mentioned problems without a qualifier. The third group consists of the essays for which the LLM used a qualifier (e.g., \u201cseveral\u201d, \u201cmultiple\", etc.) to emphasize the presence of mistakes. We used Spacy's DependencyMatcher to process the LLM explanations and extract mentions of grammatical, spelling, punctuation, and capitalization mistakes from the LLM's explanations. Dependency matching involves identifying and extracting linguistic patterns based on the syntactic dependencies be- tween words in a sentence and generalizes over lexical methods relying on regular expressions, for ex- ample. We randomly selected 60 samples from the explanations generated by ChatGPT to manually hand-craft dependency patterns. Figure 2 shows a partial dependency tree from the actual sentence pro- vided by ChatGPT as part of an explanation. Our patterns also took into consideration the case where different kinds of mistakes were present, as in There are grammatical and spelling mistakes in the essay. In this sentence, the token grammatical is conjoined with another token spelling. To represent such a conjunct relationship, we curated patterns to handle up to five different mentions as conjunction. We used the same patterns for both language models (ChatGPT and Llama).\""}, {"title": "Evaluation Metric", "content": "We performed a correlation analysis between human scores and LLM-provided scores. Furthermore, we have leveraged the ASAP++ dataset to compare the human provided trait scores of Task 1 to LLM's scores. We have also performed correlation analyses between all scoring methods and various features extracted from essays and explanations to examine different aspects of human and LLM essay scoring. Our primary focus centers on convention scores of the ASAP++ dataset, as there are currently no other available NLP tools for validating the remaining four criteria [27]. Convention scores are derived from assessments related to punctuation, spelling, grammar, and capitalization. Notably, the explanation provided by ChatGPT and Llama often mentions spelling and grammar issues. Consequently, we opt for these two metrics to assess the quality of LLM's provided explanation. We use Pearson correlation analysis to measure the agreement or similarity between the scores assigned by human raters and those generated by LLMs. The Pearson correlation coefficient is denoted as r ranges from -1 to 1, where:\n\u2022 r = 1 indicates a perfect positive correlation, meaning that as one variable increases, the other variable also increases proportionally.\n\u2022 r = -1 indicates a perfect negative correlation, meaning that as one variable increases, the other variable decreases proportionally.\n\u2022 r = 0 indicates no linear correlation between the variables.\nwe classified the magnitude of Pearson correlation (r) values to assess the strength of relationships between variables as follows: very weak (0.0 to 0.19), weak (0.2 to 0.39), moderate (0.4 to 0.59), strong (0.6 to 0.79), and very strong (0.8 to 1.0)."}, {"title": "Results", "content": "For Task 1, both ChatGPT and Llama assigned valid overall scores ranging from 1 to 6 to all 1783 essays. Table 3 presents the descriptive statistics for the four scoring methods, while Figure 3 illustrates the score distribution. It is notable that LLMs tend to assign lower scores compared to human raters.\nIn Task 7, ChatGPT and Llama generated four trait scores, along with a final overall score that sums all trait scores for a total of 1569 essays. Table 4 presents the descriptive statistics for the four grading methods. ChatGPT (M = 2.07, SD = 1.83) and Llama (M = 2.66, SD = 2.68) both assigned significantly lower mean scores than both human raters (M = 8.02, SD = 2.42 and M = 8.04, SD = 2.52) revealing a trend similar to that observed in Task 1 where LLMs tended to assign lower scores than human raters. Notably, ChatGPT and"}, {"title": "LLM and human scores do not correlate", "content": "Tables 5 indicates that ChatGPT scores show a weak positive correlation with the scores provided by both human raters (r_{rater1} = 0.23, p < 0.001, d' = -3.30 and r_{rater2} = 0.21, p < 0.001,d = \u22123.38) in Task 1. While Llama scores moderately positively correlate (r_{rater1} = 0.59,p < 0.001,d = \u22120.85 and r_{rater2} = 0.58, p < 0.001, d = \u22120.87) with human scores exceptionally for Task 1. Also, we observed no strong inter-LLM correlation (r = 0.32,p < 0.001,d = 2.8) between Llama and ChatGPT. All the detailed statistical analyses can be found in Appendix D.\nA similar trend is observed in Table 6 for ChatGPT in Task 7, there is negligible positive correlation across"}, {"title": "ASAP++ traits: weak with ChatGPT, strong with Llama", "content": "Moving forward, we assess the relationship between various essay trait scores provided by human raters in the ASAP++ dataset for Task 1 and scores given by human raters in the ASAP dataset and LLMs. Table 7 demonstrates that all the ASAP++ trait scores given by human raters show a strong positive correlation with the overall scores provided by human raters in the ASAP dataset (r_{rater1} = 0.63 \u2013 0.67, p < 0.001, d_{mean} = -0.56 and r_{rater2} = 0.62 0.67, p < 0.001,d_{mean} = -0.58). Moreover, we observed that trait scores are strongly correlated with Llama scores (r = 0.59 \u2013 0.61,p < 0.001, d_{mean} = 0.16) than ChatGPT scores (r = 0.33 -0.36, p < 0.001, d_{mean} = 2.35). This finding further suggests that ChatGPT and human raters may approach the essay scoring task very differently, and Llama can mimic human raters better than ChatGPT."}, {"title": "RQ2: What are the possible reasons behind the similarity/difference in scores?", "content": "To delve into the reasons behind the weak correlation between human and LLM scores, we have examined various essay features, including the count of language errors, readability indices and length-related attributes such as the number of sentences and tokens. More details on the extraction of these features are outlined in section 3.2.2. All the detailed statistical analyses can be found in Appendix D."}, {"title": "Human scores highly correlate with essay features", "content": "From Table 8, it is evident that human scores exhibit a strong positive correlation with length-related essay features (number of tokens and sentences) for both tasks. This suggests that human raters tend to assign higher scores to longer essays. It goes without saying that better writers are capable of producing more high-quality sentences and longer sentences than average and weaker writers. Therefore, Occam's razor would lead us to believe that the length of the essay has no real bearing on its score. Instead, all that we observe is that good-quality essays are longer because they were written by better students who could accomplish more in the allotted time and so graded with higher scores. Additionally, we observed that while Llama scores show mostly a moderate correlation (r = 0.42 \u2013 0.66) with essay length features (both the number of tokens and sentences), including Task 1 being an exception where the correlation is exceptionally high. ChatGPT grades do not show any strong correlation (r = 0.0 - 0.20) with essay features. This observation indicates that ChatGPT is not lenient towards longer essays in scoring compared to the human raters and Llama.\nIn analyzing the use of transition and \u201cFANBOYS\u201d words showing Table 8, a weak to moderate positive correlation (r = 0.31 0.50) has been found with human scores across both tasks. The higher correlation with the total count of \u201cFANBOYS\u201d words, as opposed to the unique count, suggests that students tend to rely repeatedly on these connecting words (such as \u201cfor, and, but, or, yet, so\u201d) in their writing. The more students"}, {"title": "LLM and human scores do not correlate with readability indices", "content": "Here, we compare well-known readability indices with scores provided by LLMs and human raters. As demon- strated in Table 10, we can see that the scores provided by both human raters have very negligible correlations (r = 0.01 -0.3) across all well-known readability indices. This suggests a minimal influence of readability on human scoring.\nInterestingly, the tables also demonstrate a lack of correlation between LLM scores and readability metrics. Since we are using LLMs as a zero-shot learner and since we do not mention readability explicitly in the prompt (See Appendix A for an example), this result implies that LLM's interpretation of the question and its response are not associated with the numerous mentions to readability found on the web (and very likely used in the pre- training of the LLM models). Furthermore, some correlations with all scoring methods are negative, indicating that both humans and LLMs may assign high grades to less readable texts and low grades to highly readable ones."}, {"title": "LLM scores negatively correlate with the count of mistakes", "content": "We utilized LanguageTool and Aspell to detect various language mistakes in the essays, including grammar, spelling, style, punctuation, and capitalization, as described in section 3.2.2. Following this, we computed the number of mistakes for all essays and conducted correlation analysis across four scoring methods. In Table 11, we observed that ChatGPT consistently displayed a weak but negative correlation, whereas other scorers mostly showed a positive correlation with mistake count.\nThe negative correlation observed with mistakes aligns with the rubric, where a higher number of language mistakes typically corresponds to lower scores. This logical relationship underscores ChatGPT's reliability in accurately identifying mistakes and adjusting scores accordingly. Additionally, Llama also demonstrated a negative correlation in some instances, albeit with mostly negligible positive scores. Conversely, human graders exhibited a surprising positive correlation with mistake count. This suggests potential challenges for human graders in consistently identifying and adjusting grades based on spelling and grammar errors, highlighting the systematic ability of LLMs like ChatGPT and Llama in language error detection. On the other hand, this may be attributed to potential inaccuracies in the misspelling count reported by the spelling checker tools. To investigate this matter, we manually inspected the words flagged by Aspell as misspelled in randomly selected essays (approx 10) and cross-checked with another spell checker LanguageTool. We have also performed a correlation analysis of these two spell checkers, details can be found in section E.1. Our findings indicate that it strongly agrees with Aspell that the likelihood of the last scenario occurring is unlikely.\nWe extended our analysis by examining the relationship between mistake count and trait-wise scores in the ASAP++ dataset. In Table 12, we observed a negative correlation between the LanguageTool's grammar, spelling, and punctuation error counts and trait scores, although the correlation was very weak. Specifically, for the convention trait score, most mechanical error counts displayed a negative correlation. This indicates that trait scores offer detailed insights pinpointing specific areas for skill enhancement, particularly in identifying mechanical mistakes with greater precision than overall scoring by human raters."}, {"title": "RQ3: Do LLMs offer explanations in a tone that reflects their scores?", "content": "We have extracted basic statistical features and applied the VADER sentiment analyzer from the ChatGPT explanation as described in section 3.2.3. In table 13, explanation length (sentences) and explanation length (tokens) refer to the number of sentences and the number of words found in ChatGPT explanation respectively. Explanation sentiment (average) refers to the overall compound sentiment score of ChatGPT response pro- vided by VADER. Explanation sentiment (max) means sentence-wise maximum compound score in ChatGPT response provided by VADER sentiment analyzer. We perform correlation analysis between these four features with human and LLM scores.\nIt's worth noting that the explanation features show a positive weak correlation, but they are relatively stronger with ChatGPT's scores compared to human-assigned or Llama scores. This suggests that ChatGPT has a better understanding of the prompt and provides explanations that align with the assigned scores, indicating consistency. For instance, lower scores receive more negative explanations, while higher scores get more positive ones. This implies that ChatGPT is not generating explanations randomly but is aware of the scores it has provided.\nHowever, we observed that ChatGPT scores have smaller coefficient scores (r = 0.13 \u2013 0.33), indicating that ChatGPT may not be explained in a positive tone. Considering that ChatGPT's overall scores are quite low and LLMs have a tendency to generate toxic content [7], it's possible that the tone of the explanations may not appear positive due to the low scores. Since we cannot rule out the possibility that the explanation generated"}, {"title": "ChatGPT gives consistently harsh explanation", "content": "We have extracted basic statistical features and applied the VADER sentiment analyzer from the ChatGPT explanation as described in section 3.2.3. In table 13, explanation length (sentences) and explanation length (tokens) refer to the number of sentences and the number of words found in ChatGPT explanation respectively. Explanation sentiment (average) refers to the overall compound sentiment score of ChatGPT response pro- vided by VADER. Explanation sentiment (max) means sentence-wise maximum compound score in ChatGPT response provided by VADER sentiment analyzer. We perform correlation analysis between these four features with human and LLM scores. It's worth noting that the explanation features show a positive weak correlation, but they are relatively stronger with ChatGPT's scores compared to human-assigned or Llama scores. This suggests that ChatGPT has a better understanding of the prompt and provides explanations that align with the assigned scores, indicating consistency. For instance, lower scores receive more negative explanations, while higher scores get more positive ones. This implies that ChatGPT is not generating explanations randomly but is aware of the scores it has provided. However, we observed that ChatGPT scores have smaller coefficient scores (r = 0.13 \u2013 0.33), indicating that ChatGPT may not be explained in a positive tone. Considering that ChatGPT's overall scores are quite low and LLMs have a tendency to generate toxic content [7], it's possible that the tone of the explanations may not appear positive due to the low scores. Since we cannot rule out the possibility that the explanation generated by ChatGPT could have been influenced by the score it produced, there might be a side effect of using LLMs as zero-shot classifiers, affected by their tendency towards generating toxic content (despite concerted efforts by LLM developers to curb this tendency)."}, {"title": "Llama gives moderately positive explanation", "content": "We conducted a comprehensive analysis of the Llama explanations using the VADER sentiment analyzer, similar to our approach with the ChatGPT explanations. The results detailed in Table 14, reveal that the Llama scores exhibit notably higher correlation (r = 0.42 \u2013 0.54) with its explanations. This finding suggests that in contrast to ChatGPT, Llama generates explanations with a moderately positive sentiment highlighting Llama's ability to convey positive explanations to learners."}, {"title": "RQ4: Can LLMs correctly identify and assess spelling and grammatical mistakes and score accordingly?", "content": "To investigate whether the mention of mistakes (misspellings and grammatical errors) in the explanation reflects the quality of the essays, we have exploited the LLM explanation as described in 3.2.3 further. Now we have the information for each essay sample on whether LLM has identified any language mistakes. Next, we categorized these LLM responses (for Task 1 and Task 7 of the ASAP dataset) based on the mention of grammatical and spelling errors separately into three distinct groups. The first group contains samples without mention of spelling errors, the second group comprises only samples with mentions of misspellings, and the last group contains essay samples with mentions of misspellings along with a qualifier (such as several, numerous, multiple). We followed this grouping process for grammatical errors for both ChatGPT and Llama. For these three groups, we have calculated the average of spelling mistakes by LanguageTool and Aspell, LLM scores, and sentiment score of LLM explanation and compared them against each other. We determined whether the differences between group means are statistically significant by conducting the ANOVA test and reported p- values and F-stat. We have also calculated effect sizes using Omega squared (\u03c9\u00b2) 8, which is widely recognized as a less biased alternative to eta-squared, particularly when dealing with small sample sizes."}, {"title": "Conclusions", "content": "Our research aims to evaluate the effectiveness of Large Language Models (LLMs) in assessing essay quality, generally, and in Automatic Essay Scoring (AES), specifically. By comparing scores generated by LLMs to those given by human raters, we have uncovered several insights into the performance and alignment of these models. We found that while ChatGPT and Llama exhibit remarkable capabilities in understanding essay prompts and generating self-consistent numerical and textual responses, there is a significant disparity between their assessments and those of human raters. Human raters show strong inter-rater agreement, often awarding higher scores to longer essays and excelling in evaluating the logical progression of ideas. In contrast, LLMs demon- strate distinct grading behaviors, with ChatGPT being stricter in its scoring compared to both human raters and Llama. On the other hand, Llama shows a closer alignment to human scoring patterns. Both human and LLM scores seem minimally influenced by readability indices; however, essays that were harder to read tended to receive higher marks from both humans and LLMs. LLMs demonstrated a strong ability to detect and account for spelling and grammatical errors, highlighting a key difference from human raters, who seem to overlook such clerical mistakes in grading. The reason for this behaviour is impossible to obtain from the data at our disposal. Nevertheless, our findings suggest that future AES research needs to use richer data that can help capture the nuances of the human grading process. However, specific trait-wise human scores have shown promise in identifying mechanical mistakes more effectively than an overall score provided by human raters. In terms of providing explanations for the grades, ChatGPT often delivered explanations in a harsh tone, whereas Llama used generally less negative language in their output. Both LLMs showed a better under- standing of the prompt and provided explanations that aligned with the assigned scores, indicating a degree of consistency. Lower scores are accompanied by more negative explanations, while higher scores have led to more positive explanations, suggesting that LLMs are aware of the scores they provide and do not generate explanations randomly. Overall, our findings suggest that while LLMs hold potential for automatic essay scoring (AES) tasks, they should be used with human supervision. LLMs may not completely replace human raters, but their ability to understand guidelines, coupled with their consistent explanation capabilities, make them valuable tools for educators facing the demands of modern education. Additionally, our experiment with one of the more recent LLMs Llama-3 reveals that in Task 7, Llama-3 demonstrates a 29.5% improvement in correlation with human scores compared to Llama-2 and a remarkable 173% improvement compared to ChatGPT-3.5-Turbo. In Task 1, although Llama-3 is 16% less correlated with human scores than Llama-2, it still achieves a 130% improve- ment in correlation with human raters compared to ChatGPT-3.5-Turbo. Overall, Llama-3 shows substantial advancements in performance compared to ChatGPT-3.5-Turbo across both tasks, despite some variability compared to Llama-2."}, {"title": "Limitations and Future work", "content": "One limitation of this study is that we evaluated the explanations generated by LLMs based solely on their ability to justify the provided grade, without considering how these explanations might affect learners. This narrow focus overlooks the potential impact of formative feedback on student learning and motivation, which is crucial for educational applications. Another limitation is the quality and nature of the dataset used. The dataset primarily contains numeric scores, which tend to align with basic length features of the essays. This suggests that human graders may prioritize certain aspects of the rubric over others. For example, longer essays with numerous spelling mistakes often receive higher scores from human raters, potentially because they value coherence and interest over technical accuracy. This inconsistency indicates that the human graders' evaluations may not perfectly adhere to the rubric, which could lead to misleading conclusions when comparing human and LLM scores. Additionally, LLMs tend to apply the rubric strictly and may not compensate for errors in the same way human graders do. This could explain why LLMs, particularly ChatGPT, tend to be harsher and less aligned with human scores. Human graders might overlook certain mistakes in favor of overall essay quality, while LLMs strictly follow the rubric guidelines. Therefore, comparing LLM scores with human scores from the ASAP dataset, as done in our study, or training machine learning models to replicate these scores, as done in previous studies, could be misguided. Such comparisons might not accurately reflect the nuanced judgments made by human graders. Future research should investigate the impact of LLM-generated explanations on student revisions and writing improvement. Additionally, our study focused on ChatGPT and Llama; future work should include comparisons with other models like Google Gemini [49] and similar tools to provide a more comprehensive evaluation of automatic scoring in education. Further exploration can be done on the effects of fine-tuning models or prompting LLMs with more annotated scores and explanations data by human raters to better mimic human grading practices and provide valuable textual insights. It is also important to note that the performance of AI tools is subject to their development stage at the time of the experiment. As these tools continue to evolve with new training data, future studies should reassess their effectiveness and alignment with human grading to ensure continued relevance and accuracy in educational assessments."}, {"title": "Conflict of Interest", "content": "All authors declare that we have no conflicts of interest to disclose related to this research. This article is based primarily on the first author, Anindita Kundu's Masters thesis. Partial financial support was awarded through the Deepmind scholarship program as part of her graduate studies."}, {"title": "Task 1 Question (given to the students)", "content": "More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising enjoying nature, and interacting with family and friends. Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you."}, {"title": "Task 1 Rubric", "content": "Score Point 1: An undeveloped response that may take a position but offers no more than very minimal support. Typical elements:\nContains few or vague details.\nIs awkward and fragmented.\nMay be difficult to read and understand.\nMay show no awareness of audience.\nScore Point 2: An under-developed response that may or may not take a position. Typical elements:\nContains only general reasons with unelaborated and/or list-like details.\nShows little or no evidence of organization.\nMay be awkward and confused or simplistic.\nMay show little awareness of audience.\nScore Point 3: A minimally-developed response that may take a position, but with inadequate support and details. Typical elements:\nHas reasons with minimal elaboration and more general than specific details.\nShows some organization.\nMay be awkward in parts with few transitions.\nShows some awareness of audience.\nScore Point 4: A somewhat-developed response that takes a position and provides adequate support. Typical elements:\nHas adequately elaborated reasons with a mix of general and specific details.\nShows satisfactory organization.\nMay be somewhat fluent with some transitional language.\nShows adequate awareness of audience.\nScore Point 5: A developed response that takes a clear position and provides reasonably persuasive support. Typical elements:\nHas moderately well elaborated reasons with mostly specific details\nExhibits generally strong organization.\nMay be moderately fluent with transitional language throughout.\nMay show a consistent awareness of audience.\nScore Point 6: A well-developed response that takes a clear and thoughtful position and provides persuasive support. Typical elements:\nHas fully elaborated reasons with specific details.\nExhibits strong organization.\nIs fluent and uses sophisticated transitional language.\nMay show a heightened awareness of audience."}]}