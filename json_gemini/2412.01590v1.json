{"title": "NCDD: Nearest Centroid Distance Deficit for Out-Of-Distribution Detection in Gastrointestinal Vision", "authors": ["Sandesh Pokhrel*", "Sanjay Bhandari*", "Sharib Ali", "Tryphon Lambrou", "Anh Nguyen", "Yash Raj Shrestha", "Angus Watson", "Danail Stoyanov", "Prashnna Gyawali", "Binod Bhattarai"], "abstract": "The integration of deep learning tools in gastrointestinal vision holds the potential for significant advancements in diagnosis, treatment, and overall patient care. A major challenge, however, is these tools' tendency to make overconfident predictions, even when encountering unseen or newly emerging disease patterns, undermining their reliability.\nWe address this critical issue of reliability by framing it as an out-of-distribution (OOD) detection problem, where previously unseen and emerging diseases are identified as OOD examples. However, gastrointestinal images pose a unique challenge due to the overlapping feature representations between in- Distribution (ID) and OOD examples. Existing approaches often overlook this characteristic, as they are primarily developed for natural image datasets, where feature distinctions are more apparent. Despite the overlap, we hypothesize that the features of an in-distribution example will cluster closer to the centroids of their ground truth class, resulting in a shorter distance to the nearest centroid. In contrast, OOD examples maintain an equal distance from all class centroids. Based on this observation, we propose a novel nearest-centroid distance deficit (NCCD) score in the feature space for gastrointestinal OOD detection.\nEvaluations across multiple deep learning architectures and two publicly available benchmarks, Kvasir2 and Gastrovision, demonstrate the effectiveness of our approach compared to several state-of-the-art methods. The code and implementation details are publicly available at: https://github.com/bhattarailab/NCDD", "sections": [{"title": "I. INTRODUCTION", "content": "Gastrointestinal diseases are among the most prevalent worldwide, with over seven billion incidents and 2.8 million cases reported in 2019 [1]. 38.4% of the prevalent diseases in the same year had digestive etiology, with a significant portion being gastrointestinal disorders [2]. A more concerning fact is that even with the advancement of technology, the number of deaths and crude deaths per population increased significantly from 2000 to 2019 on account of these diseases.\nA relatively common method of diagnosing gastrointestinal diseases is endoscopy, which is used to detect and diagnose any abnormalities present in the gastrointestinal tract with the help of a light pipe. With growing cases, the need for fast and accurate diagnosis has increased, leading to technological advancements like capsule endoscopy. However, manually diagnosing such cases has become increasingly challenging [3] for practitioners and requires aid through automated diagnosis.\nDeep learning has shown significant promise in this regard. Especially, CNNs have been used to recognize anatomical landmarks seen in the endoscope and have also been effective in detecting and segmenting abnormalities including but not limited to gastric cancer [4], polyps, ulcerative colitis, and esophagitis [5]\u2013[7].\nAlthough deep learning algorithms perform fairly well in close-set classification tasks, such as distinguishing anatomical landmarks and well-studied abnormalities, they fail to do so in open-set classification tasks, such as detecting rare and novel anomalies. These models do not account for rare diseases and unseen examples of abnormalities properly, with sparse data examples to train on leading to over-confident prediction of unknown cases questioning their overall trustworthiness and reliability [8].\nIn existing methods, abnormality detection in the GI tract has been modeled as a closed-set classification problem and can recognize only a known abnormality [9], [10]. In some scenarios, the search for abnormalities in gastrointestinal vision is framed as a detection problem. Different cases of abnormalities are categorized as objects on which the model is trained to learn features and make predictions on unseen images [11]. These classification and detection-based models are trained on a fixed set of seen abnormalities, and thus, the expectation of discovering and flagging rare, unseen, or emerging diseases, which pertain more to novelty detection, is not feasible with these models. Furthermore, the resources required to annotate different types of abnormalities, including rare cases, are challenging.\nFor robust identification of abnormalities in gastrointestinal images and to make this process reliable, we formulate the process as an out-of-distribution detection (OOD) problem. OOD is a popular problem in computer vision [8], [12] and natural language processing [13]. However, it has received little attention in the medical domain, limited to magnetic resonance imaging and computed tomography [14]\u2013[16], ultrasounds [17]. OOD enables us to identify the close-set categories and flag any example different from the close-set examples, also called in-domain (ID) examples. This characterization is extremely useful for making AI algorithms reliable, rather than making unnecessary wrong predictions on unseen and rare examples, and providing opportunities for human intervention. Moreover, such features are essential in the medical domain where a single mistake in the algorithm can cost someone's life. By integrating clinicians in the loop for the cases where an algorithm is uncertain in its decisions, we can move towards trustworthiness in the practical application of these algorithms.\nIn this paper, we tackle the challenge of developing reliable learning algorithms for gastrointestinal vision by introducing a novel OOD detection framework. In our study, ID examples are healthy anatomical landmarks of the GI tract such as Z- line, Cecum, and Pylorus, as shown in Figure 2 (top). In contrast, OOD examples correspond to any abnormalities in these landmarks, as illustrated in Figure 2 (bottom). We take a supervised learning approach to classify normal anatomical landmarks in gastrointestinal endoscopic images while simultaneously detecting any abnormalities as OOD cases.\nTowards this, we combine the information about the proximity of an image in feature space to the nearest ID class centroid and information about how far it is from non-nearest ID class centroids. Without loss of generality, the minimization of loss function for classification of ID examples minimize intra-class distance while maximizing inter-class distance, making the representations of images belonging to a class more compact. In contrast, the model is suboptimal for OOD examples, causing them to be more scattered in the feature space. This forms the basis of our proposed OOD detection method, the Nearest Centroid Distance Deficit (NCDD) score, which outperforms existing methods for the OOD detection tasks in the gastrointestinal domain.\nOur specific contributions can be summarized as follows:\n\u2022 We formulate abnormality detection as an OOD problem for models trained on the classification of normal anatomical landmarks, allowing us to detect the presence of pathologies in endoscopic images without the need for the model to be explicitly trained on it.\n\u2022 We introduce a novel distance-based OOD detection method, NCDD, that utilizes the information from the nearest-class cluster and the non-nearest-class cluster to distinguish ID from OOD.\n\u2022 Our method is easy-to-use, and post-hoc method. This means it's straightforward to implement and can be applied across various model architectures.\nOur method is evaluated on two different gastrointestinal datasets and four separate classifier backbones, ResNet-18 [19], ViT [20], DEiT [21] and MLP-Mixer [21], demonstrating its superior performance compared to existing methods."}, {"title": "II. LITERATURE REVIEW", "content": "Deep latent spaces and embedding clustering-based methods have been used for some time for out-of-distribution (OOD) detection. Dinari et al. [22] trained a conditional variational model with Kullback-Leibler loss, a triplet loss, and a distancing loss that pushes classes away from each other to improve class separation in the latent space. During inference, the class- dependent log-likelihood values of a deep feature ensemble of the test point are also weighted based on reconstruction errors, further improving the decision rule. Another approach [23] focused on multi-label datasets, utilizing B-VAEs trained on partitioned data to identify sensitive latent variables for detecting OOD samples concerning specific generative factors. Sinhamahapatra et al. [24] investigated clustering in the embedding space by applying k-means/Gaussian Mixture Model (GMM) clustering on the learned embeddings, utilizing distance metrics and probability scores derived from cluster distributions to perform OOD detection. Apart from deep latent spaces and clustering-based methods, many logits-based [25]\u2013[27], probability-based [28], gradient-based [29], and feature space-based [30] approaches have been popular in the field of OOD detection literature and have also proved promising. Hendricks et al. [26] proposed MSP, using maximum predicted softmax probability as the ID score. The Entropy method [28] sums over the entropy of predicted probabilities. The Maximum Logit method [27], also by Hendricks et al., is effective even in multi-class settings. The Energy score [25] uses the logsumexp of the logits to distinguish ID from OOD. Odin [29] uses probability and gradient information to generate an OOD score.\nThe non-parametric KNN-OOD method [30] uses the distance between a sample image in feature space and the k-th nearest neighbour from the training set to determine ID and OOD samples. More recently, methods such as BLOOD [31] have looked into the gradient information within separate layers of the network to separate OOD from ID data. Another recent OOD method, Neco [32], exploits the geometric properties of neural collapse and the principal component spaces for OOD detection. In making OOD detection fast as well as effective, Liu. et al. proposed FDBD [33], wherein they utilize the rich feature information to generate a decision boundary with the help of which they can mark data points sufficiently away from the boundary as OOD. Although these methods are used in generalized real-world settings of computer vision, are not studied in medical scenarios, and fewer have been tested in gastrovision.\nIn medical deep learning, out-of-distribution (OOD) detection has attracted much attention recently in detecting abnormality as OOD [16], [17] rather than as a class. Arnau et al. [34] showed that self-supervised learning in capsule endoscopy could help to discern abnormalities and healthy sections of the abdomen, but there are no explorations on the supervised front for gastrointestinal OOD. COOpD [16] formulated abnormalities as OOD to separate homogeneous healthy regions from heterogeneous pathological images of chest X-rays. Mehta et al. [35] proposed a mixup strategy to correct the long-tail data problem in realistic settings of a skin lesion for OOD detection. The need for OOD algorithms has been made evident by various cases of strong and well-trained models failing in out-of-the-wild examples, which often have to be dealt with in real-world scenarios [12]. The importance of being able to flag OODs in medical scenarios is especially crucial as they support experts in detecting incidental findings that could otherwise be overlooked [14].\nDespite this, there is a lack of study on how well existing OOD scoring methods perform on gastrointestinal images, especially when trained only on anatomical landmarks and normal findings, and whether these models can be used to identify abnormalities. To this end, our method reinforces trustworthiness in gastrointestinal vision through a novel NCDD method, which utilises non-nearest class centroid information from the conventional classification method to effectively discern abnormalities as OOD."}, {"title": "III. METHOD", "content": "A. Preliminaries: Out-of-Distribution Detection\n1) Setup: We consider supervised multi-class classification, where $\\mathcal{X}$ is the input space, $\\mathcal{Y} = \\{1,2,...,C\\}$ is the label space and $P_{xy}$ is the distribution over $\\mathcal{X} \\times \\mathcal{Y}$. Let $P_{in}$ be the marginal distribution of $\\mathcal{X}$ which is comprised of the healthy person data (In-Distribution).\n2) Supervised Learning: For supervised learning, a neural network $f: \\mathcal{X} \\rightarrow \\mathbb{R}^{|\\mathcal{Y}|}$, is trained on i.i.d samples extracted from $P_{x,y}$, to minimize the loss function $\\mathcal{L}$ over the input dataset. Here, $f: \\mathcal{X} \\rightarrow \\mathbb{R}^{|\\mathcal{Y}|}$ is the classification predicted for input $x$ by the model $f$. The model $f$ consists of encoder $\\varphi$ and a fully connected layer FC. Feature vector $z: \\varphi(x)$ is produced from the encoder $\\varphi$.\n3) OOD Detection: At test time, the trained network can be presented input from a distribution $P_{out}$, quite different from $P_{in}$. The main objective of the out-of-distribution detection is to differentiate whether the sample belongs to $P_{in}$ or $P_{out}$. Our work reformulates out-of-distribution detection as binary classification problem where a decision function $g(x)$ is defined by the scores produced by the scoring function $SC(x)$ and a threshold $\\lambda$, as given by:\n$g(x) = \\begin{cases} OOD, & \\text{if score} < \\lambda \\\\ ID, & \\text{otherwise} \\end{cases}$  (1)\nThe threshold $\\lambda$ is usually chosen to have a true positive rate of 95% over the input dataset. Figure 3 compares our method with the vanilla classification method.\nGiven that our problem is based on a classification problem, as illustrated in Figure 3, we employ the cross-entropy loss function to learn the model parameters. The cross-entropy loss function is defined as:\n$\\mathcal{L} = - \\sum_{i=1}^{C} y_i \\log(\\hat{y_i})$ (2)\nwhere:\n\u2022 $C$ is the number of classes\n\u2022 $y_i$ is the true probability of class i (usually 0 or 1 for hard labels)\n\u2022 $\\hat{y_i}$ is the predicted probability of class i\nWhile our approach uses cross-entropy loss, it can be adapted to other objective functions suitable for close-set classification problems. Cross-entropy loss influences the feature space by enabling the model to assign high probabilities ($\\hat{y_i}$ close to 1) to the correct class ($y_i$) and low probabilities to incorrect classes during training. As the loss minimizes, the feature space evolves, pushing examples from different ID classes further apart, creating distinct clusters, and separating their corresponding centroids. Simultaneously, the data points within the same classes are compressed, resulting in tighter clusters around their respective centroids [36], which is depicted in the embedding space in Figure III. Based on this clustering behaviour induced by cross-entropy loss, we propose an out-of-distribution (OOD) detection method that examines how OOD data positions itself relative to these class centroids in feature space.\nYet another recent study on natural images [30] has shown that in-distribution (ID) data cluster around their respective class centroids, while out-of-distribution (OOD) data, when mapped onto the same feature space, generally remain distant from any specific ID centroid, reflecting its characteristics different from ID classes. As a result, an ID test sample exhibits a smaller nearest-neighbour distance to its closest cluster compared to an OOD test example. However, this behaviour does not hold in the medical domain, where abnormalities labeled as OOD can exhibit characteristics similar to ID samples, making it a near-OOD detection case as we can see in a t-SNE plot of ID/OOD embeddings shown in Figure 4. Thus, we argue that considering only the distance to the nearest centroid is not sufficient, and hence, we propose a scoring method that considers not only the distance of a test sample to the nearest cluster centroid but also the sum of its distances to other centroids, capturing a more nuanced representation of near-OOD instances.\nOOD samples tend to have similar distances to their nearest ($D_{o \\mu n}$) and non-nearest centroids ($D_{o1}, D_{o2}$) i.e. $D_{o \\mu n} \\sim D_{o1} \\sim D_{o2}$. This is because OOD samples do not align strongly with any specific class clusters as they are unseen during training. In contrast, ID samples are naturally closer to their nearest cluster centroid ($D_{i \\mu n}$) than to non-nearest centroids ($D_{i1}, D_{i2}$) i.e. $D_{i \\mu n} < D_{i1} \\sim D_{i2}$. This behaviour reflects the clustering effect created by training.\nAdditionally, due to the intra-class attraction induced by cross-entropy ID samples exhibit smaller distances to their nearest centroid compared to OOD samples i.e. ($D_{i \\mu n} < D_{o \\mu n}$). At the same time, ID samples show larger distances to non-nearest centroids again attributed to the inter-class repulsion effect of the same loss function. OOD samples, being more dispersed around feature space, have a relatively smaller combined distance to non-nearest centroids ($D_{o1} + D_{o2}$). Figure 4 illustrates these patterns showing that the distances to non-nearest centroids for OOD samples ($D_{o1} + D_{o2}$) smaller compared to ID samples ($D_{i1} + D_{i2}$). These findings are consistent with the ablation study on distances seen in ID and OOD data presented in Table VI."}, {"title": "B. Nearest Centroid Distance Deficit", "content": "Based on our observation mentioned earlier, we propose a novel scoring method, Nearest Centroid Distance Difference (NCDD), to enhance the robustness of OOD detection. Our pipeline is illustrated in Figure 3 (b) taking the reference of Kvasir [18] dataset with three different normal anatomical landmarks as ID categories, and any type of anomalies as OOD. Following feature extraction, our approach comprises three key steps: (1) class-specific Centroid estimation of ID examples, (2) NCDD score calculation, and (3) OOD detection decision. We discuss these steps in detail in the following sections.\n1) Centroid estimation: To compute a class-specific centroid, we average the feature representations of examples belonging to a class, as defined by the following equation:\n$\\text{Class-wise centroid : } \\mu_c = \\frac{1}{N_c} \\sum_{i=1}^{N_c} z_i$,  (3)\nwhere $N_c$ is the total number of samples belonging to class $c$, and $z$ represents the feature representation of the $i$-th sample in class $c$ within the training dataset.\n2) NCDD: Given a test image $x$, we obtain its feature representation as $z$. Then, the Euclidean distance to class centroids is calculated as $D_c = ||\\mu_c - z||_2$. We then define the distance of a test sample to its nearest cluster centroid as the nearest centroid distance $D_{\\mu n}$ i.e\n$D_{\\mu n} = \\underset{c}{\\text{argmin}}D_c$ (4)\nand the sum of its distances to all other centroids as the non-nearest distance $D_{\\mu m}$ i.e.\n$D_{\\mu m} = \\sum_{c \\neq \\underset{c}{\\text{argmin}}D_c} D_c$ (5)\nFinally, our proposed OOD score for a test image is computed as the sum of its distances to non-nearest centroids, minus the distance to the nearest neighbour,\n$NCDD = \\alpha \\cdot D_{\\mu m} - \\beta \\cdot D_{\\mu n}$, (6)\nwhere $\\alpha$ and $\\beta$ represent the weight for each distance measure and $D_{\\mu m}$ as defined in 5. Motivated by the work of Zihan Zhang et al. [37], which demonstrated that log of L1-norm of the penultimate layer feature, i.e., $||z||_1 = \\sum_{i=1}^{n} |z_i|$, captures the notion of OOD detection, we incorporate this into the weight terms $\\alpha$ and $\\beta$ further to enhance the effectiveness of our proposed OOD score. Thus, $\\alpha$ and $\\beta$ are defined as:\n$\\alpha = \\log(\\frac{100}{||Z||_1}) \text{ and } \\beta = \\log(\\frac{100}{\\sum_{i=1}^{n} |z_i|})$ (7)\nwhere $\\alpha_1$ and $\\alpha_2$ serve as hyperparameters, tuned during test time and based on the unique attributes of the datasets. By incorporating the logarithm, the weights $\\alpha$ and $\\beta$ become more sensitive to changes in the magnitude of the feature vector of individual OOD data, further scaling the importance of $D_{\\mu m}$ and $D_{\\mu n}$ accordingly.\nThe decision for whether a test image is OOD or not can determined based on NCDD score. If the score is smaller than the threshold, estimated by cross-validation, it is OOD or ID example. Algorithm 1 shows the pseudocode to estimate NCDD."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets and Implementation Details\nTo test our method in real-world settings, we employed two multi-class endoscopy datasets designed for Gastrointestinal Disease Detection: Kvasir [18] and Gastrovision [38]. These datasets include images from normal and anatomical findings and the most common abnormalities occurring in various upper and lower GI tract regions. We preprocessed this dataset into an OOD setting [34] and designated the classes related to normal findings and anatomical landmarks as in-distribution (ID) classes while segregating the remaining categories as out-of-distribution (OOD) classes.\n1) Datasets: The Kvasirv2 [18] dataset comprises 8 categories depicting anatomical landmarks, pathological findings, or endoscopic procedures within the gastrointestinal (GI) tract with 1000 images for each class. The anatomical landmarks in the Kvasirv2 dataset encompass the Z-line, Pylorus, and Cecum. At the same time, pathological findings include Esophagitis(ESO), Polyps(POL), and Ulcerative colitis(UC) and images related to polyp removal, namely the \u201cdyed and lifted polyp\u201d (DLP) class and the \u201cdyed resection margins\u201d(DRM). We designated the classes derived from anatomical landmarks in the Kvasirv2 dataset as in-distribution (ID) classes, treating the remaining classes as out-of-distribution representing unhealthy cases. We randomly selected 2400 in- distribution images for model training and employed 600 in-distribution images, combined with 5000 out-of-distribution images, to assess the final model's performance in OOD detection.\nThe Gastrovision [38] dataset comprises a total of twenty-seven categories, featuring 8,010 images obtained from examinations of both the upper and lower gastrointestinal (GI) tracts. The dataset is categorized into normal findings, anatomical landmarks(11 classes) and pathological findings, therapeutic interventions(16 classes). To facilitate model training, we randomly selected 3804 in-distribution images(normal findings and anatomical landmarks), utilizing a combination of 955 ID images and 3241 OOD images to comprehensively evaluate the final model's performance in OOD detection.\n2) Implementation Details: We trained several wide ranges of models, including ResNet-18 [19], ViT-Small [20], DeiT-base [21], and MLP-Mixer-small [39], on both the Kvasirv2 and GastroVision datasets. For the Kvasirv2 dataset, Resnet- 18, ViT-Small, DeiT-base and MLP-mixer-small models were trained for 20, 20, 50 and 50 epochs, respectively. And for the Gastrovision dataset, Resnet-18, ViT-Small, DeiT-base and MLP-mixer-small models were trained for 20, 50, 50 and 50 epochs, respectively.\nIn all experiments, we used a batch size of 32 and the Adam optimizer, starting with an initial learning rate of 1\u00d710\u22124. All models were initialized with ImageNet pre-trained weights and employed the standard cross-entropy loss function. The input images were resized to 224x224 pixels.\nThe models were trained and tested using PyTorch v2.1.0 on an NVIDIA A100 GPU. To tune the hyperparameters \u03b11 and \u03b12, we created synthetic validation OOD data. This OOD data is curated by randomly selecting a rectangle region in the image replacing the region with random values in ID validation data and then further adding speckle noise following Hendrycks et al. [40]."}, {"title": "V. RESULTS", "content": "We assessed the efficacy of our method's OOD detection in gastrointestinal settings against established state-of-the-art OOD detection methods from existing literature, logit-based methods MSP [26], ODIN [29], Energy [25], Entropy [28], MaxLogit [27], and feature-based methods KNN [30]. The model's performance for OOD evaluation was optimal for the respective downstream task, Table.I.\nA. Quantative Results\n1) Metrics: AUC and FPR95 measures were used to quantify the performance of our OOD detection method. AUC measures the area under the receiver operating characteristic curve, with higher values indicating better performance. FPR95 represents the false positive rate when the true positive rate is 95%, with smaller values indicating better performance. Low FPR95 is essential in the OOD detection task as it reduces the false positive rate, ultimately enhancing the trustworthiness of the model.\nResults in Table II, compares our method with existing SOTA methods on Resnet18 with feature dimension 512. The generalizability and model-agnostic nature of our method is supported by the fact that NCDD consistently outperforms established OOD methods in both AUC and FPR95 scores across various model architectures like ViT (table III), MLP-Mixer (table V) and DEiT (table IV).\nAlso, from Figure 6, we can see that our method increases the distinction capacity of the model on individual OOD classes, i.e. on separate cases of abnormalities. The Esophagitis OOD class shares significant feature overlap with the Normal Z-line class in training (ID) data, leading to a decline in the performance of feature-based OOD detection methods when it comes to identifying Esophagitis. In cases where the distinction is noticeable, and the model can capture appropriate feature information, we can see significant improvement in OOD detection by NCDD over other methods. Its consistent performance across different architectures and diverse datasets makes it promising for OOD detection in endoscopic images, enhancing the trustworthiness of an AI-assisted procedure in diagnosis.\nB. Qualitative Results\nThe qualitative performance comparison of our proposed method, NCDD, with popular OOD methods like KNN- OOD, Neco, and FDBD, as presented in Figure 5, clearly demonstrates the superior robustness of NCDD across various OOD categories. Our method consistently outperforms other methods, particularly in challenging scenarios where existing methods struggle.\nFor instance, in the case of severe OOD examples like Polyps (POL), which pose significant classification challenges, NCDD exhibits remarkable accuracy. While methods like KNN-OOD and Neco often misclassify these complex cases as in-distribution, our method successfully identifies these samples as OOD.\nC. Ablation Studies\nAblation experiments were conducted to investigate the impact of various distance terms on our out-of-distribution (OOD) score performance. The results presented in Table VI and VII demonstrate that while utilizing only the nearest centroid distance yields better performance than considering both nearest centroid distance and non-nearest centroid distance, achieving optimal performance requires appropriately scaling both terms."}, {"title": "VI. CONCLUSION", "content": "We reformulate abnormality detection in gastrointestinal images as an OOD problem to enable the recognition of abnormalities in the GI tract through a model trained only on abundant normal anatomical findings data. We introduced a novel distance-based OOD detection method, Nearest Centroid Distant Deficit (NCDD), which utilizes the feature space distance of ID data in multi-class gastrointestinal image datasets and its relative positioning in feature space to discern them from OOD samples. NCDD outperforms the previous state- of-the-art OOD detection methods in gastrointestinal vision tested in two datasets on AUC and FPR95 metrics over four model architectures.\nOverall, we demonstrate tremendous potential for supervised models on OOD detection as a general-purpose tool for flagging abnormalities in endoscopy images from gastrointestinal settings, an unexplored area in medical imaging research. The clinical applicability of this method is most promising in the current medical context as it is relatively easy to integrate and also allows for expert intervention, unlike fully autonomous AI methods."}]}