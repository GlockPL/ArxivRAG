{"title": "Transformer-based Graph Neural Networks for Battery Range Prediction in AIoT Battery-Swap Services", "authors": ["Zhao Li", "Yang Liu", "Chuan Zhou", "Xuanwu Liu", "Xuming Pan", "Buqing Cao", "Xindong Wu"], "abstract": "The concept of the sharing economy has gained broad recognition, and within this context, Sharing E-Bike Battery (SEB) have emerged as a focal point of societal interest. Despite the popularity, a notable discrepancy remains between user expectations regarding the remaining battery range of SEBS and the reality, leading to a pronounced inclination among users to find an available SEB during emergency situations. In response to this challenge, the integration of Artificial Intelligence of Things (AIoT) and battery-swap services has surfaced as a viable solution. In this paper, we propose a novel structural Transformer-based model, referred to as the SEB-Transformer, designed specifically for predicting the battery range of SEBs. The scenario is conceptualized as a dynamic heterogeneous graph that encapsulates the interactions between users and bicycles, providing a comprehensive framework for analysis. Furthermore, we incorporate the graph structure into the SEB-Transformer to facilitate the estimation of the remaining e-bike battery range, in conjunction with mean structural similarity, enhancing the prediction accuracy. By employing the predictions made by our model, we are able to dynamically adjust the optimal cycling routes for users in real-time, while also considering the strategic locations of charging stations, thereby optimizing the user experience. Empirically our results on real-world datasets demonstrate the superiority of our model against nine competitive baselines. These innovations, powered by AIoT, not only bridge the gap between user expectations and the physical limitations of battery range but also significantly improve the operational efficiency and sustainability of SEB services. Through these advancements, the shared electric bicycle ecosystem is evolving, making strides towards a more reliable, user-friendly, and sustainable mode of transportation.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of the sharing economy has become ubiquitous across diverse facets of modern society [1]-[3], spotlighting Sharing E-Bike Battery (SEB) [4] as a focal point of attention. Furthermore, the sharing battery assumes a central role in numerous intelligent systems, particularly in the context of AIoT with battery-swap services. As an illustration, in e-bike battery-swap services, range prediction functions as the primary propulsion source, delivering the necessary driving force for the entire vehicle's operation. The performance of SEB plays a pivotal role in determining critical factors, including driving range [5], fuel efficiency [6], and safety standards [7] within the realm of electric vehicles. Additionally, a noticeable disparity persists between user expectations and the actual remaining range of SEBs. This discrepancy is particularly evident when users urgently require access to available SEBs, a situation that carries significant implications for alleviating range-related concerns during their journeys. The provision of accurate range prediction and strategically optimal placement of exchange stations can greatly aid users in their route planning endeavors [8]-[10], thereby enhancing operational efficiency and promoting energy conservation initiatives.\nRange anxiety of riders need to swap batteries primarily due to insufficient battery range, as e-bikes can only travel a limited distance on a single charge. This necessitates swapping when the battery depletes. Additionally, long charging times contribute to the need for swapping, as waiting for a battery to recharge is often impractical, especially for riders using e-bikes for delivery services or commuting. Battery degradation over time also means batteries hold less charge, requiring more frequent swaps. Operational efficiency, particularly for commercial use, favors swapping over recharging to ensure continuous operation. Convenience plays a significant role, as swapping stations offer a quick solution compared to the lengthy recharging process. In areas with inadequate charging infrastructure, swapping becomes a practical necessity. Lastly, range anxiety\u2014the fear of running out of power without access to charging\u2014prompts riders to prefer battery swapping to ensure their e-bikes are always ready for use. This study focuses primarily on this aspect, with a specific emphasis on the refinement and enhancement of range prediction accuracy."}, {"title": "II. RELATED WORKS", "content": "This section provides an overview of related works, including models utilized in SEB-Transformer, such as GNNs and Transformer. Additionally, we will discuss the connection to our work.\na) GNNs and Transformer: The fusion of graph neural networks (GNNs) [19] and Transformers marks a pivotal development in machine learning, with GNNs excelling in graph data analysis and Transformers advancing sequence tasks. Prior to Transformers [20]\u2013[23], RNNs, including LSTMS [24] and GRUs [21], dominated sequence processing but fell short in distributed computing. The advent of the attention mechanism, epitomized by Google's BERT [25], revolutionized NLP by enhancing focus on relevant data segments during processing. This synergy between GNNs for local structure and Transformers for global dependencies is now under active investigation, aiming to unify their strengths for improved performance across tasks.\nb) Transformer for prediction task: Deep neural networks have increasingly become the foundational framework for prediction tasks [26], paralleled by the substantial development of the Transformer model. Transformer model is optimally utilized in scenarios where it can simultaneously generate predictions for interrelated tasks. Research into employing Transformers for prediction tasks has become a critical focus within the machine learning domain. Originally designed for natural language processing tasks [20], [21], Transformers have showcased exceptional ability in identifying complex sequential patterns and managing long-range dependencies. Expanding the use of Transformers [26]\u2013[28] beyond traditional sequence-based applications to include a variety of domains like time-series forecasting, image classification, and financial prediction has broadened their applicability. This study delves into the use of Transformers in predictive modeling, highlighting their strengths in deciphering complex data relationships.\nc) Transformer for combinatorial task: Beyond the widespread adoption of the simulated annealing technique for a range of combinatorial optimization challenges [29]- [31], recent scholarly efforts have explored the integration of RNNs in this sphere. Although RNNs have not uniformly achieved error rates as minimal as those attained through annealing methods, the findings have been promising, showcasing significant improvements in solution speed. This suggests a strategic equilibrium between precision and high efficiency, a compromise that might be particularly appealing to stakeholders like Google Maps users and crew schedulers seeking to refine their scheduling processes. The exploration of Transformer models for combinatorial tasks has become a focal point in contemporary machine learning research. Initially conceived for natural language processing tasks, Transformers have demonstrated exceptional proficiency in identifying complex dependencies and patterns, prompting their adoption in a variety of fields, such as combinatorial optimization challenges."}, {"title": "III. PRELIMINARIES", "content": "Notation. We denote a graph as G and its edges and nodes as E and V respectively. We represent an ordinary graph as a set of edges $E = \\{(V_i, V_j) | V_i, V_j \\in V\\}$, where n is the number of observed edges. For each node v and edge e, we use their bold version v and e to denote their embeddings. We use bold capital letters. e.g., A, B, W to denote matrices and use $|| ||$ to denote the Euclidean norm of vectors or the Frobenius norm of matrices. Due to space limitations, we summarize this paper's main symbols and notations in Table I.\nA. Scenario of SEBS\nThe scenario involving Sharing E-Bike Battery (SEB) with shared battery presents a complex landscape, requiring a detailed representation of the dynamic interactions between riders and e-bike batteries within the framework of a temporal graph. When users replace batteries, it becomes feasible for the same battery to appear at multiple exchange stations, thus enabling its reuse by a succession of different users across the system. In order to accurately capture the system's diverse characteristics, we utilize the notation HG = V, E, representing the heterogeneous graph input that encompasses the intricate relationships within the SEB ecosystem. Within this framework, the set of nodes is comprehensive, including both user nodes and battery nodes, which are defined as $V = V_U \\cup V_B$, illustrating the system's dual aspects of interaction and utility. By interlinking users and batteries through edges, the structure of HG naturally conforms to the characteristics of a bipartite graph, reflecting the two distinct but interconnected groups within the system. For clarity and further understanding, we present a simplified illustration of the SEB system in Fig.2, which visualizes the conceptual framework discussed.\nB. Graph Neural Networks\nMost Graph Neural Networks (GNNs) [19] conform to the messing passing between neighboring nodes, which can be described as the following iteration functions:\n$m_{v}^{l+1} = f_o(h_{v}^{l}, \\{h_{u}^{l} | u \\in N_{v} \\})$\n$h_{v}^{l+1} = o'(g'(h_{v}^{l}, m_{v}^{l+1}))$,\nwhere $f_o, o', g'$ denote parametric functions, i.e., neighborhood aggregation function, activation function (e.g. sigmoid, ReLU), and combination function (e.g. summation, mean), at the l-layer messaging passing on graphs. $N_{v}$ denotes the neighborhood for node v and $h_{v}^{l}$ denotes the hidden embedding for v. Such message passing in Eq.1 will repeat L times ($l \\in \\{1, 2, ..., L\\}$) until converge. For the combinatorial optimization task in this paper, the information will only pass across the entire graph. Taking GCN for example, the message-passing function in graph convolutional network [19] is explicitly written as:\n$h^{l+1} = \\sigma ( \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{l}W^{l} + B^{l}h^{l})$\nwhere $W^{l}$ and $B^{l}$ are learnable parameters in l-layer.\nC. Transformer\nTransformer [21], [35] signifies a notable achievement in the realm of natural language processing (NLP) and machine learning. Departing from traditional recurrent or convolutional architectures, the Transformer leverages a self-attention mechanism to capture contextual dependencies across input sequences, enabling it to excel in tasks such as language translation, summarization, and question-answering. The model's innovative architecture eliminates sequential dependencies, allowing for parallelization during training and significantly accelerating processing times.\nFor a given input vector $X = [X_1,X_2, ..., X_N] \\in R^{N \\times D}$, where $x_i \\in R^{D}$, transform embeds X into the output H in the following two steps:\nStep 1: We project X into three matrices: Q (query matrix), K (key matrix), V (value matrix) via linear transformation as follows:\n$Q = XW^{Q}, K = XW^{K}, V = XW^{V}$\n$Q = [q(1), q(2), ..., q(N)]$\n$K = [k(1), k(2),...,k(N)]$\n$V = [v(1), v(2), ..., v(N)]$\nwhere $W^{Q}, W^{K} \\in R^{D_{qk} \\times D}$ and $W^{v} \\in R^{D \\times D}$.\nStep 2: The output vector $H = [h(1), h(2), ...,h(N)] \\in R^{N \\times D_{qk}}$ is then computed as follows:\n$H = Softmax(\\frac{Q K^{T}}{\\sqrt{D_{qk}}})V$."}, {"title": "IV. FRAMEWORK", "content": "A. Dynamic SEB scenario\nThe scenario of sharing e-bike battery is complex and we describe the interaction between users and batteries as a temporary graph at time t. When users replace the battery, the same battery may appear at different exchange stations and subsequently be utilized by different users. We use $HG^{(t)} = \\{V^{(t)}, E^{(t)}\\}$ to represent the input heterogeneous graph at time t. The set of nodes includes user nodes and battery nodes as $V^{(t)} = v_{u}^{(t)} \\cup v_{b}^{(t)}$. Thus, graph neural networks (see Section III-B) can readily embed $HG^{(t)}$. Edges are connected by users and batteries, and then $HG^{(t)}$ can be described as a bipartite graph.\nB. SEB-Transformer\nHere, we introduce the details of SEB-Transformer. Initially, the dataset is bifurcated into two classifications: the first comprises standard time series Euclidean data, while the second encompasses non-Euclidean data representing the temporal dynamic graph (see Section IV-A). Then we use SEB-Transformer to update the features on edges and nodes. We update the embeddings of edge features by GNNs as Eq.3. Also, we use a Transformer to update the original features to predict the range of e-bike battery. The above process can be formulated as follows:\n$X_v^{(t)} = GNN(v_u^{(t)}, v_b^{(t)}, HG^{(t)})$\n$X_v^{(t)} = Transformer (v_u^{(t)}, v_b^{(t)}, x_v^{(t)})$\nThen, we can obtain the predicted labels as:\n$\\hat{Y}^{(t)} = MLP (X_v^{(t)})$\nwhere $X_v^{(t)} = x_u^{(t)} \\oplus x_b^{(t)}$.\nC. S\u00b3 IM\nWe give a genetic form for the strong structural similarity index on graph S\u00b3IM: $R^{n} \\times R^{n} \\rightarrow R$ as follows:\n$S^{3}IM(x,y) = f(r_{1}(x,y), r_{2}(x, y), r_{3}(x, y))$\n$f(r_1, r_2, r_3) = [r_1]^{\\alpha} [r_2]^{\\beta} [r_3]^{\\gamma}$\nwhere $\\alpha, \\beta, \\gamma$ are parameters used to adjust the relative importance of the three terms.\nThe S\u00b3IM term contains three components which will be presented in the next subsection with details. As the previous paper [36] says, the similarity measure should obey the following conditions:\nNext, we will introduce the details of each component $r_{1,2,3}$ shown in Eq.12 and we call them (1) luminance term, (2) contrast term, and (3) structure term, respectively.\n(1) luminance term $r_1$. As the first term, we can define the luminance as follows\n$r_{1}(x,y) = \\frac{2\\mu_x\\mu_y + C_1}{\\mu_x^2 + \\mu_y^2 + C_1}$\nwhere $C_1 > 0$ is a constant to avoid the denominator being zero. Usually, we choose $C_1 = (K_1L)^2$, L is the length of the output and $K_1 \\ll 1$ is a pretty small constant.\n(2) contrast term $r_2$. The second term, we can define the contrast function as\n$r_2(x,y) = \\frac{2\\sigma_x \\sigma_y + C_2}{\\sigma_x^2 + \\sigma_y^2 + C_2}$\nwhere $C_2 = (K_2L)^2$ and $K_2 \\ll 1$.\n(3) structure term $r_3$. The third term, we can refer to the following form\n$r_3(x, y) = \\frac{\\sigma_{xy} + C_3}{\\sigma_x\\sigma_y + C_3}$\nwhere\n$\\sigma_{xy} = \\frac{1}{n-1} \\sum_{i=1}^{n}(x_{i} - \\mu_{x})(y_{i} - \\mu_{y})$\nOverview. The primary aim of this section is to furnish a comprehensive overview of the SEB-Transformer, delineating its core principles and operational framework. Prior to embarking on the prediction of battery range, we precisely define the dynamic scenario of Sharing E-Bike Battery (SEB) as a heterogeneous graph, which effectively captures the intricate web of interactions occurring between users and batteries. Following this, we proceed to develop a structural Transformer model that meticulously incorporates topological information, achieving a seamless integration of both graph-based and sequential data, thereby enriching the model's predictive capabilities. Furthermore, to augment the model's ability to recognize global structural information, we implement the advanced regularization technique S\u00b3IM, which significantly improves the"}, {"title": "V. EXPERIMENTS", "content": "This section provides a detailed exposition of our experiments. Section V-A presents a real-world dataset collected by our team, accompanied by an in-depth analysis of its statistical information from various perspectives. Section V-B outlines several competitive baselines that serve as benchmarks for comparison, aiming to surpass their performance. Section V-C highlights the main results assessed by our model.\nA. Dataset\nFor our experimental investigations, we utilize a real-world dataset that has been meticulously collected by our team. Our dedicated engineering team meticulously curated a dataset comprising 16,000 order samples, each enriched with 64 distinct timing data points. Subsequently, we provide an overview of statistical information pertaining to our dataset. In Fig.3, we provided visualizations depicting sequence length information. Observation of the dataset reveals that it approximates a normal distribution, with an average value of 275, thereby suggesting that the data we gathered is both reasonable and reflective of societal dynamics.\nB. Baseline\nWithin this section, we present an overview of the competitive baseline methods utilized in our study, namely SVR, LR, XGBoost, MLP, and the vanilla Transformer model. These methods are implemented for our task.\nC. Main Results\nThe experimental results have been succinctly summarized in Table II. The last two models represent variants of the SEB-Transformer architecture. An additional regularization term, denoted as S\u00b3IM, has been incorporated into the SEB-Transformer* architecture. The aforementioned models serve as ten competitive baselines, providing a robust benchmark for evaluation. Remarkably, our model surpasses these baselines by a substantial margin. Furthermore, Fig.6 has been included to visually illustrate the effects of each improvement. This visual representation effectively demonstrates the impact of individual improvements. Additionally, the Fig.5 showcases the best MAE achieved by our model. This presentation serves to underscore that our model consistently outperforms others even in dynamic stream conditions.\nFig.7 presents a visual depiction of the loss curve as observed throughout the training and validation phases, providing a graphical representation of the model's performance over these periods. When compared to the baseline models, our framework demonstrates a decrease in loss, indicating its"}, {"title": "VI. APPLICATION WITH WEB SERVICE", "content": "Within Sharing E-Bike Battery (SEB), the app and web service are key to user experience and system efficiency. Future improvements promise to enhance SEB functionality and accessibility, fostering wider use and greater satisfaction. By addressing these application perspectives, SEB have the opportunity to enhance user satisfaction, optimize system efficiency, and contribute to the widespread adoption of sustainable transportation solutions."}, {"title": "VII. DISCUSSION AND CONCLUSION", "content": "Why We Need Sharing E-Bike Battery. (1) Bridging Expectation Gaps: A notable discrepancy exists between user expectations and the actual remaining battery range of SEBs, especially in urgent needs for accessible SEBs. (2) Operational Efficiency and Energy Conservation: Accurate battery range prediction and strategic placement of exchange stations can significantly enhance route planning, operational efficiency, and energy conservation. (3) Promoting Sustainable Transportation: By overcoming limitations associated with traditional charging infrastructure, sharing e-bike batteries can lead to a more efficient and environmentally friendly transportation ecosystem.\nConclusion. In this paper, we have conceptualized the scenario involving Sharing E-Bike Battery (SEB) as a dynamic heterogeneous graph. Concurrently, we introduce the innovative SEB-Transformer, which is specifically designed for the purpose of battery range prediction. Furthermore, it is noteworthy that our model significantly outperforms the conventional Transformer models, indicating a substantial improvement in predictive performance. The contributions of our work represent a notable advancement within the domain of web services, characterized by a dual achievement: the reduction of carbon emissions and the enhancement of user satisfaction, thereby marking a pivotal step forward in sustainable urban mobility. We propose a novel, task-oriented model termed as SEB-Transformer, which employs a structural Transformer-based methodology specifically devised for the accurate prediction of battery range in SEBs. Utilizing the prediction, we can dynamically modify optimal cycling routes for users in real-time, taking into account the locations of charging stations. Experimental results shows that our model outperforms nine other competitive baseline models on real-world datasets, with a significant reduction in the MAE (Mean Absolute Error) metric compared to the basic Transformer model."}]}